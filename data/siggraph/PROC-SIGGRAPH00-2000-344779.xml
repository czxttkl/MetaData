<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date></start_date>
		<end_date></end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[]]></city>
		<state></state>
		<country></country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>344779</proc_id>
	<acronym>SIGGRAPH '00</acronym>
	<proc_desc>Proceedings of the 27th annual conference</proc_desc>
	<conference_number></conference_number>
	<proc_class>conference</proc_class>
	<proc_title>Computer graphics and interactive techniques</proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn>1-58113-208-5</isbn>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>2000</copyright_year>
	<publication_date>07-01-2000</publication_date>
	<pages>547</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<publisher>
		<publisher_id>PUB26</publisher_id>
		<publisher_code>ACMAD</publisher_code>
		<publisher_name>ACM Press/Addison-Wesley Publishing Co.</publisher_name>
		<publisher_address>1515 Broadway, 17th Floor               New York, NY</publisher_address>
		<publisher_city></publisher_city>
		<publisher_state></publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10036</publisher_zip_code>
		<publisher_contact>Jono</publisher_contact>
		<publisher_phone></publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url></publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node>I.3.0</cat_node>
			<descriptor/>
			<type/>
		</primary_category>
		<other_category>
			<cat_node>G.0</cat_node>
			<descriptor></descriptor>
			<type></type>
		</other_category>
		<other_category>
			<cat_node>I.4.0</cat_node>
			<descriptor></descriptor>
			<type></type>
		</other_category>
	</categories>
	<ccs2012>
		<concept>
			<concept_id>0.10010147.10010371.10010382.10010383</concept_id>
			<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Image processing</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10010147.10010178.10010224</concept_id>
			<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10002950</concept_id>
			<concept_desc>CCS->Mathematics of computing</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10010147.10010371.10010382</concept_id>
			<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10010147.10010371</concept_id>
			<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
			<concept_significance>500</concept_significance>
		</concept>
	</ccs2012>
	<general_terms>
		<gt>Algorithms</gt>
		<gt>Design</gt>
		<gt>Documentation</gt>
		<gt>Experimentation</gt>
		<gt>Human Factors</gt>
		<gt>Languages</gt>
		<gt>Management</gt>
		<gt>Measurement</gt>
		<gt>Performance</gt>
		<gt>Reliability</gt>
		<gt>Theory</gt>
		<gt>Verification</gt>
	</general_terms>
	<chair_editor>
		<ch_ed>
			<person_id>PP39089905</person_id>
			<author_profile_id><![CDATA[81310493227]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>1</seq_no>
			<first_name><![CDATA[Judith]]></first_name>
			<middle_name><![CDATA[R.]]></middle_name>
			<last_name><![CDATA[Brown]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[Coralville, IA]]></affiliation>
			<role><![CDATA[Chairman]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
		<ch_ed>
			<person_id>PP39091473</person_id>
			<author_profile_id><![CDATA[81100563035]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>2</seq_no>
			<first_name><![CDATA[Kurt]]></first_name>
			<middle_name><![CDATA[]]></middle_name>
			<last_name><![CDATA[Akeley]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[]]></affiliation>
			<role><![CDATA[Chairman]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
	</chair_editor>
</proceeding_rec>
<content>
	<article_rec>
		<article_id>344792</article_id>
		<sort_key>13</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Computer-generated pen-and-ink illustration of trees]]></title>
		<page_from>13</page_from>
		<page_to>18</page_to>
		<doi_number>10.1145/344779.344792</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344792</url>
		<abstract>
			<par><![CDATA[<p>We present a method for automatically rendering pen-and-ink illustrations of trees. A given 3-d tree model is illustrated by the tree skeleton and a visual representation of the foliage using abstract drawing primitives. Depth discontinuities are used to determine what parts of the primitives are to be drawn; a hybrid pixel-based and analytical algorithm allows us to deal efficiently with the complex geometric data. Using the proposed method we are able to generate illustrations with different drawing styles and levels of abstraction. The illustrations generated are spatial coherent, enabling us to create animations of sketched environments. Applications of our results are found in architecture, animation and landscaping.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[biological systems]]></kw>
			<kw><![CDATA[frame buffer tricks]]></kw>
			<kw><![CDATA[non-realistic rendering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010383</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Image processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP15021799</person_id>
				<author_profile_id><![CDATA[81100092246]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Deussen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universit&#228;tsplatz 2, D-39106 Magdeburg, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14041774</person_id>
				<author_profile_id><![CDATA[81100089205]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Strothotte]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Faculty of Computer Science, University of Magdeburg, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[M. Aono and T. L. Kunii. Botanical tree image generation. IEEE Computer Graphics and Applications, 4(5): 10-34, May 1984.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>351683</ref_obj_id>
				<ref_obj_pid>351631</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[O. Deussen, J. Hamel, A. Raab, S. Schlechtweg, and T. Strothotte. An illustration technique using hardware-based intersections and skeletons. In P1vceedings of Graphics Intelface 99, pages 175-182. Canadian Human-Computer Communications Society, 1999.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[L. Evans. The New Complete Illustration Guide: The Ultimate Trace File for Architects, Designers, Artists, and Students. Van Nostrand Reinhold Company, 1996.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[R.W. Floyd and L. Steinberg. An adaptive algorithm for spatial grey scale. P~vc. Soc. Inf. Display, 17:75-77, 1976.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Greenworks GbR. Home page of the xfrog modelling software, http://www. greenworks.de.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192186</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[S. Hsu and I. Lee. Drawing and animation using skeletal strokes. In SIGGRAPH '94 Conference P1vceedings, pages 109-118. ACM SIGGRAPH, July 1994.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311607</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[M. Kowalski, L. Markosian, J. D. Northrup, L. Burdev, R. Barzel, L. Holden, and J. F. Hughes. Art-based rendering of fur, grass, and trees. In SIGGRAPH '99 Conference P1vceedings. ACM SIGGRAPH, August 1999.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618590</ref_obj_id>
				<ref_obj_pid>616056</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[B. Lintermann and O. Deussen. Interactive modeling of plants. IEEE Computer Graphics and Applications, 19(1):56-65, January/February 1999.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[F. Lohan. The drawing handbook. Contemporary Books, Chicago, 1993.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258894</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[L. Markosian, M. A. Kowalski, S. J. Trychin, L. D. Bourdev, D. Goldstein, and J. F. Hughes. Real-time nonphotorealistic rendering. In T. Whitted, editor, SIG- GRAPH '97 Conference P1vceedings, pages 415-420. ACM SIGGRAPH, 1997.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[T. McReynolds and D. Blyth. Advanced graphics programming techniques using OpenGL. SIGGRAPH '98 Course Notes, ACM SIGGRAPH, 1998.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[J. R. Parker. Extracting vectors from raster images. Computers &amp; Graphics, 12(1):75-79, 1988.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>300539</ref_obj_id>
				<ref_obj_pid>300523</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[R. Raskar and M. Cohen. Image precision silhouette edges. In 1999 ACM Symposium on Interactive 3D Graphics, pages 135-140. ACM SIGGRAPH, April 1999.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325250</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[W.T. Reeves and R. Blau. Approximate and probabilistic algorithms for shading and rendering structured particle systems. In Computer Graphics (SIGGRAPH '85 P~vceedings), volume 19, pages 313-322, July 1985.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97901</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[T. Saito and T. Takahashi. Comprehensive rendering of 3-d shapes. In Computer Graphics (P1vc. SIGGRAPH 90), volume 24(4), pages 197-206. ACM SIGGRAPH, 1990.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258890</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[M. Salisbury, M. Wong, J. F. Hughes, and D. Salesin. Orientable textures for image-based pen-and-ink illustration. In SIGGRAPH '97 Conference P~vceedings. ACM SIGGRAPH, 1997.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>27491</ref_obj_id>
				<ref_obj_pid>27485</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[T.T. Sasada. Drawing natural scenery by computer graphics. Computer Aided Design, 19(4):212-218, 1987.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>238398</ref_obj_id>
				<ref_obj_pid>238386</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[J. Schumann, T. Strothotte, A. Raab, and S. Laser. Assessing the effect of nonphotorealistic images in computer-aided design. In ACM Human Factors in Computing Systems, SIGCHI '96, pages 35-41, April 13-15 1996.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808571</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[A.R. Smith. Plants, fractals and formal languages. Computer Graphics (SIG- GRAPH '84 P1vceedings), 18(3):1-10, July 1984.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15911</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[S. Strassmann. Hairy brushes. Computer Graphics (SIGGRAPH '86 P~vceedings), 20(3):225-232, 1986.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>286062</ref_obj_id>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[C. Strothotte and T. Strothotte. Seeing Between the Pixels: Pictures in Interactive Systems. Springer-Verlag, Berlin-Heidelberg-New York, 1997.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[T. Strothotte, B. Preim, A. Raab, J. Schumann, and D. R. Forsey. How to render frames and influence people. Computer Graphics Forum, 13(3):455-466, 1994.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192184</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[G. Winkenbach and D. Salesin. Computer-generated pen-and-ink illustration. In SIGGRAPH '94 Conference P1vceedings, pages 91-100. ACM SIGGRAPH, 1994.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237287</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[G. Winkenbach and D. Salesin. Rendering parametric surfaces in pen and ink. In SIGGRAPH '96 Conference P1vceedings, pages 469-476. ACM SIGGRAPH, 1996.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807443</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[C.I. Yessios. Computer drafting of stones, wood, plant and ground materials. Computer Graphics (P1vceedings of SIGGRAPH 79), 13(3):190-198, 1979.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer-Generated Pen-and-Ink Illustration of Trees Oliver Deussen* Thomas Strothotte Faculty of Computer 
Science, University of Magdeburg, Germany Abstract We present a method for automatically rendering pen-and-ink 
illus­trations of trees. A given 3-d tree model is illustrated by the tree skeleton and a visual representation 
of the foliage using abstract drawing primitives. Depth discontinuities are used to determine what parts 
of the primitives are to be drawn; a hybrid pixel-based and analytical algorithm allows us to deal ef.ciently 
with the com­plex geometric data. Using the proposed method we are able to generate illustrations with 
different drawing styles and levels of ab­straction. The illustrations generated are spatial coherent, 
enabling us to create animations of sketched environments. Applications of our results are found in architecture, 
animation and landscaping. CR Categories: I.3.3 [Picture/Image Generation]: Display algorithms [I.3.7]: 
Three-Dimensional Graphics and Realism Animation Keywords: Biological Systems, Frame Buffer Tricks, Non-Realistic 
Rendering 1 Introduction During the last years, a variety of techniques have been proposed to sketch 
and non-photorealistically render objects. Research in this area was driven by the realization that drawings 
are able to convey visual information in a different way than photorealistic images do [21]. This is 
one of the reasons why a large percentage of images in many books are drawings (cf. [22]). While the 
proposed methods allow creating line drawings of many objects and in many different styles, the illustration 
of plants has so far been neglected. This is surprising because drawings of these objects are needed 
in areas like architecture and landscaping. In both cases early designs are preferentially visualized 
as abstract line drawings that often include many trees [18]. In this paper we propose a method for automatic 
pen-and-ink il­lustration of trees. The approach allows us to create a variety of illustration styles. 
The underlying models are realistic 3-d plant geometries generated with the xfrog modeling system proposed 
by Lintermann and Deussen [8], but any other surface-oriented plant model can also be used. *Universit¨atsplatz 
2, D-39106 Magdeburg, Germany, odeussen@acm.org http://isgwww.cs.uni-magdeburg.de/ deussen Permission 
to make digital or hard copies of part or all of this work or personal or classroom use is granted without 
fee provided that copies are not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on 
servers, or to redistribute to lists, requires prior specific permission and/or a fee. SIGGRAPH 2000, 
New Orleans, LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 In comparison to the art-based illustration 
styles for trees invented by Kowalski et al. [7], we are more interested in visually represent­ing speci.c 
plants than to create generic representations. Our aim is to provide the user with a transition from 
a tree illustration with a realistic plant-speci.c look to an abstract representation consisting of only 
a few strokes. This enables the user to select a global degree of abstraction while at the same time 
enabling the system to draw plants in the background with a higher abstraction level. In combi­nation 
with different drawing styles, this helps to adapt the visual appearance of the plants to other objects 
and also, for instance, al­lows the user to focus the viewer s attention on a certain part of the scene. 
Among the various plant types and their combinations, we focus on complex trees and bushes. Collections 
of these objects are most interesting in architecture and landscaping. Also both categories require abstract 
visual representations as it is impossible to draw all the geometry in detail. 1.1 Related Work Related 
work in illustrating trees was done in the .eld of non­photorealistic rendering and also in botanical 
plant generation. Probably the .rst article with illustrated plants was presented by Yessios [25]. In 
an architectural framework he used abstract plant symbols and combined them with stones and ground materials. 
Alvy Ray Smith, one of the early authors dealing with fractals and formal plant descriptions created 
a cartoon tree with small disks representing bunches of leaves [19]. A similar representation with smaller 
disks was used by Reeves and Blau [14] to constitute their structured particle systems for rendering 
realistic trees. The idea of representing numerous botanical leaves by an abstract geometric primitive 
inspired us (like Kowalski et al. [7]) to work on pen-and­ink illustrations of trees. A line drawing 
is usually created by combining a number of brush or pencil strokes. Researchers in non-photorealistic 
rendering re­semble that process by using virtual brushes. Strassmann [20] pro­posed the path-and-stroke 
metaphor: a path is de.ned and a phys­ically simulated brush is used to generate the stroke. Hsu et al. 
[6] extended the metaphor by using general objects like textures, im­ages and recursively de.ned fractals 
that are drawn along a given path. Salisbury et al. [16] described a method for directing the strokes 
in line drawings on the basis of vector .elds. In their paper they also showed an interactively generated 
tree image. Winkenbach and Salesin [23, 24] presented a variety of methods for the automatic generation 
of pen-and-ink illustrations. In contrast to Strassmann and Hsu et al. they do not work with individual 
strokes but with artistically elaborate stroke textures. Sasada [17] presented some tree sketches in 
an architectural en­vironment. He used images of synthetic tree skeletons that were mapped onto view-facing 
polygons. The method of Aono and Ku­nii [1] was used to create the skeletons, the foliage was not visual­ized 
in their computer-generated trees. Kowalski et al. [7] generated abstract sketches of trees by using 
ge­ometric primitives like spheres for de.ning rough approximations of a tree s foliage. These primitives 
were rendered conventionally to achieve gray-scale images. In a second step the images were used to place 
graftals small objects representing leaves or hair on the surfaces by applying the difference image 
algorithm pro­posed earlier by Salisbury et at. [16]. Doing so it is possible to create sketched images 
of generic trees, bushes, and grass. In our work we start from a different point. Our models are de­tailed 
tree models consisting of a tree skeleton and leaves. Our line drawings are the result of visually combining 
many drawing primi­tives instead of placing graftal objects on some large geometries. A drawback of our 
approach is that we potentially have to deal with more input data. The solution to this problem is to 
represent a tree at several levels of detail. This makes it possible to adapt the geo­metric representation 
to what should be presented on the screen: If a more detailed drawing is to be created, a more detailed 
geometric tree description is used. The use of realistic tree models thus offers some major advantages: 
We can make use of existing tree libraries, our tree illustrations can be abstract but we are also able 
to draw a speci.c plant. If the scene is to be rendered photorealistically later, the visual representation 
does not differ much from its illustrated counterpart. Having access to the detailed 3-d data enables 
us also to animate the complex line drawings with suf.cient spatial and temporal coherency. Another advantage 
is the correct, tree-like shadow generation of our models. The main contribution of our work is an ef.cient 
way of generat­ing the illustration of realistic plant models using abstract drawing primitives; furthermore, 
we present a depth difference algorithm to determine important silhouette lines, which allows us to generate 
different levels of visual abstraction. The remainder of this paper is organized as follows: Section 
2 re­views the artistic work on illustrating trees, in Section 3 our syn­thetic illustration algorithm 
is given. Section 4 shows results, and in Section 5 we give some conclusions.  2 Traditional Illustration 
of Trees Among the various styles used by artists to render trees (for a large set of examples see [3]) 
one can distinguish between .at styles that solely represent the shape of a tree and others that also 
approximate natural light interaction (cf. [9]). The tree skeleton is usually drawn up to the second 
branching level, primarily by silhouette lines and crosshatching on the stem surface. The shape of the 
foliage is either represented by an abstract outline or by a collection of many small objects which do 
not necessarily resemble natural leaves but instead represent the characteristics of the foliage. In 
addition, the outline is sometimes drawn by many small line segments or just a few strokes. The visual 
appearance of the foliage can be divided into three areas. The top of the tree is usually in the direct 
light and is therefore visualized by only some details and its outline. In the half shadow, more details 
are drawn to achieve an appropriate gray level. In this area the outline of the leaves is often drawn 
in detail. The third area is the shaded part. The three areas are generally not found in a single illustration, 
often only the half shadow and the full shadow region is drawn. Sometimes the complete foliage is represented 
uniformly. Artists use different methods to generate shadows on the foliage: in many styles more details 
are drawn and thick lines are used, sometimes with whole areas being drawn in black. Other styles add 
crosshatching to the foliage. A method for the synthetic illustration of trees must propose solutions 
to several subproblems: First, the stem skeleton must be represented properly by silhouette lines and 
crosshatching. Second, an abstract leaf representation must be found that enables the user to represent 
different types of leaves as well as different illustration styles. Third, drawing the leaves must be 
modulated by the three areas: the leaves in the light must be represented solely by the outline of the 
foliage, leaves in the half shadow should be drawn with detailed outline or additional crosshatching, 
and regions of deep shadow are to be added appropriately. Tree I Tree II Tree III Figure 1: Photorealistically 
rendered images of the synthetic sam­ple trees: Tree I: complex tree; Tree II: young lime tree; Tree 
III: conifer. 3 Automated Illustration of Trees The .rst step to create a tree illustration is to create 
a tree with a conventional tree modeling program. As mentioned above, we use the xfrog modeling system 
[5, 8] for that purpose. The .nal model some of them are shown in Figure 1 is preprocessed and two .les 
are created. In the .rst .le, the geometry of the tree skeleton is stored. Like artists we only draw 
the trunk and branches up to the second or­der in most of our illustrations with higher order branches 
being removed. The second .le stores the leaves as particles each with a position and a normal vector. 
The normal vectors are obtained by using the normal vector of the original leaves. If too much data is 
generated for all the leaves Tree I in Figure 1 has about 183,000 leaves we reduce them in the modeling 
system by reducing the number of leaves at each twig. If this is still too much we position the particles 
at the branching positions of the highest-order twigs. In the case of Tree I we end up with 8,800 particles. 
The illustrations are generated as follows: The trunk and branches are drawn by applying techniques known 
from non-photorealistic rendering. The foliage is rendered by representing each leaf by a drawing primitive 
 a disk or arbitrary polygon facing the viewer and by applying the depth difference algorithm to determine 
which part of the primitive outlines are to be drawn. Shadows can be ap­plied at this stage, vegetation 
on the ground can also be added and is processed the same way. The resulting drawings are then composed 
to constitute the .nal image. 3.1 Drawing the tree skeleton The tree skeleton is an assembly of generalized 
cylinders each rep­resenting a branch. The surface is more or less smooth, which al­lows us to apply 
analytical silhouette algorithms such as the one proposed by Markosian et al. [10] or the hybrid solution 
of Rakar and Cohen [13] to generate the outline. The depth difference algo­rithm proposed below can also 
be applied (see Figure 2). In addition, the skeleton is shaded conventionally to .nd dark re­gions. These 
regions are then crosshatched in the drawing. The Difference Image Algorithm [16] that places individual 
strokes according to the local gray tone of an image is one solution to this problem. For our purpose 
a simpler method is suf.cient that works with a variant of the Floyd Steinberg method [4]. The algorithm 
places short strokes instead of pixels if the cumulated gray scale is above a given threshold. The area 
of the stroke is determined and the corresponding error value is subtracted from the neighboring pixel 
values. The direction of the strokes is either at random or affected by the normal vector of the stem 
geometry. A similar technique for directing strokes was already used in [10].  Figure 2: The trunk and 
main branches of Tree I are extracted and rendered by silhouette lines and cross hatching. 3.2 Drawing 
the foliage The foliage of a tree differs by its very nature from all smooth sur­faces and therefore 
must be handled separately. Several thousand individual surfaces must be combined visually into a shape 
or a set of strokes. In our .rst experiments, we placed special textures on the leaves of our realistic 
tree models that looked like strokes. This is a fast and simple method, but the generated images never 
appeared like drawings. The observation that artists do not draw leaves correctly but try to represent 
their visual appearance led us to use abstract drawing primitives. Each leaf is represented by the outline 
of such a primitive, whereas its position is determined by the 3-d leaf position and the size is controlled 
by the user. A very simple drawing primitive is a view-facing disk. While other abstract drawing primitives 
are given below, we .rst describe the second ingredient of our approach, the depth difference algorithm, 
by using this primitive. Depth differences Depth differences are used to determine what part of each 
drawing primitive is to be drawn to constitute the foliage. Saito and Taka­hashi [15], two of the early 
authors in non-photorealistic render­ing, used the depth-buffer to determine the outline of objects which 
were used to enhance photorealistic images. First and second order derivatives in the depth-buffer were 
additionally computed to .nd important lines on the surface of the objects. While .rst and second order 
depth derivatives are helpful to .nd important lines on smooth surfaces, zero order derivatives are help­ful 
for determing important lines in collections of isolated surfaces like assemblies of drawing primitives: 
The outline of a primitive is drawn if the maximal depth difference of the surface to the neigh­boring 
surfaces is above a given threshold. Instead of computing the differences analytically -which in the 
case of complex tree models is computationally expensive -we use the depth buffer for this purpose. The 
primitives are drawn as solids, the depth buffer is obtained, and for each pixel the depth difference 
is computed by comparing its depth value with all neighbor values. The maximal positive difference for 
each pixel is taken. This value indicates how far the pixel is in front of its neighboring pixels. It 
is stored in a separate buffer. For interactive applications those pixels with a depth difference exeeding 
a given depth difference threshold are directly used to cre­ate a bitmap of the outlines. For printing 
purposes a vectorization is performed to obtain stroke pathes (see Section 3.4). It is well known that 
the values in the depth buffer have a non-linear characteristic. The depth z in the camera coordinate 
system or eye coordinates rsp. is determined from a depth value d (d . [0..1]) by z1z0(d1-d0) z1-z0 z 
= (1) d - (z1+z0)(d1-d0) - (d1+d0) 2(z1-z0)2 where d0 and d1 are minimal and maximal values represented 
in the depth buffer, and z0 and z1 the corresponding depth values of the near and far clipping plane 
in the camera projection (cf. [11]). The depth differences can be computed for the depth values in eye 
coordinates to achieve linear differences or directly for the depth buffer values. In the second case 
depth differences for remote ob­jects correspond to much larger differences in eye coordinates. In consequence 
the objects are represented by fewer lines. To determine a depth difference threshold suf.cient for the 
eye coordinates we compute the depth range of the tree and choose a percentage of this range, for example 
10 percent. Analogously this is done with depth buffer values. The examples in this pa­per were rendered 
using depth buffer values directly by setting d0 =0,d1 = 65535,z0 =1, and z1 = 11. The depth differ­ence 
in eye coordinates (z1 - z0) is approximately the one of real trees. (a) (b) Figure 3: Tree I rendered 
with varying disk size and depth differ­ence threshold: a) size=0.15, threshold=1000; b) size=0.7, thres­hold=2000. 
Figure 3 shows two sketches of Tree I. In Figure 3(a) small disks are used and the threshold is low. 
This results in high detail and a good approximation of the real model. A more abstract rendering is 
achieved if disk size and threshold are enlarged (Figure 3(b)). The threshold can be constant over the 
whole image or can be modulated by other buffers. In Figure 4(c) a shadow buffer was used to reduce the 
threshold in the shadow. The resulting image shows more detail in this area. Abstract drawing primitives 
Apart from disks, a number of drawing primitives can be used to represent the leaves. In Figure 4(a) 
a set of nine polygons was gen­erated to represent leaves from different views. The normals of the given 
particles were used to interpolate the individual shapes of the leaves from the polygons. Using this 
interpolation scheme, a 3-d shape can be denoted without strictly adhering to perspective trans­formations. 
If appropriate polygons are used, a representation similar to the graftals in [7] can be generated, but 
our interpolation method offers more freedom, allowing nearly all forms of leaves to be used. The user 
is also able to decide to what extent the 3-d impression is generated: the leaves in Figure 4(b) are 
not drawn from the full range of views, instead a subset is used to generate a style between uniform 
shapes and the full 3-d impression. In Figure 4(c) the shape of the leaves is drawn only in the shadow 
region, additionally the linewidth is increased. (a) (b) (c) Figure 4: Two sketches of Tree II. a) The 
leaves are rendered using interpolated polygons from the nine given samples; b) Shadow is drawn in black, 
threshold=100. c) Threshold is set to 6,000, shadow is represented by detail.  3.3 Level-of-Abstraction 
As mentioned above, the differences in the depth buffer have a non-linear characteristic. If they are 
used directly instead of re­projecting them into eye coordinates, the same tree that is drawn in the 
front with high detail will be sketched automatically by a few strokes if it is at the back. The effect 
can be modulated by changing the z1 to z0 ratio of the perspective projection which is the basis for 
Equation (1). A small ratio causes a small non-linearity, a large ratio above 100:1 results in less depth 
resolution in the background and therefore in a small number of strokes. This visual level-of-abstraction 
can be supported by scaling the primitive size for trees in the background. In [7] a formula for a scale 
factor r for the object size of graftals is suggested which uses a weighted average between a scaling 
d/s (d desired screen space, s current screen space of the object) that generates primitives of visual 
constant size and primitives that have a constant object size r = w(d/s) + (1 - w) w . [0..1]. In our 
case, we additionally allow w to be above one and in this case omit the second term. Now, the abstract 
drawing primitives appear larger in the background, which helps to achieve clear object shapes here. 
In Figure 5 the process is shown. In the tree sequence of Figure 5(a) level-of-abstraction was done on 
the basis of depth differences only, in Figure 5(b) the size of the drawing primitives is doubled for 
the tree at the back. 3.4 Software Framework The proposed method was designed to work in two environments. 
First, a fast method for interactive systems was needed. Second, high quality images should be produced 
for printouts, animations and architectural sketches. As a consequence the software works in stages that 
are partly omitted for the interactive process. In the .rst step, depth differences have to be determined. 
In the interactive environment stem and foliage are rendered together, the depth buffer is obtained and 
all pixels above the given depth differ­ence threshold are drawn in black. The resulting bitmap is directly 
used and blended with other geometries of the scene to constitute the .nal image. For drawing purposes 
-and also for animations with high temporal coherency -the stem and the foliage are rendered separately, 
the images are combined by their depth buffer values to handle occlu­sion. For each image a separate 
depth difference threshold is used later. For many styles shadows have to be introduced. We have to use 
a software implementation of shadows because volume shadows based on stencil buffering (cf. [11]) do 
not work for the huge num­ber of isolated surfaces in the foliage. The result is stored in a sep­arate 
shadow buffer. In the interactive case, shadows are omitted. Now the threshold is applied and the pixels 
above the threshold are marked. As mentioned above, the threshold can be modulated by a shadow buffer, 
other G-buffers (cf. [15]) or by an arbitrary spatial function. For generating high quality images, the 
bitmaps of the stem and the foliage are vectorized. We implemented two methods: The .rst algorithm determines 
vectors globally for the bitmaps by applying least square .tting [12]. The second algorithm adds an index 
buffer, a bitmap that stores at each pixel position the primitive identi.ca­tion as a color value. For 
each depth value above the threshold, it is now possible to ob­tain the primitive number, therefore vectorization 
can be performed for each primitive separately. This results in a higher image quality, for instance 
closed primitive outlines can now easily be determined and represented by closed polygons. As a drawback, 
the method which is slow already needs even more time since the index buffer has to be rendered and processed 
additionally. In both cases the polygons are drawn by spline interpolation, and line styles may be applied. 
As an example, line styles are responsi­ble for the shading effect on the tree in Figure 3(b). Among 
varying the line width, which was done here, the styles may also affect the direction of the line or 
alter the endpoints. (a) (b) Figure 5: Tree I rendered for three different distances. a) Primitive 
sizes and threshold are constant for all distances. Visual abstraction is achieved automatically. b) 
Primitive sizes are enlarged up to the factor of two for the tree in the back.  4 Results References 
In Figure 6(a) and (b), Tree III is drawn using view-facing elliptic primitives of random orientation. 
After determining which part of each primitve has to be drawn, a small deformation was applied to each 
outline. This helps to achieve a more sketched drawing style. In Figure 6(b) all visible outlines are 
drawn, and a threshold of 400 is used. The drawing of Figure 6(a) was created using a slight modi.cation 
of the algorithm: Only the lower part of each ellipse is drawn when visible, the threshold having a value 
of 100. Rendering is performed in 10 seconds on our SGI Octane (Maximum Impact), the conifer consists 
of 13,200 particles. The maple tree of Figure 6(c) consists of 16,200 particles which is far below the 
original model with 200,000 leaves. The parametriza­tion of Figure 6(a) was used, threshold was set to 
1,000. Figure 6(d) was created similar to Figure 6(a). Only 2,300 particles are used, this causes nearly 
each ellipse to be visible, as a result a lot of semicircles appear. Figure 6(e) used drawing primitives 
in the form of real leaves, a very small threshold of 10 causes all visible outlines to be drawn. The 
tree in Figure 6(f) consists of 90,000 particles, very small el­lipses were used, shadow is added as 
black regions. The ground is represented by 23,000 elliptic primitives of larger size. Only the shadow 
is drawn, no primitive outlines are used. In this case ren­dering is performed in about one minute. In 
the interactive version of the proposed algorithm it is possible to render three trees consisting of 
20,000 primitives each and 25,000 ground particles with three frames per second on our SGI Onyx 2 at 
lower image quality. We hope to improve this in the future. 5 Conclusion and Future Work We have presented 
a framework for rendering trees in pen-and-ink. The tree skeleton and the foliage are processed separately. 
The trunk and branches are represented by silhouette lines augmented by crosshatching in dark areas. 
The foliage is drawn by using ab­stract drawing primitives that represent leaves. Such primitives can 
be circles, ellipses or other polygons. An interpolation scheme al­lows us to adapt the form of the primitives 
to the normal vector of the particles that are used as input. Depth differences are used to determine 
what part of the primitives are drawn. Our experiments reveal that it is possible to create various illustra­tion 
styles with our approach, and they have opened several areas of future research: So far, shadows were 
introduced into the images by shadow buffers or by raising detail in shadow regions. As mentioned in 
Section 2 artists sometimes use crosshatching on the leaves to represent shadow. The hatching lines in 
this case must in­teract with the leaves. An intersection method as proposed in [2] can be applied here. 
 To reduce the amount of geometric data, level-of-detail has to be applied to the tree models. Currently 
we work with some discrete representations that sometimes cause visual artifacts if representations are 
changed. A continuous level-of-detail algorithm for trees will improve performance while maintain­ing 
the visual quality.  The primary goal of our paper was to provide pen-and-ink illustrations for architecture 
and landscaping. Another impor­tant application are cartoons. New styles and colored versions of our 
images need to be developed for that purpose.  [1] M. Aono and T. L. Kunii. Botanical tree image generation. 
IEEE Computer Graphics and Applications, 4(5):10 34, May 1984. [2] O. Deussen, J. Hamel, A. Raab, S. 
Schlechtweg, and T. Strothotte. An illustra­tion technique using hardware-based intersections and skeletons. 
In Proceedings of Graphics Interface 99, pages 175 182. Canadian Human-Computer Commu­nications Society, 
1999. [3] L. Evans. The New Complete Illustration Guide: The Ultimate Trace File for Architects, Designers, 
Artists, and Students. Van Nostrand Reinhold Company, 1996. [4] R. W. Floyd and L. Steinberg. An adaptive 
algorithm for spatial grey scale. Proc. Soc. Inf. Display, 17:75 77, 1976. [5] Greenworks GbR. Home page 
of the xfrog modelling software. http://www. greenworks.de. [6] S. Hsu and I. Lee. Drawing and animation 
using skeletal strokes. In SIGGRAPH 94 Conference Proceedings, pages 109 118. ACM SIGGRAPH, July 1994. 
[7] M. Kowalski, L. Markosian, J. D. Northrup, L. Burdev, R. Barzel, L. Holden, and J. F. Hughes. Art-based 
rendering of fur, grass, and trees. In SIGGRAPH 99 Conference Proceedings. ACM SIGGRAPH, August 1999. 
[8] B. Lintermann and O. Deussen. Interactive modeling of plants. IEEE Computer Graphics and Applications, 
19(1):56 65, January/February 1999. [9] F. Lohan. The drawing handbook. Contemporary Books, Chicago, 
1993. [10] L. Markosian, M. A. Kowalski, S. J. Trychin, L. D. Bourdev, D. Goldstein, and J. F. Hughes. 
Real-time nonphotorealistic rendering. In T. Whitted, editor, SIG-GRAPH 97 Conference Proceedings, pages 
415 420. ACM SIGGRAPH, 1997. [11] T. McReynolds and D. Blyth. Advanced graphics programming techniques 
using OpenGL. SIGGRAPH 98 Course Notes, ACM SIGGRAPH, 1998. [12] J. R. Parker. Extracting vectors from 
raster images. Computers &#38; Graphics, 12(1):75 79, 1988. [13] R. Raskar and M. Cohen. Image precision 
silhouette edges. In 1999 ACM Sym­posium on Interactive 3D Graphics, pages 135 140. ACM SIGGRAPH, April 
1999. [14] W. T. Reeves and R. Blau. Approximate and probabilistic algorithms for shading and rendering 
structured particle systems. In Computer Graphics (SIGGRAPH 85 Proceedings), volume 19, pages 313 322, 
July 1985. [15] T. Saito and T. Takahashi. Comprehensive rendering of 3-d shapes. In Com­puter Graphics 
(Proc. SIGGRAPH 90), volume 24(4), pages 197 206. ACM SIGGRAPH, 1990. [16] M. Salisbury, M. Wong, J. 
F. Hughes, and D. Salesin. Orientable textures for image-based pen-and-ink illustration. In SIGGRAPH 
97 Conference Proceed­ings. ACM SIGGRAPH, 1997. [17] T. T. Sasada. Drawing natural scenery by computer 
graphics. Computer Aided Design, 19(4):212 218, 1987. [18] J. Schumann, T. Strothotte, A. Raab, and S. 
Laser. Assessing the effect of non­photorealistic images in computer-aided design. In ACM Human Factors 
in Com­puting Systems, SIGCHI 96, pages 35 41, April 13-15 1996. [19] A. R. Smith. Plants, fractals and 
formal languages. Computer Graphics (SIG-GRAPH 84 Proceedings), 18(3):1 10, July 1984. [20] S. Strassmann. 
Hairy brushes. Computer Graphics (SIGGRAPH 86 Proceed­ings), 20(3):225 232, 1986. [21] C. Strothotte 
and T. Strothotte. Seeing Between the Pixels: Pictures in Interactive Systems. Springer-Verlag, Berlin-Heidelberg-New 
York, 1997. [22] T. Strothotte, B. Preim, A. Raab, J. Schumann, and D. R. Forsey. How to render frames 
and in.uence people. Computer Graphics Forum, 13(3):455 466, 1994. [23] G. Winkenbach and D. Salesin. 
Computer-generated pen-and-ink illustration. In SIGGRAPH 94 Conference Proceedings, pages 91 100. ACM 
SIGGRAPH, 1994. [24] G. Winkenbach and D. Salesin. Rendering parametric surfaces in pen and ink. In SIGGRAPH 
96 Conference Proceedings, pages 469 476. ACM SIGGRAPH, 1996. [25] C. I. Yessios. Computer drafting of 
stones, wood, plant and ground materials. Computer Graphics (Proceedings of SIGGRAPH 79), 13(3):190 198, 
1979. Figure 6: Several trees shaded with different styles. See Section 4 for details.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344795</article_id>
		<sort_key>19</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A simple, efficient method for realistic animation of clouds]]></title>
		<page_from>19</page_from>
		<page_to>28</page_to>
		<doi_number>10.1145/344779.344795</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344795</url>
		<abstract>
			<par><![CDATA[<p>This paper proposes a simple and computationally inexpensive method for animation of clouds. The cloud evolution is simulated using cellular automaton that simplifies the dynamics of cloud formation. The dynamics are expressed by several simple transition rules and their complex motion can be simulated with a small amount of computation. Realistic images are then created using one of the standard graphics APIs, OpenGL. This makes it possible to utilize graphics hardware, resulting in fast image generation. The proposed method can realize the realistic motion of clouds, shadows cast on the ground, and shafts of light through clouds.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[animation]]></kw>
			<kw><![CDATA[atmospheric effects]]></kw>
			<kw><![CDATA[graphics hardware]]></kw>
			<kw><![CDATA[rendering]]></kw>
			<kw><![CDATA[volume rendering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Modeling and recovery of physical attributes</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.2</cat_node>
				<descriptor>Earth and atmospheric sciences</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010432.10010437</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Earth and atmospheric sciences</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010370</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation evaluation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010342.10010344</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis->Model verification and validation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010342</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P306681</person_id>
				<author_profile_id><![CDATA[81100329647]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yoshinori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dobashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hokkaido University, Faculty of Engineering, Kita 13, Nishi 8, Kita-ku, Sapporo 060-8628 Japan and Hiroshima City University, 3-4-1, Ozukahigashi, Asaminami-ku, Hiroshima, 731-3194 Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P158352</person_id>
				<author_profile_id><![CDATA[81100412545]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kazufumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kaneda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hiroshima University, 1-4-1, Kagamiyama, Higashi-hiroshima, 739-8527 Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14213081</person_id>
				<author_profile_id><![CDATA[81100617737]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hideo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamashita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hiroshima University, 1-4-1, Kagamiyama, Higashi-hiroshima, 739-8527 Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31091317</person_id>
				<author_profile_id><![CDATA[81332519296]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tsuyoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Okita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hiroshima City University, 3-4-1, Ozukahigashi, Asaminami-ku, Hiroshima, 731-3194 Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP36036594</person_id>
				<author_profile_id><![CDATA[81100539710]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Tomoyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tokyo, 7-3-1, Hongo, Bunkyo-ku, Tokyo, 113-0033 Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[D. Blythe, "Advanced Graphics Programming Techniques Using OpenGL," Course Note #29 of SIGGRAPH 99, 1999.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Y. Dobashi, T. Nishita, K. Kaneda, H. Yamashita, "A Fast Display Method of Sky Color Using Basis Functions," The Journal of Visualization and Computer Graphics, Vol. 8, No. 2, 1997, pp. 115-127.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Y. Dobashi, T. Nishita, T. Okita, "Animation of Clouds Using Cellular Automaton," Proc. of Computer Graphics and Imaging'98, 1998, pp. 251-256.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Y. Dobashi, T. Nishita, H. Yamashita, T. Okita, "Using Metaballs to Modeling and Animate Clouds from Satellite Images," The Visual Computer, Vol. 15, No. 9, 1998, pp. 471-482.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97918</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[D. S. Ebert, R. E. Parent, "Rendering and Animation of Gaseous Phenomena by Combining Fast Volume and Scanline A-Buffer Techniques," Computer Graphics, Vol. 24, No. 4, 1990, pp. 357-366.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[D. S. Ebert, W. E. Carlson, R. E. Parent, "Solid Spaces and Inverse Particle Systems for Controlling the Animation of Gases and Fluids," The Visual Computer, 10, 1990, pp. 471- 483.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>259233</ref_obj_id>
				<ref_obj_pid>259081</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[D. S. Ebert, "Volumetric Modeling with Implicit Functions: A Cloud is Born," Visual Proc. of SIGGRAPH'97, 1997, pp. 147.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[D. S. Ebert, "Simulating Nature: From Theory to Application," Course Note #26 of SIGGRAPH 99, 1999, pp. 5.1-5.52.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258838</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[N. Foster, D. Metaxas, "Modeling the Motion of a Hot, Turbulent Gas," Proc. of SIGGRAPH'97, 1997, pp. 181-188.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325248</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[G.Y. Gardner, "Visual Simulation of Clouds," Computer Graphics, Vol.19, No. 3, 1985, pp. 279-303.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>113028</ref_obj_id>
				<ref_obj_pid>113023</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[M. Inakage, "Volume Tracing of Atmospheric Environments, " The Visual Computer, 7, 1991, pp. 104-113.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280925</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[H. W. Jansen, P. H. Christensen, "Efficient Simulation of Light Transport in Scenes with Participating Media using Photon Maps," Proc. of SIGGRAPH'98, 1998, pp. 311-320.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808594</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[J. T. Kajiya, B. P. V. Herzen, "Ray Tracing Volume Densities," Computer Graphics, 1984, Vol. 18, No. 3, pp. 165-174.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[K. Kaneda, T. Okamoto, E. Nakamae, T. Nishita, "Photorealistic Image Synthesis for Outdoor Scenery under Various Atmospheric Conditions," The Visual Computer, 7(5&amp;6), 1991, pp. 247-258.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[T. Kikuchi, K. Muraoka, and N. Chiba, "Visual Simulation of Cumulonimbus Clouds," The Journal of The Institute of Image Electronics and Electronics Engineers of Japan, Vol. 27, No. 4, 1998, pp. 317-326 (in Japanese).]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5515</ref_obj_id>
				<ref_obj_pid>5513</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[N. Max, "Light Diffusion through Clouds and Haze," Graphics and Image Processing, Vol. 13, No. 3, 1986, pp. 280-292.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15899</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[N. Max, "Atmospheric Illumination and Shadows," Computer Graphics, Vol. 20, No. 4, 1986, pp. 117-124.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>949719</ref_obj_id>
				<ref_obj_pid>949685</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[N. Max, R. Crawfis, D. Williams, "Visualizing Wind Velocities by Advecting Cloud Textures," Proc. of Visualization'92, 1992, pp. 179-183.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[N. Max, "Efficient Light Propagation for Multiple Anisotropic Volume Scattering," Proc. of the Fifth Eurographics Workshop on Rendering, 1994, pp. 87-104.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>614427</ref_obj_id>
				<ref_obj_pid>614274</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[K. Meuller, N. Shareef, J. Huang, R. Crawfis, "Hight-Quality Splatting on Rectilinear Grids with Efficient Culling of Occluded Voxels," IEEE Trans. on Visualization and Computer Graphics, Vol. 5, No. 2, 1999, pp. 116-134.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[K. Nagel, E. Raschke, "Self-Organizing Criticality in Cloud Formation?," Physica A, 182, 1992, pp. 519-531.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[F. Neyret, "Qualitative Simulation of Convective Clouds Formation and Evolution," Proc of Eurographics Computer Animation and Simulation Workshop'97, 1997, pp. 113-124.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37437</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[T. Nishita, Y. Miyawaki, E. Nakamae, "A Shading Model for Atmospheric Scattering Considering Distribution of Light Sources," Computer Graphics, Vol. 21, No. 4, 1987, pp. 303-310.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192261</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[T. Nishita, E. Nakamae, "Method of Displaying Optical Effects within Water using Accumulation Buffer," Proc. of SIGGRAPH'94, 1994, pp. 373-379.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237277</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[T. Nishita, Y. Dobashi, E. Nakamae, "Display of Clouds Taking into Account Multiple Anisotropic Scattering and Sky Light," Proc. of SIGGRAPH'96, 1996, pp. 379-386.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311545</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[A. J. Preetham, P. Shirley, B. Smits, "A Practical Analytic Model for Daylight," Proc. of SIGGRAPH'99, 1999, pp. 91- 100.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37436</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[H. E. Rushmeier, K. E. Torrance, "The Zonal Method for Calculating Light Intensities in The Presence of a Participating Medium," Computer Graphics, Vol. 21, No. 4, 1987, pp. 293-302.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[G. Sakas, M. Gerth, "Sampling and Anti-Aliasing of Discrete 3-D Volume Density Textures," Proc. of EUROGRAPHICS'91, 1991, pp. 87-102.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166163</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[J. Stam, E. Fiume, "Turbulent Wind Fields for Gaseous Phenomena," Proc. of SIGGRAPH'93, 1993, pp. 369-376.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[J. Stam, "Stochastic Rendering of Density Fields," Proc. of Graphics Interface'94, 1994, pp. 51-58.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218430</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[J. Stam, E. Fiume, "Dipicting Fire and Other Gaseous Phenomena Using Diffusion Processes," Proc. of SIGGRAPH'95, 1995, pp. 129-136.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311548</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[J. Stam, "Stable Fluids," Proc. of SIGGRAPH'99, 1999, pp. 121-128.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[R. Voss, "Fourier Synthesis of Gaussian Fractals: 1/f noises, landscapes, and flakes," SIGGRAPH'83: Tutorial on State of the Art Image Synthesis, 10, 1983.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97919</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[L. Westover, "Footprint Evaluation for Volume Rendering," Computer Graphics, Vol. 24, No. 4, 1990, pp. 367-376.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[S. Wolfram, "Cellular automata as models of complexity," Nature, Vol. 311, No. 4, 1984, pp. 419-424.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
			<ref>
				<ref_obj_id>90995</ref_obj_id>
				<ref_obj_pid>90967</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[G. Wyvill, A. Trotman, "Ray-Tracing Soft Objects," Proc. of CG International, 1990, pp. 439-475.]]></ref_text>
				<ref_id>36</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 ' = = = = == = = = = = = = ¬ =¬ = =¬ = = = =  v(zk), is assumed to return integer values. The transition 
rules are as follows. .hum(i - v(zk ), j,k,ti ), i - v(zk ) > 0 hum(i, j,k,ti+1) =. , (8) 0, otherwise 
. .cld(i - v(zk ), j,k,ti ), i - v(zk ) > 0 cld(i, j,k,ti+1) =. , (9) 0, otherwise . .act(i - v(zk ), 
j,k,ti ), i - v(zk ) > 0 act(i, j,k,ti+1) =. , (10) 0, otherwise . In this paper, the velocity function 
v(zk) is specified as a piecewise linear function. 4.4 Fast Simulation Using Bit Field Manipulation Functions 
Each variable can be stored in one bit since its state is either 0 or 1. This means that simulations 
with large numbers of cells can be executed in a small amount of memory. The computation time is also 
reduced because of the following reasons. Let us assume all the variables are stored in an array of unsigned 
integers. Let m be the bit length of the unsigned integer variable. By making use of bit field manipulation 
functions of higher level language, such as C language, transitions of m cells can be computed at the 
same time. This realizes fast simulation. Most difficulties in implementing this idea lie in transition 
rules concerning cloud extinction, expressed by Eqs. 5 through 7, since random numbers have to be generated 
at each cell, i.e., each bit field. This may result in increasing the computation time. We used a look-up 
table that stores random bit sequences to save the computational cost. See Appendix A for more details. 
4.5 Controlling Cloud Motion Using Ellipsoids As mentioned in section 4.2, the animator can design the 
cloud motion by controlling vapor probability, phase transition probability, and cloud extinction probability. 
Ellipsoids are used to do this in this paper. When wet air parcels move upward and reach the height of 
the dew point, clouds are gradually formed. Ellipsoids are used to simulate the air parcels. The vapor 
probability and phase transition probability are assumed to be higher at their centers than at their 
edges. Inversely, the cloud extinction probability is assumed to be lower at the center since the extinction 
hardly ever occurs at the center of the air parcel. Ellipsoids also move in the direction of the wind. 
By controlling ellipsoid parameters, such as sizes and positions, different kinds of clouds can be simulated. 
The animator specifies the regions for ellipsoids to be generated. In our experiment, even the ellipsoids 
generated using uniform random numbers result in a realistic animation as shown in section 6. 5. RENDERING 
METHOD Methods for generating realistic images are proposed in this section. First, the density distribution 
of clouds is calculated by making use of the results of the simulation. Images are then rendered using 
OpenGL. Details of the methods are described in the following sections. 5.1 Continuous Density Distribution 
Calculation The density distribution of clouds in the real world is continuous from 0 to 1. The distribution 
obtained from the simulation, sunlight light from background viewpoint scattered light Figure 3: Calculation 
of cloud color. cumulative density store attenuation ratio billboard light metaball Figure 4: Billboard 
and its texture. however, has only two values, that is, 0 or 1. Therefore, the proposed method calculates 
continuous distribution by smoothing the binary distribution, or two-valued distribution. First, the 
density, q(i,j,k,ti), is calculated at each cell (i,j,k) at each time step ti using the following equation. 
t 10 q(i, j,k,t) = i . (2t0 +1)(2k0 +1)(2 j0 +1)(2i0 +1) t'=-t 0 (11) k0 j0 i0 w(i', j',k',t')cld(i + 
i', j + j',k + k',t + t'),  ... i k'=-k0 j'=- j0 i'=-i0 where w is a weighting function and i0, j0, 
k0, t0 are sizes for the smoothing. As expressed by Eq. 11, we include time as well as space for the 
smoothing since the distribution is discrete in space and time. The density at an arbitrary point, x, 
is then obtained as a weighted sum of a simple basis function, f. Gaussians are often used for the basis 
function [29, 31, 34]. In this paper, however, we use a field function of metaballs proposed by Wyvill 
et al [36]. The reason for this is as follows. A metaball has a parameter, an effective radius, which 
represents its size. This means that it is much easier to specify the domain of influence than Gaussians 
that have an infinite domain. Furthermore, the shape of Wyvill s field function is very similar to the 
Gaussians [36]. As a result, the density at point x is given by the following equation. N .(x, ti ) =.q(i, 
j, k, ti ) f (| x - xi, j,k |) , (12) i, j,k..(x,R) where R is the effective radius, .(x,R) is a set 
of cells those centers are within the distance R from the point x, N is the number of elements of .(x,R), 
and xi,j,k is the coordinate corresponding to the center of the cell (i,j,k). For the field function, 
f, see Appendix B. As shown in Eq. 12, the continuous density distribution is expressed by a set of metaballs. 
The user specifies the effective    ¬ = = = == =  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344801</article_id>
		<sort_key>29</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Animating explosions]]></title>
		<page_from>29</page_from>
		<page_to>36</page_to>
		<doi_number>10.1145/344779.344801</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344801</url>
		<abstract>
			<par><![CDATA[<p>In this paper, we introduce techniques for animating explosions and their effects. The primary effect of an explosion is a disturbance that causes a shock wave to propagate through the surrounding medium. The disturbance determines the behavior of nearly all other secondary effects seen in explosion. We simulate the propagation of an explosion through the surrounding air using a computational fluid dynamics model based on the equations for compressible, viscous flow. To model the numerically stable formation of shocks along blast wave fronts, we employ an integration method that can handle steep pressure gradients without introducing inappropriate damping. The system includes two-way coupling between solid objects and surrounding fluid. Using this technique, we can generate a variety of effects including shaped explosive charges, a projectile propelled from a chamber by an explosion, and objects damaged by a blast. With appropriate rendering techniques, our explosion model can be used to create such visual effects as fireballs, dust clouds, and the refraction of light caused by a blast wave.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[animation]]></kw>
			<kw><![CDATA[atmospheric effects]]></kw>
			<kw><![CDATA[computational fluid dynamics]]></kw>
			<kw><![CDATA[natural phenomena]]></kw>
			<kw><![CDATA[physically based animation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Modeling and recovery of physical attributes</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P94424</person_id>
				<author_profile_id><![CDATA[81341498615]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gary]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Yngve]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GVU Center and College of Computing, Georgia Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P133156</person_id>
				<author_profile_id><![CDATA[81100311781]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[O'Brien]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GVU Center and College of Computing, Georgia Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14028670</person_id>
				<author_profile_id><![CDATA[81100049661]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jessica]]></first_name>
				<middle_name><![CDATA[K.]]></middle_name>
				<last_name><![CDATA[Hodgins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GVU Center and College of Computing, Georgia Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[J. D. Anderson Jr. Modern compressible flow: with historical perspective. McGraw-Hill, Inc., 1990.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[W. E. Baker.Explosions in air. University of Texas Press, 1973.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>184932</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[D. Ebert, K. Musgrave, D. Peachy, K. Perlin, and S. Worley. Texturing and Modeling: A Procedural Approach. AP Professional, 1994.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258838</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[N. Foster and D. Metaxas. Modeling the motion of a hot, turbulent gas. Proceedings of SIGGRAPH 97, pages 181Z-188, August 1997.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>241077</ref_obj_id>
				<ref_obj_pid>241020</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[N. Foster and D. Metaxas. Realistic animation of liquids. Graphics Interface '96, pages 204-212, May 1996.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[H. L. Green and W. R. Lane. Particulate Clouds: Dusts, Smokes and Mists. D. Van Nostrand Company, Inc., 1964.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[A. M. KuetheandC.Chow. Foundations of aerodynamics: bases of aerodynamic design. John Wiley and Sons, Inc., 1998.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[C. L. Madder. Numerical modeling of detonations. University of California Press, 1979.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[K. H. Martin. Godzilla: The sound and the fury. Cinefex, pages 82-107, July 1998.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>351688</ref_obj_id>
				<ref_obj_pid>351631</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[O. Mazarak, C. Martins, and J. Amanatides. Animating exploding objects. Graphics Interface '99, pages 211-218, June 1999.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[J. R. Meyer-Arendt. Introduction to classical and modern optics. Prentice-Hall, Inc., 1984.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>351686</ref_obj_id>
				<ref_obj_pid>351631</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[M. Ne and E. Fiume. A visual model for blast waves and fracture. Graphics Interface '99, pages 193-202, June 1999.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[F.S. Nooruddin and G. Turk. Simplification and repair of polygonal models using volumetric techniques. Technical Report GIT- GVU-99-37, Georgia Institute of Technology, 1999.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311550</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[J. F. O'Brien and J. K. Hodgins. Graphical modeling and animation of brittle fracture. Proceedings of SIGGRAPH 99, pages 137-146, August 1999.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311545</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[A. J. Preetham, P. Shirley, and B. E. Smits. A practical analytic model for daylight. Proceedings of SIGGRAPH 99, pages 91- 100, August 1999.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>357320</ref_obj_id>
				<ref_obj_pid>357318</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[W. T. Reeves. Particle systems-a technique for modeling a class of fuzzy objects. ACM Transactions on Graphics, 2(2):91-108, April 1983.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97923</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[K. Sims. Particle animation and rendering using data parallel computation. Computer Graphics (Proceedings of SIGGRAPH 90), 24(4):405-413, August 1990.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311548</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[J. Stam. Stable uids. Proceedings of SIGGRAPH 99, pages 121-128, August 1999.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218430</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[J. Stam and E. Fiume. Depicting are and other gaseous phenomena using dicusion processes. Proceedings of SIGGRAPH 95, pages 129-136, August 1995.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[R. Street. Volcano: Toasting the coast. Cinefex, pages 56-84, September 1997.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311544</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[J. Tumblin and G. Turk. LCIS: A boundary hierarchy for detailpreserving contrast reduction. Proceedings of SIGGRAPH 99, pages 83-90, August 1999.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[M. C. Vaz. Journey to Armageddon. Cinefex, pages 68-93, October 1998.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Animating Explosions Gary D. Yngve James F. O Brien Jessica K. Hodgins GVU Center and College of Computing 
Georgia Institute of Technology Abstract In this paper, we introduce techniques for animating explo­sions 
and their e.ects. The primary e.ect of an explosion is a disturbance that causes a shock wave to propagate 
through the surrounding medium. This disturbance determines the behavior of nearly all other secondary 
e.ects seen in explo­sions. We simulate the propagation of an explosion through the surrounding air using 
a computational .uid dynamics model based on the equations for compressible, viscous .ow. To model the 
numerically stable formation of shocks along blast wave fronts, we employ an integration method that 
can handle steep pressure gradients without introducing inap­propriate damping. The system includes two-way 
coupling between solid objects and surrounding .uid. Using this tech­nique, we can generate a variety 
of e.ects including shaped explosive charges, a projectile propelled from a chamber by an explosion, 
and objects damaged by a blast. With ap­propriate rendering techniques, our explosion model can be used 
to create such visual e.ects as .reballs, dust clouds, and the refraction of light caused by a blast 
wave. CR Categories: I.3.5 [Computer Graphics]: Computa­tional Geometry and Object Modeling Physically 
based modeling; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism Animation; I.6.8 [Simulation 
and Modeling]: Types of Simulation Animation Keywords: Animation, Atmospheric E.ects, Compu­tational 
Fluid Dynamics, Natural Phenomena, Physically Based Animation 1 Introduction Explosions are among the 
most dramatic phenomena in na­ture. A sudden burst of energy from a mechanical, chemical, or nuclear 
source causes a pressure wave to propagate out­ward through the air. The blast wave shocks up, creating 
a nearly discontinuous jump in pressure, density, and temper­ature along the wave front. The wave is 
substantially denser than the surrounding .uid, allowing it to travel supersoni­cally and to cause a 
noticeable refraction of light. The air at the shock front compresses, turning mechanical energy into 
College of Computing, Georgia Institute of Technology, Atlanta, GA 30332. gyngve@acm.org, job@acm.org, 
jkh@cc.gatech.edu. Permission to make digital or hard copies of part or all of this work or personal 
or classroom use is granted without fee provided that copies are not made or distributed for profit or 
commercial advantage and that copies bear this notice and the full citation on the first page. To copy 
otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission 
and/or a fee. SIGGRAPH 2000, New Orleans, LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 Figure 
1: An image of a projectile propelled from a cham­ber by an explosion. On the right is a cross-section 
of the three-dimensional .uid volume using a colormap where hot­ter colors indicate higher densities. 
heat. The waves re.ect, di.ract, and merge, allowing them to exhibit a wide range of behavior. An explosion 
causes a variety of visual e.ects in addition to the light refraction by the blast wave. An initial chemi­cal 
or nuclear reaction often causes a blinding .ash of light. Dust clouds are created as the blast wave 
races across the ground, and massive objects are moved, deformed, or frac­tured. Hot gases and smoke 
form a rising .reball that can trigger further combustion or other explosions and scorch surrounding 
objects. We present a physically based model of an explosion and show how it can be used to simulate 
many of these e.ects. We model the explosion post-detonation as compressible, viscous .ow and solve the 
.ow equations with an integra­tion method that handles the extreme shocks and supersonic velocities inherent 
in explosions. We cannot capture many of the visual e.ects of an explosion in a complex setting if we 
rely only on an analytical model of the blast wave; a .uid dynamics model of the air is necessary to 
capture these e.ects. The system includes a two-way coupling between dy­namic objects and .uid that allows 
the explosions to move objects. Figure 1 illustrates this phenomenon with a pro­jectile propelled from 
a chamber. We also use the pressure wave generated by the explosion to fracture and deform ob­jects. 
The user can simulate arbitrarily complex scenarios by positioning polygonal meshes to represent explosions 
and objects. The user controls the scale and visual qualities of the explosion with a few physically 
motivated parameters. Our .uid model of an explosion simulates many phenom­ena of blast waves that existing 
graphics techniques do not capture. Figure 2 shows a cross-section of pressures for a three-dimensional 
explosion near a wall. The initial dis­turbance in the .rst image interacts with the surrounding .uid 
and causes a pressure wave to propagate through the medium. In the second image, the blast wave has shocked 
up, as is evident by the large di.erences in pressure across the shock front. The blast wave re.ects 
o. the wall and the ground in the third image. In the fourth, the wave that re.ected o. the ground merges 
with the initial blast wave  Figure 2: Shown here is a cross-section of pressures for a three-dimensional 
explosion near an immovable wall. The timestep between frames is 10 ms. Many of the behaviors of the 
blast wave can be seen, speci.cally the Mach stem formed from the blast wave merging with the wave re.ected 
o. the ground plane, and the di.racted wave formed when the blast wave crests over the wall. to form 
a Mach stem, which has pressure values twice that of the initial wave. In the .nal two images, the blast 
wave crests over the wall and forms a weaker di.racted wave. In the entertainment industry, explosions 
are currently created at full scale in the real world, in miniature, or us­ing heuristic graphics techniques[20, 
9, 22]. Each of these methods has signi.cant disadvantages, and we believe that in many scenarios, simulation 
may provide an easier solu­tion. When explosions are generated and .lmed at full scale in the real world, 
they often must be faked to appear danger­ous and destructive by using multiple charges and chemicals 
with a low .ashpoint. Because of the cost and danger of ex­ploding full-size objects, many explosions 
are created using miniatures. With miniatures, the greatest challenge is often scaling the objects and 
the physics to create a realistic ef­fect. Current graphics techniques for creating explosions are based 
on heuristics, analytical functions, or recorded data, and although they produce nice e.ects for spherical 
blast waves, they are not adequate for the complex e.ects required for many of the scenarios used in 
the entertainment industry. Physically based simulations of explosions o.er several po­tential advantages 
over these three techniques. In contrast to real physical explosions, simulations can be used in an it­erative 
fashion, allowing the director many chances to mod­ify or shape the e.ect. The rendering of the explosion 
is to a large extent decoupled from the simulation, allowing the visual characteristics of the dust clouds 
or .reball to be determined as a post-process. Unlike heuristic or analyt­ical graphical methods, physically 
based simulations allow the computation of arbitrarily complex scenes with multiple interacting explosions 
and objects. The next section discusses relevant previous work in ex­plosions, .uids, .ame, and fracture. 
The following section introduces the explosion model in the context of computa­tional .uid dynamics. 
The next two sections discuss cou­pling between the .uid and solids and other secondary e.ects such as 
refraction and .reballs. We close with a discussion of our results.  2 Previous Work Explosions used 
in the entertainment industry tend to be vi­sually rich. Because of the inherent computational complex­ity 
of these explosions, researchers largely neglected this .eld after the publication of particle simulation 
techniques[16, 17]. Procedural methods can generate .ery, billowy clouds that could be used as explosions[3]. 
Recently two papers speci.cally addressed explosions. Mazarak and colleagues simulate the damage done 
by an ex­plosion to voxelized objects[10]. They model the explosion as an ideal spherical blast wave 
with a pressure pro.le curve approximated by an analytic function based on the modi­.ed Friedlander equation 
and scaled according to empirical laws[2]. The spherical blast wave expands independent of existing obstacles, 
and forces are applied to objects in the direction of the blast radius. Objects are modeled as con­nected 
voxels and based on various heuristics, these radial forces may cause the voxels to disconnect. Ne. and 
Fiume use data from empirical blast curves to model an explosion[12]. The blast curves relate the pres­sure 
and velocity of the blast wave to time and are scalable. Unlike Mazarak and colleagues, they use a curve 
represent­ing the re.ection coe.cient to apply forces to objects based on the angle of incidence of the 
blast wave. They assume quasi-static loading conditions where the blast wave encloses the entire object 
and e.ects due to re.ected waves are ig­nored. They also model explosion-induced fracture in planar surfaces 
using a procedural pattern generator. An alternative to these analytic and empirical models is a computational 
.uid dynamic simulation of the blast wave and the surrounding air. Foster and Metaxas presented a so­lution 
for incompressible, viscous .ow and used it to animate liquids[5] and hot, turbulent gas[4]. They modeled 
.uid as a three-dimensional voxel volume with appropriate boundary conditions. The .uid obeys the Navier-Stokes 
equations; gas also follows an equation that represents thermal buoyancy. Using an explicit scheme, they 
update velocities and tem­peratures every timestep via Euler integration and readjust the values to guarantee 
conservation of mass. The .uid is rendered by tracing massless particles along the interpolated .ow .eld. 
Their work with liquids included dynamic objects that were moved by the .uid, although they assumed that 
the objects were small enough not to in.uence the .uid. Recently Stam addressed the computational cost 
of guar­anteeing stability by introducing extra damping to a.ord larger timesteps and using an implicit 
method to solve a sparse system of equations[18]. Stam s method is inappropri­ate for shocks and explosions 
because his integration scheme achieves stability by encouraging the .uid to dissipate. In the dramatic 
e.ects produced by the entertainment industry, a .reball is often the most salient visible charac­teristic 
of an explosion. Stam and Fiume modeled .ame and the corresponding .uid .ow and rendered the results 
using a sophisticated global illumination method[19]. The gases be­haved according to advection-di.usion 
equations; Stam and Fiume solve these equations e.ciently by reformulating the problem from a grid to 
warped blobs. Illumination from gas is a.ected by emission and anisotropic scattering and absorption. 
They only consider continuous emissions from blackbody radiation and ignore line emissions from electron 
excitation. They develop a heuristic for smoke emission due to the lack of a scienti.c analytic model. 
Compressible .ow has been studied for years in the com­putational .uid dynamics community[1, 2, 8]. We 
have built on this work by taking the governing equations and the donor-acceptor method of integration 
from this litera­ture. However, the reasons for simulating explosions, com­bustion, detonation, and supersonic 
.ow in engineering di.er signi.cantly from those in computer graphics. Engineering problems often require 
focusing on one element such as the boundary layer and simulating the other elements only to the extent 
that they a.ect the phenomenon under study. For example, engineering simulations are often two-dimensional 
and assume symmetry in the third dimension. Because they are focused on a speci.c event, their simulations 
may run for only a few microseconds. In computer graphics, on the other hand, we need to produce a visually 
appealing view of the behavior throughout the explosion. As a result, we need a more complete model with 
less quantitative accuracy.  3 Explosion Modeling An explosion is a pressure wave caused by some initial 
dis­turbance, such as a detonation. In the results presented here, we assume that the detonation has 
occurred and that its properties are de.ned in the initial conditions of the sim­ulation. This assumption 
is reasonable for most chemical explosions because the detonation is complete within mi­croseconds. We 
animate explosions by modeling the pres­sure wave and the surrounding air as a .uid discretized over 
a three-dimensional rectilinear grid. The following two sub­sections describe the governing equations 
for .uid dynamics and the computational techniques used to solve them. The remaining two subsections 
describe the parameters available to the user for controlling the appearance of the explosion via the 
boundary conditions and initial conditions. 3.1 Fluid Dynamics In nearly all engineering problems, including 
the analysis of explosions, .uids are modeled as a continuum. They are rep­resented as a set of equations 
in terms of density p (kg/m 3), pressure P (N/m 2), velocity v(m/s), temperature T (K), the internal 
energy per unit mass N (J/kg), and the total energy per unit mass E = N + 12 v 2 (J/kg). The equations 
that gov­ern these quantities are de.ned in an Eulerian fashion, that is, they apply to a di.erential 
volume of space that is .lled with .uid rather than to the .uid itself. In addition to the Navier-Stokes 
equations, which model the conservation of momentum, the equations for compressible, viscous .ow in­clude 
governing equations for the conservation of mass and energy and for the .uid s thermodynamic state[7]. 
We introduce several simplifying assumptions that make the equations easier to compute but nevertheless 
allow us to capture the e.ects of compressible, viscous .ow. We dis­count changes in the vibrational 
energies of molecules and assume air to be at chemical equilibrium; we ignore the ef­fects from dissociation 
or ionization. These assumptions, which are commonly used in the engineering literature[1], allow us 
to reduce to constants many of the coe.cients that vary with temperature. The resulting deviation in 
the values of the coe.cients is negligible at temperatures below 1000 K; only minor deviations occur 
below 2500 K. Our implemen­tation produces aesthetic results with temperatures above 100000, K, although 
deviations in constants could be on the order of a magnitude or two. The .rst governing equation of .uid 
dynamics arises from the conservation of mass. Because .uid mass is conserved, the change of .uid density 
in a di.erential volume must be equal to the net .ux across the volume s boundary, giving .p = - ·(pv). 
(1) .t The second governing equation, commonly known as the Navier-Stokes equation, concerns the conservation 
of mo­mentum. For a Stokes .uid, where the normal stress is in­dependent of the rate of dilation, the 
equation for the x 1e+06 8e+05 6e+05 4e+05 2e+05 0 Figure 3: Pressure pro.le (N/m 2)over time (ms)near 
an explosion. component of the .uid velocity is given by  .vx µ.v p = pf- P + ·+µ 2 vx -p(v· )vx, (2) 
.t x 3.x where frepresents the body forces such as gravity and µ is the coe.cient of viscosity. The equations 
for the y and z components are similar. The .rst two terms on the right­hand side of the equation model 
accelerations due to body forces and forces arising from the pressure gradient; the next two terms model 
accelerations due to viscous forces. The last term is not a force-related term; rather it is a convective 
term that models the transport of momentum as the .uid .ows. This distinction between time derivative 
(force) terms and convective terms will be important for the integration scheme. The .nal governing equation 
enforces the conservation of energy in the system. The First Law of Thermodynamics dictates that the 
change in energy is equal to the amount of heat added and the work done to the system. Accounting for 
the amount of work done from pressure and viscosity and the heat transferred from thermal conductivity 
yields p .N = k 2T -P ·v+. -p(v· )N, (3) .t where k is the thermal conductivity constant and . is the 
viscous dissipation given by .= - 2µ 3 ( ·v)2 + µ 2 i,jE{x,y,z} .vi .j + .vj .i 2 . (4) As with equation 
(2), the last term of equation (3) is a con­vective term and models the transport of energy as the .uid 
.ows. In addition to the three governing equations, we need equations of state that determine the relationship 
between energy, temperature, density, and pressure. They are N = cV T, P = pRT, (5) where the coe.cient 
cV is the speci.c heat at constant vol­ume and R is the gas constant of air. Figure 4: This .gure illustrates 
the donor-acceptor method in which the amount of mass transferred is proportional to the mass of the 
donor. The voxels on the left show the transfer of mass and energy according to the .ow, indicated by 
thebluearrow. Thetwo voxels on theright representthe scenario with reversed .ow of the same magnitude. 
Density is represented as height, and unit energy is represented as color. Corresponding amounts of energy 
are sent with the mass. 3.2 Discretization and Numerical Integration The equations in the previous section 
describe the behavior of a .uid in a continuous fashion. However, implementing them in a form suitable 
for numerical computation requires that the space .lled by the .uid be discretized in some man­ner and 
that a stable method for integrating the governing equations forward in time be devised. Finite di.erences 
are used to discretize the space into a regular lattice of cubical cells. These .nite voxels take the 
place of the di.erential volumes used to de.ne the continu­ous equations, and the governing equations 
now hold for each voxel. Fluid properties such as pressure and velocity are as­sociated with each voxel 
and these properties are assumed to be constant across the voxel. The spatial derivatives used in the 
governing equations are approximated on the lattice using central di.erences. For example, the x component 
of the pressure gradient, P ,at voxel [i, j, k]isgiven by .P P[i+1,j,k] - P[i-1,j,k] . , (6) .x 2h where 
subscripts in square brackets index voxel locations and h is the voxel width. After the governing equations 
have been expressed in terms of discrete variables using .nite di.erences, they may be used as the update 
rules for an explicit integration scheme. However, rapid pressure changes created by steep pressure gradients 
moving at supersonic speeds would cause such a scheme to diverge rapidly. (See Figure 3.) To deal with 
this problem, we improved the basic integration tech­nique using two modi.cations described in the .uid 
dynam­ics literature[2, 8]. The .rst modi.cation involves updating equations (2) and (3) in two steps, 
.rst using only the tem­poral portion of the derivatives and second using the con­vective derivatives. 
The second modi.cation is called the donor-acceptor method and is described in detail below. It addresses 
problems that arise when mass, momentum, and energy are convected across steep pressure gradients. The 
modi.ed update scheme operates by applying the fol­lowing algorithm to each voxel at every timestep: 
1. Approximate the .uid acceleration at the current time, aat =(.v/.t)t, using the non-convective (.rst 
four) terms of equation (2). 2. Compute the tentative velocity at the end of the timestep, vat+6t = 
vt +ftaat, and the approximate av­  erage velocity during the timestepvt =(vat+6t +vt)/2. Figure 5: 
This .gure shows cross-sections of pressure for three-dimensional explosions of equal-volume charges 
in the shape of a sphere, cylinder, torus, and wedge. The timestep between frames is 5 ms. The donor-acceptor 
method transfers mass proportional to the mass of the voxel in the upstream direction, or donor voxel. 
Suppose we have voxel i and one of its six neighbors j in direction dfrom i.Let vij = 12 (vi + vj ) · 
d.If vij > 0, then .ow is going from i to j, i is the donor, and pi is used in equation (1) to compute 
the change in voxel i s density. Like­wise, if vij < 0, then j is the donor, and the density of j is 
used when computing the change in voxel i s density. These calculations are repeated for the six neighbors 
of i to obtain the new density for i, pt+6t. The velocity and energy con­vection are then scaled by pt/pt+6t 
to conserve momentum and energy. Figure 4 illustrates the donor-acceptor method. The left and right diagrams 
show .ows in opposite direc­tions of the same magnitude. The sent mass is proportional to the mass of 
the donor and carries with it corresponding amounts of energy.  3.3 Boundary Conditions The system has 
several types of boundary conditions that allow the .uid to exhibit a wide range of behaviors. Free boundaries 
allow blast waves to travel beyond the voxel vol­ume as if the voxel volume were arbitrarily large. This 
type of boundary allows us to model slow, long-term aspects of ex­plosions, such as .reballs and dust 
clouds. Hard boundaries force .uid velocity normal to them to be zero while leaving all other .uid attributes 
unchanged. We treat these bound­aries as smooth surfaces, so tangential .ow is una.ected. We ¯ implement 
a third boundary condition to achieve faster exe­ 3. Approximate change in internal energy, N, using 
the cution. If a voxel and its neighbors have pressure di.erences non-convective terms of equation (3) 
and substituting less than a threshold, the voxel is treated as a free boundary ¯ vt 4. Usingfor the 
.uid velocity. ¯ vt for the .uid velocity, compute the new density, pt+6t with equation (1). 5. Calculate 
the complete vt+6t and Nt+6t with equa­tions (2) and (3) using the convective terms and the new value 
of p. 6. Use state equations (5) to update secondary quantities such as temperature.  Although this 
update scheme is more stable than a sim­ple Euler integration, sharp gradients in .uid density may still 
allow small .ows from nearly empty voxels to gener­ate negative .uid densities and cause inappropriately 
large changes to both velocity and internal energy. To prevent these problems, we use a donor-acceptor 
method when com­puting - pvof the convective terms in steps 4 and 5 above. and is never evaluated. This 
optimization prunes out the majority of the volume while the blast wave is expanding.  3.4 Initial Conditions 
The user speci.es the pressure and temperature of the air, and the initial values of other variables 
are determined from the state equations (5). The detonation is initialized by specifying a region of 
the volume with higher temperature or pressure. For example, a chemical explosion might have a temperature 
of 2900 K and a pressure of 1000 atm with the surrounding air at 290 K and 1 atm. The creation of the 
explosion may be time-delayed or may be triggered when the .uid around the charge reaches a threshold 
temperature. The detonation may have an arbitrary geometry repre­sented by a manifold polygonal mesh. 
The mesh is vox­elized to initialize the appropriate voxels in the .uid simu­lation. By controlling the 
geometry, the user can produce a variety of e.ects that could not be achieved with a spher­ical model. 
In blast theory, planar, cylindrical, and spher­ical blast waves can be modeled by analytic functions[2]; 
however nonstandard shapes can create surprising and in­teresting e.ects. Figure 5 shows cross-sections 
of pressure for three-dimensional explosions from equal-volume charges in the shape of a sphere, cylinder, 
torus, and wedge. The inner blast wave of the torus merges to create a strong ver­tical blast wave. The 
wedge concentrates its force directly to the right, while leaving the surrounding area relatively untouched. 
 4 Interaction with Solids People use explosions to impart forces on objects for both constructive and 
destructive purposes. The movements of these objects and the resulting displacement of air create many 
of the compelling visual e.ects of an explosion. In this section, we present methods to implement a two-way 
cou­pling between the .uid and solids. The coupling from .uid to solid allows us to model phenomena such 
as a projectile being propelled by an explosion. The coupling from solid to .uid can be used to model 
a piston compressing or the shock wave formed as a projectile moves through the air superson­ically. 
We also extend previously published techniques for fracture to allow the pressure wave to shatter objects. 
To allow the two-way coupling, objects have two represen­tations: a polygonal mesh that is used to apply 
forces to the object from the .uid, and a volume representation in vox­els that is used to displace .uid 
based on the motion of the object. We incorporate the coupling into the .uid dynamics code in the following 
way: 1. Apply forces on the objects from the .uid and compute the rigid body motion of the objects. 
2. If the object has moved more than a fraction of a voxel, recompute the voxelization of the object. 
 3. Displace .uid based on object movement. 4. Update the .uid using the techniques described in Sec­tion 
3.2.  We explain the .rst three of these items in greater detail in the following subsections. 4.1 Coupling 
from Fluid to Solid An object embedded in a .uid experiences two separate sets of forces on its surface, 
those arising from hydrostatic pres­sure and those arising from dynamic forces due to .uid mo­mentum. 
The forces due to hydrostatic pressure act normal to the surface and are generated by the incoherent 
motions of the .uid molecules against the surface. The dynamic forces are generated by the coherent motion 
of the continuous .uid and can be divided into a force normal to the surface of the object and a tangential 
shearing force. We neglect the tan­gential shearing force because in the context of explosions, it is 
negligible in comparison to the force due to hydrostatic pressure. We assume that the object is in equilibrium 
under ambient air pressure and the hydrostatic forces are computed using the overpressure P¯, which is 
the di.erence between the hydrostatic pressure P and ambient pressure. The magnitude of the normal force 
per unit area on the surface is given by the dynamic overpressure: ¯¯1 n)2 Pdyn = P + p (vrel · , (7) 
2 Figure 6: A glass window is shattered by a blast wave. The blast wave pressure is approximately 3atm 
when it reaches the window. The images show the scene at 0 ms, 13 ms, 40 ms, 67 ms, 107 ms, and 160 ms. 
where vrel is the velocity of the .uid relative to the surface, and is the outward surface normal. n 
We assume that the triangles composing each object are small enough that the force is constant over each 
triangle. The force on a triangle with area A is then nA ¯ f= - Pdyn, (8) where the .uid properties are 
measured at the centroid of the triangle. The forces are computed for all triangles of an object, and 
the translational and angular velocities of the object are updated accordingly. In addition to acting 
on rigid objects, the forces can also be applied to .exible objects that deform and fracture[14]. The 
explosion simulation results in pressures, velocities, and densities for each voxel in the discretization 
of the .uid. The fracture simulation uses this information to compute the forces that should be applied 
to a .nite element model of the objects in the scene. The force computation is similar to that for rigid 
objects. This method was used to simulate the breaking window shown in Figure 6 and the breaking wall 
shown in Figure 7. This coupling is one-way in that the .uid applies forces to the .nite-element model, 
but the .uid is not moved by the fragments that pass through it. 4.2 Coupling from Solid to Fluid To 
allow the solid to displace .uid, the triangular mesh rep­resenting the object is converted to voxels[13], 
which are then used to de.ne the hard boundaries in the .uid volume dynamically. The objects move smoothly 
through the .uid, but because of the discrete nature of the voxelization, large changes in the amount 
of .uid displaced may occur on each timestep. To address this problem, the .uid displaced or the void 
created by the movement of the objects is handled over a period of time rather than instantaneously. 
The voxelization returns a value between zero and one representing the proportion of the voxel that is 
not interior Figure 7: A wall is knocked over by a blast wave from an explosion 3.5 m away. The images 
are spaced 250 ms apart. to any object. This value is independent of geometric con­siderations about 
the exact shape of the occupied volume. If any dimension of an object is smaller than the size of a voxel, 
the appropriate voxels will have partial volumes, but because there are no fully occupied voxels, the 
.uid will ap­pear to move through the object. Nonzero partial volumes below a certain threshold are set 
to zero to increase stability. The implementation of partial volumes requires slight mod­i.cations to 
the donor-acceptor method to conserve mass, momentum, and energy because two adjacent voxels could have 
di.erent volumes. When any part of an object moves more than a fraction of a voxel, the object is revoxelized, 
and the hard boundaries of the .uid are updated. When this process occurs, the par­tial volume in a voxel 
might change, resulting in .uid .ow. We allow this .ow to occur smoothly by sacri.cing conserva­tion 
of mass and energy in the short-term. The voxelization determines the partial volumes in an instantaneous 
fashion, but the .uid displacement routine maintains internal partial volumes that change more slowly 
and are used to compute the pressure, density, and temperature of the a.ected vox­els. The internal partial 
volumes change proportionally to the velocities of the moving objects, and mass and energy are restored 
over time. To compute a smooth change in the internal partial vol­ume from V1 to V2, we model an object 
moving into a voxel as a piston compressing or decompressing .uid. We simplify the computation of the 
change in partial volume by assum­ing that the piston is acting along one of the axes of the voxelization. 
The appropriate axis is selected based on the largest axial component of the velocity of the object, 
vp. The displacement of the piston after t seconds and the cor­responding change in partial volume of 
a voxel with width h are l= vpt, fV = V2 - V1 = h2 vpt. (9) The displacement occurs linearly over fV 
t = (10) h2 vp at a velocity of vp. Given this model of the change in internal partial vol­ume, we know 
that p1V1 = p2V2 because mass is conserved. However, the .uid is compressible, so mechanical energy is 
not conserved (otherwise P1V1 = P2V2). To obtain the new pressures and densities of the .uid, we use 
a thermodynamic equation relating the work done to the system from changing the volume (or density), 
. ./(.-1) P2 p2 T2 == , (11) P1 p1 T1 where . =1+ R/cV (. is about 1.4 for air and is closer to 1 the 
more incompressible the .uid)[1]. Internal unit en­ergy and total unit energy are then updated by the 
state equations. When the partial volume of a voxel changes from one nonzero value to another, the resulting 
pressure changes cause .uid to move to or from a neighbor based on the gov­erning equations. However, 
when the partial volume of a voxel changes from zero to nonzero or vice versa, the sit­uation must be 
handled as a special case by treating the a.ected voxel and one of its neighbors as a single larger voxel 
with a nonzero partial volume. The neighbor is se­lected based on the largest axial component of the 
object s velocity, vp. We calculate the internal partial volume for each of the involved voxels A and 
B as VaA1 and VaB1 . Figure 8: Refraction of light from a blast wave. Each frame is 10 ms apart. The 
index of refraction is exaggerated tenfold to enhance the e.ect. When VA1 , the original partial volume 
of A,is zero and VA2 , the new partial volume of A, is nonzero, we use initial volumes VaA1 and VaB1 
such that VaA1 +VaB1 = VB1 (the initial volume is conserved) and VaA1 VB2 = VaB1 VA2 (the voxels are 
treated as a single larger voxel). The change in volume is VB1 fVA = VA2 - VaA1 = VA2 - VA2 , (12) VA2 
+ VB2 VB1 fVB = VB2 - VaB1 = VB2 - VB2 . (13) VA2 + VB2 When the original partial volume of A, VA1 ,is 
nonzero and the new partial volume of A, VA2 , is zero, we force VaA1 to be zero and treat VaB1 as a 
single larger voxel. To treat the two voxels as one, we .rst average the properties of A and its neighbor 
B, transferring any lost kinetic energy to internal energy. The change in volume is then pAVA1 + pBVB1 
fVB = VB2 - VaB1 = VB2 - (14) pB by making sure that pBVaB1 = pAVA1 + pBVB1 (mass is con­served).  5 
Secondary E.ects An explosion creates a number of visual secondary e.ects including the refraction of 
light, .reballs, and dust clouds. These secondary e.ects do not signi.cantly a.ect the simu­lation, so 
they can be generated and edited as a post-process. One of the most stunning, but often ignored, e.ects 
of an explosion is the bending of light from the blast wave. Because the blast wave is substantially 
denser than the sur­rounding air, it has a higher index of refraction, ..Light travels at the same velocity 
between molecules, but near molecules it is slowed down from interactions with electrons. This concept 
is expressed numerically as . - 1= kp, k = 2.26 × 10-4 m 3/kg, by the Dale-Gladestone law[11]. We capture 
the refraction of light by ray tracing through the .uid volume. (See Figure 8.) As the ray is traced 
through the volume, the index of refraction is continually updated based on the interpolated density 
of the current position. For simplicity, we compute the density of each point using a trilinear interpolation 
of the densities of the neighboring voxels. When the index of refraction changes by more than a threshold, 
the new direction of the ray is computed via Snell s law using the density gradient as the surface normal. 
The trilinear interpolation results in minor faceting e.ects that cause small errors in the re.ected 
direction.  An advantage of using a full volumetric .uid represen­tation for explosions is that the 
simulation can be used to model a .reball in addition to the blast wave. We assume that the .reball is 
composed of detonated material from in­side the explosive. To track this material, the system initial­izes 
the .reball by placing particles inside the shape speci.ed by the user for the explosion. The particles 
are massless and .ow with the .uid, allowing the .uid dynamics model to cap­ture e.ects critical for 
a .reball such as thermal conductivity and buoyancy. Some .uid simulations[4, 18] model thermal buoyancy 
explicitly; in our simulation, thermal buoyancy is a behavior derived from the governing equations. For 
ren­dering, each particle takes on a temperature that is inter­polated based on its position in the volume. 
The particles are rendered as Gaussian blobs with values for red, green, blue, and opacity. The color 
values are based on blackbody radiation at appropriate wavelengths given the temperature of the particle[11]. 
Figure 9 shows a .reball and correspond­ing tracer particles after one second of simulation. Figure 10 
shows a .reball coming around a corner; the hallway is illu­minated by the .ames. The tracer particles 
couple the appearance of the .reball to the motion of the .uid, and although heat generated by the initial 
explosion is added to the .uid model, any addi­tional heat generated by post-detonation combustion is 
ig­nored. Radiative energy released at detonation could also be modeled for rendering. Much like the 
di.culties encountered with rendering the sun[15], the high contrast of this e.ect may require contrast-reduction 
techniques such as LCIS[21]. The blast wave and other secondary waves create dust clouds by disturbing 
.ne particles resting on surfaces. The creation of dust clouds is di.cult to quantify either exper­imentally 
or analytically, so the rate at which the dust be­comes airborne is left as a control for the animator. 
Once a dust particle is airborne, its behavior is dictated by its size. The smaller it is, the more it 
is in.uenced by drag forces and the less it is in.uenced by inertial forces. Smaller dust parti­cles 
have lower terminal velocities and exhibit more Brown­ian motion. With the exception of coagulated particles, 
ex­periments reveal that most dust particles are approximately ellipsoidal with low eccentricity, and 
the particles do not orient themselves to the .uid .ow[6]. The di.erence in dy­namics between these particles 
and spherical particles is not that signi.cant, so we assume dust particles to be spheri­cal. We implement 
dust as metaparticles, each representing a Gaussian density of homogeneous dust particles. The dust size 
for each metaparticle is chosen according to size dis­tributions from experimental data for blasted shale[6]. 
The Example (.gure) h (m) .t (ms) ttot (ms) V0 (m 3) P0 (atm) T0 (K) projectile (1) 1.0 0.10 450 73.60 
1000 2900 barrier (2) 0.2 0.01 25 0.52 1000 2900 shapes (5) 1.0 0.10 30 1000.00 1000 2900 fracture (6,7) 
0.2 0.02 20 0.52 1000 2900 .reball (8,9) 1.0 0.10 1000 65.40 1000 2900 corner (10) 1.0 0.10 10000 268.08 
1000 2900 city (11) 1.0 0.10 5000 65.40 1000 2900 nuclear (12) 50.0 0.50 30000 9.1×107 345 1×105 Table 
1: Parameters for simulations: voxel width, timestep, total simulation time, and initial volume, pressure, 
and tem­perature of detonation. metaparticles travel through the .uid as if single particles were located 
at their centers. Their variances grow accord­ing to the mean Brownian di.usion per unit time. Figure 
11 shows dust clouds in a city scene. 6 Results and Discussion We ran the system with several scenarios. 
The physical con­stants used in the simulation were constants for air that were taken from an engineering 
handbook[7]. Table 6 shows the voxel width, timestep, total simulation time, initial volume of the explosion 
(proportional to yield), and initial pressure and temperature of the explosion. The timesteps ft increase 
by a factor of .ve once the blast wave leaves the volume. The simulations ran on a single 195 MHz R10K 
proces­sor and used a 101 × 101 × 101 volume. The running times per timestep varied considerably from 
several seconds to two minutes because of the pruning described in Section 3.3. For coupling with fracture, 
I/O became a major factor because in each iteration the entire volume was written to disk; how­ever, 
using better compression would reduce this expense. Running times of the simulations varied from a few 
hours (Figure 5) to overnight (Figures 2, 6, 7, and 8) to a few days (Figures 1, 9, 10, and 12). We use 
an explicit integration technique to compute the motion of the pressure wave caused by the detonation. 
De­spite its magnitude, the wave does not transport .uid large distances. Previously, .uid dynamics has 
been used most of­ten in computer graphics to capture the e.ects of macroscale .uid transport where the 
.uid does move a signi.cant dis­tance. Implicit integration techniques with large timesteps are appropriate 
for these situations because they achieve sta­bility by damping high frequencies. The propagation of 
the pressure wave in our sti. equations, however, is character­ized by these high frequencies and it 
is essential that they not be arti.cially damped. We chose, therefore, to use an ex­plicit integration 
technique; however, an implicit integration technique could be used to simulate the .reball and dust 
clouds after the blast wave and the secondary waves have left the volume. Using an implicit integration 
technique in the slow .ow regime could allow larger timesteps and faster execution times.  We assume 
that the voxels in the .uid volume are of a size appropriate for the phenomena that we wish to capture. 
In particular, if solid objects have a dimension smaller than a voxel, then they will not create a hard 
boundary that pre­vents .uid .ow. For example, a wall that is thinner than a voxel will permit the blast 
wave to travel through it because partial volumes do not maintain any geometric information about the 
sub-voxel shape of the object. The di.culty of a two-way coupling with fracturing objects stems from 
hav­ing to model subvoxel cracks, which should allow .ow to go through. If small objects are required, 
the voxel size could be decreased or dynamic remeshing techniques could be used to create smaller voxels 
in the areas around boundaries. There are e.ects from explosions that we have not in­vestigated. Although 
smoke is often a visible feature of an explosion that includes a .reball, we do not have a physically 
based model for smoke creation. Incomplete combustion at lower temperatures results in smoke, and that 
observation could be used as a heuristic to determine where smoke should be created in the .reball and 
how densely. Stam and Fiume used a similar heuristic model[19]. Textures of objects could be modi.ed 
to show soot accumulation and scorching over time. Dust clouds are created when an object fractures or 
pulverizes. Dust could be introduced into our system when the .nite-element model produces small tetrahedra 
or when cracks form. We made several assumptions in constructing our model of explosions. Most discounted 
e.ects that did not con­tribute noticeably to the .nal rendered images; however, some could produce a 
noticeable change in behavior in cer­tain situations and may warrant further investigation. We only model 
the blast wave traveling through air. However, waves travel through other media, including solid objects, 
and complex interface e.ects occur when a wave travels be­tween two di.erent media. For large-scale explosions, 
me­teorological conditions such as the change in pressure with respect to altitude or the interface between 
atmospheric lay­ers (the tropopause) may need to be considered. Our goal in this work has been to create 
a physically re­alistic model of explosions. However, this model should also lend itself to creating 
less realistic e.ects. Even though our model does not incorporate high-temperature e.ects such as ionization, 
we can still obtain interesting results on high­temperature explosions. The .reball in Figure 12 resulted 
from an initial detonation at 105 K. Explosions used in feature .lms often include far more dramatic 
.reballs than would occur in the actual explosions that they purport to mimic. By using more tracer particles 
and adjusting the rendering parameters of the .reballs, we should be able to reproduce this e.ect. Noise 
could be added either to the velocity .elds or particle positions post-process to make the explosion 
look more turbulent. Similarly, explosions in space are often portrayed as more colorful and violent 
than explo­sions that occurred outside of the atmosphere should be. Im­parting an initial outward velocity 
to the explosion, turning o. gravity, and increasing the thermal buoyancy by modify­ing the state equations 
might create a similar e.ect. 7 Acknowledgments This project was supported in part by NSF NYI Grant 
No. IRI-9457621, Mitsubishi Electric Research Laboratory, and a Packard Fellowship. The second author 
was supported by a Fellowship from the Intel Foundation.  References [1] J. D. Anderson Jr. Modern 
compressible .ow: with historical perspective. McGraw-Hill, Inc., 1990. [2] W.E.Baker. Explosions in 
air. University of Texas Press, 1973. [3] D. Ebert, K. Musgrave, D. Peachy, K. Perlin, and S. Worley. 
Tex­turing and Modeling: A Procedural Approach. AP Professional, 1994. [4] N. Foster and D. Metaxas. 
Modeling the motion of a hot, turbu­lent gas. Proceedings of SIGGRAPH 97, pages 181 188, August 1997. 
[5] N. Foster and D. Metaxas. Realistic animation of liquids. Graph­ics Interface 96, pages 204 212, 
May 1996. [6] H. L. Green and W. R. Lane. Particulate Clouds: Dusts, Smokes and Mists. D. Van Nostrand 
Company, Inc., 1964. [7] A.M.Kuethe and C. Chow. Foundations of aerodynamics: bases of aerodynamic design. 
John Wiley and Sons, Inc., 1998. [8] C. L. Madder. Numerical modeling of detonations. University of California 
Press, 1979. [9] K. H. Martin. Godzilla: The sound and the fury. Cinefex, pages 82 107, July 1998. [10] 
O. Mazarak, C. Martins, and J. Amanatides. Animating explod­ing objects. Graphics Interface 99, pages 
211 218, June 1999. [11] J. R. Meyer-Arendt. Introduction to classical and modern op­tics. Prentice-Hall, 
Inc., 1984. [12] M. Ne. and E. Fiume. A visual model for blast waves and frac­ture. Graphics Interface 
99, pages 193 202, June 1999. [13] F.S. Nooruddin and G. Turk. Simpli.cation and repair of polyg­onal 
models using volumetric techniques. Technical Report GIT­GVU-99-37, Georgia Institute of Technology, 
1999. [14] J. F. O Brien and J. K. Hodgins. Graphical modeling and ani­mation of brittle fracture. Proceedings 
of SIGGRAPH 99, pages 137 146, August 1999. [15] A. J. Preetham, P. Shirley, and B. E. Smits. A practical 
analytic model for daylight. Proceedings of SIGGRAPH 99, pages 91 100, August 1999. [16] W. T. Reeves. 
Particle systems a technique for modeling a class of fuzzy objects. ACM Transactions on Graphics, 2(2):91 
108, April 1983. [17] K. Sims. Particle animation and rendering using data parallel computation. Computer 
Graphics (Proceedings of SIGGRAPH 90), 24(4):405 413, August 1990. [18] J. Stam. Stable .uids. Proceedings 
of SIGGRAPH 99, pages 121 128, August 1999. [19] J. Stam and E. Fiume. Depicting .re and other gaseous 
phe­nomena using di.usion processes. Proceedings of SIGGRAPH 95, pages 129 136, August 1995. [20] R. 
Street. Volcano: Toasting the coast. Cinefex, pages 56 84, September 1997. [21] J. Tumblin and G. Turk. 
LCIS: A boundary hierarchy for detail­preserving contrast reduction. Proceedings of SIGGRAPH 99, pages 
83 90, August 1999. [22] M. C. Vaz. Journey to Armageddon. Cinefex, pages 68 93, October 1998. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344809</article_id>
		<sort_key>37</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Computer modelling of fallen snow]]></title>
		<page_from>37</page_from>
		<page_to>46</page_to>
		<doi_number>10.1145/344779.344809</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344809</url>
		<abstract>
			<par><![CDATA[<p>In this paper, we present a new model of snow accumulation and stability for computer graphics. Our contribution is divided into two major components, each essential for modelling the appearance of a thick layer of snowfall on the ground.</p><p>Our <italic>accumulation model</italic> determines how much snow a particular surface receives, allowing for such phenomena as flake flutter, flake dusting and wind-blown snow. We compute snow accumulation by shooting particles upwards towards the sky, giving each source surface independent control over its own sampling density, accuracy and computation time. Importance ordering minimises sampling effort while maximising visual information, generating smoothly improving global results that can be interrupted at any point.</p><p>Once snow lands on the ground, our <italic>stability model</italic> moves material away from physically unstable areas in a series of small, simultaneous avalanches. We use a simple local stability test that handles very steep surfaces, obstacles, edges, and wind transit. Our stability algorithm also handles other materials, such as flour, sand, and flowing water.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[avalanches]]></kw>
			<kw><![CDATA[natural phenomena]]></kw>
			<kw><![CDATA[snow]]></kw>
			<kw><![CDATA[stability]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>J.2</cat_node>
				<descriptor>Earth and atmospheric sciences</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010432.10010437</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Earth and atmospheric sciences</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Reliability</gt>
			<gt>Theory</gt>
			<gt>Verification</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39071678</person_id>
				<author_profile_id><![CDATA[81342493994]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fearing]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>933439</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Paul Fearing. The Computer Modelling of Fallen Snow. PhD thesis, Dept. of Computer Science, University of British Columbia, July 2000.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258849</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Michael Garland and Paul Heckbert. Surface Simplification Using Quadric Error Metrics. SIGGRAPH97 Conference Proceedings, pages 209-216, August 1997.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166139</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Pat Hanrahan and Wolfgang Krueger. Reflection From Layered Surfaces Due To Subsurface Scattering. Computer Graphics (SIGGRAPH 93 Conference Proceedings), 27:165-174, August 1993.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617946</ref_obj_id>
				<ref_obj_pid>616034</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Siu-chi Hsu and Tien-tsin Wong. Simulating Dust Accumulation. IEEE Computer Graphics and Applications, 15 (1): 18-22, January 1995.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378513</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Wolfgang Krueger. Intensity Fluctuations And Natural Texturing. Computer Graphics (SIGGRAPH 88 Conference Proceedings), 22(4):213-220, August 1988.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Daisuke Kuroiwa, Yukiko Mizuno, and Masao Takeuchi. Micrometrical Properties Of Snow. In International Conference on Low Temperature Science (Physics of Snow and ice), volume 1, Part II, pages 722-751. Institute for Low Temperature Science, Aug 1966.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166162</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Xin Li and Michael Moshell. Modeling Soil: Realtime Dynamic Models For Soil Slippage And Manipulation. SIGGRAPH 93 Conference Proceedings, 27:361- 368, August 1993.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[A. Luciani, A. Habibi, and E. Manzotti. A Multi-Scale Physical Model Of Granular Materials. In Proceedings of Graphics interface, pages 136-137. Canadian Information Processing Society, 1995.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[David McClung and Peter Schaerer. The Avalanche Handbook. The Mountaineers, Seattle, Washington, 1993.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Malcolm Mellor. Engineering Properties Of Snow. Journal of Glaciology, 19(81):15-66,1977.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Claude Monet. Wheatstacks, Snow Effect, Morning. Painting: oil on canvas, J. Paul Getty Museum, Los Angeles, 1891.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[K. Muraoka and N. Chiba. A Visual Simulation Of Melting Snow. The Journal of the institute of image Electronics Engineers of Japan, 27(4):327-338,1998.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[K. Muraoka, N. Chiba, and I. Ohtawara. Snowfall Model For Simulating Close Views Of Snowy Landscapes. The Journal of the institute of Television Engineers of Japan, 49(10): 1252-1258,1995.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74337</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[F.K. Musgrave, C.E Kolb, and R.S. Mace. The Synthesis And Rendering Of Eroded Fractal Terrains. Computer Graphics (SIGGRAPH 89 Conference Proceedings), 23(3):41-50, July 1989.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[T. Nishita, H. Iwasaki, Y. Dobashi, and E. Nakamei. A Modeling And Rendering Method For Snow By Using Metaballs. In Proc. EUROGRAPHICS, volume 16. European Association for Computer Graphics, 1997.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383827</ref_obj_id>
				<ref_obj_pid>2383815</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[S. Premoze, W. Thompson, and P. Shirley. Geospecific Rendering Of Alpine Terrain. In EurographicsRendering Workshop. European Association for Computer Graphics, June 1999.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[D.A Robinson. Northern Hemisphere Snow Cover Charts. National Snow and Ice Data Center, http://www-nsidc.colorado.edu/NSIDC/EDUCATION/SNOW /snow_Robinson.html, as of April 10, 2000.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>926441</ref_obj_id>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Robert G. Scharein. interactive Topological Drawing. PhD thesis, Department of Computer Science, The University of British Columbia, 1998.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Mikio Shinya and Alain Fournier. Stochastic Motion - Motion Under The Influence Of Wind. In Proc. EUROGRAPHICS, pages 119-128. European Association for Computer Graphics, 1992.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97923</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Karl Sims. Particle Animation And Rendering Using Data Parallel Computation. Computer Graphics (SIGGRAPH 90 Conference Proceedings), 24(4):405-413, August 1990.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[R. Sumner, J. O'Brien, and J. Hodgins. Animating Sand, Mud and Snow. In Proceedings of Graphics interface, pages 125-132. Canadian Information Processing Society, 1998.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>107217</ref_obj_id>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Alan Watt and Mark Watt. Advanced Animation and Rendering Techniques. Addison-Wesley Publishing, Don Mills, Ontario, 1992.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>3690</ref_obj_id>
				<ref_obj_pid>3674</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[D.E. Willand. New Data Structures For Orthogonal Queries. SiAM Journal of Computing, 14(1):232-253,1985.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Modelling Of Fallen Snow Paul Fearing University of British Columbia.  Figure 1: A sudden 
snowfall comes to the North Pole. Abstract In this paper, we present a new model of snow accumulation 
and stability for computer graphics. Our contribution is divided into two major components, each essential 
for modelling the appearance of a thick layer of snowfall on the ground. Our accumulation model determines 
how much snow a particular surface receives, allowing for such phenomena as .ake .utter, .ake dusting 
and wind-blown snow. We compute snow accumulation by shooting particles upwards towards the sky, giving 
each source sur­face independent control over its own sampling density, accuracy and computation time. 
Importance ordering minimises sampling ef­fort while maximising visual information, generating smoothly 
im­proving global results that can be interrupted at any point. Once snow lands on the ground, our stability 
model moves mate­rial away from physically unstable areas in a series of small, simul­taneous avalanches. 
We use a simple local stability test that handles very steep surfaces, obstacles, edges, and wind transit. 
Our stabil­ity algorithm also handles other materials, such as .our, sand, and .owing water. CR Categories: 
I.3.5 [Computer Graphics]: Computational Geometry and Ob­ject Modelling Physically based modelling; J.2 
[Physical Sciences and Engineering]: Earth and atmospheric sciences; Keywords: snow, avalanches, stability, 
natural phenomena .email: fearing@cs.ubc.ca Permission to make digital or hard copies of part or all 
of this work or personal or classroom use is granted without fee provided that copies are not made or 
distributed for profit or commercial advantage and that copies bear this notice and the full citation 
on the first page. To copy otherwise, to republish, to post on servers, or to redistribute to lists, 
requires prior specific permission and/or a fee. SIGGRAPH 2000, New Orleans, LA USA &#38;#169; ACM 2000 
1-58113-208-5/00/07 ...$5.00 1 Introduction One of nature s greatest beauties is the way fresh snow covers 
the world in a perfect blanket of crystalline white. It replaces sharp an­gles with gentle curves, and 
clings to surfaces to form ghostly sil­houettes. In many countries, snow is a common fact of life during 
the win­ter months. For example, January snow coverage in the Northern Hemisphere has ranged between 
41.7 -49.8 million square kilome­tres [17], or nearly half of the hemisphere s total land mass. A phe­nomenon 
that is so common and pervasive is clearly of interest and importance. Despite the ubiquitous nature 
of snow, the entire season of winter has been almost completely ignored by computer graphics research 
and applications, with the exception of distant snow-capped moun­tains, and falling snow.akes. Without 
an automatic model of fallen snow, animators have so far relied upon intuition to produce snow­covered 
surfaces -an extremely tedious, time-consuming and poten­tially inaccurate task. A single tree might 
have a hundred branches, each with a complex drapery of snow, and each avalanching onto branches below, 
producing subtle second-order accumulation ef­fects. Besides the practicalities of research and application, 
there is an­other reason for investigating snowfall. Snow transforms common­place scenes into fantastic 
wonderlands, greatly changing the ap­pearance and mood of the landscape, allowing us to see familiar 
sights in a fresh, exciting way. This paper presents a new method of snow pack modelling for computer 
graphics. We are primarily concerned with creating and simulating fallen snow at a scale where the thickness 
is clearly ev­ident to the viewer. Our main emphasis is on a framework for ef.­ciently handling large 
scenes with limited resources, and to a much lesser extent on a physically correct model of the snow 
itself. Snow is arguably one of the world s most complex naturally occurring substances, and accurate 
simulation is still a signi.cant challenge to snow hydrologists and researchers.1 1We refer the reader 
to [9] or [1] for a discussion of the real substance.  Figure 2: A snow-covered gazebo with a hole in 
the roof. All snow was generated automatically, including snow on the mountains. In order to generate 
images of a snowy world, we need to solve two major problems. Snow accumulation requires us to determine 
how much snow falls upon a scene, and where it accumulates. We simulate this with an adaptive particle/surface 
hybrid that addresses the proper allocation and conservation of snow mass around and un­der obstacles, 
the random nature of snow.ake motion, and simple in-transit wind effects. As snow accumulates, we compute 
snow stability in order to de­termine how much mass any particular surface can support. If not blocked 
by an obstacle, unstable surfaces release avalanches onto lower surfaces, also potentially covered in 
snow. We compute snow stability using a set of sequential local equations providing us with good results 
at a reasonable computational cost. Our approach al­lows us to simulate varying properties of both snow 
and like ma­terials, as well as provide a simple model of mass transport due to wind. Finally, we transform 
our model of accumulated, stable snow pack into a set of smoothly joining 3D surfaces that can be included 
in scenes or animations. During this step we can include bridging effects between nearby surfaces, as 
well as wind cornices. We aug­ment our thick snow surfaces with .ake dusting textures to pro­vide extra 
noise and visual complexity. Because of the sheer size and complexity of snowy scenes, our method is 
also inherently concerned with the practical issues of speed and control. Our primary contribution to 
this area is the counter-intuitive idea that snow.akes are shot upwards from indi­vidual surfaces, rather 
than dropped downwards from the sky. Giv­ing individual surfaces control over their own snowy destiny 
al­lows us to prioritise computational effort on any number of crite­ria, including surface slope, area, 
distance to the camera, likelihood of interesting occlusions, or other measures of visual interest. Our 
algorithm provides a continuous, ever-improving result that can be terminated at any time, and still 
display the full snow depth. As shown in Figure 3, our snow-adding algorithms are part of a larger pipeline 
involving a popular commercial animation pack­age. Since the underlying scene remains unchanged, we retain 
the original lighting and animation and can rely upon strong commer­cial support for shader libraries 
and rendering. This makes it quite easy to add snow to a wide range of existing models and animations. 
 2 Related Work Despite snow s common presence in many parts of the world, there has been little previous 
research towards a comprehensive model of snow for computer graphics. Premoze et al. [16] generate realistic 
mountainous terrains that are likely the most convincing snow-covered scenes so far. Starting with a 
digital elevation model enhanced with an aerial photo, they use a detailed model of snow pack evolution 
to add zero-thickness patches of seasonal snow cover. The nature of the incoming satellite data restricts 
the technique to a scale much larger than our primary area of interest. Muraoka et al. [13] simulate 
thick snow pack by dropping vol­ume elements on the landscape, with provisions for snow evolution  Figure 
3: Overview of the snow pipeline. The underlying scene remains unchanged except for the inclusion of 
new snow surfaces. [12]. Unfortunately,inordertocoverthelandscapewithacomputa­tionally feasible number 
of particles, single-scale elements must be in.ated to the point where they overwhelm underlying base 
surface detail. Other work tangentially involving snow include Sims [20], and Shinya and Fournier [19], 
but both approaches are concerned only with falling and zero-thickness snow. Sumner et al. [21] simulate 
tracks in mud, sand, and snow using a regular height .eld and mod­els of compression and erosion, but 
do not address snow accumu­lation. Nishita et. al [15] introduce a method of snow pack render­ing based 
on multiple scattering of light within the snow volume. Snow surfaces were composed of individual metaballs 
placed by hand. Other work on snow illumination properties includes Hanra­han and Krueger [3] and Krueger 
[5]. Although not about snow, several other papers describe the mo­tion of granular materials. Hsu and 
Wong [4] model zero-thickness dust accumulation with textures. Luciani et al. [8] introduce a multi-scale 
physical model for granular materials, designed to sim­ulate such granular phenomena as piling, arching, 
and avalanching in the 2D plane. Li and Moshell [7] were responsible for a dynamic soil model on a constant 
regular grid, allowing for volume conser­vation, soil slippage, and manipulation of soil with a simulated 
bull­dozer. Musgrave et al. [14] describe terrain generation, including an erosion and thermal weathering 
model that is quite applicable to snow stability.  3 Snow Accumulation Peculiar to snow is the idea 
of .ake .utter , where falling ice crys­tals are affected by crystal shape and atmospheric micro-turbulence. 
These local disturbances can prevent falling snow from descending in a straight line, instead allowing 
.akes to sidestep blocking obsta­cles and land underneath on surfaces that have no direct exposure to 
the sky. Thus, simulating and modelling an accumulation pattern is akin to raytracing for light, except 
that we are interested in path (instead of straight-line) visibility. Where an obstacle, such as a porch 
or a bush, blocks the ground underneath, the .ake .utter effect eventually produces an occlusion boundary 
between completely blocked and unblocked areas. An example of this can be seen in Figure 4(a), where 
snow accumu­lates well underneath the overhang of the bush. Over billions of .akes, these occlusion boundaries 
exhibit a smooth drop-off, where the shape of the curve and amount of snow under an object depends on 
the size, shape, and number of blocking occlusions, the closeness of the occlusion to the ground, and 
the magnitude of the .uttering effect. For objects with many occluding components (such as a pine tree) 
the occlusion boundaries are still present, but are much less pronounced. Mostfalling snowaccumulateson 
the uppermostlayer of branches, but some accumulateson the next layer, and most lower branches and the 
ground get at least a small dusting. This con­tributes to the visual impression that snow is everywhere 
in a scene, and not just sitting on the uppermost surfaces exposed to the sky. 3.1 Computing the Snowfall 
Accumulation Pattern Our goal is to generate an accumulation pattern for every surface in the model, 
where the amount of snow each surface receives is pro­portional to the occlusion factors described above. 
Our approach is to allow launch sites on each surface to emit a se­ries of particles aimed upwards towards 
a sky bounding plane. As particles .utter upwards, they are checked for intersection with in­tervening 
surfaces, where a hit indicates that a particle is some­how blocked, and cannot contribute snow to its 
source surface. A miss means that the particle made it through or around all block­ing obstacles and 
reached the sky. As particles reach or are blocked from the sky they slowly build a picture of a given 
launch site s sky occlusion. Whenever a launch site has a suf.ciently different sky occlusionfrom an 
adjacent neigh­bour, a new launch site is added at the perturbed midpoint to re­.ne the transition. Likewise, 
launch sites can be merged whenever all surrounding neighbours have identical sky occlusions, usually 
in caseswheresites are consistentlycon.dentthatthey are eithercom­pletely exposed or completely occluded. 
As soon as we have generated a mass accumulation picture that meets some resource criteria (compute time, 
number of samples, size of sample or some other importance-driven function) we can add an appropriate 
(and arbitrary) amount of snow. This generates a complete set of 3D snow surfaces that rise off the base 
model. Since the addition of a layer of blocking and obscuring snow changes the previously computed mass 
accumulation pattern, we can repeat the accumulation step as often desired, increasing accuracy at the 
cost of computation time.  3.2 Importance Ordering The rationale for shooting upwards generally arises 
from the need for control: the idea that each individual surface can locally in.u­ence its resolution 
by deciding how many launch sites it needs, and how many particles each site should shoot. Since our 
sampling rate is orders of magnitude less complete than Nature s, prioritising the few samples we do 
have allows us to make better use of them. This ensures that even the tiniest surface is guaranteed at 
least a rough es­timate of snow accumulation. This is a major advantage over poten­tial approaches that 
drop blobby particles, since small surfaces are often missed at the expense of covering large ones. Figure 
16 shows Figure 4: (a) A bush leaning out over a wall provides a real example of the .ake-.utter phenomena. 
(b) After 10 seconds, importance ordering has found the general shape of the boundary. (c) After 100 
seconds, the boundary shape is essentially the same as (b), due to the importance ordering of launch 
areas. For illustration, neither (b) nor (c) have been smoothed. (d) Initial meshing of a crude bush 
model. No measurement of the real bush was done. (e) The denser mesh re.ects the more interesting areas. 
A signi.cant amount of re.nement occurs behind the bush and is not visible from this viewpoint. (f) The 
denser mesh after 100 seconds.  how our multi-scale approach covers individual blades of hay in the 
middle of a very large snowy .eld. Each launch site is given an importance ordering used to deter­mine 
order of site testing, determine the number of particles to shoot per site, and decide if more sites 
are needed nearby to improve the resolution. As long as the allocated time has not expired, the most 
important launch site shoots a small batch of particles, gets a new importance based on the results, 
and is placed back in sorted order. The importance ordering is a heuristic weighting based on the fol­lowing 
factors: . Completeness. Launchsiteswithnopreviouschancestoshoot are more important than sites that have 
had at least one chance, ensuring a crude global approximation exists before any fur­ther re.nement begins. 
. Area. As the area of a launch site increases, particles from a single site will pass through less of 
the volume immediately overhead. To prevent missing occlusions, large areas may need more particles per 
launch site and more initial sites. Oc­clusion boundaries in large areas are more visually obvious, and 
so gain preferential allocation of new re.nement sites. . Neighbourhoods. If the particle hit percentage 
of two neigh­bouring sites is suf.ciently different, it implies that there is a nearby obstacle causing 
some kind of occlusion boundary. Both sites gain importance, asking for more particles to im­prove knowledge 
of the shape, orientation and magnitude of the boundary. If the neighbours are suf.ciently different 
and important, a new re.nement site may be added to the per­turbed midpoint. Likewise, launch sites that 
are the same as all nearby neighbours become less important, and may be can­didates for removal. . Effort. 
If all other factors are equal, launch sites should use approximately the same number of particles, aiming 
for con­sistency of con.dence. . Limits. The user can set several parameters that limit the ap­proximate 
scale of the .nest allowable increase in resolution. This prevents launch sites from increasing inde.nitely 
along very complex occlusion boundaries. If all sites have been re­solved to this limit, the phase can 
terminate early. . Steepness. Very steep launch sites are swept of what little snow they accumulate; 
in most scenes, these avalanches are negligible compared to accumulations on nearby stable sites. . Camera. 
When optionally enabled, sites closer to the camera receive more particles, greater re.nement, and improved 
ac­curacy at the cost of imposed view dependence. . User. Importance ordering allows users to arbitrarily 
tag sur­faces as being boring -useful for ignoring areas that will eventually be occluded or matted out. 
We defer the reader to [1] for the actual parameterised importance weighting and further discussion of 
each factor. The important idea is that some launch sites get priority access to a limited sampling budget, 
based on criteria important to the user for a particular scene. Figure 4(a) shows the occlusion boundary 
under a real snow­covered bush, illustrating the type of visual effects we want sam­pling to determine. 
After 10 seconds, the importance ordering has found the boundary, and generated an initial approximation. 
Spend­ingan additional90secondsresults in moresubtle improvement,re­.ning launch sites of less visual 
interest. Background unoccluded areas are of very low importance, and so undergo almost no im­provement. 
 3.3 Launch Site Meshing Launch site surfaces are represented as triangles, generated from the original 
(potentially non-polygonal) base models. Once snow has been generated, the polygonal approximation of 
the underlying model is discarded, allowing snow to accumulate on the original, unchanged base scene. 
All upwards-facing triangles in the approximation of the under­lying model are initially allocated at 
least one launch site. Addi­tional launch sites are allocated based on the importance ordering of the 
surface, user-set resolution parameters, and the magnitude of the .ake-.utter. In order to properly allocate 
snow, each launch site must be re­sponsible for some non-overlapping portion of the surface, ideally 
the area immediately surrounding the sample point. We have cho­sen a strategy based upon Voronoi diagrams, 
although there are nu­merous other valid meshing possibilities. Launch sites are con­nected in a constrained 
Delaunay triangulation, where each launch site is responsible for its own immediately surrounding Voronoi 
area, clipped to the edge of the triangle for maximal surface inde­pendence. Advantages of this approach 
include fast point-in-area tests and neighbour location, and the ability to quickly generate tri­angulations 
for intersection testing. Figure 4 (d) shows an example of a sparse initial mesh undergo­ing the addition 
of more and more launch sites, shown in Figures 4 (e) and (f). Note how neighbouring constrained-Voronoi 
areas vary in size at the transition zones, and mostly minimise extreme angles. In practice, many surfaces 
are small and isolated (such as the brush and pine needles in Figure 1), and meshes are reduced to the 
trivial case of one or two samples in a triangle. Signi.cant meshing occurs on large, connected surfaces, 
such as the ground. Launch sites and their associated meshes are additionally divided into edge groups, 
which are isolated world objects, projected into the XY plane, bordered by the XY silhouette edges. Edge 
groups are used primarily for avalanche resolution, denoting sharp boundaries where snow may slide off 
from one edge group to another. Project­ing into XY implies that launch sites can only be placed on surfaces 
with an angle of repose of [0::90). . Since edge group silhouettes are not necessarily convex, we must 
do some additional processing to break constrained Delaunay neighbour links that cross a silhou­ette 
boundary or a hole in the mesh. A single edge group may also be arbitrarily broken into smaller edge 
groups, although this is inef.­cient since moving snow across group boundaries is more expensive than 
moving snow within the same edge group. Figure 5 shows how a sphere is converted into an edge group. 
 Figure 5: An isolated object (a), bordered by XY silhouette edges (in red) forms an edge group -top 
view (b), side view (c). Our particular meshing strategy means that we have trouble with certain types 
of connected models that overlap in Z, such as a helix. However, this can be .xed by either splitting 
the model s natural ob­ject hierarchy, or increasing the number of edge groups, ultimately reaching the 
level of a group per polygon, if needed. Figure 6 shows an overlapping Z model that our meshing algorithm 
considers hard. Note that although the knot was split into 200 edge groups, bound­aries between the groups 
are not visible in the .nal result. Figure 6: An object our meshing strategy considers hard . Knot model 
courtesy of [18]. 3.4 Locating Particles in the Sky When a launch site reaches the head of the importance 
queue, it shoots a batch of particles towards the sky. Batch size is user de.n­able, but generally within 
the order of 10-15 .akes. Particles orig­inate from the launch site s snow surface, potentially reaching 
the sky plane unimpeded and contributing to the growth of the parent. We use a simple bucketing and .ltering 
scheme to allocate the suc­cessful .akes to the total mass of the sky s available snow, while en­suring 
that small local areas of sky do not over-contribute. This is important, since the number of particles 
hitting any particular area of the sky may vary dramatically depending on the complexity of the underlying 
surfaces. We must ensure that a large concentration of .akes (say, directly above a tree), draws the 
same total snow as would the sky above a sparse .at surface. Furthermore, importance ordering implies 
that not all launch sites shoot the same number of particles. We divide the sky into a grid of constant 
size buckets. When a .ake reaches the sky successfully, we spread its representative area (de.ned as 
the launch site s projected area divided by the number of .akes in the current batch) across one or more 
buckets, as shown in Figure 7. When the snow accumulation phase .nishes, all sky buckets are allocated 
some mass based upon the arbitrary depth of snow desired. Each bucket b computes a mass per area value, 
based on available mass of and the summation of all representative .ake areas extend­ b ing into b. An 
individual launch site l then receives new mass pro­portional to the summation of the representative 
area of all .akes belonging to l that hit b. A single launch site may receive snow from multiple buckets. 
Flake area .ltering is done at the end of the accu­mulation phase, when a given launch site cannot change 
in area due to added or removed re.nement sites. Since a launch site s accumulation pattern may change 
with the addition of blocking snow, it is sometimes useful to split the de­siredsnowdepthupandrunthe 
accumulationphasemorethanonce. Depending on the time allocated for each phase, lower-importance launch 
sites may not get a chance to shoot particles every pass. To flake area is allocated to sky buckets 
Figure 7: Allocating .ake area to sky buckets. allow fair mass allocation to those launch sites, we keep 
.ake infor­mation in the sky until replaced by a fresher set of shot particles from a new pass. The allocation 
of snow mass to sky buckets is usually constant, although interesting effects can be obtained by multiplying 
bucket mass by an input image. Figure 8 shows a scene where the sky gen­erates very uneven amounts of 
snow. Figure 8: Non-constant allocation of snow mass to sky bucketing can be used to write the SIGGRAPH 
2000 logo with snow 3.5 Snow.ake Motion We simulate snow.ake motion with a series of straight-line vectors 
approximating a curved path, where vector length and end position are determined with a random walk process 
based upon a circle of radius fr, and Z step resolution is in.uenced by the importance or­dering. At 
each step, the value of is randomly chosen from a f r normal distribution. As approaches zero, .akes 
duplicate ver­ f r tical raycasting, producing no partial occlusion. As increases, f r the area of effect 
of a .ake widens, generally blurring occlusion boundaries and making it less obvious where bumps and 
depres­sions came from. In practice, it is hard to match .ake-.utter parameters with ob­served real scenes. 
We currently compare a grid of generated im­ages to .nd the parameters that best match the shape of boundaries 
found in a real scene. 3.6 Determining Particle/Surface Intersection In order to .nd particle/surface 
intersections, we allocate surfaces into a regular grid of XY buckets. Within each bucket, we com­pute 
the minimum and maximum Z values of the surface as it passes through the bucket bounding box. We then 
insert all Z ranges into a per-box range tree [23]. For a tree containing nranges, it takes O(log 2 n) 
per insert and delete, and O(log 2 +) to return a list nk of the kelements that overlap the Z query range. 
During the accu­mulation phase, rebucketing is only needed upon completion, when snow mass is added. 
During stability, rebucketing is done more of­ten, although with a considerable lag for ef.ciency reasons. 
 3.7 Surface Construction After snow allocation, each launch site is elevated by recently accumulated 
snow mass divided by the current launch site area. The polygonal top snow bounding surface is then the 
constrained-Delaunay triangulation of elevated launch sites, with corner ver­tices set to the minimum 
of adjacent neighbours. Additional vertical planes are included around edge group boundaries to close 
the sur­face down to the base plane. 3.8 Flake Dusting In many instances, accumulated snow is not thick 
enough to com­pletely obscure the underlying surface, appearing instead as a light dusting of .akes. 
This phenomena often occurs in areas of low snowfall, high instability, or on surfaces with microtexture 
bumps, such as tree bark. Since it is not practical to model dusting as thick 3D objects, we use already-computed 
snow occlusion percentages to generate procedural noise textures of the appropriate averaged dusting 
density. Dusting textures are semi-transparent, textured polygons oriented to .oat slightly in front 
of the original model. Figure 9 compares the texture dusting of a (slightly tilted) real and a computer 
generated sign. Figure 6 shows an example of the tran­sition between thick surfaces and .ake dusting 
textures. In a view-dependent scene, .ake dusting can be used to replace suf.ciently thin and distant 
snow layers with a white texture, reduc­ing the polygon count.  4 Snow Stability The snow stability 
phase of the algorithm is responsible for re­distributing recently accumulated snow mass into a con.guration 
that is stable, according to some very simple surface and snow prop­erties. It can be run at intermittent 
times as computational power and desired accuracy permit, usually immediately after snow accumula­tion. 
All launch sites are initially sorted by absolute Z height plus ac­cumulation, and placed in a list of 
unresolved sites The list u1. is examined in decreasing Z order, immediately resolving unstable launch 
sites as they are discovered. The resolution of a single launch site may affect a number of nearby neighbours: 
lower sites may s receive new snow from , while the loss of snow from may cre­ ss ate unstable angles 
with previously stable higher neighbours. Af­fected samples also include sites receiving edge-transit 
snow from s , or sites newly created to improve resolution. If not there already, all launch sites affected 
by , including ,are ss placed in a new sorted list u2. At the completion of an entire pass through 1, 
the list is destroyed and replaced with 2, and the entire uu pass is repeated until termination. The 
length of is not guaranteed to decrease on each pass, u 1 and in fact may increase, or undergo large 
.uctuations. Consider a large amount of very unstable snow on a wide .at surface. On the .rst pass, the 
vast majority of interior samples are considered stable, since they are at the same height as their neighbours. 
The band of instability exists only at the edges, where unsupported snow avalanches off into the void. 
As edge sites lose mass, adjacent inte­rior neighbours are affected, and the area of instability widens. 
For­tunately, the erosion of snow from the edges towards the centre is very physically plausible.  Figure 
9: (a) A real sign covered with real snow. (b) A computer generated sign covered with computer generated 
snow. Note how dusting density increases near the top and edges in both models. 4.1 Angle of Repose Despite 
the wide range of physical factors in.uencing real snow, for simplicity we base our stability test mainly 
on the angle of repose (AOR) of a particular snow type. The AOR measures the static fric­tion of a pile 
of granular material, and is one of the major parameters in.uencing our scene. It can range [6] [9] from 
near 90.in fresh dendritic snow to 15.in extreme slush conditions. For a given type of snow, we use a 
transition curve that models the probability of stability over a range of angles around the AOR. Increasing 
the width of the transition curve gives a stability solu­tion with bumpier surfaces and increased variation 
at snow bound­aryedgesnearthecriticalangle. Anarrowcurvegeneratessmoother surfaces with less variation. 
The AOR is based on the relative heights of accumulated snow, and not on the .xed angle of launch sites 
on the underlying surface. As snow drains from one launch site to another, the AOR changes continually. 
This means that launch sites on very steep surfaces may still support snow if the AOR of neighbouring 
sites is low enough, possibly because snow is blocked from moving away. Figure13 shows an example of 
this using water (AOR = 0.) .ll­ing a fountain basin. The basin sides are too steep to support water, 
so mass avalanches towards the basin bottom. As the basin .lls, this downward movement is blocked by 
the rising water level. Eventu­ally, the basin .lls to the brim, leaving a stable .at surface supported 
by the steep sides of the bowl. 4.2 Stability Test The actual stability test for a single iteration 
on launch site s can be described as follows: 1. compute AOR between and all neighbours lower than s 
sni 2. for each with an AOR too steep to support snow, perform an i obstacle test between and s ni i. 
if there is a non-snow obstacle in the way, the avalanche is blocked, and the neighbour is ignored. ni 
ii. if there is a vertical snow surface (an edge group bound­ary) in the way, there an interpenetrating 
surface car­rying snow between s and ni, so the avalanche is also blocked. iii. if there is a non-vertical 
snow surface in the way, there is an interpenetrating surface B between s and ni,where the interpenetrating 
surface could potentially receive the snow destined for ni. Replace with the closest launch ni site on 
B. 3. evenly shift snow from to all neighbours still in con­ s ni tention, until at least one neighbour 
becomes stable. 4. repeat steps 1 to 3 until all there are no unstable neighbours left, or s is bare 
of snow. n n ni i i   (a) (b) (c) Figure 10: (a), (b), (c) illustrate stability test obstacle cases 
i, ii, and iii respectively The obstacle test (step 2) checks to make sure that avalanche mo­tion is 
not blocked by intervening surfaces or snow belonging to other objects. If an obstacle is found, snow 
is blocked and forced to pile up unless there is an alternative escape direction or snow rises above 
the intervening obstacle. Figure 13 shows how blocked water rises above the level of the basin sides, 
transferring to the top of the basin edge, eventually over.owing into the next basin. Step 2 is expensive. 
Practically,we achievelarge speedupsby re­ducing the frequency of this step -from every test, to every 
pass, to once per stability phase, with corresponding decreases in accuracy. The most infrequent testing 
is usually suf.cient for models where there is little inter-object penetration, although some blocking 
due to rising snow will be missed. Figure 1 was computed using the fastest method. Figure 16, containing 
thousands of interpenetrat­ing and closely spacedgrass blades, was computedusing the slowest method. 
Any time there is a non-snow obstacle between two adjacent neighbours, we can optionally improve the 
way snow builds up against the obstacle by adding re.nement launch sites just before the intersection 
point. 4.3 Moving Snow over Edges If an unstable launch site has no downhill neighbours, it is next 
to an edge. Before snow cascades over an edge into the air, we per­form an intersection test with a very 
short vector oriented in the di­rection of avalanche motion. If an intersection is found, then some surfaceornearbysnowis 
suf.cientlycloseto theavalancheorigin to block movement. Blocked avalanches continue to accumulate un­til 
the origin launch site has enough snow to pass over the obstacle. If no intersection is found, the avalanche 
heads over the edge and is approximated as a few (usually . 5) avalanche particles mov­ing on a simple 
projectile trajectory. Avalanche particles are tracked downwards, bouncing off surfaces until reaching 
a surface support­ing launch sites. If the edge is on a shared boundary with an adjacent edge group, 
the particles end up hopping to the adjacent group via very short projectile motion. When an avalanche 
particle comes to rest, it contributes its snow load to the nearest launch site on the destination surface. 
Depending on user-set parameters, new launch sites may be created if existing launch sites are not dense 
enough to capture the pattern of falling snow. 4.4 Stability Termination Criteria A single pass of the 
stability algorithm reaches completion when it runs out of time, when the unresolved list becomes empty, 
or u1 whenallavalanchesin thelastpassmovedonlya verysmallamount of snow. In most scenes, the .rst few 
passes through u1resolve a major­ity of the unstable snow, with subsequent passes handling smaller and 
smaller avalanches. Forced early termination may leave unsta­ble areas, but all launch sites will usually 
have avalanched at least once. Our multi-pass approach avoids driving a large wave of snow downwards 
in a single pass, which leads to chaotic results on early termination. If the stability phase completes 
before the alloted time expires, we re-run the entire phase to compensate for some speed-accuracy tradeoffs, 
such as missed obstacle testing, and lag in the rebucket­ing of changing snow surfaces. The extra phase 
usually .xes a few missed sites and completes immediately.   5 Implicit Functions Importance-ordering 
accumulation algorithms are surface-based, implying that snow can only accumulate on supporting objects. 
To allow for unsupported snow, such as gap bridging, edge bulges and wind cornices, we perform an additional 
(optional) conversion step using implicit functions. Figure 2 shows an example where snow on many closely-spaced 
pine needles has formed unsupported bridges and clumps. Each snow volume is converted into one of several 
different im­plicit function types, as shown in Figure 12. Generator functions do not radiate uniformly. 
The one-sided edge function allows bulging and cornice formation, where size, bulging and direction are 
based upon wind velocity. The limited top function blends with snow directly above the generator surface, 
but does not blend much with adjacent neighbours. The resulting isosurface is polygonalized in O(n 2)space 
[22]. In order to reduce blending discontinuities and apparent mass in­.ation at function boundaries, 
we use known adjacency information to shrink and clip implicit functions so that the isosurface is coin­cident 
with the polygonal top surface. A small variable-radius line generator function blends cracks between 
adjacent functions, and smoothes over sharp creases in the snow. Our method is not en­tirely satisfactory, 
since surface cracks often remain visible -how­ever, they are often minimised suf.ciently to be destroyed 
during mesh reduction [2] after polygonization. blending linestop surfaces isosurface edge generator 
 (a) (b) Figure 12: (a) Side view of adjacent snow volumes. (b) Side view of adjacent top and edge generator 
functions, with crack-.lling blend­ing lines. Implicit functions potentially allow us to add animal tracks, 
wind ripples, and other patterns to snow surfaces by stamping the snow surface with appropriately scaled 
negative functions. By interrupting the pipeline before the implicit function step in Section 5, we obtain 
polygonal results with no bridging or smooth­ing effects and a much lower polygon count. These compact 
inter­mediate results are appropriate for scene setup and real-time view­ing, and may actually be suf.cient 
for the .nal image. Figures 4, 13 and 16 were computed without the smoothing step. As well, inter­mediate 
polygonal results can be used as the underlying model for a completely new snow accumulation run, producing 
the effect of true snow layers. 6 Rain and Wind By setting AOR = 0.and .ake-.utter fr 0 the basic snow 
al­gorithm can also simulate the accumulation of water, from the sky or elsewhere. Figure 13 shows an 
example of an empty fountain slowly .lling up with water. Only the patch of sky shown as a red square 
has any mass to contribute, approximating how water ap­pears at a spout, .lls the .rst basin, and over.ows 
to lower basins. Wind is a major factor in the large-scale transport of snow, pro­ducing some very compelling 
and interesting effects. Although we cannotclaimto duplicatetheseeffects,we atleasthaveaframework for 
simple wind phenomena in both snow accumulation and snow stability phases. During snow accumulation, 
wind in.uence is easily included by modifying a .ake s direction and distance by a velocity vector. Wind 
velocity vectors can be approximated with a constant direc­tion, or much more accurately computed of.ine. 
The foreground haystack in Figure 16 shows the asymmetrical accumulation effects of a very slight breeze 
to the right, where the wind in.uence is glob­ally constant. During stability, we widen our single-site 
stability test to include neighbours that are within 90. of the downwind direction. Snow transport is 
then dependent on the neighbour s angle with respect to the local wind vector, the duration of the wind 
in.uence, and the car­rying capacity of a given wind velocity, based on [10]. The instabil­ity vector 
is moved according to the rules of Section 4.2, including obstacle testing. We use a simple heuristic 
to compensate for the different num­ber of times each launch site may be stability tested. All launch 
sites compute a .ux maximum that is reduced and moved over some small number of stability trials. Unfortunately, 
if the stability phase is terminated early, some areas may not get a chance to move all al­lowable wind 
transport snow. Figure 14 shows an example of wind and stability effects using a simple, globally constant 
wind vector.  7 Validation Validation of snow-covered scenes is hard, in that snow observed outdoors 
is the result of uncontrollable and unknown environmen­tal factors. Creating arti.cial snow is beyond 
our capabilities as a graphics lab, so instead we restrict validation to observation, ask­ing the question: 
does our algorithm produce phenomena and/or effects that are observable in nature? However, we were able 
to perform a few simple experiments to show that our snow stability algorithms are at least plausible. 
We substituted sifted .our for snow, to improve controllability and show that our algorithms work for 
materials other than snow. Figure 15 shows a side-by-side comparison of real and computer generated .our 
scenes. Figure 9 shows an additional side-by-side validation image of .ake dusting.  8 Future Work Our 
initial focus was on a framework for snow generation, and as a result we ignored, simpli.ed, and actively 
avoided many extremely important physical properties and effects, including snow compres­sion and packing, 
layers, slab avalanches, snow creep, snow pack metamorphosis, melting, and solar in.uence. Other priorities 
include improving the overall smoothness of the .nal results. Our sampling method is very noisy, mainly 
due to the (relatively) tiny number of .akes used to extrapolate snow depth. Additionally, avalanching 
real snow distributes snow in a much wider and more complex cloud that we currently model with our few 
particles, leading to snow stalagmite artifacts, such as those near the foreground wall in Figure 1. 
Although we are able to arti.cially en­force surface smoothing, we have not done so in this paper. Timing 
results are not fully applicable to our importance ordering scheme, as models are usually allocated a 
running time convenient to the user. However, the timing bottleneck of snow as a useful ef­fect is the 
rendering phase, which is outside the scope of our current work. Large models such as Figures 1 and 2 
were given overnight for snow accumulation, yet required weeks to raytrace animations of several hundred 
frames. Rendering is aggravated by aliasing in moving scenes -such as the distant, tiny, white snowpatches 
resting on distant, tiny, dark needles shown in Figure 1. We are interested in physically realistic, 
multi-resolution snow shaders or rendering models that are fast and accurate. 9 Conclusions This paper 
describes a new algorithm for the creation of snow­covered models, using a novel particle location scheme 
that allows surfaces to independently control sampling effort needed to deter­mine accumulation. Separability 
of surface accumulation produces many useful side effects, including importance ordering, adaptive re.nement, 
smooth degradation upon early termination, and greater control of the .nal result. Our accumulation algorithm 
allows us simulate effects such as accumulation under obstacles, .ake dust­ing, wind, falling rain, and 
snow-writing . We have also presented a simple model of snow stability that han­dles avalanches, edge-transit 
snow, obstacles supporting and block­ing snow, materials other than snow, and mass transport due to wind. 
Additional features of the approach include support for snow bridges, cornices and various levels of 
model detail. Integration with commercial software allows us to snow upon existing models in a variety 
of formats, providing greater .exibility, power, and ease of use. Finally, we have shown that our approach 
is able to handle large, complex outdoor scenes consisting of hundreds of thousands of surfaces. It is 
our hope that this work will open up an entire new season to computer graphics, and will stimulate other 
researchers to explore the natural, glorious beauties of winter. 10 Acknowledgements Alain Fournier 
provided guidance and the haystack model, while colleagues and the anonymous reviewers provided many 
helpful suggestions. Most of base models were provided courtesy of Plat­inum Pictures.  models courtesy 
of Alain Fournier.   References [1] Paul Fearing. The Computer Modelling of Fallen Snow. PhD thesis, 
Dept. of Computer Science, University of British Columbia, July 2000. [2] Michael Garland and Paul Heckbert. 
Surface Simpli.cation Using Quadric Error Metrics. SIGGRAPH 97 Conference Proceedings, pages 209 216, 
August 1997. [3] Pat Hanrahan and Wolfgang Krueger. Re.ection From Layered Surfaces Due To Subsurface 
Scattering. Computer Graphics (SIGGRAPH 93 Conference Pro­ceedings), 27:165 174, August 1993. [4] Siu-chi 
Hsu and Tien-tsin Wong. Simulating Dust Accumulation. IEEE Com­puter Graphics and Applications, 15(1):18 
22, January 1995. [5] Wolfgang Krueger. Intensity Fluctuations And Natural Texturing. Com­puter Graphics 
(SIGGRAPH 88 Conference Proceedings), 22(4):213 220, Au­gust 1988. [6] Daisuke Kuroiwa, Yukiko Mizuno, 
and Masao Takeuchi. Micrometrical Proper­ties Of Snow. In InternationalConferenceon LowTemperature Science(Physics 
of Snow and Ice), volume 1, Part II, pages 722 751. Institute for Low Tempera­ture Science, Aug 1966. 
[7] Xin Li and Michael Moshell. Modeling Soil: Realtime Dynamic Models For Soil Slippage And Manipulation. 
SIGGRAPH 93 Conference Proceedings, 27:361 368, August 1993. [8] A. Luciani, A. Habibi, and E. Manzotti. 
A Multi-Scale Physical Model Of Gran­ular Materials. In Proceedings of Graphics Interface, pages 136 
137. Canadian Information Processing Society, 1995. [9] David McClung and Peter Schaerer. The Avalanche 
Handbook. The Moun­taineers, Seattle, Washington, 1993. [10] Malcolm Mellor. Engineering Properties Of 
Snow. Journal of Glaciology, 19(81):15 66, 1977. [11] Claude Monet. Wheatstacks, Snow Effect, Morning. 
Painting: oil on canvas, J. Paul Getty Museum, Los Angeles, 1891. [12] K. Muraoka and N. Chiba. A Visual 
Simulation Of Melting Snow. The Journal of the Institute of Image Electronics Engineers of Japan, 27(4):327 
338,1998. [13] K. Muraoka, N. Chiba, and I. Ohtawara. Snowfall Model For Simulating Close Views Of Snowy 
Landscapes. The Journal of the Institute of Television Engineers of Japan, 49(10):1252 1258,1995. [14] 
F.K. Musgrave, C.E Kolb, and R.S. Mace. The Synthesis And Rendering Of Eroded Fractal Terrains. Computer 
Graphics (SIGGRAPH 89 Conference Pro­ceedings), 23(3):41 50,July 1989. [15] T. Nishita, H. Iwasaki, Y. 
Dobashi, and E. Nakamei. A Modeling And Rendering Method For Snow By Using Metaballs. In Proc. EUROGRAPHICS, 
volume 16. European Association for Computer Graphics, 1997. [16] S.Premoze,W.Thompson,andP.Shirley.Geospeci.cRenderingOfAlpineTer­rain. 
In EurographicsRendering Workshop. European Association for Computer Graphics, June 1999. [17] D.A Robinson. 
Northern Hemisphere Snow Cover Charts. National Snow and Ice Data Center, http://www-nsidc.colorado.edu/NSIDC/EDUCATION/SNOW 
/snow Robinson.html, as of April 10, 2000. [18] Robert G. Scharein. Interactive Topological Drawing. 
PhD thesis, Department of Computer Science, The University of British Columbia, 1998. [19] Mikio Shinya 
and Alain Fournier. Stochastic Motion Motion Under The In.u­ence Of Wind. In Proc. EUROGRAPHICS, pages 
119 128. European Associa­tion for Computer Graphics, 1992. [20] Karl Sims. Particle Animation And Rendering 
Using Data Parallel Computation. Computer Graphics (SIGGRAPH 90 Conference Proceedings), 24(4):405 413, 
August 1990. [21] R. Sumner, J. O Brien, and J. Hodgins. Animating Sand, Mud and Snow. In Proceedings 
of Graphics Interface, pages 125 132. Canadian Information Pro­cessing Society, 1998. [22] Alan Watt 
and Mark Watt. Advanced Animation and Rendering Techniques. Addison-Wesley Publishing, Don Mills, Ontario, 
1992. [23] D.E. Willand. New Data Structures For Orthogonal Queries. SIAM Journal of Computing, 14(1):232 
253, 1985.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344810</article_id>
		<sort_key>47</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Time-dependent visual adaptation for fast realistic image display]]></title>
		<page_from>47</page_from>
		<page_to>54</page_to>
		<doi_number>10.1145/344779.344810</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344810</url>
		<abstract>
			<par><![CDATA[<p>Human vision takes time to adapt to large changes in scene intensity, and these transient adjustments have a profound effect on visual appearance. This paper offers a new operator to include these appearance changes in animations or interactive real-time simulations, and to match a user's visual responses to those the user would experience in a real-world scene.</p><p>Large, abrupt changes in scene intensities can cause dramatic compression of visual responses, followed by a gradual recovery of normal vision. Asymmetric mechanisms govern these time-dependent adjustments, and offer adaptation to increased light that is much more rapid than adjustment to darkness. We derive a new tone reproduction operator that simulates these mechanisms. The operator accepts a stream of scene intensity frames and creates a stream of color display images.</p><p>All operator components are derived from published quantitative measurements from physiology, psychophysics, color science, and photography. ept intentionally simple to allow fast computation, the operator is meant for use with real-time walk-through renderings, high dynamic range video cameras, and other interactive applications. We demonstrate its performance on both synthetically generated and acquired &#8220;real-world&#8221; scenes with large dynamic variations of illumination and contrast.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[adaptation model]]></kw>
			<kw><![CDATA[background intensity]]></kw>
			<kw><![CDATA[realistic image display]]></kw>
			<kw><![CDATA[rendering]]></kw>
			<kw><![CDATA[time course of adaptation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Filtering</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14020467</person_id>
				<author_profile_id><![CDATA[81350599694]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sumanta]]></first_name>
				<middle_name><![CDATA[N.]]></middle_name>
				<last_name><![CDATA[Pattanaik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Program of Computer Graphics, Cornell University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P130685</person_id>
				<author_profile_id><![CDATA[81100216430]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jack]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tumblin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Program of Computer Graphics, Cornell University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14194512</person_id>
				<author_profile_id><![CDATA[81543470756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hector]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Program of Computer Graphics, Cornell University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P68459</person_id>
				<author_profile_id><![CDATA[81100196982]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Donald]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Greenberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Program of Computer Graphics, Cornell University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Time-Dependent Visual Adaptation For Fast Realistic Image Display Program of Computer Graphics, Cornell 
University ABSTRACT Human vision takes time to adapt to large changes in scene inten­sity, and these 
transient adjustments have a profound effect on visual appearance. This paper offers a new operator to 
include these appearance changes in animations or interactive real­time simulations, and to match a user's 
visual responses to those the user would experience in a real­world scene. Large, abrupt changes in scene 
intensities can cause dramatic compression of visual responses, followed by a gradual recovery of normal 
vision. Asymmetric mechanisms govern these time­dependent adjustments, and offer adaptation to increased 
light that is much more rapid than adjustment to darkness. We derive a new tone reproduction operator 
that simulates these mechanisms. The operator accepts a stream of scene intensity frames and creates 
a stream of color display images. All operator components are derived from published quantitative measurements 
from physiology, psychophysics, color science, and photography. Kept intentionally simple to allow fast 
computa­tion, the operator is meant for use with real­time walk­through renderings, high dynamic range 
video cameras, and other interac­tive applications. We demonstrate its performance on both syn­thetically 
generated and acquired "real­world" scenes with large dynamic variations of illumination and contrast. 
CR Categories: 1.3.3 [Computer Graphics]: Picture/image generation - Display algorithms; 1.4.3 [Image 
Processing and Computer Vision]: Enhancement - Filtering. Keywords: Rendering, realistic image display, 
time course of adaptation, background intensity, adaptation model. 1. INTRODUCTION The human visual system 
can accept a huge range of scene inten­sities (from about 1O-6 to 1O+8 cd/m2, or 14 logIO units), because 
it continually adjusts to the available light in any viewed scene. These adjustments for viewed intensity, 
known as visual adapta­tion, occur almost entirely within the retina [4]. Surprisingly, the eye's iris 
diameter only mildly affects adaptation; its 2­8mm ad­justment range varies retinal illumination by only 
about 1 logIO unit [28]. Adaptation and its changes over time have profound effects on the visual appearance 
of any viewed scene. Continual adjustment helps keep the visual system acutely sensitive to scene content 
over a wide range of illumination, but adaptation also tends to hide or obscure any very slow changes 
in scene intensity or spec­tral content. For example, on an overcast late afternoon, an auto­mobile driver 
may not notice the loss of daylight; adaptation can hide the gradual lighting reduction until another 
car's headlights reveal the darkness. Adaptation can also exaggerate large, rapid changes in scene intensity. 
For example, on a sunny day (.1O+4 cd/m2), people entering a dim motion picture theater (.1O-I cd/m2) 
may see only blackness and the movie screen. Their vision is restored after tens of minutes of adaptation, 
and they may even see popcorn spilled under the seats (.1O-3 cd/m2). On leaving, people see only blinding 
whiteness, but in a few seconds their adaptation restores the normal appearance of a sunny day. This 
paper offers a practical, accurate, and fully automatic way to reproduce similar visual experiences caused 
by time­dependent adaptation, even when scene intensity changes greatly exceed display device abilities. 
We present a new time­dependent tone reproduction operator that can rapidly create readily displayable 
color image sequences from any desired input scene, either static or dynamic, real or synthetic. The 
operator is simple, uses global rather than local adaptation models, and may be robust enough for real­time 
use with interactive renderings, with output from high dynamic range video cameras, or for rapid evaluations 
of lighting designs. Scene Intensities Display Intensities Figure 1: Tone Reproduction Operator Overview. 
Our new operator follows the tone reproduction framework pro­posed by Tumblin and Rushmeier [19], and 
is built from a for­ward and inverse instance of a pair of perceptual models, as shown in Figure 1. The 
adaptation model transforms viewed 0 0 0 1 0 1 2 0 2 0 4 0 Time (seconds) Figure 2: Increment Thresholds 
During Dark Adaptation. Ability to sense small illumination changes develops slowly in the dark. Test 
subjects were first adapted thoroughly to a uniform background intensity. Experimenters removed the background 
light and then periodi­cally measured the test subject's thresholds in darkness. Five curves show results 
from five widely­spaced initial background intensities. Both rod and cone thresholds fall asymptotically, 
but cones (dashed lines, filled symbols) adapt to darkness more rapidly, and dominate threshold meas­urements 
until slower rod thresholds (solid lines, open symbols) can fall below them ([IOJ, Table III). scene 
intensities to retinal­response­like vectors R, and the ap­pearance model converts R to appearance vectors 
Q that express correlates of "whiteness/blackness" and "colorfulness." The up­per model pair computes 
viewed scene appearance, and the lower pair of inverse models computes display intensities that match 
the scene appearance. Our forward adaptation model is an abbrevi­ated version of an authoritative, sophisticated 
model of static color vision by R.W.G. Hunt and colleagues [15], augmented with exponential filters for 
time­dependent adaptation mechanisms. After reviewing related work in Sections 2 and 3, Section 4 pres­ents 
detailed derivations of the entire operator, and Section 5 demonstrates its performance on time­varying 
scenes. 2. PREVIOUS WORK There is a wealth of published psychophysical work measuring the performance 
of the human visual system as a function of steady background intensity [6, 8, 14, and 28]. These books 
and papers provide data about how thresholds increase, visual acuity im­proves, and motion, color, and 
contrast sensitivity increases with additional available light. During the last decade, computer graphics 
researchers [5, 17, 19, 2O, 25, 26 and others] have used these data to construct more perceptually accurate 
scene­to­display mappings. Published mapping methods compute displayed images with improved appearance 
for a wide range of scene inten­sities as they might appear under static (steady­state) viewing conditions. 
These models capture many of the stationary light­dependent aspects of viewed scene appearance. However, 
we found surprisingly little published work on time­dependent models of visual adaptation that is suitable 
for com­puter graphics. Though several authors, such as Graham &#38; Hood [9], Sperling &#38; Sondhi 
[18], Walraven &#38; Valeton [23] and Wil­son [27] have published extensive models of adaptation processes, 
the work primarily addressed psychophysical threshold experi­ments rather than the appearance of arbitrary 
image sequences. Accordingly, these models do not address appearance effects or consider the problems 
of displaying computed results. 1n the computer graphics literature, only Ferwerda [5] offers any time­dependent 
method, but his simple and clever model is not in ac­cordance with psychophysical data on response compression, 
and the method is restricted to step­like scene intensity changes. The work presented here is novel in 
three ways. First, the model is general: it accepts time sequences of arbitrary scene intensities. Next, 
it captures the appearance of widely varying amounts of adaptation, and includes both bleaching and network 
effects. Finally, the model is firmly grounded in published research results from psychophysics, physiology, 
and color science.  3. BACKGROUND Almost all known adaptation mechanisms are found within the retina, 
and each mechanism has its own time course. 1n addition to the mild effect of pupil diameter adjustments, 
the combined effects of receptor types, photopigment bleaching and retinal con­nection networks explain 
the huge span of human vision [4]. The human retina holds two types of photoreceptor cells. Cones sense 
color and respond well in dim to bright light (.1O-I to 1O +8 cd/m2), and rods respond best between darkness 
and moderate light (.1O-6 to .1O+I cd/m2), but are blinded by saturation above 1O +2 cd/m2[14]. Within 
their response ranges, receptors react when one of its "visual pigment" molecules captures a photon. 
The captured photon triggers a complex cascade of reactions known as "bleaching" that desensitizes the 
molecule. Bright light rapidly reduces a receptor's usable photopigment concentration, but slow retinal 
mechanisms restore it [4]. Photopigment con­centration sets an upper limit on receptor sensitivity, and 
simple rate equations can predict reasonably well how these concentra­tions change with time and light 
[14]. Unlike film or television camera sensors, individual receptor cells share interdependent signals. 
Two more neural cell layers in the retina process these signals (see [4] Chapter 4 for a masterful summary) 
and their interactions strongly affect adaptation and its time course. Extensive psychophysical experiments 
(see [12] and [9]) have revealed rapid multiplicative and subtractive adaptation mechanisms, and more 
may exist within the retina ([24], pg. 76). 3.1 Adaptation Measurements and Models Adaptation processes 
greatly complicate visual response function measurements because varying the test stimulus may cause 
adap­tation that changes the response function as well. At least two very different approaches to this 
problem are common in the vi­sion research literature. Psychophysicists often measure a test subject's 
ability to see test stimuli made so small, fast, or weak that adaptation does not change significantly, 
and physiologists measure the underlying biological mechanisms responsible for adaptation and light sensitivity. 
Both approaches offer only par­tial explanations of how adaptation affects visual appearance. 1ncrement 
threshold tests may offer the simplest measurements of adaptation. Test subjects first stare at a wide 
blank screen for enough time to adjust to its uniform "adapting intensity" 1a. Against this background, 
psychophysicists then show a small test spot of intensity 1a+.1 and quickly find the smallest detectable 
.1. For moderate spot sizes and all 1a greater than about 1O-4 cd/m2 , larger adapting backgrounds 1a 
cause larger increment thresholds .1. Over much of this range (.1/1a) is nearly constant, a relation 
known for over 14O years as the Weber­Fechner fraction. This fraction suggests adaptation acts as a normalizer, 
scaling scene intensities to preserve our ability to sense contrasts within it. A sudden change to background 
1a temporarily disrupts the simple monotonic (.1 vs. 1a) function. Figure 2 shows how log(.1) IO changes 
over time when test subjects are suddenly plunged into complete darkness after thoroughly adapting to 
one of five 1a in­tensities. Rods dominate retinal response while adapting from quickly but pauses near 
the minimum cone threshold (about -1 log(td)), then as rod thresholds finally fall below cone thresholds, 
1 slowly approaches the rod dark­adapted value. Direct cellular measurements on isolated and whole rat 
retinas by Dowling (1963), Weinstein, et al., (1967) and others (see sum­mary in [4], Chapter 7) offer 
further help. Their work showed dark adaptation in both rods and cones begins with a rapid de­crease 
in threshold governed almost entirely by retinal network interconnections, but this fall is limited to 
a level directly pre­dicted by photopigment concentrations. More recently, works by [1], [12] and [9] 
suggest these neural processes are complete in about 2OOmS for non­bleaching changes in adaptation. Figure 
3 shows increment threshold changes for a fully dark­adapted observer exposed to bright background light 
[2]. The entire light­adaptation process is much faster than dark adapta­tion, with a markedly different 
effect on thresholds. At the onset of the bright adapting light, 1 jumps immediately to a very high value, 
then quickly settles back towards its static value. Light adaptation also includes both a fast neural 
component [1, 9] and a slower, pigment­limited process. Though initially puzzling, these threshold behaviors 
are reasonably well explained by examining retinal mechanisms. Most retinal cells vary their response 
only within a range of inten­sities that is very narrow if compared against the entire range of vision. 
Adaptation processes dynamically adjust these narrow response functions to conform better to the available 
light. Direct cellular measurements of response functions for cone, rod, and bipolar cells [4] and firing 
rates for sustained ON­center retinal ganglia [24] closely follow: I n R(I) = Rmax (1) In +( n an S­shaped 
curve (see multiple examples in Figure 5) where I is light intensity, R is neural response (O < R < Rmaa), 
semi­saturation constant a is the I value that causes the half­maximum response, and n is a sensitivity 
control similar to gamma for video, film, and CRTs. 1ntroduced by Naka and Rushton in 1966 to describe 
fish S­potentials [14], this hyperbolic function appears repeatedly in both psychophysical experiments 
with flashed test stimuli [13, 1, 23, 27] and widely diverse, direct neural measurements [4, 7, 8, 22]. 
Psychophysical experiments modeling adaptation and satu­ration in rods [1] and cones [14] using Equation 
1 show both Rmaa and a depend on both I and time. Equation 1 helps explain why threshold values differ 
so markedly during dark and light adaptation in Figure 2 and Figure 3. Sup­pose we choose values for 
Rmax and a to describe the visual re­sponse of a light­adapted observer, and make the simple assump­tion 
(as did [9],[14]) that thresholds measure some small fixed increment in response value R. Sudden darkness 
will not imme­diately change the viewer's response function, and though most scene intensities will fall 
well below a in Equation 1, at first the threshold value LI is only weakly affected (for I near zero, 
R . (I/ay ; to change R by some small constant amount requires a 1 value that is nearly unaffected by 
I). Over time, adaptation will ITrolands (td) merge scene intensity with pupil area to estimate retinal 
illuminance: td = (1ntensity in cd/m2).(pupil area in mm2). 4.0 dim light (1=2.64 log(tdI)) to darkness, 
and 1 falls asymptoti­2.0 aIO cally to its dark­adapted value in just a few minutes. The time = course 
of cone adaptation becomes important in the transition 3.5 > = from bright light (4.67 log(td)) to darkness. 
First 1 drops IO 00 ; = 1.0 0= 3.0 0.0 Time (seconds) Time (seconds) > Figure 3: Increment Thresholds 
During Light Adaptation. Dark­adapted observers suddenly exposed to bright background light of O.5 log 
Trolands for rods (left [IJ) or to 3.7 log Trolands for cones (right [2J) initially experience very high 
thresholds, but these rapidly decay back to their static values. gradually reduce the threshold by adjusting 
a and Rmax to their dark­adapted values as shown in Figure 2. However, once fully dark­adapted, suddenly 
large I values (I »»a) cause large response values near Rmax, and any noticeable increase in this response 
requires a huge additional 1, a property known as "response compression" [23]. Only when adaptation brings 
a nearer to the new, much brighter I values will the response and threshold fall to their static values. 
Beginning in 198O, R. W. G. Hunt and colleagues have assembled and continually refined an intricate mathematical 
model of human color vision for use in printing, photography, and video [15]. His model is a masterful 
synthesis of published data from psycho­physics, physiology and color science, and is suitable for critical 
evaluations of many forms of color image reproduction. Briefly, Hunt's model uses Equation 1 to estimate 
rod and cone responses to a viewed image, along with careful modeling of color response, to include numerous 
important subtleties. We will use parts of Hunt's model in Section 4.1.  3.2 Display Interpretation 
Light­adapted response alone does not entirely explain the visual appearance of displayed images: humans 
easily accept and under­stand reduced­contrast image renditions printed in newspapers or seen on poorly 
adjusted CRTs under high ambient light levels. We prefer higher contrasts, as confirmed by both formal 
studies and advertisements for film, CRTs and printers, but do not require them. For example, bright 
office light has limited CRT display contrasts to only 18:1 at the computer used to write this text. 
Nevertheless, the displayed text and figures appear as dark, rich black against a clean, paper­white 
background. Extensive studies of visual appearance and preference in photo­graphic prints and transparencies 
by Jones, Nelson, Condit, Bar­telson, Breneman and others (summary in [16]) offer some useful insights. 
Each surmised that viewers estimate scene intensities by comparing display intensities against mental 
estimates of refer­ence white and possibly reference black. These values describe the display intensities 
needed to represent scene objects with very high and very low diffuse reflectances viewed under prevalent 
scene lighting. Hunt uses both reference white and reference black in his model, and we closely follow 
his work.  4. DERIVING THE OPERATOR This section provides a complete description of all parts of our 
time­dependent tone reproduction operator. The operator makes a displayable sequence of RGB images from 
an input sequence of scene values expressed in cd/m2 or similar units, even if the input values are not 
displayable. The operator creates one display im­age for each frame of input data, and keeps only a handful 
of 20 40 60 80 0.0 0.5 1.0 1.5 80 0 1.o o.  o.  o.s Figure 4: Detailed Tone Reproduction Block Diagram. 
The . o o.2 o.1 o.o s 1.o o.   o.2 o.1 o.o Figure 5: Model of Rod &#38; Cone Response. These plots 
of Rrod and adaptation and appearance models of Equations 2­8 are diagrammed and assembled here according 
to the scheme shown in Figure I. Thick lines carry pixel­by­pixel image data; thin lines convey scalars 
or vectors that apply to all pixels in an image but may vary over time. Dotted line divi­sions denote 
major sections shown in Figure I. time­dependent state variables from frame to frame; multiple frame 
buffers are not required. By following the outline given in Figure 1, we construct the complete operator 
as diagrammed in Figure 4. The next two subsections describe the operator's adap­tation and appearance 
models. 4.1 Adaptation Model The adaptation model acts as an idealized, film­like eye with uni­form 
resolution and no localized differences in adaptation. At any instant, the same function governs response 
to light at all points in the scene. For each scene pixel, our model computes retina­like response signals 
Rrod and Reo e for rod and cone luminance and response vector Reolor for color information. Our adaptation 
model is a judicious simplification of Hunt's static model of color vision [15] that adds new, time­dependent 
adapta­tion components. These four components separately mimic the fast neural adaptation attributed 
to retinal interconnections (net­work) and the much slower process of photopigment bleaching and regeneration 
in both rods and cones. 4.1.1 Static Response As shown in Figure 4, we begin by converting scene RGB 
values or radiances into luminance values for rods and cones (C1E stan­dard Y', Y), labeled Lrod and 
Leo e respectively. We use only color­ratio components (red/Leo e, green/Leo e, blue/Leo e) for color. 
As in Hunt's model (see [15], pg. 712, 721), we compute both rod and cone luminance responses using Equation 
2, with the maxi­mum response R given by a photopigment bleaching term B: maa Reone vs. luminances Lrod 
and Leone were drawn with fixed adaptation lu­-52 -42 +62 minance amounts Aeone = Arod = 2 IO, IO, . 
, IO, IO+7cdlll. Crosses and circles mark response to adaptation luminance and "reference white" respectively. 
Note that rods adapted above about IO cdlll are saturated, with little or no response, as are cone above 
IO+5.cdlll. nn LL rod cone R rod = Brod n , Rcone = Bcone n(2) Ln +( L +( rod rod cone conen As before, 
R is the response to luminance L, and both B and a are determined by adaptation to overall scene luminance. 
Our model is primarily concerned with variable responses to luminance, and discards the sophisticated 
color calculations performed by Hunt's model. 1nstead, we only approximate the amount of color com­pression 
caused by Equation 2 at Leo e and apply it to the color ratio vector C to make color response vector 
Reolor: yScolor R = ((red,green,blue y/ L , color cone where: dRn . BL ( == (3) d log(L ) (L + ( y 
 Hunt slightly modified the direct cellular measurements of Vale­ton and Van Norren [22] to restore 
pupil area effects removed from their data. He decreased n slightly to n = O.73 and broad­ened the response 
range by about one log unit. Following Hunt's IO suggestion to define "reference white" as five times 
the adaptation luminance, the half­saturation parameters a for rods and cones become: 2.5874 A :od ( 
rod = (4) ll19000 A:od + 0.2615(1- )4 A.::od. 12.9223 Acone (= (5) eone 4 42 II3 kA +O.171(1 -k ) A cone 
cone 11 where: = and k = . 5 5x1O A+15A +1 rod eone Hunt's bleaching parameters for rods and cones are: 
6 2 x l0 0.04 B = and B = (6) cone 6 rod 2 x l0 + A 0.04 + A cone rod Note B shrinks rapidly towards 
zero for A»»1 cd/m2 to mimic rodrod rod saturation. The resulting static response model produces the 
curves for rods and cones shown in Figure 5. Our dynamic model will vary only the horizontal position 
(a) and amplitude (B) of these curves. Unlike Hunt's static values, we use separately computed, time­varying 
adaptation amounts (A, A) to compute a, and bleaching and regeneration kinetics of pigments to compute 
B values for rods and cones. We rename Hunt's static (A, A) rodeo erodeo e values as (G,G), the 'goal' 
adaptation values eventually reached if the current scene is held fixed. 1n the next section, the goal 
amounts computed for each frame drive calculations of time­varying adaptation effects. rodeo e   4.1.2 
Dynamic Response Hunt's model assumes scene viewers have achieved a static, steady state of adaptation 
where A= Gand A= G. rod rod eo e eo e Typically, adaptation is measured by the amount of light required 
for a viewer to reach the same state while staring at a uniform blank background. Our new time­dependent 
or dynamic model will eventually reach this same state given enough time, but to model transient effects 
we use four separate time­dependent terms. 'A' terms describe fast, symmetric neural effects and are 
used to compute the avalues of Equations 4 and 5. 'B' terms model slower asymmetric effects from pigment 
bleaching, regen­eration and saturation effects, and set response amplitudes as R maa did in Equation 
1. To compute 'A' and 'B' values, we first find the steady­state or goal adaptation values Gand Gfor 
the current input frame rod eo e of scene data. Several methods are plausible, and the best choice may 
depend on the application. 1n accordance with Hunt's 'refer­ence white' values, we chose G values as 
one­fifth of the paper­white reflectance patch in the Macbeth chart for the image se­quence on this paper's 
title page, but, in the movie sequence ex­cerpted in Figure 7, we used the 1­degree foveal weighting 
method found in Ward­Larson et al.[26], and directed the foveal center to the roadway surface as a driver 
might. Users may also wish to aim this foveal weighting interactively as was done by Tumblin et al. [21]. 
Either method is a valid choice for the block labeled "Adaptation Goal Finder" in Figure 4. Exponential 
decay functions are often used to model temporal processing of the visual system (see [18, 7, 1, 23, 
12, 9]). Fol­lowing this tradition, we chose to model the four adaptation sig­nals A, A, B, and Bwith 
two forms of exponential smoothing filters applied to the adaptation goal signals G or rodeo erodeo e 
rod Gcomputed for every frame. The outputs of these filters are eo e smoothed, delayed versions of their 
inputs, and Figure 6 illus­trates our discrete implementation for both types. We compute the fast, neurally­driven 
adaptation values A and rod Afrom goal values Gand Grespectively using simple eo e rod eo e fixed exponential 
filters where J and K functions are a fixed scale factor F: J(x)=K(x)=Fx. The response of these filters 
to a unit­  height step­like input as in Figure 6 is given by (1-ey, where t is time and tis the "time 
constant." To find the constant F, just O  apply the discrete time­step size T to get F=(1-e y. We chose 
tvalues by curve­fitting to the dark­adaptation time course O data from [3] after discounting regeneration 
effects on measured thresholds, yielding t= 15OmS and t= 8OmS. These data were measured on stimuli that 
caused significant bleaching, and our independent estimates agree reasonably well with non­bleaching 
measurements published by [1], [12] and [9]. We do not distinguish between multiplicative and subtractive 
adaptation in our model because the former is usually complete within one or two frame times. O rod O 
eo e in   LJ(x)L s LxL out LK(x)L s LxLout:in: in: Figure 6: Discrete Exponential Smoothing Filter. 
We compute time­dependent bleaching factors B, Bby ex­ rodeo e tending Hunt's static expressions in Equation 
6 to include pig­ment kinetics (see [14], page 5­55, eqns. 1O­17). The exponential filter of Figure 6 
supplies the kinetics by its J and K functions. The J() functions describe the pigment depletion rate 
that depends on both the current amounts of light (G) and on the pigment con­centration (B). The K() 
functions describe the competing process of pigment regeneration, which depends only on the pigment con­centration 
(B). For rods, let in = G(current value) and out = rod B, (previous result), then find B for the next 
time­step using: rodrod out 1-out J (y=T ..in; K (y out =T . . in (7a) rod rod 16 Trod Similarly for 
cones, let in = G(current value) and out = B eo e eo e (previous result) then find B for next timestep 
using: eo e out 1-out in inT . . J (y=T .. ; K (y out = (7b) cone cone 2.2 x1O8 Tcone A published consensus 
on regeneration time constants is '=11O eo e seconds for cones and '=4OO seconds for rods [14]. Be sure 
to rod use cd/m2 units in Equations 4­7 to agree with all constants. 4.2 Visual Appearance Model Our 
visual appearance model is simple but extremely important to the behavior of the overall tone reproduction 
operator. As seen in Figure 5, even after adaptation, the response to a bright scene can be much stronger 
or weaker than any response achievable by a dim CRT display or a photographic print. Our appearance model 
assumes humans can assign equivalent appearance to dim displays and very bright or very dark scenes by 
a simple linear mapping of visual responses. The model determines "reference white" and "reference black" 
responses from among the current visual re­sponses, and judges the appearance of any visual response 
against these reference standards. As shown Figure 4, our appearance model computes luminance appearance 
values QLum by subtracting reference black response REFblk from Rlum, where Rlum = R + rod R. We follow 
Hunt's suggestion and determine reference eo e white as five times the current adaptation level and reference 
 black as 1/32 the intensity of reference white. For our time­dependent adaptation model, we find the 
response to reference white and black as: REF = R + R (8) ,ht rod Lrod Arod eone -.· -.· Lcone Acone 
REF=R + R      (vertical line means "evaluated when"). We also compute the width and midrange of 
visual response as Qspa = (REFwht­REF) blk and Qmid =O.5(REFwht + REF). Color appearance values Qare 
set by Rvalues of the scene. blk eolor eolor 4.3 The Tone Reproduction Operator We can now assemble a 
new, time­dependent, tone reproduction operator by devising an inverse appearance and adaptation model 
to convert scene appearance values Q, Q, Qmid and Qspa eolorLum backwards into display intensity or RGB 
values as shown in Figure 4. We will explain these last models in reverse for clarity. The inverse adaptation 
model finds display RGB values for a given set of visual response values Rand RFor simplicity, lum eolor. 
we assume the display device gamma is 1.O, forcing proportional­ity between RGB and display intensity 
values. Compared to the input range of the human visual system, the output range of most displays is 
quite small and usually cannot cause large changes in the visual adaptation values we compute for the 
display observer. (Some exceptions exist; brilliant video projectors viewed in an otherwise dark room 
can change viewer adaptation dramatically). For simplicity, we assume display observers have fixed, steady­state 
adaptation amounts. For the results shown in this paper, we assume a typical CRT display in ordinary 
office lighting, and set A = A = Ldisplay = 25 cd/m2 , rodeo e REF wht = 125 cd/m2 , REF= 4 cd/m2 , blk 
a = 722 cd/m2 , a = 646 cd/m2 , rod eo e B= 1, B= O.OO16. eo e rod We compute display luminance using 
these constants and the inverse of Equation 2. For simplicity, we do not compute Equa­tion 3 for color, 
but instead compute a constant display S value d from a forward­difference estimate of the slope of Equation 
2 measured between the display REFwht and display REF. For our blk typical CRT with a maximum contrast 
of 32:1, we set S= O.1383. d We raise color appearance value Qto the power 1/Sto con­ eolor d vert it 
to display color ratio C. d The inverse appearance model is only slightly more complicated. The model 
attempts to do the least harm to visual appearance in translation to the display. We assume display observers 
will ac­cept minimal amounts of response offset and compression, but will object to any temporal discontinuities 
or response exaggera­tions. We also assume the display minimum and maximum values will evoke REFand REFwht 
responses in the viewer, and our blk inverse observer model attempts to map scene appearance values Q 
to a display observer's response values with as little distortion as possible using the following rules: 
1. 1F the display can directly reproduce scene visual responses, do so. Exactly cancel the offset to 
Rthat was applied by Lum the forward appearance model. ELSE 2. 1F scene Qspa »display Qspa , compress 
and offset scene Q Lum to match scene REFwht and REFto display REFwht and blk REF. ELSE blk 3. 1F scene 
Qmid»display Qmid, offset scene Qdownwards only L enough to ensure scene REFwht : display REFwht. ELSE 
4. Offset scene Qupwards only enough to ensure display Lum REF: scene REF. blk blk 5. RESULTS We have 
constructed two examples to demonstrate the perform­ance of our time­dependent tone reproduction operator 
on both real­world and synthetically­generated scene intensity data. The strip of images across the top 
of the title page of this paper shows display images computed by our operator from two photographs of 
the same scene under widely different illumination conditions. The first frame in the sequence (t=O) 
shows predicted scene ap­pearance for an observer statically adapted to the moonlight illu­mination from 
the side of the scene, where A= A= O.O1 cd/m2. 1mmediately after this frame, the scene was suddenly lit 
by brilliant overhead illumination equivalent to mid­day sun: adapta­ rod eo e tion goal values are G= 
G= 1OOOcd/m2 . 1n the next frame (T=3OmS) the lighting has changed, but the scene viewer's adap­tation 
state has not; a combination of clipping and response com­pression produces a displayed image that is 
almost entirely white. 1n subsequent frames, rapid retinal network adaptation increases rod and cone 
a values to reduce response compression and restore the colorful appearance of daylight illumination. 
rod eo e 1n the second example, we simulate driving through a long high­way tunnel on a sunny day, and 
Figure 7 shows frames from a videotape that accompanies this submission. As the daylight­adapted driver 
enters the tunnel, scene lighting falls quickly from about 5,OOO cd/m2 to 5 cd/m2 , and the driver is 
temporarily blinded while driving at highway speeds due to response com­pression. The driver's vision 
is again disrupted, though only very briefly, on leaving the tunnel. Graphs below each video frame show 
the time­varying scene­to­display mapping applied by the tone reproduction operator. The effects of the 
exponential filters used to drive a and a are rod eo e evident in slower adjustments on entering than 
on leaving the tunnel, and are demonstrated by the shift in the scene­to­display graph in response to 
large changes in scene lighting. 6. CONCLUSION AND FUTURE WORK We have presented a simple time­dependent 
tone reproduction operator to reproduce the appearance of scenes that evoke changes to visual adaptation. 
Though the operator uses a broad range of published data, its global model of adaptation does not require 
extensive processing and is suitable for use in real­time applica­tions. The operator is entirely automatic. 
We have demonstrated its effectiveness on both real­world and synthetic sources, and in both still and 
moving image sequences. Though the results are simple to compute and have a pleasing and plausible appearance, 
tremendous opportunities for further im­provements remain. An obvious refinement would include more of 
R. W. G. Hunt's model of static color vision and provide dy­namic color adaptation. The operator could 
be improved by add­ing other secondary effects of adaptation, such as after­images, noise processes, 
and even loss of acuity under low­light condi­tions as already addressed by Ferwerda [5] and Ward [26]. 
More substantially, local adaptation effects are a vitally important part of visual appearance, and multiple 
instances of the tone reproduc­tion function developed here might be applied to localized com­ponents 
of the scene if the proper scene decomposition could be found. Finally, a time­varying inverse adaptation 
model might further increase the accuracy of the displayed images. ACKNOWLEDGEMENTS We thank SuAnne Fu 
for designing the tunnel model used in our illustration, and Jonathan Corson­Rikert and Peggy Anderson 
for carefully proof­reading the paper. This work was supported by the NSF Science and Technology Center 
for Computer Graphics and Scientific Visualization (ASC­892O219) and by the MRA parallel global illumination 
project ASC­9523483, and performed using equipment generously do­nated by Hewlett­Packard and 1ntel Corporation. 
REFERENCES [1] Adelson, E. H. (1982). Saturation and adaptation in the rod system. Vision Res., 22, 1299­1312. 
[2] Baker, H. D. (1949). The course of foveal light adaptation measured by the threshold intensity increment. 
Journal of the Optical Society of America, 39, 172­179. [3] Crawford, B.H. (1937). Change of visual sensitivity 
with time. Proc. of the Royal Soc., B123, 69­89. [4] Dowling, J. E. (1987). The Retina: An approachable 
part of the brain. Cambridge: Belknap. [5] Ferwerda, J. A., Pattanaik, S.N., Shirley, P. and Greenberg, 
D. P. (1996). A model of visual adaptation for realistic im­age synthesis, S1GGRAPH 96, 249­258. [6] 
Finkelstein, M. A., Harrison, M., and Hood, D.C. (199O). Sites of sensitivity control within a long wavelength 
cone pathway. Vision Research, 3O, 1145­1158. [7] Geisler, W. S. (1981). Effects of bleaching and back­grounds 
on the flash response of the cone system. Journal of Physiology, 312, 413­434. [8] Graham, N. (1989). 
Visual pattern analyzers, New York: Oxford University Press. [9] Graham, N. and Hood, D. C. (1992). Modeling 
the dynam­ics of light adaptation: the merging of two traditions. Vision Research, 32, 1373­1393. [1O] 
Haig, C. (1941). The course of rod dark adaptation as influ­ enced by the intensity and duration of pre­adaptation 
to light. Journal of General Physiology, 24, 735­751. [11] Havard, J. (1991). New techniques and technology 
for tun­nel lighting. Public works, November 1991, 122, 66­68. [12] Hayhoe, M. M., Benimoff, N. 1. and 
Hood, D. C. (1987). The time­course of multiplicative and subtractive adaptation process. Vision Research, 
27, 1981­1996. [13] Hood, D. C. and Finkelstein, M. A. (1979). Comparison of changes in sensitivity and 
sensation: implications for the re­sponse­intensity function of the human photopic system. Journal of 
Experimental Psychology: Human Perceptual Performance. 5, 391­4O5. [14] Hood, D. C. and Finkelstein, 
M. A. (1986). Sensitivity to light. 1n Boff, K. R., Kaufman, L. R. and Thomas, J. P. (ed.), Handbook 
of Perception &#38; Human Performance, Chapter 5, New York: Wiley. [15] Hunt, R. W. G. (1995). The Reproduction 
of Colour, Chapter, Fountain Press, England. [16] Nelson, C. N.(1966). The theory of tone reproduction, 
in James, T. H. (ed.), The Theory of the Photographic Process, 3rd ed. Chap. 22, 464­498, New York. [17] 
Pattanaik, S. N., Ferwerda, J. A., Fairchild, M. and Green­berg D. P. (1998). A multiscale model of adaptation 
&#38; spa­tial vision for realistic image display. Proceedings of S1GGRAPH 98, 287­298. [18] Sperling, 
G. and Sondhi, M. M. (1968). Model for visual luminance discrimination and flicker detection. Journal 
of the Optical Society of America, 58, 1133­1145. [19] Tumblin, J. and Rushmeier, H. (1993). Tone reproduction 
for realistic images, 1EEE Computer Graphics and Applica­tions, 13(6), 42­48. [2O] Tumblin, J. and Turk, 
G. (1999). LC1S: A boundary hierar­chy for detail­preserving contrast reduction. Proceedings of S1GGRAPH 
99, 83­9O, Los Angeles. [21] Tumblin, J. Hodgins, J., and Guenter, B. (1999) Two meth­ods for display 
of high contrast images. ACM Transactions on Graphics, 18(1), 56­94. [22] Valeton, J. M. and van Norren, 
D. (1983). Light adaptation of primate cones: An analysis based on extracellular data. Vision Research, 
23, 1539­1547. [23] Walraven, J. and Valeton, J. M. (1984). Visual adaptation and response saturation. 
1n van Doorn, A. J., van de Grind W. A. and Koenderink J. J. (Ed.), Limits of Perception, The Netherlands:VNU 
Science Press. [24] Walraven, J., Enroth­Cugell, C., Hood, D. C., MacLeod, D. 1. A., and Schnapf, J.L. 
(199O) The Control of Visual Sen­sitivity. 1n Spillmann, L., and Werner, J. A. (ed.), Visual Perception: 
The Neurophysiological Foundations, Chapter 5, San Diego CA: Academic Press. [25] Ward, G. (1994). A 
contrast­based scale­factor for lumi­nance display. 1n Heckbert, P. S. (Ed.), Graphics Gems 1V, Boston: 
Academic Press Professional. [26] Ward­Larson, G., Rushmeier, H. and Piatko, C. (1997) A visibility matching 
tone reproduction operator for high dy­namic range scenes. 1EEE Transactions on Visualization and Computer 
Graphics, 3(4), 291­3O6. [27] Wilson, R.H. and Kim, J. (1998). Dynamics of a divisive gain control in 
human vision, Vision Research, 38, 2735­2741. [28] Wyszecki, G., and Stiles, W. S. (1982). Color Science. 
New York: Wiley.  Figure 7: Tunnel Lighting Predictions Selected frames from a video sequence (available 
on Proceedings videotape) that combines our operator with global illumination solutions are used here 
to predict the appearance and safety of highway tunnel lighting. Roadway intensity varies between 5 and 
5,OOO cd/m . Tunnel designers ordinarily pro­ vide strong lighting just inside tunnel entrances to allow 
daytime drivers sufficient time at highway speeds to adapt to dim interior lighting [11], but excluding 
the lights herecauses a dangerous momentary loss of vision while driving. Graphs below each frame show 
the time­varying, scene­to­display mapping curves computed by our tone repro­duction operator; note the 
time­varying position and shape of the curves, and the pronounced but temporary response compression 
at the ends of the tunnel.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344812</article_id>
		<sort_key>55</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Toward a psychophysically-based light reflection model for image synthesis]]></title>
		<page_from>55</page_from>
		<page_to>64</page_to>
		<doi_number>10.1145/344779.344812</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344812</url>
		<abstract>
			<par><![CDATA[<p>In this paper we introduce a new light reflection model for image synthesis based on experimental studies of surface gloss perception. To develop the model, we've conducted two experiments that explore the relationships between the physical parameters used to describe the reflectance properties of glossy surfaces and the perceptual dimensions of glossy appearance. In the first experiment we use multidimensional scaling techniques to reveal the dimensionality of gloss perception for simulated painted surfaces. In the second experiment we use magnitude estimation methods to place metrics on these dimensions that relate changes in apparent gloss to variations in surface reflectance properties. We use the results of these experiments to rewrite the parameters of a <italic>physically-based</italic> light reflection model in <italic>perceptual</italic> terms. The result is a new <italic>psychophysically-based light reflection model</italic> where the dimensions of the model are perceptually meaningful, and variations along the dimensions are perceptually uniform. We demonstrate that the model can facilitate describing surface gloss in graphics rendering applications. This work represents a new methodology for developing light reflection models for image synthesis.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[experimentation]]></kw>
			<kw><![CDATA[gloss]]></kw>
			<kw><![CDATA[human factors]]></kw>
			<kw><![CDATA[light reflection models]]></kw>
			<kw><![CDATA[visual perception]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Modeling and recovery of physical attributes</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Human Factors</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP43116383</person_id>
				<author_profile_id><![CDATA[81100112064]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fabio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pellacini]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Program of Computer Graphics, Cornell University, 580 Rhodes Hall, Ithaca, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P132198</person_id>
				<author_profile_id><![CDATA[81100459651]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Ferwerda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Program of Computer Graphics, Cornell University, 580 Rhodes Hall, Ithaca, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P68459</person_id>
				<author_profile_id><![CDATA[81100196982]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Donald]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Greenberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Program of Computer Graphics, Cornell University, 580 Rhodes Hall, Ithaca, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Aida, T. (1997) Glossiness of colored papers and its application to specular glossiness measuring instruments. Systems and Computers in Japan, 28(1), 1106-1118.]]></ref_text>
				<ref_id>Aida97</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[American Society for Testing and Materials. (1989) Standard practice for establishing color and gloss tolerances (Designation: D3134-89). Annual Book of ASTM Standards, 324-329.]]></ref_text>
				<ref_id>Astm89</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Billmeyer, F.W. and O'Donnell, F.X.D. (1987) Visual gloss scaling and multidimensional scaling analysis of painted specimens. Color Res. App. 12(6), 315-326.]]></ref_text>
				<ref_id>Bill87</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Blake, A. and Bulthofl, H. (1990)Does the brain know the physics of specular reflection? Nature, 343, 165-168.]]></ref_text>
				<ref_id>Blak90</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563893</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Blinn, J.F. (1977) Models of light reflection for computer synthesized pictures. Computer Graphics (SIGGRAPH 77 Conference Proceedings), 11 (4), 192- 198.]]></ref_text>
				<ref_id>Blin77</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Borg, I. and Groenen, P. (1997) Modern Multidimensional Sealing: Theory and Applications. Springer: New York.]]></ref_text>
				<ref_id>Borg97</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Braje, W. L. and Knill, D. C. (1994) Apparent surface shape affects perceived specular reflectance of curved surfaces. Invest. Ophth. Vis. Sci. Suppl. 35(4), 1628.]]></ref_text>
				<ref_id>Braj94</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Busing, F., Commandeur, J., and Heiser, W. (1997) PROXSCAL: a multidimensional scaling program for individual differences scaling with constraints. In W. Bandilla and Faulbaum (Eds.), Advances in Statistical Software, 6, Lucius &amp; Lucius: Stuttgart, 67-73.]]></ref_text>
				<ref_id>Busi97</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806819</ref_obj_id>
				<ref_obj_pid>800224</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Cook, R.L. and Torrance, K.E. (1981) A reflectance model for computer graphics. Computer Graphics (SIGGRAPH 81 Conference Proceedings), 15(4), 187- 196.]]></ref_text>
				<ref_id>Cook81</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Fairchild, M.D. (1998) Color Appearance Models. Addison-Wesley, Reading, MA.]]></ref_text>
				<ref_id>Fair98</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122738</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[He, X.D., Torrance, K.E., Sillion, F.X., and Greenberg, D.P. (1991) A comprehensive physical model for light reflection. Computer Graphics (SIGGRAPH 91 Conference Proceedings), 25(4), 175- 186.]]></ref_text>
				<ref_id>He91</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Helmholtz, H. von (1924) Treatise on Physiological Optics (vol. II), (Trans. by J.P. Southhall). Optical Society of America.]]></ref_text>
				<ref_id>Helm24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Hering, E. (1964) Outlines of a Theory of the Light Sense, (Trans. by L. Hurvich and D. Jameson). Harvard University Press: Cambridge, MA.]]></ref_text>
				<ref_id>Heri64</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Hunter, R.S. and Harold R.W. (1987) The Measurement of Appearance (2nd edition). Wiley, New York.]]></ref_text>
				<ref_id>Hunt87</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Judd, D.B. (1937) Gloss and glossiness. Am. Dyest. Rep. 26, 234-235.]]></ref_text>
				<ref_id>Judd37</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258801</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Lafortune, E.P., Foo, S.C., Torrance, K.E., and Greenberg, D.P. (1997) Non-linear approximation of reflectance functions. SIGGRAPH 97 Conference Proceedings, 117-126.]]></ref_text>
				<ref_id>Lafo97</ref_id>
			</ref>
			<ref>
				<ref_obj_id>11676</ref_obj_id>
				<ref_obj_pid>11675</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Mingolla, E. and Todd, J.T. (1986) Perception of solid shape from shading. Bio. Cyber. 53(3), 137-151.]]></ref_text>
				<ref_id>Ming86</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Nishida, S. and Shinya, M. (1998) Use of image-based information in judgements of surface reflectance properties. J. Opt. Soc. Am., 15(12), 2951-2965.]]></ref_text>
				<ref_id>Nish98</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280922</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Pattanaik. S. Ferwerda, J.A., Fairchild, M.D. and Greenberg, D.P. (1998) A multiscale model of adaptation and spatial vision for realistic image display. SIGGRAPH 98 Conference Proceedings, 287- 298.]]></ref_text>
				<ref_id>Patt98</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360839</ref_obj_id>
				<ref_obj_pid>360825</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Phong B.T. (1975) Illumination for computer generated pictures. Comm. ACM 18(6), 311-317.]]></ref_text>
				<ref_id>Phon75</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Schlick, C. (1993) A customizable reflectance model for everyday rendering. Proc. 4th Eurographics Workshop on Rendering, 73-83.]]></ref_text>
				<ref_id>Schl93</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311546</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Stamm, J. (1999)Difl?action shaders. SIGGRAPH 99 Conference Proceedings, 101-110.]]></ref_text>
				<ref_id>Stam99</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617603</ref_obj_id>
				<ref_obj_pid>616014</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Strauss, P. S. (1990) A realistic lighting model for computer animators. IEEE Comp. Graph. &amp; Appl. 10(6), 56-64.]]></ref_text>
				<ref_id>Stra90</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Todd, J.T. and Mingolla, E. (1983)Perception of surface curvature and direction of illumination from patterns of shading. J. Exp. Psych.: Hum. Percept. and Perf. 9(4), 583-595.]]></ref_text>
				<ref_id>Todd83</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Torgerson, W.S. (1960) Theory and Methods" of Scaling. Wiley: New York.]]></ref_text>
				<ref_id>Torg60</ref_id>
			</ref>
			<ref>
				<ref_obj_id>300783</ref_obj_id>
				<ref_obj_pid>300776</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Tumblin, J., Hodgins J.K., and Guenter, B.K. (1999) Two methods for display of high contrast images. ACM Trans. on Graph., 18(1), 56-94]]></ref_text>
				<ref_id>Tumb99</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134078</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Ward, G.J. (1992) Measuring and modeling anisotropic reflection. Computer Graphics (SIGGRAPH 92 Conference Proceedings), 26(2), 265- 272.]]></ref_text>
				<ref_id>Ward92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>614380</ref_obj_id>
				<ref_obj_pid>614268</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Ward-Larson, G., Rushmeier H., and Piatko, C. (1997) A visibility matching tone reproduction operator for high dynamic range scenes. IEEE Trans. on Vis. and Comp. Graph., 3(4):291-306.]]></ref_text>
				<ref_id>Ward97</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Wyszecki, G. and Stiles, W.S. (1982) Color Science: Concepts" and Methods', Quantitative Data and Formulae (2nd ed.), Wiley: New York.]]></ref_text>
				<ref_id>Wysz82</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Toward a Psychophysically-Based Light Reflection Model for Image Synthesis Fabio Pellacini* James A. 
Ferwerda* Donald P. Greenberg* Program of Computer Graphics Cornell University ABSTRACT In this paper 
we introduce a new light reflection model for image synthesis based on experimental studies of surface 
gloss perception. To develop the model, we ve conducted two experiments that explore the relationships 
between the physical parameters used to describe the reflectance properties of glossy surfaces and the 
perceptual dimensions of glossy appearance. In the first experiment we use multidimensional scaling techniques 
to reveal the dimensionality of gloss perception for simulated painted surfaces. In the second experiment 
we use magnitude estimation methods to place metrics on these dimensions that relate changes in apparent 
gloss to variations in surface reflectance properties. We use the results of these experiments to rewrite 
the parameters of a physically-based light reflection model in perceptual terms. The result is a new 
psychophysically-based light reflection model where the dimensions of the model are perceptually meaningful, 
and variations along the dimensions are perceptually uniform. We demonstrate that the model can facilitate 
describing surface gloss in graphics rendering applications. This work represents a new methodology for 
developing light reflection models for image synthesis. Keywords I.3.7 Three-Dimensional Graphics and 
Realism, Human Factors, Experimentation, Light Reflection Models, Gloss, Visual Perception. 1. INTRODUCTION 
Color and gloss are two fundamental visual attributes used to describe the appearances of objects in 
synthetic images. In a typical graphics rendering application a user specifies an object s color as an 
RGB triple and describes its gloss in terms of the parameters of a light reflection model such as Phong 
[Phon75]. In addition to RGB, many rendering applications allow users to describe color in more perceptually 
meaningful color spaces such as HSV, Munsell, or CIELAB, that have grown out of the science of colorimetry 
[Wysz82]. Working in these spaces makes it easier to specify color, because the dimensions of the spaces 
are representative of our visual experience of color, and the scaling of the dimensions is perceptually 
uniform. Unfortunately similar perceptually-based spaces for specifying * 580 Rhodes Hall, Ithaca NY, 
14853 http://www.graphics.cornell.edu/-{fabio,jaf,dpg}@graphics.cornell.edu Permission to make digital 
or hard copies of part or all of this work or personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. SIGGRAPH 2000, New Orleans, 
LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00  Figure 1: Coffee mugs with different gloss 
attributes. surface gloss do not yet exist. At the present time the parameters used to describe gloss 
are either based on ad-hoc lighting models such as Phong, or are motivated by research into the physical 
aspects of light reflection [Blin77, Cook81, He91, Ward92, Schl93, LaFo97, Stam99]. In either case, the 
visual effects of the parameters are relatively unintuitive and interactions among different parameters 
make it difficult to specify and modify surface gloss properties. A light reflection model grounded in 
the visual psychophysics of gloss perception would greatly facilitate the process of describing surface 
gloss properties in computer graphics renderings, and could lead to more efficient and effective rendering 
methods. In this paper we introduce a new light reflection model for image synthesis based on experimental 
studies of surface gloss perception. To develop the model, we have conducted two psychophysical studies 
to explore the relationships between the physical parameters used to describe the reflectance properties 
of glossy surfaces and the perceptual dimensions of glossy appearance. We use the results of these experiments 
to rewrite the parameters of a physically-based light reflection model in perceptual terms. The result 
is a new psychophysically-based light reflection model where the dimensions of the model are perceptually 
meaningful, and variations along the dimensions are perceptually uniform. We demonstrate that the model 
is useful for describing and modifying surface gloss properties in graphics rendering applications. However, 
the long-term impact of this work may be even more important because we present a new methodology for 
developing psychophysical models of the goniometric aspects of surface appearance to complement widely 
used colorimetric models.  2. BACKGROUND To develop a psychophysically-based light reflection model 
for image synthesis we first need to understand the nature of gloss perception. In his classic text, 
Hunter [Hunt87] observed that there are at least six different visual phenomena related to apparent gloss. 
He identified these as: specular gloss perceived brightness associated with the specular reflection 
from a surface contrast gloss perceived relative brightness of specularly distinctness-of-image (DOI) 
gloss perceived sharpness of images reflected in a surface haze perceived cloudiness in reflections 
near the specular direction sheen perceived shininess at grazing angles in otherwise matte surfaces 
absence-of-texture gloss perceived surface smoothness and uniformity Judd [Judd37] operationalized Hunter 
s definitions by writing expressions that related them to the physical features of surface reflectance 
distribution functions (BRDFs). Hunter and Judd s work is important, because it is the first to recognize 
the multidimensional nature of gloss perception. In 1987 Billmeyer and O Donnell [Bill87] published an 
important paper that tried to address the issue of gloss perception from first principles. Working with 
a set of black, gray, and white paints with varying gloss levels, O Donnell collected ratings of the 
apparent difference in gloss between pairs of samples and then used multidimensional scaling techniques 
to discover the dimensionality of perceived gloss. He concluded that for his sample set and viewing conditions 
(flat samples, structured/direct illumination, black surround) the appearance of high gloss surfaces 
is best characterized by a measure similar to distinctness­of-image gloss, while the appearance of low 
gloss surfaces is better described by something like contrast gloss. In the vision literature, studies 
of gloss have focused primarily on its effects on the perception of shape from shading. Todd and Mingolla 
[Todd83, Ming86] found that gloss generally enhances the perception of surface curvature. Blake [Blak90] 
found categorical changes in surface appearance and shape depending on the 3d location of the specular 
highlight. Braje [Braj94] found interactions between apparent shape and apparent gloss, showing that 
a directional reflectance pattern was perceived as more or less glossy depending on the shape of its 
bounding contour. More recently Nishida [Nisi98] also studied interactions between shape and gloss, and 
found that subjects are poor at matching the Phong parameters of bumpy surfaces with different frequency 
and amplitude components. Finally, in computer graphics, while there has been extensive work on developing 
physically-based light reflection models, there has been relatively little effort to develop models whose 
dimensions are perceptually meaningful. One exception is Strauss s model [Stra90], a hybrid of Phong 
and Cook-Torrance, that describes surface properties with five parameters: color, smoothness, metalness, 
transparency, and refractive index. He reports that users find it much easier to specify surface gloss 
with this model than with others. There is still much work to be done in this area. First, with the exception 
of Billmeyer and O Donnell s work there has been little investigation of the multidimensional nature 
of glossy appearance from first principles. Hunter s observations about visual gloss phenomena are insightful 
but we need studies that quantify these different appearance dimensions and relate them to the physical 
properties of materials. Second, all previous gloss studies have looked exclusively at locally illuminated 
surfaces in uniform surrounds. This practice is understandable given the difficulty of controlling complex 
environments, but it s strange considering that one of the most salient things about glossy surfaces 
is their ability to reflect their surroundings. To really understand how we perceive surface gloss, we 
need to study three-dimensional objects in realistically rendered environments. Fortunately, image synthesis 
gives us a powerful tool to study the perception of surface gloss. Physically-based image synthesis methods 
let us make realistic images of three-dimensional objects in complex, globally-illuminated scenes, and 
gives us precise control over object properties. By using image synthesis techniques to conduct psychophysical 
experiments on gloss perception we should be able to make significant progress toward our goal of developing 
a psychophysically-based light reflection model that can describe the appearance of glossy materials. 
 3. EXPERIMENTS 3.1 Motivation In many ways the experiments that follow are analogous to early research 
done to establish the science of colorimetry. In that work, researchers wanted to understand the relationships 
between the physical properties of light energy, and our perception of color. Many of the earliest experiments 
focused on determining the dimensionality of color perception, culminating with Young s trichromatic 
theory [Helm24]. Following this, further experiments were done to find perceptually meaningful axes in 
this three-dimensional color space. Hering s work [Heri64] on opponent color descriptions, falls into 
this category. Finally, many experiments have been done to scale these axes and create perceptually uniform 
color spaces. Munsell, Judd, and MacAdam s efforts to develop uniform color scales are good examples 
(see [Wysz82] for a review). Although we recognize the great effort involved in the development of color 
science, our overall goals with respect to understanding gloss are similar: we are conducting experiments 
to understand gloss perception with the goal of building a psychophysical model of gloss that relates 
the visual appearance of glossy surfaces to the underlying physical properties of the surfaces. . In 
Experiment 1 we will use multidimensional scaling techniques to reveal both the dimensionality of gloss 
perception, and to suggest perceptually meaningful axes in visual gloss space . In Experiment 2 we will 
use magnitude estimation techniques to place quantitative metrics on these axes and create a perceptually 
uniform gloss space. . Finally we will use these results to develop a psychophysically-based light reflection 
model for image synthesis.  Gloss is a visual attribute of a wide variety of materials including plastics, 
ceramics, metals, and other man-made and organic substances. Eventually we would like to develop a model 
that can explain the appearances of all these kinds of materials, but initially we need to restrict our 
studies to a manageable subclass. To start, we ve chosen to study a set of achromatic glossy paints. 
We chose paints because they exhibit a wide variety of gloss levels from flat to high gloss; their reflectance 
properties have been measured extensively so there are good models to describe their physical characteristics, 
and they are widely used in art and industry, so hopefully our findings will be immediately useful. 3.2 
Experiment 1: Finding the perceptual dimensions of gloss space 3.2.1 Purpose The purpose of Experiment 
1 is to determine the dimensionality of gloss perception for painted surfaces in synthetic images and 
to find perceptually meaningful axes in this visual gloss space. To do this we ve designed an experiment 
based on multidimensional scaling techniques. 3.2.2 Methodology: Multidimensional scaling Multidimensional 
scaling (MDS) is statistical method for finding the latent dimensions in a dataset [Borg97]. Multidimensional 
scaling takes a set of measures of the distances between pairs of objects in a dataset and reconstructs 
a space that explains the dataset s overall structure. This concept is best illustrated by example. Table 
1 shows a matrix of the distances between a number of U.S. cities. This matrix indicates how far one 
city is from another but gives no sense of their spatial relations. If this proximity matrix is used 
as input to the PROXSCAL MDS algorithm [Busi97], it attempts to reconstruct the spatial positions of 
the cities to best explain the proximity measures. The two-dimensional MDS solution produced by the algorithm 
is shown in Figure 2, where you can see that MDS has recovered the true spatial layout of the cities 
(the outline of the U.S. map is overlaid for reference). Since distances in a space are unaffected by 
rotations or inversions, MDS solutions are only specific up to these transformations, and it is the experimenter 
s job to find meaningful axes in the solution. Although a two-dimensional MDS solution is shown in Figure 
2, MDS can produce solutions in any number of dimensions to try to achieve the best fit to the data. 
The goodness of the fit is known as the stress of the solution. The stress formula used in the example 
is: stress = . [di, j - d(xi , xj )]2 (1) i, j where di,j are the input proximities, xi and xj are the 
recovered locations in the nth dimensional solution, and d is a measure of the distance between them. 
The MDS algorithm attempts to minimize the stress for each of the solutions. Figure 3 plots the stress 
values for solutions running from 1 to 5 dimensions. The stress curve will drop sharply as dimensions 
are added that explain more of the data and will decline more slowly as further superfluous dimensions 
are added. Standard practice is to choose the dimensionality indicated by this inflection point in the 
stress curve. The stress curve in Figure 3 indicates that a two­dimensional solution provides the best 
fit to the data, but this is to be expected since the dataset is inherently two dimensional, and error 
in the proximity measures is negligible, providing a perfect two-dimensional fit. In typical experimental 
datasets, noise in the data results in a stress curve that drops then asymptotes as greater­than-necessary 
dimensions are added. MDS algorithms come in a variety of flavors that depend on the form of the stress 
function the algorithm uses. In our work we use a variant called weighted Euclidean non-metric MDS [Borg97] 
that allows us to combine data from multiple subjects, compensate for individual differences, and analyze 
datasets where the Atl Chi Den Hou LA Mia NYC SF Sea DC Atlanta 0 Chicago 587 0 Denver 1212 920 0 Houston 
701 940 879 0 LA 1936 1745 831 1374 0 Miami 604 1188 1726 968 2339 0 NYC 748 713 1631 1420 2451 1092 
0 SF 2139 1858 949 1645 347 2594 2571 0 Seattle 2182 1737 1021 1891 959 2734 2406 678 0 DC 543 597 1494 
1220 2300 923 205 2442 2329 0 Table 1: Proximity matrix of distances between U.S. cities. N E Figure 
2: MDS reconstruction of the U.S. map. proximities may only reflect ordinal rather than interval relations 
in the data. We also use a second variant called confirmatory MDS [Borg97] which let us test hypotheses 
about the functional forms of the dimensions and their orthogonality.  3.2.3 Experimental Procedure 
 3.2.3.1 Stimuli To apply MDS to the problem of finding the dimensionality of gloss perception, we first 
need to construct a stimulus set with objects that vary in gloss, and then collect measures of the apparent 
differences in gloss between pairs of objects in the set. These apparent gloss differences then serve 
as the proximities that the MDS algorithm uses to construct a representation of visual gloss space . 
A composite image of the stimulus set used in Experiment 1 is shown in Figure 4. The environment consisted 
of a sphere enclosed in a checkerboard box illuminated by an overhead area light source. Images were 
generated using a physically-based Monte Carlo path-tracer that used an isotropic version of Ward s [Ward92] 
light reflection model: . 22 d exp[- tan d / a ] (2) .(. ,f ,. ,f ) = +.· p 4pa 2 iioo s cos . cos 
. io where .(.i,fi,.o,fo) is the surface BRDF, .i,fi, and .o,fo are spherical coordinates for the incoming 
and outgoing directions, and d is the half-angle between them. Ward s model uses three parameters to 
describe the BRDF: .d the object s diffuse reflectance; .s the energy of its specular component, and 
a the spread of the specular lobe. Our reason for choosing Ward s model is that we wanted the objects 
in the stimulus set to be representative of the gloss properties of real materials, and Ward gives parameters 
that represent measured properties of a range of glossy paints. The parameters used in our stimulus set 
span this range. Each parameter was set to three levels. .s values were (0.033, 0.066, 0.099), a values 
were (0.04, 0.07, 0.10), and .d was set to (0.03, 0.193, 0.767) which are the diffuse reflectance factors 
corresponding to Munsell values (N2, N5, and N9). The Stress 0.01 0 12345 Dimensionality of solution 
Figure 3: Stress vs. dimensionality graph for MDS solution.  Figure 4 : Composite image of the stimulus 
set used in Experiment 1. Labels indicate the diffuse color (white, gray, black), and . s and a values. 
Symbols are included as an aid for interpreting subsequent figures. black and white checks in the checkerboard 
surround were completely diffuse and had .d s of 0.03 and 0.767 respectively. By using all combinations 
of the .d, .s, and a parameters for the sphere objects, we produced the 27 images shown in Figure 4. 
Choosing a tone reproduction operator to map from calculated image radiances to display values presented 
a challenge because the images had high dynamic ranges caused by the visible reflection of the light 
source. We experimented with a number of tone reproduction operators including simple clipping and gamma 
compression as well as Pattanaik [Patt98] and Ward-Larson s [Ward97] high dynamic range operators but 
we abandoned these methods because they produced objectionable artifacts such as halos and banding. We 
settled on Tumblin s [Tumb99] Rational Sigmoid function which compresses the light source highlight without 
abrupt clipping and allows all other scene values to be directly mapped to the display. One of the consequences 
of the limited dynamic range of display devices is that any gloss attribute related to the absolute intensity 
of a highlight is not likely to play much of a role in how glossy surfaces appear in images. Given the 
amount of effort that has gone into developing physically accurate light reflection models for realistic 
image synthesis, addressing the particular dynamic range problems caused by trying to display images 
of glossy surfaces is certainly a subject that merits future work.  3.2.3.2 Procedure Nine subjects 
participated in Experiment 1. The subjects were the first two authors and seven graduate and undergraduate 
Computer Science students. All had normal or corrected to normal vision. With the exception of the authors, 
all were naïve to the purpose and methods of the experiment. In the experimental session, the subjects 
viewed pairs of images displayed on a calibrated SXGA monitor. Minimum and maximum monitor luminances 
were 0.7 and 108 cd/m2 and the system gamma was 2.35. The images were presented on a black background 
in a darkened room. The monitor was viewed from a was displayed below the images. The ends of the slider 
scale were labeled 0, small difference and 100, large difference . A readout below the slider indicated 
the numeric position along the scale. Subjects judged the apparent gloss differences of all 378 object 
pairs in the stimulus set. The pairs were presented in random order. For each subject, the apparent gloss 
differences measured in the experiment were used to fill out a 27 x 27 proximity matrix. All nine proximity 
matrices were used as input to the PROXSCAL MDS algorithm using the weighted Euclidean non­metric stress 
formulation.  3.2.4 Analysis/Discussion Recall that our goal in this experiment is to discover the dimensionality 
of gloss perception for the painted surfaces and to find perceptually meaningful axes in this gloss space. 
To do this we observed how the stress varied with the dimensionality of the MDS solution. Figure 5 plots 
stress values for solutions running from 1 to 5 dimensions. The stress value drops significantly with 
the change from a 1-dimensional to a 2-dimensional solution, but declines more slowly with the addition 
of higher dimensions which are probably only accommodating noise in the dataset. From this pattern of 
results we infer that under these conditions apparent gloss has two dimensions. The two-dimensional gloss 
space recovered by MDS is shown in Figure 6. In the Figure, MDS has placed the objects at locations that 
best reflect the differences in apparent gloss reported by the subjects. As stated earlier, since distances 
in this space are invariant under rotation, inversion or scaling, it is our job to look for perceptually 
meaningful axes in the space. The cross in the lower right corner of the diagram indicates two important 
trends in the data that are related to properties of the reflected images formed 0.15 Subjects were 
asked to judge the apparent difference in gloss between the pair of objects shown in the images. They 
entered 0 12345 their responses using a mouse to vary the position of a slider that Dimensionality of 
solution Gloss appearance parameters are specified for the display conditions described in the experiments. 
Appearance in the Figure 5: Dimensionality vs. stress graph for Experiment 1. printed images is subject 
to the limitations of the printing process. distance of 60 inches to ensure that the display raster was 
0.1 invisible. At this viewing distance each image subtended 3.2 Stress degrees of visual angle. 0.05 
 Figure 6 : Two-dimensional MDS solution for Experiment 1. by the surfaces. First, the apparent contrast 
of the reflected image increases from the lower left to the upper right of the diagram. Second, the apparent 
sharpness or distinctness of the reflected image increases from lower right to upper left. We believe 
these dimensions are qualitatively similar to the contrast gloss and distinctness-of-image (DOI) gloss 
attributes Hunter observed and so we will name our dimensions c for contrast gloss and d for DOI gloss. 
However, to foreshadow the results of the next experiment, we will differ significantly from Hunter (and 
Judd) in the quantitative formulation of relationship between these perceptual dimensions and the physical 
dimensions used to describe surface BRDFs. 3.3 Experiment 2: Creating a perceptually uniform gloss space 
  3.3.1 Purpose In Experiment 1 we discovered the dimensionality of gloss perception and identified 
perceptually meaningful axes in visual gloss space for painted surfaces in synthetic images. The purpose 
of Experiment 2 is to place psychophysical metrics on these axes and rescale them to create a perceptually 
uniform gloss space. To do this we ve designed an experiment based on magnitude estimation techniques. 
 3.3.2 Methodology: Magnitude estimation Magnitude estimation is one of a family of psychophysical scaling 
techniques designed to reveal functional relationships between the physical properties of a stimulus 
and its perceptual attributes [Torg60]. In the basic magnitude estimation procedure, subjects are presented 
with a random sequence of stimuli that vary along some physical dimension, and they are asked to assign 
a number to each stimulus that indicates the apparent magnitude of the corresponding perceptual attribute. 
Magnitude estimates are then used to derive a psychophysical scale. 3.3.3 Experimental Procedure 3.3.3.1 
Stimuli Two magnitude estimation studies were performed in Experiment 2 to scale the perceptual gloss 
dimensions found in Experiment 1. In both cases the stimuli used were subsets of the stimuli used in 
Experiment 1, supplemented by new stimuli with 100 100 80 80 60 40 Gloss rating Gloss rating d = 1 - 
a c = 3 . s + . d 2 - 3 . d 2 20 20 0 0.8 0.85 0.9 0.95 1 d Figure 7: Magnitude estimates and fit for 
DOI gloss d. parameters intermediate to those in the original set. In the contrast gloss scaling study 
24 images were used, showing objects with combinations of .d levels of (0.03, 0.087, 0.193, 0.420, 0.767) 
(black, dark/medium/light gray, white) and .s levels of (0.017 0.033, 0.050, 0.066, 0.083 0.099) (low 
to high specular energy), the a parameter was fixed at 0.04 (small spread) to make variations along the 
contrast gloss dimension as salient as possible. In the DOI gloss scaling study, a was varied in 11 levels 
from 0.01 to 0.19 (small to large spread), and the .d and .s parameters were fixed at 0.03 (black) and 
0.099 (high specular energy) to make variations along the DOI gloss dimension as salient as possible. 
 3.3.3.2 Procedure The subjects in Experiment 2 were the same as those in Experiment 1, and the same 
display techniques, viewing conditions, and data gathering methods were used. In each magnitude estimation 
study, subjects viewed single images from the new stimulus sets. Images were presented in a random sequence 
and each sequence was repeated three times. On each trial subjects were asked to judge the apparent glossiness 
of the object in the image on a scale from 0 to 100 by adjusting the on-screen slider.  3.3.4 Analysis/Discussion 
Our goal in these experiments is to derive psychophysical scaling functions that relate changes in apparent 
gloss along the perceptual dimensions we discovered in Experiment 1 to variations in the parameters of 
the physical light reflection model. To achieve this goal we tested various hypotheses about functional 
relationships between the physical and perceptual dimensions, first with least squares fitting techniques 
on the magnitude estimation data and then with confirmatory MDS on the full dataset from Experiment 1. 
This approach allowed us to verify that the scaling functions are task independent and to determine whether 
the perceptual dimensions are orthogonal. First we examined the d (DOI gloss) dimension. Our hypothesis 
was that d is inversely related to the a parameter. In Figure 7 subjects gloss ratings are plotted versus 
the function d = 1 -a. The line was obtained through linear regression and the r2 value of the fit was 
0.96. Polynomial fits only increased r2 by less than  0.01 so we concluded that the relationship is 
linear. Interpreting the c (contrast gloss) dimension was less straightforward. In the MDS solution from 
Experiment 1 (Figure 6) it is clear that c varies with diffuse reflectance, since the white, gray, and 
black objects form distinct clusters that occupy different ranges along the c dimension. Our first hypothesis 
was that c is a simple function of the physical contrast (luminance ratio) of the black and white patches 
in the reflected image but this provided a very poor fit to the data (r2 = 0.76). Our second hypothesis 
was that contrast in this situation is a function of the difference in 0 0 0.05 0.1 0.15 0.2 0.25 c Figure 
8: Magnitude estimates and fit for contrast gloss c. apparent lightness of the two patches, where lightness 
is defined as in CIELAB [Fair98]. This second formulation provided a much better fit to the magnitude 
estimation data (r2 = 0.87). However when we tested this second hypothesis on the full dataset from Experiment 
1 using confirmatory MDS, we found that the fit was poor for surfaces with large a values where the physical 
contrast in the image plane drops as the reflected image gets blurrier. We then tested a third hypothesis 
that subjects lightness judgments are based on inferred object-space reflectance values rather than image-space 
intensity values (i.e. subjects show lightness constancy [Fair98], compensating for blur-related image 
contrast losses). This hypothesis is formalized in Equation 4 which we derived using standard integration 
techniques under the assumption of small a values and high environmental contrast. Figure 8 plots the 
data from the contrast gloss scaling study, which shows how subjects gloss ratings relate to this final 
formulation for the c dimension. The line was obtained through linear regression and is a good fit to 
the data with an r2 value of 0.94. This result shows that subjects appear to be compensating for the 
decrease in physical image contrast caused by blurring in making their judgments of the lightnesses of 
the reflected patches. Using this formulation also decreased the stress value in a subsequent confirmatory 
MDS test on the full dataset, which indicates that the c and d axes are independent, and therefore orthogonal 
in gloss space. Equations 3 and 4 show the final formulas for the c and d axes. These formulas define 
psychophysical metrics that relate changes in apparent gloss along these two axes to variations in the 
physical parameters of the light reflection model. d =1-a (3) 3 3 c = . +. 2 - . 2 (4) s d d These 
axes are perceptually linear, but to make the space perceptually uniform, we need to find weighting factors 
for the axes so that distances in the space can be measured. These weights are given as a byproduct of 
the confirmatory MDS tests we ran which lets us write the distance as: 22 Dij . [ci - cj ] +[1.78·(di 
- dj )] (5) Figure 9 shows a visualization of the perceptually uniform gloss space with the stimuli from 
Experiment 1 placed at their predicted locations. The Figure shows the contrast gloss (c) and DOI gloss 
(d) dimensions form a two-dimensional space, (which is also shown in the inset), and surface lightness 
(L) (which we will incorporate in the following section) is an orthogonal third dimension. Like perceptually 
uniform color spaces, this perceptually uniform gloss space has a number of important properties. For 
example, it allows us to: . predict the visual appearance of a glossy paint from its physical reflectance 
parameters  Figure 9 : The perceptually uniform gloss space derived from Experiment 2. . compare two 
paints with respect to the two visual gloss dimensions . produce paints with different physical reflectance 
values that match in terms of apparent gloss . calculate isogloss contours that describe paints that 
differ equally in apparent gloss from a standard.  4. A PSYCHOPHYSICALLY-BASED LIGHT REFLECTION MODEL 
To take full advantage of this new space, we are going to rewrite the parameters of the physically-based 
light reflection model (Equations 6,7,8) in perceptual terms to create a psychophysically-based light 
reflection model that can be used to describe both the physical and visual characteristics of the paints 
we studied. To do this, we need to introduce a perceptually linear parameter related to diffuse reflectance. 
For compatibility with perceptually uniform color spaces we chose CIELAB lightness (L). This final addition 
allows us to express the physical where f is the CIELAB lightness function normalized in [0,1]. Figure 
10 illustrates the influence of the lightness of the diffuse component on perceived gloss. Here the solid 
curve plots the maximum contrast gloss c achievable for different lightness values (derived by enforcing 
energy conservation of the BRDF). This defines the envelope of gloss space with respect to lightness. 
We also plotted how contrast gloss varies with lightness for a fixed energy of the specular lobe. This 
curve shows that for the same specular energy, contrast gloss is smaller for lighter objects. That is 
to say, if two surfaces are painted with black and white paints having the same physical formulations, 
the black surface will appear glossier than the white one. Strictly speaking, the model we ve developed 
is only predictive Maximum contrast Fixed energy contrast 1 0.8 0.6 parameters in terms of the perceptual 
ones through the following 0.4 equations: 0.2 .d = f -1(L) (6) 0 3 0 0.2 0.4 0.6 0.8 1 . = s ... c 
+ 3 f -1 (L)/2 ... - f -1 (L)/2 (7) L a=1- d (8) Figure 10: Effect of surface lightness on apparent 
gloss. within the range of our stimuli, which covers a substantial range of measured glossy paints. 
However we feel confident that the model can be applied outside this range to cover the space of physically 
plausible BRDFs expressible using the Ward model, but we believe that the physical parameters should 
be maintained in the range of the ones measured for real materials. In particular, the a value should 
not be much larger than 0.2 since the specular lobe of the BRDF is not normalized for larger values [Ward92]. 
  5. APPLYING THE MODEL In the previous section we used the results of our gloss perception studies 
to develop a psychophysically-based light reflection model for image synthesis where the dimensions of 
the model are perceptually meaningful and variations along these dimensions are perceptually uniform. 
In this section we demonstrate the power of the model by showing how it can be used to facilitate the 
process of describing surface appearance in graphics rendering applications. 5.1 Describing differences 
in apparent gloss One of the benefits of working in a perceptually uniform description space is that 
steps along the dimensions produce equal changes in appearance. This is true of uniform color spaces 
such as CIELAB where equal numerical steps in lightness (L) or chroma (a,b) produce perceptually equal 
changes in color appearance. The perceptually uniform gloss space our light reflection model is based 
on has similar properties. Figure 11 shows isogloss difference contours with respect to the object in 
the lower left corner of the diagram (c = 0.087, d = 0.93). According to the model, the objects falling 
on the circular contours are equally different in apparent gloss from the reference object. The concentric 
circles show two degrees of isogloss difference (.c = 0.04, .d = 0.22 = 0.04/1.78). It s important to 
observe that because the gloss space is two­dimensional (c,d), objects equidistant from a reference object 
may have different reflectance properties even though they will be judged to be equally different in 
gloss from the reference. For example, the two objects at 12 and 3 o clock in Figure 11 have Figure 
12 : Matching apparent gloss: white, gray, and black objects having the same physical gloss parameters 
(top row) and perceptual gloss parameters (bottom row). very different reflectance properties: the one 
at 12 o clock produces a sharp but low contrast reflection, while the one at 3 o clock makes a blurry 
but high contrast reflection, still the model predicts that they will be judged to be equally different 
in gloss from the reference object. This prediction was supported by an informal ranking study we ran 
using the stimulus set from Experiment 1. Objects whose parameters fell along isogloss contours with 
respect to a low gloss reference object received similar rank values implying that they appeared equally 
glossy but in different ways. This demonstration shows that our model provides the ability to specify 
differences in apparent gloss. This should make it much easier to modify object gloss properties in controlled 
ways in graphics rendering applications. 5.2 Matching apparent gloss Many studies of gloss perception 
[Hunt87, Bill87] have noted that apparent gloss is affected by the diffuse reflectance of a surface, 
with light colored surfaces appearing less glossy than dark ones having the same finish. This effect 
is illustrated in the top row of Figure 12 where the white, gray and black objects have the same physical 
gloss parameters (.s = 0.099, a = 0.04) but differ in apparent gloss with the white sphere appearing 
least glossy and the black sphere appearing most glossy. This phenomenon makes it difficult to create 
objects with different lightnesses that match in apparent gloss. The bottom row of Figure 12 shows the 
results produced with our psychophysically­based gloss model. When the objects are assigned the same 
perceptual gloss values (c = 0.057, d = 0.96) they appear to have similar gloss despite differences in 
their lightnesses. This property of the model should make it much easier to create objects that have 
the same apparent gloss, since the parameters that describe object lightness (L) and gloss (c,d) have 
been decoupled. 5.3 A new tool for modeling surface appearance in computer graphics In the previous subsections 
we have demonstrated that our new model has two important features: it allows us to describe differences 
in apparent gloss, and it lets us make objects match in apparent gloss. These features should make it 
much easier to specify surface appearance in graphics rendering applications. To demonstrate how the 
model might be used, Figure 13 shows a prototype of a perceptually-based color/gloss picker for painted 
surfaces that could be incorporated into an application. We add color to the model by assuming (as suggested 
in [Astm89] and [Aida97]), that surface chromaticity and apparent gloss are Figure 13 : Prototype of 
a perceptually-based color/gloss picker for painted surfaces. Surface appearance is specified by three 
color parameters CIELAB lightness (L) and chroma (a,b) and two gloss parameters (c) contrast gloss and 
(d) distinctness-of-image gloss. relatively independent. For consistency with the lightness parameter 
(L) we use CIELAB chroma (a,b) to specify color. In the interface, surface appearance is specified by 
these three color parameters and by the two gloss parameters (c,d). Figure 14 shows an image where this 
five parameter color/gloss description has been used to match the apparent gloss of the dark red and 
light blue mugs. Notice that the glossy appearance of the mugs is similar even though they differ significantly 
in lightness and color. This image suggests that psychophysically-based light reflection model we have 
developed through our experiments may be usefully applied under more general conditions, however further 
testing and validation are clearly necessary.  6. CONCLUSIONS/FUTURE WORK In this paper we ve introduced 
a new light reflection model for image synthesis based on experimental studies of surface gloss perception. 
To develop the model we conducted two experiments that explored the relationships between the physical 
parameters used to describe the reflectance properties of glossy surfaces and the perceptual dimensions 
of glossy appearance in synthetic images. We used the results of these experiments to develop a psychophysically-based 
light reflection model where the dimensions of the model are perceptually-meaningful and variations along 
the dimensions are perceptually uniform. We ve demonstrated that the model can facilitate the process 
of describing surface appearance in graphics rendering applications. Although we feel that these results 
are promising, there is clearly much more work to be done. First, we want to make clear that strictly 
speaking, the model we ve developed only accurately predicts appearance within the range of glossy paints 
we studied, under the viewing conditions we used. Although we believe our results will generalize well, 
if the goal is to develop a comprehensive psychophysically-based light reflection model for image synthesis, 
many more studies need to be done: 1) to investigate different classes of materials like plastics, metals, 
and papers (possibly requiring different BRDF models); and 2) to determine how object properties like 
shape, pattern, texture, and color, and scene properties like illumination quality, spatial proximity, 
and environmental contrast and texture affect apparent gloss. Additionally, even though in our experiments 
we found that apparent gloss has two dimensions, we fully expect that for other materials and under other 
conditions different gloss attributes such as sheen and haze may play a greater role. Finally, we feel 
that a very important topic for future work is to develop better tone reproduction methods for Figure 
14 : Demonstration that the model can be used effectively in a typical rendering application (3D Studio 
MAX ). The model was used to make the dark red and light blue mugs match in apparent gloss. accurately 
reproducing the appearance of high dynamic range glossy surfaces within the limited ranges of existing 
display devices. By using physically-based image synthesis techniques to conduct psychophysical studies 
of surface appearance, we should be able to make significant progress in these areas. This will allow 
us to develop models of the goniometric aspects of surface appearance to complement widely used colorimetric 
models. 7. ACKNOWLEDGMENTS Thanks to Steve Westin and our anonymous reviewers for their helpful comments 
on the preparation of this paper. Thanks to Will Alonso, Steve Berman, Reynald Dumont, Bill Feth, Suanne 
Fu, Clint Kelly, Rich Levy, and Corey Toler for serving as subjects in the experiments. Special thanks 
to James Cutting for his useful comments throughout this research project and for his help with the experimental 
design and data analysis. 8. REFERENCES [Aida97] Aida, T. (1997) Glossiness of colored papers and its 
application to specular glossiness measuring instruments. Systems and Computers in Japan, 28(1), 1106-1118. 
[Astm89] American Society for Testing and Materials. (1989) Standard practice for establishing color 
and gloss tolerances (Designation: D3134-89). Annual Book of ASTM Standards, 324-329. [Bill87] Billmeyer, 
F.W. and O Donnell, F.X.D. (1987) Visual gloss scaling and multidimensional scaling analysis of painted 
specimens. Color Res. App. 12(6), 315-326. [Blak90] Blake, A. and Bulthoff, H. (1990) Does the brain 
know the physics of specular reflection? Nature, 343, 165-168. [Blin77] Blinn, J.F. (1977) Models of 
light reflection for computer synthesized pictures. Computer Graphics (SIGGRAPH 77 Conference Proceedings), 
11(4), 192­ 198. [Borg97] Borg, I. and Groenen, P. (1997) Modern Multidimensional Scaling: Theory and 
Applications. Springer: New York. [Braj94] Braje, W. L. and Knill, D. C. (1994) Apparent surface shape 
affects perceived specular reflectance of curved surfaces. Invest. Ophth. Vis. Sci. Suppl. 35(4), 1628. 
[Busi97] Busing, F., Commandeur, J., and Heiser, W. (1997) PROXSCAL: a multidimensional scaling program 
for individual differences scaling with constraints. In W. Bandilla and Faulbaum (Eds.), Advances in 
Statistical Software, 6, Lucius &#38; Lucius: Stuttgart, 67-73. [Cook81] Cook, R.L. and Torrance, K.E.. 
(1981) A reflectance model for computer graphics. Computer Graphics (SIGGRAPH 81 Conference Proceedings), 
15(4), 187 196. [Fair98] Fairchild, M.D. (1998) Color Appearance Models. Addison-Wesley, Reading, MA. 
[He91] He, X.D., Torrance, K.E., Sillion, F.X., and Greenberg, D.P. (1991) A comprehensive physical model 
for light reflection. Computer Graphics (SIGGRAPH 91 Conference Proceedings), 25(4), 175 186. [Helm24] 
Helmholtz, H. von (1924) Treatise on Physiological Optics (vol. II), (Trans. by J.P. Southhall). Optical 
Society of America. [Heri64] Hering, E. (1964) Outlines of a Theory of the Light Sense, (Trans. by L. 
Hurvich and D. Jameson). Harvard University Press: Cambridge, MA. [Hunt87] Hunter, R.S. and Harold R.W. 
(1987) The Measurement of Appearance (2nd edition). Wiley, New York. [Judd37] Judd, D.B. (1937) Gloss 
and glossiness. Am. Dyest. Rep. 26, 234-235. [Lafo97] Lafortune, E.P., Foo, S.C., Torrance, K.E., and 
Greenberg, D.P. (1997) Non-linear approximation of reflectance functions. SIGGRAPH 97 Conference Proceedings, 
117-126. [Ming86] Mingolla, E. and Todd, J.T. (1986) Perception of solid shape from shading. Bio. Cyber. 
53(3), 137-151. [Nish98] Nishida, S. and Shinya, M. (1998) Use of image-based information in judgements 
of surface reflectance properties. J. Opt. Soc. Am., 15(12), 2951-2965. [Patt98] Pattanaik. S. Ferwerda, 
J.A., Fairchild, M.D. and Greenberg, D.P. (1998) A multiscale model of adaptation and spatial vision 
for realistic image display. SIGGRAPH 98 Conference Proceedings, 287­ 298. [Phon75] Phong B.T. (1975) 
Illumination for computer generated pictures. Comm. ACM 18(6), 311 317. [Schl93] Schlick, C. (1993) A 
customizable reflectance model for everyday rendering. Proc. 4th Eurographics Workshop on Rendering, 
73 83. [Stam99] Stamm, J. (1999) Diffraction shaders. SIGGRAPH 99 Conference Proceedings, 101-110. [Stra90] 
Strauss, P. S. (1990) A realistic lighting model for computer animators. IEEE Comp. Graph. &#38; Appl. 
10(6), 56-64. [Todd83] Todd, J.T. and Mingolla, E. (1983) Perception of surface curvature and direction 
of illumination from patterns of shading. J. Exp. Psych.: Hum. Percept. and Perf. 9(4), 583-595. [Torg60] 
Torgerson, W.S. (1960) Theory and Methods of Scaling. Wiley: New York. [Tumb99] Tumblin, J., Hodgins 
J.K., and Guenter, B.K. (1999) Two methods for display of high contrast images. ACM Trans. on Graph., 
18(1), 56-94 [Ward92] Ward, G.J. (1992) Measuring and modeling anisotropic reflection. Computer Graphics 
(SIGGRAPH 92 Conference Proceedings), 26(2), 265 272. [Ward97] Ward-Larson, G., Rushmeier H., and Piatko, 
C. (1997) A visibility matching tone reproduction operator for high dynamic range scenes. IEEE Trans. 
on Vis. and Comp. Graph., 3(4):291-306. [Wysz82] Wyszecki, G. and Stiles, W.S. (1982) Color Science: 
Concepts and Methods, Quantitative Data and Formulae (2nd ed.), Wiley: New York.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344814</article_id>
		<sort_key>65</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[A microfacet-based BRDF generator]]></title>
		<page_from>65</page_from>
		<page_to>74</page_to>
		<doi_number>10.1145/344779.344814</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344814</url>
		<abstract>
			<par><![CDATA[<p>A method is presented that takes as an input a 2D microfacet orientation distribution and produces a 4D bidirectional reflectance distribution function (BRDF). This method differs from previous microfacet-based BRDF models in that it uses a simple shadowing term which allows it to handle very general microfacet distributions while maintaining reciprocity and energy conservation. The generator is shown on a variety of material types.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[reflectance &amp; shading models]]></kw>
			<kw><![CDATA[rendering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P196238</person_id>
				<author_profile_id><![CDATA[81100609852]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ashikmin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Utah]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14208993</person_id>
				<author_profile_id><![CDATA[81100604694]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Simon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Premo&#382;e]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Utah]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39078930</person_id>
				<author_profile_id><![CDATA[81100449948]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shirley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Utah]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{1} BECKMANN, P. Shadowing of random rough surfaces. <i>IEEE Transactions on Antennas and Propagation 13</i> (1965), 384-388.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>563893</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{2} BLINN, J. F. Models of light reflection for computer synthesized pictures. <i>Computer Graphics (Proceedings of SIGGRAPH 77) 11</i>, 2 (July 1977), 192-198.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37434</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{3} CABRAL, B., MAX, N., AND SPRINGMEYER, R. Bidirectional reflectance functions from surface bump maps. <i>Computer Graphics 21</i>, 4 (July 1987), 273- 282. ACM Siggraph '87 Conference Proceedings.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>806819</ref_obj_id>
				<ref_obj_pid>800224</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{4} COOK, R. L., AND TORRANCE, K. E. A reflectance model for computer graphics. <i>Computer Graphics 15</i>, 3 (August 1981), 307-316. ACM Siggraph '81 Conference Proceedings.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192202</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{5} GONDEK, J. S., MEYER, G. W., AND NEWMAN, J. G. Wavelength dependent reflectance functions. In <i>Proceedings of SIGGRAPH '94 (Orlando, Florida, July 24-29, 1994)</i> (July 1994), A. Glassner, Ed., Computer Graphics Proceedings, Annual Conference Series, ACM SIGGRAPH, ACM Press, pp. 213-220.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{6} GOURAUD, H. Continuous shading of curved surfaces. <i>Communications of the ACM 18</i>, 6 (June 1971), 623-629.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258914</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{7} GREENBERG, D. P., TORRANCE, K. E., SHIRLEY, P., ARVO, J., FERWERDA, J. A., PATTANAIK, S., LAFORTUNE, E. P. F., WALTER, B., FOO, S.-C., AND TRUMBORE, B. A framework for realistic image synthesis. <i>Proceedings of SIGGRAPH 97</i> (August 1997), 477-494.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122738</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{8} HE, X. D., TORRENCE, K. E., SILLION, F. X., AND GREENBERG, D. P. A comprehensive physical model for light reflection. <i>Computer Graphics 25</i>, 4 (July 1991), 175-186. ACM Siggraph '91 Conference Proceedings.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{9} LAFORTUNE, E. P., AND WILLEMS, Y. D. Using the modified phong BRDF for physically based rendering. Tech. Rep. CW197, Computer Science Department, K. U. Leuven, November 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258801</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[{10} LAFORTUNE, E. P. F., FOO, S.-C., TORRANCE, K. E., AND GREENBERG, D. P. Non-linear approximation of reflectance functions. <i>Proceedings of SIGGRAPH 97</i> (August 1997), 117-126.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[{11} LU, R., KOENDERINK, J. J., AND KAPPERS, A. M. L. Optical properties (bidirectional reflection distribution functions) of velvet. <i>Applied Optics 37</i>, 25 (1998), 5974-5984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383829</ref_obj_id>
				<ref_obj_pid>2383815</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[{12} MARSCHNER, S. R., WESTIN, S. H., LAFORTUNE, E. P. F., TORRANCE, K. E., AND GREENBERG, D. P. Image-based BRDF measurement including human skin. <i>Eurographics Rendering Workshop 1999</i> (June 1999).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[{13} NEUMANN, L., NEUMANN, A., AND SZIRMAY-KALOS, L. Compact metallic reflectance models. <i>Computer Graphics Forum 18</i>, 13 (1999).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192213</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[{14} OREN, M., AND NAYAR, S. K. Generalization of lambert's reflectance model. In <i>Proceedings of SIGGRAPH '94 (Orlando, Florida, July 24-29, 1994)</i> (July 1994), A. Glassner, Ed., Computer Graphics Proceedings, Annual Conference Series, ACM SIGGRAPH, ACM Press, pp. 239-246.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>360839</ref_obj_id>
				<ref_obj_pid>360825</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[{15} PHONG, B.-T. Illumination for computer generated images. <i>Communications of the ACM 18</i>, 6 (June 1975), 311-317.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97909</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[{16} POULIN, P., AND FOURNIER, A. A model for anisotropic reflection. <i>Computer Graphics 24</i>, 3 (August 1990), 267-282. ACM Siggraph '90 Conference Proceedings.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[{17} PROVOT, X. Deformation constraints in a mass-spring model to describe rigid cloth behavior. In <i>Proceedings of Graphics Interface '95</i> (1995), pp. 147-154.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[{18} SANCER, M. I. Shadow corrected electromagnetic scattering from randomly rough surfaces. <i>IEEE Transactions on Antennas and Propagation AP-17</i>, 5 (September 1969), 577-585.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[{19} SCHLICK, C. An inexpensive BRDF model for physically-based rendering. <i>Computer Graphics Forum 13</i>, 3 (1994), 233-246.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>826423</ref_obj_id>
				<ref_obj_pid>826026</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[{20} SHIRLEY, P., HU, H., SMITS, B., AND LAFORTUNE, E. A practitioners' assessment of light reflection models. In <i>Pacific Graphics</i> (October 1997), pp. 40-49.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[{21} SMITH, B. G. Geometrical shadowing of a random rough surface. <i>IEEE Transactions on Antennas and Propagation 15</i> (1967), 668-671.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311546</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[{22} STAM, J. Diffraction shaders. <i>Proceedings of SIGGRAPH 99</i> (August 1999), 101-110.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[{23} TORRANCE, K. E., AND SPARROW, E. M. Theory for off-specular reflection from roughened surfaces. <i>Journal of Optical Society of America 57</i>, 9 (1967).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[{24} VAN GINNEKEN, B., STAVRIDI, M., AND KOENDERINK, J. J. Diffuse and specular reflectance from rough surfaces. <i>Applied Optics 37</i>, 1 (1998), 130-139.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[{25} WAGNER, R. J. Shadowing of randomly rough surfaces. <i>Journal of Acoustic Society of America 41</i> (1967), 138-147.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134078</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[{26} WARD, G. J. Measuring and modeling anisotropic reflection. <i>Computer Graphics 26</i>, 4 (July 1992), 265-272. ACM Siggraph '92 Conference Proceedings.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134075</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[{27} WESTIN, S. H., ARVO, J. R., AND TORRANCE, K. E. Predicting reflectance functions from complex surfaces. <i>Computer Graphics 26</i>, 2 (July 1992), 255- 264. ACM Siggraph '92 Conference Proceedings.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Microfacet-based BRDF Generator Michael Ashikhmin Simon Premo.ze Peter Shirley University of Utah 
www.cs.utah.edu Permission to make digital or hard copies of part or all of this work for personal or 
classroom use is granted without fee provided that copies are not made or distributed for profit or commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for components 
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy 
otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission 
and/or a fee. Abstract A method is presented that takes as an input a 2D microfacet ori­entation distribution 
and produces a 4D bidirectional re.ectance distribution function (BRDF). This method differs from previous 
microfacet-based BRDF models in that it uses a simple shadowing term which allows it to handle very general 
microfacet distributions while maintaining reciprocity and energy conservation. The gener­ator is shown 
on a variety of material types. CR Categories: I.3.7 [Computing Methodologies ]: Computer Graphics 3D 
Graphics Keywords: Re.ectance &#38; Shading Models, Rendering 1 Introduction Physically-based rendering 
systems describe re.ection behavior us­ing the bidirectional re.ectance distribution function (BRDF) 
[7]. At a given point on a surface the BRDF is a function of two direc­tions, one toward the light and 
one toward the viewer. The char­acteristics of the BRDF will determine what type of material the viewer 
thinks the displayed object is composed of, so the choice of BRDF model and its parameters is important. 
There are a variety of basic strategies for modeling BRDFs that we categorize as follows. Direct measurement. 
BRDFs can be measured directly us­ing goniore.ectometers which mechanically vary the direction to a small 
light source and a spectral sensor and thus collect a large num­ber of point samples for the BRDF [7]. 
Simpler and less accurate devices can also be constructed using CCD imaging devices [26]. More complex 
CCD devices can also be used which gather data quickly with accuracy almost that of full goniore.ectometry 
[12]. If enough is known about the microstructure of a material, a BRDF can be simulated by using a virtual 
goniore.ectometer,where sta­tistical ray tracing followed by density estimation is used to create BRDF 
data [3, 5, 27]. Empirical methods. There exist a variety of purely empirical re­.ection models, the 
most familiar being the models introduced by Gouraud [6] and Phong [15]. These two initial models were 
meant to be used with hand-chosen parameters, and thus these parame­ters are intuitive. A variety of 
more complex methods have been introduced to improve characteristics of the Phong model for ef.­ciency 
[19], to include anisotropy [26], and enforce physical con­straints such as reciprocity [9]. Other models 
have been developed to .t measurement data as opposed to being intuitive [10]. Figure 1: Images generated 
using the new BRDF model with un­usual microfacet distributions. The BRDFs used to create these images 
are both reciprocal and energy-conserving. The only illu­mination is a small distant source, and the 
highlights will stay un­changed if the spheres rotate about the axes through their north and south poles. 
Height correlation methods. In these methods a random rough surface is a realization of some Gaussian 
random process. Such a process can be described by its correlation function which is di­rectly related 
to surface height correlations. This is the most com­plete surface representation used in computer graphics. 
Some of the most detailed descriptions of light scattering by a surface, including wave optics effects, 
were obtained using this approach [8, 22]. Microfacet methods. Somewhere between the height correla­tion 
methods and empirical methods lie models based on microfacet theory [2, 4]. Microfacet models assume 
the surface consists of a large number of small .at micromirrors (facets) each of which re­.ect light 
only in the specular direction. By computing the number of visible microfacets at the appropriate orientation 
to specularly re.ect light from the source to the viewer, one can determine the BRDF. All of these methods 
have their place. In applications where little is known about the low-level properties of the surface, 
mea­surement is essential. Where physical optics effects are important, height correlation methods should 
be used. Our interest is in visual computer graphics applications which do not have obvious physi­cal 
optics effects (e.g. metal with relatively large scratches, fabric). The lesson from empirical models 
is that in many cases viewers are not particularly sensitive to the .ne details of light scattering as 
long as the main character of the re.ection is conveyed correctly. This paper uses this aspect of human 
sensitivity to suggest a new micro­facet model speci.cally intended to capture the main character of 
re.ection. Microfacet models are able to capture the main character of re­.ection for surfaces whose 
appearance is dominated by surface scattering. Although microfacet models lack the precision of height 
correlation methods, they tend to be more intuitive with simpler ex­pressions. However, to date there 
has been no microfacet model that is reasonably general in its assumptions, maintains a simple formu­lation, 
and conserves energy. In this paper we develop a model with all of these characteristics by introducing 
assumptions about surfaces that we believe are reasonable. These assumptions allow Figure 2: Geometry 
of re.ection. Note that k1, k2,and h share a plane, which usually does not include n. On the left, the 
microfacet can see in directions k1 and k2 so it contributes to the BRDF. On the right, direction k2 
is blocked and the microfacet does not contribute. Note that the microfacet distribution is not restricted 
to height .elds. us to create a relatively simple formula for the probability that a microfacet at a 
certain orientation is visible to the light/viewer. The BRDF produced by this process is compact, reciprocal 
and energy­conserving with only mild restrictions on the distribution of micro­facet orientation (e.g., 
the very general distributions in Figure 1). Our assumptions and guiding principles in relation to microfacet 
theory are given in Section 2. Formalisms are developed in Sec­tion 3. The key development of the paper, 
a simpli.ed shadowing term, is introduced in Section 4, and the resulting BRDF is derived. Section 5 
shows that this BRDF model conserves energy, and de­rives a diffuse term to account for secondary and 
subsurface re.ec­tion. The model is applied to a variety of surfaces in Section 6. This last section 
serves as a set of case-studies which both show how the model can be applied, and that it is more general 
than previous microfacet approaches. We believe the only other method which is able to handle such a 
diverse set of surface microgeometries is the virtual gonire.ectometer approach involving explicit model­ing 
of the surface structure and statistical averaging the results of light scattering simulations. 2 Overview 
The strategy behind our model is in balancing issues of practicality and accuracy to produce a simple 
formulation that is still expres­sive, reciprocal, and conserves energy. In this section we discuss the 
basic ideas of microfacet models, as well as our strategy for us­ing this theory to produce BRDFs. Important 
symbols used in the paper are listed in Table 1. Microfacet models assume that the surface consists of 
a large number of small .at micromirrors (facets) each of which re.ects light only in the specular direction 
with respect to its own normal h (Figure 2) and the overall appearance of the surface is governed by 
two assumptions: the microfacet normals have an underlying probability density function p(h).  a microfacet 
contributes to BRDF for a given pair of direc­tions if and only if it is visible (not shadowed) relative 
to the lighting direction k1 and the viewing direction k2.  The BRDF for a given direction pair (k1, 
k2) is determined entirely by the Fresnel re.ectance for that angle, the fraction of microfacets with 
normal vector h exactly between k1 and k2, and the shadow­ing term: the fraction of those microfacets 
which are visible to both eye and light (Figure 2). Microfacet theory s only knowledge of the (ab) scalar 
(dot) product of vectors a and b k1 normalized vector to light k2 normalized vector to viewer n surface 
normal to macroscopic surface d(k1, k2) BRDF h normalized half-vector between k1 and k2 p(h) probability 
density function of microfacet nor­ mals F (cos .) Fresnel re.ectance for incident angle . P (k1, k2, 
h) Probability that light from k1 re.ecting in di­ rection k2 is not shadowed .f ) average of function 
f over distribution p(h) (see Equation 9) O+(k) set of directions h where (hk) > 0 (see Fig­ ure 4) g(k) 
average of positive (hk) (see Equation 18) Table 1: Important terms used in the paper surface con.guration 
is p(h), and this alone does not uniquely de­termine the shadowing term. However, the shadowing term 
is still heavily constrained by energy conservation. The shadowing term is the most complex part of most 
microfacet-based models, even if additional p(h)-speci.c informa­tion about the surface geometry is used. 
Because there are many possible surface geometries that are consistent with a given p(h), it is the case 
that no speci.c shadowing function is right . We believe that in most cases the shape of p(h) function 
itself has a much greater impact on the appearance than the shadowing. This suggests the key idea in 
this paper: the shadowing term should be made as simple as possible while remaining physically plausible. 
Such a shadowing term is developed in Section 4. This key ex­tension of the standard microfacet theories 
allows us to construct a general procedure to create a BRDF for a statistical surface starting from p(h). 
Note that surface description in the language of p(h) is less de­tailed than that of using height correlation 
functions. Nevertheless, we believe that the microfacet normal distribution is more intuitive to deal 
with than the correlation functions. As we will emphasize in Section 6, enough useful information about 
p(h) can be obtained from general notion of surface structure obtained through visual examination of 
the surface and the specular re.ection highlight. Moreover, attempting to obtain more detailed information 
about the distribution might not be worth the effort. As in any other model, we make simpli.cation in 
our approach which affect the .nal re­sult, but what we are trying to do is generate a physically plausible 
BRDF having the general character of the surface re.ection while restricting the range of allowed surface 
microstructures as little as possible. This is in contrast to most other physics-based approaches which 
concentrate on a particular type of surface, usually Gaussian height .eld, and emphasize the need for 
precise knowledge of sur­face characteristics. Some care should be exercised when specifying p(h). In 
partic­ular, because we do not make the common assumption of a surface being a height .eld, in this general 
case p(h) should refer only to the distribution of visually important or surface part of the mi­crofacets. 
For example, a homogeneous porous substance thought of as a collection of microfacets will have an overall 
volume dis­tribution of microfacets pv(h)= const over the whole sphere of directions. However, that most 
of these microfacets will be com­pletely hidden and will not be of any signi.cance for the scattering 
process which occurs on the surface. In this case it is rather dif.cult to separate surface from the 
rest of the substance and judge the exact shape of p(h). Fortunately, because we are not trying to reproduce 
all the details of the re.ection function, a reasonable guess for p(h) is all we need and for this surface; 
it might be p(h)= const in the upper hemisphere and p(h)=0 in the lower one. Note, that by making this 
particular choice for p(h) the surface is restricted to be a height .eld. The initial choice can be re.ned 
later if necessary but in this particular case it the surface will be mostly diffuse and small re.nements 
will not dramatically change the appearance. We are concerned with single-bounce re.ections from the 
micro­facets and stay within the limits of geometric optics and Fresnel re­.ection. The result is a new 
form of the specular component of the BRDF which constitutes the main contribution of the paper. The 
complete BRDF can also have a diffuse term which accounts for multiple bounces and subsurface scattering. 
This issue along with other important properties of the BRDFs produced with the gener­ator are brie.y 
discussed in Section 5. Our framework is modular and allows the user to choose the form of the .nal BRDF 
most ap­propriate for the particular application. 3 Microfacet Theory We now review the main results 
of microfacet theory as developed by Torrance and Sparrow [23] and later introduced to computer graphics 
by Cook and Torrance [4]. We follow their approach of considering a collection of microfacets of small 
but .nite size, and we derive the basic formula for BRDF in terms of quantities conve­nient for our model. 
The quantity we wish to derive an expression for is the BRDF d(k1, k2) which gives the ratio of radiance 
observed by a viewer in the direction k2 to irradiance from in.nitesimal solid angle about k1. Throughout 
the paper, all vectors are shown in bold.They are assumed to be normalized, and all quantities with subscript 
1 refer to incident direction while those with subscript 2 belong to the outgoing direction. Both k1 
and k2 and all normals point outward from the surface. If we expose the surface to a uniform radiance 
of L1 coming from a small solid angle fs1 around k1, the outgoing radiance in direction k2 will be L2 
= d(k1, k2) L1 (k1n)fs1, (1) where n is the surface geometric normal and two vectors written next to 
each other in parenthesis denotes their scalar product, i.e., the cosine of the angle between them. The 
use of f is not stan­dard notation, but is used to make the algebra less cluttered without losing the 
gist of the argument. By the de.nition of radiance, if (k2n)A is the projected surface element area in 
the direction k2 and fE(k1 k2) is the power re.ected by the surface in the di­rection k2,then fE(k1 k2) 
L2 = , (2) A(k2n)fs2 and BRDF can be written as fE(k1 k2) d(k1, k2)= . (3) AL1(k2n)(k1n)fs1fs2 Only a 
fraction of all microfacets will participate in scattering the energy from k1 to k2. If the number of 
these active microfacets is Nactive and all microfacets have the same area Amf , their total projected 
area in the direction of k1 is NactiveAmf (kh) and the total scattered power is fE(k1 k2)= L1fs1NactiveAmf 
(kh)F((kh)), (4) where h is the normalized half-vector between k1 and k2 and F((kh)) is Fresnel coef.cient 
giving the fraction of incoming light which is specularly re.ected by a microfacet. Note that we will 
drop subscripts in our notations if either of incoming and outgoing direction can be used in an expression 
(e.g., (kh)). Out of the total of N surface microfacets, only Np(h)fsh will have their normals oriented 
in the appropriate direction. The den­sity p(h) does not specify all surface properties uniquely, but 
in our simpli.ed approach this is the only characteristic of the surface we will use in our analysis. 
Note that this function operates in the domain of microfacet normals which is different from the space 
of incoming and outgoing light directions. In particular, for the case of specularly re.ecting microfacets, 
the relationship between ele­mentary solid angles [23] can be shown to be fs2 =4(k1h)fsh. (5) Even if 
a microfacet has the required orientation, it might still not contribute to the single-bounce highlight 
if it is shadowed by other microfacets for either incoming or outgoing direction. Introducing the probability 
for a microfacet not to be shadowed in either incom­ing or outgoing directions as 0 * P(k1, k2, h) * 
1 we will have Nactive = Np(h)P(k1, k2, h)fsh and BRDF in the form NAmf p(h)P(k1, k2, h)F((kh)) d(k1, 
k2)= . (6) 4A(k1n)(k2n) Equation 6 is a somewhat modi.ed version of the original result of Torrance and 
Sparrow who present its more detailed derivation [23]. The area A of the surface element can be written 
as a sum of the projected areas of all microfacets: A =Amf (hn)P(n, h), (7) facets where we introduce 
probability P(n, h) for a microfacet not to be shadowed in the surface normal direction n by other microfacets. 
If the surface is a height .eld, P(n, h)=1 but in the general case some microfacets may not contribute 
to the area A of the projection. This question is related to the general shadowing term P(k1, k2, h) 
and we postpone its discussion until the next section. The P is used with a variable number of arguments 
that depend on what assumptions are in play for that equation. Given a large number of microfacets, Equation 
7 can be rewritten using the average over the ensemble of microfacets as A = NAmf .(hn)P(n, h))ens, (8) 
where ....)ens denotes the averaging procedure. One of the most fundamental results in statistics states 
that as the size of the en­semble increases, for a certain function f of a random variable its average 
over ensemble .f)ens converges with probability one to its average .f) over the distribution of the random 
variable. In our case we can write for any quantity f(h): 1 .f(h))ens = .f(h)) = f(h)p(h)dsh, (9) O where 
the integration is done over the unit sphere O of microfacet normal directions (Gaussian sphere). So, 
for the BRDF we .nally have p(h)P(k1, k2, h)F((kh)) d(k1, k2)= , (10) 4(k1n)(k2n).(nh)P(n, h)) and in 
the important special case of surface being a height .eld, p(h)P(k1, k2, h)F((kh)) d(k1, k2)= . (11) 
4(k1n)(k2n).(nh)) Although we have assumed that all microfacets have equal area Amf the result does not 
change if there is an arbitrary distribution of microfacet areas so long as this distribution is not 
correlated with p(h), the distribution of normals. Given a density p(h), all terms in Equation 11 are 
straightfor­ward to compute except for the shadowing term P(k1, k2, h).We now turn to the discussion 
of this shadowing term which is neces­sary to complete our formulation of the specular part of BRDF. 
 4 Shadowing Term Most of the complexity of microfacet-based models arise from the shadowing function 
P (k1, k2, h). In this section we describe how previous models deal with this term and introduce a new 
simpli.ed shadowing term. 4.1 Previous Shadowing Terms On any rough surface it is likely that some microfacets 
will either not receive light, or light re.ected by them will be blocked by other microfacets. The .rst 
situation is referred to by many authors as shadowing and the second as masking. However, these events 
are symmetrical and for simplicity we will refer to both of them as shadowing. A rigorous derivation 
of the probability that a point on the surface is both visible and illuminated (also known as the bistatic 
shadowing function) leads to very complicated expressions and a set of approximations is made to make 
the problem tractable. Several forms of the shadowing term have been derived in different .elds [1, 18, 
21, 23, 25] and some of them (usually after further simpli.cation) were later introduced to computer 
graphics re.ec­tion models [4, 8, 22]. The most popular shadowing functions currently used are mod­i.cations 
of those of Smith [21], Sancer [18] and the original Tor­rance and Sparrow shadowing term [23]. The .rst 
two formula­tions are rather complex and are designed only for Gaussian height .elds. Smith, in addition, 
assumes an isotropic surface. The shad­owing function by Torrance and Sparrow is simple, but assumes 
an inconsistent model of an isotropic surface exclusively made by very long V-cavities. None of the existing 
functions is .exible enough to accommodate a suf.ciently general distribution of microfacets. Also, most 
of the formulations operate with height distributions, not the more intuitive normal distribution p(h). 
In addition to space limitations, this is the reason we do not present the expressions of previously 
derived shadowing functions here. The reason most authors deal with height distribution functions is 
that shadowing is clearly a non-local event intimately related to the height distribution of the surface 
and this information is neces­sary for rigorous treatment of shadowing. In the next subsection we will, 
however, make several assumptions which allows us to derive a very general form of the shadowing term 
P (k1, k2, h)suf.cient for our purposes. 4.2 New Shadowing Term As indicated by the preceding discussion, 
we cannot treat shadow­ing rigorously if we assume a general form for the microfacet nor­mal density 
function. Therefore, our generator is most appropriate in cases where the effects of shadowing are secondary 
compared with the in.uence of normal distribution shape. Even in these cases, however, we cannot ignore 
the shadowing term P (k1, k2, h).As can be seen from Equation 10, at the very least shadowing should 
take care of the divergence at grazing angles where the denominator terms disappear: (k1n)(k2n)0. The 
shadowing term can be written as P (k1, k2, h)=P (k1, h)P (k2, h | k1), (12) where P (k1, h)is the probability 
of not being shadowed in the di­rection k1 and P (k2, h | k1)is conditional probability of not being 
shadowed in the direction k2 given that the facet is not shadowed in direction k1. In general, P (k2, 
h | k1)8 =P (k2, h). For example, it is easy to see that in the extreme case where k1 = k2 we have P 
(k2, h | k1)=1. This shows that visibilities in the incoming and outgoing directions are correlated. 
Most of shadowing func­tions, however, are derived under the assumption of uncorrelated visibilities. 
Van Ginneken et al. [24] considered how this correla­tion affects Smith s shadowing function, and found 
that its effect can be accounted for by modifying the uncorrelated expression. In most of this paper 
we will use the uncorrelated form of the shadowing term written as a product of the two independent factors 
for each of the two directions: P (k1, k2, h)=P (k1, h)P (k2, h). (13) This leads to some underestimation 
of the BRDF if directions k1 and k2 are close to each other. If the viewing conditions are such that 
this arrangement is of particular importance (in a night driving simulator, for example) or if retrore.ection 
is one of the pronounced features of surface appearance (see Section 6.4) we propose using a different 
form of the shadowing term: P (k1, k2, h)=(1- t(4))P (k1, h)P (k2, h)+ t(4)min(P (k1, h),P (k2, h)), 
(14) where -J<4 <J is the angle between the projections of vectors k1 and k2 onto the tangent plane and 
t(4)is a correlation factor with values between 0 and 1. The case t(4)=0corresponds to the completely 
uncorrelated case. This form of correlated shadow term was chosen because it is simple and the resulting 
BRDF will still conserve energy with arbitrary t(4), as will be shown in Sec­tion 5.3. We have not done 
extensive experimentation with the par­ticular form of t(4)but we do not believe it makes a large difference 
as long as t(0) = 1and t(4)monotonically decreases to almost zero as |4| increases. The range of correlation 
effects was found in [24] to be on the order of 15-25 degrees, so we use a Gaussian in 4 with the width 
of 15 degrees. Allweneed nowisanexpression for P (k, h), the probability for a microfacet to be visible 
in a given direction k. Note that P (n, h) in Equations 7, 8 and 10 of the previous section is just a 
special case of this probability with k =n. The key assumption we make is that probability for a microfacet 
to be visible in direction k does not depend on the microfacet s orientation h as long as it is not turned 
away from k (not self-shadowed), namely { P (k) if (kh)> 0 P (k, h)=(15) 0 if (kh)* 0 This assumption 
is equivalent to the absence of correlation between the microfacet orientation and its position. This 
distant shadower assumption has been invoked before to simplify complicated shad­owing expressions obtained 
in other .elds [1, 21, 25] but we will use it in a different way -as a basis for deriving a simple and 
gen­eral shadowing function. Intuitively, it corresponds to rather rough surfaces and does not hold if 
the microfacets with certain orienta­tion are more likely to be found at a certain height. For example, 
a surface made of cylinders as shown in Figure 3a will not obey this assumption while a very similar 
surface in Figure 3b might. In gen­eral, the more correlated the surface microfacets are, the less likely 
P (k, h)is to obey Equation 15. The two surfaces in Figure 3 may still have the same distribu­tion p(h)and 
there is no way for us to distinguish between the two cases. Similarly, we will not be able to distinguish, 
for ex­ample, between positive and negative cylinders of Poulin and Fournier [16] but from their images 
it is clear that the differences in appearance due to microfacet visibility issues and not to the dis­tribution 
of microfacets are minor in this case. If .ner details of microfacet arrangement not captured by p(h)are 
expected to sub­stantially affect the appearance, some different framework should be used (see also Section 
6.4). The total projected area of a surface element onto direction k is A(kn). It can also can be written 
in a way similar to Equation 7: A(kn)= Amf (hk)+P (k). (16) facets  Figure 3: Examples of surface microgeometry. 
Top: microfacets with almost vertical orientation are more likely to be found near the bottom of the 
surface and, therefore, are more likely to be shad­owed. Bottom: orientation and height are largely uncorrelated. 
Here the subscript + refers to the fact that the summation is per­formed only over microfacets turned 
towards k, namely the ones with (hk) > 0. Introducing averaging over microfacets and, as before, replacing 
it by averaging over distribution, we get A(kn)=NAmf P (k).(hk)+). (17) We areabletotake P (k)out of 
the averaging integral because of our assumption that it does not depend on h. Because of the great importance 
of quantity .(hk)+)we introduce a new notation 1 g(k)=.(hk)+)=(hk)+p(h)dsh, (18) O+(k) where the integration 
is done in h-space over the hemisphere O+(k)of directions (hk)> 0(Figure 4). Note that if the surface 
is a height .eld, P (n)=1and Equations 8 and 17 immediately give a useful expression for P (k): (kn)g(n) 
P (k)= . (19) g(k) In this special case p(h)=0in the lower hemisphere and the aver­aging in g(n)is effectively 
done over the complete distribution. To handle a more general case, we note that each microfacet turned 
away from the direction k will have a shadow with area Amf (hk). This area must be subtracted from the 
contribution of microfacets turned towards k. Again replacing sums by averages over ensemble and then 
over distribution, we write the projected area on the right-hand side of Equation 17 as NAmf P (k).(hk)+) 
= NAmf .(hk)+)+NAmf .(hk)-), (20) or .(hk)-) P (k)=1+ . (21) g(k) The second term is negative and the 
integration in it is done over the part O-(k)of distribution complimentary to O+(k)(Figure 4). It is 
clear from this equation that P (k)*1as it should be. For a dis­tribution of microfacet normals p(h)to 
represent a valid surface, at the very least the average normal vector over the entire distribution must 
lie in the direction of the geometric normal n of the surface: 11 hp(h)dsh + hp(h)dsh = O+(k)O-(k) 1 
hp(h)dsh =n(.h)n) (22) O Figure 4: Integration domain for g(k) Multiplying both sides of this equation 
by scalar k we have .(hk)+)+.(hk)-)=(kn).(hn)), (23) or .(hk)-)=(kn).(hn))-g(k). (24) Substituting this 
into Equation 21 we obtain an expression for P (k): (kn).(hn)) P (k)= . (25) g(k) Averaging in the numerator 
is done over the complete sphere Oof directions. Note that Equation 19 is now just a special case of 
Equa­tion 25 and that Equations 21 and 25 show that for any physically valid distribution p(h)our probability 
of being visible will indeed lie between 0 and 1. The combination of Equations 10, 13 (or 14) and 25 
completely describes the specular part of BRDF. Using the uncorrelated form of shadowing term of Equation 
13, we get p(h).(hn))F ((kh)) d(k1, k2)= . (26) 4g(k1)g(k2) Note the interesting fact that p.d.f. p(h)does 
not even have to be normalized to be used in this equation. The above formula is well­suited to evaluation. 
Given p(h), it is straightforward to evaluate the BRDF. Equation 26 is the main contribution of this 
paper. For the rest of the paper we will discuss implications and applications of this formula.  5 Extensions 
and Discussion In this section we discuss several issues related to the specular-only single bounce BRDF 
model derived in the last section. In particu­lar, we discuss an energy-conserving diffuse term, implementation 
issues, extension to non-Fresnel microfacets, and prove energy con­servation. 5.1 Diffuse Term Equation 
26 describes the part of scattering process due to single­bounce re.ections from microfacets. In addition 
to this specular part there will be other scattering events, such as multiple bounces and subsurface 
scattering. A complete description of these pro­cesses is rarely attempted in a general-purpose BRDF 
model and their combined contribution is usually represented by adding a dif­fuse component to the specular 
BRDF. The most common form of the diffuse term is Lambertian: kddd d(k1, k2)= + ksds(k1, k2), (27) J 
where 0 * dd * 1 is diffuse albedo of the surface while kd and ks are user-speci.ed constants controlling 
the relative importance of specular and diffuse re.ections. This is a perfectly valid option in our case 
as well. We can simply use Equation 26 for ds and ensure that kd + ks * 1 to preserve the energy conservation 
achieved for the specular part (Section 5.3). However, this simple form of diffuse term has problems. 
First of all, it is not obvious how to choose weights kd and ks. Second, it is clear that as more light 
is being re.ected specularly, less of it is available for diffuse scattering, so the relative weights 
kd and ks of diffuse and specular re.ections should not be constants. If Fresnel effects can cause ks 
to approach one for grazing angles, kd must be set to zero for all angles (since it is a constant). To 
take this effect into account in a way preserving reciprocity, we use a method of Shirley et al. [20] 
and write for kd kd(k1, k2)= c(1 - R(k1))(1 - R(k2)), (28) where 1 R(k)= ds(k, k')(k'n)dskl (29) is the 
directional hemispherical re.ectance of the specular term, where k' is the mirrored direction of k. We 
also completely dis­pose of ks by allowing the specular re.ection to have its way and adjust the diffuse 
term so that it consistently follows the specular re.ection. The normalization constant c is computed 
such that for dd =1 the total incident and re.ected energies are the same. A complete BRDF will have 
the form d(k1, k2)= c(1 - R(k1))(1 - R(k2))dd + ds(k1, k2). (30) This form of diffuse term implicitly 
assumes that there is no absorp­tion on the surface and all the energy which is not re.ected specu­larly 
is available for diffuse scattering. The situation is different in case of metals. First, if f0 is the 
normal re.ectance of the metal, only approximately f0 fraction of incoming light is not absorbed by a 
.at metal surface. Second, diffuse scattering here is exclusively due to multiple bounces and thus the 
diffusely scattered light has a more saturated color of the metal than the primary re.ection does. We 
attempt to take both of these effects into account by replacing ones in Equation 30 by f0 and assigning 
dd for a metal (which oth­erwise does not have any physical sense) to be f0. Because the true fraction 
of non-absorbed light is greater than f0, factor (f0 -R(k)) can become negative for some surfaces due 
to our approximation. We simply set the diffuse term to zero in such cases. 5.2 Implementation Issues 
Implementation of our model in a rendering system is straightfor­ward. For the Fresnel coef.cient we 
use Schlick s approximate for­mula [19] F ((kn)) = f0 +(1 - f0)(1 - (kn))5 (31) where again f0 is the 
Fresnel factor at normal incidence. Note that we could also use the full Fresnel equations, but we use 
Schlick s formula only for convenience. This should not lead to signi.cant accuracy problems as for the 
error introduced by Schlick s formula is smaller than one percent compared with the full Fresnel expres­sion 
[19]. To generate a BRDF for a new distribution p(h) all we need, in addition to the implementation of 
p(h) itself, are values for g(k) and R(k). Unfortunately, because of the non-standard in­tegration domain 
of g(k), analytical expressions for this function can be obtained only for the most trivial p(h) s and 
we need to resort to numerical integration. However, the integrals are well-behaved and the results are 
smooth functions for non-singular p(h). This allows us to compute values of both g(k) and R(k) on a very 
coarse grid using available numerical packages, store the results in a table and use bilinear in­terpolation 
during the rendering process. We have used a total of 200 grid points (for many distributions an even 
coarser grid should be suf.cient). Integration was done using both Matlab and a simple home-built Monte 
Carlo routine. Two sets of computed R(k) (one with f0 =1 and one with f0 =0) are suf.cient to compute 
R(k) for a material with arbitrary f0 for a given microfacet distribution. In the BRDF generation phase 
we start from p(h) and output a compact numerical representation of three two-dimensional func­tions: 
g(k), R(k) with f0 =0 and R(k) with f0 =1.The last two functions are only used for the diffuse term and 
are not required for its simpler form in Equation 27. During rendering we use these data to compute the 
full four dimensional BRDF for arbitrary k1 and k2. At this stage we also use data for normal re.ectance 
f0 and diffuse albedo dd. Wavelength dependence of these quantities controls the color of the surface. 
We have not done a careful perfor­mance analysis but from our experience for a non-trivial p(h) most 
of the BRDF computation time is due to evaluating this normal dis­tribution function. Note that most 
distributions have some symmetry which can be exploited to further reduce the amount of data and/or generation 
time. Data for an anisotropic Gaussian distribution of normals, for example, need be computed only over 
a quarter of the hemisphere and for any isotropic distribution functions g(k) and R(k) become one dimensional. 
Finally, if a particular type of parameterized distribution (Gaus­sian, for example) is used often it 
should be possible to approximate g(k) with a simple function of k and distribution parameters as is 
commonly done to increase the ef.ciency of re.ection models. The same is true for R(k) but these functions 
usually have more com­plex shapes. 5.3 Energy Conservation By inspection of the formulas, it is clear 
that generated BRDFs are reciprocal. We now prove now that they also conserve energy for any physically 
plausible p(h). To do this, we assume the worst­case scenario of F ((kh)) = 1 and shadowing term in Equation 
14 with t(4)=1 (because P (k) * 1 this corresponds to the largest possible shadowing term for our model). 
The BRDF in this case will be p(h)min(P (k1),P (k2)) d(k1, k2)= * 4.(hn))(k1n)(k2n) p(h)P (k1) 4.(hn))(k1n)(k2n) 
Hemispherical re.ectance for a given incoming direction is 1 R(k1)= ds(k1, k2)(k2n)ds2 * 1 P (k1) p(h)ds2 
= 4.(hn))(k1n) 1 P (k1) p(h)4(k1h)dsh4.(hn))(k1n) The last transition is done using Equation 5. The integration 
is done over a complex region of h-space which is in any case contained in the hemisphere O+(k1). Extending 
the integral over the whole O+(k1)and using de.nitions 18 of g(k)and25of P (k)we com­plete the proof: 
1 P (k1) R(k1)* (k1h)p(h)dsh = .(hn))(k1n) O+(k1) P (k1)g(k1) =1 (32) .(hn))(k1n) The only fact we used 
in our proof is that P (k)* 1for any k.In Section 4, in turn, this was shown to be the case for any p(h)whose 
average normal vector .h) is parallel to the geometric normal of the surface. This is the only restriction 
on microfacet distribution p(h). If it is satis.ed, the generated BRDF will conserve energy. 5.4 Non-Fresnel 
Microfacets Our model is not restricted to perfectly specular microfacets. In general, microfacets with 
many orientations will contribute to sur­face BRDF for given incoming and outgoing directions and integra­tion 
of their contribution is necessary. Let all microfacets have elementary BRDF p. Then we can re­peat with 
some modi.cations the derivation from Sections 3 and 4 to arrive at the result P (k1, k2) d(k1, k2)= 
(k1n)(k2n).(nh)) 1 p(k1, k2)(k1h)+(k2h)+p(h)dsh (33) The integration is done over the sector where both 
(k1h)and (k2h) are positive and any of shadowing terms P (k1, k2)from Section 4 can be used. Note that 
p(k1, k2)is usually speci.ed with respect to microfacet s local coordinate system and a coordinate transforma­tion 
is necessary to obtain its value for the integral in Equation 33. Although this extension considerably 
broadens the range of sur­faces our model is applicable to, we also lose one of the main ad­vantages 
of our approach: compactness. Before, we could represent a general four dimensional BRDF using only two 
dimensional func­tions. The integral in Equation 33, however, is a four dimensional function by itself 
and does not, in general, allow lower dimensional representation. For some special cases, such as Lambertian 
elemen­tary BRDF coupled with isotropic p(h)the integral becomes three dimensional and, therefore, feasible 
to compute, store and use in a way similar to that described in Section 5.2. For an isotropic Gaus­sian 
distribution of Lambertian microfacets the general behavior of the generated BRDF is similar to that 
of Oren-Nayar s model [14], namely, retrore.ection is increased compared to a Lambertian sur­face (Figure 
8).  6 Applications In this section we apply our model to a variety of surface types. Al­though we have 
implemented our model in a Monte Carlo ray tracer capable of handling complex geometries and illumination 
effects, our images in this section intentionally show very simple objects and lighting conditions. In 
particular, illumination is coming from a single small light source far from the scene and indirect lighting 
is not included. This is done to emphasize effects due to BRDF of the material and to make the comparison 
with previous results easier. Re.ectance data of gold are used as f0 (see Section 5.2) for all metal 
objects while for non-metals f0 is set to 5%across the visible spectrum. R R Figure 5: Anisotropic Gaussian 
golden spheres with Ox =0.1, Oy =1.0. Left: Ward. Right: new model. 1 1 0.5 0.5 1.5 1.5 0 0 1 1 1 1 0.5 
0.5 0.5 0.5 00 phi 00 phi (kn) (kn) Figure 6: Directional hemispherical re.ectance as a function of incoming 
angle for perfectly re.ecting microfacets with Gaussian distribution Ox =0.1, Oy =0.2. For an ideal .at 
surface R should be 1.0 everywhere. Left: Ward. Right: new model. Figure 7: Anisotropic Gaussian golden 
painted plastic spheres with Ox =0.1, Oy =0.2. Left: Ward. Right: new model. Figure 8: Gaussian spheres 
with Lambertian microfacets. Right: new model with Ox =Oy =1.0. Left: Oren-Nayar with compatible parameters. 
 6.1 Gaussian Surfaces By far the most popular distribution used in BRDF research liter­ature is Gaussian. 
This is due to both its practical importance and nice mathematical properties. Gaussians are used in 
all four ma­jor categories of BRDF models outlined in the introduction. While some of this work is closer 
to our approach in its theoretical foun­dations, we feel that from the practical point of view our model 
is closest to that of Ward [26]. Ward s BRDF is simple, handles anisotropic distributions and seeks to 
reproduce the main character of the material s re.ectance behavior without attempting an overly detailed 
description. Other previous models do not simultaneously possess all these properties. To create an anisotropic 
Gaussian BRDF, we use the distribution 2 22 22 p(h)=c * exp(- tan .(cos4/Ox +sin 4/Oy)) (34) where . 
is the angle between the half vector h and the surface nor­mal, 4 is the azimuth angle of h and c is 
a normalization constant. Two side-by-side comparisons of our model with Ward s are shown in Figures 
5 and 7. Note that the shape of highlight is nearly identical while there are some differences in the 
diffuse part of images which is due to Ward effectively using a simpler form (Equation 27) of the diffuse 
component. In particular, for our metal sphere on Figure 5 the diffuse component appears automatically 
when there is enough energy left after single-bounce scattering. To achieve the same effect in Ward s 
model (and any other using the popular Lambertian diffuse term) it would be necessary to manually adjust 
the diffuse re.ectance parameter. This .gure also shows that the highlight is brighter for our BRDF. 
The general reason for this is clear from Figure 6 where the hemispherical re.ectance R is plotted versus 
the incoming light direction. To make the plots directly comparable, we show data for most re.ecting 
specular BRDF in both cases (f0 =1for our model and ds =1in Equation 5 of Ward s paper [26]) and do not 
include the diffuse term. For the values of parameters shown, the surface is quite close to being .at, 
so one would expect that R should be close to that of .at surface, 1.0 in this case. One can see from 
the plots that our model behaves as expected while Ward s does not. Note also that the true value for 
R at the grazing angle ((kn)=0)is in.nite for Ward s model [13] and we simply extrapolate previous behavior 
to get the data point at the grazing angle. While our approach does require an extra generation step, 
com­putation time during the rendering process of our BRDF is close to that of Ward s and our model is 
a viable alternative where energy conservation is of great importance for a particular application. Figure 
8 compares a BRDF generated for an isotropic Gaussian distribution of Lambertian microfacets with an 
extension of our process (Section 5.4) and Oren-Nayar model with compatible pa­rameters. Both BRDFs have 
the tendency to make objects appear .atter than the Lambertian BRDF due to increased retrore.ec­tion. 
 6.2 Grooved Surface A surface consisting of ideal V-grooves all running in a given di­rection will have 
its p(h)proportional to the sum of two delta func­tions, each accounting for microfacets forming one 
side of a groove. Replacing these delta functions with narrow Gaussians (O =0.1) to account for imperfections 
and going through our generation pro­cess, we create a BRDF which correctly shows the main feature of 
a grooved surface s re.ectance, double re.ections. Figure 9 shows a piece of grooved metal illuminated 
by a single light source. The orientation of the grooves on the left is perpendicular to the viewing 
direction while on the right they are parallel. Figure 9: Double highlights from a single light source 
for the same metallic grooved surface at two orientations of the grooves. Grooves are symmetrical with 
the angle of 40 degrees 6.3 Satin The microstructure of woven cloth is usually thought of as a sym­metric 
pattern of interwoven cylindrical .bers running in perpen­dicular directions. While it would be possible 
to generate a BRDF corresponding to this structure with our approach, the surface of particular fabric 
we studied had a different microstructure shown in Figure 10. It is created almost exclusively by .bers 
running in one direction with about 70%of the .ber length lying in the rel­atively .at part of the .ber 
while the other 30% corresponding to the bent parts at the ends. We model the distribution of mi­crofacets 
as a linear combination of two terms corresponding to these .at and bent parts of the cylindrical .ber: 
p(h)=0.7* pflats(h)+0.3* pends(h). The coef.cients re.ect mutual area contributions of the two parts 
to the complete distribution. Both pflats(h)and pends(h)were chosen to be cylindrical Gaussian height.elds 
(Oy = 0, p(h)=0 for (hn) < 0) with different widths. Values Ox =0.1for pflats(h)and Ox =0.3for pends(h) 
were used. Strictly speaking, the shape of real pends(h) would probably be more accurately modeled by 
a distribution with .atter top and faster drop-offs than that of a Gaussian. This was attempted but the 
results were almost identical visually, so a simpler Gaussian distribution was used for the .nal image. 
This is consistent with our belief that the very precise characterization of the microfacet distri­bution 
is not needed for visual applications. Note that because g(k) is linear in p(h), no new integration is 
necessary to compute g(k) if g s corresponding to pflats(h) and pends(h)are already com­puted. This suggests 
an ef.cient way of creating new distributions as a linear combination of ones for which g(k)has been 
previously computed. For example, small contribution due to perpendicular .bers can be added in this 
manner if necessary. Because the appearance of real cloth is dramatically affected by the presence of 
characteristic wrinkles, we used a dynamic simu­lation method [17] to create cloth geometry. The left 
side of Fig­ure 11 shows a satin tablecloth rendered with generated BRDF. It is interesting to contrast 
this image with the image on the right us­ing the same geometric model with the BRDF described in the 
next section. 6.4 Velvet Velvet is another example of a material with interesting re.ectance properties 
not easily conveyed by conventional BRDFs. In their vir­tual goniore.ectometer, Westin et al. [27], model 
velvet microstruc­ture as a forest of narrow cylinders (.bers) with the orientation of each cylinder 
perturbed randomly. While it is dif.cult to write an exact p(h)corresponding to such surface for the 
reasons outlined in Section 2, a simple intuitive form of this function written as an inverse Gaussian 
height.eld is enough to capture the main char­acter of the distribution:   p(h)= c * exp(- cot 2 ./O2), 
(35) with O =0.5 for the image on the right of Figure 11 which shows a material with distinct velvet-like 
re.ectance properties. Because retrore.ection is one of the most pronounced re.ection properties of velvet 
[11], we used the correlated form of shadowing term (Equation 14) to generate both this and slanted .ber 
(see below) velvet BRDFs. Contrary to Westin et al. we ignore the tips of the .bers due to their very 
small area. If there were any specular highlights due to the tips, their contribution can be easily added 
by forming a linear combination of an inverse Gaussian with a regular Gaussian distribution. Although 
this approach produced good results, a symmetric for­est of .bers was not what we saw when we examined 
a piece of real velvet. More realistic structure is shown on the left of Fig­ure 12. The fabric consists 
of rows of tightly woven bundles of .lament. Each bundle is slanted with the angle of about 40 degrees 
with respect to the geometric normal of the cloth surface. We can call this arrangement milliscale geometry 
in contrast with micro­geometry formed by the thin .bers themselves. Similar geometry was credited as 
the major reason for velvet anisotropic re.ection behavior by Lu et al. [11]. Strictly speaking, our 
model does not take into account visibility issues due to this higher-order arrange­ment of microfacets. 
The most consistent approach therefore would be to model this structure explicitly, for example as a 
collection of slanted cylinders applying two different BRDFs (both of which can be generated by our process) 
to the tops and to the sides of these cylinders. An easier alternative would be an attempt to create 
a simple distribution of microfacets p(h) which, although potentially non-physical, can account for the 
milliscale visibility and produce a BRDF with necessary re.ection properties. Looking carefully at the 
velvet highlight structure we saw that it is the sides of the bundles and not the tops which contribute 
the most to the re.ection. This suggests that we can try to reproduce most of the behavior with a specular 
BRDF based exclusively on the p(h) accounting for the microfacets on the sides of the bundles. A slanted 
version of cylindrical Gaussian distribution (Oy = 0, Ox =0.5) schematically shown on the right of Figure 
12 was used. The only place where we used the part of distribution due to the tops of the bundles is 
the computation of .(hn)) when we double this value due to the tops contribution. Note two facts about 
this distribution: it is not a height .eld and its average vector .h) does not point in the direction 
of geometric normal. While the .rst fea­ture does not present any problem in our approach, the second 
one shows that this distribution is not physically realizable and, as a re­sult, the energy conservation 
of the generated BRDF is not guaran­teed. Computations of R(k) show that this quantity indeed exceeds 
one for 14 out of our 200 directional data points in the hypothetical case of perfectly re.ecting (f0 
=1.0) .ber material but was never a problem for our f0 =0.05 synthetic .bers. Figure 13 shows the results 
of this process. The illumination and viewing directions are almost parallel but due to the slant of 
the .bers the left side of the tablecloth is substantially brighter than the right one. This is in good 
agreement with the behavior of real velvet we observed. The right image of Figure 13 shows some limitations 
of our approach. Because we do not handle the details of multiple­bounce scattering and simply introduce 
a diffuse term to account for them, the right side of the red tablecloth does not look as it does for 
the real velvet. In the real material, light experiences multiple bounces among the red .bers for this 
viewing geometry acquiring a deep dark (almost black) color in the process. This is not captured by our 
simple diffuse term. 6.5 Unusual Distributions We can take to extreme the use of the desired re.ection 
properties as the only guidance in creating the distribution p(h) regardless of whether a material described 
by this function exists or is even physi­cally possible. For example, we can modulate a Gaussian p(h) 
with an arbitrary function or even an image to create the unusual high­lights shown in Figure 1. As long 
as the modulation is symmetric enough to keep the average vector .h) in the normal direction (such as 
the distribution used for the image on the left of Figure 1), the BRDF will be energy conserving. A more 
general modulation may result in .h) no longer parallel to n but in practice we notice that as long as 
this effect is not very strong, the energy conservation is not affected. For example, image on the right 
of Figure 1 was created with an energy conserving BRDF. While such unusual dis­tributions are not of 
great value in realistic image synthesis, they clearly demonstrate the generality of our approach and 
can poten­tially .nd applications in the special effects industry.  7 Conclusion The new BRDF model 
presented in this paper is well-suited to sur­faces whose primary characteristic is the shape of the 
specular high­light. We have found it reasonably straightforward to design new BRDFs for surfaces because 
the diffuse term and energy conser­vation are handled in a natural manner that does not require sub­stantial 
user intervention, and the parameters used in the model are intuitive. However, for surfaces whose appearance 
is not dominated by the specular highlight, our model is not well-suited. We have found that using our 
model does not require much hand­tuning of parameters; the images in the last section were generated 
with very few iterations on parameter values. We speculate that a model for subsurface effects in a similar 
spirit to our model is possible. The user would specify some simple parameters analo­gous to p(h) and 
a BRDF would be generated. We also believe that there should ultimately be separate terms for the components 
of the BRDF accounted for by primary specular re.ection, multiple­bound specular re.ection, and subsurface 
scattering.  Acknowledgements This work was supported by NSF grants 96 23614, 97 96136, 97 31859, and 
98 18344. Thanks to Robert McDermott for his help with production issues. The tablecloth models were 
done using Maya software generously donated by Alias/Wavefront. References [1] BECKMANN, P. Shadowing 
of random rough surfaces. IEEE Transactions on Antennas and Propagation 13 (1965), 384 388. [2] BLINN, 
J. F. Models of light re.ection for computer synthesized pictures. Com­puter Graphics (Proceedings of 
SIGGRAPH 77) 11, 2 (July 1977), 192 198. [3] CABRAL,B., MAX,N., AND SPRINGMEYER, R. Bidirectional re.ectance 
functions from surface bump maps. Computer Graphics 21, 4 (July 1987), 273 282. ACM Siggraph 87 Conference 
Proceedings. [4] COOK,R. L., AND TORRANCE, K. E. A re.ectance model for computer graph­ics. Computer 
Graphics 15, 3 (August 1981), 307 316. ACM Siggraph 81 Conference Proceedings. [5] GONDEK,J.S., MEYER,G.W., 
AND NEWMAN, J. G. Wavelength dependent re.ectance functions. In Proceedings of SIGGRAPH 94 (Orlando, 
Florida, July 24 29, 1994) (July 1994), A. Glassner, Ed., Computer Graphics Proceedings, Annual Conference 
Series, ACM SIGGRAPH, ACM Press, pp. 213 220. [6] GOURAUD, H. Continuous shading of curved surfaces. 
Communications of the ACM 18, 6 (June 1971), 623 629. [7] GREENBERG,D.P., TORRANCE,K.E., SHIRLEY,P., 
ARVO,J., FERWERDA, J. A., PATTANAIK,S., LAFORTUNE,E.P.F., WALTER,B., FOO,S.-C., AND TRUMBORE, B. A framework 
for realistic image synthesis. Proceedings of SIGGRAPH 97 (August 1997), 477 494. [8] HE,X. D., TORRENCE,K. 
E., SILLION,F. X., AND GREENBERG,D. P. A comprehensive physical model for light re.ection. Computer Graphics 
25,4 (July 1991), 175 186. ACM Siggraph 91 Conference Proceedings. [9] LAFORTUNE,E. P., AND WILLEMS, 
Y. D. Using the modi.ed phong BRDF for physically based rendering. Tech. Rep. CW197, Computer Science 
Department, K.U.Leuven, November 1994. [10] LAFORTUNE, E.P.F., FOO,S.-C.,TORRANCE,K. E., AND GREENBERG, 
D. P. Non-linear approximation of re.ectance functions. Proceedings of SIG-GRAPH 97 (August 1997), 117 
126. [11] LU,R., KOENDERINK,J. J., AND KAPPERS, A. M. L. Optical properties (bidirectional re.ection 
distribution functions) of velvet. Applied Optics 37,25 (1998), 5974 5984. [12] MARSCHNER,S. R., WESTIN,S. 
H., LAFORTUNE,E. P. F., TORRANCE, K. E., AND GREENBERG, D. P. Image-based BRDF measurement including 
human skin. Eurographics Rendering Workshop 1999 (June 1999). [13] NEUMANN,L., NEUMANN,A., AND SZIRMAY-KALOS, 
L. Compact metallic re.ectance models. Computer Graphics Forum 18, 13 (1999). [14] OREN,M., AND NAYAR, 
S. K. Generalization of lambert s re.ectance model. In Proceedings of SIGGRAPH 94 (Orlando, Florida, 
July 24 29, 1994) (July 1994), A. Glassner, Ed., Computer Graphics Proceedings, Annual Conference Series, 
ACM SIGGRAPH, ACM Press, pp. 239 246. [15] PHONG, B.-T. Illumination for computer generated images. Communications 
of the ACM 18, 6 (June 1975), 311 317. [16] POULIN,P., AND FOURNIER, A. A model for anisotropic re.ection. 
Com­puter Graphics 24, 3 (August 1990), 267 282. ACM Siggraph 90 Conference Proceedings. [17] PROVOT, 
X. Deformation constraints in a mass-spring model to describe rigid cloth behavior. In Proceedings of 
Graphics Interface 95 (1995), pp. 147 154. [18] SANCER, M. I. Shadow corrected electromagnetic scattering 
from randomly rough surfaces. IEEE Transactions on Antennas and Propagation AP-17,5 (September 1969), 
577 585. [19] SCHLICK, C. An inexpensive BRDF model for physically-based rendering. Computer Graphics 
Forum 13, 3 (1994), 233 246. [20] SHIRLEY,P.,HU,H.,SMITS,B., ANDLAFORTUNE,E.Apractitioners assess­ment 
of light re.ection models. In Paci.c Graphics (October 1997), pp. 40 49. [21] SMITH, B. G. Geometrical 
shadowing of a random rough surface. IEEE Trans­actions on Antennas and Propagation 15 (1967), 668 671. 
[22] STAM, J. Diffraction shaders. Proceedings of SIGGRAPH 99 (August 1999), 101 110. [23] TORRANCE,K. 
E., AND SPARROW, E. M. Theory for off-specular re.ection from roughened surfaces. Journal of Optical 
Society of America 57, 9 (1967). [24] VAN GINNEKEN,B., STAVRIDI,M., AND KOENDERINK, J. J. Diffuse and 
specular re.ectance from rough surfaces. Applied Optics 37, 1 (1998), 130 139. [25] WAGNER, R. J. Shadowing 
of randomly rough surfaces. Journal of Acoustic Society of America 41 (1967), 138 147. [26] WARD, G. 
J. Measuring and modeling anisotropic re.ection. Computer Graph­ics 26, 4 (July 1992), 265 272. ACM Siggraph 
92 Conference Proceedings. [27] WESTIN,S. H., ARVO,J. R., AND TORRANCE, K. E. Predicting re.ectance functions 
from complex surfaces. Computer Graphics 26, 2 (July 1992), 255 264. ACM Siggraph 92 Conference Proceedings. 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344824</article_id>
		<sort_key>75</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Monte Carlo evaluation of non-linear scattering equations for subsurface reflection]]></title>
		<page_from>75</page_from>
		<page_to>84</page_to>
		<doi_number>10.1145/344779.344824</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344824</url>
		<abstract>
			<par><![CDATA[<p>We describe a new mathematical framework for solving a wide variety of rendering problems based on a non-linear integral scattering equation. This framework treats the scattering functions of complex aggregate objects as first-class rendering primitives; these scattering functions accurately account for all scattering events inside them. We also describe new techniques for computing scattering functions from the composition of scattering objects. We demonstrate that solution techniques based on this new approach can be more efficient than previous techniques based on radiance transport and the equation of transfer and we apply these techniques to a number of problems in rendering scattering from complex surfaces.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Chandrasehkar's equation]]></kw>
			<kw><![CDATA[Monte Carlo techniques]]></kw>
			<kw><![CDATA[adding equations]]></kw>
			<kw><![CDATA[equation of transfer]]></kw>
			<kw><![CDATA[illumination]]></kw>
			<kw><![CDATA[invariant imbedding]]></kw>
			<kw><![CDATA[principles of invariance]]></kw>
			<kw><![CDATA[reflectance and shading models]]></kw>
			<kw><![CDATA[rendering]]></kw>
			<kw><![CDATA[scattering function]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.3</cat_node>
				<descriptor>Probabilistic algorithms (including Monte Carlo)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Monte Carlo</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010342</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003671</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic algorithms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003670.10003682</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic reasoning algorithms->Sequential Monte Carlo methods</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003670.10003677</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic reasoning algorithms->Markov-chain Monte Carlo methods</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010342.10010344</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis->Model verification and validation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003670.10003677</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic reasoning algorithms->Markov-chain Monte Carlo methods</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003670.10003682</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic reasoning algorithms->Sequential Monte Carlo methods</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Reliability</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31090209</person_id>
				<author_profile_id><![CDATA[81100088896]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Matt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pharr]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15033698</person_id>
				<author_profile_id><![CDATA[81100482576]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pat]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hanrahan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>97886</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[James Arvo and David Kirk, Particle transport and image synthesis, Computer Graphics 24 (1990), no. 4, 63-66.]]></ref_text>
				<ref_id>AK90</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[V.A. Ambarzumian, A new method for computing light scattering in turbid media, Izv. Akad. Nauk SSSR 3 (1942).]]></ref_text>
				<ref_id>Amb42</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[V.A. Ambarzumian (ed.), Theoretical astrophysics, Pergamon Press, New York, New York, 1958.]]></ref_text>
				<ref_id>Amb58</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Richard Bellman and Robert Kalaba, On the principle of invairant imbedding and propagation through inhomoheneous media, Proceedings of the National Academy of Sciences 42 (1956), 629-632.]]></ref_text>
				<ref_id>BK56</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Richard E. Bellman, Robert E. Kalaba, and Marcia C. Prestrud, Invariant imbedding and radiative transfer in slabs of finite thickness, American Elsevier Publishing Company, New York, 1963.]]></ref_text>
				<ref_id>BKP63</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801255</ref_obj_id>
				<ref_obj_pid>965145</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[James E Blinn, Light reflection functions for simulation of clouds and dusty surfaces, Computer Graphics 16 (1982), no. 3, 21-29.]]></ref_text>
				<ref_id>Bli82</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Richard E. Bellman and G. M. Wing, An introduction to invariant imbedding, John Wiley &amp; Sons, New York, 1975.]]></ref_text>
				<ref_id>BW75</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[S. Chandrasekar, Radiative transfer, Dover Publications, New York, 1960, Originally published by Oxford University Press, 1950.]]></ref_text>
				<ref_id>Cha60</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311560</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Julie Dorsey, Alan Edelman, Justin Legakis, Henrik Wann Jensen, and Hans K~hling Pedersen, Modeling and rendering of weathered stone, Proceedings of SIGGRAPH 99 (August 1999), 225-234.]]></ref_text>
				<ref_id>DEL+99</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[R.M. Goody and Y. L. Yung, Atmospheric radiation, Oxford University Press, 1989.]]></ref_text>
				<ref_id>GY89</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[L.G. Henyey and J. L. Greenstein, Diffuse radiation in the galaxy, Astrophysical Journal 93 (1941), 70-83.]]></ref_text>
				<ref_id>HG41</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166139</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Pat Hanrahan and Wolfgang Krueger, Reflection from layered surfaces due to subsurface scattering, Computer Graphics Proceedings, August 1993, pp. 165-174.]]></ref_text>
				<ref_id>HK93</ref_id>
			</ref>
			<ref>
				<ref_obj_id>146452</ref_obj_id>
				<ref_obj_pid>146443</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Chet S. Haase and Gary W. Meyer, Modeling pigmented materials for realistic image synthesis, ACM Transactions on Graphics 11 (1992), no. 4, 305-335.]]></ref_text>
				<ref_id>HM92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617946</ref_obj_id>
				<ref_obj_pid>616034</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Siu-Chi Hsu and Tien-Tsin Wong, Simulating dust accumulation, IEEE Computer Graphics and Applications 15 (1995), no. 1, 18-25.]]></ref_text>
				<ref_id>HW95</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325167</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[James T. Kajiya, Anisotropic reflection models, Computer Graphics (SIGGRAPH '85 Proceedings), vol. 19, July 1985, pp. 15-21.]]></ref_text>
				<ref_id>Kaj85</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15902</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[James T. Kajiya, The rendering equation, Computer Graphics 20 (1986), no. 4, 143-150.]]></ref_text>
				<ref_id>Kaj86</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74361</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[James T. Kajiya and Timothy L. Kay, Rendering fur with three dimensional textures, Computer Graphics 23 (1989), no. 3,271-280.]]></ref_text>
				<ref_id>KK89</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[P. Kubelka and E Munk, Ein Be#rag zur Optik der Farbanstriche, Z. Tech. Physik. 12 (1931), 593.]]></ref_text>
				<ref_id>KM31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>7050</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Malvin H. Kalos and Paula A. Whitlock, Monte Carlo methods: Volume I: Basics, John Wiley &amp; Sons, New York, 1986.]]></ref_text>
				<ref_id>KW86</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Eric Lafortune, Mathematical models and Monte Carlo algorithms for physically based rendering, Ph.D. thesis, Katholieke Universiteit Leuven, February 1996.]]></ref_text>
				<ref_id>Laf96</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Eric Lafortune and Yves Willems, A theoretical framework for physically based rendering, Computer Graphics Forum 13 (1994), no. 2, 97-107.]]></ref_text>
				<ref_id>LW94</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Gavin Miller and Marc Mondesir, Rendering hyper-sprites in real time, Eurographics Rendering Workshop 1998 (1998), 193-198.]]></ref_text>
				<ref_id>MM98</ref_id>
			</ref>
			<ref>
				<ref_obj_id>731968</ref_obj_id>
				<ref_obj_pid>647651</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Nelson Max, Curtis Mobley, Brett Keating, and En-Hua Wu, Plane-parallel radiance transport for global illumination in vegetation, Eurographics Rendering Workshop 1997, Eurographics, Springer Wien, June 1997, pp. 239-250.]]></ref_text>
				<ref_id>MMKW97</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Curtis D. Mobley, Light and water: Radiative transfer in natural waters, Academic Press, 1994.]]></ref_text>
				<ref_id>Mob94</ref_id>
			</ref>
			<ref>
				<ref_obj_id>614392</ref_obj_id>
				<ref_obj_pid>614269</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Fabrice Neyret, Modeling, animating, and rendering complex scenes using volumetric textures, IEEE Transactions on Visualization and Computer Graphics 4 (1998), no. 1.]]></ref_text>
				<ref_id>Ney98</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Fred E. Nicodemus, J. C. Richmond, J. J. Hisa, I. W. Ginsberg, and T. Limperis, Geometrical considerations and nomenclature for reflectance, Monograph number 160, National Bureau of Standards, Washington DC, 1977.]]></ref_text>
				<ref_id>NRH+77</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[H.H. Natsuyama, S. Ueno, and A. P. Wang, Terrestial radiative transfer, Springer-Verlag, Hong Kong, 1998.]]></ref_text>
				<ref_id>NUW98</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325247</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Ken Perlin, An image synthesizer, Computer Graphics (SIGGRAPH '85 Proceedings), vol. 19, July 1985, pp. 287- 296.]]></ref_text>
				<ref_id>Per85</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Glenn H. Peebles and Milton S. Plesset, Transmission of gamma-rays through large thicknesses of heavy materials, Physical Review 81 (1951), no. 3,430-439.]]></ref_text>
				<ref_id>PP51</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Rudolph W. Preisendorfer, Invariant imbedding relation for the principles of invariance, Proceedings of the National Academy of Sciences 44 (1958), 320-323.]]></ref_text>
				<ref_id>Pre58</ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Rudolph W. Preisendorfer, Radiative transfer on discrete spaces, Pergamon Press, Oxford, 1965.]]></ref_text>
				<ref_id>Pre65</ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[R.W. Preisendorfer, Hydrologic optics, U.S. Department of Commerce, National Oceanic and Atmospheric Administration, Honolulu, Hawaii, 1976, Six volumes.]]></ref_text>
				<ref_id>Pre76</ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Matt Pharr and Eric Veach, Shading with closures, In preparation, 2000.]]></ref_text>
				<ref_id>PV00</ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Holly Rushmeier, Charles Patterson, and Aravindan Veerasamy, Geometric simplification for indirect illumination calculations, Proceedings of Graphics Interface '93, May 1993, pp. 227-236.]]></ref_text>
				<ref_id>RPV93</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218434</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Francois Sillion and George Drettakis, Feature-based control of visibility error: A multi-resolution clustering algorithm for global illumination, SIGGRAPH 95 Conference Proceedings, Addison Wesley, August 1995, pp. 145-152.]]></ref_text>
				<ref_id>SD95</ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Francois Sillion, G. Drettakis, and Cyril Soler, A clustering algorithm for radiance calculation in general environments, Eurographics Rendering Workshop 1995, Eurographics, June 1995.]]></ref_text>
				<ref_id>SDS95</ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[George Stokes, On the intensity of the light reflected from or transmitted through a pile of plates, Proceedings of the Royal Society (1862), Reprinted in Mathematical and Physical Papers of Sir George Stokes, Volume IV, Cambridge, 1904.]]></ref_text>
				<ref_id>Sto62</ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[S. Twomey, H. Jacobowitz, and H. B. Howell, Matrix methods for multiple-scattering problems, Journal of the Atmospheric Sciences 23 (1966), 289-296.]]></ref_text>
				<ref_id>TJH66</ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[K.E. Torrance and E. M. Sparrow, Theory for off-specular reflection from roughened surfaces, Journal of the Optical Society of America 57 (1967), no. 9.]]></ref_text>
				<ref_id>TS67</ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Hendrik Christoffel van de Hulst, Multiple light scattering, Academic Press, New York, 1980, Two volumes.]]></ref_text>
				<ref_id>van80</ref_id>
			</ref>
			<ref>
				<ref_obj_id>927297</ref_obj_id>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[Eric Veach, Robust Monte Carlo methods for light transport simulation, Ph.D. thesis, Stanford University, December 1997.]]></ref_text>
				<ref_id>Vea97</ref_id>
			</ref>
			<ref>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[Eric Veach and Leonidas Guibas, Bidirectional estimators for light transport, Fifth Eurographics Workshop on Rendering (Darmstadt, Germany), June 1994, pp. 147-162.]]></ref_text>
				<ref_id>VG94</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218498</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[Eric Veach and Leonidas J. Guibas, Optimally combining sampling techniques for Monte Carlo rendering, Computer Graphics Proceedings, August 1995, pp. 419-428.]]></ref_text>
				<ref_id>VG95</ref_id>
			</ref>
			<ref>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[Alan E Wang, Basic equations of three-dimensional radiative transfer, Journal of Mathematical Physics 31 (1990), no. 1,175-181.]]></ref_text>
				<ref_id>Wan90</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134075</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[Stephen Westin, James Arvo, and Kenneth Torrance, Predicting reflectance functions from complex surfaces, Computer Graphics 26 (1992), no. 2, 255-264.]]></ref_text>
				<ref_id>WAT92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311558</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[Douglas E. Zongker, Dawn M. Werner, Brian Curless, and David H. Salesin, Environment matting and compositing, SIGGRAPH 99 Conference Proceedings, Addison Wesley, August 1999, pp. 205-214.]]></ref_text>
				<ref_id>ZWCS99</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Monte Carlo Evaluation Of Non-Linear Scattering Equations For Subsurface Re.ection Matt Pharr Pat Hanrahan 
Stanford University Abstract We describe a new mathematical framework for solving a wide vari­ety of 
rendering problems based on a non-linear integral scattering equation. This framework treats the scattering 
functions of com­plex aggregate objects as .rst-class rendering primitives; these scat­tering functions 
accurately account for all scattering events inside them. We also describe new techniques for computing 
scattering functions from the composition of scattering objects. We demon­strate that solution techniques 
based on this new approach can be more ef.cient than previous techniques based on radiance trans­port 
and the equation of transfer and we apply these techniques to a number of problems in rendering scattering 
from complex surfaces. CR Categories: I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism 
Color, shading, shadowing, and texture; I.3.3 [Computer Graphics]: Picture/Image Generation Keywords: 
Rendering, Illumination, Monte Carlo Techniques, Re.ectance and Shading Models, Scattering Function, 
Invariant Imbedding, Principles of Invariance, Equation of Transfer, Adding Equations, Chandrasehkar 
s Equation  1 Introduction In this paper we describe a new framework for solving a broad class of rendering 
problems. It consists of a non-linear integral scattering equation that describes the overall scattering 
behavior of an object or volume accounting for all of the scattering events that happen inside of it, 
and a set of adding equations that describe aggregate scattering functions from the composition of objects 
with known scattering functions.1 In some situations, techniques based on these equations can be much 
more ef.cient than corresponding techniques based on the equation of transfer [Cha60] (i.e. the ren­dering 
equation [Kaj86]). Unlike the equation of transfer, these equations describe the scat­tering from an 
object directly; thus they re.ect a shift in focus from energy transport to scattering behavior independent 
of a particular illumination setting. This approach has been developed over the 1We will use the term 
scattering function to describe the generic light scattering behavior of a surface or object, and we 
will use the term scatter­ing equation to describe our Equation 3.5, which is equal to the scattering 
function in the general three-dimensional case. past .fty years, primarily in astrophysics, where these 
equations are often used to compute light scattering [Amb42, Cha60, van80]. In this paper, we will focus 
on their application to computing sub­surface scattering. The resulting scattering functions generalize 
the concept of the bidirectional re.ectance distribution function (BRDF). The BRDF is based on the simplifying 
assumption that light exits the sur­face at the same point it enters, or equivalently that the surface 
is homogeneous and uniformly illuminated over a reasonably large area [NRH77]. This reduces the re.ection 
function to a four­dimensional function over pairs of angles. BRDFs are the one­dimensional case since 
the surface may be inhomogeneous in the z dimension (the direction along the surface normal), but not 
in x and y. More generally, Nicodemus et al. have introduced the bidi­rectional subsurface re.ectance 
distribution function (BSSRDF) which accounts for light entering the surface at a different place than 
it exits [NRH77]. This is a three-dimensional scattering func­tion that accounts for inhomogeneity in 
all dimensions underneath the surface; it gives re.ectance along an outgoing ray due to illu­mination 
along an incoming ray. A minor generalization lifts the restriction to a planar surface, giving a scattering 
function of ten di­mensions: .ve to specify the origin and direction of each ray. This setting lets us 
treat mathematically the scattering functions of gen­eral three-dimensional objects. The BRDF and BSSRDF 
are both important abstractions in that they describe scattering behavior phe­nomenologically such that 
lower-level scattering processes can be ignored. Max et al. were the .rst graphics researchers to recognize 
the importance of the scattering equations [MMKW97]. They used the one-dimensional scattering equation 
to compute light scatter­ing in tree canopies by deriving a system of ordinary differential equations. 
They solved this system with an adaptive Runge-Kutta method, using a clever application of the Fourier 
transform to avoid 3 an Onmatrix multiplication. Because their solution technique discretizes the hemisphere 
into sets of angles, it becomes increas­ingly expensive for strongly peaked phase functions. More impor­tantly, 
the viability of the extension of this solution method to 3D scattering problems has not been demonstrated. 
We develop the scattering equation in a more general setting that makes it possi­ble to derive both one 
and three-dimensional scattering equations. We also derive and use the adding equations and apply a more 
gen­eral solution technique, Monte Carlo integration, that scales to the three-dimensional setting. In 
this paper, we discuss the history of these scattering equations and previous work in graphics that has 
used different techniques to compute generalized scattering functions. We derive the scattering equations 
in integral form in a very general setting and describe the derivation of the adding equations. Given 
this basis, we describe the use of Monte Carlo techniques to compute solutions to these equations, demonstrate 
their ef.ciency, and apply them to a number of problems in rendering light re.ection from complex surfaces. 
We conclude with discussion and directions for future work. 2 Background and previous work 2.1 History 
of the scattering and adding equations In a classic paper from the nineteenth century, Stokes derived 
ex­pressions for the amount of light re.ected and transmitted from a stack of glass layers [Sto62]. He 
introduced the innovations that overall scattering could be computed directly in terms of the re.ec­tion 
and transmission functions of the individual layers, and that the re.ection and transmission for two 
layers together could be com­puted based on the already-computed re.ection and transmission functions 
of each one. This work was the intellectual basis for the development of general scattering equations 
over the last .fty years. The scattering and adding equations were .rst derived as a new way to compute 
1D scattering functions without using the equation of transfer. The .rst applications were to the standard 
problem in astrophysics: given a slab of thickness z with known optical prop­erties that do not vary 
in x or y and assuming that parallel beams of radiation are incident from a direction O', we wish to 
know how much radiation is re.ected in the direction O (Figure 1). Building on the ideas that Stokes 
developed, Ambarzumian de­rived a non-linear integral equation that describes scattering from semi-in.nite 
homogeneous isotropic atmospheres directly in terms of the low-level scattering properties of the layers 
of the atmo­sphere [Amb42, Amb58].2 Chandrasekhar greatly extended Am­barzumian s results and derived 
a non-linear integro-differential scattering equation that describes scattering from .nite anisotropic 
atmospheres [Cha60]. Bellman and Kalaba extended this work to include inhomogeneity in depth and were 
the .rst to derive the purely integral form of this equation for theoretical analysis of solutions to 
the scattering equation [BK56, BKP63]. These one­dimensional scattering equations have been applied to 
a variety of other areas, including neutron transport, radiative transfer, and hy­drologic optics [Mob94]. 
Recently, Wang has derived a scattering equation in the three­dimensional case where incident illumination 
from a distant source is constant over the entire upper boundary of the region and where the phase function 
varies only in z [Wan90] [NUW98, Section 4.6]. Unfortunately, this form is not generally useful for problems 
en­countered in graphics. The adding equations were developed by van De Hulst and Twomey et al. in the 
1960s [van80, TJH66], and were later gen­eralized by Preisendorfer [Pre76]. They were .rst discovered 
in the .eld of neutron transport by Peebles and Plesset [PP51] and have since been applied to a wide 
variety of scattering problems. 2.2 One-dimensional scattering functions Computing scattering functions 
that hide the complexity of light scattering from surfaces has long been a research problem in graph­ics 
and optics. Examples include the Torrance Sparrow re.ection model [TS67], an analytic approximation to 
light scattering from rough surfaces; Blinn s model for dusty surfaces, which uses a single-scattering 
approximation [Bli82]; Kajiya s discussion of re­placing complex geometry with re.ection functions [Kaj85]; 
and Westin et al. s computation of BRDF samples by simulating light scattering from micro-geometry [WAT92]. 
When no closed-form expression or approximation for multiple scattering at a surface is available, previous 
work has either ignored multiple scattering (e.g. [Bli82]), or based solutions on the equation of transfer 
and the de.nition of the BRDF (e.g. [WAT92]), where re.ected radiance in the outgoing direction is computed 
given dif­ferential irradiance from the incident direction. 2Homogeneity refers to whether or not the 
atmosphere has scattering properties that vary as a function of depth, and isotropy refers to the proper­ties 
of the phase function inside the atmosphere; an isotropic phase function scatters light equally in all 
directions. Figure 1: Basic viewing geometry for the 1D (left) and 3D (right) scattering functions. 
All vectors and rays are speci.ed in the outgoing direction. 2.3 Three-dimensional scattering functions 
In recent years, a number of researchers have worked on comput­ing scattering functions that describe 
the aggregate scattering be­havior of complex volumetric and geometric objects. Kajiya and Kay s volume 
texels were an early example [KK89], and Neyret extended their framework to include more general geometries 
and demonstrated applications to reducing aliasing due to level-of­detail changes [Ney98]. Rushmeier 
et al. approximated scatter­ing from clusters of geometry by averaging the re.ectance of sur­faces hit 
by random rays [RPV93]. Sillion and Drettakis approx­imated occlusion due to complex objects as volume 
attenuation functions [SD95] and Sillion et al. approximated aggregate scatter­ing functions from clusters 
of objects [SDS95]. However, none of these approaches accounts for multiple scattering inside the object 
or for light that enters the object at a different point than it exits. Miller and Mondesir computed 
hypersprites that encoded spec­ular re.ection and refraction from objects [MM98], and Zongker et al. 
have described an apparatus for computing the scattering and transmission functions of glossy and specular 
real-world ob­jects [ZWCS99]. Dorsey et al. have rendered rich images of stone and marble by computing 
BSSRDFs at rendering time [DEL 99]. Their solutions are based on the equation of transfer and photon 
mapping to accelerate multiple scattering computations, and they clearly showed the importance of this 
effect for some materials. This is the only previous application in graphics of rendering scat­tering 
from surfaces with BSSRDFs. In general, scattering from an object can be described by the for­mal solution 
of the inverse of the light transport equation [Pre65, Section 22]. Veach and Guibas derived rendering 
algorithms based on recursive expansion of this solution operator [VG94, Vea97] and Lafortune used the 
Neumann expansion of the solution oper­ator to derive recursively-de.ned integral equations that describe 
scattering from a collection of surfaces; he called this the global re.ectance distribution function 
(GRDF) and also used it to derive new light transport algorithms [LW94, Laf96]. 2.4 Composing scatterers 
A variety of techniques have previously been used to compute ag­gregate re.ection functions from a set 
of layers. The Kubelka-Munk model [KM31] is similar to a one-dimensional radiosity solu­tion; it accounts 
for multiple scattering but not angular dependence. It was .rst introduced to graphics by Haase and Meyer 
[HM92] and has been widely used. However, due to assumptions built into the model, either glossy specular 
re.ection has to be ignored or multi­ple re.ection between the specular component and the added layer 
is lost. A different approach to layer composition is due to Hanra­han and Krueger [HK93]; they compose 
scattering layers consid­ering only one level of inter-re.ection. This misses the effect of multiple 
internal re.ections before light leaves the layer, which is important except for objects with very low 
albedos. x Generic point O Generic direction r A ray through space, with origin x r and direction O r 
µr Cosine of ray s direction with surface normal 8 x Delta function: Kronecker or Dirac, depending on 
context S2 The sphere of all directions . The hemisphere around thez direction M 2 A 2D manifold R Ray 
space: a set of rays going through a set of locations in a set of directions L r' ' (( Radiance along 
the ray r p xOO Phase function at a point. k rr',( Scattering kernel ' S rr Scattering function for light 
re.ected along ray r due to incident light along ray r pa x Volume absorption coef.cient at x ps x Volume 
scattering coef.cient t pt x Volume attenuation coef.cient, ps xpa x . x Albedo ps xpt x z ( ( Depth 
in one-dimensional medium R zOi Oo Re.ection function from slab of depth z T zOi Oo Transmission function 
from slab of depth z Figure 2: Table of notation.  3 Scattering Equations In this section, we derive 
the integral scattering equation that de­scribes how an object or volume scatters light. We also describe 
techniques for computing the scattering functions of composite ob­jects directly from the scattering 
functions of their constituent parts. Our treatment is in terms of the scattering of a single wavelength 
of light; the extension to multiple wavelengths is straightforward. We will consider scattering from 
objects in an axis-aligned rect­angular region of space with height z. This does not require that the 
object be parallelepiped-shaped; it is just a convenient param­eterization of space. This parameterization 
also makes it possible to ignore the issue of non-convex regions of space, where illumina­tion may exit 
and later re-enter the space. That setting is tractable, though the notation is more complex. 3.1 Ray 
space and operator notation Previous work in graphics has used a variety of parameterizations of surfaces 
and directions for the expression of the rendering equa­tion (e.g. Kajiya used an integral over pairs 
of points on surfaces). Veach has recently introduced abstractions based on ray space that have a number 
of advantages: in addition to simplifying and clar­ifying formulas, ray space makes clear that any particular 
parame­terization of surfaces and directions is an arbitrary choice, mathe­matically equivalent to any 
other [VG95, Vea97]. In this setting, ray space R is the set of rays given by the Carte­ 3 sian product 
of points in three-space pand all directions S2: R hIp3 dS2 . We will de.ne two specializations of R 
. First is RM 2 , which is the subset of R where all rays start on a given two­dimensional manifold M 
2: RM 2 hM 2 dS2. A particular instance of RM 2 that is often useful is RM 2!zt{, where the manifold 
is the plane at zhz'. Another useful specialization is to limit the direc­tions of rays R to the hemisphere 
around the surface normal; we denote this by RM 2 . The negation of a ray r is de.ned as the ray with 
the same origin as r but going in the opposite direction. The scattering kernel k describes light scattering 
at a point. In ray space it is k r'(r h8 xr xr' ps xr p xr O r'(O r where p xr O r'(O r is the phase 
function at the point xr for scattering from O r'to O r and we have included the scat­tering coef.cient 
ps in k in order to simplify subsequent formulas (see Chandrasekhar for a summary of phase functions, 
scattering coef.cients, etc. [Cha60]). In contrast to the phase function, the scattering function S r'(r 
is potentially non-zero for any pair of rays because of multiple scattering; it is not necessary that 
the rays meet at a point for light along one ray to affect the response along another. Though the general 
scattering function is ten-dimensional, when we are considering scattering from a speci.c object, it 
is often more con­venient to consider the eight-dimensional specialization where all rays originate on 
a parameterized two-dimensional manifold that bounds it. For the remainder of this paper, this is the 
only type of scattering function we will consider. In particular, we will just con­sider the scattering 
function from rays on a planar boundary of an object. In order to be able to do integrals over RM 2 and 
RM 2 !z{, we de.ne a differential measure: dr hdOO r dA-xr hµr dOO r dA xr where xr is the origin of 
r, O r is its direction, A is the area measure on RM 2 , and dO is the differential solid angle measure. 
Given an object s scattering function, outgoing radiance along a ray r is computed by integrating its 
product with incident radiance over the object s boundary. Lo r h1 S r'(tr Li r'dr' 4 RM 2 µr µr This 
is the three-dimensional analogue to integrating the product of incident radiance and the BRDF at a point 
to compute outgoing ra­diance. Its added complexity stems from the fact that incident light scatters 
inside the object and may exit far from where it entered. We will de.ne operators k and S, where bold 
text signi.es the operator and Roman text its kernel. Both operators are de.ned such that applying them 
to other functions gives: S fr h S r'(r fr'dr' R µr µrt M 2 We can de.ne compositions like kS, or SaSbSc, 
etc. These will be useful in computing new scattering functions that describe the scattering of multiple 
objects in terms of their individual scattering functions (see Section 3.3). r'(r hSn r'(r1 S1Sn RM 
2RM 2 S1 rn 1 (r drn 1 dr1 (3.1) µ2 µ2 rn 1r1 3.2 Derivation of the scattering equation With operator 
notation in hand, we will derive a general integro­differential scattering equation in ray space. This 
equation de­scribes how the scattering function of a complex object changes as layers with known scattering 
properties are added or removed from it. It can either be solved in integro-differential form or as a 
purely integral equation. Our derivation follows the invariant imbedding method [BK56, Pre58, BKP63, 
BW75]. We will consider the change in scattering behavior of this object as thin layers &#38;z are added 
on top of it. Because multiple scattering S kkS Sk SkS .z z  Figure 3: The .ve types of scattering events 
to be considered in the invariant imbedding derivation of the scattering equa­tion. The S events re.ect 
the aggregate multiple scattering inside the z slab. All other scattering events, such as kSk, are gathered 
in an o &#38;z2 term in Equation 3.2. in &#38;z occurs with probability o &#38;z2 , we just gather all 
multiple scattering in an o &#38;z2 term. Later we will divide by &#38;z and take 2 the limit as &#38;z 
(0, at which point all of the o &#38;zterms disappear. As such, there are only .ve types of scattering 
events that need to be accounted for (see Figure 3): 1. S: Light that is attenuated in &#38;z, scattered 
by the original ob­ject, and attenuated again in &#38;z. 2. k: Light that is scattered in &#38;z so 
that it leaves the new layer without reaching the original object. 3. kS: Light that is scattered in 
&#38;z so that it passes into the origi­nal object, is scattered, and then is attenuated in &#38;z as 
it exits. 4. Sk: Light that is attenuated in &#38;z, scattered by the object, and is then scattered 
by &#38;z such that it leaves the object. 5. SkS: Light that is attenuated in &#38;z, scattered by the 
object, scattered in &#38;z back into the object, scattered again by the object, and then attenuated 
again.  Accounting for each of the .ve modes of scattering in turn, a new scattering function for z 
&#38;z can be written S z &#38;z he ct !.z µi {e ct !.z µo{)lS z We now need a boundary condition in 
order to convert this non­linear integro-differential equation into an integral equation. If we assume 
that the object is bounded by a perfect absorber from be­low i.e. S0 h0 then application of the Laplace 
transform gives Equation 3.4. General boundary conditions are most easily handled with the adding equations; 
see the next section. S z hz e ct !1 µi 1 µo {!zzt{+ k z'k z'S z' 0 S z'k z'S z'k z'S z'++dz'(3.4) We 
have written this with the operators expanded out; see Fig­ure 4. This is a formidable equation, but 
like the rendering equation, it expresses a simple fact about light scattering. With computers and numerical 
methods, it can be solved. We will discuss previous solu­tion techniques and some new Monte Carlo approaches 
for solving it in Section 4. 3.3 Adding equations An important advantage of treating scattering functions 
directly is that it is possible to compute the combined scattering functions of aggregate objects from 
their individual scattering functions. These new scattering functions can be written directly in terms 
of the orig­inal ones and account for all scattering between the objects. Consider two non-overlapping 
objects a and b with scattering functions Sa and Sb. The scattering functions of the two objects to­gether 
can be derived by considering all of the possible interactions between them. For example, consider the 
new scattering function for a pair of rays ra and r', both of which originate on a s bound­ aary. Light 
may enter at a, be scattered by Sa, and then exit without interacting with b. This is the .rst term of 
Equation 3.6a. Or, it may be scattered in a so that it enters b, get scattered by b back into a, and 
then be scattered through a out to ra; this gives the next term. By considering all such inter-re.ections 
between a and b, we have the .rst adding equation [Pre65, Section 25]. Sa SbSa (3.6a) Sala hSaSaSbSaSbSa 
h . nL0  ISaSb S zz &#38;z S zz &#38;z S z S z S zz &#38;z S z S zz 2 &#38;z S zo &#38;z ) (3.2) SaSb 
nSa (3.6b) hh 1Sa SbSal (3.6c) (3.6d) a  Sa Sa where S z is the scattering operator for a slab of thickness 
z and S ab is the scattering operator for the portion of the slab from Given a ray ra that enters a and 
another ray rb that exits from b, we depth a to b. can derive a similar equation: c.z We can simplify 
this further by replacing the e term with 1 c&#38;zo &#38;z2 and taking advantage of an approximation 
to SSalb hSbSa SbSaSbSa (3.7a) h for a layer that is in.nitesimally thin [GY89]: S zz &#38;z h . nL0 
 Sb ISaSb 1Sa (3.7c) S SbSa SbSaal Sb SaSb nSa (3.7b) 2 hh k&#38;zo &#38;z Making these simpli.cations 
and then taking the difference be­tween the new scattering operator S z &#38;z and the scattering op­ 
(3.7d) b  erator of the original layer S z , we have These equations are most easily understood by reading 
each term 1 1 from right to left to see the order of scattering events. S z &#38;z S z ho &#38;z2 lpt 
S z Computing new scattering functions with the adding equations µi µo + can be done much more ef.ciently 
than by recomputing the scat­ k zz &#38;z k zz &#38;z S z S z k zz &#38;z tering functions of the aggregate 
object from scratch [van80]. This stems from the fact that Sa and Sb already incorporate all of the +) 
 S z k zz &#38;z S z&#38;z multiple scattering events inside a and b, so we need only to com­pute the 
effect of multiple scattering between the two objects. After Now we divide by &#38;z and take the limit 
as &#38;z (0, which gives a few terms, the series usually converges quickly, as long as not us the in.nitesimal 
change in the scattering function due to the ad-too much of the light is re-scattered at each step. Analysis 
based dition of the new layer. on the operator norm of each term could be used to describe the convergence 
more precisely. Since the results of this computation dS h=pt l11 )+ + S kkS Sk SkS(3.3) are new scattering 
functions, they can themselves be used in further dzµi µo computations of new scattering functions. 
 z S zri (ro he !ct !xi { µi ct !xo { µo {!zzt{lk ri z'(ro z' 1k ro (r'S z'ri (r' dr' 0 4 Rµ2 M 2 zt 
rt 1S z'r'(ro k ri (r' dr'1S z'r''(ro k r'(r''S z'ri (r' dr'dr'')dz'(3.5) 22 22 4 Rµ16 RRµµ M 2 zt rtM 
2 zt M 2 zt rtrtt Figure 4: The three dimensional integral scattering equation, 3.4, with operators expanded 
out and where the ray r t is a new ray along the same line as r, constructed by offsetting the origin 
by distance t along the z axis and xi hxri zz'and xo hxro zz'. 3.4 One-dimensional setting There are 
useful special cases of the general scattering equation and the adding equations in the one-dimensional 
setting; this was Figure 5: The two re.ection and trans­mission functions of a slab. where they were 
.rst derived. In one dimen­sion, position in x and y is irrelevant, so the delta functions in the integral 
from the phase function disappear, leading to simpler formu­las and easier implementation. A .nite slab 
then has four scattering functions (see Fig­ure 5): given illumination at the top, one gives the amount 
of light re.ected at the top R and another gives the amount of light transmitted at the bottom T [Cha60]. 
The other two, R and T , give re.ection and transmission due to light incident at the bottom. R and T 
are given in Equations 3.8 and 3.9, on the next page. The application of the re.ection and transmission 
operators to a function f is 11 R f O'(O hR O'(O f O'dO' 4 µ n which gives us a nearly familiar equation 
for computing re.ected radiance at a point: 11 Lo O hR O'(O Li O'dO' 4 µ n The re.ection function of 
a surface is thus related to its BRDF fr by R O')(O h4 fr O' (O µ'µ. The adding equations are similarly 
simpli.ed to integrals over just directions. In operator form, the scattering functions of two combined 
slabs a and b are RhRa ab RhRb ab ThTb Ta ab Th ab Ta Tb 3.5 Discussion Ta Rb Ta Ta Rb Ra Rb Ta Tb 
Ra Tb Tb Ra Rb Ra Tb Tb Ra Rb Ta Ta Rb Ra Tb The scattering equations thus bring us to a new framework 
for con­sidering rendering problems. Note that there are no fundamentally new types of rendering problems 
that the scattering equations make accessible: as noted in Section 2.3, the formal inverse of the op­erator 
rendering equation can be used to solve the same kinds of scattering problems as well. For example, Hanrahan 
and Krueger effectively used a Neumman series expansion of the inverse to es­timate four-dimensional 
scattering functions. This method could be easily extended to higher-dimensional scattering problems, 
and more sophisticated Monte Carlo techniques could be applied. Conversely, the scattering equation can 
be used for more than just pre-computing scattering functions. Given knowledge of par­ticular viewing 
conditions, particular lighting conditions, or both, we can directly compute estimates of integrals such 
as SLe (where Le is emitted radiance), rather than .rst computing S and then pass­ing emitted light through 
it. Since both approaches are based on formulae that directly describe the physics of light scattering, 
it is not surprising that the two approaches are connected in this way. In fact (and reassuringly), the 
scattering equation can be derived directly from the equation of transfer [Pre65]. In the next section, 
we will see that solving the scattering equation involves sampling chains of scattering events through 
a medium and evaluating their contribution precisely how the equa­tion of transfer is typically solved. 
Considered in light of its con­nections with the equation of transfer, we can use the scattering equation 
as a path to insights about how to solve the equation of transfer, and vice versa. This has the potential 
to lead to new ways of considering some classic rendering problems.  4 Monte Carlo Solution A previously 
uninvestigated technique for solving the scattering and adding equations is Monte Carlo integration. 
Monte Carlo is a par­ticularly effective technique for solving high dimensional integrals and integrals 
with discontinuities in the integrand. Its generality makes it possible to compute integrals where the 
functions in the integrand vary almost arbitrarily [KW86]; here, it allows wide va­riety in the possible 
phase functions, scattering and attenuation co­ef.cients, and geometric shapes. Techniques previously 
used to solve the scattering and adding equations have been based on the integro-differential form such 
as Equation 3.3. Typically, the set of angles is discretized and a system of non-linear differential 
equations is solved to compute scattering at the discrete angles (Max et al. took this approach). See 
van de Hulst [van80] for a survey and comparison of many of the varia­tions of these techniques. These 
methods all break down in the face of complexity in the scattering medium: given highly anisotropic phase 
functions or non-homogeneous media, they are either not ap­plicable due to the assumptions made in their 
derivations, or be­come increasingly inef.cient because .ner discretizations are re­quired and the systems 
of equations become large. Furthermore, the generalization of these methods to higher-dimensional settings 
quickly becomes intractable, which has stymied the development of the more general theory. 4.1 Random 
walk solution We will describe a simple recursive solution of the integral scatter­ing equation. Because 
S z in Equation 3.4 is written recursively in terms of integrals of scattering functions of S z', we 
can evaluate an estimate of S z based on a random walk. (The spirit of this al­gorithm is similar to 
Kajiya s path tracing solution to the rendering equation.) We follow a two step process: 1. First we 
sample the integral over depth by choosing z', where 0 z') z. For constant attenuation functions, the 
exponential term can be importance sampled directly: to sample the inte­gral 0 z eaztdz', where a hapt 
1 µi 1 µo , we .rst .nd the z R z Oi (Oo h0 e ct !zt{!1 µi 1 µo {!zzt{ps z'lp z'Oi (Oo 41 n p z'O'(Oo 
R z'Oi (O' dµO'' 41 n R z'O'(Oo p z'Oi (O' dµO'' 16 1 nn R z'O''(Oo p z'O'(O''R z'Oi (O' dOµ''dµO'''')dz'(3.8) 
2 z T z Oi (Oo h8 µi µo e- 0 ct !zt{dztµi e ct !z{!zzt{µi ps z'le ztµo p z'Oi (Oo z 0 e ztµo 41 n p 
z'O'(Oo R z Oi (O' dµO'' 41 n T z'O'(Oo p z'Oi (O' dµO'' 1T z'O''(Oo p z'O'(O''R z'Oi (O' dO'dO'')dz'(3.9) 
2 16 nn µ'µ'' Figure 6: The integral forms of the one-dimensional re.ection and transmission equations. 
Since the medium is assumed to be homoge­neous in x and y and that incident illumination is constant 
over a large area relative to pt , the equations are expressed in terms of depths and a pair of directions, 
rather than all of ray space. Note that an additional term is added to T to account for directly transmitted 
light. probability density function pdf z'heaaz eazt 1 . The cumulative z distribution function Pz is 
0 pdf z'dz'. Given a random number s between 0 and 1, we set s hPz'and solve for z': s eaz log1 1 z'h 
a More generally, if the attenuation term varies with depth, the pdf cannot be computed analytically. 
In this case, we sample an optical thickness and march through the medium until that distance has been 
covered. In either case, the resulting sample is weighted by the exponential term at z'divided by the 
pdf. 2. We compute the product of the weight and estimates of the terms k z'k z'S z'S z'k z'S z'k z'S 
z'. In computing the terms, we will come to have new estimates of the scattering equation S to compute; 
we proceed recursively. This process is most easily understood in the one-dimensional case (Equation 
3.8). Figure 7 gives pseudo-code for evaluating the 1D re.ection function. There is an important difference 
between this process and ran­dom walk solutions of the equation of transfer: as the recursion continues, 
the z'at which we are estimating S is monotonically de­creasing. Once we have chosen a depth at which 
to estimate S, all scattering above z'is irrelevant; it has already been accounted for. In effect, we 
are able to make a single pass through the medium from top to bottom, peeling off layers and solving 
scattering prob­lems for thinner sub-objects. In comparison, standard approaches to solving the equation 
of transfer do not create a progressively sim­pler problem as they proceed.  4.2 Three-dimensional case 
In the 3D case, this sampling process is less straightforward due to the delta function in the ray space 
phase function. Fortunately, delta functions generally .t easily into Monte Carlo sampling schemes. For 
example, given two rays r and r', the k z'term of Equation 3.4 is zero unless both r and r'start at the 
same point and the z'depth sampled in step 1 above matches that point. In general two rays in 3D do not 
meet at all. Therefore, in the process of sampling the integrals, whenever we have a choice of rays to 
sample, some­times we must carefully choose a ray and a depth such that this delta function is non-zero. 
To make this easier, we separate S into two components, Ss, scattering due to a single scattering event, 
and Sm, scattering due to multiple scattering events. This is anal­ogous to distribution ray tracing 
with a mixed pure specular and diffuse surface where the two parts need to be sampled separately. z z 
Ss z hI0 e k dz'and Sm z hI0 e kS Sk SkS dz'. Thus, S hSs Sm. Consider the speci.c case of estimating 
Lo hSLe for a given outgoing ray r and a single point light source. Separating S, we have two integrals, 
SsLe SmLe. The .rst term is easily handled: it just represents single scattering of emitted light in 
the medium, so all scattering events are along r s path through the object. We choose positions for scattering 
events (i.e. x'in Figure 8a) by im­portance sampling points along r as above. Given these points, the 
incoming ray r'follows directly since the light is a point source; for an area light, a point can be 
chosen on the source and r then follows. Moving on to SmLe, and in particular the term, SkLe hSskLe SmkLe 
(treatment of kS is analogous). We .rst randomly sample a point x'on the surface where the k scattering 
event happens, using an exponential distribution centered around xr (Figure 8b). This strategy is based 
on the assumption that the longer the distance light travels under the surface, the more it will be attenuated 
and the less impact it will have. This de.nes a ray r'to the light due to the point light assumption 
(as above, area lights are a straightforward extension). The second single scattering event must be along 
r s path through the medium and must have a direction such that it passes through xr'in order for all 
of the respective delta functions to be non-zero. We therefore chose a depth along r with importance 
sampling. There is more freedom in sampling from the SmkLe term (Fig­ure 8c). We choose a ray r'as above, 
and still must have the k scat­tering event at xr'for the delta function in k to be non-zero. How­ever, 
the direction of r''can be chosen arbitrarily since Sm doesn t have the delta function along the path 
of r through the medium that Ss does. We simply importance sample the phase function based on O r'to 
get the ray direction for the r''. Finally, the SkSLe term is slightly different: we also need to choose 
two rays that meet at a point where the k term will be eval­uated (Figure 8d). We sample the shared ray 
origin from an expo­nential distribution centered around the midpoint between xr and xr'. Given this 
origin that the two new rays share, we again use importance sampling with the phase function to choose 
the two ray directions. 4.3 Solving the adding equations Monte Carlo estimation of the adding equations 
introduces two is­sues: how many terms to evaluate of the in.nite sum of products of scattering functions, 
and how to estimate individual terms of the sum. We solve the .rst problem and compute an unbiased estimate 
of the in.nite sum by probabilistically terminating the series; after Procedure R(z, Oi, Oo ) (zt, pdf 
):LsampleDepth(ct , z) 11 z z Ot scale :Le µi µo / pdf result :Lcs * p(Oi, Oo ) if (not terminate()) 
then (Ot, pdf ):LsampleAngle(p, Oo ) result :Lresult + cs * p(-Ot, Oo ) * R(zt, Oi, Ot)/ (cos Ot* pdf 
) (Ott, pdf ):LsampleAngle(p, Oi ) result :Lresult + R(zt, Ott, Oo )* cs * p(Oi,-Ott)/ (cos Ot* pdf ) 
result :Lresult + R(zt, Ot, Oo )* cs * p(-Ott,-Ot)* R(zt, Oi, Ott) / (cos Ot* cosOtt* pdf * pdf ) endif 
return result * scale Figure 7: Pseudo-code for evaluation of the one-dimensional re.ection equation. 
The phase function p , ps, and pt are all potentially varying with depth. The terminate function prob­abilistically 
stops the recursion using Russian roulette based on the weighted contribution that this estimate of R 
will make to the .nal solution. The sampleAngle function uses im­portance sampling to choose an outgoing 
angle based on the phase function and the incoming angle; it returns the new di­rection and its probability 
density. computing estimates of the .rst few terms, we terminate with some probability after each successive 
term. When we continue on, sub­sequent terms until we do terminate are multiplied by a correction factor 
so that the .nal result is unbiased [AK90]. Given a particular term of the form of Equation 3.1, we have 
the multiple integral represented by the composition of a set of scatter­ing functions to estimate. Given 
an incident and a re.ected ray, we need to sample a set of rays ri that connect the two of them together. 
These can be sampled in any order the key is to .nd chains of rays where the scattering functions make 
a large contribution; this is the same problem faced in light transport problems [Vea97]. We have implemented 
routines that solve the 1D adding equa­tions. We have implemented them in a modular fashion: they are 
given abstract data types describing the top and bottom layers as well as the incoming and outgoing directions. 
The layer objects provide a small number of operations to the adding routines. They are: evaluation given 
two angles, importance sampling one direc­tion given the other (for layers where distributions for importance 
sampling are not easily computed, a default implementation uni­formly samples the hemisphere), returning 
the probability density function of sampling one direction given the other direction (this is useful 
for multiple importance sampling), and a boolean function which tells if its scattering functions are 
delta functions (see be­low). These operations make it possible to implement a variety of representations 
for layers and easily add them together.3 Delta functions may be present in this series due to direct 
trans­mission (Equation 3.9) as well due to layers that specularly re.ect or refract light (e.g. a mirror 
re.ector at the bottom, or a Fresnel layer at the top). These are tricky because the delta functions 
can­not be evaluated, but only sampled the evaluation routines always return zero. However, when such 
a layer samples a new direction given an incident or outgoing direction, it can pick the appropri­ate 
scattered direction. For example, when computing the term T R T when the top layer is a Fresnel re.ector, 
we compute both the incident and outgoing directions to the bottom layer by 3See Pharr and Veach for 
applications of similar abstractions to combin­ing procedural shading with physically based rendering 
[PV00]. Figure 8: Sampling rays r'in the 3D case; hilighted regions denote terms that are free to be 
sampled. From left to right: for direct lighting, all scattering events are along r s path through the 
medium; for Ss, we sample a distance along r to .nd a scattering event this gives a ray that connects 
through the point x'on the boundary; for Sm, we have more freedom to sample the direction of r''and can 
use a variety of importance functions; .nally, for SkS, we sample a point for the scatter­ing event k 
and then sample the two outgoing directions. sampling T and T given the outgoing and incident directions 
at the top, respectively; the re.ection function R has no choice in sampling its incident and outgoing 
directions, as it would never be able to randomly .nd an outgoing direction that is transmitted into 
the .nal outgoing direction.  5 Results In this section, we start by demonstrating the ef.ciency of 
the scat­tering equations for solving rendering problems. We then demon­strate the use of the 1D scattering 
and adding equations to render­ing complex surfaces and show applications of the 3D scattering equation 
to accurate rendering of surfaces, accounting for light that enters the surface some distance from where 
it exits. 5.1 Accuracy and ef.ciency We tested our implementation s accuracy against a model that cor­responds 
to the standard problem in astrophysics. This model is speci.ed by the atmosphere s optical thickness, 
albedo, and phase function. The resulting scattering functions have been computed and tabularized by 
many authors. We compared our results to ta­bles from Bellman et al. [BKP63], which have results computed 
by using Gaussian quadrature to generate a system of differential equations which were then solved via 
the Runge-Kutta method. For a set of roughly forty randomly-selected albedos, thick­nesses, and pairs 
of angles, we found excellent agreement with the scattering function values our routines computed. We 
have also ver­i.ed our implementation of the adding equations by comparing the scattering function values 
computed by applying the adding equa­tions to two halves of an object to those computed directly for 
the aggregate. Finally, we veri.ed that our 3D implementation gave the same results as the 1D equation 
for uniformly illuminated pla­nar objects that have homogeneous scattering properties in xy. We then 
conducted a series of experiments to compare the ef.­ciency of our solution method to a standard solution 
method that uses the equation of transfer. We implemented a Monte Carlo sampling routine that uses the 
equation of transfer to estimate the scattering function of a medium for a pair of angles based on a 
random walk. Our implementation is similar to the algorithm de­scribed by Hanrahan and Krueger [HK93]: 
a particle is injected into the medium from the incident direction and followed along a path through 
the medium. The walk is biased so that at each scat­tering event, the attenuation to the surface in the 
outgoing direction is computed and the result is accumulated to estimate the function s value. Russian 
roulette is used to terminate this process, based on the accumulated weight of the path. After verifying 
that both methods converged to the same results, we compared their relative ef.ciency. For a variety 
of thicknesses, scattering coef.cients, absorption coef.cients, and phase functions, we computed accurate 
estimates of the scattering function for a pair of angles. We used a phase function due to Henyey and 
Green­stein [HG41]; it takes an asymmetry parameter, g that is the average value of the product of the 
phase function with the cosine of the an­gle between O'and O. The range of g is from 1 to 1, correspond­ing 
to total back-scattering to total forward scattering, respectively. We then applied both solution methods 
to computing estimates of the scattering functions for the pair of angles, giving each the same amount 
of processor time. The same Russian roulette termination parameters were used for each method and importance 
sampling was applied in analogous places (e.g. for sampling the outgoing di­rection of the phase function 
at scattering events for the equation of transfer). Our implementation generally computed .ve to ten 
esti­mates with the equation of transfer in the time it took to compute one estimate with the scattering 
equation. The graphs in Figure 9 show some of the results. We com­puted the ratio of variance of the 
equation of transfer solution to the scattering equation solution, after giving each the same amount 
of processor time. The scattering equation solution often had 5 to 10 times less variance, though for 
some con.gurations (strongly anisotropic phase functions and very thick objects), it sometimes had over 
100 times less variance. Although the scattering equa­tion generally performed quite well, for cases 
with high albedos the 1000 -0.5 0.0 100 0.5 10 1 0.1 0.01 0.001 0.0 0.2 0.4 0.6 0.8 1.0 albedo Ratio 
of variance as a function of albedo for various g. Depth = 2. 1000 1.0 20.0 100 10 1 0.1 0.01 0.001 
-1.0 -0.5 0.0 0.5 1.0 g Ratio of variance as a function of g for various depths. Albedo = 0.4 Figure 
9: Comparing our solutions of the standard prob­lem. After giving each method the same amount of processor 
time to compute the best possible solution, we have graphed the ratio of variance when the equation of 
transfer is sam­pled to the variance when the scattering equation is sampled. Points above 1 on the y 
axis indicate situations where the scattering equation is more ef.cient. Because both sampling methods 
converge at the same rate asymptotically, the ratio of running time to compute solutions of equivalent 
quality is proportional to the variance ratio. equation of transfer was sometimes more ef.cient. We believe 
that this is due to our recursive sampling process: because we compute a geometrically-increasing number 
of recursive estimates of R at each level of recursion, it is possible to end up computing a large number 
of estimates that have a relatively little in.uence on the .nal result. The scattering equation also 
did well for most phase function parameters, except for extreme backward scattering. In this case, although 
most of incident light is quickly scattered back out of the top, we still continue to work through the 
z depth of the medium, not allocating effort as well as we might.  5.2 Scattering from surfaces To 
illustrate the use of the 1D adding and scattering equations for rendering, we took a dragon model with 
a standard specular and diffuse shading model and added scattering layers to it, using the adding equations 
to compute the new scattering function that de­scribes the composition of the base surface layer with 
the new scat­tering layer. As such, the result is an accurate simulation of sub­surface light transport. 
When the routines that compute the adding equations needed to evaluate the re.ection or transmission 
func­tions of the added layer, a new Monte Carlo estimate for that pair of angles was computed. With 
a not-very-optimized implementation, the images each took a few minutes to render on a modern PC. The 
series of images in Figure 10 shows the results. The .rst image shows the object shaded with the standard 
shading model. As the thickness of the new layer increases going from left to right, the shiny copper 
base surface is gradually overwhelmed by the grey and more diffuse added layer. Eventually just a shadow 
of the specular highlights is left and .nally no trace of the base surface once the new layer is suf.ciently 
thick. Notice that the silhouette edges are affected more strongly by the added layer; this is because 
the rays traveling at oblique angles go a longer distance through the new layer. The rightmost image 
shows the result of procedurally varying the thickness of the added layer based on the local surface 
normal in an effort to simulate scattering effects of dust (modeled in a manner similar to Hsu and Wong 
[HW95]).  5.3 Scattering from volumes As a .nal example, we generated some images to demonstrate the 
use of the 3D scattering equation to compute re.ection from com­plex surfaces and performed some experiments 
to understand the properties of subsurface light transport. To determine how distance from the point 
of illumination af­fected the intensity of re.ected light, we illuminated half of a slab from the direction 
along its normal and looked at the scattering function s magnitude in the normal direction at a series 
of points moving away from the illuminated area. Figure 11 shows the re­sults. As one might expect, re.ectance 
drops off roughly exponen­tially. Other experiments showed that as the object gets thinner, light entering 
from far away becomes less important, because more light scatters out of the object before traveling 
very far. These ob­servations help validate some of the assumptions made in designing importance sampling 
techniques for 3D scattering equation. Inspired by the images of Dorsey et al., we rendered some im­ages 
of marble, in the form of a marble block. Scattering prop­erties were computed procedurally using noise 
functions [Per85]. Figure 12 shows a comparison of rendering a block with the 1D scattering equation 
(left) compared to a rendering with the 3D scat­tering equation (right). The right halves of the blocks 
were brightly illuminated by a directional light source, while the left halves were lit dimly. There 
are a number of signi.cant differences between the two images. Most strikingly, when subsurface light 
transport is accounted for we can see the effect of light that entered in the illuminated half and then 
scattered into the unilluminated half. Fur­thermore, the veins of the marble, where the attenuation coef.cient 
is high, cast shadows inside the volume; this effect is missing in the 1D case. A subtle difference between 
the two can be seen along the edges: they are more transparent in the 3D version, since the geometry 
of the object is accounted for in computing subsurface scattering and rays leave the object after a short 
distance.   6 Summary and Conclusion In this paper we have introduced a new theoretical framework for 
light scattering to computer graphics. This theory has scattering as its basic foundation, rather than 
light transport. We have applied the theory to rendering subsurface scattering from complex objects using 
Monte Carlo integration. For some rendering problems (and with the sampling algorithms we used), the 
scattering equation can be solved more ef.ciently than the equation of transfer. The adding equations 
exhibit ef.ciencies by providing a way to break render­ing problems into smaller parts and then reassemble 
the partial so­lutions; this gives a theoretical basis to clustering algorithms and a new way to apply 
clustering to Monte Carlo rendering algorithms. Part of the advantage from the scattering equation solution 
stems from the fact that its recursive expansion has a bidirectional effect paths are constructed in 
both directions and meet in the middle. Our sampling of the SkS term re.ects a non-local sampling strat­egy 
[Vea97], where a scattering event at k is chosen before either of its adjacent scattering events have 
been sampled. This is in con­trast to previous bidirectional sampling strategies that incrementally build 
paths by .nding new vertices directly from a previous vertex. As such, understanding the connections 
between the path sampling strategies that we have used and previous bidirectional path sam­pling strategies 
is important future work. In particular, techniques that ameliorate the exponential nature of the recursive 
sampling and more effectively re-use sub-paths should improve performance in cases where the albedo is 
high. Another area for further investiga­tion is better importance sampling techniques for the 3D case 
and the application of multiple importance sampling to reduce variance. Our example of subsurface scattering 
as a demonstration of the three-dimensional scattering equation re.ects a choice in scale 0.3 0.2 0.1 
0.0 0 1 2 3 Distance Figure 11: Re.ection function magnitude in the unillumi­nated part of an object 
as a function of distance from the boundary of illuminated region. pa h05, ps h05,g h0 Reflectance fraction 
rather than limitation of theory. The scattering and adding equations have applications to computing 
scattering from complex volumet­ric objects at larger scales, such as clouds, smoke, sunbeams, etc. As 
such, this approach has applications to the level-of-detail prob­lem. The 3D scattering and adding equations 
provide the correct mathematical setting for two of the outstanding problems in level­of-detail identi.ed 
by Kajiya and Kay [KK89]: automatic compu­tation of texels from complex geometry, and computation of 
aggre­gate texels that represent two nearby texels. Furthermore, scatter­ing functions are the correct 
abstraction to use to replace geometry; techniques based on BRDFs (e.g. [Kaj85, Ney98]) are inaccurate 
in that they do not correctly incorporate the effect of light that enters an object at a different place 
than it exits. This theory has applications to many classic problems in ren­dering, including replacing 
geometry with scattering functions and ef.ciently re-rendering scenes with changes in illumination or 
as objects are added to or removed from them. Equally important, it has promise as a way to suggest new 
sampling strategies for solving the rendering equation more effectively. Understanding the connec­tions 
between solution techniques that have previously been used for each of these approaches gives many directions 
for future work.  Acknowledgements Discussions with Eric Veach about this work and about connections 
with bidirectional light transport algorithms in particular were very useful. Thanks also to the reviewers 
for insightful comments. The dragon model was provided by the Stanford 3D Scanning Reposi­tory. Matt 
Pharr was supported by a Pixar Animation Studios grad­uate fellowship, DARPA contract DABT63-95-C-0085 
and NSF contract CCR-9508579. Figure 12: Comparison of rendering subsurface scattering from a side-lit 
marble cube with the 1D scattering equation (left) and the 3D scattering equation (right). Since the 
3D so­lution considers light that enters the surface away from where it exits, subsurface light transport 
is more accurately mod­eled. References [AK90] James Arvo and David Kirk, Particle transport and image 
synthesis, Computer Graphics 24 (1990), no. 4, 63 66. [Amb42] V. A. Ambarzumian, A new method for computing 
light scattering in turbid media, Izv. Akad. Nauk SSSR 3 (1942). [Amb58] V. A. Ambarzumian (ed.), Theoretical 
astrophysics, Perga­mon Press, New York, New York, 1958. [BK56] Richard Bellman and Robert Kalaba, On 
the principle of invairant imbedding and propagation through inhomohe­neous media, Proceedings of the 
National Academy of Sci­ences 42 (1956), 629 632. [BKP63] Richard E. Bellman, Robert E. Kalaba, and Marcia 
C. Pre­strud, Invariant imbedding and radiative transfer in slabs of .nite thickness, American Elsevier 
Publishing Company, New York, 1963. [Bli82] James F. Blinn, Light re.ection functions for simulation 
of clouds and dusty surfaces, Computer Graphics 16 (1982), no. 3, 21 29. [BW75] Richard E. Bellman and 
G. M. Wing, An introduction to invariant imbedding, John Wiley &#38; Sons, New York, 1975. [Cha60] S. 
Chandrasekar, Radiative transfer, Dover Publications, New York, 1960, Originally published by Oxford 
Univer­sity Press, 1950. [DEL 99] Julie Dorsey, Alan Edelman, Justin Legakis, Henrik Wann Jensen, and 
Hans Køhling Pedersen, Modeling and render­ing of weathered stone, Proceedings of SIGGRAPH 99 (Au­gust 
1999), 225 234. [GY89] R. M. Goody and Y. L. Yung, Atmospheric radiation, Ox­ford University Press, 1989. 
[HG41] L. G. Henyey and J. L. Greenstein, Diffuse radiation in the galaxy, Astrophysical Journal 93 (1941), 
70 83. [HK93] Pat Hanrahan and Wolfgang Krueger, Re.ection from lay­ered surfaces due to subsurface scattering, 
Computer Graphics Proceedings, August 1993, pp. 165 174. [HM92] Chet S. Haase and Gary W. Meyer, Modeling 
pigmented materials for realistic image synthesis, ACM Transactions on Graphics 11 (1992), no. 4, 305 
335. [HW95] Siu-Chi Hsu and Tien-Tsin Wong, Simulating dust accu­mulation, IEEE Computer Graphics and 
Applications 15 (1995), no. 1, 18 25. [Kaj85] James T. Kajiya, Anisotropic re.ection models, Computer 
Graphics (SIGGRAPH 85 Proceedings), vol. 19, July 1985, pp. 15 21. [Kaj86] James T. Kajiya, The rendering 
equation, Computer Graph­ics 20 (1986), no. 4, 143 150. [KK89] James T. Kajiya and Timothy L. Kay, Rendering 
fur with three dimensional textures, Computer Graphics 23 (1989), no. 3, 271 280. [KM31] P. Kubelka and 
F. Munk, Ein Beitrag zur Optik der Far­banstriche, Z. Tech. Physik. 12 (1931), 593. [KW86] Malvin H. 
Kalos and Paula A. Whitlock, Monte Carlo meth­ods: Volume I: Basics, John Wiley &#38; Sons, New York, 
1986. [Laf96] Eric Lafortune, Mathematical models and Monte Carlo algorithms for physically based rendering, 
Ph.D. thesis, Katholieke Universiteit Leuven, February 1996. [LW94] Eric Lafortune and Yves Willems, 
A theoretical framework for physically based rendering, Computer Graphics Forum 13 (1994), no. 2, 97 
107. [MM98] Gavin Miller and Marc Mondesir, Rendering hyper-sprites in real time, Eurographics Rendering 
Workshop 1998 (1998), 193 198. [MMKW97] Nelson Max, Curtis Mobley, Brett Keating, and En-Hua Wu, Plane-parallel 
radiance transport for global illumina­tion in vegetation, Eurographics Rendering Workshop 1997, Eurographics, 
Springer Wien, June 1997, pp. 239 250. [Mob94] Curtis D. Mobley, Light and water: Radiative transfer 
in natural waters, Academic Press, 1994. [Ney98] Fabrice Neyret, Modeling, animating, and rendering com­plex 
scenes using volumetric textures, IEEE Transactions on Visualization and Computer Graphics 4 (1998), 
no. 1. [NRH 77] Fred E. Nicodemus, J. C. Richmond, J. J. Hisa, I. W. Ginsberg, and T. Limperis, Geometrical 
considerations and nomenclature for re.ectance, Monograph number 160, Na­tional Bureau of Standards, 
Washington DC, 1977. [NUW98] [Per85] [PP51] [Pre58] [Pre65] [Pre76] [PV00] [RPV93] [SD95] [SDS95] [Sto62] 
[TJH66] [TS67] [van80] [Vea97] [VG94] [VG95] [Wan90] [WAT92] [ZWCS99] H. H. Natsuyama, S. Ueno, and A. 
P. Wang, Terrestial ra­diative transfer, Springer-Verlag, Hong Kong, 1998. Ken Perlin, An image synthesizer, 
Computer Graphics (SIGGRAPH 85 Proceedings), vol. 19, July 1985, pp. 287 296. Glenn H. Peebles and Milton 
S. Plesset, Transmission of gamma-rays through large thicknesses of heavy materials, Physical Review 
81 (1951), no. 3, 430 439. Rudolph W. Preisendorfer, Invariant imbedding relation for the principles 
of invariance, Proceedings of the National Academy of Sciences 44 (1958), 320 323. Rudolph W. Preisendorfer, 
Radiative transfer on discrete spaces, Pergamon Press, Oxford, 1965. R. W. Preisendorfer, Hydrologic 
optics, U.S. Department of Commerce, National Oceanic and Atmospheric Adminis­tration, Honolulu, Hawaii, 
1976, Six volumes. Matt Pharr and Eric Veach, Shading with closures, In prepa­ration, 2000. Holly Rushmeier, 
Charles Patterson, and Aravindan Veerasamy, Geometric simpli.cation for indirect illumina­tion calculations, 
Proceedings of Graphics Interface 93, May 1993, pp. 227 236. Franc¸ois Sillion and George Drettakis, 
Feature-based con­trol of visibility error: A multi-resolution clustering algo­rithm for global illumination, 
SIGGRAPH 95 Conference Proceedings, Addison Wesley, August 1995, pp. 145 152. Franc¸ois Sillion, G. Drettakis, 
and Cyril Soler, A clus­tering algorithm for radiance calculation in general envi­ronments, Eurographics 
Rendering Workshop 1995, Euro­graphics, June 1995. George Stokes, On the intensity of the light re.ected 
from or transmitted through a pile of plates, Proceedings of the Royal Society (1862), Reprinted in Mathematical 
and Phys­ical Papers of Sir George Stokes, Volume IV, Cambridge, 1904. S. Twomey, H. Jacobowitz, and 
H. B. Howell, Matrix meth­ods for multiple-scattering problems, Journal of the Atmo­spheric Sciences 
23 (1966), 289 296. K. E. Torrance and E. M. Sparrow, Theory for off-specular re.ection from roughened 
surfaces, Journal of the Optical Society of America 57 (1967), no. 9. Hendrik Christoffel van de Hulst, 
Multiple light scattering, Academic Press, New York, 1980, Two volumes. Eric Veach, Robust Monte Carlo 
methods for light trans­port simulation, Ph.D. thesis, Stanford University, Decem­ber 1997. Eric Veach 
and Leonidas Guibas, Bidirectional estimators for light transport, Fifth Eurographics Workshop on Ren­dering 
(Darmstadt, Germany), June 1994, pp. 147 162. Eric Veach and Leonidas J. Guibas, Optimally combining 
sampling techniques for Monte Carlo rendering, Computer Graphics Proceedings, August 1995, pp. 419 428. 
Alan P. Wang, Basic equations of three-dimensional radia­tive transfer, Journal of Mathematical Physics 
31 (1990), no. 1, 175 181. Stephen Westin, James Arvo, and Kenneth Torrance, Pre­dicting re.ectance functions 
from complex surfaces, Com­puter Graphics 26 (1992), no. 2, 255 264. Douglas E. Zongker, Dawn M. Werner, 
Brian Curless, and David H. Salesin, Environment matting and compositing, SIGGRAPH 99 Conference Proceedings, 
Addison Wesley, August 1999, pp. 205 214.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344829</article_id>
		<sort_key>85</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Displaced subdivision surfaces]]></title>
		<page_from>85</page_from>
		<page_to>94</page_to>
		<doi_number>10.1145/344779.344829</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344829</url>
		<abstract>
			<par><![CDATA[<p>In this paper we introduce a new surface representing, the <italic>displaced subdivision surface</italic>. It represents a detailed surface model as a scalar-valued displacement over a smooth domain surface. Our representation defines both the domain surface and the displacement function using a unified subdivision framework, allowing for simple and efficient evaluation of analytic surface properties. We present a simple, automatic scheme for converting detailed geometric models into such a representation. The challenge in this conversion process is to find a simple subdivision surface that still faithfully expresses the detailed model as its offset. We demonstrate that displaced subdivision surfaces offer a number of benefits, including geometry compression, editing, animation, scalability, and adaptive rendering. In particular, the encoding of fine detail as a <italic>scalar</italic> function makes the representation extremely compact.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[animation]]></kw>
			<kw><![CDATA[bump maps]]></kw>
			<kw><![CDATA[displacement maps]]></kw>
			<kw><![CDATA[geometry compression]]></kw>
			<kw><![CDATA[multiresolution editing]]></kw>
			<kw><![CDATA[multiresolution geometry]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010395</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image compression</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P10007</person_id>
				<author_profile_id><![CDATA[81100385346]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Aaron]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, Princeton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31087297</person_id>
				<author_profile_id><![CDATA[81100412948]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Henry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Moreton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P112851</person_id>
				<author_profile_id><![CDATA[81100397561]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hugues]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoppe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>555371</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Apodaca, A. and Gritz, L. Advanced RenderMan - Creating CGI for Motion Pictures, Morgan Kaufmann, San Francisco, CA, 1999.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166141</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Becker, B. and Max, N. Smooth transitions between bump rendering algorithms. Proceedings of SIGGRAPH 93, Computer Graphics, Annual Conference Series, pp. 183-190.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>507101</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Blinn, J. F. Simulation of wrinkled surfaces. Proceedings of SIGGRAPH 78, Computer Graphics, pp. 286-292.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37434</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Cabral, B., Max, N. and Springmeyer, R. Bidirectional reflection functions from surface bump maps. Proceedings of SIGGRAPH 87, Computer Graphics, Annual Conference Series, pp.273-281.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Catmull, E., and Clark, J. Recursively generated B-spline surfaces on arbitrary topological meshes. Computer Aided Design 10, pp. 350-355 (1978).]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237213</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Certain,A.,Popovic,J.,DeRose,T.,Duchamp,T.,Salesin,D.and Stuetzle, W. Interactive multiresolution surface viewing. Proceedings of SIGGRAPH 96, Computer Graphics, Annual Conference Series, pp. 91-98.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>266794</ref_obj_id>
				<ref_obj_pid>266774</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Chan, K., Mann, S., and Bartels, R. World space surface pasting. Graphics Interface '97, pp. 146-154.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280832</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Cohen, J., Olano, M. and Manocha, D. Appearance preserving Simplification. Proceedings of SIGGRAPH 98, Computer Graphics, Annual Conference Series, pp. 115-122.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808602</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Cook, R. Shade trees. Computer Graphics (Proceedings of SIGGRAPH 84), 18(3), pp. 223-231.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218391</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Deering, M. Geometry compression. Proceedings of SIGGRAPH 95, Computer Graphics, Annual Conference Series, pp. 13-20.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280826</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[DeRose, T., Kass, M., and Truong, T. Subdivision surfaces in character animation. Proceedings of SIGGRAPH 98, Computer Graphics, Annual Conference Series, pp. 85-94.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Do Carmo, M. P. Differential Geometry of Curves and Surfaces. Prentice-Hall, Inc., Englewood Cliffs, New Jersey, 1976.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Doo, D., and Sabin, M. Behavior of recursive division surfaces near extraordinary points. Computer Aided Design 10, pp. 356-360 (1978).]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218440</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Eck, M., DeRose, T., Duchamp, T., Hoppe, H., Lounsbery, M., and Stuetzle, W. Multiresolution analysis of arbitrary meshes. Proceedings of SIGGRAPH 95, Computer Graphics, Annual Conference Series, pp. 173-182.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>221665</ref_obj_id>
				<ref_obj_pid>221659</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Forsey, D., and Bartels, R. Surface fitting with hierarchical splines. ACM Transactions on Graphics, 14(2), pp. 134-161 (April 1995).]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258849</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Garland, M., and Heckbert, P. Surface simplification using quadric error metrics. Proceedings of SIGGRAPH 97, Computer Graphics, Annual Conference Series, pp. 209-216.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237244</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Gottschalk, S., Lin, M., and Manocha, D. OBB-tree: a hierarchical structure for rapid interference detection. Proceedings of SIGGRAPH 96, Computer Graphics, Annual Conference Series, pp. 171-180.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280836</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Gumhold, S., and Stra~er, W. Real time compression of triangle mesh connectivity. Proceedings of SIGGRAPH 98, Computer Graphics, Annual Conference Series, pp. 133-140.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311578</ref_obj_id>
				<ref_obj_pid>311534</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Gumhold, S., and H~ttner, T. Multiresolution rendering with displacement mapping. SIGGRAPH workshop on Graphics hardware, Aug 8-9, 1999.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>344831</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Guskov, I., Vidimce, K., Sweldens, W., and Schr~der, P. Normal meshes. Proceedings of SIGGRAPH 2000, Computer Graphics, Annual Conference Series.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192233</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Hoppe, H., DeRose, T., Duchamp, T., Halstead, M., Jin, H., McDonald, J., Schweitzer, J., and Stuetzle, W. Piecewise smooth surface reconstruction. Proceedings of SIGGRAPH 94, Computer Graphics, Annual Conference Series, pp. 295-302.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237216</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Hoppe, H. Progressive meshes. Proceedings of SIGGRAPH 96, Computer Graphics, Annual Conference Series, pp. 99-108.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Kobbelt, L., Bareuther, T., and Seidel, H. P. Multi-resolution shape deformations for meshes with dynamic vertex connectivity. Proceedings of EUROGRAPHICS 2000, to appear.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>789608</ref_obj_id>
				<ref_obj_pid>789085</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Kolarov, K. and Lynch, W. Compression of functions defined on surfaces of 3D objects. In J. Storer and M. Cohn, editors, Proc. of Data Compression Conference, IEEE, pp. 281-291, 1997.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237270</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Krishnamurthy, V., and Levoy, M. Fitting smooth surfaces to dense polygon meshes. Proceedings of SIGGRAPH 96, Computer Graphics, Annual Conference Series, pp. 313-324.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280828</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Lee, A., Sweldens, W., Schr~der, P., Cowsar, L., and Dobkin, D. MAPS: Multiresolution adaptive parameterization of surfaces. Proceedings of SIGGRAPH 98, Computer Graphics, Annual Conference Series, pp. 95-104.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Loop, C. Smooth subdivision surfaces based on triangles. Master's thesis, University of Utah, Department of Mathematics, 1987.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237750</ref_obj_id>
				<ref_obj_pid>237748</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Lounsbery, M., DeRose, T., and Warren, J. Multiresolution analysis for surfaces of arbitrary topological type. ACM Transactions on Graphics, 16(1), pp. 34-73 (January 1997).]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Mann, S. and Yeung, T. Cylindrical surface pasting. Technical Report, Computer Science Dept., University of Waterloo (June 1999).]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Marschner, S., Guenter, B., and Raghupathy, S. Modeling and rendering for realistic facial animation. Submitted for publication.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258873</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Peercy, M., Airey, J. and Cabral, B. Efficient bump mapping hardware. Proceedings of SIGGRAPH 97, Computer Graphics, Annual Conference Series, pp. 303-306.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>87556</ref_obj_id>
				<ref_obj_pid>87526</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Peters, J. Local smooth surface interpolation: a classification. Computer Aided Geometric Design, 7(1990), pp. 191-195.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218439</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Schr~der, P., and Sweldens, W. Spherical wavelets: efficiently representing functions on the sphere. Proceedings of SIGGRAPH 95, Computer Graphics, Annual Conference Series, pp. 161-172.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Shoham, Y. and Gersho, A. Efficient bit allocation for an arbitrary set of quantizers. IEEE Transactions on Acoustics, Speech, and Signal Processing, Vol. 36, No. 9, pp. 1445-1453, Sept 1988.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218473</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Taubin, G. A signal processing approach to fair surface design. Proceedings of SIGGRAPH 95, Computer Graphics, Annual Conference Series, pp. 351-358.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
			<ref>
				<ref_obj_id>274365</ref_obj_id>
				<ref_obj_pid>274363</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Taubin, G. and Rossignac, J. Geometric compression through topological surgery. ACM Transactions on Graphics, 17(2), pp. 84- 115 (April 1998).]]></ref_text>
				<ref_id>36</ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Taubman, D. and Zakhor, A. Multirate 3-D subband coding of video. IEEE Transactions on Image Processing, Vol. 3, No. 5, Sept, 1994.]]></ref_text>
				<ref_id>37</ref_id>
			</ref>
			<ref>
				<ref_obj_id>253314</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Zhang, H., and Hoff, K. Fast backface culling using normal masks. Symposium on Interactive 3D Graphics, pp. 103-106, 1997.]]></ref_text>
				<ref_id>38</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258863</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Zorin, D., Schr~der, P., and Sweldens, W. Interactive multiresolution mesh editing. Proceedings of SIGGRAPH 97, Computer Graphics, Annual Conference Series, pp. 259-268.]]></ref_text>
				<ref_id>39</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Displaced Subdivision Surfaces Aaron Lee Henry Moreton Hugues Hoppe Department of Computer Science 
NVIDIA Corporation Microsoft Research Princeton University moreton@nvidia.com http://research.microsoft.com/~hoppe 
http://www.aaron-lee.com/ ABSTRACT In this paper we introduce a new surface representation, the displaced 
subdivision surface. It represents a detailed surface model as a scalar-valued displacement over a smooth 
domain surface. Our representation defines both the domain surface and the displacement function using 
a unified subdivision framework, allowing for simple and efficient evaluation of analytic surface properties. 
We present a simple, automatic scheme for converting detailed geometric models into such a representation. 
The challenge in this conversion process is to find a simple subdivision surface that still faithfully 
expresses the detailed model as its offset. We demonstrate that displaced subdivision surfaces offer 
a number of benefits, including geometry compression, editing, animation, scalability, and adaptive rendering. 
In particular, the encoding of fine detail as a scalar function makes the representation extremely compact. 
Additional Keywords: geometry compression, multiresolution geometry, displacement maps, bump maps, multiresolution 
editing, animation. 1. INTRODUCTION Highly detailed surface models are becoming commonplace, in part 
due to 3D scanning technologies. Typically these models are represented as dense triangle meshes. However, 
the irregularity and huge size of such meshes present challenges in manipulation, animation, rendering, 
transmission, and storage. Meshes are an expensive representation because they store: (1) the irregular 
connectivity of faces, (2) the (x,y,z) coordinates of the vertices, (3) possibly several sets of texture 
parameterization (u,v) coordinates at the vertices, and (4) texture images referenced by these parameterizations, 
such as color images and bump maps. An alternative is to express the detailed surface as a displacement 
from some simpler, smooth domain surface (see Figure 1). Compared to the above, this offers a number 
of advantages: (1) the patch structure of the domain surface is defined by a control mesh whose connectivity 
is much simpler than that of the original detailed mesh; (2) fine detail in the displacement field can 
be captured as a scalar-valued function which is more compact than traditional vector-valued geometry; 
  Permission to make digital or hard copies of part or all of this work or personal or classroom use 
is granted without fee provided that copies are not made or distributed for profit or commercial advantage 
and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, 
to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. SIGGRAPH 
2000, New Orleans, LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 (3) the parameterization of 
the displaced surface is inherited from the smooth domain surface and therefore does not need to be stored 
explicitly; (4) the displacement field may be used to easily generate bump maps, obviating their storage. 
  (a) control mesh (b) smooth domain surface Figure 1: Example of a displaced subdivision surface. 
A simple example of a displaced surface is terrain data expressed as a height field over a plane. The 
case of functions over the sphere has been considered by Schröder and Sweldens [33]. Another example 
is the 3D scan of a human head expressed as a radial function over a cylinder. However, even for this 
simple case of a head, artifacts are usually detectable at the ear lobes, where the surface is not a 
single-valued function over the cylindrical domain. The challenge in generalizing this concept to arbitrary 
surfaces is that of finding a smooth underlying domain surface that can express the original surface 
as a scalar-valued offset function. Krishnamurthy and Levoy [25] show that a detailed model can be represented 
as a displacement map over a network of B-spline patches. However, they resort to a vector-valued displacement 
map because the detailed model is not always an offset of their B­spline surface. Also, avoiding surface 
artifacts during animation requires that the domain surface be tangent-plane (C1) continuous, which involves 
constraints on the B-spline control points. We instead define the domain surface using subdivision surfaces, 
since these can represent smooth surfaces of arbitrary topological type without requiring control point 
constraints. Our representation, the displaced subdivision surface, consists of a control mesh and a 
scalar field that displaces the associated subdivision surface locally along its normal (see Figure 1). 
In this paper we use the Loop [27] subdivision surface scheme, although the representation is equally 
well defined using other schemes such as Catmull-Clark [5]. Both subdivision surfaces and displacement 
maps have been in use for about 20 years. One of our contributions is to unify these two ideas by defining 
the displacement function using the same subdivision machinery as the surface. The scalar displacements 
are stored on a piecewise regular mesh. We show that simple subdivision masks can then be used to compute 
analytic properties on the resulting displaced surface. Also, we make displaced subdivision surface practical 
by introducing a scheme for constructing them from arbitrary meshes. We demonstrate several benefits 
of expressing a model as a displaced subdivision surface: Compression: both the surface topology and 
parameterization are defined by the coarse control mesh, and fine geometric detail is captured using 
a scalar-valued function (Section 5.1). Editing: the fine detail can be easily modified since it is a 
scalar field (Section 5.2). Animation: the control mesh makes a convenient armature for animating the 
displaced subdivision surface, since geometric detail is carried along with the deformed smooth domain 
surface (Section 5.3). Scalability: the scalar displacement function may be converted into geometry or 
a bump map. With proper multiresolution filtering (Section 5.4), we can also perform magnification and 
minification easily. Rendering: the representation facilitates adaptive tessellation and hierarchical 
backface culling (Section 5.5).  2. PREVIOUS WORK Subdivision surfaces: Subdivision schemes defining 
smooth surfaces have been introduced by Catmull and Clark [5], Doo and Sabin [13], and Loop [27]. More 
recently, these schemes have been extended to allow surfaces with sharp features [21] and fractionally 
sharp features [11]. In this paper we use the Loop subdivision scheme because it is designed for triangle 
meshes. DeRose et al. [11] define scalar fields over subdivision surfaces using subdivision masks. Our 
scalar displacement field is defined similarly, but from a denser set of coefficients on a piecewise 
regular mesh (Figure 2). Hoppe et al. [21] describe a method for approximating an original mesh with 
a much simpler subdivision surface. Unlike our conversion scheme of Section 4, their method does not 
consider whether the approximation residual is expressible as a scalar displacement map. Displacement 
maps: The idea of displacing a surface by a function was introduced by Cook [9]. Displacement maps have 
become popular commercially as procedural displacement shaders in RenderMan [1]. The simplest displacement 
shaders interpolate values within an image, perhaps using standard bicubic filters. Though displacements 
may be in an arbitrary direction, they are almost always along the surface normal [1]. Typically, normals 
on the displaced surface are computed numerically using a dense tessellation. While simple, this approach 
requires adjacency information that may be unavailable or impractical with low-level APIs and in memory-constrained 
environments (e.g. game consoles). Strictly local evaluation requires that normals be computed from a 
continuous analytic surface representation. However, it is difficult to piece together multiple displacement 
maps while maintaining smoothness. One encounters the same vertex enclosure problem [32] as in the stitching 
of B-spline surfaces. While there are well-documented solutions to this problem, they require constructions 
with many more coefficients (9× in the best case), and may involve solving a global system of equations. 
In contrast, our subdivision-based displacements are inherently smooth and have only quartic total degree 
(fewer DOF than bicubic). Since the displacement map uses the same parameterization as the domain surface, 
the surface representation is more compact and displaced surface normals may be computed more efficiently. 
Finally, unifying the representation around subdivision simplifies implementation and makes operations 
such as magnification more natural. Krishnamurthy and Levoy [25] describe a scheme for approximating 
an arbitrary mesh using a B-spline patch network together with a vector-valued displacement map. In their 
scheme, the patch network is constructed manually by drawing patch boundaries on the mesh. The recent 
work on surface pasting by Chan et al. [7] and Mann and Yeung [29] uses the similar idea of adding a 
vector-valued displacement map to a spline surface. Gumhold and Hüttner [19] describe a hardware architecture 
for rendering scalar-valued displacement maps over planar triangles. To avoid cracks between adjacent 
triangles of a mesh, they interpolate the vertex normals across the triangle face, and use this interpolated 
normal to displace the surface. Their scheme permits adaptive tessellation in screen space. They discuss 
the importance of proper filtering when constructing mipmap levels in a displacement map. Unlike our 
representation, their domain surface is not smooth since it is a polyhedron. As shown in Section 5.3, 
animating a displaced surface using a polyhedral domain surface results in many surface artifacts. Kobbelt 
et al. [23] use a similar framework to express the geometry of one mesh as a displacement from another 
mesh, for the purpose of multiresolution shape deformation. Bump maps: Blinn [3] introduces the idea 
of perturbing the surface normal using a bump map. Peercy et al. [31] present recent work on efficient 
hardware implementation of bump maps. Cohen et al. [8] drastically simplify meshes by capturing detail 
in the related normal maps. Both Cabral et al. [4] and Apodaca and Gritz [1] discuss the close relationship 
of bump mapping and displacement mapping. They advocate combining them into a unified representation 
and resorting to true displacement mapping only when necessary. Multiresolution subdivision: Lounsbery 
et al. [28] apply multiresolution analysis to arbitrary surfaces. Given a parameterization of the surface 
over a triangular domain, they compress this (vector-valued) parameterization using a wavelet basis, 
where the basis functions are defined using subdivision of the triangular domain. Zorin et al. [39] use 
a similar subdivision framework for multiresolution mesh editing. To make this multiresolution framework 
practical, several techniques have been developed for constructing a parameterization of an arbitrary 
surface over a triangular base domain. Eck et al. [14] use Voronoi/Delaunay diagrams and harmonic maps, 
while Lee et al. [26] track successive mappings during mesh simplification. In contrast, displaced subdivision 
surfaces do not support an arbitrary parameterization of the surface, since the parameterization is given 
by that of a subdivision surface. The benefit is that we need only compress a scalar-valued function 
instead of vector-valued parameterization. In other words, we store only geometric detail, not a parameterization. 
The drawback is that the original surface must be expressible as an offset of a smooth domain surface. 
An extremely bad case would be a fractal snowflake surface, where the domain surface cannot be made much 
simpler than the original surface. Fortunately, fine detail in most practical surfaces is expressible 
as an offset surface. Guskov et al. [20] represent a surface by successively applying a hierarchy of 
displacements to a mesh as it is subdivided. Their construction allows most of the vertices to be encoded 
using scalar displacements, but a small fraction of the vertices require vector displacements to prevent 
surface folding. 3. REPRESENTATION OVERVIEW A displaced subdivision surface consists of a triangle control 
mesh and a piecewise regular mesh of scalar displacement coefficients (see Figure 2). The domain surface 
is generated from the control mesh using Loop subdivision. Likewise, the displacements applied to the 
domain surface are generated from the scalar displacement mesh using Loop subdivision. Figure 2: Control 
mesh (left) with its piecewise regular mesh of scalar displacement coefficients ( k =3). Displacement 
map: The scalar displacement mesh is stored for each control mesh triangle as one half of the sample 
grid (2k +1)×(2k +1) ,where k depends on the sampling density required to achieve a desired level of 
accuracy or compression. To define a continuous displacement function, these stored values are taken 
to be subdivision coefficients for the same (Loop) subdivision scheme that defines the domain surface. 
Thus, as the surface is magnified (i.e. subdivided beyond level k), both the domain surface geometry 
and the displacement field are subdivided using the same machinery. As a consequence, the displacement 
field is C1 even at extraordinary vertices, and the displaced subdivision surface is C1 everywhere except 
at extraordinary vertices. The handling of extraordinary vertices is discussed below. For surface minification, 
we first compute the limit displacements for the subdivision coefficients at level k, and we then construct 
a mipmap pyramid with levels l0,,k - . 1q by successive filtering of these limit values. We cover filtering 
possibilities in Section 4.5. As with ordinary texture maps, the content author may sometimes want more 
precise control of the filtered levels, so it may be useful to store the entire pyramid. (For our compression 
analysis in Section 5.1, we assume that the pyramid is built automatically.) For many input meshes, it 
is inefficient to use the same value of k for all control mesh faces. For a given face, the choice of 
k may be guided by the number of original triangles associated it, which is easily estimated using MAPS 
[26]. Those regions with lower values of k are further subdivided logically to produce a mesh with uniform 
k. Normal Calculation: We now derive the surface normal for a pointS on the displaced subdivision surface. 
LetS be the displacement of the limit pointP on the domain surface: S =+PDn , where D is the limit 
displacement and n=/ nnn is the unit i normal on the domain surface. The normal n is obtained as nPP 
Pand Pare computed =× where the tangent vectors uv uv using the first derivative masks in Figure 3. The 
displaced subdivision surface normal at S is defined as nn n =×SS where each tangent vector has the form 
s uv SP +Dn . =+Dn uuu u If the displacements are relatively small, it is common to ignore the third 
term, which contains second-order derivatives [3]. However, if the surface is used as a modeling primitive, 
then the displacements may be quite large and the full expression must be n n evaluated. The difficult 
term n u= u / may be derived using u the Weingarten equations [12]. Equivalently, it may be expressed 
as: nu -n (nu ·n ) . . ... n u= where n =P ×P +P ×P . u uuv uuv n At a regular (valence 6) vertex, the 
necessary partial derivatives aregiven by asimpleset of masks(seeFigure3). At extraordinary vertices, 
the curvature of the domain surface vanishes and we omit the second-order term. In this case, the standard 
Loop tangent masks may be used to compute the first partial derivatives. Since there are few extraordinary 
vertices, this simplified normal calculation has not proven to be a problem. 1 1 2 1 1 2 x/12 x/6 x/6 
PP P uv u v x/1 x/1 x/2 PPP uuvv uv Figure 3: Loop masks for limit position P and first and second derivatives 
at a regular control vertex. Bump map: The displacement map may also be used to generate a bump map during 
the rendering of coarser tessellations (see Figure 13). This improves rendering performance on graphics 
systems where geometry processing is a bottleneck. The construction of this bump map is presented in 
Section 5.4. Other textures: The domain surface parameterization is used for storing the displacement 
map (which also serves to define a bump map). It is natural to re-use this same inherent parameterization 
to store additional appearance attributes for the surface, such as color. Section 4.4 describes how such 
attributes are re-sampled from the original surface. Alternatively, one could define more traditional 
surface parameterizations by explicitly specifying (u,v) texture coordinates at the vertices of the control 
mesh, as in [11]. However, since the domain of a (u,v) parameterization is a planar region, this generally 
requires segmenting the surface into a set of charts.    4. CONVERSION PROCESS To convert an arbitrary 
triangle mesh (Figure 5a) into a displaced subdivision surface (Figure 5b), our process performs the 
following steps: Obtain an initial control mesh (Figure 5c) by simplifying the original mesh. Simplification 
is done using a traditional sequence of edge collapse transformations, but with added heuristics to attempt 
to preserve a scalar offset function.  Globally optimize the control mesh vertices (Figure 5d) such 
that the domain surface (Figure 5e) more accurately fits the original mesh.  Sample the displacement 
map by shooting rays along the domain surface normals until they intersect the original mesh. At the 
ray intersection points, compute the signed displacement, and optionally sample other appearance attributes 
like surface color. (The black line segments visible in Figure 5f correspond to rays with positive displacements.) 
 4.1 Simplification to control mesh We simplify the original mesh using a sequence of edge collapse 
transformations [22] prioritized according to the quadric error metric of Garland and Heckbert [16]. 
In order to produce a good domain surface, we restrict some of the candidate edge collapses. The main 
objective is that the resulting domain surface should be able to express the original mesh using a scalar 
displacement map. Our approach is to ensure that the space of normals on the domain surface remains locally 
similar to the corresponding space of normals on the original mesh. To maintain an efficient correspondence 
between the original mesh and the simplified mesh, we use the MAPS scheme [26] to track parameterizations 
of all original vertices on the mesh simplified so far. (When an edge is collapsed, the parametrizations 
of points in the neighborhood are updated using a local 1-to-1 map onto the resulting neighborhood.) 
For each candidate edge collapse transformation, we examine the mesh neighborhood that would result. 
In Figure 4, the thickened 1-ring is the neighborhood of the unified vertex. For vertices on this ring, 
we compute the subdivision surface normals (using tangent masks that involve vertices in the 2-ring of 
the unified vertex). The highlighted points within the faces in the 1-ring represent original mesh vertices 
that are currently parameterized on the neighborhood using MAPS.  Figure 4: Neighborhood after candidate 
edge collapse and, for one face, the spherical triangle about its domain surface normals. (a) original 
mesh (b) displaced subdivision surface (c) initial control mesh (d) optimized control mesh (e) smooth 
domain surface (f) displacement field Figure 5: Steps in the conversion process. For each face in the 
1-ring neighborhood, we gather the 3 subdivision surface normals at the vertices and form their spherical 
triangle on the Gauss sphere. Then, we test whether this spherical triangle encloses the normals of the 
original mesh vertices parameterized using MAPS. If this test fails on any face in the 1-ring, the edge 
collapse transformation is disallowed. To allow simplification to proceed further, we have found it useful 
to broaden each spherical triangle by pushing its three vertices an additional 45 degrees away from its 
inscribed center, as illustrated in Figure 4. We observe that the domain surface sometimes has undesirable 
undulations when the control mesh has vertices of high valence. Therefore, during simplification we also 
disallow an edge collapse if the resulting unified vertex would have valence greater than 8. 4.2 Optimization 
of domain surface Having formed the initial control mesh, we optimize the locations of its vertices such 
that the associated subdivision surface more accurately fits the original mesh. This step is performed 
using the method of Hoppe et al. [21]. We sample a dense set of points from the original mesh and minimize 
their squared distances to the subdivision surface. This nonlinear optimization problem is approximated 
by iteratively projecting the points onto the surface and solving for the most accurate surface while 
fixing those parameterizations. The result of this step is shown in Figure 5d-e. Note that this geometric 
optimization modifies the control mesh and thus affects the space of normals over the domain surface. 
Although this invalidates the heuristic used to guide the simplification process, this has not been a 
problem in our experiments. A more robust solution would be to optimize the subdivision surface for each 
candidate edge collapse (as in [21]) prior to testing the neighborhood normals, but this would be much 
more costly. 4.3 Sampling of scalar displacement map We apply k steps of Loop subdivision to the control 
mesh. At each of these subdivided vertices, we compute the limit position and normal of the domain surface. 
We seek to compute the signed distance from the limit point to the original surface along the normal 
(Figure 5f). The directed line formed by the point and normal is intersected with the original surface, 
using a spatial hierarchy [17] for efficiency. We disregard any intersection point if the intersected 
surface is oriented in the wrong direction with respect to the directed line. If multiple intersection 
points remain, we pick the one closest to the domain surface. Figure 6 illustrates a possible failure 
case if the domain surface is too far from the original. Figure 6: The displacement sampling may fold 
over itself if the domain surface is too distant from the original mesh. Near surface boundaries, there 
is the problem that the domain surface may extend beyond the boundary of the original surface, in which 
case the ray does not intersect any useful part of the original surface. (We detect this using a maximum 
distance threshold based on the mesh size.) In this case, the surface should really be left undefined, 
i.e. trimmed to the detailed boundary of the original mesh. One approach would be to store a special 
illegal value into the displacement map. Instead, we find the closest original triangle to the subdivided 
vertex, and intersect the ray with the plane containing that triangle. Precise surface trimming can be 
achieved using an alpha mask in the surface color image, but we have not yet implemented this. 4.4 Resampling 
of appearance attributes Besides sampling the scalar displacement function, we also sample other appearance 
attributes such as diffuse color. These attributes are stored, filtered, and compressed just like the 
scalar displacements. An example is shown in Figure 11. 4.5 Filtering of displacement map Since our 
displacement field has the same structure as the domain surface, we can apply the same subdivision mask 
for magnification. This is particular useful when we try to zoom in a tiny region on our displaced subdivision 
surface. For sampling the displacements at minified levels of the displacement pyramid, we compute the 
samples at any level l<k by filtering the limit displacements of level l+1. We considered several filtering 
operations and opted for the non-shrinking filter of Taubin [35]. Because the displacement magnitudes 
are kept small, their filtering is not extremely sensitive. In many rendering situations much of the 
visual detail is provided by bump mapping. As has been discussed elsewhere [2], careful filtering of 
bump maps is both important and difficult. 4.6 Conversion results The following table shows execution 
times for the various steps of the conversion process. These times are obtained on a Pentium III 550 
MHz PC. Model armadillo venus bunny dinosaur Conversion Statistics Original mesh #F 210,944 100,000 
69,451 342,138 Control mesh #F 1,306 748 526 1,564 Maximum level k 4 4 4 4  Execution Times (minutes) 
Simplification 61 28 19 115 Domain surface optimiz. 25 11 11 43 Displacement sampling 2 2 1 5 Total 88 
41 31 163   5. BENEFITS 5.1 Compression Mesh compression has recently been an active area of research. 
Several clever schemes have been developed to concisely encode the combinatorial structure of the mesh 
connectivity, in as few as 1-2 bits per face (e.g. [18] [35]). As a result, the major portion of a compressed 
mesh goes to storing the mesh geometry. Vertex positions are typically compressed using quantization, 
local prediction, and variable-length delta encoding. Geometry can also be compressed within a multiresolution 
subdivision framework as a set of wavelet coefficients [28]. To our knowledge, all previous compression 
schemes for arbitrary surfaces treat geometry as a vector-valued function. In contrast, displaced subdivision 
surfaces allow fine geometric detail to be compressed as a scalar-valued function. Moreover, the domain 
surface is constructed to be close to the original surface, so the magnitude of the displacements tends 
to be small. To exploit spatial coherence in the scalar displacement map, we use linear prediction at 
each level of the displacement pyramid, and encode the difference between the predicted and actual values. 
For each level, we treat the difference coefficients over all faces as a subband. For each subband, we 
use the embedded quantizer and embedded entropy coder described in Taubman and Zakhor [37]. The subbands 
are merged using the bit allocation algorithm described by Shoham and Gersho [34], which is based on 
integer programming. An alternative would be to use the compression scheme of Kolarov and Lynch [24], 
which is a generalization of the wavelet compression method in [33]. Figure 10 and Table 1 show results 
of our compression experiments. We compare storage costs for simplified triangle meshes and displaced 
subdivision surfaces, such that both compressed representations have the same approximation accuracy 
with respect to the original reference model. This accuracy is measured as L2 geometric distance between 
the surfaces, computed using dense point sampling [16]. The simplified meshes are obtained using the 
scheme of Garland and Heckbert [16]. For mesh compression, we use the VRML compressed binary format inspired 
by the work of Taubin and Rossignac [36]. We vary the quantization level for the vertex coordinates to 
obtain different compressed meshes, and then adjust our displacement map compression parameters to obtain 
a displaced surface with matching L2 geometric error. For simplicity, we always compress the control 
meshes losslessly in the experiments (i.e. with 23-bits/coordinate quantization). Our compression results 
would likely be improved further by adapting the quantization of the control mesh as well. However, this 
would modify the domain surface geometry, and would therefore require re-computing the displacement field. 
Also, severe quantization of the control mesh would result in larger displacement magnitudes. Table 1 
shows that displaced subdivision surfaces consistently achieve better compression rates than mesh compression, 
even when the mesh is carefully simplified from detailed geometry. 5.2 Editing The fine detail in the 
scalar displacement mesh can be edited conveniently, as shown in the example of Figure 7. Figure 7: 
In this simple editing example, the embossing effect is produced by enhancing the scalar displacements 
according to a texture image of the character B projected onto the displaced surface. 5.3 Animation 
Displaced subdivision surfaces are a convenient representation for animation. Kinematic and dynamics 
computation are vastly more efficient when operating on the control mesh rather than the huge detailed 
mesh. Because the domain surface is smooth, the surface detail deforms naturally without artifacts. Figure 
8 shows that in contrast, the use of a polyhedron as a domain surface results in creases and folds even 
with a small deformation of a simple surface. Subdivision control mesh Polyhedral control mesh Domain 
surfaces Displaced surfaces Figure 8: Comparison showing the importance of using a smooth domain surface 
when deforming the control mesh. The domain surface is a subdivision surface on the left, and a polyhedron 
on the right. Figure 12 shows two frames from the animation of a more complicated surface. For that example, 
we used 3D Studio MAX to construct a skeleton of bones inside the control mesh, and manipulated the skeleton 
to deform this mesh. (The complete animation is on the accompanying video.) Another application of our 
representation is the fitting of 3D head scans [30]. For this application, it is desirable to re-use 
a common control mesh structure so that deformations can be conveniently transferred from one face model 
to another. 5.4 Scalability Depending on the level-of-detail requirements and hardware capabilities, 
the scalar displacement function can either be: rendered as explicit geometry: Since it is a continuous 
representation, the tessellation is not limited to the resolution of the displacement mesh. A scheme 
for adaptive tessellation is presented in Section 5.5.  converted to a bump map: This improves rendering 
performance on graphics systems where geometry processing is a bottleneck. As described in [31], the 
calculation necessary for tangent-space bump mapping involves computing the displaced subdivision surface 
normal relative to a coordinate frame on the domain surface. A convenient coordinate frame is formed 
by the  domain surface unit normal n.and a tangent vector such as Pu . Given these vectors, the coordinate 
frame is:  tt =P / u ttu . bt,,nt where {}t =×t P bnt t Finally, the normal n s to the displaced subdivision 
surface relative to this tangent space is computed using the transform: A T nA =bt,A ,n A ·nA . {} tangent 
space s P The computations of n , u,and n sare described in Section 3. Note that we use the precise 
analytic normal in the bump map calculation. As an example, Figure 13 shows renderings of the same model 
with different boundaries between explicit geometry and bump mapping. In the leftmost image, the displacements 
are all converted into geometry, and bump-mapping is turned off. In the rightmost image, the domain surface 
is sampled only at the control mesh vertices, but the entire displacement pyramid is converted into a 
bump map. 5.5 Rendering Adaptive tessellation: In order to perform adaptive tessellation, we need to 
compute the approximation error of any intermediate tessellation level from the finely subdivided surface. 
This approximation error is obtained by computing the maximum distance between the dyadic points on the 
planar intermediate level and their corresponding surface points at the finest level (see Figure 9). 
Note that this error measurement corresponds to parametric error and is stricter than geometric error. 
Bounding parametric error is useful for preventing appearance fields (e.g. bump map, color map) from 
sliding over the rendered surface [8]. These precomputed error measurements are stored in a quadtree 
data structure. At runtime, adaptive tessellation prunes off the entire subtree beneath a node if its 
error measurement satisfies given level-of-detail parameters. By default, the displacements applied to 
the vertices of a face are taken from the corresponding level of the displacement pyramid. Note that 
the pruning will make adjacent subtrees meet at different levels. To avoid cracks, if a vertex is shared 
among different levels, we choose the finest one from the pyramid. Also, we perform a retriangulation 
of the coarser face so that it conforms to the vertices along the common edges. Figure 14 shows some 
examples of adaptive tessellation. Figure 9: Error computation for adaptive tessellation. Backface patch 
culling: To improve rendering performance, we avoid rendering regions of the displaced subdivision surface 
that are entirely facing away from the viewpoint. We achieve this using the normal masks technique of 
Zhang and Hoff [38]. On the finely subdivided version of the domain surface, we compute the vertex normals 
of the displaced surface as described in Section 3. We convert these into a normal mask for each subdivided 
face. During a bottom-up traversal of the subdivision hierarchy, we propagate these masks to the parents 
using the logical or operation. Given the view parameters, we then construct a viewing mask as in [38], 
and take its logical and with the stored masks in the hierarchy. Generally, we cull away 1/3 to 1/4 of 
the total number of triangles, thereby speeding up rendering time by 20% to 30%.  6. DISCUSSION Remeshing 
creases: As in other remeshing methods [14] [26], the presence of creases in the original surface presents 
challenges to our conversion process. Lee et al. [26] demonstrate that the key is to associate such creases 
with edges in the control mesh. Our simplification process also achieves this since mesh simplification 
naturally preserves sharp features. However, displaced subdivision surfaces have the further constraint 
that the displacements are strictly scalar. Therefore, the edges of the control mesh, when subdivided 
and displaced, do not generally follow original surface creases exactly. (A similar problem also arises 
at surface boundaries.) This problem can be resolved if displacements were instead vector-based, but 
then the representation would lose its simplicity and many of its benefits (compactness, ease of scalability, 
etc.). Scaling of displacements: Currently, scalar displacements are simply multiplied by unit normals 
on the domain surface. With a rubbery surface, the displaced subdivision surface behaves as one would 
expect, since detail tends to smooth as the surface stretches. However, greater control over the magnitude 
of displacement is desirable in many situations. A simple extension of the current representation is 
to provide scale and bias factors (, )at control mesh vertices. These added controls enhance the sb basic 
displacement formula: SP (sD + =+ b)n Exploring such scaling controls is an interesting area of future 
work.  7. SUMMARY AND FUTURE WORK Nearly all geometric representations capture geometric detail as a 
vector-valued function. We have shown that an arbitrary surface can be approximated by a displaced subdivision 
surface, in which geometric detail is encoded as a scalar-valued function over a domain surface. Our 
representation defines both the domain surface and the displacement function using a unified subdivision 
framework. This synergy allows simple and efficient evaluation of analytic surface properties. We demonstrated 
that the representation offers significant savings in storage compared to traditional mesh compression 
schemes. It is also convenient for animation, editing, and runtime level-of­detail control. Areas for 
future work include: a more rigorous scheme for constructing the domain surface, improved filtering of 
bump maps, hardware rendering, error measures for view-dependent adaptive tessellation, and use of detail 
textures for displacements. ACKNOWLEDGEMENTS Our thanks to Gene Sexton for his help in scanning the dinosaur. 
REFERENCES [1] Apodaca, A. and Gritz, L. Advanced RenderMan Creating CGI for Motion Pictures, Morgan 
Kaufmann, San Francisco, CA, 1999. [2] Becker, B. and Max, N. Smooth transitions between bump rendering 
algorithms. Proceedings of SIGGRAPH 93, Computer Graphics, Annual Conference Series, pp. 183-190. [3] 
Blinn, J. F. Simulation of wrinkled surfaces. Proceedings of SIGGRAPH 78, Computer Graphics, pp. 286-292. 
[4] Cabral, B., Max, N. and Springmeyer, R. Bidirectional reflection functions from surface bump maps. 
Proceedings of SIGGRAPH 87, Computer Graphics, Annual Conference Series, pp.273-281. [5] Catmull, E., 
and Clark, J. Recursively generated B-spline surfaces on arbitrary topological meshes. Computer Aided 
Design 10, pp. 350-355 (1978). [6] Certain,A., Popovic, J.,DeRose, T.,Duchamp,T., Salesin, D. and Stuetzle, 
W. Interactive multiresolution surface viewing. Proceedings of SIGGRAPH 96, Computer Graphics, Annual 
Conference Series, pp. 91-98. [7] Chan, K., Mann, S., and Bartels, R. World space surface pasting. Graphics 
Interface '97, pp. 146-154. [8] Cohen, J., Olano, M. and Manocha, D. Appearance preserving Simplification. 
Proceedings of SIGGRAPH 98, Computer Graphics, Annual Conference Series, pp. 115-122. [9] Cook, R. Shade 
trees. Computer Graphics (Proceedings of SIGGRAPH 84), 18(3), pp. 223-231. [10] Deering, M. Geometry 
compression. Proceedings of SIGGRAPH 95, Computer Graphics, Annual Conference Series, pp. 13-20. [11] 
DeRose, T., Kass, M., and Truong, T. Subdivision surfaces in character animation. Proceedings of SIGGRAPH 
98, Computer Graphics, Annual Conference Series, pp. 85-94. [12] Do Carmo, M. P. Differential Geometry 
of Curves and Surfaces. Prentice-Hall, Inc., Englewood Cliffs, New Jersey, 1976. [13] Doo, D., and Sabin, 
M. Behavior of recursive division surfaces near extraordinary points. Computer Aided Design 10, pp. 356-360 
(1978). [14] Eck, M., DeRose, T., Duchamp, T., Hoppe, H., Lounsbery, M., and Stuetzle, W. Multiresolution 
analysis of arbitrary meshes. Proceedings of SIGGRAPH 95, Computer Graphics, Annual Conference Series, 
pp. 173-182. [15] Forsey, D., and Bartels, R. Surface fitting with hierarchical splines. ACM Transactions 
on Graphics, 14(2), pp. 134-161 (April 1995). [16] Garland, M., and Heckbert, P. Surface simplification 
using quadric error metrics. Proceedings of SIGGRAPH 97, Computer Graphics, Annual Conference Series, 
pp. 209-216. [17] Gottschalk, S., Lin, M., and Manocha, D. OBB-tree: a hierarchical structure for rapid 
interference detection. Proceedings of SIGGRAPH 96, Computer Graphics, Annual Conference Series, pp. 
171-180. [18] Gumhold, S., and Straßer, W. Real time compression of triangle mesh connectivity. Proceedings 
of SIGGRAPH 98, Computer Graphics, Annual Conference Series, pp. 133-140. [19] Gumhold, S., and Hüttner, 
T. Multiresolution rendering with displacement mapping. SIGGRAPH workshop on Graphics hardware, Aug 8-9, 
1999. [20] Guskov, I., Vidimce, K., Sweldens, W., and Schröder, P. Normal meshes. Proceedings of SIGGRAPH 
2000, Computer Graphics, Annual Conference Series. [21] Hoppe, H., DeRose, T., Duchamp, T., Halstead, 
M., Jin, H., McDonald, J., Schweitzer, J., and Stuetzle, W. Piecewise smooth surface reconstruction. 
Proceedings of SIGGRAPH 94, Computer Graphics, Annual Conference Series, pp. 295-302. [22] Hoppe, H. 
Progressive meshes. Proceedings of SIGGRAPH 96, Computer Graphics, Annual Conference Series, pp. 99-108. 
[23] Kobbelt, L., Bareuther, T., and Seidel, H. P. Multi-resolution shape deformations for meshes with 
dynamic vertex connectivity. Proceedings of EUROGRAPHICS 2000, to appear. [24] Kolarov, K. and Lynch, 
W. Compression of functions defined on surfaces of 3D objects. In J. Storer and M. Cohn, editors, Proc. 
of Data Compression Conference, IEEE, pp. 281-291, 1997. [25] Krishnamurthy, V., and Levoy, M. Fitting 
smooth surfaces to dense polygon meshes. Proceedings of SIGGRAPH 96, Computer Graphics, Annual Conference 
Series, pp. 313-324. [26] Lee, A., Sweldens, W., Schröder, P., Cowsar, L., and Dobkin, D. MAPS: Multiresolution 
adaptive parameterization of surfaces. Proceedings of SIGGRAPH 98, Computer Graphics, Annual Conference 
Series, pp. 95-104. [27] Loop, C. Smooth subdivision surfaces based on triangles. Master s thesis, University 
of Utah, Department of Mathematics, 1987. [28] Lounsbery, M., DeRose, T., and Warren, J. Multiresolution 
analysis for surfaces of arbitrary topological type. ACM Transactions on Graphics, 16(1), pp. 34-73 (January 
1997). [29] Mann, S. and Yeung, T. Cylindrical surface pasting. Technical Report, Computer Science Dept., 
University of Waterloo (June 1999). [30] Marschner, S., Guenter, B., and Raghupathy, S. Modeling and 
rendering for realistic facial animation. Submitted for publication. [31] Peercy, M., Airey, J. and Cabral, 
B. Efficient bump mapping hardware. Proceedings of SIGGRAPH 97, Computer Graphics, Annual Conference 
Series, pp. 303-306. [32] Peters, J. Local smooth surface interpolation: a classification. Computer Aided 
Geometric Design, 7(1990), pp. 191-195. [33] Schröder, P., and Sweldens, W. Spherical wavelets: efficiently 
representing functions on the sphere. Proceedings of SIGGRAPH 95, Computer Graphics, Annual Conference 
Series, pp. 161-172. [34] Shoham, Y. and Gersho, A. Efficient bit allocation for an arbitrary set of 
quantizers. IEEE Transactions on Acoustics, Speech, and Signal Processing, Vol. 36, No. 9, pp. 1445-1453, 
Sept 1988. [35] Taubin, G. A signal processing approach to fair surface design. Proceedings of SIGGRAPH 
95, Computer Graphics, Annual Conference Series, pp. 351-358. [36] Taubin, G. and Rossignac, J. Geometric 
compression through topological surgery. ACM Transactions on Graphics, 17(2), pp. 84­115 (April 1998). 
[37] Taubman, D. and Zakhor, A. Multirate 3-D subband coding of video. IEEE Transactions on Image Processing, 
Vol. 3, No. 5, Sept, 1994. [38] Zhang, H., and Hoff, K. Fast backface culling using normal masks. Symposium 
on Interactive 3D Graphics, pp. 103-106, 1997. [39] Zorin, D., Schröder, P., and Sweldens, W. Interactive 
multiresolution mesh editing. Proceedings of SIGGRAPH 97, Computer Graphics, Annual Conference Series, 
pp. 259-268.  Original mesh Simplified mesh Compressed simplified mesh Displaced subdivision surface 
342,138 faces; 1011 KB 50,000 faces; 169 KB (12-bits/coord.); 68 KB 1564 control mesh faces; 18 KB Original 
mesh Simplified mesh Compressed simplified mesh Displaced subdivision surface 100,000 faces; 346 KB 20,000 
faces; 75 KB (12-bits/coord.); 33 KB 748 control mesh faces; 16 KB Figure 10: Compression results. Each 
example shows the approximation of a dense original mesh using a simplified mesh and a displaced subdivision 
surface, such that both have comparable L2 approximation error (expressed as a percentage of object bounding 
box). Dinosaur Original mesh Compressed simplified mesh Displaced subdivision surface (k=4) #V=171,074 
#V=25,005 #V0=787 #F=342,138 #F=50,000 #F0=1564 = 6.5KB Quantization (bits/coord.) L2 error Size (KB) 
L2 error Size (KB) L2 error Size (KB) Size ratio 23 0.002% 1011 0.024% 169 0.025% 22 7.7 12 0.014% 322 
0.028% 68 0.028% 18 3.8 10 0.053% 217 0.059% 50 0.058% 10 5.0 8 0.197% 169 0.21% 35 0.153% 7 5.0 Venus 
Original mesh Compressed simplified mesh Displaced subdivision surface (k=4) #V=50,002 #V=10,002 #V0=376 
#F=100,000 #F=20,000 #F0=748 = 3.4KB Quantization (bits/coord.) L2 error Size (KB) L2 error Size (KB) 
L2 error Size (KB) Size ratio 23 0.001% 346 0.027% 75 0.027% 17 4.4 12 0.014% 140 0.030% 33 0.031% 16 
2.0 10 0.054% 102 0.059% 26 0.053% 8 3.2 8 0.207% 69 0.210% 18 0.149% 4 4.5 Table 1: Quantitative compression 
results for the two examples in Figure 10. Numbers in red refer to figures above. Original colored mesh 
Displaced subdivision surface Domain surface Displacement samples (k=4) Figure 11: Example of a displaced 
subdivision surface with resampled color. Original mesh Control mesh Displaced subdiv. surface Modified 
control mesh Resulting deformed surface Figure 12: The control mesh makes a convenient armature for animating 
the displaced subdivision surface. Level 4 (134,656 faces) Level 3 (33,664 faces) Level 2 (8,416 faces) 
Level 1 (2,104 faces) Level 0 (526 faces) Figure 13: Replacement of scalar displacements by bump-mapping 
at different levels. Threshold = 1.87% diameter Threshold = 0.76% diameter Threshold = 0.39% diameter 
12,950 triangles; L2 error = 0.104% 88,352 triangles; L2 error = 0.035% 258,720 triangles; L2 error = 
0.016% Figure 14: Example of adaptive tessellation, using the view-independent criterion of comparing 
residual error with a global threshold.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344831</article_id>
		<sort_key>95</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Normal meshes]]></title>
		<page_from>95</page_from>
		<page_to>102</page_to>
		<doi_number>10.1145/344779.344831</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344831</url>
		<abstract>
			<par><![CDATA[<p>Normal meshes are new fundamental surface descriptions inspired by differential geometry. A normal mesh is a multiresolution mesh where each level can be written as a normal offset from a coarser version. Hence the mesh can be stored with a single float per vertex. We present an algorithm to approximate any surface arbitrarily closely with a normal semi-regular mesh. Normal meshes can be useful in numerous applications such as compression, filtering, rendering, texturing, and modeling.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[irregular connectivity]]></kw>
			<kw><![CDATA[meshes]]></kw>
			<kw><![CDATA[multiresolution]]></kw>
			<kw><![CDATA[subdivision]]></kw>
			<kw><![CDATA[surface parameterization]]></kw>
			<kw><![CDATA[wavelets]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.2</cat_node>
				<descriptor>Approximation of surfaces and contours</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003624.10003633.10010918</concept_id>
				<concept_desc>CCS->Mathematics of computing->Discrete mathematics->Graph theory->Approximation algorithms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39072806</person_id>
				<author_profile_id><![CDATA[81100021939]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Igor]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guskov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Caltech]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P161907</person_id>
				<author_profile_id><![CDATA[81100046507]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kiril]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vidim&#269;e]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mississippi State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P300467</person_id>
				<author_profile_id><![CDATA[81100340025]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Wim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sweldens]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bell Laboratories]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40023875</person_id>
				<author_profile_id><![CDATA[81100117380]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schr&#246;der]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Caltech]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>237213</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[CERTAIN, A., POPOVIC, J., DEROSE, T., DtJCHAMP, T., SALESIN, D., AND STUETZLE, W. Interactive Multiresolution Surface Viewing. Proceedings of S1GGRAPH 96 (1996), 91 98.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280832</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[COHEN, J., OLANO, M., AND MANO( HA, D. Appearance-Preserving Simplification. Proceedings of S1GGRA PH 98 (1998), 115122.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808602</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[COOK, R. L. Shade trees. Computer Graphics (Proceedings of S1GGRAPH84) 8, 3 (1984), 223 231.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[DAUBECtlIES, I., GUSKOV, I., AND SWELDENS, W. Regularity of Irregular Subdivision. Constr. Approx. 15 (1999), 381426.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311576</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[DESBRUN, M., MEYER, M., SCIIRODER, P., AND BARR, A. H. Implicit Fairing of Irregular Meshes Using Diffusion and Curvature Flow. Proceedings of" S1GGRAPH 99 (1999), 317324.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[DONOHO, D. L. Interpolating wavelet transforms. Preprint, Department of Statistics, Stanford University, 1992.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[DONOtlO, D. L. Unconditional Bases are Optimal Bases for Data Compression and for Statistical Estimation. AppL Comput. Harmon. Anal l (1993), 100115.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>78958</ref_obj_id>
				<ref_obj_pid>78956</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[DYN, N., LEVIN, D., AND GREGORY, J. A. A Butterfly Subdivision Scheme for Surface Interpolation with Tension Control. ACM Transactions on Graphics 9, 2 (1990), 160169.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218440</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[ECK, M., DEROSE, T., DUCIIAMP, T., HOPPE, H., LOUNSBERY, M., AND STUETZLE, W. Multiresolution Analysis of Arbitrary Meshes. Proceedings" of S1GGRAPH 95 (1995), 173 182.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>248308</ref_obj_id>
				<ref_obj_pid>248299</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[FLOATER, M. S. Pammeterization and Smooth Appmximation of Surface Triangulations. Computer Aided Geometric Design l 4 (1997), 231 250.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258849</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[GARLAND, M., AND HECKBERT, P. S. Surface Simplification Using Quadric Error Metrics. In Proceedings of S1GGRAPH 96, 209216, 1996.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>248979</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[GOLUB, G. H., AND LOAN, C. F. V. Matrix Computations, 2nd ed. The John Hopkins University Press, Baltimore, 1983.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311577</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[GUSKOV, 1., SWELDENS, W., AND SCtlRODER, P. Multiresolution Signal Processing for Meshes. Proceedings of S1GGRAPH 99 (1999), 325334.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>344922</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[KIIODAKOVSKY; A., SCtlRODER, P., SWELDENS, W. Progressive Geometry Compression. Proceedings q/'SIGGRAPH 2000 (2000).]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237270</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[KRIStlNAMURrtlY, V., AND LEVOY, M. Fitting Smooth Surfaces to Dense Polygon Meshes. Proceedings ofS1GGRA PH 96 (1996), 313 324.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311586</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[LEE, A. W. F., DOBK{N, D., SWELDENS, W., AND SCHRODER, P. Multiresolution Mesh Morphing. Proceedings of S1GGRAPH 99 (1999), 343350.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>344829</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[LEE, A. W. F., MORETON, H., HOPPE, H. Displaced Subdivision Surfaces. Proceedings of SIGGRAPH O0 (2000).]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280828</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[LEE, A. W. F., NWELDENS, W., SCIIRODER, P., COWSAR, L., AND DOBKIN, D. MAPS: Multiresolution Adaptive Pammeterization of Surfaces. Proceedings oJ'S1GGRAPH 98 (1998), 95104.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1889714</ref_obj_id>
				<ref_obj_pid>1889712</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[LEVOY, M. The Digital Michelangelo Project. In Proceedings" of the 2nd International Co@rence on 3D Digital Imaging and Modeling, October 1999.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280930</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[LEVY, B., AND MALLET, J. Non-Distorted Texture Mapping for Sheared Triangulated Meshes. Proceedings q/'S1GGRAPH 98 (1998), 343352.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237750</ref_obj_id>
				<ref_obj_pid>237748</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[LOUNSBERY, M., DEROSE, T. D., AND WARREN, J. Multiresolution Analysis for Surfaces of Arbitrary Topological Type. ACM Transactions on Graphics 16, 1 (1997), 3473. Originally available as TR-93-10-05, October, 1993, Department of Computer Science and Engineering, University of Washington.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218439</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[SC~tRODER, R, AND SWELDENS, W. Spherical Wavelets: Efficiently Representing Functions on the Sphere. Proceedings of S1GGRAPH 95 (1995), 161 .... 172.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[ZORIN, D., AND S Ct{RODER, P., Eds. Subdivision for Modeling and Animation. Course Notes. ACM SIGGRAPH, 1999.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237254</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[ZORIN, D., SClIRODER, P., AND SWELDENS, W. Interpolating Subdivision for Meshes with Arbitrary Topology. Proceedings of S1GGRAPH 96 (1996), 189192.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258863</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[ZORIN, D., S(HR()DER, P., AND SWELDENS, W. Interactive Multiresolution Mesh Editing. Proceedings of S1GGRAPH 97 (1997), 259268.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Normal Meshes Igor Guskov Kiril Vidim.ce Wim Sweldens Peter Schr¨oder Caltech Mississippi State University 
Bell Laboratories Caltech  Figure 1: Left: original mesh (3 .oats/vertex). Middle: two stages of our 
algorithm. Right: normal mesh (1 .oat/vertex). (Skull dataset courtesy Headus, Inc.) Abstract Normal 
meshes are new fundamental surface descriptions inspired by differential geometry. A normal mesh is a 
multiresolution mesh where each level can be written as a normal offset from a coarser version. Hence 
the mesh can be stored with a single .oat per ver­tex. We present an algorithm to approximate any surface 
arbitrarily closely with a normal semi-regular mesh. Normal meshes can be useful in numerous applications 
such as compression, .ltering, ren­dering, texturing, and modeling. CR Categories and Subject Descriptors: 
I.3.5 [Computer Graphics]: Computa­tional Geometry and Object Modeling -curve, surface, solid, and object 
representa­tions; hierarchy and geometric transformations; G.1.2 [Numerical Analysis]: Ap­proximation 
-approximation of surfaces and contours, wavelets and fractals Additional Keywords: Meshes, subdivision, 
irregular connectivity, surface parame­terization, multiresolution, wavelets. 1 Introduction The standard 
way to parameterize a surface involves three scalar functions x(u, v), y(u, v), z(u, v). Yet differential 
geometry teaches us that smooth surfaces locally can be described by a single scalar height function 
over the tangent plane. Loosely speaking one can say that the geometric information of a surface can 
be contained Permission to make digital or hard copies of part or all of this work or personal or classroom 
use is granted without fee provided that copies are not made or distributed for profit or commercial 
advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, 
to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or 
a fee. SIGGRAPH 2000, New Orleans, LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 in only a 
single dimension, the height over this plane. This obser­vation holds in.nitesimally; only special cases 
such as terrains and star-shaped surfaces can globally be described with a single func­tion. In practice 
we often approximate surfaces using a triangle mesh. While describing meshes is relatively easy, they 
have lost much of the structure inherent in the original surface. For example, the above observation 
that locally a surface can be characterized by a scalar function is not re.ected in the fact that we 
store 3 .oats per vertex. In other words, the correlation between neighboring sample locations implied 
by the smoothness assumption is not re.ected, leading to an inherently redundant representation. While 
vertex locations come as 3-dimensional quantities, the above considerations tell us that locally two 
of those dimensions represent parametric information and only the third captures geo­metric, or shape, 
information. For a given smooth shape one may choose different parameterizations, yet the geometry remains 
the same. In the case of a mesh we can observe this by noticing that in.nitesimal tangential motion of 
a vertex does not change the ge­ometry, only the sampling pattern, or parameterization. Moving in the 
normal direction on the other hand changes the geometry and leaves parameter information undisturbed. 
1.1 Goals and Contributions Based on the above observations, the aim of the present paper is to compute 
mesh representations that only require a single scalar per vertex. We call such representations normal 
meshes. The main in­sight is that this can be done using multiresolution and local frames. A normal mesh 
has a hierarchical representation so that all detail coef.cients when expressed in local frames are scalar, 
i.e., they only have a normal component. In the context of compression, for ex­ample, this implies that 
parameter information can be perfectly pre­dicted and residual error is entirely constrained to the normal 
direc­tion, i.e., contains only geometric information. Note that because of the local frames normal mesh 
representations are non-linear. Of course we cannot expect a given arbitrary input mesh to pos­sess a 
hierarchical representation which is normal. Instead we de­scribe an algorithm which takes an arbitrary 
topology input mesh and produces a semi-regular normal mesh describing the same ge­ometry. Aside from 
a small amount of base domain information, our normal mesh transform converts an arbitrary mesh from 
a 3 parameter representation into a purely scalar representation.We demonstrate our algorithm by applying 
it to a number of models and experimentally characterize some of the properties which make normal meshes 
so attractive for computations. The study of normal meshes is of interest for a number of rea­sons: they 
 bring our computational representations back towards the .rst principles of differential geometry; 
 are very storage and bandwidth ef.cient, describing a surface as a succinctly speci.ed base shape plus 
a hierarchical normal map;  are an excellent representation for compression since all vari­ance is squeezed 
into a single dimension.  1.2 Related Work Ef.cient representations for irregular connectivity meshes 
have been pursued by a number of researchers. This research is mo­tivated by our ability to acquire densely 
sampled, highly detailed scans of real world objects [19] and the need to manipulate these ef­.ciently. 
Semi-regular or subdivision connectivity meshes offer many advantages over the irregular setting due 
of their well devel­oped mathematical foundations and data structure simplicity [23]; many powerful algorithms 
require their input to be in semi-regular form [21, 22, 25, 1]. This has led to the development of a 
number of algorithms to convert existing irregular meshes to semi-regular form through remeshing. Eck 
et al. [9] use Voronoi tiling and har­monic maps to build a parameterization and remesh onto a semi­regular 
mesh. Krischnamurthy and Levoy [15] demonstrated user driven remeshing for the case of bi-cubic patches, 
while Lee et al. [18] proposed an algorithm based on feature driven mesh reduc­tion to develop smooth 
parameterizations of meshes in an automatic fashion. These methods use the parameterization subsequently 
for semi-regular remeshing. Our work is related to these approaches in that we also construct a semi-regular 
mesh from an arbitrary connectivity input mesh. However, in previous work prediction residuals, or detail 
vectors, were not optimized to have properties such as normality. The main focus was on the establishment 
of a smooth parameterization which was then semi-regularly sampled. The discussion of parameter versus 
geometry information orig­inates in the work done on irregular curve and surface subdivi­sion [4] [13] 
and intrinsic curvature normal .ow [5]. There it is shown that unless one has the correct parameter side 
information, it is not possible to build an irregular smooth subdivision scheme. While such schemes are 
useful for editing and texturing applica­tions, they cannot be used for succinct representations because 
the parameter side-information needed is excessive. In the case of nor­mal meshes these issues are entirely 
circumvented in that all pa­rameter information vanishes and the mesh is reduced to purely ge­ometric, 
i.e., scalar in the normal direction, information. Finally, we mention the connection to displacement 
maps [3], and in particular normal displacement maps. These are popular for modeling purposes and used 
extensively in high end render­ing systems such as RenderMan. In a sense we are solving here the associated 
inverse problem. Given some geometry, .nd a sim­pler geometry and a set of normal displacements which 
together are equivalent to the original geometry. Typically, normal displacement maps are single level, 
whereas we aim to build them in a fully hi­erarchical way. For example, single level displacements maps 
were used in [15] to capture the .ne detail of a 3D photography model. Cohen et al. [2] sampled normal 
.elds of geometry and maintained these in texture maps during simpli.cation. While these approaches all 
differ signi.cantly from our interests here, it is clear that maps of this and related nature are of 
great interest in many contexts. In independent work, Lee et al. pursue a goal similar to ours [17]. 
They introduce displaced subdivision surfaces which can be seen as a two level normal mesh. Because only 
two levels are used, the base domain typically contains more triangles than in our case. Also the normal 
offsets are oversampled while in our case, the normal offsets are critically sampled.  2 Normal Polylines 
Before we look at surfaces and normal meshes, we introduce some of the concepts using curves and normal 
polylines. A curve in the plane is described by a pair of parametric functions s(t)= (x(t),y(t)) with 
t . [0,1]. We would like to describe the points on the curve with a single scalar function. In practice 
one uses poly­lines to approximate the function. Let l(p,p ) be the linear segment between the points 
p and p . . A standard way to build a polyline multiresolution approximation is to sample the curve at 
points sj,k where sj,k = sj+1,2k and de.ne the jth level approximation as  Lj = l(sj,k,sj,k+1). 0=k<2j 
To move from Lj to Lj+1 we need to insert the points sj+1,2k+1 (Figure 2, left). Clearly this requires 
two scalars: the two coordi­nates of sj+1,2k+1. Alternatively one could compute the difference sj+1,2k+1 
- m between the new point and some predicted point m, say the midpoint of the neighboring points sj,k 
and sj,k+1. This detail has a tangential component m - b and a normal com­ponent b - sj+1,2k+1. The normal 
component is the geometric information while the tangential component is the parameter infor­mation. 
The way to build polylines that can be described with one sj+1,2k+1 sj,k+1sj,k+1 Figure 2: Removing 
one point sj+1,2k+1 in a polyline multiresolu­tion and recording the difference with the midpoint m. 
On the left a general polyline where the detail has both a normal and a tangen­tial component. On the 
right a normal polyline where the detail is purely normal. scalar per point, is to make sure that the 
parameter information is always zero, i.e., b = m, see Figure 2, right. If the triangle sj,k, sj+1,2k+1, 
sj,k+1 is Isosceles, there is no parameter information. Consequently we say that a polyline is normal 
if a multiresolution structure exists where every removed point forms an Isosceles trian­gle with its 
neighbors. Then there is zero parameter information and the polyline can be represented with one scalar 
per point, namely the normal component of the associated detail. For a general polyline the removed triangles 
are hardly ever ex­actly Isosceles and hence the polyline is not normal. Below we describe a procedure 
to build a normal polyline approximation for any continuous curve. The easiest is to start building Isosceles 
tri­angles from the coarsest level. Start with the .rst base l(s0,0,s0,1), see Figure 3. Next take its 
midpoint and check where the normal direction crosses the curve. Because the curve is continuous, there 
has to be at least one such point. If there are multiple pick any one. 3,3 2,1 0,1 Figure 3: Construction 
of a normal polyline. We start with the coarsest level and each time check where the normal to the midpoint 
crosses the curve. For simplicity only the indices of the sj,k points are shown and only certain segments 
are subdivided. The polyline (0,0)-(2,1)-(3,3)-(1,1)-(0,1) is determined by its endpoints and three scalars, 
the heights of the Isosceles triangles. Call this point s1,1 and de.ne the .rst triangle. Now split the 
curve into two parts and repeat the procedure on each subcurve. Each time sj+1,2k+1 is found where the 
normal to the midpoint of sj,k and sj,k+1 crosses the portion of the curve between sj,k and sj,k+1. Thus 
any continuous curve can be approximated arbitrarily closely with a normal polyline. The result is a 
series of polylines Lj all of which are normal with respect to midpoint prediction. Effectively each 
level is parameterized with respect to the one coarser level. Because the polylines are normal, only 
a single scalar value, the normal component, needs to be recorded for each point. We have a polyline 
with no parameter information. One can also consider normal polylines with respect to fancier predictors. 
For example one could compute a base point and nor­mal estimate using the well known 4 point rule. Essentially 
any predictor which only depends on the coarser level is allowed. For example one can also use irregular 
schemes [4]. Also one does not need to follow the standard way of building levels by downsam­pling every 
other point, but instead could take any ordering. This leads to the following de.nition of a normal polyline: 
De.nition 1 A polyline is normal if a removal order of the points exists such that each removed point 
lies in the normal direction from a base point, where the normal direction and base point only de­pend 
on the remaining points. Hence a normal polyline is completely determined by a scalar com­ponent per 
vertex. Normal polylines are closely related to certain well known frac­tal curves such as the Koch Snow.ake1, 
see Figure 4. Here each time a line segment is divided into three subsegments. The left and right get 
a normal coef.cient of zero, while the middle receives a normal coef.cient such that the resulting triangle 
is equilateral. Hence the polylines leading to the snow.ake are normal with re­spect to midpoint subdivision. 
 Figure 4: Four normal polylines converging to the Koch snow.ake. 1Niels Fabian Helge von Koch (Sweden, 
1870-1924) There is also a close connection with wavelets. The normal co­ef.cients can be seen as a piecewise 
linear wavelet transform of the original curve. Because the tangential components are always zero there 
are half as many wavelet coef.cients as there are origi­nal scalar coef.cients. Thus one saves 50% memory 
right away. In addition of course the wavelets have their usual decorrelation prop­erties. In the functional 
case the above transform corresponds to an unlifted interpolating piecewise linear wavelet transform 
as intro­duced by Donoho [6]. There it is shown that interpolating wavelets with no primal, but many 
dual moments are well suited for smooth functions. Unlike in the function setting, not all wavelets from 
the same level j have the same physical scale. Here the scale of each coef.cient is essentially the length 
of the base of its Isosecles trian­gle.  3 Normal Meshes We begin by establishing terminology. A triangle 
mesh Mis a pair (P,K), where Pis a set of N point positions P = {pi = (xi,yi,zi) .R3 |1 =i =N}, and Kis 
an abstract simplicial complex which contains all the topological, i.e., adjacency infor­mation. The 
complex Kis a set of subsets of {1,...,N}. These subsets come in three types: vertices {i}, edges {i,j}, 
and faces {i,j,k}. Two vertices i and j are neighbors if {i,j}.E. The 1-ring neighbors of a vertex iform 
a set V(i)= {j |{i,j}.E}. We can derive a de.nition of normal triangle meshes inspired by the curve case. 
Consider a hierarchy of triangle meshes Mj built using mesh simpli.cation with vertex removals. These 
meshes are nested in the sense that Pj .Pj+1. Take a removed vertex pi .Pj+1 \Pj. For the mesh to be 
normal we need to be able to .nd a base point b and normal direction N that only depend on Pj, so that 
pi -b lies in the direction N. This leads to the following de.nition. De.nition 2 A mesh Mis normal in 
case a sequence of vertex removals exists so that each removed vertex lies on a line de.ned by a base 
point and normal direction which only depends on the remaining vertices. Thus a normal mesh can be described 
by a small base domain and one scalar coef.cient per vertex. As in the curve case, a mesh is in general 
not normal. The chance that the difference between a removed point and a predicted base point lies exactly 
in a direction that only depends on the remaining vertices is essentially zero. Hence the only way to 
obtain a normal mesh is to change the triangulation. We decide to use semi-regular meshes, i.e., meshes 
whose connectivity is formed by successive quadrisection of coarse base domain faces. As in the curve 
setting, the way to build a normal mesh is to start from the coarse level or base domain. For each new 
vertex we compute a base point as well as a normal direction and check where the line de.ned by the base 
point and normal intersects the surface. The situation, however, is much more complex than in the curve 
case for two reasons: (1) There could be no intersection point. (2) There could be many intersection 
points, but only one correct one. In case there are no intersection points, strictly speaking no fully 
normal mesh can be built from this base domain. If that happens, we relax the de.nition of normal meshes 
some and allow a small number of cases where the new points do not lie in the normal di­rection. Thus 
the algorithm needs to .nd a suitable non-normal lo­cation for the new point. In case there are many 
intersection points the algorithm needs to .gure out which one is the right one. If the wrong one is 
chosen the normal mesh will start folding over itself or leave creases. Any algorithm which blindly picks 
an intersection point is doomed. Parameterization In order to .nd the right piercing point or suggest 
a good alternate, one needs to be able to easily navigate around the surface. The way to do this is to 
build a smooth pa­rameterization of the surface region of interest. This is a basic building block of 
our algorithm. Several parameterization meth­ods have been proposed and our method takes components from 
each of them: mesh simpli.cation and polar maps from MAPS [18], patchwise relaxation from [9], and a 
speci.c smoothness functional similar to the one used in [10] and [20]. The algorithm will use lo­cal 
parameterizations which need to be computed fast and robustly. Most of them are temporary and are quickly 
discarded unless they can be used as a starting guess for another parameterization. Consider a region 
Rof the mesh homeomorphic to a disc that we want to parameterize onto a convex planar region B, i.e., 
.nd a bijective map u :R.B. The map u is .xed by a boundary con­dition .R..Band minimizes a certain energy 
functional. Sev­eral functionals can be used leading to, e.g., conformal or harmonic mappings. We take 
an approach based on the work of Floater [10]. In short, the function u needs to satisfy the following 
equation in the interior: u(pi)= aiku(pk), (1) k.V(i) where V(i) is the 1-ring neighborhood of the vertex 
i and the weights aik come from the shape-preserving parameterization scheme [10]. The main advantage 
of the Floater weights is that they are always positive, which, combined with the convexity of the parametric 
region, guarantees that no triangle .ipping can oc­cur within the parametric domain. This is crucial 
for our algorithm. Note that this is not true in general for harmonic maps which can have negative weights. 
We use the iterative biconjugate gradient method [12] to obtain the solution to the system (1). Given 
that we often have a good starting guess this converges quickly. Algorithm Our algorithm consists of 
7 stages which are de­scribed below, some of which are shown for the molecule model in Figure 5. The 
molecule is a highly detailed and curved model. Any naive procedure for .nding normal meshes is very 
unlikely to succeed. The .rst four stages of the algorithm prepare the ground for the piercing procedure 
and build the net of curves splitting the original mesh into triangular patches that are in one-to-one 
correspondence with the faces of the base mesh, i.e., the coarsest level of the semi­regular mesh we 
build. 1. Mesh simpli.cation: We use the Garland-Heckbert [11] simpli.cation based on half-edge collapses 
to create a mesh hierar­chy (Pj, Kj). We use the coarsest level (P0, K0)as an initial guess for our base 
domain (Q0, K0). The .rst image of Figure 5 shows the base domain for the molecule. 2. Building an initial 
net of curves: The purpose of this step is to connect the vertices of the base domain with a net of non 
in­tersecting curves on the different levels of the mesh simpli.cation hierarchy. This can easily be 
done using the MAPS parameteri­zation [18]. MAPS uses polar maps to build a bijection between a 1-ring 
and its retriangulation after the center vertex is removed. The concatenation of these maps is a bijective 
mapping between differ­ent levels (Pj, Kj)in the hierarchy. The desired curves are simply the image of 
the base domain edges under this mapping. Because of the bijection no intersection can occur. Note that 
the curves start and .nish at a vertex of the base domain, but need not follow the edges of the .ner 
triangulation, i.e., they can cut across triangles. These curves de.ne a network of triangular shaped 
patches corre­sponding to the base domain triangles. Later we will adjust these curves on some intermediate 
level and again use MAPS to propa­gate these changes to other levels. The top middle image of Figure 
5 shows these curves for some intermediate level of the hierarchy.  3. Fixing the global vertices: A 
normal mesh is almost com­pletely determined by the base domain. One has to choose the base domain vertices 
Q0 very carefully to reduce the number of non­normal vertices to a minimum. The coarsest level of the 
mesh sim­pli.cation P0 is only a .rst guess. In this section we describe a procedure for repositioning 
the global vertices qi with {i}.K0. We impose the constraint that the qi needs to coincide with some 
vertex pk of the original mesh, but not necessarily pi. The repositioning is typically done on some intermediate 
level j. Take a base domain vertex qi. We build a parameterization from the patches incident to vertex 
qi to a disk in the plane, see Fig­ure 6. Boundary conditions are assigned using arclength parame­terization, 
and parameter coordinates are iteratively computed for each level j vertex inside the shaded region. 
It is now easy to re­place the point qi with any level point from Pj in the shaded region. In particular 
we let the new qi . be the point of Pj that in the param­eter domain is closest to the center of the 
disk. The exact center of the disk, in general, does not correspond to a vertex of the mesh. Once a new 
position qi . is chosen, the curves can be redrawn by taking the inverse mapping of straight lines from 
the new point in the parameter plane. One can keep iterating this procedure, but we found that if suf.ces 
to cycle once through all base domain vertices. We also provide for a user controlled repositioning. 
Then the user can replace the center vertex with any Pj point in the shaded region. The algorithm again 
uses the parameterization to recompute the curves from that point. The top right of Figure 5 shows the 
repositioned vertices. Notice how some of them like the rightmost one have moved considerably. u(b) 
c a d Figure 6: Base domain vertex repositioning. Left: original patches around qi, middle: parameter 
domain, right: repositioned qi and new patch boundaries. This is replaced with the vertex whose pa­rameter 
coordinate are the closest to the center. The inverse map­ping (right) is used to .nd the new position 
qi . and the new curves. 4. Fixing the global edges: The image of the global edges on the .nest level 
will later be the patch boundaries of the normal mesh. For this reason we need to improve the smoothness 
of the as­sociated curves at the .nest level. We use a procedure similar to [9]. For each base domain 
edge {i, k}we consider the region formed on the .nest level mesh by its two incident patches. Let l and 
m be the opposing global vertices. We then compute a parameter function . within the diamond-shaped region 
of the surface. The boundary condition is set as .(qi)=.(qk)=0, .(ql)=1, .(qm)=-1, with linear variation 
along the edges. We then compute the param­eterization and let its zero level set be our new curve. Again 
one could iterate this procedure till convergence but in practice one cy­cle suf.ces. The curves of the 
top right image in Figure 5 are the result of the curve smoothing on the .nest level. Note that a similar 
result can be achieved by allowing the user to position the global vertices and draw the boundaries of 
the patches manually. Indeed, the following steps of the algorithm do not de­pend on how the initial 
net of surface curves is produced.  Figure 5: The entire procedure shown for the molecule model. 1. 
Base domain. 2. Initial set of curves. 3. Global vertex repositioning 4. Initial Parameterization 5. 
Adjusting parameterization 6. Final normal mesh. (HIV protease surface model courtesy of Arthur Olson, 
The Scripps Research Institute) 5. Initial parameterization: Once the global vertices and edges are 
.xed, one can start .lling in the interior. This is done by computing the parameterization of each patch 
to a triangle while keeping the boundary .xed. The parameter coordinates from the last stage can serve 
as a good initial guess. We now have a smooth global parameterization. This parameterization is shown 
in the bot­tom left of Figure 5. Each triangle is given a triangular checker­board texture to illustrate 
the parameterization. 6. Piercing: In this stage of the algorithm we start building the actual normal 
mesh. The canonical step is for a new vertex of the semi-regular mesh to .nd its position on the original 
mesh. In quadrisection every edge of level jgenerates a new vertex on level j+1. We .rst compute a base 
point using interpolating Butter­.y subdivision [8] [24] as well as an approximation of the normal. This 
de.nes a straight line. This line may have multiple intersec­tion points in which case we need to .nd 
the right one, or it could have none, in which case we need to come up with a good alternate.  Suppose 
that we need to produce the new vertex q that lies halfway along the edge {a,c} with incident triangles 
{a,c,b} and {c,a,d}, see Figure 7. Let the two incident patches form the re­gion R. Build the straight 
line Lde.ned by the base point s predicted by the Butter.y subdivision rule [24] and the direction of 
the normal computed from the coarser level points. We .nd all the intersection points of Lwith the region 
R by checking all triangles inside. If there is no intersection we take the point v that lies midway 
between the points a and c in the parameter domain: u(v)= (u(a)+u(c))/2. This is the same point a standard 
parameteri­zation based remesher would use. Note that in this case the detail vector is non-normal and 
its three components need to be stored. In the case when there exist several intersections of the mesh 
re­gion R with the piercing line Lwe choose the intersection point that is closest to the point u(v)in 
the parameter domain. Let us denote by u(q)the parametric coordinates of that piercing point.  u(c) 
Figure 7: Upper left: piercing, the Butter.y point is s, the surface is pierced at the point q, the parametrically 
suggested point v lies on the curve separating two regions of the mesh. Right: parameter do­main, the 
pierced point falls inside the aperture and gets accepted. Lower left: the parameterization is adjusted 
to let the curve pass through q. We accept this point as a valid point of the semi-regular mesh if lu(q)- 
u(v)l <.lu(a)- u(v)l, where .is an aperture pa­rameter that speci.es how much the parameter value of 
a pierced point is allowed to deviate from the center of the diamond. Oth­erwise, the piercing point 
is rejected and the mesh takes the point with the parameter value u(v), resulting in a non-normal detail. 
7. Adjusting the parameterization: Once we have a new piercing point, we need to adjust the parameterization 
to re.ect this. Essentially, the adjusted parameterization u should be such that the piercing point has 
the parameters u(v)=: u(q). When impos­ing such an isolated point constraint on the parameterization, 
there is no mathematical guarantee against .ipping. Hence we draw a new piecewise linear curve through 
u(q)in the parameter domain. This gives a new curve on the surface which passes through q, see Figure 
7. We then recompute the parameterization for each of the patches onto a triangle separately. We use 
a piecewise linear bound­ary condition with the half point at q on the common edge. When all the new 
midpoints for the edges of a face of level j are computed, we can build the faces of level j +1. This 
is done by drawing three new curves inside the corresponding region of the original mesh, see Figure 
8. Before that operation happens we need to ensure that a valid parameterization is available within 
the patch. The patch is parameterized onto a triangle with three piecewise lin­ear boundary conditions 
each time putting the new points at the midpoint. Then the new points are connected in the parameter 
do­main which allows us to draw new .ner level curves on the original mesh. This produces a metamesh 
similar to [16], so that the new net of curves replicates the structure of the semi-regular hierarchy 
on the surface of the original. The construction of the semi-regular mesh can be done adaptively with 
the error driven procedure from MAPS [18]. An example of parameterization adjustment after two levels 
of adaptive subdivision is shown in the bottom middle of Fig­ure 5. Note that as the regions for which 
we compute parameteriza­tions become smaller, the starting guesses are better and the solver convergence 
becomes faster and faster. b u(b)  u(c) Figure 8: Face split: Quadrisection in the parameter plane 
(left) leads to three new curves within the triangular patch (right). The aperture parameter . of the 
piercing procedure provides control over how much of the original parameterization is preserved in the 
.nal mesh and consequently, how many non-normal details will appear. At . =0we build a non-normal mesh 
entirely based on the original global parameterization. At . =1we attempt to build a purely normal mesh 
independent of the parameterization. In our experience, the best results were achieved when the aper­ture 
was set low (0.2) at the coarsest levels, and then increased to 0.6 on .ner levels. On the very .ne levels 
of the hierarchy, where the geometry of the semi-regular meshes closely follows the origi­nal geometry, 
one can often simply use a naive piercing procedure without parameter adjustment. One may wonder if the 
continuous readjustment of parameteri­zations is really necessary. We have tried the naive piercing pro­cedure 
without parameterization from the base domain and found that it typically fails on all models. An example 
is Figure 9 which shows 4 levels of naive piercing for the torus starting from a 102 vertex base mesh. 
Clearly, there are several regions with .ipped and self-intersecting triangles. The error is about 20 
times larger than the true normal mesh. Figure 9: Naive piercing procedure. Clearly, several regions 
have .ipped triangles and are self-intersecting. Dataset Size Base Normal Not normal % L2 Time mesh size 
(%) error (min) Feline 49864 156 40346 729 (1.8%) .015 4 Molecule 10028 37 9521 270 (2.8%) .075 1.5 Rabbit 
16760 33 8235 196 (2.4%) .037 2 Torus3 5884 98 5294 421 (8.0%) .03 3 Skull 20002 112 25376 817 (3.2%) 
.02 2.5 Horse 48485 234 59319 644 (1.1%) .004 6.8 Table 1: Summary of normal meshing results for different 
models. The normal mesh is computed adaptively and contains roughly the same number of triangles as the 
original mesh. The relative L2 errors are computed with the I.E.I.-CNR Metro tool. The times are reported 
on a 700MHz Pentium III machine.  4 Results We have implemented the algorithms described in the preceding 
section, and performed a series of experiments in which normal meshes for various models were built. 
The summary of the results is given in Table 1. As we can see from the table, the normal semi­regular 
meshes have very high accuracy and hardly any non normal details. One interesting feature of our normal 
meshing procedure is the following: while the structure of patches comes from performing simpli.cation 
there are far fewer restrictions on how coarse the base mesh can be. Note for example that the skull 
in Figure 1 was meshed with the tetrahedron as base mesh. This is largely due to the robust mesh parameterization 
techniques used in our approach. Figure 10 shows normal meshes for rabbit, torus, feline, and skull, 
as well as close-up of feline (bottom left) normal mesh. Note how smooth the meshes are across global 
edges and global vertices. This smoothness mostly comes from the normality, not the param­eterization. 
It is thus an intrinsic quantity. One of the most interesting observations coming from this work is that 
locally the normal meshes do not differ much from the non­normal ones, while offering huge bene.ts in 
terms of ef.ciency of representation. For example, Table 2 shows how the aperture pa­rameter . that governs 
the construction of normal meshes affects the number of detail coef.cients with non-trivial tangential 
com­ponents for the model of the three hole torus (these numbers are typical for other models as well). 
In particular, we see that already a very modest acceptance strategy (. =0.2) gets rid of more than 90% 
of the tangential components in the remeshed model, and the more aggressive strategies offer even more 
bene.ts without affect­ing the error of the representation. 5 Summary and Conclusion In this paper we 
introduce the notion of normal meshes. Normal meshes are multiresolution meshes in which vertices can 
be found in the normal direction, starting from some coarse level. Hence only one scalar per vertex needs 
to be stored. We presented a robust . normal error (10-4) 0 0% 1.02 0.2 91.9% 1.05 0.4 92.4% 1.04 best 
98.3% 1.02 Table 2: The relation between the acceptance strategy during the piercing procedure and the 
percentage of perfectly normal details in the hierarchy. The original model has 5884 vertices, all the 
nor­mal meshes have 26002 vertices (4 levels uniformly), and the base mesh contained 98 vertices. The 
best strategy in the last line used . =0.2on the .rst three levels and afterward always accepted the 
piercing candidates. algorithm for computing normal semi-regular meshes of any input mesh and showed 
that it produces very smooth triangulations on a variety of input models. It is clear that normal meshes 
have numerous applications. We brie.y discuss a few. Compression Usually a wavelet transform of a standard 
mesh has three components which need to be quantized and encoded. In­formation theory tells us that the 
more non uniform the distribution of the coef.cients the lower the .rst order entropy. Having 2/3 of 
the coef.cients exactly zero will further reduce the bit budget. From an implementation viewpoint, we 
can almost directly hook the nor­mal mesh coef.cients up to the best known scalar wavelet image compression 
code. Filtering It has been shown that operations such as smoothing, enhancement, and denoising can be 
computed through a suitable scaling of wavelet coef.cients [7]. In a normal mesh any such al­gorithm 
will require only 1/3 as many computations. Also large scaling coef.cients in a standard mesh will introduce 
large tangen­tial components leading to .ipped triangles. In a normal mesh this is much less likely to 
happen. Texturing Normal semi-regular meshes are very smooth inside patches, across global edges, and 
around global vertices even when the base domain is exceedingly coarse, cf. the skull model. The im­plied 
parameterizations are highly suitable for all types of mapping applications. Rendering Normal maps are 
a very powerful tool for decora­tion and enhancement of otherwise smooth geometry. In particular in the 
context of bandwidth bottlenecks it is attractive to be able to download a normal map into hardware and 
only send smooth co­ef.cient updates for the underlying geometry. The normal mesh transform effectively 
solves the associated inverse problem: con­struct a normal map for a given geometry. The concept of normal 
meshes opens up many new areas of re­search. Our algorithm uses interpolating subdivision to .nd the 
base point. Building normal meshes with respect to approximating subdivision is not straightforward. 
 The theoretical underpinnings of normal meshes need to be studied. Do continuous variable normal descriptions 
of surfaces exist? What about stability? What about connections with cur­vature normal .ow which acts 
to reduce normal information?  We only addressed semi-regular normal meshes here, while the de.nition 
allows for the more .exible setting of progressive ir­regular mesh hierarchies.  Purely scalar compression 
schemes for geometry need to be compared with existing coders.  Generalize normal meshes to higher dimensions. 
It should be possible to represent a M dimensional manifold in N dimen­sions with N - M variables as 
opposed to the usual N.  The current implementation only works for surfaces without boundaries and does 
not deal with feature curves. We will ad­dress these issues in our future research. Acknowledgments This 
work was supported in part by NSF (ACI-9624957, ACI-9721349, DMS-9874082, DMS 9872890), Alias|Wavefront, 
a Packard Fellow­ship, and a Caltech Summer Undergraduate Research Fellowship (SURF). Special thanks 
to Nathan Litke for his subdivision library, to Andrei Khodakovsky, Mathieu Desbrun, Adi Levin, Arthur 
Olson, and Zo¨e Wood for helpful discussions, Chris John­son for the use of the SGI-Utah Visual Supercomputing 
Center resources, and to Cici Koenig for production help. Datasets are courtesy Cyberware, Headus, The 
Scripps Research Institute, and University of Washington.  References [1] CERTAIN, A., POPOVIC, J., 
DEROSE,T., DUCHAMP,T., SALESIN, D., AND STUETZLE, W. Interactive Multiresolution Surface Viewing. Proceedings 
of SIGGRAPH 96 (1996), 91 98. [2] COHEN, J., OLANO, M., AND MANOCHA, D. Appearance-Preserving Simpli­.cation. 
Proceedings of SIGGRAPH 98 (1998), 115 122. [3] COOK, R. L. Shade trees. Computer Graphics (Proceedings 
of SIGGRAPH 84) 18, 3 (1984), 223 231. [4] DAUBECHIES, I., GUSKOV, I., AND SWELDENS, W. Regularity of 
Irregular Subdivision. Constr. Approx. 15 (1999), 381 426. [5] DESBRUN, M., MEYER, M., SCHR ¨ ODER,P., 
AND BARR, A. H. Implicit Fair­ ing of Irregular Meshes Using Diffusion and Curvature Flow. Proceedings 
of SIGGRAPH 99 (1999), 317 324. [6] DONOHO, D. L. Interpolating wavelet transforms. Preprint, Department 
of Statistics, Stanford University, 1992. [7] DONOHO, D. L. Unconditional Bases are Optimal Bases for 
Data Compression and for Statistical Estimation. Appl. Comput. Harmon. Anal. 1 (1993), 100 115. [8] DYN, 
N., LEVIN, D., AND GREGORY, J. A. A Butter.y Subdivision Scheme for Surface Interpolation with Tension 
Control. ACM Transactions on Graphics 9, 2 (1990), 160 169. [9] ECK, M., DEROSE,T., DUCHAMP,T., HOPPE, 
H., LOUNSBERY, M., AND STUETZLE, W. Multiresolution Analysis of Arbitrary Meshes. Proceedings of SIGGRAPH 
95 (1995), 173 182. [10] FLOATER, M. S. Parameterization and Smooth Approximation of Surface Tri­angulations. 
Computer Aided Geometric Design 14 (1997), 231 250. [11] GARLAND, M., AND HECKBERT, P. S. Surface Simpli.cation 
Using Quadric Error Metrics. In Proceedings of SIGGRAPH 96, 209 216, 1996. [12] GOLUB, G. H., AND LOAN,C. 
F. V. Matrix Computations, 2nd ed. The John Hopkins University Press, Baltimore, 1983. [13] GUSKOV, I., 
SWELDENS,W., AND SCHR¨ ODER, P. Multiresolution Signal Pro­cessing for Meshes. Proceedings of SIGGRAPH 
99 (1999), 325 334. [14] KHODAKOVSKY, A., SCHR ¨ ODER,P., SWELDENS, W. Progressive Geometry Compression. 
Proceedings of SIGGRAPH 2000 (2000). [15] KRISHNAMURTHY,V., AND LEVOY, M. Fitting Smooth Surfaces to 
Dense Polygon Meshes. Proceedings of SIGGRAPH 96 (1996), 313 324. [16] LEE,A. W. F., DOBKIN, D., SWELDENS,W., 
AND SCHR¨ ODER, P. Multireso­lution Mesh Morphing. Proceedings of SIGGRAPH 99 (1999), 343 350. [17] LEE,A. 
W. F., MORETON, H., HOPPE, H. Displaced Subdivision Surfaces. Proceedings of SIGGRAPH 00 (2000). [18] 
LEE,A. W. F., SWELDENS,W., SCHR¨ ODER,P., COWSAR, L., AND DOBKIN, D. MAPS: Multiresolution Adaptive Parameterization 
of Surfaces. Proceedings of SIGGRAPH 98 (1998), 95 104. [19] LEVOY, M. The Digital Michelangelo Project. 
In Proceedings of the 2nd Inter­national Conference on 3D Digital Imaging and Modeling, October 1999. 
[20] L´ EVY, B., AND MALLET, J. Non-Distorted Texture Mapping for Sheared Tri­angulated Meshes. Proceedings 
of SIGGRAPH 98 (1998), 343 352. [21] LOUNSBERY, M., DEROSE, T. D., AND WARREN, J. Multiresolution Analysis 
for Surfaces of Arbitrary Topological Type. ACM Transactions on Graphics 16,1 (1997), 34 73. Originally 
available as TR-93-10-05, October, 1993, Department of Computer Science and Engineering, University of 
Washington. [22] SCHR¨ ODER,P., AND SWELDENS, W. Spherical Wavelets: Ef.ciently Repre­senting Functions 
on the Sphere. Proceedings of SIGGRAPH 95 (1995), 161 172. [23] ZORIN, D., AND SCHR ¨ ODER, P., Eds. 
Subdivision for Modeling and Animation. Course Notes. ACM SIGGRAPH, 1999. [24] ZORIN, D., SCHR ¨ W. ODER,P., 
AND SWELDENS, Interpolating Subdivision for Meshes with Arbitrary Topology. Proceedings of SIGGRAPH 96 
(1996), 189 192. [25] ZORIN, D., SCHR ¨ ODER,P., AND SWELDENS, W. Interactive Multiresolution Mesh Editing. 
Proceedings of SIGGRAPH 97 (1997), 259 268. Figure 10: Colorplate. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344835</article_id>
		<sort_key>103</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[&#8730;3-subdivision]]></title>
		<page_from>103</page_from>
		<page_to>112</page_to>
		<doi_number>10.1145/344779.344835</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344835</url>
		<abstract>
			<par><![CDATA[<p>A new stationary subdivision scheme is presented which performs <italic>slower</italic> topological refinement than the usual dyadic split operation. The number of triangles increases in every step by a factor of 3 instead of 4. Applying the subdivision operator <italic>twice</italic> causes a uniform refinement with <italic>tri</italic>-section of every original edge (hence the name &radic;3-subdivision) while two dyadic splits would <italic>quad</italic>-sect every original edge. Besides the finer gradation of the hierarchy levels, the new scheme has several important properties: The stencils for the subdivision rules have minimum size and maximum symmetry. The smoothness of the limit surface is <italic>C</italic><supscrpt>2</supscrpt> everywhere except for the extraordinary points where it is <italic>C</italic><supscrpt>1</supscrpt>. The convergence analysis of the scheme is presented based on a new general technique which also applies to the analysis of other subdivision schemes. The new splitting operation enables locally adaptive refinement under built-in preservation of the mesh consistency without temporary crack-fixing between neighboring faces from different refinement levels. The size of the surrounding mesh area which is affected by selective refinement is smaller than for the dyadic split operation. We further present a simple extension of the new subdivision scheme which makes it applicable to meshes with boundary and allows us to generate sharp feature lines.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.1</cat_node>
				<descriptor>Smoothing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.7</cat_node>
				<descriptor>Convergence and stability</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003722</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Interpolation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003727.10003728</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Differential equations->Ordinary differential equations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP15023875</person_id>
				<author_profile_id><![CDATA[81100137073]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Leif]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kobbelt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Max-Planck Institute for Computer Sciences, Im Stadtwald, 66123 Saarbr&#252;cken, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[H. Biermann, A. Levin, D. Zorin, Piecewise smooth subdivision sur-faces with normal control, Preprint]]></ref_text>
				<ref_id>BLZ99</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[E. Catmull, J. Clark, Recursively generated B-spline surfaces on arbi-trary topological meshes, CAD 10 (1978), 350-355]]></ref_text>
				<ref_id>CC78</ref_id>
			</ref>
			<ref>
				<ref_obj_id>574752</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[A. Cavaretta, W. Dahmen, C. Micchelli, Stationary Subdivision, Mem-oirs of the AMS 93 (1991), pp. 1-186]]></ref_text>
				<ref_id>CDM91</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[D. Doo, M. Sabin, Behaviour of recursive division surfaces near ex-traordinary points, CAD 10 (1978), 356-360]]></ref_text>
				<ref_id>DS78</ref_id>
			</ref>
			<ref>
				<ref_obj_id>78958</ref_obj_id>
				<ref_obj_pid>78956</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[N. Dyn, J. Gregory, D. Levin, A Butterfly Subdivision Scheme for Sur-face Interpolation with Tension Controll, ACM Trans. Graph. 9 (1990), pp. 160-169]]></ref_text>
				<ref_id>DGL90</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[N. Dyn, Subdivision Schemes in Computer Aided Geometric Design, Advances in Numerical Analysis II, Wavelets, Subdivisions and Radial Functions, W.A. Light ed., Oxford University Press, 1991, pp: 36-104.]]></ref_text>
				<ref_id>Dyn91</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311577</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[I. Guskov, W. Sweldens, P. Schr~ oder, Multiresolution signal processing for meshes, SIGGRAPH 99 Proceedings, 1999, pp. 325 - 334]]></ref_text>
				<ref_id>GSS99</ref_id>
			</ref>
			<ref>
				<ref_obj_id>248979</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[G. Golub, C. van Loan, Matrix Computations, 3rd, Johns Hopkins Univ Press, 1996]]></ref_text>
				<ref_id>GvL96</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[M. Griebel, C. Zenger, S. Zimmer, Multilevel Gauss-Seidel-Algorithms for Full and Sparse Grid Problems, Computing 50, 1993, pp. 127-148]]></ref_text>
				<ref_id>GZZ93</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[I. Guskov, Multivariate subdivision schemes and divided differences, Preprint, Princeton University, 1998]]></ref_text>
				<ref_id>Gus98</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[W. Hackbusch, Multi-Grid Methods and Applications, Springer, Berlin, 1985]]></ref_text>
				<ref_id>Hac85</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192233</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[H. Hoppe, T. DeRose, T. Duchamp, M. Halstead, H. Jin, J. McDonald, J. Schweitzer, W. Stuetzle, Piecewise smooth surface reconstruction, SIGGRAPH 1994 Proceedings, 1994, pp. 295-302]]></ref_text>
				<ref_id>HDD+94</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[L. Kobbelt, Interpolatory Subdivision on Open Quadrilateral Nets with Arbitrary Topology, Computer Graphics Forum 15 (1996), Eurographics '96 Conference Issue, pp. 409-420]]></ref_text>
				<ref_id>Kob96</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[L. Kobbelt, K. Daubert, H-P. Seidel, Ray-tracing of subdivision sur-faces, 9th Eurographics Workshop on Rendering Proceedings, 1998, pp. 69 - 80]]></ref_text>
				<ref_id>KDS98</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280831</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[L. Kobbelt, S. Campagna, J. Vorsatz, H-P. Seidel, Interactive multires-olution modeling on arbitrary meshes, SIGGRAPH 98 Proceedings, 1998, pp. 105-114]]></ref_text>
				<ref_id>KCVS98</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311541</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[A. Levin, Interpolating nets of curves by smooth subdivision surfaces, SIGGRAPH 99 Proceedings, 1999, pp. 57 - 64]]></ref_text>
				<ref_id>Lev99</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[C. Loop, Smooth subdivision surfaces based on triangles, Master The-sis, Utah University, USA, 1987]]></ref_text>
				<ref_id>Loo87</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[H. Prautzsch, Smoothness of subdivision surfaces at extraordinary points, Adv. Comp. Math. 14 (1998), pp. 377 - 390]]></ref_text>
				<ref_id>Pra98</ref_id>
			</ref>
			<ref>
				<ref_obj_id>211166</ref_obj_id>
				<ref_obj_pid>211163</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[U. Reif, A unified approach to subdivision algorithms near extraordi-nary vertices, CAGD 12 (1995), pp. 153-174]]></ref_text>
				<ref_id>Rei95</ref_id>
			</ref>
			<ref>
				<ref_obj_id>263851</ref_obj_id>
				<ref_obj_pid>263834</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[U. Reif, J. Peters, The simplest subdivision scheme for smoothing poly-hedra, ACM Trans. Graph. 16 (1998), pp. 420 - 431]]></ref_text>
				<ref_id>RP98</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[M. Sabin, Recursive Division, in The Mathematics of Surfaces, Claren-don Press, 1986, pp. 269 - 282]]></ref_text>
				<ref_id>Sab87</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280945</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[J. Stam, Exact evaluation of Catmull/Clark subdivision surfaces at ar-bitrary parameter values, SIGGRAPH 98 Proceeding, 1998, pp. 395 - 404]]></ref_text>
				<ref_id>Sta98</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[L. Velho, J. Gomes, Quasi-stationary subdivision using four directional meshes, Preprint]]></ref_text>
				<ref_id>VG99</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[L. Velho, J. Gomes, Semi-regular 4-8 refinement and box spline sur-faces, Preprint]]></ref_text>
				<ref_id>VG00</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[M. Vasilescu, D. Terzopoulos, Adaptive meshes and shells: Irregular triangulation, discontinuities and hierarchical subdivision, Proceedings of the Computer Vision and Pattern Recognition Conference, 1992, 829 - 832]]></ref_text>
				<ref_id>VT92</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[R. Verf~ urth, A review of a posteriori error estimation and adaptive mesh refinement techniques, Wiley-Teubner, 1996]]></ref_text>
				<ref_id>Ver96</ref_id>
			</ref>
			<ref>
				<ref_obj_id>580358</ref_obj_id>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[J. Warren, Subdivision methods for geometric design, unpublished manuscript]]></ref_text>
				<ref_id>War00</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237254</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[D. Zorin, P. Schr~ oder, W. Sweldens, Interpolating Subdivision for Meshes with Arbitrary Topology, SIGGRAPH 96 Proceedings, 1996, pp. 189-192]]></ref_text>
				<ref_id>ZSS96</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[D.Zorin, C k Continuity of Subdivision Surfaces, Thesis, California In-stitute of Technology, 1997]]></ref_text>
				<ref_id>Zor97</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258863</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[D. Zorin, P. Schr~ oder, W. Sweldens, Interactive multiresolution mesh editing, SIGGRAPH 97 Proceedings, 1997, pp. 259-268]]></ref_text>
				<ref_id>ZSS97</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 .  3-Subdivision Leif Kobbelt* Max-Planck Institute for Computer Sciences  Abstract A new stationary 
subdivision scheme is presented which performs slower topological re.nement than the usual dyadic split 
operation. The number of triangles increases in every step by a factor of 3 instead of 4. Applying the 
subdivision operator twice causes a uni­form re.nement with tri-section of every original edge (hence 
the name v3-subdivision) while two dyadic splits would quad-sect ev­ery original edge. Besides the .ner 
gradation of the hierarchy lev­els, the new scheme has several important properties: The stencils for 
the subdivision rules have minimum size and maximum symme­try. The smoothness of the limit surface is 
C2 everywhere except for the extraordinary points where it is C1. The convergence analysis of the scheme 
is presented based on a new general technique which also applies to the analysis of other subdivision 
schemes. The new splitting operation enables locally adaptive re.nement under built­in preservation of 
the mesh consistency without temporary crack­.xing between neighboring faces from different re.nement 
levels. The size of the surrounding mesh area which is affected by selec­tive re.nement is smaller than 
for the dyadic split operation. We further present a simple extension of the new subdivision scheme which 
makes it applicable to meshes with boundary and allows us to generate sharp feature lines. 1 Introduction 
The use of subdivision schemes for the ef.cient generation of freefrom surfaces has become commonplace 
in a variety of geo­metric modeling applications. Instead of de.ning a parameteric surface by a functional 
expression F (ubv)to be evaluated over a planar parameter domain O EIR2 we simply sketch the surface 
by a coarse control mesh M 0 that may have arbitrary connectivity and (manifold) topology. By applying 
a set of re.nement rules, we gen­erate a sequence of .ner and .ner meshes M 1bm m . mbM kbm m m which 
eventually converge to a smooth limit surface M 8. In the literature there have been proposed many subdivision 
schemes which are either generalized from tensor-products of curve generation schemes [DS78, CC78, Kob96] 
or from 2-scale relations in more general functional spaces being de.ned over the three­directional grid 
[Loo87, DGL90, ZSS96]. Due to the nature of the re.nement operators, the generalized tensor-product schemes 
natu­ _ Max-Planck Institute for Computer Sciences, Im Stadtwald, 66123 Saarbr¨ucken, Germany, kobbelt@mpi-sb.mpg.de 
 Permission to make digital or hard copies of part or all of this work or personal or classroom use is 
granted without fee provided that copies are not made or distributed for profit or commercial advantage 
and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, 
to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. SIGGRAPH 
2000, New Orleans, LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 Figure 1: Subdivision schemes 
on triangle meshes are usually based on the 1-to-4 split operation which inserts a new vertex for every 
edge of the given mesh and then connects the new vertices. rally lead to quadrilateral meshes while the 
others lead to triangle meshes. A subdivision operator for polygonal meshes can be considered as being 
composed by a (topological) split operation followed by a (geometric) smoothing operation. The split 
operation performs the actual re.nement by introducing new vertices and the smooth­ing operation changes 
the vertex positions by computing averages of neighboring vertices (generalized convolution operators, 
relax­ation). In order to guarantee that the subdivision process will al­ways generate a sequence of 
meshes M k that converges to a smooth limit, the smoothing operator has to satisfy speci.c necessary 
and suf.cient conditions [CDM91, Dyn91, Rei95, Zor97, Pra98]. This is why special attention has been 
paid by many authors to the design of optimal smoothing rules and their analysis. While in the context 
of quad-meshes several different topolog­ical split operations (e.g. primal [CC78, Kob96] or dual [DS78]) 
have been investigated, all currently proposed stationary schemes for triangle meshes are based on the 
uniform 1-to-4 split [Loo87, DGL90, ZSS96] which is depicted in Fig 1. This split operation introduces 
a new vertex for each edge of the given mesh. Recently, the concept of uniform re.nement has been general­ized 
to irregular re.nement [GSS99, KCVS98, VG99] where new vertices can be inserted at arbitrary locations 
without necessarily generating semi-uniform meshes with so-called subdivision con­nectivity. However, 
the convergence analysis of such schemes is still an open question. In this paper we will present a new 
subdivision scheme for trian­gle meshes which is based on an alternative uniform split operator that 
introduces a new vertex for every triangle of the given mesh (Section 2). As we will see in the following 
sections, the new split operator enables us to de.ne a natural stationary subdivision scheme which has 
stencils of minimum size and maximum symmetry (Section 3). The smoothing rules of the subdivision operator 
are derived from well-known necessary conditions for the convergence to smooth limit surfaces. Since 
the standard subdivision analysis machinery cannot be applied directly to the new scheme, we derive a 
modi­.ed technique and prove that the scheme generates C2 surfaces for regular control meshes. For arbitrary 
control meshes we .nd the limit surface to be C2 almost everywhere except for the extraordi­nary vertices 
(valence = 6) where the smoothness is at least C1 (see the Appendix).  Figure 2: The v3-subdivision 
scheme is based on a split operation which .rst inserts a new vertex for every face of the given mesh. 
Flipping the original edges then yields the .nal result which is a 30 degree rotated regular mesh. Applying 
the v3-subdivision scheme twice leads to a 1-to-9 re.nement of the original mesh. As this corresponds 
to a tri-adic split (two new vertices are introduced for every original edge) we call our scheme v3-subdivision. 
  Inserting a new vertex into a triangular face does only affect that single face which makes locally 
adaptive re.nement very effective. The global consistency of the mesh is preserved automatically ifv3-subdivision 
is performed selectively. In Section 4 we compare adaptively re.ned meshes generated by dyadic subdivision 
with ourv3-subdivision meshes and .nd that v3-subdivision usually needs fewer triangles and less effort 
to achieve the same approximation tolerance. The reason for this effect is the better localization, i.e., 
only a relatively small region of the mesh is affected if more vertices are inserted locally. For the 
generation of surfaces with smooth boundary curves, we need special smoothing rules at the boundary faces 
of the given mesh. In Section 5 we propose a boundary rule which reproduces cubic B-splines. The boundary 
rules can also be used to generate sharp feature lines in the interior of the surface. . 2 3-Subdivision 
The most wide-spread way to uniformly re.ne a given triangle mesh M 0 is the dyadic split which bi-sects 
all the edges by in­serting a new vertex between every adjacent pair of old ones. Each triangular face 
is then split into four smaller triangles by mutually connecting the new vertices sitting on a face s 
edges (cf. Fig. 1). This type of splitting has the positive effect that all newly inserted vertices have 
valence six and the valences of the old vertices does not change. After applying the dyadic split several 
times, the re­.ned meshes M k have a semi-regular structure since the repeated 1-to-4 re.nement replaces 
every triangle of the original mesh by a regular patch with 4k triangles. A straightforward generalization 
of the dyadic split is the n-adic split where every edge is subdivided into n segments and conse­quently 
every original face is split into n2 sub-triangles. However, in the context of stationary subdivision 
schemes, the n-adic split operation requires a speci.c smoothing rule for every new vertex (modulo permutations 
of the barycentric coordinates). This is why subdivision schemes are mostly based on the dyadic split 
that only requires two smoothing rules: one for the old vertices and one for the new ones (plus rotations). 
In this paper, we consider the following re.nement operation for triangle meshes: Given a mesh M 0 we 
perform a 1-to-3 split for every triangle by inserting a new vertex at its center. This introduces three 
new edges connecting the new vertex to the surrounding old ones. In order to re-balance the valence of 
the mesh vertices we then .ip every original edge that connects two old vertices (cf. Fig 2). This split 
operation is uniform in the sense that if it is applied to a uniform (three-directional) grid, a (rotated 
and re.ned) uniform grid is generated (cf. Fig. 2). If we apply the same re.nement oper­ator twice, the 
combined operator splits every original triangle into nine subtriangles (tri-adic split). Hence one single 
re.nement step can be considered as the square root of the tri-adic split. In a dif­ferent context, this 
type of re.nement operator has been considered independently in [Sab87] and [Gus98].   Analyzing the 
action of the v3-subdivision operator on arbitrary triangle meshes, we .nd that all newly inserted vertices 
have ex­actly valence six. The valences of the old vertices are not changed such that after a suf.cient 
number of re.nement steps, the mesh M k has large regions with regular mesh structure which are dis­turbed 
only by a small number of isolated extraordinary vertices. These correspond to the vertices in M 0 which 
had valence =6 (cf. Fig. 3). There are several arguments why it is interesting to investigate this particular 
re.nement operator. First, it is very natural to sub­divide triangular faces at their center rather than 
splitting all three edges since the coef.cients of the subsequent smoothing operator can re.ect the threefold 
symmetry of the three-directional grid. Second, the v3-re.nement is in some sense slower than the stan­dard 
re.nement since the number of vertices (and faces) increases by the factor of 3 instead of 4. As a consequence, 
we have more levels of uniform resolution if a prescribed target complexity of the mesh must not be exceeded. 
This is why similar uniform re­.nement operators for quad-meshes have been used in numerical applications 
such as multi-grid solvers for .nite element analysis [Hac85, GZZ93]. From the computer graphics point 
of view the v3-re.nement has the nice property that it enables a very simple implementation of adaptive 
re.nement strategies with no inconsistent intermediate states as we will see in Section 4. In the context 
of polygonal mesh based multiresolution represen­tations [ZSS96, KCVS98, GSS99], the v3-hierarchies can 
provide an intuitive and robust way to encode the detail information since the detail coef.cients are 
assigned to faces (.tangent planes) in­stead of vertices.  3 Stationary smoothing rules To complete 
the de.nition of our new subdivision scheme, we have to .nd the two smoothing rules, one for the placement 
of the newly inserted vertices and one for the relaxation of the old ones. For the sake of ef.ciency, 
our goal is to use the smallest possible stencils while still generating high quality meshes. There are 
well-known necessary and suf.cient criteria which tell whether a subdivision scheme S is convergent or 
not and what smoothness properties the limit surface has. Such criteria check if the eigenvalues of the 
subdivision matrix have a certain distribution and if a local regular parameterization exists in the 
vicinity of every vertex on the limit surface [CDM91, Dyn91, Rei95, Zor97, Pra98].     Figure 3: 
The v3-subdivision generates semi-regular meshes since all new vertices have valence six. After an even 
number 2k of re.nement steps, each original triangle is replaced by a regular patch with 9k triangles. 
By de.nition, the subdivision matrix is a square matrix S which maps a certain sub-mesh V EM k to a topologically 
equivalent sub­mesh S(V)EM kb1 of the re.ned mesh. Every row of this matrix is a rule to compute the 
position of a new vertex. Every column of this matrix tells how one old vertex contributes to the vertex 
positions in the re.ned mesh. Usually, V is chosen to be the neighborhood of a particular vertex, e.g., 
a vertex p and its neighbors up to the k-th order (k-ring neighborhood). To derive the weight coef.cients 
for the new subdivision scheme, we use these criteria for some kind of reverse engineering process, i.e., 
instead of analyzing a given scheme, we derive one which by construction satisi.es the known necessary 
criteria. The justi.ca­tion for doing this is that if the necessary conditions uniquely deter­mine a 
smoothing rule then the resulting subdivision scheme is the only scheme (with the given stencil) that 
is worth being considered. In the Appendix we will give the details of the suf.cient part of the convergence 
analysis. Since the v3-subdivision operator inserts a new vertex for every triangle of the given mesh, 
the minimum stencil for the correspond­ing smoothing rule has to include at least the three (old) corner 
vertices of that triangle. For symmetry reasons, the only reasonable choice for that smoothing rule is 
hence 1 q : pi +p j +pk b(1) 3 i.e., the new vertex q is simply inserted at the center of the triangle 
i (pi bp j bpk ). The smallest non-trivial stencil for the relaxation of the old ver­tices is the 1-ring 
neighborhood containing the vertex itself and its direct neighbors. To establish symmetry, we assign 
the same weight to each neighbor. Let p be a vertex with valence n and p0 bm m m .bpng1 its directly 
adjacent neighbors in the unre.ned mesh then we de.ne 1 ng1 S(p): (1 an )p +an . pi (2) n i 0 The remaining 
question is what the optimal choice for the param­eter an would be. Usually, the coef.cient depends on 
the valence of p in order to make the subdivision scheme applicable to control meshes M 0 with arbitrary 
connectivity. The rules (1) and (2) imply that the 1-ring neighborhood of a vertex S(p)EM kb1 only depends 
on the 1-ring neighborhood of the corresponding vertex p EM k. Hence, we can set-up a (n +1x )(n +1)matrix 
which maps p and its n neighbors to the next re.nement level. Arranging all the vertices in a vector 
  i Figure 4: The application of the subdivision matrix S causes a ro­tation around p since the neighboring 
vertices are replaced by the centers of the adjacent triangles. pbp0 bm m m .bpng1]we derive the subdivision 
matrix [ x uvv vv 111 00 ... . 10 .. .. .... 1 S ... . . (3) ... . . 3 .....0 .. .. l 10 ..1 11 001 
with u 3 (1 an )and v 3an /n. However, when analysing the eigenstructure of this matrix, we .nd that 
it is not suitable for the construction of a convergent subdivision scheme. The reason for this defect 
is the rotation around p which is caused by the appli­cation of S and which makes all eigenvalues of 
S complex. Fig. 4 depicts the situation. From the last section we know that applying the v3-subdivision 
operator two times corresponds to a tri-adic split. So instead of analysing one single subdivision step, 
we can combine two succes­sive steps since after the second application of S, the neighborhood of S2 
(p)is again aligned to the original con.guration around p. Hence, the back-rotation can be written as 
a simple permutation matrix [ 1000 0001 . . R01. 0 .. ... ... .. l ... 00 10.. The resulting matrix SRS2 
now has the correct eigenstructure for S  the analysis. Its eigenvalues are: 11 n 1 2 9b(23an )b2 
+2 cos(2p )b. m m mb2 +2 cos(2p ) (4) 9nn From [Rei95, Zor97] it is known that for the leading eigenvalues, 
sorted by decreasing modulus, the following necessary conditions have to hold .11 .2 .3 .i bi 4bm m . 
mbn +1 (5) Additionally, according to [Pra98, Zor97], a natural choice for the eigenvalue .4 is .4 .22 
since the eigenstructure of the subdivision matrix can be interpreted as a generalized Taylor-expansion 
of the limit surface at the point p. The eigenvalue .4 then corresponds to a quadratic term in that expansion. 
Consequently, we de.ne the value for an by solving 2 22 2 +2 cos(2p 1 n ) an 39 . which leads to 2p 4 
2 cos() n an (6) 9 where we picked that solution of the quadratic equation for which the coef.cient an 
always stays in the interval 0b1]and (2) is a con­vex combination. The explanation for the existence 
of a second solution is that we actually analyse a double step SSRS2. The real 2 eigenvalue (an )2 of 
SScorresponds to the eigenvalue 2 an of 33 S both with the same eigenvector 3an b1bm m m mb1]which is 
invariant under R. Obviously we have to choose an such that negative real eigenvalues of S are avoided 
[Rei95]. Equations (1), (2) and (6) together completely de.ne the smooth­ing operator for our stationary 
subdivision scheme since they pro­vide all the necessary information to implement the scheme. Notice 
that the spectral properties of the matrices S and SSare not suf.cient for the actual convergence analysis 
of the subdivision scheme. It is only used here to derive the smoothing rule from the necessary con­ditions! 
The suf.cient part of the convergence analysis is presented in the Appendix.  4 Adaptive re.nement strategies 
Although the complexity of the re.ned meshes M k grows slower under v3-subdivision than under dyadic 
subdivision (cf. Fig. 13), the number of triangles still increases exponentially. Hence, only relatively 
few re.nement steps can be performed if the resulting meshes are to be processed on a standard PC. The 
common tech­niques to curb the mesh complexity under re.nement are based on adaptive re.nement strategies 
which insert new vertices only in those regions of the surface where more geometric detail is ex­pected. 
Flat regions of the surface are suf.ciently well approxi­mated by large triangles. The major dif.culties 
that emerge from adaptive re.nement are caused by the fact that triangles from different re.nement lev­els 
have to be joined in a consistent manner (conforming meshes) which often requires additional redundancy 
in the underlying mesh data structure. To reduce the number of topological special cases and to guarantee 
a minimum quality of the resulting triangular faces, the adaptive re.nement is usually restricted to 
balanced meshes where the re.nement level of adjacent triangles must not differ by more than one generation. 
However, to maintain the mesh balance at any time, a local re.nement step can trigger several addi­tional 
split operations in its vicinity. This is the reason why adaptive re.nement techniques are rated by their 
localization property, i.e.,     Figure 5: The gap between triangles from different re.nement levels 
can be .xed by temporarily replacing the larger face by a triangle fan. Figure 6: The gap .xing by triangle 
fans tends to produce degen­erate triangles if the re.nement is not balanced (left). Balancing the re.nement, 
however, causes a larger region of the mesh to be affected by local re.nement (right). by the extend 
to which the side-effects of a local re.nement step spread over the mesh. For re.nement schemes based 
on the dyadic split operation, the local splitting of one triangular face causes gaps if neighboring 
faces are not re.ned (cf. Fig. 5). These gaps have to be removed by replacing the adjacent (unre.ned) 
faces with a triangle fan. As shown in Fig. 6 this simple strategy tends to generate very badly shaped 
triangles if no balance of the re.nement is enforced. If further split operations are applied to an already 
adaptively re­.ned mesh, the triangle fans have to be removed .rst since the cor­responding triangles 
are not part of the actual re.nement hierarchy. The combination of dyadic re.nement, mesh balancing and 
gap .x­ing by temporary triangle fans is well-known under the name red­green triangulation in the .nite 
element community [VT92, Ver96]. There are several reason why v3-subdivision seems better suited for 
adaptive re.nement. First, the slower re.nement reduces the ex­pected average over-tesselation which 
occurs when a coarse triangle slightly fails the stopping criterion for the adaptive re.nement but the 
result of the re.nement falls signi.cantly below the threshold. The second reason is that the localization 
is better than for dyadic re.nement and no temporary triangle fans are necessary to keep the mesh consistent. 
In fact, the consistency preserving adaptive re­.nement can be implemented by a simple recursive procedure. 
No re.nement history has to be stored in the underlying data structure since no temporary triangles are 
generated which do not belong to the actual re.nement hierarchy. To implement the adaptive re.nement, 
we have to assign a gen­eration index to each triangle in the mesh. Initially all triangles of the given 
mesh M 0 are generation 0. If a triangle with even gener­ation index is split into three by inserting 
a new vertex at its center, the generation index increases by 1 (giving an odd index to the new triangles). 
Splitting a triangle with odd generation index requires to .nd its mate , perform an edge .ip, and assign 
even indices to the resulting triangles. For an already adaptively re.ned mesh, further splits are per­formed 
by the following recursive procedure  Figure 7: Adaptive re.nement based on v3-subdivision achieves 
an improved localization while automatically preventing degener­ate triangles since all occuring triangles 
are a subset of the under­lying hierarchy of uniformly re.ned meshes. Let us assume the hor­izontal coarse 
scale grid lines in the images have constant integer y coordinates then the two images result from adaptively 
re.ning all triangles that intersect a certain y const. line. In the left image y was chosen from 12 
]and in the right image y 1 +e which 3 b3 explains the different localization. split(T) if (T.index 
is even) then compute midpoint P  split T(A,B,C) into T[1](P,A,B),T[2](P,B,C),T[3](P,C,A) for i = 
1,2,3 do T[i].index = T.index + 1 if (T[i].mate[1].index == T[i].index) then swap(T[i],T[i].mate[1]) 
 else if (T.mate[1].index == T.index -2) split(T.mate[1]) split(T.mate[1]) /* ... triggers edge swap 
*/ which automatically preserves the mesh consistency and implic­itly maintains some mild balancing 
condition for the re.nement lev­els of adjacent triangles. Notice that the ordering of the vertices in 
the 1-to-3 split is chosen such that reference mate[1] always points to the correct neighboring triangle 
(outside the parent trian­gle T). The edge .ipping procedure is implemented as swap(T1,T2) change T1(A,B,C), 
T2(B,A,D) into T1(C,A,D), T2(D,B,C) T1.index++ T2.index++ All the triangles that are generated during 
the adaptive vre.nement form a proper subset of the uniform re.nement hier­archy. This implies that the 
shape of the triangles does never de­generate. The worst triangles are those generated by an 1-to-3 split. 
Edge .ipping then mostly re-improves the shape. Fig. 7 shows two adaptively re.ned example meshes. Another 
approach to adaptive mesh re.nement with built-in consistency is suggested in [VG00]. When adaptive re.nement 
is performed in the context of station­ary subdivision, another dif.culty arises from the fact that for 
the application of the smoothing rules a certain neighborhood of ver­tices from the same re.nement level 
has to be present. This puts some additional constraints on the mesh balance. In [ZSS97] this is explained 
for Loop subdivision with dyadic re.nement. For v3-subdivision it is suf.cient to slightly modify the 
recur­sive splitting procedure such that before splitting an even-indexed triangle by vertex insertion, 
all older odd-indexed neighbors have to be split (even-indexed neighbors remain untouched). This guar­antees 
that enough information is available for later applications of the smoothing rule (2). The rule (1) is 
always applicable since it only uses the three vertices of the current triangle. Notice that the 1-to-3 
split is the only way new vertices enter the mesh. Moreover, every new vertex eventually has valence 
six although some of its neighbors might not yet be present.   The modi.cation of the recursive procedure 
implies that when a new vertex p is inserted, its neighboring vertices p1 bm m m .bp6 either exist already, 
or at least the triangles exist at whose centers these vertices are going to be inserted. In any case 
it is straightforward to compute the average 1 .i pi which is all we need for the application of (2). 
The remaining technical problem is that in an adaptively re.ned mesh, the geometric location of a mesh 
vertex is not always well­de.ned. Ambiguities occur if triangles from different re.nement levels share 
a common vertex since the smoothing rule (2) is non­interpolatory. We solved this problem by implementing 
a multi­step smoothing rule which enables direct access to the vertex po­sitions at any re.nement level. 
Accessing a Vertex-object by Vertex::pos(k) returns the vertex coordinates corresponding to the kth re.nement 
level. Vertex::pos(inf)returns the cor­responding point on the limit surface which is the location that 
is eventually used for display. Multi-step rules are generalizations of the rule (2) which allow direct 
evaluation of arbitrary powers of S. As we already discussed in Section 3, the 1-ring neighborhood pbp0 
bm m . mbpng1]of a vertex p is mapped to (a scaled version of) itself under application of the subdivision 
scheme. This is re.ected by the matrix S in (3). If we compute the mth power of the subdivision matrix 
in (3), we .nd in the .rst row a linear combination of pbp0 bm m m mbpng1]which directly yields Sm (p). 
For symmetry reason this multi-step rule can, again, be written as a linear combination of the original 
vertex p and the average of its neighbors 1 .i pi. n By eigenanalysis of the matrix S it is fairly straightforward 
to derive a closed form solution for the multi-step rule [Sta98]: ng1 Sm (p): (1 ßn (m)m)p +ßn (m) 1 
. pi (7) n i 0 with 3an 3an ( an ) 3 ßn (m)2 m 1 +3an especially ßn (8)3an 13an f+ Since the point p 
8. S8 (p)on the limit surface is particularly im­portant, we rewrite (7) by eliminating the average of 
p s neighbors Sm (p): .n (m)p +(1 .n (m).)p . 8. (8) with 2 m .n (m) an 3 In our implementation, every 
Vertex-object stores its original po­sition p (at the time it was inserted into the mesh) and its limit 
f 8) position p . The vertex position at arbitrary levels can then be computed by (8).  5 Boundaries 
In practical and industrial applications it is usually necessary to be able to process control meshes 
with well-de.ned boundary poly­gons which should result in surfaces with smooth boundary curves. As the 
neighborhood of boundary vertices is not complete, we have to .gure out special re.nement and smoothing 
rules. When topologically re.ning a given open control mesh M 0 by the v3-operator we split all triangular 
faces 1-to-3 but .ip only the     Figure 8: The boundary is subdivided only in every other step such 
that a uniform 1-to-9 re.nement of the triangular faces is achieved. Figure 9: The use of univariate 
smoothing rules at the boundaries enables the generation of sharp feature lines where two separate control 
meshes share an identical boundary polygon. interior edges. Edge .ipping at the boundaries is not possible 
since the opposite triangle-mate is missing. Hence, the boundary polygon is not modi.ed in the .rst v3-subdivision 
step. As we already discussed in Section 2, the application of a second v3-step has the overall effect 
of a tri-adic split where each original triangle is replaced by 9 new ones. Consequently, we have to 
apply a univariate tri-section rule to the boundary polygon and connect the new vertices to the corresponding 
interior ones such that a uniform 1-to-9 split is established for each boundary triangle (cf. Fig. 8). 
The smoothing rules at the boundaries should only use boundary vertices and no interior ones. This is 
the simplest way to enable the generation of C0 creases in the interior of the surface (feature lines) 
since it guarantees that control meshes with identical bound­ary polygons will result in smooth surfaces 
with identical boundary curves [HDD+94] (cf. Fig. 9). More sophisticated techniques for the design of 
optimal boundary smoothing rules with normal con­trol can be found in [BLZ99]. For our v3-subdivision 
scheme we choose, for simplicity, a univariate boundary subdivision scheme which reproduces cubic splines 
(maximum smoothness, minimum stencil). From the trivial tri-section mask for linear splines we can easily 
obtain the corre­sponding tri-section mask for cubic splines by convolution 1 .12 1b2b3b2b1](1b1b1]) 
33 1 .1 1b3b6b7b6b3b1]1b1b1] 93 1 1b4b10b16b19b16b10b4b1] 27 Hence the resulting smoothing rules are 
1 b p 3ig1 27 (10pig1 +16pi +pi1 ) . p . 1 (4pig1 +19pi +4pib1 )(9) 3i 27 . 1 b p3ib1 27 (pig1 +16pi 
+10pi1 )       i  6 Examples To demonstrate the quality of the v3-subdivision surfaces we show 
a mesh generated by uniformly re.ning a decimated version of the Stanford bunny (cf. Fig 10). The C2 
smoothness of the limit surface guarantees curvature continuity and the relaxing properties of the smoothing 
rules with only positive weights lead to a fair distribution of the curvature. We made several numerical 
experiments to check the relative complexity of the adaptively re.ned meshes M k generated either by 
v3-subdivision or by Loop-subdivision. For the stopping cri­terion in the adaptive re.nement we used 
the local approximation error of the current mesh (with all vertices projected onto the limit surface) 
to the limit surface. A reliable estimation of the exact ap­proximation error can be computed by constructing 
tight bounding envelopes as described in [KDS98]. After testing various models with different geometric 
complexi­ties over the range 10g2 b10g7]for the approximation tolerance, we found that adaptive v3-subdivision 
meshes usually need fewer tri­angles than adaptive Loop-subdivision surfaces to obtain the same approximation 
tolerance. The improvement is typically between 5% and 25% with an average at 10%. Fig. 11 shows the 
typical relation between approximation tolerance and mesh complexity. Fig. 12 shows another example mesh 
generated by the adaptive v3-subdivision scheme in comparison to the corresponding Loop subdivision surface 
de.ned by the same control mesh. This time weuse a curvature dependent adaptive re.nement strategy: The 
subdivision level is determined by a discrete local curvature esti­mation.    7 Conclusion We presented 
a new stationary subdivision scheme which itera­tively generates high quality C2 surfaces with minimum 
compu­tational effort. It shares the advantages of the well-known stan­dard schemes but has important 
additional properties. Especially the slower increase of the mesh complexity and the suitability for 
adaptive re.nement with automatic consistency preservation makes it a promising approach for practical 
and industrial applications. The analysis technique we present in the Appendix provides a simple tool 
to analyse a very general class of subdivision schemes which are not necessarily based on some known 
polynomial spline basis function and not generated by taking the tensor-product of some univariate scheme. 
Future modi.cations and extensions of the v3-subdivision scheme should aim at incorporating more sophisticated 
boundary rules [BLZ99] and interpolation constraints [Lev99]. Modi.cations of the smoothing rules with 
different stencils could lead to new sub­division schemes with interesting properties.  Acknowledgements 
I would like to thank Stephan Bischoff and Ulf Labsik for imple­menting the v3-subdivision scheme and 
performing some of the ex­periments. References [BLZ99] H. Biermann, A. Levin, D. Zorin, Piecewise smooth 
subdivision sur­faces with normal control, Preprint [CC78] E. Catmull, J. Clark, Recursively generated 
B-spline surfaces on arbi­trary topological meshes, CAD 10 (1978), 350 355   [CDM91] A. Cavaretta, 
W. Dahmen, C. Micchelli, Stationary Subdivision, Mem­ oirs of the AMS 93 (1991), pp. 1-186 [DS78] D. 
Doo, M. Sabin, Behaviour of recursive division surfaces near ex­ traordinary points, CAD 10 (1978), 356 
360 [DGL90] N. Dyn, J. Gregory, D. Levin, A Butter.y Subdivision Scheme for Sur­ face Interpolation with 
Tension Controll, ACM Trans. Graph. 9 (1990), pp. 160 169 [Dyn91] N. Dyn, Subdivision Schemes in Computer 
Aided Geometric Design, Advances in Numerical Analysis II, Wavelets, Subdivisions and Radial Functions, 
W.A. Light ed., Oxford University Press, 1991, pp: 36-104. [GSS99] I. Guskov, W. Sweldens, P. Schr¨oder, 
Multiresolution signal processing for meshes, SIGGRAPH 99 Proceedings, 1999, pp. 325 334 [GvL96] G. 
Golub, C. van Loan, Matrix Computations, 3rd, Johns Hopkins Univ Press, 1996 [GZZ93] M. Griebel, C. Zenger, 
S. Zimmer, Multilevel Gauss-Seidel-Algorithms for Full and Sparse Grid Problems, Computing 50, 1993, 
pp. 127 148 [Gus98] I. Guskov, Multivariate subdivision schemes and divided differences, Preprint, Princeton 
University, 1998 [Hac85] W. Hackbusch, Multi-Grid Methods and Applications, Springer, Berlin, 1985 [HDD+94] 
H. Hoppe, T. DeRose, T. Duchamp, M. Halstead, H. Jin, J. McDonald, J. Schweitzer, W. Stuetzle, Piecewise 
smooth surface reconstruction, SIGGRAPH 1994 Proceedings, 1994, pp. 295 302 [Kob96] L. Kobbelt, Interpolatory 
Subdivision on Open Quadrilateral Nets with Arbitrary Topology, Computer Graphics Forum 15 (1996), Eurographics 
96 Conference Issue, pp. 409 420 [KDS98] L. Kobbelt, K. Daubert, H-P. Seidel, Ray-tracing of subdivision 
sur­ faces, 9th Eurographics Workshop on Rendering Proceedings, 1998, pp. 69 80 [KCVS98] L. Kobbelt, 
S. Campagna, J. Vorsatz, H-P. Seidel, Interactive multires­ olution modeling on arbitrary meshes, SIGGRAPH 
98 Proceedings, 1998, pp. 105 114 [Lev99] A. Levin, Interpolating nets of curves by smooth subdivision 
surfaces, SIGGRAPH 99 Proceedings, 1999, pp. 57 64 [Loo87] C. Loop, Smooth subdivision surfaces based 
on triangles, Master The­ sis, Utah University, USA, 1987 [Pra98] H. Prautzsch, Smoothness of subdivision 
surfaces at extraordinary points, Adv. Comp. Math. 14 (1998), pp. 377 390 [Rei95] U. Reif, A uni.ed 
approach to subdivision algorithms near extraordi­ nary vertices, CAGD 12 (1995), pp. 153 174 [RP98] 
U. Reif, J. Peters, The simplest subdivision scheme for smoothing poly­ hedra, ACM Trans. Graph. 16 (1998), 
pp. 420 431 [Sab87] M. Sabin, Recursive Division, in The Mathematics of Surfaces, Claren­ don Press, 
1986, pp. 269 282 [Sta98] J. Stam, Exact evaluation of Catmull/Clark subdivision surfaces at ar­ bitrary 
parameter values, SIGGRAPH 98 Proceeding, 1998, pp. 395 404 [VG99] L. Velho, J. Gomes, Quasi-stationary 
subdivision using four directional meshes, Preprint [VG00] L. Velho, J. Gomes, Semi-regular 4-8 re.nement 
and box spline sur­ faces, Preprint [VT92] M. Vasilescu, D. Terzopoulos, Adaptive meshes and shells: 
Irregular triangulation, discontinuities and hierarchical subdivision, Proceedings of the Computer Vision 
and Pattern Recognition Conference, 1992, 829 832 [Ver96] R. Verf¨urth, A review of a posteriori error 
estimation and adaptive mesh re.nement techniques, Wiley-Teubner, 1996 [War00] J. Warren, Subdivision 
methods for geometric design, unpublished manuscript [ZSS96] D. Zorin, P. Schr¨oder, W. Sweldens, Interpolating 
Subdivision for Meshes with Arbitrary Topology, SIGGRAPH 96 Proceedings, 1996, pp. 189 192 [Zor97] D.Zorin, 
Ck Continuity of Subdivision Surfaces, Thesis, California In­ stitute of Technology, 1997 [ZSS97] D. 
Zorin, P. Schr¨oder, W. Sweldens, Interactive multiresolution mesh editing, SIGGRAPH 97 Proceedings, 
1997, pp. 259 268  Although the quality of the limit surfaces is the same (C2), v3-subdivision uses 
an alternative re.nement operator that increases the number 9 of triangles slower than Loop s. The relative 
complexity of the corresponding meshes from both rows is (from left to right) 3 075, 4 16 056, and 27 
042. Hence the new subdivision scheme yields a much .ner gradation of uniform hierarchy levels. 64  
   Appendix: Convergence analysis The convergence analysis of stationary subdivision schemes is gen­erally 
done in two steps. In the .rst step, the smoothness of the limit surface is shown for regular meshes, 
i.e. for triangle meshes with all vertices having valence 6. Due to the nature of the topo­logical re.nement 
operator, subdivided meshes M k are regular al­most everywhere. Once the regular case is shown, the convergence 
in the vicinity of extraordinary vertices (with valence =6) can be proven. For many existing subdivision 
schemes, the .rst part of the proof is trivial since a closed form representation of the limit sur­face 
in the regular case is known, e.g. B-splines for Catmull/Clark or Doo/Sabin surfaces, Box-splines for 
Loop-surfaces. For the two steps in the proof different techniques have to be used. The smoothness of 
the limit surface for regular control meshes follows from the contractivity of certain difference schemes 
Sn. These are generalized subdivision schemes which map direc­tional forward differences of control points 
directly to directional forward differences (instead of the original subdivision scheme S mapping control 
points to control points). In the vicinity of the extraordinary vertices, the convergence analysis is 
based on the eigenstructure of the local subdivision ma­trix. It is important to notice that the criteria 
for the eigenstructure of the subdivision matrix do only apply if the convergence in the regular regions 
of the mesh is guaranteed [Rei95, Zor97]. In the following we present a general technique for the analy­sis 
of subdivision schemes on regular meshes which we will use to prove the smoothness of the v3-subdivision 
limit surface. Never­theless, the technique also applies to a larger class of non-standard subdivision 
schemes. Another analysis technique that is also based on a matrix formulation is used in [War00]. Regular 
meshes Instead of using the standard generating function notation for the handling of subdivision schemes 
[Dyn91], we propose a new matrix formulation which is much easier to handle due to the analogy with the 
treatment of the irregular case. In fact, rotational symmetries of the subdivision rules are re.ected 
by a blockwise circulant structure of the respective matrices just like in the vicinity of extraordinary 
vertices. Our matrix based analysis requires only a few matrix com­putations which can easily be performed 
with the help of Mapleor MatLab. In contrast, the manipulation of the corresponding gen­erating functions 
would be quite involved if the subdivision scheme does not have a simple factorization (cf. [CDM91, Dyn91]). 
To prove the contractivity of some difference scheme, it is suf­.cient to consider a local portion of 
a (virtually) in.nite regular triangulation. This is due to the shift invariance of the subdivision scheme 
(stationary subdivision). Hence, similarly to the treatment of extraordinary vertices, we can pick an 
arbitrary vertex p and a 10, D01D2 01D10, and D013 . 10,D2 Figure 15: The two re.ned neighborhoods 
Sm (Vp )and Sm (Vq ) (grey areas) of the (formerly) adjacent vertices p and q have to overlap (dark area) 
such that every possible directional difference can be computed from either one. suf.ciently large neighborhood 
V around it. The size of this neigh­borhood is determined by the order n of the differences that we want 
to consider and by the number m of subdivision steps we want to combine (the analysis of one single subdivision 
step often does not yield a suf.cient estimate to prove contractivity). For a given subdivision scheme 
S the neighborhoods have to be chosen such that for two adjacent vertices p and q in M k the corresponding 
sets Sm ()and Sm ()in the re.ned mesh M kbhave enough Vp Vq m overlap to guarantee that the support of 
each nth order directional difference is contained in either one (cf. Fig 14). In our case we want to 
prove C2 continuity and hence have to show contractivity of the 3rd directional difference scheme. For 
technical reasons we always combine an even number of v3­subdivision steps since this removes the 30 
degree rotation of the grid directions (just like we did in Section 3). To guarantee the re­quired overlap, 
we hence have to use a 3-ring neighborhood if we analyse one double v3-step and a 6-ring neighborhood 
if we anal­yse two double v3-steps. The corresponding subdivision matrices are 37 37 and 127 127 respectively 
(cf. Fig 15). We start by introducing some notation: A regular triangulation is equivalent to the three 
directional grid which is spanned by the directions 101 v01bv10bv11 0 1 1 in index space. Hence the 
two types of triangular faces in the mesh are given by i(pi j bpib1 j bpib1 jb1 )and i(pi j bpib1 jb1 
bpi jb1 ). Accordingly, we de.ne the three directional difference operators Duv : pi j pibu jbv pi j 
with (ubv) E(1b0)bm(0b1)bm(1b1M ). If we apply these difference op­erators Duv to a .nite neighborhood 
V we obtain all possible differ-   10, one for D2 twist vectors for the mixed derivative D10D01. ences 
where both pi j and pibu jbv are elements of V. For a .xed neighborhood V the operator Duv can be represented 
by a matrix that has two non-zero entries in every row, e.g., 0110 1 10 t  V bbbbbb 0 0 1 1 0 1 1 .. 
implies 110 0 0 00 001 1 0 00 [l D10 100 0 1 00 000 0 0 11 See Fig. 16 for a geometric interpretation. 
Based on the difference operators, we can build the Jet-operators D10D10D10 D10D10 D10 D10D10D01J1D01 
J2 D10D01 J3 [D10D01D01 l(10) D01D01 D01D01D01 which map the control vertices in V to the complete set 
of indepen­dent directional differences Jn (V)of a given order n. Let S be the subdivision scheme which 
maps control vertices k p f)from the kth re.nement level to the (k 1)st re.nement level f b) f)+k1k p 
S(p ). Again, if we consider the action of S on a local neighborhood V only, we can represent S by a 
matrix with each row containing an af.ne combination that de.nes the position of one new control vertex. 
For the convergence analysis we need a so-called differ­ k ence scheme Sn which maps the differences 
Jn (V f))directly to f ff b k1kk Jn (V ))Jn (S(V ))m)Sn (Jn (V )).). From [Dyn91] it is well­known that 
the subdivision scheme S generates Cn limit surfaces b (for regular control meshes) if the scheme hnSn 
1 is contractive, i.e., if Sn b1 n q hgn with respect to an appropriate matrix norm. Here, the factor 
hn takes the implicit parameterization into account. For subdivision schemes which are based on the dyadic 
split operation, edges are bi-sected in every step and hence h 2. This is true for all standard schemes. 
However, for our new v3­subdivision scheme we have to choose h 3 since we are analysing the double application 
of the v3-operator which corresponds to an edge tri-section. In the univariate case these difference 
schemes Sn can be ob­tained by simple factorization of the corresponding generating function representations. 
In the bivariate case the situation is much more dif.cult since jets are mapped to jets! In general we 
can­not .nd a simple scheme which maps, e.g., the differences D10 (V) to D10 (S(V).)because the directional 
differences are not indepen­dent from each other. Hence we have to .nd a more general matrix scheme D10 
(S(V)m)D10 (V) D01 (S(V)m) S1D01 (V) which maps J1 (V)to J1 (S(V)m)by allowing D10 (S(V)m)to depend 
on both D10 (V)and D01 (V). As this construction requires quite involved factorizations and other polynomial 
transformations, we now suggest a simpler approach where most of the computation can be done automatically. 
 g1 Let Jn by the nth jet-operator restricted to V and Jn its SVD pseudo-inverse. Because Jn has a non-trivial 
kernel (containing all con.gurations where the points in V are uniformly sampled from a degree n 1 polynomial) 
its inverse cannot be well-de.ned. At least we know that g1 JnJJn Jn n g1 which means that if Jn is applied 
to a set of nth order differences Jn (V)it reconstructs the original data up to an error e which lies 
in the kernel of Jn, i.e., Jg1 (Jn (V).)V +e with Jn (e)0. n If the subdivision scheme S has polynomial 
precision of order n 1 this implies that S maps the kernel of Jn into itself: . S(ker(Jn )m)ker(Jn )(11) 
As a consequence Jn (S(e)m)0 as well, and therefore g1 JnSJJn JnS n Since the operator on the right hand 
side of this equation maps the b f k) vertices of the control mesh V to the nth differences on the next 
k1 re.nement level Jn (V f)), the operator g1 Sn : Jn SJn (12) does map the nth differences Jn (V f))directly 
to the nth differences f bk 1) on the next level Jn (V k). This is exactly the difference scheme that 
we have been looking for! In order to prove the convergence of the subdivision scheme, we have to show 
that the maximum norm g of hn1Sn is below 1. Alternatively, it is suf.cient to show that g the maximum 
singular value of the matrix hn1Sn is smaller than 1 since this provides a monotonically decreasing upper 
bound for the maximum nth difference. To verify the polynomial precision (11) for a given subdivision 
matrix S we .rst generate another matrix K whose columns span the kernel of Jn. Notice that the dimension 
of ker(Jn )is the di­mension of the space of bivariate degree n 1 polynomials which 1 is dim .2 ng1 (n 
+1)n. The matrix K can be read off from the 2 SVD decomposition of Jn [GvL96]. The polynomial reproduction 
is then guaranteed if the equation SK KX (13) g has a matrix solution X (KTK)1KT SK. If this is satis.ed, 
we .nd the nth difference scheme Sn by (12). For the analysis of our v3-subdivision scheme we let V be 
the 6-ring neighborhood of a vertex which consists of 127 vertices. Let S be the single-step v3-subdivision 
matrix, R be the back-rotation­by-permutation matrix and D10 the directional difference matrix. Although 
these matrices are quite large, they are very sparse and  can be constructed quite easily (by a few 
lines of MatLab-code) due to their block-circulant structure. From these matrices we compute SSRS2 and 
a second di­g2 rectional difference operator D01 R2 D10 R. The two direc­tional differences are combined 
to build the 3rd order jet-operator J3 (cf. (10)). Here we use the 3rd differences since we want to prove 
C2 continuity. From the singular value decomposition of J3 we obtain the matrix K whose columns span 
the kernel of J3 and g the pseudo-inverse J1. The matrix K is then used to prove the 3 quadratic precision 
of S (cf. (13)) and the pseudo-inverse yields the difference scheme SJ3 SJg1. The contractivity of the 
3rd S3 S 3 order difference scheme .nally follows from the numerical esti­mation SS2 J3 SS2 Jg1 078 3g4 
which proves that the 33 v3-subdivision scheme S generates C2 surfaces for regular control meshes.  
Extraordinary vertices In the vicinity of the extraordinary vertices with valence =6 we have to apply 
a different analysis technique. After the convergence in the regular mesh regions (which for subdivision 
meshes means almost everywhere ) has been shown, it is suf.cient to analyse the behavior of the limit 
surface at the remaining isolated extraordinary points. The intuition behind the suf.cient convergence 
criteria by [Rei95, Zor97, Pra98] is that the representation of the local neigh­borhood V with respect 
to the eigenvector basis of the local subdivi­sion matrix S corresponds to a type of Taylor-expansion 
of the limit surface at that extraordinary point. Hence, the eigenvectors ( eigen­functions ) have to 
satisfy some regularity criteria and the leading eigenvalues have to guarantee an appropriate scaling 
of the tangen­tial and higher order components of the expansion. Especially the conditions (5) have to 
be satis.ed for all valences n 3bm m . mbnmax. When checking the eigenstructure of the subdivision matrix 
S we have to use a suf.ciently large r-ring neighborhood V of the center vertex p. In fact the neighborhood 
has to be large enough such that the regular part of it de.nes a complete surface ring around p by itself 
[Rei95]. In the case of v3-subdivision we hence have to use r 4 rings around p (since 4 is the diameter 
of the subdivision basis function s support). This means we have to analyse a (10n +1)(10n +1)matrix 
where n is p s valence. Luckily the subdivision matrix S has a block circulant structure and it turns 
out that the leading eigenvalues of S are exactly the eigenvalues we found in (4). Since those eigenvalues 
satisfy (5) we conclude that the matrix S has the appropriate structure for C1 convergence. The exact 
condition on the eigenvectors and the injectivity of the corresponding characteristic map are quite dif.cult 
to check strictly. We therefore restrict ourselves to the numerical veri.cation by sketching the iso-parameter 
lines of the characterisitc map in Fig. 17.    
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344841</article_id>
		<sort_key>113</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Piecewise smooth subdivision surfaces with normal control]]></title>
		<page_from>113</page_from>
		<page_to>120</page_to>
		<doi_number>10.1145/344779.344841</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344841</url>
		<abstract>
			<par><![CDATA[<p>In this paper we introduce improved rules for Catmull-Clark and Loop subdivision that overcome several problems with the original schemes, namely, lack of smoothness at extraordinary boundary vertices and folds near concave corners. In addition, our approach to rule modification allows the generation of surfaces with prescribed normals, both on the boundary and in the interior, which considerably improves control of the shape of surfaces.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[boundary control]]></kw>
			<kw><![CDATA[subdivision surfaces]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.1</cat_node>
				<descriptor>Interpolation formulas</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.3</cat_node>
				<descriptor>Eigenvalues and eigenvectors (direct and iterative methods)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003722</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Interpolation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P109318</person_id>
				<author_profile_id><![CDATA[81100014491]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Henning]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Biermann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[New York University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14143101</person_id>
				<author_profile_id><![CDATA[81100404854]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Adi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Levin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tel Aviv University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39037916</person_id>
				<author_profile_id><![CDATA[81100328351]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Denis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zorin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[New York University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Subdivision for modeling and animation. SIGGRAPH 2000 Course Notes.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ed Catmull and James Clark. Recursively generated B-spline surfaces on arbitrary topological meshes. Computer Aided Design, 10(6):350-355, 1978.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280826</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Tony DeRose, Michael Kass, and Tien Truong. Subdivision surfaces in character animation. In Michael Cohen, editor, SIGGRAPH 98 Conference Proceedings, Annual Conference Series, pages 85-94. ACM SIGGRAPH, Addison Wesley, July 1998. ISBN 0-89791-999-8.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[D. Doo. A subdivision algorithm for smoothing down irregularly shaped polyhedrons. In Proceedings on Interactive Techniques in Computer Aided Design, pages 157-165, Bologna, 1978.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[D. Doo and M. Sabin. Analysis of the behaviour of recursive division surfaces near extraordinary points. Computer Aided Design, 10(6):356-360, 1978.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>314305</ref_obj_id>
				<ref_obj_pid>314304</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Ayman Habib and Joe Warren. Edge and vertex insertion for a class of C 1 subdivision surfaces. Computer Aided Geometric Design, 16(4):223-247, 1999.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166121</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Mark Halstead, Michael Kass, and Tony DeRose. Efficient, fair interpolation using Catmull-Clark surfaces. In Computer Graphics Proceedings, Annual Conference Series, pages 35-44. ACM Siggraph, 1993.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192233</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Hugues Hoppe, Tony DeRose, Tom Duchamp, Mark Halstead, Huber Jin, John McDonald, Jean Schweitzer, and Werner Stuetzle. Piecewise smooth surface reconsruction. In Computer Graphics Proceedings, Annual Conference Series, pages 295-302. ACM Siggraph, 1994.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311541</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Adi Levin. Interpolating nets of curves by smooth subdivision surfaces. In Alyn Rockwood, editor, SIGGRAPH 99 Conference Proceedings, Annual Conference Series, pages 57-64. Addison Wesley, 1999.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Charles Loop. Smooth subdivision surfaces based on triangles. Master's thesis, University of Utah, Department of Mathematics, 1987.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[A. Nasri. Interpolation of open B-spline curves by recursive subdivision surfaces. In Tim Goodman and Ralph Martin, editors, Mathematics of Surfaces VII, pages 173 -188. Institute of mathematics and its applications, Information Geometers, 1997.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>27628</ref_obj_id>
				<ref_obj_pid>27625</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Ahmad H. Nasri. Polyhedral subdivision methods for free-form surfaces. ACM Transactions on Graphics, 6(1):29-73, January 1987.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>124971</ref_obj_id>
				<ref_obj_pid>124966</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Ahmad H. Nasri. Boundary corner control in recursive subdivision surfaces. Computer Aided Design, 23(6):405-410, 1991.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>116112</ref_obj_id>
				<ref_obj_pid>116105</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Ahmad H. Nasri. Surface interpolation on irregular networks with normal conditions. Computer Aided Geometric Design, 8:89-96, 1991.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>276485</ref_obj_id>
				<ref_obj_pid>276465</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Jorg Peters and Ulrich Reif. Analysis of algorithms generalizing B-spline subdivision. SIAM Journal on Numerical Analysis, 35(2):728-748 (electronic), 1998.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>730746</ref_obj_id>
				<ref_obj_pid>647586</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[H. Prautzsch and G. Umlauf. A G 2 -subdivision algorithm. In Geometric modelling (Dagstuhl, 1996), pages 217-224. Springer, Vienna, 1998.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Hartmut Prautzsch and Ulrich Reif. Degree estimates for C k -piecewise polynomial subdivision surfaces. Adv. Comput. Math., 10(2):209-217, 1999.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>211166</ref_obj_id>
				<ref_obj_pid>211163</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Ulrich Reif. A unified approach to subdivision algorithms near extraordinary points. Computer Aided Geometric Design, 12:153-174, 1995.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>923941</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[J. E. Schweitzer. Analysis and Application of Subdivision Surfaces. PhD thesis, University of Washington, Seattle, 1996.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280942</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Thomas W. Sederberg, Jianmin Zheng, David Sewell, and Malcolm Sabin. Nonuniform recursive subdivision surfaces. In Michael Cohen, editor, SIGGRAPH 98 Conference Proceedings, Annual Conference Series, pages 387-394. ACM SIGGRAPH, Addison Wesley, July 1998. ISBN 0-89791-999-8.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280945</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Jos Stam. Exact evaluation of Catmull-Clark subdivision surfaces at arbitrary parameter values. In SIGGRAPH 98 Converence Proceedings, Annual Conference Series, pages 395-404. Addison Wesley.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Joe Warren. Subdivision methods for geometric design. Unpublished manuscript, November 1995.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>588547</ref_obj_id>
				<ref_obj_pid>588310</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Denis Zorin. A method for analysis of C 1 -continuity of subdivision surfaces. 1998. submitted to SIAM Jornal of Numerical Analysis.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Denis Zorin. Smoothness of subdivision on irregular meshes. Constructive Approximation, 16(3), 2000.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Denis Zorin, Tom Duchamp, and H. Biermann. Smoothness of subdivision surfaces on the boundary. Technical report, New York University, Dept. of Computer Scinece, 2000.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>927147</ref_obj_id>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Denis N. Zorin. Subdivision and Multiresolution Surface Representations.PhD thesis, Caltech, Pasadena, California, 1997.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Piecewise Smooth Subdivision Surfaces with Normal Control Henning Biermann* Adi Levin Denis Zorin New 
York University Tel Aviv University New York University Abstract In this paper we introduce improved 
rules for Catmull-Clark and Loop subdivision that overcome several problems with the origi­nal schemes, 
namely, lack of smoothness at extraordinary bound­ary vertices and folds near concave corners. In addition, 
our ap­proach to rule modi.cation allows the generation of surfaces with prescribed normals, both on 
the boundary and in the interior, which considerably improves control of the shape of surfaces. CR Categories 
and Subject Descriptors: I.3.5 [Computer Graphics]: Computa­tional Geometry and Object Modeling Curve, 
surface, solid, and object representa­tions; Boundary representations. Additional Keywords: Subdivision 
surfaces, boundary control. 1 Introduction Subdivision surfaces are rapidly gaining popularity in computer 
graphics. A number of commercial systems use subdivision as a surface representation: Alias|Wavefront 
s Maya, Pixar s Render­man, Nichimen s Mirai, and Micropace Lightwave 3D, to name just a few. The greatest 
advantage of subdivision algorithms is that they ef.ciently generate smooth surfaces from arbitrary initial 
meshes. Subdivision algorithms are also attractive because they are conceptually simple and can be easily 
modi.ed to create surface features without making major changes to the algorithm. At the same time, one 
of the drawbacks of subdivision is a lack of precise de.nition of the schemes with guaranteed behavior 
for a suf.ciently general type of control meshes. Anyone who tries to implement a subdivision scheme 
can observe that more often than not it is unclear how rules should be speci.ed in certain cases (most 
commonly on boundaries and creases). Ad hoc solutions have to be used, which often have unexpected undesirable 
behavior. The lack of precise and complete de.nition makes it more dif.cult to exchange data between 
applications, reuse control meshes, and de­sign new algorithms based on subdivision. The dif.culty in 
de.ning a reasonably complete set of subdivi­sion rules is related to the fact that subdivision algorithms 
allow a large variety of data as input: an arbitrary polygonal or triangular mesh, possibly with boundary, 
marked edges, and vertices. Subdi­vision rules for the interior of a control mesh are well understood, 
while the boundary rules have received less attention. Boundary rules are quite important for a variety 
of reasons. The boundary of the surface, together with the contour lines, forms the visual outline. *biermann@mrl.nyu.edu 
adilev@math.tau.ac.il dzorin@mrl.nyu.edu Permission to make digital or hard copies of part or all of 
this work or personal or classroom use is granted without fee provided that copies are not made or distributed 
for profit or commercial advantage and that copies bear this notice and the full citation on the first 
page. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior 
specific permission and/or a fee. SIGGRAPH 2000, New Orleans, LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 
...$5.00 Often, only an approximate de.nition is required for the interior of the surface, whereas the 
boundary conditions may be signi.cantly more restrictive. For example, it is often necessary to join 
several surfaces along their boundaries. Boundary subdivision rules lead to rules for sharp creases [8] 
and soft creases [3]. In addition to specifying the boundary or crease curves, it is often desirable 
to be able to specify tangent planes on the boundary; existing subdivision schemes do not allow to control 
tangent plane behavior. The goal of this paper is to present two complete sets of subdivi­sion rules 
for generating piecewise-smooth, C1-continuous, almost everywhere C2 subdivision surfaces, with tangent 
plane control. Our rules extend the well-known subdivision schemes of Catmull-Clark [2] and Loop [10]. 
The properties of our schemes were rigor­ously veri.ed. We use a uniform approach to derive a set of 
rules, including new rules for concave corners, improved smooth bound­ary rules, new rules for tangent 
plane modi.cation, and C2 rules. While our approach is based on a number of known ideas, its advan­tage 
is that all desired features are handled in a uni.ed framework. Our approach to building a complete set 
of rules can be applied to any stationary subdivision scheme. In this paper, we focus on the Loop and 
Catmull-Clark subdivision schemes as schemes hav­ing the greatest practical importance. The code implementing 
our algorithms is available on the Web1.  2 Previous Work A number of subdivision schemes have been 
proposed since Cat­mull and Clark introduced subdivision surfaces in 1978 [2]. A de­tailed survey of 
subdivision can be found in [1]. Theoretical analysis of subdivision rules was performed in [18, 15, 
6, 19, 24, 23]. Most of this work has focused on closed sur­faces; while the general theory does not 
impose symmetry restric­tions on the subdivision rules, almost all theoretical analysis of spe­ci.c schemes 
relies on the rotational symmetry of the subdivision rules and applies only to the interior rules. Subdivision 
rules for Doo-Sabin dual surfaces for the boundary were discussed by Doo [4] and Nasri [12, 13, 11], 
but only partial theoretical analysis was performed. Our work builds on the work of Hoppe et al. [8] 
and partially on the ideas of Nasri [14]. To the best of our knowledge, the boundary subdivision rules 
proposed in work [8] are the only ones that result in provably C1­continuous surfaces (the analysis can 
be found in Schweitzer [19]). However, these rules suffer from two problems: The shape of the boundary 
of the generated surface depends on the control points in the interior;  Only one rule for corners is 
de.ned, which works well for convex  corners but does not work well for concave corners. Standard Catmull-Clark 
rules, when applied to the boundary, suffer from the same problems. Sederberg et al. [20] proposed a 
generalization of Catmull-Clark and Doo-Sabin subdivision rules that contains NURBS as a subset. For 
some applications it is important to include NURBS patches, however, the complexity of the algorithms 
is increased and the be­havior of the surface near the extraordinary points becomes dif.cult 1http://www.mrl.nyu.edu/biermann/sub 
D HQ1 Q3 Q0 Figure 1: The charts for a surface with piecewise smooth boundary. to analyze and predict. 
The smooth crease effects that are obtained by manipulating NURBS weights for subdivision surfaces can 
be achieved using an elegant technique proposed by DeRose et al. [3]. Our approach to C2 subdivision 
is similar to the approach of [16]. Levin recently introduced a combined subdivision scheme which interpolates 
a network of curves [9]. There are two main distinc­tions between the present work and [9]. First, we 
are solving a dif­ferent problem: rather than assuming that we are given a network of smooth curves that 
has to be interpolated, we assume only a dis­crete mesh with tags, which controls the behavior of our 
surface, but no interpolation is required. Second, Levin s combined subdi­vision schemes are an interesting 
new research direction; not much is known and understood about their behavior, especially on arbi­trary 
meshes. In contrast, we focus on completing the subdivision toolbox with provably reliable tools. Halstead 
et al. [7] describe a method of interpolating posi­tions and normal direction on subdivision surfaces. 
However, this method involves the solution of a global system of equations, unlike our local subdivision 
rules. 3 Piecewise smooth surfaces Piecewise smooth surfaces. Our goal is to design subdivision schemes 
for the class of piecewise smooth surfaces. This class in­cludes common modeling primitives such as quadrilateral 
free-form patches with creases and corners. However, we exclude certain sin­gularities (e.g., cone-like 
singularities and corners). Here we give a somewhat informal description of piecewise­smooth surfaces, 
mathematical details will be presented elsewhere [25]. For simplicity, we consider only surfaces without 
self­intersection. Recall that for a closed C1-continuous surface in R3, each point has a neighborhood 
that can be smoothly deformed (that is, there is a C1 map of maximal rank) into an open planar disk D. 
A sur­face with a smooth boundary can be described in a similar way, but neighborhoods of boundary points 
can be smoothly deformed into a half-disk H, with closed boundary (Figure 1). In order to allow piecewise 
smooth boundaries, we introduce two additional types of local charts: concave and convex corner charts, 
Q3 and Q1. We conclude that a C1-continuous surface with piecewise smooth boundary looks locally like 
one of the domains D, H, Q1, or Q3. Piecewise-smooth surfaces are constructed out of surfaces with piecewise 
smooth boundaries joined together. If two surface patches have a common boundary, but different normal 
directions along the boundary, the resulting surface has a sharp crease. We allow two adjacent smooth 
segments of a boundary to be joined, producing a crease ending in a dart (cf. [8]). For dart ver­tices 
an additional chart Q0 is required; the surface near a dart can be deformed into this chart smoothly 
everywhere except at an open edge starting at the center of the disk. It is important to observe that 
convex and concave corners, while being equivalent topologically, are not differentially equivalent. 
That is, there is no C1 nondegenerate map from Q1 to Q3. There­fore, a single subdivision rule can not 
produce both types of cor­ners [26]. In general, any complete set of subdivision rules should contain 
separate rules for all chart types. Most, if not all, known schemes miss some of the necessary rules. 
 4 Problems with common rules In this section, we demonstrate some problems of existing subdivi­sion 
rules. We will see that not all piecewise-smooth surfaces can be adequately represented in these schemes. 
Concave corners. Concave corners often arise in modeling tasks (e.g., surfaces with holes). In an attempt 
to model such a corner with subdivision surfaces, one might arrange the control mesh in a concave con.guration 
and expect the surface to approximate the con.guration. However, the corner rules of popular subdivision 
schemes (e.g., [8]) can only generate convex corners. If the control mesh is in a concave con.guration, 
the rules force the surface to ap­proach the corner from the outer, convex, side, causing the surface 
to develop a fold (Figure 2). Figure 2: Upper row: behavior of a subdivision surface when rules of Hoppe 
et al. [8] are applied near a corner of the control mesh. As the corner of the control mesh is moved, 
the surface develops a fold. Lower row: our concave corner rules applied to the same mesh. The concave 
rules produce a small fold if applied to a con­vex control mesh con.guration (not visible in the picture). 
For a concave con.guration, our rule produces surfaces without folds. Boundary rules. Hoppe et al. [8] 
observed that standard subdivi­sion rules fail to produce smooth surfaces at extraordinary bound­ary 
vertices. They propose to change the subdivision scheme for the boundary curve in order to generate smooth 
surfaces. However, the boundary curve now depends on the interior of the control mesh. More speci.cally, 
the number of the interior vertices adjacent to each boundary vertex. This side effect is undesirable 
if one wants to join surfaces along their boundary curves: Two separate meshes might initially have the 
same boundary, but after subdivision a gap between the meshes can appear (Figure 6). Moreover, even though 
the rules of [8] are formally smooth, they might produce undesirable sharp normal transitions if the 
control mesh is twisted (Figure 7).  5 Subdivision and eigenanalysis In this section, we brie.y state 
several facts of the theory of subdi­vision [1], which are helpful to understand the problems described 
above and our solutions. Subdivision algorithms recursively re.ne a control mesh, recom­puting vertex 
positions and inserting new vertices on edges (and possibly faces). Our method of constructing subdivision 
rules is based on manip­ulating the eigenstructure of subdivision matrices associated with most common 
subdivision rules. This idea can be traced back to [5]. Consider a vertex v, and let pbe the vector of 
control points in a neighborhood of the vertex (Figure 3). Let Sbe the matrix of subdivision coef.cients 
relating the vector of control points p m on subdivision level m to the vector of con­trol points p m+1 
on a similar neighborhood on the next subdivision level. Suppose the size of the matrix is N. Many properties 
of the subdivision scheme can be deduced from the eigenstructure of S. Let us decompose the vector of 
control points pwith respect to the 012 eigenbasis {x i}, i=0..N-1,of S, p =a0x +a1x +a2x +... (it exists 
in the cases of importance to us). Note that we decompose a vector of 3D points: the coef.cients ai are 
3D vectors, which are componentwise multiplied with eigen­vectors x i . We assume that the eigenvectors 
x i are arranged in the order of non-increasing eigenvalues. For a convergent scheme, the .rst eigenvalue 
p0 is 1, and the eigenvector x0 has all components equal to one; this is also required for invariance 
with respect to rigid and, more generally, arbitrary af.ne transformations. Subdividing the surface mtimes 
means that the subdivision ma­trix is applied mtimes to the control point vector p. m 0 m 1 m 2 Sm p 
=p0 a0x +p1 a1x +p2 a2x +··· (Iterated Subdivision) If we further assume that p1 and p2 are real and 
equal, and p1 = p2 =p> |p3|, we see from this formula that the vector of control 0 m12 points p m can 
be approximated by a0x +p(a1x +a2x ); the rest of the terms decay to zero faster. If a1 ×a2 is not zero, 
then all of the control points p mi are close to the plane passing through a0 and spanned by vectors 
a1 and a2.As m.=, the positions of all points converge to a0. This means that the limit position of the 
center vertex is a0; the tangent directions at this position are a1 and a2. We compute these values using 
the left eigenvectors of S (i.e., vectors li, satisfying (li ,x i)=1and (li ,xj )=0if i li ,p). =j): 
ai =( These observations form the basis of our method: to ensure con­vergence to the tangent plane, we 
decrease the magnitudes of all eigenvalues except for those that correspond to the vectors a1, a2 spanning 
the desired tangent plane. We also modify the vectors a1 and a2 to change the direction of the normal. 
It should be noted that obtaining the correct spectrum of the subdivision matrix is not suf.cient for 
smoothness analysis of subdivision; once our rules are formulated, we still have to prove that the resulting 
surfaces are C1 , using the characteristic map analysis [25]. m q 1 m q 0 pc pc Figure 3: Neighborhoods 
of a vertex on different subdivision levels. The subdivision matrix relates the vector of control points 
p m to the control points on the next level p m+1 . For a neighborhood of m mmm m c 0 k triangles p = 
{p ,p ...pk-1}, for k quadrilaterals p = mm mmm {pc ,p0 ...pk-1,q0 ...qk-1}  6 Algorithm 6.1 Tagged 
meshes Before describing our set of subdivision rules, we start with the de­scription of the tagged meshes 
which our algorithms accept as in­put. We use these meshes to represent piecewise-smooth surfaces: edges 
and vertices of the mesh can be tagged to generate the singu­larities described in Section 3. The complete 
list of tags is as follows. Edges can be tagged as crease edges. A vertex with incident crease edges 
receives one of the following tags: crease vertex: joins exactly two incident crease edges smoothly. 
 corner vertex: connects two or more creases in a corner (convex or concave).  dart vertex: causes 
the crease to blend smoothly into the surface. We require that all edges on the boundary of the mesh 
are tagged as crease edges. Boundary vertices are tagged as corner or crease vertices. Crease edges divide 
the mesh into separate patches, several of which can meet in a corner vertex. At a corner vertex, the 
creases meeting at that vertex separate the ring of triangles around the ver­tex into sectors. We label 
each sector of the mesh as convex sector or concave sector indicating how the surface should approach 
the corner. The only restriction that we place on sector tags is that we require  concave sectors to 
consist of at least two faces. An example of a tagged mesh is given in Figure 4. Figure 4: Crease edges 
meeting in a corner with two convex (light grey) and one concave (dark grey) sectors. Our subdivision 
scheme modi.es the rules for edges incident to crease vertices (e.g., e1) and corners (e.g. e2). In our 
implementation, the user applies the tags interactively, and the user interface prohibits an inconsistently 
tagged mesh (for ex­ample, there cannot be a corner vertex with some sector untagged). Also, the user 
can specify normal directions and .atness parame­ters for untagged vertices, crease vertices, and for 
each sector at a corner vertex. The .atness parameter determines how quickly the surface approaches the 
tangent plane in the neighborhood of a con­trol point. This parameter is essential to our concave corner 
rules. Additionally, it improves the user control over the surface, for ex­ample, one can .atten a twist 
in the mesh (as shown in Figure 7). It is important to note however, that while manipulating these pa­rameters 
is possible, it is not necessary: we provide default values reasonable for most situations (Section 6.2). 
 6.2 Subdivision rules We describe our sets of rules for the triangular and quadrilateral schemes in 
parallel, as they are structurally very similar. Our algorithm consists out of two stages, which, if 
desired, can be merged, but are conceptually easier to understand separately. The .rst stage is a single 
iteration over the mesh during which we re.ne the position of existing vertices (vertex points) and insert 
new vertices on edges (edge points). For the quadrilateral scheme, we also need to insert vertices in 
the centers of faces (face points). The .rst stage is similar to one subdivision step of standard algorithms, 
but the weights that we use are somewhat different. In the following we refer to the rules of Loop and 
Catmull-Clark as standard rules. Vertex points. We apply the standard vertex rules to reposition un­tagged 
vertices and dart vertices. The new control point at a vertex is the weighted average of the control 
points in its neighborhood. If a vertex has k adjacent polygons, then its new position is a combination 
of the old position with weight 5/8and of the sum all surrounding control points with weight 3/8k, for 
k . =3In case k =3we use a special set of coef.cients with the weight of the central vertex equal to 
7/16 [22]. For the quadrilateral scheme, the center vertex has weight 1- (1 - (2, while all adjacent 
vertices have weight (1/k; the remaining vertices in the ring receive weight (2/kwith (1 =3/(2k)and (2 
=1/(4k). A crease vertex is re.ned as the average of its old position with weight 3/4and the two adjacent 
crease vertices with weight 1/8 each. Corner vertices are interpolated. Face points. For the quadrilateral 
scheme we insert a vertex at the centroid of each face; only one rule is necessary. Edge points. This 
is the most complicated case. We choose the rule for an edge point depending on the tag of the edge and 
the tags of adjacent vertices and sectors. In the absence of tags, we apply the standard edge rules. 
The averaging masks are given in Figure 5. 111 111 8 16 16 8 1616 3 8  . 111 111 8 1616 8 16 16 Standard 
edge rules Modi.ed edge rules Figure 5: Edge rules for triangular and quadrilateral schemes. These rules 
apply to untagged edges. When both endpoints are untagged, we use standard rules. In case of a tagged 
endpoint we modify the rule such that the tagged endpoint (marked with a circle) receives coef.cient 
3/4- .. We insert a new vertex on a crease edge as the average of the two adjacent vertices. The remaining 
case of an untagged edge e adjacent to a tagged vertex v is illustrated in Figure 4. We modify the standard 
edge rule in the following way: we parameterize the rule by ßk, which depends on the adjacent vertex 
tag and sector tag. Let the vertices be labeled as in Figure 3, and let the position of the tagged endpoint 
be p mc , the other endpoint is p mi . We insert a vertex on the edge at position p mi +1. The edge rule 
for the triangular scheme is m+1 mm mm pi =(3/4- .)pc +.pi +1/8(pi-1 +pi+1). We use a similar rule for 
the quadrilateral case: m+1 mm mmmm pi =(3/4- .)pc +.pi +1/16(pi-1 +pi+1 +qi-1 +qi ). The subdivision 
masks are illustrated in Figure 5. In each case . is given in terms of parameter ßk: .(ßk)=1/2- 1/4cosßk 
(triangular scheme) .(ßk)=3/8- 1/4cosßk (quadrilateral scheme). For a dart vertex v, we use ßk =21/k, 
where kis the total number of polygons adjacent to v.If vis a crease vertex, we use ßk =1/k, where kis 
the number of polygons adjacent to vin the sector of e. At a corner vertex v we differentiate whether 
eis in a convex or concave sector. For a convex corner we use ßk = ./k, where . is the angle between 
the two crease edges spanning the sector (kas above), for concave corners ßk =(21- .)/k. 6.3 Flatness 
and normal modi.cation The second stage of the algorithm is always applied at concave cor­ner vertices 
and vertices with prescribed normals. It can be also applied at other boundary and interior vertices 
when it is desirable to increase .atness near a vertex or achieve C2-continuity. There are two slightly 
different types of position modi.cations performed at this stage: normal and .atness modi.cation. When­ever 
we compute a vertex position in the neighborhood of a vertex subject to normal or .atness modi.cation 
we compute the position using the rules above and modify it in a second step. The required eigenvectors 
for these modi.cations are listed in the appendix A. Flatness modi.cation. We observe that we can control 
how quickly the control points in a neighborhood converge towards the tangent plane. The equation for 
iterated subdivision suggests to ac­celerate the convergence by reducing eigenvalues pi,i =3...N- 1. 
We introduce a .atness parameter sand modify the subdivision rule to scale all eigenvalues except p0 
and p =p1 =p2 by factor 1- s. The vector of control points p after subdivision in a neigh­borhood of 
a point is modi.ed as follows: () new 012 p =(1- s)p+sa0x +a1x +a2x, where ai =(li ,p), and 0 : s : 1. 
Geometrically, the modi.ed rule blends between control point positions before .atness modi.­cation and 
certain points in the tangent plane, which are typically close to the projection of the original control 
point. The limit posi­tion a0 of the center vertex remains unchanged. The .atness modi.cation is always 
applied at concave corner vertices; the default values for the parameter sis s =1- 1/(2+ cosßk - coskßk), 
which ensures that the surface is C1 in this case. In other cases, scan be taken to be 0 by default. 
C2-modi.cation. The .atness modi.cation can be also used to make the subdivision scheme C2, similar to 
the .at spot modi.ca­tions [16]. It is known from the theory of subdivision that under certain conditions 
a scheme which is C2 away from extraordinary vertices, generates surfaces which are C2 at extraordinary 
vertices if all eigenvalues excluding 1and p are less than the squared sub­dominant eigenvalue. This 
can be easily achieved using .atness modi.cation: sis taken to be less than |p|2/maxi>3 |pi|. In gen­eral, 
values of s close to this quantity produce surfaces of better shape, but with greater curvature oscillations. 
It is worth noting that this approach has a fundamental problem: the resulting surface has zero curvature 
at the extraordinary vertex; the results of [17] indicate that for schemes with small support this is 
inevitable. Normal modi.cation. We introduce a similar modi.cation, which allows one to interpolate given 
tangent and normal position at a vertex v. As above, we modify the control point positions in v s neighborhood 
after each subdivision step. In this case, the param­eter tblends between the unmodi.ed positions and 
positions in the prescribed tangent plane, while the limit position a0 of v remains unchanged. '' For 
a prescribed tangent vector pair a1 and a2, we modify (()()) new '1 '2 p =p+ta1 - a1x +a2 - a2x ; where 
ai =(li ,p)and 0: t : 1. In case of a prescribed normal ' direction nwe compute the tangent vectors as 
ai =ai - (ai,n)n. We observe that the subdivision rules are no longer applied to each coordinate of the 
control points separately; rather, the whole 3D vector is required. We can think of this as a generalized 
form of subdivision, where the coef.cients are matrices rather than scalars. m+1 Thus, a control point 
position pi in a neighborhood with pre­scribed normal n on level m +1 can be explicitly expressed as 
sij Id - ti lkskj + xi l  m+1 m 1 1 2 2 T p =p x kskjnn ij j k where sij are entries of the original 
subdivision matrix S and Id the 3 × 3 identity matrix. It should be noted that our analysis in [25] applies 
only to the case t =1, which we use as a default value; the analysis of the general case is still an 
open question.  7 Discussion We have presented a number of simple extensions to the standard Catmull-Clark 
and Loop subdivision schemes that resolve some problems with existing rules. Our rules are designed to 
coincide with cubic endpoint interpo­lating B-splines rules along a crease. As a consequence, the gener­ated 
crease curves depend only on the crease control points. There­fore, it is possible to modify the interior 
of a surface patch without any effect on the bounding crease curves; moreover, one can join piecewise-smooth 
surfaces without gaps and combine them with other surface representations supporting B-spline boundaries. 
A complete C1-continuity analysis of our subdivision rules is outside of the scope of this paper, and 
will be given elsewhere [25]. Here we describe only the intuition behind our construction. We can understand 
the behavior of the surface in a neighbor­hood of a corner or crease vertex from the eigenstructure of 
the corresponding subdivision matrix. If we apply the standard rules in the neighborhood of a crease 
vertex, the eigenvalue 1/2 corresponding to the tangent vector of the crease is not subdominant. As a 
result, the surface contracts at a different rate from the crease, leading to a degenerate con.guration 
without tangent plane (Figure 2). The situation for corner vertices is similar as both tangent vectors 
are determined from crease curve segments with eigenvalue 1/2. Our subdivision rules ensure that 1/2 
is the subdominant eigen­value in both cases. It it not dif.cult to see that 1/2 is an eigenvalue: Consider 
a planar fan of k congruent polygons, where each poly­gon contributes an angle ßk to the total angle 
ß = kßk. If we treat this con.guration as a crease or corner neighborhood and apply our modi.ed subdivision 
rules, then the center vertex does not change its position, and for each adjacent edge we insert a vertex 
at exactly the midpoint. Thus, the con.guration is scaled down by a factor of 1/2, i.e., 1/2 is an eigenvalue. 
It turns out that p =1/2 is indeed subdominant for crease ver­tices and convex corners. For concave corners 
we ensure subdom­inance by reducing all other eigenvalues (except p0 =1) using the .atness modi.cation 
with parameter s satisfying (1 - s)(2 + cos ßk - cos kßk) < 2. Figure 11 demonstrates how the .atness 
modi.cation pulls the neighborhood of a convex corner into its tan­gent plane. Our implementation of 
the rules is available on the Web. We have also developed explicit evaluation rules for our schemes, 
extending [21]. 8 Results and Conclusions Surfaces with creases and corners of various types are illustrated 
in Figures 12 and 13(b). All the surfaces in Figure 12 are generated from the same control mesh by applying 
different tags. Note how convex and concave sectors meet along the crease of the torus. Figures 8 and 
10 demonstrate normal interpolation for boundary, corner and interior vertices; directions of normals 
are adjusted to obtain desired shapes without modifying the control mesh. Other applications are possible: 
we have applied normal modi.cation to create certain surface characteristics: randomly perturbing the 
top­level normals produces a wavy doughnut from a torus-like control mesh; perturbing normals on the 
.rst subdivision levels creates a noisy doughnut (Figure 13(c) and (d)). Conclusions and future work. 
We have presented a simple modi­.cation of the two most popular subdivision schemes that improves the 
behavior of the generated surfaces on boundary and creases and provides additional controls for surface 
modeling. Even though the class of surfaces considered in this paper is quite general, we have excluded 
many types of surface singulari­ties. Future work might explore which other singularities are useful 
for modeling purpose and how to construct subdivision rules to cre­ate such features.  9 Acknowledgments 
We are greatly indebted to Peter Schr¨ oder for his support and sug­gestions, and for trying out the 
schemes and discovering numerous hard-to .nd typos in the formulas. This work has its origin in dis­cussions 
with Tom Duchamp. We would like to thank the anony­mous reviewers for their comments. A portion of this 
work was supported by NSF award ACI­9978147. References [1] Subdivision for modeling and animation. SIGGRAPH 
2000 Course Notes. [2] Ed Catmull and James Clark. Recursively generated B-spline surfaces on arbi­trary 
topological meshes. Computer Aided Design, 10(6):350 355, 1978. [3] Tony DeRose, Michael Kass, and Tien 
Truong. Subdivision surfaces in character animation. In Michael Cohen, editor, SIGGRAPH 98 Conference 
Proceedings, Annual Conference Series, pages 85 94. ACM SIGGRAPH, Addison Wesley, July 1998. ISBN 0-89791-999-8. 
[4] D. Doo. A subdivision algorithm for smoothing down irregularly shaped poly­hedrons. In Proceedings 
on Interactive Techniques in Computer Aided Design, pages 157 165, Bologna, 1978. [5] D. Doo and M. Sabin. 
Analysis of the behaviour of recursive division surfaces near extraordinary points. Computer Aided Design, 
10(6):356 360, 1978. [6] Ayman Habib and Joe Warren. Edge and vertex insertion for a class of C1 subdivision 
surfaces. Computer Aided Geometric Design, 16(4):223 247, 1999. [7] Mark Halstead, Michael Kass, and 
Tony DeRose. Ef.cient, fair interpolation using Catmull-Clark surfaces. In Computer Graphics Proceedings, 
Annual Con­ference Series, pages 35 44. ACM Siggraph, 1993. [8] Hugues Hoppe, Tony DeRose, Tom Duchamp, 
Mark Halstead, Huber Jin, John McDonald, Jean Schweitzer, and Werner Stuetzle. Piecewise smooth surface 
reconsruction. In Computer Graphics Proceedings, Annual Conference Series, pages 295 302. ACM Siggraph, 
1994. [9] Adi Levin. Interpolating nets of curves by smooth subdivision surfaces. In Alyn Rockwood, editor, 
SIGGRAPH 99 Conference Proceedings, Annual Conference Series, pages 57 64. Addison Wesley, 1999. [10] 
Charles Loop. Smooth subdivision surfaces based on triangles. Master s thesis, University of Utah, Department 
of Mathematics, 1987. [11] A. Nasri. Interpolation of open B-spline curves by recursive subdivision surfaces. 
In Tim Goodman and Ralph Martin, editors, Mathematics of Surfaces VII, pages 173 188. Institute of mathematics 
and its applications, Information Geometers, 1997. [12] Ahmad H. Nasri. Polyhedral subdivision methods 
for free-form surfaces. ACM Transactions on Graphics, 6(1):29 73, January 1987. [13] Ahmad H. Nasri. 
Boundary corner control in recursive subdivision surfaces. Computer Aided Design, 23(6):405 410, 1991. 
[14] Ahmad H. Nasri. Surface interpolation on irregular networks with normal con­ditions. Computer Aided 
Geometric Design, 8:89 96, 1991.  (a) (b) (c) Figure 6: Subdivision on meshes with boundaries: Beethoven 
s face and hair are modeled as separate meshes with identical boundaries. (a) and (b): the rules of [8] 
result in a gap between the surfaces due to extraordinary vertices. (b) A close-up on the gaps at the 
ear. (c) With our rules no gap is created. (a) (b) (c) (d) Figure 7: (a) Control mesh with a twist on 
the boundary. (b) Normal varies rapidly near the point although the surface is formally smooth: there 
is a single bright spot on the front-facing boundary. (c), (d) Our algorithm reduces the variation: the 
highlights become larger. (a) (b) (c) (d) Figure 8: Normal interpolation for quadrilateral subdivision. 
Prescribed directions: (a) tilted downwards, (b) horizontal, (c) no modi.cation,  (d) vertical.   
(a) (b) (c) (d) Figure 9: Features: (a) concave corner, (b) convex corner, (c) smooth crease, (d) corner 
with two convex sectors.  (a) (b) (c) (d) (a) (b) (c) (d) (e) (a) (b) (c) (d) (a) (b) (c) (d)  Figure 
12: Surface manipulation with corners. (a) Smooth boundary curves. (b) Concave corners on top, convex 
corners on bottom. (c) Corners with convex and concave sectors. (d) Creases and corners as for (c) but 
with prescribed normal direction on concave sectors. Figure 13: Manipulating a torus. (a) The original 
surface. (b) A surface with creases and convex/concave corners. (c) Wavy torus: we deform the torus by 
randomly perturbing normals of the control mesh. (d) Noisy torus: we perturb the normals on the .rst 
four subdivision levels. [15] J¨ org Peters and Ulrich Reif. Analysis of algorithms generalizing B-spline 
subdi­vision. SIAM Journal on Numerical Analysis, 35(2):728 748 (electronic), 1998. [16] H. Prautzsch 
and G. Umlauf. A G2-subdivision algorithm. In Geometric mod­elling (Dagstuhl, 1996), pages 217 224. Springer, 
Vienna, 1998. [17] Hartmut Prautzsch and Ulrich Reif. Degree estimates for Ck-piecewise polyno­mial subdivision 
surfaces. Adv. Comput. Math., 10(2):209 217, 1999. [18] Ulrich Reif. A uni.ed approach to subdivision 
algorithms near extraordinary points. Computer Aided Geometric Design, 12:153 174, 1995. [19] J. E. Schweitzer. 
Analysis and Application of Subdivision Surfaces. PhD thesis, University of Washington, Seattle, 1996. 
[20] Thomas W. Sederberg, Jianmin Zheng, David Sewell, and Malcolm Sabin. Non­uniform recursive subdivision 
surfaces. In Michael Cohen, editor, SIGGRAPH 98 Conference Proceedings, Annual Conference Series, pages 
387 394. ACM SIGGRAPH, Addison Wesley, July 1998. ISBN 0-89791-999-8. [21] Jos Stam. Exact evaluation 
of Catmull-Clark subdivision surfaces at arbitrary pa­rameter values. In SIGGRAPH 98 Converence Proceedings, 
Annual Conference Series, pages 395 404. Addison Wesley. [22] Joe Warren. Subdivision methods for geometric 
design. Unpublished manuscript, November 1995. [23] Denis Zorin. A method for analysis of C1-continuity 
of subdivision surfaces. 1998. submitted to SIAM Jornal of Numerical Analysis. [24] Denis Zorin. Smoothness 
of subdivision on irregular meshes. Constructive Ap­proximation, 16(3), 2000. [25] Denis Zorin, Tom Duchamp, 
and H. Biermann. Smoothness of subdivision sur­faces on the boundary. Technical report, New York University, 
Dept. of Com­puter Scinece, 2000. [26] Denis N. Zorin. Subdivision and Multiresolution Surface Representations.PhD 
thesis, Caltech, Pasadena, California, 1997.  A Coef.cients for left and right subdominant eigenvectors 
Here we list the left and right eigenvectors necessary for the subdivision rules described above. Recall 
that the eigenvector coef.cients are applied to a control points of a polygon ring/fan. A subscript c 
denotes the coef.cient corresponding to the center vertex. For the quadrilateral scheme, we mark edgepoint 
coef.cients with subscript p and facepoint coef.cients with q. We de.ne the degree of a vertex as the 
number of polygons asjacent to this vertex; note that this de.nition is different from the standard one 
(the number of incident edges) for boundary vertices. The crease degree is the number of polygons adjacent 
to a crease or corner vertex with respect to a speci.c sector. Also, recall that dominant right eigenvector 
x 0 is the vector consisting of ones. Loop scheme. Interior vertex of degree k. In all cases iis in the 
range 0...k-1,and .k =2s/k. 1 (8/3)j lc 0 = ,l0 i = 1+(8k/3)j1+(8k/3)j 1 212 x =x =l=l=0 c ccc 22 121 
2 x =sini.k,x =cosi.k,l= sini.k,l= cosi.k iii i kk Smooth crease vertex of crease degree k.Let .k =s/k; 
then 000 0 c 1 k l =2/3,l=l=1/6,li =0,i =1...k- 1 For k =1, 1 11 222 x =-1/3,x1 =2/3,x2 =2/3,x =0,x1 
=1,x =-1 c c 2 111 222 l =-1,l1 =1/2,l2 =1/2,l=0,l1 =1/2,l=-1/2; c c 2 12 otherwise x =x =l1 =0, c cc 
12 xi =sini.k,xi =cosi.k,i=0...k 11 1 l0 =1/2,l=-1/2,l=0,i=1...k ki (()) 22 2 l =-- aa1 - ba3 c k 3 (()) 
2 a 11 l2 =l2 =- + a1 + ba3 0 k k 2622 l2 i = sini.k,i=1...k- 1 k where 1 23 -a 4 (1+cos 6k) a = b = 
3( 12 - 14 cos 6k) k¥k¥ cos 2 sin 6k sin 6k cos a1 = a3 = 2 1-cos 6k cos ¥-cos 6k ß =arccos(cos.k - 1) 
Convex/concave corner vertex of crease degree kwith parameter .k.Let . =k.k. lc 0 =1,li 0 =0,i =0...k 
121 2 sini.k sin(k- i).k x c =x c =0,xi = ,xi = ,i =0...k sin. sin. 1 11 l =-1,l=1,l=0,i =0...k- 1 c 
ki 2 22 l =-1,l0 =1,l=0,i=1...k ci Catmull-Clark scheme. Interior vertex of degree k.Let .k =2s/kand 
ifrom 0 to k- 1. k 41 00 0 l = ,l= ,l= c pi qi k+5 k(k+5) k(k+5) 1 212 x =x =l=l=0 c ccc 11 12 x = sini.k 
x = cosi.k pi pi aa sini.k +sin(i+1).k cosi.k +cos(i+1).k 1 2 x = ,x = qi qi a(4p- 1) a(4p- 1) 12 l=4sini.k 
l=4cosi.k pi pi sini.k +sin(i+1).k cosi.k +cos(i+1).k 1 2 l= ,l= qi qi 4p- 1 4p- 1 ( ) where p =5/16+1/16cos.k 
+cos.k/29+cos2.k,and a = 1+cos 6k k(2+ . (4--1)2 ) Smooth crease vertex of crease degree k.Let .k =s/k. 
0 00 000 l =2/3,l=l=1/6,l=l=l=0,i=1...k- 1 cp1 pk q0 qi pi For k =1, 11 1 1 cp0 p1 q0 x =1/18,x =-2/18,x 
=-2/18,x =-5/18 22 2 2 cp0 x =0,x =-1/2,x p1 =1/2,x q0 =0 cp0 p1 l1 =6,l1 =-3,l1 =-3,lq10 =0 cp0 l2 =0,l2 
=-1,lp21 =1,lq20 =0; 12 otherwise l2 =x =x =0,and ccc xpi 1 =sini.k,xpi 2 =cosi.k,i =0...k 1 xqi =sini.k 
+sin(i+1).k,i =0...k- 1 2 xqi =cosi.k +cos(i+1).k,i=0...k- 1 2 2 222 lp0 =1/2,l=-1/2,l=l=l=0,i =1...k- 
1 pk q0 qi pi 1 11 l =4R(cos.k - 1),l=l=-R(1+2cos.k) cp0 pk 1 4sini.k l= ,i =1...k- 1 pi (3+cos.k)k 4(sini.k 
+sin(i+1).k) 1 l= ,i=0...k- 1 qi (3+cos.k)k cos 6k+1 where R = k sin 6k(3+cos 6k) . Convex/concave corner 
vertex of crease degree kwith parameter .k.Let . = k.k. Left eigenvectors are the same as for Loop with 
zeroes everywhere except lc,lp0 and lpk. sini.k sin(k- i).k pi pi 12 x = ,x = ,i=0...k sin. sin. sini.k 
+sin(i+1).k 1 x = ,i =0...k- 1 qi sin. sin(k- i).k +sin(k- i- 1).k 2 x = ,i=0...k- 1 qi sin.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344844</article_id>
		<sort_key>121</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Environment matting extensions]]></title>
		<subtitle><![CDATA[towards higher accuracy and real-time capture]]></subtitle>
		<page_from>121</page_from>
		<page_to>130</page_to>
		<doi_number>10.1145/344779.344844</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344844</url>
		<abstract>
			<par><![CDATA[<p>Environment matting is a generalization of traditional bluescreen matting. By photographing an object in front of a sequence of structured light backdrops, a set of approximate light-transport paths through the object can be computed. The original environment matting research chose a middle ground&#8212;using a moderate number of photographs to produce results that were reasonably accurate for many objects. In this work, we extend the technique in two opposite directions: recovering a more accurate model at the expense of using additional structured light backdrops, and obtaining a simplified matte using just a single backdrop. The first extension allows for the capture of complex and subtle interactions of light with objects, while the second allows for video capture of colorless objects in motion.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[alpha channel]]></kw>
			<kw><![CDATA[augmented reality]]></kw>
			<kw><![CDATA[blue spill]]></kw>
			<kw><![CDATA[blue-screen matting]]></kw>
			<kw><![CDATA[clip art]]></kw>
			<kw><![CDATA[colored transparency]]></kw>
			<kw><![CDATA[environment map]]></kw>
			<kw><![CDATA[environment matte]]></kw>
			<kw><![CDATA[image-based rendering]]></kw>
			<kw><![CDATA[real-time capture]]></kw>
			<kw><![CDATA[reflection]]></kw>
			<kw><![CDATA[refraction]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Modeling and recovery of physical attributes</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.8</cat_node>
				<descriptor>Heuristic methods</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010205.10010206</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Search methodologies->Heuristic function construction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Reliability</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P308194</person_id>
				<author_profile_id><![CDATA[81350582710]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yung-Yu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chuang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P70068</person_id>
				<author_profile_id><![CDATA[81100437187]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Douglas]]></first_name>
				<middle_name><![CDATA[E.]]></middle_name>
				<last_name><![CDATA[Zongker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P142230</person_id>
				<author_profile_id><![CDATA[81100525227]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Joel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hindorff]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14202804</person_id>
				<author_profile_id><![CDATA[81100587184]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Curless]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P63622</person_id>
				<author_profile_id><![CDATA[81100188207]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Salesin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington and Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15023234</person_id>
				<author_profile_id><![CDATA[81100122769]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Szeliski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>57361</ref_obj_id>
				<ref_obj_pid>57360</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Paul Besl. Active optical range imaging sensors. In Jorge L.C. Sanz, editor, Advances in Machine Vision, chapter 1, pages 1-63. Springer-Verlag, 1989.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360353</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[J. F. Blinn and M. E. Newell. Texture and reflection in computer generated images. Communications of the ACM, 19:542-546, 1976.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[G. Chazan and N. Kiryati. Pyramidal intensity ratio depth sensor. Technical Report 121, Center for Communication and Information Technologies, Department of Electrical Engineering, Technion, Haifa, Israel, October 1995.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>840039</ref_obj_id>
				<ref_obj_pid>839277</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[B. Curless and M. Levoy. Better optical triangulation through spacetime analysis. In Proceedings of IEEE International Conference on Computer Vision, pages 987-994, June 1995.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258884</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Paul E. Debevec and Jitendra Malik. Recovering high dynamic range radiance maps from photographs. In P1vceedings of SIGGRAPH 97, pages 369-378, August 1997.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Gerd Hfiusler and Dieter Ritter. Parallel three-dimensional sensing by colorcoded triangulation. Applied Optics, 32(35):7164-7169, December 1993.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218446</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[David J. Heeger and James R. Bergen. Pyramid-based texture analysis/synthesis. P1vceedings of SIGGRAPH 95, pages 229-238, August 1995.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>825397</ref_obj_id>
				<ref_obj_pid>523428</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Eli Horn and Nahum Kiryati. Toward optimal structured light patterns. In P1vceedings of the International Conference on Recent Advances in Three- Dimensional Digital Imaging and Modeling, pages 28-35, 1997.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[T. Kanade, A. Gruss, and L. Carley. A very fast VLSI rangefinder. In 1991 IEEE International Conference on Robotics and Automation, volume 39, pages 1322-1329, April 1991.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>78304</ref_obj_id>
				<ref_obj_pid>78302</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[R Perona and J. Malik. Scale space and edge detection using anisotropic diffusion. IEEE Trans. on Pattern Analysis and Machine Intelligence, 12(7):629-639, July 1990.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808606</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Thomas Porter and Tom Duff. Compositing digital images. In P~vceedings of SIGGRAPH 84, volume 18, pages 253-259, July 1984.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>148286</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian R Flannery. Numerical Recipes in C: The Art of Scientific Computing (2nd ed.). Cambridge University Press, 1992.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[K. Sato and S. Inokuchi. Three-dimensional surface measurement by space encoding range imaging. Journal of Robotic Systems, 2:27-39, 1985.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>839842</ref_obj_id>
				<ref_obj_pid>839276</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Erhard Schubert. Fast 3d object recognition using multiple color coded illumination. In P1vc. IEEE Conference on Acoustics, Speech, and Signal P1vcessing, pages 3057-3060, 1997.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237263</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Alvy Ray Smith and James F. Blinn. Blue screen matting. In P~vceedings of SIGGRAPH 96, pages 259-268, August 1996.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134078</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Gregory J. Ward. Measuring and modeling anisotropic reflection. In Edwin E. Catmull, editor, Computer Graphics (SIGGRAPH '92 P~vceedings), volume 26, pages 265-272, July 1992.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>528718</ref_obj_id>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[G. Wolberg. Digital Image Warping. IEEE Computer Society Press, 1990.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Yung Yu Chuang, Douglas E. Zongker, Joel Hindorff, Brian Curless, David H. Salesin, and Richard Szeliski. Environment matting extensions: Towards higher accuracy and real-time capture. Technical Report 2000-05-01, University of Washington, 2000.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311558</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Douglas E. Zongker, Dawn M. Werner, Brian Curless, and David H. Salesin. Environment matting and compositing. In P~vceedings of SIGGRAPH 99, pages 205-214, August 1999.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Environment Matting Extensions: Towards Higher Accuracy and Real-Time Capture Yung-Yu Chuang1 Douglas 
E. Zongker1 Joel Hindorff1 1University of Washington Abstract Environment matting is a generalization 
of traditional bluescreen matting. By photographing an object in front of a sequence of struc­tured light 
backdrops, a set of approximate light-transport paths through the object can be computed. The original 
environment mat­ting research chose a middle ground using a moderate number of photographs to produce 
results that were reasonably accurate for many objects. In this work, we extend the technique in two 
oppo­site directions: recovering a more accurate model at the expense of using additional structured 
light backdrops, and obtaining a simpli­.ed matte using just a single backdrop. The .rst extension allows 
for the capture of complex and subtle interactions of light with ob­jects, while the second allows for 
video capture of colorless objects in motion. CR Categories: I.2.10 [Arti.cial Intelligence]: Vision 
and Scene Understanding modeling and recovery of physical attributes; I.3.3 [Computer Graphics]: Pic­ture/Image 
Generation display algorithms; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism color, 
shading, shadowing, and texture Keywords: Alpha channel, augmented reality, blue-screen matting, blue 
spill, clip art, colored transparency, environment map, environment matte, image-based rendering, real-time 
capture, re.ection, refraction. 1 Introduction Conventional matting consists of .lming a foreground 
object against a known background and determining the foreground color and opacity at each image pixel. 
Conventional image compositing simply layers the foreground over an arbitrary background using the opacity 
to control the relative contributions at each pixel. Environ­ment matting and compositing generalize 
the conventional methods by modeling arbitrary transport paths from the background through the foreground 
object to the camera. After making a set of approxi­mations, Zongker et al. [19] demonstrate the ability 
to capture and render the effects of re.ection, refraction, scatter, and colored .l­tering of light from 
a background. These effects, none of which are modeled with conventional matting and compositing, make 
a dra­matic contribution to the visual realism of the .nal image. The original environment matting method 
employs a sequence of structured backdrops to estimate mappings from the background through the foreground 
object. These backdrops consist of a hier­archy of .ner and .ner horizontal and vertical square-wave 
stripes from which the matte can be extracted with O(log k) images for an k × k pixel grid. This choice 
of backdrops is inspired by a related Permission to make digital or hard copies of part or all of this 
work or personal or classroom use is granted without fee provided that copies are not made or distributed 
for profit or commercial advantage and that copies bear this notice and the full citation on the first 
page. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior 
specific permission and/or a fee. SIGGRAPH 2000, New Orleans, LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 
...$5.00 Brian Curless1 David H. Salesin1,2 Richard Szeliski2 2Microsoft Research Figure 1 Sample composite 
images constructed with the techniques of this paper: slow but accurate on the left, and a more restricted 
example acquired at video rates on the right. technique developed for 3D range scanning [1]. In practice, 
how­ever, this approach has a number of shortcomings. The previous approach of Zongker et al. assumes, 
.rst of all, that each image pixel collects light from a single region of the back­ground, augmented 
with an alpha component for straight-through partial coverage. This assumption fails when we consider 
the ef­fects of simultaneous re.ection and refraction at a dielectric. Sec­ond, the method is tuned to 
capturing highly specular interactions, but breaks down in the presence of surfaces that are even moder­ately 
rough. Third, the mappings that are captured assume axis­aligned .ltering of background pixels. At pixels 
where this assump­tion does not hold, this axis alignment results in excessive blurring and degrades 
the quality of the .nal composite. Fourth, the original method accounts for colored .ltering of light, 
but does not account for the effects of dispersion, which necessitate different mappings per color channel 
and give rise to prismatic rainbowing effects. Fi­nally, the number of images required is typically about 
20 per matte, and thus the technique does not lend itself to real-time acquisition. In this paper, we 
address each of these limitations. Again taking in­spiration from the 3D range-scanning literature, we 
follow two dis­tinct paths. In the .rst approach, the background consists of a single stripe swept over 
time in the vertical, horizontal, and two diagonal directions. In each case, the stripe orientation is 
perpendicular to the sweeping direction. By combining sweeps with stripes of varying widths and intensity 
pro.les, we demonstrate an ef.cient method for extracting the highest quality environment mattes to date. 
This method, however, requires O(k) images and is even less suitable for real-time acquisition than the 
previous method. Our second approach utilizes a single background image consist­ing of a color ramp. 
Through careful simpli.cation of the govern­ing equations, we demonstrate a method capable of extracting 
pure specular refractive and re.ective mappings through a moving, de­forming colorless object, such as 
water pouring into a glass. This technique is real-time in the sense that the data it requires can be 
collected in real-time, though the analysis and extraction of the matte must still be performed of.ine. 
1.1 Related work Environment matting [19] builds upon and substantially extends re­search and practice 
in blue screen matting. Pioneered by Vlahos [15], conventional (and commercial) blue screen matting relies 
on a single-color background suf.ciently different from the foreground objects to extract an alpha and 
foreground color at each pixel. Smith and Blinn [15] use two backdrops to lift restrictions on the color 
of foreground objects. Neither of these techniques models transport paths beyond scalar (non-color) attenuation 
and partial pixel cov­erage. In addition, the Smith and Blinn technique does not easily extend to real-time 
capture, given the two-frame requirement. Our real-time method for capturing environment mattes is actually 
sim­ilar in spirit to the Vlahos work in that we must place constraints on the object and lighting in 
order to achieve our results. The structured-light range-scanning literature suggests many possi­ble 
ways to capture spatially varying properties of an object [1]. We should note that the end goals are 
substantially simpler in the range-scanning case. Range scanners attempt to recover just a handful of 
parameters per pixel: primarily, depth and re.ectance. Environment-matting procedures, on the other hand, 
generally need to recover a continuous, wavelength-dependent mapping from a background to the image plane. 
Even with the approximations de­scribed in Section 3, we must estimate at least 21 parameters per pixel, 
and sometimes many more. Despite the dissimilarities, the range-scanning illumination pat­terns do inspire 
a number of techniques for environment matting. The most brute force range-scanning method is to sweep 
a beam of light over an object in a raster pattern. Such an approach, while O(k2) in time since each 
range-image pixel is acquired sequentially, is actually practical for triangulation and imaging radar 
systems [1], since the re.ected light seen by the sensor is known to have fol­lowed a straight line from 
the object. As a result, objects can be imaged with fast 1D (triangulation) or 0D (imaging radar) sensors. 
These faster sensors make the acquisition speeds comparable to, and in some cases better than, the O(k) 
swept-stripe techniques described below. By contrast, typical objects used in environment matting will 
cause light from the background to bend through or re.ect off of the object in unpredictable ways, thus 
requiring a full 2D sensor array to capture the light. In this case, the O(k2) penalty is prohibitive. 
Using a swept plane of light, O(k) images can provide shape infor­mation through optical triangulation 
[1]. The .rst environment mat­ting technique described in Section 3 uses such a pattern, though multiple 
oriented sweeps are required to capture all the parame­ters. Note that this particular environment matting 
technique bears some resemblance to the space-time analysis described by Kanade et al. [9] and Curless 
and Levoy [4], in which the authors study the time evolution of re.ected light and triangulate over space 
and time. By projecting a hierarchy of progressively .ner stripe patterns, the required number of images 
for optical triangulation can be reduced to O(log k) [13]. Zongker et al. [19] use such a stripe hierarchy 
with some accompanying compromises over the swept-stripe technique. Finally, using a color ramp, researchers 
have demonstrated single­frame triangulation, that is, an acquisition with time complexity O(1), albeit 
with increased susceptibility to noise [6, 14]. In Sec­tion 4, we apply a similar pattern to extract 
environment mattes in real time. To combat the effects of noise, we apply a non-linear, discontinuity-preserving 
.lter [10] to the resulting matte sequence. Hybrid stripe-ramp methods have been proposed to manage the 
trade-off between number of images and susceptibility to noise [3, 8]. We have not explored such methods 
for environment matting, as they will likely yield results of lower quality than the swept-stripe method 
we describe here, and will still require multiple frames, making them unsuitable for real-time capture. 
Our work also has some connection to BRDF acquisition. Though we do not explicitly solve for the BRDF, 
one could certainly imag­ine using environment matting to capture re.ection functions over a uniformly 
coated surface of known geometry, such as a sphere. A more direct connection lies in the BRDF .tting 
work of Ward [16]. Using an elliptical Gaussian model for rough specular re.ec­tion, he achieves excellent 
matches to goniometric samples. This model is the motivation for our choice of oriented, elliptical, 
Gaus­sian weighting functions described in Section 3. 1.2 Overview In Section 2 we describe the general 
environment matting model. The two following sections describe the two extensions we have developed higher 
accuracy mattes in Section 3 and real-time­capture mattes in Section 4. Each of Sections 3 and 4 describes 
the assumptions made to reduce the general matting equation to something that can be captured, then describes 
the experimental procedure used and shows results. We conclude in Section 5 with a summary and ideas 
for future work.  2 The environment matting equation We begin by developing a general expression for 
the environment matting equation and showing how it reduces to the traditional com­positing equation, 
as well as the equation developed by Zongker et al. [19]. An imaging system, such as a CCD camera, records 
a discrete set of samples over an image plane. Let s assume for the moment that we have a camera that 
measures the irradiance at each wavelength separately. Then, for a given pixel, the camera records a 
value C for each wavelength.1 Following the environment mapping work of Blinn and Newell [2], we can 
express this color in terms of an in.nitely distant environment illumination E(w): C =W(w) E(w) dw. 
(1) The weighting function W comprises all means of transport of envi­ronment lighting from all directions 
wthrough a foreground object to the camera, including any blurring due to the camera optics and area 
integration at a sensor cell. This equation holds under the as­sumption that none of the materials that 
are scattering light from the environment exhibit any wavelength coupling (e.g., .uorescence). Next, 
we rewrite this equation as a spatial integral over a bound­ing surface (e.g., an environment map). Further, 
we augment the equation to include an additive foreground color F. This foreground color is typically 
due to some additional lighting that is separate from the environment map, though it could encompass 
object emis­sivity as well. Under these assumptions, our equation becomes C = F +W(x) T(x) dx . (2) 
From this equation, we can develop a series of approximations that allow us to embed a foreground object 
in a new environment with varying degrees of quality. 1Throughout this paper, we use ordinary italics 
for scalar quantities (e.g., a position x); bold-italics for functions of more than one spatial parameter 
(e.g., an area A); colored italics for functions of wavelength (e.g., a color C); and colored bold-italics 
for functions of both wavelength and more than one spatial parameter (e.g., a texture map T). To arrive 
at the traditional image compositing equation [11], we as­sume that the straight-through background pixel 
is the only environ­ment sample that affects the camera pixel. Let P be the rectangular­area support 
of the pixel p on the background. Then we describe the (in this case, monochromatic) weighting function 
as W(x)=(1 - .) I(x; P), (3) where . represents the foreground s transparency or partial pixel coverage, 
and I(x; A) is the box function of unit volume supported over an arbitrary axis-aligned area A.Nextwe 
de.ne M(T, A)as the texture-mapping operator that performs the area integral and returns the average 
value of the texture T over region A: M(T, A) v I(x; A) T(x) dx . (4) Finally, de.ning the .ltered background 
B to be the integral over the pixel s support B vM(T, P), (5) and substituting the previous three equations 
into Equation 2, gives the traditional compositing equation: C = F +(1 - .) B . (6) Note that . does 
not have any wavelength dependence and thus can­not model color-.ltered transparency. In addition, F 
is a measured quantity that is added directly to the attenuated background in ef­fect, it is pre-multiplied 
by .. Zongker et al. model more complex lighting effects by approximat­ing the environment as a set of 
m texture maps Ti(x) (the six sides of a bounding cube for instance), and by using more general light 
transport paths. Their weighting function is m t W(x)=(1 - .) I(x; P)+ Ri I(x; Ai). (7) i=1 In their 
formulation, the Ai represent various axis-aligned regions, each lying on a different texture map (corresponding, 
typically, to a different face of the environment cube). The Ri are re.ectance co­ef.cients describing 
the amount of light from the designated area of texture map i that is re.ected or transmitted by the 
object at a given wavelength. In this formulation, Ri captures color-.ltered transparency, and . represents 
only partial pixel coverage of the object. Substituting this weighting function into Equation 2 gives 
the environment matting equation used by Zongker et al.: m t C = F +(1 - .) B + Ri M(Ti, Ai). (8) i=1 
Note that this approach not only permits colored .ltering of light, but also enables effects such as 
re.ection and refraction since the light contributing to a pixel can be scattered from parts of the en­vironment 
other than just the pixel directly behind the object. This approach, however, does have several distinct 
limitations. First, the components of the weighting function are assumed to be separable products of 
wavelength functions Ri and spatial functions I(x; Ai). Thus, phenomena such as dispersion are not handled, 
since these require the weighting functions to shift spatially with wavelength. Second, the axis-aligned 
rectangle weighting functions do not sim­ulate the effects of, for example, smooth BRDF s, which when 
mapped onto a background have a smooth, oriented footprint. Fi­nally, other than the straight-through 
.-component, the approach models only a single mapping from a texture face to the camera. In reality, 
multiple mappings to the same face can and do happen and must be modeled, for example, when re.ection 
and refraction at an interface cause view rays to split into distinct groups that strike the same backdrop. 
Our .rst objective, then, is to choose a different model for the weighting function that is more physically 
motivated and whose parameters are still easy to acquire using a simple apparatus. 3 Towards higher 
accuracy To address the limitations of the weighting function described in Zongker et al., we generalize 
it to a sum of Gaussians: n t W(x)= Ri Gi(x). (9) i=1 In our formulation, we allow any number of contributions 
from a single texture map. Here, Ri is an attenuation factor, and each Gi is the unit-area, elliptical, 
oriented 2D Gaussian: Gi(x) v G2D(x; ci, Oi, ei) , (10) where G2D is de.ned as [22 ] 1 uv v exp -- (11) 
G2D(x; c, O, e) 22 2sOuOv 2Ou 2Ov with u =(x - cx)cos e - (y - cy)sin e v =(x - cx)sin e +(y - cy)cos 
e. Here, x =(x, y) are the pixel coordinates, c =(cx, cy) is the center of each Gaussian, O=(Ou, Ov) 
are the unrotated widths (a.k.a. standard deviations) in a local uv-coordinate system, and e is the orientation. 
Figure 2 illustrates these parameters. Thus, our weight­ing function is some n-modal Gaussian with each 
term contributing a re.ective or refractive effect from the object. Substituting into Equation 2, we 
arrive at a new form of the matting equation: n t C = F + Ri G2D(x; ci, Oi, ei) T(x) dx . (12) i=1 (In 
this equation, we use T(x) to represent the set of all texture maps. The n modes of the weighting function 
are distributed over m textures, where n may be larger than m in general. The choice of the particular 
texture map used in computing a given Gaussian contribution i should be assumed to be implicitly controlled 
by the position ci of the Gaussian weighting function.) The key advantages of this weighting function 
over the one used by Zongker et al. are that: (1) the spatial variation can be coupled with wavelength 
to permit modeling of dispersion; (2) it supports multi­ple mappings to a single texture; and (3) it 
approximates the behav­ior of BRDF s more closely (by using oriented Gaussian weighting functions rather 
than box functions). In practice, each of the colored values C, F, Ri, ci, Oi, ei and T in Equation 12 
is implemented as an rgb vector. So, in practice, this equation actually represents three independent 
equations, one for each of the color components. Our unknowns are F, Ri, ci, Oi, ei, which means that 
each pixel encodes 3 + 18n parameters. 3.1 Swept Gaussians for environment matting Recovering the environment 
matte requires taking a set of images of an object in front of a sequence of backdrops. Our method consists 
of three steps: (1) identifying pixels outside the object silhouette, (2) recovering the foreground color, 
and (3) applying a set of novel background stimulus functions to estimate the remaining parame­ters in 
the matte. Figure 2 Illustration of the variables used in recovering an unknown ellip­tical, oriented 
Gaussian by sweeping out convolutions with known Gaussian stripes. As a tilted stripe T(x; r)ofwidth 
Is and position r sweeps across the background in direction s, it passes under the elliptical Gaussian 
weight­ing function W(x) associated with a single camera pixel. The camera records the integral of the 
product of the stripe and the weighting function, which describes a new, observed function C(r) as the 
stripe sweeps. The center cr and width Ir of this observed function are related to the center c and width 
u =(Iu, Iv ) of the weighting function and the width of the stripe through Equations 20 and 21. In the 
.rst step of our high-accuracy matting method, we identify pixels that are outside the silhouette of 
the object. This step is de­sirable for two reasons: it saves us the computational effort of es­timating 
the matte parameters at these pixels, and it prevents us from making potentially noisy estimates of how 
straight-through background pixels map to the image, which would result in shim­mering artifacts when 
rendering. To identify these pixels, we use the method of Zongker et al. In particular, we display a 
coarse-to­.ne sequence of horizontal and vertical square-wave background patterns with and without the 
object. If we measure the same color (within a user-speci.ed tolerance) at a pixel both with and without 
the object for each background, then we consider the pixel to map straight through. The overhead of taking 
these additional images is small compared to the total acquisition time. To recover the foreground color, 
we photograph the object against two solid backgrounds. Replacing T(x) in Equation 12 with a single backdrop 
of constant color T and integrating, we get C = F + RT (13) n where R vi=1 Ri. Given the two images, 
we have two equa­tions in two unknowns for each color channel, i.e., the foreground color F and the aggregate 
attenuation factor R. Solving the system of equations yields the foreground color. Once we have the silhouette 
mask and the foreground color, we can solve for the remaining parameters of Equation 12 using a large 
set of controlled backdrops (i.e., stimulus functions). Zongker et al. use a hierarchical set of square-wave 
stripe patterns in both the ver­tical and horizontal directions. They encounter dif.culties with this 
method for two reasons: (1) the square waves are not good stimuli for recovering smooth functions, and 
(2) there is no obvious way to recover multiple mappings to the backdrop using these stimuli. To combat 
the .rst problem, we choose a smooth set of stimulus func­tions. To address the second, we constrain 
the stimuli to be narrow in one dimension, sweeping over time to reveal multiple mappings to the same 
background. Our choice of stimulus function, then, is a set of swept Gaussian stripes. Let s see how 
we can use sweeping stripes to recover some of the parameters of our weighting functions. To begin, let 
us assume that the weighting function is unimodal and axis-aligned (n =1 and e = 0). Under these assumptions, 
we can omit the summation and the subscript i in Equations 9 and 10 and then decompose the 2D Gaussian 
weighting function into two 1D components: W(x)= R G1D(x; cx, Ou) G1D(y; cy, Ov) , (14) where G1D(x; 
c, O)= 1 -2sO exp [- (x - c)2 2O2 ]. (15) Our .rst stimulus function will be a vertical stripe that 
is constant in y and has a 1D Gaussian intensity pro.le in x with width Os: T(x)= G1D(x;0, Os) . (16) 
Now consider sweeping the stripe horizontally, displacing it at each step by some offset r: T(x; r)= 
G1D(x - r;0, Os)= G1D(r - x;0, Os) . (17) The camera observation at a pixel is then given by: C(r)= W(x) 
T(x; r) dx = R G1D(x; cx, Ou) G1D(y; cy, Ov) G1D(r-x,0, Os) dx dy = R G1D(x; cx, Ou) G1D(r-x;0, Os) dx 
· G1D(y; cy, Ov) dy = R G1D(r; cx, Ou) * G1D(r;0, Os) · 1 J = R G1D(r; cx, Ou 2+ Os 2). (18) Thus, at 
each pixel, we expect to record a Gaussian evolving over time. Given an illumination stripe of known 
width, we can now es­timate the rgb parameters cx and Ou using the procedure described below in Section 
3.2. By symmetry, we can recover the vertical cen­ter coordinate and width by sweeping a horizontal Gaussian 
stripe in the vertical direction behind the foreground object. Thus, for the case of a single, unoriented 
Gaussian weighting function, a hori­zontal and a vertical swept Gaussian stripe are enough to estimate 
all the remaining parameters of the environment matte. Figure 2 illustrates the more general case of 
a sweeping stripe that is constant in the t-direction and has Gaussian pro.le in the s­direction. This 
stripe is oriented at an angle . with respect to the xy-coordinate system and travels in the s-direction. 
Under these cir­cumstances, it is straightforward to show that the observation at a pixel will be: C(r)= 
R G1D(r; cr, Or) , (19) where cr = cx cos . + cy sin . (20) J Or = Ou 2 cos2(. - e)+ Ov 2 sin2(. - e)+ 
Os 2 (21) Here, cr is the center of the weighting function projected onto the r-axis, and Or is the projected, 
convolved standard deviation of the observed Gaussian. Horizontally and vertically swept stripes alone 
(. =0° and 90°, respectively) are not enough to determine the weighting function, Figure 3 The green 
concentric rings depict a possible bimodal weight­ing function. The curves around the image indicate 
the convolved projec­tions resulting from sweeping horizontal, vertical, and left and right diagonal 
Gaussian stripes across the screen independently. The horizontal and vertical sweeps alone (purple) are 
insuf.cient to determine the mode positions, but by adding additional diagonal sweeps (yellow) the correct 
modes can be deter­mined. so we introduce two diagonal passes at . =45 ° and -45 ° .The additional oriented 
stripes serve another purpose: disambiguating multiple mappings to the backdrop. As shown in Figure 3, 
a bi­modal weighting function results in two Gaussian images over time at a pixel as the stripe sweeps 
across. If we use just the horizontal and vertical stripes, the two modes recorded in each sweep yield 
multiple indistinguishable interpretations of the bimodal weighting function. The oriented stripes can 
be used to disambiguate these choices, as described below. 3.2 Estimating the matte parameters In practice, 
our acquisition process entails stepping each Gaussian stripe across a computer screen and recording 
a set of samples for each sweep and for each color channel. Given this data, we seek the best set of 
parameters that explain the measurements. We estimate these parameters (separately for each color channel) 
in four steps: (1) identifying the number of Gaussians modes in the response, (2) solving for the projected 
centers and widths associated with each Gaussian mode, (3) intersecting the centers to localize the Gaussian 
modes, and (4) computing the parameters for each Gaussian mode. To identify the number of Gaussians for 
the response to a given stripe sweep, we search for a series of peaks above the noise .oor of the sensor. 
To make this process more robust, we .rst .lter the 1D response function, and then identify the peaks. 
The locations of the peaks are the starting points for the projected centers of the projected modes. 
If the projected modes are clearly separated, we also estimate the projected widths by examining the 
extent of the signal that is above the noise .oor. For two overlapping modes, we compute the distances 
from the left mode to the left extent and the right mode to the right extent and then estimate widths 
accordingly. For more overlapping modes, we compute the total width and di­vide by the number of modes. 
In any case, these center and width estimates are simply starting points for a Levenburg-Marquardt op­timization 
procedure [12] that takes the original data, the number of Gaussians, and the initial center and width 
estimates in order to .nd the best centers and widths that explain the data. Next, we use the sets of 
projected centers to choose the most likely locations of the Gaussian modes Gi (from Equation 10). The 
cen­ters computed in the previous step should each correspond to the center of a Gaussian mode as projected 
onto the axis de.ned by Figure 4 A photograph of our experimental setup. In this instance an accurate 
environment matte is being acquired. the stripe. We then construct a line passing through each projected 
center point running parallel to the stripe s t direction. We consider all 4-tuples of horizontal, vertical, 
and two diagonal lines, and hy­pothesize their intersections by computing the point closest to each set 
of four lines. We measure the distance of that point to each of the lines, and apply a user-speci.ed 
tolerance to reject or accept the purported intersection. Finally, given the set of Gaussian modes selected 
by the intersec­tion process, we determine the parameters of each Gaussian mode. For each identi.ed mode, 
we have estimates of the convolved, pro­jected parameters according to Equations 20 and 21. We compute 
R as the average of individually computed R s. The center ci of Gi is simply the closest point to the 
lines as described in the previ­ous step. Finally, we have four equations that relate the width and orientation 
of each Gaussian mode to the four measured widths. We solve this over-constrained, non-linear system 
of equations by .nely sampling the space of possible orientations, solving for the linear-least-squares-best 
width parameters, and then choosing the orientation and widths that yield the lowest overall error. The 
result of this sequence of steps is a reasonable estimate for the number of Gaussian modes and their 
parameters. As a .nal step, we apply a full Levenburg-Marquardt optimization to .nd the best ci, Oi,and 
ei that explain all of the measurements. 3.3 Results of accurate matting Figure 4 shows our experimental 
setup. A Sony DCR-TRV900 dig­ital video camera records images of an object as one of three moni­tors 
presents a sequence of stimulus functions. We correct for non­linearities in the video camera using Debevec 
and Malik s method [5]. To calibrate each monitor s brightness settings, we display a sequence of solid 
gray images and record them with the radiometri­cally corrected camera. After averaging the gray values 
within each image, we have a mapping between gray values on the computer and displayed radiance. Each 
stripe image is adjusted so that the pro.le is Gaussian in radiance space. After calibration, we begin 
imaging the object by extracting the foreground color and silhouette mask as described in the previous 
section. Next, we display the sequence of background patterns. We translate each Gaussian stripe across 
the screen in steps of Os/2to ensure enough samples for the estimation procedure. We typically use Os 
= 2 or 4 (measured in camera pixels) requiring about 300 or 150 stripe positions, respectively, per horizontal, 
vertical, or diago­nal sweep. Due to the lack of synchronization between the monitor refresh and the 
camera, we are unable to capture at video rates; in­stead, a typical capture plus digital video transfer 
requires roughly 30 minutes. (With a synchronized system and real-time transfer to PC memory, we expect 
acquisition could take less than a minute.) Processing time for an environment matte is typically about 
20 min­utes on a 400 MHz Pentium II PC with 128 MB of RAM. We demonstrate the accuracy of our new environment 
matting al­gorithm on three objects. For each example, we render the matte by explicitly integrating 
the oriented Gaussian .lters over the back­ground. The results are shown in Figures 5 and 6. The .rst 
object is a crystal in the shape of a regularly triangu­lated sphere, shown in Figure 5(a). The planar 
facets give rise to prismatic rainbowing effects due to dispersion. This effect is cap­tured by our new 
matting algorithm because we estimate a different Gaussian weighting function (with a different center) 
for each color channel. This effect is not modeled by the old matting algorithm, which breaks down even 
further due to the multiple mappings at pixels that straddle crystal facets. The multiple mapping problem 
is more clearly demonstrated by our next object, a beer glass laid on its side (Figure 5(b)). Due to 
the grazing angle, simultaneous re.ection and refraction at the top of the glass results in bimodal mappings 
to the background. The old method simply cannot handle this phenomenon, whereas the new method captures 
the effect realistically. Finally, we captured an environment matte for a pie tin with a rough surface, 
oriented to cause tilted re.ections from the backdrop. Fig­ure 5(c) demonstrates the failure of the old 
method to capture the large, smooth weighting function indicative of surface roughness, in contrast to 
the new method s success. Figure 6 demonstrates the importance of capturing the orientation of the weighting 
function. When we apply the new method without estimating orientation (i.e., by simply using the widths 
determined by the horizontal and vertical sweeps), the texture lines running at 25 ° off of vertical 
are signi.cantly blurred. After estimating the orientation, we obtain a matte that faithfully preserves 
these details. 4.1 Simplifying the matting equation Using the high-accuracy matting process, we enjoy 
the luxury of having many samples per pixel. In the current process, we only have three samples per pixel: 
red, green, and blue. Our .rst objec­tive, then, is to simplify the problem just enough to be solvable 
that is, so that there will be only three unknowns remaining. We choose these simpli.cations to maximize 
the visual impact of the .nal matte. Then, using some carefully designed heuristics, we at­tempt to recover 
more variables in order to signi.cantly improve visual appearance. We begin by examining the original 
environment matting equation [19], written here as Equation 8. Assuming a single backdrop tex­ture, we 
drop the summation and the subscript i: C = F +(1 - .) B + R M(T, A) . (22) We can now count variables, 
keeping in mind that where we see a wavelength dependence, we can assume an rgb vector in practice. The 
unknowns are then F and R (rgb vectors), .,and A.The A term can be broken into {c, w},where c =(cx, cy) 
is the center of the area and w =(wx, wy) is the width in x and y.Thus,we have eleven variables. The 
properties of the matte that we would most like to preserve are, in order: (1) the capacity to refract, 
re.ect, and attenuate the background (A, R); (2) smooth blending with the background on silhouettes (.); 
and (3) specular highlights due to foreground light­ing (F). Let s focus on preserving the .rst property, 
which still has seven unknowns. We can simplify this set to three unknowns under the assumption that 
the object is both colorless and specularly re.ec­tive and refractive (i.e., has no roughness or translucency). 
If the object is colorless, then R becomes a scalar d, with no wavelength dependence. In addition, pure 
specularity implies that neighboring pixels do not have overlapping support in their weighting functions. 
Thus, c is now an image warping function, and w, derived from the warping function, indicates the size 
of the .lter support for proper antialiasing [17]. We have found the following approximation for w to 
work well in practice: 1 2[cx(x +1, y) - cx(x - 1, y)] (23) x y v cx .wx   4 Towards real-time capture 
1 .. 2 cy(x, y +1) - cy(x, y - 1) . (24) v cywy In the previous section, we considered ways to increase 
the accu­racy of composites produced with the environment matting tech­nique, at the expense of increasing 
the number of input images re­quired. Now we will attempt to go the other way to see how much realism 
can be maintained when we restrict the input to just a single image of the object. The single-image case 
is interesting for two reasons. First, it rep­resents a de.nite, extreme end of the image-count-versus-accuracy 
spectrum a sort of lower bound on the quality of the whole en­vironment matting technique. Secondly, 
unlike a solution that re­quires even two or three images of the object, a single-image so­lution makes 
it straightforward to capture video environment mat­tes of moving objects in front of a still, structured 
background. The original environment matting work [19] produced video using a sort of stop-motion technique, 
where a rigid object was placed on a mo­tion platform, allowing multiple photographs to be taken of each 
pose. This severely limited the kinds of motions that could be cap­tured. In contrast, a method utilizing 
just a single background image could be used to capture breaking glass, sloshing liquids, and other kinds 
of non-repeatable, uninterruptible motions. Although the data capture itself is real-time, the matte 
extraction process is performed off-line at a slower speed. The matte extraction process analyzes the 
frames of a captured video and constructs a matte for each frame. Thus, our rgb environment matting equation 
becomes C = d M(T, A) (25) 4.2 Single image matte recovery What kind of stimulus function could we use 
to recover all the pa­rameters in this problem? A logical choice would be a smooth func­tion, where we 
de.ne smoothness as M(T, A) . T(c) (26) for any area A. Such a function would have the property under 
our simpli.ed model that C . d T(c) . (27) Treating this equation as three equations in r, g,and b, we 
can easily solve for the three unknowns d, cx, cy. Backgrounds that are smooth according to the de.nition 
in Equation 26 include constant color functions and linear color ramps. However, our function must also 
 Figure 5 Comparisons between the composite results of the previously published algorithm, the higher 
accuracy environment matting technique described here, and reference photographs of the matted objects 
in front of background images. Lighting in the room contributed a yellowish foreground color F that appears, 
e.g., around the rim of the pie tin in the bottom row. (a) A faceted crystal ball causes rainbowing due 
to prismatic dispersion, an effect successfully captured by the higher accuracy technique since shifted 
Gaussian weighting functions are determined for each color channel. (b) Light both re.ects off and refracts 
through the sides of a glass. This bimodal contribution from the background causes catastrophic failure 
with the previous unimodal method, but is faithfully captured with the new multi-modal method. (c) The 
weighting functions due to re.ections from a roughly-textured pie tin are smooth and fairly broad. The 
new technique with Gaussian illumination and weighting functions handles such smooth mappings successfully, 
while the previous technique based on square-wave illumination patterns and rectangular weighting functions 
yields blocky artifacts. (a) (b) (c) (d) Figure 6 Oriented weighting functions re.ected from a pie tin. 
(a) As in Figure 5, the previous method yields blocky artifacts for smooth weighting functions. (b) Using 
the higher accuracy method with unoriented Gaussians (e = 0) produces a smoother result. (c) Results 
improve signi.cantly when we orient the Gaussians and solve for e. In this case, e . 25° over most of 
the bottom surface (facing up) of the pie tin. (d) Reference photograph. be invertible, so that we can 
identify which pixel we are seeing from its color. Obviously, a constant color function does not .ll 
this requirement. Instead, we display a color ramp background which is a slice through the rgb cube. 
Before extracting the environment matte against a ramp back­ground, we add a step that will allow us 
to recover an estimate of ., the second desirable property on our list. In Section 3, we described a 
method for classifying pixels as to whether they belong to the ob­ject. We can think of this classi.cation 
as choosing between either .= 1 (i.e., the pixel belongs to a foreground object) or .= d=0 (i.e., the 
pixel belongs to the background). To classify pixels, we .rst take a series of pictures of the back­ground 
without the object and average them together to give us a low noise estimate of the ramp background. 
Once we begin record­ing video of the object, we apply a simple difference threshold to each frame, comparing 
the image of the object to the image of the background alone. This step separates foreground and background 
pixels. We then use some morphology operations (dilation followed by hole-.lling followed by erosion) 
to clean up this binary map, giving us a reasonably accurate mask of the pixels covered by the object. 
To avoid a sharp discontinuity, we slightly feather the alpha at the boundaries of the object as a post-processing 
step. Thus, we arrive at the improved matting equation: C =(1 - .)B + dM(T, A) (28) which, for a smooth 
background, reduces to C =(1 - .) B + dT(c) . (29) We can now begin to recover an environment matte. 
Because we assume that F = 0 everywhere, we photograph the object in a dark room, lit only by the structured 
backdrop. The structured back­ground is a smoothly-varying wash of color, in particular, a planar slice 
through the rgb cube. Due to non-linearities in the system, in­cluding crosstalk between the spectra 
of the monitor phosphors and the CCD elements, the gamma of the backdrop display, and pro­cessing in 
the camera s electronics, this plane in color space will be distorted, becoming a curved 2D manifold 
lying within the rgb cube, as in Figure 7(b). To extract matte parameters at each pixel, we consider 
the line join­ing the observed color and the black point in rgb space. The point where this line intersects 
the background-color manifold gives us the point c, and the fractional distance of the observed color 
to the manifold gives us d, as illustrated in Figure 7(a). The manifold is dif.cult to characterize algebraically, 
so rather than projecting the observed color onto it, we do a multiresolution search to .nd the point 
on the manifold closest to the construction line. A single frame of video captured with a CCD camera 
will have considerable noise. While we can capture several seconds of the empty background and average 
frames to create a nearly noise­free reference, we get only one frame of the object in front of the backdrop. 
This leads to grainy composite images, as seen in Fig­ure 8(a). One way to combat this effect is to .lter 
the input im­ages to smooth out noise before matte extraction. However, our extraction process is so 
sensitive to noise that we have not been successful in obtaining smooth mappings without also signi.cantly 
smoothing away detail in the images. Instead, we .nd that directly smoothing the extracted warping function, 
c, is most effective. To this end, we apply the edge-preserving smoothing operator of Per­ona and Malik 
[10] to the cx and cy channels. This operator aver­ages each pixel with its neighborhood, with unequal 
contributions from neighboring pixels. The relative contributions are determined by the difference of 
the pixels values, so that similarly-valued pix­els affect each other more. This .lter smoothes out regions 
with low-to-moderate noise levels while preventing signi.cant energy W background manifold B(x) C´ 
C O (a) (b) Figure 7 (a) 2D version of the algorithm for constructing environment mattes from a single 
image, assuming that F = 0. The observed color C is projected from black onto the background manifold. 
The position of point C. on the manifold gives the position x,and . = OC/OC.. (b) In reality, the background 
colors lie on a 2D manifold within the rgb cube, and we recover an (x, y) position. (a) (b) Figure 8 
A composite created from a single-frame environment matte. Part (a) shows the results when no .ltering 
is applied. Filtering the matte both spatially and temporally reduces noise in the composite considerably, 
as seen in part (b).  transfer across sharp edges. For video, the best results are obtained when this 
operator is applied temporally as well as spatially, giving frame-to-frame coherence, which is especially 
important in areas of the object that are not moving.  4.3 Heuristics for specular highlights The most 
noticeable visual effect of this restricted model is the loss of specular highlights. The objects we 
capture are typically curved glass, and highlights are both important for communicat­ing the shape of 
the object and for making it visually appealing. In this section, we develop a method for recovering 
the intensity of the foreground color F, under the restriction that it is white. Thus, F = f W where 
W = (1, 1, 1), so that only one additional parameter, f is added to the matting equation. This new single-image 
environ­ment matting equation then becomes C = f W +(1 - .) B + dM(T, A) (30) or, for a smooth background: 
C = f W +(1 - .) B + dT(c) . (31) We extend our simple model by allowing the objects to be pho­tographed 
with bright, near-point light sources. Because the sur­faces of our objects are curved, such light sources 
primarily create bright spots and highlight contours where the normal is equal to the halfway vector 
between the viewing and lighting rays. When applied to images taken with lighting other than the back­drop, 
the recovery algorithm of the last section will discover some points where d>1, i.e., where the observed 
color point lies on the side of the background manifold closer to white. The theory of light transport 
[18] tells us that this should not happen when the object  Figure 9 Sample frames from four environment 
matte video sequences. Rows (a) and (b) show bubbles being blown in an Ehrlenmeyer .ask .lled with glycerin, 
while rows (c) and (d) show a glass being .lled with water. Sequences (a) and (c) were captured with 
no lighting other than the backdrop, so that the foreground color is zero. Sequences (b) and (d) were 
captured separately, shot with the lights on, and the foreground estimation technique is used to recover 
the highlights. is lit only by the backdrop, so we assume that wherever dexceeds unity there must be 
some F-term contribution to the pixel. (In prac­tice, sensor noise means that dcan occasionally exceed 
unity even where there is no highlight. Our algorithm actually looks for high­lights only in regions 
where d>1+ ., and clamps smaller dvalues to the range [0, 1]. For our (fairly noisy) video camera, a 
.in the range 0.03 0.10 is typically used.) Since the highlights we observe will be small or narrow, 
we make the assumption that the refraction direction and transparency will be smoothly varying in the 
area of the highlight. We estimate the pa­rameters d, cx,and cy for the neighborhood around the highlight 
by interpolating from the values at nearby pixels outside the highlight area and then applying Gaussian 
smoothing. Once we have esti­mates for these parameters, we can use Equation 31 to compute f independently 
for each color channel, e.g.: fg = Cg - (1 - .) Bg - dTg(c) , (32) where the g subscript on each variable 
represents the green color component. Similarly, we compute fr and fb for the red and blue color components, 
respectively, and then combine them to esti­mate f : f = max{fr , fg, fb} . (33) In principle, fr, fg,and 
fb should all be the same, but in practice some or all channels of the observed color C can be clipped, 
resulting in an arti.cially low f value. We compute f using each channel sepa­rately and take the maximum 
to counter the effects of such clipping. In practice, the resulting f values may still appear too dim, 
so we scale them up when compositing to produce a more vivid highlight. (a) (b) Figure 10 Image (a) 
is an image of a glass .sh sculpture captured in front of a photograph displayed on a computer monitor. 
Image (b) shows a single frame environment matte composite the same .sh photographed once in front of 
a color wash and digitally composited onto the fruit image. The background image was darkened using the 
color histogram matching technique of Heeger and Bergen [7] to approximate the darkening produced by 
photographing the image on a monitor. In addition, we currently solve for f before feathering ., though 
it would be straightforward to solve for f after feathering. Figure 9 shows some still images taken from 
an environment matte video captured with this method. While this technique is most effec­tive for video, 
still images can also be used for gauging the quality of the extracted mattes. The .rst four rows all 
show examples of liq­uids in motion, which could not be captured by a technique requir­ing multiple images 
of the same object pose. Figure 10 compares the results of video environment matting to a photograph. 
Some de­tails, such as the ridges on the .sh s dorsal .n, which are clearly visible in the photograph, 
have become indistinct with the environ­ment matte, but the environment matte has done a reasonable job 
of capturing the gross refraction pattern of the object.  5 Conclusion Environment matting involves 
an inherent tradeoff: the amount of input data required versus the quality of the resulting matte. The 
original environment matting and compositing paper by Zongker et al. [19] provided one data point in 
this space a reasonably accu­rate model obtained using a reasonably small (logarithmic) number of photographs. 
In this work, we have presented two additional data points in the environment matting design space. The 
.rst captures a higher quality model in which each pixel can see one or more dif­ferent Gaussian regions 
of the environment on a per-channel basis. This approach allows for accurate capture of objects with 
multi­modal refraction and re.ection qualities, or with prismatic color dispersion. In the second approach, 
we limited ourselves to a sin­gle input image to see how much quality could be retained. While the range 
of modeled effects must be severely pared down, in many interesting situations the composite images created 
are still quite convincing. The effect is greatly enhanced by matting an object in motion, an effect 
made possible with a single-frame solution. One general area of future work is to develop more sophisticated 
mathematical tools for extracting environment mattes from our in­put data. For the higher accuracy method, 
we are developing a sen­sitivity analysis of our parameter estimation process in hopes of selecting a 
new, smaller set of basis functions that exhibit greater noise immunity. For the fast, lower accuracy 
method, we are re­searching a more principled Bayesian approach to .tting matte pa­rameters given noisy 
image streams. Finally, our accurate environment matting methods should enable us to capture the behaviors 
of surfaces with bimodal BRDF s, e.g., having specular and diffuse components. Our initial experiments 
in this direction have yielded promising results. However, we have found that monitor illumination is 
too weak when re.ected off of a diffuse surface. By acquiring high dynamic range radiance maps [5], we 
hope to solve this problem and demonstrate interactive light­ing of these more complex environment mattes. 
We also expect that Gaussian weighting functions will not accurately model diffuse re­.ection. Choosing 
new sets of weighting functions to handle such cases is another area for future work.  Acknowledgements 
We would like to thank Eric Veach for his insights on light transport. This work was supported by NSF 
grants 9803226 and 9875365, and by industrial gifts from Intel, Microsoft, and Pixar, and by the Intel 
Fellowship program. References [1] Paul Besl. Active optical range imaging sensors. In Jorge L.C. Sanz, 
editor, Advances in Machine Vision, chapter 1, pages 1 63. Springer-Verlag, 1989. [2] J. F. Blinn and 
M. E. Newell. Texture and re.ection in computer generated images. Communications of the ACM, 19:542 546, 
1976. [3] G. Chazan and N. Kiryati. Pyramidal intensity ratio depth sensor. Technical Re­port 121, Center 
for Communication and Information Technologies, Department of Electrical Engineering, Technion, Haifa, 
Israel, October 1995. [4] B. Curless and M. Levoy. Better optical triangulation through spacetime analysis. 
In Proceedings of IEEE International Conference on Computer Vision, pages 987 994, June 1995. [5] Paul 
E. Debevec and Jitendra Malik. Recovering high dynamic range radiance maps from photographs. In Proceedings 
of SIGGRAPH 97, pages 369 378, Au­gust 1997. [6] Gerd H¨ausler and Dieter Ritter. Parallel three-dimensional 
sensing by color­coded triangulation. Applied Optics, 32(35):7164 7169, December 1993. [7] David J. Heeger 
and James R. Bergen. Pyramid-based texture analysis/synthesis. Proceedings of SIGGRAPH 95, pages 229 
238, August 1995. [8] Eli Horn and Nahum Kiryati. Toward optimal structured light patterns. In Proceedings 
of the International Conference on Recent Advances in Three-Dimensional Digital Imaging and Modeling, 
pages 28 35, 1997. [9] T. Kanade, A. Gruss, and L. Carley. A very fast VLSI range.nder. In 1991 IEEE 
International Conference on Robotics and Automation, volume 39, pages 1322 1329, April 1991. [10] P. 
Perona and J. Malik. Scale space and edge detection using anisotropic diffu­sion. IEEE Trans. on Pattern 
Analysis and Machine Intelligence, 12(7):629 639, July 1990. [11] Thomas Porter and Tom Duff. Compositing 
digital images. In Proceedings of SIGGRAPH 84, volume 18, pages 253 259, July 1984. [12] William H. Press, 
Saul A. Teukolsky, William T. Vetterling, and Brian P. Flan­nery. Numerical Recipes in C: The Art of 
Scienti.c Computing (2nd ed.).Cam­bridge University Press, 1992. [13] K. Sato and S. Inokuchi. Three-dimensional 
surface measurement by space en­coding range imaging. Journal of Robotic Systems, 2:27 39, 1985. [14] 
Erhard Schubert. Fast 3d object recognition using multiple color coded illumi­nation. In Proc. IEEE Conference 
on Acoustics, Speech, and Signal Processing, pages 3057 3060, 1997. [15] Alvy Ray Smith and James F. 
Blinn. Blue screen matting. In Proceedings of SIGGRAPH 96, pages 259 268, August 1996. [16] Gregory J. 
Ward. Measuring and modeling anisotropic re.ection. In Edwin E. Catmull, editor, Computer Graphics (SIGGRAPH 
92 Proceedings), volume 26, pages 265 272, July 1992. [17] G. Wolberg. Digital Image Warping. IEEE Computer 
Society Press, 1990. [18] Yung Yu Chuang, Douglas E. Zongker, Joel Hindorff, Brian Curless, David H. 
Salesin, and Richard Szeliski. Environment matting extensions: Towards higher accuracy and real-time 
capture. Technical Report 2000-05-01, University of Washington, 2000. [19] Douglas E. Zongker, Dawn M. 
Werner, Brian Curless, and David H. Salesin. Environment matting and compositing. In Proceedings of SIGGRAPH 
99, pages 205 214, August 1999.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344849</article_id>
		<sort_key>131</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[The digital Michelangelo project]]></title>
		<subtitle><![CDATA[3D scanning of large statues]]></subtitle>
		<page_from>131</page_from>
		<page_to>144</page_to>
		<doi_number>10.1145/344779.344849</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344849</url>
		<abstract>
			<par><![CDATA[<p>We describe a hardware and software system for digitizing the shape and color of large fragile objects under non-laboratory conditions. Our system employs laser triangulation rangefinders, laser time-of-flight rangefinders, digital still cameras, and a suite of software for acquiring, aligning, merging, and viewing scanned data. As a demonstration of this system, we digitized 10 statues by Michelangelo, including the well-known figure of David, two building interiors, and all 1,163 extant fragments of the Forma Urbis Romae, a giant marble map of ancient Rome. Our largest single dataset is of the David - 2 billion polygons and 7,000 color images. In this paper, we discuss the challenges we faced in building this system, the solutions we employed, and the lessons we learned. We focus in particular on the unusual design of our laser triangulation scanner and on the algorithms and software we developed for handling very large scanned models.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[3D scanning]]></kw>
			<kw><![CDATA[cultural heritage]]></kw>
			<kw><![CDATA[graphics systems]]></kw>
			<kw><![CDATA[mesh generation]]></kw>
			<kw><![CDATA[range images]]></kw>
			<kw><![CDATA[rangefinding]]></kw>
			<kw><![CDATA[reflectance and shading models]]></kw>
			<kw><![CDATA[sensor fusion]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Modeling and recovery of physical attributes</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Digitizing and scanning</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Range data</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.2.8</cat_node>
				<descriptor>Image databases</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003253</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Multimedia databases</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010506</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Document scanning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Management</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Reliability</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP58022955</person_id>
				<author_profile_id><![CDATA[81100593780]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Levoy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39077496</person_id>
				<author_profile_id><![CDATA[81100567347]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kari]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pulli]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14202804</person_id>
				<author_profile_id><![CDATA[81100587184]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Curless]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science and Engineering, University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14080481</person_id>
				<author_profile_id><![CDATA[81100203803]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Szymon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rusinkiewicz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP55041448</person_id>
				<author_profile_id><![CDATA[81366590822]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P173997</person_id>
				<author_profile_id><![CDATA[81100147958]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Lucas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pereira]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14041667</person_id>
				<author_profile_id><![CDATA[81351598092]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Matt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ginzton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P261301</person_id>
				<author_profile_id><![CDATA[81100101151]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Sean]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Anderson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14208622</person_id>
				<author_profile_id><![CDATA[81100603625]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Davis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP58023052</person_id>
				<author_profile_id><![CDATA[81100414712]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>10</seq_no>
				<first_name><![CDATA[Jeremy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ginsberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P147447</person_id>
				<author_profile_id><![CDATA[81100530281]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>11</seq_no>
				<first_name><![CDATA[Jonathan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shade]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science and Engineering, University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P70286</person_id>
				<author_profile_id><![CDATA[81361594436]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>12</seq_no>
				<first_name><![CDATA[Duane]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fulk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cyberware Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>825389</ref_obj_id>
				<ref_obj_pid>523428</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Beraldin, J.-A., Cournoyer,L., Rioux, M., Blais, F., El-Hakim, S.F., Godin, G., ''Object model creation from multiple range images: acquisition, cali-bration, model building and verification,'' Proc. 1st Int'l Conf.on 3-D Digital Imaging and Modeling,IEEE, 1997, pp. 326-333.]]></ref_text>
				<ref_id>Beraldin97</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1889718</ref_obj_id>
				<ref_obj_pid>1889712</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Beraldin, J.-A., Blais, F., Cournoyer,L., Rioux, M., El-Hakim, S.F., Rodell, R., Bernier,F., Harrison, N., ''Digital 3D imaging system for rapid response on remote sites,'' Proc. 2nd Int'l Conf.on 3-D Digital Imaging and Modeling,IEEE, 1999, pp. 34-43.]]></ref_text>
				<ref_id>Beraldin99</ref_id>
			</ref>
			<ref>
				<ref_obj_id>228563</ref_obj_id>
				<ref_obj_pid>228550</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bergevin, R., Soucy, M., Gagnon, H., Laurendeau, D., ''Towards a gen-eral multi-view registration technique,'' IEEE Trans. PAMI,Vol. 18, No. 5, May, 1996, pp. 540-547.]]></ref_text>
				<ref_id>Bergevin96</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Bernardini, F.,Martin, I., Rushmeier,H., ''High-Quality Texture Syn-thesis from Multiple Scans,'' IBM Research Report RC 21656(97598), February, 2000, IBM Research, Yorktown Heights, NY.]]></ref_text>
				<ref_id>Bernardini00</ref_id>
			</ref>
			<ref>
				<ref_obj_id>132022</ref_obj_id>
				<ref_obj_pid>132013</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Besl, P.,McKay,N., ''A Method for Registration of 3-D Shapes,'' IEEE Trans. PAMI,Vol. 14, No. 2, February,1992, pp. 239-256.]]></ref_text>
				<ref_id>Besl92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>138633</ref_obj_id>
				<ref_obj_pid>138628</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Chen, Y.,Medioni, G., ''Object modeling by registration of multiple range images,'' Image and Vision Computing,Vol. 10, No. 3, April, 1992, pp. 145-155.]]></ref_text>
				<ref_id>ChenMed92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383828</ref_obj_id>
				<ref_obj_pid>2383815</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Rocchini, C., Cignoni, P., Montani, C., Scopigno, R., ''Multiple textures stitching and blending on 3D objects,'' Proc. 10th Eurographics Rendering Work-shop, Springer-Verlag, 1999, pp. 119-130.]]></ref_text>
				<ref_id>Rocchini99</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237269</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Curless, B., Levoy,M., ''A Volumetric Method for Building Complex Models from Range Images,'' Proc. SIGGRAPH '96, ACM, 1996, pp. 303-312.]]></ref_text>
				<ref_id>Curless96</ref_id>
			</ref>
			<ref>
				<ref_obj_id>300778</ref_obj_id>
				<ref_obj_pid>300776</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Dana, K.J., Nayer,S.K., Ginneken, B.V,Koenderink, J.J., ''Reflectance and Texture of Real-World Surfaces,'' ACM Trans. on Graphics,Vol. 18, No. 1, Jan-uary 1999, pp. 1-34.]]></ref_text>
				<ref_id>Dana99</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258884</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Debevec, P.E., Malik, J., ''Recovering high dynamic range radiance maps from photographs,'' Proc. SIGGRAPH '97, ACM, 1997, pp. 369-378.]]></ref_text>
				<ref_id>Debevec97</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311560</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Dorsey, J., Edelman, A., Jensen, H.-W., Legakis, J., Pedersen, H.-K., ''Modeling and rendering of weathered stone,'' Proc. SIGGRAPH '99, ACM, 1999, pp. 225-234.]]></ref_text>
				<ref_id>Dorsey99</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Giovannini, P., ''Il 'San Matteo' di Michelangelo: analisi delle tracce di lavorazione, studio degli strumenti e osservazioni sulla tecnica di scultura,'' Riv-ista dell'Opificio delle Pietre Dure,Vol. 10, 1998, pp. 205-228.]]></ref_text>
				<ref_id>Giovannini99</ref_id>
			</ref>
			<ref>
				<ref_obj_id>628700</ref_obj_id>
				<ref_obj_pid>628320</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Hebert, M., Ikeuchi, K., Delingette, H., ''A Spherical Representation for Recognition of Free-Form Surfaces,'' IEEE Trans. PAMI,Vol. 17, No. 7, July, 1995, pp. 681-690.]]></ref_text>
				<ref_id>Hebert95</ref_id>
			</ref>
			<ref>
				<ref_obj_id>794489</ref_obj_id>
				<ref_obj_pid>794189</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Heikkila . J., Silve~n O., ''A four-step camera calibration procedure with implicit image correction,'' Proc. CVPR '97,IEEE, 1997, pp. 1106-1112.]]></ref_text>
				<ref_id>Heikkil&#228;97</ref_id>
			</ref>
			<ref>
				<ref_obj_id>825373</ref_obj_id>
				<ref_obj_pid>523428</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Johnson, A.E., Hebert, M., ''Surface Registration by Matching Oriented Points,'' Proc. 1st Int'l Conf.on 3D Digital Imaging and Modeling,IEEE, 1997, pp. 121-128.]]></ref_text>
				<ref_id>Johnson97</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1889741</ref_obj_id>
				<ref_obj_pid>1889712</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Jokinen, O., ''Self-calibration of a light striping system by matching multi-ple 3-D profile maps,'' Proc. 2nd Int'l Conf.on 3-D Digital Imaging and Model-ing, IEEE, 1999, pp. 180-190.]]></ref_text>
				<ref_id>Jokinen99</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237199</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Levoy,M., Hanrahan, P., ''Light Field Rendering,'' Proc. SIGGRAPH '96, ACM, 1996, pp. 31-42.]]></ref_text>
				<ref_id>Levoy96</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192244</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Miller,G., ''Efficient Algorithms for Local and Global Accessibility Shad-ing, '' Proc. SIGGRAPH '94, ACM, 1994, Computer Graphics pp. 319-326.]]></ref_text>
				<ref_id>Miller94</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Nayar,S.K., Ikeuchi, K., Kanade, T., ''Shape from Interreflections,'' Proc. ICCV '90,IEEE, 1990, pp. 2-11.]]></ref_text>
				<ref_id>Nayar90</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192213</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Oren, M., Nayar,S.K., ''Generalization of Lambert's reflectance model,'' Proc. SIGGRAPH '94, ACM, 1994, pp. 239-246.]]></ref_text>
				<ref_id>Oren94</ref_id>
			</ref>
			<ref>
				<ref_obj_id>628466</ref_obj_id>
				<ref_obj_pid>628301</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Maver, J., Bajcsy,R., ''Occlusions as a Guide for Planning the Next View,'' IEEE Trans. PAMI,Vol. 15, No. 5, May,1993, pp. 417-433.]]></ref_text>
				<ref_id>Maver93</ref_id>
			</ref>
			<ref>
				<ref_obj_id>845284</ref_obj_id>
				<ref_obj_pid>844381</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Pito, R., ''A sensor-based solution to the next best view problem,'' Proc. ICPR '96,1996, pp. 941-945.]]></ref_text>
				<ref_id>Pito96</ref_id>
			</ref>
			<ref>
				<ref_obj_id>925813</ref_obj_id>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Pulli, K., Surface Reconstruction and Display from Range and Color Data, Ph.D. dissertation, University of Washington, 1997.]]></ref_text>
				<ref_id>Pulli97</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1889738</ref_obj_id>
				<ref_obj_pid>1889712</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Pulli, K., ''Multiview registration for large data sets,'' Proc. 2nd Int'l Conf.on 3-D Digital Imaging and Modeling,IEEE, 1999, pp. 160-168.]]></ref_text>
				<ref_id>Pulli99</ref_id>
			</ref>
			<ref>
				<ref_obj_id>732105</ref_obj_id>
				<ref_obj_pid>647651</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Rushmeier,H., Taubin, G., Gue~ziec, A., ''Applying shape from lighting variation to bump map capture,'' Proc. 8th Eurographics Rendering Workshop, Springer-Verlag, 1997, pp. 35-44.]]></ref_text>
				<ref_id>Rushmeier97</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Rushmeier,H., Bernardini, F., Mittleman, J., Taubin, G., ''Acquiring input for rendering at appropriate levels of detail: digitizing a Pieta`,'' Proc. 9th Eurographics Rendering Workshop,Springer-Verlag, 1998, pp. 81-92.]]></ref_text>
				<ref_id>Rushmeier98</ref_id>
			</ref>
			<ref>
				<ref_obj_id>344940</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Rusinkiewicz, S., Levoy,M., ''Qsplat: a multiresolution point render-ing system for large meshes,'' Proc. SIGGRAPH 2000 (these proceedings).]]></ref_text>
				<ref_id>Rusinkiewicz00</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258885</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Sato, Y.,Wheeler,M.D., Ikeuchi, K., ''Object shape and reflectance modeling from observation,'' Proc. SIGGRAPH '97, ACM, 1997, pp. 379-387.]]></ref_text>
				<ref_id>Sato97</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[De Tolnay,C., ''Michelangelo,'' Princeton University Press, 1945.]]></ref_text>
				<ref_id>Tolnay45</ref_id>
			</ref>
			<ref>
				<ref_obj_id>344925</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Wood, D., Azuma, D., Aldinger,K., Curless, B., Duchamp, T., Salesin, D., Stuetzle, W., ''Surface Light Fields for 3D Photography,'' Proc. SIGGRAPH 2000 (these proceedings).]]></ref_text>
				<ref_id>Wood00</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311559</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Yu, Y., Debevec, P., Malik, J., Hawkins, T., ''Inverse global illumination: recov-ering reflectance models of real scenes from photographs,'' Proc. SIGGRAPH '99, ACM, 1999, pp. 215-224.]]></ref_text>
				<ref_id>Yu99</ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Zhang, D., Hebert, M., ''Harmonic maps and their applications in surface matching,'' Proc. Computer Vision and Pattern Recognition (CVPR) '99,IEEE, 1999, pp. 525-530.]]></ref_text>
				<ref_id>Zhang99</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344855</article_id>
		<sort_key>145</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[Acquiring the reflectance field of a human face]]></title>
		<page_from>145</page_from>
		<page_to>156</page_to>
		<doi_number>10.1145/344779.344855</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344855</url>
		<abstract>
			<par><![CDATA[<p>We present a method to acquire the reflectance field of a human face and use these measurements to render the face under arbitrary changes in lighting and viewpoint. We first acquire images of the face from a small set of viewpoints under a dense sampling of incident illumination directions using a light stage. We then construct a reflectance function image for each observed image pixel from its values over the space of illumination directions. From the reflectance functions, we can directly generate images of the face from the original viewpoints in any form of sampled or computed illumination. To change the viewpoint, we use a model of skin reflectance to estimate the appearance of the reflectance functions for novel viewpoints. We demonstrate the technique with synthetic renderings of a person's face under novel illumination and viewpoints.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[facial animation]]></kw>
			<kw><![CDATA[image-based modeling]]></kw>
			<kw><![CDATA[rendering and lighting]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Intensity, color, photometry, and thresholding</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Photometry</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Radiometry</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP40023545</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Division, University of California at Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43117244</person_id>
				<author_profile_id><![CDATA[81100179328]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hawkins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Division, University of California at Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31098981</person_id>
				<author_profile_id><![CDATA[81100346087]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tchou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Division, University of California at Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31076846</person_id>
				<author_profile_id><![CDATA[81100137101]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Haarm-Pieter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Duiker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Division, University of California at Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P298139</person_id>
				<author_profile_id><![CDATA[81100336222]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Westley]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sarokin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Division, University of California at Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31092816</person_id>
				<author_profile_id><![CDATA[81322505026]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sagar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[LifeF/X, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ADELSON, E. H., AND BERGEN, J. R. Computational Models of Visual Processing. MIT Press, Cambridge, Mass., 1991, ch. 1. The Plenoptic Function and the Elements of Early Vision.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134003</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BEIER, Y., AND NEELY, S. Feature-based image metamorphosis. Computer Graphics (Proceedings of SIGGRAPH 92) 26, 2 (July 1992), 35-42.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311556</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BLANZ, V., AND VETTER, T. A morphable model for the synthesis of 3d faces. Proceedings of SIGGRAPH 99 (August 1999), 187-194.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258880</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BREGLER, C., COVELL, M., AND SLANEY, M. Video rewrite: Driving visual speech with audio. Proceedings of SIGGRAPH 97 (August 1997), 353-360.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[BUSBRIDGE, I. W. The Mathematics of Radiative Transfer. Cambridge University Press, Bristol, UK, 1960.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806819</ref_obj_id>
				<ref_obj_pid>800224</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[COOK, R. L., AND TORRANCE, K. E. A reflectance model for computer graphics. Computer Graphics (Proceedings of SIGGRAPH 81) 15, 3 (August 1981), 307-316.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>794511</ref_obj_id>
				<ref_obj_pid>794189</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[DANA, K. J., GINNEKEN, B., NAYAR, S. K., AND KOENDERINK, J.J. Reflectance and texture of real-world surfaces. In Proc. IEEE Conf. on Comp. Vision and Patt. Recog. (1997), pp. 151-157.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280864</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[DEBEVEC, P. Rendering synthetic objects into real scenes: Bridging traditional and image-based graphics with global illumination and high dynamic range photography. In SIGGRAPH 98 (July 1998).]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258884</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[DEBEVEC, P. E., AND MALIK, J. Recovering high dynamic range radiance maps from photographs. In SIGGRAPH 97 (August 1997), pp. 369-378.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[DEBEVEC, P. E., YU, Y., AND BORSHUKOV, G. D. Efficient viewdependent image-based rendering with projective texture-mapping. In 9th Eurographics workshop on Rendering (June 1998), pp. 105-116.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>649053</ref_obj_id>
				<ref_obj_pid>645311</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[FUA, P., AND MICCIO, C. From regular images to animated heads: A least squares approach. In ECCV98 (1998).]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[GERSHUN, A. Svetovoe Pole (the Light Field, in English). Journal of Mathematics and Physics XVIII (1939), 51--151.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237200</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[GORTLER, S. J., GRZESZCZUK, R., SZELISKI, R., AND COHEN, M. F. The Lumigraph. In SIGGRAPH 96 (1996), pp. 43-54.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280822</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[GUENTER, B., GRIMM, C., WOOD, D., MALVAR, H., AND PIGHIN, F. Making faces. Proceedings of SIGGRAPH 98 (July 1998), 55-66.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[HAEBERLI, P. Synthetic lighting for photography. Available at http://www.sgi.com/grafica/synth/index.html, January 1992.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166139</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[HANRAHAN, P., AND KRUEGER, W. Reflection from layered surfaces due to subsurface scattering. Proceedings of SIGGRAPH 93 (August 1993), 165-174.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[KARNER, K. F., MAYER, H., AND GERVAUTZ, M. An image based measurement system for anisotropic reflection. In EUROGRAPHICS Annual Conference Proceedings (1996).]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383838</ref_obj_id>
				<ref_obj_pid>2383815</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[KAUTZ, J., AND McCOOL, M. D. Interactive rendering with arbitrary BRDFs using separable approximations. Eurographics Rendering Workshop 1999 (June 1999).]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258801</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[LAFORTUNE, E. P. F., FOG, S.-C., TORRANCE, K. E., AND GREENBERG, D. P. Non-linear approximation of reflectance functions. Proceedings of SIGGRAPH 97 (August 1997), 117-126.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218407</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[LEE, Y., TERZOPOULOS, D., AND WATERS, K. Realistic modeling for facial animation. Proceedings of SIGGRAPH 95 (August 1995), 55-62.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237199</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[LEVOY, M., AND HANRAHAN, P. Light field rendering. In SIG- GRAPH 96 (1996), pp. 31-42.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383829</ref_obj_id>
				<ref_obj_pid>2383815</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[MARSCHNER, S. R., WESTIN, S. H., LAFORTUNE, E. P. F., TOR- RANCE, K. E., AND GREENBERG, D. P. Image-based BRDF measurement including human skin. Eurographics Rendering Workshop 1999 (June 1999).]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[MILLER, G. S. P., RUBIN, S., AND PONCELEON, D. Lazy decompression of surface light fields for precomputed global illumination. Eurographics Rendering Workshop 1998 (June 1998), 281-292.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>248634</ref_obj_id>
				<ref_obj_pid>248633</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[NAYAR, S., FANG, X., AND BOULT, T. Separation of reflection components using color and polarization. IJCV 21, 3 (February 1997), 163-186.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[NICODEMUS, F. E., RICHMOND, J. C., HSIA, J. J., GINSBERG, I. W., AND LIMPERIS, T. Geometric considerations and nomenclature for reflectance.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[NIMEROFF, J. S., SIMONCELLI, E., AND DORSEY, J. Efficient rerendering of naturally illuminated environments. Fifth Eurographics Workshop on Rendering (June 1994), 359-373.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192213</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[OREN, M., AND NAYAR, S. K. Generalization of Lambert's reflectance model. Proceedings of SIGGRAPH 94 (July 1994), 239- 246.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>569955</ref_obj_id>
				<ref_obj_pid>800193</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[PARKE, F. I. Computer generated animation of faces. Proc. ACM annual conf. (August 1972).]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280825</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[PIGHIN, F., HECKER, J., LISCHINSKI, D., SZELISKI, R., AND SALESIN, D. U. Synthesizing realistic facial expressions from photographs. Proceedings of SIGGRAPH 98 (July 1998), 75-84.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192200</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[SAGAR, M. A., BULLIVANT, D., MALLINSON, G. D., HUNTER, P. J., AND HUNTER, I. W. A virtual environment and model of the eye for surgical simulation. Proceedings of SIGGRAPH 94 (July 1994), 205-213.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[SATO, Y., AND IKEUCHI, K. Temporal-color space analysis of reflection. JOSA-A 11, 11 (November 1994), 2990-3002.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258885</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[SATO, Y., WHEELER, M. D., AND IKEUCHI, K. Object shape and reflectance modeling from observation. In SIGGRAPH 97 (1997), pp. 379-387.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[SMITH, B., AND ROWE, L. Compressed domain processing of JPEG- encoded images. Real-Time Imaging 2, 2 (1996), 3-17.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[TERZOPOULOS, D., AND WATERS, K. Physically-based facial modelling, analysis, and animation. Journal of Visualization and Computer Animation 1, 2 (August 1990), 73-80.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[TORRANCE, K. E., AND SPARROW, g. M. Theory for off-specular reflection from roughened surfaces. Journal of Optical Society of America 57, 9 (1967).]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[VAN GEMERT, M. F. C., JACQUES, S. L., STERENBERG, H. J. C. M., AND STAR, W.M. Skin optics. IEEE Transactions on Biomedical Engineering 36, 12 (December 1989), 1146-1154.]]></ref_text>
				<ref_id>36</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134078</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[WARD, G. J. Measuring and modeling anisotropic reflection. In SIG- GRAPH 92 (July 1992), pp. 265-272.]]></ref_text>
				<ref_id>37</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97906</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[WILLIAMS, L. Performance-driven facial animation. Computer Graphics (Proceedings of SIGGRAPH 90) 24, 4 (August 1990), 235- 242.]]></ref_text>
				<ref_id>38</ref_id>
			</ref>
			<ref>
				<ref_obj_id>731971</ref_obj_id>
				<ref_obj_pid>647651</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[WONG, T.-T., HERO, P.-A., OR, S.-H., AND NO, W.-Y. Imagebased rendering with controllable illumination. Eurographics Rendering Workshop 1997 (June 1997), 13-22.]]></ref_text>
				<ref_id>39</ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Wu, Y., THALMANN, N. M., AND THALMANN, D. A dynamic wrinkle model in facial animation and skin aging. Journal of Visualization and Computer Animation 6, 4 (October 1995), 195-206.]]></ref_text>
				<ref_id>40</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311559</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[Yu, Y., DEBEVEC, P., MALIK, J., AND HAWKINS, T. Inverse global illumination: Recovering reflectance models of real scenes from photographs. Proceedings of SIGGRAPH 99 (August 1999), 215-224.]]></ref_text>
				<ref_id>41</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311558</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[ZONGKER, D. E., WERNER, D. M., CURLESS, B., AND SALESIN, D.H. Environment matting and compositing. Proceedings of SIG- GRAPH 99 (August 1999), 205-214.]]></ref_text>
				<ref_id>42</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344859</article_id>
		<sort_key>157</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[As-rigid-as-possible shape interpolation]]></title>
		<page_from>157</page_from>
		<page_to>164</page_to>
		<doi_number>10.1145/344779.344859</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344859</url>
		<abstract>
			<par><![CDATA[<p>We present an object-space morphing technique that blends the interiors of given two- or three-dimensional shapes rather than their boundaries. The morph is rigid in the sense that local volumes are least-distorting as they vary from their source to target configurations. Given a boundary vertex correspondence, the source and target shapes are decomposed into isomorphic simplicial complexes. For the simplicial complexes, we find a closed-form expression allocating the paths of both boundary and interior vertices from source to target locations as a function of time. Key points are the identification of the optimal simplex morphing and the appropriate definition of an error functional whose minimization defines the paths of the vertices. Each pair of corresponding simplices defines an affine transformation, which is factored into a rotation and a stretching transformation. These local transformations are naturally interpolated over time and serve as the basis for composing a global coherent least-distorting transformation.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[compatible triangulation]]></kw>
			<kw><![CDATA[shape blending]]></kw>
			<kw><![CDATA[vertex path problem]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.1</cat_node>
				<descriptor>Interpolation formulas</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.2.2</cat_node>
				<descriptor>Path and circuit problems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Size and shape</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003624.10003633.10003640</concept_id>
				<concept_desc>CCS->Mathematics of computing->Discrete mathematics->Graph theory->Paths and connectivity problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010242</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Shape representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003722</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Interpolation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14091648</person_id>
				<author_profile_id><![CDATA[81100235480]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Alexa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Darmstadt University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40030281</person_id>
				<author_profile_id><![CDATA[81100264399]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cohen-Or]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tel Aviv University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40026909</person_id>
				<author_profile_id><![CDATA[81100404814]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Levin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tel Aviv University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[M. Alexa. Merging Polyhedral Shapes with Scattered Features. The Visual Computer, 16, 1, 2000]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>160839</ref_obj_id>
				<ref_obj_pid>160837</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[B. Aronov, R. Seidel, and D. Souvaine. On compatible triangulations of simple polygons. Computational Geometry: Theory and Applications 3, pp. 27-35, 1993]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134003</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[T. Beier and S. Neely. Feature-based Image Metamorphosis. SIGGRAPH '92 Proceedings, pp. 35-42, 1992]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>261226</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[M. de Berg, M. van Krefeld, M. Overmars, and O. Schwarzkopf. Computational Geometry - Algorithms and Applications. Springer, Berlin, 1997]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[B. Chazelle. Triangulating a simple polygon in linear time. Proc/ 31st Symp. on Foundations of Computer Science (FOCS), pp. 220-230, 1990]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[E. Carmel, D. Cohen-Or. Warp-guided Object Space Morphing. The Visual Computer, 13, 1997]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[S. Cohen, G. Elber, R. Bar Yehuda. Matching of freeform curves. CAD, 19, 5, pp. 369-378, 1997]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>274366</ref_obj_id>
				<ref_obj_pid>274363</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[D. Cohen-Or, D. Levin, and A. Solomovici. Three dimensional distance field metamorphosis. ACM Transactions on Graphics, 1998]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>614365</ref_obj_id>
				<ref_obj_pid>614265</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[M. Etzion and A. Rappoprt. On Compatible Star Decompositions of Simple Polygons. IEEE Transactions on Visualization and Computer Graphics, 3, 1, pp. 87-95, 1997]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>309532</ref_obj_id>
				<ref_obj_pid>309520</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[M. S. Floater and C. Gotsman. How to Morph Tilings Injectively. J. Comp. Appl. Math., 101, pp. 117-129, 1999]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[L.A. Freitag, M.T. Jones, and P.E. Plassmann. An efficient parallel algorithm for mesh smoothing. 4th Int. Meshing Roundtable, pp. 47-58, 1995]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[G.H. Golub and C.F. van Loan. Matrix Computations. The Johns Hopkins University Press, Baltimore, 1983]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[E. Goldstein and C. Gotsman. Polygon Morphing using a Multiresolution Representation. Graphics Interface '95, pp. 247- 254, 1995]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>791540</ref_obj_id>
				<ref_obj_pid>521641</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[A. Greogory, A. State, M. Lin, D. Manocha, and M. Livingston. Feature-based surface decomposition for correspondence and morphing between polyhedra. Proceedings of Computer Animation '98, pp. 64-71, 1998]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>951107</ref_obj_id>
				<ref_obj_pid>951087</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[T. He, S. Wang, and A. Kaufman. Wavelet-basedVolume Morphing. Proceedings of Visualization, IEEE Computer Society, pp. 85-91, 1994]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134004</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[J.F. Hughes. Scheduled Fourier Volume Morphing. Computer Graphics (SIGGRAPH '92 Proceedings), 26, 2, pp. 43-46, 1992]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[B. Joe. Geompack. ftp://ftp.cs.ualberta.ca/pub/geompack]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>826413</ref_obj_id>
				<ref_obj_pid>826026</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[T. Kanai, H Suzuki, and F. Kimura. 3D geometric metamorphosis based on harmonic maps. Proceedings of Pacific Graphics '97, pp. 97-104, 1997]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134007</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[J.R. Kent, W.E. Carlson, and R.E. Parent. Shape Transformation for polyhedral objects. Computer Graphics, 26, pp. 47-54, 1992]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311586</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[A.W.F. Lee, D. Dobkin, W. Sweldens, and P. Schr6der. Multiresolution Mesh Morphing. SIGGRAPH '99 Proceedings, pp. 343-350, 1999]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218501</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[S.Y. Lee, K.Y. Chwa, S.Y. Shin, and G. Wolberg. Image Metamorphosis Using Snakes and Free-Form Deformations. SIG- GRAPH '95 Proceedings, pp. 439-448, 1995]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218502</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[A. Lerios, C.D. Garfinkle, and M. Levoy. Feature-Based Volume Metamorphis. SIGGRAPH '95 Proceedings, pp. 449- 456, 1995]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134001</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[T.W. Sederberg and E. Greenwood. A physically based approach to 2D shape blending. Computer Graphics, 26, pp. 25- 34, 1992]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[T.W. Sederberg, P. Gao, G. Wang, and H. Mu. 2-D shape blending: An intrinsic solution to the vertex-path problem. Computer Graphics, 27, pp. 15-18, 1993]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617969</ref_obj_id>
				<ref_obj_pid>616035</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[M. Shapira and A. Rappoport. Shape blending using the starskeleton representation. IEEE CG&amp;A, 15, pp. 44-51, 1993]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[A. Shapiro and A. Tal. Polyhedron realization for shape transformation. The Visual Computer, 14, 8/9, 1998]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>155324</ref_obj_id>
				<ref_obj_pid>155294</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[K. Shoemake and T. Duff. Matrix Animation and Polar Decomposition. Proceedings of Graphics Interface '92, pp. 258- 264, 1992]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[A. Tal and G. Elber. Image Morphing with Feature Preserving Texture. Computer Graphics Forum (Eurographics '99 Proceedings), 18, 3, pp. 339-348, 1999]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[G. Wolberg. Digital Image Morphing. IEEE Computer Society Press, 1990]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[G. Wolberg. Image Morphing Survey. The Visual Computer, 14, 8/9, 1998]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618357</ref_obj_id>
				<ref_obj_pid>616041</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Y. Zhang. A Fuzzy Approach to Digital Image Warping. IEEE Computer Graphics and Applications, pp. 33-41, 1996]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 As-Rigid-As-Possible Shape Interpolation Marc Alexa Daniel Cohen-Or David Levin Darmstadt University 
of Technology Tel Aviv University Tel Aviv University Figure 1: The Statue of Liberty becomes the Christ 
statue on the Corcovado. Abstract We present an object-space morphing technique that blends the in­teriors 
of given two-or three-dimensional shapes rather than their boundaries. The morph is rigid in the sense 
that local volumes are least-distorting as they vary from their source to target con.gura­tions. Given 
a boundary vertex correspondence, the source and tar­get shapes are decomposed into isomorphic simplicial 
complexes. For the simplicial complexes, we .nd a closed-form expression al­locatingthepathsofbothboundaryandinterior 
verticesfrom source to target locations as a function of time. Key points are the identi.­cation of the 
optimal simplex morphing and the appropriate de.ni­tion of an error functional whose minimization de.nes 
the paths of the vertices. Each pair of corresponding simplices de.nes an af.ne transformation, which 
is factored into a rotation and a stretching transformation. These local transformations are naturally 
interpo­lated over time and serve as the basis for composing a global coher­ent least-distorting transformation. 
CR Categories: I.3.3 [Computer Graphics]: Picture/Image Generation Display algorithms; I.3.5 [Computer 
Graphics]: Com­putational Geometry and Object Modeling Curve, surface, solid, and object representations; 
I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism Animation; I.4.7 [Image Processing]: 
Feature Measurement Size and shape Keywords: shape blending, vertex path problem, compatible tri­angulation 
 Permission to make digital or hard copies of part or all of this work or personal or classroom use is 
granted without fee provided that copies are not made or distributed for profit or commercial advantage 
and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, 
to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. SIGGRAPH 
2000, New Orleans, LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 1 Introduction Techniques 
that transform one graphical object into another have gained widespread use in recent years. These techniques, 
known as morphing or blending techniques, involve the creation of a smooth transition from an initial 
object to a target object. They have proven to be powerful for visual effects, and there are now many 
breath­taking examples in .lm and television. In most cases, these morph­ing effects are generated using 
image-based techniques [3, 21, 30] where the geometry of the source and target object are somewhat similar, 
or the process requires extensive user labor. A primary chal­lenge in this area is to devise algorithms 
to blend two given ob­jects of signi.cantly different shape, with minimal user interaction, so that the 
in-between objects retain their original appearance and properties as much as possible. The morph example 
in Figure 1 is dif.cult to achieve with an image-based technique, because fore­ground and background 
behave differently. Object-space morphing treats explicit representations of the ob­jects [14, 20] instead 
of discretizations of space (images, volumes). Assuming the objects are either polygons (in 2D) or polyhedra 
(in 3D), the morphing process consists of generating a correspondence between the geometric features 
of the representation, known as the vertex correspondence problem [23, 19, 6, 18, 14, 1, 20], and then 
interpolating the positions of the boundary representations along predetermined paths, known as the vertex 
path problem [24, 25, 13, 8]. Most of the cited object-space morphing techniques are con­cerned with 
the correspondence problem while simply linearly in­terpolating the corresponding vertices, not taking 
into account that the blended shapes are implicitly representing meta-physical en­tities. Generally speaking, 
aesthetic and intuitive shape blending should aim at treating the objects as rigidly as possible and 
avoid super.uous global or local deformations. In [24], Sederberg intro- Figure 2: Morph sequence of 
Haring-like .gures showing their homeomorphic dissections a) b) Figure 3: Contour blends of the elephant-giraffe 
example. Simple linear vertex interpolation in (a) vs. as-rigid-as-possible shape in­terpolation in (b). 
Figure 4: The homeomorphic dissections of the shapes in the elephant-giraffe example duced techniques 
that minimize the deformation of the boundaries. Shapira and Rappaport [25] suggested that a proper morph 
cannot be expressed merely as a boundary interpolation, but as a smooth blend of the interior of the 
objects. To achieve such an interior inter­polation, they represented the interior of the 2D shapes by 
compati­ble skeletons and applied the blend to the parametric description of the skeletons. The automatic 
creation of corresponding equivalent skeletons of two shapes is involved, and though theoretically possi­ble 
for all shapes, it seems natural for similar shapes, but ambiguous for rather different shapes like the 
letters U and T. In this paper, we present an object-space morphing technique that blends the interior 
of the shapes rather than their boundaries to achieve a sequence of in-between shapes which is locally 
least­distorting. Assuming that a boundary vertex correspondence of the source and target shapes is given, 
we apply an algorithm for dis­secting the source and target shapes into isomorphic simplicial com­plexes, 
i.e. triangles or tetrahedra. Then, we develop a method for interpolating the locations of corresponding 
vertices, both boundary and interior, along their paths from the source to the target object. Simplicial 
complexes allow the local deformation of the shapes to be analyzed and controled. Floater and Gotsman 
have used barycentric coordinates to morph compatible triangulations with convex boundary so that no 
triangles .ip on their way from the source to the target con.guration [10]. However, interpolation of 
barycentric coordinates is not motivated by or related to physical or esthetical principles. We start 
by determining an optimal least-distorting morphing be­tween a source simplex and a target simplex (triangles 
in the 2D case and tetrahedra in the 3D case). Then, the general idea is to .nd a transformation which 
is locally as similar as possible to the optimal transformation between each pair of corresponding simplices. 
 2 Transforming Shapes Given two objects together with a set of point-to-point correspon­dences between 
user-de.ned control (anchor) points, one can de­.ne an elastic transformation between the objects that 
exactly sat­is.es the correspondences. However, to reduce the distortion of the in-between shapes, it 
is advisable to determine the rigid part of the transformation and interpolate it separately from the 
elastic part [8, 31]. The rotational component of the rigid part should be interpolated so that the object 
is non-deforming, e.g. using quater­nion interpolation [27]. The rigid-elastic decomposition of the warp 
function and its particular interpolation are so chosen to minimize the distortion of the intermediate 
shapes. The rigid part performs the general positional changes, while the .ne details are gradually changed 
by the elastic part. In many applications, this decomposition does improve the mor­phing results, though 
it cannot prevent local distortions in cases of body movements which are more involved as may be found 
in ar­ticulated objects. The underlying assumption in [6, 8, 31] is that the movement can roughly be 
approximated by rotation, stretching and translation. If we consider objects such as animals bodies or 
so­phisticated mechanical objects, such as industrial robots, it is clear that even the simplest movements 
cannot be well approximated by a single rotation and translation. To reduce distortions in transforma­tionsofbodiescomprisinglocalrotations, 
thedecompositionshould be more elaborate. The idea is to determine local non-distorting motions rather 
than a global one. The composed shape morphing should behave locally as close as possible to the ideal 
local ones. Figure 3 shows a blend between an elephant and a giraffe. The two shapes are aligned and 
a single rotation cannot prevent the distor­tions of a linear interpolation, whereas the locally least-distorting 
interpolation yields a pleasing blend of such articulated objects. Based on a compatible dissection of 
the interiors of the shapes (see Figures 2 and 4), we .rst de.ne local af.ne transformations. Each of 
the local linear maps can be separately decomposed into a rotation and a stretch. Thus, locally, we know 
how to achieve a non-distorting morph. Then, these local transformations are com­posed into a global 
coherent non-distorting transformation, which minimizes the overall local deformation. It should be noted 
that our transformation is (globally) rigidly reducible; that is, if there is a sin­gle rigid transformation 
that aligns the objects, the morph follows such a path. We only consider simplicial complexes as dissections 
of shapes. Speci.cally, a two-dimensional shape is a polygon and its dissection a triangulation, and 
a three-dimensional shape is a polyhedron and its dissection a tetrahedralization. In the following, 
we introduce an interpolation technique for determining vertex paths in shape blend­ing, given a source 
and a target shape represented by homeomor­phic (compatible) triangulations. In Section 3, we show how 
to compute such homeomorphic dissections from boundary represen­tations. Note that we describe the concept 
of determining the vertex paths in two dimensions for clarity; the extension to three or more dimensions 
is straightforward. 2.1 Least-Distorting Triangle-to-Triangle Morph­ing Suppose the triangulation of 
the source and target shapes consists of only one triangle each. Let the source vertices be Pp~1;p~2;p~3) 
and the target vertices be Qq~1;q~2;q~3), where vertices with the same index correspond. An af.ne mapping 
represented by matrix Atransforms Pinto Q: a1 a2 lx Ap~i+ ~l p~i+ q~i;i2f1;2;3g(1) a3 a4ly a) b) c) 
d) Figure 5: Transformations of a single triangle. (a) Linear vertex in­terpolation. (b-d) An af.ne map 
from the source to the target trian­gle is computed and factored into rotational and scale-shear parts. 
Intermediate triangles are constructed by linearly interpolating the angle(s) of rotation, the scaling 
factors, and the shear parameter. (b) corresponds to Equation 4; (c) shows the results of reducing the 
overall angle of (b) by subtracting 27from one of the angles; (d) corresponds to Equation 5 and represents 
the method of our choice. The last column in all rows shows plots of the vertex paths. We do not take 
the translation ~linto account for shape interpola­tion since it does not describe any property of the 
shape itself except for its placing in the scene. Rather, we want to describe intermedi­ate shapes by 
varying the rotational and scaling parts comprising A, over time. Note that the coef.cients of Aare linear 
in the coordi­nates of the target shape. Intermediate shapes Vt)v~1t);~v2t);~v3t))are described as Vt)At)P. 
The question is how to de.ne At)reasonably? The simplest solution would be: At)1,t)I+tA. However, some 
properties of At)seem to be desirable, calling for a more elaborate approach: The transformation should 
be symmetric with respect to t.  The rotational angle(s) and scale should change linearly.  The triangle 
should keep its orientation, i.e. should not be re­.ected.  The resulting vertices paths should be simple. 
 The basic idea is to factor Ainto rotations (orthogonal matrices) and scale-shear parts with positive 
scaling components. We have examined several decompositions. A natural candidate is the singu­lar value 
decomposition (SVD) [12], since the resulting decompo­sition is symmetric and well-de.ned for arbitrary 
dimensions: sx0 ARRDR(RRR(;sx;sy>0(2) 0sy However, through experimentation, we have found a decomposition 
into a single rotation and a symmetric matrix, to yield the visually­best transformations. This result 
is supported by Shoemake in [27] for mathematical, as well as psychological, reasons. The decompo­sition 
can be deduced from the SVD as follows: ARDR(R(R()DR(RRRT ss Txh RRR()R(DR()R,SR,(3) ss hy with sx;sy>0. 
Based on the decomposition, At)is computed by linearly interpolating the free parameters in the factorizations 
in (2) and (3) , i.e. AR;(t)RtR1,t)I+tD)Rt( (4) and respectively, A,t)Rt,1,t)I+tS) (5) Figure 5 illustrates 
the resulting transformations from a source to a target triangle. For comparison, 5(a) shows linear interpolation 
of vertex coordinates. The transformation resulting from a singu­lar value decomposition and linear interpolation 
AR;(t)is depicted in 5(b). Note that the result is symmetric and linear in the rotation angle but still 
unsatisfactory, since a rotation of more than 7is un­necessary. However, if we subtract 27from one of 
the angles (de­picted in 1(c)) the result is even more displeasing. We have found that decomposing Ainto 
one rotation and a symmetric matrix and using A,t)yields the best results (Figure 5(d)). It avoids unnec­essary 
rotation or shear compared to the SVD and is usually more symmetric than a QR decomposition-based approach. 
Note that the rotation of the triangle does not contribute to its shape. However, this is no longer true 
for more than a single triangle, as we shall see in the next section, which discusses the generalization 
to more than one triangle. 2.2 Closed-Form Vertex Paths for a Triangulation Wenowconsidera triangulation 
TfTfi;j;kggrather than a single triangle. Each of the source triangles Pfi;j;kgp~i;~pj;~pk)corre­sponds 
to a target triangle Qfi;j;kgq~i;~qj;~qk). For each pair of triangles, we compute a mapping Afi;j;kg, 
which can be factored by Eq. 5 to determine Afi;j;kgt). Since most of the vertices corre­spond to more 
than one triangle, a mapping of all vertices could not (in general) be conforming with all the individual 
ideal transforma­tions Afi;j;kgt). Let Vt)v~1t);:::;v~nt));t2[0;1] (6) be the desired paths of the vertices, 
satisfying V0)p~1;:::;p~n) V1)q~1;:::;q~n): We de.ne Bfi;j;kgt)to be the matrix in the af.ne transformation 
from Pfi;j;kgto v~it);~vjt);~vkt),i.e. ~ Bfi;j;kgt)p~f+lv~ft);f2fi;j;kg (7) Note that the coef.cients 
of Bfi;j;kgt)are linear in v~it);~vjt);~vkt). We de.ne an intermediate shape Vt)as the vertex con.guration 
which minimizes the quadratic error between the actual matrices Bfi;j;kgt)and the desired ones Afi;j;kgt). 
This quadratic error functional is expressed as X 2 EV(t) Afi;j;kgt),Bfi;j;kgt) ;(8) fi;j;kg2T where 
k kis the Frobenius norm. In this expression, the Afi;j;kgt) are known for a .xed time tand each Bfi;j;kgis 
linear in the v~it);~vjt);~vkt). Thus, EV(t)is a positive quadratic form in the elements of Vt). In order 
to have a unique minimizer to EV(t),we should predetermine the location of one vertex, say v1xt);v1yt), 
for example, by linear interpolation. Figure 6: Transformations of different shapes representing solid 
ob­jects. Note that parts of the shapes transform rigidly whenever pos­sible. The lowest row shows an 
example where the shapes have no obvious common skeleton. The functional EV(t)can be expressed in matrix 
form, setting u T 1;v2t);v2y t);:::;vnt);vny t))as xx GT u T cu; (9) EV(t) GH where c2IRrepresents the 
constant, G2IR2nx1the linear, and H2IR2nx2nthe mixed and pure quadratic coef.cients of the quadratic 
form EV(t). The minimization problem is solved by set­ting the gradient rEV(t)over the free variables 
to zero: 0 v2t) 1 x v2y t) H@A,G (10) . . . Note that His independent of t. This means we can invert 
Hand .nd solutions for time tby computing the corresponding Gt)and a single matrix multiplication: Vt),H,1Gt) 
(11) In practice, we compute the LU decomposition of Hand .nd Vt) by back substitution. Furthermore, 
the computations are separable and are performed independently for the two coordinates. Note that only 
Gdepends on the dimension, while His the same for the xand ycomponents. Thus, His effectively of size 
n,1xn,1,which means the dominating factor of the computation is independent of the dimension. The above 
de.nition has the following notable properties: For a given t, the solution is unique.  The solution 
requires only one matrix inversion for a speci.c source and target shape. Every intermediate shape is 
found by multiplying the inverted matrix by a vector.  The vertex path is in.nitely smooth, starts exactly 
in the source shape, and ends exactly in the target shape. These are proper­ties typically dif.cult to 
achieve in physically-based simula­tions.  Figure 6 shows transformations of some simple shapes produced 
with the described method. 2.3 Symmetric Solutions While we were satis.ed with the degree of symmetry 
the previosly explained approach exhibited in most of our test cases, a symmetric solution can be advantageous 
 in particular, if the corresponding triangles in the source and target shapes have largely differing 
area. We can make the solution symmetric by simply blending the opti­mization problems from both directions. 
Let A! ft)be the af.ne transformation of triangle ffrom source to intermediate shape at time t,and Ajt)the 
respective transformation coming from the f target shape. Similarly, we de.ne B!t)and Bjt).We de.ne ffintermediate 
EV(t), the vertex con.guration at time t,by EV(t) 1 ,t)E! V(t)+tEj V(t) (12) where E! V(t) X A! f t),B! 
f t) 2 (13) f2Tri Ej V(t) X Ajf 1,t),Bjf 1,t) 2 (14) f2Tri With this de.nition, we lose the advantage 
of only one matrix in­version for given source and target shapes. Instead, every time t requires the 
solution of a linear system of equations. Whether the computation times are acceptable depends on the 
shapes and the de­sired application.  3 Isomorphic Dissections of Shapes In this section, we construct 
isomorphic dissections given two shapes in boundary representation. We assume that the correspon­dence 
of the boundaries has been established, i.e. a bijective map between boundary vertices is given. For 
polygons, reasonable cor­respondence can be found automatically [23, 7]. In dif.cult cases, few correspondences 
could be speci.ed manually and the remain­ing vertices are matched automatically. For polyhedral objects, 
sev­eral techniques exist, which are based on topological merging intro­duced by Kent et al. [19]. Recent 
work [14, 20] also allows the spec­i.cation of corresponding features which seems suf.cient to pro­duce 
acceptable results for a variety of polyhedral models. 3.1 Polygons The problem of constructing a common 
triangulation for two given polygons is discussed in the literature as compatible triangulation [2]. 
Triangulating a single polygon 7is possible using only the ver­tices of the polygon (e.g. [5]). However, 
this is usually not possi­ble for two different polygons. Aronov et al. [2] show how to tri­angulate 
two polygons in a compatible way if at most On 2)addi­tional vertices (so-called Steiner points) are 
allowed. The general scheme [2] is to .rst triangulate each polygon independently. Then, both polygons 
are mapped to a regular n-gon so that corresponding boundary vertices coincide. The compatible triangulation 
is estab­lished by overlaying the two edge sets in the convex n-gon. The resulting new interior vertices 
are then mapped back into the orig­inal polygons, yielding compatible triangulations of the source and 
target shapes. We would like to stress that the quality of the blend, in terms of the quality of the 
in-between shapes, strongly depends on the shape of the simplices. In particular, skinny triangles (or 
tetrahedra in 3D) cause numerical problems. Thus, in the following, we describe how this schemecanbe 
enhancedto yield compatibletriangulations with a signi.cantly better triangle shape. First, we apply 
Delaunay triangulations (see any textbook on Computational Geometry, e.g. [4]) as the initial triangulation 
since Figure 7: A comparison of compatible triangulations. The upper row shows triangulations generated 
from using ear-capping for the initial triangulation step. Initial triangulations are overlaid on a con­vex 
domain to produce compatible triangulations. The triangula­tions in the lower row were generated with 
the same general proce­dure, but using initial Delaunay triangulations. Far fewer triangles are induced, 
since Delaunay triangulations yield similar partitioning for similar regions. Delaunay triangulations 
maximize the minimum interior angle and, thus, avoid skinny triangles. Of course, any skinny triangle 
in the independent triangulations is inherited by the merged triangulation. Moreover, Delaunay triangulations 
are unique, and similar regions in the shapes will result in similar triangulations. Thus, skinny tri­angles 
resulting from the overlay process can be avoided. Nevertheless, the merged triangulations still have 
skinny trian­gles, and further enhancement is required to avoid numerical prob­lems. We optimize the 
triangulations by further maximizing the minimum interior angle, which is known to be a reasonable trian­gulation 
quality criterion (see e.g. [4]). We use two independent operations: 1. Moving interior vertices. Freitag 
et al. [11] show how to .nd vertex positions which maximize the minimum angle for a given triangulation. 
 2. Flipping interior edges simultaneously in both triangulations. This procedure follows the edge .ip 
criteria used in Delaunay triangulation. Given that an edge .ip is legal in both triangu­lations, it 
is performed if the operation increases the overall minimum angle.  The above two operations are applied 
in turn until no valid .ips are necessary. Convergence is assured since each step can only in­crease 
the minimum angle. We call this procedure compatible mesh smoothing. The smoothing step optimizes the 
compatible triangu­lations without changing the vertex count. However, we also consider changing the 
vertex count by means of splitting edges. The split operation is well-de.ned in terms of topology, if 
it is applied to both triangulations simultaneously, the isomorphy remains. The idea is to split long 
edges to avoid long skinny triangles. Splitting edges according to their lengths does not guarantee an 
increase in triangle quality. In practice, smaller trian­gles are more likely to be improved by the smoothing 
step. After each edge split, the triangulations are smoothed. This avoids the Figure 8: The mesh re.nement 
process. In the .rst row, the merged Delaunay triangulations from Figure 7 are re.ned by edge splits 
un­til all edge lengths are bounded. The second row shows the result of compatible mesh smoothing on 
this triangulation. The third row shows the actual technique, where splitting and smoothing is per­formed 
concurrently. Note that the edge length bound is the same in the .rst and third row. generationofedgesin 
regionswherethesmoothingoperationwould produce nicely-shaped triangles. Figure 8 illustrates the results 
of splitting edges, as well as of the smoothing process. 3.2 Polyhedra To the best of our knowledge, 
the three-dimensional analog to com­patible triangulations has not been discussed in the literature. 
Work has been done to dissect polyhedra into simplicial complexes, a process referred to as tetrahedralization. 
However, the work of Aronov et al. [2] can be extended to genus 0 polyhedra. First, the source and target 
polyhedra are tetrahedralized independently us­ing common techniques, e.g. Barry Joe s Geompack [17]. 
Then, thetetrahedralizationsaremappedto acorrespondingconvexshape. Alexa [1] as well as Shapiro and Tal 
[26] describe methods to map an arbitrary genus 0 polyhedron to a convex shape. Since the source and 
target polyhedra are assumed to have the same vertex-edge topology and the convexi.cation process is 
deterministic, the poly­hedra are mapped to the same convex shape. The interior vertices of their tetrahedralizations 
are mapped using barycentric coordinates. The fact that vertices are mapped to a convex shape using barycen­tric 
coordinatesforinteriorverticesassuresthatnotetrahedrawill be .ipped. Then, an overlay of the two tetrahedralizations 
is computed, where faces are cut against faces, resulting in new edges. Note that the intersection of 
two tetrahedra results in four-, .ve-, or six-sided convexshapes,whichareeasytotetrahedralize. Theresultingstruc­ture 
is mapped back into original polyhedra. In case the source and target shapes are not genus 0, they have 
to be cut into genus 0 pieces which are independently treated as explained above.  4 Results and Conclusion 
We have applied the techniques explained above to various inputs. The two-dimensional shapes are generated 
by extracting a contour out of an image. For the correspondence of contours, we de.ned manually several 
vertex-to-vertex correspondences, while the re­Figure 9: The contour of a maple leaf blended with a plane 
using as-rigid-as-possible shape interpolation. Note that the features of the plane grow out of contour 
according to the current direction of wings and not their .nal position.  maining vertices were automatically 
aligned. The resulting poly­gons were dissected as described above. In Figures 10, 11, 12, and 15, the 
triangulations were used to map a texture to the shape (as was suggested by Tal and Elber in [28]). Textures 
were extracted with the contours from the source images. More elaborate techniques for space-time control 
(e.g. [21]) could be easily integrated in our work to give the user more control as to what is transformed 
and when. Also note that the techniques are not restricted to simple polygons. Since our technique interpolates 
shapes naturally in the sense that it preserves parts that just change relative position or orienta­tion, 
itcouldbealsousedtoextrapolatebeyondthesourceandtarget shapes. Figure 13 demonstrates this with the example 
of Leonardo DaVinci s studies on proportions (see Figure 15 for the interpola­tion). We can generate 
shapes for time values -0.5 and 1.5 while preserving the proportions of the human .gure. We have also 
applied the interpolation technique to three­dimensional models. The examples in Figure 14 were generated 
by using deformed versions of a polyhedral model. Note the differ­ence between linear vertex interpolation 
(upper row) and as-rigid­as-possible interpolation (lower row). In Figure 16, morphable polyhedral models 
were generated using topological merging. As in the two-dimensional case, the vertex paths result from 
de.ning transformations for each pair of corresponding tetrahedra by factor­ing the af.ne transform into 
rotational and stretching components and, then, minimizing the deviation from these ideal transforma­tions. 
The current implementation seems to be robust and fast. The most time-consuming step is optimizing triangle 
shape. Without op­timizing triangle shape numerical problems are likely to occur. In allourexamplesnosimplexchangedorientation(i.e. 
.ipped),how­ever, we have not been able to prove this to be a property of our ap­proach. The examples 
clearly demonstrate the superior quality of our ap­proach compared to plain linear vertex interpolation. 
Additionally, it offers the possibility to texture the shapes, so that shape blending becomes applicable 
to images. In turn, traditional image morphing techniques could serve to generate the homeomorphic dissections 
of the shapes and, thus, make use of more advanced vertex/pixel inter­polation technique(s). However, 
the quality of a morph lies in the eye of the beholder. Nevertheless, there is a clear consensus that 
­lacking other information -the geometry along the morph sequence should change monotonically with no 
super.uous distortions. The idea of as-rigid-as-possible shape interpolation is to avoid distor­tions 
as much as possible and let angles and scales change linearly. Webelievethatthis capturesthenotionoftheabove-mentionedcon­sensus. 
Despite this, shape blending is always an aesthetic problem and no automatic method will meet the needs 
that arise in different ap­plications. Consequently, user interaction will always be necessary to produce 
the desired results. Nevertheless, we believe that more elaborate methods for shape blending simplify 
and minimize the in­volvement of the designer. Finally, we want to mention that dissections of shapes 
seem to extend the concept of skeletons while fully capturing their informa­tion. Dissections are more 
powerful in representing the mechanics Figure 11: Contour blend of a penguin and a dolphin using only 
the texture of the penguin.  of shapes as they allow .ne grained analysis of local behaviour. In many 
cases, shapes naturally have no skeleton or their metamor­phoses could not be described in terms of a 
skeleton. These bene.ts come along with easier and less ambiguous computation of dissec­tions as compared 
to skeletons.  Acknowledgements We thank Kai Kreuzer for helping with the implementation and many examples. 
Craig Gotsman, George Wolberg, Thomas Seder­berg, Herbert Edelsbrunner, and the anonymous reviewers provided 
usefuladviceandsuggestions. Thanksto Wolfgang Mu¨ller andJose Encarnac¸ao for their support. Work on 
this paper has been partially supported by the Hermann Minkowski Minerva Center for Geom­etry at Tel 
Aviv University and by The Israel Academy of Sciences.  References [1] M. Alexa. Merging Polyhedral 
Shapes with Scattered Fea­tures. The Visual Computer, 16, 1, 2000 [2] B. Aronov, R. Seidel, and D. Souvaine. 
On compatible trian­gulations of simple polygons. Computational Geometry: The­ory and Applications 3, 
pp. 27-35, 1993 [3] T. Beier and S. Neely. Feature-based Image Metamorphosis. SIGGRAPH 92 Proceedings, 
pp. 35-42, 1992 [4] M. de Berg, M. van Krefeld, M. Overmars, and O. Schwarzkopf. Computational Geometry 
-Algorithms and Applications. Springer, Berlin, 1997 [5] B. Chazelle. Triangulating a simple polygon 
in linear time. Proc/ 31st Symp. on Foundations of Computer Science (FOCS), pp. 220-230, 1990 [6] E. 
Carmel, D. Cohen-Or. Warp-guided Object Space Morph­ing. The Visual Computer, 13, 1997 [7] S. Cohen, 
G. Elber, R. Bar Yehuda. Matching of freeform curves. CAD, 19, 5, pp. 369-378, 1997  [8] D. Cohen-Or, 
D. Levin, and A. Solomovici. Three dimen­sional distance .eld metamorphosis. ACM Transactions on Graphics, 
1998 [9] M. Etzion and A. Rappoprt. On Compatible Star Decomposi­tions of Simple Polygons. IEEE Transactions 
on Visualization and Computer Graphics, 3, 1, pp. 87-95, 1997 [10] M. S. Floater and C. Gotsman. How 
to Morph Tilings Injec­tively. J. Comp. Appl. Math., 101, pp. 117-129, 1999 [11] L.A. Freitag, M.T. Jones, 
and P.E. Plassmann. An ef.cient par­allel algorithm for mesh smoothing. 4th Int. Meshing Round­table, 
pp. 47-58, 1995 [12] G.H. Golub and C.F. van Loan. Matrix Computations. The Johns Hopkins University 
Press, Baltimore, 1983 [13] E. Goldstein and C. Gotsman. Polygon Morphing using a Mul­tiresolution Representation. 
Graphics Interface 95, pp. 247­254, 1995 [14] A. Greogory, A. State, M. Lin, D. Manocha, and M. Liv­ingston. 
Feature-based surface decomposition for correspon­dence and morphing between polyhedra.Proceedings of 
Com­puter Animation 98, pp. 64-71, 1998 [15] T.He,S.Wang,andA.Kaufman.Wavelet-basedVolumeMor­phing. Proceedings 
of Visualization, IEEE Computer Society, pp. 85-91, 1994 [16] J.F. Hughes. Scheduled Fourier Volume Morphing. 
Computer Graphics (SIGGRAPH 92 Proceedings), 26, 2, pp. 43-46, 1992 [17] B. Joe. Geompack. ftp://ftp.cs.ualberta.ca/pub/geompack 
 Figure 14: A simple example of three-dimensional objects. The difference of linear and as-rigid-as-possible 
vertex interpolation is demonstrated on a bent cigar-like shape. [18] T. Kanai, H Suzuki, and F. Kimura. 
3D geometric meta­morphosis based on harmonic maps. Proceedings of Paci.c Graphics 97, pp. 97-104, 1997 
[19] J.R. Kent, W.E. Carlson, and R.E. Parent. Shape Transforma­tion for polyhedralobjects.ComputerGraphics,26,pp.47-54, 
1992 [20] A.W.F. Lee, D. Dobkin, W. Sweldens, and P. Schro¨der. Mul­tiresolution Mesh Morphing. SIGGRAPH 
99 Proceedings, pp. 343-350, 1999 [21] S.Y. Lee, K.Y. Chwa, S.Y. Shin, and G. Wolberg. Image Meta­morphosis 
Using Snakes and Free-Form Deformations. SIG-GRAPH 95 Proceedings, pp. 439-448, 1995 [22] A. Lerios, 
C.D. Gar.nkle, and M. Levoy. Feature-Based Vol­ume Metamorphis. SIGGRAPH 95 Proceedings, pp. 449­456, 
1995 [23] T.W. Sederberg and E. Greenwood. A physically based ap­proach to 2D shape blending. Computer 
Graphics, 26, pp. 25­34, 1992 [24] T.W. Sederberg, P. Gao, G. Wang, and H. Mu. 2-D shape blending: An 
intrinsic solution to the vertex-path problem. Computer Graphics, 27, pp. 15-18, 1993 [25] M. Shapira 
and A. Rappoport. Shape blending using the star­skeleton representation. IEEE CG&#38;A, 15, pp. 44-51, 
1993 [26] A. Shapiro and A. Tal. Polyhedron realization for shape trans­formation. The Visual Computer, 
14, 8/9, 1998 [27] K. Shoemake and T. Duff. Matrix Animation and Polar De­composition. Proceedings of 
Graphics Interface 92, pp. 258­264, 1992 [28] A. Tal and G. Elber. Image Morphing with Feature Preserving 
Texture. Computer Graphics Forum (Eurographics 99 Pro­ceedings), 18, 3, pp. 339-348, 1999 [29] G. Wolberg. 
Digital Image Morphing. IEEE Computer Society Press, 1990 [30] G. Wolberg. Image Morphing Survey. The 
Visual Computer, 14, 8/9, 1998 [31] Y. Zhang. A Fuzzy Approach to Digital Image Warping. IEEE Computer 
Graphics and Applications, pp. 33-41, 1996   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344862</article_id>
		<sort_key>165</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[Pose space deformation]]></title>
		<subtitle><![CDATA[a unified approach to shape interpolation and skeleton-driven deformation]]></subtitle>
		<page_from>165</page_from>
		<page_to>172</page_to>
		<doi_number>10.1145/344779.344862</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344862</url>
		<abstract>
			<par><![CDATA[<p>Pose space deformation generalizes and improves upon both shape interpolation and common skeleton-driven deformation techniques. This deformation approach proceeds from the observation that several types of deformation can be uniformly represented as mappings from a <italic>pose space</italic>, defined by either an underlying skeleton or a more abstract system of parameters, to displacements in the object local coordinate frames. Once this uniform representation is identified, previously disparate deformation types can be accomplished within a single unified approach. The advantages of this algorithm include improved expressive power and direct manipulation of the desired shapes yet the performance associated with traditional shape interpolation is achievable. Appropriate applications include animation of facial and body deformation for entertainment, telepresence, computer gaming, and other applications where direct sculpting of deformations is desired or where real-time synthesis of a deforming model is required.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[animation]]></kw>
			<kw><![CDATA[applications]]></kw>
			<kw><![CDATA[deformation]]></kw>
			<kw><![CDATA[facial animation]]></kw>
			<kw><![CDATA[morphing]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.1</cat_node>
				<descriptor>Interpolation formulas</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003722</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Interpolation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP77037796</person_id>
				<author_profile_id><![CDATA[81409595942]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Lewis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Centropolis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P194312</person_id>
				<author_profile_id><![CDATA[81414609502]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Matt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cordner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Centropolis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31078175</person_id>
				<author_profile_id><![CDATA[81538225356]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Nickson]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Centropolis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[R. Barnhill, R. Dube, and F. Little, Properties of Shepard's Surfaces. Rocky Mountain J. Math., vol.13, 1983, pp. 365-382.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134003</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[T. Beier and S. Neely, Feature-Based Image Metamorphosis. Computer Graphics vol. 26, no. 2 (Proc. SIGGRAPH 92), pp 35-42.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[R Bergeron and R Lachapelle, Controlling Facial Expression and Body Movements in the Computer Generated Short 'Tony de Peltrie'. SIGGRAPtt 85 Tutorial Notes, ACM, 1985.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>235248</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[C. Bishop, Neural Networks for Pattern Recognition, Oxford: Clarendon, 1995.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>30394</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[A. Blake and A. Zisserman, VisuaIReconstruction, MIT Press, Cambridge, 1988.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360357</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[N. Burtnyk and M. Wein, Interactive Skeleton Techniques for Enhancing Motion Dynamics in Key Frame Animation, Comm. ACM, vol. 19, no. 10 (October 1976), pp. 564-569.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74358</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[J. Chadwick, D. Haumann, and R. Parent, Layered Construction for Deformable Animated Characters. Computer Graphics vol. 23 no. 3 (Proc. SIGGRAPH 89), pp. 243-252.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134016</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[D.T. Chen and D. Zeltzer, Pump It Up: Computer Animation Based Model of Muscle Using the Finite Element Method. Computer Graphics vol. 26, (Proc. SIGGRAPH 92), pp. 89-98.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Cinefex, Riverside, CA, various issues, e.g. vol. 66 (June 1996), p. 52 (Dragonheart); vol. 64 (Dec 1995), p. 62 (Jumanji).]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280826</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[T. DeRose, M. Kass, and T. Truong, Subdivision Surfaces in Character Animation. Proc. SIGGRAPH 98, ACM, pp. 85-94.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[D. Forsey, A Surface Model for Skeleton-Based Character Animation. In Second Eurographics Workshop on Animation and Simulation, Vienna, Austria, 1991, pp. 55-73.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134036</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[W.M. Hsu, J. F. Hughes, and H. Kaufman, Direct Manipulation of Free-Form Deformations, Computer Graphics vol. 26 (Proc. SIGGRAPH 92), pp. 177-184.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[K. Komatsu, "Human Skin Model Capable of Natural Shape Variation," The Visual Computer, vol. 4, no. 3, 1988, pp. 265-271.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[J. R Lewis, Creation by Refinement: A Creativity Paradigm for Gradient Descent Learning Networks. International Conf. on Neural Networks'. San Diego, 1988, Ih 229-233.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[J.P. Lewis, Probing the Critic: Approaches to Connectionist Pattern Synthesis. IEEE International Joint Conference on Neural Networks' Seattle, July 1991.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192270</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[R Litwinowicz and L. Williams, Animating Images with Drawings, P~vc. SIG- GRAPH 94, pp. 409-412.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237247</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[R. MacCracken and K. Joy, Free-Form Deformations with Lattices of Arbitrary Topology. P1vc. SIGGRAPH 96, pp. 181-180.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>519831</ref_obj_id>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[G. Maestri, Digital Character Animation 2, Vol 1. New Rider, Indianapolis, 1999. ISBN 1056205-930-0.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[L. Nedel and D. Thalmann, Modeling and Deformation of the Human Body Using an Anatomically-Based Approach, www preprint.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[C.W.A.M. van Overveld, A Technique for Motion Specification in Computer Animation, Visual Computer, vol. 6 (1990), p. 106-116.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>889041</ref_obj_id>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[T. Poggio and R. Brunelli. A novel approach to graphics. AI Memo 1354, C.B.hP Paper 71, MIT, 1992.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280946</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[K. Singh and E. Fiume, Wires: A Geometric Deformation Technique. P~vc. SIG- GRAPH 98, ACM, pp. 405-414.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>102317</ref_obj_id>
				<ref_obj_pid>102313</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[N. Magnenat-Thalmann, R. Laperriere, and D. Thalmann. Joint-Dependent Local Deformations for Hand Animation and Object Grasping. P~vc. Graphics Intelface, 1988, pp. 26-33.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>111168</ref_obj_id>
				<ref_obj_pid>111154</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[N. Magnenat-Thalmann and D. Thalmann. Human Body Deformations using Joint-Dependent Local Operators and Finite Element Theory. In N. Badler, B. Barsky, and D. Zeltzer, eds., Making Them Move: Mechanics, Cont~vl, and Animation of Articulated Figures San Mateo, CA: Morgan Kaufmann, 1991, pp. 243-262.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Maya Alias/Wavefront, Santa Barbara, CA, 1998.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[F. h Parke, Parameterized Models for Facial Animation. IEEE Computer Graphics and Applications, vol. 2, no. 9, November 1982, pp. 61-68.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[S. Pieper, Physically-Based Animation of Facial Tissue for Surgical Simulation, SIGGRAPH 89 Tutorial Notes: State of the Art in FaciaI Animation, ACM, 1989.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[J.D. Powell, The Theory of Radial Basis Function Approximation. Cambridge University Numerical Analysis Report, 1990.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>48433</ref_obj_id>
				<ref_obj_pid>48424</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[M.J.D. Powell, Radial Basis Functions for Multivariable Interpolation: A Review. In J. Mason and M. Cox, Eds., Algorithms for Approximation, Oxford: Clarendon, pp. 143-167.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[J.A. Russel, A Circomplex Model of Affect. J. Personality and Social Psychology, vol. 39, p. 1161-1178, 1980.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258827</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[F. Scheepers, R. Parent, W. Carlson, and S. May, Anatomy-Based Modeling of the Human Musculature. P~vc. SIGGRAPH 97, ACM, pp. 163-172.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15903</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[T. Sederberg and S. Parry, Free Form Deformations of Solid Geometric Models. Computer Graphics, vol. 20 no. 4, (Proc. SIGGRAPH 86), pp. 150-161.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[B. Shneiderman, The Future of Interactive Systems and the Emergence of Direct Manipulation. Behaviour and Information Technology, 1, pp. 237-356.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Pat Taylor, Disney/Dream Quest Mighty Joe Young facial animation, personal communication.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[R. Turner and D. Thalmann, The Elastic Surface Layer Model for Animated Character Construction. in N. M. Thalmann and D. Thalmann, eds., P~vc. Computer Graphis International, New York: Springer Verlag, 1993, pp. 399-412.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258833</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[J. Wilhelms and A. Van Gelder, Anatomically Based Modeling. P~vc. SIG- GRAPH 97, pp. 173-180.]]></ref_text>
				<ref_id>36</ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[G. Wyvill, C. McPheeters, and B. Wyvill, Animating Soft Objects. Visual Compute~, 2, 235-242, 1986.]]></ref_text>
				<ref_id>37</ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Xinmin Zhao, Disney Dinosaur project, personal communication.]]></ref_text>
				<ref_id>38</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Pose Space Deformation: A Uni.ed Approach to Shape Interpolation and Skeleton-Driven Deformation J. 
P. Lewis:, Matt Cordner, Nickson Fong Centropolis Abstract Pose space deformation generalizes and improves 
upon both shape interpolation and common skeleton-driven deformation techniques. This deformation approach 
proceeds from the observation that sev­eral types of deformation can be uniformly represented as mappings 
from a pose space, de.ned by either an underlying skeleton or a more abstract system of parameters, to 
displacements in the ob­ject local coordinate frames. Once this uniform representation is identi.ed, 
previously disparate deformation types can be accom­plished within a single uni.ed approach. The advantages 
of this algorithm include improved expressive power and direct manipula­tion of the desired shapes yet 
the performance associated with tradi­tional shape interpolation is achievable. Appropriate applications 
include animation of facial and body deformation for entertainment, telepresence, computer gaming, and 
other applications where direct sculpting of deformations is desired or where real-time synthesis of 
a deforming model is required. CR Categories: I.3.5 [Computer Graphics]: Computational Geometry and Object 
Modeling Curve, surface, solid and ob­ject modeling I.3.6 [Computer Graphics]: Methodology and Techniques 
Interaction techniques I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism Animation Keywords: 
Animation, Deformation, Facial Animation, Morph­ing, Applications. 1 Introduction Free form deformation 
has been approached from several distinct perspectives. As an abstract and general problem, good methods 
have been obtained both using the well known technique that bears this name [32, 12, 17] and other kinematic 
surface deformation techniques, and with physical models that simulate the time evo­lution of a membrane 
or solid. The animation of human and creature skin deformation is ar­guably the most common and important 
application of free form de­formation in computer graphics. While such creature animation can be considered 
a special case of general free form deformation, its importance and dif.culty have lead researchers to 
propose a number of domain-speci.c algorithms that will be reviewed in Section 2. The problem of realistic 
facial animation is being actively and successfully addressed by image-based and hybrid techniques. These 
techniques are not yet suitable for all applications, however: *zilla@computer.org Permission to make 
digital or hard copies of part or all of this work or personal or classroom use is granted without fee 
provided that copies are not made or distributed for profit or commercial advantage and that copies bear 
this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. SIGGRAPH 2000, New Orleans, 
LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 while a purely image-based approach can achieve 
very realistic im­ages, this advantage may be lost if one needs to introduce geome­try and surface re.ectance 
in order to re-light characters to match preexisting or dynamically computed environments. Film and en­tertainment 
applications require fanciful creatures that fall outside the scope of image-based approaches. Some of 
the most impressive examples of geometry-based (as opposed to image-based) human and creature animation 
have been obtained in the entertainment industry. These efforts traditionally use shape interpolation 
for facial animation and a standard but variously-named algorithm that we will term skeleton subspace 
de­formation (SSD) for basic body deformation [25, 9]. While shape interpolation is well-liked by production 
animators, it is not suitable for skeleton-driven deformation. On the other hand SSD produces characteristic 
defects and is notoriously dif.cult to control. These issues, which will be detailed in the next section, 
lead us to look for a more general approach to surface deformation. We consider the following to be desirable 
characteristics of a skeleton­based surface deformation algorithm: The algorithm should handle the general 
problem of skeleton­in.uenced deformation rather than treating each area of anatomy as a special case. 
New creature topologies should be accommodated without programming or considerable setup efforts.  It 
should be possible to specify arbitrary desired deformations at arbitrary points in the parameter space, 
with smooth inter­polation of the deformation between these points.  The system should allow direct 
manipulation of the desired deformations [33].  The locality of deformation should be controllable, 
both spa­tially and in the skeleton s con.guration space (pose space).  In addition, we target a conventional 
animator-controlled work process rather than an approach based on automatic sim­ulation. As such we require 
that animators be able to visual­ize the interaction of a reasonably high-resolution model with an environment 
in real time (with high resolution de.ned in accord with current expectations). Real time synthesis is 
also required for applications such as avatars and computer games.  Our solution, termed pose space 
deformation, provides a uni­form and expressive approach to both facial skin deformation and skeleton-driven 
deformation. It addresses the previously mentioned drawbacks of shape interpolation and SSD while retaining 
the sim­plicity and performance associated with these techniques. The next section reviews various approaches 
to free form de­formation and describes shape interpolation and skeleton subspace deformation algorithms. 
The pose space deformation algorithm re­quires well behaved and ef.cient scattered data interpolation 
in high dimensional spaces; Section 3 considers this issue. The pose-space deformation algorithm itself 
is described in Section 4; examples and applications are shown in the last section.  2 Background Recent 
research has delivered signi.cant improvements in many ar­eas of character animation, including surface 
representation, model capture, performance capture, and hybrid (partially image-based) rendering approaches. 
In this literature review we focus speci.cally on milestones in the surface deformation models and necessarily 
omit other important contributions. 2.1 Surface Deformation Models Continuous deformation of a character 
skin was .rst addressed in Parke s pioneering facial animation work [26]. In this work, control vertices 
were deformed by custom algorithmic implementation of carefully selected high-level parameters ( raise-upper-lip 
, etc.). Komatsu [13] and Magnenat-Thalmann et. al. [23] demonstrated human body deformation driven by 
an underlying skeleton. The region and shape of deformation is algorithmically de.ned in each of these 
approaches. Magnenat-Thalmann et. al. developed algo­rithms for each of the various joints in the hand. 
The discussion in Komatsu focuses on the elbow and shows how the skin crease on the acute side can be 
obtained by a suitable algorithmic manipula­tion of the surface control vertices. The algorithms in this 
early work do not suffer the collapsing elbow characteristic of the SSD algorithm (below). On the other 
hand, the algorithms are speci.c to particular types of joints and are perhaps too simple to portray 
the complexity and individual variability of real anatomy. The short .lm Tony de Peltrie [3] popularized 
the use of shape in­terpolation for facial animation. Forsey [11] describes a character­oriented deformation 
scheme in which the bending of a smooth surface can be controlled by anchoring levels of a multi-resolution 
spline surface to the underlying skeleton. These efforts are distin­guished from the previous purely 
algorithmic approaches in giving the modeler control of and responsibility for the deformation. The speci.cation 
and animation of surface deformation remains an active area of investigation [17, 10]. The Wires technique 
[22] is one interesting recent contribution; this approach is notable in providing a direct manipulation 
interface in a form immediately fa­miliar to sculptors (armatures). 2.2 Multi-Layered and Physically 
Inspired Models Chadwick, Haumann, and Parent [7] introduced a multi-layered and physically inspired 
approach to skin deformation. In their model a free-form deformation abstractly represents underlying 
body tis­sues and mediates skin movement. Chadwick et. al. demonstrated expressive three-dimensional 
cartoon characters but deformation of a realistic character was not shown. Other researchers have investigated 
modeling the underlying body tissues in greater depth [27, 24, 8, 35]. Most recently, sev­eral groups 
have undertaken ambitious efforts to produce anatom­ically inspired multi-layered models of animals and 
humans with considerable verisimilitude. Nedel and Thalmann [19] simulate the surface deformation of 
muscles using spring mesh dynamics; a modeled skin cross section is reshaped by a ray-casting procedure 
that .nds the maximum displacement of the underlying tissue. Sev­eral papers by Wilhelms and coworkers 
have shown anatomically representative human and animal models. In Wilhelms and Van Gelder [36] several 
classes of muscles are algorithmically modeled with attention to volume conservation; skin is a spring 
mesh an­chored to underlying tissue or bone in appropriate areas. Scheepers et. al. [31] produced convincing 
representations of muscles as well as preliminary but promising skin deformation.  2.3 Common Practice 
In recent years character animation has moved beyond being a re­search topic and sophisticated deforming 
characters routinely ap­pear in .lms and on television. Various techniques are employed, q q q" Figure 
1: The skeleton subspace deformation algorithm. The deformed position of a I point p lies on the line 
ppII de.ned by the images of that point rigidly transformed by the neighboring skeletal coordinate frames, 
resulting in the characteristic collapsing elbow problem (solid line). including manually animated FFDs 
and custom procedural ap­proaches in the spirit of [26, 23, 13]. Arguably the most com­mon practice in 
character animation (as re.ected in commercial software, animation books and courses, and some custom 
software) is founded on the twin techniques of shape interpolation and SSD [18, 9]. 2.3.1 Shape Interpolation 
Shape interpolation (also called shape blending and multi-target morphing) is probably the most widely 
used approach to skin de­formation for facial animation [3, 18, 9]. Surface control vertices are simply 
an animated linear combination (not necessarily con­vex, i.e., individual weights can be greater than 
one or less than zero) of the corresponding vertices on a number of key shapes Sk: k=0 wkSk. A variation 
of this technique uses a single base shape S0 and a number of delta shapes, S0 + wk(Sk - S0).By .k=1 
.writing the delta shape form as (1 - 1 wk)S0 + 1 wkSk it is clear that the space of achievable shapes 
is identical in both varia­tions.1 An attractive feature of shape interpolation is that the desired expressions 
can be directly speci.ed by sculpting. The limitations of shape interpolation. Given the popularity and 
effectiveness of this simple approach, it would be desirable to em­ploy it on regions of the body other 
than the face. The blending of rigid shapes is inconsistent with regions of the body that are bend­ing 
under the action of an underlying skeleton, however. Of course the key shapes could be deformed to the 
moving articulated .gure using some other algorithm, but this defeats the purpose of propos­ing shape 
interpolation as the means of obtaining the deformation in question. Shape interpolation also has some 
drawbacks for its intended role of facial animation. For one, the interpolation is not always smooth. 
Consider interpolating from a smile (shape A) to a neutral pose (B) and then to a frown (C). An individual 
vertex travels in a straight line between A and B and again in a line between B and C. Selecting smoothly 
changing weights with dw/dt =0 at the key shapes merely causes the deformation to ease in and stop at 
each key pose before continuing on the time derivative of control point motion is smooth, but the motion 
path itself is only piecewise linear (parametric versus geometric continuity). In practice ani­mators 
object to the linear nature of the interpolation [34] and have sometimes compensated by sculpting new 
key shapes as often as every three to .ve frames [38]. These comments will be revisited in the discussion 
of the pose space approach later in the paper. 1Provided that the weights sum to one. This is enforced 
in the delta shape formulation. It is not enforced in the (non-delta) shape interpolation formulation 
as written, but weights that do not sum to one are a separate effect they cause the face to change overall 
scale.  k p Figure 2: The collapsing elbow in action, c.f. Figure 1.  2.3.2 Skeleton-Subspace Deformation 
This simple algorithm has been repeatedly conceived and appears in commercial software packages under 
several rather uninformative names such as skinning, enveloping, etc. The algorithm is unpub­lished but 
is subsumed by more general published schemes such as [23]. The position of a control vertex p on the 
deforming sur­face of an articulated object lies in the subspace de.ned by the rigid transformations 
of that point by some number of relevant skeletal coordinate frames (Figure 1). This may be notated p¯= 
wkLk(p) p (in more detail) kL 0 -1 0 p p¯= wkLL Figure 3: The forearm in the twist pose, as in turning 
a door handle, computed by SSD. As the twist approaches 1800 the arm collapses. A second dif.culty with 
SSD is that, unlike shape interpolation, it does not permit direct manipulation; artists instead directly 
or indirectly edit the meshes of weights wk (for each control vertex on a surface there is one weight 
per skeletal frame that affects the vertex). SSD algorithms consequently have the reputation for being 
tedious and dif.cult to control. Artists with a poor understanding of the underlying algorithm have dif.culty 
distinguishing between results that can be further improved by adjusting weights and results that cannot 
be improved since the desired result lies outside the achievable subspace, resulting in the impression 
of unpredictability ( sometimes adjusting the weights helps, sometimes it doesn t ). In some cases the 
SSD defects can be manually corrected us­ing FFDs and other techniques, and one could consider a scheme 
whereby these .xes are procedurally invoked as the skeleton articu­ lates. But although FFDs work well 
(and have a direct manipulation algorithm [12]) the layered FFDs do not reduce the dif.culty in ad­justing 
the underlying SSD. The algorithm introduced in the subse­ 0 where L is the transform from the surface 
containing p to the p 0 k world coordinate system, Lis the transform from the stationary quent sections 
removes the need for such layered .x-it approaches -1 and permits direct speci.cation of the desired 
deformations. 00 skeletal frame k to the world system (Ltogether represent p L k p in the coordinate 
system of skeletal frame k), and L k expresses the moving skeletal frame k in the world system. The deformation 
is controlled by the user through the weights wk. SSD is fairly versatile. For example, secondary animation 
effects such as muscle bulging and swelling of the chest can be achieved by variably weighting the surface 
to an abstract bone whose transla­tion or scale is manually animated. The limitations of SSD. The .rst 
major shortcoming of SSD re­sults directly from the fact that the deformation is restricted to the indicated 
subspace. In common situations such as shoulders and elbows the desired deformation does not lie in this 
subspace, hence no amount of adjusting the algorithm weights will produce good re­sults. This fact leads 
to considerable frustration by users of the algo­rithm the character of the deformation changes as the 
weights are changed, sometimes sustaining the incorrect assumption that some combination of weights will 
produce good results. In fact, the SSD algorithm can be easily identi.ed in animations by its characteristic 
collapsing joint defect (Figures 1, 2). This problem is extreme in the case of simulating the twist of 
a human forearm (the pose taken in turning a door handle, Fig­ure 3). In this case the subspace basis 
consists of surface points rigidly transformed by the forearm frame (no axis rotation) and the wrist 
frame (axis rotation). With a rotation of 180 degrees this line crosses the axis of the arm, i.e., the 
forearm collapses entirely as the SSD weights transition at some point from the forearm to wrist frames. 
 2.3.3 Uni.ed Approaches Several published algorithms and commercial packages combine aspects of skeleton-driven 
deformation and shape interpolation in ways that anticipate our approach. In the pioneering work of Burt­nyk 
and Wein, two dimensional characters were animated using a polygonal rubber sheet that afforded both 
skeletal and local defor­mation control [6]. Van Overveld described a two-dimensional an­imation system 
in which animation is controlled by a skeleton and character deformation is driven from this skeleton 
through a scat­tered interpolation [20]. This work is similar in spirit to ours but dif­fers in that 
it used the image plane as a global interpolation domain rather than introducing a pose space. Litwinowicz 
and Williams s system [16] is also a precedent and introduced sophisticated scat­tered interpolation 
(again in the image domain). Several papers consider animation (and indeed image synthesis in general) 
as a special case of neural net learning and interpolation/extrapolation [14, 15, 21]. While this viewpoint 
is valid, in practice it is per­haps excessively general, for example, a skeleton is merely learned rather 
than being an intrinsic part of the model. While employed at Industrial Light and Magic the .rst author 
of the present paper developed a system that attempted to blend shape interpolation and SSD algorithms; 
a small portion of it remains in use in their well known Caricature animation system. Drawbacks of this 
work in­cluded both a complicated dependence on the details of SSD and its overall conception as a correction 
to SSD. Some commercial  Figure 4: Shepard s interpolant operating on a set of colinear points. The 
derivative is zero at the data points, and the curve extrapolates to the average of the data values. 
packages allow blending between two sculpted deformations as a function of a single-joint rotation, thereby 
combining shape inter­polation and skeleton-driven deformation in a limited but useful set­ting.  2.4 
Kinematic or Physical Simulation? The depth of simulation is a prevalent issue in computer graph­ics, 
albeit one that is not always consciously considered. Early approaches to animation were purely kinematic; 
an emphasis on physically based modeling appeared in the literature later. Recent sophisticated approaches 
allow a hybrid of animator-controlled and physically governed animation as needed. In rendering we perhaps 
see the opposite trend much of the literature a decade ago focused on ever deeper simulations of reality, 
whereas shallower image­based approaches are attracting attention at present. Similarly, in character 
deformation both deep and shallow ap­proaches have their place. Deep models promise universally accu­rate 
simulation, and the importance of representing humans justi.es the needed effort. The authors of these 
approaches acknowledge that producing anatomically plausible models is a daunting task, however. Pose 
space deformation is a shallow, purely kinematic approach to deformation (i.e. without reference to underlying 
forces, mass, volume), and it has consequent disadvantages. In particular, accu­racy is reliant on the 
modeler/animator rather than being guaranteed by the simulation. On the other hand, our algorithm has 
clear ad­vantages with respect to simplicity and generality, direct manipula­tion, real-time synthesis, 
and other criteria listed in the introduction.  3 Deformation as Scattered Interpolation In abstract, 
we wish to express the deformation of a surface as a function of either the pose of an underlying skeleton, 
or equiva­lently as a function of some other set of parameters such as the {smile, raise-eyebrow,...} 
controls desirable in facial animation. We also wish to directly sculpt the desired deformation at various 
points in the parameter space, rather than working in a more abstract space such as the coef.cients on 
various coordinate frames as required by the SSD algorithm. A scattered data interpolation method is 
required because defor­mations will be sculpted at arbitrary (rather than regularly spaced) poses. Since 
this interpolation is central to our application (the re­sults of the interpolation will be directly 
visible in the animating deformation), we will consider the available scattered interpolation approaches 
before settling on a candidate. 3.1 Shepard s Method Shepard s method [1, 2] is a frequently employed 
scattered data interpolation scheme in computer graphics. In this method the in­terpolated value is a 
weighted sum of the surrounding data points 2 Figure 5: Radial basis functions ¢(x)=exp(-x/2(2),(=10interpolating 
the same set of colinear points as in Figure 4. A different y scale is used to .t the curve. The curve 
extrapolates to zero. normalized by the sum of the weights, wk(x)dk d(x)= . wk(x) with weights set to 
an inverse power of the distance: wk(x)=Ix- xkI-p. (This is singular at the data points xk and should 
computed as (||x - xkI +E)-p). With p> 1the interpolation surface is once differentiable. Unfortunately 
this simple scheme has some potentially undesirable properties. Far from the data the weights will be 
approximately the same, d (0)= wo dk/wo 1= dk/N , i.e. the interpolated surface converges to the average 
of the data values. A serious drawback for some applications is that the derivative of the surface is 
zero at the data points (Figure 4). 3.2 Radial Basis Functions Radial basis functions [28, 29] have 
become a popular choice for scattered interpolation. The interpolant is a linear combination of nonlinear 
functions of distance from the data points: N d (x)= wk4(Ix - xkI) (1) k If N values of d are available 
then the weights can be easily solved by a linear system; this can be derived either by least squares 
.t or by subspace projection. Taking the latter approach, we reconsider the available data points as 
a single point d in an N dimensional space, and consider 4k() = 4(Ixj - xkI)as the kth basis vec­tor. 
The best approximation to d in the space spanned by 4k()oc­curs (in direct analogy with the three-dimensional 
case) when the weights are such that the error d - w (with 4k()comprising the columns of ) is orthogonal 
to each of the 4k(): T ( w - d)=0 so (the so-called normal equation ) T w = T d can be solved for the 
familiar w =( T )-1 T d A least squares approach leads to the identical result. Any nonlinear function 
4() will interpolate the data, includ­ing odd choices such as 4(x)= x (which is nonlinear since x = Ix 
- xkI is the argument), provided that the columns of are independent. On the other hand a smooth 4()will 
result in a smooth interpolant (a weighted sum of continuous functions is con­tinuous). In fact radial 
basis functions have a universal convergence property similar to Fourier series, though the convergence 
de.nition is different. The preceding description maps a k-dimensional input space (ar­bitrary k) to 
a one dimensional range, i.e., it is the k-dimensional version of a height .eld. Surfaces can of course 
be interpolated by allowing different combinations of the same basis functions in different dimensions, 
i.e., vector valued wk. The distance II can be generalized to Mahalanobis distance (effectively rotating 
and stretching the basis function) [4]. 3.3 Energy Functionals and Non-Convex Methods Various visual 
reconstruction schemes can be adapted for scattered data interpolation. In these schemes the interpolated 
or approxi­mated surface is found as the minimum of a functional such as |d (x) - d(x)|2dx + sP (d ) 
where the .rst term penalizes deviation of the surface d from the available data d and the second regularizing 
term votes for surface smoothness e.g. by integrating the squared second derivative of the surface. With 
small s many of these schemes can serve as scat­tered data interpolants; reference [5] is a good introduction 
to these approaches. In some of the most powerful formulations of scattered interpo­lation the regularizer 
is considered to hold everywhere except at an unknown set of edges this is the piecewise-smooth prior 
desirable in image reconstruction. Since the unknown edges may exist (or not exist) at any location in 
the domain, all combinations of possi­ble edge locations must be considered and the interpolation cost 
is prima facie exponential in the surface resolution.  4 Pose Space Deformation The crux of our approach 
is the identi.cation of an appropriate space for de.ning deformations. As discussed above, the inter­polation 
domain is (a subset of) the pose space of an articulated character, or equivalently the space de.ned 
by some set of parame­ters such as facial controls. In concept the range of the interpolation function 
could simply be the desired movement of the surface control vertices. To make the job easier for the 
interpolation we instead interpolate the desired deviation of a surface vertex (expressed in the local 
frame) from its initially computed position (the rigidly transformed position in the case of an articulated 
model). Several reasons for this choice will be mentioned shortly. Thus the deforming surface is de.ned 
by p + d5 with p moved rigidly by the skeleton or other underlying system, and d 5 = finterp(con.guration) 
where con.guration is the con.guration of the set of joints or pa­rameters controlled by the animator. 
Our scheme can be bootstrapped on top of an existing software system: the model is posed as desired and 
the desired surface at that pose is sculpted. Our algorithm computes the difference between the initial 
and resculpted model at that pose. This deformation is associated with the joints or other parameters 
that have moved from their default positions to create the particular pose. One or more deformations 
will then be interpolated in this subspace using a scattered data approach. We now have enough criteria 
to select a particular interpolation scheme. Although it would be desirable to allow deformations to 
change both continuously and discontinuously with respect to the pose space, creature deformations that 
are discontinuous with re­spect to pose seem unlikely. As such the expensive energy func­tional and non-convex 
schemes are not necessary. In addition we want d5 to approach zero away from the data, and the width 
of this falloff should be selectable. 2 -(Ix-xkI) Together these comments support 4k(x)= exp( ) 2-2 as 
one possible choice of radial basis (Figure 5). Gaussian radial basis functions are reputed to be well 
behaved and our experience supports this judgement. Gaussian radial basis functions with ad­justable 
placement and ( are discussed in the neural net literature and optimizing over these parameters is possible. 
This issue does not arise in our application, however, since the animator decides where in the parameter 
space to sculpt a pose (effectively decid­ing the basis function placement). The falloff ( is also speci.ed 
explicitly by the animator, as described below. 4.1 Algorithm Summary The steps in a pose space deformation 
(PSD) algorithm will now be described consecutively. De.nitions. A pose is de.ned as the con.guration 
of any pose controls (joints or abstract manipulators) that have changed from their default values. An 
abstract manipulator is a UI control or ar­bitrary piece of geometry whose movement will control the 
inter­polation of some deformation, such as a muscle bulge or a desired facial attribute such as happiness. 
A self-relative con.guration of the controls is actually considered, for example, an elbow involves two 
skeletal frames but only one joint angle. The pose space is the space spanned by the variations of these 
controls. If n =2 pose controls are active and each has three degrees of freedom then a 3(n - 1) pose 
space is de.ned, and the particular position of the controls de.nes a point in that space. Sculpt. The 
artist .rst positions some set of pose controls and then sculpts a deformation for that pose. The artist 
also assigns a falloff (Gaussian (), either as a symmetric radius across all controls or to each control 
individually (axis stretched falloff). De.ne d5(pose). Any control vertices that have moved from their 
rest position are found. This is done in the local coordinate frame, i.e., rigid body articulated motion 
results in zero d5.The d5 values for the deformed vertices are computed (again in the local coordi­nate 
system) and they are saved in a database together with their corresponding location in a pose space. 
(At the boundary of sev­eral surface patches there may be shared vertices that need to be coincident 
to maintain surface continuity. Unlike some SSD imple­mentations interpolation in pose space by de.nition 
cannot separate such vertices). Solve. When several such deformations have been saved (or when the artist 
is ready to try animating) it is necessary to solve the interpolation problem. For each control vertex 
that was moved during sculpting there are now one or more d5 values at points in the pose space. Note 
that the dimension of the pose space can vary across vertices, for example, a particular vertex might 
be modi.ed in three sculpted deformations but a neighboring vertex might have been modi.ed in only two 
deformations. The interpolation is done independently for each control vertex (but see additional details 
be­low); in our experience using patch surfaces this has not been prob­lematic. Singular T is interpreted 
as a user error; in practice this has turned out to be the result of saving new deformations without 
moving any pose controls rather than a result of actual numerical problems. Synthesis. The model is now 
moved to an arbitrary pose. The location in pose space is determined from the concatenated relative degrees 
of freedom of the pose controls (simply interpreted as in­ dependent dimensions). For each deforming 
control vertex a d5 is interpolated from the delta values at the stored poses using Eq. (1). Evaluate 
and Repeat. At this point the model interpolates through the previously de.ned deformation(s). The most 
recently de.ned deformation may extend too far (or not far enough) in pose space, however. There is a 
rich literature of schemes for optimizing radial basis parameters including ( [4]. On the other hand, 
anima­tors consider detailed control of the animation to be part of their craft and are quite happy to 
have interpolation parameters exposed to them. We have found that this potentially abstract parameter 
is comprehensible so long as it is possible to explore the effect of different values. At a minimum axis-aligned 
scaling of the falloff should be available; we have not experimented with Mahalanobis rotation of the 
basis. Based on the evaluation the artist may decide to sculpt additional poses as needed to achieve 
the desired motion. A detail that was omitted previously will now be mentioned: when a deformed vertex 
is found the associated pose space is de­termined as described above. If there are previous deformations 
of this vertex in the same pose space then the new deformation is sim­ply another point to interpolate. 
The new deformation s pose space may, however, be different from the previous spaces associated with 
the vertex! In such a case a new pose space is started, and the d5 is computed as a delta from the previous 
layered PSD synthesis rather than from the base model. This ensures that the previous deforma­tions are 
interpolated while allowing the artist complete freedom in determining the extent of the deformation 
and the associated pose controls. While there is an issue of commutativity, in our experi­ence artists 
consider this iterative layered re.nement to be a natural process. In the preceeding discussion we have 
not described the represen­tation of rotations and other transformations. This is a well known issue; 
well behaved transformations are fundamental and are hope­fully addressed early in the development of 
any character animation system. 4.2 Cost With n poses three matrices of size n must be inverted for 
each surface control vertex. Typically n will be between 1 and 10, say, so this cost is small. Also it 
is incurred at a convenient time during setup (as a pose is saved) rather than during synthesis. For 
synthesis, the basis function 4(x) can be implemented by interpolated table lookup and the sqrt required 
in the Euclidean distance can be composed with 4(x) in the table. The cost of Eq. (1) is then not much 
greater than the cost of traditional shape interpola­tion, i.e., real time synthesis is possible with 
signi.cant models on current machines.  5 Applications and Discussion 5.1 PSD for Skeleton-Driven Deformation 
An articulated model such as a human will typically have a num­ber of different deformation subspaces, 
each with one or several deformations; the deformations in different subspaces may overlap spatially 
e.g. to simulate the in.uence of different muscles. The deformations needed for an elbow, for example, 
will be interpo­lated in the one-dimensional subspace de.ned by the elbow joint angle. Deformations in 
a shoulder area will need to consider two or more degrees of freedom. The neck/chest/leg blend area of 
many quadrupeds is a more complex case the motion of the skin surface in these regions may depend on 
the relative con.guration of several leg bones as well as the rib cage and possibly the neck region of 
the spine. PSD handles all these cases simply and uniformly. Figures 8 and 9 are a simple comparison 
of PSD and SSD algo­rithms in action on human elbow and shoulder regions. 5.2 PSD for Facial Animation 
The application of PSD to facial animation is best described by comparison with shape interpolation (SI). 
 In both approaches a set of key shapes (or delta shapes) are sculpted. The same set of shapes can be 
used in both ap­proaches.  Whereas shape interpolation is (despite the name) a super­position of a set 
of shapes, PSD interpolates among these shapes.  The animator s task in PSD is to choose the interpolation 
path (and adjust interpolation parameters such as falloff if de­sired). With SI the animator s task is 
to choose the interpola­tion path but also solve the problem of representing this path by using a set 
of (non-orthogonal!) basis shapes. In practice this has been considered the major dif.culty in applying 
SI when high quality animation demands large numbers of basis shapes [38].  In shape interpolation the 
key shapes and the animation pa­rameter space are one and the same the keys de.ne the axes of the animation 
parameter space. In PSD the key shapes are positioned as desired in a space of desired dimensionality. 
 One can assign each shape in PSD to a separate dimension, exactly as with SI. On the other hand, PSD 
allows one to sculpt intermediate expressions (half-smile) and situate them half-way along the relevant 
(full-smile) axis. Similarly a sculpted pose that represents the simultaneous activation of several parameters 
(e.g. happy but surprised, or smiling with awink) can simply be saved at the appropriate location in 
the pose space. Psychological research has shown that human facial expressions are largely described 
by two emotional axes [30] (Figure 6); this two-dimensional space would be a convenient high-level pose 
space for controlling facial anima­tion.  The PSD interpolation is smooth if so desired.  To illustrate 
these comments consider Figure 7, which abstractly represents both SI and PSD with an identical set of 
expressions (neutral, half-smile, full-smile, frown). In the SI side of the dia­gram expressions are 
arranged as independent (but not orthogonal) dimensions as required by SI. In the PSD diagram the expressions 
are situated in an expression space having a happy-unhappy axis; a second axis (arousal) and an expression 
(delighted) on that axis are added to show a multidimensional space. As illustrated, a PSD path from 
neutral to half-smile to full-smile is monotonic, as might be expected; the motion of a surface point 
over this interpolation is also smooth. To interpolate these emotions using SI requires a zig-zag pattern 
of weights: the half-smile weight goes from zero to one, and then back to zero as the full-smile weight 
goes from zero to one.The motion of a surface point would also be piecewise linear using SI in this scenario. 
 5.3 PSD for Secondary Animation Switches and dials. Additional dimensions of deformation can be added 
at any time by adding a new parameter and associating additional poses with the movement of this parameter. 
For example, a limb can be modeled in a particular pose both in an unloaded state and with muscles sculpted 
to express carrying a heavy load. The heavy pose can be associated with the on state of an abstract parameter 
(e.g. an isolated bone moved into the vertical position); light and heavy loads can then be controlled 
by .ipping this switch. Similarly one can imagine setting up a dial that causes the character to morph; 
this would of course require a signi.cant set of additional deformation poses.  6 Conclusions Pose space 
deformation is not the last word in surface deformation for character animation; high quality anatomically 
based models are certainly preferable. Nevertheless both anatomically based and alarmed excited astonished 
angry afraid delighted annoyed frustrated happy pleased content serene, miserable calm depressed relaxed 
bored tired sleepy Figure 6: Schematic diagram of emotion space obtained by multi-dimensional scal­ing 
from pairwise similarity ratings, simpli.ed from [30]. N=neutral S=smile H=half-smile FH F=frown arousal 
delighted H FN S N S happiness Shape Interpolation Pose Space Deformation Figure 7: Abstract comparison 
of shape interpolation versus pose space deformation using the same set of facial expressions. purely 
kinematic models have their place. In the current computer animation culture animators generally practice 
their craft by direct and exhaustive speci.cation of the desired motion combined with quick evaluation 
using real-time playback. Deeper simulation ap­proaches intrinsically take away some of this control, 
and animators often argue (rightly or not) that automated processes are inferior or will not produce 
a human feel. The performance of current anatom­ically based models prohibits animation preview and other 
real-time applications such as telepresence and gaming (one published result is several orders of magnitude 
slower than real time), and the effort needed to produce an anatomically accurate model is not always 
justi.ed, nor even appropriate if the model is of a fanciful creature whose surface appearance may be 
inconsistent with any plausible internal anatomy in any case. PSD uni.es and improves upon two techniques 
that have been common graphics practice for more than a decade. This relatively simple algorithm uniformly 
handles a variety of deformation situ­ations ranging from a simple elbow to secondary animation. The 
setup cost of the algorithm is insigni.cant, and the synthesis cost is only slightly more than that of 
shape interpolation, so real-time syn­thesis is possible at effective resolutions on current hardware. 
We expect that this algorithm will be a useful complement to current techniques. Acknowledgements The 
authors wish to thank Sean Jenkins, David Miya, Amir Nasrabadi, Steven Puri, Kosta Saric, Steffen Wild, 
Lance Williams, Raymond Yeung, and the anonymous reviewers.   References [1] R. Barnhill, R. Dube, 
and F. Little, Properties of Shepard s Surfaces. Rocky Mountain J. Math., vol.13, 1983, pp. 365-382. 
[2] T. Beier and S. Neely, Feature-Based Image Metamorphosis. Computer Graphics vol. 26, no. 2 (Proc. 
SIGGRAPH 92), pp 35-42. [3] P. Bergeron and P. Lachapelle, Controlling Facial Expression and Body Move­ments 
in the Computer Generated Short Tony de Peltrie . SIGGRAPH 85 Tuto­rial Notes, ACM, 1985. [4] C. Bishop, 
Neural Networks for Pattern Recognition, Oxford: Clarendon, 1995. [5] A. Blake and A. Zisserman, Visual 
Reconstruction, MIT Press, Cambridge, 1988. [6] N. Burtnyk and M. Wein, Interactive Skeleton Techniques 
for Enhancing Motion Dynamics in Key Frame Animation, Comm. ACM, vol. 19, no. 10 (October 1976), pp. 
564-569. [7] J. Chadwick, D. Haumann, and R. Parent, Layered Construction for Deformable Animated Characters. 
Computer Graphics vol. 23 no. 3 (Proc. SIGGRAPH 89), pp. 243-252. [8] D. T. Chen and D. Zeltzer, Pump 
It Up: Computer Animation Based Model of Muscle Using the Finite Element Method. Computer Graphics vol. 
26, (Proc. SIGGRAPH 92), pp. 89-98. [9] Cinefex, Riverside, CA, various issues, e.g. vol. 66 (June 1996), 
p. 52 (Dragon­heart); vol. 64 (Dec 1995), p. 62 (Jumanji). [10] T. DeRose, M. Kass, and T. Truong, Subdivision 
Surfaces in Character Anima­tion. Proc. SIGGRAPH 98, ACM, pp. 85-94. [11] D. Forsey, A Surface Model 
for Skeleton-Based Character Animation. In Second Eurographics Workshop on Animation and Simulation, 
Vienna, Austria, 1991, pp. 55-73. [12] W. M. Hsu, J. F. Hughes, and H. Kaufman, Direct Manipulation of 
Free-Form Deformations, Computer Graphics vol. 26 (Proc. SIGGRAPH 92), pp. 177-184. [13] K. Komatsu, 
Human Skin Model Capable of Natural Shape Variation, The Visual Computer, vol. 4, no. 3, 1988, pp. 265-271. 
[14] J. P. Lewis, Creation by Re.nement: A Creativity Paradigm for Gradient Descent Learning Networks. 
International Conf. on Neural Networks. San Diego, 1988, II: 229-233. [15] J. P. Lewis, Probing the Critic: 
Approaches to Connectionist Pattern Synthesis. IEEE International Joint Conference on Neural Networks 
Seattle, July 1991. [16] P. Litwinowicz and L. Williams, Animating Images with Drawings, Proc. SIG-GRAPH 
94, pp. 409-412. [17] R. MacCracken and K. Joy, Free-Form Deformations with Lattices of Arbitrary Topology. 
Proc. SIGGRAPH 96, pp. 181-180. [18] G. Maestri, Digital Character Animation 2, Vol 1. New Rider, Indianapolis, 
1999. ISBN 1056205-930-0. [19] L. Nedel and D. Thalmann, Modeling and Deformation of the Human Body Us­ing 
an Anatomically-Based Approach, www preprint. [20] C. W. A. M. van Overveld, A Technique for Motion Speci.cation 
in Computer Animation, Visual Computer, vol. 6 (1990), p. 106-116. [21] T. Poggio and R. Brunelli. A 
novel approach to graphics. AI Memo 1354, C.B.I.P Paper 71, MIT, 1992. [22] K. Singh and E. Fiume, Wires: 
A Geometric Deformation Technique. Proc. SIG-GRAPH 98, ACM, pp. 405-414. [23] N. Magnenat-Thalmann, R. 
Laperriere, and D. Thalmann. Joint-Dependent Lo­cal Deformations for Hand Animation and Object Grasping. 
Proc. Graphics In­terface, 1988, pp. 26-33. [24] N. Magnenat-Thalmann and D. Thalmann. Human Body Deformations 
using Joint-Dependent Local Operators and Finite Element Theory. In N. Badler, B. Barsky, and D. Zeltzer, 
eds., Making Them Move: Mechanics, Control, and Animation of Articulated Figures San Mateo, CA: Morgan 
Kaufmann, 1991, pp. 243-262. [25] Maya Alias/Wavefront, Santa Barbara, CA, 1998. [26] F. I. Parke, Parameterized 
Models for Facial Animation. IEEE Computer Graph­ics and Applications, vol. 2, no. 9, November 1982, 
pp. 61-68. [27] S. Pieper, Physically-Based Animation of Facial Tissue for Surgical Simulation, SIGGRAPH 
89 Tutorial Notes: State of the Art in Facial Animation, ACM, 1989. [28] J. D. Powell, The Theory of 
Radial Basis Function Approximation. Cambridge University Numerical Analysis Report, 1990. [29] M. J. 
D. Powell, Radial Basis Functions for Multivariable Interpolation: A Re­view. In J. Mason and M. Cox, 
Eds., Algorithms for Approximation, Oxford: Clarendon, pp. 143-167. [30] J. A. Russel, A Circomplex Model 
of Affect. J. Personality and Social Psychol­ogy, vol. 39, p. 1161-1178, 1980. [31] F. Scheepers, R. 
Parent, W. Carlson, and S. May, Anatomy-Based Modeling of the Human Musculature. Proc. SIGGRAPH 97, ACM, 
pp. 163-172. [32] T. Sederberg and S. Parry, Free Form Deformations of Solid Geometric Models. Computer 
Graphics, vol. 20 no. 4, (Proc. SIGGRAPH 86), pp. 150-161. [33] B. Shneiderman, The Future of Interactive 
Systems and the Emergence of Direct Manipulation. Behaviour and Information Technology, 1, pp. 237-356. 
[34] Pat Taylor, Disney/Dream Quest Mighty Joe Young facial animation, personal communication. [35] R. 
Turner and D. Thalmann, The Elastic Surface Layer Model for Animated Character Construction. in N. M. 
Thalmann and D. Thalmann, eds., Proc. Com­puter Graphis International, New York: Springer Verlag, 1993, 
pp. 399-412. [36] J. Wilhelms and A. Van Gelder, Anatomically Based Modeling. Proc. SIG-GRAPH 97, pp. 
173-180. [37] G. Wyvill, C. McPheeters, and B. Wyvill, Animating Soft Objects. Visual Com­puter, 2, 235-242, 
1986. [38] Xinmin Zhao, Disney Dinosaur project, personal communication.  Figure 9. Comparison of PSD 
(at left) and SSD on the extreme pose of an elbow.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>352172</article_id>
		<sort_key>173</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[The EMOTE model for effort and shape]]></title>
		<page_from>173</page_from>
		<page_to>182</page_to>
		<doi_number>10.1145/344779.352172</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=352172</url>
		<abstract>
			<par><![CDATA[<p>Human movements include limb gestures and postural attitude. Although many computer animation researchers have studied these classes of movements, procedurally generated movements still lack naturalness. We argue that looking only at the psychological notion of gesture is insufficient to capture movement qualities needed by animated charactes. We advocate that the domain of movement observation science, specifically Laban Movement Analysis (LMA) and its Effort and Shape components, provides us with valuable parameters for the form and execution of qualitative aspects of movements. Inspired by some tenets shared among LMA proponents, we also point out that Effort and Shape phrasing across movements and the engagement of the whole body are essential aspects to be considered in the search for naturalness in procedurally generated gestures. Finally, we present EMOTE (Expressive MOTion Engine), a 3D character animation system that applies Effort and Shape qualities to independently defined underlying movements and thereby generates more natural synthetic gestures.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[animation systems]]></kw>
			<kw><![CDATA[expression]]></kw>
			<kw><![CDATA[gestures]]></kw>
			<kw><![CDATA[human body simulation]]></kw>
			<kw><![CDATA[procedural modeling]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Modeling and recovery of physical attributes</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Motion</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Motion</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010380</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010380</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P66532</person_id>
				<author_profile_id><![CDATA[81100330233]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Diane]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Center for Human Modeling and Simulation, University of Pennsylvania]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P202662</person_id>
				<author_profile_id><![CDATA[81538291456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Monica]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Costa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Center for Human Modeling and Simulation, University of Pennsylvania]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP77037672</person_id>
				<author_profile_id><![CDATA[81410592116]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Liwei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Center for Human Modeling and Simulation, University of Pennsylvania]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15038213</person_id>
				<author_profile_id><![CDATA[81452608047]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Norman]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Badler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Center for Human Modeling and Simulation, University of Pennsylvania]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>241079</ref_obj_id>
				<ref_obj_pid>241020</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Amaya, K., Bruderlin, A., Calvert, T. Emotion from motion. In Davis, W.A., Bartels, R., editors, Graphics Interface'96, pp. 222-229. Canadian Information Processing Society, Canadian Human-Computer Comm. Society, May 1996.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Badler, N. A computational alternative to effort notation. In Gray, J.A., editor, Dance Technology: Current Applications and Future Trends. National Dance Association, VA, 1989.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>791568</ref_obj_id>
				<ref_obj_pid>791217</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Badler, N., Chi, D., Chopra S. Virtual human animation based on movement observation and cognitive behavior models. In Computer Animation Conf., Geneva, Switzerland, May 1999. IEEE Computer Society Press.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356760</ref_obj_id>
				<ref_obj_pid>356757</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Badler, N., Smoliar, S. Digital representations of human movement. ACM Computing Surveys, 11 (1): 19-38, March 1979.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>242543</ref_obj_id>
				<ref_obj_pid>242521</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Badler, N., Webber, B., Becket, W., Geib, C., Moore, M., Pelachaud, C., Reich, B., and Stone, M. Planning for animation. In N. Magnenat-Thalmann and D. Thalmann (eds), Interactive Computer Animation, Prentice-Hall, pp. 235-262, 1996.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Bartenieff, I., Davis, M. Effort-Shape analysis of movement: The unity of expression and function. In Davis, M., editor, Research Approaches to Movement and Personality. Arno Press Inc., New York, 1972.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Bartenieff, I., Lewis, D. Body Movement: Coping with the Environment. Gordon and Breach Science Publishers, New York, 1980.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>791499</ref_obj_id>
				<ref_obj_pid>791215</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Becheiraz, P., Thalmann, D. A model of nonverbal communication and interpersonal relationship between virtual actors. Pro. Computer Animation 1996, IEEE Computer Society Press, pp.58-67, 1996.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Bishko, L. Relationships between Laban Movement Analysis and computer animation. In Dance and Technology 1: Moving Toward The Future, pp. 1-9, 1992.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218421</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Bruderlin, A., Williams, L. Motion signal processing. In Proc. of SIGGRAPH' 95, pp. 97-104, August 1995.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Calvert, T.W., Chapman, J., Patla, A. Aspects of the kinematic simulation of human movement. 1EEE Computer Graphics &amp; Applications, 2:41-48, November 1982.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>553895</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Capin, T., Pandzic, I., Magnenat-Thalmann, N., Thalmann, D. Avatars in Networked Virtual Environments. Wiley, Chichester, England, 1999.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>332075</ref_obj_id>
				<ref_obj_pid>332051</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Cassell, J. Not just another pretty face: Embodied conversational interface agents. Comm. of the ACM, 2000. (to appear).]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192272</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Cassell, J., Pelachaud, C., Badler, N., Steedman, M., Achorn, B., Becket, W., Douville, B., Prevost, S., Stone, M. Animated conversation: Rule-based generation of facial expression, gesture and spoken intonation for multiple conversational agents. In Computer Graphics, Annual Conf. Series, pp. 413-420. ACM, 1994.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>929276</ref_obj_id>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Chi, Diane. A Motion Control Scheme for Animating Expressive Arm Movements. PhD thesis, University of Pennsylvania, 1999.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Davis, M. Effort-Shape analysis: Evaluation of its logic and consistency and its systematic use in research. In Bartenieff, I., Davis, M. and Paula, F., editors, Four Adaptations of Effort Theory in Research and Teaching. Dance Notation Bureau, Inc., New York, 1970.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Dell, C. A Primer for Movement Description: Using Effort-Shape and Supplementary Concepts. Dance Notation Bureau, Inc., New York, 1970.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Jack 2.2 Toolkit Reference Guide, Engineering Animation, Inc., 1999.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Kendon, A. Gesticulation and speech: Two aspects of the process of utterance. In Key, M.R., editor, The Relation between Verbal and Nonverbal Communication, pp. 207-227. Mouton, 1980.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808575</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Kochanek, D.H.U., Bartels, R.H. Interpolating splines with local tension, continuity, and bias control. In Proc. of SIGGRAPH'84, volume 18, pp. 33-41, July 1984.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192266</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Koga, Y., Kondo, K., Kuffner, J., Latombe, J. Planning motions with intentions. In Proc. of SIGGRAPH'94, pp. 395-408, July 1994.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Laban, R. The Mastery of Movement. Plays, Inc., Boston, 1971.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Laban, R., Lawrence, F. C. Effort: Economy in Body Movement. Plays, Inc., Boston, 1974.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Lamb, W. Posture and Gesture: An Introduction to the Study of Physical Behavior. Duckworth &amp; Co., London, 1965.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37407</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Lasseter, J. Principles of traditional animation applied to 3D computer animation. In Proc. of SIGGRAPH'87, volume 21, pp. 35-44, July 1987.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Maletic, V. Body, Space, Expression: The Development of Rudolf Laban's Movement and Dance Concepts. Mouton de Gruyte, New York, 1987.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[McNeil, D. Hand and Mind: What Gestures Reveal about Thought. University of Chicago, 1992.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Moore, C.-L., Yamamoto, K. Beyond Words: Movement Observation and Analysis. Gordon and Breach Science Publishers, New York, 1988.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>93274</ref_obj_id>
				<ref_obj_pid>93267</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Morawetz, C., Calvert, T. Goal-directed human animation of multiple movements. In Proceedings of Graphics Interface '90, pp. 60-67, May 1990.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618751</ref_obj_id>
				<ref_obj_pid>616065</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[O'Brien, J. F., Zordan, V. B., Hodgins, J. K. Combining Active and Passive Simulations for Secondary Motion. 1EEE Computer Graphics &amp; Applications. In Press.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>614291</ref_obj_id>
				<ref_obj_pid>614257</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Perlin, K. Real time responsive animation with personality. 1EEE Transactions on Visualization and Computer Graphics, 1(1):5-15, March 1995.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37406</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Reynolds, C.W. Flocks, herds, and schools: A distributed behavioral model. In Proc. of SIGGRAPH'87, volume 21, pp. 25-34, July 1987.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618560</ref_obj_id>
				<ref_obj_pid>616054</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Rose, C., Cohen, M.F. and Bodenheimer, B. Verbs and adverbs: Multidimensional motion interpolation. 1EEE Computer Graphics &amp; Applications, 18(5), September-October 1998.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325243</ref_obj_id>
				<ref_obj_pid>325334</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Steketee, S., Badler, N. Parametric keyframe interpolation incorporating kinetic adjustment and phasing control. In Proc. of SIGGRAPH'85, volume 19, pp. 225-262, July 1985.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Thomas, F., Johnston, O. lllusion of Life: Disney Animation. Hyperion, New York, 1995.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Tolani, D. Inverse Kinematics Methods for Human Modeling and Simulation. PhD thesis, University of Pennsylvania, 1998.]]></ref_text>
				<ref_id>36</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192170</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Tu, X., Terzopoulos, D. Artificial fishes: Physics, locomotion, perception, behavior. In Proc. of SIGGRAPH'94, pp. 43-50, July 1994.]]></ref_text>
				<ref_id>37</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218419</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Unuma, M., Anjyo, K., Takeuchi, R. Fourier principles for emotionbased human figure animation. In Proc. of SIGGRAPH'95, pp. 91-96, August 1995.]]></ref_text>
				<ref_id>38</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280843</ref_obj_id>
				<ref_obj_pid>280765</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Vilhjalmsson, H.H., Cassell J. Bodychat: Autonomous communicative behaviors in avatars. In Proc. of the Second International Conference on Autonomous Agents, pp. 269-277. ACM, May 1998.]]></ref_text>
				<ref_id>39</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218422</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Witkin, A., Popovic, Z. Motion warping. Proceedings of SIGGRAPH '95, pp. 105-108, Los Angeles, CA, August, 1995.]]></ref_text>
				<ref_id>40</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The EMOTE Model for Effort and Shape Diane Chi, Monica Costa, Liwei Zhao, Norman Badler Center for Human 
Modeling and Simulation University of Pennsylvania {chi | monicac | lwzhao | badler}@graphics.cis.upenn.edu 
 ABSTRACT Human movements include limb gestures and postural attitude. Although many computer animation 
researchers have studied these classes of movements, procedurally generated movements still lack naturalness. 
We argue that looking only at the psychological notion of gesture is insufficient to capture movement 
qualities needed by animated characters. We advocate that the domain of movement observation science, 
specifically Laban Movement Analysis (LMA) and its Effort and Shape components, provides us with valuable 
parameters for the form and execution of qualitative aspects of movements. Inspired by some tenets shared 
among LMA proponents, we also point out that Effort and Shape phrasing across movements and the engagement 
of the whole body are essential aspects to be considered in the search for naturalness in procedurally 
generated gestures. Finally, we present EMOTE (Expressive MOTion Engine), a 3D character animation system 
that applies Effort and Shape qualities to independently defined underlying movements and thereby generates 
more natural synthetic gestures.  Keywords Animation systems, human body simulation, gestures, procedural 
modeling, expression 1. INTRODUCTION Human movement ranges from voluntary, goal-oriented movements to 
involuntary, subconscious movements. Voluntary movements include task-driven actions, such as walking 
to get somewhere or speaking. Involuntary movements occur for physiological or biological purposes; for 
instance, blinking, balancing, and breathing. A wide class of movement falls in between these two. In 
general, this class is characterized as consisting of movements which occur in concert and perhaps unconsciously 
with other activities. We note two interesting subclasses of this class of movements. One subclass consists 
of low-level motor controls that assist the accomplishment of a larger coordinated task. For instance, 
unconscious finger controls form grasps, leg and foot coordination enable walking or running, and lip 
movements generate speech. Another interesting subclass is the set of movements that accompany communicative 
acts: facial expressions, limb gestures, and postural attitude. While computer animation researchers 
have actively studied all these classes of human movements, it remains difficult to procedurally generate 
convincing, natural limb and postural movements . We pose the problem as follows: What parameters characterize 
body or limb motions in real people performing communicative acts? The foremost computational approach 
to this issue has been through the gesture models proposed by McNeil [27], and elaborated with computer 
implementations primarily by groups led by Cassell [14,39,13], Badler [2,3], and Thalmann [8,12]. McNeil 
s approach is to characterize communicative arm gestures into several categories: Iconics represent 
some feature of the subject matter, such as the shape or spatial extent of an object.  Metaphorics represent 
an abstract feature of the subject matter, such as exchange, emergence, or use.  Deictics indicate a 
point in space that may refer to people or spatializable things.  Beats are hand movements that occur 
with accented spoken words and speaker turn-taking.  Emblems are stereotypical patterns with understood 
semantics, such as a good-bye wave, the OK-sign, or thumbs-up.  Such an approach has served to make 
conversational characters appear to gesture more-or-less appropriately while they speak and interact 
with each other or actual people. The impression that one gets when watching even the most recent efforts 
in making convincing conversational characters is that the synthetic movements still lack some qualities 
that make them look right . Indeed, the characters seem to be doing the right things, but with a kind 
of robotic awkwardness that quickly marks the performance as synthetic. It is not a computer animation 
problem per se . conventional but skilled key-pose animators are able to produce excellent gestures in 
3D characters. So there is some gap between what such an animator intuits in a character (and is therefore 
able to animate) and what happens in a procedurally synthesized movement. Key pose animators have managed 
to bridge the technology gap by careful application of classic rules for conventional animation [35,25]. 
The McNeil/Cassell approach to gesture is rooted in psychology and experimental procedures that use human 
observers to manually note and characterize a subject s gestures during a story-telling or conversational 
situation. The difficulty in this approach is hidden within the decision to call something a gesture. 
That is, the observer notes the occurrence of a gesture and then records its type. This kind of recording 
fails to capture the parameters of movement that makes one particular gesture appear over another, as 
well as what makes the gesture appear at all. This issue is crucial in the studies of Kendon [19], who 
tries to understand the deeper question: What makes a movement a gesture or not? In his work, a gesture 
is a particular act that appears in the arms or body during discourse. There may be movements that are 
not gestures and there may be movements that are perceived as gestures in some cultures but not in others. 
So clearly, the notion of gesture as a driver for computer­generated characters cannot be - in itself 
- the primary motivator of natural movements. Further, we note that these approaches are limited by their 
basis in linguistics. To address this, we look toward movement representations outside the constraints 
of communicative acts. We find that the Effort and Shape components of Laban Movement Analysis (LMA) 
[22,23,17,7,28] provide us with a more comprehensive set of parameters for describing the form and execution 
of the qualitative aspects of movements. Our approach to gesture augments the McNeil/Cassell approach 
by addressing a missing dimension: movement exists not just because it has underlying linguistic relationships 
but also because it has some distinctiveness in its Effort and Shape parameters. Effort and Shape provide 
a means to describe the aspect of human movement that relates to individual character, cultural norms 
and distinctions. Our approach meshes perfectly with the perspective offered by the LMA proponents: Gesture 
... is any movement of any body part in which Effort or Shape elements or combinations can be observed 
[7]. Our approach to gesture also complies with two other important LMA concepts. The first one is synthesized 
by Bartenieff when she observes that it is not just the main movement actions that let us identify behavior 
but it is the sequence and phrasing of Effort and Shape components that express and reinforce content 
[7]. The other concept is best expressed by Lamb: a gesture localized in the limbs alone lacks impact, 
but when its Effort and Shape characteristics spread to the whole body, a person appears to project full 
involvement, conviction, and sincerity [24]. We present EMOTE (Expressive MOTion Engine), a 3D character 
animation system that allows the specification of Effort and Shape parameters to modify independently 
defined arm and torso movements. The underlying movements of the arms and torso are specified through 
key time and pose information much like conventional computer animation. However, rather than performing 
a simple linear interpolation, we apply Effort and Shape parameters to these motion templates to create 
expressive movements. Our approach allows users to specify separate parameter values for different body 
parts, as well as phrasing parameter values across the key poses. We note that the key pose values may 
be generated synthetically, by inverse kinematics, motion capture, or otherwise pre-stored movement patterns. 
In the next section, we present related work, followed by a brief overview of the Effort and Shape components 
of LMA. Then, we present the EMOTE model for Effort and Shape. Next, we discuss several animations that 
were created to demonstrate the power of our approach. Finally, we point to some directions that guide 
our future investigations and conclude with the main contributions of our work. 2. RELATED WORK In addition 
to the use of computational implementations of gesture models to animate synthetic humans during communicative 
acts [14,39,13,3,8,12], many researchers have addressed the issue of generating more natural movements 
in broader contexts. Several researchers have suggested methods of adding expressiveness to animated 
motions using such methods as stochastic noise functions [31], Fourier function models [38], or emotional 
transforms [1]. Such methods require an off-line modeling process for each different type of expression. 
Others have implemented tools that modify or interpolate existing motions to display different expressions 
or fit other constraints [10,40,33]. Various researchers have developed behavioral animation systems 
to generate animations of multiple creatures with varying personalities and/or goals [32,36,5,8]. Although 
creatures in behavioral animation systems display different high-level behaviors, their low-level movements 
are often very simple, non-expressive, or drawn from a small library of movements. A task-level animation 
system that generates arm motions of a human figure moving an object to a goal location has been developed 
using an inverse kinematics algorithm based on neurophysiological studies [21]. The focus of this system 
is on the intention of moving an object from one location to another and not on the underlying movement 
qualities of the character. The use of secondary motions has been proposed as a way to enliven computer 
generated animations. One approach adds secondary movements to walking characters based on user­specified 
personality and mood [29]. Another approach focuses on passive motions like the movement of clothing 
and hair, generated in response to environmental forces or the movements of characters and other objects 
[30]. Badler originally proposed (but did not implement) the use of Effort to provide users with expressive 
movement control of articulated figures [2]. Bishko suggested analogies between the Twelve Principles 
of Animation [35] and Laban Movement Analysis [8]. She shows that there is an abstract relationship between 
LMA and traditional animation techniques, but does not provide a computational means of exploiting this 
relationship. Others have done work with computerizing Labanotation (a notation, primarily used for recording 
dance, based on Laban s work that focuses on the structural aspects of movement) [4,11], but are only 
beginning to address the more qualitative aspects of movement provided by the Effort and Shape components. 
 3. BACKGROUND Rudolf Laban (1879-1958) made significant contributions to the study of movement, bringing 
together his experiences as a dancer, choreographer, architect, painter, scientist, notator, philosopher, 
and educator. He observed the movement of people performing all types of tasks: from dancers to factory 
workers, fencers to people performing cultural ceremonies, mental patients to managers and company executives. 
His theories on movement, which were significantly extended and applied by his students and colleagues 
have resulted in a rich vocabulary for describing and analyzing movement, leading to the development 
of Laban Movement Analysis1 [7,28,17,26]. LMA has evolved into a comprehensive system that has been used 
in dance, drama, nonverbal research, psychology, anthropology, ergonomics, physical therapy, and many 
other movement-related fields [6,15]. Laban Movement Analysis has five major components: Body, Space, 
Shape, Effort, and Relationship. Together these components constitute a textual and symbolic language 
for describing movement. Body deals with the parts of the body that are used and the initiation and sequencing 
of a motion. Space describes the locale, directions, and paths of a movement. Shape involves the changing 
forms that the body makes in space. Effort describes how the body concentrates its exertion while performing 
movements. Effort is often compared to dynamic musical terms such as legato, forte, dolce, etc., which 
give information on how a piece of music should be performed. Relationship describes modes of interaction 
with oneself, others, and the environment. 1 LMA is promoted by the Laban/Bartenieff Institute of Movement 
Studies (LIMS), 234 Fifth Avenue, Room 203, New York, NY 10001; (212)477-4299; www.limsonline.org. Relationship 
examples include facings, contact, and group forms. As part of our approach to gesture, we developed 
a computational model of the Effort and Shape components of LMA. Effort comprises four motion factors: 
Space, Weight, Time, and Flow. Each motion factor is a continuum between two extremes: (1) indulging 
in the quality and (2) fighting against the quality. In LMA these extreme Effort Elements are seen as 
basic, irreducible qualities, meaning they are the smallest units needed in describing an observed movement. 
The eight Effort Elements are: Indirect/Direct, Light/Strong, Sustained/Sudden, and Free/Bound. The eight 
Elements can be combined and sequenced for innumerable variations of phrasings and expressions. Table 
1 illustrates the motion factors, listing their opposing Effort Elements with textual descriptions and 
examples. Space: attention to the surroundings Indirect: flexible, meandering, wandering, multi-focus 
Examples: waving away bugs, slashing through plant growth Direct: single focus, channeled, undeviating 
Examples: pointing to a particular spot, threading a needle Weight: sense of the impact of one s movement 
Light: buoyant, delicate, easily overcoming gravity, marked by decreasing pressure Examples: dabbing 
paint on a canvas, describing the movement of a feather Strong: powerful, having an impact, increasing 
pressure into the movement Examples: punching, pushing a heavy object, expressing a firmly held opinion 
Time: lack or sense of urgency Sustained: lingering, leisurely, indulging in time Examples: stretching 
to yawn, stroking a pet Sudden: hurried, urgent Examples: swatting a fly, grabbing a child from the path 
of danger Flow: attitude towards bodily tension and control Free: uncontrolled, abandoned, unable to 
stop in the course of the movement Examples: waving wildly, shaking off water Bound: controlled, restrained, 
able to stop Examples: moving in slow motion, tai chi, carefully carrying a cup of hot liquid Table 1: 
Motion Factors and Effort Elements The Shape component involves three distinct qualities of change in 
the form of movement: Shape Flow, Directional Movement, and Shaping. A Shape Flow attitude primarily 
reflects the mover s concern with the changing relationship among body parts. These changes can be sensed 
as the increasing or decreasing volume of the body s form or a moving toward or away from the body center. 
Shape Flow can be seen from these two different perspectives. The first one emphasizes the torso, which 
can be said to Grow or Shrink. A continuous breathing pattern reveals changes in Shape Flow as seen from 
the torso perspective. The other perspective emphasizes the limbs, which are said to be Opening or Closing 
with respect to the longitudinal axis. Shrinking from the cold or stretching to wake up would be characterized 
as having a Shape Flow quality. While Shape Flow is mainly concerned with sensing the body s shape changes 
within itself, Directional Movement describes the mover s intent to bridge the action to a point in the 
environment. These movements can be simple spoke-like or arc-like actions to reach a direction or object, 
such as a reach to shake a hand or to touch an object or to move to a specific location. Shaping Movement 
depicts the changes in movement form that demonstrate a carving or molding attitude as the body interacts 
with the environment. This form can be dictated by objects in space or simply created by the mover. An 
active adapting of the body shape in order to move through a crowd, or a gesture describing an elaborately 
carved sculpture might illustrate a Shaping mode. Shape changes in movement can be described in terms 
of three dimensions: Horizontal, Vertical and Sagittal. Each one of these dimensions is in fact associated 
with one of the three main dimensions (Width, Length, and Depth) as well as one of the three planes (Horizontal, 
Vertical, and Sagittal) related to the human body. Changes in Shape in the Horizontal dimension occur 
mainly in the side-open and side-across directions; as the movement becomes planar there would be more 
of a forward-backward component added to the primary side component. Changes in the Vertical dimension 
are manifested primarily in the upward-downward directions; the plane would add more sideward component 
to the up-down. Finally, changes in the Sagittal dimension are more evident in the body s depth or the 
forward-backward direction; planar movement would add an upward-downward component. We note that while 
there is distinct vocabulary for each quality Shape Flow, Directional Movement, and Shaping in the 
various dimensions, we have merged these three concepts (using them interchangeably) and chosen to use 
the Shaping terminology. The terms we are using to describe the opposing changes in these dimensions 
are Spreading and Enclosing, Rising and Sinking, Advancing and Retreating. It is important to point out 
that limbs and torso movements are not required to involve the same Shape qualities at a given time. 
In this way, Shape Flow functions as a breathing baseline to support Directional and Shaping movement 
of the limbs. In another example, a traffic officer might hold up one arm with a Directional reach, while 
the other arm gestures in a circular Shaping mode, and the head does small tilting Shape Flow actions 
to accompany the Shaping arm. Horizontal Spreading: affinity with Indirect Examples: opening arms to 
embrace, sprawling in a chair Enclosing: affinity with Direct Examples: clasping someone in a hug, huddling 
in the cold Vertical Rising: affinity with Light Examples: reaching for something in a high shelf Sinking: 
affinity with Strong Examples: stamping the floor with indignation Sagittal Advancing: affinity with 
Sustained Examples: reaching out to shake hands Retreating: affinity with Sudden Examples: avoiding a 
punch Table 2: Shaping Dimensions and Affinities Another LMA concept is Reach Space in the Kinesphere 
(near, middle, and far). Our current approach regards Reach Space only from the perspective of the limbs 
in relation to the distance from the body center. Though this is a simplified view, it adds an important 
feature to the limb range of movement. Shape changes can occur in affinity with corresponding Effort 
Elements. Table 2 shows the opposing attitudes towards Shape, some examples, and their affinities with 
Effort Elements. 4. THE EMOTE APPROACH TO GESTURE Our current implementation of EMOTE uses a commercially 
available, fully articulated, human model [18]. At this point, we focus on expressive gestures involving 
arm and torso movements. EMOTE has four features which we believe are essential for creating gestures 
that convey naturalness and expressiveness: 1. A given movement may have Effort and Shape parameters 
applied to it independent of its geometrical definition. 2. A movement s Effort and Shape parameters 
may be varied along distinct numerical scales. 3. Different Effort and Shape parameters may be specified 
for different parts of the body involved in the same movement. 4. The Effort and Shape parameters may 
be phrased (coordinated) across a set of movements.  The underlying movements of a gesture are specified 
through key time and pose information defined for the arms and the torso. An external process, such as 
using a specific gesture stored in a motion library, a procedurally generated motion, or motion captured 
from live performance, could be used to generate these underlying movements. Key pose information could 
be extracted from these movements and used as input into EMOTE. With the key pose information, the EMOTE 
parameters could then be applied to vary the original performance (property 1). Effort and Shape qualities 
are expressed using numeric parameters that can vary along distinct scales (property 2). Each Effort 
and Shape factor is associated with a scale ranging from -1 to +1. The extreme values in these scales 
correspond to the extreme attitudes of the corresponding factors. For example: a +1 value in Effort s 
Weight factor corresponds to a very Strong movement; a -1 value in Shape s Vertical dimension corresponds 
to a Rising movement. Effort parameters are translated into low-level movement parameters, while Shape 
parameters are used to modify key pose information. By interactively using one or many of the Effort 
and Shape dimensions, we can search for the desired quality of a particular movement. During procedural 
synthesis, EMOTE parameters can be applied directly based on parameter values dependent on a character 
s particular utterance, reactions, or personality. EMOTE permits independent specification of Effort 
and Shape parameters for each part of the body (property 3). In its current implementation however, Effort 
parameters do not apply to torso movements. Although Shape parameters have proven to be effective in 
the specification of expressive torso movements, further investigation should be carried out to identify 
how Effort qualities are manifested in the torso. Moreover, the Shape parameters are mainly applied to 
torso movement. The general Space concept of Kinespheric Reach Space is used in the arms. Table 3 summarizes 
which dimensions of Effort and Shape can be used to modify the movements of the different parts of the 
human body. Allowing the definition of expressive gestures that include the legs can be similarly done, 
however, additional constraints need to be carefully considered in order to provide static and dynamic 
balance and stability. Moreover, using Effort and Shape parameters to modify locomotion is a more complex 
task and involves the identification of a different set of low-level movement parameters, including an 
exploration of the pelvic­femoral movement rhythm. Furthermore, including the legs may also affect the 
movement of the torso and arms, because changing the qualities in the legs may result in a reworking 
of the posture. For instance, the additional effort in the legs is reflected and reinforced by the exertion 
of the torso and placement of the arms. Right Arm Left Arm Torso Effort Space yes yes no Weight yes yes 
no Time yes yes no Flow yes yes no Shape Horizontal yes yes yes Vertical yes yes yes Sagittal yes yes 
yes Reach Spc yes yes no Table 3: Body Parts and Effort and Shape Dimensions Figure 1: Effort Phrase 
Editor Finally, our approach allows the specification of different sets of values for the Effort and 
Shape parameters across any series of keys that define the underlying motions (property 4). By property 
(3), this can be done separately for each part of the body. Figure 1 depicts a graph editor used to specify 
Effort parameters across a series of keyframes defined for the arms. 4.1 Expressive Arms The underlying 
key poses of the arms are defined as end-effector positions (keypoints). Keypoints can be defined as 
being global or local. Local keypoints are defined relative to the human s shoulders. Global keypoints, 
on the other hand, establish a constraint relative to the environment. Keypoints can also be classified 
into Goal or Via points. Goal points define a general movement path; the hand follows this path, stopping 
at each Goal point. Via points direct the motion between keyframes without pausing. For instance, a Via 
point might be used to generate a semi-circular path between two Goal points. EMOTE uses an arm model 
with a 1 degree-of-freedom (DOF) elbow joint and spherical (3 DOF) shoulder and wrist joints. An analytical 
inverse kinematics algorithm (IKAN) computes the shoulder and elbow rotations, given a goal specified 
by three­dimensional position coordinates and an elbow swivel angle [36]. Wrist rotations are determined 
according to Effort settings (as described below). Reflecting Effort and Shape definitions provided by 
the LMA system, Shape parameters are used to modify the keypoints that specify arm movements, while Effort 
parameters affect the execution of those movements resulting from the modified keypoints. 4.1.1 Applying 
Shape to Arm Movements Let us first consider the Horizontal, Vertical and Sagittal dimensions of Shape 
and show how the parameters associated with them are used to modify the keypoints. Because we are striving 
to simulate volume-like changes in the movement we are associating the Shape changes more with planar 
action than with strictly dimensional movement. For a particular keypoint, let the variables hor, ver 
and sag in the interval [-1, +1] represent the parameters corresponding to the Horizontal, Vertical and 
Sagittal dimensions, respectively. We define two constants abratio > 1 and maxd. . For each one of the 
above dimensions, we find an ellipse containing the keypoint and lying in a plane parallel to the plane 
associated with that dimension (as described in Section 3). The center of the ellipse is the projection 
of the shoulder joint position on that plane. The major axis of the ellipse is parallel to the direction 
mostly affected by changes in that dimension and its minor axis is parallel to the other direction affected 
by such changes. The quotient between its major radius a and its minor radius b is abratio. We calculate 
the angle .. [0, 2p ) formed by the major axis of the ellipse and the segment whose endpoints are the 
center of the ellipse and the keypoint. We find the contributions of that dimension to the modified keypoint 
by rotating the keypoint by d. , a fraction of maxd. determined by the numeric parameter associated with 
the dimension being considered. Figure 2 illustrates how we calculate vdy and vdz, the contributions 
of the Vertical parameter ver to a particular keypoint. Figure 2: A Keypoint Modified by theVertical 
Parameter Let x, y and z be the coordinates of the keypoint in Figure 2. We find . such that . y .. = 
atan( * abratio) (1) . -z .. 0 =. < 2p The major radius a of the ellipse is calculated by the following 
equation: - z a = (2) cos(. ) The angle formed by the rotated keypoint and the major axis of the ellipse 
is given by the function rot defined as follows: . 0 ver = 0 . min(.- ver * maxd. ,2p ) ver < 0,. =p 
. rot(. ) =. max(.+ ver * maxd. ,0) ver < 0,. <p (3) . max(.- ver * maxd. ,p ) ver > 0,. =p . min(.+ 
ver * maxd. ,p ) ver > 0,. <p Finally, the contributions vdy and vdz are calculated as follows: vdz =- 
(a * cos(rot(. ))) - z (4) 1 vdy = (a ** sin(rot(. ))) - y (5) abratio We use the same model as described 
above to determine the contributions of the Horizontal (hdy, hdx) and Sagittal (sdx, sdz) parameters 
to the modified keypoint. We find the x , y and z coordinates of the modified keypoint by adding the 
appropriate contributions to the coordinates of the original keypoint. Then, x = x + hdx + sdx (6) y 
= y + hdy + vdy (7) z = z + vdz + sdz (8) Let us now consider how the Kinespheric Reach Space parameter 
affects a particular keypoint. When considered from the perspective of the arms, Reach Space design describes 
the limb relationship with the body as it moves toward or away from the body center. Therefore, our Shape 
model modifies a particular keypoint by moving it along the direction that passes through the keypoint 
and the center of mass of the human figure. We use the Reach Space parameter rs to calculate the amount 
by which the keypoint is moved toward or away from the center of mass. This Reach Space modifier is considered 
after the keypoint has been modified according to its Horizontal, Vertical and Sagittal parameters. When 
the achievement of the modified keypoint requires shoulder angles outside the human body limits, stored 
joint limits avoid unattainable configurations of the body. As they establish a constraint relative to 
the environment, Global keypoints are not affected by the Shape parameters. 4.1.2 Applying Effort to 
Arm Movements The translation of the qualitative Effort Elements into quantitative, low-level movement 
parameters was the key task in defining a computational model of the Effort component of LMA. Initially, 
we tried to deduce movement characteristics from motion capture data. We collected 3D motion capture 
data of a Certified Movement Analyst (CMA) trained in LMA performing numerous examples of combinations 
of Effort Elements. Analysis of the motion capture data led to only the most obvious conclusions; i.e.: 
Sudden is short in duration, Sustained is longer in duration, and Strong tends to have large accelerations. 
The inability to deduce the more subtle characteristic qualities of Effort arose from several factors. 
First, Effort reflects complex inner physiological processes that are related to a being s inner drive 
to respond to the physical forces in nature. Thus, Effort is embodied in the whole person and manifested 
in all body parts, whereas we were interested solely in the physical embodiment and visual result of 
inner attitudes on movement, particularly that of the arms. Furthermore, numerous other movements such 
as visual attention, changes in muscular tension, facial expressions, and breath patterns are not adequately 
captured by current motion capture technology. As a result, we turned to other methods for deducing the 
low-level movement parameters and corresponding settings for Effort. We defined underlying quantitative 
structures that model each Effort Element. Visual analysis of the motion capture data played an important 
role in extracting other manifestations of Effort and focusing our attention solely on the influence 
of Effort on arm movements. Other methods we used to derive an empirical model of Effort included descriptions 
of Effort from the literature [7,17,26,28], application of traditional animation principles [25,35], 
and much experimentation with feedback from a CMA. First, we describe the set of low-level, quantitative 
movement parameters. Then, we show how these parameters are set based on the settings for the Effort 
parameters. There are three types of low-level movement parameters: those that affect the arm trajectory, 
those that affect timing, and flourishes that add to the expressiveness of the movement. 4.1.2.1 Trajectory 
Definition We define the arm trajectory for a given animation with two parameters: Path curvature: determines 
the straightness or roundness of the path segments between keypoints. We control the path curvature using 
the tension parameter introduced by Kochanek and Bartels for interpolating splines [20]. The tension 
parameter Tval ranges from -1 to +1.  The interpolation space: defines the space in which the interpolation 
is performed: end-effector position, joint angle, or elbow position.  For end-effector interpolation, 
we use the end-effector position and swivel angle stored for each keypoint. We define an interpolating 
spline between the positions at keypoints using the tension parameter to determine the curvature of the 
path. We also interpolate between swivel angle values with an interpolating spline. For joint angle interpolation, 
we compute and store the shoulder and elbow rotations at keypoints. We then generate an interpolating 
spline between the elbow angle values at keypoints and perform spherical linear interpolation to determine 
the shoulder rotations. For interpolation in elbow position space, we compute and store the elbow position 
at keypoints using the posture defined by the end-effector position and swivel angle. We then define 
an interpolating spline between these positions, which are later used to set the shoulder rotations. 
The elbow rotations for elbow position interpolation are the same as those for end­effector interpolation. 
Interpolation in elbow position space gives smooth elbow motions, but a less path-driven movement than 
interpolation in end-effector position space. The Effort settings determine which interpolation space 
is used. The default interpolation space uses end-effector position. Free movements use angular interpolation 
to achieve a less path-driven and less controlled movement. Our empirical studies show that Indirect 
movements tend to be driven by the elbow, and thus use interpolation in elbow position space. 4.1.2.2 
Parameterized Timing Control We separate timing control from trajectory definition by using a variation 
of the double interpolant method introduced by Steketee and Badler [34]. The interpolating splines that 
define the trajectory (described in the preceding section) compute values between keypoints using an 
interpolation parameter s that varies from 0 to 1 over the interval from keypoint i to keypoint i +1 
[20]. Let the trajectory be defined by some function P(s,i). We now need a method of translating frame 
numbers into s and i. At the ith keypoint, s = 0 . For in-between frames, we define a variable t . [0,1], 
a frame s relative time between the previous and following keypoints. Let prev equal the frame number 
of the previous keypoint, next equal the frame number of the next keypoint, and curr equal the current 
frame number. Then, curr - prev t = (9) next - prev We define a frame number-to-time function Q(t , I 
) = s , parameterized by a set of variables I to achieve various timing effects (described further below). 
For each in-between frame, we normalize the frame number to produce t', use function Q to compute s, 
and then input s and the corresponding keypoint number i into function P to compute the position values 
(or joint angle values for angular interpolation) for the given frame. We provide several parameters 
for timing control: The number of frames between keypoints is initially set according to the user s 
specified key times, but these values get adjusted according to the Effort settings.  Input variables 
to the keyframe-to-time function (I) include inflection time ti, time exponent texp, start velocity v0, 
and end velocity v1.  Our parameterized frame number-to-time function Q assumes every movement (from 
one Goal keypoint to the next) starts and ends at rest. Also, every movement has a constant acceleration 
a until time ti, followed by a constant deceleration. We introduce velocities v0 at time t0 and v1 at 
time t1 to achieve the traditional animation effects of anticipation and overshoot [25]. This model gives 
us the following velocity function (Figure 3): .- v0 t [0,t0) . t . 0 .- (v + t )t +vt + tt 0 i 0 i 
0 i [t ,t ) . 0 i (10) t - t v(t ) =. 0 i - (v + t )t +vt + tt 0 i 1 i 1 i . [ti ,t1). t1 - ti .- v1t 
+v1 [t1,1] . .t - 1 . 1 where ( )t exp t = t . (11) The function Q is the integral of Equation (10). 
ti -v0 -v1 Figure 3: Velocity Function The set of input variables I to the frame number-to-time function 
Q provides control to the acceleration/deceleration pattern of the movement, as well as allowing for 
anticipation and overshoot. The inflection point ti represents the point (between 0 and 1) where the 
movement changes from accelerating to decelerating. A value of 0.5 gives a basic ease-in/ease-out curve. 
A value greater than 0.5 corresponds to a primarily accelerating motion, while a value less than 0.5 
gives a decelerating motion. The default time exponent (texp) value of 1.0 does not affect the velocity 
curve; however, values greater than 1.0 magnify an acceleration, while values less than 1.0 exaggerate 
a deceleration. The start (v0) and end (v1) velocities2 default to 0. Increasing v0 generates movements 
with anticipation, where the hand pulls back before extending in preparation for a Strong movement. Decreasing 
v1 generates movements with overshoot, such as in Free movements where an indulgence in flow causes one 
to swing out past a target before hitting it. We set t0 to 0.01 and t1 to 0.99, which gives us natural-looking 
anticipation and overshoot effects; however, these values can easily be included in I as variable low-level 
movement parameters. 4.1.2.3 Flourishes Flourishes are miscellaneous parameters that add to the expressiveness 
of the movements. These are listed below: Wrist bend is determined by the wrist bend multiplier wbmag 
and the wrist extension magnitude wxmag. The wbmag parameter is a multiplier that represents the magnitude 
of the wrist bend. If the wbmag is set for a flexed wrist, the wrist bend is set to 0.6 radians about 
the x-axis. Otherwise, the wrist bend is set using wrist _ bend = wbmag * sin(2p (t +0.75)) +1 - wxmag) 
(12) where t . [0,1] and represents the normalized time between two keypoints. This results in a wrist 
that gradually bends inwards and back out. The value of wxmag shifts the sinusoidal graph, setting the 
beginning wrist extension to be positive (outward) or negative (inward). 2 As mentioned, each movement 
begins and ends at rest. The start and end velocities represent shortly after the beginning or shortly 
before the end of a movement, respectively. They are so named to emphasize that they are not initial 
and final velocities, which remain 0. Arm twist is parameterized by wrist twist magnitude wtmag, wrist 
frequency wfmag, elbow twist magnitude etmag, and elbow frequency efmag. The wrist twist is measured 
in radians about the z-axis and is determined by: wrist _ twist = wtmag * sin(wfmag *p t ) . (13) Elbow 
twist is set using a similar equation, replacing wtmag and wfmag with etmag and efmag, respectively. 
Displacement magnitude is a multiplier dmag that adds a sinusoidal displacement to the elbow angle elbow 
_ angle = elbow _ angle * (1 + dmag * sin(2p t )) (14) where t is the normalized time between two keypoints. 
 4.1.2.4 Parameter Settings To determine the mapping of the four Effort Elements into our low-level movement 
parameters, we first determined the default settings for each of the eight Effort Elements by trial and 
error using visual analysis and testing by a CMA. For example, the default interpolation space is set 
to elbow position for Indirect, joint angle for Free, and end-effector for the other Effort Elements. 
The default tension of the path curvature Tval is set to -1 for Indirect, +1 for Direct, and 0 for the 
other Effort Elements. Once we had the default settings for the individual Effort Elements, we generated 
the range between opposing Effort Elements by interpolating continuous variables and using the nearest 
value for discrete variables such as the interpolation space. We note that these may lead to discontinuities 
in the animation if Space, Weight, or Flow cross zero when they are phrased across the keyframes. In 
[15], we show that such discontinuities occur only if the zero crossings occur at points that are not 
Goal keypoints, which is fairly uncommon in real human movements. In general, combinations of Effort 
Elements are achieved in a straightforward manner. The magnitude of an Effort Element is used to weight 
its contribution for a parameter setting. If more than one Effort Element contributes to a parameter 
setting, we take the maximum value of the weighted contributions. Several parameters undergo minor adjustments 
when combining Effort Elements from different motion factors. Finally, we express our Effort model as 
a set of equations. Let the variables ind, dir, lgt, str, sus, sud, fre, and bnd represent the magnitudes 
for Indirect, Direct, Light, Strong, Sustained, Sudden, Free, and Bound, respectively. Each of these 
variables is in [0,1]. Variables within the same motion factor are related as such: if one Effort Element 
variable is positive, then its opposing Effort Element variable is zero. To adjust parameters for combined 
Effort settings, we use the function f: . aa = b f (a, b) = (15) ..b otherwise Our model for translating 
Effort into low-level movement parameters is given by the following equations: Tval = (- 1* ind + 1* 
f (ind , fre)) + dir (16) wbmag = max(0.6*ind ,0.5*lgt,0.4* fre) (17) wxmag =- 0.3*lgt + (0.3* fre - 
0.9* f (str, fre)) (18) dmag = etmag = wtmag = 0.4 * ind (19) efmag = wfmag = 2* ind (20) ti = 0.5 + 
0.4* max(str, sud ) (21) - 0.4* max(lgt, sus) + 0.8* f (bnd,lgt) v0 = 0.1* str - max(0.06 * f (sus, str),0.1* 
f ( fre, str)) (22) v1 = max(0.03* max(lgt, sus),0.2 * fre - 0.1* f (ind, fre)) (23) texp = 1 + 2* sud 
+ (0.2 * f (str, sud ) - f ( fre, sud )) - 0.2 * max(str, f (dir, sus)) (24) - 0.4 * fre - 0.5* f (ind, 
fre)  4.2 Expressive Torso The underlying key poses of the torso involve, in fact, the neck joint, 
the spine, the pelvis and the two clavicle joints. The neck has 3 DOF, the spine has 17 joints with 3 
DOF each, the pelvis has 3 DOF and each clavicle has 2 DOF. A key pose consists of angles for the neck, 
the pelvis, and the clavicles, in addition to the configuration of the spine [18]. When, for a particular 
keyframe, no pose information is provided, the system assumes a neutral posture, where all the angles 
are 0. We use an ease-in/ease-out curve to interpolate the angles in the keyframes and hence calculate 
the angles in the in-between frames. In summary, Shape changes essentially provide squash and stretch 
within the limits of a fixed segment length articulated skeleton. 4.2.1 Applying Shape to Torso Movements 
As seen before, EMOTE allows the definition of Shape parameters for the torso corresponding to the Horizontal, 
Vertical and Sagittal dimensions, which are used to modify the key poses. Each parameter lies in a scale 
ranging from -1 to +1. Our Shape model for the torso associates each main body dimension (upward/downward, 
sideward-open/sideward-across, and forward/backward) with specific parts of the body. We note that our 
Shape model was designed considering the available controls in our selected articulated figure model 
[18]. In particular, the torso could not be expanded and contracted for breath and other volume-changing 
movements. We based our Shape to body part associations on the suitability of each body part in producing 
changes in the form of the body in given directions. Thus, we associate the upward-downward direction 
with the neck and the spine; the sideward direction with the clavicles, and the forward-backward direction 
with the pelvis. Therefore, changes in the Horizontal dimension, which occur mainly in the sideward direction 
but also have a forward-backward component as the movement becomes planar, affect mostly the angles of 
the clavicles but also slightly alter pelvis rotations. Changes in the Vertical dimension, which are 
manifested primarily in the upward-downward direction but also have a sideward component in planar movement, 
affect mostly the angles of the neck and the spine but also change clavicle angles. Finally, changes 
in the Sagittal dimension, which are more evident in the forward-backward direction but also involve 
an upward-downward component in planar movement, mainly affect pelvis rotations but also change the angles 
of the neck and spine. For each opposing attitude associated with the above dimensions (Spreading, Enclosing, 
Rising, Sinking, Advancing, and Retreating), we define maximum displacement angles for all the body parts 
that are affected by changes in that dimension. For instance, for the opposing attitudes in the Horizontal 
dimension, we define the following constants: spreading_clavicle_angle, enclosing_clavicle_angle, spreading_pelvis_angle, 
and enclosing_pelvis_angle. The first two angles represent clavicle rotations about the z-axis and the 
latter represent pelvis rotations about the y-axis. For a particular keyframe, let the variables spr, 
enc, ris, sin, adv, and ret represent the magnitudes for Spreading, Enclosing, Rising, Sinking, Advancing, 
and Retreating, respectively. Each of these variables is in [0,1]. Variables referencing the same dimension 
are related such that if one variable is positive, then its opposing variable is zero. We modify the 
angles in the key pose by adding the weighted contribution of all the dimensions that affect the particular 
body part being considered. For instance, if the clavicle rotation about the z-axis is represented by 
the variable clavicle_angle and the pelvis rotation about the y-axis is represented by the variable pelvis_angle, 
then we modify those angles as follows: clavicle_a ngle = clavicle_ angle + ris*risin g_clavicle _angle 
 (25) + sin*sinking_clavicle _angle + spr*sprea ding_clavi cle_angle + enc*enclo sing_clavi cle_angle 
pelvis_ang le = pelvis_an gle + spr*spread ing_pelvis _angle (26) + enc*enclos ing_pelvis _angle + 
adv*advanc ing_pelvis _angle + ret*retrea ting_pelvi s_angle  where rising_clavicle_angle and sinking_clavicle_angle 
are the maximum displacement angles of the clavicles corresponding to the opposing attitudes towards 
the Vertical dimension, and advancing_pelvis_angle and retreating_pelvis_angle are the maximum displacement 
rotations of the pelvis corresponding to the opposing attitudes towards the Sagittal dimension.  5. 
EXAMPLES To demonstrate the power of our approach to gesture we have created a series of animations shown 
on the accompanying video. All the examples were generated in real-time. The first series of animations 
are all generated from the same set of key poses and try to mimic an actor during an actual performance. 
We vary the values of the Effort and Shape parameters across the animations and show how these variations 
can completely alter the meaning of the dramatization enacted by the synthetic actor. By suppressing 
its Shape parameters, we also show the important role that the torso plays in gesture and in the depiction 
of a convincing character. The second video series emphasizes the slight differences in dynamic qualities 
of movements superimposed on American Sign Language phrases (from an ASL sign library) and tries to capture 
the nuances of meaning represented by these differences. The movement of the hands in the video is implemented 
using forward kinematics and linear interpolations. 6. DISCUSSION Our EMOTE computational model of Effort 
and Shape components allows the animation of characters with natural­looking gestures through the usage 
of high-level parameters that represent qualitative aspects of movements. By using EMOTE interactively 
we hope to avoid the hassle that the animator goes through while working with a large number of low-level 
parameters. In order to further assess the advantages of using Effort and Shape parameters from the perspective 
of user interaction, formal methods of evaluation of our approach should be devised. We did a preliminary 
evaluation of the Effort Elements of EMOTE [15]. Using a stylized character with head, arms, and hands, 
we created a 16-minute video of randomly selected Effort Elements. In the first part of the tape, Effort 
gestures with 16 two-keypoint and 16 five-keypoints were paired with a neutral (no Effort Element) animation. 
The second part of the tape consisted of 30 long (5 keypoint) animations with various Effort combinations. 
The tape was given to 3 CMAs and the project consultant CMA. They were asked to view it once to get a 
feeling for the presentation and then a second time while marking a coding sheet. They were asked to 
mark the primary overall Effort Element(s) they observed as present (-1 or 1) or neutral (0). The results 
are presented in Table 4. The first row indicates the percentage of correct responses where the CMA 
either marked the Effort that we were trying to display in the animation or marked neutral when we were 
trying to display neutrality along a given motion factor. The second row indicates the percentage of 
neutral responses where the CMA marked neutral when we were trying to display an Effort or where the 
CMA marked an Effort when we were trying to display neutral along a give motion factor range. The third 
row indicates the percentage of opposite responses where the CMA marked the Effort opposite from the 
one we were trying to portray. The low but significant percentage of neutral responses is partially attributed 
to the fact that most of the animation segments on our video showed combinations of the Effort Elements 
 thus, a more prominent Effort may have masked other displayed Effort Elements. One consequence of this 
experiment for us was to increase the maximum movement rate for the limbs. For example, the Sudden movements 
did not appear fast enough to trained observers. Also, the Shape elements were not included in this experiment. 
Note that the normal CMA observer situation is to watch motions repeatedly; by limiting their samples 
to two we were forcing them to pick distinctive Effort features in a slightly unnatural setting. The 
results were encouraging enough, however, for us to proceed with refinements to the Effort Elements and 
the incorporation of the torso and Shape components. Consultant CMA 1 CMA 2 CMA 3 Correct 76.6 55.6 53.2 
60.1 Neutral 22.6 38.7 39.1 37.1 Opposite 0.81 5.6 7.7 2.8 Table 4: Overall Percentages for Effort Element 
Evaluation Our attempt to bridge the gap between characters manually animated and characters animated 
by procedures establishes a new layer in the motion control process in which expressiveness is represented 
by a small number of parameters. We expect that this layer of control will give rise to yet another layer, 
where characters controlled by natural language commands show different performances according to adverbs 
that convey manner. These adverbs should be automatically mapped into Effort and Shape parameters. For 
example, carefully might translate into Light and slightly Sustained Effort portrayed during arm movements 
and a little Retreating Shape displayed by the torso; proudly might translate into a Rising posture. 
Furthermore, we expect to find connections between emotions and personality and our high-level parameters 
and so be able to synthesize movements that reflect these inner states. 7. CONCLUSIONS We have introduced 
a new approach to procedural human animation that tries to close the gap between characters animated 
by the use of manual techniques and characters animated procedurally. This approach goes beyond the realm 
of psychology of gestures and linguistic-based approaches by exploring the domain of movement observation. 
This approach uncovers the movement qualities, which can be combined together to reveal different manners, 
inner states, personalities and emotions. The EMOTE approach to gesture proposes a computational model 
of the Effort and Shape components of Laban Movement Analysis and associates with each one of their dimensions 
numerical parameters that modify pre-defined movements. Two other important aspects of EMOTE are inspired 
by the tenets of movement observation. The first is the ability to phrase Effort and Shape parameters 
across a set of movements. We believe that a character s gestures should be phrased similarly to communicative 
phrasing with an expressive content consonant with the principal utterance; for example, a strong accent 
in speech should be correlated by a strong Effort in gesture. Since Effort plays a key role in the interpretation 
of a character s action, a gesture must display Effort qualities that match his/her intentions, motivations, 
and mood. Otherwise, the character s message appears conflicted and confused. Furthermore, EMOTE reflects 
our belief that, even if a character moves its arms with appropriate gestures, it will lack conviction 
and naturalness if the rest of the body is not appropriately engaged. If the empirical principles of 
movement science hold up when transformed into computer code implementations, we should be able to animate 
engaging, committed, expressive, and believable characters consistently and automatically. 8. ACKNOWLEDGEMENTS 
Janis Pforsich (courante@juno.com) was our LMA consultant for EMOTE. She played a key role in the development 
of our Effort and Shape model, was an enthusiastic teacher of movement observation, ensured the accuracy 
of the project and its documentation with LMA theory, and acted as a devoted proponent of our work. We 
are very grateful for her generous contributions. This research is partially supported by U.S. Air Force 
F41624-97-D-5002, Office of Naval Research K-5­55043/3916-1552793, and AASERTs N00014-97-1-0603 and N0014-97-1-0605, 
NSF EIA98-09209, SBR-8900230, and IIS­9900297, NASA NRA NAG 5-3990, and Engineering Animation Inc. (EAI). 
Support for Monica Costa by the National Scientific and Technological Development Council (CNpq) of Brazil 
is also gratefully acknowledged.  9. REFERENCES [1] Amaya, K., Bruderlin, A., Calvert, T. Emotion from 
motion. In Davis, W.A., Bartels, R., editors, Graphics Interface 96, pp. 222-229. Canadian Information 
Processing Society, Canadian Human-Computer Comm. Society, May 1996. [2] Badler, N. A computational alternative 
to effort notation. In Gray, J.A., editor, Dance Technology: Current Applications and Future Trends. 
National Dance Association, VA, 1989. [3] Badler, N., Chi, D., Chopra S. Virtual human animation based 
on movement observation and cognitive behavior models. In Computer Animation Conf., Geneva, Switzerland, 
May 1999. IEEE Computer Society Press. [4] Badler, N., Smoliar, S. Digital representations of human movement. 
ACM Computing Surveys, 11(1):19-38, March 1979. [5] Badler, N., Webber, B., Becket, W., Geib, C., Moore, 
M., Pelachaud, C., Reich, B., and Stone, M. Planning for animation. In N. Magnenat-Thalmann and D. Thalmann 
(eds), Interactive Computer Animation, Prentice-Hall, pp. 235-262, 1996. [6] Bartenieff, I., Davis, M. 
Effort-Shape analysis of movement: The unity of expression and function. In Davis, M., editor, Research 
Approaches to Movement and Personality. Arno Press Inc., New York, 1972. [7] Bartenieff, I., Lewis, D. 
Body Movement: Coping with the Environment. Gordon and Breach Science Publishers, New York, 1980. [8] 
Becheiraz, P., Thalmann, D. A model of nonverbal communication and interpersonal relationship between 
virtual actors. Pro. Computer Animation 1996, IEEE Computer Society Press, pp.58-67, 1996. [9] Bishko, 
L. Relationships between Laban Movement Analysis and computer animation. In Dance and Technology I: Moving 
Toward The Future, pp. 1-9, 1992. [10] Bruderlin, A., Williams, L. Motion signal processing. In Proc. 
of SIGGRAPH 95, pp. 97-104, August 1995. [11] Calvert, T.W., Chapman, J., Patla, A. Aspects of the kinematic 
simulation of human movement. IEEE Computer Graphics &#38; Applications, 2:41-48, November 1982. [12] 
Capin, T., Pandzic, I., Magnenat-Thalmann, N., Thalmann, D. Avatars in Networked Virtual Environments. 
Wiley, Chichester, England, 1999. [13] Cassell, J. Not just another pretty face: Embodied conversational 
interface agents. Comm. of the ACM, 2000. (to appear). [14] Cassell, J., Pelachaud, C., Badler, N., Steedman, 
M., Achorn, B., Becket, W., Douville, B., Prevost, S., Stone, M. Animated conversation: Rule-based generation 
of facial expression, gesture and spoken intonation for multiple conversational agents. In Computer Graphics, 
Annual Conf. Series, pp. 413-420. ACM, 1994. [15] Chi, Diane. A Motion Control Scheme for Animating Expressive 
Arm Movements. PhD thesis, University of Pennsylvania, 1999. [16] Davis, M. Effort-Shape analysis: Evaluation 
of its logic and consistency and its systematic use in research. In Bartenieff, I., Davis, M. and Paula, 
F., editors, Four Adaptations of Effort Theory in Research and Teaching. Dance Notation Bureau, Inc., 
New York, 1970. [17] Dell, C. A Primer for Movement Description: Using Effort-Shape and Supplementary 
Concepts. Dance Notation Bureau, Inc., New York, 1970. [18] Jack 2.2 Toolkit Reference Guide, Engineering 
Animation, Inc., 1999. [19] Kendon, A. Gesticulation and speech: Two aspects of the process of utterance. 
In Key, M.R., editor, The Relation between Verbal and Nonverbal Communication, pp. 207-227. Mouton, 1980. 
 Permission to make digital or hard copies of part or all of this work or personal or classroom use is 
granted without fee provided that copies are not made or distributed for profit or commercial advantage 
and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, 
to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. SIGGRAPH 
2000, New Orleans, LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 [20] Kochanek, D.H.U., Bartels, 
R.H. Interpolating splines with local tension, continuity, and bias control. In Proc. of SIGGRAPH 84, 
volume 18, pp. 33-41, July 1984. [21] Koga, Y., Kondo, K., Kuffner, J., Latombe, J. Planning motions 
with intentions. In Proc. of SIGGRAPH 94, pp. 395-408, July 1994. [22] Laban, R. The Mastery of Movement. 
Plays, Inc., Boston, 1971. [23] Laban, R., Lawrence, F. C. Effort: Economy in Body Movement. Plays, Inc., 
Boston, 1974. [24] Lamb, W. Posture and Gesture: An Introduction to the Study of Physical Behavior. Duckworth 
&#38; Co., London, 1965. [25] Lasseter, J. Principles of traditional animation applied to 3D computer 
animation. In Proc. of SIGGRAPH 87, volume 21, pp. 35-44, July 1987. [26] Maletic, V. Body, Space, Expression: 
The Development of Rudolf Laban s Movement and Dance Concepts. Mouton de Gruyte, New York, 1987. [27] 
McNeil, D. Hand and Mind: What Gestures Reveal about Thought. University of Chicago, 1992. [28] Moore, 
C.-L., Yamamoto, K. Beyond Words: Movement Observation and Analysis. Gordon and Breach Science Publishers, 
New York, 1988. [29] Morawetz, C., Calvert, T. Goal-directed human animation of multiple movements. In 
Proceedings of Graphics Interface 90, pp. 60-67, May 1990. [30] O Brien, J. F., Zordan, V. B., Hodgins, 
J. K. Combining Active and Passive Simulations for Secondary Motion. IEEE Computer Graphics &#38; Applications. 
In Press. [31] Perlin, K. Real time responsive animation with personality. IEEE Transactions on Visualization 
and Computer Graphics, 1(1):5-15, March 1995. [32] Reynolds, C.W. Flocks, herds, and schools: A distributed 
behavioral model. In Proc. of SIGGRAPH 87, volume 21, pp. 25-34, July 1987. [33] Rose, C., Cohen, M.F. 
and Bodenheimer, B. Verbs and adverbs: Multidimensional motion interpolation. IEEE Computer Graphics 
&#38; Applications, 18(5), September-October 1998. [34] Steketee, S., Badler, N. Parametric keyframe 
interpolation incorporating kinetic adjustment and phasing control. In Proc. of SIGGRAPH 85, volume 19, 
pp. 225-262, July 1985. [35] Thomas, F., Johnston, O. Illusion of Life: Disney Animation. Hyperion, New 
York, 1995. [36] Tolani, D. Inverse Kinematics Methods for Human Modeling and Simulation. PhD thesis, 
University of Pennsylvania, 1998. [37] Tu, X., Terzopoulos, D. Artificial fishes: Physics, locomotion, 
perception, behavior. In Proc. of SIGGRAPH 94, pp. 43-50, July 1994. [38] Unuma, M., Anjyo, K., Takeuchi, 
R. Fourier principles for emotion­based human figure animation. In Proc. of SIGGRAPH 95, pp. 91-96, August 
1995. [39] Vilhjalmsson, H.H., Cassell J. Bodychat: Autonomous communicative behaviors in avatars. In 
Proc. of the Second International Conference on Autonomous Agents, pp. 269-277. ACM, May 1998. [40] Witkin, 
A., Popovic, Z. Motion warping. Proceedings of SIGGRAPH 95, pp. 105-108, Los Angeles, CA, August, 1995. 
  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344865</article_id>
		<sort_key>183</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[Style machines]]></title>
		<page_from>183</page_from>
		<page_to>192</page_to>
		<doi_number>10.1145/344779.344865</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344865</url>
		<abstract>
			<par><![CDATA[<p>We approach the problem of stylistic motion synthesis by learning motion patterns from a highly varied set of motion capture sequences. Each sequence may have a distinct choreography, performed in a distinct sytle. Learning identifies common choreographic elements across sequences, the different styles in which each element is performed, and a small number of stylistic degrees of freedom which span the many variations in the dataset. The learned model can synthesize novel motion data in any interpolation or extrapolation of styles. For example, it can convert novice ballet motions into the more graceful modern dance of an expert. The model can also be driven by video, by scripts or even by noise to generate new choreography and synthesize virtual motion-capture in many styles.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[animation]]></kw>
			<kw><![CDATA[behavior simulation]]></kw>
			<kw><![CDATA[character behavior]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>E.4</cat_node>
				<descriptor>Data compaction and compression</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Motion</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.3</cat_node>
				<descriptor>Time series analysis</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.9</cat_node>
				<descriptor>Kinematics and dynamics</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Motion</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Performing arts (e.g., dance, music)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010213.10010215</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Control methods->Motion path planning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010380</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10002952.10002971.10003451.10002975</concept_id>
				<concept_desc>CCS->Information systems->Data management systems->Data structures->Data layout->Data compression</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010380</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003688.10003693</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Statistical paradigms->Time series analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP33033316</person_id>
				<author_profile_id><![CDATA[81406592826]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Matthew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Brand]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mitsubishi Electric Research Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14018548</person_id>
				<author_profile_id><![CDATA[81100015154]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Aaron]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hertzmann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NYU Media Research Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[L. Baum. An inequality and associated maximization technique in statistical estimation of probabilistic functions of Markov processes. Inequalities, 3:1-8, 1972.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[M. Brand. Pattern discovery via entropy minimization. In D. Heckerman and C. Whittaker, editors, Artificial Intelligence and Statistics #7. Morgan Kaufmann., January 1999.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>657946</ref_obj_id>
				<ref_obj_pid>645529</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[M. Brand. Exploring variational structure by cross-entropy optimization. In P. Langley, editor, P1vceedings, International Conference on Machine Learning, 2000.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311537</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[M. Brand. Voice puppetry. P1vceedings of SIGGRAPH 99, pages 21-28, August 1999.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>851583</ref_obj_id>
				<ref_obj_pid>850924</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[M. Brand. Shadow puppetry. P1vceedings oflCCV 99, September 1999.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>794493</ref_obj_id>
				<ref_obj_pid>794189</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[C. Bregler. Learning and recognizing human dynamics in video sequences. P1vceedings of CVPR 97, 1997.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218421</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[A. Bruderlin and L. Williams. Motion signal processing. P1vceedings of SIG- GRAPH 95, pages 97-104, August 1995.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>895346</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[J. Buhmann. Empirical risk approximation: An induction principle for unsupervised leaming. Technical Report IAI-TR-98-3, Institut fiir Intbrmatik III, Universitfit Bonn. 1998., 1998.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[R.M. Corless, G. H. Gonnet, D. E. G. Hare, D. J. Jeffrey, and D. E. Knuth. On the Lambert W function. Advances in Computational Mathematics, 5:329-359, 1996.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>794495</ref_obj_id>
				<ref_obj_pid>794189</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[W.T. Freeman and J. B. Tenenbaum. Learning bilinear models for two-thctor problems in vision. In P~vceedings, Conf. on Computer Vision and Pattern Recognition, pages 554-560, San Juan, PR, 1997.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>253321</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[M. Gleicher. Motion editing with spacetime constraints. 1997 Symposium on Interactive 3D Graphics, pages 139-148, April 1997.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280820</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[M. Gleicher. Retargeting motion to new characters. P~vceedings of SIGGRAPH 98, pages 33-42, July 1998.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280816</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[R. Grzeszczuk, D. Terzopoulos, and G. Hinton. Neuroanimator: Fast neural network emulation and control of physics-based models. P~vceedings of SIG- GRAPH 98, pages 9-20, July 1998.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[N.R. Howe, M. E. Leventon, and W. T. Freeman. Bayesian reconstruction of 3d human motion from single-camera video. In S. Solla, T. Leend, and K. Muller, editors, Advances in Neural Information P~vcessing Systems, volume 10. MIT Press, 2000.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311539</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[J. Lee and S. Y. Shin. A hierarchical approach to interactive motion editing for human-like figures. P1vceedings of SIGGRAPH 99, pages 39-48, August 1999.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311536</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Z. Popovi6 and A. Witkin. Physically based motion transformation. P~vceedings of SIGGRAPH 99, pages 11-20, August 1999.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[L.R. Rabiner. A tutorial on hidden Markov models and selected applications in speech recognition. P1vceedings of the IEEE, 77(2):257-286, Feb. 1989.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618560</ref_obj_id>
				<ref_obj_pid>616054</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[C. Rose, M. F. Cohen, and B. Bodenheimer. Verbs and adverbs: Multidimensional motion interpolation. IEEE Computer Graphics &amp; Applications, 18 (5): 32-40, September - October 1998.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[J.B. Tenenbaum and W. T. Freeman. Separating style and content. In M. Mozer, M. Jordan, and T. Petsche, editors, Advances in Neural Information P~vcessing Systems, volume 9, pages 662-668. MIT Press, 1997.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218419</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[M. Unuma, K. Anjyo, and R. Takeuchi. Fourier principles for emotion-based human figure animation. P~vceedings of SIGGRAPH 95, pages 91-96, August 1995.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>317049</ref_obj_id>
				<ref_obj_pid>317043</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[A. Wilson and A. Bobick. Parametric hidden markov models for gesture recognition. IEEE Trans. Pattern Analysis and Machine Intelligence, 21(9), 1999.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218422</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[A. Witkin and Z. Popovi6. Motion warping. Pivceedings of SIGGRAPH 95, pages 105-108, August 1995.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>264167</ref_obj_id>
				<ref_obj_pid>265493</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[S. C. Zhu, Y. Wu, and D. Mumford. Minimax entropy principle and its applications to texture modeling. Neural Computation, 9(8), 1997.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Style machines Matthew Brand Aaron Hertzmann Mitsubishi Electric Research Laboratory NYU Media Research 
Laboratory A pirouette and promenade in .ve synthetic styles drawn from a space that contains ballet, 
modern dance, and different body types. The choreography is also synthetic. Streamers show the trajectory 
of the left hand and foot. Abstract We approach the problem of stylistic motion synthesis by learn­ing 
motion patterns from a highly varied set of motion capture se­quences. Each sequence may have a distinct 
choreography, per­formed in a distinct style. Learning identi.es common choreo­graphic elements across 
sequences, the different styles in which each element is performed, and a small number of stylistic degrees 
of freedom which span the many variations in the dataset. The learned model can synthesize novel motion 
data in any interpolation or extrapolation of styles. For example, it can convert novice bal­let motions 
into the more graceful modern dance of an expert. The model can also be driven by video, by scripts, 
or even by noise to generate new choreography and synthesize virtual motion-capture in many styles. CR 
Categories: I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism Animation; I.2.9 [Arti.cial 
Intelligence]: Robotics Kinematics and Dynamics; G.3 [Mathematics of Com­puting]: Probability and Statistics 
Time series analysis; E.4 [Data]: Coding and Information Theory Data compaction and compression; J.5 
[Computer Applications]: Arts and Humanities Performing Arts Keywords: animation, behavior simulation, 
character behavior. 1 Introduction It is natural to think of walking, running, strutting, trudging, 
sashay­ing, etc., as stylistic variations on a basic motor theme. From a di­rectorial point of view, 
the style of a motion often conveys more meaning than the underlying motion itself. Yet existing animation 
tools provide little or no high-level control over the style of an ani­mation. Permission to make digital 
or hard copies of part or all of this work or personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. SIGGRAPH 2000, New Orleans, 
LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 In this paper we introduce the style machine 
a statistical model that can generate new motion sequences in a broad range of styles, just by adjusting 
a small number of stylistic knobs (parameters). Style machines support synthesis and resynthesis in new 
styles, as well as style identi.cation of existing data. They can be driven by many kinds of inputs, 
including computer vision, scripts, and noise. Our key result is a method for learning a style machine, 
including the number and nature of its stylistic knobs, from data. We use style machines to model highly 
nonlinear and nontrivial behaviors such as ballet and modern dance, working with very long unsegmented 
motion-capture sequences and using the learned model to generate new choreography and to improve a novice 
s dancing. Style machines make it easy to generate long motion sequences containing many different actions 
and transitions. They can of­fer a broad range of stylistic degrees of freedom; in this paper we show 
early results manipulating gender, weight distribution, grace, energy, and formal dance styles. Moreover, 
style machines can be learned from relatively modest collections of existing motion­capture; as such 
they present a highly generative and .exible alter­native to motion libraries. Potential uses include: 
Generation: Beginning with a modest amount of motion capture, an animator can train and use the result­ing 
style machine to generate large amounts of motion data with new orderings of actions. Casts of thousands: 
Random walks in the machine can produce thousands of unique, plausible motion choreographies, each of 
which can be synthesized as motion data in a unique style. Improvement: Motion capture from unskilled 
performers can be resynthesized in the style of an expert athlete or dancer. Retargetting: Motion capture 
data can be resynthesized in a new mood, gender, energy level, body type, etc. Acquisition: Style machines 
can be driven by computer vision, data-gloves, even impoverished sensors such as the computer mouse, 
and thus offer a low-cost, low-expertise alternative to motion-capture.  2 Related work Much recent 
effort has addressed the problem of editing and reuse of existing animation. A common approach is to 
provide interac­tive animation tools for motion editing, with the goal of capturing the style of the 
existing motion, while editing the content. Gle­icher [11] provides a low-level interactive motion editing 
tool that searches for a new motion that meets some new constraints while minimizing the distance to 
the old motion. A related optimization method method is also used to adapt a motion to new characters 
[12]. Lee et al. [15] provide an interactive multiresolution mo­tion editor for fast, .ne-scale control 
of the motion. Most editing Figure 1: Schematic illustrating the effects of cross-entropy minimization. 
[A]. Three simple walk cycles projected onto 2-space. Each data point represents the body pose observed 
at a given time. [B]. In conventional learning, one .ts a single model to all the data (ellipses indicate 
state-speci.c isoprobability contours; arcs indicate allowable transitions). But here learning is overwhelmed 
by variation among the sequences, and fails to discover the essential structure of the walk cycle. [C]. 
Individually estimated models are hopelessly over.t to their individual sequences, and will not generalize 
to new data. In addition, they divide up the cycle differently, and thus cannot be blended or compared. 
[D]. Cross-entropy minimized models are constrained to have similar qualitative structure and to identify 
similar phases of the  i cycle. [E]. The generic model abstracts all the information in the style-speci.c 
models; various settings of the style variable will recover all the speci.c models plus any interpolation 
or extrapolation of them. systems produce results that may violate the laws of mechanics; Popovi´ c and 
Witkin [16] describe a method for editing motion in a reduced-dimensionality space in order to edit motions 
while main­taining physical validity. Such a method would be a useful comple­ment to the techniques presented 
here. An alternative approach is to provide more global animation con­trols. Signal processing systems, 
such as described by Bruderlin and Williams [7] and Unuma et al. [20], provide frequency-domain controls 
for editing the style of a motion. Witkin and Popovi´ c [22] blend between existing motions to provide 
a combination of mo­tion styles. Rose et al. [18] use radial basis functions to interpolate between and 
extrapolate around a set of aligned and labeled exam­ple motions (e.g., happy/sad and young/old walk 
cycles), then use kinematic solvers to smoothly string together these motions. Simi­lar functionality 
falls out of our framework. Although such interactive systems provide .ne control, they rely on the labors 
of skilled animators to produce compelling and con­vincing results. Furthermore, it is generally dif.cult 
to produce a new motion that is substantially different from the existing motions, in style or in content 
(e.g., to convert by hand a ponderous walk to a jaunty dance, etc.) The above signal-processing methods 
also require that the exam­ple motions be time-warped; in other words, that sequential corre­spondences 
can be found between each component of each motion. Unfortunately, it is rarely the case that any set 
of complex motions will have this property. Style machines automatically compute .ex­ible many-to-many 
correspondences between sequences using fast dynamic programming algorithms. Our work unites two themes 
that have separate research histories in motion analysis: estimation of dynamical (in the sense of time­evolving) 
models from examples, and style and content separation. Howe et al. [14] analyze motion from video using 
a mixture-of-Gaussians model. Grzeszczuk et al. [13] learn control and phys­ical systems from physical 
simulation. Several authors have used hidden Markov Models to analyze and synthesize motion. Bregler 
[6] and Brand [5] use HMMs to recognize and analyze motion from video sequences. Brand [4] analyzes and 
resynthesizes animation of human speech from example audio and video. With regard to styles, Wilson and 
Bobick [21] use parametric HMMs, in which motion recognition models are learned from user-labeled styles. 
Tenenbaum and Freeman [19, 10] separate style from content in general domains under a bilinear model, 
thereby modeling factors that have individually linear but cooperatively multiplicative effects on the 
output, e.g., the effects of lighting and pose in images of faces. These style/content models depend 
on large sets of hand-labeled and hand-aligned samples (often exponential in the number of stylistic 
degrees of freedom DOFs) plus an explicit statement of what the stylistic DOFs are. We now introduce 
methods for extracting this information directly and automatically from modest amounts of data.  3 
Learning stylistic state-space models We seek a model of human motion from which we can generate novel 
choreography in a variety of styles. Rather than attempt to engineer such a model, we will attempt to 
learn it to extract from data a function that approximates the data-generating mechanism. We cast this 
as an unsupervised learning problem, in which the goal is to acquire a generative model that captures 
the data s es­sential structure (traces of the data-generating mechanism) and dis­cards its accidental 
properties (particulars of the speci.c sample). Accidental properties include noise and the bias of the 
sample. Es­sential structure can also be divided into two components, which we will call structure and 
style. For example, walking, running, strutting, etc., are all stylistic variations on bipedal locomotion, 
a dynamical system with particularly simple temporal structure a deterministic loop. It is up to the 
modeler to make the structure/style distinction. State-space representations are very useful here: We 
take the structure of bipedal locomotion to be a small set of dynamically­signi.cant qualitative states 
along with the rules that govern changes of state. We take style to be variations in the mapping from 
qualitative states to quantitative observations. For example, shift­ing one s weight load onto the right 
leg is a dynamically-signi.cant state common to all forms of bipedal locomotion, but it will look quite 
different in running, trudging, etc. An appropriate state-space model for time-series data is the hid­den 
Markov model (HMM). An HMM is a probabilistic .nite-state machine consisting of a set of discrete states, 
state-to-state tran­sition probabilities, and state-to-signal emission probabilities in this paper, each 
state has a Gaussian distribution over a small space d of full-body poses and motions. (See A for a concise 
HMM tutorial; see [17] for a detailed tutorial.) We will add to the HMM a multidi­ i mensional style 
variablethat can be used to vary its parameters, and call the result a stylistic HMM (SHMM), or time-series 
style ma­ d chine. (See B for formal de.nitions.) The SHMM de.nes a space i of HMMs; .xing the parameteryields 
a unique HMM. Here we show how to separate structure, style, and accidental properties in a dataset by 
minimizing entropies in the SHMM. The main advantages of separating style from structure is that we wind 
up with simpler, more generative models for both, and we can do so with signi.cantly less data than required 
for the general learn­ing setting. Our framework is fully unsupervised and automatically identi.es the 
number and nature of the stylistic degrees of freedom (often much fewer than the number of variations 
in the dataset). The discovered degrees of freedom lend themselves to some intu­itive operations that 
are very useful for synthesis: style mixtures, exaggerations, extrapolations, and even analogies. 3.1 
Generic and style-speci.c models We begin with a family of training samples. By family we mean that all 
the samples have some generic data-generating mechanism in common, e.g., the motor program for dancing. 
Each sample may instantiate a different variation. The samples need not be aligned, e.g., the ordering, 
timing, and appearance of actions may vary from sample to sample. Our modeling goal is to extract a single 
parame­terized model which covers the generic behavior in the entire family of training samples, and 
which can easily be made to model an in­dividual style, combination of styles, or extrapolation of styles, 
just by choosing an appropriate setting of the style variable v . Learning involves the simultaneous 
estimation of a generic model and a set of style-speci.c models with three objectives: (1) each model 
should .t its sample(s) well; (2) each speci.c model should be close to the generic model; and (3) the 
generic model should be as simple as possible, thereby maximizing probability of correct generalization 
to new data. These constraints have an information-theoretic expression in eqn. 1. In the next section 
we will explain how the last two constraints interact to produce a third desirable property: The style-speci.c 
models can be expressed as small variations on the generic model, and the space of such varia­tions can 
be captured with just a few parameters. We .rst describe the use of style machines as applied to pure 
signal data. Details speci.c to working with motion-capture data are described in 4. 3.2 Estimation 
by entropy minimization In learning we minimize a sum of entropies which measure the ambiguity in a probability 
distribution and cross-entropies which measure the divergence between distributions. The principle of 
minimum entropy, advocated in various forms by [23, 2, 8], seeks the simplest model that explains the 
data, or, equivalently, the most complex model whose parameter estimates are fully supported by the data. 
This maximizes the information extracted from the train­ing data and boosts the odds of generalizing 
correctly beyond it. The learning objective has three components, corresponding to the constraints listed 
above: 1. The cross-entropy between the model distribution and statis­tics extracted from the data measures 
the model s mis.t of the data. 2. The cross-entropy between the generic and a speci.c model measures 
inconsistencies in their analysis of the data. 3. The entropy of the generic model measures ambiguity 
and lack of structure in its analysis of the data.  Minimizing #1 makes the model faithful to the data. 
Minimizing #2 essentially maximizes the overlap between the generic and speci.c models and congruence 
(similarity of support) between their hid­den states. This means that the models behave similarly and 
their hidden states have similar meanings. For example, in a dataset of bipedal motion sequences, all 
the style-speci.c HMMs should con­verge to similar .nite-state machine models of the locomotion cy­cle, 
and corresponding states in each HMM to refer to qualitatively similar poses and motions in the locomotion 
cycle. E.g., the n th state in each model is tuned to the poses in which the body s weight shifts onto 
the right leg, regardless of the style of motion (see .g­ure 1). Minimizing #3 optimizes the predictiveness 
of the model by making sure that it gives the clearest and most concise picture of the data, with each 
hidden state explaining a clearly delineated phenomenon in the data. of entropy and cross-entropy, respectively. 
Gaussian distributions are visualized as ellipsoid iso-probability contours. Putting this all together 
gives the following learning objective where is a vector of model parameters;is a vector of expected 
Ee function -log posterior E x  E H ah 1:-log likelihoodk m ah os E k H a E k -log prior m a E e 
s E k E data entropy mis.t 3:model entropy 2:incongruence (1) suf.cient statistics describing the data 
X ; parameterizes a ref- H atnak erence model (e.g., the generic); is an entropy measure; and matnak 
is a cross entropy measure. Eqn. 1 can also be formulated as a Bayesian posterior anaEqa nkoanah alEkoanaEkanaalEko 
with likelihood function X rdf-fWn(wWn)f-Dnn(wwPhX(nanaEkordf-fWn(X(n)f-Dnn(XPt PhX(n and a prior . The 
data entropy term, not mentioned above, arises in the normaliza­tion of the likelihood function; it measures 
ambiguity in the data­descriptive statistics that are calculated vis-` a vis the model. 3.3 Effects 
of the prior It is worth examining the prior because this is what will give the Ex .nal model its special 
style-spanning and generative properties. rdf-fWn(Xln The prior term expresses our belief in the parsimony 
principle a model should give a maximally concise and minimally uncertain explanation of the structure 
in its training set. This is an optimal bias for extracting as much information as possible from the 
data [3]. We apply this prior to the generic model. The prior has an interesting effect on the SHMM s 
emission distributions over pose and velocity: It gradually removes dimensions of variation, because 
.attening a distribution is the most effective way to reduce its volume and therefore its entropy (see 
.gure 2). The prior termrf-Dnn(XitsPhX(n keeps style models close and con­gruent to the generic model, 
so that corresponding hidden states in two models have similar behavior. In practice we assess this prior 
only on the emission distributions of the speci.c models, where it has the effect of keeping the variation-dependent 
emission distri­butions clustered tightly around the generic emission distribution. Consequently it minimizes 
distance between corresponding states in the models, not between the entire models. We also add a term 
inTthnmaEesEk that allows us to vary the strength of the cross­entropy prior in the course of optimization. 
By constraining generic and style-speci.c Gaussians to overlap, and constraining both to be narrow, we 
cause the distribution of state-speci.c Gaussians across styles to have a small number of degrees of 
freedom. Intuitively, if two Gaussians are narrow and overlap, then they must be aligned in the directions 
of their nar­rowness (e.g., two overlapping disks in 3-space must be co-planar). Figure 2 illustrates. 
The more dimensions in which the overlap­ping Gaussians are .at, the fewer degrees of freedom they have 
relative to each other. Consequently, as style-speci.c models are drawn toward the generic model during 
training, all the models set­tle into a parameter subspace (see .gure 3). Within this subspace, all the 
variation between the style-speci.c models can be described with just a few parameters. We can then identify 
those degrees of freedom by solving for a smooth low-dimensional manifold that contains the parameterizations 
of all the style-speci.c models. Our Figure 3: Schematic of style DOF discovery. LEFT: Without cross­entropy 
constraints, style-speci.c models (pentagons) are drawn to their data (clouds), and typically span all 
dimensions of parameter space. RIGHT: When also drawn to a generic model, they settle into a parameter 
subspace (indicated by the dashed plane). experiments showed that a linear subspace usually provides 
a good low-dimensional parameterization of the dataset s stylistic degrees of freedom. The subspace is 
easily obtained from a principal com­ponents analysis (PCA) of a set of vectors, each representing one 
model s parameters. It is often useful to extend the prior with additional functions of En H aEk . For 
example, adding inT and varying T gives determin­istic annealing, an optimization strategy that forces 
the system to explore the error surface at many scales as Tori instead of my­opically converging to the 
nearest local optimum (see C for equa­tions). 3.4 Optimization In learning we hope to simultaneously 
segment the data into motion primitives, match similar primitives executed in different styles, and estimate 
the structure and parameters for minimally ambigu­ous, maximally generative models. Entropic estimation 
[2] gives us a framework for solving this partially discrete optimization prob­lem by embedding it in 
a high-dimensional continuous space via entropies. It also gives us numerical methods in the form of 
max­imum a posteriori (MAP) entropy-optimizing parameter estimators. These attempt to .nd a best data-generating 
model, by gradually extinguishing excess model parameters that are not well-supported by the data. This 
solves the discrete optimization problem by caus­ing a diffuse distribution over all possible segmentations 
to collapse onto a single segmentation. Optimization proceeds via Expectation-Maximization (EM) [1, 17], 
a fast and powerful .xpoint algorithm that guarantees con­vergence to a local likelihood optimum from 
any initialization. The estimators we give in D modify EM to do cross-entropy op­timization and annealing. 
Annealing strengthens EM s guarantee to quasi-global optimality global MAP optimality with probabil­ity 
approaching 1 as the annealing shedule lengthens a necessary assurance due to the number of combinatorial 
optimization prob­lems that are being solved simultaneously: segmentation, labeling, alignment, model 
selection, and parameter estimation. The full algorithm is: 1. Initialize a generic model and one style-speci.c 
model for each motion sequence. 2. EM loop until convergence: (a) E step: Compute expected suf.cient 
statistics of each motion sequence relative to its model. (b) M step: (generic): Calculate maximum a 
posteriori pa­   ) ) rameter values 0 with the minimum-entropy prior, us­ing E-step statistics from 
the entire training set. (c) M step: (speci.c): Calculate maximum a posteriori ) parameter values 0 with 
the minimum-cross-entropy prior, only using E-step statistics from the current se­quence. (d) Adjust 
the temperature (see below for schedules). 3. Find a subspace that spans the parameter variations between 
models. E.g., calculate a PCA of the differences between the generic and each style-speci.c model. Initialization 
can be random because full annealing will obliterate initial conditions. If one can encode useful hints 
in the initial model, then the EM loop should use partial annealing by starting at a lower temperature. 
HMMs have a useful property that saves us the trouble of hand­segmenting and/or labelling the training 
data: The actions in any particular training sequence may be squashed and stretched in time, oddly ordered, 
and repeated; in the course of learning, the ba­sic HMM dynamic programming algorithms will .nd an optimal 
segmentation and labelling of each sequence. Our cross-entropy prior simply adds the constraint that 
similarly-labeled frames ex­hibit similar behavior (but not necessarily appearance) across se­quences. 
Figure 4 illustrates with the induced state machine and labelling of four similar but incongruent sequences, 
and an induced state machine that captures all their choreographic elements and transitions.  4 Working 
with Motion Capture As in any machine-learning application, one can make the problem harder or easier 
depending on how the data is represented to the al­gorithm. Learning algorithms look for the most statistically 
salient patterns of variation the data. For motion capture, these may not be the patterns that humans 
.nd perceptually and expressively salient. Thus we want to preprocess the data to highlight sources of 
varia­tion that tell the story of a dance, such as leg-motions and com­pensatory body motions, and suppress 
irrelevant sources of varia­tion, such as inconsistent marker placements and world coordinates between 
sequences (which would otherwise be modeled as stylistic variations). Other sources of variation, such 
as inter-sequence vari­ations in body shapes, need to be scaled down so that they do not dominate style 
space. We now describe methods for converting raw marker data into a suitable representation for learning 
motion. 4.1 Data Gathering and Preprocessing We .rst gathered human motion capture data from a variety 
of sources (see acknowledgements in 8). The data consists of the 3D positions of physical markers placed 
on human actors, acquired over short intervals in motion capture studios. Each data source pro­vided 
data with a different arrangement of markers over the body. We de.ned a reduced 20 marker arrangement, 
such that all of the markers in the input sequences could be converted by combining and deleting extra 
markers. (Note that the missing markers can be recovered from synthesized data later by remapping the 
style ma­chines to the original input marker data.) We also doubled the size of the data set by mirroring, 
and resampled all sequences to 60Hz. Captured and synthetic motion capture data in the .gures and animations 
show the motions of markers connected by a fake skele­ton. The bones of this skeleton have no algorithmic 
value; they are added for illustration purposes only to make the markers easier to follow. The next step 
is to convert marker data into joint angles plus limb lengths, global position and global orientation. 
The coccyx (near the base of the back) is used as the root of the kinematic tree. Joint angles alone 
are used for training. Joint angles are by nature periodic (for example, ranging fromi to o1 ); because 
training as­sumes that the input signal lies in the in.nite domain of n , we took some pain to choose 
a joint angle parameterization without discontinuities (such as a jump from o1 to ) in the training data.1 
Howe w ver, we were not able to fully eliminate all discontinuities. (This is partially due to some perversities 
in the input data such as inverted knee bends.) Conversion to joint angles removes information about 
which ar­ticulations cause greatest changes in body pose. To restore this in­formation, we scale joint 
angle variables to make statistically salient those articulations that most vary the pose, measured in 
the data set by a procedure similar to that described by Gleicher [11]. To re­duce the dependence on 
individual body shape, the mean pose is subtracted from each sequence, Finally, noise and dimensionality 
are reduced via PCA; we typically use ten or fewer signi.cant di­mensions of data variation for training. 
 4.2 Training Models are initialized with a state transition matrix P j w that has probabilities declining 
exponentially off the diagonal; the Gaus­sians are initialized randomly or centered on every th frame 
of the sequence. These initial conditions save the learning algorithm the gratuitous trouble of selecting 
from among a factorial number of permutationally equivalent models, differing only in the ordering of 
their states. We train with annealing, setting the temperature T high and mak­ing it decay exponentially 
toward zero. This forces the estimators to explore the error surface at many scales before committing 
to a par­ticular region of parameter space. In the high-temperature phase, we set the cross-entropy temperature 
Tt to zero, to force the varia­tion models to stay near the generic model. At high temperatures, any 
accidental commitments made in the initialization are largely obliterated. As the generic temperature 
declines, we brie.y heat up the cross-entropy temperature, allowing the style-speci.c models to venture 
off to .nd datapoints not well explained by the generic model. We then drive both temperatures to zero 
and let the estima­tors converge to an entropy minimum. These temperature schedules are hints that guide 
the optimiza­tion: (1) Find global structure; (2) of.oad local variation to the speci.c models; (3) then 
simplify (compress) all models as much as possible. The result of training is a collection of models 
and for each model, a distribution , over its hidden states, where / ( o r (yy) = (state i explains 
frame, , given all the information in the sequence y ). Typically this distribution has zero or near-zero 
entropy, mean­ing that , has collapsed to a single state sequence that explains the data., (or the sequence 
of most probable states) encodes the content of the data; as we show below, applying either one to a 
dif­ferent style-speci.c model causes that content to be resynthesized in a different style. We use , 
to remap each model s emission distributions to joint angles and angular velocities, scaled according 
the importance of each joint. This information is needed for synthesis. Remapping means re-estimating 
emission parameters to observe a time-series that is synchronous with the training data. 4.3 Making 
New Styles We encode a style-speci.c HMM in a vector by concatentating its state means J , square-root 
covariances (K . K s for i :gj ), and state dwell times (on average, how long a model stays in one state 
before transitioning out). New styles can be created by inter­polation and extrapolation within this 
space. The dimensionality of the space is reduced by PCA, treating each HMM as a single ob­servation 
and the generic HMM as the origin. The PCA gives us a 1For cyclic domains, one would ideally use von 
Mises distribution, es­sentially a Gaussian wrapped around a circle, but we cannot because no analytic 
variance estimator is known. subspace of models whose axes are intrinsic degrees of variation across 
the styles in the training set. Typically, only a few stylistic DOFs are needed to span the many variations 
in a training set, and these become the dimensions of the style variable v . One interpo­lates between 
any styles in the training set by varying v between the coordinates of their models in the style subspace, 
then recon­stituting a style-speci.c HMM from the resulting parameter vector. Of course, it is more interesting 
to extrapolate, by going outside the convex hull of the training styles, a theme that is explored below 
in t 5. 4.4 Analyzing New Data To obtain the style coordinates of a novel motion sequence y , we begin 
with a copy of the generic model (or of a style-speci.c model which assigns y high likelihood), then 
retrain that model on y , us­ing cross-entropy constraints with respect to the original generic model. 
Projection of the resulting parameters onto the style mani­fold gives the style coordinates. We also 
obtain the sample s state occupancy matrix , . As mentioned before, this summarizes the content of the 
motion sequence y and is the key to synthesis, de­scribed below. 4.5 Synthesizing Virtual Motion Data 
Given a new value of the style variable v and a state sequence ' (yy) Ea,n (yw) encoding the content 
of y , one may resynthesize y in the new style y by calculating the maximum-likelihood path n Wh ne w 
 (y lvn ' (yy) ) . Brand [4] describes a method for calcu­lating the maximum-likelihood sample in n oT 
) time for T time­ t steps. F generalizes and improves on this result, so that all the information in 
, is used. This resulting path is an inherently smooth curve that varies even if the system dwells in 
the same hidden state for several frames, because of the velocity constraints on each frame. Motion discon­tinuities 
in the synthesized samples are possible if the difference in velocities between successive states is 
large relative to the frame (sampling) rate. The preprocessing steps are then reversed to pro­duce virtual 
motion-capture data as the .nal output. Some actions take longer in different styles; as we move from 
style to style, this is accommodated by scaling dwell times of the state sequence to match those of the 
new style. This is one of many ways of making time .exible; another is to incorporate dwell times directly 
into the emission distributions and then synthesize a list of varying-sized time-steps by which to clock 
the synthsized motion­capture frames. In addition to resynthesizing existing motion-capture in new styles, 
it is possible to generate entirely new motion data directly from the model itself. Learning automatically 
extracts motion prim­itives and motion cycles from the data (see .gure 4), which take the form of state 
sub-sequences. By cutting and pasting these state se­quences, we can sequence new choreography. If a 
model s state machine has an arc between states in two consecutively scheduled motion primitives, the 
model will automatically modify the end and beginning of the two primitives to transition smoothly. Otherwise, 
we must .nd a path through the state machine between the two primitives and insert all the states on 
that path. An interesting effect can also be achieved by doing a random walk on the state machine, which 
generates random but plausible choreography.  5 Examples We collected a set of bipedal locomotion time-series 
from a va­riety of sources. These motion-capture sequences feature a vari­ety of different kinds of motion, 
body types, postures, and marker placements. We converted all motion data to use a common set of markers 
on a prototypical body. (If we do not normalize the body,  walk Male ballet Female expert ballet Female 
modern dance Female lazy ballet 1... ...time... ...1440 1... ...time... ...1277 1... ...time... ...1603 
1... ...time... ...1471 Figure 4: TOP: State machine learned from four dance sequences totalling 6000+ 
frames. Very low-probability arcs have been removed for clarity. Motion cycles have been labeled; other 
primitives are contained in linear sequences. BOTTOM: Occupancy matrices (constructed while learning) 
indicate how each sequence was segmented and labeled. Note the variations in timing, ordering, and cycles 
between sequences. Figure 5: Completion of the analogy walking:running::strutting:X via synthesis of 
stylistic motion. Stick .gures show every 5 frames; streamers show the trajectories of the extremities. 
X extrapolates both the energetic arm swing of strutting and the power stride of running. Figure 6: 
Five motion sequences synthesized from the same choreography, but in different styles (one per row). 
The actions, aligned vertically, are tiptoeing, turning, kicking, and spinning. The odd body geometry 
re.ects marker placements in the training motion-capture. the algorithm typically identi.es variations 
in body geometry as the principalstylistic DOFs.) n As a hint to the algorithm, we .rst trained an HMM 
on a very low­dimensional representation of the data. Entropic estimation yielded a model which was essentially 
a phase-diagram of the locomotive cycle. This was used as an initialization for the full SHMM training. 
The SHMM was lightly annealed, so it was not constrained to use this hint, but the .nal generic model 
did retain some of the information in the initialization. The PCA of the resulting style-speci.c models 
revealed that 3 stylistic degrees of freedom explained 93% of the variation between the 10 models. The 
most signi.cant stylistic DOF appears to be global pose, e.g., one tilts forward for running, back for 
funny­walking. It also contains information about the speed of motion. Style DOF #2 controls balance 
and gender; varying it modi.es the hips, the distance of the footfall to the midline, and the compensat­ing 
swing of the arms. Finally, style DOF #3 can be characterized as the amount of swagger and energy in 
the motion; increasing it yields sequences that look more and more high-spirited. Extrapo­lating beyond 
the hull of speci.c models yields well-behaved mo­tion with the expected properties. E.g., we can double 
the amount of swagger, or tilt a walk forward into a slouch. We demonstrate with analogies. Analogies 
are a particularly interesting form of extrapola­tion. Given the analogicial problem walking:running::strutting:X, 
we can solve for X in terms of the style coordinates X=strutting+(running-walking), which is equivalent 
to com­pleting the parallelogram having the style coordinates for walking, running, and strutting as 
three of its corners. run ? walk strut The resulting synthesized sequence (.gure 5) looks like a fast­advancing 
form of skanking (a rather high-energy pop dance style). Similarly, the analogy walking:running::cat-walking:X 
gives some­thing that looks like how a model might skip/run down a catwalk in a fashion show. Now we 
turn to examples that cannot be handled by existing time-warping and signal processing methods. In a 
more complicated example, the system was trained on four performances by classically trained dancers 
(man, woman­ballet, woman-modern-dance, woman-lazy-ballet) of 50-70 sec­onds duration each, with roughly 
20 different moves. The per­formances all have similar choreographies but vary in the timing, ordering, 
and style of moves. A 75-state model took roughly 20 minutes to train on 6000+ frames, using interpreted 
Matlab code on a single CPU of a 400MHz AlphaServer. Parameter extinction left a 69-state SHMM with roughly 
3500 parameters. Figure 4 shows that the system has discovered roughly equivalent qualitative structure 
in all the dances. The .gure also shows a .owchart of the chore­ography discovered in learning. We then 
took a 1600-frame sequence of a novice dancer attempt­ing similar choreography, but with little success, 
getting moves wrong and wrongly ordered, losing the beat, and occasionally stum­bling. We resynthesized 
this in a masculine-modern style, obtaining notable improvements in the grace and recognizability of 
the dance. This is shown in the accompanying video. We then generated new choreography by doing a random 
walk on the state machine. We used the resulting state sequence to synthesize new motion-capture in a 
variety of styles: . ballet-. languid; modern+male, etc. These are shown in the video. Fig­ure 6 illustrates 
how different the results are by showing poses from aligned time-slices in the different synthesized 
performances. Finally, we demonstrate driving style machines from video. The essence of our technique 
is the generation of stylistically var­ied motion capture from HMM state sequences (or distributions 
over states). In the examples above, we obtained state sequences from existing motion capture or random 
walks on the HMM state machine. In fact, such state sequences can be calculated from arbitrary sig­nals: 
We can use Brand s shadow puppetry technique [5] to infer statesequencesand/or 3Dbodyposeandvelocityfromvideoimage 
sequences. This means that one can create animations by acting out a motion in front of a camera, then 
use style machines to map some­one else s (e.g. an expert s) style onto one s choreography. In the accompanying 
video we show some vision-driven motion-capture and stylistic variations thereon.    6 Discussion 
Our unsupervised framework automates many of the dreariest tasks in motion-capture editing and analysis: 
The data needn t be seg­mented, annotated, or aligned, nor must it contain any explicit state­ment of 
the theme or the stylistic degrees of freedom (DOFs). All these things are discovered in learning. In 
addition, the algorithms automatically segment the data, identify primitive motion cycles, learn transitions 
between primitives, and identify the stylistic DOFs that make primitives look quite different in different 
motion-capture sequences. This approach treats animation as a pure data-modeling and in­ference task: 
There is no prior kinematic or dynamic model; no rep­resentation of bones, masses, or gravity; no prior 
annotation or seg­mentation of the motion-capture data into primitives or styles. Ev­erything needed 
for generating animation is learned directly from the data. However, the user isn t forced to stay data-pure. 
We expect that our methods can be easily coupled with other constraints; the quadratic synthesis objective 
function and/or its linear gradi­ent (eqn. 21) can be used as penalty terms in larger optimizations that 
incorporate user-speci.ed constraints on kinematics, dynam­ics, foot placement, etc. That we have not 
done so in this paper and video should make clear the potential of raw inference. Our method generalizes 
reasonably well off of its small training set, but like all data-driven approaches, it will fail (gracefully) 
if given problems that look like nothing in the training set. We are currently exploring a variety of 
strategies for incrementally learning new motions as more data comes in. An important open question is 
the choice of temperature sched­ules, in which we see a trade-off between learning time and quality of 
the model. The results can be sensitive to the time-courses of mmt and and we have no theoretical results 
about how to choose optimal schedules. Although we have concentrated on motion-capture time-series, the 
style machine framework is quite general and could be applied to a variety of data types and underlying 
models. For example, one could model a variety of textures with mixture models, learn the stylistic DOFs, 
then synthesize extrapolated textures. 7 Summary Style machines are generative probabilistic models 
that can synthe­size data in a broad variety of styles, interpolating and extrapolating stylistic variations 
learned from a training set. We have introduced a cross-entropy optimization framework that makes it 
possible learn style machines from a sparse sampling of unlabeled style examples. We then showed how 
to apply style machines to full-body motion­capture data, and demonstrated three kinds of applications: 
resyn­thesizing existing motion-capture in new styles; synthesizing new choreographies and stylized motion 
data therefrom; and synthesiz­ing stylized motion from video. Finally, we showed style machines doing 
something that every dance student has wished for: Superim­posing the motor skills of an expert dancer 
on the choreography of a novice. 8 Acknowledgments The datasets used to train these models were made 
available by Bill Freeman, Michael Gleicher, Zoran Popovic, Adaptive Optics, Biovision, Kinetix, and 
some anonymous sources. Special thanks to Bill Freeman, who choreographed and collected several dance 
sequences especially for the purpose of style/content analysis. Egon Pasztor assisted with converting 
motion capture .le formats, and Jonathan Yedidia helped to de.ne the joint angle parameterization.  
References [1] L. Baum. An inequality and associated maximization technique in statistical estimation 
of probabilistic functions of Markov processes. Inequalities, 3:1 8, 1972. [2] M. Brand. Pattern discovery 
via entropy minimization. In D. Heckerman and C. Whittaker, editors, Arti.cial Intelligence and Statistics 
#7. Morgan Kauf­mann., January 1999. [3] M. Brand. Exploring variational structure by cross-entropy optimization. 
In P. Langley, editor, Proceedings, International Conference on Machine Learn­ing, 2000. [4] M. Brand. 
Voice puppetry. Proceedings of SIGGRAPH 99, pages 21 28, Au­gust 1999. [5] M. Brand. Shadow puppetry. 
Proceedings of ICCV 99, September 1999. [6] C. Bregler. Learning and recognizing human dynamics in video 
sequences. Proceedings of CVPR 97, 1997. [7] A. Bruderlin and L. Williams. Motion signal processing. 
Proceedings of SIG-GRAPH 95, pages 97 104, August 1995. [8] J. Buhmann. Empirical risk approximation: 
An induction principle for unsu­pervised learning. Technical Report IAI-TR-98-3, Institut f¨ ur Informatik 
III, Universit¨at Bonn. 1998., 1998. [9] R. M. Corless, G. H. Gonnet, D. E. G. Hare, D. J. Jeffrey, and 
D. E. Knuth. On the Lambert W function. Advances in Computational Mathematics, 5:329 359, 1996. [10] 
W. T. Freeman and J. B. Tenenbaum. Learning bilinear models for two-factor problems in vision. In Proceedings, 
Conf. on Computer Vision and Pattern Recognition, pages 554 560, San Juan, PR, 1997. [11] M. Gleicher. 
Motion editing with spacetime constraints. 1997 Symposium on Interactive 3D Graphics, pages 139 148, 
April 1997. [12] M. Gleicher. Retargeting motion to new characters. Proceedings of SIGGRAPH 98, pages 
33 42, July 1998. [13] R. Grzeszczuk, D. Terzopoulos, and G. Hinton. Neuroanimator: Fast neural network 
emulation and control of physics-based models. Proceedings of SIG-GRAPH 98, pages 9 20, July 1998. [14] 
N. R. Howe, M. E. Leventon, and W. T. Freeman. Bayesian reconstruction of 3d human motion from single-camera 
video. In S. Solla, T. Leend, and K. Muller, editors, Advances in Neural Information Processing Systems, 
volume 10. MIT Press, 2000. [15] J. Lee and S. Y. Shin. A hierarchical approach to interactive motion 
editing for human-like .gures. Proceedings of SIGGRAPH 99, pages 39 48, August 1999. [16] Z. Popovi´c 
and A. Witkin. Physically based motion transformation. Proceed­ings of SIGGRAPH 99, pages 11 20, August 
1999. [17] L. R. Rabiner. A tutorial on hidden Markov models and selected applications in speech recognition. 
Proceedings of the IEEE, 77(2):257 286, Feb. 1989. [18] C. Rose, M. F. Cohen, and B. Bodenheimer. Verbs 
and adverbs: Multidi­mensional motion interpolation. IEEE Computer Graphics &#38; Applications, 18(5):32 
40, September -October 1998. [19] J. B. Tenenbaum and W. T. Freeman. Separating style and content. In 
M. Mozer, M. Jordan, and T. Petsche, editors, Advances in Neural Information Processing Systems, volume 
9, pages 662 668. MIT Press, 1997. [20] M. Unuma, K. Anjyo, and R. Takeuchi. Fourier principles for emotion-based 
human .gure animation. Proceedings of SIGGRAPH 95, pages 91 96, August 1995. [21] A. Wilson and A. Bobick. 
Parametric hidden markov models for gesture recog­nition. IEEE Trans. Pattern Analysis and Machine Intelligence, 
21(9), 1999. [22] A. Witkin and Z. Popovic.´Motion warping. Proceedings of SIGGRAPH 95, pages 105 108, 
August 1995. [23] S. C. Zhu, Y. Wu, and D. Mumford. Minimax entropy principle and its appli­cations to 
texture modeling. Neural Computation, 9(8), 1997.  time Figure 7: Graphical models of an HMM (top), 
SHMM (middle), and path map (bottom). The observed signal m- is explained by a discrete-valued hidden 
state variable ly g) which changes over time, and in the case of SHMMs, a vector-valued style variable 
v . Both v and ly g) are hidden and must be inferred probabilistically. Arcs in­dicate conditional dependencies 
between the variables, which take the form of parameterized compatibility functions. In this paper we 
give rules for learning (inferring all the parameters associated with the arcs), analysis (inferring 
v and ly g) ), and synthesis of novel but consistent behaviors (inferring a most likely Y for arbitrary 
settings of v and ly g) ).  A Hidden Markov models An HMM is a probability distribution over time-series. 
Its de­pendency structure is diagrammed in .gure 7. It is speci.ed by pone ncw yPfjw yPcjtwfjwPolftg 
mW)t} where nne dw w, } is the set of discrete states; yPcjtwf stochastic matrix gives the probability 
of transitioning from state j to statei ; yPf stochastic vector is the probability of a sequence beginning 
in state i ; olftgmW) emission probability is the probability of observ­ olfrgmo)n ing m while in state 
i , typically a Gaussian wjTnffntday( ai G)y TcaGy( ai G)y/r2 gmW ;Jf) gP oi)y ( TKf TKf with mean J 
and covariance . We cover HMM essentials here; see [17] for a more detailed tutorial. It is useful to 
think of a (continuous-valued) time-series X as a path through con.guration space. An HMM is a state-space 
model, meaning that it divides this con.guration space into re­gions,s each of which is more or less 
owned by a particular hid­den state, according to its emission probability distribution. The new w likelihood 
of a path Xm dwmy2m bb} with respect to par­ ne w w w ticular sequence of hidden states 2y)y2r)yb )} 
is the probability of each point on the path with respect to the cur­rent hidden state ( b ggo a(h gf;gm 
;) ), times the probability of the state sequence itself, which is the product of all its state transitions 
y a(hf b gg 2y a(h fotwa(h gf ( a ). When this is summed over all possible hidden state sequences, one 
obtains the likelihood of the path with respect to the entire HMM: bo gpny a(hf)o a(hfjg m) gg 2 y a(h 
afgtwa(h gfyo a(h gfjgm ;) X) (2) A maximum-likelihood HMM may be estimated from data X via alternating 
steps of Expectation computing a distribution over the hidden states and maximization computing locally 
optimal pa­rameter values with respect to that distribution. The E-step contains a dynamic programming 
recursion for eqn. 2 that saves the trouble nb of summing over the exponential number of state sequences 
in : o gpn f X) b (3) a  a ( f n olftg c yPcjtwffWnyPfqyPftg m ;) aa a m) (4) is called the forward 
variable; a similar recursion gives the back­ward variable : ( f n c,olc gyPfytwc " g m g )b f n (5) 
w In the E-step the variables are used to calculate the expected neesawo suf.cient statistics } that 
form the basis of new param­eter estimates. These statistics tally the expected number of times the HMM 
transitioned from one state to another tcjtwfn bc yPcjtwf olftg ( fP eyngpw a m ;)X) (6) gg 2a f and 
the probability that the HMM was in hidden statewhen ob­serving datapoint m fn f a ( f( ( f ( a ( f( 
( (7) These statistics are optimal with respect to all the information in the entire sequence and in 
the model, due to the forward and back­ward recursions. In the M-step, one calculates maximum likelihood 
parameter estimates which are are simply normalizations of : yPfytwcn tfytwc tfytwc (8) f f fnJ ( 
m ( (9) y fn ( frg fg f f m y J)m y J) T ( (10) After training, eqns. 9 and 10 can be used to remap 
the model to any synchronized time-series. In y D we replace these with more powerful entropy-optimizing 
estimates. B Stylistic hidden Markov models A stylistic hidden Markov model (SHMM) is an HMM whose pa­rameters 
are functionally dependent on a style variable v (see .g­ure 7). For simplicity of exposition, here we 
will only develop olftg the case where the emission probability functions m ;) are Gaus­sians whose means 
and covariances are varied by v . In that case pne ncw yPfjw yPcjtwfjwfwyfjwtmIfjw ah the SHMM is speci.ed 
by Jfjwvn} where f yf mean vector J , covariance matrix , variation matrices mIfjw ah f and style vector 
v parameterize the multivariate olftg Gaussian probability m ;) of observing a datapoint m while in state 
i : o gm fngf mcfwyf h f )m ; jJvv-) yf h f where the stylized covariance matrix v is kept pos­itive 
de.nite by mapping its eigenvalues to their absolute val­ues (if necesary). e yPfjw yPcjtwfwfwyf The 
parameters eemcfjw ah Jf} are obtained from data via en­tropic estimation; } are the dominant eigenvectors 
ob­tained in the post-training PCA of the style-speci.c models; and v can be estimated from data and/or 
varied by the user. If we .x the value of v , then the model becomes a standard discrete-state, n Gaussian-output 
HMM. We call the v case the generic HMM. A simpler version of this model has been treated before in a 
supervised context by [21]; in their work, only the means vary, and one must specify by hand the structure 
of the model s transi­ yPcjtwf tion function , the number of dimensions of stylistic variation dim(v 
), and the value of v for every training sequence. Our frame­work learns all of this automatically without 
supervision, and gen­eralizes to a wide variety of graphical models. C Entropies and cross-entropies 
The .rst two terms of our objective function (eqn. 1) are essentially the likelihood function, which 
measures the .t of the model to the data: ggpngpnp n) wI ) -£ X) -llo g yngX) (11) The remaining terms 
measure the .t of the model to our beliefs. Their precise forms are derived from the likelihood function. 
For pon mLe dw m e e kL w multinomials with parameters } , g(pn  f L f L fjw ) llo g (12) g(p 8 pn f 
8 g L8 f L f I ) llo g ) (13)y For d -dimensional Gaussians of mean J , and covariance , g(pn" yw ) 1[d 
llo g oi tllo gll (14) g(p 8 pn"yy 8 f cgyaf c g gy 8 af c I ) 1[llo g -llo g ))) g 8 yag 8 Jr J)TJr 
J) -d (15) The SHMM likelihood function is composed of multinomials and Gaussians by multiplication 
(for any particular setting of the hid­den states). When working with such composite distributions, we 
optimize the sum of the components entropies, which gives us a measure of model coding length, and typically 
bounds the (usually uncalculable) entropy of the composite model. As entropy declines the bounds become 
tight. -log posterior Optimization by continuation Figure8: TOP:Expectationmaximization.ndsalocaloptimumby 
repeatedly constructing a convex bound that touches the objective function at the current parameter estimate 
(E-step; blue), then cal­culating optimal parameter settings in the bound (M-step; red). In this .gure 
the objective function is shown as energy = -log poste­rior probability; the optimum is the lowest point 
on the curve. BOT-TOM: Annealing adds a probabilistic guarantee of .nding a global optimum, by de.ning 
a smooth blend between model-.tting the hard optimization problem symbolized by the foremost curve and 
maximizing entropy an easy problem represented by the hindmost curve then tracking the optimum across 
the blend. D Estimators The optimal Gaussian parameter settings for minimizing cross­entropy vis-`a-vis 
datapoints m and a reference Gaussian parame­ 8 fy 8 terized by mean J and covariance are +-Z8 n DN m 
DJ J +Z, (16) D ja'j)T ++Zaja f.jXaja-f.j)T +j fZkD X( ND jZa'jX( ZX(XZZfZf (17) ++++Z7 The optimal 
multinomial parameter settings vis-`a-vis event counts and reference multinomial distribution parameterized 
by proba­bilitiesrf are given by the .xpoint jD D++Z D fj D ZDk e X( D+ P +j + l + ) ZK G, (18) D D++-Z 
D fj Zk X( D++1 l D D++ D (19) D, D j l W l x k++Z where W is de.nedX [9]. The factors , vary the strength 
of the entropy and cross-entropy priors in anneal­ing. Derivations will appear in a technical report 
available from http://www.merl.com. These estimators comprise the maximization step illustrated in .gure 
8. E Path maps A path map is a statistical model that supports predictions about the time-series behavior 
of a target system from observations of a cue system. A path map is essentially two HMMs that share a 
backbone of hidden states whose transition function is derived from the target system (see .gure 7). 
The output HMM is characterized by per-state Gaussian emission distributions over both target con.gurations 
and velocities. Given a path through cue con.guration space, one calcu­lates a distribution p over the 
hidden states as in m A, eqn. 7. From this distribution one calculates an optimal path through target 
con­.guration space using the equations inm F. F Synthesis Here we reprise and improve on Brand s [4] 
solution for likeliest motion sequence given a matrix of state occupancy probabilities p . As the entropy 
ofp declines, the distribution over all possible motion sequences becomes l.l.k K a a . a  a )Y . 
. P ... l .X(Y .pj, (20) where the vector yiD Bk7uyBjaD,XyBjyBK GjjaeD ]T is the target position and 
velocity at time . minus the mean of state , andp is a constant. The means aD and covariances fD are 
from the synthesizing model s set of Gaussian emission probabilities a nXyB,yeBjk g7XuyB,yBjyBK G] auaa 
n,aea n],fa nj . Break­ P ( ( ( ( fK Gk B B ing each inverse covariance into four submatrices D7.B.. 
B , we can obtain the maximum likelihood trajectory Y-j (most likely motion sequence) by solving the 
weighted system of linear equa­tions j + BK G DX -D+ DjTtBK G P B K Gn D D+ BK G DX D+-D++BD++ Djm++ 
B( D DtB j + B( DXBD+ DjtBYG k+ BK G D Dj + B( D D, (21) Dk7uBD D]u D D]T Dk7uDD]u D D]T + D where, 
-and the endpoints are obtained by dropping the appropriate terms from equation 21: DTt D+D GjBDj Dt 
Gkj +D G D (22) DDT DDj-j t K GkD D + D+-D+ BD+ Dt + (23) This generalizes Brand s geodesic [4] to use 
all the information in the occupancy matrix p , rather than just a state sequence. ka The least-squares 
solution of this oY system can be cal­culated in sX sj time becauseo is block-tridiagonal. We introduce 
a further improvement in synthesis: If we set Y to the training data in eqn. 21, then we can solve for 
the set of Gaussian ktan means naG,,7e7e7l that minimizes the weighted squared­error in reconstructing 
that data from its state sequence. To do so, kakqnq we factor the r.h.s. of eqn. 21 intooY where is an 
indicator matrix built from the sequence of most probable states. koYqK G The solution forn via the calculation 
n tends to be of enormous dimension and ill-conditioned, so we precondition to kK G make the problem 
well-behaved:nX(QTYjX(QTQj , where qK G Qko . One caution: There is a tension between perfectly .t­ting 
the training data and generalizing well to new problems; unless we have a very large amount of data, 
the minimum-entropy setting of the means will do better than the minimum squared error setting.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344866</article_id>
		<sort_key>193</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>20</seq_no>
		<title><![CDATA[Timewarp rigid body simulation]]></title>
		<page_from>193</page_from>
		<page_to>200</page_to>
		<doi_number>10.1145/344779.344866</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344866</url>
		<abstract>
			<par><![CDATA[<p>The traditional high-level algorithms for rigid body simulation work well for moderate numbers of bodies but scale poorly to systems of hundreds or more moving, interacting bodies. The problem is unnecessary synchronization implicit in these methods. Jefferson's <italic>timewarp</italic> algorithm [22] is a technique for alleviating this problem in parallel discrete event simulation. Rigid body dynamics, though a continuous process, exhibits many aspects of a discrete one. With modification, the timewarp algorithm can be used in a uniprocessor rigid body simulator to give substantial performance improvements for simulations with large numbers of bodies. This paper describes the limitations of the traditional high-level simulation algorithms, introduces Jefferson's algorithm, and extends and optimizes it for the rigid body case. It addresses issues particular to rigid body simulation, such as collision detection and contact group management, and describes how to incorporate these into the timewarp framework. Quantitative experimental results indicate that the timewarp algorithm offers significant performance improvements over traditional high-level rigid body simulation algorithms, when applied to systems with hundreds of bodies. It also helps pave the way to parallel implementations, as the paper discusses.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[animation]]></kw>
			<kw><![CDATA[physics based modeling]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Continuous</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Discrete event</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10010354</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Discrete-event simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10010357</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Continuous simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010370</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation evaluation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P32838</person_id>
				<author_profile_id><![CDATA[81100055289]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mirtich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MERL, A Mitsubishi Electric Research Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>280821</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[D. Baraffand A. Witkin. Large Steps in Cloth Simulation. In Michael Cohen, editor, SIGGRAPH 98 Conference Proceedings, Annual Conference Series, pages 43-54. ACM SIGGRAPH, Addison Wesley, July 1998.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97881</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[David Baraff. Curved Surfaces and Coherence for Non-Penetrating Rigid Body Simulation. In Computer Graphics (SIGGRAPH 90 Conference Proceedings), volume 24, pages 19-28. August 1990.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>165682</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[David Baraff. Dynamic Simulation of Non-Penetrating Rigid Bodies. Ph.D. thesis, Department of Computer Science, Cornell University, March 1992.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192168</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[David Baraff. Fast Contact Force Computation for Nonpenetrating Rigid Bodies. In SIGGRAPH 94 Conference Proceedings, Annual Conference Series, pages 23-34. ACM SIGGRAPH, Addison Wesley, 1994.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618272</ref_obj_id>
				<ref_obj_pid>616036</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[David Baraff. Interactive Simulation of Solid Rigid Bodies. IEEE Computer Graphics and Applications, 15(3):63-75, May 1995.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378509</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Ronen Barzel and Alan H. Barr. A Modeling System Based on Dynamic Constraints. In Computer Graphics (SIGGRAPH 88 Conference Proceedings), volume 22, pages 179-188. August 1988.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>314435</ref_obj_id>
				<ref_obj_pid>314161</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[J. Basch, L.J. Guibas, and J. Hershberger. Data Structures for Mobile Data. In Proceedings of 8th Symposium on Discrete Algorithms. 1997. To appear in J. of Algorithms.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Raymond M. Brach. Mechanical impact Dynamics; Rigid Body Collisions. John Wiley &amp; Sons, Inc., 1991.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618563</ref_obj_id>
				<ref_obj_pid>616054</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[David C. Brogan, Ronald A. Metoyer, and Jessica K. Hodgins. Dynamically Simulated Characters in Virtual Environments. IEEE Computer Graphics and Applications, 18(5):58-69, September 1998.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Stephen Cameron. Enhancing GJK: Computing Minimum Penetration Distances between Convex Polyhedra. In Proceedings of international Conference on Robotics and Automation. IEEE, April 1997.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>266777</ref_obj_id>
				<ref_obj_pid>266774</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Deborah A. Carlson and Jessica K. Hodgins. Simulation Levels of Detail for Real-time Animation. In Proc. of Graphics interface '97, pages 1-8. 1997.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Anindya Chatterjee and Andy Ruina. A New Algebraic Rigid Body Collision Law Based on Impulse Space Considerations. Journal of Applied Mechanics, 65:939-951, December 1998.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618608</ref_obj_id>
				<ref_obj_pid>616057</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Stephen Chenney, Jeffrey Ichnowski, and David Forsyth. Dynamics Modeling and Culling. IEEE Computer Graphics and Applications, 19(2):79-87, March/April 1999.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>199437</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Jonathan D. Cohen, Ming C. Lin, Dinesh Manocha, and Madhav K. Ponamgi. I-COLLIDE: An Interactive and Exact Collision Detection System for Large- Scaled Environments. In Symposium on interactive 3D Graphics, pages 189- 196. ACM SIGGRAPH, April 1995.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[J. Dongarra, A. Lumsdaine, R. Pozo, and K. Remington. A Sparse Matrix Library in C++ for High Performance Architectures. In Proceedings of the Second Object Oriented Numerics Conference, pages 214-218. 1992. www. math. nis t. gov/iml + +.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[R. Featherstone. The Calculation of Robot Dynamics Using Articulated-Body Inertias. international Journal of Robotics Research, 2(1): 13-30, 1983.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237244</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[S. Gottschalk, M. C. Lin, and D. Manocha. OBB-Tree: A Hierarchical Structure for Rapid Interference Detection. In Holly Rushmeier, editor, SIGGRAPH 96 Conference Proceedings, Annual Conference Series. ACM SIGGRAPH, Addison Wesley, August 1996.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97883</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Brian Von Herzen, Alan H. Barr, and Harold R. Zatz. Geometric Collisions for Time-Dependent Parametric Surfaces. In Computer Graphics (SIGGRAPH 90 Conference Proceedings), pages 39-48.1990.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258822</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Jessica K. Hodgins and Nancy S. Pollard. Adapting Simulated Behaviors for New Characters. In Turner Whitted, editor, SIGGRAPH 97 Conference Proceedings, Annual Conference Series, pages 153-162. ACM SIGGRAPH, Addison Wesley, August 1997.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218414</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Jessica K. Hodgins, Wayne L. Wooten, David C. Brogan, and James F. O'Brien. Animating Human Athletics. In SIGGRAPH 95 Conference Proceedings, Annual Conference Series, pages 71-78. ACM SIGGRAPH, Addison Wesley, 1956.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>231732</ref_obj_id>
				<ref_obj_pid>231731</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Philip M. Hubbard. Approximating Polyhedra with Spheres for Time-Critical Collision Detection. ACM Transactions on Graphics, 15(3), July 1996.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>3988</ref_obj_id>
				<ref_obj_pid>3916</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[David R. Jefferson. Virtual Time. ACM Transactions on Programming Languages and Systems, 7(3):404-425, July 1985.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[V.V. Kamat. A Survey of Techniques for simulation of Dynamic Dynamic Collision Detection and Response. Computer Graphics in india, 17(4):379-385, 1993.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>530219</ref_obj_id>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Kathryn W. Lilly. Efficient Dynamic Simulation of Robotic Mechanisms. Kluwer Academic Publishers, Norwell, 1993.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237222</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Victor J. Milenkovic. Position-Based Physics: Simulating the Motion of Many Highly Interacting Spheres and Polyhedra. In Holly Rushmeier, editor, SIG- GRAPH 96 Conference Proceedings, Annual Conference Series, pages 129-136. ACM SIGGRAPH, Addison Wesley, August 1996.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>924581</ref_obj_id>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Brian Mirtich. impulse-based Dynamic Simulation of Rigid Body Systems. Ph.D. thesis, University of California, Berkeley, December 1996.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>285860</ref_obj_id>
				<ref_obj_pid>285857</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Brian Mirtich. V-Clip: Fast and Robust Polyhedral Collision Detection. ACM Transactions on Graphics, 17(3):177-208, July 1998. Mitsubishi Electric Research Lab Technical Report TR97-05.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166160</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[J. Thomas Ngo and Joe Marks. Spacetime Constraints Revisited. In SIGGRAPH 93 Conference Proceedings, Annual Conference Series, pages 343-350. ACM SIGGRAPH, Addison Wesley, 1993.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>152574</ref_obj_id>
				<ref_obj_pid>152566</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[M. Overmars. Point Location in Fat Subdivisions. information Processing Letters, 44:261-265, 1992.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>148286</ref_obj_id>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian R. Flannery. Numerical Recipes in C: The Art of Scientific Computing. Cambridge University Press, Cambridge, second edition, 1992.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Edward J. Routh. Elementary Rigid Dynamics. Macmillan, London, 1905.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192167</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Karl Sims. Evolving Virtual Creatures. In SIGGRAPH 94 Conference Proceedings, Annual Conference Series, pages 15-22. ACM SIGGRAPH, 1994.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166158</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[John M. Snyder, Adam R. Woodbury, Kurt Fleischer, Bena Currin, and Alan H. BAIT. Interval Methods for Multi-Point Collisions between Time-Dependent Curved Surfaces. In SIGGRAPH 93 Conference Proceedings, Annual Conference Series, pages 321-333. ACM SIGGRAPH, Addison Wesley, 1993.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Peng Song, Peter R. Kraus, Vijay Kumar, and Pierre Dupont. Analysis of Rigid Body Dynamic Models for Simulation of Systems with Frictional Contacts, June 1999. Submitted to ASME Journal of Applied Mechanics.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[D.E. Stewart and J.C. Trinkle. An Implicit Time-Stepping Scheme for Rigid Body Dynamics with Inelastic Collisions and Coulomb Friction. international Journal of Numerical Methods in Engineering, 39:2673-2691, 1996.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[J.C. Trinkle, J.S. Pang, S. Sudarsky, and G. Lo. On Dynamic Multi-Rigid-Body Contact Problems with Coulomb Friction. Zeitschriftfur Angewandte Mathematik und Mechanik, 77(4):267-279, 1997.]]></ref_text>
				<ref_id>36</ref_id>
			</ref>
			<ref>
				<ref_obj_id>91400</ref_obj_id>
				<ref_obj_pid>91394</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Andrew Witkin, Michael Gleicher, and William Welch. Interactive Dynamics. Computer Graphics, 24(2): 11-22, March 1990.]]></ref_text>
				<ref_id>37</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Timewarp Rigid Body Simulation  Brian Mirtich MERL -A Mitsubishi Electric Research Lab  Figure 1: 
Avalanche: 300 rocks tumble down a mountainside. Abstract The traditional high-level algorithms for rigid 
body simulation work well for moderate numbers of bodies but scale poorly to sys­tems of hundreds or 
more moving, interacting bodies. The problem is unnecessary synchronization implicit in these methods. 
Jeffer­son s timewarp algorithm [22] is a technique for alleviating this problem in parallel discrete 
event simulation. Rigid body dynamics, though a continuous process, exhibits many aspects of a discrete 
one. With modi.cation, the timewarp algorithm can be used in a uniprocessor rigid body simulator to give 
substantial performance improvements for simulations with large numbers of bodies. This paper describes 
the limitations of the traditional high-level simula­tion algorithms, introduces Jefferson s algorithm, 
and extends and optimizes it for the rigid body case. It addresses issues particular to rigid body simulation, 
such as collision detection and contact group management, and describes how to incorporate these into 
the time­warp framework. Quantitative experimental results indicate that the timewarp algorithm offers 
signi.cant performance improvements over traditional high-level rigid body simulation algorithms, when 
applied to systems with hundreds of bodies. It also helps pave the way to parallel implementations, as 
the paper discusses. CR Categories: I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism 
Animation; I.6.8 [Simulation and Modeling]: Types of Simulation Continuous, Discrete Event, Animation 
Keywords: Physics Based Modeling, Animation.  1 Introduction Today rigid body simulation is a mature 
technology. The major components have been well studied and made practical: fast, ro­ mirtich@merl.com 
 Permission to make digital or hard copies of part or all of this work or personal or classroom use is 
granted without fee provided that copies are not made or distributed for profit or commercial advantage 
and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, 
to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. SIGGRAPH 
2000, New Orleans, LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 bust collision detection algorithms 
[10, 17, 21, 27]; impact models of varying accuracy [8, 12, 31]; methods to enforce general motion constraints 
[6, 37], especially the ubiquitous non-penetration con­straints [3, 4, 35, 36]; and control strategies 
for articulated bodies [19, 20, 28, 32]. Thus rigid body simulation is available in many an­imation and 
CAD packages and used in computer games. Yet areas for signi.cant improvement remain. An important one 
is increasing the number of moving, interacting bodies that can be simulated. We are concerned with general 
rigid body simulation, meaning that the bodies have nontrivial geometries, all pairs can potentially 
collide, and second-order physics governs the motion. There are numerous techniques to simulate large 
numbers of rigid bodies by relaxing some of these assumptions. Milenkovic ef.ciently simu­lates vast 
numbers of interacting spheres and non-rotating polyhe­dra using linear programming techniques and zeroeth-order 
physics [25]. Carlson and Hodgins use different motion levels of detail, from fully dynamic to fully 
kinematic, to obtain an order of mag­nitude increase in the number of legged creatures that can be simu­lated 
in real time [11]. Chenney et. al. cull dynamics computations for off-screen objects; when they enter 
the .eld of view initial states are computed by sampling a probability distribution over their state 
space [13]. Brogan et. al. simulate large herds of fully dynamic agents in distributed virtual environments, 
but without full collision detection [9]. Despite these excellent techniques, the general case is worth 
pursuing because of its wide applicability; sometimes full collision detection and dynamics cannot be 
avoided. Traditional techniques for the general problem become inef.­cient and even intractable with 
many-bodied systems for one of two reasons. Either the integration steps1 become very small, or the amount 
of work that is wasted because of unpredictable events (like collisions) becomes very large. The problems 
are not in the component algorithms but in the glue holding them together the high-level simulation loop. 
It imposes a synchronization between bodies that is usually unnecessary and wasteful. These problems 
are explored in depth in Section 2. Jefferson s timewarp algorithm [22], discussed in Section 3, is an 
elegant paradigm designed to alleviate similar problems in parallel discrete event simulation by running 
processes as asynchronously as possible. An optimistic, non-interaction assumption prevails, and when 
it is violated only the computation that is provably invalid is undone. Although rigid 1Throughout this 
paper, integration step means the time interval passed to the integrator, not the smaller steps it may 
take internally. body dynamics is a continuous process, it exhibits many traits of a y discreteprocess. 
With some modi.cation, the timewarp algorithm can be used in rigid body simulators, improving both their 
speed and scalability. The method is described in Section 4, and Section 5 presents results from an actual 
implementation. Timewarp rigid body simulation also supports the long-range goal of a highly parallel 
implementation. Rigid body simulation offers unlimited potential for modeling the complex and unantici­pated 
interactions of rich virtual environments, but current technol­ogy cannot support this. Meeting this 
challenge will certainly re­quire a multiprocessor approach, with perhaps hundreds of proces­sors computing 
motion throughout the environment. Such a simu­lation farm is akin to the rendering farms that generate 
today s high quality computer animation. Section 6 touches on these issues. 2 Simulation Discontinuities 
The dominating computation in a rigid body simulator is that of nu­merically integrating the dynamic 
states of bodies forward in time. The differential equations of motion have been known for centuries; 
the true dif.culty lies in processing simulation discontinuities, here de.ned as events that change the 
dynamic states or the equations of motion of some subset of the bodies. Examples include col­lisions, 
new contacts, transitions between rolling and sliding, and control law changes. Integrators cannot blithely 
pass through dis­continuities. Instead the integration must be stopped, the states or equations of motion 
updated, and then the integrator restarted from that point. Compounding this complication is the fact 
that the times of most discontinuities are impossible to predict. Thus the integra­tion must be interrupted 
even more frequently than the rate at which discontinuities occur, just to check if they have occurred. 
There are two common approaches for coping with discontinuities, both of which have been shown practical 
for moderate numbers of bodies. 2.1 Retroactive Detection Retroactive detection (RD) is the most common 
approach to han­dling discontinuities. The simulator takes small steps forward and checks for discontinuities 
after each step [2, 23]. For example, inter-body penetration indicates that a collision occurred at some 
time during the most recent integration step. A root .nding method localizes the exact moment of the 
discontinuity. After resolution, the integration is restarted from that point. All of the bodies must 
be backed up to their states at the time of the discontinuity because (1) the discontinuity may have 
affected their motion, and (2) the bod­ies directly involved in the discontinuity must certainly be backed 
up to this time, and there is no framework for maintaining bodies 2.2 Conservative Advancement Conservative 
advancement (CA) is an alternative to RD based on the idea of never integrating over a discontinuity. 
Conservative lower bounds on the times of discontinuities are maintained in a pri­ority queue sorted 
by time, and the simulator repeatedly advances all simulated bodies to the bound at the front of the 
queue. The sim­ulator tends to creep up to each discontinuity, taking smaller steps as it gets closer. 
Von Herzen et. al. use this approach to detect col­lisions between time-dependent parametric surfaces 
[18], and Mir­tich uses it to support impulse-based simulation [26]. Snyder et. al. use a related approach 
to locate multi-point collisions by using in­terval inclusions to bound surfaces in time and space [33]. 
Finally, CA forms the basis for kinetic data structures pioneered by Basche et. al. [7]. These are used 
to solve a host of problems from dynamic computational geometry, such as maintaining the convex hull 
of a moving point set, by maintaining bounds on when the combinato­rial structure may change. For rigid 
body simulation the advantage of CA is that it does not waste work by integrating bodies beyond a discontinuity. 
Unfortunately, as the number of bodies increases the average time to the next discontinuity check decreases, 
and the problem is exacerbated since it is dif.cult to compute tight bounds on times of collisions and 
contact changes. Stopping the integra­tion of all bodies at each check is very inef.cient, and CA becomes 
intractable with many bodies. 2.3 Step Sizes and Ef.ciency Figure 2 graphically demonstrates the problem 
with small integra­tion steps. It shows the computational cost of computing the 10­second trajectory 
of a ballistic, tumbling brick using a .fth order adaptive Runge-Kutta integrator [30] under various 
step sizes. The two qualitatively similar curves correspond to different integrator error tolerances. 
At small step sizes the integrator does not need to subdivide the integration step into smaller pieces 
to meet the error tolerance. Thus computation is proportional to the number of invo­cations: halving 
the step size doubles the work. At large step sizes the integrator breaks the requested step into smaller 
pieces to meet the error tolerance, so computation is insensitive to step size. Un­fortunately, even 
with a moderate number of bodies, a simulator s operating point is to the left of the elbow in these 
curves. Thus, reducing the step size signi.cantly increases computational cost. Brick Integration Experiment 
1E+08 1E+07 at different times the bodies must be kept synchronized. The .rst problem is avoidable 
by bounding a discontinuity s in.uence. A certain collision may provably have no in.uence on the motion 
of a distant body over the current integration step. However, the sec­ond problem is fundamental to RD. 
It does not suf.ce to maintain states at two different times, the time of the discontinuity and the time 
at the end of the step, because multiple discontinuities can oc­cur at different times in a single step. 
Also, earlier discontinuities computational cost (flops) ' 1E+06 may cause or prevent later ones, and 
it is hard to determine which one occurred .rst without localizing the times of each. In practice, all 
bodies are backed up to the point of each discontinuity. This method is correct since it eventually processes 
all real discontinu­ities and no spurious ones, and Baraff has shown it to be ef.cient and eminently 
practical for moderate numbers of interacting bodies [5]. As the number of bodies increases, so does 
the the rate of dis­continuities, and the wasted work per discontinuity increases since more bodies must 
be backed up. Shrinking the step size to reduce the amount of backup is not a good solution as we shall 
see. Even­tually RD becomes intractable due to the amount of wasted work. 1E+05 1E-04 1E-03 1E-02 (30 
Hz) 1E-01 1E+00 1E+01 integration timestep (seconds) Figure 2: Cost of computing the trajectory of a 
brick versus inte­gration step size (eps is the integrator error tolerance).  3 The Timewarp Algorithm 
The problems of RD and CA result from unnecessary synchroniza­tion. Each discontinuity affects only a 
small fraction of the bodies, yet under RD every body must be backed up when a discontinuity occurs,and 
under CA integration of every body must stop for a dis­ .continuity check. The inef.ciencies are tolerable 
as long as there are not too many bodies. Similar issues arise in discrete event sim­ulation (DES) , 
which is often applied to very large models such as cars on a freeway system. These simulations are often 
done in parallel or distributed settings. The simulated agents are parti­tioned among a number of processors, 
each of which advances its agents forward in time. There are causality relationships that must be preserved 
(e.g. a car suddenly braking causes the car behind it to brake), and the crux of the problem is that 
one agent may trig­ger an action of another agent on a different processor. Obviously communication by 
message passing or other means is needed. Conservative DES protocols guarantee correctness by requiring 
that each processor advance its agents forward to a certain time only when it has provably received all 
relevant events from other processors occurring before that time. Optimistic protocols were a key breakthrough 
in distributed DES. These allow each proces­sor to advance its agents forward in time by assuming all 
relevant events have been received, thereby avoiding idle time. The catch is that when an agent receives 
an event in its past, the agent needs to be returned to the state it was in when the event occurred, 
its own actions since that time must be undone, and the intervening compu­tation is wasted. Jefferson 
was among the .rst to de.ne a provably correct, optimistic synchronization protocol along with a simple, 
elegant implementation called the timewarp mechanism [22]. We now give a brief, simpli.ed description 
of this seminal algorithm. Each process maintains the state of some portion of the mod­eled system. Each 
process also has a local clock measuring local virtual time (LVT) at that process. The local clocks are 
not syn­chronized, and processes communicate only by sending messages. Every message is time stamped2 
with a time not earlier than the sender s LVT but possibly earlier than the receiver s LVT when the message 
is received. Processes must process events in time order to maintain causality constraints. When a received 
message has a timestamp later than the receiver s LVT, it is inserted into an input queue sorted by timestamp. 
A process s basic execution loop is to advance LVT to the time of the .rst event in its input queue, 
remove the event, and process it. Advancing to a new time means creating a new state, and these are queued 
in time order in a state queue. If the .rst event in a process s input queue has a receive time earlier 
than LVT, the process performs a rollback by returning to the latest state in its state queue before 
the exceptional event s time. This becomes the new current state, its time becomes the new LVT, and all 
subsequent states in the queue are deleted. Already pro­cessed events occurring after the new LVT are 
placed back in the input queue. Messages the processor sent to other processes at times after the new 
LVT are unsent via antimessages. When a process sends a message, it adds a corresponding antimessage 
to its output queue. This is a negative copy of the sent message, identical to it except for a .ipped 
sign bit. When a process is rolled back to a new LVT, all antimessages in the output queue later than 
this time are sent. When a message and antimessage are united in a process s input queue, they annihilate 
one another, and the net effect is as if a message were never sent. Rollback is recursive: antimessages 
may trigger rollbacks that generate new antimessages. Global virtual time (GVT) is the minimum of all 
LVTs among the processes and all times of unprocessed messages. It represents a line of commitment during 
the simulation: states earlier than GVT are provably valid while states beyond GVT are subject to rollback. 
Individual LVTs occasionally jump backwards, but GVT monoton­ically increases. Since rollback never goes 
to a point before GVT, each state queue needs only to maintain one state before GVT. Ear­lier states 
as well as saved messages prior to GVT may be deleted. 2Each message actually has two timestamps, a send 
and receive time, but one suf.ces for our purposes.  4 Timewarp Rigid Body Simulation Rigid body simulation 
computes a continuous process but exhibits traits of DES. Bodies communicate through collisions and per­sistent 
contact. Collisions are in fact usually modeled as discrete events. Contact is a continuous phenomenon, 
but it can be viewed as occurring within a collection of bodies rather than between indi­vidual bodies. 
This view facilitates the adaptation of the timewarp algorithm to uniprocessor rigid body simulation. 
The result is a high-level simulation algorithm that does not suffer from the wasted work problem of 
RD nor the small timestep problem of CA. 4.1 Overview First consider a simulation without connected or 
contacting bodies. Each body is a separate timewarp process with a state queue con­taining the dynamic 
state (position and velocity) of the body at the end of each integration step. The times of these states 
are different for different bodies. A global event queue contains events for all simulated bodies; this 
corresponds to a union of all the individual input queues in Jefferson s algorithm. Each event has a 
timestamp and a list of the bodies that receive it. One iteration of the main simulation loop consists 
of removing the event from the front of the event queue, integrating the receiving body or bodies to 
the event time, and then processing the event. Most events are rescheduled after they are processed. 
Our system supports four types of events: 1. Collision check events are received by pairs of bodies, 
causing a collision check to be performed between them at the given time. Processing these events may 
lead to collision resolution. 2. Group check events trigger collision checking between con­tacting bodies 
and also checking for when groups of such bod­ies should be split. They can also lead to collision resolution. 
 3. Redraw events exist for every rendered body. Processing one involves writing the current position 
of the body to a recording buffer. Rescheduling occurs at .xed frame intervals. 4. Callback events are 
received by arbitrary sets of bodies and in­voke user functions written in Scheme that, for example, 
drive control systems. Rescheduling is user-speci.ed.  4.2 Collisions and Rollback If penetration is 
discovered in processing a collision check or group check event, then a collision has occurred at a time 
preceding the time of the event. This may be a normal collision or a soft collision producing a new persistent 
contact. Either way, the colliding bod­ies must be rolled back to the collision time. This behavior differs 
from that of standard timewarp events which only cause rollback up to the time of the event; it occurs 
in rigid body simulation because exact collision times cannot be predicted. To implement collision rollback 
each collision check and group check event has an addi­tional timestamp, a safe time, which is the time 
when the pair or group of bodies was last veri.ed to be disjoint. When a collision check or group check 
event is processed, and there is no penetra­tion, the safe time is updated to the time of the check. 
When pene­tration is detected, the safe time forms a lower bound on the search for the collision time. 
Since rollback never proceeds to a point be­fore the safe time, GVT can be computed as the minimum of 
all LVTs and all event safe times. This insures there are always states to back up to when a collision 
occurs. The antimessage mechanism is more general than what is needed for uniprocessor rigid body simulation. 
Still considering only iso­lated bodies, the only inter-body communication is through colli­sions; a 
suitable record of these drives the rollback. Pairs of cor­responding post-collision states are linked 
together, turning the in­ p dividualstate queues into a dynamic state graph as shown at the top of Figure 
3. The .gure depicts the actions taken when bod­ies Aand Bcollide. Body Ais rolled back by deleting all 
of its states after the post-collision state. (If Balso had such states, a twin rollback operation would 
begin in its own state queue). Some of the deleted states are linked via collisions to states in other 
bod­ies. These inter-body communications are now suspect due to theA-Bcollision, thus rollback proceeds 
across the collision links and then recursively forward through other bodies state queues. Upon completion 
of rollback, all states that were possibly affected by theA-Bcollision and no others are deleted. In 
this example the rollback invalidates a substantial amount of work. It is an unusual case but one the 
simulator must be prepared for. A B frontier D E time GVT A B D E timenew GVT Figure 3: Top: State 
graph of a .ve body simulation. The verti­cal connections link post-collision states. The gray states 
are new post-collision states found while processing an A-Bcollision check event. Bottom: The rollback 
operation triggered by the collision. Crossed states are deleted and represent wasted work, but forward 
progress is indicated by the advancement of GVT. Events must also be rolled back. This corresponds to 
placing messages back in a process s input queue in Jefferson s original al­gorithm. An event needs to 
be rolled back only if it involves a body whose state queue was rolled back to a time earlier than the 
sched­uled time of the event. Event rollback is type-speci.c. Redraw events are simply rescheduled to 
the .rst frame time following the rollback time. Fixed-rate callback events are handled similarly. If 
the rollback time is earlier than the safe time of a collision check or group check event, the event 
is rescheduled to the rollback time. If the rollback time is between the safe time and the scheduled 
event time, the system optimistically assumes no action is necessary. This is a gamble since a collision 
may make the previously computed collision check time inaccurate, but the timewarp algorithm can re­cover 
gracefully from poorly predicted collision times. In total the timewarp algorithm requires little overhead 
and few additional data structures when compared to a conventional simu­lator. Any simulator computes 
sequences of body states; the main change is that these are kept in queues and linked together at the 
collision points. Rollback is implemented with a simple recursive traversal of the state graph. 4.3 
Multibodies Multibodies (or articulated bodies) are collections of rigid bodies connected by joints, 
as in a human .gure. The trajectory of a sin­gle multibody link cannot be determined in isolation; the 
motion of all links must be computed together. Little change is needed to incorporate multibodies into 
the timewarp framework. A single state queue serves for the entire multibody; it is advanced as a unit. 
Most events are still handled on a per rigid body (per link) basis. When, for example, a particular multibody 
link must be integrated to a certain time for a collision check, the whole multibody is inte­grated to 
that time. As a result, states are more densely distributed along multibody state queues than along rigid 
body state queues, especially for multibodies with many links. A collision involving a single link causes 
the whole multibody to be rolled back. Clearly timewarp does not offer much improvement if all of the 
bodies are connected into only a few multibodies. 4.4 Contact Groups Contact groups are collections 
of rigid bodies and multibodies in persistent contact; the component bodies exert continuous forces on 
each other. The components must again be integrated as a unit, but unlike multibodies contact groups 
are .uid: bodies may join or leave groups, and groups are created and destroyed during a sim­ulation. 
Contact groups have no analog in the classical timewarp algorithm, which is designed for a static set 
of processes. Most of the added work in implementing timewarp rigid body simulation is in managing contact 
groups. To impart some order we require that groups comprise a .xed set of bodies; when the set must 
change a new group is created. Groups are created by fusions and .ssions. A fusion is a suitably soft 
collision between two bodies, after which they are considered to remain in contact. Either body may be 
part of a multibody or another group. A .ssion is a splitting of a group into two or more isolated bodies 
or separate (non-contacting) groups. The complexities of contact group evolution are best explained by 
example. The top of Figure 4 shows the state graph for .ve rigid bodies labeled A-Eand the various contact 
groups that exist over the time interval [taoo uta nJ(body Fdoes not have a state queue since it is .xed). 
The bottom of the .gure depicts the physical con.guration at three distinct times. At timetuo, only bodiesBand 
Eare isolated; the others are members of two contact groups, ABFand CtDtF. Only kinematically controlled 
bodies, of which .xed bodies are a special case, may be members of multiple groups at a given time; such 
bodies do not link groups together since their motion and the forces they exert on other bodies are independent 
of the forces exerted on them. Dotted horizontal lines indicate intervals without isolated states since 
the body is part of a group. The .rst change after taois a fusion collision between Aand B, creating 
a new group, A BbF. D and Ethen collide, but this is a standard (non-fusion) collision so E remains isolated 
and CtDtFintact. The D-Ecollision does set Din motion, eventually leading to an A-Dfusion collision. 
This latter collision causes two previously separate groups to fuse into a single one, which is the situation 
at time tal. Next Dbreaks contact with C, triggering the .ssion of ABBbCtDtFinto A BbDtFand CtF. No collision 
occurred here; .ssions can be caused simply by breaking contacts. Still sliding, Dpushes Boff of A, causing 
Bto leave the contact group and return to an isolated state. Finally, Elands and settles onto D, fusing 
into a new group ABDtEeF. A D CDF time B joins fusion of fission of B leaves E joinst0 AF CDF &#38; 
ABF t1 ABCDF ABDF ADF t2 t0 t1 t2 Figure 4: Top: The state graph for a portion of a six body sim­ulation. 
Circles are isolated body states and squares are contact group states. Bottom: The physical con.guration 
of the bodies at three distinct times. Moving bodies in contact groups are colored to match the top part 
of the .gure. See text for details. The state graph in the .gure only shows states relevant to the discussion. 
There would actually be many more states along all of the state queues generated by other events and 
discontinuities. For example there are usually many non-fusion collisions leading up to a fusion collision 
as bodies settle. At any time coordinate each non-kinematic body is isolated or a member of exactly one 
group. Thus there is never ambiguity about what the state of a body is at a given time, or from which 
state to integrate when computing a new state of a body. To compute the state of body Bat time ta, integration 
proceeds from the latest isolated state of Bprior to t. To compute the state of Bat time tml, integration 
proceeds from the latest state of group ABBbCtDtFprior to tml. To facilitate this, the state graph has 
additional pointers not shown in the .gure. A fusion collision points to the new group it creates, if 
any. Also, the last state of every .ssured group points to the newly isolated bodies and subgroups that 
succeed it. These pointers make it possible to .nd for any body Band time tthe latest state of B, possibly 
in a group, prior to t. The search begins within B s own (isolated) state queue and extends into contact 
groups if necessary by following pointers. Sometimes several pointers and contact groups must be traversed 
to .nd the proper prior state. The pointers also facilitate rollback. When a fusion collision state is 
deleted, the rollback proceeds to the new group formed by the collision, if any. When the last state 
of a .ssured group is deleted, rollback proceeds to the isolated body and subgroup states that succeeded 
it. Over the interval shown in Figure 4, six new contact groups are created in addition to the two that 
existed at tao. At taonly two remain. Groups are terminated when they fuse into new groups or when they 
.ssure into pieces. Termination does not mean the group can be deleted since rollback can cause event 
processing in non­temporal order. For example it may be necessary to determine the state of body Bat 
time tmlafter the group ABBbCtDtFis terminated. Once GVT passes the last state in a terminated group, 
however, the group is obsolete and the storage can be reclaimed. A group is also deleted when a rollback 
operation annihilates all of its states. Intra-group collision detection is handled in one of two ways. 
If bodies Aand Bare in the same group but not currently in con­tact, the standard A-Bcollision check 
event triggers collision de­tection between them. Each group has a group check event that performs all 
of the collision detection between already contacting bodies. The distinction is needed since most collision 
time predic­tors do not compute meaningful results when the separation dis­tance is near zero. Instead, 
group check events are scheduled at a .xed, user-speci.ed rate. While collision detection between A and 
Bis being handled by a group check event, the ordinary A-B collision detection event is disabled. Group 
check events are also responsible for detecting .ssions. A graph is constructed in which the group s 
non-kinematic bodies are vertices and contacts are edges. A standard connected component algorithm is 
performed on this graph. Multiple components indicate that the group can be split. There is .exibility 
in the time to split a group. Integrating a group with multiple connected components does not give a 
wrong answer; it is simply inef.cient since smaller groups can be integrated faster than a single combined 
one.  4.5 Collision Checks At any given point in a simulation, collision checking is enabled between 
certain active pairs of bodies, which are hopefully small in number compared to the total number of pairs 
[21]. Every non­contacting active pair requires a collision check event. The bodies state queues provide 
a simple way to keep the number of active pairs small. An axis-aligned bounding box is maintained around 
the set of states currently computed for each rigid body (hence there are multiple boxes for multibodies 
and groups). This swept volume grows as new states are computed; it shrinks when states are deleted as 
GVT moves past them. Using six heaps to maintain the minimum and maximumpa uytand zcoordinates of the 
rigid body at each state, the swept volume over ystates is updated in Od( l oogdyn time. The pairs of 
swept volumes that overlap can be maintained us­ing a hierarchical hash table [29] or by sorting coordinates 
along the three coordinate axes [3, 14]. If the swept volumes of bodiesAand Bdo not overlap, then Aand 
Bare known to be collision free over the interval [GVT,t((Ab aBB )J, where t((Ab eBb is the min min time 
of A s or B s latest state, whichever is earlier. As long as the swept volumes remain disjoint, Aand 
Bare not an active pair. Now suppose integration of Bcauses its swept volume to overlap the previously 
disjoint swept volume of A. To avoid missing col­lisions, a new collision check event for Aand Bis scheduled 
for the time given by the value of t((Ab uBB before Bwas integrated min (Figure 5). The bodies are known 
to be collision free before this point. This new event is in B s past, but the timewarp algorithm can 
accommodate it; if a collision did occur then rollback will rec­tify the situation. The collision check 
event for Aand Bremains active as long as their swept volumes overlap. This method works even though 
the swept volumes exist over different time intervals and may have no states at common times. Inactive 
pairs do not need to be synchronized in order to remain inactive, which avoids costly integration interruptions 
for the vast majority of body pairs. Figure 5: When Bis integrated to the state shown in gray, swept 
volume overlap occurs. Aand Bbecome an active pair, and a col­lision check is scheduled at the earlier 
time of the two red states. simu­lation duration # of rigid bodies # of discont­inuities avg time between 
disconts avg integr n step # of integr ns total integr n / moving total rollback / moving comp time / 
frame simulation (s) moving/total (thousands) (ms) (ms) (millions) body (s) body (s) (s) atoms 120 302 
/ 308 51.9 2.31 6.25 6.04 125 (+4.2%) 0.278 (0.23%) 0.767 cars 60 428 / 524 17.8 3.38 14.9 1.98 69.2 
(+15%) 1.57 (2.6%) 0.904 robots 120 240 / 430 26.8 4.48 9.88 3.00 124 (+3.3%) 1.45 (1.2%) 0.707 avalanche 
45 300 / 824 217 0.208 3.39 5.84 66.0 (+47%) 7.15 (16%) 97.0 Table 1: Data collected over the four simulations. 
 4.6 Callback Functions It is dif.cult to completely hide the underlying timewarp nature of the system 
from user callback functions. Because the bodies LVTs are not synchronized, callback functions involving 
different bodies are not invoked in strict temporal order. In fact, a callback for a single body may 
not be invoked at monotonically increasing times due to rollback. Thus, a collision callback that counts 
a body s collisions by incrementing a global counter is .awed since it may get called with the same collision 
multiple times. One convention that guarantees correct behavior is to forbid callback functions from 
accessing global data. The function should only use the data passed in: the time of the event and the 
states of the relevant bodies at that time. Data that must persist across callback invocations are supported 
by adjoining new slots to the states of bodies. Unlike position and velocity values, the values in these 
slots are simply copied from state to state since there is no need to integrate them, but callback functions 
can access and modify these values. Changes are appropriately undone when the state queues are rolled 
back. The collision counter is implemented correctly by attaching an integer slot to the body state. 
The callback function increments the counter, and rollback may cause the counter to decrease.  5 Results 
We now describe the results of simulating four different systems with a timewarp rigid body simulator 
(Figures 1 and 7). Our imple­mentation draws from a myriad of component algorithms and tech­niques described 
in the literature; Appendix A describes the major ones. Robustness always an issue in rigid body simulation 
is paramount for the kinds of simulations studied here. Anything that can go wrong certainly will when 
simulating large systems over long times. Our implementation favors robustness over ef.ciency. The issues 
are not the underlying components nor the absolute ef­.ciency of this particular implementation but the 
degree to which timewarp improves any implementation s performance. Atoms simulates 200 spheres and 100 
water-like molecules bouncing in a divided box. During the simulation the divider com­presses one compartment 
and lifts to allow the gasses to mix. Cars simulates four multibody vehicles with active wheel velocity 
and steering angle controllers. These drive over a course with speed bumps and an array of 400 spherical 
pendulums. Robots simulates 20 eight-link manipulators that repeatedly pick up boxes and throw them. 
The robots are fully dynamic objects, controlled via joint torques commanded by callback functions. Callbacks 
also use an inverse kinematic model for motion planning. Finally avalanche simulates 300 rigid bodies 
tumbling down a mountainside, creating a vast number of interactions. With the exception of atoms, all 
sim­ulations use realistic values for length, mass, time and earth gravity. Each was generated from a 
single run. 5.1 Full Timewarp Simulation Data Table 1 shows data collected over the course of performing 
the full simulations. The percentages in the total integration and rollback columns are with respect 
to the simulation duration. Computation times were measured on an SGI Onyx (200MHz R10000 CPU). Integration 
and rollback intervals of multibodies and groups were weighted by the number of individual rigid bodies 
involved. The reason that total integration minus rollback exceeds duration is be­cause of the added 
integration involved in localizing discontinu­ities. When a discontinuity is detected over an interval, 
the sim­ulator must compute new states of the relevant bodies in order to localize it. This means re-integrating 
over certain time intervals, increasing the total integration time.3 Worth noting is the amount by which 
the average integration step exceeds the average interval between discontinuities. This of course is 
a key advantage of the timewarp algorithm: integration of a body does not halt at every discontinuity 
but only at the ones which are relevant to it. The fact that the actual integration steps are 2 16 times 
larger than the average interval between discontinuities is especially noteworthy since any simulation 
strategy (RD, CA, or timewarp) must check for discontinuities at a much higher rate than they actually 
occur. In our experiments, checks outnumbered ac­tual discontinuities by two orders of magnitude. Under 
RD or CA, all bodies are halted at every check, although the problem is less severe under RD since collision 
checks are synchronized. Table 1 also shows that rollback is a modest cost. Through judicious undo­ing, 
timewarp avoids the large amount of wasted work inherent in RD as the number of bodies increases. In 
several performance measures, the avalanche simulation is an outlier. The slow simulation speed is not 
because timewarp is not working. The ratio of average integration step to average time between discontinuities 
is quite good, and the total integration per body, while high, is not prohibitive. The main dif.culty 
is the complexity of the contact groups: over 16,000 groups are formed, some having as many as 64 moving 
bodies and 217 simultaneous contacts. Simulating an avalanche using particle or position-based physics 
may be more practical, but the example shows that time­warp can handle even extreme cases well. 5.2 
Comparative Simulation Data Table 1 suggests the timewarp algorithm is a good idea. Further ex­periments 
give a more quantitative measure of the improvement it brings. We added alternate main loops to the simulator 
to let it use RD and CA policies instead of timewarp (TW). The RD algorithm is parameterized by the basic 
timestep to attempt on each iteration; we used values of 0.001, 0.01, and 1/30 second. All .ve algorithms 
were run on a two-second segment of an atoms simulation, with the divider stationary in the middle of 
the box and with the num­ber of bodies varying from 25 to 200. The upper part of Figure 6 shows the average 
integration step taken by the simulator under the various algorithms. The results con.rm the key problem 
with CA: as the number of bodies increases the average time to the next dis­continuity check decreases. 
As Figure 2 shows, the small steps 3Baraff cleverly avoids this waste by using internal values of the 
Runge-Kutta integrator to obtain a polynomial approximation of the state over an integration step for 
free [5]. Average Integration Step number of moving bodies Total Length of Integration Intervals Per 
Body  number of moving bodies Figure 6: Integration statistics for various atoms simulations. have a 
drastic effect on computational cost. RD s average timestep is not as sensitive to the number of bodies 
since it always tries to take a .xed size step forward. TW is also not sensitive to it since the bodies 
are decoupled. The lower portion of the .gure exposes the problem with RD: wasted work. For a two-second, 
200-body simulation and a frame rate timestep, RD integrates each body an average of 30 seconds. This 
is to be compared with TW s value of 2.3 seconds and the modest percentages in the total integration 
column of Table 1. CA never integrates more than two seconds per body since it uses a one-sided approach 
to each discontinuity. Actual execution times shed further light. For 100 atoms, RD­1/30 is the narrow 
winner at 0.142 s/frame, while TW was 0.147 s/frame and CA was 1.15 s/frame. By 200 atoms, TW is clearly 
superior at 0.388 s/frame, while RD-0.01, the fastest RD algorithm, was 2.61 s/frame, and CA was 4.74 
s/frame.  6 Conclusion Timewarp rigid body simulation is clearly able to simulate larger systems with 
more interactions than traditional synchronized sim­ulation algorithms. The most obvious avenues for 
future research involve parallel rigid body simulation. Timewarp simulation helps pave the way to this 
goal since the individual bodies are evolved asynchronously. If the algorithm runs on multiple processors, 
de­lays due to communication latencies are handled in the same way as bad predictions of discontinuity 
times: with minimal rollback. The simplest way to structure a parallel simulator would be to have one 
master processor that repeatedly sends integration tasks to a bevy of slave processors. All of the global 
data structures could be kept on the master processor, requiring little change in the algo­rithms presented 
here. This could signi.cantly boost performance over the uniprocessor case but suffers from a bottleneck 
at the mas­ter. An egalitarian approach in which bodies are distributed among processors is ultimately 
more scalable. Important open questions are how to parcel the bodies among processors and how to balance 
workloads. At odds are the goals of minimizing inter-processor communication by keeping bodies in the 
same spatial region on a common processor and minimizing idle time by shifting bodies to idle processors. 
At any rate some method and strategy for migrating the bodies between processors seems appropriate. Rollback 
proba­bly requires a full antimessage mechanism since the state graph is likely to be distributed. Other 
questions surround how and where to store data structures like the spatial hash table, which is frequently 
accessed by all bodies. Events involving multiple bodies might be redundantly stored and processed on 
multiple processors or on only one of the relevant processors. Finally, there are various protocol choices 
for passing state information between processors. Clearly there are many challenges to building a large 
simulation farm. Yet the prospect of rich virtual environments built on a physics-based substrate is 
adequate motivation to pursue them. A Implementation Details Our system is implemented in C++. All geometries 
are modeled as convex polyhedra or unions thereof. The v-clip algorithm [27] is used for narrow-phase 
collision detection; a hierarchical spatial hash table [26, 29] containing axes-aligned bounding boxes 
is used for the broad phase. For nearby bodies not in contact, times to im­pact are estimated from current 
positions and velocities as in [26], but the predictions are not conservative. Persistent contact is 
mod­eled using a penalty force method; spring and damper constants are speci.ed per body pair. Inspired 
by [1] we use an implicit in­tegrator (4th order Rosenbrock [30]) with a sparse solver [15] to handle 
stiffness induced by the penalty method. This is only nec­essary for contact groups; isolated bodies 
are integrated with a 5th order Runge-Kutta integrator [30]. We use a smooth nonlinear fric­tion law, 
Irga!oIr u=ttn one n( vogu!rco[34]; static friction is not modeled. Reduced coordinates are used for 
multibodies, with dynamics com­puted by a generalized Featherstone algorithm [16] in the isolated case 
and by the spatial composite-rigid-body algorithm [24] within contact groups. The latter is more suited 
to generating the acceler­ation Jacobian required by the implicit integrator.  References [1] D. Baraff 
and A. Witkin. Large Steps in Cloth Simulation. In Michael Cohen, ed­itor, SIGGRAPH 98 Conference Proceedings, 
Annual Conference Series, pages 43 54. ACM SIGGRAPH, Addison Wesley, July 1998. [2] David Baraff. Curved 
Surfaces and Coherence for Non-Penetrating Rigid Body Simulation. In Computer Graphics (SIGGRAPH 90 Conference 
Proceedings), volume 24, pages 19 28. August 1990. [3] David Baraff. Dynamic Simulation of Non-Penetrating 
Rigid Bodies. Ph.D. thesis, Department of Computer Science, Cornell University, March 1992. [4] David 
Baraff. Fast Contact Force Computation for Nonpenetrating Rigid Bodies. In SIGGRAPH 94 Conference Proceedings, 
Annual Conference Series, pages 23 34. ACM SIGGRAPH, Addison Wesley, 1994. [5] David Baraff. Interactive 
Simulation of Solid Rigid Bodies. IEEE Computer Graphics and Applications, 15(3):63 75, May 1995. [6] 
Ronen Barzel and Alan H. Barr. A Modeling System Based on Dynamic Con­straints. In Computer Graphics 
(SIGGRAPH 88 Conference Proceedings), vol­ume 22, pages 179 188. August 1988. [7] J. Basch, L.J. Guibas, 
and J. Hershberger. Data Structures for Mobile Data. In Proceedings of 8th Symposium on Discrete Algorithms. 
1997. To appear in J. of Algorithms. [8] Raymond M. Brach. Mechanical Impact Dynamics; Rigid Body Collisions. 
John Wiley &#38; Sons, Inc., 1991. [9] David C. Brogan, Ronald A. Metoyer, and Jessica K. Hodgins. Dynamically 
Simulated Characters in Virtual Environments. IEEE Computer Graphics and Applications, 18(5):58 69, September 
1998. [10] Stephen Cameron. Enhancing GJK: Computing Minimum Penetration Distances between Convex Polyhedra. 
In Proceedings of International Conference on Robotics and Automation. IEEE, April 1997. [11] Deborah 
A. Carlson and Jessica K. Hodgins. Simulation Levels of Detail for Real-time Animation. In Proc. of Graphics 
Interface 97, pages 1 8. 1997.  Figure 7: Left to Right: snapshots from the atoms, cars and robots simulations 
(thanks to Larry Gritz for Blue Moon Rendering Tools). [12] Anindya Chatterjee and Andy Ruina. A New 
Algebraic Rigid Body Collision Law Based on Impulse Space Considerations. Journal of Applied Mechanics, 
65:939 951, December 1998. [13] Stephen Chenney, Jeffrey Ichnowski, and David Forsyth. Dynamics Model­ing 
and Culling. IEEE Computer Graphics and Applications, 19(2):79 87, March/April 1999. [14] Jonathan D. 
Cohen, Ming C. Lin, Dinesh Manocha, and Madhav K. Ponamgi. I-COLLIDE: An Interactive and Exact Collision 
Detection System for Large-Scaled Environments. In Symposium on Interactive 3D Graphics, pages 189 196. 
ACM SIGGRAPH, April 1995. [15] J. Dongarra, A. Lumsdaine, R. Pozo, and K. Remington. A Sparse Ma­trix 
Library in C++ for High Performance Architectures. In Proceedings of the Second Object Oriented Numerics 
Conference, pages 214 218. 1992. www.math.nist.gov/iml++. [16] R. Featherstone. The Calculation of Robot 
Dynamics Using Articulated-Body Inertias. International Journal of Robotics Research, 2(1):13 30, 1983. 
[17] S. Gottschalk, M. C. Lin, and D. Manocha. OBB-Tree: A Hierarchical Structure for Rapid Interference 
Detection. In Holly Rushmeier, editor, SIGGRAPH 96 Conference Proceedings, Annual Conference Series. 
ACM SIGGRAPH, Addi­son Wesley, August 1996. [18] Brian Von Herzen, Alan H. Barr, and Harold R. Zatz. 
Geometric Collisions for Time-Dependent Parametric Surfaces. In Computer Graphics (SIGGRAPH 90 Conference 
Proceedings), pages 39 48. 1990. [19] Jessica K. Hodgins and Nancy S. Pollard. Adapting Simulated Behaviors 
for New Characters. In Turner Whitted, editor, SIGGRAPH 97 Conference Proceed­ings, Annual Conference 
Series, pages 153 162. ACM SIGGRAPH, Addison Wesley, August 1997. [20] Jessica K. Hodgins, Wayne L. Wooten, 
David C. Brogan, and James F. O Brien. Animating Human Athletics. In SIGGRAPH 95 Conference Proceedings, 
An­nual Conference Series, pages 71 78. ACM SIGGRAPH, Addison Wesley, 1956. [21] Philip M. Hubbard. Approximating 
Polyhedra with Spheres for Time-Critical Collision Detection. ACM Transactions on Graphics, 15(3), July 
1996. [22] David R. Jefferson. Virtual Time. ACM Transactions on Programming Lan­guages and Systems, 
7(3):404 425, July 1985. [23] V. V. Kamat. A Survey of Techniques for simulation of Dynamic Dynamic Col­lision 
Detection and Response. Computer Graphics in India, 17(4):379 385, 1993. [24] Kathryn W. Lilly. Ef.cient 
Dynamic Simulation of Robotic Mechanisms. Kluwer Academic Publishers, Norwell, 1993. [25] Victor J. Milenkovic. 
Position-Based Physics: Simulating the Motion of Many Highly Interacting Spheres and Polyhedra. In Holly 
Rushmeier, editor, SIG-GRAPH 96 Conference Proceedings, Annual Conference Series, pages 129 136. ACM 
SIGGRAPH, Addison Wesley, August 1996. [26] Brian Mirtich. Impulse-based Dynamic Simulation of Rigid 
Body Systems. Ph.D. thesis, University of California, Berkeley, December 1996. [27] Brian Mirtich. V-Clip: 
Fast and Robust Polyhedral Collision Detection. ACM Transactions on Graphics, 17(3):177 208, July 1998. 
Mitsubishi Electric Re­search Lab Technical Report TR97 05. [28] J. Thomas Ngo and Joe Marks. Spacetime 
Constraints Revisited. In SIGGRAPH 93 Conference Proceedings, Annual Conference Series, pages 343 350. 
ACM SIGGRAPH, Addison Wesley, 1993. [29] M. Overmars. Point Location in Fat Subdivisions. Information 
Processing Let­ters, 44:261 265, 1992. [30] William H. Press, Saul A. Teukolsky, William T. Vetterling, 
and Brian R. Flan­nery. Numerical Recipes in C: The Art of Scienti.c Computing. Cambridge University 
Press, Cambridge, second edition, 1992. [31] Edward J. Routh. Elementary Rigid Dynamics. Macmillan, London, 
1905. [32] Karl Sims. Evolving Virtual Creatures. In SIGGRAPH 94 Conference Proceed­ings, Annual Conference 
Series, pages 15 22. ACM SIGGRAPH, 1994. [33] John M. Snyder, Adam R. Woodbury, Kurt Fleischer, Bena 
Currin, and Alan H. Barr. Interval Methods for Multi-Point Collisions between Time-Dependent Curved Surfaces. 
In SIGGRAPH 93 Conference Proceedings, Annual Confer­ence Series, pages 321 333. ACM SIGGRAPH, Addison 
Wesley, 1993. [34] Peng Song, Peter R. Kraus, Vijay Kumar, and Pierre Dupont. Analysis of Rigid Body 
Dynamic Models for Simulation of Systems with Frictional Contacts, June 1999. Submitted to ASME Journal 
of Applied Mechanics. [35] D.E. Stewart and J.C. Trinkle. An Implicit Time-Stepping Scheme for Rigid 
Body Dynamics with Inelastic Collisions and Coulomb Friction. International Journal of Numerical Methods 
in Engineering, 39:2673 2691, 1996. [36] J.C. Trinkle, J.S. Pang, S. Sudarsky, and G. Lo. On Dynamic 
Multi-Rigid-Body Contact Problems with Coulomb Friction. Zeitschrift fur Angewandte Mathe­matik und Mechanik, 
77(4):267 279, 1997. [37] Andrew Witkin, Michael Gleicher, and William Welch. Interactive Dynamics. Computer 
Graphics, 24(2):11 22, March 1990.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344876</article_id>
		<sort_key>201</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>21</seq_no>
		<title><![CDATA[Interactive control for physically-based animation]]></title>
		<page_from>201</page_from>
		<page_to>208</page_to>
		<doi_number>10.1145/344779.344876</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344876</url>
		<abstract>
			<par><![CDATA[<p>We propose the use of interactive, user-in-the-loop techniques for controlling physically-based animated characters. With a suitably designed interface, the continuous and discrete input actions afforded by a standard mouse and keyboard allow for the creation of a broad range of motions. We apply our techniques to interactively control planar dynamic simulations of a bounding cat, a gymnastic desk lamp, and a human character capable of walking, running, climbing, and various gymnastic behaviors. The interactive control techniques allows a performer's intuition and knowledge about motion planning to be readily exploited. Video games are the current target application of this work.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[physically based animation]]></kw>
			<kw><![CDATA[user interfaces]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Interaction styles (e.g., commands, menus, forms, direct manipulation)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P149166</person_id>
				<author_profile_id><![CDATA[81100081962]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Joseph]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Laszlo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40036809</person_id>
				<author_profile_id><![CDATA[81319502903]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michiel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[van de Panne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43117278</person_id>
				<author_profile_id><![CDATA[81100188679]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Eugene]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fiume]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>241079</ref_obj_id>
				<ref_obj_pid>241020</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[K. Amaya, A. Bruderlin, and T. Calvert. Emotion from mo-tion. In Graphics Interface '96, pages 222-229, May 1996.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[W. W. Armstrong and M. Green. The dynamics of articu-lated rigid bodies for purposes of animation. Proceedings of Graphics Interface '85, pages 407-415, 1985.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[N. I. Badler, B. Barsky, and D. Zeltzer. Making Them Move. Morgan Kaufmann Publishers Inc., 1991.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[R. M. Baecker. Interactive computer-mediated animation. PhD thesis, Massachusetts Institute of Technology, 1969.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74357</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[A. Bruderlin and T. W. Calvert. Goal-directed anima-tion of human walking. Proceedings of ACM SIGGRAPH, 23(4):233-242, 1989.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[A. Bruderlin and T. W. Calvert. Interactive animation of per-sonalized human locomotion. Proceedings of Graphics Inter-face, pages 17-23, 1993.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>867808</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[B. R. Donald and F. Henle. Using Haptic Vector Fields for An-imation Motion Control. Technical Report PCS-TR99-353, Dartmouth College, Computer Science, Hanover, NH, May 1999.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>225295</ref_obj_id>
				<ref_obj_pid>225294</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[J. Auslander et al. Further experience with controller-based automatic motion synthesis for articulated figures. ACM Transactions on Graphics, October 1995.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218414</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[J. K. Hodgins et al. Animating human athletics. Proceedings of SIGGRAPH 95, ACM Computer Graphics, pages 71-78, 1995.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[T. Ertl et al. Interactive control of biomechanical animation. The Visual Computer, pages 459-465, 1993.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>102315</ref_obj_id>
				<ref_obj_pid>102313</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[D. Forsey and J. Wilhelms. Techniques for interactive ma-nipulation of articulated bodies using dynamic analysis. In Proceedings of Graphics Interface '88, pages 8-15, 1988.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>31465</ref_obj_id>
				<ref_obj_pid>31462</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[M. Girard. Interactive design of computer-animated legged animal motion. IEEE Comptuer Graphics and Applications, 7(6):39-51, June 1987.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218411</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[R. Grzeszczuk and D. Terzopoulos. Automated learning of muscle-actuated locomotion through control abstraction. Pro-ceedings of SIGGRAPH 95, ACM Computer Graphics, pages 63-70, 1995.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280816</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[R. Grzeszczuk, D. Terzopoulos, and G. Hinton. Neuroani-mator: Fast neural network emulation and control of physics-based models. Proceedings of SIGGRAPH 98, pages 9-20, July 1998. ISBN 0-89791-999-8. Held in Orlando, Florida.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1437356</ref_obj_id>
				<ref_obj_pid>1435699</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[H. Ko and N. Badler. Animating human locomotion with in-verse dynamics. IEEE Computer Graphics and Applications, pages 50-59, 1996.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>791501</ref_obj_id>
				<ref_obj_pid>791215</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[E. Kokkevis, D. Metaxas, and N. Badler. User-controlled physics-based animation for articulated figures. In Proceed-ings of Computer Animation '96, 1996.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>554939</ref_obj_id>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[A. Menache. Understanding Motion Capture for Computer Animation and Video Games. Morgan Kaufmann, 1999.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Namco. Tekken, tekken2, and tekken3. computer game, 1998, 1999.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166160</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[J. T. Ngo and J. Marks. Spacetime constraints revisited. Pro-ceedings of SIGGRAPH 93, pages 343-350, 1993.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Interplay Productions. Die by the sword. computer game, 1998.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[M. Rosenthal. Dynamic digital hosts. Computer Graphics World's Digital Magic, pages 39-42, August 1998.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134291</ref_obj_id>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[T. B. Sheridan. Telerobotics, Automation, and Human Super-visory Control. MIT Press, 1992.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192167</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[K. Sims. Evolving virtual creatures. Proceedings of SIG-GRAPH 94, ACM Computer Graphics, pages 15-22, 1994.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618495</ref_obj_id>
				<ref_obj_pid>616050</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[D. J. Sturman. Computer puppetry. IEEE Computer Graphics and Applications, 18(1):38-45, January/February 1998.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[J. Troy. Dynamic Balance and Walking Control of Biped Mechanisms. PhD thesis, Iowa State University, 1995.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[J. Troy. Real-time dynamic balancing and walking control of a 7-link planar biped. In Proceedings of ASME Design Engineering Technical Conferences, 1998.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[J. Troy and M. Vanderploeg. Interactive simulation and con-trol of planar biped walking devices. In Workshop on Simula-tion and Interaction in Virtual Environments, pages 220-224, July 1995.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218419</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[M. Unuma, K. Anjyo, and R. Takeuchi. Fourier principles for emotion-based human figure animation. Proceedings of SIG-GRAPH 95, ACM Computer Graphics, pages 91-96, 1995.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166159</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[M. van de Panne and E. Fiume. Sensor-actuator networks. Proceedings of SIGGRAPH 93, pages 335-342, 1993.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[M. van de Panne, R. Kim, and E. Fiume. Virtual wind-up toys for animation. Proceedings of Graphics Interface '94, pages 208-215, 1994.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Interactive Control For Physically-Based Animation Joseph Laszlo Michiel van de Panne Eugene Fiume Department 
of Computer Science University of Toronto.  Abstract We propose the use of interactive, user-in-the-loop 
techniques for controlling physically-based animated characters. With a suitably designed interface, 
the continuous and discrete input actions afford­ed by a standard mouse and keyboard allow for the creation 
of a broad range of motions. We apply our techniques to interactively control planar dynamic simulations 
of a bounding cat, a gymnas­tic desk lamp, and a human character capable of walking, running, climbing, 
and various gymnastic behaviors. The interactive control techniques allows a performer s intuition and 
knowledge about mo­tion planning to be readily exploited. Video games are the current target application 
of this work. CR Categories: I.3.6 [ Computer Graphics]: Methodology and Techniques Interaction Techniques; 
I.3.7 [ Computer Graphic­s]: Three-Dimensional Graphics and Realism Animation; I.6.8 [ Simulation and 
Modeling]: Types of Simulation Animation Keywords: physically based animation, user interfaces 1 Introduction 
Interactive simulation has a long history in computer graphics, most notably in .ight simulators and 
driving simulators. More recently, it has become possible to simulate the motion of articulated human 
models at rates approaching real-time. This creates new opportu­nities for experimenting with simulated 
character motions and be­haviors, much as .ight simulators have facilitated an unencumbered exploration 
of .ying behaviors. Unfortunately, while the controls of an airplane or an automo­bile are well known, 
the same cannot be said of controlling human or animal motions where the interface between our intentions 
and muscle actions is unobservable, complex, and ill-de.ned. Thus, in order to create a tool which allows 
us to interactively experiment with the dynamics of human and animal motions, we are faced with the task 
of designing an appropriate interface for animators. Such an interface needs to be suf.ciently expressive 
to allow the creation of a large variety of motions while still being tractable to learn. Performance 
animation and puppetry techniques demonstrate how well performers can manage the simultaneous control 
of a 1{j.aszloIvanIelf}@dgp.utoronto.ca http://www.dgp.utoronto.ca/.j.aszlo/interactive-control.html 
 Permission to make digital or hard copies of part or all of this work or personal or classroom use is 
granted without fee provided that copies are not made or distributed for profit or commercial advantage 
and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, 
to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. SIGGRAPH 
2000, New Orleans, LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 large number of degrees of 
freedom. However, they are fundamen­tally kinematic techniques; if considerations of physics are to be 
added, this is typically done as a post-process. As a result, they do not lend themselves well to giving 
a performer a sense of embodi­ment for animated .gures whose dynamics may differ signi.cantly from that 
of the performer. In contrast, the physics of our simulated characters constrains the evolution of their 
motions in signi.cant ways. We propose techniques for building appropriate interfaces for interactively-controlled 
physically-based animated characters. A variety of characters, motions, and interfaces are used to demon­strate 
the utility of this type of technique. Figure 1 shows an exam­ple interface for a simple articulated 
.gure which serves as a start­ing point for our work and is illustrative of how a simple interface can 
provide effective motion control. Figure 1: Interactive control for Luxo, the hopping lamp. This planar 
model of an animated desk lamp has a total of 5 degrees of freedom (DOF) and 2 actuated joints, capable 
of exert­ing joint torques. The motion is governed by the Newtonian laws of physics, the internal joint 
torques, the external ground forces, and gravity. The joint torques are computed using a proportional­ 
e derivative (PD) controller, namely T=kp(ed.e).kde.The motions of the two joints are controlled by linearly 
mapping the mouse position, (mX), to the two desired joint angles, ed. my Using this interface, coordinated 
motions of the two joints corre­spond to tracing particular time-dependent curves with the mouse. A rapid 
mouse motion produces a correspondingly rapid joint mo­tion. With this interface one can quickly learn 
how to perform a variety of interactively controlled jumps, shuf.es, .ips, and kips, as well as locomotion 
across variable terrain. With suf.cient prac­tice, the mouse actions become gestures rather than carefully-traced 
trajectories. The interface thus exploits both an animator s motor learning skills and their ability 
to reason about motion planning. Figure 2 shows an example of a gymnastic tumbling motion cre­ated using 
the interface. This particular motion was created using several motion checkpoints. As will be detailed 
later, these facilitate correcting mistakes in executing particularly unstable or sensitive motions, 
allowing the simultion to be rolled back to previous points in time. Figure 3 shows a user-controlled 
motion over variable ter­rain and then a slide over a ski jump, in this case performed without Figure 
2: Example of an interactively controlled back head-springs and back-.ip for Luxo.  Figure 3: Example 
of an interactively controlled animation, con­sisting of hops across variable terrain and a carefully 
timed push off the ski jump. the use of any checkpoints. The sliding on the ski hill is modelled by reducing 
the ground friction coef.cient associated with the sim­ulation, while the jump is a combined result of 
momentum coming off the lip of the jump and a user-controlled jump action. This initial example necessarily 
provokes questions about scala­bility, given that for more complex characters such as a horse or a cat, 
one cannot hope to independently control as many input DOF as there are controllable DOF in the model. 
One possible solu­tion is to carefully design appropriate one-to-many mappings from input DOF to output 
DOF. These mappings can take advantage of frequently occuring synergetic joint motions as well as known 
sym­metry and phase relationships. We shall also explore the use of discrete keystrokes to comple­ment 
and/or replace continuous input DOF such as that provided by the mouse. These enrich the input space 
in two signi.cant ways. First, keys can each be assigned their own action semantics, thereby allowing 
immediate access to a large selection of actions. This ac­tion repertoire can easily be further expanded 
if actions are selected based upon both the current choice of keystroke and the motion context of the 
keystroke. Second, each keystroke also de.nes when to perform an action as well as the selection of what 
action. The timing of keystrokes plays an important role in many of our proto­type interfaces. In its 
simplest form, our approach can be thought of as sitting squarely between existing virtual puppetry systems 
and physically­based animation. It brings physics to virtual puppetry, while bring­ing interactive interfaces 
to physically-based animation. The sys­tem allows for rapid, free-form exploration of the dynamic capabil­ities 
of a given physical character design. The remainder of this paper is structured as follows. Section 2 
re­views previous related work. Section 3 describes the motion primi­tives used in our prototype system. 
Section 4 illustrates a variety of results. Finally, section 5 provides conclusions and future work. 
 2 Previous Work Building kinematic or dynamic motion models capable of repro­ducing the complex and 
graceful movements of humans and an­imals has long been recognized as a challenging problem. The book 
Making Them Move[3] provides a good interdisciplinary primer on some of the issues involved. Using physical 
simula­tion techniques to animate human .gures was proposed as early as 1985[2]. Since then, many efforts 
have focussed on methods of computing appropriate control functions for the simulated actuators which 
will result in a desired motion being produced. Among the more popular methods have been iterative optimization 
techniques [8, 13, 19, 23, 29, 30], methods based on following kinematically­speci.ed reference trajectories 
[15, 16], suitably-designed state ma­chines [9], machine learning techniques[14], and hybrids[5, 12]. 
A number of efforts have examined the interactive control of dynamically-simulated articulated .gures[10, 
11] or procedurally­driven articulated .gures[6]. The mode of user interaction used in these systems 
typically involves three steps: (1) setting or changing speci.c parameters (2) running the simulation, 
and (3) observing the result. This type of observe and edit tools is well suited to pro­ducing highly 
speci.c motions. However, the interaction is less immediate than we desire, and it does not lend a performer 
a sense of embodiment in a character. Motion capture and virtual puppetry both allow for user-in-the­loop 
kinematic control over motions[17, 21, 24], and have proven effective for speci.c applications demanding 
real-time animation. The use of 2d user gestures to specify object motion[4] is in an interesting early 
example of interactive computer mediated anima­tion. Physical animatronic puppets are another interesting 
prece­dent, but they cannot typically move in an uncontrained and dy­namic fashion in their environment. 
The system described in [7] is a novel application of using a haptic feedback device for animation control 
using a mapping which interactively interpolates between a set of existing animations. Our work aims 
to expand the scope of interactive real-time interfaces by using physically-based simu­lations, as well 
as exploring interfaces which allow various degrees of motion abstraction. Such interfaces could then 
perhaps also be applied to the control of animatronic systems. The work of Troy[25, 26, 27] proposes 
the use of manual manip­ulation of several input devices to perform low-level control of the movement 
of bipedal characters. The work documents experiments with a variety of input devices and input mappings 
as having been performed, although detailed methods and results are unfortunate­ly not provided for the 
manual control method. Nevertheless, this work is among the .rst we know of that points out the potential 
of user-in-the-loop control methods for controlling unstable, dynamic motions such as walking. Computer 
and video games offer a wide variety of interfaces based both on continuous-input devices (mice, joysticks, 
etc.) and button-presses and/or keystrokes. However, the current generation of games do not typically 
use physically-based character anima­tion, nor do they allow much in the way of .ne-grained motion control. 
Exceptions to the rule include .ghting games such as Die by the Sword[20] and Tekken[18]. The former 
allows mouse and keyboard control of a physically-based model, limited to the mo­tion of the sword arm. 
The latter, while kinematic in nature, affords relatively low-level control over character motions. Telerobotic­s 
systems[22] are a further suitable example of interactive control of dynamical systems, although the 
robots involved are typically anchored or highly stable, and are in general used in constrained settings 
not representative of many animation scenarios. 3 Motion Primitives The motion primitives used to animate 
a character can be charac­terized along various dimensions, including their purpose and their implementation. 
In this section we provide a classi.cation based largely on the various interface methods employed in 
our example scenarios. The joints of our simulated articulated .gures are all controlled by the use of 
PD controllers. Motion primitives thus control mo­ time sequence of intervals interval instant non-robust 
joint robust to initial state limb robust to future state inter-limb coordination state/environment structure 
Figure 4: Three dimensions of control abstraction. tions by varying the desired joint angles used by 
the PD controllers, as well as the associated stiffness and damping parameters. PD controllers provide 
a simple, low-level control mechanism which allows the direct speci.cation of desired joint angles. Cop­ing 
with more complex characters and motions necessitates some form of abstraction. Figure 4 shows three 
dimensions along which such motion abstraction can take place. The interfaces explored in this paper 
primarily explore abstractions in time and structure by using stored control sequences and coordinated 
joint motions, respectively. The remaining axis of abstraction indicates the desir­ability of motion 
primitives which perform correctly irrespective of variations in the initial state or variations in the 
environment. This third axis of abstraction is particularly challenging to address in an automated fashion 
and thus our examples rely on the user-in-the­loop to perform this kind of abstraction. 3.1 Continuous 
Control Actions The most obvious way to control a set of desired joint angles is us­ing an input device 
having an equivalent number of degrees of free­dom. The mouse-based interface for the hopping lamp (Figure 
1) is an illustration of this. It is interesting to note for this particu­lar example that although cursor 
coordinates are linearly mapped to desired joint angles, a nonlinearity is introduced by the acceler­ation 
features present in most mouse-drivers. This does not seem to adversely impact the continuous control. 
In general, continuous control actions are any mappings which make use of continuously varying control 
parameters, and are thus not limited to direct map­pings of input DOF to output DOF. The availability 
of high DOF input devices such as data-gloves and 6 DOF tracking devices means that the continuous control 
of input DOF can potentially scale to control upwards of 20 degrees of freedom. However, it is perhaps 
unreasonable to assume that a per­former can learn to simultaneously manipulate such a large number of 
DOF independently, given that precedents for interfaces in clas­sical puppetry and virtual puppetry are 
typically not this ambitious. 3.2 Discrete Control Actions Discrete actions, as implemented by keystrokes 
in our interfaces, al­low for an arbitrary range of action semantics. Action games have long made extensive 
use of keystrokes for motion speci.cation, al­though not at the level of detail that our interfaces strive 
to provide. The following list describes the various action semantics used in prototype interfaces, either 
alone or in various combinations. Some of the actions in this list refer directly to control actions, 
while oth­ers serve as meta-actions in that they modify parameters related to the simulation and the 
interface itself. set joint position (absolute) Sets desired position of joint or a set of joints to 
a prespeci.ed value(s). If all joints are set simul­taneously in order to achieve a desired pose for 
the .gure, this becomes a form of interactive dynamic keyframing. adjust joint position (relative) Changes 
the desired position of a joint or set of joints, computed relative to current desired joint positions. 
grasp, release Causes a hand or foot to grasp or release a nearby point (e.g., ladder rung) or to release 
a grasped point. select IK target Selects the target point for a hand or foot to reach toward using a 
.xed-time duration IK trajectory, modelled with a Hermite curve. The IK solution is recomputed at every 
time step. initiate pose sequence Initiate a prespeci.ed sequence of full or partial desired poses. select 
next control state Allows transitions between the states of a .nite-state machine; useful for modelling 
many cyclical or otherwise well-structured motions, leaving the timing of the transitions to the performer. 
rewind, reset state Restarts the simulation from a previous state checkpoint. set joint stiffness and 
damping Sets the stiffness and damping parameters of the PD joint controllers to desired values. select 
control mode Chooses a particular mapping for a continu­ous input device, such as which joints the mouse 
controls. set simulation rate Speeds up or slows down the current rate of simulation; helps avoid a motion 
happening too fast or too slow to properly interact with it. set state checkpoint Stores the system state 
(optionally during re­play/review of a motion) so that simulation may be reset to the same state later 
if desired. modify physical parameters Effects changes to simulation pa­rameters such as gravity and 
friction. toggle randomized motion Begins or halts the injection of small randomized movements, which 
are useful for introducing mo­tion variation. Our default model for arbitration among multiple actions 
which come into con.ict is to allow the most recent action to pre-empt any ongoing actions. Ongoing actions 
such as IK-based trajectories or pose sequences are respectively preempted only by new IK-based trajectories 
or pose sequences. 3.3 State Machines Given the cyclic or strongly structured nature of many motions, 
s­tate machine models are useful in helping to simplify the complexi­ties of interactive control. For 
example, they allow separate actions such as take left step and take right step to be merged into a single 
action take next step , where a state machine provides the necessary context to disambiguate the action. 
As with many other animations systems, state machines serve as the means to provide apriori knowledge 
about the sequencing of actions for particular classes of motion.  4 Implementation and Results Our 
prototype system is based on a number of planar articulated .gures. The planar dynamics for these .gures 
can easily be com­puted at rates suitable for interaction (many in real-time) on most current PCs and 
offer the additional advantage of having all aspects of their motion visible in a single view, thereby 
providing unob­structed visual feedback for the performer. Our tests have been conducted primarily on 
a 450 Mhz Mac and a 366 Mhz PII PC. While hard-coded interfaces were used with the original prototyp­ing 
system behind many of our results, our more recent system uses Tclas a scripting language for specifying 
the details of any given interface. This facilitates rapid iteration on the design of any given interface, 
even potentially allowing changes during the course of a simulation. 4.1 Luxo Revisited Using the continuous-mode 
mouse-based interface shown in Fig­ure 1, the desklamp is capable of executing a large variety of hops, 
back-.ips, a kip manoevre, head-stands, and motion across variable terrain. This particular interface 
has been tested on a large number of users, most of whom are capable of performing a number of the simpler 
movements within 10 15 minutes, given some instruction on using the interface. Increasing the stiffness 
of the joints or scal­ing up the mapping used for translating mouse position into desired joint angles 
results in the ability to perform more powerful, dynam­ic movements, although this also makes the character 
seem rather too strong during other motions. We have additionally experimented with a keystroke-based 
in­terface using 14 keys, each key invoking a short sequence of pre­speci.ed desired poses of .xed duration. 
The various key actions result in a variety of hops and somersaults if executed from the ap­propriate 
initial conditions. The repertoire of action sequences and associated keystrokes are given in the Appendix. 
The animator or performer must choose when to execute keystrokes and by doing so selects the initial 
conditions. The initiation of a new action over­rides any ongoing action. The keystroke-based interface 
was created after gaining some experience with the continuous-mode interface. It provides an in­creased 
level of abstraction for creating motions and is easier to learn, while at the same time trading away 
some of the .exibili­ty offered by the continuous-mode interface. Lastly, user-executed continuous motions 
can be recorded and then bound to a keystroke. 4.2 Animating a Cat Experiments with a planar bounding 
cat and a planar trotting cat are a useful test of scalability for our interactive interface techniques. 
Figure 5 illustrates the planar cat as well as sets of desired angles assumed by the legs for particular 
keystrokes. In one control mode, the front and back legs each have 6 keys assigned to them, each of which 
drives the respective leg to one of the 6 positions illustrated in the .gure. The keys chosen for each 
pose are assigned a spatial layout on the keyboard which re.ects the layouts of the desired pos­es shown 
in the .gure. An additional pose is provided which allows each leg to exert a larger pushing force than 
is otherwise available with the standard set of 6 poses. This can be achieved by temporar­ily increasing 
the stiffness of the associated leg joints, or by using a set of hyperextended joint angles as the desired 
joint positions. We use the latter implementation. This seventh overextended pose is invoked by holding 
the control key down when hitting the key associated with the backwards extended leg pose. The animation 
sequence shown in Figure 6 was accomplished using 12 checkpoints. A checkpoint lets the performer restart 
the p1 p3  q w e  u i o   j k l a s d  p4 rear leg fore leg p2 Figure 5: Parameterization 
of limb movements for cat. simulation from a given point in time, allowing the piecewise in­teractive 
construction of sequences that would be too long or too error-prone to perform successfully in one uninterrupted 
attemp­ t. Checkpoints can be created at .xed time intervals or at will by the performer using a keystroke. 
Some of the sequences between checkpoints required only 2 or 3 trials, while particularly dif.cult aspects 
of the motion required 10 20 trials, such as jumping the large gap and immediately climbing a set of 
steps (second-last row of Figure 6). The cat weighs 5 gand is approximately 50 mlong, as mea­ ksured 
from the tip of the nose to the tip of the tail. Its small size leads to a short stride time and requires 
the simulation to be slowed down considerably from real-time in order to allow suf.cient reac­tion time 
to properly control its motions. The cat motions shown in Figure 6 were controlled using a slowdown factor 
of up to 40., which allows for 10 15 seconds to control each bound. It is important to note that there 
is a sweet spot in choosing the speed at which to interact with a character. Important features of the 
dynamics become unintuitive and uncontrollable if the interac­tion rate is either too slow or too fast. 
When the simulation rate is too fast, the user is unable to react quickly enough to correct er­rors before 
the motion becomes unsavable. When the motion is too slow, the user tends to lose a sense of the necessary 
rhythm and tim­ing required to perform the motion successfully and lacks suf.cient immediate feedback 
on the effects of the applied control actions. For basic bounding, a slowdown factor around 10, giving 
a bound time of 2-3 seconds is suf.cient. For more complex motions such as leaping over obstacles, a 
factor of up to 40+ is required. Figure 7 shows a trotting motion for a planar 4-legged cat mod­el. The 
trotting was interactively controlled using only the mouse. The xymouse coordinates are used to linearly 
interpolate between prede.ned poses corresponding to the six leg poses shown in Fig­ure 5. The poses 
are laid out in a virtual 2.3grid and bilinear interpolation is applied between the nearest 4 poses according 
to the mouse position. The simplest control method assumes a .xed phase relationship among the 4-legs, 
allowing the mouse to simul­taneously effect coordinated control of all legs. A more complex method uses 
the same mapping to control one leg at a time. This latter method met with less success, although was 
not pursued at length. The cat model is comprised of 30 articulated links, which makes it somewhat slow 
to simulate, given that we currently do not employ O(n)forward dynamics methods. 4.3 Bipedal Locomotion 
We have experimented with a number of bipedal systems which are capable of more human-like movements 
and behaviors such as walking and running. For these models, we make extensive use Figure 6: Cat bounding 
on variable terrain using piecewise interac­tive key-based control. The frames shown are manually selected 
for visual clarity and thus do not represent equal samples in time. The arrows indicate when the various 
checkpoints were used, denoting the position of the shoulders at the time of the checkpoint. Figure 7: 
Cat trot using continuous mouse control. The animation reads from top-to-bottom, left-to-right. The .rst 
seven frames rep­resent a complete motion cycle. The frames are equally spaced in time. of a hybrid control 
technique which mixes continuous and discrete input actions in addition to purely discrete methods similar 
to those used with the cat and Luxo models. We have also experimented with a wide variety of other bipedal 
motions in addition to walking and running, including a number of motions such as a long jump attempt 
and a fall-recovery sequence that are readily explored using interactive control techniques. Figure 8 
shows the interface for an interactive walking control experiment. The mouse is used to control the desired 
angles for the hip and knee joints of the swing leg. A keypress is used to control when the exchange 
of stance and swing legs occurs and therefore changes the leg currently under mouse control. The stance 
leg as­sumes a straight position throughout the motion. The bipedal .gure has human-like mass and dimensions, 
although it does not have a separate ankle joint. In our current implementation, joint limits are not 
enforced, although such constraints can easily be added to the simulation as desired. An example of the 
resulting motion is shown in Figure 9. With some practice, a walk cycle can be sustained inde.nitely. 
With sig­ni.cant practice, the walk can be controlled in real-time, although a simulation speed of 2 
3 times slower than real-time provides a (keypress) (keypress) right leg left leg Figure 8: Interface 
for interactive control of bipdal walking. sweet-spot for consistent interactive control. It is also 
possible to choose a particular (good) location for the mouse, thus .xing the desired joint angles for 
the swing leg, and achieve a marching gait by specifying only the the time to exchange swing and stance 
legs by pressing a key. This marching motion is quite robust and is able to traverse rugged terrain with 
reasonable reliability. Yet another mode of operation can be achieved by automatically triggering the 
swing-stance exchange when the forward lean of the torso exceeds a .xed threshold with respect to the 
vertical. With this automatic mechanism in place, it is then possible to transition from a march­ingwalkto 
a runand back againbyslowlymoving onlythe mouse through an appropriate trajectory. Figure 9: Bipedal 
walking motion Figure 10 shows the results of a biped performing a long-jump after such an automatic 
run. This particular biped dates from earlier experiments and is smaller in size and mass than the more 
anthro­pomorphic biped used for the walking experiments. This motion makes use of the same interface 
as for the bipedal walking motion, shown in in Figure 8. A slowdown factor of up to .0was necessary because 
of the small size of the character, as well as the precision required to achieve a .nal landing position 
having the legs extended and the correct body pitch. Approximately 20 trials are required to achieve 
a recognizable long jump, each beginning from a motion checkpoint one step before the .nal leap. However, 
we anticipate that the interface can be also be improved upon signi.cantly by us­ing a more reasonable 
default behavior for the uncontrolled leg. Figure 10: A long jump attempt. 4.4 Bipedal Gymnastics Several 
other experiments were carried out using the bipedal .gures with continuous-mode mouse control and one 
or more keys to select the mapping of the continuous input onto the model s desired joint angles. The 
basic types of motion investigated include a variety of climbing modes both with the bipedal model facing 
the view plane and in pro.le in the view plane, and swinging modes both with arms together and separated. 
Nearly every mapping for these control modes uses the mouse ycoordinate to simultaneously drive the motion 
of all limb joints (hips, knees, shoulders and elbows) in a coordinated fashion and the mouse xcoordinate 
to drive the bending of the waist joint to alter the direction of the motion. The control modes differ 
from each other primarily in the par­ticular symmetries shared between the joints. Figure 11 illustrates 
two forms of symmetry used for climbing gaits similar in pat­tern to those of a quadruped trotting and 
bounding. The mapping of the mouse xcoordinate onto the waist joint is also shown. The control modes 
can produce interactive climbing when coupled with a state machine that grasps and releases the appropriate 
hands and feet each time a key is pressed (assuming that the hands and feet are touching a graspable 
surface). Swinging modes perform in a simi­lar manner but use the mouse xor ycoordinate to swing the 
arms either back-and-forth at the shoulder or in unison and can make use of either graspable surfaces 
or ropes that the user can extend and re­tract from each hand on demand. When used on the ground without 
grasping, these same modes of interaction can produce a range of gymnastic motions including handstands 
and different types of .ips and summersaults, in addition to a continuously controlled running motion. 
Among the various interesting motions that are possible is a back.ip done by running off a wall, a gymnastic 
kip from a supine position to a standing position and a series of giant swings as might be performed 
on a high bar. While not illustrated here, these motions are demonstrated in the video segments and CD-ROM 
an­imations associated with this paper.  4.5 Using IK Primitives Figures 12 and 13 illustrate interactively-controlled 
movements on a set of irregularly-spaced monkeybars and a ladder, respectively. These are movements which 
require more precise interactions of the hands and feet with the environment than most of the other mo­tions 
discussed to date. To deal with this, we introduce motion primitives which use inverse kinematics (IK) 
to establish desired joint angles. In general, IK provides a rich, abstract motion primitive that can 
appropriately hide the control complexity inherent in many semantically-simple goal-directed actions 
that an interactive char­acter might want to perform. This reduces the associated learning curve the 
user faces in trying to discover how to perform the ac­tion(s) from .rst principles while still taking 
good advantage of the user s intuition about the motion. The interface for monkey-bar traversal consists 
of keystrokes and a state machine. IK-based trajectories for the hands and feet are in­voked on keystrokes. 
The hand-over-hand motion across the mon­keybars is controlled by keys which specify one of three actions 
for the next release-and-regrasp motion. The actions causes the hand to release its grasp on the bar 
and move towards the previous bar, the current bar, or the following bar. These actions can also be in­voked 
even when the associated hand is not currently grasping a bar, which allows the .gure to recover when 
a grasp manoevre fails due to bad timing. The interface does not currently safeguard against the premature 
execution of a regrasp motion with one hand while the other has not yet grasped a bar. The character 
will thus fall in such situations. A grasp on a new bar is enacted if the hand passes close to the target 
bar during the reaching action, where close is de.ned to be a .xed tolerance of 4 cm in our example. 
Controlling the motion thus involves carefully choosing the time in a swing at which a new reach-and-grasp 
action should be initiated, as well as when to pull up with the current support arm. More information 
about the particulars of the interface is given in the Appendix. The ladder climbing example is made 
up of a number of keys which serve to position the body using the hands and feet which are in contact 
with the ladder, as well as a key to initiate the next limb movement as determined by the state machine. 
The details of the interface are given in the appendix, as well as the speci.c keystroke sequence used 
to create Figure 13. Note, however, that the keystroke sequence by itself is insuf.cient to precisely 
recreate the given motion, as the timing of each keystroke is also important in all the motions discussed. 
 Finally, Figure 14 illustrates a standing up motion, followed by a few steps, a forward fall, crouching, 
standing up, and, lastly, a backwards fall. A set of 18 keys serves as the interface for this scenario, 
as documented in the appendix.  5 Conclusions and Future Work We have presented prototype interfaces 
for the interactive control of physically-based character animation. The techniques illustrate the feasibility 
of adding physics to virtual puppetry, or, alternatively, adding interactive interfaces to physically-based 
animation. They allow human intuition about motions to be exploited in the interac­tive creation of novel 
motions. The results illustrate that dynamic motions of non-trivial articu­lated .gure models can be 
reasonably controlled with user-in-the­loop techniques. Our experiments to date have focussed on .rst 
achieving a large action repertoire for planar .gures, with the goal of using this experience as a suitable 
stepping stone towards 3D motion control. While it is not clear that the interaction techniques will 
scale to the type of nuanced 3D motion control need for fore­ground character animation in production 
animation, the interfaces could be readily applied to a new generation of physically-based interactive 
video games. The interfaces provide a compelling user experience in fact, we found the interactive control 
experiments to be quite addictive. One of the drawbacks of using interactive control is the effort re­quired 
in both designing an appropriate interface and then learning to effectively use the interface. These 
two nested levels of exper­imentation necessitate a degree of expertise and patience. We are optimistic 
that tractable interfaces can be designed to control sylis­tic variations of complex motions and that 
animators or game play­ers can learn these interfaces with appropriate training and practice. Our work 
has many directions which require further investiga­tion. We are still far from being able to reproduce 
nuanced dynam­ic motions for 3D human or animal characters[1, 28]. The large variety of high-DOF input 
devices currently available offers a pos­sible avenue of exploration. Haptic devices may also play a 
useful role in constructing effective interfaces[7]. Nuanced performance may potentially require years 
of training, as it does for other arts (key-frame animation, dancing, music) and sports. We can perhaps 
expect that the instruments and interfaces required for composing motion to undergo continual evolution 
and improvement. A large community of users would offer the potential for a rapidly evolving set of interfaces 
for particular motions or characters. Many dynamic motions would bene.t from additional sensory feedback, 
such as an animated update of the location of the cen­ter of mass[25]. Going in the opposite direction, 
one could use an interactive environment like ours to conduct experiments as to the minimal subset of 
sensory variables required to successfully control a given motion. Questions regarding the transfer of 
skills between interfaces and between character designs are also important to ad­dress if broad adoption 
of interactive control techniques is to be feasible. The derivation of high-level abstractions of motion 
control is of interest in biomechanics, animation, and robotics. The training data and insight gained 
from having a user-in-the-loop can potentially help in the design of autonomous controllers or high-level 
motion abstractions. A variety of hybrid manual/automatic control methods are also likely to be useful 
in animation scenarios. Beyond its application to animation, we believe the system al­so has potential 
uses in exploring deeper issues involved in con­trolling motions for biomechanics, robotics, or animation. 
What constitutes a suitable motor primitive or motor program ? How can these primitives be sequenced 
or overlaid in order to synthe­size more complex motions? In what situations is a particular mo­tion 
primitive useful? Our experimental system can serve as a tool towards exploring these questions by allowing 
interactive control over the execution and sequencing of user-designed motion-control primitives. Acknowledements: 
We would like to thank all of the following for their help: the anonymous reviewers for their comments; 
the Imager lab at UBC for hosting the second author during much of the work on this paper; and David 
Mould for suggestions and assistance investigating the automatic bipedal marching and running motions. 
This work was supported by grants from NSERC and CITO. A Appendix Details of keystrokes interface for 
Luxo: k small hop l medium hop o large hop i high backward hop u medium backward hop j small backward 
hop y back somersault s sitting to upright (slow) d standing to sitting / LB to sitting f LB to standing 
(small height) / standing to sitting e LB to standing (medium height) / standing to LB w standing to 
sitting / sitting to LB / LB to standing q big jump from base to LB / fwd somersault from LB a LB to 
standing with small jump A/B = a single action that performs either A or B depending on initial state 
LB = lying on back Interface and keystrokes for monkeybar example: a grasp rung previous to CR s grasp 
CR d grasp rung following CR f grasp rung two rungs following CR q release with both hands, relax arms 
e pull up using support arm R reset to initial state t toggle defn of support/grasp arm CR = closest 
rung Interface and keystrokes for ladder climbing example: q release both hands, fall from ladder f grasp 
two rungs higher with next grasp arm h shift body up b lower body down n pull body in with arms g push 
body out with arms j push body out with legs m pull body in with legs R reset to initial state Interface 
and keystrokes for the fall recovery example: p ST, prepare for forwards fall o ST, prepare for backwards 
fall t HK, step back with left arm y HK, step back with right arm q HK, shift body back w HK, bend elbows, 
prepare for push up W HK, straighten elbows, push up 1 CR, straighten hips, knees, ankles 2 CR, assume 
intermediate pose towards being upright 3 CR, assume .nal upright pose c ST, step backwards with left 
leg v ST, step forwards with left leg b ST, step backwards with right leg n ST, step forwards with right 
leg j ST, lean back at hips R reset to initial state M checkpoint current state L restart at checkpoint 
state ST = standing HK = on hands and knees CR = crouched   References [1] K. Amaya, A. Bruderlin, 
and T. Calvert. Emotion from mo­tion. In Graphics Interface 96, pages 222 229, May 1996. [2] W. W. Armstrong 
and M. Green. The dynamics of articu­lated rigid bodies for purposes of animation. Proceedings of Graphics 
Interface 85, pages 407 415, 1985. [3] N. I. Badler, B. Barsky, and D. Zeltzer. Making Them Move. Morgan 
Kaufmann Publishers Inc., 1991. [4] R. M. Baecker. Interactive computer-mediated animation. PhD thesis, 
Massachusetts Institute of Technology, 1969. [5] A. Bruderlin and T. W. Calvert. Goal-directed anima­tion 
of human walking. Proceedings of ACM SIGGRAPH, 23(4):233 242, 1989. [6] A. Bruderlin and T. W. Calvert. 
Interactive animation of per­sonalized human locomotion. Proceedings of Graphics Inter­face, pages 17 
23, 1993. [7] B. R. Donald and F. Henle. Using Haptic Vector Fields for An­imation Motion Control. Technical 
Report PCS-TR99-353, Dartmouth College, Computer Science, Hanover, NH, May 1999. [8] J. Auslander et 
al. Further experience with controller-based automatic motion synthesis for articulated .gures. ACM Transactions 
on Graphics, October 1995. [9] J. K. Hodgins et al. Animating human athletics. Proceedings of SIGGRAPH 
95, ACM Computer Graphics, pages 71 78, 1995. [10] T. Ertl et al. Interactive control of biomechanical 
animation. The Visual Computer, pages 459 465, 1993. [11] D. Forsey and J. Wilhelms. Techniques for interactive 
ma­nipulation of articulated bodies using dynamic analysis. In Proceedings of Graphics Interface 88, 
pages 8 15, 1988. [12] M. Girard. Interactive design of computer-animated legged animal motion. IEEE 
Comptuer Graphics and Applications, 7(6):39 51, June 1987. [13] R. Grzeszczuk and D. Terzopoulos. Automated 
learning of muscle-actuated locomotion through control abstraction. Pro­ceedings of SIGGRAPH 95, ACM 
Computer Graphics, pages 63 70, 1995. [14] R. Grzeszczuk, D. Terzopoulos, and G. Hinton. Neuroani­mator: 
Fast neural network emulation and control of physics­based models. Proceedings of SIGGRAPH 98, pages 
9 20, July 1998. ISBN 0-89791-999-8. Held in Orlando, Florida. [15] H. Ko and N. Badler. Animating human 
locomotion with in­verse dynamics. IEEE Computer Graphics and Applications, pages 50 59, 1996. [16] E. 
Kokkevis, D. Metaxas, and N. Badler. User-controlled physics-based animation for articulated .gures. 
In Proceed­ings of Computer Animation 96, 1996. [17] A. Menache. Understanding Motion Capture for Computer 
Animation and Video Games. Morgan Kaufmann, 1999. [18] Namco. Tekken, tekken2, and tekken3. computer 
game, 1998, 1999. [19] J. T. Ngo and J. Marks. Spacetime constraints revisited. Pro­ceedings of SIGGRAPH 
93, pages 343 350, 1993. [20] Interplay Productions. Die by the sword. computer game, 1998. [21] M. Rosenthal. 
Dynamic digital hosts. Computer Graphics World s Digital Magic, pages 39 42, August 1998. [22] T. B. 
Sheridan. Telerobotics, Automation, and Human Super­visory Control. MIT Press, 1992. [23] K. Sims. Evolving 
virtual creatures. Proceedings of SIG-GRAPH 94, ACM Computer Graphics, pages 15 22, 1994. [24] D. J. 
Sturman. Computer puppetry. IEEE Computer Graphics and Applications, 18(1):38 45, January/February 1998. 
[25] J. Troy. Dynamic Balance and Walking Control of Biped Mechanisms. PhD thesis, Iowa State University, 
1995. [26] J. Troy. Real-time dynamic balancing and walking control of a 7-link planar biped. In Proceedings 
of ASME Design Engineering Technical Conferences, 1998. [27] J. Troy and M. Vanderploeg. Interactive 
simulation and con­trol of planar biped walking devices. In Workshop on Simula­tion and Interaction in 
Virtual Environments, pages 220 224, July 1995. [28] M. Unuma, K. Anjyo, and R. Takeuchi. Fourier principles 
for emotion-based human .gure animation. Proceedings of SIG-GRAPH 95, ACM Computer Graphics, pages 91 
96, 1995. [29] M. van de Panne and E. Fiume. Sensor-actuator networks. Proceedings of SIGGRAPH 93, pages 
335 342, 1993. [30] M. van de Panne, R. Kim, and E. Fiume. Virtual wind-up toys for animation. Proceedings 
of Graphics Interface 94, pages 208 215, 1994. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344880</article_id>
		<sort_key>209</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>22</seq_no>
		<title><![CDATA[Interactive manipulation of rigid body simulations]]></title>
		<page_from>209</page_from>
		<page_to>217</page_to>
		<doi_number>10.1145/344779.344880</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344880</url>
		<abstract>
			<par><![CDATA[<p>Physical simulation of dynamic objects has become commonplace in computer graphics because it produces highly realistic animations. In this paradigm the animator provides few physical parameters such as the objects' initial positions and velocities, and the simulator automatically generates realistic motions. The resulting motion, however, is difficult to control because even a small adjustment of the input parameters can drastically affect the subsequent motion. Furthermore, the animator often wishes to change the end-result of the motion instead of the initial physical parameters.</p><p>We describe a novel interactive technique for intuitive manipulation of rigid multi-body simulations. Using our system, the animator can select bodies at any time and simply drag them to desired locations. In response, the system computes the required physical parameters and simulates the resulting motion. Surface characteristics such as normals and elasticity coefficients can also be automatically adjusted to provide a greater range of feasible motions, if the animator so desires. Because the entire simulation editing process runs at interactive speeds, the animator can rapidly design complex physical animations that would be difficult to achieve with existing rigid body simulators.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[animation with constraints]]></kw>
			<kw><![CDATA[physically based animation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.7</cat_node>
				<descriptor>Boundary value problems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Modeling and recovery of physical attributes</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003727.10003728</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Differential equations->Ordinary differential equations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P149785</person_id>
				<author_profile_id><![CDATA[81100620337]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jovan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Popovi&#263;]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P270322</person_id>
				<author_profile_id><![CDATA[81407593498]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Steven]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Seitz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP311232800</person_id>
				<author_profile_id><![CDATA[81100323014]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Erdmann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P310471</person_id>
				<author_profile_id><![CDATA[81100620346]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Zoran]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Popovi&#263;]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P18516</person_id>
				<author_profile_id><![CDATA[81100295587]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Witkin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>192168</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[David Baraff. Fast Contact Force Computation for Nonpenetrating Rigid Bodies. In Computer Graphics (Proceedings of SIGGRAPH 94), Annual Conference Series, pages 23-34. ACM SIGGRAPH, July 1994.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280821</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[David Baraff and Andrew Witkin. Large Steps in Cloth Simulation. In Computer Graphics (Proceedings of SIGGRAPH 98), Annual Conference Series, pages 43-54. ACM SIG- GRAPH, July 1998.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378509</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Ronen Barzel and Alan H. Barr. A Modeling System Based On Dynamic Constraints. In Computer Graphics (Proceedings of SIGGRAPIt 87), Annual Conference Series, pages 179-188. ACM SIGGRAPH, August 1988.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>274989</ref_obj_id>
				<ref_obj_pid>274976</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Ronen Barzel, John F. Hughes, and Daniel N. Wood. Plausible Motion Simulation for Computer Graphics Animation. In Computer Animation and Simulation '96, Proceedings of the Eurographics Workshop, pages 184-197, Poitiers, France, September 1996.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Dimitri R Bertsekas. Nonlinear Programming. Athena Scientific, Belmont, Massachusetts, 1995.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378531</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Lynne Shapiro Brotman and Arun N. Netravali. Motion Interpolation by Optimal Control. In Computer Graphics (Proceedings of SIGGRAPH 88), volume 26 of Annual Conference Series, pages 309-315. ACM SIGGRAPH, August 1988.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>344882</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Stephen Chenney and D. A. Forsyth. Sampling Plausible Solutions to Multi-body Constraint Problems. In Computer Graphics (Proceedings of SIGGRAPH 2000), Annual Conference Series. ACM SIGGRAPH, July 2000.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134083</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Michael F. Cohen. Interactive Spacetime Control for Animation. In Computer Graphics (Proceedings of SIGGRAPH 92), Annual Conference Series, pages 293-302. ACM SIG- GRAPH, July 1992.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>174150</ref_obj_id>
				<ref_obj_pid>174147</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Bruce Donald, Patrick Xavier, John Canny, and John Reif. Kinodynamic Motion Planning. Journal of the ACM, 40(5): 1048-1066, November 1993.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>244315</ref_obj_id>
				<ref_obj_pid>244304</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Nick Foster and Dimitri Metaxas. Realistic Animation of Liquids. Graphical Models and Image Processing, 5(58):471- 483, 1996.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258849</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Michael Garland and Paul S. Heckbert. Surface Simplification Using Quadric Error Metrics. In Computer Graphics (Proceedings of SIGGRAPH 97), Annual Conference Series, pages 209-216. ACM SIGGRAPH, August 1997.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Philip E Gill, Walter Murray, and Margaret H. Wright. Practical Optimization. Academic Press, London, 1989.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Michael Gleicher and Andrew Witkin. Differential Manipulation. In Graphics Interface, pages 61-67, June 1991.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134088</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Michael Gleicher and Andrew Witkin. Through-the-Lens Camera Control. In Computer Graphics (Proceedings of SIGGRAPH 92), Annual Conference Series, pages 331-340. ACM SIGGRAPH, July 1992.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>781241</ref_obj_id>
				<ref_obj_pid>781238</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[F. Sebastian Grassia. Practical Parameterization of Rotation Using the Exponential Map. Journal of Graphics Tools, 3(3):29-48, 1998.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280816</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Radek Grzeszczuk, Demetri Terzopoulos, and Geoffrey Hinton. NeuroAnimator: Fast Neural Network Emulation and Control of Physics-Based Models. In Computer Graphics (Proceedings of SIGGRAPH 98), Annual Conference Series, pages 9-20. ACM SIGGRAPH, July 1998.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218443</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Mikako Harada, Andrew Witkin, and David Baraff. Interactive Physically-Based Manipulation of Discrete/Continuous Models. In Computer Graphics (Proceedings of SIGGRAPH 95), Annual Conference Series, pages 199-208. ACM SIG- GRAPH, August 1995.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Wilfred Kaplan. Advanced Calculus. Addison-Wesley Publishing Company, Reading, Massachusetts, 1984.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192169</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Zicheng Liu, Steven J. Gortler, and Michael F. Cohen. Hierarchical Spacetime Control. In Computer Graphics (Proceedings of SIGGRAPH 94), Annual Conference Series, pages 35- 42. ACM SIGGRAPH, July 1994.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378528</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Matthew Moore and Jane Wilhelms. Collision Detection and Response for Computer Animation. In Computer Graphics (Proceedings of SIGGRAPH 88), Annual Conference Series, pages 289-298. ACM SIGGRAPH, August 1988.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166160</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[J. Thomas Ngo and Joe Marks. Spacetime Constraints Revisited. In Computer Graphics (Proceedings of SIGGRAPH 93), Annual Conference Series, pages 343-350. ACM SIG- GRAPH, August 1993.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311550</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[James F. O'Brien and Jessica K. Hodgins. Graphical Modeling and Animation of Brittle Fracture. In Computer Graphics (Proceedings of SIGGRAPH 99), Annual Conference Series, pages 111-120. ACM SIGGRAPH, August 1999.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311536</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Zoran Popovid and Andrew Witkin. Physically Based Motion Transformation. In Computer Graphics (Proceedings of SIG- GRAPH 99), Annual Conference Series, pages 11-20. ACM SIGGRAPH, August 1999.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311548</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Jos Stam. Stable Fluids. In Computer Graphics (Proceedings of SIGGRAPH 99), Annual Conference Series, pages 121- 128. ACM SIGGRAPH, August 1999.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Robert F. Stengel. Optimal Control and Estimation. Dover Books on Advanced Mathematics, New York, 1994.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Keith R. Symon. Mechanics, Third Edition. Addison-Wesley Publishing Company, Reading, Massachussetts, 1971.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Diane Tang, J. Thomas Ngo, and Joe Marks. N-Body Spacetime Constraints. Journal of Visualization and Computer Animation, 6:143-154, 1995.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>91400</ref_obj_id>
				<ref_obj_pid>91385</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Andrew Witkin, Michael Gleicher, and William Welch. Interactive Dynamics. In Proceedings of the 1990 symposium on Interactive 3D graphics, pages 11-21, March 1990.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378507</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Andrew Witkin and Michael Kass. Spacetime Constraints. In Computer Graphics (Proceedings of SIGGRAPH 88), Annual Conference Series, pages 159-168. ACM SIGGRAPH, August 1988.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Interactive Manipulation of Rigid Body Simulations C Jovan Popovic´Steven M. Seitz Michael Erdmann Zoran 
Popovic´Andrew Witkin Carnegie Mellon University University of Washington Pixar Animation Studios Abstract 
Physical simulation of dynamic objects has become commonplace in computer graphics because it produces 
highly realistic anima­tions. In this paradigm the animator provides few physical param­eters such as 
the objects initial positions and velocities, and the simulator automatically generates realistic motions. 
The resulting motion, however, is dif.cult to control because even a small adjust­ment of the input parameters 
can drastically affect the subsequent motion. Furthermore, the animator often wishes to change the end­result 
of the motion instead of the initial physical parameters. We describe a novel interactive technique for 
intuitive manipula­tion of rigid multi-body simulations. Using our system, the anima­tor can select bodies 
at any time and simply drag them to desired locations. In response, the system computes the required 
physical parameters and simulates the resulting motion. Surface characteris­tics such as normals and 
elasticity coef.cients can also be automat­ically adjusted to provide a greater range of feasible motions, 
if the animator so desires. Because the entire simulation editing process runs at interactive speeds, 
the animator can rapidly design complex physical animations that would be dif.cult to achieve with existing 
rigid body simulators. CR Categories: I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism 
Animation; I.3.6 [Computer Graphics]: Methodology and Techniques Interaction techniques; G.1.7 [Nu­merical 
Analysis]: Ordinary Differential Equations Boundary value problems Keywords: Physically Based Animation, 
Animation with Con­straints 1 Introduction Physical simulation programs provide powerful tools for creating 
realistic motion in animated shorts and feature .lms. These meth­ods enable quick and easy generation 
of complex physical behav­iors such as a ball bouncing, window breaking [22], cloth folding [2], and 
water .owing [10, 24]. An attractive feature of physical simulation is that the animation is generated 
automatically the an­ 'Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 15213 3891. http://www.cs.cmu.edu/ 
jovan Permission to make digital or hard copies of part or all of this work or personal or classroom 
use is granted without fee provided that copies are not made or distributed for profit or commercial 
advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, 
to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or 
a fee. SIGGRAPH 2000, New Orleans, LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 Figure 1: 
The animator manipulates the simulation by .rst .xing the hat s landing position on the coatrack with 
a nail constraint. While the animator rotates the hat at an earlier time to achieve the desired spin, 
the constraint maintains the desired landing location. imator only needs to specify a few physical parameters 
such as ini­tial positions and velocities. Despite the appeal of simulation techniques, their primary 
draw­back is lack of intuitive control over the resulting motion. The ani­mator often wishes to adjust 
the motion to achieve a desired effect such as a new end-position or a more pleasing look. However, di­rectly 
altering the underlying physical parameters to try to achieve the desired effect is often cumbersome 
and nonintuitive. In many cases, the animator would prefer to edit the animation itself, by di­rect manipulation 
of positions and velocities. We introduce a novel interactive technique for manipulating rigid body simulations. 
Throughout the interaction, our system displays the entire trajectory of all objects in the scene. The 
animator is free to manipulate the entire motion by grabbing and changing the state of the object (position, 
velocity, etc.) at any location on its trajectory. For example, suppose the animator wants to design 
a scene in which an actor successfully tosses his hat onto a nearby coatrack, but instead has an animation 
of the hat falling to the .oor. In our paradigm, the animator .rst selects the hat at its landing po­sition 
and simply drags it onto the coatrack. There are many ways in which the hat can land on the coatrack 
and the current motion may not have the desired style. The animator can adjust the style by .rst .xing 
the landing position on the coatrack to ensure the de­sired landing location and then rotating the hat 
at an earlier time until the hat motion achieves the desired spin (Figure 1). This hat example illustrates 
the use of position constraints to control rigid body animations. More generally, our system pro­vides 
the ability to set arbitrary position, orientation, and velocity constraints on multiple objects at multiple 
points in time. Further­more, we also provide .oating time constraints that may be satis.ed at any point 
in time. For example, an animator can adjust where a mug hits the ground on its third bounce, without 
.xing the time when that bounce occurs. A key problem in controlling dynamic simulations is obtaining 
suf.cient degrees of freedom to avoid over-constrained problems. In our system the animator may add degrees 
of freedom by vary­ing physical parameters that specify the internal properties of the environment, including 
shapes, masses, surface normals, elastic­ity coef.cients, and moments of inertia. Often the best choice 
for these parameters is not at all obvious to the animator but yet can have a very dramatic effect on 
the resulting animation. Our sys­tem automatically adjusts these physical parameters according to the 
animator s desired effect. We have found this capability to be useful even in under-constrained problems. 
In particular, motion in chaotic systems is highly sensitive to small perturbations in the ini­tial conditions. 
Adding control variables near desired manipulation times (i.e. variation of surface normals at the previous 
collision) im­proves the conditioning without affecting the perceived realism of the animation [4]. Furthermore, 
the additional parameters increase the range of feasible motions, enhancing the animator s ability to 
create interesting effects. Interaction is an integral component of our approach. The ani­mator is able 
to directly control the motion without manipulating the underlying physical parameters, and immediately 
sees the re­sults of her adjustments. As a result, she can quickly explore the space of possible motions 
to achieve the desired behavior. Unlike previous motion-construction tools [29, 8, 21, 19, 23], our system 
does not evaluate the quality of motion with an objective criterion such as time-optimal motion. Instead, 
the animator imparts her aes­thetic or other subjective criteria and interactively guides the system 
towards the desired motion. Internally, our system represents the entire motion of bodies by the physical 
parameters that control the simulation (i.e., initial posi­tions and velocities, surface normal variations 
and other parameters included by the animator). As the animator interactively manip­ulates the motion, 
the system computes the new physical param­eters that achieve the desired motion update. This is achieved 
in real time using a fast differential update procedure in concert with a rigid body simulator. Motion 
discontinuities pose an additional challenge (e.g. when a point of collision changes to a different facet 
on a body s polyhedral mesh) because the motion changes abruptly. When this happens, our system performs 
a local discrete search in physical parameter space to compute the motion that most closely complies 
with the desired adjustments. The remainder of the paper is divided into six sections. In Sec­tion 2, 
we discuss related work. We outline the basic algorithm in Section 3, and discuss further details in 
Section 4 and Section 5. In Section 6, we outline the speci.cs of our prototype implemen­tation and report 
on the experimental results, and in Section 7, we conclude and describe directions for future work. 
 2 Related Work Dynamics and motion of mechanical systems are important to many .elds. Optimal control 
theory provides the groundwork for maxi­mizing the performance of evolving dynamic systems [25]. In robot 
path planning, kinodynamic planning refers to the problem of com­puting robot paths that satisfy both 
dynamic and kinematic con­straints [9]. In computer graphics, the spacetime constraints tech­nique for 
animation of characters computes optimal motion subject to constraints [29, 8, 21, 19, 23]. Other techniques 
[3, 6, 16] also rely on gradient information to compute the motion that satis.es certain constraints. 
All of the above techniques solve for the actu­ating forces which produce a motion. Because of this, 
they do not directly apply to our problem of controlling the rigid body simula­tions because we wish 
to control passive objects (i.e. objects with­out any self-propelling forces). Several researchers have 
addressed the inverse problem of con­structing a dynamic rigid body motion given the desired body con­.gurations. 
Tang et al. [27] formulated this inverse problem as an optimization task and extended the genetic algorithm 
technique to compute solutions for a class of 2-D N-body problems. For a 2-D billiards simulation, Barzel 
and colleagues [4] computed successful shots using a backward search from the desired .nal locations 
of billiard balls. Chenney et al. [7] applied the Markov Chain Monte Carlo (MCMC) method to construct 
3-D motions that achieve de­sired body con.gurations. The MCMC technique excels at con­structing motions 
for systems with chaotic behavior such as the mo­tion of pins after collision with a bowling ball. The 
main drawback of these approaches is lack of interactivity: these systems may re­quire several hours 
to construct a solution. If the animator does not like the resulting motion, she must adjust the desired 
body con.gu­rations and start again. We argue that interactivity is essential when aesthetics is a primary 
concern. Our interactive technique is related to the method for geometric modeling described by Witkin 
et al. [28]. Similar techniques have also been devised for drawing applications [13], interactive cam­era 
control [14] and others. In its treatment of motion discontinu­ities, our approach most closely resembles 
that of Harada et al. [17], which combines continuous and discrete optimization for applica­tions in 
architectural design. In this approach, when the imposed architectural constraints can no longer be enforced 
with the contin­uous parameters, the solver performs a local discrete search to .nd a new room arrangement 
in which the constraints are satis.ed.  3 Interactive Manipulation Our algorithm computes the required 
physical parameters so that the resulting motion satis.es desired constraints. In this section, we de.ne 
some basic concepts and give a top-level description of our algorithm. 3.1 Simulation Function Following 
the Lagrangian approach, we describe mechanical sys­tems in terms of their generalized coordinates and 
velocities [26]. A system of one or more rigid bodies is described by a generalized state vector q whose 
components are the generalized coordinates and velocities of the bodies in the system. The behavior of 
a system is described by a set of ordinary second order differential equations [26], which we write in 
vector form as a coupled .rst order differ­ential equation, d q(t)=F(t, q(t)), (1) dt where F(t, q(t))is 
derived from the Newton s law (e.g. see Equa­tion 8). As mentioned in Section 1, our technique varies 
several physical parameters in addition to the initial position and velocity q0 to modify the simulation. 
We encode all of these parameters in the control vector u, and extend the differential equation appro­priately: 
d q(t)=F(t, q(t), u). (2) dt This equation of motion completely describes the system in free .ight (i.e. 
when there are no collisions): integrating Equation 2 yields . t q(t)=q0(u)+ F(t, q(t), u)dt. (3) t0 
Collisions can be handled in a number of ways, but for computer animations the simple Poisson collision 
model suf.ces [20]. This model can represent elastic and inelastic impacts by applying in­stantaneous 
impulses to the colliding bodies. The system simulates the motion during free .ight by numerically solving 
Equation 2. At collision times additional impulses are applied to the system. Because the control vector 
u includes physical parameters such as surface normals at collisions and elasticity coef.cients, the 
impulse I(q - , u) directly depends on the control vector u. At collisions the simulator maps the generalized 
state an instant before the col­lision q - into the state an instant after the collision q + (e.g. see 
Equation 10): + -- q = q + I(q , u). (4) More abstractly, given the control vector u the rigid body 
simu­lator computes the simulation function S, which speci.es the state of the bodies in the world at 
every point in time: Figure 2: The simulation function for the motion of a particle bounce. (5) q(t)= 
S(t, u). In principle, the animator could manipulate the motion q(t) by ad­ describe the particle s path 
in free .ight. The solution to this differ­ justing the control vector u. However, such a form of control 
would ential equation yields the simulation function: be tedious because the relation between u and q(t) 
is complex and nonintuitive. Instead, we would like to allow the animator to spec­ S(t, q0)= . v ()tx 
v(t) = ( (( v . .. ) . . (9) 1-. 0 2 gt ify the state of bodies q(ti) at speci.c times ti = t0,... ,tn,and 
let the algorithm compute the control vector u that produces the de­ sired motion. This is a dif.cult 
problem [27, 7] for three reasons. First, the domain of the simulation function S is high-dimensional: 
x(0) + v(0)t + v 2 0 v(0) + -gt for a single 3-D body, the components of the generalized state q are 
the body s position, orientation, linear, and angular velocity If the particle collides with an immovable 
obstacle, the Poisson (i.e. q E R3 × SO(3) × R3 × R3). Second the simulation func­collision model applies 
an impulse to change the particle s velocity. tion is highly nonlinear. A consequence of the integral 
nature of the simulation function is that small changes in the initial condi­tions can result in drastic 
modi.cations of the motion. Third, the simulation function is not continuous. Each collision event (e.g., 
different vertices of an object colliding with the ground) bifurcates the simulation function. We adopt 
a differential approach for manipulating the simulation function. The animator adjusts the motion by 
specifying a differ­ential change of motion .qi in the generalized state q(ti) at time the animator gradually 
guides the system toward the desired solu­tion. To compute a new control vector that reshapes the motion 
in compliance with the differential changes .qi, we locally linearize Thesystemrespondsbyreshapingthecurrentmotiontocom-t.iplywiththeadjustments.Continuingtheinteractivemanipulation, 
Equation5, ( aS(ti,u) .qi = .u. (6) au We combine all animator-speci.ed constraints into a linear system 
which we solve for .u by conjugate gradient technique. The dif­ferential vector .u describes the direction 
in which to change the current control vector u to obtain the desired motion change .qi. The differential 
update is simply a small step in the computed di­rection, 1 u= u + ..u. (7) Given the new, updated control 
vector u1, a rigid body simulator computes the new motion and displays the result. At this point the 
entire process repeats.  3.2 2-D Particle Example To help provide an intuition for the issues underlying 
our approach, we begin with an illustrative example. Suppose that a single 2-D particle moves under the 
action of gravity. The generalized state q E R4 encodes the particle s position x E R2 and velocity v 
E R2.If g is the acceleration of gravity, the equations of motion, .) . d x(t) (8) . vv(t) 0 = , 
dt v(t) -g For frictionless collisions, the impulse acts in the direction of the surface normal n at 
the point of collision. The equation, + -- v = v - 2(n · v )n, (10) applies an impulse to instantaneously 
change the particle s velocity before the collision v - into its velocity after the collision v + . Given 
these analytical expressions for the particle s motion, we can plot the space of all possible trajectories 
for the particle as a function of the initial conditions and the environment. For concrete­ness, suppose 
the particle collides with a single parabolic obstacle. For notational convenience, we introduce a unit 
circle around the obstacle: the particle enters the circle at some angle p0 with unit velocity vector 
directed towards a point at height h above the tip, bounces off the obstacle, and exits the circle at 
another angle pf (Figure 2). Our objective is to determine pf as a function of p0 and h. In this example, 
the simulation function S : R2 -R maps the control vector u =(p0,h) into the particle s .nal, exit position 
pf . Given an initial entering p0 and exiting pf state, our gradient-based interactive technique can 
smoothly transform this solution to one which satis.es one or more constraints, for example to achieve 
a different exiting state pf1. Our technique converges easily because the simulation function is smooth 
over the domain of control pa­rameters (Figure 2). The general motion of many rigid bodies is much like 
this sim­ple particle example. To describe the state of a single 3-D rigid body, we increase the dimensions 
of the generalized state, adding the components of orientation, angular velocity, and extending the position 
and linear velocity to 3-D. Two or more rigid bodies are modeled by adding additional components to the 
generalized state. Surface parameters such as normals and elasticity coef.cients may also be added, if 
desired. Note that the number of rigid objects is not explicitly represented, we are merely expressing 
the cumulative degrees of freedom of the system. Our implementation makes use of this representation 
to enable complex multi-object simulations with the same computation techniques and data structures used 
to implement particle simulations.  4 Manipulation without Discontinuities The algorithm outlined in 
Section 3 relies on the ef.cient computa­ tion of the Jacobian matrix aS(ti, u)/au. Computing the Jacobian 
matrix with .nite differences is expensive because of the need to perform multiple simulations. In addition, 
the inaccuracies of the .nite differences approach would be prohibitive for our approach. Instead we 
use a specialized automatic differentiation technique. We decompose the simulation function S into analytically 
differ­entiable functions and numerically compose the Jacobian matrix using the chain rule. For example, 
suppose that a single collision occurs at time tc and the simulation function S(tf , u)describes the 
body s state at some time after the collision tf >tc. We decom­pose S(tf , u)into three functions: Ftc 
: pre-collision free-.ight function, which maps the initial con­ ditions and perhaps additional elements 
of the control vec­ tor u into the body s state at tc, an instant before collision (e.g. Equation 9 for 
2-D particles); Ctc : collision function, which applies the impulse and maps the body s state an instant 
before collision into the body s state at tc, an instant after collision (e.g. Equation 10 for 2-D parti­cles); 
Ftf : post-collision free-.ight function, which maps the body s state an instant after the collision 
into the body s state at tf . The functional composition expressing S(tf , u)becomes:1 S(tf , u)=Ftf 
(u) Ctc (u) Ftc (u). (11) Although the free-.ight motion of the particle in Section 3.2 has a closed-form 
and is analytically differentiable, this is generally not the case for 3-D rigid body motion.2 To compute 
the derivatives of aFtc (u)/au, we .rst integrate the equations of motion (Equa­tion 1) until time tc, 
tc(u) Ftc (u)=q0(u)+ F(t, q, u)dt, t0 and take the derivative of both sides with respect to u v. . tc(u) 
aFtc (u) a = q0(u)+ F(t, q, u)dt . au au t0 To evaluate this expression we apply the Leibnitz rule [18] 
to inter­change the integral and the derivative:3 aF (u) dtc(u) tc =F(tc(u), q, u)+ au du tc(u) (12) 
aq0(u) aF(t, q, u) + dt. au au t0 The simulator computes the value of F(tc(u), q, u) at the collision. 
To compute the collision time derivative dtc(u)/du we de.ne a smooth collision event function E(t, q) 
such that at the collision time tc(u), E(tc(u), q)=0. (13) For the 2-D particle, for example, the collision 
event function E can be de.ned as the signed distance function between the particle and the obstacle. 
1The Equation 11 is written in this form for notational convenience. More precisely, this equation is 
S(tf , u)= Ftf (u, Ctc (u, Ftc (u))). 2For the special case of freely rotating 3-D rigid body (no torques), 
there is an analytic Poinsot s solution [26]. 3The conditions for applying the Leibnitz rule require 
that F is contin­uous and has a continuous derivative aF/au. These conditions are met under reasonable 
assumptions about external forces. Differentiating Equation 13 and solving for the collision time derivative 
we obtain dtc(u)(aE/aq)·(aq/au) =- . (14) du aE/at The derivatives on the right-hand side of Equation 
14 are computed analytically, with the exception of aq/au, which is de.ned by the integral expression 
(second and third term in the sum) in Equa­tion 12. We compute this integral expression by numerically 
in­tegrating differential equation daq(t) aF(t, q, u) = , dt au au until time tc with the initial condition 
aq0(u)/au. The computation of aFtf (u)/au is similar: we apply the Leib­nitz rule to obtain aF (u) dtc(u) 
tf =-F(tc(u), q, u)+ au du aCtc (u) tf aF(t, q, u) + dt au au tc(u) and evaluate the right-hand terms 
as before. To compute the derivatives of aCtc (u)/au we differentiate the Equation 4: aC(u) aF(u) aI(q 
- , u) tc = tc + . au au au Once all derivatives of the sub-functions have been computed we .nd the 
simulation function derivatives by applying the chain rule: v . aS(tf , u) aFtf aCtc aFtc aCtc aFtf = 
++ au aCaFau au au tc tc Although we have shown the derivative computations for the composition of three 
phases, an arbitrary number of such phases can be composed in an analogous manner. 4.1 Differential Update 
Having computed the Jacobians, we can formulate the constraint equations (Equation 6). Given n such equations, 
we solve for the differential vector .u. Because this system is often under­constrained (Section 1), 
we solve the following minimization in­stead: min .u T M .u +dT .u(15) ou aS(t1, u) subject to .q1 = 
.u au . . . aS(tn, u) .qn = .u. au The minimized objective function has a dual purpose: it seeks the 
smallest change from the current state of the simulation and the smallest deviation from the desired 
values of the simulation param­eters such as surface normals at the collision. The diagonal matrix M 
describes the relative scale between parameters in the control vector u. The animator can describe the 
desired scaling to specify how the system should change the parameters. For example, the animator may 
instruct the system to favor changing the initial posi­tion rather than the initial velocity of a body. 
The vector d de.nes desired values for physical parameters. For example, if the system varies the surface 
normal at a collision we can specify the true ge­ometric normal as the desired value and the system will 
attempt to stay as close as possible once all constraints are satis.ed to the true surface normal. Speci.cally, 
if .ud is the desired change in the control vector u then setting d = -.ud and optimizing Equa­tion 15 
will minimize (.u-.ud)T (.u-.ud). Because the objec­tive is quadratic and all constraints are linear, 
we use the Lagrangian multipliers to reformulate the minimization as a linear system and solve for .u 
[12]. Our technique is a form of gradient descent: we continuously linearize the problem and move in 
the gradient direction .u. For a large gradient stepsize ., the gradient descent method may diverge. 
Line minimization is the preferred method for choosing the step­size in a gradient method, but it requires 
considerable computation. In practice, a small .xed stepsize has good convergence properties while also 
enabling interactive update rates. The gradient descent converges only to a local optimum [5]. Lo­cal 
convergence is suf.cient and effective for our interactive set­ting: the animator drags a body towards 
the intended position guiding the system out of undesirable local minima and the sys­tem quickly reshapes 
the motion to comply with the change. 4.2 Manipulation Constraints When the animator speci.es the constraints, 
the system maps these constraints to the appropriate differential changes of motion .qi. We distinguish 
three types of constraints: state constraints, expres­sion constraints, and .oating constraints. State 
constraints occur when the animator nails down objects (e.g., .xing position, orientation, linear velocity 
or angular velocity to speci.c values). Suppose that the animator wants the body A at time ti to have 
the state q 1 ,and that qA is a subset of the gener- A alized state of the whole system q which describes 
the state of the body A. We write the desired differential change as qA 1 -qA(ti). In this case the nail 
constraint is enforced at a speci.c time instant ti. Expression constraints are generalizations of the 
state con­straints. Any differentiable expression of the generalized state q can represent a constraint. 
For example, the animator can equate the speed of two bodies with the constraint |v(qA(ti))|-|v(qB(ti))|. 
Both state and expression constraints can be speci.ed without .xing the time of evaluation ti. The animator 
can express a con­straint at a particular event say, the .fth collision in the simula­tion. Time of collision 
tc(u) is not .xed and thus the time of the constraint can .oat. For example, we can reduce the angular 
ve­locity . of body A with the constraint -.(qA(ti)) · .(qA(ti)). Subsequent modi.cation of various simulation 
parameters will change the time at which the collision occurs, but the .oating con­straint will still 
be enforced.  5 Manipulation with Discontinuities When the simulation function is continuous, the interactive 
manip­ulation technique described in Section 4 effectively converges to the desired motion. In general, 
however, the simulation function con­tains discontinuities that may cause this technique to diverge. 
In this section we describe a method for improving the convergence for piecewise continuous simulation 
functions. The simulation function is discontinuous whenever polygonal (piecewise linear) meshes are 
involved in collisions. For example, suppose we modify the particle example from Section 4 and replace 
the smooth, curved obstacle with a piecewise linear polygonal curve (Figure 3). As long as the particle 
collides with the same edge, the simulation function remains continuous. On the other hand, when the 
particle collides with a different edge, the surface normal on the Figure 3: Sample particle bounce motions 
with polygonal obstacle and the corresponding piecewise smooth simulation function 4 p0 3 3.5 2.5 pf 
1.5 2 pf 1 0.5 0 -0.5 -1 0.5 h 0 -0.5 h 0 -1 0.5 1 1.5 p0 2 2.5 3 3.5 obstacle changes abruptly and 
thus the collision impulse applied in Equation 10 is discontinuous. This abrupt change carries over to 
the subsequent particle motion and corresponds to a discontinuity in the simulation function. We cannot 
disregard piecewise linear approximations because the interactive rigid body simulators often approximate 
smooth geometric models with polygonal (piecewise linear) meshes mostly because meshes facilitate faster 
and easier collision detection. In general, the simulation function is piecewise continuous. A connected 
set of control vectors for which the simulation function is continuous de.nes a connected component in 
the control space. We call these connected components smooth components because on a smooth component 
the simulation function is continuously dif­ferentiable. For example, a set of control vectors for which 
the par­ticle collides with the same edge of the obstacle de.nes a smooth component (Figure 3). In this 
example, the four smooth compo­nents correspond to motions of the particle colliding with each of the 
four edges. The .gure emphasizes two main problems caused by discontinuities: the loss of physical feasibility 
and degradation of convergence. We describe these problems and our solutions in the remainder of this 
section. 5.1 Physical Feasibility As shown in Figure 3, the polygonal approximation of the obstacle restricts 
the physically feasible exit points for the particle. Note that some values of pf are unattainable because 
the surface normal near the origin is discontinuous: the particle cannot exit at the section of the circle 
directly above the origin (pf near ./2). This restriction of feasible results becomes especially evident 
when the animator over-constrains the system with many desired body con.gurations. Finer polygonal approximations 
reduce the gaps in the piecewise smooth function, but overly .ne approximations increase the colli­sion 
detection time and reduce interactivity. Our approach to this problem is twofold. First, we introduce 
ad­ditional control parameters to vary the surface normals on a polyg­onal mesh and to simulate a collision 
with a smooth obstacle. If the mesh approximates a smooth surface the desired normal can be computed 
from a smooth local interpolant or, if available, from the true smooth surface. The normal can then be 
adjusted dynamically by including the normal deviation within the control vector u.As Figure 4 illustrates 
varying surface normals extends the range of smooth components to increase the physically feasible regions. 
Second, we use curvature-dependent polygonal approximations in our simulations because they keep the 
facet count low for fast collision detection and simulation, but also provide good .rst-order approximations 
to the original surface [11]. For discontinuities due to polygonal approximations of smooth surfaces, 
the computed dif­ferential change .u continues to contain valuable information. Ap­proximating smooth 
surfaces with polygonal meshes is well stud­ied in computer graphics. In general, good approximations 
allo­cate many facets to areas of high surface curvature and fewer facets to near-planar surface regions. 
For these polygonal meshes, de­spite the discontinuity in the surface normals, the currently collid­ 
4 3.5 p0 3 pf 2.5 2 pf1.5 1 0.5 0 -0.5 -1 0.5 h 3.5 -1 0 p0 Figure 4: Varying surface normals reduces 
the gaps to increase physically feasible regions. ing facet is also a good .rst-order approximation to 
the underlying surface. In this case the differential change .u continues to be a good predictor for 
the differential update because the .rst-order ap­proximation is suf.ciently accurate for linear Equation 
6. 5.2 Convergence The interactive technique of Section 4 converges to the desired motion if there exists 
a path from the initial to the desired con­trol vector within a single smooth component. With discontinu­ities, 
such a path may not exist. The discrete search must guide the control vector between the appropriate 
components, piecing to­gether a path that crosses discontinuities. Especially in higher di­mensions, 
this is a daunting task for an interactive system. In gen­eral, the search must take into account physically 
feasible regions and jump to smooth components in possibly distant regions of a high-dimensional control 
space. The most important criterion for selecting smooth components is that they facilitate convergence 
to the desired motion. In addition, unless instructed otherwise, the components should preserve the style 
of the current motion, as that may be of primary importance. For example, if an animator desires a successful 
off-the-backboard basketball shot, it is unde­sirable to jump to a smooth component corresponding to 
a direct, nothing-but-net motion. Lastly, the discrete search must com­plete quickly to maintain interactivity. 
Our solution relies on two concepts: sampling and interaction. Sampling In the presence of discontinuities 
our technique be­comes more sensitive to the stepsize . and the direction .u in the differential update 
(Equation 7). With a large stepsize .,the gradient-descent method may diverge. The approximation errors 
in .u also adversely affect convergence. To improve convergence, we use sampling to .nd the best values 
for these parameters. To .nd a good stepsize . we use a form of the successive stepsize re­duction technique:4 
our discrete search chooses an initial stepsize . and reduces it a few times to select the motion that 
most closely matches the desired result. Convergence results for gradient meth­ods with non-random errors, 
such as approximation errors in .u, exist [5], but there are no standard techniques for improving the 
convergence. Recall from Section 5.1 that for discontinuities due to polygonal approximations, the update 
vector .u is a good heuris­tic for the new samples. Thus, when the simulation is directed off the edge 
of the smooth component, our system samples the con­trol space from the normal distribution centered 
around the sug­gested update .u. Each such sample may produce a point on a new smooth component. We evaluate 
how well the corresponding mo­tions comply with the constraints and jump to the most promising component. 
The animator perceives the jump as a minor pop in the resulting motion and typically, following the jump, 
the contin­uous manipulation continues. The sampling procedure also causes 4Successive stepsize reduction 
is not theoretically sound because the im­provement at each iteration is not enough to guarantee convergence. 
Never­theless, it often works in practice [5]. a momentary lag. While the lag could be reduced with a 
faster im­plementation, the visual pop is unavoidable in situations where the underlying motion is discontinuous. 
If sampling does not produce any reasonable smooth component, the system remains within the current smooth 
component. The animator is thus blocked from ad­justing the motion in a particular way, but can continue 
to guide the system in a different way. Interaction Of course, to guarantee convergence we would have 
to search through the entire control space. Our system does not ad­dress this more general problem the 
high dimension of the control space makes the search especially dif.cult. Instead, our technique relies 
on the animator to guide the system to a motion that satis.es given constraints. For example, a body 
that initially .ies over a wall may have to bounce off the wall and .y in the opposite direction to accomplish 
the desired constraint. Our technique will not make these transformations automatically. For a large 
class of motion de­sign tasks, this behavior is desirable and suf.cient. The interaction allows the animator 
to quickly experiment and guide the system to­ward the desired collision sequence. For example, to transform 
the motion of a basketball during a successful free throw, the anima­tor may want to bounce the ball 
off the backboard before it goes through the hoop. In this case, the animator .rst guides the ball into 
a backboard collision, and then guides it through the hoop. We emphasize that the single constraint specifying 
a successful shot does not uniquely determine the desired collision con.gurations: the ball may bounce 
off the backboard, off the .oor or even off the scoreboard. An automatic system would have to choose 
the desired motion (or keep track of a possibly exponential number of motions) according to some objective 
criteria. Instead, our system provides the animator with interactive, direct control over the motion 
and allows her to guide the system to the appropriate solution.  6 Implementation and Results Implementation 
The implementation of our system is decom­posed into three parts: (1) a differential control module, 
(2) a rigid body dynamics simulator, and (3) a user interface for motion dis­play and editing. The control 
of the system is animator-driven. In response to an edit (a mouse event), the control module recomputes 
the control parameters u needed to accomplish the desired motion adjustments. These parameters are then 
provided to the physical simulator, which recomputes the motion and updates the display. We use the general-purpose 
rigid body simulator developed by Baraff [1]. Alternatively, specialized simulators could be used that 
provide tighter integration with the differential control module. Our manipulation tool controls the 
simulator at two points: (1) it pro­vides the control vector u for the simulation and (2) it modi.es 
the impulses at collisions using the modi.ed surface normals and elas­ticity coef.cients. The simulator, 
in turn, computes the new motion and returns the new collision events. The computed motion is used to 
update the display and the collisions are used to de.ne a new expression for the equations of motion 
(Equation 11). For example, a single-bounce motion has a decomposition corre­sponding to Equation 11. 
A change in the control parameters may cause another bounce to occur. In this case, the simulator detects 
the additional collision. In response, our system automatically updates the equations of motion by adding 
an additional collision function and two more .ight phases to expression in Equation 11. We use the exponential 
map parameterization of orientations [15] in the control vector u, .nding that it yields better results 
than the normalized quaternions. Examples This section demonstrates the use of our system to construct 
several physically based animations. All of these exam­ples were created by direct manipulation in real-time, 
and each re­quired between two and ten minutes of editing to achieve the de­sired animation. For each 
of these examples, Figure 5 shows the animations before interaction, at an intermediate point, and after 
the desired motion is obtained. Each image in the .gure displays the entire simulation by tracing out 
the trajectories of one or two points on the moving objects (shown in black). After experiment­ing with 
a variety of different interfaces, we have found that this display minimizes clutter yet provides the 
animator with a sense of the cumulative motion that is suf.cient for most interaction tasks. Of course 
the animator can choose to view the complete motion as a traditional frame sequence at any time during 
the interaction. The objective of the .rst example is to have two eggs collide in the air and land successfully 
into two buckets on the ground (Fig­ure 5(a)). Creating such a motion by simply adjusting initial posi­tions 
and velocities of the objects would be extremely dif.cult due to the complexity of the motion and the 
constraint that the buckets themselves cannot be moved. In contrast, the desired animation is easily 
created from scratch using our interactive manipulation tech­nique. First, the starting positions of 
the eggs are .xed, and the velocities and orientations are assigned arbitrarily. By clicking at a point 
on its .ight path, the animator then interactively drags the .rst egg s trajectory towards the second 
egg so that the two objects collide in the air. Running at roughly 20 frames per second, the system computes 
the required changes in the initial orientation and velocity of both eggs to achieve the desired motion 
updates. Once one egg is in the bucket, the animator applies a nail constraint to .x its ending state 
and then drags the second egg into the other bucket. In the second example, the animator s goal is to 
drop a plank onto two supports to form a table. The problem is made more dif.cult by requiring the plank 
to collide with a pyramid object in the air, prior to landing centered on the supports. This example 
requires the sys­tem to solve for the initial plank position, orientation, and velocity (both linear 
and angular) in order to achieve the desired con.gura­tion after the collision. Like the previous example, 
this is achieved by allowing the animator to directly manipulate the plank s desired position and orientation 
while the system interactively computes the corresponding physical parameters. This manipulation occurs 
in two steps: .rst the animator selects the plank after it collides with the pyramid, and positions it 
above the supports. Second, the plank s orientation is interactively aligned so that it lands squarely 
on the supports (Figure 5(b)). The third example demonstrates the use of normal and elasticity parameters 
to aid editing operations, and the use of .oating time constraints. Suppose the animator wishes to keep 
a falling mug from tipping over without changing its initial position, orientation, or velocity. This 
is accomplished by adding new control parameters to control the surface normal and elasticity parameters 
of the .oor at the points where the mug hits the .oor. To keep the mug from tipping over, the animator 
.rst straightens the mug so that it is up­right at the fourth bounce. The system accommodates this change 
by modifying the .oor normal at the third bounce. Note that this change in the normal will typically 
alter the time at which the fourth bounce occurs, requiring a .oating time constraint (Section 4.2). 
Due to its angular velocity, however, the mug still tips over (Fig­ure 5(c), center). This is prevented 
by constraining its angular ve­locity to be zero after the fourth bounce, resulting in a motion where 
the mug does not tip over (right). The changes in surface normals are perceived as changes in the surface 
texture of the .oor. The .nal example illustrates the ability to edit the style of an animation by modifying 
a previously constructed motion. In this example, a scissors is thrown into the air and lands on a coatrack 
(Figure 5(d)). This initial animation is constructed by starting with a motion in which the scissors 
falls on the .oor and then interac­tively dragging it to the coatrack. By selecting and manipulating 
the scissors at different points in its trajectory, this motion is trans­formed into one in which the 
scissors .rst bounces off the ground, performs a somersault in the air, and still successfully lands 
on the coatrack. This example demonstrates how progressively more inter­esting and complex motions may 
be created from simpler motions using our interactive editing approach. 7 Conclusion In this paper, 
we have described a new interactive technique for manipulating rigid multi-body simulations. Instead 
of changing the simulation parameters directly, we provide an intuitive inter­face through which the 
animator can manipulate the position and velocity of objects directly. Using our system, the animator 
can rapidly design dif.cult physical animations that would be dif.cult to achieve with existing rigid 
body simulators. For some design tasks, the interactive paradigm is not the most effective. For example, 
the animator may be hard pressed to chart out the sequence of collisions that will lead all billiard 
balls into pockets. In general, this is a dif.cult problem that, in some cases, may not even have a solution. 
Other motion-construction tech­niques [7, 27] address these problems and, in some scenarios, con­struct 
appropriate motions after extensive computation. We envi­sion a hybrid system that integrates a motion-construction 
tech­nique with our interactive manipulation tool to improve the effec­tiveness of the interactive paradigm. 
For the Jacobian evaluation (Section 4), our technique assumes that the collision function is analytically 
differentiable. This is not always the case with the rigid body simulator we use in our proto­type implementation. 
During a resting (i.e. sustained) contact or for multiple simultaneous collisions the applied impulses 
are solutions to a linear complementarity problem (LCP) [1]. In general, LCP problems do not have closed-form, 
analytically differentiable solu­tions. There are many alternative formulations which may facilitate 
analytic differentiation. Further, the interactive manipulation tech­nique would bene.t from a specialized 
rigid body simulator. For example, the simulator could simultaneously integrate both body states and 
their derivatives. Lastly, the interactive manipulation is not possible for all rigid multi-body systems: 
in some scenarios simulation alone requires considerable computational time. In these cases the animators 
will have to resort to an off-line motion-construction technique.  Acknowledgements We would like to 
thank the members of the CMU graphics lab for helping us shape our ideas. Sebastian Grassia and Paul 
Heckbert provided valuable suggestions and insights. We especially thank David Baraff for kindly providing 
the rigid body simulator. We also thank Elly Winner and Ivan Soki´ c for helping us in the .nal stages 
of our paper submission. The support of the Microsoft Corporation is gratefully acknowledged.  References 
[1] David Baraff. Fast Contact Force Computation for Nonpen­etrating Rigid Bodies. In Computer Graphics 
(Proceedings of SIGGRAPH 94), Annual Conference Series, pages 23 34. ACM SIGGRAPH, July 1994. [2] David 
Baraff and Andrew Witkin. Large Steps in Cloth Sim­ulation. In Computer Graphics (Proceedings of SIGGRAPH 
98), Annual Conference Series, pages 43 54. ACM SIG-GRAPH, July 1998. [3] Ronen Barzel and Alan H. Barr. 
A Modeling System Based On Dynamic Constraints. In Computer Graphics (Proceed­ings of SIGGRAPH 87), Annual 
Conference Series, pages 179 188. ACM SIGGRAPH, August 1988. [4] Ronen Barzel, John F. Hughes, and Daniel 
N. Wood. Plau­sible Motion Simulation for Computer Graphics Animation. In Computer Animation and Simulation 
96, Proceedings of the Eurographics Workshop, pages 184 197, Poitiers, France, September 1996. [5] Dimitri 
P. Bertsekas. Nonlinear Programming. Athena Sci­enti.c, Belmont, Massachusetts, 1995. [6] Lynne Shapiro 
Brotman and Arun N. Netravali. Motion In­terpolation by Optimal Control. In Computer Graphics (Pro­ceedings 
of SIGGRAPH 88), volume 26 of Annual Conference Series, pages 309 315. ACM SIGGRAPH, August 1988. [7] 
Stephen Chenney and D. A. Forsyth. Sampling Plausible Solutions to Multi-body Constraint Problems. In 
Computer Graphics (Proceedings of SIGGRAPH 2000), Annual Confer­ence Series. ACM SIGGRAPH, July 2000. 
[8] Michael F. Cohen. Interactive Spacetime Control for Ani­mation. In Computer Graphics (Proceedings 
of SIGGRAPH 92), Annual Conference Series, pages 293 302. ACM SIG-GRAPH, July 1992. [9] Bruce Donald, 
Patrick Xavier, John Canny, and John Reif. Kinodynamic Motion Planning. Journal of the ACM, 40(5):1048 
1066, November 1993. [10] Nick Foster and Dimitri Metaxas. Realistic Animation of Liq­uids. Graphical 
Models and Image Processing, 5(58):471 483, 1996. [11] Michael Garland and Paul S. Heckbert. Surface 
Simpli.ca­tion Using Quadric Error Metrics. In Computer Graphics (Proceedings of SIGGRAPH 97), Annual 
Conference Series, pages 209 216. ACM SIGGRAPH, August 1997. [12] Philip E Gill, Walter Murray, and Margaret 
H. Wright. Prac­tical Optimization. Academic Press, London, 1989. [13] Michael Gleicher and Andrew Witkin. 
Differential Manipu­lation. In Graphics Interface, pages 61 67, June 1991. [14] Michael Gleicher and 
Andrew Witkin. Through-the-Lens Camera Control. In Computer Graphics (Proceedings of SIGGRAPH 92), Annual 
Conference Series, pages 331 340. ACM SIGGRAPH, July 1992. [15] F. Sebastian Grassia. Practical Parameterization 
of Rota­tion Using the Exponential Map. Journal of Graphics Tools, 3(3):29 48, 1998. [16] Radek Grzeszczuk, 
Demetri Terzopoulos, and Geoffrey Hin­ton. NeuroAnimator: Fast Neural Network Emulation and Control of 
Physics-Based Models. In Computer Graphics (Proceedings of SIGGRAPH 98), Annual Conference Series, pages 
9 20. ACM SIGGRAPH, July 1998. [17] Mikako Harada, Andrew Witkin, and David Baraff. Inter­active Physically-Based 
Manipulation of Discrete/Continuous Models. In Computer Graphics (Proceedings of SIGGRAPH 95), Annual 
Conference Series, pages 199 208. ACM SIG-GRAPH, August 1995. [18] Wilfred Kaplan. Advanced Calculus. 
Addison-Wesley Pub­lishing Company, Reading, Massachusetts, 1984. [19] Zicheng Liu, Steven J. Gortler, 
and Michael F. Cohen. Hier­archical Spacetime Control. In Computer Graphics (Proceed­ings of SIGGRAPH 
94), Annual Conference Series, pages 35 42. ACM SIGGRAPH, July 1994. [20] Matthew Moore and Jane Wilhelms. 
Collision Detection and Response for Computer Animation. In Computer Graphics (Proceedings of SIGGRAPH 
88), Annual Conference Series, pages 289 298. ACM SIGGRAPH, August 1988. [21] J. Thomas Ngo and Joe Marks. 
Spacetime Constraints Re­visited. In Computer Graphics (Proceedings of SIGGRAPH 93), Annual Conference 
Series, pages 343 350. ACM SIG-GRAPH, August 1993. [22] James F. O Brien and Jessica K. Hodgins. Graphical 
Model­ing and Animation of Brittle Fracture. In Computer Graphics (Proceedings of SIGGRAPH 99), Annual 
Conference Series, pages 111 120. ACM SIGGRAPH, August 1999. [23] Zoran Popovi´c and Andrew Witkin. Physically 
Based Motion Transformation. In Computer Graphics (Proceedings of SIG-GRAPH 99), Annual Conference Series, 
pages 11 20. ACM SIGGRAPH, August 1999. [24] Jos Stam. Stable Fluids. In Computer Graphics (Proceedings 
of SIGGRAPH 99), Annual Conference Series, pages 121 128. ACM SIGGRAPH, August 1999. [25] Robert F. Stengel. 
Optimal Control and Estimation. Dover Books on Advanced Mathematics, New York, 1994. [26] Keith R. Symon. 
Mechanics, Third Edition. Addison-Wesley Publishing Company, Reading, Massachussetts, 1971. [27] Diane 
Tang, J. Thomas Ngo, and Joe Marks. N-Body Space­time Constraints. Journal of Visualization and Computer 
An­imation, 6:143 154, 1995. [28] Andrew Witkin, Michael Gleicher, and William Welch. Inter­active Dynamics. 
In Proceedings of the 1990 symposium on Interactive 3D graphics, pages 11 21, March 1990. [29] Andrew 
Witkin and Michael Kass. Spacetime Constraints. In Computer Graphics (Proceedings of SIGGRAPH 88), Annual 
Conference Series, pages 159 168. ACM SIGGRAPH, Au­gust 1988.  (a) (b) (c) (d)   Figure 5: Physical 
motions (left) are interactively edited to satisfy desired constraints (right). Intermediate motions 
during the editing process are shown at center. (a) An egg is dragged into a bucket after collision with 
a second egg. The second egg is required to fall into a second bucket with a nail constraint. (b) A table 
top is made to land on its legs after collision with a pyramid. (c) A tumbling mug is kept from tipping 
over by editing its orientation and angular velocity at the fourth collision with the ground. (d) A bounce 
and a .ip is added to an animation where a scissors lands on a coatrack. All interaction occurs in real 
time. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344882</article_id>
		<sort_key>219</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>23</seq_no>
		<title><![CDATA[Sampling plausible solutions to multi-body constraint problems]]></title>
		<page_from>219</page_from>
		<page_to>228</page_to>
		<doi_number>10.1145/344779.344882</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344882</url>
		<abstract>
			<par><![CDATA[<p>Traditional collision intensive multi-body simulations are difficult to control due to extreme sensitivity to initial conditions or model parameters. Furthermore, there may be multiple ways to achieve any one goal, and it may be difficult to codify a user's preferences before they have seen the available solutions. In this paper we extend simulation models to include plausible sources of uncertainty, and then use a Markov chain Monte Carlo algorithm to sample multiple animations that satisfy constraints. A user can choose the animation they prefer, or applications can take direct advantage of the multiple solutions. Our technique is applicable when a probability can be attached to each animation, with &#8220;good&#8221; animations having high probability, and for such cases we provide a definition of physical plausibility for animations. We demonstrate our approach with examples of multi-body rigid-body simulations that satisfy constraints of various kinds, for each case presenting animations that are true to a physical model, are significantly different from each other, and yet still satisfy the constraints.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Markov chain Monte Carlo]]></kw>
			<kw><![CDATA[motion synthesis]]></kw>
			<kw><![CDATA[plausible motion]]></kw>
			<kw><![CDATA[spacetime constraints]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.3</cat_node>
				<descriptor>Probabilistic algorithms (including Monte Carlo)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Modeling and recovery of physical attributes</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor>Modeling methodologies</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Monte Carlo</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010349</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003670.10003677</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic reasoning algorithms->Markov-chain Monte Carlo methods</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003670.10003682</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic reasoning algorithms->Sequential Monte Carlo methods</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003670.10003677</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic reasoning algorithms->Markov-chain Monte Carlo methods</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010342.10010343</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis->Modeling methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010342.10010344</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis->Model verification and validation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010342</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003670.10003682</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic reasoning algorithms->Sequential Monte Carlo methods</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003671</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic algorithms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39036309</person_id>
				<author_profile_id><![CDATA[81100292724]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Stephen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chenney]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California at Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40030467</person_id>
				<author_profile_id><![CDATA[81100502370]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[D.]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Forsyth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California at Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>225295</ref_obj_id>
				<ref_obj_pid>225294</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Joel Auslander, Alex Fukunaga, Hadi Partovi, Jon Christensen, Lloyd Hsu, Peter Reiss, Andrew Shuman, Joe Marks, and J. Thomas Ngo. Further Experience with Controller- Based Automatic Motion Synthesis for Articulated Figures. ACM Transactions on Graphics, 14(4):311-336, October 1995.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378509</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ronan Barzel and Alan H. Barr. A Modeling System Based on Dynamic Constraints. In Computer Graphics (SIGGRAPH 88 Conf. Proc.), volume 22, pages 179-188, August 1988.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>274989</ref_obj_id>
				<ref_obj_pid>274976</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Ronan Barzel, John F. Hughes, and Daniel N. Wood. Plausible Motion Simulation for Computer Graphics Animation. In Computer Animation and Simulation '96, pages 184-197, 1996. Proceedings of the Eurographics Workshop in Poitiers, France, August 31-September 1, 1996.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>849669</ref_obj_id>
				<ref_obj_pid>846238</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[David Brogan and Jessica Hodgins. Group Behaviors for Systems with Significant Dynamics. In Proceedings of the 1995 IEEE/RSJ International Conference on Intelligent Robots and Systems, volume 3, pages 528-534, 1995.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378531</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Lynne Shapiro Brotman and Arun N. Netravali. Motion Interpolation by Optimal Control. In Computer Graphics (SIG- GRAPH 88 Conf. Proc.), volume 22, pages 309-315, August 1988.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>312121</ref_obj_id>
				<ref_obj_pid>311625</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Stephen Chenney. Asynchronous, Adaptive, Rigid-Body Simulation. SIGGRAPH 99 Technical Sketch. In Conference Abstracts and Applications, page 233, August 1999.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618608</ref_obj_id>
				<ref_obj_pid>616057</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Stephen Chenney, Jeffrey Ichnowski, and David Forsyth. Dynamics Modeling and Culling. IEEE Computer Graphics and Applications, 19(2):79-87, March/April 1999.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Jon Christensen, Joe Marks, and J. Thomas Ngo. Automatic Motion Synthesis for 3D Mass-Spring Models. The Visual Computer, 13(3):20-28, January 1997.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134083</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Michael F. Cohen. Interactive Spacetime Control for Animation. In Computer Graphics (SIGGRAPH 92 Conf. Proc.), volume 26, pages 293-302, July 1992.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Afonso G. Ferreira and Janez ~;erovnik. Bounding the Probability of Success of Stochastic Methods for Global Optimization. Computers and Mathematics with Applications, 25(10):1-8, 1993.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[George S. Fishman. Monte Carlo : concepts, algorithms, and applications. Springer-Verlag, 1996.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Walter R Gilks, Sylvia Richardson, and David J Spiegelhalter. Markov Chain Monte Carlo in Practice. Chapman &amp; Hall, 1996.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>253321</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Michael Gleicher. Motion Editing with Spacetime Constraints. In Proceedings 1997 Symposium on Interactive 3D Graphics, pages 139-148, April 1997. Providence, RI, April 27-30.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218411</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Radek Grzeszczuk and Demetri Terzopoulos. Automated Learning of Muscle-Actuated Locomotion Through Control Abstraction. In SIGGRAPH 95 Conference Proceedings, pages 63-70. ACM SIGGRAPH, August 1995.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280816</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Radek Grzeszczuk, Demetri Terzopoulos, and Geoffrey Hinton. NeuroAnimator: Fast Neural Network Emulation and Control of Physics-Based Models. In SIGGRAPH 98 Conference Proceedings, pages 9-20. ACM SIGGRAPH, July 1998.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258822</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Jessica Hodgins and Nancy Pollard. Adapting Simulated Behaviors for New Creatures. In SIGGRAPH 97 Conference Proceedings, pages 153-162. ACM SIGGRAPH, August 1997.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>614412</ref_obj_id>
				<ref_obj_pid>614272</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Jessica K. Hodgins, James F. O'Brien, and Jack Tumblin. Perception of Human Motion With Different Geometric Models. IEEE Transactions on Visualization and Computer Graphics, 4(4):307-316, 1998.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>76077</ref_obj_id>
				<ref_obj_pid>76071</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Mark Jerrum and Alistair Sinclair. Approximating the Permanent. SIAM Journal of Computing, 18:1149-1178, 1989.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>241950</ref_obj_id>
				<ref_obj_pid>241938</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Mark Jerrum and Alistair Sinclair. The Markov Chain Monte Carlo Method: an approach to approximate counting and integration. In D.S.Hochbaum, editor, Approximation Algorithms for NP-hard Problems. PWS Publishing, Boston, 1996.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192169</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Zicheng Liu, Steven J. Gortler, and Michael F. Cohen. Hierarchical Spacetime Control. In SIGGRAPH 94 Conference Proceedings, pages 35-42. ACM SIGGRAPH, July 1994.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258887</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[J. Marks, B. Andalman, EA. Beardsley, W. Freeman, S. Gibson, J. Hodgins, T. Kang, B. Mirtich, H. Pfister, W. Ruml, K. Ryall, J. Seims, and S. Shieber. Design Galleries: A General Approach to Setting Parameters for Computer Graphics and Animation. In SIGGRAPH 97 Conference Proceedings, pages 389-400. ACM SIGGRAPH, August 1997.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Brian Mirtich, Yan Zhuang, Ken Goldberg, John Craig, Rob Zanutta, Brian Carlisle, and John Canny. Estimating Pose Statistics for Robotic Part Feeders. In Proceedings 1996 IEEE International Conference on Robotics and Automation, volume 2, pages 1140-1146, 1996.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166160</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[J. Thomas Ngo and Joe Marks. Spacetime Constraints Revisited. In SIGGRAPH 93 Conference Proceedings, pages 343- 350. ACM SIGGRAPH, August 1993.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>344880</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Jovan Popovi6, Steven Seitz, Michael Erdmann, Zoran Popovi6, and Andrew Witkin. Interactive Manipulation of Rigid Body Simulations. In SIGGRAPH 2000 Conference Proceedings. ACM SIGGRAPH, July 2000.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311536</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Zoran Popovi6 and Andrew Witkin. Physically Based Motion Transformation. In SIGGRAPH 99 Conference Proceedings, pages 11-20. ACM SIGGRAPH, August 1999.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192167</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Karl Sims. Evolving Virtual Creatures. In SIGGRAPH 94 Conference Proceedings, pages 15-22. ACM SIGGRAPH, July 1994.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Alistair Sinclair, 1999. Personal communication.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74338</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Richard Szeliski and Demetri Terzopoulos. From Splines to Fractals. In Computer Graphics (SIGGRAPH 89 Conf. Proc.), volume 23, pages 51-60, July 1989.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Diane Tang, J. Thomas Ngo, and Joe Marks. N-Body Spacetime Constraints. The Journal of Visualization and Computer Animation, 6:143-154, 1995.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258775</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Eric Veach and Leonidas J. Guibas. Metropolis Light Transport. In SIGGRAPH 97 Conference Proceedings, pages 65- 76. ACM SIGGRAPH, August 1997.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>614385</ref_obj_id>
				<ref_obj_pid>614268</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Baba C Vemuri, Chhandomay Mandal, and Shang-Hong Lai. A Fast Gibbs Sampler for Synthesizing Constrained Fractals. IEEE Transactions on Visualization and Computer Graphics, 3(4):337-351, 1997.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378507</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Andrew Witkin and Michael Kass. Spacetime Constraints. In Computer Graphics (SIGGRAPH 88 Conf. Proc.), volume 22, pages 159-168, August 1988.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Sampling Plausible Solutions to Multi-body Constraint Problems Stephen Chenney D. A. Forsyth University 
of California at Berkeley  Abstract Traditional collision intensive multi-body simulations are dif.cult 
to control due to extreme sensitivity to initial conditions or model parameters. Furthermore, there may 
be multiple ways to achieve any one goal, and it may be dif.cult to codify a user s preferences before 
they have seen the available solutions. In this paper we ex­tend simulation models to include plausible 
sources of uncertainty, and then use a Markov chain Monte Carlo algorithm to sample mul­tiple animations 
that satisfy constraints. A user can choose the an­imation they prefer, or applications can take direct 
advantage of the multiple solutions. Our technique is applicable when a proba­bility can be attached 
to each animation, with good animations having high probability, and for such cases we provide a de.ni­tion 
of physical plausibility for animations. We demonstrate our approach with examples of multi-body rigid-body 
simulations that satisfy constraints of various kinds, for each case presenting ani­mations that are 
true to a physical model, are signi.cantly different from each other, and yet still satisfy the constraints. 
CR Descriptors: I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism -Animation; I.3.5 [Computer 
Graphics]: Computational Geometry and Object Modeling -Physically based modeling; I.6.5 [Simulation and 
Modeling]: Model Development -Modeling methodologies G.3 [Probability and Statistics]: Prob­abilistic 
algorithms; Keywords: plausible motion, Markov chain Monte Carlo, motion synthesis, spacetime constraints 
 1 INTRODUCTION Collision intensive multi-body simulations are dif.cult to constrain because they exhibit 
extreme sensitivity to initial conditions or other simulation parameters. Adding uncertainty to a model 
helps when looking for animations that satisfy constraints [3], because it adds physically motivated 
degrees of freedom in useful places. For ex­ample, we can control tumbling dice by placing random bumps 
in speci.c places on the table, rather than by adjusting the initial con­ditions of the throw. The bumps 
are more effective because a small change to a bump part-way through the animation has a limited ef­fect 
on where the dice land, but a small change in the initial condi­tions generally has an unpredictable 
effect. It is dif.cult to design ef.cient control algorithms for the latter case. email: schenney,daf 
@cs.berkeley.edu Permission to make digital or hard copies of part or all of this work or personal or 
classroom use is granted without fee provided that copies are not made or distributed for profit or commercial 
advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, 
to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or 
a fee. SIGGRAPH 2000, New Orleans, LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 As discussed 
by Barzel, Hughes and Wood [3], adding random­ness to a simulation gives additional bene.ts: The real 
world contains .ne scale variation that traditional simulation models generally ignore. We can use randomness 
to model this variation by, for instance, replacing a perfectly .at surface with one speckled with random 
bumps (the same random bumps used for control above). Animations generated with the new model can more 
accurately re.ect the behavior of the world. In training environments, this results in the sub­ject developing 
skills more compatible with the real world: a driver trained on simulations of bumpy roads will be better 
prepared for real world road surfaces. Visually, procedural animations can be more believable when uncertainty 
is added. Without uncertainty, a perfectly round ball dropped vertically onto a perfectly .at table moves 
strangely, a situation that may be improved by slightly perturb­ing the collisions to make the ball deviate 
from the vertical. In a world with uncertainty, we generally expect a constrained problem to have multiple 
solutions. It is dif.cult to know before­hand what solutions are available, which compounds any dif.culties 
a user may have in codifying their preferences. Hence, it is perverse to use a solution strategy that 
seeks a single answer, rather, we pre­fer a technique that produces many solutions that re.ect the range 
of possible outcomes. While for feature animation a user is expected to choose the one animation they 
prefer, other applications bene.t directly from multiple solutions:  Computer game designers can use 
different animations each time a game is played, making it less predictable and poten­tially more entertaining. 
  Training environments can present trainees with multiple physically consistent scenarios that re.ect 
the physics and va­riety of the real world.  We generate multiple animations that satisfy constraints 
by ap­plying a Markov chain Monte Carlo (MCMC) algorithm to sam­ple from a randomized model. A user supplies 
the model of the world, including the sources of uncertainty and the simulator that will generate an 
animation in the world. The user also supplies a function that gives higher values for good animations 
 those that are likely in the world and satisfy the constraints. Finally, a user must provide a means 
of proposing a new animation given an exist­ing one. The algorithm we describe in this paper generates 
an arbi­trarily long sequence of animations in which good animations are likely to appear. In this paper, 
along with the algorithm, we describe the sorts of models we use and how we sample from them, discussing 
examples from the domain of collision intensive rigid-body simulation. No previous algorithm has been 
shown for the range and complexity of the multi-body simulations we present.  2 RELATED WORK The idea 
of plausible motion simulation, including the exploita­tion of randomness to satisfy constraints, was 
introduced by Barzel, Hughes and Wood [3]. They show solutions to constrained prob­lems where, for instance, 
a billiard ball is controlled by randomly varying the collision normal each time it hits a rail. We extend 
their work by introducing the idea of sampling (instead of search­ing), giving a precise de.nition of 
plausibility, and by demonstrat­ing MCMC s effectiveness on a wide range of dif.cult examples. Motion 
synthesis algorithms aim to achieve a goal by .nding an optimal set of control parameters and (sometimes) 
initial conditions. The goals described in the literature include .nding good locomo­tion parameters 
[1, 8, 14, 16, 23, 26] and .nding trajectories that sat­isfy constraints [2, 5, 9, 13, 15, 20, 32]. Some 
techniques [2, 5, 9, 13, 15, 20, 32] exploit explicit gradient information, but fail if the problem is 
too large (Popovi´c discusses ways to reduce the prob­lem size [25]) or the constraints are highly sensitive 
to, or discon­tinuous in, the control parameters. Randomized algorithms, such as simulated annealing 
[14, 16] (not a panacea [10, 11]), stochas­tic hill climbing [8], or evolutionary computing [1, 23, 26, 
29], do not require gradients and may be suitable for collision intensive sys­tems Tang, Ngo and Marks 
[29] describe an example. Most of these methods return a single best animation, and hence may ig­nore 
other equally good, or even preferable solutions. The evolu­tionary computing solutions can exhibit variations 
within a popula­tion, which Auslander et. al. [1] refer to as different styles, but the number of examples 
is limited by the population size. Multi-body constraint problems are good candidates for a De­sign Galleries 
[21] interface, in which a user browses through sam­ple solutions to locate the one they prefer. Our 
work addresses the sampling aspect of a Design Galleries interface for multi-body con­strained animations, 
but we do not consider other aspects of the in­terface. 3 ANIMATION DISTRIBUTIONS The MCMC algorithm 
distinguishes itself from motion synthesis approaches by generating multiple, different, good animations 
that satisfy a set of constraints, but no best animation. To gen­erate multiple plausible constrained 
animations, we must provide a model of the world de.ning:  The objects in the world and their properties, 
including the sources of uncertainty.  The simulator for generating animations in the world.  The 
constraints to be satis.ed by the animations.  For example, in a 2D animation of a ball bouncing on 
the table, we might have uncertainty in the normal vectors at the collision points, a constraint on the 
resting place of the ball, and a simulator that de­termines what happens when a 2D ball bounces on a 
table with arbi­trary surface normals. We will use this example, from [3], through­out the next two sections. 
A simulator used with our approach need not be physically accu­rate, or even physically based. Our 2D 
ball simulator is obviously non-physical, and the simulator we use in other examples has some problems 
with complex frictional behavior (section 5.2.3). In any case, we assume that if the simulator is given 
a plausible world as input it will produce a plausible animation, according to some de.­nition of plausibility 
(see section 3.3). 3.1 Incorporating Uncertainty We de.ne a function, , representing the probability 
of any possible animation that might arise in the world model. Intu­itively, should be large for animations 
that are likely in the world, and low for unlikely animations. For the 2D ball example, should be high 
if all the normal vectors used to generate the animation were close to vertical, and low if most of them 
were far from vertical. Let us further insist that  be non-negative and have .nite integral over the 
domain implied by the random vari­ables in the model, so that we can view as an unnormalized probability 
density function de.ned on the space of animations. Expanding on the 2D ball example, let us describe 
the direction of the normal vector for each collision as an independent random variable,  , distributed 
according to the (bell-shaped) Gaussian dis­tribution with standard deviation of, say, 10.0 degrees. 
In that case we get: which is the product of density functions for each collision normal. Note that 
we are ignoring normalization constants, an omission we justify in section 4. Also, we could in principle 
measure a real table to infer the true distribution of surface normals, and use that instead. 3.2 Constraints 
If we restrict our attention to animations that satisfy constraints, we are concerned with the distribution 
function , which is the conditional distribution of  given that it satis.es the constraints . For 
the 2D ball example, if we want the ball to land in a particular place, we could generate samples from 
 using an in­verse approach: join the ball s start point to its end point using a se­quence of parabolic 
hops and then infer which normal vectors were required to generate such a trajectory. However, using 
this approach we cannot directly ensure that the animation we generate is likely in the world, because 
it is dif.cult to know which hops to use to get a set of likely normal vectors. Unfortunately, it is 
frequently impractical to sample directly from , because there is no way to .nd, without considerable 
ef­fort, any reasonable animation in which the constraints are satis.ed. For example, in multi-body simulations 
a forward simulation ap­proach doesn t work because no published algorithm can directly specify a set 
of control parameters leading to satisfaction of multi­body constraints, without doing some form of iterative, 
expensive search. The inverse approach also looks intractable: it is not clear how to set trajectories 
for all the participants such that, for instance, objects do not pass through each other. In such cases 
(like all the examples in this paper), we expand to include a term for the constraints, resulting in 
a function . The new intuition is that will be large for animations that are likely in the world and 
satisfy the constraints, and small for animations that are either implausible in the world or don t satisfy 
the constraints. We will refer to as the probability of an ani­mation. Note that now even animations 
that don t satisfy the con­straints have non-zero probability, so if we sample from we may get an animation 
that doesn t satisfy the constraints, which we must discard. For the examples in this paper, we de.ne: 
 where depends only on how well the animation satis.es the constraints. If we want our 2D ball to land 
at a point whose distance, , from the origin is small, we can de.ne   which is the Gaussian density 
function with standard deviation , which we discuss in section 5.1. This function gives higher val­ues 
for distances near zero, and lower values as distances increase. Hence, for the 2D ball example: This 
paper describes a technique for generating animations such that those with high probability will appear 
more frequently than those with lower probability, but even some low probability events will occur as 
in the real world, unlikely things sometimes hap­pen. In other words, we will sample according to the 
distribution de.ned by . 3.3 What does Plausible mean? The restrictions on are quite weak, so we can 
describe many types of uncertainty and a wide variety of constraints. By phrasing the problem as one 
involving probabilities, we can leverage a wide range of mathematical tools for talking about plausible 
motion, and make strong statements about the properties of the animations we generate (see section 4). 
We can also outline what it means to be physically plausible: A model, including its simulator, is plausible 
if the im­ portant statistics gathered from samples distributed ac­ cording to are suf.ciently close 
to the real world statistics we care about. This is a very general de.nition of plausibility, because 
we say noth­ing about which statistics we might care about, or what it means to be suf.ciently close. 
For example, to validate a pool table model we could run simulations of virtual balls on a table, and 
analyze video of real balls on a real table, then compare statistics such as how long a ball rolls before 
coming to rest. For entertainment applications, we would care less about the quality of the match than 
if we were trying to build a training simulator for budding young pool sharks. Our measure extends the 
traditional graphics idea of plausibil­ity if it looks right it is right by allowing for de.nitions 
of statistical similarity other than a user s ability to detect a fake. However, for many applications, 
particularly involving motion, a viewer s ability to distinguish real from arti.cial remains the pri­mary 
concern [17].  4 MCMC FOR ANIMATIONS We use the Markov chain Monte Carlo (MCMC) method [12, 19] to 
sample animations from the distribution de.ned by . MCMC has several advantages for this task:  MCMC 
generates a sequence, or chain, of samples, , that are distributed according to a given distribution, 
in this case .  Apart from the initial sample, each sample is derived from the previous sample, which 
allows the algorithm to .nd and move among animations that satisfy constraints. If available, domain 
speci.c information can be incorporated into the algorithm, making it more ef.cient for special cases. 
On the other hand, the algorithm does not rely on any speci.c features of a model or simulator, allowing 
its application in a variety of situations. Our MCMC algorithm for generating animations begins with 
an ini­tial animation then repeatedly proposes changes, which may be ac­cepted or rejected. Explicitly: 
1 2 3 4 5 6 7 8 9 10 Line 1 gives initial values to all the random variables in the world model. On line 
4, a new animation, , is proposed by making a random change to the previous animation, . The details 
of this change are application speci.c. For example, in the 2D ball model of section 3 it might involve, 
for each normal, choosing to change it with probability one half and, if it is to be changed, adding 
a ran­dom offset uniformly distributed on degrees (for reasons discussed in section 5.1). The probability 
of making changes is de­.ned by the transition probability,  , which is the probability of proposing 
animation  if the current animation is . For the 2D ball, the transition probability is: where is 
the total number of collisions (assumed .xed) and is the number of collisions that were changed. The 
.rst factor is the probability of choosing the particular set of normals to change, and the second factor 
codes the probability of choosing a particular off­set for each normal that is changed. The transition 
probabilities, along with the probabilities of the an­imations, are used in computing the acceptance 
probability, which is the probability of accepting the proposed candidate (line 7):  Often, as in the 
2D ball example, the transition probabilities are symmetric and will cancel. Note also that only the 
ratios of probabilities appear, so we can use functions that are only proportional to true probability 
density functions (sec­tion 3.1).   The proposal mechanism is one of the key factors in how well the 
algorithm will perform in a particular application. In practice, proposals are designed through intuitive 
reasoning and experimen­tation, using past experience as a guide. In section 5 we describe the motivation 
for our proposal mechanisms. The MCMC algorithm guarantees that the samples in the chain will be distributed 
according to , as the number of samples approaches in.nity and provided certain technical conditions 
are met [12]. Hence we can be certain that the samples our algorithm generates truly re.ect the underlying 
model, and if this model is plausible (section 3.3), the collection of samples will be plausible. It 
is also the case that the samples in the chain will never satisfy the constraints if the underlying model 
says they cannot be satis­.ed. For instance, if a bowling simulator cannot capture complex frictional 
effects, animations that bowl the seven-ten split can never be found (see section 5.2.3). MCMC has been 
used in graphics to generate fractal terrain that satis.es point constraints [28, 31]. The samples generated 
by an MCMC algorithm may also be used to estimate expectations, as in Veach s Metropolis algorithm for 
computing global illumination so­lutions [30]. In this paper we are not concerned with expectations, 
so we can use short chains, just long enough to satisfy a user with several different animations  EXAMPLES 
We are interested in four things when designing an MCMC algo­rithm for generating animations:  Is the 
motion plausible? We assume that the simulator pro­duces plausible motion, so we are left to ensure that 
the dis­tributions we use for the model are reasonable.  How long does it take to .nd a sample that 
satis.es the con­straints?  How rapidly does the chain move among signi.cantly differ­ent samples, 
or mix? Chains that mix faster are desirable be­cause they produce many different animations quickly. 
  How many of the samples satisfy the constraints well enough to be useful?  The following examples 
discuss issues in building models, de.ning constraints and selecting proposal strategies, all of which 
in.uence the behavior of the algorithm. 5.1 A 2D Ball In the 2D ball example of section 3 a ball bounces 
on a table, starting in a .xed location and undergoing, for simplicity, a .xed number of collisions. 
For each collision we specify a random normal vector. The aim is to sample these normal vectors such 
that the ball comes to rest close to a particular location. As a speci.c case, we will drop the ball 
from above the origin at a height of , where is the diameter of the ball, use .ve collisions, and specify 
that it come to rest near  on the sixth collision. The simulation model is: the ball moves ballistically 
between each collision, when the velocity of the ball is re.ected about the corresponding normal vector 
and the normal component of velocity is scaled by . This model is not physically plausible (for instance, 
 we are ignoring rotation effects), but for this example we value sim­plicity. 5.1.1 Uncertainty and 
Constraints The probability of an animation is described in section 3.1, but prob­abilities (the values 
of density functions) can be very large numbers, so in practice we work with their logarithm. In this 
case, with the horizontal position of the sixth collision:  for some constant , which will cancel 
out when computing the ac­ceptance probability. The value of the constraint standard deviation, , has 
a major ef­fect on the samples generated by the chain. Say we choose a small value for  , corresponding 
to a very tight constraint because only values of  very close to give high values for and all other 
landing points have very low probability. From the initial anima­tion, the chain will move to some high 
probability animation close to the constraint. But, once there, almost no new proposals are ac­cepted 
(most candidates will be far from the constraint and have very low probability) and the user sees few 
different animations an undesirable situation. Alternatively, say we choose a large value for the standard 
devi­ation, corresponding to a weak constraint. Then is relatively high for a wide range of landing 
positions. The result is undesirable: the chain will contain many high probability animations that are 
far from the constraints. Hence we must choose a value for  that is high enough to pro­mote different 
samples but low enough to enforce the constraint. In this example we use a value of  , where  is the 
diameter of the ball, which, as .gure 2 shows, leads to the generation of very different samples that 
generally are close to the constraint. In this case, the algorithm is not very sensitive to the exact 
value for (anything within a factor of .ve works .ne) and it is possible to ex­perimentally evaluate 
a few values on short chains and choose the best, which in this case took only a few minutes.   In 
other applications there is no guarantee that we can achieve both good constraints and good mixing. In 
such cases the algorithm must run for many iterations to generate different samples, which may take prohibitively 
long. The tumbling dice example of sec­tion 5.4 is a borderline example in which we can satisfy constraints 
but mixing is poor. In such cases it is possible to run multiple chains in parallel. 5.1.2 Proposals 
The proposal mechanism, which speci.es normal vectors for a can­didate animation, , given those for 
the current animation, , pro­vides a means of moving around the space of possible normal vec­tors: 
  random random    This proposal changes some of the normals by an amount between minus one half 
and half their standard deviation of degrees. For good mixing it is important to allow more than one 
normal to be changed at once, because the effect of each change on the land­ing position (and hence the 
constraint) can then cancel. The alter­native, changing only one normal, makes it very dif.cult to change 
the .rst collision normal, because any but the smallest change will move the ball far from the desired 
landing position, and hence be rejected. The size of the offset we add is chosen to allow both small 
changes and relatively large changes, but not so large as to shift the normals too far from their mean 
in one step, which would reduce their probabilities and result in rejection of the candidate animation. 
 5.1.3 An Example Chain  Figure 1: Three sample paths from the 2D ball example, plotting the trajectory 
of the center of the ball (although the plot is 3D, the ball moves only in 2D). The green target is centered 
on the con­straint. Each red arrow is located at a collision point and indi­cates the direction of the 
normal vector used at that point. Note that in each example one of the earlier normals pushes the ball 
toward the constraint, and later normals re.ne the .nal position. One ball bounces slightly away from 
the constraint before moving toward it, which is not implausible. We ran the MCMC algorithm and generated 
a chain containing one thousand samples (many of these are repeats, arising when a candidate is rejected). 
Figure 2 plots the horizontal resting posi­tion of each sample. The .rst sample was initialized with 
randomly chosen normals, and came to rest a long way from the constraint. But within twenty iterations 
the chain moved toward a good loca­tion. The bumpiness of the graph indicates good mixing, because .at 
spots would indicate many repetitions of one sample as candi­dates were rejected. The majority of animations 
have the ball com­ing to rest within of the desired position, indicating that is suf.ciently small 
to enforce the constraint. Three (randomly chosen) samples from the chain are shown in .gure 1. They 
do not differ greatly from what one would expect: the ball tends to take an early bounce toward the constraint 
and keep moving in that direction, with later collisions adjusting it s .nal po­sition.  5.2 Bowling 
In this scenario the aim is to animate any particular ten-pin bowling shot (a goal suggested by Tang, 
Ngo and Marks [29]). The physi­cal model is implemented by an impulse-based rigid-body simula­tor [6]. 
We model the bowling ball, the lane with simpli.ed gutters and side walls, and the pins. All the models 
are roughly based on the rules of bowling, including variations allowed by those rules (see appendix 
A.1 for details): The ball is simulated as a sphere, with variable radius, density, initial position, 
initial velocity and initial angular velocity. Strike    Spare Figure 3: Frames from three bowling 
examples. The initial conditions for the ball and the pin locations are random variables. Given an initial 
and .nal pin con.guration, the MCMC algorithm samples particular values for the random variables that 
lead to the desired shot. In this case, we demanded a strike, a six-seven split and the corresponding 
spare.  The lane is .xed with regulation length and width, and in­cludes rectangular gutters and side 
walls starting in line with the front pin.  Each pin, of .xed shape and mass, has its initial position 
on the lane perturbed by a small random amount.  The coef.cients of friction and restitution between 
all the compo­nents are .xed. The probability is proportional to the prod­uct of the distribution functions 
for each of the random variables in the model. 5.2.1 Constraints The simulation begins with a subset 
of pins speci.ed by the user, so we can specify the initial conditions for bowling spares. The user also 
sets the constraint by stating which pins should be knocked down and which should remain standing. We 
are unable to propose candidates for the MCMC algorithm that are certain to satisfy the constraints (section 
3.2), so we assign non-zero probability to every possible outcome, but assign higher probability to those 
outcomes that are closer to the target, and the highest probability to outcomes matching the target. 
This is achieved with the Gibbs distribution function: for some constant with  the number of pins 
that end up correctly standing or knocked down, and  the number of standing pins that have not moved 
far beyond their initial position. Anima­tions that do not meet the goals will sometimes appear in the 
chain  (they have non-zero probability), but these would not be shown to a user. The samples that remain 
are correctly distributed according to the conditional probability , the distribution of animations 
in which the constraints are fully satis.ed. The constraint involves a term derived from the pins .nal 
position because some simulations result in the pins being pushed but not knocked down behavior we wish 
to discourage. The value of  affects the proportion of animations in the chain that must be discarded 
for not satisfying the constraints. High values for  give animations satisfying the constraints much 
higher proba­bility, making them more likely to appear in the chain. But the chain mixes better if some 
bad animations appear. Say only perfect an­imations appear, then getting to a signi.cantly different 
animation requires making a big change that also happens to get all the pins correct, which is unlikely. 
If some pins are not correct, a big change only has to get the same number of pins correct, and they 
can be dif­ferent pins. A low value for  makes it easier to accept an animation with some incorrect 
pins, make big changes, and then move toward a different, fully correct state.  For this example, we 
used , which gives a wide variety of animations that satisfy the constraints. Animations that improve 
the constraints are favored enough to ensure that good animations come up often, but not so much as to 
inhibit mixing. Our use of the Gibbs distribution was motivated by other applica­tions of the MCMC algorithm, 
such as counting the number of per­fect matchings in a graph. It is known [18] that there is an optimal 
that balances the concerns outlined above, but that the algorithm  Sequence of Final Positions x (ball 
diameters) 1.2 1.15 1.1 1.05 1 0.95 0.9 0.85 0.8  Iteration Figure 2: The resting position of the .rst 
one thousand samples in a chain for the 2D ball example. The roughness of this graph indicates good mixing, 
and most samples are close to the constraint (the majority within ). The position of the .rst few samples 
are far from the constraint (off the graph), but the chain moves to samples within twenty iterations. 
 is relatively insensitive to its exact value. Experience suggests that many applications may exhibit 
similar behavior [27]: there exists a range of values for that give the chain good properties, and one 
such value may be found through experiment. Our results are con­sistent with this (also see section 5.3). 
 5.2.2 Proposals Our proposal mechanism for bowling randomly chooses to do one of several things:  Sample 
new values for all the random variables.  Change the radius, density or initial conditions of the ball. 
  Change the initial position of some pins.  The details are given in appendix A.2. The .rst proposal 
strategy, which changes every random variable in the simulation, serves to make very large changes in 
the simu­lation. These are desirable as a means of escaping low probability regions, which we discuss 
in more detail in the next example (sec­tion 5.3). The other transitions are based on ideas similar to 
those in section 5.1: we must move around among possible values for the random variables, and we wish 
to do so with both large and small steps, but not so large as to make the new value highly unlikely un­der 
the model. 5.2.3 Sample Animations We tested this model with three sets of constraints:  Bowl a strike. 
  Bowl a ball that leaves a six-seven split.  Bowl the spare that knocks down the six-seven split. 
 Frames from example animations appear in .gure 3. The strike ex­ample is the easiest, because strikes 
are quite likely given our sim­ulator. Bowling the six-seven spare is not dif.cult either, because the 
various solutions probably form a connected set in state space, so once a single solution is found, the 
others can be explored ef.­ciently. Bowling the ball that leaves a six-seven split is the hardest example, 
intuitively because it is hard to knock down the pins be­hind the six pin while leaving it in place. 
We also attempted to bowl the seven-ten split (.gure 4). This shot depends on the precise frictional 
properties of the ball and lane. Our simulator s friction model could not capture the required effect 
(we Pin 7 Pin 10 Figure 4: The seven-ten split, in which the aim is to knock down both the seven and 
ten pins in one shot. The technique used by bowlers relies on the fact that a bowling ball will slide 
while spin­ning about an inclined axis, then, at some point, friction will cause the ball to grip, converting 
the angular momentum of the spin into linear momentum across the lane (dashed line). The seven pin must 
be struck behind its center of mass, so that it initially moves away from the ten pin (dotted line), 
bounces off the wall and moves back across the lane to hit the ten pin. Our simulator cannot model fric­tion 
well enough to simulate this shot (we are not aware of any that can). are not aware of any that can), 
so we could not make the shot. This demonstrates that the MCMC algorithm will only generate samples that 
are plausible according to our model (section 4). Our simulation model says that balls never take really 
big hooks, so we never see animations involving big hooks, regardless of the constraints.  5.3 Balls 
that Spell In these experiments we drop a stream of balls into a box partitioned into bins so that, when 
everything has come to rest, the balls form letters or symbols (.gure 5). Details of the model appear 
in ap­pendix B.1. We don t care which ball ends up in which designated bin. We use an impulse-based rigid-body 
simulator, as in the bowl­ing example. The uncertainty in this world arises from the shape of the parti­tions 
and the location from which each ball is dropped. The top sur­face of the partitions depends on a set 
of partition vertices, each of which is randomly perturbed about a default position. Each ball is dropped 
from a random location. The constraint we impose is that, when all the balls have come to rest, each 
ball is in a designated bin. We .x the maximum number of balls, so if each ball falls into a designated 
bin there can be no ball in an undesignated bin. We face a situation in which we cannot propose animations 
that are certain to completely satisfy the con­straints, so, as for the bowling example, we use the Gibbs 
distribu­tion for the constraint probability , where is the num­ber of balls in designated bins at 
the end of the animation. To facilitate mixing we allow the number of balls in the simula­tion to vary 
between zero and the minimum number required to form the word, by .ipping each ball between active and 
inactive states: inactive balls do not take part in the simulation. If all the designated bins are .lled, 
removing a ball frees up a bin for another ball to move into, making a signi.cant change to the animation. 
Removing the ball entirely, rather than just having it go into an undesignated bin, reduces the amount 
of interaction between the balls, possibly mak­ing it easier to make acceptable proposals. It also speeds 
the simula­tion when balls that aren t contributing anything are removed. Our initial experiments used 
a .xed number of balls, and the chain failed to mix well. The probability of an animation depends on 
how many balls are participating, the initial locations of the balls and the offsets of each partition 
vertex (see appendix B.1). 5.3.1 Proposals The proposal algorithm we use performs one of .ve actions: 
Example 1   Figure 5: Two examples of the spelling balls model, in this case spelling HI in a seven 
by .ve grid. The shape of the boxes is al­ 10 Number of correct balls: lambda=5 8 6 4 2 0 0 2000 
4000 6000 8000 10000 iteration 12000 14000 16000 18000 20000 lowed to vary slightly, as are the initial 
conditions of each ball. Our algorithm chooses box shapes and ball initial conditions that lead to the 
formation of a speci.c word.  The change-all strategy: change all the partition vertices and change 
all the balls.  Change a subset of partition vertices.  Change an active ball.  Activate some balls 
(possibly none).  Deactivate some balls (possibly none).  The change-all strategy appears as a means 
of escaping from low probability regions (.gure 7). When an animation is found that sat­is.es the constraints, 
subsequent animations tend to also satisfy the constraints, but their probabilities degrade. This occurs 
because the reduction in probability for a partition vertex change may be quite small, and such proposals 
are likely to be accepted. The downward trend can continue, moving the chain into a region of low probabil­ity. 
Then, a change-all proposal can reset all the partition vertices to much higher probability values, and 
even though the constraints are no longer satis.ed, the net change in probability will be positive and 
the proposal will be accepted. This change-all effect is good for mixing, because the next fully correct 
sample will generally be very different from the last. The second and third proposals are designed to 
move around the state space by modifying balls or partitions, similar to proposals in previous examples. 
The proposals to activate or deactivate some balls let us change the number of balls in the simulation. 
The pro­posal strategy we use makes the probability of adding or deleting any given ball independent 
of the maximum number of balls. We .rst tried a proposal that chose a single ball and .ipped its status, 
but if the maximum number of balls in the scenario is large, the proba­bility of removing a ball goes 
up as more balls are activated while the probability of adding a ball goes down, making it dif.cult to 
get all the balls into the simulation. The considerations in choosing a value for in this example are 
identical to those in the bowling example (a balance between good animations and good mixing), with an 
additional requirement due to the change-all effect: the constraint probability should be balanced against 
the model probability (in this case the probabilities of the partition vertices). If the constraint probability 
is too high, almost no change in partition vertices can overcome a well satis.ed constraint. Good balance 
is achieved when a much better set of model values can overcome a constraint that is satis.ed but uses 
poor model pa­rameters. As a speci.c example, we chose a bin designation that spells HI on a seven by 
.ve grid (.gure 5). We used for this word. A plot of , the number of designated bins that are .lled, 
for each 3560 3550 3540 0 2000 4000 6000 8000 10000 12000 14000 16000 18000 20000 iteration Figure 7: 
The value of at each iteration of the chain in .gure 6. The graph is quite bumpy, indicating good mixing. 
The dashed vertical lines correspond to all the iterations where the number of correct balls drops sharply 
(.gure 6), yet all those it­erations show a sharp rise in probability. This effect, due to the change-all 
proposal strategy, is discussed in the text. iteration of an example chain is shown in .gure 6. The important 
feature of this graph is that the chain tends to rapidly reach correct spellings, stays there for a short 
period, then drops back to incom­plete spellings. The twenty thousand iterations shown here took a few 
hours to compute on a 200MHz Pentium Pro PC. The change-all effect is evident in this chain. Figure 7 
plots the probability of the sample for each iteration. Places are marked where there is a sharp reduction 
in the number of correct balls, and these correspond to sharp increases in probability. At each of these 
sharp changes, a change-all proposal has been accepted that replaces a poor set of partition vertex offsets 
with a much more likely set, even though this breaks the constraint. We experimented with different values 
of , both higher and lower, but they lead to less satisfactory chains. Values of that are Figure 6: 
The number of correctly positioned balls for each of twenty thousand iterations of the HI model, with 
 . The maximum number of correct balls is ten. The chain .nds its .rst good animation after around six 
thousand iterations (we have seen chains that .nd good animations within one thousand samples). This 
graph indicates good mixing because the chain spends only a short period of time near similar solutions, 
then makes signi.cant changes before rapidly moving to a new good solution. Probability of Samples 3630 
3620 3610 3600 log(p(A)) 3590 3580 3570 too low result in chains that have trouble .nding correct animations, 
because the chance of accepting a poor proposal (from the point of view of the constraints) is too high. 
Values of that are too high make it less likely that a change-all proposal will be accepted, and also 
make it hard for the chain to abandon poor near-solutions. It takes only a few thousand iterations to 
see enough of the chain to know how lambda should be changed, and the range of acceptable values is reasonably 
large (our experiments show that chains with are not much worse than those for ) so little time must 
be spent in tuning parameters. We also performed a larger experiment, with 30 of the 105 bins on a .fteen 
by seven grid to be .lled (.gure 8). In this example we used a value of after experimenting with other 
values of  between six and eight. The higher value for  is required be­cause there are more partition 
vertices and more balls. The greater number of partition vertices allow the change-all proposal to remain 
effective at higher   values, so we still see adequate mixing. In fact, higher values are required 
to make it harder for a change-all pro­posal to succeed, so that the chain has enough time between major 
changes to converge to good animations.  5.4 Random Tables with Dice This summarized example demonstrates 
objects bouncing on a ran­dom table, coming to rest in constrained con.gurations. Dice are used as random 
number generators in the real world because they are exceptionally hard to control [3], yet our technique 
is capable of .nding animations in which dice come to rest near a particular place with a particular 
face showing. The 2D ball example (section 5.1) used a very simple table model, with two main drawbacks 
due to the use of independent nor­mals at each collision:  An object bouncing in place will appear to 
have the table change underneath it as a different normal vector is chosen for each collision.  Nearby 
points on the table are not correlated, as points on a real, bumpy table would be, which reduces the 
plausibility of the animations.  In this example we use a continuous, bumpy surface for the table. Rather 
than describe random normals directly, we specify a random b-spline surface via control points on a grid 
with .xed spacing but random vertical offsets. We can also specify random restitution and friction values 
at the control points, to be interpolated by the spline, thus extending the model to include the concept 
of springy or sticky regions on the table (such as spilt beer). The b-splines de.ning the table shape 
and properties de.ne random .elds over the surface. In principle, we could measure real tables, model 
their particular ran­dom .elds, and use those in our simulation. The simulator used in this example simulates 
only one object at a time bouncing on the random b-spline surface. It uses special tech­niques to manage 
the large number of control points required for a table with .ne bumps. In this example, constraints 
can be de.ned for any aspect of the object s 3D state at any point in time. Initial conditions for the 
ob­ject are speci.ed by constraining its state at the start of the simu­lation ( ). The probability of 
an animation in this world con­tains components for the control vertices de.ning the table s shape, friction 
and restitution, and a component for each constraint on the object. An animation generated from this 
type of scenario is shown in .gure 9. Each of six dice is dropped and told to land in a speci.c place 
showing a speci.c side up. The dice are treated individually and do not interact the table is not the 
same for each die. It took an hour or so of processing time to .nd a good animation for each die (a few 
hours for the complete animation). However, the chain does not mix well, so it takes many hours to .nd 
signi.cantly different animations. Figure 9: A composite of six sample animations showing the con­trol 
of a single bouncing die. Each die in the image was animated separately. Each had a different target 
location and desired side­up, but started with the same distribution on initial conditions. Proposals 
were made by changing one control point at a time, or one initial condition component at a time, or everything 
at once, the choice being made according to user supplied relative probabilities. Changes were made by 
adding a random offset to the current value, resulting in symmetric transition probabilities. The ability 
to make changes at any point in the simulation, through the surface control points, makes it easier to 
.nd good ani­mations in this world. Control points near the .rst few collisions get the die somewhere 
close to the target, and later collisions re.ne the location. This is not an explicitly coded strategy, 
rather it emerges naturally from the chain. However, a better proposal strategy might make explicit use 
of the behavior.  6 FUTURE WORK The models we use arise naturally in the real world, and we provide 
a means of verifying the plausibility of simulations. With further work it should be possible to experimentally 
obtain more accurate models, and test simulation algorithms on such models, to obtain re­sults like those 
of Mirtich et. al. [22]. It is an open problem to determine the dif.culty of a particular ex­ample without 
experimentation. Computation time can be adversely affected because the simulation itself is slower, 
or more iterations are required to .nd good animations, or both. For example, our bowling and spelling 
ball examples take comparable times to com­pute, the former due to slow simulation and the latter due 
to dif.­cult constraints. Simulation time dominates the cost of each itera­tion, so it is reasonable 
to spend more time making better proposals to improve mixing and hence reduce the total number of iterations. 
For example, in the bowling simulation we might bias changes in the ball s initial conditions according 
to which pins were knocked down. Constraints in our approach are speci.ed as probability density functions, 
which allows almost any type of constraint. In particu­lar, it might be possible to constrain collisions 
or other events to oc­cur at speci.c times (or frames). This would allow physically-based animations 
to be choreographed to music, or collisions to occur at frame boundaries. Popovi´c et. al. [24] describean 
interactive algorithmfor manipu­lating colliding bodies. As they suggest, a system might be designed 
to take as input animations generated by our MCMC approach and allow users to .ne tune the outcome as 
desired using local, interac­tive operations. Example 1    Figure 8: Balls that spell ACM. The box 
contains 105 bins, of which 30 are designated to contain balls. We show two animations, one on each row, 
generated from a single chain. Each has the bins being .lled in a different order, evidence that the 
chain produces a good mix of samples. We have only touched on the possibilities of plausible motion with 
constraints, focusing entirely on rigid body dynamics. Our techniques may also work in other domains 
that are hard to con­strain, including group behaviors [4] and deformable objects [8]. Another goal is 
to develop real time systems in which speci.c events are forced to occur in a plausible manner. For example, 
in a com­puter game we might like the monster to surprise the player in a particular way, with a plausibility 
model that takes into account the viewer s knowledge of the monster s state and how it moves [7].  Acknowledgements 
We thank Ronen Barzel, John Hughes and Joe Marks for their very extensive and helpful comments on this 
work in general and on earlier drafts of this paper. This work was funded by ONR grant N00014-96-11200. 
 References [1] Joel Auslander, Alex Fukunaga, Hadi Partovi, Jon Chris­tensen, Lloyd Hsu, Peter Reiss, 
Andrew Shuman, Joe Marks, and J. Thomas Ngo. Further Experience with Controller-Based Automatic Motion 
Synthesis for Articulated Figures. ACM Transactions on Graphics, 14(4):311 336, October 1995. [2] Ronan 
Barzel and Alan H. Barr. A Modeling System Based on Dynamic Constraints. In Computer Graphics (SIGGRAPH 
88 Conf. Proc.), volume 22, pages 179 188, August 1988. [3] Ronan Barzel, John F. Hughes, and Daniel 
N. Wood. Plau­sible Motion Simulation for Computer Graphics Animation. In Computer Animation and Simulation 
96, pages 184 197, 1996. Proceedings of the Eurographics Workshop in Poitiers, France, August 31-September 
1, 1996. [4] David Brogan and Jessica Hodgins. Group Behaviors for Sys­tems with Signi.cant Dynamics. 
In Proceedings of the 1995 IEEE/RSJ International Conference on Intelligent Robots and Systems, volume 
3, pages 528 534, 1995. [5] Lynne Shapiro Brotman and Arun N. Netravali. Motion In­terpolation by Optimal 
Control. In Computer Graphics (SIG-GRAPH 88 Conf. Proc.), volume 22, pages 309 315, August 1988. [6] 
Stephen Chenney. Asynchronous, Adaptive, Rigid-Body Sim­ulation. SIGGRAPH 99 Technical Sketch. In Conference 
Ab­stracts and Applications, page 233, August 1999. [7] Stephen Chenney, Jeffrey Ichnowski, and David 
Forsyth. Dy­namics Modeling and Culling. IEEE Computer Graphics and Applications, 19(2):79 87, March/April 
1999. [8] Jon Christensen, Joe Marks, and J. Thomas Ngo. Automatic Motion Synthesis for 3D Mass-Spring 
Models. The Visual Computer, 13(3):20 28, January 1997. [9] Michael F. Cohen. Interactive Spacetime Control 
for Anima­tion. In Computer Graphics (SIGGRAPH 92 Conf. Proc.), vol­ume 26, pages 293 302, July 1992. 
[10] Afonso G. Ferreira and Janez .Zerovnik. Bounding the Prob­ability of Success of Stochastic Methods 
for Global Opti­mization. Computers and Mathematics with Applications, 25(10):1 8, 1993. [11] George 
S. Fishman. Monte Carlo : concepts, algorithms, and applications. Springer-Verlag, 1996. [12] Walter 
R Gilks, Sylvia Richardson, and David J Spiegelhal­ter. Markov Chain Monte Carlo in Practice. Chapman 
&#38; Hall, 1996. [13] Michael Gleicher. Motion Editing with Spacetime Con­straints. In Proceedings 1997 
Symposium on Interactive 3D Graphics, pages 139 148, April 1997. Providence, RI, April 27-30. [14] Radek 
Grzeszczuk and Demetri Terzopoulos. Automated Learning of Muscle-Actuated Locomotion Through Control 
Abstraction. In SIGGRAPH 95 Conference Proceedings, pages 63 70. ACM SIGGRAPH, August 1995. [15] Radek 
Grzeszczuk, Demetri Terzopoulos, and Geoffrey Hin­ton. NeuroAnimator: Fast Neural Network Emulation and 
Control of Physics-Based Models. In SIGGRAPH 98 Confer­ence Proceedings, pages 9 20. ACM SIGGRAPH, July 
1998. [16] Jessica Hodgins and Nancy Pollard. Adapting Simulated Be­haviors for New Creatures. In SIGGRAPH 
97 ConferencePro­ceedings, pages 153 162. ACM SIGGRAPH, August 1997. [17] Jessica K. Hodgins, James F. 
O Brien, and Jack Tumblin. Per­ception of Human Motion With Different Geometric Models. IEEE Transactions 
on Visualization and Computer Graphics, 4(4):307 316, 1998. [18] Mark Jerrum and Alistair Sinclair. Approximating 
the Perma­nent. SIAM Journal of Computing, 18:1149 1178, 1989. [19] Mark Jerrum and Alistair Sinclair. 
The Markov Chain Monte Carlo Method: an approach to approximate counting and inte­gration. In D.S.Hochbaum, 
editor, Approximation Algorithms for NP-hard Problems. PWS Publishing, Boston, 1996. [20] Zicheng Liu, 
Steven J. Gortler, and Michael F. Cohen. Hierar­chical Spacetime Control. In SIGGRAPH 94 Conference Pro­ceedings, 
pages 35 42. ACM SIGGRAPH, July 1994. [21] J. Marks, B. Andalman, P.A. Beardsley, W. Freeman, S. Gib­son, 
J. Hodgins, T. Kang, B. Mirtich, H. P.ster, W. Ruml, K. Ryall, J. Seims, and S. Shieber. Design Galleries: 
A Gen­eral Approach to Setting Parameters for Computer Graphics and Animation. In SIGGRAPH 97 Conference 
Proceedings, pages 389 400. ACM SIGGRAPH, August 1997. [22] Brian Mirtich, Yan Zhuang, Ken Goldberg, 
John Craig, Rob Zanutta, Brian Carlisle, and John Canny. Estimating Pose Statistics for Robotic Part 
Feeders. In Proceedings 1996 IEEE International Conference on Robotics and Automation, vol­ume 2, pages 
1140 1146, 1996. [23] J. Thomas Ngo and Joe Marks. Spacetime Constraints Revis­ited. In SIGGRAPH 93 Conference 
Proceedings, pages 343 350. ACM SIGGRAPH, August 1993. [24] Jovan Popovi´c, Steven Seitz, Michael Erdmann, 
Zoran Popovi´c, and Andrew Witkin. Interactive Manipulation of Rigid Body Simulations. In SIGGRAPH 2000 
Conference Proceedings. ACM SIGGRAPH, July 2000. [25] Zoran Popovi´c and Andrew Witkin. Physically Based 
Motion Transformation. In SIGGRAPH 99 Conference Proceedings, pages 11 20. ACM SIGGRAPH, August 1999. 
[26] Karl Sims. Evolving Virtual Creatures. In SIGGRAPH 94 Conference Proceedings, pages 15 22. ACM SIGGRAPH, 
July 1994. [27] Alistair Sinclair, 1999. Personal communication. [28] Richard Szeliski and Demetri Terzopoulos. 
From Splines to Fractals. In Computer Graphics (SIGGRAPH 89 Conf. Proc.), volume 23, pages 51 60, July 
1989. [29] Diane Tang, J. Thomas Ngo, and Joe Marks. N-Body Space­time Constraints. The Journal of Visualization 
and Computer Animation, 6:143 154, 1995. [30] Eric Veach and Leonidas J. Guibas. Metropolis Light Trans­port. 
In SIGGRAPH 97 Conference Proceedings, pages 65 76. ACM SIGGRAPH, August 1997. [31] Baba C Vemuri, Chhandomay 
Mandal, and Shang-Hong Lai. A Fast Gibbs Sampler for Synthesizing Constrained Fractals. IEEE Transactions 
on Visualization and Computer Graphics, 3(4):337 351, 1997. [32] Andrew Witkin and Michael Kass. Spacetime 
Constraints. In Computer Graphics (SIGGRAPH 88 Conf. Proc.), volume 22, pages 159 168, August 1988. 
 A Bowling Details A.1 Uncertainty model Our bowling model is derived from data found online (http://www.brittanica.com). 
The sources of uncertainty in the model are: Ball radius Distributed uniformly on , where is speci.ed 
by the rules of the game (approximately 11cm) and . Ball density Distributed uniformly on  , with and 
.  Ball initial position Fixed in line with the end of the lane, at some point uniformly distributed 
across the width of the lane, and at a height distributed according to , the distribution de.ned by 
the Gaussian density function with mean and standard deviation . Ball initial velocity The component 
down the lane (measured in ) is distributed according to . The compo­nent across the lane is distributed 
according to  . The vertical component is set to zero. Ball initial angular velocity About a vertical 
axis (measured in ), distributed according to .  Each pin Fixed shape and mass, offset from its proper 
location on the lane in a random direction by a distance distributed accord­ing to (mm). With these 
distributions:  where is the ball s height above the lane, and are the ve­locity components down 
and across the lane, is the angular ve­locity about the vertical axis, and is the distance of pin from 
its center location. The above formula for is valid if all the uni­formly distributed variables are 
within their range, and all the .xed variables have their correct values, otherwise . We can ignore 
the uniformly distributed variables in computing be­cause their distribution function is proportional 
to one.  A.2 Proposals Our proposal mechanism samples a value , uniform on , and then: if , we sample 
new values for all the random vari­ables. if , we change the radius of the ball by adding an offset 
distributed according to . If the radius lies outside the allowable range, we wrap it back into the 
range. if , we change the density of the ball by adding an offset distributed according to , wrapping 
to keep inside the allowable range. if , we change the initial position of the ball. We add a horizontal 
offset distributed according to (  thewidthof thelane), wrappingifnecessary,andadda ver­tical offset 
distributed according to . if , we change the initial velocity of the ball. To its component down 
the lane, we add an offset distributed according to . To its component across the lane, we add an offset 
distributed according to . if , we change the initial angular velocity by adding an offset distributed 
according to . otherwise, for each pin, with probability , change its location by moving it in a random 
direction by a distance distributed according to  (mm). All of these proposals are symmetric, so there 
is no need to compute the transition probabilities (they cancel when computing the accep­tance probability). 
 Outer box 36mm high All but the last two proposals are symmetric. If a ball is disabled, the ratio 
 (the area of the rectangle from which the ball may be dropped). If a ball is enabled,   .  Figure 
10: The dimensions and tesselation for the box in the spelling ball example.  B Spelling Ball Details 
B.1 Model details Each bin has a side length of 20mm, and each partition is 2mm thick and 12mm high (.gure 
10). The .oor beneath each bin is domed to help the balls come to rest, and the box in which the bins 
sit is 36mm deep. Each partition vertex is offset in a random direction by a distance distributed according 
to (mm). Each ball is dropped from rest at a uniformly random location within a rectangle 72mm above 
the bottom of the box and centered above it. The size and density of the balls is intended to resemble 
marbles. The probability of an animation is:  where is the number of designated bins that are .lled, 
 is the set of partition vertices, is the offset distance of vertex (in mm), is the area of the rectangular 
region from which the balls may be dropped and is the set of active balls, which can vary in size for 
different animations as balls are activated or deactivated. In this case we must include a term for the 
uniformly distributed drop po­sition of each ball because the number of balls can vary. B.2 Proposals 
Our proposal mechanism uniformly samples a random and then applies one of four strategies: if , we 
change all the partition vertices, giving them new randomly chosen offsets, and change all the balls, 
giving them a new active status and a new initial position. , for each partition vertex, we randomly 
decide, with probability 0.02, to change its location by adding an offset in a random direction with 
length distributed accord­ (mm). we uniformly randomly select an active ball to change, and offset its 
starting position in a random di­rection for a distance distributed according to , where is the bin 
size. We wrap the edges of the region from which balls may be dropped. if , for each enabled ball we 
uniformly sam­ple  and disable the ball if . otherwise, for each disabled ball we uniformly sample 
   and enable the ball if .  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344886</article_id>
		<sort_key>229</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>24</seq_no>
		<title><![CDATA[Conservative volumetric visibility with occluder fusion]]></title>
		<page_from>229</page_from>
		<page_to>238</page_to>
		<doi_number>10.1145/344779.344886</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344886</url>
		<abstract>
			<par><![CDATA[<p>Visibility determination is a key requirement in a wide range of graphics algorithms. This paper introduces a new approach to the computation of <italic>volume visibility</italic>, the detection of occluded portions of space as seen from a given region. The method is conservative and classifies regions as occluded only when they are guaranteed to be invisible. It operates on a discrete representation of space and uses the opaque interior of objects as occluders. This choice of occluders facilitates their extension into adjacent opaque regions of space, in essence maximizing their size and impact. Our method efficiently detects and represents the regions of space hidden by such occluders. It is the first one to use the property that occluders can also be extended into empty space provided this space is itself occluded from the viewing volume. This proves extremely effective for computing the occlusion by a set of occluders, effectively realizing <italic>occluder fusion</italic>. An auxiliary data structure represents occlusion in the scene and can then be queried to answer volume visibility questions. We demonstrate the applicability to visibility preprocessing for real-time walkthroughs and to shadow-ray acceleration for extended light sources in ray tracing, with significant acceleration in both cases.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Graphics data structures and data types</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.2.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003624</concept_id>
				<concept_desc>CCS->Mathematics of computing->Discrete mathematics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010394</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics file formats</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P97015</person_id>
				<author_profile_id><![CDATA[81100323171]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gernot]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schaufler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Laboratory for Computer Science, Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P150906</person_id>
				<author_profile_id><![CDATA[81100369597]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Julie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dorsey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Laboratory for Computer Science, Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31076022</person_id>
				<author_profile_id><![CDATA[81100124823]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Xavier]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Decoret]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[iMAGIS, GRAVIR/IMAG, INRIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P84966</person_id>
				<author_profile_id><![CDATA[81100402503]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Fran&#231;ois]]></first_name>
				<middle_name><![CDATA[X.]]></middle_name>
				<last_name><![CDATA[Sillion]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[iMAGIS, GRAVIR/IMAG, INRIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>91416</ref_obj_id>
				<ref_obj_pid>91385</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Airey, John M., John H. Rohlf and Frederick P. Brooks, Jr., "Towards Image Realism with Interactive Update Rates in Complex Virtual Building Environments," Symposium on Interactive 3D Graphics 1990, pp 41-50.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Catmull, Edwin, E., Computer Display of Curved Surface, IEEE Conference on Computer Graphics, Pattern Recognition and Data Structures, May 1975, pp 11-17.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>147159</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Chin, Norman and Steven Feiner, Fast Object-Precision Shadow Generation for Area Light Sources Using BSP Trees, Symposium on Interactive Graphics (1992), pp 21-30.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237242</ref_obj_id>
				<ref_obj_pid>237218</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Coorg, Satyan and Seth Teller, Temporally Coherent Conservative Visibility, Proc. Twelfth Annual ACM Symposium on Computational Geometry, Philadelphia, PA, May 24-26, 1996, pp 78-87.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Cohen-Or, Daniel and Amit Shaked, Visibility and Dead- Zones in Digital Terrain Maps, EUROGRAPHICS '95 14 3 (1995) pp 171-180.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Cohen-Or, D., G. Fibich, D. Halperin and E. Zadicario, Conservative Visibility and Strong Occlusion for Viewspace Partitioning of Densely Occluded Scenes, Computer Graphics Forum, 17 (3) 1998, pp 243-253.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>792947</ref_obj_id>
				<ref_obj_pid>792757</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Chrysanthou, Y., D. Cohen-Or, and D. Lischinski, Fast Approximate Quantitative Visibility for Complex Scenes, Computer Graphics International '98, June 1998, pp 220-229.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192207</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Drettakis George, Fiume Eugene L., A Fast Shadow Algorithm for Area Light Sources Using Backprojection, SIG- GRAPH 94, pp 223-230.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258785</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Durand, Fredo, George Drettakis and Claude Puech, The Visibility Skeleton: A Powerful and Efficient Multi-Purpose Global Visibility Tool, SIGGRAPH 97, pp 89-100.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Durand, Fredo, 3D Visibility: Analytical Study and Applications, PhD Dissertation, Universite Joseph Fournier, Grenoble, France.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>344891</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Durand, Fredo, George Drettakis, Joelle Thollot and Claude Puech, Conservative Visibility Preprocessing using Extended Projections, SIGGRAPH 2000.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83821</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Foley, James, Andries van Dam, Steven Feiner and John Hughes, Computer Graphics: Principles and Practice, Addison-Wesley Publishing Co., ISBN 0-201-12110-7, 1990.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>16584</ref_obj_id>
				<ref_obj_pid>16564</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Greene, Ned Approximating Visibility with Environment Maps, Graphics Interface '86, pp 108-114.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166147</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Greene, Ned, Michael Kass and Gavin Miller, Hierarchical Z-Buffer Visibility, SIGGRAPH 93, pp 231-238.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Haines, A. Eric and Donald P. Greenberg, The Light Buffer: A Ray Tracer Shadow Testing Accelerator, IEEE Computer Graphics and Applications, 6 9, 1986, pp 6-16.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Haines, A. Eric and John R. Wallace, Shaft Culling for Efficient Ray Cast Radiosity, Photorealistic Rendering in Computer Graphics (Proceedings of the Second Eurographics Workshop on Rendering), Springer-Verlag, New York, 1994, pp 122-138.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311551</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Hart, David, Philip Dutre and Donald P. Greenberg, Direct Illumination with Lazy Visibility Evaluation, SIGGRAPH 99, pp 147-154.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>262847</ref_obj_id>
				<ref_obj_pid>262839</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Hudson, Tom, Dinesh Manocha, Jonathan Cohen, Ming Lin, Kenneth E. Hoff III, Hansong Zhang, Occlusion Culling using Shadow Volumes, Proceedings of 13th Symposium on Computational Geometry, Nice, France, June 4-6 1997, pp 1- 10.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>253326</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Murali T.M., and Thomas A. Funkhouser, Consistent Solid and Boundary Representations from Arbitrary Polygonal Data, Symposium on Interactive 3D Graphics), 1997, pp 155-162.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>111137</ref_obj_id>
				<ref_obj_pid>111135</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Plantinga, Harry. and Charles Dyer, Visibility, Occlusion and the Aspect Graph, International Journal of Computer Vision, 5(2) 1990, pp 137-160.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Saona-Vazquez, Carlos, Isabel Navazo and Pere Brunet, The Visibility Octree. A Data Structure for 3D Navigation, TR LSI-99-22-R, Universitat Politecnica de Catalunya, Spain.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>77587</ref_obj_id>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Samet, Hanan, "Applications of Spatial Data Structures, Addison-Wesley, Reading, MA, ISBN 0-201-50300-X, 1990.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192210</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Stewart, James and S. Ghali, Fast computation of shadow boundaries using spatial coherence and backprojections, SIGGRAPH 94, pp 231-238.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>731964</ref_obj_id>
				<ref_obj_pid>647651</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Stewart, James, Hierarchical Visibility in Terrains, Eurographics Rendering Workshop, June 1997, pp 217-228.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Tanaka, Toshimitsu, and Tokiichiro Takahashi, Fast Analytic Shading and Shadowing for Area Light Sources, Computer Graphics Forum, Vol. 16, 3, 1997, pp 231-240.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122725</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Teller, Seth J. and Carlo H. Sequin, Visibility Preprocessing For Interactive Walkthroughs SIGGRAPH 91, pp 61-69.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134029</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Teller, Seth, Computing the Antipenumbra of an Area Light Source, SIGGRAPH 92, pp 139-148.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192279</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Teller, Seth, and Pat Hanrahan, Global Visibility Algorithms for Illumination Computation, SIGGRAPH 94, pp 443-450.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Ward, Gregory J. and Paul Heckbert, Irradiance Gradients, Third Eurographics Workshop on Rendering, May 1992, pp 85-98.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>93317</ref_obj_id>
				<ref_obj_pid>93267</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Woo, Andrew and John Amanatides, Voxel Occlusion Testing: A Shadow Determination Accelerator for Ray Tracing, Graphics Interface '90, pp 213-219.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Yagel, Roni, and William Ray, Visibility Computation for Efficient Walkthrough of Complex Environments, PRESENCE, Vol.5, No. 1, Winter 1996, pp 1-16.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258781</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Zhang, Hansong, Dinesh Manocha, Tom Hudson and Kenneth E. Hoff, Visibility Culling using Hierarchical Occlusion Maps, SIGGRAPH 97, pp 77-88.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Conservative Volumetric Visibility with Occluder Fusion Gernot Schau.er Julie Dorsey Xavier Decoret 
Franc¸ois X. Sillion Laboratory for Computer Science iMAGIS Massachusetts Institute of Technology GRAVIR/IMAG 
 INRIA Abstract Visibility determination is a key requirement in a wide range of graphics algorithms. 
This paper introduces a new approach to the computation of volume visibility, the detection of occluded 
portions of space as seen from a given region. The method is conservative and classi.es regions as occluded 
only when they are guaranteed to be invisible. It operates on a discrete representation of space and 
uses the opaque interior of objects as occluders. This choice of oc­cluders facilitates their extension 
into adjacent opaque regions of space, in essence maximizing their size and impact. Our method ef.ciently 
detects and represents the regions of space hidden by such occluders. It is the .rst one to use the property 
that occluders can also be extended into empty space provided this space is itself occluded from the 
viewing volume. This proves extremely effec­tive for computing the occlusion by a set of occluders, effectively 
realizing occluder fusion. An auxiliary data structure represents occlusion in the scene and can then 
be queried to answer volume visibility questions. We demonstrate the applicability to visibility preprocessing 
for real-time walkthroughs and to shadow-ray accel­eration for extended light sources in ray tracing, 
with signi.cant acceleration in both cases. 1 Introduction Determining visibility is central in many 
computer graphics algo­rithms. If visibility information were available in advance, scan­line renderers 
would not need to rasterize hidden geometry, and ray-tracers could avoid tracing shadow rays from points 
in shadow and testing objects that could not be hit. However, computing and storing all possible view 
con.gurations for a scene the aspect graph [20] is impractical for complex scenes. Even calculating 
all the visual events in a scene has very high complexity [9] and poses numerical stability problems. 
It is generally easier to conservatively overestimate the set of po­tentially visible objects (PVS [1, 
26]) for a certain region of space (referred to as a viewcell throughout this paper). While effective 
methods exist to detect occlusions in indoor scenes [1, 26] and ter­rain models [24], in more general 
types of complex scenes previous approaches [4, 6, 21] consider single convex occluders only to de­termine 
objects, or portions of space, that are completely hidden from the viewcell. This is known as volume 
visibility. Permission to make digital or hard copies of part or all of this work or personal or classroom 
use is granted without fee provided that copies are not made or distributed for profit or commercial 
advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, 
to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or 
a fee. SIGGRAPH 2000, New Orleans, LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 In many cases, 
objects are hidden due to the combination of many, not necessarily convex, occluders. This situation 
is exac­erbated by the lack of large polygons in today s .nely tessellated models. Figure 7 in Section 
4.3 compares the number of occlusions detected using single convex occluders to the number detected with 
our method. Combining the effect of multiple, arbitrary occluders is complicated by the many different 
kinds of visual events that occur between a set of objects [9] and by various geometric degeneracies. 
As a new solution to these problems, this paper proposes to cal­culate volume visibility on a conservative 
discretization of space. Occlusion is explicitly represented in this discretization and can be queried 
to retrieve visibility information for arbitrary scene ob­jects either static, dynamic or newly added. 
We use opaque regions of space as blockers and automatically derive them from the scene description instead 
of expecting large convex occluders to be present in the scene. Our representation de­couples the scene 
complexity from the accuracy and computational complexity at which visibility is resolved. We show that 
hidden regions of space are valid blockers and that any opaque blocker can be extended into such regions 
of space. This effectively combines fuses [32] one blocker with all the other blockers that have caused 
this region to be occluded and re­sults in a dramatic improvement in the occlusions detected. Collec­tions 
of occluders need not be connected or convex. The rest of the paper is organized as follows. In the next 
section, we review previous approaches to visibility computation with spe­cial emphasis on volume visibility 
methods. Next, we describe our approach in 2D and then extend it to 3D and 2 1/2 D. We present results 
for PVS computation and reducing the number of shadow rays in ray-tracing. We conclude with a discussion 
of our results and suggestions for future work.  2 Previous Work The central role of visibility has 
resulted in many previously pub­lished approaches. We classify them into the following three cat­egories: 
exact, point-sampled and conservative visibility computa­tions and focus the discussion on volume visibility 
approaches. Ex­amples of exact visibility representations are the aspect graph [20] or the visibility 
skeleton [9] and exact shadow boundaries [3, 8, 23, 27]. As mentioned above, they are impractical for 
complex scenes. Point-sampling algorithms calculate visibility up to the accu­racy of the display resolution 
[5, 7, 13]. One sample ray is sent into the scene and the obtained visible surface is reused over an 
area (e.g. a pixel or solid angle on the hemisphere). Today s most widely used approach is a hardware-accelerated 
z-buffer [2] or its variants, the hierarchical z-buffer [14] and hierarchical occlusion maps [32]. Visibility 
results obtained from these algorithms can­not be extended to volume visibility without introducing error. 
For volume visibility, projections are not feasible, as no single center of projection is appropriate. 
To cope with the complexity of today s models, researchers have investigated conservative subsets of 
the hidden scene portion. Airey et al. [1] and Teller et al. [26, 28] propose visibility preprocessing 
for indoor scenes. They identify objects that are visible through sequences of portals. Yagel et al. 
[31] apply similar ideas in 2D for visibility in caves. Stewart [24] provides a solution for the case 
of terrain. Unfortunately, these algorithms do not generalize to volume visibility for more general types 
of complex scenes. Conservative, but accurate, volume visibility computations for general scenes are 
limited to considering one convex occluder at a time for identifying hidden objects. Cohen-Or et al. 
[6] .nd hid­den buildings in cities. Saona-Vazquez et al. [21] apply a similar strategy to the nodes 
in an octree. They intersect the PVS as seen from the eight corners of each voxel to obtain the PVS for 
the voxel. Coorg et al. [4] use supporting planes between the blocker and an occludee to determine occlusion. 
These planes also allow them to determine when the occluder will no longer hide the occludee. All these 
single occluder approaches share the dif.culties of identifying good occluders, and none performs occluder 
fusion. Unfortunately, in practice many scenes do not contain any large polygons or con­vex objects. 
Durand [11] calculates volume visibility by projecting potential occluders onto planes. He modi.es point-sampled 
projec­tions and convolution to obtain a conservative algorithm. In volume visibility it seems to be 
inherently dif.cult to combine the effects of multiple occluders in a provably accurate and ef.cient 
way. We believe that this is due to the nature of the occluders con­sidered convex polygons or objects 
 and because the portions of occluded space have not been explicitly represented or used in the computations. 
Our visibility algorithm works entirely on a volumetric scene representation. We propose to abandon considering 
polygons as occluders, and instead let the volumetric nature of opaque objects occlude the space behind 
them. Several authors [4, 6, 10, 21] have required convex decompositions of arbitrary objects in order 
for their algorithms, operating on convex blockers, to work. Indeed, volumetric representations, such 
as octrees, provide such a convex decomposition. They also represent space itself so that ef.cient blockers 
can be found using occluded regions as described below. In our approach, we construct shafts around blockers 
as seen from the viewcell similar to Haines et al. s shaft culling [16] and Teller et al. s inter-cell 
visibility algorithm [28]. The difference is that Haines shafts lie between the viewcell and the occludee, 
whereas ours lie behind the occluder as seen from the viewcell. 3 De.nitions and Overview Our goal is 
to determine occlusion from within a viewcell on a con­servative discretization of space employing these 
de.nitions: A viewcell is an axis-aligned box that is either identical to or known to bound a volume 
of viewpoints of interest.  An occluder (or blocker) is an axially-aligned box causing occlusion by 
opacity or other properties established by the al­gorithm (see Sections 4.2 and 4.3).  A shaft is the 
convex intersection of half-spaces constructed from the visual events between the viewcell and the occluder. 
If the viewcell is considered as an extended light source, the volume inside the shaft is identical to 
the umbra of the oc­cluder.  A leaf voxel in the spatial subdivision is labeled opaque if it is completely 
inside an object, empty if it is completely outside all objects, or boundary [22] if it contains a portion 
of any object s surface. For this classi.cation we require the blockers of the scene to be water-tight 
solids.  We seek a tight overestimate of the PVS as seen from a viewcell so that quick visibility queries 
with respect to any viewpoint in the viewcell are possible. Given a viewcell, these are the necessary 
steps: Scene discretization. Rasterize the boundary of scene objects into the discretization of space 
and determine which voxels of space are completely inside an object and therefore opaque. Blocker extension. 
Traverse the discretization of space and .nd an opaque voxel that is not already hidden. Group this blocker 
with neighboring opaque voxels to obtain an effective blocker. Shaft construction. Construct a shaft 
that encompasses the region of space hidden by this blocker as seen from the viewcell. Occlusion tracking. 
Use the shaft to classify the voxels into par­ tially or completely outside the shaft and fully inside 
and thus occluded. Take note of occluded voxels. The major contributions of this approach are a conservative 
scene discretization that decouples the effectiveness of visibility calculations from how the scene was 
modeled (i.e. presence of large polygons or convex objects, or number of polygons), and the intro­duction 
of blocker extension as a means of both .nding ef.cient blockers and performing effective occluder fusion. 
Finally, we im­prove on shafts by observing that 3D shafts can be treated entirely in 2D. We begin by 
describing our algorithm in the simple setting of two dimensions and note that this case is already suited 
to solve visibility queries on scenes such as a 2D .oor plan or the map of a city.  4 2D Case Our subdivisions 
of space are a quadtree in 2D and 2 1/2D, and an octree in 3D. In anticipation of the extension to 2 
1/2D and 3D we will uniformly call a node in the tree a voxel. Its shape is always a (2D or 3D) axis-aligned 
box. 4.1 Scene Discretization Our method requires that the interiors of objects can be distin­guished 
from their exteriors. Objects need to have a solid volume. The discretization represents a cube of space 
containing the scene. All voxels containing a surface are marked as boundary voxels1. After this step, 
boundary voxels completely separate empty regions of space from opaque regions. Next we classify the 
non-boundary voxels into empty and opaque voxels using the odd-parity rule [12] for point-in-polygon 
or point-in-closed-shape testing. We accelerate voxel classi.cation by propagating the voxel sta­tus 
with depth-.rst seed-.lling [22] up to the boundary voxels. Figure 1 shows an example taken from a simple 
test scene used throughout Section 4. Note that the blockers are not axis-aligned. Such an arrangement 
would improve the ef.ciency of the spatial discretization, but is not a requirement of the algorithm. 
Our algorithm deals with scenes containing non-solid objects in several ways. If a voxel size is known, 
such that after marking boundary voxels the empty regions of space form a connected set of voxels, a 
single call to seed-.lling will determine all empty vox­els. The opaque voxels are the remaining non-boundary 
voxels. If some objects have holes or interior faces, the algorithm can still run and use the opaque 
interior of other objects. Objects with holes 1Near the surfaces, the spatial hierarchy is subdivided 
down to a maxi­mum level. This maximizes the number of opaque voxels inside the objects.  Figure 1: 
Left: ten blocks of buildings to be projected onto the ground. Right: Marking empty space between the 
buildings with .ood-.ll.  Figure 2: Left: a viewcell in red and a blocker with its extension outlined 
in yellow. The blocker hides the voxels in blue. Right: occlusion without blocker extension. (Opaque 
voxels are shown in black.) have no de.ned interior and therefore are not used as occluders. In­terior 
faces are ignored by .ood-.lling only the empty space around objects. Alternatively, degenerate input 
data can be cleaned up and interior faces can be removed using a method such as the one given by Murali 
et al. [19].  4.2 Blocker Extension We recursively traverse the tree until we .nd an opaque voxel. This 
is the blocker used with the viewcell to construct the region of oc­cluded space. To maximize occlusion 
we .rst extend the blocker. This extension must not hinder the ease of constructing the hidden region, 
so we require the extension to keep the box-shape of the blocker (see left side of Figure 2). Other opaque 
voxels are shown in black and will be considered as blockers next. We extend the blocker along the coordinate 
axis, which maxi­mizes the angle subtended by the blocker. Extension proceeds on both sides in this direction 
until there is a non-opaque voxel within the width of the blocker. In the cases where two sides of the 
blocker are visible from the viewcell, the blocker can additionally be ex­tended into an L-shape as shown 
in Figure 3. From the viewcell, this L-shape appears exactly the same as the big dashed box en­closing 
the L-shape that is the blocker s .nal extension. We use two optimizations to quickly .nd large hidden 
regions of space. First, we use the blockers in the order from the viewcell outwards; hence, blockers 
that subtend a wide solid angle compared to their size get used .rst. Second, we use large blockers in 
high levels of our spatial data structure .rst, as these can be expected to occlude large regions of 
space quickly. Hidden regions are not considered further for blocker selection. Figure 3: Example of 
L-shaped blocker extension. First the blocker is extended laterally. If after this step more than one 
side of the blocker is visible from the viewcell, the blocker is extended along this side away from the 
viewcell. The resulting occlusion is larger. Only the regions enclosed by the thick lines need to be 
opaque. Figure 4: Because C is hidden by A, blocker B can be extended into C to create a much bigger 
region of occlusion, which hides the object on the right. Neither A nor B alone would occlude this object 
as seen from the viewcell. 4.3 Blocker Extension into Hidden Space Despite its simplicity, the idea 
of extending blockers into adjacent opaque space has not been used in previous approaches. In addi­tion, 
we make an even stronger point with blocker extension into hidden space, regardless of whether this space 
is empty or opaque. Extending blockers into hidden regions of space is based on the following observation 
(see Figure 4). An observer inside the viewcell is unable to distinguish whether a hidden voxel is opaque 
or empty. For the sake of .nding large occluders for this viewcell, we can assume that the voxel is opaque, 
and blocker extension can proceed into hidden voxels just as into opaque voxels. In fact, one can construct 
different scenes that look exactly the same from within the viewcell by arbitrarily changing hidden parts 
of the model. One well-known application of this property is the PVS, where all polygons in hidden areas 
of the scene are removed. Figure 5: Left: marking the occlusion for one occluder. Right: extending a 
second occluder through the region hidden by the .rst one. Figure 6: The .rst three steps of the algorithm 
as it marks occlu­sions for the red viewcell. Note how blockers are extended across the streets between 
the buildings. It is therefore licit to change hidden voxels into opaque ones, with the bene.t that larger 
blockers can be constructed. The blocker on the right side of Figure 5 is extended across blocks through 
empty space connecting one block of buildings to another. Nonetheless, by the argument introduced above, 
extending the blocker this far is valid. It effectively fuses the current blocker with the blocker shown 
on the left side of Figure 5. It was this blocker that caused all the voxels between buildings to be 
hidden. As shown in Figure 4, occluder fusion occurs if the second blocker overlaps the umbra of the 
.rst blocker. In general, any blocker can be arbitrarily extended inside the umbra of another blocker 
as the umbra is the region of space completely hidden from the viewcell. Figure 6 shows the progress 
of the algorithm after using only three extended blockers. In general, especially with scenes con­taining 
no major large polygons, occluder fusion is essential as is obvious from the comparison given in Figure 
7. It shows the occlu­sions detected after all blockers have been used for three different approaches 
from left to right: triangles as single convex occluders, opaque voxels without blocker extension, and 
opaque voxels with blocker extension. A few large polygons have caused some occlu­sion to be detected 
in the upper left of the triangle-based approach. If large polygons are present in the scene database, 
they can be used to bootstrap our approach to occlusion detection. 4.4 Shaft Construction and Occlusion 
Tracking We construct a shaft around the occluded region from the support­ing planes2 between the viewcell 
and the blocker. Details are given in the appendix. Our implementation differs from the one by Haines 
et al. [16] in that we replace set manipulations on box corners with table lookups. 2A supporting plane 
contains an edge from one object and a vertex from another object, such that both objects lie on the 
same side of the plane [4]. Figure 7: The blocks of buildings from above. Left: occlusions de­tected 
using triangles from the database as convex occluders. Mid­dle: occlusions detected without blocker extension. 
Right: occlu­sions detected with blocker extension. Once the shaft has been constructed, a recursive 
traversal of the spatial data structure .ags hidden voxels as occluded in the high­est tree node possible. 
Subtrees outside or inside the shaft are not traversed. If all the children of a voxel in the spatial 
data structure are found to be hidden, the parent can be marked as being hidden as well. An exception 
occurs in the 2 1/2 D case, which will be discussed in Section 6. Propagating visibility up the tree 
is useful for accelerat­ing traversal of the tree for blocker extension, for marking occluded regions, 
and for querying occlusion of original objects. 4.5 Querying Occlusion of Scene Objects Occlusion of 
the original objects is determined by inserting their bounding boxes into the tree and checking that 
all the voxels they overlap are hidden. When a bounding hierarchy exists on the input scene say on the 
block-, house-, and triangle-level occlusion queries can be further accelerated by interleaving the 
traversal of the bounding box hierarchy with the traversal of the tree. Note that we can also determine 
the visibility of objects that were not initially inserted into the tree. This allows objects unlikely 
to cause a lot of occlusion to be ignored when constructing the tree. Also, the visibility status can 
be determined for moving objects or objects that have been added to the scene.  5 3D Case Usually visibility 
algorithms for 2D are dif.cult to extend to 3D because the number of occlusion boundaries grows from 
O(n 2) to O(n 4) [10], and because the occlusion boundaries are no longer planar in general. In our case, 
however, the shaft construction ex­tends to 3D in a straightforward fashion. This is due to our choice 
of viewcells and blockers as axis-aligned boxes in which case oc­clusion boundaries remain planar. 5.1 
Shafts in 3D Haines et al. [16] simplify the construction of a shaft s plane equa­tions by noting that 
a shaft around axis-aligned boxes consists en­tirely of planes, the normals of which have at least one 
zero coordi­nate. In addition, a 3D shaft can be more ef.ciently treated as the intersection of three 
2D shafts, namely the shaft s projections onto the three coordinate planes as shown in Figure 8. A bounding 
box is then inside the shaft if it is inside each of the three 2D shafts. We ensure that planes orthogonal 
to the coordinate axes are included only in one 2D shaft. 5.2 Blocker Extension in 3D In 3D, a blocker 
must be extended along more than one dimension to subtend a large solid angle. We .rst extend the blocker 
along one Figure 8: A 3D shaft can be treated as the intersection of three 2D shafts (the shaft s orthographic 
projection along the coordinate axes).   blocker and first extension reduction to half extent orthogonal 
extension final blocker Figure 9: Blocker extension in 3D. axis, then reduce its size to half its length, 
then extend it orthogo­nally to the .rst extension, and .nally, extend again along the .rst axis as shown 
in Figure 9. We have tried to maximize the solid angle subtended by the blocker over different reduction 
fractions (see second step of Fig­ure 9). We did not observe any noticeable increase in the occlusions 
identi.ed, but CPU time increased considerably. It is our experi­ence that occluder fusion more than 
compensates for any subopti­mal blocker extension. Similar to the L-shaped blocker extension in 2D, the 
blocker is also extended along additionally visible faces as shown in Fig­ure 10. Other than that, occlusion 
detection in 3D works exactly the same as in 2D: voxels in the tree that are completely inside the shaft 
of the blocker are marked as occluded.  6 2 1/2D Case Memory requirements for a 3D octree are sometimes 
a concern. However, scenes such as terrains or cities can often be dealt with in 2 1/2D. The 2 1/2D quadtree 
is constructed by recording the height of the highest and lowest points of every primitive falling within 
a certain voxel (i.e. a square on the ground plane). No ray-casting or seed-.lling is necessary. Our 
tree construction algorithm strictly enforces the 2 1/2D characteristic of the input data set and .lters 
 viewcell extension box eventuallyextension used viewcell two additional faces visible Figure 10: Additional 
step of blocker extension in the direction orthogonal to the extensions shown in Figure 9. If two additional 
faces are visible, their minimum extension must be used.  out multiple triangles above a single location 
on the ground plane. Voxels are occluded if they are inside the shaft up to their maximum height and 
can be used as an occluder up to their minimum height. We perform occluder extension the same way as 
in the 2D case including L-shaped extension; shaft construction is the same as in the 3D case three 
2D shafts are generated. The shaft plane co­incident with the ground is ignored. Even though only one 
shaft in the ground plane and one plane connecting the highest point on the viewcell with the lowest 
point on the blocker could be used, we found the three 2D shafts to be more effective as they allow easy 
tracking of occluded height in the 2D projections. We store this height per hidden voxel. An important 
difference to the previous two cases of 2D and 3D occlusion detection occurs with the propagation of 
visibility up the tree. Even if all the children are hidden, the parent is not necessar­ily hidden everywhere 
up to its maximum height. Calculating and storing the height up to which a voxel is hidden helps both 
with visibility propagation up the tree and also with occluder extension, because higher occluders can 
be extended into a certain voxel. Figure 11 shows examples of detected occlusion in a terrain model with 
the maximum tree subdivision of 6, 8 and 10.  7 Applications and Results We apply occlusion detection 
to the following rendering algo­rithms: visibility preprocessing for real-time walkthroughs and shadow-ray 
culling in ray-tracing. We use the 2 1/2D quadtree to .nd both viewcells and their PVS in an outdoor 
city environment. The 3D version is then applied to .nd the surfaces not receiving direct light from 
extended light sources in a ray-tracer. 7.1 Visibility Preprocessing for Walkthroughs Since cities and 
other outdoor scenes are predominantly 2 1/2D, we use the 2 1/2D quadtree described in Section 6 for 
occlusion calculations. The model shown in Figure 12 consists of 316,944 triangles that are organized 
into 665 buildings and 125 blocks com­plete with streets and sidewalks. Table 1 shows statistics on how 
long the quadtree takes to build based on the maximum subdivi­sion level in the tree (e.g. subdivision 
8 stands for a 2562 maximum subdivision). Figure 12 shows the tree portions in blue found to be occluded 
for one viewcell. Using a moderate tree subdivision level (up to 8 levels) visibility queries are possible 
at interactive rates (a few tenths of a second). This includes querying objects in the tree. To avoid 
this expense of computation at runtime, we pre-compute PVSs for those regions of our model reachable 
during visual navigation. Our walkthrough system accepts the navigable space as a set of triangles describing 
the streets or paths. We .rst determine the PVS for every triangle in the street mesh by constructing 
a 3D bounding box around it and marking occluded sections of the model in the quadtree. Then we look 
up the objects such as buildings, terrain and street portions, trees, cars and people in the quadtree 
and only add those to the triangle s PVS that are not fully occluded. A total of 2,538 triangles in the 
streets have been automatically grouped into 700 street sections for database paging as shown in Figure 
13. By imposing an upper limit on the difference between the size of the triangles PVSs in one street 
section, our greedy merge algorithm limits the amount of overdraw within one sec­tion. The walkthrough 
system pre-fetches the geometry for adja­cent street sections as the user moves around so that exploration 
is possible without interruption. Such predictive database paging is impossible with online point-visibility 
methods. Preprocessing this scene took 55 minutes, the vast majority of which was spent to .nd the PVS 
for every viewcell. The remaining time is used for building the tree and grouping viewcells into street 
sections. The average time for .nding the PVS of a triangle is less than a second. tree levels nodes 
memory (kB) time to build (sec) 7 11,949 574 2.669 8 39,625 1,902 3.704 9 133,577 6,412 6.381 10 472,561 
22,683 10.197 Table 1: 2 1/2 D quadtree generation statistics: number of nodes, memory, and build time 
as a function of the maximum subdivision level in the tree on a MIPS R10k processor running at 250 MHz. 
 We have retrieved the PVS for every triangle with two different granularities from the tree for comparison: 
blocks and buildings. For block-based PVS, an average of 33.96 blocks were found po­tentially visible 
out of 125 blocks, the minimum count of visible blocks was eight, the maximum 82. It is apparent that 
querying the bounding box of a whole block in the quadtree results in a rather high over-estimation of 
the PVS. For building-based PVS, the average building count was 54 out of a total of 665 buildings, sidewalks, 
and road segments. The min­imum and maximum numbers were 12 and 156. In this case, we queried the smallest 
bounding boxes available in the bounding-box hierarchy of our model. We are unaware of a method that 
could compute an accurate ref­erence solution to this problem. Instead we have tried to compute a good 
approximation to the true solution using point sampling. From every triangle in the street mesh we took 
twenty 360 degree block-based total: 125 blocks (about 2,500 tris on avg) PVS size our method point samples 
difference min 8 6 0 max 82 48 47 avg 33.96 20.3 13.64 building-based total: 665 buildings (about 475 
tris on avg) PVS size our method point samples difference min 12 8 0 max 156 90 81 avg 53.87 33.99 19.88 
 Table 2: Comparison of PVS sizes for block-based (125 blocks) and building-based (665 buildings) PVS 
computation. Going from block-based to building-based PVSs reduced the average difference between the 
two methods from 10.91% to 2.98% of the complete model. 2562 pixel images recording the visible object 
per pixel into an item buffer. The set union of the objects visible in these images was saved as the 
PVS for that triangle. We noted a couple of dif.culties in this point-sampled reference solution: narrow 
gaps between buildings can still be missed and the blocks or buildings visible through these gaps are 
not reported as visible. Also, in views looking down long straight streets, the sidewalks and streets 
project to very small areas in screen space and can be missed as well. This supports the need for methods 
such as the one presented in this paper. We give the comparison between our PVS and the point-sampled 
solution in Table 2. Note that the difference is always positive or zero, which demonstrates that our 
method is conservative. Finally, Figure 14 shows the difference in drawing time mea­sured on an SGI In.nite 
Reality system. The left shows results for block-based PVS, the right shows results for building-based 
PVS compared to IRIS Performer view-frustum culling.  7.2 Shadow Ray Acceleration When ray-tracing complex 
scenes with many lights, tracing shadow rays and computing object intersections can account for 95% of 
the rendering time [15]. It is therefore desirable to minimize the number of rays traced. Methods have 
been developed to accelerate shadow rays for point sources [15, 30], but the work published for extended 
lights is either approximate [17] or not directly applicable to a ray tracer [25]. Extended lights are 
preferable, as they cast soft shadows rather than the unnaturally looking sharp shadows caused by point 
lights. Our shadow ray acceleration is a generalization of the method of Woo and Amanatides [30] from 
point light sources to extended light sources. Figure 15 shows an example of the artifacts to be expected 
if one tries to apply a point-light acceleration technique to an extended light source. The point light 
causes a sharp shadow which is contained in the region of penumbra of an area light source at the same 
position. A point-light acceleration algorithm falsely identi.es regions of the penumbra as in shadow 
and pronounced boundaries appear in the umbra where there should be a continuous light-intensity variation. 
The acceleration technique uses the full 3D version of our algo­rithm with space represented as an octree, 
as described in Section 5. We .nd the bounding box for every light source in the model and use it as 
a viewcell. For every light source we copy and keep the visible portion of the octree3. In the case of 
many light sources, memory consumption is a concern, and only a speci.c number of top levels in the tree 
are copied with only a small increase in the 3Figure 19 shows that these copies require memory comparable 
to the initial octree 0.06 0.06 0.04 0.04 0.02 0.02 0.00 0.00  number of occlusions that are not detected 
(as shown in the results below). Our representation of the octree uses eight bytes per node. As the ray-tracer 
renders the scene, every light s octree is queried to determine whether the current sample point is potentially 
visible to the light. Shadow rays need only to be cast for potentially visible points. The top of Figure 
16 shows a view of a city block at night with ten street lights around it. Below is a false-color image 
of the same block from above where every pixel is given a color based on which light sources would be 
queried with shadow rays. All images in this section are rendered with one sample per pixel, and global 
illumination taken into account using stochastic sampling optimized with irradiance gradient caching 
[29]. The ray tracer uses hierarchical grids as the general acceleration data structure.  Preprocessing 
took 53.3 seconds on a Pentium II 400MHz pro­cessor, and the time to render this image was reduced from 
about 250 seconds to 100 seconds (a 60% saving). This is particularly ad­vantageous for animation sequences 
with static lighting, where the Rendering Time Rendering Time preprocessing is amortized over the full 
set of frames. 50 Figure 17 shows more complex scenes: a gallery with paintings 40  8 and people (17,701 
triangles and 23 light sources) and a residen­ tial area (616,509 triangles, 459 light sources). The 
bars on the left of Figures 18 and 19 give tree construction time and initial tree 6 time (min) 30 20 
4 memory usage respectively as a function of the maximum octree 10 2 subdivision level. The right graph 
in Figure 18 plots rendering time as a function of the number of levels kept in the octree per light. 
The right graph in Figure 19 gives the memory requirements for the octree copies. The original tree is 
no longer needed during render­ing. By considering both charts together, one observes that seven lev­els 
in the octree are not suf.cient to accurately capture the occluders in the gallery scene (eight in the 
residential area scene). However, there is no longer a substantial difference between the rendering times 
using a 2563 octree or a 5123 octree (5123 or 10243 for the residential area scene). It is surprising 
that three or more levels can be discarded from the light source octree without paying a sig­ni.cant 
price in the number of occlusions missed. However, the savings in memory are quite substantial. Finally, 
Figure 20 shows the effectiveness of the method by giv­ing the percentage of shadow rays successfully 
culled and the per­centage of traced shadow rays, which were found to intersect geom­etry. This is the 
percentage of rays that was not reported as occluded even though the sample point is hidden from the 
light source. Note 0 0 Figure 18: Rendering times for the two models: left: gallery, right: residential 
area. Different octree depths are shown, the number of levels kept per light varies along the x-axis. 
how this number decreases as the octree resolution increases. In the case of the residential area, the 
percentage of rays blocked although not reported as such remains quite high, because the lamp geome­try 
was not inserted into the octree and, therefore, sample points on the facades at a height above the light 
could not be found to be in shadow. The results can be summarized as follows: with a doubled memory consumption 
rendering is accelerated by a factor of three for the gallery scene and by a factor of four for the residential 
area scene. Memory Usage Memory Usage such as kd-trees or more sophisticated boundary nodes to improve 
60 300 memory (MB) 200 100 0 Efficiency Efficiency 100 the effectiveness of the method. We also want 
to investigate the success of blocker extension in other previously proposed visibility methods. Moreover, 
by rasterizing freeform patches or constructive solid geometry our visibility algorithm could be extended 
to these modeling approaches.   Acknowledgments We would like to thank Henrik Wann Jensen for allowing 
us to build on his Dali ray tracer for the results presented in Section 7.2. Max Chen built the city 
model that appears in Figure 17; Byong Mok Oh contributed shaders to render it. This work was supported 
by an NSF Postdoctoral Research As­sociates award (EIA-9806139), an NSF CISE Research Infrastruc­ture 
award (EIA-9892229), an NSF-INRIA Cooperative Research award (INT-9724005), and a grant from Intel Corporation. 
iMAGIS is a joint research project of CNRS, INRIA, INPG and the Joseph Fournier University of Grenoble. 
 80 60 efficiency (%)  References [1] Airey, John M., John H. Rohlf and Frederick P. Brooks, Jr., Towards 
Image Realism with Interactive Update Rates in Complex Virtual Building Environments, Symposium on In­ 
40 20 4 6 810 teractive 3D Graphics 1990, pp 41-50. levels kept levels kept Figure 20: Effectiveness 
of the octree for eliminating shadow rays: left: gallery, right: residential area. The percent of shadow 
rays eliminated is shown as a solid line, the number of shadow rays that resulted in an intersection, 
but were not reported as occluded is shown as a dashed line. The number of levels kept per light varies 
along the x-axis. 8 Conclusions and Future Work We have presented a method to compute a conservative 
approxima­tion of the space hidden as seen from a viewcell. Voxels from a dis­cretization of space are 
classi.ed into empty, opaque, and boundary. Opaque voxels are used as blockers, and a shaft is constructed 
to de­termine the portion of space hidden behind them. We apply blocker extension both into adjacent 
opaque voxels and hidden voxels to maximize the size of blockers. Blocker extension into hidden regions 
of space is motivated by the fact that arbitrary assumptions can be made about scene por­tions hidden 
to the viewer. For the sake of blocker extension, we assume them to be opaque and extend blockers into 
them, thereby fusing blockers with any blocker or group of blockers that caused this region to be hidden. 
We have applied the method to visibility preprocessing and have obtained a tight superset of the actual 
PVS as seen from a region of space. Such information is also useful for subdividing the space of reachable 
viewpoints into sections for managing on-the-.y paging of geometry. In the context of ray-tracing, we 
have successfully eliminated a major fraction of shadow rays cast toward extended lights. These rays 
are usually necessary to calculate the regions of shadow as cast by the light. Despite the memory requirements 
for a .ne dis­cretization of space during preprocessing, a moderate amount of memory is suf.cient during 
rendering to capture occlusions with high .delity even for a large number of light sources. In the future 
we would like to investigate further applications of the available visibility information, automatic 
ways of choosing a suf.cient octree subdivision level, and other subdivision schemes [2] Catmull, Edwin, 
E., Computer Display of Curved Surface, IEEE Conference on Computer Graphics, Pattern Recognition and 
Data Structures, May 1975, pp 11-17. [3] Chin, Norman and Steven Feiner, Fast Object-Precision Shadow 
Generation for Area Light Sources Using BSP Trees, Symposium on Interactive Graphics (1992), pp 21-30. 
[4] Coorg, Satyan and Seth Teller, Temporally Coherent Con­servative Visibility, Proc. Twelfth Annual 
ACM Symposium on Computational Geometry, Philadelphia, PA, May 24-26, 1996, pp 78-87. [5] Cohen-Or, Daniel 
and Amit Shaked, Visibility and Dead-Zones in Digital Terrain Maps, EUROGRAPHICS 95 14 3 (1995) pp 171-180. 
[6] Cohen-Or, D., G. Fibich, D. Halperin and E. Zadicario, Con­servative Visibility and Strong Occlusion 
for Viewspace Par­titioning of Densely Occluded Scenes, Computer Graphics Forum, 17 (3) 1998, pp 243-253. 
[7] Chrysanthou, Y., D. Cohen-Or, and D. Lischinski, Fast Ap­proximate Quantitative Visibility for Complex 
Scenes, Com­puter Graphics International 98, June 1998, pp 220-229. [8] Drettakis George, Fiume Eugene 
L., A Fast Shadow Algo­rithm for Area Light Sources Using Backprojection, SIG-GRAPH 94, pp 223-230. [9] 
Durand, Fredo, George Drettakis and Claude Puech, The Visibility Skeleton: A Powerful and Ef.cient Multi-Purpose 
Global Visibility Tool, SIGGRAPH 97, pp 89-100. [10] Durand, Fredo, 3D Visibility: Analytical Study and 
Applica­tions, PhD Dissertation, Universite Joseph Fournier, Greno­ble, France. [11] Durand, Fredo, George 
Drettakis, Joelle Thollot and Claude Puech, Conservative Visibility Preprocessing using Ex­tended Projections, 
SIGGRAPH 2000. [12] Foley, James, Andries van Dam, Steven Feiner and John Hughes, Computer Graphics: 
Principles and Practice, Addison-Wesley Publishing Co., ISBN 0-201-12110-7, 1990. [13] Greene, Ned Approximating 
Visibility with Environment Maps, Graphics Interface 86, pp 108-114. [14] Greene, Ned, Michael Kass and 
Gavin Miller, Hierarchical Z-Buffer Visibility, SIGGRAPH 93, pp 231-238. [15] Haines, A. Eric and Donald 
P. Greenberg, The Light Buffer: A Ray Tracer Shadow Testing Accelerator, IEEE Computer Graphics and Applications, 
6 9, 1986, pp 6-16. [16] Haines, A. Eric and John R. Wallace, Shaft Culling for Ef.cient Ray Cast Radiosity, 
Photorealistic Rendering in Computer Graphics (Proceedings of the Second Eurographics Workshop on Rendering), 
Springer-Verlag, New York, 1994, pp 122-138. [17] Hart, David, Philip Dutre and Donald P. Greenberg, 
Direct Illumination with Lazy Visibility Evaluation, SIGGRAPH 99, pp 147-154. [18] Hudson, Tom, Dinesh 
Manocha, Jonathan Cohen, Ming Lin, Kenneth E. Hoff III, Hansong Zhang, Occlusion Culling us­ing Shadow 
Volumes, Proceedings of 13th Symposium on Computational Geometry, Nice, France, June 4-6 1997, pp 1­ 
10. [19] Murali T.M., and Thomas A. Funkhouser, Consistent Solid and Boundary Representations from Arbitrary 
Polygonal Data, Symposium on Interactive 3D Graphics), 1997, pp 155-162. [20] Plantinga, Harry. and Charles 
Dyer, Visibility, Occlusion and the Aspect Graph, International Journal of Computer Vision, 5(2) 1990, 
pp 137-160. [21] Saona-Vazquez, Carlos, Isabel Navazo and Pere Brunet, The Visibility Octree. A Data 
Structure for 3D Navigation, TR LSI-99-22-R, Universitat Politecnica de Catalunya, Spain. [22] Samet, 
Hanan, Applications of Spatial Data Structures, Addison-Wesley, Reading, MA, ISBN 0-201-50300-X, 1990. 
[23] Stewart, James and S. Ghali, Fast computation of shadow boundaries using spatial coherence and backprojections, 
SIGGRAPH 94, pp 231-238. [24] Stewart, James, Hierarchical Visibility in Terrains, Euro­graphics Rendering 
Workshop, June 1997, pp 217-228. [25] Tanaka, Toshimitsu, and Tokiichiro Takahashi, Fast Analytic Shading 
and Shadowing for Area Light Sources, Computer Graphics Forum, Vol. 16, 3, 1997, pp 231-240. [26] Teller, 
Seth J. and Carlo H. Sequin, Visibility Preprocessing For Interactive Walkthroughs , SIGGRAPH 91, pp 
61-69. [27] Teller, Seth, Computing the Antipenumbra of an Area Light Source, SIGGRAPH 92, pp 139-148. 
[28] Teller, Seth, and Pat Hanrahan, Global Visibility Algorithms for Illumination Computation, SIGGRAPH 
94, pp 443-450. [29] Ward, Gregory J. and Paul Heckbert, Irradiance Gradients, Third Eurographics Workshop 
on Rendering, May 1992, pp 85-98. [30] Woo, Andrew and John Amanatides, Voxel Occlusion Test­ing: A Shadow 
Determination Accelerator for Ray Tracing, Graphics Interface 90, pp 213-219. [31] Yagel, Roni, and William 
Ray, Visibility Computation for Ef­.cient Walkthrough of Complex Environments, PRESENCE, Vol.5, No. 1, 
Winter 1996, pp 1-16. [32] Zhang, Hansong, Dinesh Manocha, Tom Hudson and Ken­neth E. Hoff, Visibility 
Culling using Hierarchical Occlusion Maps, SIGGRAPH 97, pp 77-88.  Appendix In 2D the shaft between 
two boxes is delimited by lines through the corners of the boxes and a subset of the blocker faces. Table 
3 lists the box corners that need to be connected. Figure 21 gives a pictorial overview of the cases. 
Case X: 0 X: 1 X: 2 X: 3 Y: 0 1,2 3,2 3,0 1,0 Y: 1 1,3 - 2,0 all Y: 2 0,3 0,1 2,1 2,3 Y: 3 0,2 all 3,1 
- Table 3: Box corners to be connected based on the relative posi­tion of the boxes. Cases where four 
connections must be made are marked all , but are only relevant for the 2 1/2D and 3D extensions if viewcells 
are limited to empty space. YYYY Figure 21: First row: cases of mutual positions between blocker and 
viewcell along the X-axis. The shafts are shown in grey. The rest of the .gure gives a pictorial impression 
of the contents of Table 3. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344891</article_id>
		<sort_key>239</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>25</seq_no>
		<title><![CDATA[Conservative visibility preprocessing using extended projections]]></title>
		<page_from>239</page_from>
		<page_to>248</page_to>
		<doi_number>10.1145/344779.344891</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344891</url>
		<abstract>
			<par><![CDATA[<p>Visualization of very complex scenes can be significantly accelerated using <italic>occlusion culling</italic>. In this paper we present a visibility preprocessing method which efficiently computes potentially visible geometry for volumetric viewing cells. We introduce novel <italic>extended projection</italic> operators, which permits efficient and conservative occlusion culling with respect to all viewpoints within a cell, and takes into account the combined occlusion effect of multiple occluders. We use extended projection of occluders onto a set of projection planes to create extended occlusion maps; we show how to efficiently test occludees against these occlusion maps to determine occlusion with respect to the entire cell. We also present an improved projection operator for certain specific but important configurations. An important advantage of our approach is that we can re-project extended projections onto a series of projection planes (via an <italic>occlusion sweep</italic>), and accumulate occlusion information from multiple blockers. This new approach allows the creation of effective occlusion maps for previously hard-to-treat scenes such as leaves of trees in a forest. Graphics hardware is used to accelerate both the extended projection and reprojection operations. We present a complete implementation demonstrating significant speedup with respect to view-frustum culling only, without the computational overhead of on-line occlusion culling.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[PVS]]></kw>
			<kw><![CDATA[occlusion culling]]></kw>
			<kw><![CDATA[visibility determination]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14030619</person_id>
				<author_profile_id><![CDATA[81100055904]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fr&#233;do]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Durand]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[iMAGIS, GRAVIR/IMAG, INRIA and Laboratory for Computer Science, MIT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40026929</person_id>
				<author_profile_id><![CDATA[81100408270]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[George]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Drettakis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[iMAGIS, GRAVIR/IMAG, INRIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40037153</person_id>
				<author_profile_id><![CDATA[81319502458]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jo&#235;lle]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Thollot]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[iMAGIS, GRAVIR/IMAG, INRIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31024721</person_id>
				<author_profile_id><![CDATA[81100047435]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Claude]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Puech]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[iMAGIS, GRAVIR/IMAG, INRIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>300554</ref_obj_id>
				<ref_obj_pid>300523</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[D. Aliaga, J. Cohen, A. Wilson, Eric Baker, H. Zhang, C. Erikson, K. Hoff, T. Hudson, W. Smerzlinger, R. Bastos, M. Whitton, F. Brooks, and D. Manocha. MMR: An interactive massive model rendering system using geometric and image-based acceleration. In ACM Syrup. on Interactive 3D Graphics, 1999.]]></ref_text>
				<ref_id>ACW+99</ref_id>
			</ref>
			<ref>
				<ref_obj_id>91416</ref_obj_id>
				<ref_obj_pid>91385</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[J. Airey, J. Rohlf, and F. Brooks, Jr. Towards image realism with interactive update rates in complex virtual building environments. In ACM Syrup. on Interactive 3D Graphics, 1990.]]></ref_text>
				<ref_id>ARB90</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325171</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[M. Cohen and D. Greenberg. The hemicube: A radiosity solution for complex environments. In Computer Graphics (Proc. Siggraph), 1985.]]></ref_text>
				<ref_id>CG85</ref_id>
			</ref>
			<ref>
				<ref_obj_id>266777</ref_obj_id>
				<ref_obj_pid>266774</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[D.A. Carlson and J. K. Hodgins. Simulation levels of detail for realtime animation. In Graphics Interface, 1997.]]></ref_text>
				<ref_id>CH97</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360354</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[J.H. Clark. Hierarchical geometric models for visible surface algorithms. Communications of the ACM, October 1976.]]></ref_text>
				<ref_id>Cla76</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[D. Cohen-Or, G. Fibich, D. Halperin, and E. Zadicario. Conservative visibility and strong occlusion for visibility partitionning of densely occluded scenes. In Eurographics, 1998.]]></ref_text>
				<ref_id>COFHZ98</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[D. Cohen-Or and E. Zadicario. Visibility streaming for network-based walkthroughs. In Graphics Interface, 1998.]]></ref_text>
				<ref_id>COZ98</ref_id>
			</ref>
			<ref>
				<ref_obj_id>310420</ref_obj_id>
				<ref_obj_pid>310917</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[S. Coorg and S. Teller. Temporally coherent conservative visibility. In ACM Symp. On Computational Geometry, 1996.]]></ref_text>
				<ref_id>CT96</ref_id>
			</ref>
			<ref>
				<ref_obj_id>253312</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[S. Coorg and S. Teller. Real-time occlusion culling for models with large occluders. In ACM Symp. on Interactive 3D Graphics, 1997.]]></ref_text>
				<ref_id>CT97</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Fr6do Durand. 3D Visibility, analysis and applications. PhD thesis, U. Joseph Fourier, Grenoble, 1999. http://www-imagis.imag, fr.]]></ref_text>
				<ref_id>Dur99</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166149</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[T. Funkhouser and C. S6quin. Adaptive display algorithm for interactive frame rates during visualization of complex virtual environments. In Computer Graphics (Proc. Siggraph), 1993.]]></ref_text>
				<ref_id>FS93</ref_id>
			</ref>
			<ref>
				<ref_obj_id>199418</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[T. Funkhouser. RING - A client-server system for multi-user virtual environments. ACM Symp. on Interactive 3D Graphics, 1995.]]></ref_text>
				<ref_id>Fun95</ref_id>
			</ref>
			<ref>
				<ref_obj_id>241024</ref_obj_id>
				<ref_obj_pid>241020</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[T. Funkhouser. Database management for interactive display of large architectural models. In Graphics Interface, 1996.]]></ref_text>
				<ref_id>Fun96</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166147</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[N. Greene, M. Kass, and G. Miller. Hierarchical Z-buffer visibility. In Computer Graphics, (Proc. Siggraph), 1993.]]></ref_text>
				<ref_id>GKM93</ref_id>
			</ref>
			<ref>
				<ref_obj_id>262847</ref_obj_id>
				<ref_obj_pid>262839</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[T. Hudson, D. Manocha, J. Cohen, M. Lin, K. Hoff, and H. Zhang. Accelerated occlusion culling using shadow frusta. InACMSymp. on Computational Geometry, 1997.]]></ref_text>
				<ref_id>HMC+97</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[C.B. Jones. A new approach to the 'hidden line' problem. The Computer Journal, 14(3):232-237, August 1971.]]></ref_text>
				<ref_id>Jon71</ref_id>
			</ref>
			<ref>
				<ref_obj_id>199422</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[D. Luebke and C. Georges. Portals and mirrors: Simple, fast evaluation of potentially visible sets. In ACM Symp. on Interactive 3D Graphics, 1995.]]></ref_text>
				<ref_id>LG95</ref_id>
			</ref>
			<ref>
				<ref_obj_id>300528</ref_obj_id>
				<ref_obj_pid>300523</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[F. Law and T. Tan. Preprocessing occlusion for real-time selective refinement. In ACM Symp. on Interactive 3D Graphics, 1999.]]></ref_text>
				<ref_id>LT99</ref_id>
			</ref>
			<ref>
				<ref_obj_id>108695</ref_obj_id>
				<ref_obj_pid>108693</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Max. Unified sun and sky illumination for shadows under trees. Comp. Vision, Graphics, and Image Processing. Graphical Models and Image Processing, 53(3):223-230, May 1991.]]></ref_text>
				<ref_id>Max91</ref_id>
			</ref>
			<ref>
				<ref_obj_id>111137</ref_obj_id>
				<ref_obj_pid>111135</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[H. Plantinga and C. R. Dyer. Visibility, occlusion, and the aspect graph. Int. J. of Computer Vision, 5(2), 1990.]]></ref_text>
				<ref_id>PD90</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192262</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[J. Rohlf and J. Helman. IRIS performer: A high performance multiprocessing toolkit for real-Time 3D graphics. In Computer Graphics (Proc. Siggraph), 1994.]]></ref_text>
				<ref_id>RH94</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[O. Sudarsky and C.Gotsman. Output-sensitive visibility algorithms for dynamic scenes with applications to virtual reality. In Proc. Eurographics Conf., 1996.]]></ref_text>
				<ref_id>SC96</ref_id>
			</ref>
			<ref>
				<ref_obj_id>344886</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[G. Schaufler, J. Dorsey, X. Decoret, and F. Sillion. Conservative volumetric visibility with occluder fusion. In Computer Graphics (Proc. Siggraph), 2000.]]></ref_text>
				<ref_id>SDDS00</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237209</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[J. Shade, D. Lischinski, D. Salesin, and T. DeRose. Hierarchical image caching for accelerated walkthroughs of complex environments. In Computer Graphics (Proc. Siggraph), 1996.]]></ref_text>
				<ref_id>SLSD96</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280927</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[C. Soler and F. Sillion. Fast calculation of soft shadow textures using convolution. In Computer Graphics, (Proc. Siggraph), 1998.]]></ref_text>
				<ref_id>SS98</ref_id>
			</ref>
			<ref>
				<ref_obj_id>731964</ref_obj_id>
				<ref_obj_pid>647651</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[A. James Stewart. Hierarchical visibility in terrains. Eurographics Workshop on Rendering 1997, June 1997.]]></ref_text>
				<ref_id>Ste97</ref_id>
			</ref>
			<ref>
				<ref_obj_id>171029</ref_obj_id>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[S.J. Teller. Visibility Computations in Densely Occluded Polyhedral Environments. PhD thesis, UC Berkeley, 1992.]]></ref_text>
				<ref_id>Tel92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166148</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[S. Teller and P. Hanrahan. Global visibility algorithms for illumination computations. In Computer Graphics (Proc. Siggraph), 1993.]]></ref_text>
				<ref_id>TH93</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122725</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[S. Teller and C. S6quin. Visibility preprocessing for interactive walkthroughs. In Computer Graphics (Proc. Siggraph), 1991.]]></ref_text>
				<ref_id>TS91</ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383843</ref_obj_id>
				<ref_obj_pid>2383815</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[M. van de Panne and J. Stewart. Effective compression techniques for precomputed visibility. In Eurographics Workshop on Rendering, 1999.]]></ref_text>
				<ref_id>vdPS99</ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Y. Wang, H. Bao, and Q. Peng. Accelerated walkthroughs of virtual environments based on visibility processing and simplification. In Proc. Eurographics Conf., 1998.]]></ref_text>
				<ref_id>WBP98</ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[P. Wonka and D. Schmalstieg. Occluder shadows for fast walkthroughs of urban environments. In Proc. Eurographics Conf., 1999.]]></ref_text>
				<ref_id>WS99</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258781</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[H. Zhang, D. Manocha, T. Hudson, and K. E. Hoff III. Visibility culling using hierarchical occlusion maps. In Computer Graphics (proc. Siggraph), 1997.]]></ref_text>
				<ref_id>ZMHH97</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 - . .. .  = = To appear in the SIGGRAPH 2000 conference proceedings Fig. 11. The projection of the 
occluder is the most time-consuming 15 successive planes are used for the occlusion sweep. 95.5% of task. 
A log-log linear .t reveals that the observed growth of the total the geometry was culled. Fig. 15 shows 
our sweeping process (we v running time is in n where n is the number of input polygons. If show only 
one quadrant of the forest for clarity). Observe how the the total volume of all viewing cells varies 
proportionally to the leaves aggregate on the Occlusion Map.number of input polygons, the growth is then 
n1.5 The improved P rojection presented in section 4 results in PVS 5 to 10% smaller. This is not dramatic, 
but recall that it comes : : at no cost. In addition the implementation is simpler, since only view frustum 
only supporting lines are considered. otil frime time i riw t cull t 6 :    triingles sent to i 
eline view frustum only 1 0.8 occlusion culling occlusion culling total time (s/cell) 0.6 occluder 
projection 0.4 occludee test 0.2 pyramid construction occluder selection 0 0 500 1000 1500 2000 2500 
input scene (Kpoly) (a) (b) Figure 14: Statistics gathered during interactive walkthrough for a forest 
scene scene of 7.8M polygons on an Onyx2. (a) total frame time (app+cull+draw) in seconds. (b) Number 
of triangles sent to the graphics pipeline. For a walkthrough of 30 sec, we obtained an average speed 
up of 24 for the interactive display, achieving a framerate between 8.6 and 15 fr/s (Fig.14). Figure 
11: Preprocess running time versus input scene size. We used a scene consisting of the city district 
replicated 12 times (1.8M polygons) and 3,000 moving cars of 1.4K polygons each, resulting in a total 
of 6M polygons. We performed the prepro­cess for the streets of only one district. The 1,500 initial 
visibility cells were subdivided into 6,845 leaf cells by our adaptive method (12,166 cells were evaluated, 
where parent cells are included). The average occlusion ef.ciency was 96% and the delta-PVS required 
60 MBytes of storage. The total preprocess took 165 minutes (0.81s/cell), of which 101 minutes for the 
occluder P rojection.We can extrapolate that the preprocess would take 33 hours for the streets of the 
12 districts. For an 800 frame walkthrough, an aver­age speed-up of 18 was obtained over SGI Performer 
view frustum culling (Fig. 12). This is lower than the average geometry ratio of # polys after frustum 
cull 24 (i.e. # polys after occlusion cull ) because of constant costs in the walkthrough loop of Performer. 
Fig. 13 illustrates our results. view frustum only view frustum only 7.3 Discussion We .rst have to 
note that the occlusions our method identi.es are a subset of those detected by a point-based method 
[GKM93, ZMHH97]. Advantages of those methods also include their abil­ity to treat dynamic occluders and 
the absence of preprocess or PVS storage. However, our method incurs no cost at display time, while in 
the massive rendering framework implemented at UNC [ACW+99] two processors are sometimes used just to 
perform oc­clusion culling. Moreover, for some applications (games, network­based virtual tourism, etc.), 
the preprocessing time is not really a problem since it is performed once by the designer of the applica­tion. 
Our PVS data then permits an ef.cient predictive approach to database pre-fetching which is crucial when 
displaying scenes over a network or which cannot .t into main memory. We now discuss the conditions in 
which our method succeeds or fails to detect occlusion (in.nite resolution of the Depth Map is here assumed). 
In Fig. 16 we represent in grey the volume corre­sponding to all the possible P rojections of the occluders. 
The ac­ : tual P rojections corresponds to the intersection of these P rojection otil frime timeitcullt 
riw volumes with the projection plane. The occlusion due to a single oc­ triingles sent to i eline 
 occlusion culling 6 cluder is completely encoded if the occluder is in front of the plane : and if 
its P rojection volume intersects the plane (16(a)). If the plane  occlusion culling is farther away, 
the P rojection of the occluder becomes smaller, but  so do the improved P rojections of the occludees 
(however, if reso­(a) (b) lution is taken into account, more distant planes are worse because Figure 
12: Statistics gathered during interactive walkthrough for a scene of 6M poly­gons on an Onyx2. (a) Total 
frame time (app+cull+draw) in seconds. (b) Number of triangles sent to the graphics pipeline. As an informal 
comparison, we have implemented the algorithm of Cohen-Or et al. [COFHZ98, COZ98]. For the city model, 
their algorithm declares four times more visible objects on average and the computation time in our implementation 
is 150 times higher than for extended projection. 7.2 Occlusion sweep To test the occlusion sweep, we 
used a model of a forest containing around 7,750 trees with 1,000 leaves each (7.8M triangles). The P 
rojection of the leaves close to the projection plane were com­puted using the convex occluder P rojection 
using the stencil buffer. The size of the convolution kernel was .xed to 5 pixels, and we used 15 planes 
for the sweep. The occlusion sweep took around 23 seconds per cell, 59 minutes for all 158 cells (no 
adaptive re­cursion was performed). This is slower than for the city because of our conservative rasterization). 
Figure 15: The sweeping process: (a) Part of our 7.8M polygon forest model, (b)-(d) three positions for 
the sweep projection planes. The yellow bounding boxes are the culled occludees.  ´ ´ -   ´ 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344899</article_id>
		<sort_key>249</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>26</seq_no>
		<title><![CDATA[Adaptively sampled distance fields]]></title>
		<subtitle><![CDATA[a general representation of shape for computer graphics]]></subtitle>
		<page_from>249</page_from>
		<page_to>254</page_to>
		<doi_number>10.1145/344779.344899</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344899</url>
		<abstract>
			<par><![CDATA[<p>Adaptively Sampled Distance Fields (ADFs) are a unifying representation of shape that integrate numerous concepts in computer graphics including the representation of geometry and volume data and a broad range of processing operations such as rendering, sculpting, level-of-detail management, surface offsetting, collision detection, and color gamut correction. Its structure is uncomplicated and direct, but is especially effective for quality reconstruction of complex shapes, e.g., artistic and organic forms, precision parts, volumes, high order functions, and fractals. We characterize one implementation of ADFs, illustrating its utility on two diverse applications: 1) artistic carving of fine detail, and 2) representing and rendering volume data and volumetric effects. Other applications are briefly presented.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[carving]]></kw>
			<kw><![CDATA[distance fields]]></kw>
			<kw><![CDATA[graphics]]></kw>
			<kw><![CDATA[implicit surfaces]]></kw>
			<kw><![CDATA[level of detail]]></kw>
			<kw><![CDATA[rendering]]></kw>
			<kw><![CDATA[volume modeling]]></kw>
			<kw><![CDATA[volume rendering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Graphics data structures and data types</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Shape</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Modeling packages</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Shape</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010249</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Shape inference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011066.10011070</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Development frameworks and environments->Application specific development environments</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010249</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Shape inference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010242</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Shape representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010394</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics file formats</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14175624</person_id>
				<author_profile_id><![CDATA[81100502804]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sarah]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Frisken]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MERL, Mitsubishi Electric Research Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31068003</person_id>
				<author_profile_id><![CDATA[81100036447]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ronald]]></first_name>
				<middle_name><![CDATA[N.]]></middle_name>
				<last_name><![CDATA[Perry]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MERL, Mitsubishi Electric Research Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P15779</person_id>
				<author_profile_id><![CDATA[81100334998]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Alyn]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Rockwood]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MERL, Mitsubishi Electric Research Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31081399</person_id>
				<author_profile_id><![CDATA[81100554789]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Thouis]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Jones]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MERL, Mitsubishi Electric Research Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>245054</ref_obj_id>
				<ref_obj_pid>244979</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[R. Avila and L. Sobierajski, "A haptic interaction method for volume visualization", Proc. IEEE Visualization'96, pp. 197-204, 1996.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[J. Baerentzen, "Octree-based volume sculpting", Proc. Late Breaking Hot Topics, IEEE Visualization'98, pp. 9-12, 1998.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>549676</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[J. Bloomenthal, Introduction to Implicit Surfaces, Morgan Kaufman Publishers, 1997.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>288137</ref_obj_id>
				<ref_obj_pid>288126</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[D. Breen, S. Mauch and R. Whitaker, "3D scan conversion of CSG models into distance volumes", Proc. 1998 IEEE Symposium on Volume Visualization, pp. 7-14, 1998.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>259262</ref_obj_id>
				<ref_obj_pid>259081</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[M. Chow and M. Teichmann, "A Wavelet-Based Multiresolution Polyhedral Object Representation", Visual Proc. SIGGRAPH '97, p. 175, 1997.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>197952</ref_obj_id>
				<ref_obj_pid>197938</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[P. Cignoni, L. De Floriani, C. Montani, E. Puppo, R. Scopigno, "Multiresolution Modeling and Rendering of Volume Data based on Simplicial Complexes", 1994 ACM Volume Visualization Conference Proceedings, 1994, pp. 19-26.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>274366</ref_obj_id>
				<ref_obj_pid>274363</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[D. Cohen-Or, D. Levin, and A. Solomovici, "Three-dimensional distance field metamorphosis", ACM Transactions on Graphics, 1997.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280826</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[T. DeRose, M. Kass, T. Truong, "Subdivision surfaces in character animation", Proc. SIGGRAPH '98, pp. 85-94, 1998.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>184932</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[D. Ebert, F.K. Musgrave, D. Peachy, K. Perlin, S. Worley, Texturing and Modeling a Procedural Approach, Academic Press, 1998.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83821</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[J. Foley, A. van Darn, S. Feiner, and J. Hughes, Computer Graphics: Principles and Practice, Addison-Wesley, 1992.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>288142</ref_obj_id>
				<ref_obj_pid>288126</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[S. Gibson, "Using DistanceMaps for smooth surface representation in sampled volumes", Proc. 1998 IEEE Volume Visualization Symposium, pp. 23-30, 1998.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[R. Kimmel, N. Kiryati and A. Bruckstein, "Multi-valued distance maps for motion planning on surfaces with moving obstacles", IEEE Trans. on Robotics &amp; Automation, 14, pp. 427- 436, 1998.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97915</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[J. Lengyel, M. Reichert, B. Donald and D. Greenberg, "Real-time robot motion planning using rasterizing computer graphics hardware", Proc. SIGGRAPH '90, pp. 327-335, 1990.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97916</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[T. Nishita, T.W. Sederberg and M. Kakimoto, "Ray tracing trimmed rational surface patches", Proc. SIGGRAPH '90, pp. 337-345, 1990.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>56815</ref_obj_id>
				<ref_obj_pid>56813</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[S. Osher and J. Sethian, "Fronts propagating with curvature-dependent speed: algorithms based on Hamilton-Jacobi formulation", J. Computational Physics, 79, pp. 12-49, 1988.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>614435</ref_obj_id>
				<ref_obj_pid>614275</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[S. Parker, M. Parker, Y. Livnat, P. Sloan, C. Hansen, and P. Shirley, "Interactive ray tracing for volume visualization" IEEE Transactions On Visualization and Computer Graphics, Vol. 5 (3), pp. 238-250, 1999.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617722</ref_obj_id>
				<ref_obj_pid>616021</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[B. Payne and A. Toga, "Distance field manipulation of surface models", IEEE Computer Graphics and Applications, pp. 65-71, 1992.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[A. Ricci, "A constructive geometry for computer graphics", Computer Journal, Vol. 16, No. 2, pp. 157-160, 1973.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>77589</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[H. Samet, The Design and Analysis of Spatial Data Structures. Addison-Wesley, 1989.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>951099</ref_obj_id>
				<ref_obj_pid>951087</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[W. Schroeder, W. Lorensen, and S. Linthicum, "Implicit modeling of swept surfaces and volumes," Proc. Visualization '94, pp. 40-45, 1994.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Sensable Devices' FreeForm modeling software, http://www.sensable.com/freeform.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[J. Sethian, Level Set Methods: Evolving Interfaces in Geometry, Fluid Mechanics. Computer Vision, and Material Science. Cambridge University Press, 1996.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>48045</ref_obj_id>
				<ref_obj_pid>46165</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[M. Stone, W. Cowan, J. Beatty, "Color gamut mappings and the printing of digital color images", ACM Transaction on Graphics, Vol. 7, pp. 249-292, 1988.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383821</ref_obj_id>
				<ref_obj_pid>2383815</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[R. Westermann, O. Sommer, T. Ertl, "Decoupling polygon rendering from geometry using rasterization hardware", in Proc. Eurographics Rendering Workshop '99, pp. 45-56, 1999.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>833843</ref_obj_id>
				<ref_obj_pid>832271</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[R. Yagel, S. Lu, A. Rubello, R. Miller, "Volume-based reasoning and visualization of dicastability" In Proc. IEEE Visualization '95, pp. 359-362, 1995.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[K. Zuiderveld, A. Koning, and M. Viergever, "Acceleration of ray-casting using 3D distance transforms", in Proc. Visualization in Biomedical Computing '92, pp. 324-335, 1992.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Adaptively Sampled Distance Fields: A General Representation of Shape for Computer Graphics Sarah F. 
Frisken, Ronald N. Perry, Alyn P. Rockwood, and Thouis R. Jones MERL Mitsubishi Electric Research Laboratory 
   ABSTRACT Adaptively Sampled Distance Fields (ADFs) are a unifying representation of shape that 
integrate numerous concepts in computer graphics including the representation of geometry and volume 
data and a broad range of processing operations such as rendering, sculpting, level-of-detail management, 
surface offsetting, collision detection, and color gamut correction. Its structure is uncomplicated and 
direct, but is especially effective for quality reconstruction of complex shapes, e.g., artistic and 
organic forms, precision parts, volumes, high order functions, and fractals. We characterize one implementation 
of ADFs, illustrating its utility on two diverse applications: 1) artistic carving of fine detail, and 
2) representing and rendering volume data and volumetric effects. Other applications are briefly presented. 
CR Categories: I.3.6 [Computer Graphics]: Methodology and techniques Graphics data structures; I.3.5 
Computational Geometry and Object Modeling Object modeling Keywords: distance fields, carving, implicit 
surfaces, rendering, volume rendering, volume modeling, level of detail, graphics. {frisken,perry,rockwood,jones}@merl.com 
 Permission to make digital or hard copies of part or all of this work or personal or classroom use is 
granted without fee provided that copies are not made or distributed for profit or commercial advantage 
and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, 
to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. SIGGRAPH 
2000, New Orleans, LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 1. INTRODUCTION In this paper 
we propose adaptively sampled distance fields (ADFs) as a fundamental graphical data structure. A distance 
field is a scalar field that specifies the minimum distance to a shape, where the distance may be signed 
to distinguish between the inside and outside of the shape. In ADFs, distance fields are adaptively sampled 
according to local detail and stored in a spatial hierarchy for efficient processing. We recommend ADFs 
as a simple, yet consolidating form that supports an extensive variety of graphical shapes and a diverse 
set of processing operations. Figures 1, 2, and 3 illustrate the quality of object representation and 
rendering that can be achieved with ADFs as well as the diversity of processing they permit. Figures 
1 and 2 show fine detail carved on a slab and an artistic carving on a high order curved surface. Figure 
3 depicts an electron probability distribution of a molecule that has been volume rendered with a glowing 
aura that was computed using a 3D noise function. ADFs have advantages over several standard shape representations 
because as well as representing a broad class of forms, they can also be used for a number of important 
operations such as locating surface points, performing inside/outside and proximity tests, Boolean operations, 
blending and filleting, determining the closest points on a surface, creating offset surfaces, and morphing 
between shapes. It is important to note that by shape we mean more than just the 3D geometry of physical 
objects. We use it in a broad context for any locus defined in a metric space. Shape can have arbitrary 
dimension and can be derived from measured scientific data, computer simulation, or object trajectories 
through time and space. It may even be non-Euclidean. 2. BACKGROUND Commonly used shape representations 
for geometric design include parametric surfaces, subdivision surfaces, and implicit surfaces. Parametric 
representations include polygons, spline patches, and trimmed NURBs. Localizing (or generating) surface 
points on parametric surfaces is generally simpler than with other representations and hence they are 
easier to draw, tessellate, subdivide, and bound [3]. Parametric surfaces typically need associated data 
structures such as B-reps or space partitioning structures for representing connectivity and for more 
efficient localization of primitives in rendering, collision detection, and other processing. Creating 
and maintaining such structures adds to the computational and memory requirements of the representation. 
Parametric surfaces also do not directly represent object interiors or exteriors and are subsequently 
more difficult to blend and use in Boolean operations. While subdivision surfaces provide an enhanced 
design interface, e.g., shapes are topologically unrestricted, they still suffer from many of the same 
limitations as parametric representations, e.g., the need for auxiliary data structures [8], the need 
to handle extraordinary points, and the difficulty in controlling fine edits. Implicit surfaces are defined 
by an implicit function f(x.Rn) = c, where c is the constant value of the iso-surface. Implicit functions 
naturally distinguish between interior and exterior and can be used to blend objects together and to 
morph between objects. Boolean operations defined for implicit functions provide a natural sculpting 
interface for implicit surfaces [3, 18]; however, when many operations are combined to generate a shape 
the computational requirements for interactive rendering or other processing become prohibitive. Furthermore, 
it is difficult to define an implicit function for an arbitrary object, or to chart points on its surface 
for rendering and other processing. Volumetric data consists of a regular or irregular grid of sampled 
data, frequently generated from 3D image data or numerical simulation. Object surfaces can be represented 
as iso­surfaces of the sampled values and data between sample points can be reconstructed from local 
values for rendering or other processing. Several systems have been developed for sculpting volumetric 
data using Boolean operations on sample density values [1, 2]. However, in these systems, iso-surfaces 
lack sharp corners and edges because the density values are low-pass filtered near object surfaces to 
avoid aliasing artifacts in rendering. In addition, the need to pre-select volume size and the use of 
regular sampling force these systems to limit the amount of detail achievable. Sensable DevicesTM has 
recently introduced a commercial volume sculpting system [21]. To create detailed models, very large 
volumes are required (a minimum of 512 Mbytes of RAM) and the system is advertised for modeling only 
organic forms, i.e. shapes with rounded edges and corners. Additional representations of shape for computer 
graphics include look-up tables, Fourier expansions, particle systems, grammar-based models, and fractals 
(iterated function systems), all of which tend to have focused applications [10]. The ADF representation, 
its applications, and the implementation details presented in this paper are new. Sampled distance fields 
have, however, been used previously in a number of specific applications. They have been used in robotics 
for path planning [12, 13] and to generate swept volumes [20]. In computer graphics, sampled distance 
fields were proposed for volume rendering [11], to generate offset surfaces [4, 17], and to morph between 
surface models [7, 17]. Level sets can either be generated from distance fields or they can be used to 
generate sampled distance fields [15, 22]. As with regularly sampled volumes, regularly sampled distance 
fields suffer from large volume sizes and a resolution limited by the sampling rate. These limitations 
are addressed by ADFs. 3. ADAPTIVE DISTANCE FIELDS A distance field is a scalar field that specifies 
the minimum distance to a shape, where the distance may be signed to distinguish between the inside and 
outside of the shape. As simple examples, consider the distance field of the unit sphere S in R3 given 
by h(x) = 1 (x2 + y2 + z2) ½, in which h is the Euclidean signed distance from S, or h(x) = 1 (x2 + 
y2 + z2), in which h is the algebraic signed distance from S, or h(x) = (1 (x2 + y2 + z2))2, in which 
h is an unsigned distance from S. The distance field is an effective representation of shape. However, 
regularly sampled distance fields have drawbacks because of their size and limited resolution. Because 
fine detail requires dense sampling, immense volumes are needed to accurately represent classical distance 
fields with regular sampling when any fine detail is present, even when the fine detail occupies only 
a small fraction of the volume. To overcome this limitation, ADFs use adaptive, detail-directed sampling, 
with high sampling rates in regions where the distance field contains fine detail and low sampling rates 
where the field varies smoothly. Adaptive sampling permits arbitrary accuracy in the reconstructed field 
together with efficient memory usage. In order to process the adaptively sampled data more efficiently, 
ADFs store the sampled data in a hierarchy for fast localization. The combination of detail­directed 
sampling and the use of a spatial hierarchy for data storage allows ADFs to represent complex shapes 
to arbitrary precision while permitting efficient processing. In summary, ADFs consist of adaptively 
sampled distance values organized in a spatial data structure together with a method for reconstructing 
the underlying distance field from the sampled values. One can imagine a number of different instantiations 
of ADFs using a variety of distance functions, reconstruction methods, and spatial data structures. To 
provide a clear elucidation of ADFs, we focus on one specific instance for the remainder of this paper. 
This instance is simple, but results in efficient rendering, editing, and other processing used by applications 
developed in this paper. Specifically, we demonstrate an ADF which stores distance values at cell vertices 
of an octree data structure and uses trilinear interpolation for reconstruction and gradient estimation. 
The wide range of research in adaptive representations suggest several other ADF instantiations based 
on, for example, wavelets [5] or multi-resolution Delaunay tetrahedralizations [6]. 3.1 Octree-based 
ADFs Octree data structures are well known and we assume familiarity (see [19]). For purposes of instruction, 
we demonstrate the concepts in 2D (with quadtrees), which are easily generalized to higher dimensions. 
In a quadtree-based ADF, each quadtree cell contains the sampled distance values of the cell s 4 corners 
and pointers to parent and child cells. Given a shape as in Figure 4a, subdivision of a cell in the quadtree 
depends on the variation of the distance field (shown in Figure 4c) over the parent cell. This differs 
from 3-color quadtrees [19] which represent object boundaries by assigning one of three types to each 
cell in the quadtree: interior, exterior, and boundary. In 3-color quadtrees, all boundary cells are 
subdivided to a predetermined highest resolution level. In contrast, boundary cells of ADFs are only 
subdivided when the distance field within a cell is not well approximated by bilinear interpolation of 
its corner values. Hence, large cells can be used to represent edges in regions where the shape is relatively 
smooth, resulting in significantly more compression than 3-color quadtrees. This is illustrated in Figures 
4b and 4d where the ADF of 4d requires only 1713 cells while the 3-color quadtree of 4b requires 23,573 
cells. In the ADF quadtree, straight edges of the R are represented by large cells; only corners provoke 
repeated  Figures 4a R and 4b 3-color quadtree containing 23,573 cells. Figures 4c Distance field of 
R and 4d ADF containing 1713 cells. Error Triangle ADF Cell ADF Sample Tolerance Count Count Count 6.25 
x 10-5 3.13 x 10-5 1.56 x 10-6 32,768 131,072 2,097,152 16,201 44,681 131,913 24,809 67,405 164,847 
Table 1. Comparison of triangle count for a sphere (r = 0.4) to ADF size. subdivision. Figure 4d also 
shows that even highly curved edges can be efficiently represented by ADFs. Because bilinear interpolation 
represents curvature reasonably well, cells with smoothly curved edges do not require many levels in 
the ADF hierarchy. Cells that do require many levels in the hierarchy are concentrated at corners and 
cusps. These are typical statistics for 2D objects. As another indication of ADF size, Table 1 compares 
the number of triangles required to represent a sphere of radius 0.4 to the number of cells and distance 
(sample) values of the corresponding ADF when both the triangles and the interpolated distance values 
are within a given error tolerance from the true sphere. Higher order reconstruction methods and better 
predicates for subdivision might be employed to further increase compression, but the numbers already 
suggest a point of diminishing returns for the extra effort. 3.2 Generating ADFs The generation of an 
ADF requires a procedure or function to produce the distance function, h(x) at x . Rn, where distance 
is interpreted very broadly as in Section 3. Continuity, differentiability, and bounded growth of the 
distance function can be used to advantage in rendering or other processing, but are not required. Some 
of the images in this paper utilize distance functions that are non-differentiable (Figure 8) and highly 
non-Euclidean with rapid polynomial growth (Figures 2 and 3). One example of a distance function is the 
implicit form of an object, for which the distance function can correspond directly to the implicit function. 
A second example includes procedures that determine the Euclidean distance to a parametric surface. For 
 Figure 5. A thin walled version of the Utah Teapot rendered using sampled ray casting and Phong lighting 
from an ADF computed from a 32 bicubic Bezier patch model. example, Figure 5 was rendered from the distance 
field computed for a 32 bicubic Bezier patch model of the Utah Teapot. Distances to Bezier patches were 
determined by solving 7th order Bezier equations using the Bezier clipping algorithm described in [14]. 
To define an inside and outside for the teapot, the unsigned distance from the Bezier surface is biased 
to produce the signed distance field of an offset surface, resulting in a thin-walled teapot. Other distance 
functions include Euclidean distances for a triangle model that can be computed as the minimum of the 
signed distances to each of the triangles in the model and distance fields computed by applying Boolean 
operations to the distance fields of primitive elements in a CSG representation. Given a distance function, 
there are a number of ways to generate an ADF. Two simple examples include a bottom-up and a top-down 
approach which are described briefly here. The bottom-up approach starts with a regularly sampled distance 
field of finite resolution and constructs a fully populated octree for the 3D data. Starting with the 
smallest cells in the octree, a group of 8 neighboring cells is coalesced if and only if none of the 
cells have any child cells and the sampled distances of all of the 8 cells can be reconstructed from 
the sample values of their parent to a specified error tolerance. After all cells are considered for 
coalescing at a given level in the hierarchy, groups of cells at the next level are considered. When 
no cells are coalesced at a given level or the root node is reached, the ADF generation is complete. 
In the top-down approach, first the distance values for the root node of the ADF hierarchy are computed. 
ADF cells are then recursively subdivided according to a subdivision rule. For example, if the primary 
interest is the iso-surface represented in the field, the recursive subdivision would stop if the given 
cell is guaranteed not to contain the surface, if the cell contains the surface but passes some predicate, 
or if a specified maximum level in the hierarchy is reached. One can imagine many predicates to control 
the subdivision. In examples presented in this paper, we use a simple predicate that compares distances 
within a cell computed using the distance function to distances reconstructed from the cell s sampled 
values. In this predicate, the absolute differences between the computed and reconstructed distances 
are determined at the center of the cell and the centers of each of the cell s faces and edges (i.e. 
19 differences per cell). If any of the differences are greater than a specified error tolerance, the 
cell is subdivided. 3.3 Reconstructing ADFs Each ADF cell has an associated method for reconstructing 
distance values between sampled points. In the case of the 3D octree, distance values within a cell are 
reconstructed from the 8 corner distance values stored per cell using standard trilinear interpolation. 
In addition to distance values, many operations such as rendering, collision detection, or closest point 
localization require surface normals and hence, processing an ADF may also require a method for estimating 
surface normals from the sampled data. For distance fields, the surface normal is equal to the normalized 
gradient of the distance field at the surface. There are several methods for estimating the gradient 
of sampled data. We use the analytic gradient of the trilinear reconstruction within each cell: grad(x,y,z) 
= (h(xr,y,z) -h (xl,y,z), h(x,yu,z) -h(x,yd,z), h(x,y,zf) -h(x,y,zb)), where (xr,y,z), (xl,y,z), (x,yu,z), 
etc. are projections of (x,y,z) onto the right, left, up, down, front, and back faces of the cell, respectively. 
In theory, this cell-localized gradient estimation can result in C1 discontinuities at cell boundaries 
but as can be seen from the figures, these artifacts are not noticeable with sufficient subdivision. 
 4. APPLICATIONS AND IMPLEMENTATION DETAILS ADFs have application in a broad range of computer graphics 
problems. We present two examples below to illustrate the utility of ADFs and to provide some useful 
implementation details on processing methods such as rendering and sculpting ADF models. This section 
ends with short descriptions of several other applications to give the reader an idea of the diverse 
utility of ADFs. 4.1 Precise carving Figures 1, 2, and 6 show examples of objects represented and carved 
as ADFs. Because objects are represented as distance fields, the ADF can represent and reconstruct smooth 
surfaces from sampled data. Because the ADF efficiently samples distance fields with high local curvature, 
it can represent sharp surface corners without requiring excessive memory. Carving is intuitive; the 
object is edited simply by moving a tool across the surface. It does not require control point manipulation, 
remeshing the surface, or trimming. By storing sample points in an octree, both localizing the surface 
for editing and determining ray-surface intersections for rendering are efficient. Like implicit surfaces, 
ADFs can be sculpted using simple Boolean operations applied to the object and tool distance fields. 
Figures 1, 2, and 6 show carving using the difference operator, hcarved(x) = min(hobject(x), -htool(x)). 
Other operators include addition, hcarved(x) = max(hobject(x), htool(x)), and intersection, hcarved(x) 
= min(hobject(x), htool(x)). Blending or filleting can also be defined for shaping or combining objects 
(as was done for the molecules of Figures 3 and 7). While these Boolean operations apply to the entire 
distance field, for systems where only surfaces are important, application of the operations can be limited 
in practice to a region within a slightly extended bounding box of the tool. The basic edit operation 
is much like a localized ADF generation. The first step in the editing process is to determine the smallest 
ADF cell, or set of cells, entirely containing the tool s extended bounding box (obvious consideration 
of the ADF boundaries apply). The containing cell is then recursively subdivided, applying the difference 
operator to the object and tool values to obtain new values for the carved ADF. During the recursive 
subdivision, cell values from the object are obtained either from existing sampled values or by reconstruction 
if an edited cell is subdivided beyond its original level. Subdivision rules similar to those of top-down 
generation are applied, with the exception that the containing cell must be subdivided to some minimum 
level related to the tool size. The carving examples were rendered using ray casting with analytic surface 
intersection. In this method, a surface point is determined by finding the intersection between a ray 
cast into the ADF octree from the eye and the zero-value iso-surface of the ADF. Local gradients are 
computed at surface points using the Figure 6. A close up of the carved slab in Figure 1. gradient estimation 
described above (the figures were rendered with simple Phong lighting). When the traversing ray passes 
through a leaf node of the octree, intersection between the ray and the surface reconstructed from the 
8 cell sample values is tested. We have used two different methods to find the ray-surface intersection; 
a cubic root solver that finds the exact intersection of the ray with the trilinear surface defined by 
the distance values at the cell corners (as in [16]), and a linear approximation which determines the 
distance values where the ray enters and exits the cell and computes the linear zero-crossing if the 
two values have a different sign. Both methods work well but the linear approximation has proven to be 
faster and its rendered images are not visibly different from those rendered with the cubic solver. When 
solving for intersections, we set the distance at the entry point of a cell to be equal to the distance 
at the exit from the previous cell. This avoids the crack problem discussed in [24] for rendering hierarchical 
volume data, preventing C0 discontinuities in the surface where ADF cells of different size abut. Most 
of the images shown in this paper were rendered using a supersampling of 16 rays per pixel followed by 
the application of a Mitchell filter of radius 2.0. The octree promotes efficient ray traversal even 
for very complicated scenes. Rendering the Menger Sponge (Figure 8) takes approximately the same amount 
of time as rendering less complex ADF models. As in most rendering methods based on spatial decomposition, 
rendering time is determined more by screen coverage than by model complexity. Current rendering rates 
are fast enough for interactive updating of the carving region during editing. Preliminary tests indicate 
that an order of magnitude improvement in the rendering speed of the entire image can be achieved by 
adaptive supersampling. 4.2 Volume data ADFs are also amenable to volume rendering and can be used to 
produce interesting effects. For example, offset surfaces can be used to render thick, translucent surfaces. 
Adding volume texture within the thick surface in the form of variations in color or transparency is 
relatively easy. In addition, distance values farther away from the zero-valued iso-surface can be used 
for special effects. Figure 7 shows a cocaine molecule volume rendered in a haze of turbulent mist. The 
mist was generated using a color function based on distance from the molecule surface. To achieve the 
turbulence the distance value input to the color function is modulated by a noise function based on position 
[9]. We use a ray casting volume renderer to demonstrate some of these effects. Colors and opacities 
are accumulated at equally spaced samples along each ray using a back-to-front rendering algorithm. Sample 
points that lie near the zero-value iso-surface are shaded with Phong lighting. Our sampled ray caster 
is not optimized for speed. However, properties of the ADF data structure can be used to greatly increase 
the rendering rate. For example, the octree allows us to quickly skip regions of the volume that are 
far from the surface. In addition, because distances to the closest surface are available at each sample 
point, space-leaping methods can be used to speed up rendering [26].  4.3 Other application areas 4.3.1 
Representing complexity Complexity may be considered from several viewpoints. Firstly, the visual complexity 
of an object might include factors such as surface variation and topology. Secondly, the representation 
complexity is determined by the size and intricacy of the data structure needed to represent the object. 
The third measure of complexity considers the algebraic complexity of the object, which includes such 
factors as polynomial degree, transcendental functions, and numerical routines required to define the 
object s shape. Such routines are pertinent especially when algebraic distance is employed for the distance 
field. Figure 8 shows a good example of the first two types of complexity, the Menger Sponge, which is 
a fractal created recursively by subtracting smaller and smaller crossing cuboids from an initial cube. 
In the limit there is no neighborhood of the surface that is not punctured regardless of how small the 
neighborhood is chosen. It is an infinite perforation, a 3D version of the famous Cantor set. After each 
level of subtraction there are 20 self-similar subcuboids generated. An artless approach to maintaining 
the data structure would generate order O (20n) faces for n iterations. Even if shared faces were combined 
and interior faces culled, an approach that keeps a boundary representation (B-rep) without troublesome 
T-junctions would have O (12n) faces. To be more exact, after seven iterations there would be 26 million+ 
faces in a B-rep data structure. Consider the difficulty of performing proximity tests, collision detection, 
or inside/outside tests with such a representation. In contrast, these tests are much simpler using ADFs. 
Far from being a contrived case, the complexity of the distance field of the Menger Sponge is representative 
of the distance fields of many naturally occurring shapes which would present similar problems for traditional 
methods. Figures 2 and 7 both demonstrate ADFs ability to handle algebraic complexity. While Figure 7 
reconstructs an approximate probability density field for a molecule of 43 atoms (C17H21NO4), the vase 
in Figure 2 is defined first as a rotation of a quintic Bezier curve. Mathematically, it is posed as 
a rational implicit function with a square root of a (total) degree 16 over 2. Cubic Bezier curves are 
then mapped onto the surface as paths for the carving tool. In this case, the carver is a curved chisel, 
resulting in a very high degree tubular surface on the vase. This carving path and vase create an algebraically 
very complex distance field, which is nevertheless cleanly reconstructed and rendered. Figure 8. An 
ADF of the Menger Sponge, a fractal created recursively by subtracting smaller and smaller crossing cuboids 
from an initial cube. Four levels of recursion are shown.  4.3.2 Level-of-detail models There are at 
least two approaches for representing ADF models at different levels of detail for rendering and progressive 
transmission of models. The simplest approach is to truncate the ADF at fixed levels in the octree. The 
truncation can either be done during rendering or transmission, during generation, or to an existing 
high resolution ADF. A second method uses the error generated in the test for cell subdivision during 
top-down generation of the ADF. By storing this error within the cell, an LOD model can be generated 
by truncating ADF cells with errors less than that specified for the given LOD. This provides a more 
continuously varying parameter for selecting the LOD and provides degradation of the object shape that 
is consistent for both smooth and highly curved portions of the surface as the level of the LOD model 
decreases. This second method is illustrated in Figure 9 where four LOD models with varying amounts of 
error are rendered from an ADF octree. 4.3.3 Collision detection Distance fields have been used for 
collision avoidance in robotics and for detecting collisions and computing penetration forces in haptics 
[12, 13]. Octrees or other hierarchies of bounding boxes have also been used successfully to accelerate 
collision detection algorithms. The combination of these two representations in the ADF as well as the 
ability to represent offset surfaces and surfaces at different levels of detail suggest that ADFs have 
significant potential for applications that require collision detection. 4.3.4 Color gamut representation 
Devices such as color printers and monitors have unique color characteristics. Each can represent colors 
within their own particular color gamut, which is restricted, for example, by the types of dyes used 
by the printer. When an image is acquired or designed on one system and then displayed or printed on 
another, it is often important to match colors as closely as possible. This involves correcting colors 
that fall outside of the device s gamut and sometimes requires a complicated mapping to warp the gamut 
of one system onto that of another [23]. Most color devices represent their color gamuts in large look-up-tables 
(LUTs). Usually, a binary table is used to test colors against the device s gamut to see if they fall 
in or out of gamut. If a color falls out of gamut, a set of model coefficients and look-up tables are 
used to map the color onto the closest device color. Using ADFs to represent a device s gamut has several 
advantages over the LUT approach. First, out-of-gamut tests are easily performed with ADFs and edge-sampling 
errors that occur with the use of binary tables are avoided. Second, an ADF out-of-gamut test provides 
more information than is available with binary tables; the distance indicates how far out of gamut a 
color lies and the gradient indicates the direction to the nearest in-gamut color. Third, since ADFs 
use adaptive sampling, they should provide significant compression over LUT representations. Finally, 
since distance fields can be used to warp between shapes, ADFs may prove to be a useful representation 
for mapping between device gamuts. 4.3.5 Machining ADFs provide powerful tools for computer aided machining. 
The use of a distance function for representing surfaces allows the representation of the surface, the 
interior of the object, and the material that must be removed. Knowledge of the object interior can be 
used for part testing (e.g., part thickness tests [25]). A representation of the volume outside of the 
surface as well as distances to the closest surface can be used for planning tool paths and tool sizes 
for the machining process. Offset surfaces can be used to plan rough cutting for coarse-to-fine machining 
or for designing part molds for casting. The size of cells at the surface and the object normal near 
the surface can be used to select tool size and orientation. Finally, as illustrated in Figure 6, ADFs 
can represent fine surfaces and sharp corners efficiently, making it possible to represent machining 
precision in the ADF model.  5. CONCLUSIONS Although distance fields have been used in certain specific 
applications as mentioned above, the breadth and flexibility of their application to problems in computer 
graphics has not been appreciated, in part due to their large memory requirements. ADFs address this 
issue by adaptively sampling the distance field and storing sampled values in a spatial hierarchy. For 
2D shapes, we typically achieve better than 20:1 reductions over straightforward boundary (3-color) quadtrees. 
Nevertheless, ADFs maintain the reconstruction quality of the original distance field as seen in the 
examples presented; shapes, even those with high frequency components such as edges or corners, are reconstructed 
accurately. Distance fields can embody considerable information about a shape, not just the critical 
zero-valued iso-surface, but also information about the volume in which it sits, an indication of inside 
vs. outside, and gradient and proximity information. Operations on a shape can often be achieved by operations 
on its distance field. For example, Boolean set operations become simple max/min operations on the field; 
edges and corners can be rounded by low-pass filtering; and so forth. ADFs tend to separate generation 
of shapes into a preprocess step that may require complex and time-consuming methods, and a process for 
graphical operations that is fast and tolerant of various types of complexity. Indeed, fractals and mathematically 
sophisticated or carved shapes can be processed as quickly as much simpler shapes. The wide diversity 
of such manipulations include, for example, proximity testing (for collision detection, haptics, color 
gamut correction, milling), efficient ray-surface intersection for rendering, localized reconstruction, 
surface and volume texturing, blending, filleting, offset surfaces, and shape warping. 6. FUTURE WORK 
The introduction of ADFs opens up a wide range of future directions. Considerable research is left to 
investigate the possible transformations between shape and its distance field. Different hierarchical 
structures and reconstruction methods await testing and experience. For example, wavelets show particular 
promise [5], and Delaunay tetrahedralizations have been successfully used for multiresolution representation 
of volume data [6]. The relative compactness for very complex shapes has implications for level of detail 
management and progressive transmission. Efficient conversion between ADFs and standard (e.g. triangle 
and NURB) models is a valuable undertaking. Finally, we look forward to combining ADFs with more powerful 
rendering methods; for example, we envision hierarchical radiosity using form factors based on the ADF 
cells. 7. ACKNOWLEDGEMENTS We gratefully acknowledge the help of Mars Brimhall, John Ford, and Stephan 
Roth in generating some of the images in this paper. 8. REFERENCES [1] R. Avila and L. Sobierajski, 
A haptic interaction method for volume visualization , Proc. IEEE Visualization 96, pp. 197-204, 1996. 
[2] J. Baerentzen, Octree-based volume sculpting , Proc. Late Breaking Hot Topics, IEEE Visualization 
98, pp. 9-12, 1998. [3] J. Bloomenthal, Introduction to Implicit Surfaces, Morgan Kaufman Publishers, 
1997. [4] D. Breen, S. Mauch and R. Whitaker, 3D scan conversion of CSG models into distance volumes 
, Proc. 1998 IEEE Symposium on Volume Visualization, pp. 7-14, 1998. [5] M. Chow and M. Teichmann, A 
Wavelet-Based Multiresolution Polyhedral Object Representation , Visual Proc. SIGGRAPH 97, p. 175, 1997. 
[6] P. Cignoni, L. De Floriani, C. Montani, E. Puppo, R. Scopigno, Multiresolution Modeling and Rendering 
of Volume Data based on Simplicial Complexes , 1994 ACM Volume Visualization Conference Proceedings, 
1994, pp.19-26. [7] D. Cohen-Or, D. Levin, and A. Solomovici, Three-dimensional distance field metamorphosis 
, ACM Transactions on Graphics, 1997. [8] T. DeRose, M. Kass, T. Truong, Subdivision surfaces in character 
animation , Proc. SIGGRAPH 98, pp. 85-94, 1998. [9] D. Ebert, F.K. Musgrave, D. Peachy, K. Perlin, S. 
Worley, Texturing and Modeling a Procedural Approach, Academic Press, 1998. [10] J. Foley, A. van Dam, 
S. Feiner, and J. Hughes, Computer Graphics: Principles and Practice, Addison-Wesley, 1992. [11] S. Gibson, 
Using DistanceMaps for smooth surface representation in sampled volumes , Proc. 1998 IEEE Volume Visualization 
Symposium, pp. 23-30, 1998. [12] R. Kimmel, N. Kiryati and A. Bruckstein, Multi-valued distance maps 
for motion planning on surfaces with moving obstacles , IEEE Trans. on Robotics &#38; Automation, 14, 
pp. 427­436, 1998. [13] J. Lengyel, M. Reichert, B. Donald and D. Greenberg, Real-time robot motion planning 
using rasterizing computer graphics hardware , Proc. SIGGRAPH 90, pp. 327-335, 1990. [14] T. Nishita, 
T.W. Sederberg and M. Kakimoto, Ray tracing trimmed rational surface patches , Proc. SIGGRAPH 90, pp. 
337-345, 1990. [15] S. Osher and J. Sethian, Fronts propagating with curvature-dependent speed: algorithms 
based on Hamilton-Jacobi formulation , J. Computational Physics, 79, pp. 12-49, 1988. [16] S. Parker, 
M. Parker, Y. Livnat, P. Sloan, C. Hansen, and P. Shirley, Interactive ray tracing for volume visualization 
IEEE Transactions On Visualization and Computer Graphics, Vol. 5 (3), pp. 238-250, 1999. [17] B. Payne 
and A. Toga, Distance field manipulation of surface models , IEEE Computer Graphics and Applications, 
pp. 65-71, 1992. [18] A. Ricci, A constructive geometry for computer graphics , Computer Journal, Vol. 
16, No. 2, pp. 157-160, 1973. [19] H. Samet, The Design and Analysis of Spatial Data Structures, Addison-Wesley, 
1989. [20] W. Schroeder, W. Lorensen, and S. Linthicum, "Implicit modeling of swept surfaces and volumes," 
Proc. Visualization '94, pp. 40-45, 1994. [21] Sensable Devices FreeForm modeling software. http://www.sensable.com/freeform. 
[22] J. Sethian, Level Set Methods: Evolving Interfaces in Geometry, Fluid Mechanics, Computer Vision, 
and Material Science, Cambridge University Press, 1996. [23] M. Stone, W. Cowan, J. Beatty, Color gamut 
mappings and the printing of digital color images , ACM Transaction on Graphics, Vol. 7, pp. 249-292, 
1988. [24] R. Westermann, O. Sommer, T. Ertl, Decoupling polygon rendering from geometry using rasterization 
hardware , in Proc. Eurographics Rendering Workshop '99, pp. 45-56, 1999. [25] R. Yagel, S. Lu, A. Rubello, 
R. Miller, Volume-based reasoning and visualization of dicastability In Proc. IEEE Visualization 95, 
pp. 359-362, 1995. [26] K. Zuiderveld, A. Koning, and M. Viergever, Acceleration of ray-casting using 
3D distance transforms , in Proc. Visualization in Biomedical Computing 92, pp. 324-335, 1992.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344908</article_id>
		<sort_key>255</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>27</seq_no>
		<title><![CDATA[Patching Catmull-Clark meshes]]></title>
		<page_from>255</page_from>
		<page_to>258</page_to>
		<doi_number>10.1145/344779.344908</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344908</url>
		<abstract>
			<par><![CDATA[<p>Named after the title, the PCCM transformation is a simple, explicit algorithm that creates large, smoothly joining bicubic Nurbs patches from a refined Catmull-Clark subdivision mesh. The resulting patches are maximally large in the sense that one patch corresponds to one quadrilateral facet of the initial, coarsest quadrilateral mesh before subdivision. The patches join parametrically <italic>C</italic><supscrpt>2</supscrpt> and agree with the Catmull-Clark limit surface except in the immediate neighborhood of extraordinary mesh nodes; in such a neighborhood they join at least with tangent continuity and interpolate the limit of the extraordinary mesh node. The PCCM transformation integrates naturally with array-based implementations of subdivision surfaces.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[CAD]]></kw>
			<kw><![CDATA[curves &amp; surfaces]]></kw>
			<kw><![CDATA[geometric modeling]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Splines</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.1</cat_node>
				<descriptor>Smoothing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Graphics data structures and data types</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>J.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10003636</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Approximation algorithms analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010439</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010394</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics file formats</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003722</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Interpolation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003736.10003737</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Functional analysis->Approximation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003649.10003657.10003659</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic representations->Nonparametric representations->Spline models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39065430</person_id>
				<author_profile_id><![CDATA[81100524297]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[J&#246;rg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Peters]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>42459</ref_obj_id>
				<ref_obj_pid>42458</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[A. A. Ball and D. J. T. Storry. Conditions for tangent plane continuity over recursively generated B-spline surfaces. ACM Trans. on Graphics,7:83-102, July 1988.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[E. Catmull and J. Clark. Recursively generated B-spline sur-faces on arbitrary topological meshes. Computer Aided De-sign, 10:350-355, Oct 1978.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280826</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Tony DeRose, Michael Kass, and Tien Truong. Subdivision surfaces in character animation. Proceedings of SIGGRAPH 98, pages 85-94, July 1998.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[OpenGL Foundation. http://trant.sgi.com/opengl/docs/man pages/hardcopy/GL/html/glu/nurbssurface.html.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218475</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Cindy M. Grimm and John F. Hughes. Modeling surfaces of arbitrary topology using manifolds. Proceedings of SIG-GRAPH 95, pages 359-368, August 1995.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166121</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Mark Halstead, Michael Kass, and Tony DeRose. Efficient, fair interpolation using Catmull-Clark surfaces. Proceedings of SIGGRAPH 93, pages 35-44, August 1993.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311541</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Adi Levin. Interpolating nets of curves by smooth subdivision surfaces. Computer Graphics, 33:57-64, 1999. Appendix.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>124971</ref_obj_id>
				<ref_obj_pid>124966</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[A. H. Nasri. Boundary-corner control in recusive subdivision surfaces. CAD, 23(6):405-410, 1991.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[A. H. Nasri and J. Peters. Computing volumes of solids en-closed by recursive subdivision surfaces. Comp. Gr. Forum, 16(3), September 1997.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>207497</ref_obj_id>
				<ref_obj_pid>207475</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[J. Peters. C 1 -surface splines. SIAM J. Numer. Anal., 32(2):645-666, 1995.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>276485</ref_obj_id>
				<ref_obj_pid>276465</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[J. Peters and U. Reif. Analysis of generalized B-spline subdi-vision algorithms. SIAM J. on Numer. Anal., 35(2):728-748, April 1998.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>248304</ref_obj_id>
				<ref_obj_pid>248299</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[H. Prautzsch. Freeform splines. CAGD, 14(3):201-206, 1997.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280945</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Jos Stam. Exact evaluation of Catmull-Clark subdivision surfaces at arbitrary parameter values. Proceedings of SIG-GRAPH 98, pages 395-404, July 1998.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>927147</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[D. Zorin. Subdivision and multiresolution surface represen-tation. PhD thesis, Caltech, 1997.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[D. Zorin. Implementing subdivision and multiresolution meshes. Chapter 6 of Course notes 37 of SIGGRAPH 99, Aug 1999.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Patching Catmull-Clark Meshes J¨org Peters. University of Florida  Figure 1: Catmull-Clark subdivision 
sequence completed as a collection of large Nurbs patches (top .ve patches are shown). Abstract Named 
after the title, the PCCM transformation is a simple, ex­plicit algorithm that creates large, smoothly 
joining bicubic Nurbs patches from a re.ned Catmull-Clark subdivision mesh. The result­ing patches are 
maximally large in the sense that one patch corre­sponds to one quadrilateral facet of the initial, coarsest 
quadrilateral mesh before subdivision. The patches join parametrically C2and agree with the Catmull-Clark 
limit surface except in the immediate neighborhood of extraordinary mesh nodes; in such a neighborhood 
they join at least with tangent continuity and interpolate the limit of the extraordinary mesh node. 
The PCCM transformation inte­grates naturally with array-based implementations of subdivision surfaces. 
CR Categories: I.3.5 [surface representation, splines]: I.3.6 graphics data structures Keywords: CAD, 
Curves &#38; Surfaces, Geometric Modeling 1 Motivation Catmull-Clark subdivision meshes [2] are an increasingly 
popular surface representation that comes equipped with tools for adapting shape locally and globally 
[3, 6]. The limit surfaces can be directly evaluated [13] and an ample body of analysis assures tangent 
con­tinuity [1, 11, 14]. It would be nice though if we could stop the subdivision process at any point, 
apply a simple transformation and get a compact, explicit surface representation in the form of a small 
collection of maximally large, standard spline (Nurbs) patches that join just as smoothly and largely 
agree with the Catmull-Clark limit surface. The PCCM (Patching Catmull-Clark Meshes) algorithm is such 
a transformation. *NSF NYI CCR-9457806 Permission to make digital or hard copies of part or all of this 
work or personal or classroom use is granted without fee provided that copies are not made or distributed 
for profit or commercial advantage and that copies bear this notice and the full citation on the first 
page. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior 
specific permission and/or a fee. SIGGRAPH 2000, New Orleans, LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 
...$5.00 The paper has three parts: a review of basics and literature, the speci.cation of the algorithm 
and the discussion of the properties of its output: Each Nurbs patch covers a maximally large region 
of the mesh corresponding to the coarsest level quadrilateral mesh facets. The patches join C2almost 
everywhere and are at least tan­gent continuous near the extraordinary mesh nodes. Transi­tions between 
patches are almost all parametric. The Nurbs patches are polynomial, of order 4 (degree 3) and in interpolating 
form with 4-fold knots. The Nurbs patches differ from the limit surface of the Catmull-Clark subdivision 
only near the extraordinary mesh nodes. (The patches have generically .nite curvature whereas the generic 
curvature of the Catmull-Clark limit surface at ex­traordinary mesh nodes is in.nite.) Nurbs patches, 
Catmull-Clark subdivision and the PCCM al­gorithm can use the same array-based data structures.  2 Nurbs, 
Catmull-Clark and Prior Work A Nurbs patch Q(u,v)ER3of order 4 (bicubic tensor-product spline) is de.ned 
by two nondecreasing sequences of scalars (called knots)oflength k, one for the uand one for the vparameter 
and k2control points Q..ER3. If we connect control points whose indices differ by 1 in exactly one slot, 
we obtain a control net that outlines the patch. A concrete interface for Nurbs patches is the gluNurbsSurfacespeci.cation 
in OpenGL [4]. By de.nition, Nurbs do not change geometrically under knot insertion. (Repeated insertion 
of the same knot just transforms to B´ezier a.k.a. glMap form.) Inserting numbers into the knot sequence 
subdivides the pa­rameter domain into more pieces between knots. Correspondingly, the control net must 
be re.ned or [sic] subdivided by speci.c rules. For example, if we start with a uniform (evenly spaced) 
knot se­quence consisting of even integers and insert new knots at odd in­tegers we can classify the 
new control points as (a) facet points, F, corresponding to odd knots, (b) edge points, E, corresponding 
to one odd and one even knot, and (c) vertex points, V, correspond­ing to even knots. The weights for 
new F, Eand Vtype points as averages of the old points are displayed as averaging masks: 11 161 11 F 
16E 66 6V 6 36 6 (1) 11 11 161 For example, a new Fpoint is the average of four surrounding old points. 
A central contribution of [2] was the addition of a rule for extraordinary mesh nodes Vwith n= neighbors: 
 1 66 11 n2V A A= n2 -7n.(2) 6 6 6 1 1  The idea of converting from the subdivision mesh back to 
a spline representation for rendering dates back at least to DeRose, Kass and Truong [3], before the 
availability of the full Pixar sub­division pipeline. Compared to the large patches derived below, these 
patches are smaller and more numerous and the conversion is not well-de.ned in the neighborhood of extraordinary 
mesh nodes. Nasri and Peters [9] use an approximation to the limit surface of the Doo-Sabin subdivision 
to get a quickly convergent series of approximations to the volume of the enclosed subdivision object. 
The approximation surface, however, is only position continuous. Prautzsch [12] gives an elegant solution 
to the dif.cult problem of .lling n-sided holes. G2completion of a Catmull-Clark mesh re­quires order 
7 patches and at least nine times as many as the large patches derived below. Grimm and Hughes [5] use 
subdivision as a preprocessing step to generating smooth manifolds with which they associate an interesting 
class of rationally blended spline surfaces. Their approach would generate at least nine times as many 
rational spline patches either over nonstandard domains or of high degree. 3 From Mesh to Surface As 
Figure 1 illustrates, all mesh facets are four-sided either on input or after at most one subdivision 
step. We refer to each facet of this coarsest quad-mesh as the level 0 of a quad and think of eth step 
of Catmull-Clark subdivision as subdividing the level e-1of each £ quad into 4 times as many subfacets 
for a total of subfacets at level e. Indexing. Since the goal is to transform a repeatedly subdivided 
mesh we can treat each corner of a quad in isolation as shown in Figures 2 and 3. The quads surrounding 
the corner point are ar­ranged in counterclockwise order indexed by i. The nodes P..(i) of the ith quad 
lie on a ..grid of mesh lines. The double subscripts .. are the Greville abscissae, scaled by 3, of the 
output Nurbs sur­face. Poo(i)is the corner node for all iand Pzo(i1)= Poz(i). The same indexing is used 
for the output control points Q..(i).If a quad does not have a neighbor due to a manifold boundary, we 
use the same rule as the Catmull-Clark subdivision, say [8, 15], to provide an additional outside layer 
of mesh nodes. The PCCM(e) Algorithm:The input is a meshof a etimes subdivided quads with nodes P..(i)R3 
. (If all vertices have E4 or an odd number of neighbors then e>0suf.ces other­wise e>1should hold.) 
The output is one bicubic (order 4) Nurbs surface (patch) for every quad. The patches are in stan­dard 
interpolating form.That is, the .and the .knot sequence start and end with a 4-fold knot (at 0and at 
k=2£where eis the subdivision level). Each sequence can have up to two dou­ble knots (at 1and 2£-1) and 
has single knots otherwise, e.g. 0,0,0,0,1,1,2,3,,5,6,7,7,8,8,8,8. Systems that do not al­low internal 
double knots but do allow uneven knot spacing with minimal distance Ecan be accommodated by perturbing 
one inner Figure 2: Catmull-Clark mesh in the neighborhood of an extraor­dinary mesh node of degree 5. 
Quads are labeled in counterclock­wise order from 1to naround the extraordinary mesh node so that all 
indices iare interpreted (in-1)modn1. The dou­ble subscripts 00,30and 33in quad 1 belong to the extraordinary 
mesh node Poo(1), its direct neighbor P3o(1)and its diagonal facet neighbor P33(1). The double line delineates 
(one of the four cor­ners of) a submesh used to de.ne one quad level.  Figure 3: After Knot Insertion 
the Nurbs patches abut sharing the nodes Qzo(i1)= Qoz(i). The enlargement shows the indices of the control 
points relevant for Corner Smoothing. P P0 Figure 4: Knot insertion at .0; P-3is a mesh node borrowed 
= from the neighboring quad. double knot by E. PCCM surfaces of level epreserve the blend ra­tios (or 
smoothed creases) of the Catmull-Clark mesh [10, 3] up to level e. The surface can additionally be pinched 
(.attened) at the extraordinary mesh node by decreasing the multiplier aof Anto less than 1 (more than 
1). The control points Q..(i)of the Nurbs patches are derived from the input mesh points P..(i)in two 
steps. 1. [Knot Insertion] For each quad, we de.ne a submesh of the Catmull-Clark mesh that includes 
all subfacets of the quad and those shar­ing at least one node with the quad: we borrow one layer of 
nodes from all direct and diagonal neighbor quads to arrive at the submesh delineated by the double line 
in Fig. 2. We interpret the two perpendicular families of grid lines of quad i(without P3o(j)for jE{i-1,i,i1,i2})as 
the control net of an order 4 Nurbs patch with uniform uand vknot sequences -,-3,-2,-1,0,1,2,.... To 
bring this Nurbs patch into standard interpolating form, we insert three knots .rst at u0and u=kthen 
at v0and v=k. = = Figure 4 illustrates the (standard) knot insertion procedure at u=0for a grid line 
indexed by .. The new points Q.are obtained from the old points P.via Qo =(P-3 Po P3)l6,Q1 = (2Po P3)l3. 
 For every edge with at least one extraordinary mesh node we insert a second knot, at 1for u0(at k-1for 
u=k): = Q2 =(Po 2P3)l3,Q4 = (2P3 P6)l3. All remaining Q3i,3jP3i,3j, except for the corner point = Qoo(1)=Qoo(2)=...=Qoo(n)which 
we place directly on the Catmull-Clark limit surface [6]: . nPoo(i) P3o(i) P33(i)Qoo(1)=...=Qoo(n)= 
. n(n5) 2. [Corner Smoothing] If we were to stop at this point, the Nurbs patches would only meet with 
position continuity close to extraordinary mesh nodes although with C2continuity everywhere else. To 
obtain tangent continuity, we modify the control points near each extraordinary mesh node. The enlargement 
of Figure 3 shows the relevant double subscripts. We de.ne, for every n, two nby nmatrices Anand Bnwith 
rows i=1,...,nand columns j=1,...,nand entries 2a27An(i,j)os( (i-j)) a=1(default) and = nn . (-1)n;.j 
if nis odd,Bn(i,j)= (-1)j-2ni-j(-1)j-ilnif nis even, ni-j= mod(ni-j,n). For example, B6(3,3)-1. We collect 
the points Q..(i) = E R3generated by Knot Insertion for i=1,...,nand .. E {00,10,20,0}into Q ..ERnx3.Only 
if nis even and .n = greater than 4, do we compute ri.1(-1)iQ 4o(i)ln and if r=0we add, for each i, hi=-(-1)irto 
Q4o(i)=Qo4(i-1), Q41(i)and Q14(i-1)so that .n .1(-1)iQ4o(i)=0and Q4o(i)=(Q41(i) Q14(i­ i 1))l2. Otherwise 
all coef.cients remain unchanged except Q1o=Qoo AnQ 1o, Q2o = (Q4o 6 Q1o-2Qoo)l5, . os(27 l n ) . Q11BnQ1o 
(Q4oQ2o) =-. 6 For i=1,...,n,we copy Q.o(i1)Qo.(i)for .E = Q {1,2,}and add Q2o(i)-Q2o(i)to Q21(i)and 
Q12(i-1).  4 Smoothness and Approximation We claim: the output Nurbs patches are internally parametrically 
C2and join the neighbors parametrically C2except possibly at ex­traordinary mesh nodes, across edges 
u{0,1},vE[0,1]re- E spectively v{0,1},uE[0,1]shown in bold in Figure 2. Across E these edges the surface 
is at least tangent continuous. Before the Knot Insertion step, submeshes of adjacent patches overlap 
in three layers of cubic spline control points with a single knot associated with the center control 
point. Standard spline the­ory then guarantees that the patches join parametrically C2across the boundary 
edges u{0,k},vE[1,k-1]and v{0,k},uE EE[1,k-1]. Knot insertion does not change the Nurbs surface and the 
smoothness of the transition. Therefore each patch is also internally C2before the Corner Smoothing step. 
Corner Smoothing enforces tangent continuity for the remaining parameter interval at the corner from 
0to 1: with ui,vithe param­eters of patch iand vi=ui+1=tE[0,1]the parameter along 2n the boundary between 
patch qiand patch qi+1,and os(), = n Corner Smoothing enforces the polynomial equation 2(1-t)2qi(0,t)= 
qi(0,t) qi+1(t,0). vi ui vi+1 The equation is easily veri.ed by inserting two more knots at 1 and equating 
the four resulting B´ ezier coef.cients in the variables Q. For example, at a corner point (t0) with 
n=3neighbors = =-0.5and - qil t= qil ui qi+1l vi+1. This relation is enforced by applying An. The double 
root of (1-t)2at t1 = is equivalent to the collinearity of Qz1(i), Qz(i)and Q1z(i-1) o for l=2and l. 
The one remaining equation, for the mixed = derivatives at t=0, holds due to the choice of Bnand, if 
neven and greater than 4, the perturbation by hi. The matrix Bnis derived following [10] but has slightly 
different entries in the even case. E The continuity across the knot line ui=1and vi[0,1]is that of 
an order 4 spline with a double knot and therefore is at least C1.If r=0the joint movement of the nodes 
Q41(i), Q4o(i) and Q14(i-1)preserves collinearity and thereby C1-ness of the transition between adjacent 
Nurbs patches. Unless r=0, the Nurbs patches differ from the Catmull-Clark limit surface only near the 
corners for (u,v)(0,1]2The dif- E. ference increases with oscillation of the Catmull-Clark mesh at the 
extraordinary mesh node since application of Anand Bnaverage the data and the choice of Q2oremoves in.ections. 
Each step of subdivision halves the extent of the region. 5 Conclusion The strengths of the PCCM algorithm 
are that it (a) converts Catmull-Clark meshes to closed-form, smoothly-connected, stan­dard Nurbs patches, 
(b) does so with simple, explicit formulas, (c) integrates seamlessly with the array-based view of subdivision 
(see the Appendix) and (d) remains local so that almost all patch transi­tions across patch boundaries 
are parametrically C2 . Acknowledgement: I thank the referees, David Lutterkort, Mal­colm Sabin, Andy 
Shiue and Georg Umlauf at SurfLab for their constructive comments. References [1] A. A. Ball and D. J. 
T. Storry. Conditions for tangent plane continuity over recursively generated B-spline surfaces. ACM 
Trans. on Graphics,7:83 102, July 1988. [2] E. Catmull and J. Clark. Recursively generated B-spline sur­faces 
on arbitrary topological meshes. Computer Aided De­sign, 10:350 355, Oct 1978. [3] Tony DeRose, Michael 
Kass, and Tien Truong. Subdivision surfaces in character animation. Proceedings of SIGGRAPH 98, pages 
85 94, July 1998. [4] OpenGL Foundation. http://trant.sgi.com/opengl/docs/man pages/hardcopy/GL/html/glu/nurbssurface.html. 
 [5] Cindy M. Grimm and John F. Hughes. Modeling surfaces of arbitrary topology using manifolds. Proceedings 
of SIG-GRAPH 95, pages 359 368, August 1995. [6] Mark Halstead, Michael Kass, and Tony DeRose. Ef.cient, 
fair interpolation using Catmull-Clark surfaces. Proceedings of SIGGRAPH 93, pages 35 44, August 1993. 
[7] Adi Levin. Interpolating nets of curves by smooth subdivision surfaces. Computer Graphics, 33:57 
64, 1999. Appendix. [8] A. H. Nasri. Boundary-corner control in recusive subdivision surfaces. CAD, 23(6):405 
410, 1991. [9] A. H. Nasri and J. Peters. Computing volumes of solids en­closed by recursive subdivision 
surfaces. Comp. Gr. Forum, 16(3), September 1997. [10] J. Peters. C1-surface splines. SIAM J. Numer. 
Anal., 32(2):645 666, 1995. [11] J. Peters and U. Reif. Analysis of generalized B-spline subdi­vision 
algorithms. SIAM J. on Numer. Anal., 35(2):728 748, April 1998. [12] H. Prautzsch. Freeform splines. 
CAGD, 14(3):201 206, 1997. [13] Jos Stam. Exact evaluation of Catmull-Clark subdivision surfaces at arbitrary 
parameter values. Proceedings of SIG-GRAPH 98, pages 395 404, July 1998. [14] D. Zorin. Subdivision and 
multiresolution surface represen­tation. PhD thesis, Caltech, 1997. [15] D. Zorin. Implementing subdivision 
and multiresolution meshes. Chapter 6 of Course notes 37 of SIGGRAPH 99, Aug 1999. 00 11 0 1 2 quad_mipmap 
  Figure 5: Collection and distribution of points; (left)the Æ Catmull-Clark mipmap at levels e=0,1,2,(right) 
PCCM at level e=2. 6 Appendix: array-based data structures Catmull-Clark subdivision and PCCM can be 
implemented using only the connectivity information of the quads and a mipmap of control point arrays 
as would be input to gluNurbsSurface[4]. For each quad, store a mipmap of arrays. The array at level 
eis of size k2by k2by 3, k2£ 1and contains = the .,.,.node positions of subdivision e. Entry [1][1], 
short 11(and symmetrically 1k, k1and kk), holds the position of a corner node (cf. Figs. 2, 5,left). 
If the corner node is an extraordinary mesh node then 00is unde.ned. If n=3then the entries in 01and 
10agree. PCCM outputs into an array of size kby kby 3. Entry 00 is the position of a corner coef.cient. 
 For each extraordinary mesh node, a connectivity list stores for each incident quad the global index 
and the corner of the quad corresponding to the extraordinary mesh node. Catmull-Clark subdivision with 
this data structure consists of two parallelizable steps: a. For each quad, create the mipmap level e1from 
level eby the regular subdivision rules (1). b. For each extraordinary mesh node, use the connectivity 
list to  collect at level e, Poo Poo(1)and P3o(i), P33(i)for all i = (c.f. Figures 2, 5left:if Poois 
entry 11then P3o(i)and P33(i) are entry 21and 22). Compute new locations for P3o(i)and P33(i)at level 
e1 from rules (1) and Poofrom rule (2) (or use Sabin scurvature bounded rule [7], Appendix). P Distribute 
the new locations to level e1(Entry 01receives 3o(i2)and entry 10receives P3o(i-1))). PCCM with this 
structure consists of two parallelizable steps: a. For each quad, apply Knot Insertion. b. For each 
extraordinary mesh node, use the connectivity list to collect Qoo(1)and Q{10,20,0}.  Q..(i), ..E Q Compute 
Q..(i), ..E{10,20,11}, Q2o-Q2oand possi- Q bly Q4o-Q4o. Distribute Q..(i), ..E{10,01,20,02,11}and add 
to {21,12}and possibly {0,0,1,1}. All space for subdivision level ecan be allocated at the outset, and 
the connectivity list remains unchanged throughout. The quad­arrays can be input directly to gluNurbsSurface 
or displayed as quad-meshes.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344912</article_id>
		<sort_key>259</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>28</seq_no>
		<title><![CDATA[Out-of-core simplification of large polygonal models]]></title>
		<page_from>259</page_from>
		<page_to>262</page_to>
		<doi_number>10.1145/344779.344912</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344912</url>
		<abstract>
			<par><![CDATA[<p>We present an algorithm for <italic>out-of-core simplification</italic> of large polygonal datasets that are too complex to fit in main memory. The algorithm extends the vertex clustering scheme of Rossignac and Borrel [13] by using error quadric information for the placement of each cluster's representative vertex, which better preserves fine details and results in a low mean geometric error. The use of quadrics instead of the vertex grading approach in [13] has the additional benefits of requiring less disk space and only a single pass over the model rather than two. The resulting linear time algorithm allows simplification of datasets of arbitrary complexity.</p><p>In order to handle degenerate quadrics associated with (near) flat regions and regions with zero Gaussian curvature, we present a robust method for solving the corresponding underconstrained least-squares problem. The algorithm is able to detect these degeneracies and handle them gracefully. Key features of the simplification method include a bounded Hausdorff error, low mean geometric error, high simplification speed (up to 100,000 triangles/second reduction), output (but not input) sensitive memory requirements, no disk space overhead, and a running time that is independent of the order in which vertices and triangles occur in the mesh.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>G.1.3</cat_node>
				<descriptor>Matrix inversion</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010148.10010149.10010158</concept_id>
				<concept_desc>CCS->Computing methodologies->Symbolic and algebraic manipulation->Symbolic and algebraic algorithms->Linear algebra algorithms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P224643</person_id>
				<author_profile_id><![CDATA[81100040340]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lindstrom]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ACKERMAN, M. J. The Visible Human Project. In Proceedings of the IEEE, 86(3), March 1998, pp. 504-511. Project URL: http://ww~nlm.nih.gov/ research/visible.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BERNARDINI, F., MITTLEMAN, J., and RUSHMEIER, H. Case Study: Scanning Michelangelo's Florentine Pieth. In ACM SIGGRAPH 99 Course Notes, Course 8, August 1999. Project URL: http://www.research.ibm, com/pieta.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>614442</ref_obj_id>
				<ref_obj_pid>614276</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BERNARDINI, F., MITTLEMAN, J., RUSHMEIER, H., SILVA, C., and TAUBIN, G. The Ball-Pivoting Algorithm for Surface Reconstruction. In IEEE Transactions on Visualization and Computer Graphics, 5(4), October-December 1999, pp. 349-359.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>288241</ref_obj_id>
				<ref_obj_pid>288216</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[CHIANG, Y.-J., SILVA, C. T., and SCHROEDER, W. J. Interactive Out-of-Core Isosurface Extraction. In IEEE Visualization '98 Proceedings, October 1998, pp. 167-174.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258849</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[GARLAND, M. and HECKBERT, P. S. Surface Simplification using Quadric Error Metrics. Proceedings of SIGGRAPH 97. In Computer Graphics Proceedings, Annual Conference Series, 1997, ACM SIGGRAPH, pp. 209-216.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>288221</ref_obj_id>
				<ref_obj_pid>288216</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[HOPPE, H. Smooth View-Dependent Level-of-Detail Control and its Application to Terrain Rendering. In IEEE Visualization '98 Proceedings, October 1998, pp. 35-42.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1889714</ref_obj_id>
				<ref_obj_pid>1889712</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[LEVOY, M. The Digital Michelangelo Project. In proceedings of the Second international Conference on 3D Digital imaging and Modeling, October 1999, pp. 2-11. Project URL: http://graphics.stanford, edu/ projects~ mich.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>288288</ref_obj_id>
				<ref_obj_pid>288216</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[LINDSTROM, P. and TURK, G. Fast and Memory Efficient Polygonal Simplification. In IEEE Visualization '98 Proceedings, October 1998, pp. 279-286.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>253310</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[LOW, K.-L. and TAN, T.-S. Model Simplification using Vertex-Clustering. In Proceedings of}997 Symposium on interactive 3D Graphics, April 1997, pp. 75-82.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258791</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[PHARR, M., KOLB, C., GERSHBEIN, R., and HANRAHAN, P. Rendering Complex Scenes with Memory-Coherent Ray Tracing. Proceedings of SIG- GRAPH 97. In Computer Graphics Proceedings, Annual Conference Series, 1997, ACM SIGGRAPH, pp. 101-108.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>148286</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[PRESS, W. H., TEUKOLSKY, S. A., VETTERLING, W. T., and FLANNERY, B. P. Numerical Recipes in C: The Art of Scientific Computing, Second Edition. Cambridge University Press, 1992, pp. 408-412.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[RONFARD, R. and ROSSIGNAC, J. Full-Range Approximation of Triangulated Polyhedra. Proceedings of Eurographics 96. In Computer Graphics Forum, 15(3), August 1996, pp. 67-76.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[ROSSIGNAC, J. and BORREL, P. Multi-Resolution 3D Approximations for Rendering Complex Scenes. In Modeling in Computer Graphics, edited by B. Falcidieno and T. L. Kunii, Springer-Verlag, 1993, pp. 455-465.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Out-of-Core Simpli.cation of Large Polygonal Models Peter Lindstrom Georgia Institute of Technology 
 Abstract We present an algorithm for out-of-core simpli.cation of large polygonal datasets that are 
too complex to .t in main memory. The algorithm extends the vertex clustering scheme of Rossignac and 
Borrel [13] by using error quadric information for the placement of each cluster s representative vertex, 
which better preserves .ne de­tails and results in a low mean geometric error. The use of quadrics instead 
of the vertex grading approach in [13] has the additional bene.ts of requiring less disk space and only 
a single pass over the model rather than two. The resulting linear time algorithm allows simpli.cation 
of datasets of arbitrary complexity. In order to handle degenerate quadrics associated with (near) .at 
regions and regions with zero Gaussian curvature, we present a ro­bust method for solving the corresponding 
underconstrained least­squares problem. The algorithm is able to detect these degenera­cies and handle 
them gracefully. Key features of the simpli.cation method include a bounded Hausdorff error, low mean 
geometric error, high simpli.cation speed (up to 100,000 triangles/second re­duction), output (but not 
input) sensitive memory requirements, no disk space overhead, and a running time that is independent 
of the order in which vertices and triangles occur in the mesh. 1 INTRODUCTION Polygonal simpli.cation 
has been a hot topic of research over the last decade, with a vast number of published algorithms. Many 
of the early simpli.cation algorithms were designed to handle modest size datasets of a few tens of thousands 
of triangles. As is common in most areas of computing, improvements in processor speed and memory capacity 
have served merely to promote the production of increasingly larger datasets, and a number of methods, 
particularly for out-of-core visualization, have been proposed for coping with models that are too large 
to .t in main memory, e.g. [3, 4, 10]. Following this trend, some of the more recent simpli.cation algo­rithms 
have been designed to be memory ef.cient, and typically handle models with as many as several million 
triangles. In the last few years, however, there has been an explosion in model size, in part due to 
improvements in resolution and accuracy of data acqui­sition devices, such as laser range and CT/MRI 
scanners. Indeed, submillimeter resolution datasets such as the Visible Human [1], which consists of 
well over 10 billion voxels, and the range scans of Michelangelo s sculptures made independently by research 
groups at IBM [2] and Stanford University [7] contain up to two billion triangles. These enormous datasets 
pose great challenges not only for mesh processing tools such as rendering, editing, compression, and 
surface analysis, but paradoxically also for simpli.cation meth­ods that seek to alleviate these problems. 
In addition to their large e-mail: lindstro@cc.gatech.edu web: http:// www.cc.gatech.edu/ lindstro Permission 
to make digital or hard copies of part or all of this work or personal or classroom use is granted without 
fee provided that copies are not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on 
servers, or to redistribute to lists, requires prior specific permission and/or a fee. SIGGRAPH 2000, 
New Orleans, LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 memory consumption, these algorithms 
also suffer from insuf.­cient simpli.cation speed to be practically useful for simplifying very large 
meshes. As an example, the memoryless simpli.cation scheme proposed by Lindstrom and Turk [8] one of 
the fastest and most memory ef.cient algorithms available requires a minimum of 160n bytes of internal 
storage to represent an n-vertex model and the necessary edge collapse priority queue. Simplifying a 
one billion vertex model to a few million triangles using their algorithm would require 160 gigabytes 
of RAM and, disregarding memory thrashing, would take weeks to complete on a high end worksta­tion! One 
might argue that high resolution datasets such as the ones described above are greatly oversampled, and 
that this problem should be solved more directly during the data acquisition or syn­thesis stage, e.g. 
by using adaptive sampling and tessellation during range scanning and isosurface extraction. At best, 
this simply shifts the problem to an earlier stage of the modeling pipeline, and results not only in 
a need for specialized tools for each acquisition method, but often raises a number of practical issues. 
In particular, it places an additional burden on the data acquisitor in terms of deciding how to sample 
the model and dealing with the dif.cult issues of regis­tering and integrating different resolution surface 
patches. In some cases, such a head-on approach is not even practical; one might not know in advance 
what parts of a surface should be sampled densely, or one might simply wish to retain the model at its 
full resolution and allow the end-user to resample the model in a manner that suits the given application. 
Currently, few algorithms exist for performing high quality out­of-core simpli.cation. One reason for 
this is that existing in-core methods are dif.cult to adapt to perform out-of-core simpli.cation, because 
the majority of them are based on performing simple local operations that rely on having direct access 
to the connectivity of the mesh. For example, the quality measures associated with the vertex removal 
and edge collapse operations typically depend on the triangles surrounding the vertex or edge. Consequently, 
such methods use large in-core data structures to allow ef.cient queries of the local connectivity for 
any given mesh vertex. As mentioned above, such data structures may require hundreds of bytes per ver­tex, 
which might even be too large to off-load to disk. Instead, de­velopers of out-of-core simpli.cation 
algorithms are faced with two alternatives: segmenting the model into multiple pieces and simpli­fying 
them individually, or simplifying models using only limited connectivity information, which is the approach 
taken in this paper. We propose an ef.cient and easy to implement surface simpli­.cation algorithm that 
accepts models of arbitrary complexity, and outputs a model that is small enough for in-core mesh processing 
tools to handle and store internally. The algorithm is based on uni­form sampling via vertex clustering, 
and is enhanced by a novel use of error quadrics, which were originally developed for edge collapse methods 
(cf. [5, 8]). To our knowledge, our algorithm is one of very few, if not the only one, for doing fast, 
high quality simpli.cation of arbitrarily large models.  2 PREVIOUS WORK Polygonal models have grown 
rapidly in complexity over recent years, yet surprisingly little work has been done on out-of-core sim­pli.cation. 
Because most conventional simpli.cation algorithms are not relevant in the context of out-of-core simpli.cation, 
we will restrict our discussion of related work to methods that are directly related to our algorithm, 
as well as the few methods that either perform out-of-core simpli.cation or can be adapted for that pur­pose. 
For a comparison of several well-known in-core simpli.ca­tion methods, we refer the reader to [8]. Rossignac 
and Borrel proposed one of the earliest simpli.cation algorithms [13]. Their algorithm divides the model 
into cells from a uniform rectilinear grid, and replaces all vertices in a grid cell by a single representative 
vertex. When clustering vertices together, the majority of triangles degenerate into edges or points 
and can be discarded, thereby reducing the complexity of the model. Repre­sentative vertices are computed 
by .rst estimating the impact each vertex has on the visual appearance of the model using a number of 
ad hoc heuristics. This vertex grading is then used either to compute a weighted average or to select 
the most important of the original vertices in each cluster as the representative. As alluded to, but 
not explicitly stated, their algorithm can easily be adapted to work as an out-of-core method. The appeal 
of this method lies in its simplicity and speed, although the low quality models it produces, due in 
part to existing vertex positioning schemes, has lead to the use of more sophisticated simpli.cation 
methods. Recognizing that Rossignac and Borrel s method is sensitive to translation of the underlying 
grid, Low and Tan devised a method that uses .oating cells constructed by sorting the vertices on their 
importance, and then iteratively letting the most important vertex be the center of a new cluster that 
absorbs all vertices within an arbitrarily shaped cell volume [9]. While providing higher quality results, 
one drawback of this approach is that it requires sorting the vertices, which is generally an O(n log 
n) procedure, compared to the O(n) running time for Rossignac and Borrel s original scheme. The edge 
collapse operator has been used extensively in simpli­.cation, and is generally considered to produce 
the highest qual­ity results. Ronfard and Rossignac use edge collapse to coarsen a model while maintaining 
a list of supporting planes with each vertex [12]. Initially, each vertex is assigned the planes associated 
with its incident triangles. As two vertices are merged into one by an edge collapse, the new vertex 
inherits the planes of the merged vertices, and the maximum distance from the new vertex to its sup­porting 
planes is used to measure the cost of collapsing the edge. The edges of the model are ordered by increasing 
cost in a prior­ity queue, and a greedy selection strategy is employed in which the cheapest edge is 
always collapsed. Inspired by this technique, Gar­land and Heckbert proposed using quadrics a succinct 
encoding of the local surface geometry as a 4 × 4 symmetric matrix that al­low an ef.cient computation 
of the sum of squared distances from a vertex to its supporting planes [5]. Lindstrom and Turk [8] later 
showed that recomputing the quadrics from scratch in each iteration from the partially simpli.ed surface, 
and weighting each quadric by the squared triangle area (thus measuring squared displacements in volume) 
improve the model quality. We use these area-weighted quadrics in our out-of-core simpli.cation algorithm. 
Bernardini et al. describe an algorithm that has been speci.cally designed to perform out-of-core simpli.cation 
[2]. Their method splits the model up into separate patches that are small enough to be simpli.ed separately 
in-core using a conventional simpli.cation algorithm. The patch boundaries are left intact to allow the 
different pieces to be stitched together without cracks after simpli.cation. A new set of patch boundaries 
is then used as another iteration of sim­pli.cation is performed, allowing the seams between the previous 
set of patches to be coarsened. A similar technique was proposed by Hoppe for creating hierarchical levels 
of detail for height .elds [6]. While conceptually simple, the time and space overhead of parti­tioning 
the model and later stitching it together adds to an already expensive in-core simpli.cation process, 
rendering such a method less suitable for simplifying very large meshes. 3 SIMPLIFICATION ALGORITHM 
The simpli.cation algorithm presented here is a hybrid of sev­eral schemes, including [5, 8, 13]. At 
a high level, it resembles Rossignac and Borrel s vertex clustering algorithm, but is improved both in 
execution time and quality by using the quadric error metric introduced by Garland and Heckbert, and 
later improved by Lind­strom and Turk, for positioning vertices. In particular, our linear time algorithm 
improves upon [13] by requiring only a single pass over the input model, compared to two or more, and 
does not use any disk space beyond the input mesh, whereas their algorithm re­quires an importance value 
to be stored with each vertex of the in­put model. In describing our algorithm, we will focus on its 
novel aspects and assume that the reader is familiar with vertex cluster­ing and quadrics for simpli.cation. 
We will .rst describe how the quadrics and representative vertices are computed, and follow with a description 
of the actual simpli.cation algorithm. 3.1 Quadrics In order to integrate quadrics with the general vertex 
clustering scheme, we .rst make the observation that vertex clustering is a special case of vertex pair 
contraction a generalization of edge collapse to arbitrary pairs of vertices [5]. That is, merging n 
ver­tices within a cluster cell is equivalent to performing any sequence of n - 1 contractions of pairs 
of vertices within the cluster until a single vertex remains. As a consequence, we can extend Garland 
and Heckbert s original scheme from individual vertex contractions to a prede.ned sequence of such operations. 
In fact, our algorithm is equivalent to theirs, with the exception that our priority queue is determined 
by the cluster grid rather than by the local geometry. We use the quadrics from [8], which have proven 
to give better results in the mean error sense. Based on [8], we compute for each triangle t =(x1, x2, 
x3) its associated quadric matrix Q as follows: .. A -b T Q == nn (1) -bT c .. x1 × x2 + x2 × x3 + x3 
× x1 n = (2) -[x1, x2, x3] where n is a 4-vector made up of the area-weighted triangle normal and the 
scalar triple product of its three vertices. We then distribute Q to the clusters associated with each 
of t s three vertices by adding Q to their quadric matrices. Since Q is symmetric, and since the scalar 
c is not used, only 9 scalar values need to be stored with each cluster. After adding up the quadrics 
of all the triangles in a cluster, we use the block decomposition of Q above and solve the linear system 
Ax = b for the optimal representative vertex position x. That is, x is the position that minimizes the 
sum of squared volumes of the tetrahedra formed by x and the triangles in the cell. If a cell contains 
two nearly parallel surface sheets, the quadrics will sometimes suggest a solution x that is close to 
the intersection of the extension of these two surfaces. The solution may in such cases lie far outside 
the cell itself. We handle these degeneracies by restricting the position of x, either by independently 
clamping its three coordinates to the cell bounds, or by pulling the vertex towards the cell center until 
it is suf.ciently close. 3.1.1 Robust Inversion of Quadric Matrices In the discussion above, we assumed 
that the matrix A is invertible and well-conditioned. In practice, this is often not the case, e.g. if 
the surface is locally .at or has zero Gaussian curvature. Lindstrom and Turk [8] proposed a partial 
solution to this problem by ensuring that the problem is overconstrained, and then combining linear con­straints 
that yield a suf.ciently large value for the determinant of A. In our case, however, the quadrics yield 
at most three constraints, and we use a slightly different approach that is able to both diag­nose potential 
problems and also robustly produce the best vertex in the sense that x is chosen such that its distance 
to the cell center is minimized. That is, x is the orthogonal projection of the cell center onto the 
space of all solutions to Ax = b. We accomplish this by performing a singular value decomposition A = 
U.VT,which for a real symmetric positive semide.nite matrix A is equivalent to doing an eigenvalue decomposition. 
This can be done quickly using a small number of Jacobi rotations [11]. For robustness, we set a lower 
limit on the singular values and discard (zero) the ones that are negligible: 1/Oi if Oi/O1 >E Oi + 
=(3) 0 otherwise where O1 is the largest singular value and E is a threshold parameter currently set 
to 10-3.The vertex x closest to the cell center x that satis.es Ax = b is then x = x + V.+UT(b - Ax ) 
(4) which simpli.es to A-1b whenever .+ = .-1, i.e. the above equation is used whether A is ill-conditioned 
or not, and always yields a numerically robust solution.  3.2 Vertex Clustering For performance reasons, 
it is important that the external mesh rep­resentation is conducive to the types of mesh queries needed 
for the given simpli.cation operator. Fortunately, the combination of vertex clustering and quadrics 
allows commonly used off-line data structures to represent the mesh, such as an indexed mesh in which 
each triangle is a triplet of indices associated with an ordered list of vertex coordinates. By storing 
the mesh in binary form as .xed­length records, the vertices of a triangle can be fetched from disk indirectly 
via random access. While such a format is compact, our algorithm requires no connectivity information, 
and is thus able to operate on a triangle soup in which each triangle is represented directly as a triplet 
of vertex coordinates. The triangle soup repre­sentation requires roughly twice as much disk space as 
the indexed mesh, but typically increases the simpli.cation speed by a factor of 15 20, while also accommodating 
text .le representations. In addition, since our algorithm makes a single pass over the mesh triangles, 
the triangle soup can be compressed externally and then uncompressed on-the-.y during simpli.cation. 
The model can even be split up into several .les if, for example, it is too large to store on a single 
disk. We used the triangle soup representation for the results presented in this paper. Similar to Rossignac 
and Borrel s original clustering algorithm, our algorithm also requires a bound­ing box for the model, 
which is divided into a user-speci.ed num­ber of rectilinear grid cells. We anticipate that most data 
acquisition methods are able to provide such bounds and store the mesh in ei­ther of these two formats. 
Once the cluster grid has been determined, we proceed by read­ing the mesh one triangle at a time and 
incrementally construct an in-core representation of the simpli.ed mesh. It is generally fair to assume 
that enough memory exists for this simpli.ed mesh since our goal is to produce a mesh coarse enough for 
in-core tools to process it. Given a triangle t E Tin from the original mesh, we fetch its vertex coordinates. 
For each vertex vin of t, we construct a hash key from the grid cell that the vertex falls in and do 
a hash table lookup. This dynamic hash table maps grid cells, or clusters, to the vertices Vout in the 
simpli.ed mesh. If this cell has not been visited, a new vertex identi.er vout is created (e.g. using 
consecu­tive integers) and the quadric matrix associated with vout is initial­ized to zero. If two or 
more of the triangle s vertices belong to the same cluster, then t reduces to an edge or a point, and 
is discarded. Otherwise, we add it, as a triplet of indices into Vout,to the setof simpli.ed triangles 
Tout. Before proceeding with the next triangle, we compute the quadric matrix Q associated with t. For 
each vertex of t,we add Q to the matrix of the cluster that the vertex belongs to. After the input has 
been exhausted, we are left with a list of quadrics and a list of triangles. Each quadric corresponds 
to a cluster of vertices and triangles that share a grid cell, and from the quadric matrix we compute 
the coordinates for the cluster s representative vertex vout using the procedure described above. The 
simpli.cation then ends by outputting the simpli.ed mesh (Vout,Tout) in an appropriate format. RAM (MB) 
time (h:m:s) model |Tin ||Tout| [5] [8] OoCS [5] [8] OoCS dragon 871,306 244,562 213 134 28 5:31 11:59 
0:16 dragon 871,306 113,090 214 134 11 5:55 14:12 0:12 dragon 871,306 47,228 214 134 7 6:06 15:21 0:10 
buddha 1,087,716 204,750 250 166 26 7:13 16:58 0:17 buddha 1,087,716 62,354 251 166 8 7:35 19:19 0:12 
blade 28,246,208 507,104 -3,185 63 -12:37:25 5:02 statue 386,488,573 3,122,226 --366 --1:59:20 Table 
1: Simpli.cation results of running QSlim [5], Memoryless Simpli.cation [8], and the out-of-core method 
(OoCS). All results were gathered on a 195 MHz R10000 SGI Origin with 4 GB of RAM and a standard SCSI 
disk drive.  4 RESULTS AND DISCUSSION To evaluate the performance of our algorithm, we include results 
of simplifying four large polygonal datasets: a buddha, a dragon, and a model of Michelangelo s St. Matthew 
statue created by researchers at Stanford using a range scanner, as well as a turbine blade model which 
was extracted from volume data as an isosurface. We applied two levels of Loop subdivision to the blade 
model to increase its triangle count by a factor of 16, thus making it more challenging to simplify. 
Table 1 includes the triangles counts, memory usage, and timing results of simplifying these models using 
our method as well as the in-core methods presented in [5, 8]. While being much more memory ef.cient 
than these two methods, our new algorithm is also orders of magnitude faster. Note that the reported 
memory usage is consistently higher than our implementation s theoretical usage of 63 to 72 |Tout| bytes,1 
as the former includes freed memory not reclaimed by the operating system. Figures 1a c show the original 
buddha model and two out-of­core simpli.ed models. Notice how the models in 1a and 1b are virtually indistinguishable, 
while some blocking artifacts appear in 1c, yet most details are still present. Figures 2b d show several 
simpli.cations of the dragon model. We here compare our vertex positioning scheme based on quadrics against 
1) using the mean of a cluster s vertices and 2) the vertex grading scheme of Rossignac and Borrel that 
chooses the most important vertex, and which has been improved using the technique in [9]. Fine details 
near the jaws, neck, and hind leg are washed out by the vertex averaging scheme, and the ridge along 
the back has lost its sharpness. The model pro­duced by vertex grading has a more choppy appearance with 
loss of detail in the face. Finally, Figures 3a and 3b show close-ups of the face of the St. Matthew 
statue covering less than 15% of its overall height. This complex model consists of nearly 400 million 
trian­gles, and could only be simpli.ed using our out-of-core method. Even after a reduction by a factor 
of 100, many .ne details such as the chisel marks are still preserved. While the quality of our method 
is high in comparison with other vertex clustering schemes, it does not perform adaptive sampling of 
the model, and often produces models that can be further coarsened in areas of low curvature with little 
loss in quality. For applications that require extreme reduction and very high visual quality, our al­gorithm 
can be used as a fast preprocessing step that produces a model with a few hundred thousand triangles, 
which can then be further simpli.ed by a slower in-core simpli.cation algorithm. We envision several 
avenues for future research. As suggested in [13], adaptive sampling can be handled using hierarchical 
sim­pli.cation, in which cells are recursively merged in less detailed regions. The idea is to allow 
quadrics to be merged wherever they agree on the local surface characterization. We also believe that 
the quadric information can be used to improve the connectivity of the mesh, for example by swapping 
edges in a manner that would reduce the associated quadratic function. Finally, it would be pos­sible 
to directly integrate our algorithm with the popular marching cubes algorithm for isosurface extraction, 
thereby combining iso­surfacing and simpli.cation into a single step, and eliminating the need to output 
an overly complex intermediate isosurface. 1The theoretical memory usage varies with the size and load 
of the dy­namic hash table.  Acknowledgements I would like to thank Marc Levoy and the people working 
on the Digital Michelangelo Project for providing the St. Matthew dataset, and Greg Turk, Jarek Rossignac, 
F. S. Nooruddin, and Gabriel Taubin for valuable comments and suggestions.   References [1] ACKERMAN, 
M. J. The Visible Human Project. In Proceedings of the IEEE, 86(3), March 1998, pp. 504 511. Project 
URL: http:// www.nlm.nih.gov/ research/ visible. [2] BERNARDINI,F., MITTLEMAN,J., and RUSHMEIER, H. Case 
Study: Scan­ning Michelangelo s Florentine Piet`a. In ACM SIGGRAPH 99 Course Notes, Course 8, August 
1999. Project URL: http:// www.research.ibm.com/ pieta. [3] BERNARDINI,F., MITTLEMAN,J., RUSHMEIER,H., 
SILVA,C., and TAUBIN, G. The Ball-Pivoting Algorithm for Surface Reconstruction. In IEEE Transac­tions 
on Visualization and Computer Graphics, 5(4), October December 1999, pp. 349 359. [4] CHIANG,Y.-J., SILVA,C. 
T., and SCHROEDER, W. J. Interactive Out-of-Core Isosurface Extraction. In IEEE Visualization 98 Proceedings, 
October 1998, pp. 167 174. [5] GARLAND,M. and HECKBERT, P. S. Surface Simpli.cation using Quadric Error 
Metrics. Proceedings of SIGGRAPH 97. In Computer Graphics Proceed­ings, Annual Conference Series, 1997, 
ACM SIGGRAPH, pp. 209 216. [6] HOPPE, H. Smooth View-Dependent Level-of-Detail Control and its Appli­cation 
to Terrain Rendering. In IEEE Visualization 98 Proceedings, October 1998, pp. 35 42. [7] LEVOY, M. The 
Digital Michelangelo Project. In proceedings of the Second International Conference on 3D Digital Imaging 
and Modeling, October 1999, pp. 2 11. Project URL: http:// graphics.stanford.edu/ projects/ mich. [8] 
LINDSTROM,P. and TURK, G. Fast and Memory Ef.cient Polygonal Simpli.­cation. In IEEE Visualization 98 
Proceedings, October 1998, pp. 279 286. [9] LOW,K.-L. and TAN, T.-S. Model Simpli.cation using Vertex-Clustering. 
In Proceedings of 1997 Symposium on Interactive 3D Graphics, April 1997, pp. 75 82. [10] PHARR,M., KOLB,C., 
GERSHBEIN,R., and HANRAHAN, P. Rendering Complex Scenes with Memory-Coherent Ray Tracing. Proceedings 
of SIG-GRAPH 97. In Computer Graphics Proceedings, Annual Conference Series, 1997, ACM SIGGRAPH, pp. 
101 108. [11] PRESS,W. H., TEUKOLSKY,S. A., VETTERLING,W. T., and FLANNERY, B. P. Numerical Recipes in 
C: The Art of Scienti.c Computing, Second Edi­tion. Cambridge University Press, 1992, pp. 408 412. [12] 
RONFARD,R. and ROSSIGNAC, J. Full-Range Approximation of Triangulated Polyhedra. Proceedings of Eurographics 
96. In Computer Graphics Forum, 15(3), August 1996, pp. 67 76. [13] ROSSIGNAC,J. and BORREL, P. Multi-Resolution 
3D Approximations for Rendering Complex Scenes. In Modeling in Computer Graphics, edited by 1b. OoCS 
1c. OoCS B. Falcidieno and T. L. Kunii, Springer-Verlag, 1993, pp. 455 465. 1a. Original buddha. 1,087,716 
triangles. 204,750 triangles. 62,354 triangles. 2a. Original dragon. 871,306 triangles. 2b. OoCS/Quadrics. 
47,228 triangles. 3a. Original statue. 386,488,573 triangles. 2c. OoCS/Vertex mean. 47,228 triangles. 
2d. OoCS/Vertex grading. 47,228 triangles. 3b. OoCS. 3,122,226 triangles. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344919</article_id>
		<sort_key>263</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>29</seq_no>
		<title><![CDATA[Face fixer]]></title>
		<subtitle><![CDATA[compressing polygon meshes with properties]]></subtitle>
		<page_from>263</page_from>
		<page_to>270</page_to>
		<doi_number>10.1145/344779.344919</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344919</url>
		<abstract>
			<par><![CDATA[<p>Most schemes to compress the topology of a surface mesh have been developed for the lowest common denominator: triangulated meshes. We propose a scheme that handles the topology of arbitrary polygon meshes. It encodes meshes directly in their polygonal representation and extends to capture face groupings in a natural way. Avoiding the triangulation step we reduce the storage costs for typical polygon models that have group structures and property data.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[connectivity encoding]]></kw>
			<kw><![CDATA[mesh compression]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010395</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image compression</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P192290</person_id>
				<author_profile_id><![CDATA[81100297450]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Martin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Isenburg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39024749</person_id>
				<author_profile_id><![CDATA[81100047211]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jack]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Snoeyink]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>319426</ref_obj_id>
				<ref_obj_pid>319351</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[C. Bajaj, V. Pascucci, and G. Zhuang. Progressive compression and transmission of arbitrary triangular meshes. In Visualization 99, pages 307-316,1999.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>319358</ref_obj_id>
				<ref_obj_pid>319351</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[D. Cohen-Or, D. Levin, and O. Remez. Progressive compression of arbitrary tri-angular meshes. In Visualization 99 Conference Proceedings, pages 67-72,1999.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218391</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[M. Deering. Geometry compression. In SIGGRAPH 95, pages 13-20, 1995.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[M. Denny and C. Sohler. Encoding a triangulation as a permutation of its point set. In Proc. of 9th Canadian Conf. on Comp. Geom., pages 39-43, 1997.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>245626</ref_obj_id>
				<ref_obj_pid>244979</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[F. Evans, S. S. Skiena, and A. Varshney. Optimizing triangle strips for fast ren-dering. In Visualization 96 Conference Proceedings, pages 319-326, 1996.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>282923</ref_obj_id>
				<ref_obj_pid>282918</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[L. Guibas and J. Stolfi. Primitives for the manipulation of general subdivisions and the computation of Voronoi Diagrams. ACM ToG, 4(2):74-123, 1985.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280836</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[S. Gumhold and W. Strasser. Real time compression of triangle mesh connectiv-ity. In SIGGRAPH 98 Conference Proceedings, pages 133-140,1998.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237216</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[H. Hoppe. Progressive meshes. In SIGGRAPH 96, pages 99-108, 1996.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>677322</ref_obj_id>
				<ref_obj_pid>646013</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[M. Isenburg and J. Snoeyink. Mesh collapse compression. In Proceedings of SIBGRAPI 99, Campinas, Brazil, pages 27-28, 1999.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>902236</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[M. Isenburg and J. Snoeyink. Spirale reversi: Reverse decoding of the Edge-breaker encoding. Technical Report TR-99-08, Computer Science, UBC, 1999.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>204722</ref_obj_id>
				<ref_obj_pid>204715</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[K. Keeler and J. Westbrook. Short encodings of planar graphs and maps. In Dis-crete Applied Mathematics, pages 239-252,1995.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[D. King and J. Rossignac. Guaranteed 3.67v bit encoding of planar triangle graphs. In Proc. of 11th Canadian Conf. on Comp. Geom., pages 146-149,1999.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[D. King, J. Rossignac, and A. Szymczak. Connectivity compression for irregular quadrilateral meshes. Technical Report TR-99-36,GVU, Georgia Tech, 1999.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[D. G. Kirkpatrick. Optimal search in planar subdivisions. SIAMJournal of Com-puting, 12(1):28-35, 1983.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[B. Kronrod and C. Gotsman. Efficient coding of non-triangular meshes. In Proc. of 16th Europ. Workshop on Computational Geometry, pages 24-26, 2000.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[J. Li, C. C. Kuo, and H. Chen. Mesh connectivity coding by dual graph approach. Contribution Document MPEG98/m3530Tokyo, mar 1998.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[R. Parajola and Rossignac. Compressed progressive meshes. Technical Report TR-99-05, GVU, Georgia Tech, 1999.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>614421</ref_obj_id>
				<ref_obj_pid>614273</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[J. Rossignac. Edgebreaker: Connectivity compressionfor triangle meshes. IEEE Transactions on Visualization and Computer Graphics, 5(1), 1999.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[J. Rossignac and A. Szymczak. Wrap&amp;zip: Linear decoding of planar triangle graphs. The Journal of ComputationalGeometry, Theory and Applications, 1999.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>739821</ref_obj_id>
				<ref_obj_pid>647907</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[J. Snoeyink and M. van Kreveld. Linear-time reconstruction of Delaunay trian-gulations with applications. In Proc. of Europ. Symp. Alg., pages 459-471,1997.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[D. M. Y. Sommerville. An Introductionto the Geometry of N Dimensions. Dutton Publications, New York, 1929.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280834</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[G. Taubin, A. Gu~ eziec, W.P. Horn, and F. Lazarus. Progressive forest split com-pression. In SIGGRAPH 98 Conference Proceedings, pages 123-132, 1998.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[G. Taubin, W.P. Horn, F. Lazarus, and J. Rossignac. Geometry coding and VRML. Proceedings of the IEEE, 86(6):1228-1243, 1998.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>274365</ref_obj_id>
				<ref_obj_pid>274363</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[G. Taubin and J. Rossignac. Geometric compression throughtopological surgery. ACM Transactions on Graphics, 17(2):84-115, 1998.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[C. Touma and C. Gotsman. Triangle mesh compression. In Graphics Interface 98 Conference Proceedings, pages 26-34, 1998.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[G. Turan. Succinct representations of graphs. Dis. Apl. Math., 8:289-294, 1984.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[W.T. Tutte. A census of planar triangulations. Cnd. Jrn. Math., 14:21-38, 1962.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Viewpoint. Premier Catalog (2000 Edition) www.viewpoint.com.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>214771</ref_obj_id>
				<ref_obj_pid>214762</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[I. H. Witten, R. M. Neal, and J. G. Cleary. Arithmetic coding for data compres-sion. Communications of the ACM, 30(6):520-540, 1987.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[M. Woo, J. Neider, and T. Davis. Open GL Programming Guide. A.W., 1996.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 0 2 2 (1)+(   2+ =0=0=0=0=0=1 =2 =0 =0 n> ..... . . ..... . . .. .. ..... . . ..... . . ..... . 
..... . ..... . .. ..... ... ..... . ..... . gate popped gate pushed from stack on stack ..... . .. ..... 
. .. gate popped gate pushed from stack on stack ..... . . ..... . gate removed offset2 from stack hole 
offset1 .. ..... . .. ..... . . gate inserted offset2 hole into stack offset1 Figure 3: The labels of 
the Face Fixer scheme: when they apply and the corresponding updates during encoding and decoding. labels 
are R, L, S, or E. An encoding that uses 1 bit for label F3and 3 bits each for the other labels guarantees 
a 5v;10bit encoding. Similarly, a simple quadrangle mesh with vvertices has 2v;4 edges and v;2quadrangles. 
Here v;2labels are of type F4while the remaining v;2labels are R, L, S, or E. An encoding that uses 1 
bit for label F4and 3 bits each for the other labels guarantees a 4v;8bit encoding. 3.3 Quadrilateral 
Grids Instead of .xing together faces the Face Fixer scheme can also .x together patches of faces. Then 
we have to describe in addition the interior of these patches. If a patch is a rectangular quadrilateral 
grid this can be done very ef.ciently through the number of rows and columns in this grid. The beethoven 
bust and the shark model in Figure 4 for example, contain large patches of quadrilateral grids. We introduce 
the label QGr,l,hto include such a quad grid into the active boundary. The associated integer values 
r, l,and hcount the number of quadrangles that this grid extends to the right, to the left, and across 
as seen from the active gate (see Figure 4). Optimal selection of a set of non-overlapping quad grids 
on the model is not only NP-hard, we also lack a well-de.ned optimality criterium. Including quad grids 
into the active boundary breaks up the regularity of the label stream, which in turn hampers subsequent 
arithmetic coding. However, .rst results using greedy methods are promising: The connectivity of the 
teapot, for example, compresses down to 1.069 bpv using 10 quad grids, for the shark 1.374 bpv, for the 
galleon 2.190 bpv, and for the beethoven bust 2.591 bpv.   Figure 4: The beethoven bust and the shark 
model with quad grids marked in yellow (top). The label QG encodes a quad grid by spec­ifying its left 
and right extend and its height (bottom). 4 PROPERTIES AND STRUCTURES The Face Fixer scheme as presented 
so far allows to ef.ciently com­press and uncompress the connectivity of a polygon mesh. How­ever, polygonal 
models have geometry data associated with each vertex that speci.es their physical location in 3D. Additional 
prop­erty data, such as normals, colours, and texture coordinates, is often attached to the vertices, 
the faces, or the corners of the mesh. To es­tablisha connectionbetweenthe geometryandpropertydataandthe 
vertex,face,orcornertheyareassociatedwith, wede.neanimplicit ordering on the occurrence of these mesh 
elements. Such an order­ing can be derived using any deterministic mesh traversal that starts at a known 
point. Then the encoder stores the geometry or property values in the order in which the mesh features 
they are attached to are encountered during the traversal. Decoding performs the same traversal and re-assigns 
the data to the appropriate places. Compression schemes that use the traversal order induced by the connectivity 
encoder to attach geometry and property data to the mesh are called one-pass coders. Carefully designed 
[7], they can combine connectivity, geometry and property information into a sin­gle bit-stream, which 
makes it possible to stream a mesh across a network. Then decompression can reconstruct the mesh incremen­tally 
as the bits are received. Time-critical applications bene.t from such a scheme as transmission and reconstruction 
of the mesh can run in parallel. Other one-pass encoders [25] keep connectivity data separate from the 
rest in order to compress each more ef.ciently. Reconstructing the polygon mesh in a single pass forces 
the pre­dictive encoding for geometry and property data to make its esti­mations with incomplete neighbourhood 
information. A multi-pass coder stores the connectivity data separately from property and ge­ometry data 
and traverses the mesh two or more times during en­coding and decoding. The decoder .rst reconstructs 
the complete connectivity information before re-attaching geometry and property data to their appropriate 
location. In this case the mesh traversal used to establish the implicit ordering of geometry and property 
data can be different from the one used by the connectivity encoder. 4.1 Predictive Compression The Face 
Fixer scheme can be combined with previously proposed techniques for predictive compression of geometry 
and property data [3, 24, 25]. Since the prediction rules of these schemes assume meshes with triangle 
connectivity, we could simply triangulate the polygons using a deterministic strategy that is solely 
based on the connectivity. However,eventhoughthispaperdoesnotaddresspre­dictive compression, we believe 
that the recovered polygon infor­mation can be utilized for more accurate geometry prediction. For high-quality 
polygonal models like those in the Viewpoint Premier collection [28], faces are nearly planar and convex. 
Al­though a face may not be not perfectly planar, major discontinuities are improbable to occur across 
it otherwise it would likely have been triangulated when the model was designed. This can lead to an 
improvement in predictive geometry encoding: After the posi­tions of three vertices of a planar face 
are known, the 3D problem of predicting the coordinates for the remaining vertices around the face reduces 
to 2D. The embedding planes of multiple neighbouring faces around a vertex give additional hints for 
predicting its location. The convexity constraint can lead to further improvements in the accuracy of 
the prediction. The parallelogram rule introduced by Touma and Gotsman [25] uses the assumption that 
adjacent trian­gles form a parallelogram for predictive coding. While two adja­cent triangles can violate 
this assumption quite drastically, a convex quadrangle can not. Their approach could be extended to de.ne 
a pentagon or a hexagon rule for higher degree faces. 4.2 Vertex and Face Properties A vertex-based 
property assignment is commonly used to achieve visually smooth transitions across face boundaries. In 
the same way the geometry data is shared by all faces around each vertex to avoid cracks in the surface, 
a common normal, colour, or texture co­ordinate eliminates discontinuities when interpolated shading 
(e.g. Gouraud shading) is applied. Geometry data and property data as­sociated with a vertex are stored 
in the order the vertices are encoun­tered during the traversal of the mesh. A typical example for a 
face-based property assignment is a pre­computed radiosity solution. Each face has assigned a colour 
that corresponds to the amount of light it emits or transmits. The prop­erty data associated with a face 
is stored in the order the faces are encountered during the traversal of the mesh. Obviously this is 
in­dependent from the degree of the face. Here lies another advantage of the Face Fixer method over encoding 
schemes that .rst triangu­late the input mesh. Splitting a face of degree ninto n;2trian­gles creates 
n;2copies of its properties. Instead of encoding these properties once, they need to be encoded n;2times. 
 4.3 Corner Properties A corner-based property assignment becomes necessary to re.ect physical discontinuities 
in the underlying 3D model. Vertices that lie along such a discontinuity have usually more than one associ­ated 
normal, colour or texture coordinate, each of which they share with a disjoint set of adjacent corners. 
Five of our example models have vertices with multiple normals (see Table 3). We need to establish a 
mapping between a property value and the set of corners it is associated with. Our approach is a simple 
but effective improvement on work by Taubin et al. [23]. They store a discontinuity bit with every corner 
that is 0 when this corner uses the same property as the previous corner in counterclockwise order and 
a 1 otherwise. Then the property data associated with a set of corners is stored in the order in which 
the corresponding corners marked with 1 are encountered during the traversal of the mesh. This approach 
requires as many bits as the mesh has corners. Based on the observation that not all vertices have multiple 
prop­erties, we propose a similar marking scheme that uses vertex bits and corner bits. We use one bit 
per vertex to distinguish vertices with a single property ( 1 ) from those with multiple properties ( 
0 ). The corners around every vertex with multiple properties are marked as described above (see Figure 
5). We store the property data in the same order as the corresponding 1 bits appear in the bit sequence. 
The results in Table 3 show that this encoding gives savings of 20 % to 70 % over the method proposed 
by Taubin et al. [23].  vertex corner bits bits ..1 0 0 0 1 0 0 1..  vertex/ corner bit correspondance 
 Figure 5: Encoding the mapping from properties to sets of corners. vertices with nnormals vertex corner 
name n=1 n=2 n=3 n=4 n=5 n=6 bits bits triceratops 2585 232 14 1  2832 980 galleon 1146 894 308 9 16 
 2372 4756 beethoven 1838 681 118 14 2 2 2655 3235 sandal 1120 1227 274 15  2636 6150 shark 1985 575 
34 1  2560 2300 Table 3: Example results for encoding multiple vertex normals. The number of vertex 
bits and corner bits are reported that need to be recorded during the mesh traversal to establish a mapping 
between each normal and the set of corners sharing it.  4.4 Group Structures Structural information 
that classi.es groups of faces of a polygo­nal model into logical units is present in many .le formats. 
Such face groupings allow to assign qualitative information to otherwise nameless polygons. Typically 
they establish a mapping between a meaningful part of the real world object and the set of faces of the 
polygonal model representing this part. We created and colour­coded such structural information for the 
popular teapot mesh and the cow mesh (see Figure 2). Many other well-known polygonal models, such as 
the triceratops, the cessna, the beethoven bust, and the galleon mesh contain similar group structures. 
The triceratops mesh for example has six groups that classify each face as either skin, horn, toe, mouth, 
eye, or nose. The 58 faces that belong to the mouth-group form a single connected patch on the triceratops 
mesh. The 149 faces of the horn-group form three such patches and the 205 faces of the toe-group form 
.fteen. The galleon model has a total of 17 groups. But this model con­sists of 12 unconnected components, 
which capture some of the group structure. The six sails, the three masts, the rig, and the lamp are 
separate components and form a group each. The body of the galleon however is one component with six 
groups: the hull, the keel, the deck, the aft, the windows, and the rig. Encoding such structural information 
present in a polygon mesh has not been addressed by previously reported compression schemes. In a naive 
approach, we have a list of groups and assign a group index to every face. These group indices can then 
be treated like any other face property. For models with kgroups and ffaces such an encoding requires 
at least flogkbits. When a model consists of several mesh parts we can improve on the above by specifying 
for each component the number of group­ings it contains. For mesh parts whose faces belong all to the 
same group no additional information needs to be recorded. For mesh parts with group structures we need 
only as many bits per face as necessary to distinguish among the groups of this component. This is the 
approach we will compare our results against. The concept of a super face is an natural extension of 
the Face Fixer scheme that leads to more compact and elegant encodings of face groupings. A super face 
is a collection of faces that is contained inside a single closed boundary loop. The representation power 
of super faces is illustrated in Figure 6: A simple super face composed of 9 faces (case A), a super 
face with a non-manifold vertex (case B), a superface with anon-manifoldedge(caseC), andasuperfacethat 
contains another while being adjacent to a third (case D). We introduce a new label SF to encode super 
face structure on the mesh. When the active gate is adjacent to a super face the ac­tive boundary is 
extended around the entire super face and the new gate is pushed on the stack. The super face is cut 
out of the polygon mesh and its boundary becomes the active boundary with the gate being the same as 
before (see Figure 7). The super face, which it­self may contain other super faces, is .rst processed 
in its entirety before the encoding continues on the boundary that was pushed on thestack. Theinverseoperationusedfordecodingsimply 
.xes the super face back into the mesh. The length of the super face boundary is not encoded explicitly, 
but is directly related to the cost of encod­ing a super face. In addition to the label SF there is one 
additional label of types R, L, S, or E per super face boundary edge. We use super faces to encode the 
group structure of a polygon mesh by declaring each group boundary a super face boundary. This results 
in super faces corresponding to cases A, B, and D. Ideally we would like a one to one mapping from groups 
to super faces. But when the faces of a group are not all adjacent, like the three horns of the triceratops, 
a group is represented by more than one super face. We could connect the three horns using non-manifold 
super face edges (case C) along a shortest path across the mesh. However, this requires additional computation 
and is expensive for distant patches because of the increasing length of the super face boundary. Figure 
6: The representation power of super faces.  Figure 7: The label SF for encoding and decoding super 
faces. Instead of storing one group index per face, we store one group index per super face and one group 
index per mesh component. Ta­ble 4 lists results for our pool of example meshes. Although more bits are 
needed to include the super face structure into the connec­tivity encoding, the savings in the number 
of necessary group refer­ences lead to superior compression rates overall. Hierarchical Super Faces. 
The group structures that we have discussed so far are .at structures without a hierarchy. Suppose the 
  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344922</article_id>
		<sort_key>271</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>30</seq_no>
		<title><![CDATA[Progressive geometry compression]]></title>
		<page_from>271</page_from>
		<page_to>278</page_to>
		<doi_number>10.1145/344779.344922</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344922</url>
		<abstract>
			<par><![CDATA[<p>We propose a new progressive compression scheme for arbitrary topology, highly detailed and densely sampled meshes arising from geometry scanning. We observe that meshes consist of three distinct components: geometry, parameter, and connectivity information. The latter two do not contribute to the reduction of error in a compression setting. Using semi-regular meshes, parameter and connectivity information can be virtually eliminated. Coupled with semi-regular wavelet transforms, zerotree coding, and subdivision based reconstruction we see improvements in error by a factor four (12dB) compared to other progressive coding schemes.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[compression algorithms]]></kw>
			<kw><![CDATA[hierarchical representations]]></kw>
			<kw><![CDATA[semi-regular meshes]]></kw>
			<kw><![CDATA[signal processing]]></kw>
			<kw><![CDATA[subdivision surfaces]]></kw>
			<kw><![CDATA[wavelets]]></kw>
			<kw><![CDATA[zerotree coding]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Hierarchy and geometric transformations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.2</cat_node>
				<descriptor>Approximate methods</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.5.4</cat_node>
				<descriptor>Signal processing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.2</cat_node>
				<descriptor>Approximation of surfaces and contours</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.2</cat_node>
				<descriptor>Wavelets and fractals</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>C.3</cat_node>
				<descriptor>Signal processing systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010395</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image compression</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003624.10003633.10010918</concept_id>
				<concept_desc>CCS->Mathematics of computing->Discrete mathematics->Graph theory->Approximation algorithms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10003247</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Signal processing systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10003247</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Signal processing systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010244</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Hierarchical representations</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P17577</person_id>
				<author_profile_id><![CDATA[81100387443]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Khodakovsky]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Caltech]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40023875</person_id>
				<author_profile_id><![CDATA[81100117380]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schr&#246;der]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Caltech]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P300467</person_id>
				<author_profile_id><![CDATA[81100340025]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Wim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sweldens]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bell Laboratories]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>319426</ref_obj_id>
				<ref_obj_pid>319351</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{1} BAJAJ, C. L., PASCUCCI, V., AND ZHUANG, G. Progressive Compression and Transmission of Arbitrary Triangular Meshes. <i>IEEE Visualization '99</i> (1999), 307-316.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237213</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{2} CERTAIN, A., POPOVIC, J., DEROSE, T., DUCHAMP, T., SALESIN, D., AND STUETZLE, W. Interactive Multiresolution Surface Viewing. <i>Proceedings of SIGGRAPH 96</i> (1996), 91-98.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{3} CIGNONI, P., ROCCHINI, C., AND SCOPIGNO, R. Metro: Measuring Error on Simplified Surfaces. <i>Computer Graphics Forum 17</i>, 2 (1998), 167-174.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>319358</ref_obj_id>
				<ref_obj_pid>319351</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{4} COHEN-OR, D., LEVIN, D., AND REMEZ, O. Progressive Compression of Arbitrary Triangular Meshes. <i>IEEE Visualization '99</i> (1999), 67-72.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>789583</ref_obj_id>
				<ref_obj_pid>789085</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{5} DAVIS, G., AND CHAWLA, S. Image Coding Using Optimized Significance Tree Quantization. In <i>Prodeedings Data Compression Conference</i>, 387-396, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{6} DAVIS, G., AND NOSRATINIA, A. Wavelet-based Image Coding: An Overview. <i>Applied Computational Control, Signals, and Circuits 1</i>, 1 (1998).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311576</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{7} DESBRUN, M., MEYER, M., SCHR&#214;DER, P., AND BARR, A. H. Implicit Fairing of Irregular Meshes Using Diffusion and Curvature Flow. <i>Proceedings of SIGGRAPH 99</i> (1999), 317-324.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>139640</ref_obj_id>
				<ref_obj_pid>139631</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{8} DEVORE, R. A., JAWERTH, B., AND LUCIER, B. J. Surface Compression. <i>Computer Aided Geometric Design 9</i> (1992), 219-239.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>78958</ref_obj_id>
				<ref_obj_pid>78956</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{9} DYN, N., LEVIN, D., AND GREGORY, J. A. A Butterfly Subdivision Scheme for Surface Interpolation with Tension Control. <i>ACM Transactions on Graphics 9</i>, 2 (1990), 160-169.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218440</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[{10} ECK, M., DEROSE, T., DUCHAMP, T., HOPPE, H., LOUNSBERY, M., AND STUETZLE, W. Multiresolution Analysis of Arbitrary Meshes. <i>Proceedings of SIGGRAPH 95</i> (1995), 173-182.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>248979</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[{11} GOLUB, G. H., AND LOAN, C. F. V. <i>Matrix Computations</i>, 2nd ed. The John Hopkins University Press, Baltimore, 1983.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614336</ref_obj_id>
				<ref_obj_pid>614262</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[{12} GROSS, M. H., STAADT, O. G., AND GATTI, R. Efficient Triangular Surface Approximations Using Wavelets and Quadtree Data Structures. <i>IEEE Transactions on Visualization and Computer Graphics 2</i>, 2 (1996).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280836</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[{13} GUMHOLD, S., AND STRASSER, W. Real Time Compression of Triangle Mesh Connectivity. <i>Proceedings of SIGGRAPH 98</i> (1998), 133-140.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311577</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[{14} GUSKOV, I., SWELDENS, W., AND SCHR&#214;DER, P. Multiresolution Signal Processing for Meshes. <i>Proceedings of SIGGRAPH 99</i> (1999), 325-334.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344831</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[{15} GUSKOV, I., VIDIMCE, K., SWELDENS, W., AND SCHR&#214;DER, P. Normal Meshes. <i>Proceedings of SIGGRAPH 00</i> (2000).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[{16} HOPPE, H. Efficient Implementation of Progressive Meshes. <i>Computers & Graphics 22</i>, 1 (1998), 27-36.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[{17} KING, D., AND ROSSIGNAC, J. Optimal Bit Allocation in 3D Compression. Tech. Rep. GIT-GVU-99-07, Georgia Institute of Technology, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[{18} KOBBELT, L., VORSATZ, J., LABSIK, U., AND SEIDEL, H.-P. A Shrink Wrapping Approach to Remeshing Polygonal Surfaces. <i>Computer Graphics Forum 18</i> (1999), 119-130.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>789608</ref_obj_id>
				<ref_obj_pid>789085</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[{19} KOLAROV, K., AND LYNCH, W. Compression of Functions Defined on Surfaces of 3D Objects. In <i>Proc. of Data Compression Conference</i>, J. Storer and M. Cohn, Eds., 281-291, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237270</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[{20} KRISHNAMURTHY, V., AND LEVOY, M. Fitting Smooth Surfaces to Dense Polygon Meshes. <i>Proceedings of SIGGRAPH 96</i> (1996), 313-324.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280828</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[{21} LEE, A. W. F., SWELDENS, W., SCHR&#214;DER, P., COWSAR, L., AND DOBKIN, D. MAPS: Multiresolution Adaptive Parameterization of Surfaces. <i>Proceedings of SIGGRAPH 98</i> (1998), 95-104.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1889714</ref_obj_id>
				<ref_obj_pid>1889712</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[{22} LEVOY, M. The Digital Michelangelo Project. In <i>Proceedings of the 2nd International Conference on 3D Digital Imaging and Modeling</i>, October 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[{23} LI, J., AND KUO, C. Progressive Coding of 3-D Graphic Models. <i>Proceedings of the IEEE 86</i>, 6 (1998), 1052-1063.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[{24} LOOP, C. Smooth Subdivision Surfaces Based on Triangles. Master's thesis, University of Utah, Department of Mathematics, 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237750</ref_obj_id>
				<ref_obj_pid>237748</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[{25} LOUNSBERY, M., DEROSE, T. D., AND WARREN, J. Multiresolution Analysis for Surfaces of Arbitrary Topological Type. <i>ACM Transactions on Graphics 16</i>, 1 (1997), 34-73. Originally available as TR-93-10-05, October, 1993, Department of Computer Science and Engineering, University of Washington.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[{26} PAJAROLA, R., AND ROSSIGNAC, J. Compressed Progressive Meshes. Tech. Rep. GIT-GVU-99-05, Georgia Institute of Technology, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>158415</ref_obj_id>
				<ref_obj_pid>158413</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[{27} RIEMENSCHNEIDER, S. D., AND SHEN, Z. Wavelets and Pre-Wavelets in Low Dimensions. <i>J. Approx. Th. 71</i>, 1 (1992), 18-38.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614421</ref_obj_id>
				<ref_obj_pid>614273</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[{28} ROSSIGNAC, J. Edgebreaker: Connectivity Compression for Triangle Meshes. <i>IEEE Transactions on Visualization and Computer Graphics 5</i>, 1 (1999), 47-61.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[{29} ROSSIGNAC, J., AND SZYMCZAK, A. Wrap&Zip: Linear Decoding of Planar Triangle Graphs. Tech. Rep. GIT-GVU-99-08, Georgia Institute of Technology, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2322693</ref_obj_id>
				<ref_obj_pid>2322479</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[{30} SAID, A., AND PEARLMAN, W. A New, Fast, and Efficient Image Codec Based on Set Partitioning in Hierarchical Trees. <i>IEEE Transaction on Circuits and Systems for Video Technology 6</i>, 3 (1996), 243-250.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218439</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[{31} SCHR&#214;DER, P., AND SWELDENS, W. Spherical Wavelets: Efficiently Representing Functions on the Sphere. <i>Proceedings of SIGGRAPH 95</i> (1995), 161- 172.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[{32} SHAPIRO, J. Embedded Image-Coding using Zerotrees of Wavelet Coefficients. <i>IEEE Transactions on Signal Processing 41</i>, 12 (1993), 3445-3462.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>267101</ref_obj_id>
				<ref_obj_pid>266989</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[{33} STAADT, O. G., GROSS, M. H., AND WEBER, R. Multiresolution Compression And Reconstruction. <i>IEEE Visualization '97</i> (1997), 337-346.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280834</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[{34} TAUBIN, G., GUEZIEC, A., HORN, W., AND LAZARUS, F. Progressive Forest Split Compression. <i>Proceedings of SIGGRAPH 98</i> (1998), 123-132.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>274365</ref_obj_id>
				<ref_obj_pid>274363</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[{35} TAUBIN, G., AND ROSSIGNAC, J. Geometric Compression Through Topological Surgery. <i>ACM Transactions on Graphics 17</i>, 2 (1998), 84-115.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[{36} TAUBIN, G., AND ROSSIGNAC, J., Eds. <i>3D Geometry Compression</i>. No. 21 in Course Notes. ACM Siggraph, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[{37} TOUMA, C., AND GOTSMAN, C. Triangle Mesh Compression. <i>Graphics Interface '98</i> (1998), 26-34.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237254</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[{38} ZORIN, D., SCHR&#214;DER, P., AND SWELDENS, W. Interpolating Subdivision for Meshes with Arbitrary Topology. <i>Proceedings of SIGGRAPH 96</i> (1996), 189-192.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258863</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[{39} ZORIN, D., SCHR&#214;DER, P., AND SWELDENS, W. Interactive Multiresolution Mesh Editing. <i>Proceedings of SIGGRAPH 97</i> (1997), 259-268.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Progressive Geometry Compression Andrei Khodakovsky Peter Schr¨oder Wim Sweldens Caltech Caltech Bell 
Laboratories Permission to make digital or hard copies of part or all of this work for personal or classroom 
use is granted without fee provided that copies are not made or distributed for profit or commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for components 
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy 
otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission 
and/or a fee.    476B (e:40) 1528B (e: 12) 4163B (e: 4.7) 26800B (e: 0.82) Figure 1: Partial bit-stream 
reconstructions from a progressive encoding of the Venus head model. File sizes are given in bytes and 
relative L2 reconstruction error in multiples of 10-4. The rightmost reconstruction is indistinguishable 
from the original. Abstract We propose a new progressive compression scheme for arbitrary topology, highly 
detailed and densely sampled meshes arising from geometry scanning. We observe that meshes consist of 
three dis­tinct components: geometry, parameter, and connectivity informa­tion. The latter two do not 
contribute to the reduction of error in a compression setting. Using semi-regular meshes, parameter and 
connectivity information can be virtually eliminated. Coupled with semi-regular wavelet transforms, zerotree 
coding, and subdivision based reconstruction we see improvements in error by a factor four (12dB) compared 
to other progressive coding schemes. CR Categories and Subject Descriptors: I.3.5 [Computer Graphics]: 
Computa­tional Geometry and Object Modeling -hierarchy and geometric transformations; G.1.2 [Numerical 
Analysis]: Approximation -approximation of surfaces and con­tours, wavelets and fractals; I.4.2 [Image 
Processing and Computer Vision]: Com­pression (Coding) -Approximate methods Additional Keywords: Compression 
algorithms, signal processing, wavelets, subdi­vision surfaces, semi-regular meshes, zerotree coding, 
hierarchical representations 1 Introduction Today we can accurately acquire .nely detailed, arbitrary 
topology surfaces with millions and most recently billions [22] of vertices. Such models place large 
strains on computation, storage, trans­mission, and display resources. Compression is essential in these 
settings and in particular progressive compression, where an early, coarse approximation can subsequently 
be improved through addi­tional bits. While compression of images has a long history and has achieved 
a high level of sophistication, compression of surfaces is relatively new and still evolving rapidly. 
Compression is always a tradeoff between accuracy and bit rate, i.e., bits per vertex. This tradeoff 
is the subject of classical rate­distortion theory. While rate-distortion curves are common in the image 
coding literature they have only recently appeared in geome­try coding. This is partially due to the 
fact that the error for images is easily measured using the L2 norm of the difference between original 
and approximation, while measuring error for surfaces is more involved. Since there is no immediate correspondence 
be­tween the original and compressed surface, one cannot simply sub­tract one surface from another. This 
dif.culty is typically addressed by computing a geometry error using, for example, Hausdorff dis­tance. 
Such error metrics do not depend on the particular sample locations or connectivity, but instead measure 
the distance between the geometric shapes. This is important since the original and com­pressed mesh 
may have very different sample locations and con­nectivity, especially in a progressive setting. By sample 
location we mean the precise location of the vertex within the surface. How low can such errors be? Consider 
a continuous physical sur­face, such as the Venus sculpture whose scan generated the mesh in Figure 1. 
Given that the source geometry is continuous, any dig­ital representation, such as a triangle mesh, has 
some error E as­sociated with it. This error has three components due to sampling, discretization, and 
quantization. Sampling error Es arises from ac­quisition noise. Discretization error Ed is due to the 
fact that a triangulation with edge length h can approximate a smooth geome­try no better than O(h2). 
Finally, a .nite bit representation for the vertex positions leads to quantization error Eq. The sampling 
and triangulation of the model .x Es and Ed. A standard .oat repre­sentation typically leads to a quantization 
error much smaller than Es + Ed. All existing single rate coders proceed by .rst quantizing the vertex 
positions more coarsely leading to a quantization error Eq Es + Ed followed by lossless encoding of the 
connectiv­ity and quantized vertex positions. Existing progressive coders aim to eventually recover the 
quantized sample locations and original connectivity. For small meshes with carefully layed out connectiv­ity 
and sample locations this is very appropriate. The situation is different for highly detailed, densely 
sampled meshes coming from 3D scanning: Since distortion is measured as geometric distance the sample 
locations and connectivity can be treated as additional degrees of freedom to improve the rate-distortion 
performance. As long as the .nal result has geometric error on the order of the orig­inal E, the actual 
sample locations and connectivity do not matter. We will call the information contained in the sample 
locations, the parameter information. For example, by letting the vertices slide within the surface we 
only change the parameter information and not the geometric .delity. In particular, we propose a new 
progressive geometry compres­sion method which is based on smooth semi-regular meshes, i.e., meshes built 
by successive triangle quadrisection starting from a coarse irregular mesh. Almost all vertices in a 
semi-regular mesh have valence six and their sample locations can easily be estimated. Hence, semi-regular 
meshes allow us to eliminate almost all param­eter and connectivity information. As we illustrate below, 
param­eter and connectivity information make up a considerable fraction of the bit budget in existing 
coders, but do not contribute at all to reducing geometric error. Consequently our rate-distortion curves 
are signi.cantly better than those of existing coders. For most mod­els, our error is about four times 
smaller at comparable bit rates, a remarkable 12 dB improvement! Semi-regular meshes additionally allow 
for wavelet transforms and zerotree coders. Zerotrees are amongst the best image coding algorithms today. 
Wavelets have superior decorrelation properties and allow for subdivision based reconstruction. This 
means that in regions where the encoder sets wavelet coef.cients to zero the decoder uses subdivision 
to reconstruct the geometry. Hence even highly compressed surfaces are still smooth and visually pleasing. 
Figure 1 shows a sequence of progressive reconstructions of the compressed Venus model at different bitrates. 
Goals and Contributions The main contribution of this paper is the observation that parameter information 
makes up a signi.cant fraction of the bit budget while not contributing to error reduction at all. This 
motivates our compression algorithm based on semi­regular meshes. As input our algorithm takes an irregular 
mesh describing a 2­manifold (possibly with boundary) and produces successive ap­proximations employing 
semi-regular meshes with little parameter and connectivity information. The coder .rst produces a hierar­chical 
approximation of the surface which is subsequently encoded with a zerotree progressive coder. Novel aspects 
of the algorithm include reducing parameter information through the use of semi-regular meshes;  a 
Loop based wavelet transform for high order decorrelation and subdivision based reconstruction;  a novel 
zerotree hierarchy for primal semi-regular triangle  meshes of arbitrary topology. We emphasize that 
our target application is the compression of densely sampled, highly detailed surfaces. Our algorithm 
is not ef­fective when the input geometry is well described by a small, care­fully layed out mesh. In 
this case progressive coding is generally questionable and non-progressive coders are more appropriate 
and perform exceedingly well. 1.1 Review of Related Work Mesh Compression: Algorithms for ef.cient encoding 
of ar­ bitrary connectivity meshes have been described both for the pro­gressive and non-progressive 
setting (for an excellent overview of 3D geometry compression see [36]). Most of the early efforts concentrated 
on .nding ef.cient encodings for mesh connectivity with the current state of the art at around 2-6b/v 
(bits per ver­tex) [37, 13, 35, 29, 28]. Vertex positions are dealt with by perform­ing an initial quantization 
followed by predictive coding induced by the traversal order of the connectivity encoding. In contrast 
to single target rate coders, progressive coders aim to code for a range of rates by allowing reconstruction 
of interme­diate shapes using a pre.x of the encoded bit stream. Such cod­ing schemes are typically based 
on mesh simpli.cation techniques. Examples include progressive meshes [26, 23, 16], independent set vertex 
removal strategies [4], topological surgery [34], and topo­logical layering [1]. Connectivity bits increase 
to around 4-10b/v in these schemes. Prediction of vertex positions is now more naturally performed in 
a hierarchical fashion as induced by the associated mesh simpli.cation. Examples include centroid predictors 
[34, 4] as well as higher order predictors [26]. To date, progressivity in these coders has typically 
been focused on connectivity encoding. Rate-distortion theory however says that coordinate values should 
be progressively quantized [23, 17] as well: to minimize error at a given rate one must trade off additional 
quantization bits for already present vertices against bits for new vertices and their connectivity. 
Wavelets It is well known from image coding that wavelet repre­sentations are very effective in decorrelating 
the original data [8, 6], greatly facilitating subsequent entropy coding. In essence, coarser level data 
provides excellent predictors for .ner level data, leav­ing only generally small prediction residuals 
for the coding step. For tensor product surfaces many of these ideas can be applied in a straightforward 
fashion [8, 33, 12]. However, the arbitrary topol­ogy surface case is much more challenging. To begin 
with, wavelet decompositions of general surfaces were not known until the pio­neering work in [25]. These 
constructions were subsequently ap­plied to progressive approximation of surfaces [2] as well as data 
on surfaces [31, 19]. Multiresolution surface representations based on subdivi­sion [39] and local frame 
details are closely related to our wavelet constructions and have proven to be very powerful in a variety 
of circumstances. However, they require the initial surface to be rep­resented by a semi-regular mesh. 
This has led to the development of a number of algorithms for remeshing [10, 20, 21, 18]. Zerotree Coders 
Some of the best wavelet based progressive coders are based on zerotrees [5, 32, 30]. They effectively 
exploit the fact that wavelet coef.cients at .ner scales tend to be smaller in magnitude than coef.cients 
at coarser scales in the same region. A zerotree coder encodes the location of coef.cients below threshold 
in subtrees. Standard zerotree coders for images are based on a dual formulation, i.e., coef.cients are 
associated with faces. For primal hierarchical mesh decompositions using face splits (e.g., quadrisec­tion 
of triangles) the data however lives at vertices, not faces. We show in Section 3.4 how to build zerotree 
coders for primal hierar­chies. Irregular Subdivision Our separation of parameter versus ge­ometry information 
is partially inspired by the work done on irreg­ular subdivision [14] and intrinsic curvature normal 
.ow [7]. They point out that without the parameter side information, it is impos­sible to build high 
order schemes converging to smooth meshes. Irregular parameter information is inherently hard to encode 
and hinders the performance of irregular mesh coders. 2 Geometry, Parameter, and Connectivity Information 
Elimination of parameter and connectivity information is a key in­gredient of our algorithm. In this 
section we go into more detail regarding parameter and connectivity information and how to elim­inate 
it. Previous compression approaches have typically treated triangle meshes as consisting of two distinct 
components: connectivity and vertex positions. State of the art coders are able to encode con­nectivity 
of irregular meshes with 2b/v or even less. Hence, it is argued, vertex positions are much more expensive 
and their coding needs further advancement, for example through better predictors. The main insight of 
this paper is that there are actually three components: connectivity, geometry, and parameter information. 
The parameter information captures where the sample locations are within the surface while the geometry 
information captures the ge­ometry independent of the sample locations used. So far parameter and geometry 
information were treated together. Consider a vertex of a particular Venus head triangulation. Mov­ing 
this vertex slightly within the surface, does not change the dis­cretization error or geometry information. 
It only affects the pa­rameter information. Alternatively, moving the vertex normal to the surface clearly 
changes the error and geometry information, but leaves parameter information unchanged. This illustrates 
that while geometry and parameter information are globally intertwined they disconnect locally: in.nitesimally, 
we may think of parameter in­formation as being described by displacements in the tangent plane to the 
surface. Geometry information on the other hand is normal to the surface. This implies that from a rate 
distortion point of view bits should be allocated preferentially to the local normal direction. For smooth 
parameterizations this occurs naturally since prediction residuals in the tangent plane will be small. 
Sphere Example To illustrate the power of the distinction be­tween geometry, parameter, and connectivity 
information we con­sider three triangulations of a sphere (Figure 2). All three meshes contain the same 
geometry information and carry the same dis­cretization error Ed with no sampling noise. The .rst two 
meshes have semi-regular connectivity but different parameter information. The middle one was generated 
by jiggling the sample locations within the sphere, thereby adding signi.cant parameter informa­tion. 
The rightmost has irregular connectivity and parameter infor­mation. Figure 3 shows the respective rate-distortion 
curves when using the state of the art non-progressive coder of Touma and Gotsman (TG) [37]. We always 
show non-progressive curves dashed since these points are not achievable in a progressive manner. In 
case of the smooth semi-regular mesh, the TG coder correctly noticed that it contains almost no connectivity 
information (0.1 b/v) and almost no parameter information. Its performance is essentially limited by 
the quality of the predictor used. The TG coder for the non­smooth semi-regular sphere is worse illustrating 
the bit penalty for parameter information. The TG coder for the irregular mesh (right) illustrates the 
additional overhead from irregular connectivity. This example demonstrates the tremendous pay off of 
reducing both con­nectivity and parameter information in a mesh. Finally the small curve near the y-axis 
shows the result of apply­ing our coder to the smooth semi-regular mesh. It can approximate the sphere 
with a relative error of 5 · 10-5 using 166 bytes or .5 b/v. This it not surprising since a sphere has 
very little geometric information and a smooth semi-regular mesh is essentially optimal for our coder. 
This is where the high order decorrelation and subdi­vision based reconstruction really pays off. The 
same effect we see here so pronounced for the sphere, can also be observed in smooth, regularly sampled 
regions of more general surfaces, see Section 4.  3 Algorithm Components The algorithm accepts as input 
an arbitrary connectivity 2-manifold (with boundary) triangulation. In a .rst step we compute a smooth 
Figure 2: Three spherical meshes each with 2562 vertices: smooth semi-regular (left), non-smooth semi-regular 
(middle), irregular (right). They have the same geometry information. The middle one also has parameter 
information while the right one has parameter and connectivity information. 10 Loop: smooth, semi-regular 
TG: smooth, semi-regular8 TG: non-smooth, semi-reg. TG: irregular 6 4 2 0 0 5 101520253035 Figure 3: 
Rate distortion curves for the triangle meshes from Fig­ure 2 measured in relative L2 error on a scale 
of 10-4 as a function of rate in b/v for TG coordinate quantization levels of 8 - 12b. global parameterization 
using the MAPS algorithm [21]. This al­ lows us to compute successive adaptive approximations with semi­ 
regular connectivity. These semi-regular approximations are sub­ sequently wavelet transformed and progressively 
compressed using zerotrees. The coarsest level connectivity is encoded using a stan­ dard non-progressive 
mesh encoder [37]. The decoder may produce intermediate approximations from any pre.x of the bitstream. 
We need to de.ne the distance d(X, Y ) between two surfaces X and Y . Let d(x, Y ) be the Euclidean distance 
from a point x on X to the closest point on Y . Then the L2 distance d(X, Y ) is given by 1/2 d(X, Y 
)=1 d(x, Y )2dx. area(X)x.X This distance is not symmetric and we symmetrized it by taking the max of 
d(X, Y ) and d(Y, X). For triangulations this distance can be computed using the METRO tool [3]. All 
the L2 errors reported here are relative with respect to the bounding box diagonal on a scale of 10-4, 
while rate is reported in b/v with respect to the number of vertices in the original input mesh. 3.1 
Parameterization As a .rst step, we compute a smooth parameterization of our in­put triangulation using 
MAPS [21]. An important feature of MAPS is its ability to automatically align iso-parameter lines of 
the semi­regular mesh with sharp features of the original input surface help­ing to avoid large wavelet 
coef.cients near creases. MAPS builds a bijective map between the input mesh T and a coarse base domain 
B. One can then apply quadrisection in the base domain B and use the mapping to build semi-regular approxi­mations 
of T . These approximations have some remeshing error Er with respect to T . While this error can be 
made arbitrarily small, it does not make sense to make the remeshing error Er smaller than the discretization 
error Ed. This roughly occurs when the triangles from the semi-regular mesh are about the same size as 
the triangles of the input mesh. Using smaller triangles only serves to produce a better approximation 
of the input mesh, not necessarily of the orig­inal unknown geometry. Of course one does not know Ed. 
An order estimate of Ed can be computed by measuring the distance between the input mesh T and a much 
.ner mesh S obtained by Butter.y subdividing T . The lat­ter serves as a proxy for the unknown original 
geometry. Once our semi-regular mesh error Er is below the estimated discretization error Ed there is 
no need to further re.ne the semi-regular mesh. Hence our rate distortion curves will asymptotically 
not go to zero, but converge to the Ed estimate. Table 1 gives the Ed estimate, the minimum remeshing 
error, and the connectivity coding cost in bytes of the base domain B for various models. The connectivity 
was encoded using the TG coder. Feline Bunny Horse Venus Fandisk # Vert. 49864 34835 48485 50002 6475 
Ed (10-5) 7.3 9.4 6.0 5.5 28 Er (10-5) 6.3 7.4 5.1 4.2 4.8 # Base Vert. 250 127 112 196 73 Base conn. 
(B) 122 76 62 72 46 Table 1: Statistics for example meshes. 3.2 Wavelet Transform The wavelet transform 
replaces the original mesh with a coarsest mesh and a sequence of wavelet coef.cients expressing the 
dif­ference between successive levels. Since we deal with piecewise smooth models, neighboring vertices 
are highly correlated. The wavelet transform removes a large amount of this correlation. The distribution 
of wavelet coef.cients is centered around zero and their magnitude decays at .ner levels with the rate 
of decay related to the smoothness of the original surface. This behavior of the magnitude of wavelet 
coef.cients is the key to progressive coding and justi.es the choice of the zerotree coder for the bit 
encoding of coef.cients. Several methods for building wavelet transforms on semi-regular meshes exist 
[25, 31]. These are typically based on interpolating subdivision schemes such as Butter.y [9, 38]. A 
detailed descrip­tion of the construction of lifted Butter.y wavelets can be found in [31]. The advantage 
of lifted wavelets is that both forward and inverse transforms can be computed with .nite .lters. We 
use a novel Loop [24] wavelet transform, which has the ad­vantage that the inverse transform uses Loop 
subdivision. Experi­mentally, we found it has rate distortion curves essentially identical to Butter.y, 
but typically better visual appearance. The choice of Loop subdivision .xes the low pass reconstruc­tion 
.lter Pin a wavelet construction. We require a high pass reconstruction .lter Q. Together they de.ne 
the inverse wavelet transform j j+1 p p=PQj, (1) d where pj are the usual control points and dj the wavelet 
coef.­cients at level j. For performance reasons we would like Qto have small support. One way to achieve 
this is to apply a quadrature mirror construction [27], deriving a high pass from a low pass .l­ter. 
The result is shown in the regular case in Figure 4. Note that a globally consistent choice of the sign-.ipping 
direction is possible only for orientable surfaces. Though we can use the same stencils in the general 
case, the wavelet subbands corresponding to edges of a certain orientation are well-de.ned only for orientable 
surfaces. Around irregular vertices Pis modi.ed as usual. For edges im­mediately adjacent to an irregular 
vertex, Qmust be modi.ed as well. The only taps of the Q.lter that can fall onto irregular ver­tices 
are the two -6 coef.cients left and right of the center. If one of them is irregular we essentially open 
up that part of the .lter and parameterize the coef.cients by edge number, counting from the 10 (Figure 
4, right). If an irregular vertex has valence less than six this leads to the stencil folding over on 
itself, while for valences larger than six a gap is left. There is currently no theory available for 
wavelet constructions around irregular vertices. The only justi.cation of the trick we used is that it 
does not impact the numerically computed condition numbers of our transform. Fi­nally, boundaries are 
dealt with in the usual way through re.ection. Figure 4: Low (left) and high (middle) pass Loop reconstruction 
.lters in the regular case. For irregular vertices the high pass .lter is opened as indicated on the 
right. The forward wavelet transform, which goes from .ner to coarser levels, is de.ned as the solution 
[pj , dj] of the linear system in Eq. 1 for a given pj+1 . Consequently computing the forward wavelet 
transform requires the solution of sparse linear systems. To solve these systems we use a bi-conjugate 
gradient solver [11] with diagonal preconditioning. We found the condition number for up to a 7 level 
transform to be no worse than 30 depending on the model. Of course solving a linear system makes the 
forward transform slower than the inverse transform. This is acceptable as encoding is typically done 
once off-line while decoding happens frequently and in real time. For the Venus model the Loop forward 
transform, for example, takes 30s on a 550Mhz Pentium II Xeon while the inverse transform takes 2.5s. 
In case symmetry is important one can use a lifted Butter.y wavelet for which both forward and inverse 
transforms take about 2.5s. The decorrelating power of the wavelet transform is illustrated in Figure 
5. On the left is the histogram of the magnitude of Venus vertex positions. On the right is a histogram 
of the magnitude of the wavelet coef.cients. Clearly a large amount of correlation was removed and the 
.rst order entropy has decreased considerably. 0.3 0.3 0.4 0.5 0.6 0.0 0.8 1.7 2.5 3.3 Figure 5: Left: 
histogram of vertex position magnitudes for Venus. Right: histogram of the wavelet coef.cient magnitudes, 
showing the decorrelation power of the wavelet transform. 3.3 Vector Valued Wavelet Coef.cients Since 
our wavelet coef.cients are vector valued, it is not immedi­ately clear how they should be quantized. 
There is a fair amount of correlation between the x, y, and z wavelet components. We found that representing 
the wavelet coef.cients in a local frame [39] in­duced by the surface tangent plane makes the components 
much more independent. In particular, we .nd that the variance of nor­mal wavelet components is on average 
twice as large as the variance of the tangential components. Recalling the earlier geometry ver­sus parameter 
distinction this is exactly what we want. In a smooth semi-regular mesh, the geometry information (normal 
component) is much larger than the parameter information (tangential compo­nent). Figure 6 illustrates 
this by showing the histograms of the polar angles .(the angle from the zof normal axis) of the wavelet 
coef.cients in global and local coordinate frames. The distribution becomes very non-uniform in the local 
frame with peaks around 0and pindicating that most of the wavelet vectors lie in the nor­mal direction. 
The angle along the equator is fairly uniformly dis­tributed both in the global and local frame, hence 
the choice of basis vectors in the tangent plane is not important. Recall that parameter, Figure 6: 
Histograms of wavelet coef.cient polar .angles for the Venus head model in global (left) and local (right) 
frames. Coef.­cients lie mostly in the normal direction. i.e., tangential, information does not contribute 
to the error met­ric. Unfortunately, we cannot just ignore tangential wavelet com­ponents since this 
argument only holds in the in.nitesimal limit. Especially at coarser levels, tangential wavelet coef.cients 
can still contain some geometric information. However, we did .nd that the error metric is much less 
sensitive to quantization error of tangential versus normal wavelet components. Thus, we can further 
improve the error curves by more coarsely quantizing the tangential compo­nent. A basic operation in 
a scalar zerotree coder is the coef.cient sig­ni.cance test, i.e., checking its magnitude against a threshold. 
If it is below, the coef.cient is added to a zerotree, else the location and sign of the coef.cient need 
to be transmitted. For the vector case this becomes more dif.cult and we examined three quantization 
op­tions. (1) Spherical cells are natural as we can use the magnitude for the signi.cance test. We deal 
with the quantized angular com­ponents as generalized signs. (2) For cubical cells we divide the cube 
into 64 subcubes. Coef.cients in the 8 internal cubes are in­signi.cant and all the others are signi.cant; 
their cell number again is an analog of the angular component. (3) We can deal with each vector component 
independently and encode it separately, reducing the vector case to three independent scalar passes. 
We have compared all three cases and found that three scalar passes results in the best rate distortion 
curves for all models we considered. Experimentally, we found that quantization cells for the tangential 
component were best taken to be 4 times larger than those for the normal component. 3.4 Zerotree Coding 
Given that we settled on scalar quantization, our coder consists of three independent zerotree coders. 
The bits from the three coders are interleaved to maintain progressivity. A general principle of wavelet 
coef.cient encoding is to send the highest order bits of the largest magnitude coef.cients .rst. They 
will make the most signi.cant contributions towards reducing error. Let T0 =max{|ci|} be the maximum 
magnitude of all coef.cients, then in a .rst pass the coder should send the locations (index i)of newly 
signi.cant coef.cients, |ci| >T0/2. Doing so na¨ively is expensive. However, if source and receiver agree 
on a canonical traversal order the source only has to send the result of the sig­ni.cance test S(i)=(|ci| 
>T)and, if true, the sign bit of ci.If coef.cients can be organized into canonical sets such that with 
high probability all coef.cients in a given set are simultaneously below threshold, a few set-based signi.cance 
tests can enumerate the lo­cations of the relevant coef.cients. The decay properties of wavelet coef.cients 
make their hierarchical tree organization the natural set structure [32, 30, 5]. Coding consists of a 
number of passes with exponentially decreasing thresholds Tj+1 = Tj/2. In each pass signi.cance bits 
are sent for newly signi.cant coef.cients. Addi­tionally, re.nement bits are sent for those coef.cients 
which be­came signi.cant in an earlier pass. Since source and receiver al­ready agreed on locations of 
the latter, no location bits have to be sent for them. The number of such bit plane passes depends on 
the .nal quantization level. The decoder can reconstruct the geometry associated with any pre.x of the 
bitstream by running an inverse wavelet transform on the coef.cient bits seen so far. The main distinction 
of our setting from the image case is the construction of the zerotrees. For images, one associates the 
coef­.cients with a quadrilateral face and the trees follow immediately from the face quadtree. While 
this works also for dual, i.e., face based subdivision schemes, our triangular transform is primal, i.e, 
vertex based. The main insight is that while scale coef.cients are associated with vertices, wavelet 
coef.cients have a one-to-one association with edges of the coarser mesh. Vertices do not have a tree 
struc­ture, but the edges do. Each edge is the parent of four edges of the same orientation in the .ner 
mesh as indicated in Figure 7. Hence, each edge of the base domain forms the root of a zerotree; it groups 
all the wavelet coef.cients of a .xed wavelet subband from its two incident base domain triangles. The 
grouping is consistent for arbi­trary semi-regular meshes, i.e., no coef.cient is accounted for mul­tiple 
times or left out. Figure 7: A coarse edge (left) is parent to four .ner edges of the same orientation 
(right). For brevity we do not give the complete coder/decoder algorithm here, but refer the interested 
reader to the pseudo code in [30], which is identical to our implementation with the above quadtree de.nition. 
A .nal question concerns the transmission of the scale coef.­cients from the coarsest level. These are 
quantized uniformly. Ex­perimentally, we found that it is best to send 4 bit planes initially with the 
base domain connectivity. Each remaining bitplane is sent as the zerotrees descend another bit plane. 
The zerotree encoding (10 passes) of the Venus model takes 1s while decoding takes about 0.6s bringing 
the total decompression time to about 3.1s. Of course the low rate models can be decom­pressed faster. 
 3.5 Entropy Coding The zerotree algorithm is very effective at exploiting parent-child coef.cient correlations, 
minimizing the amount of encoded signif­icance bits. However, the output of the zerotree coder can still 
be compressed further through arithmetic coding, which allows for a fractional number of bits per symbol. 
The zerotree coder output contains three different types of infor­mation, signi.cance bits, re.nement 
bits and sign bits. Re.nement and sign bits tend to be uniformly distributed; hence they are not entropy 
coded. Signi.cance bits on the other hand can be further entropy coded. For early bitplanes most coef.cients 
are insignif­icant resulting in mostly zero bits. For later bitplanes many coef­.cients become signi.cant, 
resulting in mostly one bits. An arith­metic coder naturally takes advantage of this. 10 10  10 8 8 
8 6 6 6 4 4 4 2 2 2 0 0 0 0 5 1015202530 0 5 10 15 20 250 5 10 15 20 25 10 90 PSNR 10 8 8 80 6 6 70 
4 4 60 2 2 0 0 0 5 1015202530 0 5 10 15 20 250 5 1015202530 50 Figure 8: Rate-distortion curves.  
 We found that encoding of the signi.cance bits in groups fur­ther improves performance of entropy coding 
[30]. Because chil­dren of any node always appear together during a zerotree pass we group their signi.cance 
bits to form symbols of a 2j alpha­bet (j =4, 3, 2, 1). The actual number of bits of the alphabet is 
the number of children which were left insigni.cant at the previous pass. This grouping exploits correlations 
between magnitudes of spatially close wavelet coef.cients.  4 Results We compare our Loop based coder 
against known state of the art coders for different models. The coders we used are: TG: The Touma-Gotsman 
coder, which is a non progressive coder. It can be operated at different rates by changing the coor­dinate 
quantization between 8 and 12 bits.  CPM: The compressed progressive mesh coder of Pajarola and Rossignac 
[26]. It can start with various quantization sizes. We found 10 or 12 to work best (and always show the 
best one).  MPEG: The non-progressive coder from the MPEG4 standard  which is based on topological 
surgery [35]. Figure 8 (left) shows the different curves for the Venus model for bitrates up to 25b/v. 
The top left shows relative L2 error in units of 10-4. The bottom left shows the same numbers but in 
a PSNR scale where PSNR =20 log10 peak/d, peak is the bounding box diago­nal and d is the L2 error. One 
can see that our progressive coder is about 12dB or a factor 4 better than the progressive CPM coder. 
As expected the non-progressive coders are much worse at lower rates and slightly better at higher rates. 
Our curve converges to the remeshing error which is where it crosses the TG curve. Given that the remeshing 
error is comparable to the discretization error, any compression with smaller error is only resolving 
a particular trian­gulation more accurately, but not increasing the geometric .delity. Figure 8 (right) 
shows the rate distortion curves for several addi­tional models. Our curves are again signi.cantly better. 
Typically the TG coder crosses our curve below the discretization error. For the fandisk, which is a 
model with creases, we used a tagged Loop transform which preserves the creases. The fandisk does not 
have that many triangles which is why the TG coder shows better rate­distortion performance than the 
CPM coder. Figure 9 shows renderings of the different compressed versions of the model. This demonstrates 
the visual bene.ts of using sub­division based reconstruction. Note that the feline dataset has non­trivial 
genus (tail section), while the bunny has a number of holes on the bottom. For purposes of comparison 
(in the case of the Venus head) we have also rendered a number of partial bitstream recon­structions 
produced with the CPM coder (Figure 10) at .le sizes comparable to our reconstructions (Figure 1). One 
could argue that the results of a more traditional progressive mesh coder could be improved by a smoothing 
post-process. However, even at very low bit rates, bit-plane progressivity in our coder implies that 
we see high order bits of signi.cant coef.cients at .ne levels of the hi­erarchy early on. The resulting 
reconstructions always carry more detail than a straightforward Loop smoothing of some triangle mesh 
would capture. Finally Table 2 gives numerical error values for our coder at a variety of bit rates for 
the different models. b/v 1/41/21248 venus 15 6.1 3.1 1.60 0.85 0.55 feline 32 13 5.8 2.5 1.25 0.75 horse 
9.7 4.5 2.0 1.05 0.70 0.55 bunny 22 10.8 5.1 2.5 1.40 0.95 fandisk 52 11.9 3.5 1.00 0.60 Table 2: Relative 
L2 error in units of 10-4 of our coder at various bitrates.  5 Conclusion and Future Work In this paper 
we described a progressive compression algorithm based on semi-regular meshes, wavelet transforms, and 
zerotree coders. Our rate distortion curves are signi.cantly better than the best known progressive and 
non-progressive coders. This was achieved by explicitly treating sample locations and mesh connec­tivity 
as degrees of freedom of the coder. The progressive recon­structions especially at very low bit rates 
can be of astonishingly high visual quality. There are several directions for future work:      
 Figure 9: File size in bytes and errors in units of 10-4 . A mathematically sound theory for the construction 
of Loop wavelets around extraordinary vertices, including stability anal­ysis.  Construction of Loop 
wavelet transforms for adaptive semi­regular meshes. While all our reconstructions are performed adaptively, 
currently only lifted wavelets allow for adaptive analysis.  Design of wavelet .lters more suitable 
for geometry. Careful examination of reconstructed geometry reveals some ringing ar­tifacts with our 
current wavelets.  Even for our semi-regular meshes, there is still a fair amount of tangential information 
especially on the coarse levels. Recent  work by Guskov et al. [15] shows that it is possible to construct 
normal meshes, i.e., meshes in which all wavelet coef.cients lie exactly in the normal direction. The 
issues we discuss in this paper regarding geometry versus parameterization led to ideas such as coarsely 
quantizing the tangential components. These ideas can also be used to further improve irregular mesh 
coders. Acknowledgments Andrei Khodakovsky was partially supported through an internship at Lucent Technologies. 
Other support came from NSF (ACI-9624957, ACI-9721349, DMS-9872890, DMS-9874082), Alias|Wavefront, a 
Packard Fellow­ship, and the SGI-Utah Visual Supercomputing Center. Special thanks to Cici Koenig, Igor 
Guskov, Mathieu Desbrun, Aaron Lee, and Martin Vetterli. Datasets are cour­tesy Cyberware, the Stanford 
program in Computer Graphics and Hugues Hoppe. Our implementation uses an arithmetic coder of Geoff Davis 
and John Danskin. We are par­ticularly grateful to Renato Pajarola, Craig Gotsman, and Gabriel Taubin 
for providing us with executables of their mesh compression algorithms.    References [1] BAJAJ, C. 
L., PASCUCCI,V., AND ZHUANG, G. Progressive Compression and Transmission of Arbitrary Triangular Meshes. 
IEEE Visualization 99 (1999), 307 316. [2] CERTAIN, A., POPOVIC, J., DEROSE,T., DUCHAMP,T., SALESIN, 
D., AND STUETZLE, W. Interactive Multiresolution Surface Viewing. Proceedings of SIGGRAPH 96 (1996), 
91 98. [3] CIGNONI,P., ROCCHINI, C., AND SCOPIGNO, R. Metro: Measuring Error on Simpli.ed Surfaces. Computer 
Graphics Forum 17, 2 (1998), 167 174. [4] COHEN-OR, D., LEVIN, D., AND REMEZ, O. Progressive Compression 
of Arbitrary Triangular Meshes. IEEE Visualization 99 (1999), 67 72. [5] DAVIS, G., AND CHAWLA, S. Image 
Coding Using Optimized Signi.cance Tree Quantization. In Prodeedings Data Compression Conference, 387 
396, 1997. [6] DAVIS, G., AND NOSRATINIA, A. Wavelet-based Image Coding: An Overview. Applied Computational 
Control, Signals, and Circuits 1, 1 (1998). [7] DESBRUN, M., MEYER, M., SCHR ¨ ODER,P., AND BARR, A. 
H. Implicit Fair­ing of Irregular Meshes Using Diffusion and Curvature Flow. Proceedings of SIGGRAPH 
99 (1999), 317 324. [8] DEVORE, R. A., JAWERTH, B., AND LUCIER, B. J. Surface Compression. Computer Aided 
Geometric Design 9 (1992), 219 239. [9] DYN, N., LEVIN, D., AND GREGORY, J. A. A Butter.y Subdivision 
Scheme for Surface Interpolation with Tension Control. ACM Transactions on Graphics 9, 2 (1990), 160 
169. [10] ECK, M., DEROSE,T., DUCHAMP,T., HOPPE, H., LOUNSBERY, M., AND STUETZLE, W. Multiresolution 
Analysis of Arbitrary Meshes. Proceedings of SIGGRAPH 95 (1995), 173 182. [11] GOLUB, G. H., AND LOAN,C. 
F. V. Matrix Computations, 2nd ed. The John Hopkins University Press, Baltimore, 1983. [12] GROSS, M. 
H., STAADT, O. G., AND GATTI,R. Ef.cient Triangular Surface Approximations Using Wavelets and Quadtree 
Data Structures. IEEE Transac­tions on Visualization and Computer Graphics 2, 2 (1996). [13] GUMHOLD, 
S., AND STRASSER, W. Real Time Compression of Triangle Mesh Connectivity. Proceedings of SIGGRAPH 98 
(1998), 133 140. [14] GUSKOV, I., SWELDENS,W., AND SCHR¨ ODER, P. Multiresolution Signal Pro­cessing 
for Meshes. Proceedings of SIGGRAPH 99 (1999), 325 334. [15] GUSKOV, K., SWELDENS,W., AND SCHR¨ P. NormalI., 
VIDIMCE, ODER, Meshes. Proceedings of SIGGRAPH 00 (2000). [16] HOPPE,H. Ef.cient Implementation of Progressive 
Meshes. Computers &#38; Graphics 22, 1 (1998), 27 36. [17] KING, D., AND ROSSIGNAC, J. Optimal Bit Allocation 
in 3D Compression. Tech. Rep. GIT-GVU-99-07, Georgia Institute of Technology, 1999. [18] KOBBELT, L., 
VORSATZ, J., LABSIK, U., AND SEIDEL, H.-P. A Shrink Wrap­ping Approach to Remeshing Polygonal Surfaces. 
Computer Graphics Forum 18 (1999), 119 130. [19] KOLAROV, K., AND LYNCH, W. Compression of Functions 
De.ned on Surfaces of 3D Objects. In Proc. of Data Compression Conference, J. Storer and M. Cohn, Eds., 
281 291, 1997. [20] KRISHNAMURTHY,V., AND LEVOY, M. Fitting Smooth Surfaces to Dense Polygon Meshes. 
Proceedings of SIGGRAPH 96 (1996), 313 324. [21] LEE,A. W. F., SWELDENS,W., SCHR¨ ODER,P., COWSAR, L., 
AND DOBKIN, D. MAPS: Multiresolution Adaptive Parameterization of Surfaces. Proceedings of SIGGRAPH 98 
(1998), 95 104. [22] LEVOY, M. The Digital Michelangelo Project. In Proceedings of the 2nd Inter­national 
Conference on 3D Digital Imaging and Modeling, October 1999. [23] LI, J., AND KUO, C. Progressive Coding 
of 3-D Graphic Models. Proceedings of the IEEE 86, 6 (1998), 1052 1063. [24] LOOP, C. Smooth Subdivision 
Surfaces Based on Triangles. Master s thesis, University of Utah, Department of Mathematics, 1987. [25] 
LOUNSBERY, M., DEROSE, T. D., AND WARREN, J. Multiresolution Analysis for Surfaces of Arbitrary Topological 
Type. ACM Transactions on Graphics 16,1 (1997), 34 73. Originally available as TR-93-10-05, October, 
1993, Department of Computer Science and Engineering, University of Washington. [26] PAJAROLA, R., AND 
ROSSIGNAC, J. Compressed Progressive Meshes. Tech. Rep. GIT-GVU-99-05, Georgia Institute of Technology, 
1999. [27] RIEMENSCHNEIDER, S. D., AND SHEN, Z. Wavelets and Pre-Wavelets in Low Dimensions. J. Approx. 
Th. 71, 1 (1992), 18 38. [28] ROSSIGNAC, J. Edgebreaker: Connectivity Compression for Triangle Meshes. 
IEEE Transactions on Visualization and Computer Graphics 5, 1 (1999), 47 61. [29] ROSSIGNAC, J., AND 
SZYMCZAK, A. Wrap&#38;Zip: Linear Decoding of Planar Triangle Graphs. Tech. Rep. GIT-GVU-99-08, Georgia 
Institute of Technology, 1999. [30] SAID, A., AND PEARLMAN, W. A New, Fast, and Ef.cient Image Codec 
Based on Set Partitioning in Hierarchical Trees. IEEE Transaction on Circuits and Systems for Video Technology 
6, 3 (1996), 243 250. [31] SCHR¨ ODER,P., AND SWELDENS, W. Spherical Wavelets: Ef.ciently Repre­senting 
Functions on the Sphere. Proceedings of SIGGRAPH 95 (1995), 161 172. [32] SHAPIRO, J. Embedded Image-Coding 
using Zerotrees of Wavelet Coef.cients. IEEE Transactions on Signal Processing 41, 12 (1993), 3445 3462. 
[33] STAADT, O. G., GROSS, M. H., AND WEBER, R. Multiresolution Compression And Reconstruction. IEEE 
Visualization 97 (1997), 337 346. [34] TAUBIN, G., GUEZIEC, A., HORN,W., AND LAZARUS, F. Progressive 
Forest Split Compression. Proceedings of SIGGRAPH 98 (1998), 123 132. [35] TAUBIN, G., AND ROSSIGNAC, 
J. Geometric Compression Through Topologi­cal Surgery. ACM Transactions on Graphics 17, 2 (1998), 84 
115. [36] TAUBIN, G., AND ROSSIGNAC, J., Eds. 3D Geometry Compression. No. 21 in Course Notes. ACM Siggraph, 
1999. [37] TOUMA, C., AND GOTSMAN, C. Triangle Mesh Compression. Graphics Inter­face 98 (1998), 26 34. 
[38] ZORIN, D., SCHR ¨ W. ODER,P., AND SWELDENS, Interpolating Subdivision for Meshes with Arbitrary 
Topology. Proceedings of SIGGRAPH 96 (1996), 189 192. [39] ZORIN, D., SCHR ¨ ODER,P., AND SWELDENS, W. 
Interactive Multiresolution Mesh Editing. Proceedings of SIGGRAPH 97 (1997), 259 268. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344924</article_id>
		<sort_key>279</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>31</seq_no>
		<title><![CDATA[Spectral compression of mesh geometry]]></title>
		<page_from>279</page_from>
		<page_to>286</page_to>
		<doi_number>10.1145/344779.344924</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344924</url>
		<abstract>
			<par><![CDATA[<p>We show how spectral methods may be applied to 3D mesh data to obtain compact representations. This is achieved by projecting the mesh geometry onto an orthonormal basis derived from the mesh topology. To reduce complexity, the mesh is partitioned into a number of balanced submeshes with minimal interaction, each of which are compressed independently. Our methods may be used for compression and progressive transmission of 3D content, and are shown to be vastly superior to existing methods using spatial techniques, if slight loss can be tolerated.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[compression algorithms]]></kw>
			<kw><![CDATA[signal processing]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.5.4</cat_node>
				<descriptor>Signal processing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>C.3</cat_node>
				<descriptor>Signal processing systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10003247</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Signal processing systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10003247</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Signal processing systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010395</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image compression</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14148466</person_id>
				<author_profile_id><![CDATA[81100420014]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Zachi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Karni]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Technion Israel Institute of Technology, Haifa 32000, Israel]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14109147</person_id>
				<author_profile_id><![CDATA[81100294813]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Craig]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gotsman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Technion Israel Institute of Technology, Haifa 32000, Israel]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>789628</ref_obj_id>
				<ref_obj_pid>789086</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[V. Bajaj, V. Pascucci and G. Zhuang. Single resolution compression of arbitrary triangular meshes with properties. Proceedings of the Data Compression Conference, Snowbird, 1999.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[N. Biggs. Alegbraic Graph Theory (2nd Ed.). Cambridge University Press, 1993.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>267103</ref_obj_id>
				<ref_obj_pid>266989</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[M. Chow. Geometry compression for real-time graphics. Proceedings of Visualization '97, IEEE, 1997.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>319358</ref_obj_id>
				<ref_obj_pid>319351</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[D. Cohen-Or, O. Remez, and D. Levin, Progressive compression of arbitrary triangular meshes. Proceedings of Visualization '99, IEEE, 1999.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218391</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[M. Deering, Geometry compression, Proceedings of SIGGRAPH '95, pp. 13-20, ACM, 1995.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311576</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[M. Desbrun, M. Meyer, P. Schroeder and A. Barr. Implicit fairing of irregular meshes using diffusion and curvature flow. Proceedings of SIGGRAPH '99, pp. 317-324, ACM, 1999.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>578533</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[M. Garey and D. Johnson, Computers and intractability: A guide to the theory of NP-completeness, Addison-Wesley, 1978.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280836</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[S. Gumhold and W. Strasser. Real time compression of triangle mesh connectivity. Proceedings of SIGGRAPH '98, pp. 133-140, ACM, 1998.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311577</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[I. Guskov, W. Sweldens and P. Schroeder. Multiresolution signal processing for meshes. Proceedings of SIGGRAPH '99, pp. 325-334, ACM, 1999.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>677322</ref_obj_id>
				<ref_obj_pid>646013</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[M. Isenburg and J. Snoeyink. Mesh collapse compression. Proceedings of SIBGRAPHI'99- 12th Brazilian Symposium on Computer Graphics and Image Processing, pp.27- 28, 1999.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>344919</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[M. Isenberg and J. Snoeyink. FaceFixer: Compressing polygon meshes with properties. Proceedings of SIGGRAPH 2000.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>344922</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[A. Khodakovsky, P. Schroeder and W. Sweldens. Progressive geometry compression. Proceedings of SIGGRAPH 2000.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[D. King and J. Rossignac. Guaranteed 3.67v bit encoding of planar triangle graphs. In Proceedings of 1 lth Canadian Conference on Computation Geometry, pp.146-149, 1999.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[G. Karypis and V. Kumar. MeTiS: A software package for partitioning unstructured graphs, partitioning meshes, and computing fill-reducing orderings of sparse matrices. Version 4.0, Univ. of Minnesota, Dept. of Computer Science, 1998. Available at http://wwwusers.cs.umn.edu/Nkarypis/metis/metis.html]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280831</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[L. Kobbelt, S. Campanga, J. Vorsatz and H.-P. Seidel. Interactive multi-resolution modeling on arbitrary meshes. Proceedings of SIGGRAPH '98, pp. 105-114, ACM, 1998.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[P. Lancaster and M. Tismenetsky. The theory of matrices. (2nd Ed.), Academic Press, 1985.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[J. Li and C.-C. Kuo, A dual graph approach to 3D triangular mesh compression, In Proceedings of the IEEE International Conference on Image Processing, Chicago, 1998.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>614421</ref_obj_id>
				<ref_obj_pid>614273</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[J. Rossignac. Edgebreaker: Connectivity compression for triangle meshes. IEEE Transactions on Visualization and Computer Graphics, 5(1), 1999.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>270055</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[D. Salomon. Data compression: The complete reference. Springer Verlag, 1998.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218473</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[G. Taubin. A signal processing approach to fair surface design. Proceedings of SIGGRAPH '95, pp. 351-358, ACM, 1995.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>274365</ref_obj_id>
				<ref_obj_pid>274363</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[G. Taubin and J. Rossignac. Geometric compression through topological surgery. ACM Transactions on Graphics, 17(2):84-115, 1998.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[C. Touma and C. Gotsman. Triangle mesh compression. In Proceedings of Graphics Interface '98, pp. 26-34, 1998.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344925</article_id>
		<sort_key>287</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>32</seq_no>
		<title><![CDATA[Surface light fields for 3D photography]]></title>
		<page_from>287</page_from>
		<page_to>296</page_to>
		<doi_number>10.1145/344779.344925</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344925</url>
		<abstract>
			<par><![CDATA[<p>A <italic>surface light field</italic> is a function that assigns a color to each ray originating on a surface. Surface light fields are well suited to constructing virtual images of shiny objects under complex lighting conditions. This paper presents a framework for construction, compression, interactive rendering, and rudimentary editing of surface light fields of real objects. Generalization of vector quantization and principal component analysis are used to construct a compressed representation of an object's surface light field from photographs and range scans. A new rendering algorithm achieves interactive rendering of images from the compressed representation, incorporating view-dependent geometric level-of-detail control. The surface light field representation can also be directly edited to yield plausible surface light fields for small changes in surface geometry and reflectance properties.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[3D photography]]></kw>
			<kw><![CDATA[function quantization]]></kw>
			<kw><![CDATA[image-based rendering]]></kw>
			<kw><![CDATA[light field]]></kw>
			<kw><![CDATA[lumigraph]]></kw>
			<kw><![CDATA[principal function analysis]]></kw>
			<kw><![CDATA[surface light fields]]></kw>
			<kw><![CDATA[view-dependent level-of-detail]]></kw>
			<kw><![CDATA[wavelets]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.2</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Digitizing and scanning</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Viewing algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.2</cat_node>
				<descriptor>Wavelets and fractals</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010506</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Document scanning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010542.10011714</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Other architectures->Special purpose systems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39040137</person_id>
				<author_profile_id><![CDATA[81100377878]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[N.]]></middle_name>
				<last_name><![CDATA[Wood]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P58505</person_id>
				<author_profile_id><![CDATA[81100099911]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[I.]]></middle_name>
				<last_name><![CDATA[Azuma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P159356</person_id>
				<author_profile_id><![CDATA[81100192925]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aldinger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14202804</person_id>
				<author_profile_id><![CDATA[81100587184]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Curless]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP17009957</person_id>
				<author_profile_id><![CDATA[81100301736]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Duchamp]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P63622</person_id>
				<author_profile_id><![CDATA[81100188207]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Salesin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington and Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P298076</person_id>
				<author_profile_id><![CDATA[81100357122]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Werner]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stuetzle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[D.I. Azuma. Interactive Rendering of Surface Light Fields. Technical Report UW-CSE-2000-04-01, Department of Computer Science and Engineering, University of Washington, April 2000.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>132022</ref_obj_id>
				<ref_obj_pid>132013</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[P.J. Besl and N. D. McKay. A Method for Registration of 3-D Shapes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 14(2):239-256, February 1992.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311553</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[B. Cabral, M. Olano, and P. Nemec. Reflection Space Image Based Rendering. In SIGGRAPH 98 Conference Proceedings, Annual Conference Series, pages 165-170, August 1999.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237213</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[A. Certain, J. Popovi6, T. DeRose, T. Duchamp, D. Salesin, and W. Stuetzle. Interactive Multiresolution Surface Viewing. In SIGGRAPH 96 Conference Proceedings, Computer Graphics Annual Conference Series, pages 91-98, August 1996.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280832</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[J. Cohen, M. Olano, and D. Manocha. Appearance-Preserving Simplification. In SIGGRAPH 98 Conference Proceedings, Annual Conference Series, pages 115-122. ACM SIGGRAPH, July 1998.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237269</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[B. Curless and M. Levoy. A Volumetric Method for Building Complex Models from Range Images. In SIGGRAPH 96 Conference Proceedings, Annual Conference Series, pages 303-312, August 1996.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237191</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[P. E. Debevec, C. J. Taylor, and J. Malik. Modeling and Rendering Architecture from Photographs: A Hybrid Geometry- and Image- Based Approach. In SIGGRAPH 96 Conference Proceedings, Annual Conference Series, pages 11-20, August 1996.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[P. E. Debevec, Y. Yu, and G. D. Borshukov. Efficient View- Dependent Image-Based Rendering with Projective Texture-Mapping. Eurographics Rendering Workshop 1998, pages 105-116, June 1998.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218440</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[M. Eck, T. DeRose, T. Duchamp, H. Hoppe, M. Lounsbery, and W. Stuetzle. Multiresolution Analysis of Arbitrary Meshes. In SIGGRAPH 95 Conference Proceedings, Annual Conference Series, pages 173-182, August 1995.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[H. Gagnon, M. Soucy, R. Bergevin, and D. Laurendeau. Registration of Multiple Range Views for Automatic 3-D Model Building. In IEEE Conf. Computer Vision and Pattern Recognition, pages 581-586, June 1994.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>128857</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[A. Gersho and R. M. Gray. Vector Quantization and Signal Compression. Kluwer Academic Publishers, 1991.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237200</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[S. J. Gortler, R. Grzeszczuk, R. Szeliski, and M. E Cohen. The Lumigraph. In SIGGRAPH 96 Conference Proceedings, Annual Conference Series, pages 43-54, August 1996.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311554</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[W. Heidrich and H.-P. Seidel. Realistic, Hardware-Accelerated Shading and Lighting. Proceedings of SIGGRAPH 99, pages 171- 178, August 1999.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237216</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[H. Hoppe. Progressive Meshes. In SIGGRAPH 96 Conference Proceedings, Annual Conference Series, pages 99-108, August 1996.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258843</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[H. Hoppe. View-Dependent Refinement of Progressive Meshes. In SIGGRAPH 97 Conference Proceedings, Annual Conference Series, pages 189-198, August 1997.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280828</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[A.W. E Lee, W. Sweldens, P. Schroeder, L. Cowsar, and D. Dobkin. MAPS: Multiresolution Adaptive Parameterization of Surfaces. In SIGGRAPH 98 Conference Proceedings, Annual Conference Series, pages 95-104, July 1998.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237199</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[M. Levoy and P. Hanrahan. Light Field Rendering. In SIGGRAPH 96 Conference Proceedings, Annual Conference Series, pages 31-42, August 1996.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237750</ref_obj_id>
				<ref_obj_pid>237748</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[M. Lounsbery, T. D. DeRose, and J. Warren. Multiresolution Analysis for Surfaces of Arbitrary Topological Type. ACM Transactions on Graphics, 16(1):34-73, January 1997.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258847</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[D. Luebke and C. Erikson. View-Dependent Simplification of Arbitrary Polygonal Environments. In SIGGRAPH 97 Conference Proceedings, Annual Conference Series, pages 199-208, August 1997.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[M. Magnor and B. Girod. Adaptive Block-Based Light Field Coding. Proc. 3rd International Workshop on Synthetic and Natural Hybrid Coding and Three-Dimensional Imaging, pages 140-143, September 1999.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[M. Magnor and B. Girod. Hierarchical Coding of Light Fields with Disparity Maps. Proc. IEEE International Conference on Image Processing, pages 334-338, October 1999.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[G. S. P. Miller, S. Rubin, and D. Ponceleon. Lazy Decompression of Surface Light Fields for Precomputed Global Illumination. Eurographics Rendering Workshop 1998, pages 281-292, June 1998.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[K. Nishino, Y. Sato, and K. Ikeuchi. Appearance compression and synthesis based on 3D model for mixed reality. In Proceedings of IEEE ICCV'99, pages 38 - 45, September 1999.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[K. Nishino, Y. Sato, and K. Ikeuchi. Eigen-Texture Method: Appearance Compression based on 3D Model. Proc. of Computer Vision and Pattern Recognition, 1:618-624, June 1999.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74359</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[K. Perlin and E. M. Hoffert. Hypertexture. Computer Graphics (Proceedings of SIGGRAPH 89), 23(3):253-262, July 1989.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>732115</ref_obj_id>
				<ref_obj_pid>647651</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[K. Pulli, M. Cohen, T. Duchamp, H. Hoppe, L. Shapiro, and W. Stuetzle. View-based Rendering: Visualizing Real Objects from Scanned Range and Color Data. Eurographics Rendering Workshop 1997, pages 23-34, June 1997.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[S.M. Rusinkiewicz. A New Change of Variables for Efficient BRDF Representation. In Eurographics Rendering Workshop 1998, pages 11-22. Eurographics, June 1998.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258885</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Y. Sato, M. D. Wheeler, and K. Ikeuchi. Object Shape and Reflectance Modeling from Observation. In SIGGRAPH 97 Conference Proceedings, Annual Conference Series, pages 379-388, August 1997.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218473</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[G. Taubin. A Signal Processing Approach to Fair Surface Design. In SIGGRAPH 95 Conference Proceedings, Annual Conference Series, pages 351-358. ACM SIGGRAPH, August 1995.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[R.Y. Tsai. An Efficient and Accurate Camera Calibration Technique for 3D Machine Vision. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pages 364-374, 1986.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258766</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[B. Walter, G. Alppay, E. P. E Lafortune, S. Fernandez, and D. P. Greenberg. Fitting Virtual Lights For Non-Diffuse Walkthroughs. In SIGGRAPH 97 Conference Proceedings, Annual Conference Series, pages 45-48, August 1997.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>245627</ref_obj_id>
				<ref_obj_pid>244979</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[J. C. Xia and A. Varshney. Dynamic View-Dependent Simplification for Polygonal Models. In IEEE Visualization '96. IEEE, October 1996.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311559</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Y. Yu, P. Debevec, J. Malik, and T. Hawkins. Inverse Global Illumination: Recovering Reflectance Models of Real Scenes From Photographs. In SIGGRAPH 98 Conference Proceedings, Annual Conference Series, pages 215-224, August 1999.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Surface Light Fields for 3D Photography Daniel N. Wood1 Daniel I. Azuma1 Ken Aldinger1 Brian Curless1 
Tom Duchamp1 David H. Salesin1,2 Werner Stuetzle1 1University of Washington 2Microsoft Research Abstract 
A surface light .eld is a function that assigns a color to each ray originating on a surface. Surface 
light .elds are well suited to constructing virtual images of shiny objects under complex lighting conditions. 
This paper presents a framework for construc­tion, compression, interactive rendering, and rudimentary 
editing of surface light .elds of real objects. Generalizations of vector quantization and principal 
component analysis are used to construct a compressed representation of an object s surface light .eld 
from photographs and range scans. A new rendering algorithm achieves interactive rendering of images 
from the compressed representa­tion, incorporating view-dependent geometric level-of-detail con­trol. 
The surface light .eld representation can also be directly edited to yield plausible surface light .elds 
for small changes in surface geometry and re.ectance properties. CR Categories: I.3.2. [Computer Graphics]: 
Picture/Image Generation Digitizing and scanning, Viewing algorithms Keywords: surface light .elds, 3D 
photography, lumigraph, light .eld, function quantization, principal function analysis, view-dependent 
level-of­detail, image-based rendering, wavelets. Introduction Recent advances in digital cameras, 
3D laser scanners and other imaging technology are enabling us to capture enormous quanti­ties of geometric 
and radiance data with unprecedented ease and accuracy. These advances hold great promise for 3D photography, 
the process by which both the shape and appearance of physical objects are modeled and realistically 
rendered. But to make 3D photography truly practical, quite a few open problems still need to be solved. 
First, we need a good representation for those 3D datasets. The framework described in this paper is 
based on the surface light .eld, a term coined by Miller et al. [22]. The surface light .eld is a function 
that assigns an RGB value to every ray leaving every point on a surface. When constructed from observations 
made of an object, a surface light .eld encodes suf.cient information to construct realistic images of 
the object from arbitrary viewpoints. Surface texture, rapid variation in specularity, and global effects 
like interre.ection and shadowing are all correctly represented. Some of these properties can be seen 
in Figure 1. However, a good representation by itself is only half the story. Because the datasets acquired 
by 3D photography techniques are so large, good compression algorithms are needed. Furthermore, we need 
algorithms to render those datasets ef.ciently, ideally at interactive speeds. To this end, we need to 
develop level-of-detail Permission to make digital or hard copies of part or all of this work or personal 
or classroom use is granted without fee provided that copies are not made or distributed for profit or 
commercial advantage and that copies bear this notice and the full citation on the first page. To copy 
otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission 
and/or a fee. SIGGRAPH 2000, New Orleans, LA USA Figure 1 Images of a surface light .eld demonstrating 
detailed surface texture, rapid changes in specular properties, and interre.ec­tions. The specular variations 
occur, for example, in the gold paint on the tail of this porcelain .sh. The tail also re.ects light 
onto the body, as indicated by the reddish hue on the side of the .sh in the left panel. controls for 
the rendering process, with shape and appearance under independent control. Finally, just as in traditional 
2D photography, accurately capturing the real world is not suf.cient for many appli­cations; a useful 
representation for the results of 3D photography should also be editable. In this paper, we address each 
of these problems. In particular, our contributions include: Estimation/compression. Our raw data consists 
of a set of 2D digi­tal color photographs of an object together with a collection of laser range scans. 
To make a surface light .eld tractable for rendering, the data must .t into main memory. To this end 
we present two new algorithms that simultaneously estimate and compress the surface light .eld. The .rst 
is a generalization of vector quantization; the second is a generalization of principal component analysis. 
Rendering. We demonstrate an algorithm that can render our surface light .elds at interactive frame rates. 
Evaluation of the surface color takes time proportional to the occupied screen space. The amount of time 
required to render the underlying geometry is controlled using a new view-dependent level-of-detail algorithm 
for meshes with subdivision connectivity. The level of geometric approximation does not affect the sharpness 
of the surface texture. Editing. Our representation of surface light .elds allows editing, using 3D analogs 
of image processing algorithms to .lter re.ected light, and modi.cations of surface geometry. We can 
simulate changes in the re.ectance properties of the surface, and we can generate plausible images of 
the object after it has been deformed or moved relative to its environment. &#38;#169; ACM 2000 1-58113-208-5/00/07 
...$5.00 1.1 Related work Surface light .elds .t into the broad framework of image-based rendering schemes. 
Image-based methods take a collection of photographs as input, construct a representation of the surface 
color or radiance, and use it to synthesize new images from arbitrary viewpoints. The methods tend to 
differ in the number of input images they use, the representation of the data, the degree to which they 
incorporate geometric information about the object into the image representation, and the compression 
techniques they employ. Our own approach leverages high-resolution geometry to improve image quality 
while affording a compact representation. Levoy and Hanrahan [17] acquire many hundreds of images, which 
are resampled to lie on a regular grid in a two-plane parameter­ization. New images are computed by interpolation 
between ray samples, using essentially no geometric data. They apply vector quantization to obtain compressed 
representations of light .elds. Gortler et al. [12] present a similar two-plane parameterization that 
they call a lumigraph, in which they interpolate image samples via a hierarchical push-pull algorithm. 
They use approximate surface geometry derived from photograph silhouettes (or higher-resolution geometry 
in the case of synthetic data) to perform a depth correction that substantially reduces ghosting and 
blurring artifacts. In both these methods, the representation restricts the viewpoint to lie outside 
of the convex hull of the object. Magnor and Girod [20, 21] develop an MPEG-like scheme for compressing 
two-plane light .elds that produces better compression ratios than those obtained by Levoy and Hanrahan. 
Our approach depends on both high­resolution geometry and dense sets of images. It removes the convex 
hull restriction of the two-plane light .eld and admits a new form of compressed representation that 
can be rendered in real time. For comparable data sizes, our representation yields sharper images and 
greater compression ratios than two-plane representations. View-dependent texture mapping [7, 8, 26] 
is a kind of light .eld that does not require resampling the input images. This approach uses geometric 
information to re-project each input image into the desired camera viewpoint. The re-projected input 
images are then blended together using weights based on the view direction primar­ily, and possibly other 
factors such as sampling rate. Because the blending in view-dependent texture mapping incorporates visibility 
information, this approach supports rendering within the convex hull of the object. In practice, view-dependent 
texture mapping has been used with fewer images and surfaces that are less specular than those demonstrated 
with two-plane light .elds, though this is not a fundamental limitation. As noted in Debevec et al. [8], 
a surface light .eld can be viewed as a distillation of view-dependent texture mapping into a more ef.cient 
representation. Miller et al. [22] use surface light .elds to render solutions to synthetic (non-diffuse) 
global illumination problems. They apply JPEG-like image compression techniques to sets of texture maps. 
Their technique achieves compression rates for surface light .elds that are comparable to those of Levoy 
and Hanrahan s vector quan­tization method. Walter et al. [31] also use surface light .elds to approximate 
solutions to global illumination problems. Their rep­resentation involves basis functions derived from 
hardware lighting models, which provides very fast rendering, but does not support textured surfaces, 
nor can it adequately model complex phenomena such as rapidly varying specularity. In addition, problems 
exist in the 3D photography realm that do not arise with synthetic data: most importantly, neither a 
surface parameterization nor the radiance along arbitrary rays are known apriori and must instead be 
constructed. Nishino et al. [23, 24] generate surface light .elds of real objects, though their images 
are relatively dense in only one rotational di­rection. Geometric information is represented by a coarse 
triangular mesh. They construct a set of texture maps for each triangle by projecting each image onto 
the mesh. Compression is achieved by performing a principal component analysis on each set of textures. 
(Interestingly, the vectors in their analysis are formed by holding a direction .xed and letting surface 
location vary. This is the opposite of our analysis in Section 4.6, where, to form a vector, we .x a 
surface location and let direction vary.) Their approach successfully models objects with simple geometric 
structure and smoothly varying specularity. However, it has not been demon­strated on objects that exhibit 
both high geometric complexity and rapid BRDF variation, nor does it provide real-time rendering. Inverse 
rendering is an alternative to generating a surface light .eld. The goal of these techniques is to estimate 
the surface BRDF from images and geometric data. Previous work on inverse rendering [28, 33] has assumed 
that the BRDF is piecewise linear with respect to a coarse triangulation of the surface. Our techniques 
require no such assumptions, and, of course, inverse rendering does not solve the re-rendering problem 
a non-interactive global illumination algorithm is required to produce photorealistic results. Recent 
work has extended interactive rendering techniques to a wider range of lighting models and environments. 
Cabral et al. [3] describe a technique for using radiance environment maps to render objects under arbitrary 
lighting conditions and with any isotropic BRDF. Heidrich et al. [13] use texture mapping hardware for 
the same purpose but allow a different class of BRDFs. However, these two methods do not handle global 
effects like shadows or interre.ection. 1.2 Overview We have developed algorithms for acquiring light 
.eld data of real objects, and for estimating, compressing, rendering, and editing their surface light 
.elds. We have tested these algorithms on two objects, a small ceramic .sh with a shiny surface and detailed 
texture, and a marble elephant with more complex geometry and less pronounced specular highlights. The 
following sections describe these new algorithms in detail. We begin by describing our representation 
of surface light .elds (Sec­tion 2). Next, we discuss our data acquisition process (Section 3). We then 
describe our algorithms for estimating and compressing surface light .elds and compare the quality of 
these methods to two-plane light .elds of similar size (Section 4). Finally, we discuss our algorithms 
for rendering and editing surface light .elds (Sections 5 and 6), and present ideas for future research 
(Section 7).  2Representation Roughly speaking, a surface light .eld is a function that associates a 
color to every ray originating from a surface. Our algorithm for constructing images from a surface light 
.eld relies on a good parameterization of an object s surface mesh M. The methods of either Eck et al. 
[9] or Lee et al. [16] yield a parameterization . : K0 -M . IR3, (1) whose domain K0 is a triangular 
mesh with a small number of faces, called a base mesh. We use a variant of the algorithm of Lee et al. 
to parameterize our scanned geometry. The parameterization allows us to represent the surface light .eld 
as a function L : K0 × S2 -RGB , (2) where S2 denotes the sphere of unit vectors in IR3.Radiance is 
represented by points in IR3 corresponding to RGB triples. If u is a point on the base mesh and wis an 
outward pointing direction at the surface point .(u), then L(u, w) is the RGB value of the K0 M Base 
mesh Scanned geometry Li Lumisphere Figure 2 Representation of the surface light .eld. Points u on the 
base mesh, K0, are mapped to the geometric surface, M,by '. The lumisphere, Li at the grid point ui, 
represents the radiance leaving surface point '(ui). Directions are denoted by w,or . wafter re.ection 
through the surface normal n as described in Section 4.4. light ray starting at .(u) and traveling in 
direction w. Although L(u, w) has no physical interpretation when wis inward pointing, our compression, 
rendering, and editing techniques rely on L being de.ned over the entire direction sphere. We make the 
simplifying assumption that L(u, w) is piecewise linear in w. To make this more precise we have to de.ne 
what we mean by a piecewise-linear function on S2. It is not dif.cult to verify that the map (sin-1 px,sin-1 
py,sin-1 pz) h(w) = (3) | sin-1 px| + | sin-1 py| + | sin-1 pz| is a homeomorphism between S2 and the 
regular octahedron with vertices (±1, ±1, ±1). We use h because it introduces less distor­tion than radial 
projection and yet can be evaluated quickly using a lookup table for sin-1. Composition with h induces 
a bijection between functions on the octahedron and functions on the sphere. We say that a function F(w)is 
piecewise linear if it is piecewise linear with respect to an s-times-subdivided octahedron, i.e., the 
mesh resulting from s four-to-one subdivisions of the octahedron. We call a piecewise­linear RGB-valued 
function a lumisphere,and we let Cs denote PL the vector space of all lumispheres. With these de.nitions, 
the surface light .eld L can be represented by a function, whose domain is K0 and whose range is Cs PL 
,that sends a point u on K0 to the lumisphere L(u, ·). This de.nition can be described compactly in mathematical 
notation as follows: K0 -Cs : u -L(u, ·) (4) PL We have chosen subdivision level s = 3 in all our examples. 
In this case the space of lumispheres has dimension 3 × 258 = 774. We arrived at this value experimentally. 
Setting s = 2 results in noticeable degradation in the image quality, while s = 4 gives little improvement 
at the expense of higher dimension. It is useful to think of a surface light .eld as a lumisphere-valued 
texture map, which assigns a lumisphere instead of a single color to each texel. There is one rectangular 
texture map for each triangle in K0.The K0 triangle is mapped to the lower-left corner of its rectangle, 
and the upper right corner is unused. (For compactness we store pairs of texture maps interleaved in 
memory.) As in conventional texture mapping, each texture map is divided into square texels, and these 
texels de.ne a partition of each face of K0 into cells. The surface light .eld L is thus piecewise-constant 
with respect to this partition of K0.Let ui denote the center of the i-th cell. Cell dimensions (correspondng 
to the texture map resolution) are chosen so that the images .(ui)and .(uj) of any two adjacent grid 
points ui and uj are separated by at most one pixel in the image plane of each camera. We denote the 
lumisphere at the grid point ui by Li that is, Li(w) = L(ui, w). Figure 2 illustrates key aspects of 
our notation. 3 Data acquisition Acquiring the raw data to build a surface light .eld for a real object 
requires four steps: (1) range scanning the object, (2) building a mesh to represent its geometry, (3) 
capturing a collection of images of the object, and (4) registering the images to the mesh. Because the 
techniques presented in this paper do not depend on the speci.cs of our acquisition process, we present 
only a brief summary here of the procedure that we have used successfully. Range scanning. We took a 
number of range scans of each object using a Cyberware Model 15 scanner. Glossy objects like the .sh 
and elephant are not ideal candidates for laser scanning. To improve laser returns, we coated them with 
a removable powder. The .sh was built from 36 scans, and the elephant from 49. Reconstructing the geometry. 
The scans were registered using a small number of hand-selected point correspondences to initialize a 
global iterated closest-points algorithm [2, 10]. The registered scans were merged into a single triangle 
mesh using the volumetric method described by Curless and Levoy [6]. The .nal meshes representing the 
surfaces of the .sh and elephant contain 129,664 triangles and 311,376 triangles, respectively. Acquiring 
the photographs. We used a camera attached to a spher­ical gantry arm to capture photographs from poses 
spaced roughly evenly over the sphere. The camera positions were known relative to one another, but not 
relative to the objects being photographed. We took 638 photographs of the .sh and 388 photographs of 
the elephant, together with photographs of a calibration pattern, which we used to determine the intrinsic 
camera parameters using Tsai s method [30]. During acquisition, the camera and gantry arm occasionally 
cast shadows onto the object. Because we wanted to capture the object under .xed lighting conditions, 
we manually removed photographs taken under those circumstances. Registering the photographs to the geometry. 
We registered the set of photographs to the reconstructed mesh with user assistance. By hand-selecting 
correspondences between points on the mesh and points on a small subset of the photographs, we generated 
a list of 3D point-to-ray correspondences. We then registered the photographs to the geometry using an 
iterated closest-points algorithm. 4 Estimation and compression Once we have acquired the raw image 
and geometric data, we must estimate a surface light .eld that approximates that input. This section 
describes three estimation techniques; the latter two directly create compressed representations. 4.1 
Assembling data lumispheres The .rst step in the estimation process is the construction of a useful intermediate 
representation, consisting of a data lumisphere for each grid point in the surface light .eld. A data 
lumisphere is a set of samples from a full lumisphere, each consisting of a color and a direction corresponding 
to an observation of a grid point. We use Li to denote the data lumisphere associated with point ui on 
the base mesh. Assembling data lumispheres is a resampling problem that we solve separately for each 
grid point on the base mesh K0. Consider a .xed grid point ui,and let cij denote the RGB value of the 
point in the j-th photograph de.ned by the ray from .(ui)to the (a) (b) (c) Figure 3 Lumispheres from 
a point under the elephant s trunk. (The surface normal points directly out of the page.) The swath of 
missing points were occluded by the trunk. (a) Data lumisphere. (b) Faired piecewise-linear lumisphere. 
(c) Faired lumisphere with vertices shown as constant-colored Voronoi regions (used for illustration 
only). location of the j-th camera. The value cij is computed by bilinear interpolation in the j-th 
photograph. Some or all of the cij might be invalid because the point .(ui) may not be visible from the 
j-th camera position. If .(ui) is visible, we .nd the direction vector wij from .(ui) to the location 
of camera j and add the pair (cij, wij)to the data lumisphere Li for grid point ui. Figure 3(a) shows 
the data lumisphere for a point on our elephant. To determine if .(ui) is occluded with respect to the 
j-th camera, we render, from the perspective of that camera, both the original mesh M and additional 
geometry that conservatively bounds the platform on which the object rests. (Because the platform obscures 
parts of the objects in some photographs, we add geometry representing the platform to ensure that we 
do not project the platform onto the object.) The depth buffer gives us a depth image, which we compare 
to the depth of each point .(ui) to determine if it is visible. 4.2 Pointwise fairing Our .rst estimation 
algorithm, pointwise fairing, constructs a piecewise-linear lumisphere from each data lumisphere indepen­dently 
at each surface point. If the data covered the entire direction sphere, we could estimate Li using the 
standard least-squares proce­dure of setting Li to be the lumisphere in Cs PL that best approximates 
the data lumisphere: Li =argmin Edist(F, Li) (5) FECs PL The argmin notation evaluates to the value 
of its subscript that minimizes the expression to which it is applied. Here, F is a lumisphere, and Edist(F, 
Li) measures how well F approximates Li: . Edist(F, Li) = 1 |F(wij) - cij|2 (6) |Li| jEvisible cameras 
 where |Li| is the number of observed color values in the data lumisphere Li. But the physical light 
.eld at any point on the surface is only de­.ned on the hemisphere of outward pointing directions. Moreover, 
due to self-occlusion and constraints on the camera poses, the data samples often do not cover the entire 
direction hemisphere (see Figure 3). The .tting problem (Equation (6)) is under-determined, and it is 
therefore necessary to regularize it by adding a fairing term. We use a discrete approximation to the 
thin-plate energy: . Ethin(F) = Ns | PLF(wk)|2 . (7) 4 k The sum ranges over the vertices of the s-times-subdivided 
octa­hedron (with Ns vertices, each corresponding to a direction wk), and PL denotes the umbrella Laplacian 
[29]. The regularized error function is then E (F, Li) = Edist(F, Li)+ A Ethin(F). (8) We use conjugate 
gradients to .nd the lumisphere F that minimizes Equation (8). Figures 3(b) and 3(c) show the faired 
lumisphere generated from the data lumisphere in Figure 3(a). The fairing term dampens the directional 
variation in the .tted lumisphere. It has little physical signi.cance, and our data is relatively free 
of noise; we therefore choose A small so that Edist dominates. Note that our fairing procedure assigns 
values to L(u, w) at all directions w. S2, including directions far away from any observations, and even 
directions pointing into the object. Figure 4 illustrates the effects and the limitations of pointwise 
fairing. Figure 4(a) shows one of the actual photographs of the .sh, and Figure 4(b) shows the same view 
of the uncompressed light .eld generated from all the photographs. The light .eld rendered in Figure 
4(c) was generated after all photographs from viewpoints inside a cone of radius 10° about the viewing 
direction were removed. There is little degradation. In Figure 4(d) the radius of the cone was increased 
to 20°. Clearly the gap in directions has become too large for pointwise fairing to accurately approximate 
the actual surface light .eld. 4.3 Compression overview Because Cs is a high-dimensional space, a complete 
pointwise- PL faired surface light .eld may be very large. To generate a more compact surface light 
.eld, we will represent each lumisphere as a weighted sum of a small number of prototype lumispheres 
using two distinct methods, one analogous to vector quantization, and the other analagous to principal 
component analysis. Each lumisphere Li can then be replaced by an index (as in vector quantization) or 
a set of coef.cients (as in principal component analysis) indicating contributions from the prototypes. 
A naive application of vector quantization or principal component analysis might treat as input the pointwise-faired 
lumispheres viewed as vectors in the space Cs Observe, however, that the PL. RGB values for at least 
half of each lumisphere corresponding to directions pointing into the object are mostly .ction generated 
by the fairing process. If we were to apply vector quantization or principal component analysis to the 
pointwise-faired lumispheres, these fabricated values would have the same in.uence as values in directions 
where we actually have data. This is clearly undesirable. A more principled compression approach would 
use only observed data. The data, however, is an irregular and incomplete sampling of each lumisphere. 
We have therefore developed two new esti­mation/compression methods, function quantization and principal 
function analysis, which are similar in spirit to vector quantization and principal component analysis, 
but are driven by irregularly spaced data and avoid the intermediate pointwise-fairing step. Before discussing 
our compression algorithms, we present two transformations of the surface light .eld that increase spatial 
coher­ence among lumispheres, thereby making them more compressible. 4.4 Median removal and re.ection 
The .rst transformation is median removal.Let mi denote the RGB value obtained by computing the median 
color of data lumisphere Li (separately for each color channel). We use the median rather than the mean 
because it is robust against outliers and more accurately represents the bulk of the data. The collection 
of median values can be viewed as a texture map over the surface, roughly encoding the diffuse component 
of the surface light .eld. We store this diffuse texture map separately and then encode the residual 
 (a) (b) (c) (d) (e) Figure 4 Analysis of estimation with missing data. (a) Photograph taken by a selected 
camera. (b) Faired surface light .eld using all photographs. (c) Faired surface light .eld after .rst 
removing from the input data all photographs in a cone of radius 10° about the direction shown. (d) Faired 
surface light .eld with a cone of radius 20° removed. (e) Compressed surface light .eld with principal 
function analysis of order 3 after .rst removing the same cone of radius 20°. Note that the compressed 
surface light .eld reproduces the specularity of the input better than the pointwise-faired version when 
a signi.cant portion of the input data is removed. (a) (b) (c) Figure 5 Increasing lumisphere coherence 
via re.ection reparam­eterization. (a) Surface light .eld. (b) Transect of L(u, w). (c) Transect of . 
L(u, . w). Horizontal axis shows position of u along white line across .sh (a). Vertical axis shows position 
of w(b) or w .(c) on a user-selected great circle. Note that in the right panel the specular highlights 
are much better aligned. surface light .eld after subtracting the diffuse component. This serves two 
purposes. First, if we compress only the residual surface light .eld, any diffuse texture will be exactly 
preserved. Second, the residual will be more compressible if the specular behavior of the surface is 
simpler than the diffuse (e.g., an object with diffuse texture and a glossy coat.) Median removal before 
compression is analogous to mean-removed vector quantization [11]. The second transformation, re.ection, 
is a reparametrization of the lumispheres. Let n be the unit surface normal at a surface. Then for a 
direction w. S2,let w.be the re.ection of wabout the normal n (transformed quantities will always be 
denoted with a tilde . ): w.= 2(n · w) n - w. (9) Similarly, the re.ected (and median-removed) surface 
light .eld .L is de.ned at each grid point by: . Li(w.) = Li(w) - mi . (10) Where, by Equation (9), 
wis w.re.ected around the surface normal, ni,atthe i-th grid point. Obviously, .L (plus the diffuse texture 
map) contains the same information as L. To see why we expect the re.ected reparameterization to increase 
spatial coherence, consider the three elements that determine the lumisphere Li at a point: the incoming 
radiance, the BRDF and the normal. First, assume that the incoming radiance at two points ui and uj is 
the same; this is approximately true for points that are nearby relative to the sources of light illuminating 
them. Second, assume that the BRDF is re.ective. A re.ective BRDF [3] is one that re.ects the incoming 
radiance through the surface normal and then convolves with a direction-invariant .lter (i.e., a space-invariant 
.lter, where space is restricted to the surface of the sphere of directions S2). As observed by Rusinkiewicz 
[27], many BRDFs are approximately re.ective. If these two assumptions hold, the re.ected lumispheres 
.Li and .Lj will be the same even if the normals ni and nj are different. For an example, consider the 
case of a perfect mirror surface and an environment that is in.nitely far away. Ignoring non-local effects 
such as occlusions and interre.ections, all of the reparametrized lumispheres will agree on their overlap 
because they contain parts of the same environment map. If the surface had some roughness, then the lumispheres 
would be blurred, re.ected images of the environment, but they would still roughly agree on the overlap. 
Figure 5 illustrates the effect of reparameterization for the .sh, whose environment consists of several 
small light sources. We always estimate and store median-removed and re.ected lu­mispheres; however, 
the transformations have no effect on the pointwise-fairing algorithm. 4.5 Function quantization Function 
quantization is a generalization of vector quantization to the case of irregularly sampled data. The 
goal is to construct a codebook comprised of a collection of prototype lumispheres (codewords) {P0, ..., 
Pn} and a map assigning to each grid point ui . K0 a codeword index ki, and thereby a codeword Pki .For 
agiven n, the codebook and map should minimize the combined . energy over all data lumispheres, i.e., 
i E (Pki , L.i). This formu­lation is different from vector quantization in that the inputs (data lumispheres) 
are not vectors. Function quantization starts with an initial codebook consisting of a single lumisphere 
and a small training set of randomly selected grid points. It proceeds by alternating between codebook 
.tting and codeword splitting, until the codebook reaches a user-speci.ed size. Codebook .tting is accomplished 
via Lloyd iteration [11], i.e., by repeatedly applying the following two steps: 1. Projection: For each 
grid point ui in the training set, .nd the index ki of the closest codeword: ki =argmin E (Pk, L.i). 
(11) k This partitions the training set into clusters of grid points that project to the same codeword. 
2. Optimization: For each cluster, .nd the best piecewise-linear lumisphere: . Pk =argmin E (F, L.i) 
, (12) FECs PL iEcluster k where the summation is over all of the data lumispheres L.i in the k-th cluster. 
 We perform the optimization steps using conjugate gradients. The iteration terminates when the decrease 
in error between succes­sive codebooks falls below a user-de.ned threshold. Then, if the (a) (b) (c) 
(d) Figure 6 Comparison of different estimation techniques applied to the .sh. (a) Pointwise faired surface 
light .eld. (b) Function quantization with 1024 codewords. (c) Principal function analysis with subspace 
dimension 2. (d) Principal function analysis with subspace dimension 5. Figure 7 Comparison of compressed 
elephant surface light .eld with input photographs. Left: Elephant photographs. Right: Ele­phant surface 
light .eld (5.3 megabytes encoded with principal function analysis, subspace dimension q = 2). Note that 
the image on the bottom right shows a part of the elephant that was occluded in the corresponding photograph. 
Also note that some points on the very bottom of the elephant were not seen by any camera (using our 
conservative approximation of the platform) and are black. codebook is smaller than desired, codeword 
splitting doubles the codebook size by cloning each codeword and adding a small perturbation to the clone. 
After a codebook of the desired size has been found, codewords are assigned to all grid points by projecting 
all the corresponding data lumispheres (not just those in the training sample) onto the codebook. 4.6 
Principal function analysis Principal function analysis, based on principal component analysis, is an 
alternative to function quantization. For a given set of data vectors and a given approximation dimension 
q, principal com­ponent analysis .nds the q-dimensional af.ne subspace that best approximates the data 
vectors in the least squares sense. As in the case of function quantization, we must generalize this 
approach to the case of irregularly sampled data. Cs Our goal, then, is to .nd the q-dimensional subspace 
V . PL that best approximates all of the data lumispheres in the training set. Each lumisphere .Li is 
represented by the point F . V that minimizes E (F, L.i). We call F the projection of L.i onto V,or V 
(L.i). Overloading E , we view it as a function of q-dimensional subspaces of Cs PL; it measures how 
well a subspace approximates the data lumispheres in the training set, i.e., . 1 E (V) = E ( V (L.i), 
L.i) . (13) T i The summation is over all grid point indices in the training set, and T is the size 
of the training set. While principal component analysis reduces to an eigenvalue prob­lem, we have not 
succeeded in .nding a corresponding formulation for minimizing the functional de.ned in equation (13). 
We have therefore taken a different approach. Each q-dimensional af.ne subspace of Cs PL can be expressed 
as the af.ne span of q + 1 prototype functions, and E can be regarded as a functional on the space of 
(q + 1)-tuples of prototypes. Since E depends only on the af.ne span of the prototypes, minimizing E 
will not uniquely determine the prototypes. To address the uniqueness problem, we consider a new functional: 
. E ,µ(P0, ..., Pq) = E (V)+ µ .Pk - Pmean.2 (14) kE0,...,q where P0, ..., Pq are the prototypes de.ning 
V,and Pmean is their mean, and where the projection V (L.i) of a data lumisphere L.i is restricted to 
lie inside the convex hull of the prototypes. (The squared norm of a lumisphere, .F.2, is the sum of 
the squared norms of the vertex values divided by the number of vertices.) This additional spring energy 
term penalizes widely-spaced prototypes. Minimizing it is a non-linear optimization problem, which we 
solve via conjugate gradients. After the subspace has been determined by selection of the prototypes, 
we assign barycentric coordinates to all grid points by projecting all corresponding data lumispheres 
(again, not just those in the training sample) onto the subspace. (a) (b) Figure 8 Comparison of a surface 
light .eld with a geometry-corrected two-plane light .eld. (a) Photograph. (b) Surface light .eld pointwise 
faired (180 MB). (c) Two-plane light .eld un­compressed (180 MB). (c)  4.7 Compression results We tested 
the various estimation and compression algorithms on the surface light .elds of both the .sh and elephant. 
Figure 6 compares results of the different methods. Figure 6(a) shows two views of the uncompressed (pointwise-faired) 
.sh, the entire model (top) and a closeup of the tail .n (bottom). This data set contains 176 MB of color 
data, plus 0.7 MB for geometry and normals. Figure 6(b) demonstrates function quantization with 1024 
codewords, resulting in a color data size of 2.7 MB. Figures 6(c) and (d) illustrate principal function 
analysis with subspace dimensions 2 and 5, resulting in color data sizes of 1.7 MB and 2.3 MB, respectively. 
Note that the 2-dimensional principal function analysis example, with its total .le size of 2.4 MB (1.7 
MB color + 0.7 MB geometry), results in more than 70:1 compression. Overall, principal function analysis 
leads to smoother images than function quantization; function quantization introduces artifacts such 
as jagged edges on the .sh s tail. However, function quantiza­tion is more accurate, better preserving 
the color of highlights and effects such as interre.ections that are lost during principal function analysis. 
Not surprisingly, increasing the dimension of the subspace in principal function analysis improves the 
quality of the results; e.g., dimension 5 produces highlights substantially sharper and brighter than 
dimension 2. Rendering time, however, is asymptotically linear in the dimension q. Currently, other costs 
dominate when the dimension is low, and in our examples, dimensions 2 and 5 can be rendered at roughly 
the same speed. By contrast, the rendering time for a function-quantized surface light .eld is independent 
of codebook size (ignoring the effect of the memory hierarchy). The complementary strengths of function 
quantization and principal function analysis suggest a hybrid approach (see Section 7). We achieved similar 
compression results with the elephant. A pointwise-faired elephant requires 409 MB of color data and 
1.6 MB of geometric data. Applying principal function analysis with a 2-dimensional subspace compresses 
the elephant s color data to 3.7 MB. Figure 7 compares synthesized images of the elephant with the photographs. 
The compressed surface light .eld captures most of the features of the input data, but the highlights 
are less bright. In (d) (e) (d) Surface light .eld compressed us­ing principal function analysis of 
di­mension 5 (2.5 MB). (e) Surface light .eld compressed using function quanti­zation with 1024 codewords 
(2.7 MB). (f) Two-plane light .eld compressed using vector quantization with 16384 codewords (8.1 MB). 
  (f) addition to the lower dimension of the subspace, the lower .delity may be a result of the fact 
that the scanned geometry of the elephant appeared to be of lower quality than that of the .sh. Errors 
in the geometry, particularly the normals, adversely affect the quality of the compression. Note also 
that, even though the bottom reconstruc­tion includes the feet of the elephant, which were not visible 
in the corresponding photograph, our compression algorithm succeeds in inferring plausible shading and 
highlights for that part of the model. The compressed representation is essentially a learned model of 
lumispheres; the unseen portions of data lumispheres are .lled in by .nding the closest lumisphere in 
the model. Figures 4(d) and (e) also show that principal function analysis can produce more realistic 
highlights than pointwise fairing given incomplete data lumispheres. We have done an informal comparison 
of image quality between a surface light .eld and a two-plane light .eld. We constructed a two-plane 
light .eld of the .sh with six slabs arranged along the faces of a cube. The resolution of the light 
.eld, 4002 for the far plane and 82 for the near plane, was chosen to approximately match the corresponding 
resolutions of the surface light .eld: the far-plane resolution matches the input photograph resolution, 
and the near-plane resolution approximately matches the surface light .eld s directional resolution. 
The resulting raw data size is about 180 MB, the same size as our pointwise-faired (i.e., uncompressed) 
surface light .eld. The input images were resampled into the two­plane parameterization of.ine using 
the scanned .sh geometry and view-dependent texture mapping. We then compressed the data using the vector 
quantization technique (and software) of Levoy and Hanrahan [17], using their default settings: a codebook 
of 16384 2×2×2×2×3 codewords (i.e., 2×2 camera positions, 2×2 image pixels and 3 color channels). All 
of the renderings of two-plane light .elds use the geometry correction technique of Gortler et al. [12]. 
Figure 8 compares images generated from uncompressed and com­pressed surface light .elds with corresponding 
images generated from the two-plane light .eld. The uncompressed data sets give reproductions of similar 
quality, although the two-plane light .eld s quadralinear interpolation has different .ltering characteristics. 
When compressed, the surface light .eld produces more compelling reproductions even though the compressed 
two-plane light .eld data (8.1 MB + geometry) is more than 3 times the size of the compressed surface 
light .eld (2.5 MB + geometry). False color Light field Model Faces (secs/frame) (secs/frame) top uniform 
102,400 0.36 0.59 bottom LOD 5823 0.07 0.31 Figure 9 View-dependent level-of-detail. Left: Geometry visualiza­tion. 
Right: Surface light .eld. Top: Uniform subdivision, r =4. Bottom: View-dependent level-of-detail with 
error terms chosen to match the uniform subdivision. Shown in the table, very bottom, are rendering times, 
.rst for false color only (the step that uses geometry), and second for the entire surface light .eld 
rendering algorithm. The near-plane resolution of the two-plane light .eld we con­structed, though comparable 
in angular resolution to our surface light .eld, is lower than those demonstrated by Gortler et al. and 
Levoy and Hanrahan. We have observed that lowering this resolution results in artifacts such as erroneous 
interpolation among rays that strike quite different surface points due to occlusion. Azuma [1] discusses 
this effect and other dif.culties inherent in reduction of the near-plane resolution.  5 Rendering In 
this section we present an interactive surface light rendering al­gorithm. Our implementation runs entirely 
in software and achieves interactive rates on PC-class hardware without 3D acceleration. 5.1 Basic algorithm 
Rendering a surface light .eld from an arbitrary viewpoint is conceptually straightforward. Each pixel 
in the image plane of the camera de.nes an incoming ray in some direction w. Suppose the ray intersects 
the mesh at a point .(ui), corresponding to a point ui . K0. Then the RGB value of the pixel is L(ui, 
w). Since we actually encode the reparameterized surface light .eld .L(ui, w.)at each point, we must 
re.ect the viewing ray about the normal before looking up the RGB value. To facilitate this process, 
we compute and store a normal map n(u) over the surface, so that we can quickly determine the normal 
n(ui) at a grid point. We render the surface light .eld in two passes. In the .rst pass we determine, 
for each pixel of the virtual camera, the point ui corresponding to the surface point .(ui) seen at that 
pixel, encoded as a face ID and barycentric coordinates. We do this ef.ciently by rendering the mesh 
in false color with Gouraud shading, using two of the framebuffer s four color channels to encode the 
index of the base mesh face, and the remaining two to encode the barycentric coordinates within the face. 
In the second pass, we scan the frame buffer. For each pixel in the virtual camera we incrementally compute 
the direction wof the incoming (viewing) ray using a single 3-vector addition at each pixel. We compute 
w.by re.ecting through the surface normal n(ui) at .(ui). Finally, we evaluate .L(ui, w.) by looking 
up the lumisphere associated with ui and evaluating the piecewise-linear function in direction space. 
These operations can be done quickly with just a few .oating-point operations. 5.2 View-dependent re.nement 
of geometry One feature of the surface light .eld representation is the de­coupling of the surface geometry 
from the light .eld. For best results, we can render the surface geometry at the highest resolution during 
the .rst pass of the rendering algorithm, but this can be costly. Alternatively, we can render a simpli.ed 
mesh (e.g., the embedding of the base mesh triangles in IR3) and still achieve a compelling result because 
surface light .elds, like bump-mapped lighting, suggest more geometric detail than is actually present. 
However, this simpli.ed mesh introduces some distortion; more­over, the coarse silhouettes are often 
objectionable. Instead, we have explored a middle ground between those two extremes: view­dependent re.nement 
of the subdivision-connectivity surface mesh. Most current methods for real-time, view-dependent simpli.cation 
of geometry, such as those presented by Hoppe [15] and Xia and Varshney [32], employ progressive mesh 
representations and adapt the level of detail using edge collapses and vertex splits. For a texture-mapped 
surface, however, these operations can cause considerable parametric distortion, especially near the 
boundaries of parameter domains, placing signi.cant constraints on the simpli­.cation [5]. Therefore, 
we restrict the mesh used for rendering to have four-to-one subdivision connectivity [18], and re.ne 
the mesh by adding and removing lazy wavelets [4]. This allows us to modify the geometric detail almost 
independently of the parameterization. We approximate the map . : K0 -M . IR3 by a piecewise­linear map 
.r : Kr -IR3 on the simplicial complex Kr obtained by applying r four-to-one subdivisions to the base 
complex K0 and setting .r (v)= .(v) for each vertex v of Kr. The subdivision level r is a user-de.ned 
parameter (r = 4 in Figure 9). We then compute the lazy-wavelet expansion of .r, expressing it as a sum 
of hat functions. Adapting the mesh can now be formulated as .nding a partial sum of those hat functions, 
satisfying a set of view­dependent properties. (a) Figure 10 (a) Lazy wavelet addition and subtraction. 
The support of the added hat function is shown in blue. (b) T-vertices (circled in red) are eliminated 
by adding edges. The retriangulation procedure is an incremental algorithm that ex­ploits frame-to-frame 
coherence, similar to algorithms described by Hoppe [15] and Xia and Varshney [32]. To compute the approxima­tion 
for a frame, we begin with the approximation computed for the previous frame and modify it by applying 
the lazy-wavelet addition and lazy-wavelet subtraction operations, illustrated in Figure 10(a), according 
to view-dependent criteria. To reduce the appearance of popping, we spread the visual effect of each 
operation over time by geomorphing [14]. In a second quick pass over the mesh, we add temporary edges 
to eliminate cracks caused by T-vertices, as shown in Figure 10(b). Our criteria for wavelet addition 
and subtraction are the same three view-dependent re.nement criteria described by Hoppe [15]: (1) removing 
wavelets that are completely backfacing, (2) removing wavelets lying completely outside the view frustum, 
and (3) main­taining a screen-space error bound. To accelerate computation of screen-space error, we 
construct, in preprocessing, a bounding volume around the set of geometric error vectors associated with 
a wavelet addition. We have found that an ellipsoid aligned to the surface normal generally provides 
a tighter bound than the shape used by Hoppe, while not adding signi.cantly to the cost of projecting 
the error volume. Because coarse silhouettes tend to be more noticeable than interior distortion, we 
use a smaller error tolerance near the silhouette [19]. Finally, to reduce the number of wavelet addition 
and subtraction operations that must be considered, we enforce one additional property: A hat function 
at level e : r, centered at an edge of K£-1, may appear in the sum only if the hat functions centered 
at the endpoints of the edge appear in the sum. The results of view-dependent level-of-detail are illustrated 
in Fig­ure 9, showing a close-up of the elephant s trunk. While achieving high accuracy, the top renderings 
using uniform subdivision render fairly slowly due to the large number of triangles. The bottom renderings, 
using the view-dependent level-of-detail algorithm with error threshholds set to match the .ne geometry 
renderings, are obtained with far fewer triangles yielding moderately improved frame rates with little 
visual difference. The close-up views shown in Figure 9 bene.t greatly from the view frustum test, which 
causes a considerable fraction of the model to be coarsened. In the other common case, where the entire 
model is visible, using view-dependent level-of-detail does not give as signi.cant of a performance bene.t, 
but it does no worse than a static model. Of course, if the model is very distant, the level-of­detail 
algorithm will generate a very coarse approximation.  Editing Just as the decoupling of surface geometry 
and the light .eld allows us to re.ne the geometry independently, we are now able to perform editing 
operations that are not commonly possible in an image­based rendering context. In this section we describe 
three such operations: lumisphere editing, rotating the object relative to its environment, and deforming 
the geometry. By performing simple image processing directly on the lumi­spheres, we can simulate changes 
in surface properties, such as sharpening of specular highlights. We demonstrate this particular operation 
in Figures 11(a) and (b), where the highlights in the original rendering (a) have been brightened and 
sharpened (b). We achieve this effect by applying Perlin s bias function [25] to the val­ues of every 
lumisphere. For compressed surface light .elds, we can quickly approximate this by adjusting the prototype 
lumispheres. (For principal function analysis, this is only an approximation because the bias function 
is non-linear.) The other two editing operations we illustrate, rotation of geometry relative to its 
environment and general deformation, .t into one conceptual framework: a transformation is applied to 
de.ne a new surface. The new surface can be represented by a modi.ed embedding of the base mesh .' : 
K0 -IR3. (Rotation is just a special case of general deformation.) Our goal then is to compute the corresponding 
surface light .eld L'(u, w), and our solution is operationally very simple. We compute the new surface 
normal .eld n'(u)and then set L'(u, w)= .L(u, w.'), where w.' is the re.ection of wthrough the new normal. 
Figures 11 and 12 demonstrate the geometric edits. Figure 11(a) shows the original elephant; (c) and 
(d) show the elephant rotated relative to its environment. Figure 12(a) shows the .sh as it was originally; 
Figure 12(b) shows it after deformation, with its head bent to the side.   Our method for computing 
the new surface light .eld L' from the original L is justi.ed if the environment is in.nitely far away, 
if there is no occlusion, shadowing or interre.ection, and if the BRDFs for all surface points are re.ective. 
These are the same assumptions that motivate our re.ection transformation described in Section 4.4. Even 
if all of these requirements are met, there is an additional problem. For any grid point ui . K0, the 
camera directions represented in the data lumisphere fall inside a hemi­sphere. After editing, however, 
there will in general be viewing directions that require values of L'(ui, w) for directions outside this 
hemisphere. In fact, if we rotate the object by 180 degrees, we will need values exactly on the opposite 
hemisphere. Operationally, however, inferring these values is not a problem. The estimation techniques 
guarantee that lumispheres are well-de.ned everywhere, albeit not necessarily realistic. 7 Future work 
We envision a number of areas for future work: Combining function quantization and principal function 
analy­sis. Our two compression methods can be considered extrema of a spectrum: Function quantization 
.ts the data by a collection of 0-dimensional spaces, whereas principal function analysis uses a single 
higher-dimensional space. We could do both: .t a collection of higher dimensional spaces. That approach 
might work well if the data lumispheres lie on a low-dimensional curved manifold in lumisphere space. 
Wavelet representation of a surface light .eld. Constructing a wavelet expansion of the surface light 
.eld L(u, w) might re­sult in better compression than function quantization or principal function analysis, 
and would support progressive transmission and performance-tuned rendering [4]. Hole .lling using texture 
synthesis. We have no method for assigning lumispheres to surface points not visible in any of the cameras, 
like those on the bottom of the elephant in Figure 7. A texture synthesis algorithm, suitably extended 
to operate on lumispheres instead of colors and with textures de.ned on general surfaces instead of the 
plane, could be used to .ll these holes.  Acknowledgements We would like to thank Marc Levoy for the 
use of the Stanford spherical gantry and other equipment. This work was supported by an NSF grant (DMS-9803226) 
and an Osberg Family Fellowship, as well as industrial gifts from Intel, Microsoft, and Pixar. References 
[1] D. I. Azuma. Interactive Rendering of Surface Light Fields. Technical Report UW-CSE-2000-04-01, Department 
of Computer Science and Engineering, University of Washington, April 2000. [2] P. J. Besl and N. D. McKay. 
A Method for Registration of 3-D Shapes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 
14(2):239 256, February 1992. [3] B. Cabral, M. Olano, and P. Nemec. Re.ection Space Image Based Rendering. 
In SIGGRAPH 98 Conference Proceedings, Annual Conference Series, pages 165 170, August 1999. [4] A. Certain, 
J. Popovi´c, T. DeRose, T. Duchamp, D. Salesin, and W. Stuetzle. Interactive Multiresolution Surface 
Viewing. In SIGGRAPH 96 Conference Proceedings, Computer Graphics Annual Conference Series, pages 91 
98, August 1996. [5] J. Cohen, M. Olano, and D. Manocha. Appearance-Preserving Simpli.cation. In SIGGRAPH 
98 Conference Proceedings, Annual Conference Series, pages 115 122. ACM SIGGRAPH, July 1998. [6] B. Curless 
and M. Levoy. A Volumetric Method for Building Complex Models from Range Images. In SIGGRAPH 96 Conference 
Proceed­ings, Annual Conference Series, pages 303 312, August 1996. [7] P. E. Debevec, C. J. Taylor, 
and J. Malik. Modeling and Rendering Architecture from Photographs: A Hybrid Geometry-and Image-Based 
Approach. In SIGGRAPH 96 Conference Proceedings, Annual Conference Series, pages 11 20, August 1996. 
[8] P. E. Debevec, Y. Yu, and G. D. Borshukov. Ef.cient View-Dependent Image-Based Rendering with Projective 
Texture-Mapping. Eurographics Rendering Workshop 1998, pages 105 116, June 1998. [9] M. Eck, T. DeRose, 
T. Duchamp, H. Hoppe, M. Lounsbery, and W. Stuetzle. Multiresolution Analysis of Arbitrary Meshes. In 
SIGGRAPH 95 Conference Proceedings, Annual Conference Series, pages 173 182, August 1995. [10] H. Gagnon, 
M. Soucy, R. Bergevin, and D. Laurendeau. Registration of Multiple Range Views for Automatic 3-D Model 
Building. In IEEE Conf. Computer Vision and Pattern Recognition, pages 581 586, June 1994. [11] A. Gersho 
and R. M. Gray. Vector Quantization and Signal Compres­sion. Kluwer Academic Publishers, 1991. [12] S. 
J. Gortler, R. Grzeszczuk, R. Szeliski, and M. F. Cohen. The Lumigraph. In SIGGRAPH 96 Conference Proceedings, 
Annual Conference Series, pages 43 54, August 1996. [13] W. Heidrich and H.-P. Seidel. Realistic, Hardware-Accelerated 
Shading and Lighting. Proceedings of SIGGRAPH 99, pages 171 178, August 1999. [14] H. Hoppe. Progressive 
Meshes. In SIGGRAPH 96 Conference Proceedings, Annual Conference Series, pages 99 108, August 1996. [15] 
H. Hoppe. View-Dependent Re.nement of Progressive Meshes. In SIGGRAPH 97 Conference Proceedings, Annual 
Conference Series, pages 189 198, August 1997. [16] A. W. F. Lee, W. Sweldens, P. Schroeder, L. Cowsar, 
and D. Dobkin. MAPS: Multiresolution Adaptive Parameterization of Surfaces. In SIGGRAPH 98 Conference 
Proceedings, Annual Conference Series, pages 95 104, July 1998. [17] M. Levoy and P. Hanrahan. Light 
Field Rendering. In SIGGRAPH 96 Conference Proceedings, Annual Conference Series, pages 31 42, August 
1996. [18] M. Lounsbery, T. D. DeRose, and J. Warren. Multiresolution Analysis for Surfaces of Arbitrary 
Topological Type. ACM Transactions on Graphics, 16(1):34 73, January 1997. [19] D. Luebke and C. Erikson. 
View-Dependent Simpli.cation of Arbi­trary Polygonal Environments. In SIGGRAPH 97 Conference Pro­ceedings, 
Annual Conference Series, pages 199 208, August 1997. [20] M. Magnor and B. Girod. Adaptive Block-Based 
Light Field Coding. Proc. 3rd International Workshop on Synthetic and Natural Hybrid Coding and Three-Dimensional 
Imaging, pages 140 143, September 1999. [21] M. Magnor and B. Girod. Hierarchical Coding of Light Fields 
with Disparity Maps. Proc. IEEE International Conference on Image Processing, pages 334 338, October 
1999. [22] G. S. P. Miller, S. Rubin, and D. Ponceleon. Lazy Decompression of Surface Light Fields for 
Precomputed Global Illumination. Euro­graphics Rendering Workshop 1998, pages 281 292, June 1998. [23] 
K. Nishino, Y. Sato, and K. Ikeuchi. Appearance compression and synthesis based on 3D model for mixed 
reality. In Proceedings of IEEE ICCV 99, pages 38 45, September 1999. [24] K. Nishino, Y. Sato, and 
K. Ikeuchi. Eigen-Texture Method: Appear­ance Compression based on 3D Model. Proc. of Computer Vision 
and Pattern Recognition, 1:618 624, June 1999. [25] K. Perlin and E. M. Hoffert. Hypertexture. Computer 
Graphics (Proceedings of SIGGRAPH 89), 23(3):253 262, July 1989. [26] K. Pulli, M. Cohen, T. Duchamp, 
H. Hoppe, L. Shapiro, and W. Stuet­zle. View-based Rendering: Visualizing Real Objects from Scanned Range 
and Color Data. Eurographics Rendering Workshop 1997, pages 23 34, June 1997. [27] S. M. Rusinkiewicz. 
A New Change of Variables for Ef.cient BRDF Representation. In Eurographics Rendering Workshop 1998, 
pages 11 22. Eurographics, June 1998. [28] Y. Sato, M. D. Wheeler, and K. Ikeuchi. Object Shape and Re.ectance 
Modeling from Observation. In SIGGRAPH 97 Conference Proceed­ings, Annual Conference Series, pages 379 
388, August 1997. [29] G. Taubin. A Signal Processing Approach to Fair Surface Design. In SIGGRAPH 95 
Conference Proceedings, Annual Conference Series, pages 351 358. ACM SIGGRAPH, August 1995. [30] R. Y. 
Tsai. An Ef.cient and Accurate Camera Calibration Technique for 3D Machine Vision. In Proceedings of 
IEEE Conference on Computer Vision and Pattern Recognition, pages 364 374, 1986. [31] B. Walter, G. Alppay, 
E. P. F. Lafortune, S. Fernandez, and D. P. Greenberg. Fitting Virtual Lights For Non-Diffuse Walkthroughs. 
In SIGGRAPH 97 Conference Proceedings, Annual Conference Series, pages 45 48, August 1997. [32] J. C. 
Xia and A. Varshney. Dynamic View-Dependent Simpli.cation for Polygonal Models. In IEEE Visualization 
96. IEEE, October 1996. [33] Y. Yu, P. Debevec, J. Malik, and T. Hawkins. Inverse Global Illumination: 
Recovering Re.ectance Models of Real Scenes From Photographs. In SIGGRAPH 98 Conference Proceedings, 
Annual Conference Series, pages 215 224, August 1999.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344929</article_id>
		<sort_key>297</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>33</seq_no>
		<title><![CDATA[Dynamically reparameterized light fields]]></title>
		<page_from>297</page_from>
		<page_to>306</page_to>
		<doi_number>10.1145/344779.344929</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344929</url>
		<abstract>
			<par><![CDATA[<p>This research further develops the light field and lumigraph image-based rendering methods and extends their utility. We present alternate parameterizations that permit 1) interactive rendering of moderately sampled light fields of scenes with significant, unknown depth variation and 2) low-cost, passive autostereoscopic viewing. Using a dynamic reparameterization, these techniques can be used to interactively render photographic effects such as variable focus and depth-of-field within a light field. The dynamic parameterization is independent of scene geometry and does not require actual or approximate geometry of the scene. We explore the frequency domain and ray-space aspects of dynamic reparameterization, and present an interactive rendering technique that takes advantage of today's commodity rendering hardware.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[autostereoscopic displays]]></kw>
			<kw><![CDATA[depth of field]]></kw>
			<kw><![CDATA[frequency domain analysis]]></kw>
			<kw><![CDATA[image-based rendering]]></kw>
			<kw><![CDATA[light field]]></kw>
			<kw><![CDATA[lumigraph]]></kw>
			<kw><![CDATA[multitexturing]]></kw>
			<kw><![CDATA[ray space analysis]]></kw>
			<kw><![CDATA[synthetic aperture]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Viewing algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Graphics data structures and data types</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010394</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics file formats</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P9997</person_id>
				<author_profile_id><![CDATA[81100458519]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Aaron]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Isaksen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Laboratory for Computer Science, Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14058906</person_id>
				<author_profile_id><![CDATA[81100137780]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Leonard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McMillan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Laboratory for Computer Science, Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P269895</person_id>
				<author_profile_id><![CDATA[81100259454]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Steven]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Gortler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Division of Engineering and Applied Sciences, Harvard University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Nicholas Ayache and Charles Hansen. Rectification of images for binocular and trinocular stereovision. In ICPR88, pages 11-16, 1988.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[R.C. Bolles, H.H. Baker, and D.H. Marimont. Epipolar-plane image analysis: An approach to determining structure from motion. IJCV, 1(1):7-56, 1987.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Emilio Camahort, Apostolos Lerios, and Donald Fussell. Uniformly sampled light fields. Eurographics Rendering Workshop 1998, pages 117-130, 1998.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>344932</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[J.-X. Chai, X. Tong, S.-C. Chan, and H.-Y. Shum. Plenoptic sampling. In SIG- GRAPH 2000, 2000.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>648717</ref_obj_id>
				<ref_obj_pid>645305</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Olivier D. Faugeras. What can be seen in three dimensions with an uncalibrated stereo rig? In ECCV92, pages 563-578, 1992.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Fresnel Technologies. #300 Itex Lens Array, 1999. http://www.fresneltech.com.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237200</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Steven J. Gortler, Radek Grzeszczuk, Richard Szeliski, and Michael F. Cohen. The lumigraph. SIGGRAPIt 96, pages 43-54, 1996.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>731967</ref_obj_id>
				<ref_obj_pid>647651</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Xianfeng Gu, Steven J. Gortler, and Michael F. Cohen. Polyhedral geometry and the two-plane parameterization. Eurographics Rendering Workshop 1997, pages 1-12, June 1997.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97913</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Paul E. Haeberli and Kurt Akeley. The accumulation buffer: Hardware support for high-quality rendering. SIGGRAPIt 90, 24(4):309-318, 1990.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Michael Halle. The generalized holographic stereogram. Master's thesis, Program in Media Arts and Sciences, Massachusetts Institute of Technology, 1991.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>266785</ref_obj_id>
				<ref_obj_pid>266774</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Wolfgang Heidrich, Philipp Slusalek, and Hans-Peter Seidel. An image-based model for realistic lens systems in interactive computer graphics. Graphics Interface '97, pages 68-75, 1997.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Aaron Isaksen, Leonard McMilland, and Steven J. Gortler. Dynamically reparameterized light fields. Technical Report LCS-TR-778, Massachusetts Institute of Technology, May 1999.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237199</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Marc Levoy and Pat Hanrahan. Light field rendering. SIGGRAPIt 96, pages 31-42, 1996.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Microsoft Corporation. Microsoft DirectX 7.0, 1999. http://www.microsoft.com/directx.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378514</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Don R Mitchell and Arun N. Netravali. Reconstruction filters in computer graphics. SIGGRAPIt88, 22(4):221-228, 1988.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Takanori Okoshi. Three-Dimensional Imaging Techniques. Academic Press, Inc., New York, 1976.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Paul Haeberli. A multifocus method for controlling depth of field. Technical report, SGI, October 1994. http://www.sgi.com/grafica/depth/index.html.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>28801</ref_obj_id>
				<ref_obj_pid>28795</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[A.R Pentland. A new sense for depth of field. PAMI, 9(4):523-531, July 1987.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311569</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Matthew J.R Regan, Gavin S.R Miller, Steven M. Rubin, and Chris Kogelnik. A real time low-latency hardware light-field renderer. SIGGRAPIt 99, pages 287-290, 1999.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Hartmut Schirmacher, Wolfgang Heidrich, and Hans-Peter Seidel. Adaptive acquisition of lumigraphs from synthetic scenes. Computer Graphics Forum, 18(3): 151-160, September 1999. ISSN 1067-7055.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311573</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Heung-Yeung Shum and Li-Wei He. Rendering with concentric mosaics. SIG- GRAPIt99, pages 299-306, 1999.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>253296</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Peter-Pike Sloan, Michael F. Cohen, and Steven J. Gortler. Time critical lumigraph rendering. 1997 Symposium on Interactive 3D Graphics, pages 17-24, April 1997. ISBN 0-89791-884-3.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Warren J. Smith. Practical Optical System Layout. McGraw-Hill, 1997.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[R.F. Stevens and N. Davies. Lens arrays and photography. Journal of Photographic Science, 39(5): 199-208, 1991.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>794491</ref_obj_id>
				<ref_obj_pid>794189</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Peter Storm. Critical motion sequences for monocular self-calibration and uncalibrated euclidean reconstruction. In CVPR97, pages 1100-1105, 1997.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[R.Y. Tsai. An efficient and accurate camera calibration technique for 3-d machine vision. In CVPR, pages 364-374, 1986.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[L. Yang, M. McCormick, and N. Davies. Discussion of the optics of a new 3d imaging system. Applied Optics, 27(21):4529-4534, 1988.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Z.Y. Zhang. Flexible camera calibration by viewing a plane from unknown orientations. In ICCV, pages 666-673, 1999.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344932</article_id>
		<sort_key>307</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>34</seq_no>
		<title><![CDATA[Plenoptic sampling]]></title>
		<page_from>307</page_from>
		<page_to>318</page_to>
		<doi_number>10.1145/344779.344932</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344932</url>
		<abstract>
			<par><![CDATA[<p>This paper studies the problem of plenoptic sampling in image-based rendering (IBR). From a spectral analysis of light field signals and using the sampling theorem, we mathematically derive the analytical functions to determine the minimum sampling rate for light field rendering. The spectral support of a light field signal is bounded by the minimum and maximum depths only, no matter how complicated the spectral support might be because of depth variations in the scene. The minimum sampling rate for light field rendering is obtained by compacting the replicas of the spectral support of the sampled light field within the smallest interval. Given the minimum and maximum depths, a reconstruction filter with an optimal and constant depth can be designed to achieve anti-aliased light field rendering.</p><p>Plenoptic sampling goes beyond the minimum number of images needed for anti-aliased light field rendering. More significantly, it utilizes the scene depth information to determine the minimum sampling curve in the joint image and geometry space. The minimum sampling curve quantitatively describes the relationship among three key elements in IBR systems: scene complexity (geometrical and textural information), the number of image samples, and the output resolution. Therefore, plenoptic sampling bridges the gap between image-based rendering and traditional geometry-based rendering. Experimental results demonstrate the effectiveness of our approach.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[image-based rendering]]></kw>
			<kw><![CDATA[plenoptic functions]]></kw>
			<kw><![CDATA[plenoptic sampling]]></kw>
			<kw><![CDATA[sampling]]></kw>
			<kw><![CDATA[spectral analysis]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31071750</person_id>
				<author_profile_id><![CDATA[81100205074]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jin-Xiang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University and Microsoft Research, China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP59023291</person_id>
				<author_profile_id><![CDATA[81314492380]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Xin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research, China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P264029</person_id>
				<author_profile_id><![CDATA[81100177265]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shing-Chow]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Hong Kong and Microsoft Research, China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP59023698</person_id>
				<author_profile_id><![CDATA[81365591566]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Heung-Yeung]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shum]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research, China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[E.H. Adelson and J. Bergen. The plenoptic function and the elements of early vision. In Computational Models of Visual Processing, pages 3-20. MIT Press, Cambridge, MA, 1991.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[M. Bass, editor. Handbook of Optics. McGraw-Hill, New York, 1995.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[R.C. Bolles, H. H. Baker, and D. H. Marimont. Epipolar-plane image analysis: An approach to determining structure from motion. International Journal of Computer Vision, 1:7-55, 1987.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[E. Camahort, A. Lerios, and D. Fussell. Uniformly sampled light fields. In Proc. 9th Eurographics Workshop on Rendering, pages 117-130, 1998.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[J.-X. Chai and H.-Y. Shum. Parallel projections for stereo reconstruction. In Proc. CVPR 2000, 2000.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311571</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[C. Chang, G. Bishop, and A. Lastra. Ldi tree: A hierarchical representation for image-based rendering. SIGGRAPH'99, pages 291-298, August 1999.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166153</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[S. Chen and L. Williams. View interpolation for image synthesis. Computer Graphics (SIGGRAPH'93), pages 279-288, August 1993.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[T. Feng and H.-Y. Shum. An optical analysis of light field rendering. Technical report, Microsoft Research, MSR-TR-2000-38, May 2000.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[B. Girod. Motion compensation: visual aspects, accuracy, and fundamental limits. In Motion Analysis and Image Sequence Processing. Kluwer, 1995.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237200</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[S. J. Gortler, R. Grzeszczuk, R. Szeliski, and M. F. Cohen. The lumigraph. In Computer Graphics Proceedings, Annual Conference Series, pages 43-54, Proc. SIGGRAPH'96 (New Orleans), August 1996. ACM SIGGRAPH.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[M. Halle. Holographic stereograms as discrete imaging systems. In Proc. SPIE Vol.2176, Practical Holography VIII, pages 73-84, May 1994.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>648949</ref_obj_id>
				<ref_obj_pid>645309</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[V. Hlavac, A. Leonardis, and T. Werner. Automatic selection of reference views for image-based scene representations. In Proc. ECCV, pages 526-535, 1996.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[A. Isaksen, L. McMillan, and S. Gortler. Dynamically reparameterized light fields. Technical report, Technical Report MIT-LCS-TR-778, May 1999.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[S. Kang. A survey of image-based rendering techniques. In VideoMetrics, SPIE Vol. 3641, pages 2-16, 1999.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>621027</ref_obj_id>
				<ref_obj_pid>619031</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[J. Lengyel. The convergence of graphics and vision. Technical report, IEEE Computer, July 1998.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237199</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[M. Levoy and R Hanrahan. Light field rendering. In Computer Graphics Proceedings, Annual Conference Series, pages 31-42, Proc. SIGGRAPH'96 (New Orleans), August 1996. ACM SIGGRAPH.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Z.-C. Lin and H.-Y. Shum. On the numbers of samples needed in light field rendering with constant-depth assumption. In Proc. CVPR 2000, 2000.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>253292</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[W. Mark, L. McMillan, and G. Bishop. Post-rendering 3d warping. In Proc. Symposium on I3D Graphics, pages 7-16, 1997.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218398</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[L. McMillan and G. Bishop. Plenoptic modeling: An image-based rendering system. Computer Graphics (SIGGRAPH'95), pages 39-46, August 1995.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[H. Schirmacher, W. Heidrich, and H. Seidel. Adaptive acquisition of lumigraphs from synthetic scenes. In Eurographics '99, pages 151-159, Sept 1999.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237196</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[S. M. Seitz and C. M. Dyer. View morphing. In Computer Graphics Proceedings, Annual Conference Series, pages 21-30, Proc. SIGGRAPH'96 (New Orleans), August 1996. ACM SIGGRAPH.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280882</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[J. Shade, S. Gortler, L.-W. He, and R. Szeliski. Layered depth images. In Computer Graphics (SIGGRAPH'98) Proceedings, pages 231-242, Orlando, July 1998. ACM SIGGRAPH.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311573</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[H.-Y. Shum and L.-W. He. Rendering with concentric mosaics. In Proc. SIG- GRAPH 99, pages 299-306, 1999.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>253296</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[P. R Sloan, M. F. Cohen, and S. J. Gortler. Time critical lumigraph rendering. In Symposium on Interactive 3D Graphics, pages 17-23, Providence, RI, USA, 1997.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>206698</ref_obj_id>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[A. Tekal. Digital Video Processing. Prentice Hall, 1996.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Plenoptic Sampling Jin-Xiang Chai* Xin Tong Shing-Chow Chany Heung-Yeung Shumz Microsoft Research, China 
 Abstract This paper studies the problem of plenoptic sampling in image­based rendering (IBR). From a 
spectral analysis of light .eld sig­nals and using the sampling theorem, we mathematically derive the 
analytical functions to determine the minimum sampling rate for light .eld rendering. The spectral support 
of a light .eld signal is bounded by the minimum and maximum depths only, no matter how complicated the 
spectral support might be because of depth variations in the scene. The minimum sampling rate for light 
.eld rendering is obtained by compacting the replicas of the spectral sup­port of the sampled light .eld 
within the smallest interval. Given the minimum and maximum depths, a reconstruction .lter with an optimal 
and constant depth can be designed to achieve anti-aliased light .eld rendering. Plenoptic sampling goes 
beyond the minimum number of im­ages needed for anti-aliased light .eld rendering. More signi.­cantly, 
it utilizes the scene depth information to determine the min­imum sampling curve in the joint image and 
geometry space. The minimum sampling curve quantitatively describes the relationship among three key 
elements in IBR systems: scene complexity (ge­ometrical and textural information), the number of image 
samples, and the output resolution. Therefore, plenoptic sampling bridges the gap between image-based 
rendering and traditional geometry­based rendering. Experimental results demonstrate the effective­ness 
of our approach. Keywords: sampling, plenoptic sampling, spectral analysis, plenoptic functions, image-based 
rendering. 1 Introduction Image-based modeling and rendering techniques have recently received much attention 
as a powerful alternative to traditional geometry-based techniques for image synthesis. Instead of geo­metrical 
primitives, a collection of sample images are used to ren­der novel views. Previous work on image-based 
rendering (IBR) reveals a continuum of image-based representations [15, 14] based on the tradeoff between 
how many input images are needed and how much is known about the scene geometry. At one end, traditional 
texture mapping relies on very accurate geometrical models but only a few images. In an image-based ren­ 
dering system with depth maps, such as 3D warping [18], view *Currently at Carnegie Mellon University. 
jchai@cs.cmu.edu yVisiting from University of Hong Kong. scchan@eee.hku.hk zfxtong,hshumg@microsoft.com 
Permission to make digital or hard copies of part or all of this work or personal or classroom use is 
granted without fee provided that copies are not made or distributed for profit or commercial advantage 
and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, 
to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. SIGGRAPH 
2000, New Orleans, LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 interpolation [7], view morphing 
[21] and layered-depth images (LDI) [22], LDI tree [6], etc., the model consists of a set of im­ages 
of a scene and their associated depth maps. When depth is available for every point in an image, the 
image can be rendered from any nearby point of view by projecting the pixels of the im­age to their proper 
3D locations and re-projecting them onto a new picture. At the other end, light .eld rendering uses many 
images but does not require any geometrical information. Light .eld rendering [16] generates a new image 
of a scene by appropriately .ltering and in­terpolating a pre-acquired set of samples. Lumigraph [10] 
is simi­lar to light .eld rendering but it applies approximated geometry to compensate for non-uniform 
sampling in order to improve render­ing performance. Unlike light .eld and Lumigraph where cameras are 
placed on a two-dimensional manifold, Concentric Mosaics sys­tem [23] reduces the amount of data by only 
capturing a sequence of images along a circular path. Light .eld rendering, however, typ­ically relies 
on oversampling to counter undesirable aliasing effects in output display. Oversampling means more intensive 
data acqui­sition, more storage, and more redundancy. To date, little research has been done on determining 
the lower bound or the minimum number of samples needed for light .eld rendering. Sampling analysis in 
IBR is a dif.cult problem because it in­volves the complex relationship among three elements: the depth 
and texture information of the scene, the number of sample images, and the rendering resolution. The 
topic of pre.ltering a light .eld has been explored in [16]. Similar .ltering process has been previ­ously 
discussed by Halle [11] in the context of Holographic stere­ograms. A parameterization for more uniform 
sampling [4] has also been proposed. From an initially undersampled Lumigraph, new views can be adaptively 
acquired if the rendering quality can be improved [20]. An opposite approach is to start with an over­sampled 
light .eld, and to cull an input view if it can be predicted by its neighboring frames [12, 24]. Using 
a geometrical approach and without considering textural information of the scene, Lin and Shum [17] recently 
studied the number of samples needed in light .eld rendering with constant depth assumption and bilinear 
inter­polation. However, a mathematical framework has not been fully developed for studying the sampling 
problems in IBR. In this paper, we study plenoptic sampling, or how many samples are needed for plenoptic 
modeling [19, 1]. Plenoptic sampling can be stated as: How many samples of the plenoptic function (e.g., 
from a 4D light .eld) and how much geometrical and textural information are needed to generate a continuous 
repre­sentation of the plenoptic function? Speci.cally, our objective in this paper is to tackle the 
following two problems under plenoptic sampling, with and without geomet­rical information: Minimum 
sampling rate for light .eld rendering;  Minimum sampling curve in joint image and geometry space. 
 We formulate the sampling analysis as a high dimensional signal processing problem. In our analysis, 
we assume Lambertian sur­faces and uniform sampling geometry or lattice for the light .eld.  0t (a) 
(b) Figure 1: An illustration of 2D light .eld or EPI: (a) a point is observed by two cameras 0and t; 
(b) two lines are formed by stacking pixels captured along the camera path. Each line has a uniform color 
because of Lambertian assumption on object surfaces. Rather than attempting to obtain a closed-form general 
solution to the 4Dlight .eld spectral analysis, we only analyze the bounds of the spectral support of 
the light .eld signals. A key analysis to be presented in this paper is that the spectral support of 
a light .eld signal is bounded by only the minimum and maximum depths, irre­spective of how complicated 
the spectral support might be because of depth variations in the scene. Given the minimum and maximum 
depths, a reconstruction .lter with an optimal and constant depth can be designed to achieve anti-aliased 
light .eld rendering. The minimum sampling rate of light .eld rendering is obtained by compacting the 
replicas of the spectral support of the sampled light .eld within the smallest interval without any overlap. 
Using more depth information, plenoptic sampling in the joint image and geometry space allows us to greatly 
reduce the number of images needed. In fact, the relationship between the number of images and the geometrical 
information under a given rendering resolu­tion can be quantitatively described by a minimum sampling 
curve. This minimal sampling curve serves as the design principles for IBR systems. Furthermore, it bridges 
the gap between image-based rendering and traditional geometry-based rendering. Our approach is inspired 
by the work on motion compensation .lter in the area of digital video processing, in which depth infor­mation 
has been incorporated into the design of the optimal motion compensation .lter [25, 9]. In digital video 
processing, global con­stant depth and arbitrary motion are considered for both static and dynamic scenes, 
whereas in our work, we analyze static scenes with an arbitrary geometry and with uniformly sampled camera 
setups. The remainder of this paper is organized as follows. In Section 2, a spectral analysis of 4Dlight 
.eld is introduced and the bounds of its spectral support are determined. From these bounds, the min­imum 
sampling rate for light .eld rendering can be derived analyt­ically. Plenoptic sampling in the joint 
image and geometry space is studied in Section 3. The minimum sampling curves are deduced with accurate 
and approximated depths. Experimental results are presented in Section 4. Finally we conclude our paper 
in Section 5. 2 Spectral analysis of light .eld 2.1 Light .eld representation We begin by brie.y reviewing 
the properties of light .eld represen­tation. We will follow the notations in the Lumigraph paper [10]. 
In the standard two-plane ray database parameterization, there is a camera plane, with parameter (s,t), 
and a focal plane, with param­eter (u,v). Each ray in the parameterization is uniquely determined by 
the quadruple (u,v,s,t). We refer the reader to Figure 2(a) of [10] for more details. A two dimensional 
subspace given by .xed values of sand t resembles an image, whereas .xed values of uand vgive a hypo­thetical 
radiance function. Fixing tand vgives rise to an epipolar image, or EPI [3]. An example of a 2D light 
.eld or EPI is shown in Figure 1. Note that in our analysis we de.ne (u,v)in the lo­cal coordinates of 
(s,t), unlike in conventional light .eld where (u,v,s,t)are de.ned in a global coordinate system. Assume 
the sample intervals along sand tdirections be Lsand Lt, respectively, the horizontal and vertical disparities 
between two grid cameras in the (s,t)plane are determined by k1Lsf/zand k2Ltf/z, respectively, where 
fdenotes the focal length of the camera, zis the depth value and (k1Ls,k2Lt)is the sample in­terval between 
two grid points (s,t). Similarly, we assume that the sample intervals along uand vdi­rections be Luand 
Lv, respectively. A pinhole camera model is adopted to capture the light .eld. What a camera sees is 
a blurred version of the plenoptic function because of .nite camera resolu­tion. A pixel value is a weighted 
integral of the illumination of the light arriving at the camera plane, or the convolution of the plenop­tic 
function with a low-pass .lter. 2.2 A framework for light .eld reconstruction Let l(u,v,s,t)represent 
the continuous light .eld, p(u,v,s,t)the sampling pattern in light .eld, r(u,v,s,t)the combined .ltering 
and interpolating low-pass .lter, and i(u,v,s,t)the output image after reconstruction. Let L,Prepresent 
their correspond­ ,Rand Iing spectra, respectively. In the spatial domain, the light .eld re­ construction 
can be computed as i(u,v,s,t) r(u,v,s,t)*[l(u,v,s,t)p(u,v,s,t)] (1) where *represents the convolution 
operation. In the frequency domain, we have I(0u,0v,0s,0t) R(0u,0v,0s,0t)(L(0u,0v,0s,0t) *P(0u,0v,0s,0t)) 
(2) The problem of light .eld reconstruction is to .nd a reconstruc­tion .lter r(u,v,s,t)for anti-aliased 
light .eld rendering, given the sampled light .eld signals. 2.3 Spectral support of light .elds In this 
section, we will introduce the spectral supports of con­tinuous light .eld L(0u,0v,0s,0t)and sampled 
light .eld L(0u,0v,0s,0t)*P(0u,0v,0s,0t). 2.3.1 Spectral support of continuous light .eld We assume that 
the depth function of the scene is equal to z(u,v,s,t). As shown in Figure 1(a), the same 3D point is 
ob­served at v 0and vin the local coordinate systems of cameras 0and t, respectively. The disparity between 
the two image coordinates can be computed easily as v;0 ft/. Figure 1(b) shows an vzEPI image where each 
line represents the radiance observed from different cameras. For simplicity of analysis, the BRDF model 
of a real scene is assumed to be Lambertian. Therefore, each line in Figure 1(b) has a uniform color. 
Therefore, the radiance received at the camera position (s,t)is given by fs ft l(u,v,s,t) l(u; ,v; ,0,0) 
z(u,v,s,t)z(u,v,s,t) and its Fourier transform is ZZZ 111 ;jlT x L(0u,0v,0s,0t) l(u,v,s,t)edx ;1;1;1 
;j(lss+ltt) e dsdt (3) where x T[u,v]and 0T[0u,0v]. However, computing the Fourier transform (3) is 
very compli­cated, and we will not go into the details of its derivation in this paper. Instead, we will 
analyze the bounds of the spectral support of light .elds. Also for simplicity, it is assumed that samples 
of the light .eld are taken over the commonly used rectangular sampling lattice. 2.3.2 Spectral support 
of sampled light .eld Using the rectangular sampling lattice, the sampled light .eld ls(u,v,s,t)is represented 
by X ls(u,v,s,t) l(u,v,s,t) n1,n2,k1,k22Z 5(u;n1Lu)5(v;n2Lv)5(s;k1Ls)5(t;k2Lt) (4) and its Fourier transform 
is X Ls(0u,0v,0s,0t) m1,m2,l1,l22Z 27m1 27m2 27l1 27l2 L(0u; ,0v; ,0s; ,0t; ) (5) Lu Lv Ls Lt The above 
equation indicates that Ls(0u,0v,0s,0t)consists of replicas of L(0u,0v,0s,0t), shifted to the 4Dgrid 
points (27m1/Lu,27m2/Lv,27l1/Ls,27l2/Lt), where m1,,l1,l2Z, and Zis the set of integers. m 22 These 
shifted spectra, or replicas, except the original one at m1 m2 l1 l2 0, are called the alias components. 
When Lis not bandlimited outside the Nyquist frequencies, some replicas will overlap with the others, 
creating aliasing artifacts. In general, there are two ways to combat aliasing effects in out­put display 
when we render a novel image. First, we can increase the sampling rate. The higher the sampling rate, 
the less the aliasing effects. Indeed, uniform oversampling has been consistently em­ployed in many IBR 
systems to avoid undesirable aliasing effects. However, oversampling means more effort in data acquisition 
and requires more storage. Though redundancy in the oversampled im­age database can be partially eliminated 
by compression, excessive samples are always wasteful. Second, light .eld signals can also be made bandlimited 
by .l­tering with an appropriate .lter kernel. Similar .ltering has to be performed to remove the overlapping 
of alias components during reconstruction or rendering. The design of such a kernel is, how­ever, related 
to the depth of the scene. Previous work on Lumigraph shows that approximate depth correction can signi.cantly 
improve the interpolation results. The questions are: is there an optimal .l­ter? Given the number of 
samples captured, how accurately should the depth be recovered? Similarly, given the depth information 
one can recover, how many samples can be removed from the original input?  2.4 Analysis of bounds in 
spectral support 2.4.1 A model of global constant depth Let us .rst consider the simplest scene model 
in which every point is at a constant depth (z0). The .rst frame is chosen as the reference frame, and 
l(u,v,0,0)denotes the 2D intensity distribution within the reference frame. The 4D Fourier transform 
of the light .eld signal l(u,v,s,t)with constant depth is ZZ 11 ;j(luu+lvv)L(0u,0v,0s,0t) l(u,v,0,0)e 
dudv ;1;1 ZZ 11 ff ;j(lu+ls)s ;j(lv+lt)t z0z0 e dse dt ;1 ;1 ff 47 2 L 0(0u,0v)5(0u+0)5(0v+0) st z0 z0 
 where L0(0u,0v)is the 2D Fourier transform of continuous sig­nal l(u,v,0,0)and 5(-)is the 1D Dirac delta 
function. To keep notation, representations and illustration simple, the follow­ing discussion will focus 
on the projection of the support of L(0u,0v,0s,0t)onto the (0v,0t)plane, which is denoted by L(0v,0t). 
Under the constant depth model, the spectral support of the con­tinuous light .eld signal L(0v,0t)is 
de.ned by a line 0vf/0+ z0t0, as shown in Figure 2(b). The spectral support of the corre­sponding sampled 
light .eld signals is shown in Figure 2(c). Note that, due to sampling, replicas of L(0v,0t)appear at 
intervals 27m2/Lvand 27l2/Ltin the 0vand 0tdirections, respectively. Figure 6(a) shows a constant depth 
scene (a1), its EPI image (a2), and the Fourier transform of the EPI (a3). As expected, the spectral 
support is a straight line.1 2.4.2 Spatially varying depth model Now it is straightforward to observe 
that any scene with a depth between the minimum zminand the maximum zmaxwill have its continuous spectral 
support bounded in the frequency domain, by two lines 0vf/+0 0and 0vf/z+0 0. Fig­ z mintmaxt ure 6(b3) 
shows the spectral support when two planes with constant depths are in the scene. Adding another tilted 
plane in between (Figure 6(c1)) results in no variations in the bounds of the spectral support, even 
though the resulting spectral support (Figure 6(c3)) differs signi.cantly from that in Figure 6(c2). 
This is further il­lustrated when a curved surface is inserted in between two original planes, as shown 
in Figure 6(d1). Even though the spectral supports differ signi.cantly, Figures 6(b3), (c3) and (d3) 
all have the same bounds. Another important observation is that geometrical information can help to reduce 
the bounds of the spectral support in the fre­quency domain. As will be illustrated in the following 
section, the optimal reconstruction .lter is determined precisely by the bounds of the spectral support. 
And these bounds are functions of the min­imum and maximum depths of the scene. If some information on 
the scene geometry is known, we can decompose the scene geome­try into a collection of constant depth 
models on a block-by-block basis. Each model will have a much tighter bound than the orig­inal model. 
How tight the bound is will depend on the accuracy 1The ringing effect in the vicinity of the horizontal 
and vertical axes is caused by convolving with sin(!v)/!vbecause of the rectangular image boundary. Z 
 . v Z0  . t f . t Z0 . v +. t = 0 v  (a) (b) (c) Figure 2: Spectral support of light .eld signals 
with constant depth: (a) a model of constant depth; (b) the spectral support of continuous light .eld 
signals; (c) the spectral support of sampled light .eld signals. . Z Z max  0 f Z0  t Z0 Zmin f 
vZmax (a) (b) Figure 3: Spectral support for light .eld signal with spatially varying depths: (a) a local 
constant depth model bounded by zand zmaxis min augmented with another depth value z0; (b)spectral 
support is now bounded by two smaller regions, with the introduction of the new line of z0. . v . v 
. v . v aliasing . t . t . t . t  (a) (b) (c) (d) Figure 4: Three reconstruction .lters with different 
constant depths: (a) in.nite depth; (b) in.nite depth (aliasing occurs); (c) maximum depth; (d) optimal 
depth at zc. . v . t min  (a) (b) Figure 5: (a) The smallest interval that replicas can be packed without 
any overlap is PmaxPmin, determined by the highest frequency Klv . (b) A spectral support decomposed 
into multiple layers. . . (a1). Scene. image. (a2). EPI. (a3). Fourier. transform. of. EPI. . . . (b1)Scene. 
image. . (b2). EPI. (b3). Fourier. transform. of. EPI. . . (c1). Scene. image. . (c2). EPI. (c3). Fourier. 
transform. of. EPI. . . (d1)Scene. image. . (d2). EPI. (d3). Fourier. transform. of. EPI. . Figure 
6: Spectral support of a 2D light .eld: (a) a single plane; (b) two planes; (c) a third and tilted plane 
in between; (d) a curved surface in between. of the geometry. Figure 3 illustrates the reduction in bounds, 
from [zmin,max]to max([zmin,0],[z0,]), with the introduction zzz max of another layer. 2.4.3 A model 
with truncating windows Because of the linearity of the Fourier transform, the spectral sup­port of the 
EPI image for a scene with two constant planes will be two straight lines. However, this statement is 
true only if these two planes do not occlude each other. For synthetic environments, we can construct 
such EPI images on different layers by simply ignor­ing the occlusion. In practice, we can represent 
a complicated environment using a model with truncating windows. For example, we can approximate an environment 
using truncated and piece-wise constant depth seg­ments. Speci.cally, suppose the depth can be partitioned 
as z(v) zi,for vi:v<vi+1,i 1,---,N d where v1and vNd+1are the smallest and largest vof interest re­spectively. 
Then l(v,t) li(v;ft/zi,0),if vi:v<v i+1 and Nd X vi+vi+1 L(0v,0t) exp(;j (0v+0tzi/f)) 2 i=1 vi+1;vi2sin( 
(0v+0tzi/f)) 2 Li(;0tzi/f) f0v/zi+0 t Nd X Qi(0v,0t) (6) i=1 where Liis the 1DFourier transform of li. 
x In (6), because the function sindecays fast, and Li(;0tzi/f) also decreases fast when j0tjxgrows, the 
spectral support of Qi(0v,0t)will look like a narrow ellipse. Nevertheless, because of high frequency 
leak, cut-off frequency should be used in the sam­pling analysis. An example of two constant planes in 
an environment is shown in Figures 6(b1) (original image), 6(b2) (EPI) and 6(b3) (spectral support). 
Note that the shape of each of the two spectral supports, i.e., two approximated lines, is not signi.cantly 
affected by occlu­sion because the width of each spectral support is not too large.  2.5 A reconstruction 
.lter using a constant depth Given a constant depth, a reconstruction .lter can be designed. Fig­ure 
4 illustrates four different designs of reconstruction .lters ori­ented to different constant depths. 
Aliasing occurs when replicas overlap with the reconstruction .lters in the frequency domain (0t and 
0v), as shown in Figure 4(a)(b)(d). Anti-aliased light .eld ren­dering can be achieved by applying the 
optimal .lter as shown in Figure 4(c), where the optimal constant depth is de.ned as the in­verse of 
average disparity dc, i.e., 111 dc (+)/2: zczmin zmax Figure 7 shows the effect of applying reconstruction 
.lters with different constant depths. As we sweep through the object with a constant depth plane, the 
aliasing effect is the worst at the minimum and maximum depths. The best rendering quality is obtained 
at the optimal depth (Figure 7(b)), not at the focal plane as has been commonly assumed in light .eld 
[16] or Lumigraph [10] rendering. In fact, the optimal depth can be used as a guidance for selecting 
the focal plane. For comparison, we also show the rendering result using average depth in Figure 7(c). 
Similar sweeping effects have also been discussed in the dynam­ically reparameterized light .eld [13]. 
However, an analytical solu­tion using the minimum and maximum depths has never been pre­sented before. 
 2.6 Minimum sampling rate for light .eld rendering With the above theoretical analysis, we are now ready 
to solve the problem of the minimum sampling rate for light .eld rendering. Since we are dealing with 
rectangular sampling lattice, the Nyquist sampling theorem for 1D signal applies to both directions vand 
t. According to the Nyquist sampling theorem, in order for a signal to be reconstructed without aliasing, 
the sampling frequency needs to be greater than the Nyquist rate, or two times that of the Nyquist frequency. 
Without loss of generality, we only study the Nyquist frequency along the 0tdirection in the frequency 
domain. How­ever, the Nyquist frequency along the 0vdirection can be analyzed in a similar way. The minimum 
interval, by which the replicas of spectral support can be packed without any overlapping, can be computed 
as shown in Figure 5(a) jPmaxPminj Klv f h d 2 7 K fv f h d (7) where 1 1 hd ; , zmin zmax and Kfv f 
h d min(B s v,1/(2Lv),1/(25 v )) is the highest frequency for the light .eld signal, which is deter­mined 
by the scene texture distribution (represented by the highest frequency Bs), the resolution of the sampling 
camera (Lv), and the vresolution of the rendering camera (5). The frequency Bscan be v v computed from 
the spectral support of the light .eld. Our formula­tion takes the rendering resolution into account 
because rendering at a resolution higher than the output resolution is wasteful. For simplicity, we assume 
5 Lvfrom now on. v The minimum sampling rate is equivalent to the maximum cam­era spacing Ltmax, which 
can be computed as 1 Ltmax : (8) Kfv fhd The minimum sampling rate can also be interpreted in terms 
of the maximum disparity de.ned as the projection error using the op­timal reconstruction .lter for rendering. 
From Equation 8, we have the maximum disparity sLtmaxfhd/2 1 max(Lv,1/(2B)): (9) v2Kfv Therefore, the 
disparity is less than 1pixel (i.e., the camera res­olution) or half cycle of the highest frequency (1/Bsis 
de.ned as a v cycle) presented in the EPI image because of the textural complex­ity of the observed 
scene. If the textural complexity of the scene is not considered, the min­imum sampling rate for light 
.eld rendering can also be derived in the spatial domain. For example, by considering the light .eld 
ren­dering as a synthetic aperture optical system, we present an optical analysis of light .eld rendering 
in Appendix A. The maximum camera spacing will be larger if the scene texture variation gets more uniform, 
or if the rendering camera resolution becomes lower. By setting the higher frequency part of the spec­trum 
to zero so that Bs<1/(2Lv), we can reduce the minimum vsampling rate. One way to reduce Bsis to apply 
a low-pass .lter to v  the input v-timage. This approach is similar to pre.ltering a light .eld (see 
Figure 7 in [16]). In particular, the minimum sampling rate is also determined by ;1 ;1 the relative 
depth variation f(z min;z). The closer the object maxgets to the camera, the smaller the zminis, and 
the higher the min­imum sampling rate will be. As fgets larger, the sampling camera will cover a more 
detailed scene, but the minimum sampling rate needs to be increased. Therefore, the plenoptic sampling 
problem should not be considered in the image space alone, but in the joint image and geometry space. 
 3 Minimum sampling in the joint image and geometry space In this section, we will study the minimum 
sampling problem in the joint geometry and image space. Since the CPU speed, mem­ory, storage space, 
graphics capability and network bandwidth used vary from users to users, it is very important for users 
to be able to seek the most economical balance between image samples and depth layers for a given rendering 
quality. It is interesting to note that the minimum sampling rate for light .eld rendering represents 
essentially one point in the joint image and geometry space, in which little amount of depth information 
has been utilized. As more geometrical information becomes available, fewer images are necessary at any 
given rendering resolution. Fig­ure 8 illustrates the minimum sampling rate in the image space, the minimum 
sampling curve in the joint image and geometry space, and minimum sampling curves at different rendering 
resolutions. Any sampling point above the minimum sampling curve (e.g., Fig­ure 8b) is redundant. 3.1 
Minimum sampling with accurate depth From an initial set of accurate geometrical data, we can decompose 
a scene into multiple layers of sub-regions. Accordingly, the whole spectral support can be decomposed 
into multiple layers (see Fig­ure 5b) due to the correspondence between a constant depth and its spectral 
support. For each decomposed spectral support, an op­timal constant depth .lter can be designed. Speci.cally, 
for each depth layer i 1,:::,N, the depth of optimal .lter is described d as follows 11 1 .i +(1;.i) 
(10) zi zmin zmax where i;0:5 .i Nd Therefore a depth value can be assigned to one of the depth lay­ers 
zziif ;hd 11 hd : ;: : (11)2Nd zzi 2Nd   .  The layers are quantized uniformly in the disparity 
space. This is because perspective images have been used in the light .elds. If we use parallel projection 
images instead, the quantization should be uniform in the depth space [5]. Similar to Equation 8, the 
minimum sampling in the joint image and accurate depth space is obtained when Lt 1 ,Nd.1 (12) Nd Kfv 
fhd where Ndand Ltare the number of depth layers and the sampling interval along the tdirection, respectively. 
The interval between replicas is uniformly divided into Ndsegments. The number of depth layers needed 
for scene representation is a function of the sampling and rendering camera resolution, the scene s texture 
complexity, the spacing of the sampling cameras and the depth variation relative to the focal length. 
3.1.1 Applications Based on the above quantitative analysis in the joint image and depth space for suf.cient 
rendering, a number of important appli­cations can be explored. Image-based geometry simpli.cation. 
Given the appropri­ate number of image samples an average user can afford, the minimum sampling curve 
in the joint space determines how much depth information is needed. Thus, it simpli.es the original complex 
geometrical model to the minimum while still guaranteeing the same rendering quality. Geometry-based 
image database reduction. In contrast, given the number of depth layers available, the number of im­age 
samples needed can also be reduced to the minimum for a given rendering resolution. The reduction of 
image samples is particularly useful for light .eld rendering. Level of details (LOD) in joint image 
and depth space. The idea of LOD in geometry space can be adopted in our joint image and geometry space. 
When an object becomes farther away, its relative size on screen space diminishes so that the number 
of required image samples or the number of required depth layers can be reduced accordingly. Zooming-in 
onto and zooming-out of objects also demand a dynamic change in the number of image samples or depth 
layers. Light .eld with layered depth. A general data structure for the minimum sampling curve in the 
joint image and geome­try space can be light .eld with layered depth. With different numbers of images 
and depth layers used, the trade-off be­tween rendering speed and data storage has to be studied.  o 
Number offimages o (a) (b) (c) Number offimages  o Numberfof depthflayers  Figure 8: Plenoptic sampling: 
(a) the minimum sampling rate in image space; (b) the minimum sampling curve in the joint image and geometry 
space (any sampling point above the curve is redundant); (c) minimum sampling curves at different rendering 
resolutions. 3.2 Minimum sampling with depth uncertainty Another aspect of minimum sampling in the joint 
image and geom­etry space is related to depth uncertainty. Speci.cally, minimum sampling with depth uncertainty 
describes the quantitative relation­ship between the number of image samples, noisy depth and depth uncertainty. 
It is important to study this relationship because in general the recovered geometry is noisy as modeling 
a real envi­ronment is dif.cult. Given an estimated depth zeand its depth un­certainty LT, the depth 
value should be located within the range (ze;LT,z+LT). The maximum camera spacing can be com­ e puted 
as 22 (ze+LT)(ze;LT)minze z;LT e Ltmaxmin :(13) ze 2fKLT 2fKLT fv fv In addition, geometrical uncertainty 
also exists when an accurate model is simpli.ed. Given the correct depth z0and an estimated depth ze, 
the maximum camera spacing can be computed as zez0 Ltmaxmin : (14) ze2fKfvjze;z0j 3.2.1 Applications 
Knowledge about the minimum number of images under noisy depth has many practical applications. Minimum 
sampling rate. For a speci.c light .eld rendering with no depth maps or with noisy depth maps, we can 
de­termine the minimum number of images for antialiased light .eld rendering. Redundant image samples 
can then be left out from the sampled database for light .eld rendering. Rendering-driven vision reconstruction. 
This is a very interesting application, considering that general vision al­gorithms would not recover 
accurate scene depth. Given the number of image samples, how accurately should the depth be recovered 
to guarantee the rendering quality? Rendering-driven vision reconstruction is different from clas­sical 
geometry-driven vision reconstruction in that the former is guided by the depth accuracy that the rendering 
process can have.  4 Experiments Table 1 summarizes the parameters of each light .eld data set used 
in our experiments. We assume that the output display has the same resolution as the input image. Furthermore, 
without taking into con­sideration the actual texture distribution, we assume that the highest frequency 
in images is bounded by the resolution of the capturing camera. We have used different settings of focal 
length for the Head, the Statue and the Table. We put the focal plane slightly in front of the Head. 
A smaller focal length will reduce the minimum sampling rate. For the Statue, the focal plane is set 
approximately at its fore­head. In fact, we have set the focal length (3000) very close to the optimal 
(3323). Because the Table scene has signi.cant depth vari­ations, a small camera focal length was used 
so that each image can cover a large part of the scene. First, we compare the rendering quality along 
the minimal sam­pling curve in the joint image and geometry space, with the best rendering quality we 
can obtain with all images and accurate depth. According to our theory (Eq (12)), the number of images 
is in­versely proportional to the number of depth layers in use. The rendering results corresponding 
to .ve different image and depth combinations along the minimum sampling curve are shown in Fig­ures 
11(A)-(E). For example, C(7,8) represents the rendering result using 7 layers of depth and 8x8images. 
In contrast, Figure 11(F) shows the best rendering output one can achieve from this set of data: accurate 
depth and all 32ximages2. The quality of the 32rendered images along the minimal sampling curve is almost 
indis­tinguishable3 from that of using all images and accurate depth. Figure 12(a) compares the rendering 
quality using different lay­ers of depth and a given number of image samples. With 2x2 image samples 
of the Head, images (A)-(E) in Figure 12(a) show the rendered images with different layers of depth at 
4, 8, 10, 12, and 24. According to Eq (12), the minimum sampling point with 2x2images of the Head is 
at approximately 12 layers of depth. No­ticeable visual artifacts can be observed when the number of 
depth is below the minimal sampling point, as shown in images (A)-(C) of Figure 12(a). On the other hand, 
oversampling layers of depth does not improve the rendering quality, as shown in the images (D) and (E). 
With the minimal sampling curve, we can now deduce the mini­mum number of image samples at any given 
number of depth layers available. For the Table scene, we .nd that 3 bits (or 8 layers) of depth information 
is suf.cient for light .eld rendering when com­bined with 16x16image samples (shown in image (D) of Fig­ure 
12(b)). When the number of depth layers is below the minimal sampling point, light .eld rendering produces 
noticeable artifacts, as shown in images (A)-(C) of Figure 12(b). Given a single depth layer, our analysis 
(Eq 12) shows that the number of images for antialiased rendering of the table scene re­quires 124x124images. 
Note that conventional light .eld may require even a larger number of images without using the optimal 
depth. This very large set of light .eld data is due to the signi.­ 2We were not able to use all 64X64images 
with accurate depth because of memory limitations. 3There exists little discrepancy because of the fact 
that we can not apply the optimal reconstruction .lter in rendering. (a) (b) xFigure 9: Comparison between 
conventional light .eld with 48x48images and rendering with 1616images and 3 bits of depth: (a) artifacts 
are visible on the left with conventional rendering, (b) but not present with additional geometrical 
information because minimum sampling requirement is satis.ed. cant depth variations in the Table scene. 
This perhaps explains why inside-looking-out light .eld rendering has not been used often in practice. 
Also according to our analysis, using 3 bits (8 layers) of depth helps to reduce the number of images 
needed by a factor of 60,to 16x16images. For comparison, Figure 9(a) shows con­ventional light .eld rendering 
with 48x48images and Figure 9(b) shows the rendering result with 16x16images plus 3 bits of depth. Visual 
artifacts such as double images at the edge of the wall are clearly visible in Figure 9(a). They are 
not present in Figure 9(b). Experiments using depth with uncertainty also demonstrate the effectiveness 
of our analysis. Due to space limitation, we will not present any results of minimum sampling curve using 
depth with uncertainty. 5 Conclusion and future work In this paper we have studied the problem of plenoptic 
sampling. Speci.cally, by analyzing the bounds of spectral support of light .eld signals, we can analytically 
compute the minimum sampling rate of light .eld rendering. Our analysis is based on the fact that the 
spectral support of a light .eld signal is bounded by only the min­imum and maximum depths, irrespective 
of how complicated the spectral support might be because of depth variations in the scene. Given the 
minimum and maximum depths, a reconstruction .lter with an optimal constant depth can be designed for 
anti-aliased light .eld rendering. The minimum sampling rate for light .eld render­ing is obtained by 
compacting the replicas of the spectral support of the sampled light .eld within the smallest interval. 
Our work pro­vides a solution to overcoming the oversampling problem in light .eld capturing and rendering. 
By studying plenoptic sampling in the joint image and geom­etry space, we have also derived the minimum 
sampling curve which quantitatively describes the relationship between the num­ber of images and the 
information on scene geometry, given a spe­ci.c rendering resolution. Indeed, minimum sampling curves 
with accurate depth and with noisy depth serve as the design princi­ples for a number of applications. 
Such interesting applications in­clude image-based geometry simpli.cation, geometry-assisted im­age dataset 
reduction, rendering-driven vision reconstruction, in ad­dition to depth-assisted light .eld compression, 
or the minimum sampling rate for light .eld rendering. While we have studied minimum sampling using light 
.elds in this paper, the very idea of plenoptic sampling is also applicable to other IBR systems, e.g. 
concentric mosaics, layered-depth im­age, view interpolation, and image warping, to name a few. With 
plenoptic sampling, there are a number of exciting areas for future work. For example, we have used depth 
value in this paper to encode the geometry information. Depth is also used in image-assisted ge­ometry 
simpli.cation. However, no surface normal has been con­sidered. In the future, we would like to experiment 
with different techniques to generate image-assisted geometry simpli.cation us­ing geometrical representations 
other than depth. We plan to incor­porate the surface normal into image-based polygon simpli.cation. 
The ef.ciency of geometry simpli.cation can be further enhanced by considering the standard techniques 
in geometrical simpli.ca­tion, e.g. visibility culling. Another interesting line of future work is on 
how to design a new rendering algorithm for the joint image and geometry representa­tion. The complexity 
of the rendering algorithm should be propor­tional to the number of depth in use. In addition, error-bounded 
depth reconstruction should be considered as an alternative to tra­ditional vision reconstruction, if 
the reconstruction result is to be used for rendering. Given the error bounds that are tolerable by the 
rendering algorithms, the dif.culty of vision reconstruction can be much alleviated. Lastly, we plan 
to study view-dependent plenoptic sampling. Current analysis of plenoptic sampling is based on the assumption 
that the surface is diffuse and little view-dependent variance can occur. It is conceivable that view 
dependent surface property will increase the minimum sampling rate for light .eld. 6 Acknowledgements 
The authors bene.ted from discussions with Zhouchen Lin on trun­cated models and Tao Feng on optical 
analysis of light .eld render­ing. Mr. Yin Li s incredible help on preparing Siggraph video is greatly 
appreciated. The last author also wishes to thank Pat Han­rahan for his helpful discussion on the minimum 
sampling of con­centric mosaics while visiting Stanford in April 1999. Finally, the authors thank Siggraph 
reviewers comments which have tremen­dously helped to improve the .nal manuscript. synthetic camera. 
plane length.f References [1] E. H. Adelson and J. Bergen. The plenoptic function and the elements 
of early vision. In Computational Models of Visual Processing, pages 3 20. MIT Press, Cambridge, MA, 
1991. [2] M. Bass, editor. Handbook of Optics. McGraw-Hill, New York, 1995. [3] R. C. Bolles, H. H. Baker, 
and D. H. Marimont. Epipolar-plane image analysis: An approach to determining structure from motion. 
International Journal of Computer Vision, 1:7 55, 1987. [4] E. Camahort, A. Lerios, and D. Fussell. Uniformly 
sampled light .elds. In Proc. 9th Eurographics Workshop on Rendering, pages 117 130, 1998. [5] J.-X. 
Chai and H.-Y. Shum. Parallel projections for stereo reconstruction. In Proc. CVPR 2000, 2000. [6] C. 
Chang, G. Bishop, and A. Lastra. Ldi tree: A hierarchical representation for image-based rendering. SIGGRAPH 
99, pages 291 298, August 1999. [7] S. Chen and L. Williams. View interpolation for image synthesis. 
Computer Graphics (SIGGRAPH 93), pages 279 288, August 1993. [8] T. Feng and H.-Y. Shum. An optical analysis 
of light .eld rendering. Technical report, Microsoft Research, MSR-TR-2000-38, May 2000. [9] B. Girod. 
Motion compensation: visual aspects, accuracy, and fundamental limits. In Motion Analysis and Image Sequence 
Processing. Kluwer, 1995. [10] S. J. Gortler, R. Grzeszczuk, R. Szeliski, and M. F. Cohen. The lumigraph. 
In Computer Graphics Proceedings, Annual Conference Series, pages 43 54, Proc. SIGGRAPH 96 (New Orleans), 
August 1996. ACM SIGGRAPH. [11] M. Halle. Holographic stereograms as discrete imaging systems. In Proc. 
SPIE Vol.2176, Practical Holography VIII, pages 73 84, May 1994. [12] V. Hlavac, A. Leonardis, and T. 
Werner. Automatic selection of reference views for image-based scene representations. In Proc. ECCV, 
pages 526 535, 1996. [13] A. Isaksen, L. McMillan, and S. Gortler. Dynamically reparameterized light 
.elds. Technical report, Technical Report MIT-LCS-TR-778, May 1999. [14] S. Kang. A survey of image-based 
rendering techniques. In VideoMetrics, SPIE Vol. 3641, pages 2 16, 1999. [15] J. Lengyel. The convergence 
of graphics and vision. Technical report, IEEE Computer, July 1998. [16] M. Levoy and P. Hanrahan. Light 
.eld rendering. In Computer Graphics Pro­ceedings, Annual Conference Series, pages 31 42, Proc. SIGGRAPH 
96 (New Orleans), August 1996. ACM SIGGRAPH. [17] Z.-C. Lin and H.-Y. Shum. On the numbers of samples 
needed in light .eld rendering with constant-depth assumption. In Proc. CVPR 2000, 2000. [18] W. Mark, 
L. McMillan, and G. Bishop. Post-rendering 3d warping. In Proc. Symposium on I3D Graphics, pages 7 16, 
1997. [19] L. McMillan and G. Bishop. Plenoptic modeling: An image-based rendering system. Computer Graphics 
(SIGGRAPH 95), pages 39 46, August 1995. [20] H. Schirmacher, W. Heidrich, and H. Seidel. Adaptive acquisition 
of lumi­graphs from synthetic scenes. In Eurographics 99, pages 151 159, Sept 1999. [21] S. M. Seitz 
and C. M. Dyer. View morphing. In Computer Graphics Pro­ceedings, Annual Conference Series, pages 21 
30, Proc. SIGGRAPH 96 (New Orleans), August 1996. ACM SIGGRAPH. [22] J. Shade, S. Gortler, L.-W. He, 
and R. Szeliski. Layered depth images. In Computer Graphics (SIGGRAPH 98) Proceedings, pages 231 242, 
Orlando, July 1998. ACM SIGGRAPH. [23] H.-Y. Shum and L.-W. He. Rendering with concentric mosaics. In 
Proc. SIG-GRAPH 99, pages 299 306, 1999. [24] P. P. Sloan, M. F. Cohen, and S. J. Gortler. Time critical 
lumigraph rendering. In Symposium on Interactive 3D Graphics, pages 17 23, Providence, RI, USA, 1997. 
[25] A. Tekal. Digital Video Processing. Prentice Hall, 1996. A An optical analysis of light .eld rendering 
Similar to [16, 13], we consider the light .eld rendering system as a discrete synthetic aperture optical 
system, as shown in Figure 10. Analogous to the Gaussian optical system, we can de.ne the fol­lowing 
optical parameters: Focal length f; Smallest resolvable feature (on the image plane) d; Aperture D. 
Distance between two adjacent cameras; Circle of confusion c d/f;  Hyperfocal distance DHD/c.  Let 
the plane of perfect focus be at the distance zopt, the mini­mum and maximum distances at which the rendering 
is acceptable be zminand zmax, respectively. The following relations exist ([2], vol. 1, p.1.92) DHzopt 
DHzopt zmin , and zmax , DH+zopt DH;zopt which lead to, 1 11 (+)/2 zopt zmin zmax 1 11 ( ; )/2 DH zmin 
zmax Therefore, to have the best rendering quality, no matter which optical system is used, the focus 
should be always at zopt. More­over, to guarantee the rendering quality, DHhas to be satis.ed, i.e., 
D 11 ( ; )/2 (15) d/fzmin zmax In other words, given the minimum and maximum distances, the maximum 
camera spacing can be determined in order to meet the speci.ed rendering quality. The hyperfocal distance 
describes the relationship among the rendering resolution (circle of confu­sion), the scene geometry 
(depth of .eld) and the number of images needed (synthetic aperture). Intuitively, the minimum sampling 
rate is equivalent to having the maximum disparity less than the small­est resolvable feature on the 
image plane, e.g, camera resolution or one pixel, i.e., d5v 1. The same result was also obtained by Lin 
and Shum [17] using a geometrical approach. Equation 15, not surprisingly, is almost exactly the same 
as Equation 8 because DH 2/hd. However, our approach using spectral analysis of light .eld signals incorporates 
the textural in­formation in the sampling analysis. More detailed optical analysis of light .eld rendering 
can be found in [8]. Focal Maximum Minimum (u,v) (s,t) Pixels Image Spacing length depth depth interval 
interval per image per slab Ltmax Head 160.0 308.79 183.40 0.78125 1.5625 256x256 64x64 4.41 Statue 3000.0 
5817.86 2326.39 15.625 31.25 256x256 64x64 40.38 Table 350.0 3235.47 362.67 2.4306 7.29 288x288 96x96 
5.67 Table 1: A summary of parameters used in three data sets in our experiments. .. . A(2,32). B(4,16). 
C(7,8). D(13,4). . . .. . E(25,2). F(accurate. depth,32). Number.of.Images. 35. 30. 25. 20. 15. 10. 
5.  Number.of.Depth.Layers. .  Rendered. image. A(4,2). Rendered. image. A(8,4).  .. B(8,2). C(10,2). 
B(8,6). C(8,8). D(12,2). E(24,2). D(8,16). E(8,32). . . . NumberfoffImagesf . . NumberfoffImagesf  
 NumberfoffDepthfLayersf . (a) . (b) (b) for the Table scene, when the number of depth layers is 8. 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344933</article_id>
		<sort_key>319</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>35</seq_no>
		<title><![CDATA[An autostereoscopic display]]></title>
		<page_from>319</page_from>
		<page_to>326</page_to>
		<doi_number>10.1145/344779.344933</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344933</url>
		<abstract>
			<par><![CDATA[<p>We present a display device which solves a long-standing problem: to give a true stereoscopic view of simulated objects, without artifacts, to a single unencumbered observer, while allowing the observer to freely change position and head rotation.</p><p>Based on a novel combination of temporal and spatial multiplexing, this technique will enable artifact-free stereo to become a standard feature of display screens, without requiring the use of special eyewear. The availability of this technology may significantly impact CAD and CHI applications, as well as entertainment graphics. The underlying algorithms and system architecture are described, as well as hardware and software aspects of the implementation.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[graphics hardware]]></kw>
			<kw><![CDATA[hardware systems]]></kw>
			<kw><![CDATA[object tracking]]></kw>
			<kw><![CDATA[optics]]></kw>
			<kw><![CDATA[user interface hardware]]></kw>
			<kw><![CDATA[virtual reality]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>J.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010439</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39081726</person_id>
				<author_profile_id><![CDATA[81100250413]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Perlin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Media Research Laboratory, Dept. of Computer Science, New York University, 719 Broadway, 12th Floor, New York, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P258859</person_id>
				<author_profile_id><![CDATA[81100611918]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Salvatore]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Paxia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Media Research Laboratory, Dept. of Computer Science, New York University, 719 Broadway, 12th Floor, New York, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P142243</person_id>
				<author_profile_id><![CDATA[81332509698]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Joel]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Kollin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Media Research Laboratory, Dept. of Computer Science, New York University, 719 Broadway, 12th Floor, New York, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[S. Benton, T.E. Slowe, A.B. Kropp, and S.L. Smith, Micropolarizer-based Multiple-Viewer Autostereoscopic display. SPIE Proceedings Volume 3639: Stereoscopic Displays and Virtual Reality Systems VI, (SPIE January 1999) paper 3639-10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[S. Benton. The Second Generation of the MIT Holographic Video System. In: J. Tsujiuchi, J. Hamasaki, and M. Wada, eds. +Proc. of the TAO First International Symposium on Three Dimensional Image Communication Technologies. Tokyo, 6-7 December 1993. Telecommunications Advancement Organization of Japan, Tokyo, 1993, pp. S-3-1 - 1 to -6.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[R. B6rner. Three Autostereoscopic 1.25m Diagonal Rear Projection Systems with Tracking Features. IDW'97, Proc. of 4th Int'l Display Workshop, Nagoya, Japan, Nov. 1997, p.835-838]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166153</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[S. Chen and L. Williams. View Interpolation for Image Synthesis. Computer Graphics (SIGGRAPH 93 Conference Proc.) p.279-288.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Displaytech: http ://www.displaytech.com/shutters.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Elizabeth Downing et.al. A Three-Color, Solid-State, Three-Dimensional Display. Science 273,5279 (Aug. 30, 1996), pp. 1185-118.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[D. Drascic, J. Grodski. Defence Teleoperation and Stereoscopic Video. Proc SPIE Vol. 1915, Stereoscopic Displays and Applications IV, pages 58-69, San Jose, California, Feb 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[J. Eichenlaub. Multiperspective Look-around Autostereoscopic Projection Display using an ICFLCD. Proc. SPIE Vol. 3639, p. 110-121, Stereoscopic Displays and Virtual Reality Systems VI, John O. Merritt; Mark T. Bolas; Scott S. Fisher; Eds.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[J. Eichenlaub, Lightweight Compact 2D/3D Autostereoscopic LCD Backlight for Games, Monitor, and Notebook Applications. Proc. SPIE Vol. 3295, p. 180-185, in Stereoscopic Displays and Virtual Reality Systems V, Mark T. Bolas; Scott S. Fisher; John O. Merritt; Eds. April 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[M. Flickner: http://www.almaden.ibm.com/cs/blueeyes/find.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134491</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[M. Grewal, A. Andrews, Kalman Filtering: Theory and Practice, Prentice Hall, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[T. Kanade, et al. Development of a Video Rate Stereo Machine. Proc. of International Robotics and Systems Conference (IROS-95), Pittsburgh, PA, August 7-9, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[L. Lipton, et. al., U.S. Patent #4,523,226, Stereoscopic Television System, June 11,1985]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[L. Lipton, and J. Halnon. Universal Electronic Stereoscopic Display. Stereoscopic Displays and Virtual Reality Systems III, Vol. 2653, pp. 219- 223, SPIE, 1996]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[LXD: http ://www.lxdinc.com/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[J.R. Moore, N.A. Dodgson, A.R.L. Travis and S.R. Lang. Time- Multiplexed Color Autostereoscopic Display. Proc. SPIE 2653, SPIE Symposium on Stereoscopic Displays and Applications VII, San Jose, California, Jan 28-Feb 2, 1996, pp. 10-19.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Okoshi, T. Three-Dimensional Imaging Techniques. Academic Press, New York 1976. ISBN 0-12-525250-1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Origin Systems: http://www.orin.com/3dtrack/dyst.htm]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[A. Schwerdtner and H. Heidrich. Dresden 3D display (D4D). SPIE Vol. 3295, p. 203-210, Stereoscopic Displays and Virtual Reality Systems V, Mark T. Bolas; Scott S. Fisher; John O. Merritt; Eds.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[P. St.-Hillaire, M. Lucente, J.D. Sutter, R. Pappu, C.J.Sparrell, and S. Benton. Scaling up the MIT Holographic Video System. Proc. of the Fifth International Symposium on Display Holography (Lake Forest College, July 18-22, 1994), SPIE, Bellingham, WA, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Texas Instruments: http://www.ti.com/dlp]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>169276</ref_obj_id>
				<ref_obj_pid>169238</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[R. Williams. Volumetric Three Dimensional Display Technology in D. McAllister (Ed.) Stereo Computer Graphics and other True 3D Technologies, 1993]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[G. J. Woodgate, D. Ezra, et.al. Observer-tracking Autostereoscopic 3D display systems. Proc. SPIE Vol. 3012, p. 187-198, Stereoscopic Displays and Virtual Reality Systems IV, Scott S. Fisher; John O. Merritt; Mark T. Bolas; Eds.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Xiinx: http://WWW.xilinx.com/]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 An Autostereoscopic Display Ken Perlin, Salvatore Paxia, Joel S. Kollin Media Research Laboratory*, 
Dept. of Computer Science, New York University ABSTRACT We present a display device which solves a long-standing 
problem: to give a true stereoscopic view of simulated objects, without artifacts, to a single unencumbered 
observer, while allowing the observer to freely change position and head rotation. Based on a novel combination 
of temporal and spatial multiplexing, this technique will enable artifact-free stereo to become a standard 
feature of display screens, without requiring the use of special eyewear. The availability of this technology 
may significantly impact CAD and CHI applications, as well as entertainment graphics. The underlying 
algorithms and system architecture are described, as well as hardware and software aspects of the implementation. 
 Keywords graphics hardware, hardware systems, object tracking, optics, user interface hardware, virtual 
reality. 1 INTRODUCTION 1.1 Prior and Related Work Computer graphics, even when rendered in high quality, 
still appears flat when displayed on a flat monitor. Various approaches toward creating true stereoscopy 
have been proposed so that the objects we simulate will look as though they are really in front of us 
[Okoshi, Lipton]. These fall into various categories. The most common form of stereo display uses shuttered 
or passively polarized eyewear, in which the observer wears eyewear that blocks one of two displayed 
images from each eye. Examples include passively polarized glasses, and rapidly alternating shuttered 
glasses [Lipton85]. These techniques have become workhorses for professional uses, such as molecular 
modeling and some subfields of CAD. But they have not found wide acceptance for three dimensional viewing 
among most students, educators, graphic designers, CAD users (such as engineers and architects), or consumers 
(such as computer games players). Studies have shown that observers tend to dislike wearing any invasive 
equipment over their eyes, or wearing anything that impairs their general ambient visual acuity [Drascic]. 
This consideration has motivated a number of non-invasive approaches to stereoscopic display that do 
not require the observer to don special eyewear. *Mailing Address: 719 Broadway, 12th Floor, New York, 
NY 10003 URL: http://www.mrl.nyu.edu/perlin/ Permission to make digital or hard copies of part or all 
of this work or personal or classroom use is granted without fee provided that copies are not made or 
distributed for profit or commercial advantage and that copies bear this notice and the full citation 
on the first page. To copy otherwise, to republish, to post on servers, or to redistribute to lists, 
requires prior specific permission and/or a fee. SIGGRAPH 2000, New Orleans, LA USA &#38;#169; ACM 2000 
1-58113-208-5/00/07 ...$5.00 A graphical display is termed autostereoscopic when all of the work of 
stereo separation is done by the display [Eichenlaub98], so that the observer need not wear special eyewear. 
A number of researchers have developed displays which present a different image to each eye, so long 
as the observer remains fixed at a particular location in space. Most of these are variations on the 
parallax barrier method, in which a fine vertical grating or lenticular lens array is placed in front 
of a display screen. If the observer s eyes remain fixed at a particular location in space, then one 
eye can see only the even display pixels through the grating or lens array, and the other eye can see 
only the odd display pixels. This set of techniques has two notable drawbacks: (i) the observer must 
remain in a fixed position, and (ii) each eye sees only half the horizontal screen resolution. Holographic 
and pseudo-holographic displays output a partial light-field, computing many different views simultaneously. 
This has the potential to allow many observers to see the same object simultaneously, but of course it 
requires far greater computation than is required by two-view stereo for a single observer. Generally 
only a 3D lightfield is generated, reproducing only horizontal, not vertical parallax. A display which 
creates a light field by holographic light-wave interference was constructed at MIT by [Benton]. The 
result was of very low resolution, but it showed the eventual feasibility of such an approach. Discrete 
light-field displays created by [Moore], and the recent work by Eichenlaub [Eichenlaub99], produce up 
to 24 discrete viewing zones, each with a different computed or pre-stored image. As each of the observer 
s eyes transitions from zone to zone, the image appears to jump to the next zone. A sense of depth due 
to stereo disparity is perceived by any observer whose two eyes are in two different zones. Direct volumetric 
displays have been created by a number of researchers, such as [Downing], [Williams] and [Woodgate]. 
One commercial example of such a display is [Actuality]. A volumetric display does not create a true 
lightfield, since volume elements do not block each other. The effect is of a volumetric collection of 
glowing points of light, visible from any point of view as a glowing ghostlike image. Autostereoscopic 
displays that adjust in a coarse way as the observer moves have been demonstrated by [Woodgate]. The 
Dresden display [Schwerdtner] mechanically moves a parallax barrier side-to-side and slightly forward/back, 
in response to the observer s position. Because of the mechanical nature of this adjustment, there is 
significant "settling time" (and therefore latency) between the time the observer moves and the time 
the screen has adjusted to follow. In both of these displays, accuracy is limited by the need to adjust 
some component at sub-pixel sizes. 1.2 Goals The goals of our research have been to present a single 
observer with an artifact-free autostereoscopic view of simulated or remotely transmitted three dimensional 
scenes. The observer should be able to move or rotate their head freely in three dimensions, while always 
perceiving proper stereo separation. The subjective experience should simply be that the monitor is displaying 
a three dimensional object. In order to be of practical benefit, we sought a solution that could be widely 
adopted without great expense. We also wanted a solution that would not suffer from the factor-of-two 
loss of horizontal resolution which is endemic to parallax barrier systems. These goals imposed certain 
design constraints. The user responsive adjustment could not contain mechanically moving parts, since 
that would introduce unacceptable latency. The mechanism could not rely on very high cost components. 
We also wanted the device to be able to migrate to a flat screen technology. Because we made certain 
simplifying design decisions for our first prototype, our initial test system displays only monochromatic 
images. However, this is not an inherent limitation of the technique. 1.3 Significance The significance 
of this work is in that it enables a graphic display to assume many of the properties of a true three 
dimensional object. An unencumbered observer can walk up to an object and look at it from an arbitrary 
distance and angle, and the object will remain in a consistent spatial position. For many practical purposes, 
the graphic display subjectively becomes a three dimensional object. When combined with haptic response, 
this object could be manipulated in many of the ways that a real object can. Ubiquitous non-invasive 
stereo displays hold the promise of fundamentally changing the graphical user interface, allowing CAD 
program designers, creators of educational materials, and authors of Web interfaces (to cite only some 
application domains) to create interfaces which allow users to interact within a true three dimensional 
space.  2 PRINCIPLE COMBINE SPATIAL MULTIPLEXING WITH TEMPORAL MULTIPLEXING 2.1 High level approach 
We addressed our design goals by creating a modified parallax barrier that combines spatial multiplexing 
and temporal multiplexing. Since no fixed parallax barrier geometry could accommodate arbitrary observer 
position and orientation, we create a dynamically varying parallax barrier, one that continually changes 
the width and positions of its stripes as the observer moves. The use of a virtual dynamic parallax barrier 
is reminiscent of work by [Moore] and [Eichenlaub99], but to very different ends - instead of using a 
fixed dynamic pattern to create a fixed set of viewpoints, our goal is to create a result which is continually 
exact for one moving user. As we shall show, each dynamic stripe needs to be highly variable in its width, 
in order to accommodate many different positions and orientations of the observer. For this reason, we 
make the dynamic stripes rather large, and use a correspondingly large gap between the display screen 
and the light-blocking parallax barrier. Because the stripes are large enough to be easily visible, we 
need to make them somehow unnoticeable. To do this, we rapidly animated them in a lateral direction. 
The observer then cannot perceive the individual stripes, just as a passenger in a car speeding alongside 
a picket fence cannot see the individual fence posts. This large-stripe approach requires each stripe 
to be composed from some number of very slender microstripes, each of which is an individually switchable 
liquid crystal display element. To sum up: we use a dynamic parallax barrier consisting of very large 
stripes, which are made out of many slender ones, and we move these large stripes so rapidly across the 
image that the observer cannot perceive them. 2.2 Three phases In a perfect world, a temporally multiplexed 
system could be made from just two alternating phases. Parallax barrier systems depend on the distance 
E between an observer s two eyes (generally about 2.5 inches). Suppose that a display screen D inches 
away from the observer showed alternating stripes of a left and a right image. Suppose also that a light-blocking 
shutter were placed G inches in front of this display screen in a "picket fence" stripe pattern. If the 
width of each shutter stripe were chosen as E*G/D, and the width of each image stripe as E*G/(D-G), then 
during phase 1 the observer s left eye would be able to see half of one image through the clear stripes, 
and the observer s right eye would be able to see half of the other image through the clear stripes [Figure 
1a]. If the light-blocking shutter were then flipped, and the display screen pattern simultaneously changed, 
then the observer would see the remainder of each respective image [Figure 1b]. If this flipping were 
done fast enough, then the observer would perceive two complete independent images, each visible only 
to one eye. The problem with this scenario is that the observer would need to be in precisely the correct 
position; the slightest deviation to the left or right would result in the wrong eye seeing a sliver 
of the wrong image. For this reason, we animate the stripes in three phases. During each phase, the light-blocking 
shutter lets through only one third of each stripe. After each phase the stripe pattern is shifted laterally. 
Over the course of three phases, the observer s left eye sees one entire image, and the observer s eye 
sees a different entire image. The use of three phases guarantees that there is room for error in the 
observer s lateral position [Figures 2a,2b,2c]. 2.3 Varying distance The observer can be at a wide range 
of distances, since we can always vary the stripe width so as to equal E*G/D, as described above. [Figure 
3a] shows the observer relatively far; [Figure 3b] shows the observer much closer. Microstripe resolution 
puts a practical upper limit on the observer distance, since the stripes become narrower as the observer 
s distance to the screen increases. This upper limit increases linearly both with the gap between the 
display and shutter, and with the shutter resolution. In practice, we have set these so as to be able 
to handle an observer up to about five feet away. 2.4 Head rotation In previous autostereoscopic techniques 
based on parallax barriers, all stripes were required to be of equal width. This presents a problem if 
the observer s head is facing off to the side. This will often be true when the observer has other displays 
or paperwork in his field of view, or is engaged in conversation with a colleague. In this case, one 
of the observer s eyes will be perhaps an inch or so closer to the screen than the other. When this happens, 
it no longer suffices for the barrier stripes to be all of equal width. Rather, in this case the stripes 
should vary in width in a perspective-linear pattern [Figure 4]. Our dynamically varying stripe generation 
handles this case accurately. Given any two eye positions, we compute and display the proper perspective 
linear stripe pattern. The mathematics to support this are developed in the next section. 2.5 Positioning 
the stripes In this section we develop the mathematics needed to properly place the stripes. To make 
the light blocking work properly, we need to interleave the left and right images on the display and 
also to create a corresponding set of opaque/clear stripes on the optical shutter. To compute where the 
stripes should go, we use a system of crossed lines: Starting from the right eye and the left-most point 
on the display, draw a straight line, and see where it crosses the shutter. Then draw a line from the 
left eye through this point on the shutter, and see where this new line hits the display. This process 
is continued, always starting with this next point over on the display, to produce an effective pattern 
of left/right image display stripes and light-blocking shutter stripes for that pair of eye positions. 
Starting at one side of the display, we cross the lines on the shutter as follows: 1. Draw a line from 
xn on the display, through the shutter, to the right eye; 2. Draw a line from the left eye, through 
the shutter, to xn+1 on the display; 3. Iterate  [Figures 5a, 5b] show how we construct a sequence 
of stripe positions from two eye positions (shown as a green and red dot, respectively), a display surface 
(shown as the bottom of the two horizontal lines) and a shutter surface (shown as the top of the two 
horizontal lines). Starting from the left side of the display screen, we calculate the line of sight 
through the shutter to the right eye. Then we compute the line of sight from the left eye, through this 
point, down onto the display screen. [Figure 5a] shows this process after one iteration; [Figure 5b] 
shows the same process after three iterations. In these figures, the positions at which the shutter needs 
to be transparent are circled in gray. We now describe the mathematical details for this process. To 
place the stripes properly on the display screen, assume the two eye positions are: p=(px,py) and q=(qx,qy), 
that the display screen is on the line y=0, and that the shutter is on the line y=1. Given a location 
(x,0) on the display screen, we find the line-of-sight location fp(x) on the shutter that lies between 
display screen location (x,0) and eye position p by linear interpolation: fp(x) = px py-1 + x (1 - py-1) 
 Given a location (x,1) on the shutter, we can find the corresponding line-of-sight location on the display 
screen by inverting the above equation: fp-1(x) = (x - px py-1) / (1 - py-1) Therefore, given a location 
xn on the display screen that is visible through a clear stripe on the shutter from both p and q, the 
next such location is given first by finding the location on the shutter fp(xn) in the line-of-sight 
from p, and then finding the corresponding location on the display screen which is in the line­of-sight 
from q: xn+1 = fq-1(fp(xn)) which expands out to: (px py-1 + x (1 - py-1) - qx qy-1) / (1 - qy-1) This 
can be expressed as a linear equation xn+1 = A xn + B, where: A = x (1 - py-1) / (1 - qy-1) B = (px py-1 
- qx qy-1) / (1 - qy-1) The nth location in the sequence of stripe locations on the display screen can 
be calculated by iterating xn+1 = A xn + B: x0 = 0 x1 = B x2 = AB + B x3 = A2B + AB + B xn = B (An-1 
+ ... + A + 1) In the above sequence, the even terms locate the centers of those portions of the image 
visible from the right eye, and the odd terms locate the centers of those portions of the image visible 
from the left eye. The openings in the shutter are centered at f-1 q(x0), f-1q(x2), etc.   3 IMPLEMENTATION 
Various physical arrangements could be used to implement this technique. For our first implementation, 
we used an approach that would allow us the greatest flexibility and ability to conduct tests. For the 
display screen, we used a Digital Light Processor (DLP) micro-mirror projector from Texas Instruments 
[TexasInstr], because DLP projectors handle R,G,B sequentially. This allowed us to use color to encode 
the three time-sequential phases. We used a Ferroelectric Liquid Crystal (FLC) element from [Displaytech] 
to shutter the start/stop time of each temporal phase. For the light-blocking shutter, we had a custom 
pi-cell liquid crystal screen built to our specifications by [LXD], which we drove from power ICs mounted 
on a custom-made Printed Circuit Board (PCB). To control the sub-frame timings, we used a Field Programmable 
Gate Array (FPGA) from [Xilinx]. These were all driven from a Pentium II PC, running OpenGL in Windows 
NT. 3.1 Architecture As flowcharted in [Figure 8] the steps to display a frame are: An eye tracker locates 
the observer s eyes, and sends this (1) information to the CPU. The main CPU uses the eye tracker info 
to render two 3D (2) scenes: one as seen from each eye. The main CPU also uses the eye tracker info 
to compute, for each of three phases, the proper left/right alternation (3) pattern. These are interleaved 
into three successive time phases as red, green, and blue, respectively. The main CPU also uses the 
eye info to compute the three phases of stripe on the light shutter. These are encoded (4) into three 
one-dimensional bit-maps, each indicating an on-off pattern for the shutter micro-stripes at one of the 
three phases. These bit-maps are shipped to the FPGA. The FPGA sends the three bit-patterns to the pi-cell 
light shutter in rotating sequence, every 1/180 second. The (5) timing for this is controlled by the 
DLP projector, which produces a signal every time its color wheel advances. The DLP projector displays 
the three image phases in succession. The color wheel on the projector is removed, (6) so that each of 
the red, green, and blue components displays as a gray scale image. The FLC element is modulated by 
the FPGA to block the (7) light from the DLP projector lens in a 180 Hz square wave pattern. This allows 
finer control over timing. A rear projection screen (RPS) diffuses the image from  (8)  the DLP projector. 
The pi-cell light shutter positioned in front of the RPS (9) displays a different horizontally varying 
on-off pattern every 1/180 second. Steps (5) through (9) above are part of the real-time subsystem 
which is monitored by the FPGA. These parts of the process are monitored continuously by the FPGA to 
synchronize all the events which must occur simultaneously 180 times per second. Creating the three phased 
images We use OpenGL to encode the red/green/blue sub-images which the DLP projector will turn into time 
sequential phases. To do this, we first render the compute separate left and right images in OpenGL, 
into off-screen buffers, as show in [Figures 6a,6b]. Then we slice each of these into their component 
image stripes, and reconstruct into three interleaved images that will be displayed in rapid sequence, 
as red, green, and blue components, as shown in [Figures 7a,7b,7c], respectively. If this image were 
simply displayed on an unenhanced monitor, it would appear as in [Figure 9]. When filtered through the 
light­blocking shutter, each of the observer s eyes will reconstruct a complete image from a single viewpoint. 
If the DLP projector s color wheel were engaged, then the left and right eyes would see [Figure 10a] 
and [Figure 10b], respectively. With the color wheel removed, each of the observer s eyes simply sees 
the correct stereo component image of [Figure 6a] and [Figure 6b], respectively. Timing requirements 
There are two types of timing we need to address for this display: frame time, and shutter switching 
time. In order to prevent eyestrain due to movement latency, we ideally want to maintain a frame refresh 
rate of at least 60 Hz, with a latency within 1/60 second between the moment the observer s head moves 
and the moment the correct image is seen. This consideration drove the timing design goals for the display: 
to be able to respond within the 1/60 interval from one screen refresh to the next. Within this time 
window, we make standard assumptions: that there is a known and fixed small latency to compute a frame, 
and that a Kalman filter [Grewal] can extrapolate from recent eye-tracking samples to predict reasonable 
eye positions at the moment of the next display refresh. If the user s head is moving, then the host 
computer should ideally compute the left and right images and merge them within this 1/60 second window. 
The real-time subsystem maintains a more stringent schedule: a synchronous 180 Hz cycle. The pattern 
on the light-shutter needs to switch at the same moment that the DLP projector begins its red, green, 
or blue component. This timing task is handled by the FPGA, which reads a signal produced by the projector 
every time it the color wheel cycles (about once every 1/180 second) and responds by cycling the light 
shutter pattern. To help tune the on/off timing, the FPGA modulates a ferro-electric optical switch which 
is mounted in front of the projector lens. The main CPU is not involved at all in this fine-grained timing. 
The only tasks required of the CPU are to produce left/right images, to interleave them to create a red/green/blue 
composite, and to put the result into an on-screen frame buffer, ideally (but not critically) at 60 frames 
per second.  3.2 The Parts The essential components we used to implement this process are shown in the 
photograph [Figure 11] below. In this section, each is described in some detail. FPGA Every 1/180 of 
a second (three times per frame, from the observer s point of view), we need to update the light shutter 
with a different phase pattern of on/off stripes. To do this quickly enough, we built an ISA interface 
board with a non volatile Xilinx 95C108 PLD and a reconfigurable Xilinx XC4005E FPGA. The PLD is used 
to generate the ISA Bus Chip Select signals and to reprogram the FPGA. The XC4005E is large enough to 
contain six 256 bit Dual Ported RAMs (to double buffer the shutter masks needed for our three phases), 
the ISA Bus logic, and all the hardware needed to process the DLP signals and drive the pi-cell. When 
loaded with the three desired patterns from the main CPU, this chip continually monitors the color wheel 
signals from the DLP projector. Each time it detects a change from red to green, green to blue, or blue 
to red, it sends the proper signals to the Supertex HV57708 high voltage Serial to parallel converters 
mounted on the Pi-cell, switching each of the light shutter s 256 microstripes on or off. Pi-cell A 
standard twisted nematic liquid crystal display (such as is widely used in notebook computers) does not 
have the switching speed we need; requiring about 20 msec to relax from its on state to its off state 
after charge has been removed. Instead, we use a pi­cell, which is a form of liquid crystal material 
in which the crystals twist by 180o (hence the name) rather than that 90o twist used for twisted nematic 
LC displays. Pi-cells have not been widely used partly because they tend to be bistable -they tend to 
snap to either one polarization or another This makes it difficult to use them for gray scale modulation. 
On the other hand, they will relax after a charge has been removed far more rapidly than will twisted 
nematic - a pi-cell display can be driven to create a reasonable square wave at 200 Hz. This is precisely 
the characteristic we need -an on-off light blocking device that can be rapidly switched. Cost would 
be comparable to that of twisted nematic LC displays, if produced at comparable quantities. [Figure 12a] 
and [Figure 12b] show the pi-cell device that was manufactured for us by [LXD]. The image to the left 
shows the size of the screen, the close-up image to the right shows the individual microstripes and edge 
connectors. The active area is 14"x12", and the microstripes run vertically, 20 per inch. The microstripe 
density could easily have exceeded 100 per inch, but the density chosen required us to drive only 256 
microstripes, and was sufficient for a first prototype. Edge connectors for the even microstripes run 
along the bottom; edge connectors for the odd microstripes run along the top. We used four power chips 
to maintain the required 40 volts, each with 64 pin-outs. Two chips drive the 128 even microstripes from 
a PCB on the top of the shutter, the other two drive the 128 odd microstripes from a PCB along the bottom. 
To turn a microstripe transparent, we drive it with a 5 volt square wave at 180 Hz. To turn a microstripe 
opaque, we drive it with a 40 volt square wave at 180 Hz. Ferro-electric optical switch A ferro-electric 
liquid crystal (FLC) will switch even faster than will a pi-cell, since it has a natural bias that allows 
it to be actively driven from the on-state to the off-state and back again. A ferro­electric element 
can be switched in 70 microseconds. Unfortunately ferro-electric elements are very delicate and expensive 
to manufacture at large scales, and would therefore be impractical to use as our light shutter. However, 
at small sizes they are quite practical and robust to work with. We use a small ferro-electric switch 
over the projector lens, manufactured by Displaytech [Displaytech], to provide a sharper cut-off between 
the three phases of the shutter sequence. We periodically close this element between the respective red, 
green, and blue phases of the DLP projector s cycle. While the FLC is closed, we effect the pi-cell microstripes 
transitions (which require about 1.2 ms). User tracking After surveying a number of different non-invasive 
eye tracking technologies available, we settled on the use of retroreflective camera based tracking. 
Because the back of the human eyeball is spherical, the eye will return light directly back to its source. 
A system based on this principle sends a small infrared light from the direction of a camera during only 
the even video fields. The difference image between the even and odd video fields will show only two 
glowing spots, locating the observer s left and right eyes, respectively. By placing two such light/camera 
mechanisms side­by-side, and switching them on during opposite fields (left light on during the even 
fields, and right light on during the odd fields), the system is able to simultaneously capture two parallax 
displaced images of the glowing eye spots. The lateral shift between the respective eye spots in these 
two images is measured, to calculate the distance of each eye. The result is two (x,y,z) triplets, one 
for each eye, at every video frame. A Kalman filter [Grewal] is used to smooth out these results and 
to interpolate eye position during the intermediate fields. A number of groups are planning commercial 
deployment of retroreflective-based tracking in some form, including IBM [Flickner]. For calibration 
tests we used the DynaSite from Origin Systems [Origin], which requires the user to wear a retroreflective 
dot, but does not block the user s line of sight. The user tracking provides as a pair of 3D points, 
one for each eye. As noted above, this information is used in three ways. (i) Each of these points is 
used by OpenGL as the eye point from which to render the virtual scene into an offscreen buffer; (ii) 
The proper succession lateral locations for left/right image interleaving is calculated, which is used 
to convert the left/right offscreen images into the three temporally phased images; (iii) The proper 
positions for the light shutter transitions are calculated. This information is converted to three one 
dimensional bit-maps, each indicating an on-off pattern for the shutter micro-stripes at one of the three 
phases. This information is sent to the FPGA, which then sends the proper pattern to the light shutter 
every 1/180 second, synchronously with the three phases of the DLP projector.  3.3 Experience The goals 
of this current research version of the system were (i) low latency and (ii) absence of artifacts. In 
this section we discuss how well our experience matched those goals. The most important question to answer 
is: does it work? The answer is yes. As we expected, the experience is most compelling when objects 
appear to lie near the distance of the display screen, so that stereo disparity is reasonably close to 
focus (which is always in the plane of the projection screen). When the system is properly tuned, the 
experience is compelling; as an observer looks around an object, it appears to float within the viewing 
volume. The observer can look around the object, and can position himself or herself at various distances 
from the screen as well. Special eyewear is not required. The system always kept up with the renderer. 
Our software­implemented renderer did not achieve a consistent 60 frames per second, but rather something 
closer to 30 frames per second. In practice this meant that if the observer darted his/her head about 
too quickly, the tracker could not properly feed the display subsystem when the user moved his/her head 
rapidly. The more critical issue is that of position-error based artifacts. Not surprisingly, we have 
found that it is crucial for the system to be calibrated accurately, so that it has a correct internal 
model of the observer s position. If the tracker believes the observer is too near or far away, then 
it will produce the wrong size of stripes, which will appear to the observer as vertical stripe artifacts 
(due to the wrong eye seeing the wrong image) near the sides of the screen. If the tracker believes the 
observer is displaced to the left or right, then this striping pattern will cover the entire display. 
We found in practice that a careful one-time calibration removed all such artifacts. This emphasizes 
the need for good eye position tracking. One artifact we observed, which is exhibited by all polarization­based 
stereoscopic displays, is a small amount of ghosting - a faint trace of the wrong image is seen by each 
eye. This ghosting becomes noticeable when a bright object is placed against a black background. This 
is at least partly due to imperfections in the polarization; some light is scattered in the optical shutter 
and therefore becomes wrongly polarized. Some of this ghosting may also be due to imperfections in the 
timing, so that some light from the wrong phase gets through while the pi-cell shutter is still settling. 
In ongoing work, we plan to test this hypothesis by systematically varying the timing of the ferroelectric 
switch.  4 ONGOING WORK We are designing an alternate version of this display that will work in full 
color with current stereo-ready CRT monitors. This will require a more sophisticated light-blocking shutter, 
since CRT monitors use a progressive scan, rather than displaying an entire image at once. For this reason, 
this version of the shutter will have separately addressable multiple bands from top to bottom, triggered 
at different times within the CRT monitor s scan cycle. This version would be in full color, since it 
will create phase differences by exploiting the time variation between different portions of the full-color 
CRT s vertical scan, instead of relying on sequential R,G,B to produce time phases. In parallel, we are 
working with manufacturers of rapidly switchable flat-panel displays, to create a flat panel version. 
This version would be in full color, since it would not rely on sequential R,G,B. One of our goals for 
this flat-panel based version is a hand-held "gameboy" or "pokémon" size platform, for personal autostereoscopic 
displays. The costs of the pi-cell light shutter and its associated control electronics is roughly proportional 
to display area, which leads us to believe that portable hand-held autostereoscopic displays can be a 
practical low cost platform. This configuration will also depend on the success of ongoing work in the 
development of low cost eye position tracking for handheld platforms. One of our current projects will 
use this display platform for teleconferencing. With a truly non-invasive stereoscopic display, two people 
having a video conversation can perceive the other as though looking across a table. Each person's image 
is transmitted to the other via a video camera that also captures depth [Kanade95]. At the recipient 
end, movements of the observer's head are tracked, and the transmitted depth-enhanced image is interpolated 
to create a proper view from the observer's left and right eyes, as in [Chen]. Head movements by each 
participant reinforce the sense of presence and solidity of the other, and proper eye contact is always 
maintained. We plan to implement an API for game developers, so that users of accelerator boards for 
two-person games can make use of the on-board two-view hardware support provided in those boards to simultaneously 
accelerate left and right views in our display. We are also investigating variants of this system for 
two observers. ACKNOWLEDGEMENTS This work was supported by the NYU Center for Advanced Technology, NY3D 
Inc., and the Interval Research. The authors would also like to thank Clilly Castiglia, Chris Poultney, 
Jay Konopka, Michael Wahrman Dennis Zorin, and everyone else on the 12th floor who provided their time, 
effort and moral support. References Actuality Systems: http://actuality-systems.com/ S. Benton, T.E. 
Slowe, A.B. Kropp, and S.L. Smith, Micropolarizer-based Multiple-Viewer Autostereoscopic display. SPIE 
Proceedings Volume 3639: Stereoscopic Displays and Virtual Reality Systems VI, (SPIE January 1999) paper 
3639-10. S. Benton. The Second Generation of the MIT Holographic Video System. In: J. Tsujiuchi, J. Hamasaki, 
and M. Wada, eds. +Proc. of the TAO First International Symposium on Three Dimensional Image Communication 
Technologies. Tokyo, 6-7 December 1993. Telecommunications Advancement Organization of Japan, Tokyo, 
1993, pp. S-3-1-1 to -6. R. Börner. Three Autostereoscopic 1.25m Diagonal Rear Projection 4th Systems 
with Tracking Features. IDW´97, Proc. of Int l Display Workshop, Nagoya, Japan, Nov. 1997, p.835-838 
S. Chen and L. Williams. View Interpolation for Image Synthesis. Computer Graphics (SIGGRAPH 93 Conference 
Proc.) p.279-288. Displaytech: http://www.displaytech.com/shutters.html Elizabeth Downing et.al. A Three-Color, 
Solid-State, Three-Dimensional Display. Science 273,5279 (Aug. 30, 1996), pp. 1185-118. D. Drascic, J. 
Grodski. Defence Teleoperation and Stereoscopic Video. Proc SPIE Vol. 1915, Stereoscopic Displays and 
Applications IV, pages 58-69, San Jose, California, Feb 1993. J. Eichenlaub. Multiperspective Look-around 
Autostereoscopic Projection Display using an ICFLCD. Proc. SPIE Vol. 3639, p. 110-121, Stereoscopic Displays 
and Virtual Reality Systems VI, John O. Merritt; Mark T. Bolas; Scott S. Fisher; Eds. J. Eichenlaub, 
Lightweight Compact 2D/3D Autostereoscopic LCD Backlight for Games, Monitor, and Notebook Applications. 
Proc. SPIE Vol. 3295, p. 180-185, in Stereoscopic Displays and Virtual Reality Systems V, Mark T. Bolas; 
Scott S. Fisher; John O. Merritt; Eds. April 1998. M. Flickner: http://www.almaden.ibm.com/cs/blueeyes/find.html 
M. Grewal, A. Andrews, Kalman Filtering: Theory and Practice, Prentice Hall, 1993. T. Kanade, et al. 
Development of a Video Rate Stereo Machine. Proc. of International Robotics and Systems Conference (IROS-95), 
Pittsburgh, PA, August 7-9, 1995. L. Lipton, et. al., U.S. Patent #4,523,226, Stereoscopic Television 
System, June 11,1985 L. Lipton, and J. Halnon. Universal Electronic Stereoscopic Display. Stereoscopic 
Displays and Virtual Reality Systems III, Vol. 2653, pp. 219­223, SPIE, 1996 LXD: http://www.lxdinc.com/ 
J.R. Moore, N.A. Dodgson, A.R.L. Travis and S.R. Lang. Time-Multiplexed Color Autostereoscopic Display. 
Proc. SPIE 2653, SPIE Symposium on Stereoscopic Displays and Applications VII, San Jose, California, 
Jan 28-Feb 2, 1996, pp. 10-19. Okoshi, T. Three-Dimensional Imaging Techniques. Academic Press, New York 
1976. ISBN 0-12-525250-1. Origin Systems: http://www.orin.com/3dtrack/dyst.htm A. Schwerdtner and H. 
Heidrich. Dresden 3D display (D4D). SPIE Vol. 3295, p. 203-210, Stereoscopic Displays and Virtual Reality 
Systems V, Mark T. Bolas; Scott S. Fisher; John O. Merritt; Eds. P. St.-Hillaire, M. Lucente, J.D. Sutter, 
R. Pappu, C.J.Sparrell, and S. Benton. Scaling up the MIT Holographic Video System. Proc. of the Fifth 
International Symposium on Display Holography (Lake Forest College, July 18-22, 1994), SPIE, Bellingham, 
WA, 1995. Texas Instruments: http://www.ti.com/dlp R. Williams. Volumetric Three Dimensional Display 
Technology in D. McAllister (Ed.) Stereo Computer Graphics and other True 3D Technologies, 1993 G. J. 
Woodgate, D. Ezra, et.al. Observer-tracking Autostereoscopic 3D display systems. Proc. SPIE Vol. 3012, 
p.187-198, Stereoscopic Displays and Virtual Reality Systems IV, Scott S. Fisher; John O. Merritt; Mark 
T. Bolas; Eds. Xilinx: http://www.xilinx.com/  figure 1a figure 1b figure 2a figure 2b figure 2c figure 
4  figure 6a figure 6b figure 3a figure 3b figure 5a figure 5b figure 7a figure 7b figure 7c  figure 
11  figure 9 figure 10a figure 10b    
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344935</article_id>
		<sort_key>327</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>36</seq_no>
		<title><![CDATA[Silhouette clipping]]></title>
		<page_from>327</page_from>
		<page_to>334</page_to>
		<doi_number>10.1145/344779.344935</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344935</url>
		<abstract>
			<par><![CDATA[<p>Approximating detailed with coarse, texture-mapped meshes results in polygonal silhouettes. To eliminate this artifact, we introduce silhouette clipping, a framework for efficiently clipping the rendering of coarse geometry to the exact silhouette of the original model. The coarse mesh is obtained using progressive hulls, a novel representation with the nesting property required for proper clipping. We describe an improved technique for constructing texture and normal maps over this coarse mesh. Given a perspective view, silhouettes are efficiently extracted from the original mesh using a precomputed search tree. Within the tree, hierarchical culling is achieved using pairs of anchored cones. The extracted silhouette edges are used to set the hardware stencil buffer and alpha buffer, which in turn clip and antialias the rendered coarse geometry. Results demonstrate that silhouette clipping can produce renderings of similar quality to high-resolution meshes in less rendering time.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[level of detail algorithms]]></kw>
			<kw><![CDATA[rendering algorithms]]></kw>
			<kw><![CDATA[texture mapping]]></kw>
			<kw><![CDATA[triangle decimation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010248</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Video segmentation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010243</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Appearance and texture representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010247</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Image segmentation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14034865</person_id>
				<author_profile_id><![CDATA[81319500593]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pedro]]></first_name>
				<middle_name><![CDATA[V.]]></middle_name>
				<last_name><![CDATA[Sander]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Harvard University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP77028616</person_id>
				<author_profile_id><![CDATA[81455605505]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Xianfeng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Harvard University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P269895</person_id>
				<author_profile_id><![CDATA[81100259454]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Steven]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Gortler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Harvard University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P112851</person_id>
				<author_profile_id><![CDATA[81100397561]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hugues]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoppe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39079349</person_id>
				<author_profile_id><![CDATA[81100167784]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Snyder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BLYTHE, D., GRANTHAM, B., NELSON, S., AND MCREYNOLDS, T. Advanced graphics programming techniques using OpenGL. avail from www.opengl.org.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>288224</ref_obj_id>
				<ref_obj_pid>288216</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[CIGNONI, P., MONTANI, C., ROCCHINI, C., AND SCOPIGNO, R. A general method for preserving attribute values on simplified meshes. In Visualization '98 Proceedings, IEEE, pp. 59-66.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280832</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[COHEN, J., OLANO, M., AND MANOCHA, D. Appearancepreserving simplification. SIGGRAPH '98, 115-122.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>91344</ref_obj_id>
				<ref_obj_pid>90397</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[DOBKIN, D. P., AND KIRKPATRICK, D. Determining the separation ofpreprocessed polyhedra - a unified approach. ICALP-90, LNCS 443 (1990), 4OO-413.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[GILBERT, E. G., JOHNSON, D., AND KEERTHI, S. A Fast Procedure for Computing the Distance Between Complex Objects in Three- Dimensional Space. IEEE Journal Of Robotics and Automation, 2 (April 1988), 193-203.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280950</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[GOOCH, A., GOOCH, B., SHIRLEY, P., AND COHEN, E. A Non- Photorealistic Lighting Model for Automatic Technical Illustration. SIGGRAPH 98, 447-452.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>300526</ref_obj_id>
				<ref_obj_pid>300523</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[GOOCH, B., SLOAN, P., GOOCH, A., SHIRLEY, P., AND RIESEN- FELD, R. Interactive Technical Illustration. ACM Symposium on Interactive 3D graphics 1999, 31-38.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Gu, X., GORTLER, S., HOPPE, H., MCMILLAN, L., BROWN, B., AND STONE, A. Silhouette Mapping. Technical Report TR-1-99, Department of Computer Science, Harvard University, March 1999.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[GUiZIEC, A. Surface simplification with variable tolerance. In Proceedings of the Second International Symposium on Medical Robotics and Computer Assisted Surgery (November 1995), pp. 132-139.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[HECKBERT, P., AND GARLAND, M. Survey of polygonal surface simplification algorithms. In Multiresolution surface modeling (SIC- GRAPH '97 Course notes #25). ACM SIGGRAPH, 1997.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237216</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[HOPPE, H. Progressive meshes. SIGGRAPH '96, 99-108.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258843</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[HOPPE, H. View-dependent refinement of progressive meshes. SIC- GRAPH '97, 189-198.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>317260</ref_obj_id>
				<ref_obj_pid>317259</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[JOHANNSEN, A., AND CARTER, M. B. Clustered Backface Culling. Journal of Graphics Tools 3, 1 (1998), 1-14.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[KOENDERINK, J. J. What does the occluding contour tell us about solid shape. Perception 13 (1984), 321-330.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>275483</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[KUMAR, S., MANOCHA, D., GARRETT, W., AND LIN, M. Hierarchical Back-Face Computation. Eurographics Rendering Workshop 1996, 235-244.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>628563</ref_obj_id>
				<ref_obj_pid>628309</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[L AURENTINI, A. The visual hull concept for silhouette based image understanding. IEEE PAM116, 2 (1994), 150-162.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>288288</ref_obj_id>
				<ref_obj_pid>288216</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[LINDSTROM, P., AND TURK, G. Fast and memory efficient polygonal simplification. In Visualization '98 Proceedings, IEEE, pp. 279-286.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258847</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[LUEBKE, D., AND ERIKSON, C. View-dependent simplification of arbitrary polygonal environments. SIGGRAPH '9 7, 199-208.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258894</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[MARKOSIAN, L., KOWALSKI, M., TRYCHIN, S., AND HUGUES, J. Real time non photorealistic rendering. SIGGRAPH '97, 415-420.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[MARUYA, M. Generating texture map from object-surface texture data. Computer Graphics Forum (Proceedings of Eurographics '95) 14, 3 (1995), 397-405.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[NEIDER, J., DAVIS, T., AND WOO, M. OpenGL Programming Guide, Second Edition. Addison-Wesley, 1997.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[NEWELL, M. E., AND SEQUIN, C. The Inside Story on Self- Intersecting Polygons. Lambda 1, 2 (1980), 20-24.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>300539</ref_obj_id>
				<ref_obj_pid>300523</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[RASKAR, R., AND COHEN, M. Image Precision Silhouette Edges. ACM Symposium on Interactive 3D Graphics 1999, 135-140.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[ROSSIGNAC, J., AND VAN EMMERIK, M. Hidden contours on a frame-buffer. Proceedings of the 7th Workshop on Computer Graphics Hardware (1992).]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280878</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[SNYDER, J., AND LENGYEL, J. Visibility Sorting and Compositing without Splitting for Image Layer Decompositions. SIGGRAPH '98, 219-230.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[SoucY, M., GODIN, G., AND RIOUX, M. A texture-mapping approach for the compression of colored 3D triangulations. The Visual Computer 12 (1986), 503-514.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>245627</ref_obj_id>
				<ref_obj_pid>244979</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[XIA, J., AND VARSHNEY, A. Dynamic view-dependent simplification for polygonal models. In Visualization '96 Proceedings, IEEE, pp. 327-334.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Silhouette Clipping Pedro V. Sander Xianfeng Gu Steven J. Gortler Hugues Hoppe John Snyder Harvard University 
Microsoft Research Abstract Approximating detailed models with coarse, texture-mapped meshes results 
in polygonal silhouettes. To eliminate this artifact, we introduce silhouette clipping, a framework for 
ef.ciently clip­ping the rendering of coarse geometry to the exact silhouette of the original model. 
The coarse mesh is obtained using progressive hulls, a novel representation with the nesting property 
required for original mesh proper clipping. We describe an improved technique for construct­ing texture 
and normal maps over this coarse mesh. Given a per­  spective view, silhouettes are ef.ciently extracted 
from the original mesh using a precomputed search tree. Within the tree, hierarchical culling is achieved 
using pairs of anchored cones. The extracted silhouette edges are used to set the hardware stencil buffer 
and al­pha buffer, which in turn clip and antialias the rendered coarse ge­ometry. Results demonstrate 
that silhouette clipping can produce renderings of similar quality to high-resolution meshes in less 
ren­ dering time. Keywords: Level of Detail Algorithms, Rendering Algorithms, Texture Mapping, Triangle 
Decimation.  1 Introduction Rendering detailed surface models requires many triangles, result­ing in 
a geometry processing bottleneck. Previous work shows that such models can be replaced with much coarser 
meshes by captur­ing the color and normal .elds of the surface as texture maps and normal maps respectively 
[2, 3, 20, 26]. Although these techniques offer a good approximation, the coarse geometry betrays itself 
in the polygonal silhouette of the rendering. This is unfortunate since the silhouette is one of the 
strongest visual cues of the shape of an object [14], and moreover the complexity of the silhouette is 
often only O(n) on the number n of faces in the original mesh. In this paper, we introduce silhouette 
clipping, a framework for ef.ciently clipping the rendering of coarse geometry to the exact silhouette 
of the original model. As shown in Figure 1, our system performs the following steps. Preprocess Given 
a dense original mesh: Build a progressive hull representation of the original mesh and extract from 
it a coarse mesh, which has the property that it encloses the original, allowing proper clipping (Section 
3). Construct a texture map and/or normal map over each face of the coarse mesh by sampling the color 
and/or normal .eld of the original mesh (Section 4). Enter the edges of the original mesh into a search 
tree for ef.­cient runtime extraction of silhouette edges (Section 5). http://cs.harvard.edu/ t pvs,xgu,sjg 
http://research.microsoft.com/ t hoppe,johnsny  Permission to make digital or hard copies of part or 
all of this work or personal or classroom use is granted without fee provided that copies are not made 
or distributed for profit or commercial advantage and that copies bear this notice and the full citation 
on the first page. To copy otherwise, to republish, to post on servers, or to redistribute to lists, 
requires prior specific permission and/or a fee. SIGGRAPH 2000, New Orleans, LA USA &#38;#169; ACM 2000 
1-58113-208-5/00/07 ...$5.00 Runtime rendered 2D silhouette silhouette clipped Figure 1: Overview of 
steps in silhouette clipping. Runtime Then, for a given viewpoint: Extract the silhouette edges from 
the search tree (Section 5). Create a mask in the stencil buffer by drawing the silhouette edges as 
triangle fans. Optionally, draw the edges again as an­tialiased lines to set the alpha buffer (Section 
6). Render the coarse mesh with its associated texture/normal maps, but clipped and antialiased using 
the stencil and alpha buffers. Contributions This paper describes: The framework of silhouette clipping, 
whereby low-resolution geometry is rendered with a high-resolution silhouette. A progressive hull data 
structure for representing a nested se­quence of approximating geometries. Within the sequence, any coarser 
mesh completely encloses any .ner mesh. A new method for associating texel coordinates on the coarse 
model with positions on the original model. The association is based on the simple idea of shooting along 
an interpolated surface normal. A scheme for ef.ciently extracting the silhouette edges of a model under 
an arbitrary perspective view. It is inspired by pre­vious work on backface culling [13, 15], but uses 
a convenient anchored cone primitive and a .exible n-ary tree to reduce extraction time. An ef.cient 
technique for setting the stencil buffer given the sil­houetteedges. Special care is taken to overcome 
rasterization bottlenecks by reducing triangle eccentricities. An improvement for ef.ciently antialiasing 
the silhouette with little additional cost. Demonstrations that silhouette clipping produces renderings 
of similar quality to high-resolution meshes in less time. Limitations Only the exterior silhouette is 
used for clipping and antialiasing. Internal silhouettes retain their polygonalized appearance from the 
coarse model. As in other texture mapping schemes, some minor texture slip­ ping can occur, depending 
on the accuracy of the coarse model. Ef.ciency depends on a relative sparsity of silhouettes, and therefore 
breaks down for extremely rough geometry like trees or fractal mountains. The approach only works for 
static models represented by closed, oriented, 2-dimensional manifolds. The stencil setting method assumes 
that the viewpoint is outside the convex hull of the original model. 2 Previous Work Level of Detail/Simpli.cation 
Level-of-detail (LOD) tech­niques adapt mesh complexity to a changing view. The simplest approach precomputes 
a set of view-independent meshes at differ­ent resolutions from which an appropriate approximation is 
selected based on viewer distance (see survey in [10]). A more elaborate ap­proach, termed view-dependent 
LOD [12, 18, 27], locally adapts the approximating mesh. Areas of the surface can be kept coarser if 
they are outside the view frustum, facing away from the viewer, or suf.ciently far away. In particular, 
the view-dependent error metric of Hoppe [12] automatically re.nes near mesh silhouettes. However, a 
cascade of dependencies between re.nement operations causes re.nement in areas adjacent to the silhouette, 
increasing ren­dering load. Also, the ef.ciency of these systems relies on time­coherence of the viewing 
parameters. With silhouette clipping, fewer polygons need to be processed since silhouettes are obtained 
as a 2D post-process. Antialiasing is achieved by processing only the silhouette edges rather than super­sampling 
the entire frame buffer. Texturing Maruya [20] and Soucy et al. [26] de.ne textures over a coarse domain 
by following invertible mappings through a sim­pli.cation process. The shape of the .nal parametrization 
is in.u­enced by the fairly arbitrary sequence of simpli.cation steps. Cignoni et al. [2] describe a 
simple method for de.ning a parametrization using only the geometry of the coarse and .ne models. Each 
position on the coarse model is associated with its closest point on the .ne model. This method often 
creates map­ping discontinuities in concave regions (Figure 4). In Section 4 we present a method that 
instead shoots rays along the interpolated surface normal. Although not guaranteed to produce a one-to-one 
mapping, our parametrization has far fewer discontinuities. Silhouette Extraction Silhouette information 
has been used to enhance artistic renderings of 3D objects [6, 7, 19]. Blythe et al. [1] describe a multipass 
rendering algorithm to draw silhouettes in the screen. Other work highlights the visible silhouette by 
rendering thickened edges [24] or backfaces [23] translated slightly towards the viewpoint. These works 
require the traversal of the entire geo­metric object. A number of algorithms exist for extracting silhouette 
edges from polyhedral models. Markosian et al. [19] describe a proba­bilistic algorithm that tests random 
subsets of edges and exploits view coherence to track contours. Their method is not guaranteed to .nd 
all of the silhouette components, and is too slow for models of high geometric complexity. Gooch et al. 
[7] extract silhouette edges ef.ciently using a hierarchical Gauss map. Their scheme is applicable only 
to orthographic views, whereas ours works for ar­bitrary perspective views. Backface Culling Our method 
for fast silhouette extraction is inspired by previous schemes for fast backface culling. Kumar et al. 
[15] describe an exact test to verify that all faces are back­facing. They reduce its large cost by creating 
a memory-intensive auxiliary data structure that exploits frame-to-frame coherence. Jo­hannsen and Carter 
[13] improve on this by introducing a conserva­tive, constant-time backfacing test. The test is based 
on bounding the backfacing viewpoint region with a constant number of half spaces. In our system we use 
an even simpler anchored cone test primitive. Johannsen and Carter do not address hierarchy construction, 
while Kumar et al. build their hierarchy using a dual space gridding that does not explicitly take into 
account the extraction cost. We de­scribe a general bottom-up clustering strategy, similar to Huffman 
tree construction, that is greedy with respect to predicted extraction cost. In the results section we 
report the advantage of using our method over that of Johannsen and Carter. Silhouette Mapping Our earlier 
system [8] performs silhouette clipping using an approximate silhouette, obtained using interpola­tion 
from a .xed number of precomputed silhouettes. 3 Progressive Hull In order to be properly clipped by 
the high-resolution silhouette, the coarse mesh should completely enclose the original mesh Mn . In this 
section we show how such a coarse mesh can be obtained by representing Mn as a progressive hull a sequence 
of nested approximating meshes M0 M M Mn, such that   (M0) (M1) M M (Mn)  where(M) denotes the set 
of points interior to M. A related con­struction for the special case of convex sets was explored in 
[4]. Interior volume To de.ne interior volume, we assume that Mn is oriented and closed (i.e. it has 
no boundaries). In most cases, it is relatively clear which points lie in(M). The de.nition of interior 
is less obvious in the presence of self-intersections, or when sur­faces are nested (e.g. concentric 
spheres). To determine if a point p R3 lies in (M), select a ray from p off to in.nity, and .nd all intersections 
of the ray with M. Assume without loss of generality that the ray intersects the mesh only within interiors 
of faces (i.e. not on any edges). Each intersection point is assigned a number, +1 or 1, equal to the 
sign of the dot product between the ray direc­tion and the normal of the intersected face. Let the winding 
num­ber wM (p) be the sum of these numbers [22]. Because the mesh is closed, it can be shown that wM 
(p) is independent of the chosen ray. To properly interact with the stencil algorithm described later 
in Section 6, we de.ne interior volume using the positive winding rule as (M)= p R3: wM (p) 0 . Note 
that this description only de.nes interior volume; it is not used in actual processing. Review of progressive 
mesh The progressive hull sequence is an adaptation of the earlier progressive mesh (PM) representa­tion 
[11] developed for level-of-detail control and progressive trans­mission of geometry. The PM representation 
of a mesh Mn is ob­tained by simplifying the mesh through a sequence of n edge col­lapse transformations 
(Figure 3), thus de.ning a dense family of approximating meshes M0 M M Mn . For the purpose of level-of-detail 
control, edge collapses are se­lected so as to best preserve the appearance of the mesh during sim­pli.cation 
(e.g. [3, 10, 11, 17]). We show that proper constraints on  M0 (40 faces) M30 (100 faces) M80 (200 faces) 
M230 (500 faces) M980 (2,000 faces) Original mesh Mn      ecol  Figure 3: The edge collapse transformation. 
the selection of edge collapse transformations allow the creation of PM sequences that are progressive 
hulls. Progressive hull construction For the PM sequence to be a progressive hull, each edge collapse 
transformation Mi+1Mi must satisfy the property (Mi)(Mi+1). A suf.cient condition is to guarantee that, 
at all points in space, the winding number either remains constant or increases: R3 p , wMi+1(p) wMi 
(p). Intuitively, the surface must either remain unchanged or locally move outwards everywhere. Let Fi+1 
and Fi denote the sets of faces in the neighborhood of the edge collapse as shown in Figure 3, and let 
v be the position of the uni.ed vertex in Mi. For each face f Fi+1, we constrain v to lie outside the 
plane containing face f . Note that the outside direction from a face is meaningful since the mesh is 
oriented. The resulting set of linear inequality constraints de.nes a feasible volume for the location 
of v. The feasible volume may be empty, in which case the edge collapse transformation is disallowed. 
The transformation is also disallowed if either Fi or Fi+1 contain self-intersections1. If v lies within 
the feasible volume, it can be shown that the faces Fi cannot intersect any of the faces Fi+1. Therefore, 
Fi.ip(Fi+1) forms a simply connected, non-intersecting, closed mesh enclosing the difference volume between 
Mi and Mi+1. The winding number w(p) is increased by 1 within this difference volume and remains constant 
everywhere else. Therefore, (Mi)(Mi+1). The position v is found with a linear programming algorithm, 
us­ing the above linear inequality constraints and the goal function of minimizing volume. Mesh volume, 
de.ned here as R3 wM (p)dp, p 1We currently hypothesize that preventing self-intersections in Fi and 
Fi+1 may be unnecessary. is a linear function on v that involves the ring of vertices adjacent to v (refer 
to [9, 17]). As in earlier simpli.cation schemes, candidate edge collapses are entered into a priority 
queue according to a cost metric. At each iteration, the edge with the lowest cost is collapsed, and 
the costs of affected edges are recomputed. Various cost metrics are possible. We obtain good results 
simply by minimizing the increase in volume, which matches the goal function used in positioning the 
vertex. Inner and outer hulls The algorithm described so far con­ M M structs a progressive outer hull 
sequence M0Mn . By simply reversing the orientation of the initial mesh, the same con­ struction gives 
rise to an progressive inner hull sequence M0  M M m Mn . Combining these produces a single sequence 
of hulls n0  M0 a M M m Mn a M M m = MM that bounds the original mesh from both sides, as shown in Fig­ure 
2. (Although the surface sometimes self-intersects, interior vol­ume de.ned using the winding number 
rule is still correct.) We expect that this representation will also .nd useful applica­tions in the 
areas of occlusion detection and collision detection, par­ticularly using a selective re.nement framework 
[12, 27]. 4 Texture Creation As in [2, 20, 26], we create a texture tile over each face of the sim­pli.ed 
mesh, and pack these tiles into a rectangular texture image. As illustrated in Figure 1, all tiles are 
right triangles of uniform size. The problem of texture creation is to .ll these tiles using texel values 
(colors or normals) sampled from the original mesh. In­spired by Cignoni et al. [2], our approach constructs 
a parametriza­tion from the simpli.ed mesh to the original mesh based solely on their geometries (i.e. 
independent of the simpli.cation algorithm). Whereas Cignoni et al. use a closest-point parametrization, 
we base our parametrization on a normal-shooting approach, which signi.­cantly reduces the number of 
discontinuities in the parametrization (see Figures 4 and 5). Given the original mesh and a triangle 
T of the simpli.ed mesh, we must determine how to calculate the values assigned to each texel. Our normal-shooting 
algorithm performs the following steps to compute the color or normal at each texel t of T:    coarse 
closest-point normal-shooting  Figure 4: Closest-point parametrization often produces discontinu­ities 
not present with normal-shooting. (a) original mesh (b) closest-point (c) normal-shooting Figure 5: 
Comparison of texturing the coarse mesh us­ing the closest-point parametrization and our normal-shooting 
parametrization. Note the parametric discontinuities in the concave regions for closest-point. Calculate 
the barycentric coordinates of t within the triangle T. Calculate the position p and normaln by interpolating 
the posi­ tions and normals of the vertices of T. Shoot a ray from p in the d n direction. This ray will 
intersect the original mesh at a particular point q. In the extremely rare event of a ray failing to 
hit the original model, we instead use the closest point to p. Given the triangle and barycentric coordinates 
of q in the origi­nal model, interpolate the prelit color or normal of its three ver­tices, and store 
the result in t. We adjust the sampling resolution on the texture tiles depending on the complexities 
of the original and simpli.ed meshes. For the models in Section 7, we sampled 512 texels per coarse face 
on the bunny and holes, but only 128 texels on the dragon, parasaur, and knot since these have many more 
coarse faces. These resolutions are enough to capture the desired level of detail. To allow bilin­ear 
interpolation on the resulting texture, we appropriately pad the triangle texture tiles. 5 Fast Silhouette 
Extraction We consider each geometric edge in the mesh to consist of a pair of opposite-pointing directed 
edges. For a given mesh and viewpoint p, the 3D silhouette is the subset of directed edges whose left 
adja­cent face is frontfacing and whose right adjacent face is backfacing. More formally, a directed 
edge e is on the silhouette if and only if p frontfacing(e. f1) and p frontfacing(e. f2), where the region 
frontfacing(f )= pR3 (p f .v) f .n 0 in which f .v is any vertex of f , and f .n is its outward facing 
normal. Runtime Algorithm Applying this test to all edges in a brute­force manner proves to be too slow. 
Instead, our approach is to enter the edges into a hierarchical search tree, or more properly, a forest. 
Each node in the forest contains a (possibly empty) list of edges to test. Let the face cluster F(n) 
for a node n be the set of faces attached to edges contained in that node and in all of its descendants. 
If for a given viewpoint we can determine that the faces in F(n) are entirely frontfacing or entirely 
backfacing, then none of the edges contained in the node s subtree can be silhouettes, and thus the depth-.rst 
traversal skips the subtree below n. The basic structure of the algorithm is as follows: procedure .ndSilhouetteEdges(node 
n, viewpoint p) if ( p frontfacing(F(n)) or p backfacing(F(n)) ) return; // skip this subtree for edges 
e in n.E if ( p frontfacing(e. f1) and p frontfacing(e. f2) ) output(e); for children c in n.C .ndSilhouetteEdges(c,p); 
The frontfacing and backfacing regions of a face cluster F are de.ned as frontfacing(F)= frontfacing(f 
) and fF backfacing(F)= frontfacing(f ). fF To make hierarchical culling ef.cient, we need a fast, constant­time 
algorithm to conservatively test p frontfacing(F) and p backfacing(F). We do this by approximating these 
regions using two open-ended anchored cones, af and ab, satisfying af frontfacing(F) and ab backfacing(F) 
as shown in Figure 6. Each anchored cone a is speci.ed by an anchor origin a.o, normal a.n , and cone 
angle a. . The construction of these cones will be presented shortly. Each region test then reduces to 
p a cos 1 p a.o a.n a. . p a.o For ef.ciency and to reduce storage, we store in our data struc­ture 
the scaled normal a.n s = a.n cos(a. ). With careful precom­putation, the above test can be then implemented 
with two dot prod­ucts and no square roots or trigonometric operations, via p a (p a.o) a.n s 0 and (p 
a.o) a.n s 2 p a.o 2. Because we construct af and ab to have the same cone angle and opposite cone normals, 
we can test for inclusion in both anchored cones with just two dot products. This is made possible by 
precom­puting and storing the anchor separation d =(af.o ab.o) af.n . For reference, the .nal node data 
structure is: struct node vector scaledNormal; //n s point ffAnchor; // af.o point bfAnchor; // ab.o 
.oat AnchorSeparation; // d edgeList E; childPointerList C; Anchored Cone Construction We .rst .nd the 
cone having the largest angle inside the frontfacing region. It can be shown that the central axisn of 
such a cone has the following property: if one associates a point on the unit sphere with each face normal 
in the cluster, and computes the 3D convex hull of this pointset,n must pass through the closest point 
from the origin to that convex hull. We therefore use Gilbert s algorithm [5] which directly .nds this 
closest point in linear time. (Note that an open-ended cone exists Figure 6: Anchored cones provide conservative 
bounds on the frontfacing and backfacing regions of a set of faces, illustrated here in 2D for the 4 
oriented line segments in blue.  a b parent(a,b) adopt(a,b) merge(a,b) Figure 7: The three join operations. 
if and only if the convex hull does not contain the origin.) The largest cone angle is then easily computed 
as the complement of the maximum angle from n to the set of face normals. In fact, is also the complement 
of the angular distance of n to any vertex in the closest simplex found by Gilbert s algorithm. For a 
given node, we assign af .n = ab.n = n af .= ab.= . We then .nd the best cone origins, af .o and ab.o, 
by solving the linear programs af .o = argmin no and ab.o = argmin no . o frontfacing(F) o backfacing(F) 
Tree Construction We construct our trees in a bottom-up greedy fashion much like the construction of 
Huffman trees. We begin with a forest where each edge is in its own node. Given any two nodes (a,b), 
we allow the following three join operations (see Figure 7). parent(a,b): creates a new node with two 
children a and b. adopt(a,b): gives node b to node a as an additional child node. merge(a,b): creates 
a new node whose edge and child lists are the union of those from a and b. Given these possible join 
operations, the algorithm is as follows: Forest buildOptimalForest(Forest forest) candidates = buildJoinCandidates(forest); 
candidates.heapify(); while (joinOp = candidates.removeTop()) if ( joinOp.cost 0) break; forest.applyJoin(joinOp); 
candidates.updateCosts(joinOp); return forest; Candidate join operations are ordered in the heap by 
their pre­dicted decrease in silhouette extraction cost. The silhouette extrac­tion cost is computed 
as follows. The cost of a forest is simply the sum of the costs of its roots: forestCost = rootCost(r). 
r The cost of a root node is some constant ka for the anchored cone tests, plus the possible cost of 
testing its edges and its children: rootCost(r)= ka + P(r) ke r. E + nodeCost(c, r ) , cr.C where ke 
is the cost for testing an edge, and P(r) is the probability of the node r not being culled2. To compute 
P(r), one must assume some probability distribution over the viewpoints. We assume a uniform distribution 
over a large sphere U, in which case vol(Ur. af r. ab) P(r)= . vol(U) The cost of a non-root node n with 
ancestor set A is computed recursively as: nodeCost(n, A)= ka+P(nA) ke n. E + nodeCost(c, nA) cn.C where 
P(nA) is the probability of the node n not being culled given that its ancestors A were also not culled. 
If one assumes that both anchored cones of a child are contained in its parent s, then vol(Un. af n. 
ab)P(nA)= vol(Up. af p. ab) where p is n s immediate parent. While this containment must be true of a 
node s respective frontfacing and backfacing regions, it is not necessarily true for their approximating 
anchored cones. In practice, numerical experiments have shown this approximation to be reasonable. In 
principle one might consider all n2 pairs of forest roots for candidate join operations. For computational 
ef.ciency during the preprocess, we limit the candidate set in the following way. A can­didate graph 
is initialized with a graph vertex for each root in the initial forest, each representing a single mesh 
edge. Two vertices in the graph are linked if their corresponding mesh edges share the same mesh vertex, 
or if adjacent mesh faces have normals within an angular threshold3. Then during tree construction, when 
two roots are joined, their vertices and links are merged in the candidate graph.  6 Stencil Setting 
The 3D silhouette extracted in the previous section is a set of di­rected edges. Since the mesh is closed 
and the silhouette edges separate frontfacing triangles from backfacing ones, the number of silhouette 
edges adjacent to any vertex must be even. Therefore the edges can be organized (non-uniquely) into a 
set of closed contours. Each such contour projects into the image plane as an oriented 2D polygon, possibly 
with many loops, and possibly self-intersecting. The winding number of this polygon at a 2D image location 
cor­responds to the number of frontfacing surface layers that are seen along the ray from the viewpoint 
through that image location [19]. Our approach is to accumulate these winding numbers in the hard­ware 
stencil buffer for all contours in the 3D silhouette. Then, we clip the coarse geometry to the external 
silhouette of the original ge­ometry by only rendering the coarse model where the stencil buffer values 
are positive. 2We have found that setting ka ke =4 3 gives us the best results. 3In practice we have 
found that ignoring similarity of normals (i.e., only considering mesh proximity) still provides search 
trees that are almost as good, with far less preprocessing time.  (a) original mesh (b) binary stencil 
(c) alpha matte Figure 8: Comparison of rendering the bunny ear using the orig­inal mesh (69,674 face 
model), and using a coarse hull (500 face model) whose silhouette is (b) clipped to the stencil buffer 
and (c) antialiased using the alpha buffer. Basic Algorithm The directed silhouette edges are organized 
into closed contours using a hash table. (For each directed edge, the hash key is the vertex index of 
the source vertex.) In order to render the winding number of each contour into the stencil buffer, we 
use a variation of the standard stencil algorithm for .lling con­cave polygons [21]. Each edge contour 
is drawn as a fan of triangles about an arbitrary center point, which we choose to be the 3D cen­troid 
of the contour vertices. The orientation of each triangle deter­mines whether its rendered pixels increment 
or decrement the sten­cil buffer values. To avoid testing triangle orientations in the CPU, we instead 
render the triangles twice, .rst with backface culling and stencil mode set to increment, and then with 
frontface culling and stencil mode set to decrement, as shown in the pseudocode below. The triangles 
are drawn as triangle fans for ef.ciency. procedure setStencil(contours C, viewpoint p) setStencilToZero(boundingBox(C)); 
cullFace(BACKFACE); for contours c in C point q = centroid(c. E); for edges e in c. E triangle t = makeTriangle(q, 
e.v1, e.v2); rasterizeToStencil(t, INCREMENT); cullFace(FRONTFACE); for contours c in C point q = centroid(c. 
E); for edges e in c. E triangle t = makeTriangle(q, e.v1, e.v2); rasterizeToStencil(t, DECREMENT); setDrawingToPositiveStencil(); 
Although the graphics hardware clips triangle fans to the view frustum, the setStencil algorithm remains 
correct even if parts of the model lie behind the viewer, as long as the viewer remains outside the convex 
hull of the object. This can be tracked ef.ciently by the test used in [25]. Loop Decomposition The basic 
algorithm described so far tends to draw many long, thin triangles. On many rasterizing chips (e.g. NVIDIA 
s TNT2), there is a large penalty for rendering such eccentric triangles. It is easy to show that the 
setStencil algorithm behaves best when the screen-space projection of q has a y coordi­nate at the median 
of the contour vertices. Choosing q as the 3D centroid of the contour vertices serves as a fast approximation. 
To further reduce the eccentricity of the fan triangles, we break up each large contour into a set of 
smaller loops. More precisely, we pick two vertices on the contour, add to the data structure two opposing 
directed edges between these vertices, and proceed as be­fore on the smaller loops thus formed. When 
tested with the NVIDIA s TNT2, loop decomposition gave speedups of up to a factor of 2.3 on models that 
are raster bound on the stencil setting stage.    Model Bunny Dragon Parasaur Knot Holes3 Model complexities 
(number of faces)   Original mesh 69,674 400,000 43,886 185,856 188,416 Coarse hull 500 4,000 1,020 
928 System timings (milliseconds)   Original rendering 34.7 204.7 20.63 81.12 90.3 Silhouette extraction 
4.5 24.2 4.0 6.5 4.0 Stencil setting 2.7 21.5 2.0 2.8 1.0 Coarse rendering 4.8 5.2 4.9 4.9 4.4 Total. 
7.8 50.3 6.9 10.3 5.5 Speedup factor 4.4 4.1 3.0 7.9 (Antialiasing) +3.0 +22.5 +2.9 +3.4 +1.5 Table 
1: Timings of steps in our silhouette clipping scheme, and comparison with rendering the original mesh. 
Total frame times are less than the sum due to parallelism between CPU and graphics.    Model Bunny 
Dragon Parasaur Knot Holes3 Total faces 69,674 400,000 43,866 185,856 188,416 Total edges 104,511 600,000 
65,799 278,784 282,624 Silhouette extraction statistics   Silhouette edges 3,461 23,493 3,227 3,291 
1,737 Tested edges 10,256 67,934 10,938 13,134 5,976 Tested nodes 4,282 26,291 3,538 7,926 4,594 Silhouette 
extraction times (milliseconds)   Our search tree 4.1 28.2 4.3 6.4 3.3 Brute-force 20.4 117.3 12.5 
50.6 51.4 Speedup factor 5.0 4.2 2.9 7.9 15.6 Table 2: Statistics of our silhouette extraction algorithm. 
 Figure 9: Comparison of the average silhouette extraction time with our algorithm and the brute-force 
algorithm, using bunny approxi­mations with 500, 4,000, 20,000, 50,000, and 69,674 faces. Antialiasing 
Although many graphics systems can antialias line segments, triangle antialiasing requires framebuffer 
supersampling which slows rendering except on high-end workstations. As a re­sult, the silhouette typically 
suffers from aliasing artifacts (Fig­ure 8a). The stencil buffer algorithm described in the previous 
sec­tion creates a binary pixel mask, therefore the coarse mesh clipped to this mask exhibits the same 
staircase artifacts (Figure 8b). We can antialias the silhouette by applying line antialiasing on the 
silhouette contour. First, the silhouette edges are ren­dered as antialiased line segments into the alpha 
buffer (using glBlend(GL ONE,GL ZERO)). Second, the stencil buffer is computed as in the previous section. 
This binary stencil is then transferred to the alpha buffer, i.e. pixels interior to the silhouette are 
as­signed alpha values of 1. Finally, the low-resolution geometry is rendered with these alpha buffer 
values using the over opera­tion (glBlend(GL DST ALPHA,GL ONE MINUS DST ALPHA)). The result is shown 
in Figure 8c. As the timings in Table 1 reveal, silhouette antialiasing adds little to the overall time. 
Note that antialiased sil­houette clipping on multiple models involves the non-commutative over operation, 
and thus requires visibility sorting [25].      (a) original mesh (b) simpli.ed mesh (not hull) 
(c) silhouette-clipped coarse hull  7 Results We tested our framework on the .ve models of Table 1. 
The bunny and dragon are from 3D scans at Stanford University. (The dragon was simpli.ed to 400,000 faces; 
the four boundaries in the base of the bunny were closed.) The parasaur is from the Viewpoint library. 
The 3-holed torus and knot are subdivision surfaces tessel­lated .nely to obtain an accurate silhouette. 
We used normal maps for all of our examples. Preprocessing a model consists of building a coarse hull, 
the nor­mal and/or texture map, and the edge search structures. This takes between 30 minutes and 5 hours 
depending on model complexity. We have focused our effort on optimizing the runtime algorithm. Times 
for the substeps of our scheme are shown in Table 1. These are obtained on a PC with a 550MHz Pentium 
III and a Creative Labs Annihilator 256 graphics card based on the NVIDIA GeForce 256 GPU. The execution 
times represent averages over many ran­dom views of the models. Note that the expense of extracting sil­houette 
edges is signi.cantly reduced due to parallelism between the CPU and GPU. For instance, silhouette extraction 
is nearly free for the bunny. We compare our approach of silhouette-clipping a coarse hull with rendering 
the original mesh, and .nd speedups of approximately 3 to 16. For rendering both the coarse hulls and 
the original meshes, we use precomputed triangle strips. Figure 10 compares the image quality of the 
silhouette-clipped coarse hull with a simpli.ed mesh of the same complexity and the original mesh. Figure 
11 indicates that given a .xed amount of resources, our system can render a model with a silhouette of 
much higher resolution than the brute-force method. As shown in Table 2, our hierarchical culling scheme 
results in explicit silhouette testing of only a small fraction of the edges, par­ticularly on the smooth 
models. In all cases, our extraction time is much lower than the brute-force approach of explicitly testing 
all edges. It works much like a quadtree search algorithm, which can .nd all cells that touch a line 
in O( n) time. Figure 9 shows this comparison as a function of silhouette complexity for several simpli.ed 
bunny meshes. The graph indicates that the time for our algorithm increases linearly on the number m 
of silhouette edges in the model, whereas the brute-force time increases linearly on the total number 
n of edges, which in this case is quadratic on m. We implemented Johannsen and Carter s backface culling 
algo­rithm and modi.ed it to extract silhouettes, in order to compare it with our silhouette extraction 
scheme. For this comparison we mea­sured computation based on the number of edges explicitly tested and 
nodes traversed. We did not use wall-clock time because our implementation of Johannsen and Carter was 
not overly optimized. For bunnies with 500, 4000, 20,000, 50,000, and 69,674 faces, our speedup factors 
were 1.1, 1.3, 1.5, 2.0, and 2.1, respectively.  8 Summary and Future Work We have shown that silhouette 
clipping is a practical framework for rendering simpli.ed geometry while preserving the original model 
silhouette. The operations of extracting silhouette edges and setting the stencil buffer can be implemented 
ef.ciently at runtime. With little added cost, silhouette clipping also permits antialiasing of the silhouette, 
a feature previously available only through expensive supersampling. Several areas for future work remain. 
  References BLYTHE, D., GRANTHAM, B., NELSON, S., AND MCREYNOLDS, T. Advanced graphics programming 
techniques using OpenGL. avail from www.opengl.org. CIGNONI, P., MONTANI, C., ROCCHINI, C., AND SCOPIGNO, 
R. A general method for preserving attribute values on simpli.ed meshes. In Visualization 98 Proceedings, 
IEEE, pp. 59 66. COHEN, J., OLANO, M., AND MANOCHA, D. Appearance­preserving simpli.cation. SIGGRAPH 
98, 115 122. DOBKIN, D. P., AND KIRKPATRICK, D. Determining the separation of preprocessed polyhedra 
 a uni.ed approach. ICALP-90, LNCS 443 (1990), 400 413. GILBERT, E. G., JOHNSON, D., AND KEERTHI, S. 
A Fast Proce­dure for Computing the Distance Between Complex Objects in Three-Dimensional Space. IEEE 
Journal Of Robotics and Automation,2 (April 1988), 193 203. GOOCH, A., GOOCH, B., SHIRLEY, P., AND COHEN, 
E. A Non- Photorealistic Lighting Model for Automatic Technical Illustration. SIGGRAPH 98, 447 452. GOOCH, 
B., SLOAN, P., GOOCH, A., SHIRLEY, P., AND RIESEN- FELD, R. Interactive Technical Illustration. ACM Symposium 
on In­ teractive 3D graphics 1999, 31 38. GU, X., GORTLER, S., HOPPE, H., MCMILLAN, L., BROWN, B., AND 
STONE, A. Silhouette Mapping. Technical Report TR-1-99, Department of Computer Science, Harvard University, 
March 1999. GU´ Figure 11: Comparison between silhouette clipping and brute-force rendering. The x-axis 
represents the resolution of the model used for silhouette extraction. The resolution of the coarse hull 
was .xed at 500 faces. The curves represent con.gurations that take the same amount of time to render. 
The star represents the con.guration used in the bunny ear example shown above. The complexity of the 
extracted silhouette should be adapted to the view, since it is obviously unnecessary to extract thousands 
of edges from an object covering a few pixels. Given a set of LOD meshes, our framework can use these 
for silhouette extraction by creating for each one a corresponding coarser hull. Alternatively, all of 
the silhouette meshes and their associated coarse hulls could be extracted from a single progressive 
hull. A related idea is to perform higher-order interpolation on the silhouette using projected derivatives 
or curvatures in addition to 2D points. This would result in smoother silhouettes without extracting 
more silhouette edges. Currently, silhouette clipping only improves the appearance of exterior silhouettes. 
We have considered several approaches for dealing with interior silhouettes. One possibility is to exploit 
the winding number computed in the stencil buffer. Another approach partitions the mesh and applies silhouette 
clipping to each piece in­dependently. We have performed initial experiments along these lines, but have 
not yet obtained a satisfactory solution. Since the exterior silhouette of a shape is determined by its 
visual hull [16], silhouette extraction is unaffected by any simpli.cation of the original mesh that 
preserves its visual hull. As an example, the interior concavity of a bowl can be simpli.ed until it 
spans the bowl s rim. Such simpli.cation offers an opportunity for further reducing silhouette extraction 
cost. Acknowledgments For the models we would like to thank Stanford University and Viewpoint DataLabs. 
We would also like to thank the MIT Lab­oratory for Computer Science for use of their equipment. The 
.rst three authors have been supported in part by the NSF, Sloan Foun­dation, and Microsoft Research. 
and Computer Assisted Surgery (November 1995), pp. 132 139. [10] HECKBERT, P., AND GARLAND, M. Survey 
of polygonal surface simpli.cation algorithms. In Multiresolution surface modeling (SIG-GRAPH 97 Course 
notes #25). ACM SIGGRAPH, 1997. [11] HOPPE, H. Progressive meshes. SIGGRAPH 96, 99 108. [12] HOPPE, H. 
View-dependent re.nement of progressive meshes. SIG-GRAPH 97, 189 198. [13] JOHANNSEN, A., AND CARTER, 
M. B. Clustered Backface Culling. Journal of Graphics Tools 3, 1 (1998), 1 14. [14] KOENDERINK, J. J. 
What does the occluding contour tell us about solid shape. Perception 13 (1984), 321 330. [15] KUMAR, 
S., MANOCHA, D., GARRETT, W., AND LIN, M. Hier­archical Back-Face Computation. Eurographics Rendering 
Workshop 1996, 235 244. [16] LAURENTINI, A. The visual hull concept for silhouette based image understanding. 
IEEE PAMI 16, 2 (1994), 150 162. [17] LINDSTROM,P., AND TURK,G.Fastandmemoryef.cientpolygonal simpli.cation. 
In Visualization 98 Proceedings, IEEE, pp. 279 286. [18] LUEBKE, D., AND ERIKSON, C. View-dependent simpli.cation 
of arbitrary polygonal environments. SIGGRAPH 97, 199 208. [19] MARKOSIAN, L., KOWALSKI, M., TRYCHIN, 
S., AND HUGUES, J. Real time non photorealistic rendering. SIGGRAPH 97, 415 420. [20] MARUYA, M. Generating 
texture map from object-surface texture data. Computer Graphics Forum (Proceedings of Eurographics 95) 
14, 3 (1995), 397 405. [21] NEIDER, J., DAVIS, T., AND WOO, M. OpenGL Programming Guide, Second Edition. 
Addison-Wesley, 1997. [22] NEWELL, M. E., AND SEQUIN, C. The Inside Story on Self-Intersecting Polygons. 
Lambda 1, 2 (1980), 20 24. [23] RASKAR, R., AND COHEN, M. Image Precision Silhouette Edges. ACM Symposium 
on Interactive 3D Graphics 1999, 135 140. [24] ROSSIGNAC, J., AND VAN EMMERIK, M. Hidden contours on 
a frame-buffer. Proceedings of the 7th Workshop on Computer Graphics Hardware (1992). [25] SNYDER, J., 
AND LENGYEL, J. Visibility Sorting and Compositing without Splitting for Image Layer Decompositions. 
SIGGRAPH 98, 219 230. [26] SOUCY, M., GODIN, G., AND RIOUX, M. A texture-mapping ap­proach for the compression 
of colored 3D triangulations. The Visual Computer 12 (1986), 503 514. [27] XIA, J., AND VARSHNEY, A. 
Dynamic view-dependent simpli.ca­tion for polygonal models. In Visualization 96 Proceedings, IEEE, pp. 
327 334.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344936</article_id>
		<sort_key>335</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>37</seq_no>
		<title><![CDATA[Surfels]]></title>
		<subtitle><![CDATA[surface elements as rendering primitives]]></subtitle>
		<page_from>335</page_from>
		<page_to>342</page_to>
		<doi_number>10.1145/344779.344936</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344936</url>
		<abstract>
			<par><![CDATA[<p>Surface elements (surfels) are a powerful paradigm to efficiently render complex geometric objects at interactive frame rates. Unlike classical surface discretizations, i.e., triangles or quadrilateral meshes, surfels are point primitives without explicit connectivity. Surfel attributes comprise depth, texture color, normal, and others. As a pre-process, an octree-based surfel representation of a geometric object is computed. During sampling, surfel positions and normals are optionally perturbed, and different levels of texture colors are prefiltered and stored per surfel. During rendering, a hierarchical forward warping algorithm projects surfels to a z-buffer. A novel method called visibility splatting determines visible surfels and holes in the z-buffer. Visible surfels are shaded using texture filtering, Phong illumination, and environment mapping using per-surfel normals. Several methods of image reconstruction, including supersampling, offer flexible speed-quality tradeoffs. Due to the simplicity of the operations, the surfel rendering pipeline is amenable for hardware implementation. Surfel objects offer complex shape, low rendering cost and high image quality, which makes them specifically suited for low-cost, real-time graphics, such as games.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[rendering systems]]></kw>
			<kw><![CDATA[texture mapping]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Viewing algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Graphics data structures and data types</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010243</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Appearance and texture representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010394</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics file formats</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14079289</person_id>
				<author_profile_id><![CDATA[81100199891]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hanspeter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pfister]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MERL, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP37026031</person_id>
				<author_profile_id><![CDATA[81100289561]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Matthias]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zwicker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ETH Z&#252;rich, Switzerland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14171573</person_id>
				<author_profile_id><![CDATA[81100490339]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jeroen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[van Baar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MERL, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40025406</person_id>
				<author_profile_id><![CDATA[81100260276]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Markus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gross]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ETH Z&#252;rich, Switzerland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Animatek. Caviar Technology. Web page. http://www.animatek.com/.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808585</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[L. Carpenter. The A-buffer, an Antialiased Hidden Surface Method. In Computer Graphics, volume 18 of SIGGRAPH '84 Proceedings, pages 103-108. July 1984.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>907242</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[E.E. Catmull. A Subdivision Algorithm for Computer Display of Curved Surfaces. Ph.D. thesis, University of Utah, Salt Lake City, December 1974.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311571</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[C.F. Chang, G. Bishop, and A. Lastra. LDI Tree: A Hierarchical Representation for Image-Based Rendering. In Computer Graphics, SIGGRAPH '99 Proceedings, pages 291-298. Los Angeles, CA, August 1999.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218395</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[S.E. Chen. Quicktime VR - An Image-Based Approach to Virtual Environment Navigation. In Computer Graphics, SIGGRAPH '95 Proceedings, pages 29-38. Los Angeles, CA, August 1995.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>889151</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[W. Dally, L. McMillan, G. Bishop, and H. Fuchs. The Delta Tree: An Object- Centered Approach to Image-Based Rendering. Technical Report AIM- 1604, AI Lab, MIT, May 1996.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[M. Deering. Data Complexity for Virtual Reality: Where do all the Triangles Go? In IEEE Virtual Reality Annual International Symposium (VRAIS), pages 357-363. Seattle, WA, September 1993.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>184932</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[D. Ebert, F. Musgrave, D. Peachey, K. Perlin, and S. Worley. Texturing &amp; Modeling -A ProceduraI Approach. AP Professional, second edition, 1994.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237200</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[S. Gortler, R. Grzeszczuk, R. Szeliski, and M. Cohen. The Lumigraph. In Computer Graphics, SIGGRAPH '96 Proceedings, pages 43-54. New Orleans, LS, August 1996.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[L. Gritz. Blue Moon Rendering Tools. Web page. http://www.bmrt.org/.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[J. R Grossman. Point Sample Rendering. Master's thesis, Department of Electrical Engineering and Computer Science, MIT, August 1998.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[J. R Grossman and W. Dally. Point Sample Rendering. In Rendering Techniques '98, pages 181-192. Springer, Wien, Vienna, Austria, July 1998.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>13027</ref_obj_id>
				<ref_obj_pid>13021</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[R Heckbert. Survey of Texture Mapping. IEEE Computer Graphics &amp; Applications, 6(11):56-67, November 1986.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[R Heckbert. Fundamentals of Texture Mapping and Image Warping. Master's thesis, University of California at Berkeley, Department of Electrical Engineering and Computer Science, June 17 1989.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>167718</ref_obj_id>
				<ref_obj_pid>167703</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[G.T. Herman. Discrete Multidimensional Jordan Surfaces. CVGIP: Graphical Modeling and Image P1vcessing, 54(6):507-515, November 1992.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>619635</ref_obj_id>
				<ref_obj_pid>161477</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[A. Kaufman, D. Cohen, and R. Yagel. Volume Graphics. Computer, 26(7):51- 64, July 1993.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237199</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[M. Levoy and R Hanrahan. Light Field Rendering. In Computer Graphics, SIGGRAPH '96 Proceedings, pages 31-42. New Orleans, LS, August 1996.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[M. Levoy and T. Whitted. The Use of Points as Display Primitives. Technical Report TR 85-022, The University of North Carolina at Chapel Hill, Department of Computer Science, 1985.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[D. Lischinski and A. Rappoport. Image-Based Rendering for Non-Diffuse Synthetic Scenes. In Rendering Techniques '98, pages 301-314. Springer, Wien, Vienna, Austria, June 1998.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>275476</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[N. Max. Hierarchical Rendering of Trees from Precomputed Multi-Layer Z- Buffers. In Rendering Techniques '96, pages 165-174. Springer, Wien, Porto, Portugal, June 1996.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218398</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[L. McMillan and G. Bishop. Plenoptic Modeling: An Image-Based Rendering System. In Computer Graphics, SIGGRAPH '95 Proceedings, pages 39-46. Los Angeles, CA, August 1995.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[V. Popescu and A. Lastra. High Quality 3D Image Warping by Separating Visibility from Reconstruction. Technical Report TR99-002, University of North Carolina, January 15 1999.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801167</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[W.T. Reeves. Particle Systems - A Technique for Modeling a Class of Fuzzy Objects. In Computer Graphics, volume 17 of SIGGRAPH '83 P~vceedings, pages 359-376. July 1983.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[G. Schaufler. Per-Object Image Warping with Layered Impostors. In Rendering Techniques '98, pages 145-156. Springer, Wien, Vienna, Austria, June 1998.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280882</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[J. Shade, S. J. Gortler, L. He, and R. Szeliski. Layered Depth Images. In Computer Graphics, SIGGRAPH '98 Proceedings, pages 231-242. Orlando, FL, July 1998.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[A.R. Smith. Smooth Operator. The Economist, pages 73-74, March 6 1999. Science and Technology Section.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237274</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[J. Torborg and J. Kajiya. Talisman: Commodity Real-Time 3D Graphics for the PC. In Computer Graphics, SIGGRAPH '96 Proceedings, pages 353-364. New Orleans, LS, August 1996.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192193</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[D. Voorhies and J. Foran. Reflection Vector Shading Hardware. In Computer Graphics, Proceedings of SIGGRAPH 94, pages 163-166. July 1994.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97919</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[L. Westover. Footprint Evaluation for Volume Rendering. In Computer Graphics, Proceedings of SIGGRAPH 90, pages 367-376. August 1990.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Surfels: Surface Elements as Rendering Primitives Hanspeter P.ster * Matthias Zwicker t Jeroen van Baar* 
Markus Grosst Figure 1: Surfel rendering examples. Abstract Surface elements (surfels) are a powerful 
paradigm to ef.ciently render complex geometric objects at interactive frame rates. Un­like classical 
surface discretizations, i.e., triangles or quadrilateral meshes, surfels are point primitives without 
explicit connectivity. Surfel attributes comprise depth, texture color, normal, and oth­ers. As a pre-process, 
an octree-based surfel representation of a geometric object is computed. During sampling, surfel positions 
and normals are optionally perturbed, and different levels of texture colors are pre.ltered and stored 
per surfel. During rendering, a hi­erarchical forward warping algorithm projects surfels to a z-buffer. 
A novel method called visibility splatting determines visible sur­fels and holes in the z-buffer. Visible 
surfels are shaded using tex­ture .ltering, Phong illumination, and environment mapping using per-surfel 
normals. Several methods of image reconstruction, in­cluding supersampling, offer .exible speed-quality 
tradeoffs. Due to the simplicity of the operations, the surfel rendering pipeline is amenable for hardware 
implementation. Surfel objects offer com­plex shape, low rendering cost and high image quality, which 
makes them speci.cally suited for low-cost, real-time graphics, such as games. CR Categories: I.3.3 [Computer 
Graphics]: Picture/Image Generation Viewing Algorithms; I.3.6 [Computer Graphics]: Methodology and Techniques 
 Graphics Data Structures and Data Types. Keywords: Rendering Systems, Texture Mapping. *MERL, Cambridge, 
MA. Email: [p.ster,jeroen]@merl.com tETH Z¨urich, Switzerland. Email: [zwicker,gross]@inf.ethz.ch Permission 
to make digital or hard copies of part or all of this work or personal or classroom use is granted without 
fee provided that copies are not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on 
servers, or to redistribute to lists, requires prior specific permission and/or a fee. SIGGRAPH 2000, 
New Orleans, LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 1 Introduction 3D computer graphics 
has .nally become ubiquitous at the con­sumer level. There is a proliferation of affordable 3D graphics 
hard­ware accelerators, from high-end PC workstations to low-priced gamestations. Undoubtedly, key to 
this success is interactive com­puter games that have emerged as the killer application for 3D graphics. 
However, interactive computer graphics has still not reached the level of realism that allows a true 
immersion into a virtual world. For example, typical foreground characters in real­time games are extremely 
minimalistic polygon models that often exhibit faceting artifacts, such as angular silhouettes. Various 
sophisticated modeling techniques, such as implicit sur­faces, NURBS, or subdivision surfaces, allow 
the creation of 3D graphics models with increasingly complex shapes. Higher order modeling primitives, 
however, are eventually decomposed into tri­angles before being rendered by the graphics subsystem. The 
trian­gle as a rendering primitive seems to meet the right balance between descriptive power and computational 
burden [7]. To render realis­tic, organic-looking models requires highly complex shapes with ever more 
triangles, or, as Alvy Ray Smith puts it: Reality is 80 million polygons [26]. Processing many small 
triangles leads to bandwidth bottlenecks and excessive .oating point and rasteriza­tion requirements 
[7]. To increase the apparent visual complexity of objects, texture mapping was introduced by Catmull 
[3] and successfully applied by others [13]. Textures convey more detail inside a polygon, thereby allowing 
larger and fewer triangles to be used. Today s graphics engines are highly tailored for high texture 
mapping performance. However, texture maps have to follow the underlying geometry of the polygon model 
and work best on .at or slightly curved surfaces. Realistic surfaces frequently require a large number 
of textures that have to be applied in multiple passes during rasterization. And phe­nomena such as smoke, 
.re, or water are dif.cult to render using textured triangles. In this paper we propose a new method 
of rendering objects with rich shapes and textures at interactive frame rates. Our rendering architecture 
is based on simple surface elements (surfels)asren­dering primitives. Surfels are point samples of a 
graphics model. In a preprocessing step, we sample the surfaces of complex geometric models along three 
orthographic views. At the same time, we per­form computation-intensive calculations such as texture, 
bump, or displacement mapping. By moving rasterization and texturing from   Sampling and Texture Prefiltering 
3-to-1 Reduction (optional) a)   b) Figure 2: Algorithm overview: a) Preprocessing. b) Rendering 
of the hierarchical LDC tree. the core rendering pipeline to the preprocessing step, we dramati­cally 
reduce the rendering cost. From a modeling point of view, the surfel representation pro­vides a mere 
discretization of the geometry and hence reduces the object representation to the essentials needed for 
rendering. By contrast, triangle primitives implicitly store connectivity informa­tion, such as vertex 
valence or adjacency data not necessarily available or needed for rendering. In a sense, a surfel relates 
to what Levoy and Whitted call the lingua franca of rendering in their pioneering report from 1985 [18]. 
Storing normals, pre.ltered textures, and other per surfel data enables us to build high quality rendering 
algorithms. Shading and transformations applied per surfel result in Phong illumination, bump, and displacement 
mapping, as well as other advanced ren­dering features. Our data structure provides a multiresolution 
ob­ject representation, and a hierarchical forward warping algorithm allows us to estimate the surfel 
density in the output image for speed-quality tradeoffs. The surfel rendering pipeline complements the 
existing graphics pipeline and does not intend to replace it. It is positioned between conventional geometry-based 
approaches and image-based render­ing and trades memory overhead for rendering performance and quality. 
The focus of this work has been interactive 3D applications, not high-end applications such as feature 
.lms or CAD/CAM. Sur­fels are not well suited to represent .at surfaces, such as walls or scene backgrounds, 
where large, textured polygons provide better image quality at lower rendering cost. However, surfels 
work well for models with rich, organic shapes or high surface details and for applications where preprocessing 
is not an issue. These qualities make them ideal for interactive games.  2 Related Work The use of points 
as rendering primitives has a long history in com­puter graphics. As far back as 1974, Catmull [3] observed 
that ge­ometric subdivision may ultimately lead to points. Particles were subsequently used for objects 
that could not be rendered with ge­ometry, such as clouds, explosions, and .re [23]. More recently, image-based 
rendering has become popular because its rendering time is proportional to the number of pixels in the 
source and out­put images and not the scene complexity. Visually complex objects have been represented 
by dynamically generated image sprites [25], which are quick to draw and largely retain the visual characteristics 
of the object. A similar approach was used in the Talisman rendering system [27] to maintain high and 
approximately constant frame rates. However, mapping objects onto planar polygons leads to visibility 
errors and does not allow for parallax and disocclusion effects. To address these problems, sev­eral 
methods add per-pixel depth information to images, variously called layered impostors [24], sprites with 
depth, or layered depth images [25], just to name a few. Still, none of these techniques pro­vide a complete 
object model that can be illuminated and rendered from arbitrary points of view. Some image-based approaches 
represent objects without explic­itly storing any geometry or depth. Methods such as view inter­polation 
and Quicktime VR [5] or plenoptic modeling [21] cre­ate new views from a collection of 2D images. Light.eld 
[17] or lumigraph [9] techniques describe the radiance of a scene or ob­ject as a function of position 
and direction in a four-or higher­dimensional space, but at the price of considerable storage over­head. 
All these methods use view-dependent samples to represent an object or scene. However, view-dependent 
samples are ineffec­tive for dynamic scenes with motion of objects, changes in material properties, and 
changes in position and intensities of light sources. The main idea of representing objects with surfels 
is to describe them in a view-independent, object-centered rather than image­centered fashion. As such, 
surfel rendering is positioned between geometry rendering and image-based rendering. In volume graph­ics 
[16], synthetic objects are implicitly represented with surface voxels, typically stored on a regular 
grid. However, the extra third dimension of volumes comes at the price of higher storage require­ments 
and longer rendering times. In [8], Perlin studies sur.ets, a .avor of wavelets that can be used to describe 
free-form implicit surfaces. Sur.ets have less storage overhead than volumes, but ren­dering them requires 
lengthy ray casting. Our research was inspired by the following work: Animatek s Caviar player [1] provides 
interactive frame rates for surface voxel models on a Pentium class PC, but uses simplistic projection 
and illumination methods. Levoy and Whitted [18] use points to model objects for the special case of 
continuous, differentiable surfaces. They address the problem of texture .ltering in detail. Max uses 
point samples obtained from orthographic views to model and ren­der trees [20]. Dally et al. [6] introduced 
the delta tree as an object­centered approach to image-based rendering. The movement of the viewpoint 
in their method, however, is still con.ned to particular locations. More recently, Grossman and Dally 
[12] describe a point sample representation for fast rendering of complex objects. Chang et al. [4] presented 
the LDI tree, a hierarchical space-partitioning data structure for image-based rendering. We extend and 
integrate these ideas and present a complete point sample rendering system comprising an ef.cient hierarchical 
repre­sentation, high quality texture .ltering, accurate visibility calcula­tions, and image reconstruction 
with .exible speed-quality trade­offs. Our surfel rendering pipeline provides high quality rendering 
of exceedingly complex models and is amenable for hardware im­plementation. 3 Conceptual Overview Similar 
to the method proposed by Levoy and Whitted [18], our surfel approach consists of two main steps: sampling 
and surfel rendering. Sampling of geometry and texture is done during prepro­cessing, which may include 
other view-independent methods such as bump and displacement mapping. Figure 2 gives a conceptual overview 
of the algorithm. The sampling process (Section 5) converts geometric objects and their textures to surfels. 
We use ray casting to create three orthog­onal layered depth images (LDIs) [25]. The LDIs store multiple 
surfels along each ray, one for each ray-surface intersection point. Lischinski and Rappaport [19] call 
this arrangement of three or­thogonal LDIs a layered depth cube (LDC). An important and novel aspect 
of our sampling method is the distinction between sampling of shape, or geometry, and shade, or texture 
color. A surfel stores both shape, such as surface position and orientation, and shade, such as multiple 
levels of pre.ltered texture colors. Because of the similarities to traditional texture mipmaps we call 
this hierarchical color information a surfel mipmap. From the LDC we create an ef.cient hierarchical 
data structure for rendering. Chang et al.[4] introduce the LDI tree, an octree with an LDI attached 
to each octree node. We use the same hierarchical space-partitioning structure, but store an LDC at each 
node of the octree (Section 6). Each LDC node in the octree is called a block. We call the resulting 
data structure the LDC tree. In a step called 3-to-1 reduction we optionally reduce the LDCs to single 
LDIs on a block-by-block basis for faster rendering. The rendering pipeline (Section 7) hierarchically 
projects blocks to screen space using perspective projection. The rendering is ac­celerated by block 
culling [12] and fast incremental forward warp­ing. We estimate the projected surfel density in the output 
image to control rendering speed and quality of the image reconstruction. A conventional z-buffer together 
with a novel method called visibil­ity splatting solves the visibility problem. Texture colors of visible 
surfels are .ltered using linear interpolation between appropriate levels of the surfel mipmap. Each 
visible surfel is shaded using, for example, Phong illumination and re.ection mapping. The .nal stage 
performs image reconstruction from visible surfels, including hole .lling and antialiasing. In general, 
the resolution of the output image and the resolution of the z-buffer do not have to be the same. 4 
De.nition of a Surfel We found the term surfel as an abbreviation for surface element or surface voxel 
in the volume rendering and discrete topology literature. Herman [15] de.nes a surfel as an oriented 
(n-1)­dimensional object in Rn.For n=j, this corresponds to an ori­ented unit square (voxel face) and 
is consistent with thinking of voxels as little cubes. However, for our discussion we .nd it more useful 
to de.ne surfels as follows: Asurfel is a zero-dimensional n-tuple with shape and shade at­tributes that 
locally approximate an object surface. We consider the alternative term, point sample, to be too general, 
since voxels and pixels are point samples as well. 5 Sampling The goal during sampling is to .nd an 
optimal surfel representa­tion of the geometry with minimum redundancy. Most sampling methods perform 
object discretization as a function of geometric parameters of the surface, such as curvature or silhouettes. 
This object space discretization typically leads to too many or too few primitives for rendering. In 
a surfel representation, object sampling is aligned to image space and matches the expected output resolu­tion 
of the image. 5.1 LDC Sampling We sample geometric models from three sides of a cube into three orthogonal 
LDIs, called a layered depth cube (LDC) [19] or block. Figure 3 shows an LDC and two LDIs using a 2D 
drawing. Ray casting records all intersections, including intersections with back­facing surfaces. At 
each intersection point, a surfel is created with .oating point depth and other shape and shade properties. 
Perturba­tion of the surface normal or of the geometry for bump and displace­ment mapping can be performed 
on the geometry before sampling or during ray casting using procedural shaders.   Alternatively, we 
could sample an object from predetermined di­rections on a surrounding convex hull using orthographic 
depth im­ages [6, 12]. However, combining multiple reference images and eliminating the redundant information 
is a dif.cult problem [21], and sampling geometry with reference images works best for smooth and convex 
objects. In addition, LDC sampling allows us to easily build a hierarchical data structure, which would 
be dif.cult to do from dozens of depth images. 5.2 Adequate Sampling Resolution Given a pixel spacing 
of hofor the full resolution LDC used for sampling, we can determine the resulting sampling density on 
the surface. Suppose we construct a Delaunay triangulation on the ob­ject surface using the generated 
surfels as triangle vertices. As was observed in [19], the imaginary triangle mesh generated by this 
 sampling process has a maximum sidelength Smaxofjho.The minimum sidelength Sminis 0 when two or three 
sampling rays intersect at the same surface position. Similarly to [12], we call the object adequately 
sampled if we can guarantee that at least one surfel is projected into the support of each ouptut pixel 
.lter for orthographic projection and unit mag­ni.cation. That condition is met if Smax, the maximum 
distance between adjacent surfels in object space, is less than the radius rr e of the desired pixel 
reconstruction .lter. Typically, we choose the LDI resolution to be slightly higher than this because 
of the effects of magni.cation and perspective projection. We will revisit these observations when estimating 
the number of projected surfels per pixel in Section 7.2. 5.3 Texture Pre.ltering A feature of surfel 
rendering is that textures are pre.ltered and mapped to object space during preprocessing. We use view­independent 
texture .ltering as in [12]. To prevent view-dependent texture aliasing we also apply per-surfel texture 
.ltering during ren­dering (see Sections 7.4 and 7.6). To determine the extent of the .lter footprint 
in texture space, we center a circle at each surfel on its tangent plane, as shown in Figure 4a. We call 
these circles tangent disks. The tangent disks are Object Space Texture Space b) Figure 4: Texture 
pre.ltering with tangent disks. mapped to ellipses in texture space (see Figure 4b) using the pre­de.ned 
texture parameterization of the surface. An EWA .lter [14] is applied to .lter the texture and the resulting 
color is assigned to the surfel. To enable adequate texture reconstruction, the elliptical .lter footprints 
in texture space must overlap each other. Conse­ o quently, we choose r re=Smax, the maximum distance 
between adjacent surfels in object space, as the radius for the tangent disks. This usually guarantees 
that the tangent disks intersect each other in object space and that their projections in texture space 
overlap. Figure 3: Layered depth cube sampling (shown in 2D). Grossman and Dally [12] also use view-independent 
texture .l­tering and store one texture sample per surfel. Since we use a mod­i.ed z-buffer algorithm 
to resolve visibility (Section 7.3), not all surfels may be available for image reconstruction, which 
leads to texture aliasing artifacts. Consequently, we store several (typically three or four) pre.ltered 
texture samples per surfel. Tangent disks k with dyadically larger radii r re =Smax2kare mapped to texture 
space and used to compute the pre.ltered colors. Because of its similarity to mipmapping [13], we call 
this a surfel mipmap.Fig­ure 4b shows the elliptical footprints in texture space of consecu­tively larger 
tangent disks.  6 Data Structure We use the LDC tree, an ef.cient hierarchical data structure, to store 
the LDCs acquired during sampling. It allows us to quickly estimate the number of projected surfels per 
pixel and to trade ren­dering speed for higher image quality. 6.1 The LDC Tree Chang et al. [4] use several 
reference depth images of a scene to construct the LDI tree. The depth image pixels are resampled onto 
multiple LDI tree nodes using splatting [29]. We avoid these inter­polation steps by storing LDCs at 
each node in the octree that are subsampled versions of the highest resolution LDC. The octree is recursively 
constructed bottom up, and its height is selected by the user. The highest resolution LDC acquired dur­ing 
geometry sampling is stored at the lowest level n=.If the highest resolution LDC has a pixel spacing 
of ho, then the LDC at level nhas a pixel spacing of hn =ho2n. The LDC is subdivided into blocks with 
user-speci.ed dimension b, i.e., the LDIs in a block have b2layered depth pixels. bis the same for all 
levels of the tree. Figure 5a shows two levels of an LDC tree with b=using a 2D drawing. In the .gure, 
neighboring blocks are differently shaded, Figure 5: Twolevelsof the LDC tree(shownin2D). and empty 
blocks are white. Blocks on higher levels of the octree are constructed by subsampling their children 
by a factor of two. Figure 5b shows level n=1of the LDC tree. Note that surfels at higher levels of the 
octree reference surfels in the LDC of level 0, i.e., surfels that appear in several blocks of the hierarchy 
are stored only once and shared between blocks. Empty blocks (shown as white squares in the .gure) are 
not stored. Consequently, the block dimension bis not related to the dimension of the highest resolution 
LDC and can be selected ar­bitrarily. Choosing b=1makes the LDC tree a fully volumetric octree representation. 
For a comparison between LDCs and vol­umes see [19]. 6.2 3-to-1 Reduction To reduce storage and rendering 
time it is often useful to optionally reduce the LDCs to one LDI on a block-by-block basis. Because this 
typically corresponds to a three-fold increase in warping speed, we call this step 3-to-1 reduction. 
First, surfels are resampled to integer grid locations of ray intersections as shown in Figure 6. Currently 
we use nearest neighbor interpolation, although a more LDI 1 surfels LDI 2 surfels resampled surfels 
on grid locations Figure 6: 3-to-1 reduction example. sophisticated .lter, e.g., splatting as in [4], 
could easily be imple­mented. The resampled surfels of the block are then stored in a single LDI. The 
reduction and resampling process degrades the quality of the surfel representation, both for shape and 
for shade. Resampled surfels from the same surface may have very different texture col­ors and normals. 
This may cause color and shading artifacts that are worsened during object motion. In practice, however, 
we did not encounter severe artifacts due to 3-to-1 reduction. Because our rendering pipeline handles 
LDCs and LDIs the same way, we could store blocks with thin structures as LDCs, while all other blocks 
could be reduced to single LDIs. As in Section 5.2, we can determine bounds on the surfel density on 
the surface after 3-to-1 reduction. Given a sampling LDI with pixel spacing ho, the maximum distance 
between adjacent surfels on the object surface is Smax =jho, as in the original LDC tree. The minimum 
distance between surfels increases to Smin =ho due to the elimination of redundant surfels, making the 
imaginary Delaunay triangulation on the surface more uniform.  7 The Rendering Pipeline The rendering 
pipeline takes the surfel LDC tree and renders it us­ing hierarchical visibility culling and forward 
warping of blocks. Hierarchical rendering also allows us to estimate the number of pro­jected surfels 
per output pixel. For maximum rendering ef.ciency, we project approximately one surfel per pixel and 
use the same res­olution for the z-buffer as in the output image. For maximum image quality, we project 
multiple surfels per pixel, use a .ner resolution of the z-buffer, and high quality image reconstruction. 
7.1 Block Culling We traverse the LDC tree from top (the lowest resolution blocks) to bottom (the highest 
resolution blocks). For each block, we .rst perform view-frustum culling using the block bounding box. 
Next, we use visibility cones, as described in [11], to perform the equiv­alent of backface culling of 
blocks. Using the surfel normals, we precompute a visibility cone per block, which gives a fast, con­servative 
visibility test: no surfel in the block is visible from any viewpoint within the cone. In contrast to 
[11], we perform all visi­bility tests hierarchically in the LDC tree, which makes them more ef.cient. 
 7.2 Block Warping During rendering, the LDC tree is traversed top to bottom [4]. To choose the octree 
level to be projected, we conservatively estimate for each block the number of surfels per pixel. We 
can choose one surfel per pixel for fast rendering or multiple surfels per pixel for supersampling. For 
each block at tree level n, the number of sur­fels per pixel is determined by n, the maximum distance 
be­ maxtween adjacent surfels in image space. We estimate nby divid­ max ing the maximum length of the 
projected four major diagonals of the block bounding box by the block dimension b. Thisiscorrect for 
orthographic projection. However, the error introduced by using perspective projection is small because 
a block typically projects to a small number of pixels. For each block, nis compared to the radius reof 
the de­ maxr sired pixel reconstruction .lter. rreis typically22So,where So n is the sidelength of an 
output pixel. If maxof the current block is larger than rrethen its children are traversed. We project 
the n block whose maxis smaller than rre, rendering approximately one surfel per pixel. Note that the 
number of surfels per pixel can be increased by requiring that nis a fraction of r. The result­ maxreing 
nis stored as maxwith each projected surfel for subsequent max use in the visibility testing and the 
image reconstruction stages. The radius of the actual reconstruction .lter is rre =maX(rre max) (see 
Section 7.6). To warp a block to screen space we use the optimized incre­mental block warping by Grossman 
and Dally, presented in detail in [11]. Its high ef.ciency is achieved due to the regularity of LDCs. 
It uses only 6 additions, 3 multiplications, and 1 reciprocal per sam­ple. The LDIs in each LDC block 
are warped independently, which allows us to render an LDC tree where some or all blocks have been reduced 
to single LDIs after 3-to-1 reduction. 7.3 Visibility Testing Perspective projection, high z-buffer 
resolution, and magni.cation may lead to undersampling or holes in the z-buffer. A z-buffer pixel is 
a hole if it does not contain a visible surfel or background pixel after projection. Holes have to be 
marked for image reconstruction. Each pixel of the z-buffer stores a pointer to the closest surfel and 
the current minimum depth. Surfel depths are projected to the z­buffer using nearest neighbor interpolation. 
To correctly resolve visibility in light of holes, we scan-convert the orthographic projection of the 
surfel tangent disks into the z­buffer. The tangent disks have a radius of rtn=Smax2n,where Smaxis the 
maximum distance between adjacent surfels in object space and nis the level of the block. We call this 
approach visibility splatting, shown in Figure 7. Visibility splatting effectively sepa- Object Space 
z-Buffer b) Figure 7: Visibility splatting. rates visibility calculations and reconstruction of the 
image, which produces high quality images and is amenable to hardware imple­mentation [22]. After orthographic 
projection, the tangent disks form an ellipse around the surfel, as shown in Figure 7b. We approximate 
the el­lipse with a partially axis-aligned bounding box, shown in red. The bounding box parallelogram 
can be easily scan-converted, and each z-buffer pixel is .lled with the appropriate depth (indicated 
by the shaded squares in the .gure), depending on the surfel normal N. This scan conversion requires 
only simple setup calculations, no interpolation of colors, and no perspective divide. The direction 
of the minor axis aminof the projected ellipse is parallel to the projection of the surfel normal N. 
The major axis amaxis orthogonal to amin. The length of amaxis the projection of the tangent disk radius 
rtn, which is approximated by max.This approximation takes the orientation and magni.cation of the LDC 
tree during projection into account. Next, we calculate the coordi­nate axis that is most parallel to 
amin(the y-axis in Figure 7). The short side of the bounding box is axis aligned with this coordinate 
axis to simplify scan conversion. Its height his computed by in­tersecting the ellipse with the coordinate 
axis. The width Wof the bounding box is determined by projecting the vertex at the inter­section of the 
major axis and the ellipse onto the second axis (the x-axis in Figure 7). xzand zare the partial derivatives 
of the surfel depth zwith respect to the screen xand ydirection. They are constant because of the orthographic 
projection and can be calculated from the unit normal N. During scan conversion, the depth at each pixel 
inside z z the bounding box is calculated using x and . In addition, we add a small threshold to each 
projected zvalue. The threshold pre­vents surfels that lie on the foreground surface to be accidentally 
discarded. Pixels that have a larger zthan the zvalues of the splat­ted tangent disk are marked as holes. 
If the surface is extremely bent, the tangential planes do not cover it completely, potentially leaving 
tears and holes. In addi­tion, extreme perspective projection makes orthographic projection a bad approximation 
to the actual projected tangent disk. In prac­tice, however, we did not see this as a major problem. 
If the pro­jected tangent disk is a circle, i.e., if Nis almost parallel to the viewing direction, the 
bounding box parallelogram is a bad approx­imation. In this case, we use a square bounding box instead. 
Using a somewhat related approach, Grossman and Dally [12] use a hierarchical z-buffer for visibility 
testing. Each surfel is pro­jected and the hole size around the surfel is estimated. The radius of the 
hole determines the level of the hierarchical z-buffer where the z-depth of the surfel will be set. This 
can be regarded as visibility splatting using a hierarchical z-buffer. The advantage is that the vis­ibility 
splat is performed with a single depth test in the hierarchical z-buffer. However, the visibility splat 
is always square, essentially representing a tangential disk that is parallel to the image plane. In 
addition, it is not necessarily centered around the projected surfel. To recover from those drawbacks, 
[12] introduces weights indicat­ing coverage of surfels. But this makes the reconstruction process more 
expensive and does not guarantee complete coverage of hid­den surfaces. 7.4 Texture Filtering As explained 
in Section 5.3, each surfel in the LDC tree stores sev­eral pre.ltered texture colors of the surfel mipmap. 
During render­ing, the surfel color is linearly interpolated from the surfel mipmap colors depending 
on the object mini.cation and surface orientation. Figure 8a shows all visible surfels of a sampled surface 
projected to the z-buffer. The ellipses around the centers of the surfels mark the projection of the 
footprints of the highest resolution texture pre­.lter (Section 5.3). Note that during pre.ltering, we 
try to guar­antee that the footprints cover the surface completely. In .gure 8b s z a) Figure 8: Projected 
surfel mipmaps. the number of samples per z-buffer pixel is limited to one by ap­plying z-buffer depth 
tests. In order to .ll the gaps appearing in the coverage of the surface with texture footprints, the 
footprints of the remaining surfels have to be enlarged. If surfels are discarded in a given z-buffer 
pixel, we can assume that the z-buffer pixels in the 3x3 neighborhood around it are not holes. Thus the 
gaps can be .lled if the texture footprint of each surfel covers at least the area of a z-buffer pixel. 
Consequently, the ellipse of the projected footprint has to have a minor radius of 2Szin the worst case, 
where Szis the z-buffer pixel spacing. But we ignore that worst case and use 22Sz, implying that surfels 
are projected to z-buffer pixel centers. Figure 8b shows the scaled texture footprints as ellipses around 
pro­jected surfels. To select the appropriate surfel mipmap level, we use traditional view-dependent 
texture .ltering, as shown in Figure 9. A circle with  Figure 9: Projected pixel coverage. radius 22Szis 
projected through a pixel onto the tangent plane of the surface from the direction of the view, producing 
an ellipse in the tangent plane. In this calculation, the projection of the circle is approximated with 
an orthographic projection. Similar to isotropic texture mapping, the major axis of the projected tangent 
space el­lipse is used to determine the surfel mipmap level. The surfel color is computed by linear interpolation 
between the closest two mipmap kk;l levels with pre.lter radii r reand r re, respectively. 7.5 Shading 
The illumination model is usually applied before visibility testing. However, deferred shading after 
visibility testing avoids unneces­sary work. Grossman and Dally [12] perform shading calculations in 
object space to avoid transformation of normals to camera space. However, we already transform the normals 
to camera space during visibility splatting (Section 7.3). With the transformed normals at hand, we use 
cube re.ectance and environment maps [28] to calcu­late a per-surfel Phong illumination model. Shading 
with per-surfel normals results in high quality specular highlights. 7.6 Image Reconstruction and Antialiasing 
Reconstructing a continuous surface from projected surfels is fun­damentally a scattered data interpolation 
problem. In contrast to other approaches, such as splatting [29], we separate visibility cal­culations 
from image reconstruction [22]. Z-buffer pixels with holes are marked during visibility splatting. These 
hole pixels are not used during image reconstruction because they do not contain any visible samples. 
Figure 10 shows the z-buffer after rendering of an object and the image reconstruction process. z s 
o a) Figure 10: Image reconstruction. The simplest and fastest approach, shown in Figure 10a, is to choose 
the size of an output pixel Soto be the same as the z­buffer pixel size Sz. Surfels are mapped to pixel 
centers using near­est neighbor interpolation, shown with color squares in the .gure. Holes are marked 
with a black X. Recall that during forward warp­ing each surfel stores max, an estimate of the maximum 
distance between adjacent projected surfels of a block. maxis a good esti­mate for the minimum radius 
of a pixel .lter that contains at least one surfel. To interpolate the holes, we use a radially symmetric 
Gauss .lter with a radius rreslightly larger than maxpositioned at hole pixel centers. Alternatively, 
to .ll the holes we implemented the pull-push algorithm used by Grossman and Dally [12] and de­scribed 
by Gortler et al.[9]. A high quality alternative is to use supersampling, shown in Fig­ure 10b. The output 
image pixel size Sois any multiple of the z­buffer pixel size Sz. Dotted lines in the .gure indicate 
image-buffer subpixels. Rendering for supersampling proceeds exactly the same as before. During image 
reconstruction we put a Gaussian .lter at the centers of all output pixels to .lter the subpixel colors. 
The ra­dius of the .lter is rre =maX(rremax). Thus rreis at least as large as rre = 22So, but it can 
be increased if maxindicates a low density of surfels in the output image. It is instructive to see how 
the color of an output pixel is deter­mined for regular rendering and for supersampling in the absence 
of holes. For regular rendering, the pixel color is found by nearest neighbor interpolation from the 
closest visible surfel in the z-buffer. The color of that surfel is computed by linear interpolation 
between two surfel mipmap levels. Thus the output pixel color is calculated from two pre.ltered texture 
samples. In the case of supersampling, one output pixel contains the .ltered colors of one surfel per 
z­buffer subpixel. Thus, up to eight pre.ltered texture samples may contribute to an output pixel for 
2x2supersampling. This produces image quality similar to trilinear mipmapping. Levoy and Whitted [18] 
and Chang et al. [4] use an algorithm very similar to Carpenter s A-Buffer [2] with per-pixel bins and 
compositing of surfel colors. However, to compute the correct per pixel coverage in the A-buffer requires 
projecting all visible sur­fels. Max [20] uses an output LDI and an A-buffer for high qual­ity anti-aliasing, 
but he reports rendering times of 5 minutes per frame. Our method with hierarchical density estimation, 
visibility splatting, and surfel mipmap texture .ltering offers more .exible speed-quality tradeoffs. 
  8 Implementation and Results We implemented sampling using the Blue Moon Rendering Tools (BMRT) ray 
tracer [10]. We use a sampling resolution of 5122for the LDC for 82expected output resolution. At each 
intersec­tion point, a Renderman shader performs view-independent calcu­lations, such as texture .ltering, 
displacement mapping, and bump mapping, and prints the resulting surfels to a .le. Pre-processing for 
a typical object with 6 LOD surfel mipmaps takes about one hour. A fundamental limitation of LDC sampling 
is that thin struc­tures that are smaller than the sampling grid cannot be correctly represented and 
rendered. For example, spokes, thin wires, or hair are hard to capture. The rendering artifacts are more 
pronounced after 3-to-1 reduction because additional surfels are deleted. How­ever, we had no problems 
sampling geometry as thin as the legs and wings of the wasp shown in Figure 1 and Figure 12. The surfel 
attributes acquired during sampling include a surface normal, specular color, shininess, and three texture 
mipmap levels. Material properties are stored as an index into a table. Our system does currently not 
support transparency. Instead of storing a normal we store an index to a quantized normal table for re.ection 
and environment map shading [28]. Table 1 shows the minimum storage requirements per surfel. We currently 
store RGB colors as 32-bit integers for a total of 20 Bytes per surfel. Data Storage 3 texture mipmap 
levels jxj2bits Index into normal table 16 bit LDI depth value 32 bit Index into material table 16 bit 
Total per sample: 20 Bytes Table 1: Typical storage requirements per surfel. Table 2 lists the surfel 
objects that we used for performance anal­ysis with their geometric model size, number of surfels, and 
.le size before and after 3-to-1 reduction. All models use three LODs and three surfel mipmap levels. 
The size of the LDC tree is about a factor of 1.3 larger than the LDC acquired during sampling. This 
 Data # Polys 3LDIs 3-to-1 Reduced Salamander 81 k 112 k/ 5MB 70 k/3MB Wasp 128 k 369 k / 15 MB 204 
k / 8 MB Cab 155 k 744 k / 28 MB 539 k / 20 MB Table 2: Geometric model sizes and storage requirements 
(# surfels / .le size) for full and 3-to-1 reduced LDC trees. overhead is due to the octree data structure, 
mainly because of the pointers from the lower resolution blocks to surfels of the sampled LDC. We currently 
do not optimize or compress the LDC tree. Figure 1 shows different renderings of surfel objects, including 
environment mapping and displacement mapping. Figure 12 shows an example of hole detection and image 
reconstruction. Visibility splatting performs remarkably well in detecting holes. However, holes start 
to appear in the output image for extreme closeups when there are less than approximately one surfel 
per 30 square pixels. To compare image quality of different reconstruction .lters, we rendered the surfel 
checker plane shown in Figure 11. There is an increasing number of surfels per pixel towards the top 
of the image, while holes appear towards the bottom for nearest neighbor recon­struction. However, a 
checker plane also demonstrates limitations of the surfel representation. Because textures are applied 
during sampling, periodic texture patterns are stored explicitly with the object instead of by reference. 
In addition, .at surfaces are much more ef.ciently rendered using image space rasterization, where attributes 
can be interpolated across triangle spans. Table 3 shows rendering performance broken down into percent­ages 
per major rendering tasks. The frame rates were measured on a 700 MHz Pentium III system with 256 MB 
of SDRAM using an unoptimized C version of our program. All performance numbers are averaged over one 
minute of an animation that arbitrarily rotates Data WRP VIS SHD REC CLR fps Output image: 25.x25. Salamander 
39% 3% 28% 17% 13% 11.2 Wasp 61% 4% 21% 8% 8% 6.0 Cab 91% 2% 5% 1% 1% 2.5 Output image: 8x8 Salamander 
14% 18% 31% 22% 16% 4.6 Wasp 3to1 29% 17% 29% 15% 9% 2.7 Wasp 3LDI 48% 13% 22% 11% 6% 2.0 Wasp SS 15% 
22% 28% 18% 16% 1.3 Cab 74% 7% 11% 5% 3% 1.4 Output image: 1 2x1 2 Salamander 5% 14% 26% 32% 23% 1.3 
Wasp 13% 19% 25% 26% 17% 1.0 Cab 16% 36% 24% 16% 8% 0.6 Table 3: Rendering times with breakdown for 
warping (WRP), vis­ibility splatting (VIS), Phong shading (SHD), image reconstruction (REC), and framebuffer 
clear (CLR). Reconstruction with pull-push .lter. All models, except Wasp 3LDI, are 3-to-1 reduced. Wasp 
SS indicates 2x2 supersampling. the object centered at the origin. The animation was run at three dif­ferent 
image resolutions to measure the effects of magni.cation and holes. Similar to image-based rendering, 
the performance drops almost linearly with increasing output resolution. For 25.2or object mini­.cation, 
the rendering is dominated by warping, especially for ob­jects with many surfels. For 1 22, or large 
object magni.cation, visibility splatting and reconstruction dominate due to the increas­ing number of 
surface holes. The performance difference between a full LDC tree (Wasp 3LDI) and a reduced LDC tree 
(Wasp 3to1) is mainly in the warping stage because fewer surfels have to be projected. Performance decreases 
linearly with supersampling, as shown for 2x2 supersampling at 82resolution (Wasp SS). The same object 
at 1 22output resolution with no supersampling per­forms almost identically, except for slower image 
reconstruction due to the increased number of hole pixels. To compare our performance to standard polygon 
rendering, we rendered the wasp with 128k polygons and 2.3 MB for nine tex­tures using a software-only 
Windows NT OpenGL viewing pro­gram. We used GL LINEAR MIPMAP NEAREST for texture .l­tering to achieve 
similar quality as with our renderer. The average performance was 3 fps using the Microsoft OpenGL implementa­tion 
(opengl32.lib) and 1.7 fps using Mesa OpenGL. Our unopti­mized surfel renderer achieves 2.7 fps for the 
same model, which compares favorably with Mesa OpenGL. We believe that further optimization will greatly 
improve our performance.  Choosing the block size bfor the LDC tree nodes has an in.u­ence on block 
culling and warping performance. We found that a block size of b=1.is optimal for a wide range of objects. 
However, the frame rates remain practically the same for different choices of bdue to the fact that warping 
accounts for only a fraction of the overall rendering time. Because we use a z-buffer we can render overlapping 
surfel ob­jects and integrate them with traditional polygon graphics, such as OpenGL. However, the current 
system supports only rigid body an­imations. Deformable objects are dif.cult to represent with surfels 
and the current LDC tree data structure. In addition, if the surfels do not approximate the object surface 
well, for example after 3-to­1 reduction or in areas of high curvature, some surface holes may appear 
during rendering. 9 Future Extensions A major strength of surfel rendering is that in principal we can 
con­vert any kind of synthetic or scanned object to surfels. We would like to extend our sampling approach 
to include volume data, point clouds, and LDIs of non-synthetic objects. We believe that substan­tial 
compression of the LDC tree can be achieved using run length encoding or wavelet-based compression techniques. 
The perfor­mance of our software renderer can be substantially improved by using Pentium III SSE instructions. 
Using an occlusion compat­ible traversal of the LDC tree [21], one could implement order­independent 
transparency and true volume rendering. Our major goal is the design of a hardware architecture for sur­fel 
rendering. Block warping is very simple, involving only two conditionals for z-buffer tests [11]. There 
are no clipping calcula­tions. All framebuffer operations, such as visibility splatting and image reconstruction, 
can be implemented using standard rasteri­zation and framebuffer techniques. The rendering pipeline uses 
no inverse calculations, such as looking up textures from texture maps, and runtime texture .ltering 
is very simple. There is a high degree of data locality because the system loads shape and shade simul­taneously 
and we expect high cache performance. It is also possi­ble to enhance an existing OpenGL rendering pipeline 
to ef.ciently support surfel rendering. 10 Conclusions Surfel rendering is ideal for models with very 
high shape and shade complexity. As we move rasterization and texturing from the core rendering pipeline 
to the preprocessing step, the rendering cost per pixel is dramatically reduced. Rendering performance 
is essentially determined by warping, shading, and image reconstruction oper­ations that can easily 
exploit vectorization, parallelism, and pipelin­ing. Our surfel rendering pipeline offers several speed-quality 
trade­offs. By decoupling image reconstruction and texture .ltering we achieve much higher image quality 
than comparable point sample approaches. We introduce visibility splatting, which is very effec­tive 
at detecting holes and increases image reconstruction perfor­mance. Antialiasing with supersampling is 
naturally integrated in our system. Our results demonstrate that surfel rendering is capable of high 
image quality at interactive frame rates. Increasing proces­sor performance and possible hardware support 
will bring it into the realm of real-time performance. 11 Acknowledgments We would like to thank Ron 
Perry and Ray Jones for many helpful discussions, Collin Oosterbaan and Frits Post for their contributions 
to an earlier version of the system, and Adam Moravanszky and Si­mon Schirm for developing a surfel demo 
application. Thanks also to Matt Brown, Mark Callahan, and Klaus M¨uller for contributing code, and to 
Larry Gritz for his help with BMRT [10]. Finally, thanks to Alyn Rockwood, Sarah Frisken, and the reviewers 
for their constructive comments, and to Jennifer Roderick for proof­reading the paper.   References 
[1] Animatek. Caviar Technology. Web page. http://www.animatek.com/. [2] L. Carpenter. The A-buffer, 
an Antialiased Hidden Surface Method. In Computer Graphics, volume 18 of SIGGRAPH 84 Proceedings, pages 
103 108. July 1984. [3] E. E. Catmull. A Subdivision Algorithm for Computer Display of Curved Sur­faces. 
Ph.D. thesis, University of Utah, Salt Lake City, December 1974. [4] C.F. Chang, G. Bishop, and A. Lastra. 
LDI Tree: A Hierarchical Representation for Image-Based Rendering. In Computer Graphics, SIGGRAPH 99 
Proceed­ings, pages 291 298. Los Angeles, CA, August 1999. [5] S. E. Chen. Quicktime VR An Image-Based 
Approach to Virtual Environment Navigation. In Computer Graphics, SIGGRAPH 95 Proceedings, pages 29 38. 
Los Angeles, CA, August 1995. [6] W. Dally, L. McMillan, G. Bishop, and H. Fuchs. The Delta Tree: An 
Object-Centered Approach to Image-Based Rendering. Technical Report AIM-1604, AI Lab, MIT, May 1996. 
[7] M. Deering. Data Complexity for Virtual Reality: Where do all the Triangles Go? In IEEE Virtual Reality 
Annual International Symposium (VRAIS), pages 357 363. Seattle, WA, September 1993. [8] D. Ebert, F. 
Musgrave, D. Peachey, K. Perlin, and S. Worley. Texturing &#38; Mod­eling -A Procedural Approach. AP 
Professional, second edition, 1994. [9] S. Gortler, R. Grzeszczuk, R. Szeliski, and M. Cohen. The Lumigraph. 
In Com­puter Graphics, SIGGRAPH 96 Proceedings, pages 43 54. New Orleans, LS, August 1996. [10] L. Gritz. 
Blue Moon Rendering Tools. Web page. http://www.bmrt.org/. [11] J. P. Grossman. Point Sample Rendering. 
Master s thesis, Department of Elec­trical Engineering and Computer Science, MIT, August 1998. [12] J. 
P. Grossman and W. Dally. Point Sample Rendering. In Rendering Techniques 98, pages 181 192. Springer, 
Wien, Vienna, Austria, July 1998. [13] P. Heckbert. Survey of Texture Mapping. IEEE Computer Graphics 
&#38; Applica­tions, 6(11):56 67, November 1986. [14] P. Heckbert. Fundamentals of Texture Mapping and 
Image Warping.Master s thesis, University of California at Berkeley, Department of Electrical Engineer­ing 
and Computer Science, June 17 1989. [15] G. T. Herman. Discrete Multidimensional Jordan Surfaces. CVGIP: 
Graphical Modeling and Image Processing, 54(6):507 515, November 1992. [16] A. Kaufman, D. Cohen, and 
R. Yagel. Volume Graphics. Computer, 26(7):51 64, July 1993. [17] M. Levoy and P. Hanrahan. Light Field 
Rendering. In Computer Graphics, SIGGRAPH 96 Proceedings, pages 31 42. New Orleans, LS, August 1996. 
[18] M. Levoy and T. Whitted. The Use of Points as Display Primitives. Technical Report TR 85-022, The 
University of North Carolina at Chapel Hill, Department of Computer Science, 1985. [19] D. Lischinski 
and A. Rappoport. Image-Based Rendering for Non-Diffuse Syn­thetic Scenes. In Rendering Techniques 98, 
pages 301 314. Springer, Wien, Vienna, Austria, June 1998. [20] N. Max. Hierarchical Rendering of Trees 
from Precomputed Multi-Layer Z-Buffers. In Rendering Techniques 96, pages 165 174. Springer, Wien, Porto, 
Portugal, June 1996. [21] L. McMillan and G. Bishop. Plenoptic Modeling: An Image-Based Rendering System. 
In Computer Graphics, SIGGRAPH 95 Proceedings, pages 39 46. Los Angeles, CA, August 1995. [22] V. Popescu 
and A. Lastra. High Quality 3D Image Warping by Separating Vis­ibility from Reconstruction. Technical 
Report TR99-002, University of North Carolina, January 15 1999. [23] W. T. Reeves. Particle Systems 
A Technique for Modeling a Class of Fuzzy Objects. In Computer Graphics, volume 17 of SIGGRAPH 83 Proceedings, 
pages 359 376. July 1983. [24] G. Schau.er. Per-Object Image Warping with Layered Impostors. In Rendering 
Techniques 98, pages 145 156. Springer, Wien, Vienna, Austria, June 1998. [25] J. Shade, S. J. Gortler, 
L. He, and R. Szeliski. Layered Depth Images. In Com­puter Graphics, SIGGRAPH 98 Proceedings, pages 231 
242. Orlando, FL, July 1998. [26] A. R. Smith. Smooth Operator. The Economist, pages 73 74, March 6 1999. 
Science and Technology Section. [27] J. Torborg and J. Kajiya. Talisman: Commodity Real-Time 3D Graphics 
for the PC. In Computer Graphics, SIGGRAPH 96 Proceedings, pages 353 364. New Orleans, LS, August 1996. 
[28] D. Voorhies and J. Foran. Re.ection Vector Shading Hardware. In Computer Graphics, Proceedings of 
SIGGRAPH 94, pages 163 166. July 1994. [29] L. Westover. Footprint Evaluation for Volume Rendering. In 
Computer Graph­ics, Proceedings of SIGGRAPH 90, pages 367 376. August 1990. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344940</article_id>
		<sort_key>343</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>38</seq_no>
		<title><![CDATA[QSplat]]></title>
		<subtitle><![CDATA[a multiresolution point rendering system for large meshes]]></subtitle>
		<page_from>343</page_from>
		<page_to>352</page_to>
		<doi_number>10.1145/344779.344940</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344940</url>
		<abstract>
			<par><![CDATA[<p>Advances in 3D scanning technologies have enabled the practical creation of meshes with hundreds of millions of polygons. Traditional algorithms for display, simplification, and progressive transmission of meshes are impractical for data sets of this size. We describe a system for representing and progressively displaying these meshes that combines a multiresolution hierarchy based on bounding spheres with a rendering system based on points. A single data structure is used for view frustum culling, backface culling, level-of-detail selection, and rendering. The representation is compact and can be computed quickly, making it suitable for large data sets. Our implementation, written for use in a large-scale 3D digitization project, launches quickly, maintains a user-settable interactive frame rate regardless of object complexity or camera position, yields reasonable image quality during motion, and refines progressively when idle to a high final image quality. We have demonstrated the system on scanned models containing hundreds of millions of samples.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[compression algorithms]]></kw>
			<kw><![CDATA[level of detail algorithms]]></kw>
			<kw><![CDATA[rendering systems]]></kw>
			<kw><![CDATA[spatial data structures]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Graphics data structures and data types</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010394</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics file formats</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010395</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image compression</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14080481</person_id>
				<author_profile_id><![CDATA[81100203803]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Szymon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rusinkiewicz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford Computer Graphics Lab, Gates Building 3B, Stanford University, Stanford, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15037068</person_id>
				<author_profile_id><![CDATA[81100593780]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Levoy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford Computer Graphics Lab, Gates Building 3B, Stanford University, Stanford, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>300554</ref_obj_id>
				<ref_obj_pid>300523</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Aliaga, D., Cohen, J., Wilson, A., Baker, E., Zhang, H., Erikson, C., Hoff, K., Hudson, T., Stuerzlinger, W., Bastos, R., Whitton, M., Brooks, E, and Manocha, D. "MMR: An Interactive Massive Model Rendering System Using Geometric and Image-Based Acceleration," Proc. Symposium on Interactive 3D Graphics, 1999.]]></ref_text>
				<ref_id>Aliaga 99</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[AnimaTek International, Inc., "Caviar Technology," Web page: http://www.animatek.com/products_caviar.htm]]></ref_text>
				<ref_id>Animatek</ref_id>
			</ref>
			<ref>
				<ref_obj_id>94794</ref_obj_id>
				<ref_obj_pid>94788</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Arvo, J. and Kirk, D. "A Survey of Ray Tracing Acceleration Techniques," An Introduction to Ray Tracing, Glassner, A. S. ed., Academic Press, 1989.]]></ref_text>
				<ref_id>Arvo 89</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237213</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Certain, A., Popovi~, J, DeRose, T., Duchamp, T., Salesin, D., and Stuetzle, W. "Interactive Multiresolution Surface Viewing," Proc. SIGGRAPH, 1996.]]></ref_text>
				<ref_id>Certain 96</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311571</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Chang, C., Bishop, G., and Lastra, A. "LDI Tree: A Hierarchical Representation for Image-Based Rendering," Proc. SIGGRAPH, 1999.]]></ref_text>
				<ref_id>Chang 99</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Cline, H. E., Lorensen, W. E., Ludke, S., Crawford, C. R., and Teeter, B. C. "Two Algorithms for the Three-Dimensional Reconstruction of Tomograms," Medical Physics, Vol. 15, No. 3, 1988.]]></ref_text>
				<ref_id>Cline 88</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37414</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Cook, R., Carpenter, L., and Catmull, E. "The Reyes Image Rendering Architecture," Proc. SIGGRAPH, 1987.]]></ref_text>
				<ref_id>Cook 87</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807458</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Csuri, C., Hackathorn, R., Parent, R., Carlson, W., and Howard, M. "Towards an Interactive High Visual Complexity Animation System," Proc. SIGGRAPH, 1979.]]></ref_text>
				<ref_id>Csuri 79</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237269</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Curless, B. and Levoy, M. "A Volumetric Method for Building Complex Models from Range Images," Proc. SIGGRAPH, 1996.]]></ref_text>
				<ref_id>Curless 96</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218391</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Deering, M. "Geometry Compression," Proc. SIGGRAPH, 1995.]]></ref_text>
				<ref_id>Deering 95</ref_id>
			</ref>
			<ref>
				<ref_obj_id>267028</ref_obj_id>
				<ref_obj_pid>266989</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Duchaineau, M., Wolinsky, M., Sigeti, D,. Miller, M., Aldrich, C., and Mineev-Weinstein, M. "ROAMing Terrain: Real-time Optimally Adapting Meshes," Proc. Visualization, 1997.]]></ref_text>
				<ref_id>Duchaineau 97</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218440</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Eck, M., DeRose, T., Duchamp, T., Hoppe, H., Lounsbery, M., and Stuetzle, W. "Multiresolution Analysis of Arbitrary Meshes," Proc. SIGGRAPH, 1995.]]></ref_text>
				<ref_id>Eck 95</ref_id>
			</ref>
			<ref>
				<ref_obj_id>147158</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Funkhouser, T., Sdquin, C., and Teller, S. "Management of Large Amounts of Data in Interactive Building Walkthroughs," Proc. Symposium on Interactive 3D Graphics, 1992.]]></ref_text>
				<ref_id>Funkhouser 92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166149</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Funkhouser, T. and Sdquin, C. "Adaptive Display Algorithm for Interactive Frame Rates During Visualization of Complex Virtual Environments," Proc. SIGGRAPH, 1993.]]></ref_text>
				<ref_id>Funkhouser 93</ref_id>
			</ref>
			<ref>
				<ref_obj_id>241024</ref_obj_id>
				<ref_obj_pid>241020</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Funkhouser, T. "Database Management for Interactive Display of Large Architectural Models," Graphics Interface, 1996.]]></ref_text>
				<ref_id>Funkhouser 96</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166147</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Greene, N., Kass, M., and Miller, G. "Hierarchical Z-buffer Visibility," Proc. SIGGRAPH, 1993.]]></ref_text>
				<ref_id>Greene 93</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Grossman, J. and Dally, W. "Point Sample Rendering," Proc. Eurographics Rendering Workshop, 1998.]]></ref_text>
				<ref_id>Grossman 98</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237216</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Hoppe, H. "Progressive Meshes," Proc. SIGGRAPH, 1996.]]></ref_text>
				<ref_id>Hoppe 96</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258843</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Hoppe, H. "View-Dependent Refinement of Progressive Meshes," Proc. SIGGRAPH, 1997.]]></ref_text>
				<ref_id>Hoppe 97</ref_id>
			</ref>
			<ref>
				<ref_obj_id>288221</ref_obj_id>
				<ref_obj_pid>288216</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Hoppe, H. "Smooth View-Dependent Level-of-Detail Control and its Application to Terrain Rendering," Proc. Visualization, 1998.]]></ref_text>
				<ref_id>Hoppe 98</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Huffman, D. "A Method for the Construction of Minimum Redundancy Codes," Proc. IRE, Vol. 40, No. 9, 1952.]]></ref_text>
				<ref_id>Huffman 52</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74361</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Kajiya, J. and Kay, T. "Rendering Fur with Three Dimensional Textures," Proc. SIGGRAPH, 1989.]]></ref_text>
				<ref_id>Kajiya 89</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237270</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Krishnamurthy, V. and Levoy, M. "Fitting Smooth Surfaces to Dense Polygon Meshes," Proc. SIGGRAPH, 1986.]]></ref_text>
				<ref_id>Krishnamurthy 96</ref_id>
			</ref>
			<ref>
				<ref_obj_id>275483</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Kumar, S., Manocha, D., Garrett, W., and Lin, M. "Hierarchical Back-Face Computation," Proc. Eurographics Rendering Workshop, 1996.]]></ref_text>
				<ref_id>Kumar 96</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122748</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Laur, D. and Hanrahan, P. "Hierarchical Splatting: A Progressive Refinement Algorithm for Volume Rendering," Proc. SIGGRAPH, 1991.]]></ref_text>
				<ref_id>Laur 91</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Levoy, M. and Whitted, T. "The Use of Points as a Display Primitive," Technical Report TR 85-022, University of North Carolina at Chapel Hill, 1985.]]></ref_text>
				<ref_id>Levoy 85</ref_id>
			</ref>
			<ref>
				<ref_obj_id>344849</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Levoy, M., Pulli, K., Curless, B., Rusinkiewicz, S., Koller, D., Pereira, L., Ginzton, M., Anderson, S., Davis, J., Ginsberg, J., Shade, J., and Fulk, D. "The Digital Michelangelo Project: 3D Scanning of Large Statues," Proc. SIGGRAPH, 2000.]]></ref_text>
				<ref_id>Levoy 00</ref_id>
			</ref>
			<ref>
				<ref_obj_id>288288</ref_obj_id>
				<ref_obj_pid>288216</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Lindstrom, P. and Turk, G. "Fast and Memory Efficient Polygonal Simplification," Proc. Visualization, 1998.]]></ref_text>
				<ref_id>Lindstrom 98</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258847</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Luebke, D., and Erikson, C. "View-Dependent Simplification of Arbitrary Polygonal Environments," Proc. SIGGRAPH, 1997.]]></ref_text>
				<ref_id>Luebke 97</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Max, N. and Ohsaki, K. "Rendering Trees from Precomputed Z- buffer Views," Proc. Eurographics Rendering Workshop, 1995.]]></ref_text>
				<ref_id>Max 95</ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Pajarola, R. and Rossignac, J. "Compressed Progressive Meshes," Technical Report GIT-GVU-99-05, Georgia Institute of Technology, 1999.]]></ref_text>
				<ref_id>Pajarola99</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801167</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Reeves, W. "Particle Systems - A Technique for Modeling a Class of Fuzzy Objects," Proc. SIGGRAPH, 1983.]]></ref_text>
				<ref_id>Reeves 83</ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Rossignac, J. and Borrel, E "Multi-Resolution 3D Approximations for Rendering Complex Scenes," Geometric Modeling in Computer Graphics, 1993.]]></ref_text>
				<ref_id>Rossignac 93</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807479</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Rubin, S. M. and Whitted, T. "A 3-Dimensional Representation for Fast Rendering of Complex Scenes," Proc. SIGGRAPH, 1980.]]></ref_text>
				<ref_id>Rubin 80</ref_id>
			</ref>
			<ref>
				<ref_obj_id>77587</ref_obj_id>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Samet, H. Applications of Spatial Data Structures, Addison-Wesley, 1990.]]></ref_text>
				<ref_id>Samet 90</ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Shirman, L. and Abi-Ezzi, S. "The Cone of Normals Technique for Fast Processing of Curved Patches," Proc. Eurographics, 1993.]]></ref_text>
				<ref_id>Shirman 93</ref_id>
			</ref>
			<ref>
				<ref_obj_id>267057</ref_obj_id>
				<ref_obj_pid>266989</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Swan, J., Mueller, K., M611er, T., Shareef, N., Crawfis, R., and Yagel, R. "An Anti-Aliasing Technique for Splatting," Proc. Visualization, 1997.]]></ref_text>
				<ref_id>Swan 97</ref_id>
			</ref>
			<ref>
				<ref_obj_id>274365</ref_obj_id>
				<ref_obj_pid>274363</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Taubin, G. and Rossignac, J. "Geometric Compression Through Topological Surgery," ACM Trans. on Graphics, Vol. 17, No. 2, 1998.]]></ref_text>
				<ref_id>Taubin 98</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122725</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Teller, S. and Sdquin, C. "Visibility Preprocessing for Interactive Walkthroughs," Proc. SIGGRAPH, 1991.]]></ref_text>
				<ref_id>Teller 91</ref_id>
			</ref>
			<ref>
				<ref_obj_id>329138</ref_obj_id>
				<ref_obj_pid>329129</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Westover, L. "Interactive Volume Rendering," Proc. Volume ~"- sualization Workshop, University of North Carolina at Chapel Hill, 1989.]]></ref_text>
				<ref_id>Westover 89</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1889755</ref_obj_id>
				<ref_obj_pid>1889712</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[Yemez, Y. and Schmitt, E "Progressive Multilevel Meshes from Octree Particles," Proc. 3D Digital Imaging and Modeling, 1999.]]></ref_text>
				<ref_id>Yemez 99</ref_id>
			</ref>
			<ref>
				<ref_obj_id>253314</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[Zhang, H. and Hoff, K. "Fast Backface Culling Using Normal Masks," Proc. Symposium on Interactive 3D Graphics, 1997.]]></ref_text>
				<ref_id>Zhang 97</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 QSplat: A Multiresolution Point Rendering System for Large Meshes Szymon Rusinkiewicz Marc Levoy Stanford 
Universityy Abstract Advances in 3D scanning technologies have enabled the practical cre­ation of meshes 
with hundreds of millions of polygons. Traditional algorithms for display, simpli.cation, and progressive 
transmission of meshes are impractical for data sets of this size. We describe a system for representing 
and progressively displaying these meshes that com­bines a multiresolution hierarchy based on bounding 
spheres with a rendering system based on points. A single data structure is used for view frustum culling, 
backface culling, level-of-detail selection, and rendering. The representation is compact and can be 
computed quickly, making it suitable for large data sets. Our implementa­tion, written for use in a large-scale 
3D digitization project, launches quickly, maintains a user-settable interactive frame rate regardless 
of object complexity or camera position, yields reasonable image qual­ity during motion, and re.nes progressively 
when idle to a high .nal image quality. We have demonstrated the system on scanned models containing 
hundreds of millions of samples. Categories and Subject Descriptors: I.3.3 [Computer Graphics]: Picture/Image 
Generation Display Algorithms; I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling 
 Curve, surface, solid, and object representations; I.3.6 [Computer Graph­ics]: Methodology and Techniques 
 Graphics data structures and data types. Keywords: Rendering systems, Spatial data structures, Level 
of detail algorithms, Compression algorithms 1 Introduction A signi.cant recent trend in computer graphics 
has been the shift to­wards using sampled representations of real objects during rendering. An example 
of this trend has been the increased use of 3D scanning systems, which produce three-dimensional sampled 
models of phys­ical objects. One problem with 3D scanners, however, is handling yStanford Computer Graphics 
Lab Gates Building 3B Stanford University Stanford, CA 94305 {smr,levoy}@graphics.stanford.edu  Permission 
to make digital or hard copies of part or all of this work or personal or classroom use is granted without 
fee provided that copies are not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on 
servers, or to redistribute to lists, requires prior specific permission and/or a fee. SIGGRAPH 2000, 
New Orleans, LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 the large amounts of data they produce. 
Over the past several years, improvements in the hardware and software components of 3D scan­ning systems 
have increased the practically attainable sizes of scanned meshes to hundreds of millions of samples. 
Current workstations can not display meshes of this size in real time, and the running time and space 
requirements of traditional mesh simpli.cation and progressive display algorithms make these approaches 
impractical for scanned meshes containing more than a few million samples. Moreover, many such techniques 
focus on op­timizing the placement of individual edges and vertices, expending a relatively large amount 
of e.ort per vertex. Scanned data, however, has a large number of vertices and their locations are often 
impre­cise due to noise. This suggests an alternative approach in which individual points are treated 
as relatively unimportant, and conse­quently less e.ort is spent per primitive. Recent research employing 
this paradigm includes the spline-.tting system by Krishnamurthy and Levoy [Krishnamurthy 96], the range 
image merging system by Cur­less and Levoy [Curless 96], and Yemez and Schmitt s rendering sys­tem based 
on octree particles [Yemez 99]. These algorithms do not treat range data as exact, and in fact do not 
preserve the 3D locations of any samples of the original mesh. Following this trend of algorithms with 
low per-primitive cost, we have developed a new algorithm for interactive display of large meshes. The 
implementation, dubbed QSplat, was designed during the course of a large 3D digitization project [Levoy 
00]. Because it uses a simple rendering algorithm based on traversing a bounding sphere hierarchy, it 
is suitable for browsing the models generated during this project, which contain 100 million to 1 billion 
samples. Additionally, QSplat does not maintain the connectivity of the input mesh (which in the case 
of scanned data inherently is only useful to resolve depth disconti­nuities and has little other meaning), 
instead relying on a point-based representation and splat rendering. As a result, our system has lower 
preprocessing and rendering costs than comparable polygon-based sys­tems. QSplat launches quickly, adjusts 
level of detail to maintain an interactive frame rate, and has a compact in-memory and on-disk rep­resentation. 
In this paper, we present the QSplat data structure and rendering algorithm and discuss some of the tradeo.s 
and design decisions in­volved in making it practical for large meshes. We describe the ren­dering performance 
of the system, and discuss its preprocessing costs. Finally, we consider its relation to previous algorithms 
for displaying large meshes, and describe some future extensions for rendering other kinds of large geometric 
data sets.  2 QSplat Data Structure and Algorithms QSplat uses a hierarchy of bounding spheres [Rubin 
80, Arvo 89] for visibility culling, level-of-detail control, and rendering. Each node of the tree contains 
the sphere center and radius, a normal, the width of a normal cone [Shirman 93], and optionally a color. 
One could generate such a bounding sphere hierarchy from polygons, voxels, or point clouds, though for 
our application we only needed an algorithm for generating the hierarchy from triangular meshes. The 
hierarchy is constructed as a preprocess, and is written to disk. 2.1 Rendering Algorithm Once the hierarchy 
has been constructed, the following algorithm is used for display: TraverseHierarchy(node) { if (node 
not visible) skip this branch of the tree else if (node is a leaf node) draw a splat else if (bene.t 
of recursing further is too low) draw a splat else for each child in children(node) TraverseHierarchy(child) 
} We now examine several stages of this basic algorithm in detail. Visibility Culling: As we recurse 
the bounding sphere hierarchy, we cull nodes that are not visible. Frustum culling is performed by testing 
each sphere against the planes of the view frustum. If the sphere lies outside, it and its subtree are 
discarded and not processed further. If the sphere lies entirely inside the frustum, this fact is noted 
and no further frustum culling is attempted on the children of the node. We also perform backface culling 
during rendering, using the nor­mal and cone of normals stored at each node. If the cone faces entirely 
away from theviewer, the nodeand its subtree arediscarded. Wealso detect the case of a cone pointing 
entirely towards the viewer, and mark its children as not candidates for backface culling. Determining 
When to Recurse: The heuristic used by QSplat to decide how far to recurse is based on projected size 
on the screen. That is, a node is subdivided if the area of the sphere, projected onto the viewing plane, 
exceeds a threshold. The cuto. is adjusted from frame to frame to maintain a user-selected frame rate. 
We currently use a simple feedback scheme that adjusts the threshold area by the ratio of actual to desired 
rendering time on the previous frame. Funkhouser and Séquin have demonstrated a predictive algorithm 
for LOD con­trol that results in smaller frame-to-frame variation of rendering times [Funkhouser 93]; 
however, we have not implemented this. We also have not incorporated any algorithm for smooth transitions 
as sections of the model change from one level of detail to another, such as the geomorphs in Hoppe s 
progressive mesh system [Hoppe 98]. Given the modest changes in appearance as we re.ne and the quick 
changes in viewpoint typical in our application, we have not found the absence of smoothing visually 
signi.cant; other applications, however, might bene.t from smoother transitions. Although screen-space 
area is the most popular metric for LOD control, other heuristics have been proposed for determining 
how far to recurse [Duchaineau 97, Hoppe 97]. Within the framework of our system, one could incorporate 
rules for recursing further around sil­houette edges (using per-node normals), in areas of high curvature 
(using normal cone widths), or in the central foveal region of the screen (which uses only projected 
position). The above implementation of frame rate control is used during in­teractive manipulation of 
the model. Once the user stops moving the mouse, we redraw the scene with successively smaller thresholds 
until a size of one pixel is reached. Figure 1 shows a sample scene rendered by QSplat at several levels 
of re.nement. Drawing Splats: Once we have either reached a leaf node or de­cided to stop recursing, 
we draw a splat representing the current sphere [Westover 89]. The size of the splat is based on the 
projected diameter of the current sphere, and its color is obtained from a lighting calcu­lation based 
on the current per-sphere normal and color. Splats are drawn with Z-bu.ering enabled to resolve occlusion. 
We discuss the shape of each splat in Section 3.3. 2.2 Preprocessing Algorithm Our preprocessing algorithm 
begins with a triangular mesh represent­ing the model to be encoded. Although one could build up a QSplat 
hierarchy directly from a point cloud, starting with a mesh makes it easy to compute the normals at each 
node. If we did not have a mesh, we would have to compute normals by .tting a plane to the vertices in 
a small neighborhood around each point. Beginning with a mesh also makes it possible to assign sphere 
sizes to the input vertices (which become the leaf nodes in our bounding sphere hierarchy) such that 
no holes are left during rendering. In order to guarantee this, the sizes must be chosen such that if 
two vertices are connected by an edge of the original mesh, the spheres placed at those vertices are 
large enough to touch. Our current algorithm makes the size of the sphere at a ver­tex equal to the maximum 
size of the bounding spheres of all triangles that touch that vertex. This is a conservative method 
it may result in spheres that are too large, but is guaranteed not to leave any holes. Once we have assigned 
leaf sphere sizes, we use the following algo­rithm to build up the rest of the tree: BuildTree(vertices[begin..end]) 
{ if (begin == end) return Sphere(vertices[begin]) else midpoint = PartitionAlongLongestAxis(vertices[begin..end]) 
leftsubtree = BuildTree(vertices[begin..midpoint]) rightsubtree = BuildTree(vertices[midpoint+1..end]) 
return BoundingSphere(leftsubtree, rightsubtree) } The algorithm builds up the tree by splitting the 
set of vertices along the longest axis of its bounding box, recursively computing the two subtrees, and 
.nding the bounding sphere of the two children spheres. As the tree is built up, per-vertex properties 
(such as normal and color) at interior nodes are set to the average of these properties in the subtrees. 
When the recursion reaches a single vertex, we simply create a sphere whose center is the position of 
the vertex. Because the total size of a tree depends on the branching factor at each node, we combine 
nodes in the tree to increase the average branching factor to approximately 4. This reduces the number 
of interior nodes, thereby reducing the storage requirements for the tree. The .nal step of pre­processing 
is quantizing all of the properties at each node, as described in Section 3.1.  3 Design Decisions 
and Tradeoffs Let us now consider some of the decisions made in the implementa­tion of QSplat that make 
it suitable for our application of visualizing large scanned data sets. We describe how tradeo.s in quantization, 
.le layout, splat shape, and the choice of splatting were a.ected by our goals of fast rendering and 
compact representation.  15-pixel cuto. 130,712 points 132 ms 10-pixel cuto. 259,975 points 215 ms 5-pixel 
cuto. 1,017,149 points 722 ms 1-pixel cuto. 14,835,967 points 8308 ms  Figure 1: A model of Michelangelo 
s statue of St. Matthew rendered by QSplat at several levels of re.nement. Rendering was done on an SGI 
Onyx2 with In.niteReality graphics, at a screen resolution of 1280x1024. The model was generated from 
a mesh with 127 million samples, representing a statue 2.7 meters tall at 0.25 mm resolution. The images 
at right are closeups of the outlined areas at left. (a) Bounding Sphere Hierarchy  13 bits 3 bits 14 
bits 2 bits 16 bits Figure 2: QSplat .le and node layout. (a) The tree is stored in breadth-.rst order 
(i.e., the order given by the red arrows). (b) The link from parent to child nodes is established by 
a single pointer from a group of parents to the .rst child. The pointer is not present if all of the 
parent siblings are leaf nodes. All pointers are 32 bits. (c) A single quantized node occupies 48 bits 
(32 without color). 3.1 Node Layout and Quantization The layout of each node in the bounding sphere hierarchy 
is shown in Figure 2c. A node contains the location and size of a sphere relative to its parent, a normal, 
the width of a cone of normals, an optional color, and a few bits used in representing the structure 
of the tree. We discuss the structure of the tree and the layout of nodes within the .le in Section 3.2. 
Position and radius: The position and radius of each sphere is en­coded relative to its parent in the 
bounding sphere hierarchy. In order to save space, these quantities are quantized to 13 values. That 
is, the radius of a sphere can range from 1/13to 13/13 of the radius of its par­ent, and the o.set of 
the center of a sphere relative to the center of its parent (in each of X, Y, and Z) is some multiple 
of 1/13 of the di­ameter of the parent sphere. The quantization proceeds top-down, so the position and 
size of a child sphere is encoded relative to the quan­tized position of its parent; thus, quantization 
error does not propagate down the mesh. In order to guarantee that the quantization process does not 
introduce any holes, the quantized radius is always rounded up to the nearest representable value that 
ensures that the quantized sphere completely encloses the true sphere. Note that not all of the 134 possible 
combinations of (x, y, z)center o.set and radius ratio are valid, since many result in child spheres 
that are not enclosed by their parents. In fact, only 7621 of the possible combinations are valid, which 
means that we can encode the quan­tized position and radius using only 13 bits (using a lookup table). 
For a parent sphere of radius 1, this encoding scheme gives a mean quantization error of 0.04 in the 
x, y,and z components of a child sphere, and a mean error of 0.15 in the child sphere s radius. The er­ror 
in the radius is larger than the error in position because the radius is .rst increased by the quantization 
error in the position of the sphere (to ensure that the quantized sphere encloses the true sphere), and 
is then always rounded up to the next representable value. We could ob­tain lower quantization error 
in the radius by not insisting that the quantized sphere completely enclose the original. Doing so, how­ever, 
would introduce the possibility that spheres that should touch no longer do so after the quantization. 
This could produce holes in our renderings. The idea of representing geometric quantities such as sphere 
po­sitions by encoding them incrementally, thereby essentially spreading out the bits of the quantities 
among the levels in the hierarchy, rep­resents a departure from traditional approaches to mesh compression, 
which rely on encoding the di.erences between vertex positions along some path along the edges of the 
mesh [Taubin 98]. This hierarchical delta coding is, in fact, closer to the wavelet representation of 
geom­etry used in the multiresolution analysis of Eck et. al. [Eck 95]. Our space requirement of 13 bits 
per node appears competitive with state­of-the-art geometric compression methods, which average 9-15 
bits per vertex depending on initial quantization of vertex positions. This is not an entirely valid 
comparison, however, since traditional geomet­ric compression methods also represent mesh connectivity 
(which we discard), and since our 13 bits per node also includes sphere radius. The position and radius 
of each node are decoded on-the-.y during rendering. Because of this, our data structure is not only 
compact on disk, but also requires less memory during rendering than methods that must decompress their 
data before rendering. Normals: The normal at each node is stored quantized to 14 bits. The representable 
normals correspond to points on a 52 .52 grid on each of the 6 faces of a cube, warped to sample normal 
space more uniformly. A lookup table is used during rendering do decode the rep­resentable normals. In 
practice the use of only 52 .52 .6 = 16224 dif­ferent normals (leading to a mean quantization error of 
approximately 0.01 radian) produces no visible artifacts in the di.use shading com­ponent, but some banding 
artifacts are visible around specular high­lights in broad areas of low curvature. It would be possible 
to eliminate these artifacts, as well as achieve better compression, by moving to an incremental encoding 
of each normal relative to the normal of the parent sphere. This would, however, increase the computational 
com­plexity of the inner loop of the algorithm, resulting in a time-space tradeo.. Unlike the range of 
node positions, the space of normals is bounded, so a .xed quantization table su.ces for encoding the 
nor­mals of arbitrary scenes. Therefore, at this time we have chosen to use a .xed quantization for the 
normals, which requires only a single table lookup at run time. As processor speed increases, we anticipate 
that the incremental quantization scheme will become more attractive. Colors: Colors are currently stored 
quantized 5-6-5 to 16 bits. As in the case of normals, an incremental encoding of colors would save spacebut 
be more expensiveatrun time. Normal cones: After some experimentation, we have decided to quantize the 
width of the cone of normals at each node to just 2 bits. The four representable values correspond to 
cones whose half-angles have sines of 1/16, 4/16, 9/16,and 16/16. On typical data sets, backface culling 
with these quantized normal cones discards over 90 percent of nodes that would be culled using exact 
normal cone widths. Note that we are always conservative in representing normal cone widths, so we never 
discard geometry that should be displayed. As with normals and colors, the normal cone widths could be 
represented relative to the widths at the parent nodes, but this would slow down rendering. 3.2 File 
Layout and Pointers Thenodes of thebounding spherehierarchy arelaid out (bothin memory and on disk) in 
breadth-.rst order. A primary consequence of this is that the .rst part of the .le contains the entire 
mesh at low resolution. Thus, we only need to read in the .rst part of a .le in order to visualize the 
model at low resolution; we see greater detail as more of the .le is read in from disk. We currently 
use OS­provided memory mapping as the basis for working-set management, so high-resolution data for a 
given section of the model is read in from disk when the user looks at it. This progressive loading is 
im­portant for usability with large models, for which the time to load the entire data set from disk 
may be several minutes. Because data is loaded as it is needed, rendering performance will be lower the 
.rst time theuser zooms inonsomearea ofthe model dueto our feedback-based approach to frame rate control, 
there is a glitch in the frame rate. Subsequent frames that touch the same area of the model, however, 
are rendered at full speed. Speculative prefetching has been explored as a method for reducing this performance 
varia­tion [Funkhouser 92, Funkhouser 96, Aliaga 99], but we currently do not implement this. Several 
pointerless schemes have been proposed for tree encod­ing, including linear octrees and methods based 
on complete trees [Samet 90]. These data structures, however, are inappropriate for our application. 
Linear octrees and related ideas require the entire tree to be traversed to recover its structure, which 
is impractical in our sys­tem. Data structures based on complete trees can be used for partial traversals, 
but because the algorithm we use to generate our trees is based on axis-aligned bisections, we can not 
guarantee that the result­ing trees will be complete and balanced. Furthermore, modifying the preprocessing 
algorithm to generate complete trees would not be de­sirable, since putting an equal number of vertices 
in each subtree can potentially put the splitting planes signi.cantly o.-center. Given the amount of 
quantization we perform on child sphere centers, this could lead to signi.cant inaccuracies in the compressed 
tree. Although we can not use pointerless encodings for our trees, we should at least attempt to minimize 
the number of pointers required. Given that we store the tree in breadth-.rst order, it is su.cient to 
have one pointer for each group of siblings in the tree (i.e. children of a single parent sphere), that 
points to the children of these nodes. Furthermore, that pointer is not necessary if none of these spheres 
have children (i.e. they are all leaf nodes). Using this scheme, approx­imately 8 to 10 percent of the 
total storage cost is devoted to pointers, which wejudged tobesu.ciently small that we did not pursue 
more complicated schemes for reducing pointer costs further. In order to be able to traverse the tree, 
we store at each node two bits encoding the number of children of the node (0, 2, 3, or 4 children nodes 
with a single child are not permitted), and one bit indicating whether all children of this node are 
leaf nodes. The total storage requirements for a tree may now be computed as the number of nodes in the 
tree multiplied by the cost per node, plus the overhead due to pointers. For a tree with average branching 
factor 3.5, the total number of nodes will be 1.4 times the number of leaf nodes, making the net storage 
requirements for the entire tree approximately 9 bytes times the number of leaf nodes, or 6 bytes if 
colors are not stored. 3.3 Splat Shape The choice of kernel used to represent a rendered point sample 
can have a signi.cant e.ect on the quality of the .nal image. The sim­plest, fastest option is a non-antialiased 
OpenGL point, which is ren­dered as a square. A second choice is an opaque circle, which may be rendered 
as a group of small triangles or, less expensively in most OpenGL implementations, as a single texture-mapped 
polygon. An­other possibility is a fuzzy spot, with an alpha that falls o. radially with a Gaussian or 
some approximation. The particular approximation we use is a spline in opacity that falls to 1/2 at the 
nominal radius of the splat. These last two options will be slower to draw, since they require sending 
more data to the graphics pipeline. In addition, drawing a Gaussian splat requires special care regarding 
the order in which the splats are drawn, because of the interaction between blending and Z­bu.ering. 
Levoy and Whitted discuss this problem in the context of a software-only renderer [Levoy 85]; they propose 
an approach based on buckets to ensure that both occlusion and blending happen correctly. In OpenGL we 
can use multipass rendering to implement the correct behavior. For the .rst pass, depth is o.set away 
from the viewer by some amount z0, and we render only into the depth bu.er. For the second pass we turn 
o. depth o.set and render additively into the color bu.er, with depth comparison but not depth update 
enabled. This has the e.ect of blending together all splats within a depth range z0 of the surface, while 
maintaining correct occlusion. Figure 3 com­pares these three choices of splat kernel. Because per-splat 
drawing time on current hardware is di.erent for each kernel, we present com­parisons at both constant 
splat size and constant running time. Another option we have in choosing splat shape is the choice of 
whether the splats are always round (or square in the case of OpenGL points) or elliptical. In the latter 
case, the normal at each node is used to determine the eccentricity and orientation of the ellipse. When 
the normals point towards the viewer, the splats will be circular. Other­wise, the minor axis of each 
ellipse will point along the projection of the normal onto the viewing plane, and the ratio of minor 
to major axes will equal ^v,where n is the normal of the splat and v is a vector n .^pointing towards 
the viewer. This improves the quality of silhouette edges compared to circular splats, reducing noise 
and thickening. We compare the use of circular and elliptical splats in Figure 4. Because we construct 
our bounding sphere hierarchy such that spheres placed along a continuous surface will never leave holes, 
we can guarantee that the square and circular kernels will always result in hole-free reconstructions. 
Our approximation to a Gaussian kernel is also guaranteed to produce full opacity in areas that started 
out as continuous surfaces. When we move to elliptical kernels, we can no longer guarantee hole-free 
reconstructions because normals need not be continuous along the surface. In practice, we do occasionally 
see holes when using elliptical kernels, especially around silhouette edges. We have found that restricting 
the maximum foreshortening of ellipses (e.g. clamping the maximum ratio of major to minor axis to 10) 
.lls in practically all of these holes.  3.4 Consequences of a Point-Based System Thefact thatQSplatusespointsasits 
rendering primitivesmakes it most suitable for certain kinds of scenes. In particular, point render­ 
 Figure 3: Choices for splat shape. We show a scene rendered using squares, circles, and Gaussians as 
splat kernels. In the top row, each image uses the same recursion threshold of 20 pixels. Relative to 
squares, circles take roughly twice as long to render, and Gaussians take approximately four times as 
long. The Gaussians, however, exhibit signi.cantly less aliasing. In the bottom row, the threshold for 
each image is adjusted to produce the same rendering time in each case. According to this criterion, 
the square kernels appear to o.er the highest quality. Figure 4: Circular vs. elliptical splats. In 
the left image, all splats are circular with diameter 20 pixels. In the right image, we draw elliptical 
splats rotated and foreshortened depending on per-node normals. This reduces thickening and noise around 
silhouette edges. Recursion depth has deliberately been limited to make the splats large enough to see 
in this visualization. (a) (b) (c) Points Polygons same number of primitives as (a) Polygons same 
number of vertices as (a) Same rendering time as (a) Twice the rendering time of (a) Figure 5: Comparison 
of renderings using point and polygon primitives. ing systems are most e.ective for objects with uniformly-sized 
geo­metric detail, and in applications where it is not necessary to look at the model at signi.cantly 
higher resolution than the spacing between samples. If the model has large, .at or subtly curved surfaces, 
polyg­onal models can be more compact and faster to draw. Similarly, if it is necessary to zoom in such 
that the spacing of samples is large com­pared to pixel size, polygons o.er higher visual quality, especially 
near sharp edges and corners. Figure 5 shows a comparison between point­and polygon-based renderings. 
QSplat was developed with the intent of visualizing scanned mod­els that contained signi.cant amounts 
of .ne detail at scales near the scanning resolution. We used the Volumetric Range Image Processing (VRIP) 
system [Curless 96] to merge raw scans into our .nal mod­els, and the marching cubes algorithm [Cline 
88] to extract a polyg­onal mesh. Since the latter produces samples with a uniform spac­ing, point rendering 
was well-suited for our application domain. For scenes with large, smooth regions, we expect that QSplat 
would be less e.ective relative to polygon-based systems. The visual quality of the resulting models 
would still be good, however, if the large polygons were diced, as in the REYES architecture [Cook 87]. 
For applications containing both high-frequency detail and large .at regions, hybrid point/polygon schemes 
might be appropriate.   4Performance As described in Section 3, the goal of interactivity dictated 
many de­sign decisions for our system. In addition to these, we have optimized our implementation in 
several ways in order to increase the size of the models we can visualize. 4.1 Rendering Performance 
The majority of rendering time in our system is spent in an inner loop that traverses the hierarchy, 
computes the position and radius of each node, performs visibility culling, and decides whether to draw 
a point or recurse further. This inner loop was tuned to eliminate expensive operations, especially at 
lower levels of the tree. For example, we do not perform an exact perspective divide at the low levels 
of the tree, switching to an approximation when the screen-space size of a node reaches a few pixels. 
As a result, on average our algorithm can render between 1.5 and 2.5 million points per second on an 
SGI Onyx2 once data has been read in from disk. The exact rate varies depending on caching e.ects (for 
example, we observe a speedup when the working set .ts in L2 cache) and how much data is culled at which 
levels in the tree. Our display rate may be compared to the 480 thousand polygons per second (on identical 
hardware) reported by Hoppe for his im­plementation of progressive meshes [Hoppe 98] or the 180 thousand 
polygons per second for the ROAM system [Duchaineau 97]. For our application, we typically use frame 
rates of 5-10 Hz, meaning that we draw 200 to 300 thousand points per frame during interactive ren­dering. 
Note that unlike the above two systems, QSplat makes no explicit use of frame-to-frame coherence, such 
as cached lists of prim­itives likely to be visible. QSplat s rendering performance is summa­rized in 
Figure 6. The simplicity of our algorithm makes it well suited for implemen­tation on low-end machines. 
As an extreme example, we have imple­mented QSplat on a laptop computer with no 3D graphics hardware 
(366 MHz Intel Pentium II processor, 128 MB memory). Because rendering is performed in software, the 
system is .ll limited. For a typical window size of 500x500 and frame rate of 5 Hz, the imple­mentation 
can traverse 250 to 400 thousand points per second, has a 40 million pixel per second .ll rate, and typically 
draws 50 to 70 thou­sand splats per frame. At this resolution the implementation is still comfortably 
usable. Although most present desktop systems do have 3D graphics hardware, the same is not true for 
portable and handheld systems, and in applications such as digital television set-top boxes. We believe 
that QSplat might be well-suited for such environments.  4.2 Preprocessing Performance Although preprocessing 
time is not as important as rendering time, it is still signi.cant for practical visualization of very 
large meshes. Hoppe reports 10 hours as the preprocessing time for a progressive mesh of 200 thousand 
vertices [Hoppe 97]. Luebke and Erikson report 121 seconds as the preprocessing time for 281 thousand 
ver­tices for their implementation of hierarchical dynamic simpli.cation [Luebke 97]. In contrast, our 
preprocessing time for 200 thousand vertices is under 5 seconds (on the same hardware). Figure 6 presents 
some statistics about the preprocessing time and space requirements of the models used in this paper 
s .gures. Another class of algorithms with which we can compare our pre­processing time is algorithms 
for mesh simpli.cation and decima­tion. Although these algorithms have di.erent goals than QSplat, they 
are also commonly used for generating multiresolution represen­tations or simplifying meshes for display. 
Lindstrom and Turk have published a comparison of several recent mesh simpli.cation meth­  David s head, 
1mm, color David, 2mm St. Matthew, 0.25mm Typical performance Interactive Static Interactive Static 
Interactive Static Traverse tree 22 ms 448 ms 30 ms 392 ms 27 ms 951 ms Compute position and size 19 
ms 126 ms 30 ms 307 ms 31 ms 879 ms Frustum culling 1ms 4 ms 1ms 3 ms 1ms 3 ms Backface culling 1ms 22 
ms 2ms 25ms 1ms 35ms Draw splats 77 ms 364 ms 46 ms 324 ms 50 ms 1281 ms Total rendering time 120 ms 
838 ms 109 ms 1051 ms 110 ms 3149 ms Points rendered 125,183 931,093 267,542 2,026,496 263,915 8,110,665 
 Preprocessing statistics Input points (= leaf nodes) 2,000,651 4,251,890 127,072,827 Interior nodes 
974,114 2,068,752 50,285,122 Bytes per node 6 4 4 Space taken by pointers 1.3 MB 2.7 MB 84 MB Total .le 
size 18 MB 27 MB 761 MB Preprocessing time 0.7 min 1.4 min 59 min Figure 6: Typical QSplat rendering 
and preprocessing statistics for three models. The columns marked interactive indicate typical performance 
when the user is manipulating the model. The columns labeled static are typical of performance when the 
user has stopped moving the mouse and the scene has re.ned toits highest-quality version. Variation of 
up to 30% has been observed in these timings, depending on details such as cache performance. All times 
were measured on an SGI Onyx2 with In.niteReality graphics; rendering was done at 1280x1024 resolution. 
ods [Lindstrom 98]. They report times of between 30 seconds and 45 minutes for simpli.cation of a bunny 
mesh with 35000 vertices. One method that paper did not consider was the voxel-based simpli.cation of 
Rossignac and Borrel [Rossignac 93], which takes under one second on identical hardware to that used 
by Lindstrom and Turk. Our pre­processing time for this mesh is 0.6 seconds. Thus, our algorithm is signi.cantly 
faster than most of the contemporary mesh decimation algorithms, and competitive with Rossignac and Borrel 
s method.   5 Previous Work Previous approaches for representing and displaying large models can be 
grouped into point rendering, visibility culling, level-of-detail con­trol, and geometric compression. 
5.1 Point Rendering Computer graphics systems traditionally have used triangles as ren­dering primitives. 
In an attempt to decrease the setup and rasteri­zation costs of triangles for scenes containing a large 
amount of ge­ometry, a number of simpler primitives have been proposed. The use of points as a display 
primitive for continuous surfaces was in­troduced by Levoy and Whitted [Levoy 85], and more recently 
has been revisited by Grossman and Dally [Grossman 98]. Point render­ing has been incorporated into commercial 
products the Animatek Caviar system, for example, uses point rendering for animated char­acters in video 
games [Animatek]. Particles have also been used in more specialized contexts, such as rendering .re, 
smoke, and trees [Csuri 79, Reeves 83, Max 95]. A concept related to point rendering is splatting in 
volume render­ing [Westover 89]. For large volumes, it is natural to use a hierarchical data structure 
to achieve compression of regions of empty space, and Laur and Hanrahan have investigated hierarchical 
splatting for vol­umes represented using octrees [Laur 91]. Although splatting is best suited to the 
case in which the projected voxel size is on the order of the pixel size, other regimes have also been 
examined. The di­viding cubes algorithm proposed by Cline et. al. is intended for use when voxels are 
larger than pixels [Cline 88]. For voxels smaller than pixels, Swan et. al. have proposed algorithms 
for producing correctly antialiased results [Swan 97]. 5.2 Visibility Culling Frustum and backface culling 
algorithms, such as those used by QS­plat, have appeared in a large number of computer graphics systems. 
Hierarchical frustum culling based on data structures such as oc­trees has been a standard feature of 
most systems for rendering large scenes [Samet 90]. Backface culling of primitives is commonly im­plemented 
in hardware, and Kumar and Manocha have presented an algorithm for hierarchical backface culling based 
on cones of normals [Kumar 96]. Another class of visibility culling algorithms includes methods for occlusion 
culling. Greene et. al. describe a general algorithm to discard primitives that are blocked by closer 
geometry using a hierarchical Z­bu.er [Greene 93]. Other, more specialized occlusion algorithms can also 
be used if the scene is highly structured. Systems for architec­tural .ythroughs, for example, often 
use the notion of cells and por­tals to cull away entire rooms that are not visible [Teller 91]. QSplat 
currently does not perform any sort of occlusion culling it would provide minimal bene.t for viewing 
the scanned models we are con­sidering. Occlusion culling would, however, be a useful addition for scenes 
of greater depth complexity. 5.3 Level of Detail Control Rendering a large data set at low magni.cation 
will often cause primi­tives to be smaller than output device pixels. In order to minimize ren­dering 
time in these cases, it is desirable to switch to a lower-resolution data set with primitives that more 
closely match the output display resolution. Among LOD algorithms, one may di.erentiate those that store 
entire objects at discrete levels of detail from methods that per­form .ner-grained LOD control. The 
algorithms in the latter class can control the number of primitives continuously, minimizing pop­ping 
artifacts, and often vary the level of detail throughout the scene to compensate for the varying magni.cation 
of perspective projection. Multiresolution analysis represents an object as a base mesh, with a series 
of corrections stored as wavelet coe.cients [Eck 95]. Certain et al. have implemented a real-time viewer 
based on multiresolution meshes that can select an arbitrary number of wavelet coe.cients to be used, 
and so draw a mesh with any desired number of polygons [Certain 96]. Their viewer also includes features 
such as progressive transmission and separate sets of wavelet coe.cients for geometry and color. Progressive 
meshes use a base mesh together with a series of vertex split operations [Hoppe 96]. Progressive meshes 
have been incorpo­rated into a real-time viewer that performs view-dependent re.ne­ment for real-time 
.ythroughs of scenes of several million polygons [Hoppe 97, Hoppe 98]. The viewer can not only select 
an arbitrary number of polygons to draw, but also re.ne di.erent parts of an object to di.erent resolutions. 
Other recent systems that allow level of de­tail to vary throughout the scene include the ROAM terrain 
rendering system [Duchaineau 97], and LDI trees [Chang 99]. The implemen­tation of LOD control in QSplat 
has the same goal as these systems, permitting the level of detail to vary smoothly throughout a scene 
ac­cording to projected screen size. 5.4 Geometric Compression The goal of geometric compression is 
to reduce the storage and mem­ory requirements of large meshes, as well as their transmission costs. 
Deering has presented a system for compression of mesh connectivity, vertex locations, colors, and normals, 
which was later implemented in hardware [Deering 95]. More recent research, such as the Topo­logical 
Surgery scheme by Taubin and Rossignac, has focused on re­ducing the cost of representing mesh connectivity 
and improving the compression of vertex positions [Taubin 98]. Pajarola and Rossignac have applied compression 
to progressive meshes, yielding a scheme that combines level-of-detail control and progressive re.nement 
with a compact representation [Pajarola 99]. Their algorithm, however, has higher preprocessing and decoding 
costs than QSplat.  6 Conclusions and Future Work The QSplat system has demonstrated real-time progressive 
rendering of large models. QSplat s architecture matches the rendering speed of state-of-the-art progressive 
display algorithms, has preprocessing times comparable to the fastest presently-available mesh decimators, 
and achieves compression ratios close to those of current geometric compression techniques. Because the 
QSplat viewer is lightweight and can be implemented on low-cost hardware, we believe it has the potential 
for permitting 3D rendering in applications where it was previously impractical, for example built-in 
kiosks in museums. On May 6, 1999, we set up QSplat on a computer in the Medici Chapel in Florence, displaying 
our partially-completed computer model of the statue of Dawn, and we let the tourists play (see Figure 
7). Figure 7: Tourists in the Medici Chapel using QSplat to .y around our 3D model of Michelangelo s 
statue of Dawn. We simpli.ed the interface to only al­low rotating, translating, and relighting the model. 
Nevertheless, some tourists managed to get the viewer into various confusing states, typically by zooming 
in too far. This underscores the need for a simple, robust, and constrained user interface. We found 
that most tourists appreciated having a computer model of the statue at which they were looking; having 
the capability to see the statue from other views, and to change its virtual lighting, made looking at 
the statue a more active, hands-on experience. Several previously-introduced techniques could be incorporated 
into the present QSplat framework to make it more time and space e.cient: .Hu.man coding [Hu.man 52] 
or another lossless compression scheme could be used to make the current representation more compact. 
This would be useful for o.ine storage or transmis­sion across low-bandwidth communications links, but 
would re­quire the model to be decompressed before rendering. .For cases when rendering speed is more 
important than com­pact representation, the algorithm could be sped up by elimi­nating the compression 
and incremental encoding of sphere po­sitions and sizes (as described in Section 3.1), and simply storing 
these quantities as .oats. In addition, normal masks and visibility masks, such as those used by Grossman 
and Dally, could speed up rendering if there is a signi.cant amount of large-scale occlu­sion [Zhang 
97, Grossman 98]. A further gain in speed could be achieved by parallelizing the rendering algorithm, 
distribut­ing portions of the tree to di.erent processors. We can already parallelize our preprocessing 
algorithm by breaking up the mesh into tiles, though we have reported single-processor results in this 
paper. .Further analysis is necessary to understand the temporal coher­ence and caching behavior of QSplat. 
A large amount of sys­tems research has been done on frame rate control and work­ing set management techniques 
in terrain rendering and archi­tectural walkthrough systems [Funkhouser 96], and those algo­rithms would 
improve the smoothness of user interaction with QSplat. The following are potential areas of future research 
for combining the QSplat approach with di.erent kinds of algorithms within com­puter graphics: .The bounding 
sphere hierarchy used by QSplat is well-suited as an acceleration data structure for ray tracing. Potentially, 
this could be used for high-quality renderings with advanced render­ing e.ects of models stored in the 
QSplat format. .Instancing would be easy to incorporate into our tree-based data structure and rendering 
algorithm, greatly reducing the memory requirements for many classes of procedurally-de.ned scenes. This 
could be thought of as a new form of view-dependent sprite, permitting e.cient inclusion of geometry 
at multiple lo­cations within a scene. .Items other than normals and colors could be stored at each node. 
Transparency (alpha), BRDFs, and BTDFs would be ob­vious candidates that would increase the visual complexity 
rep­resentable by QSplat, giving it capabilities similar to those of modern volumetric renderers [Kajiya 
89]. More complicated objects such as light .elds, view-dependent textures, spatially­varying BRDFs, 
and layered depth images could potentially also be stored at each node, creating hybrids of point rendering 
sys­tems and contemporary image-based renderers. Acknowledgments Thanks to Dave Koller, Jonathan Shade, 
Matt Ginzton, Kari Pulli, Lucas Pereira, James Davis, and the whole DMich gang. The Digital Michelangelo 
Project was sponsored by Stanford University, Interval Research Corporation, and the Paul Allen Foundation 
for the Arts. References [Aliaga 99] Aliaga, D., Cohen, J., Wilson, A., Baker, E., Zhang, H., Erik­son, 
C., Ho., K., Hudson, T., Stuerzlinger, W., Bastos, R., Whit­ton, M., Brooks, F., and Manocha, D. MMR: 
An Interactive Massive Model Rendering System Using Geometric and Image-Based Accelera­tion, Proc. Symposium 
on Interactive 3D Graphics, 1999. [Animatek] AnimaTek International, Inc., Caviar Technology, Web page: 
http://www.animatek.com/products_caviar.htm [Arvo 89] Arvo, J. and Kirk, D. A Survey of Ray Tracing Acceleration 
Tech­niques, An Introduction to Ray Tracing, Glassner, A. S. ed., Academic Press, 1989. [Certain 96] 
Certain, A., Popovi´c, J, DeRose, T., Duchamp, T., Salesin, D., and Stuetzle, W. Interactive Multiresolution 
Surface Viewing, Proc. SIGGRAPH, 1996. [Chang 99] Chang, C., Bishop, G., and Lastra, A. LDI Tree: A Hierarchical 
Representation for Image-Based Rendering, Proc. SIGGRAPH, 1999. [Cline 88] Cline, H. E., Lorensen, W. 
E., Ludke, S., Crawford, C. R., and Teeter, B. C. Two Algorithms for the Three-Dimensional Reconstruc­tion 
of Tomograms, Medical Physics, Vol. 15, No. 3, 1988. [Cook 87] Cook, R., Carpenter, L., and Catmull, 
E. The Reyes Image Ren­dering Architecture, Proc. SIGGRAPH, 1987. [Csuri 79] Csuri, C., Hackathorn, R., 
Parent, R., Carlson, W., and Howard, M. Towards an Interactive High Visual Complexity Animation Sys­tem, 
Proc. SIGGRAPH, 1979. [Curless 96] Curless, B. and Levoy, M. A Volumetric Method for Building Complex 
Models from Range Images, Proc. SIGGRAPH, 1996. [Deering 95] Deering, M. Geometry Compression, Proc. 
SIGGRAPH, 1995. [Duchaineau 97] Duchaineau, M., Wolinsky, M., Sigeti, D,. Miller, M., Aldrich, C., and 
Mineev-Weinstein, M. ROAMing Terrain: Real-time Optimally Adapting Meshes, Proc. Visualization, 1997. 
[Eck 95] Eck, M., DeRose, T., Duchamp, T., Hoppe, H., Lounsbery, M., and Stuetzle, W. Multiresolution 
Analysis of Arbitrary Meshes, Proc. SIGGRAPH, 1995. [Funkhouser 92] Funkhouser, T., Séquin, C., and Teller, 
S. Management of Large Amounts of Data in Interactive Building Walkthroughs, Proc. Symposium on Interactive 
3D Graphics, 1992. [Funkhouser 93] Funkhouser, T. and Séquin, C. Adaptive Display Algo­rithm for Interactive 
Frame Rates During Visualization of Complex Vir­tual Environments, Proc. SIGGRAPH, 1993. [Funkhouser 
96] Funkhouser, T. Database Management for Interactive Dis­play of Large Architectural Models, Graphics 
Interface, 1996. [Greene 93] Greene, N., Kass, M., and Miller, G. Hierarchical Z-bu.er Vis­ibility, Proc. 
SIGGRAPH, 1993. [Grossman 98] Grossman, J. and Dally, W. Point Sample Rendering, Proc. Eurographics Rendering 
Workshop, 1998. [Hoppe 96] Hoppe, H. Progressive Meshes, Proc. SIGGRAPH, 1996. [Hoppe 97] Hoppe, H. View-Dependent 
Re.nement of Progressive Meshes, Proc. SIGGRAPH, 1997. [Hoppe 98] Hoppe, H. Smooth View-Dependent Level-of-Detail 
Control and its Application to Terrain Rendering, Proc. Visualization, 1998. [Hu.man 52]Hu.man, D. A 
Method for the Construction of Minimum Redundancy Codes, Proc. IRE, Vol. 40, No. 9, 1952. [Kajiya 89] 
Kajiya, J. and Kay, T. Rendering Fur with Three Dimensional Textures, Proc. SIGGRAPH, 1989. [Krishnamurthy 
96] Krishnamurthy, V. and Levoy, M. Fitting Smooth Sur­faces to Dense Polygon Meshes, Proc. SIGGRAPH, 
1986. [Kumar 96] Kumar, S., Manocha, D., Garrett, W., and Lin, M. Hierarchical Back-Face Computation, 
Proc. Eurographics Rendering Workshop, 1996. [Laur 91] Laur, D. and Hanrahan, P. Hierarchical Splatting: 
A Progres­sive Re.nement Algorithm for Volume Rendering, Proc. SIGGRAPH, 1991. [Levoy 85] Levoy, M. and 
Whitted, T. The Use of Points as a Display Prim­itive, Technical Report TR 85-022, University of North 
Carolina at Chapel Hill, 1985. [Levoy 00] Levoy, M., Pulli, K., Curless, B., Rusinkiewicz, S., Koller, 
D., Pereira, L., Ginzton, M., Anderson, S., Davis, J., Ginsberg, J., Shade, J., and Fulk, D. The Digital 
Michelangelo Project: 3D Scanning of Large Statues, Proc. SIGGRAPH, 2000. [Lindstrom 98] Lindstrom, P. 
and Turk, G. Fast and Memory E.cient Polygonal Simpli.cation, Proc. Visualization, 1998. [Luebke 97] 
Luebke, D., and Erikson, C. View-Dependent Simpli.cation of Arbitrary Polygonal Environments, Proc. SIGGRAPH, 
1997. [Max 95] Max, N. and Ohsaki, K. Rendering Trees from Precomputed Z­bu.er Views, Proc. Eurographics 
Rendering Workshop, 1995. [Pajarola 99] Pajarola, R. and Rossignac, J. Compressed Progressive Meshes, 
Technical Report GIT-GVU-99-05, Georgia Institute of Tech­nology, 1999. [Reeves 83] Reeves, W. Particle 
Systems A Technique for Modeling a Class of Fuzzy Objects, Proc. SIGGRAPH, 1983. [Rossignac 93] Rossignac, 
J. and Borrel, P. Multi-Resolution 3D Approxi­mations for Rendering Complex Scenes, Geometric Modeling 
in Com­puter Graphics, 1993. [Rubin 80] Rubin, S. M. and Whitted, T. A 3-Dimensional Representation for 
Fast Rendering of Complex Scenes, Proc. SIGGRAPH, 1980. [Samet 90]Samet, H. Applications of Spatial Data 
Structures, Addison-Wesley, 1990. [Shirman 93] Shirman, L. and Abi-Ezzi, S. The Cone of Normals Technique 
for Fast Processing of Curved Patches, Proc. Eurographics, 1993. [Swan 97] Swan, J., Mueller, K., Möller, 
T., Shareef, N., Craw.s, R., and Yagel, R. An Anti-Aliasing Technique for Splatting, Proc. Visualization, 
1997. [Taubin 98] Taubin, G. and Rossignac, J. Geometric Compression Through Topological Surgery, ACM 
Trans. on Graphics, Vol. 17, No. 2, 1998. [Teller 91] Teller, S. and Séquin, C. Visibility Preprocessing 
for Interactive Walkthroughs, Proc. SIGGRAPH, 1991. [Westover 89] Westover, L. Interactive Volume Rendering, 
Proc. Volume Vi­sualization Workshop, University of North Carolina at Chapel Hill, 1989. [Yemez 99] Yemez, 
Y. and Schmitt, F. Progressive Multilevel Meshes from Octree Particles, Proc. 3D Digital Imaging and 
Modeling, 1999. [Zhang 97] Zhang, H. and Ho., K. Fast Backface Culling Using Normal Masks, Proc. Symposium 
on Interactive 3D Graphics, 1997.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344938</article_id>
		<sort_key>353</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>39</seq_no>
		<title><![CDATA[A fast relighting engine for interactive cinematic lighting design]]></title>
		<page_from>353</page_from>
		<page_to>358</page_to>
		<doi_number>10.1145/344779.344938</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344938</url>
		<abstract>
			<par><![CDATA[<p>We present new techniques for interactive cinematic lighting design of complex scenes that use procedural shaders. Deep-framebuffers are used to store the geometric and optical information of the visible surfaces of an image. The geometric information is represented as collections of oriented points, and the optical information is represented as bi-directional reflection distribution functions, or BRDFs. The BRDFs are generated by procedurally defined surface texturing functions that spatially vary the surfaces' appearances.</p><p>The deep-framebuffer information is rendered using a multi-pass algorithm built on the OpenGL graphics pipeline. In order to handle both physically-correct as well as non-realistic reflection models used in the film industry, we factor the BRDF into independent components that map onto both the lighting and texturing units of the graphics hardware. A similar factorization is used to control the lighting distribution. Using these techniques, lighting calculations can be evaluated 2500 times faster than previous methods. This allows lighting changes to be rendered at rates of 20Hz in static environments that contain millions of objects of with dozens of unique procedurally defined surface properties and scores of lights.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[animation]]></kw>
			<kw><![CDATA[illumination]]></kw>
			<kw><![CDATA[image-based rendering]]></kw>
			<kw><![CDATA[optics]]></kw>
			<kw><![CDATA[rendering]]></kw>
			<kw><![CDATA[rendering hardware]]></kw>
			<kw><![CDATA[texture mapping]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Bitmap and framebuffer operations</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P240369</person_id>
				<author_profile_id><![CDATA[81100660401]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Reid]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gershbein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15033698</person_id>
				<author_profile_id><![CDATA[81100482576]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pat]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hanrahan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{1} Lightscape visualization system.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>272314</ref_obj_id>
				<ref_obj_pid>272313</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{2} Ronen Barzel. Lighting controls for computer cinematography. <i>Journal of Graphics Tools</i>, 2(1): 1-20, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{3} J. F. Blinn. Raster graphics. In K. Booth, editor, <i>Tutorial: Computer Graphics</i>. IEEE Computer Society, 1979.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237211</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{4} Normand Briere and Pierre Poulin. Hierarchical view-dependent structures for interactive scene manipulation. In <i>Computer Graphics Annual Conference Series 1996</i>, pages 83-90. Siggraph, August 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{5} Sharon Calahan. Storytelling through lighting, a computer perspective. In Tony Apodaca and Larry Gritz, editors, <i>Advanced Renderman: Creating CGI for Motion Pictures</i>, pages 337-382. Morgan Kaufman Publishers, San Francisco, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808602</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{6} Robert L. Cook. Shade trees. In Hank Christiansen, editor, <i>Computer Graphics (SIGGRAPH '84 Proceedings)</i>, volume 18, pages 223-231, July 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{7} NVIDIA Corporation. <i>GeForce 256 Graphics Accelerator Specifications</i>, December 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617966</ref_obj_id>
				<ref_obj_pid>616035</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{8} J. Dorsey, J. Arvo, and D. Greenberg. Interactive design of complex time-dependent lighting. <i>IEEE Computer Graphics and Applications</i>, 15(2): 26-36, March 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122723</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{9} Julie O'B. Dorsey, Francois X. Sillion, and Donald P. Greenberg. Design and simulation of opera lighting and projection effects. <i>Computer Graphics</i>, 25(4): 41-50, July 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[{10} J. P. Grossman and Willian J. Dally. Point sample rendering. In George Drettakis and Nelson Max, editors, <i>Eurographics Rendering Workshop</i>, Eurographics, pages 181-192, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218470</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[{11} Brian Guenter, Todd B. Knoblock, and Erik Ruf. Specializing shaders. In <i>Computer Graphics Annual Conference Series 1995</i>, pages 343-350. Siggraph, August 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97911</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[{12} Pat Hanrahan and Jim Lawson. A language for shading and lighting calculations. In Forest Baskett, editor, <i>Computer Graphics (SIGGRAPH '90 Proceedings)</i>, volume 24, pages 289-298, August 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[{13} Paul S. Heckbert. Techniques for real-time frame buffer animation. In <i>Computer FX 84 Conference</i>, October 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311554</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[{14} Wolfgang Heidrich and Hans-Peter Seidel. Realistic, hardware-accelerated shading and lighting. In Alyn Rockwood, editor, <i>Computer Graphics (SIGGRAPH '99 Proceedings)</i>, pages 171-178, August 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166136</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[{15} John K. Kawai, James S. Painter, and Michael F. Cohen. Radioptimization - goal based rendering. In <i>Computer Graphics Annual Conference Series 1993</i>, pages 147-154. Siggraph, August 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[{16} Marc Levoy and Turner Whitted. The use of points as a display primitive. Technical Report 85-022, UNC-Chapel Hill Computer Science, 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[{17} Rong Lu, Jan J. Koenderink, and Astrid M. Kappers. Optical properties (bidirectional reflection distribution functions) of velvet. In <i>Applied Optics</i>, volume 37, pages 5974-5984, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[{18} M. Minnaert. The reciprocity principle in lunar photometry. In <i>Astrophys. J.</i>, volume 93, pages 403-410, 1941.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325247</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[{19} Ken Perlin. An image synthesizer. <i>Computer Graphics</i>, 19(3): 287-296, July 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>147160</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[{20} Pierre Poulin and A. Fournier. Lights from highlights and shadows. In <i>1992 Symposium on Interactive 3D Graphics</i>, pages 31-38, March 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97901</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[{21} Takafumi Saito and Tokiichiro Takahashi. Comprehensible rendering of 3-d shapes. In <i>Computer Graphics Annual Conference Series 1990</i>, pages 197-206, August 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166135</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[{22} Chris Schoeneman, Julie Dorsey, Brian Smits, James Arvo, and Donald Greenberg. Painting with light. In <i>Computer Graphics Annual Conference Series 1993</i>, pages 143-146. Siggraph, August 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134071</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[{23} Mark Segal, Carl Korobkin, Rolf van Widenfelt, Jim Foran, and Paul Haeberli. Fast shadows and lighting effects using texture mapping. In <i>Computer Graphics Annual Conference Series 1992</i>, pages 249-252, July 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74365</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[{24} Carlo H. Sequin and Eliot K. Smyrl. Parameterized ray tracing. In <i>Computer Graphics Annual Conference Series 1989</i>, pages 307-314. Siggraph, July 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[{25} Kenneth R. Sloan and Christopher M. Brown. Color map techniques. <i>Computer Graphics and Image Processing</i>, 10: 297-317, 1979.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[{26} Steve Upstill. <i>The RenderMan Companion</i>. Addison Wesley, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[{27} Gregory J. Ward. The radiance lighting simulation and rendering system.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97919</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[{28} Lee Westover. Footprint evaluation for volume rendering. <i>Computer Graphics</i>, 24(4): 367-376, August 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Permission to make digital or hard copies of part or all of this work for personal or classroom use 
is granted without fee provided that copies are not made or distributed for profit or commercial advantage 
and that copies bear this notice and the full citation on the first page. Copyrights for components of 
this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, 
to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or 
a fee. A Fast Relighting Engine for Interactive Cinematic Lighting Design Reid Gershbein Pat Hanrahan 
Stanford University Abstract We present new techniques for interactive cinematic lighting design of 
complex scenes that use procedural shaders. Deep-framebuffers are used to store the geometric and optical 
information of the visible surfaces of an image. The geometric information is represented as collections 
of oriented points, and the optical information is repre­sented as bi-directional re.ection distribution 
functions, or BRDFs. The BRDFs are generated by procedurally de.ned surface texturing functions that 
spatially vary the surfaces appearances. The deep-framebuffer information is rendered using a multi-pass 
algorithm built on the OpenGL graphics pipeline. In order to han­dle both physically-correct as well 
as non-realistic re.ection mod­els used in the .lm industry, we factor the BRDF into independent components 
that map onto both the lighting and texturing units of the graphics hardware. A similar factorization 
is used to control the lighting distribution. Using these techniques, lighting calculations can be evaluated 
2500 times faster than previous methods. This allows lighting changes to be rendered at rates of 20Hz 
in static en­vironments that contain millions of objects with dozens of unique procedurally de.ned surface 
properties and scores of lights. CR Categories: I.3.3 [Computer Graphics]: Picture/Image Gen­eration; 
Keywords: Animation, Illumination, Image-based Rendering, Optics, Rendering Hardware, Rendering, Texture 
Mapping 1 Introduction Cinematography and lighting are extremely important aspects of movie making, as 
shown by the visual richness of the computer an­imated movie Toy Story 2 and the special effects in Star 
Wars: The Phantom Menace. The cinematic goals of an animated computer­generated .lm are the same as a 
live-action picture: add depth and clarity to the images, create mood and atmosphere, and focus the viewer 
s attention on the action [5]. The similarities between the real and virtual media do not end there. 
Both use elaborate sets and perhaps thousands of props and characters; both use visually complex materials 
and detailed surfaces; and both require scores of light sources. However, there is one aspect of designing 
and evaluating lighting in computer-generated .lms that is different than a live-action .lm. In a live-action 
.lm the lights are moved and positioned by the grips and the visual result can be seen immediately by 
the cinematogra­pher. Currently, the opposite is true for computer-generated cine­matography. After the 
cinematographer directs the lighting artist to move a light, the scene must be rendered again. This is 
a major bottleneck because production rendering systems require minutes to hours to render scenes of 
movie complexity. The slow rendering rate limits the turnaround time and, ultimately, the productivity 
of the lighting artist. Moreover, since lighting artists are responsible for creating the .nal visual 
appearance of the .lm, lighting is one of the .nal stages in production and is done under extreme time 
pres­sure. Thus, building an interactive cinematic lighting system would greatly bene.t the computer 
animation industry. The goals of interactive lighting design are easy to state. What a lighting artist 
wants is the ability to add, remove, and change the position of a light in real-time. It should also 
be possible to mod­ify any of the light s attributes such as beam width, fall-off or .ap position. Hard 
and soft shadows are also important. Finally, it is important that the quality of the .nal image be maintained; 
approx­imations that reduce quality are undesirable. It should be noted that it is rare to change the 
surface proper­ties of the objects and characters in the scene during the lighting phase. Controlling 
the look by changing the lights is part of the artistic legacy of traditional .lm making and is emulated 
by com­puter animation production houses. Controlling surface properties on a shot by shot basis is largely 
impractical in the real-world; and even in the world of computers the properties of most characters are 
set once and not changed as the characters are reused in differ­ent shots. Most previous computer lighting 
design systems allow modi.cation of both surface and light source properties and, unlike our system, 
have not been optimized for only lighting design. In this paper, we present a simple rendering engine 
for interactive lighting design. Like previous systems it uses a deep-framebuffer to store the geometric 
and optical information of the visible surfaces from a .xed viewpoint. One main innovation is to treat 
this deep­framebuffer as a set of oriented surface samples so that they may be rendered using the OpenGL 
graphics pipeline. The state of the graphics pipeline is set to evaluate the re.ection from the oriented 
surface sample due to a single light source. The second innova­tion is that the optical properties of 
a surface sample are reduced to only the bi-directional re.ection distribution function (BRDF). All spatially 
varying procedural and texture-mapped surface attributes are evaluated during the process of producing 
the deep-framebuffer. Our experience is that this leads to a signi.cant speedup, since the majority of 
the time spent in most surface shaders goes into cal­culating textures and not re.ection models. The 
.nal innovation is to enhance the builtin OpenGL BRDF and light source models to be more like those used 
in production quality rendering sys­tems. Inspired by techniques recently developed by Heidrich and Seidel[14], 
we factor the BRDF and light source into independent components that may be evaluated using multiple 
rendering passes that use both the lighting and texturing units. This collection of simple techniques 
increase the rate at which lighting calculations are computed by at least three orders of magnitude and 
allow the system to render 512 by 256 images of arbitrarily complex envi­ronments with procedural surface 
and lighting functions at approx­imately 20 Hz. 2 Related Work Good examples of current lighting simulation 
systems are those available for buildings and theatrical sets. Lightscape [1] and Radi­ance [27] are 
among the best of the systems designed to accurately predict the lighting in architectural spaces. They 
allow the designer to select a light .xture from a catalog, and they use the outgoing spectral distribution 
of the light source during the simulation. They also model the effects of skylighting and have the capability 
to sim­ulate indirect lighting. These systems focus on physical simulation, and do not support programmable 
shading languages. Dorsey et al. have built a system for designing opera lighting [9] [8]. A ma­jor feature 
of this system is its ability to choreograph the complex time-dependent changes in lighting throughout 
the production. Barzel discusses the controls and features of the light source model used at Pixar Animation 
Studios [2]. This model empha­sizes the computer cinematographer s need for precise control over the 
shape and pattern of the light and its interaction with an object. Our system is designed to control 
light sources such as these. There have been a number of methods proposed for quickly ren­dering changes 
in lighting based on using linearity and basis im­ages. Linearity implies that the image formed from 
two lights is the sum of the images formed from each light separately. Linearity also implies that the 
effect of a change in color or intensity of a light can be ef.ciently recomputed by scaling the basis 
image [8]. Interactive methods have been developed for controlling light positions and parameters using 
inverse methods. Poulin and Fournier present a technique for determining light positions based on the 
position and orientation of a highlight or shadow boundary [20]. Schoeneman et al. describe a system 
in which the user paints over the image to brighten or darken different regions, and the sys­tem solves 
for the colors and intensities of a set of .xed lights using a constrained least squares approach[22]. 
In a system presented by Kawai et al., the user speci.es desired features of the lighting, such as visual 
clarity or contrast, and an optimization procedure is used to search for surface properties, such as 
re.ectivities, and light properties, such as colors, intensities, and directions, that maximize the quality 
of the lighting [15]. All these techniques are impor­tant and useful, but in isolation none of them solve 
the cinematic lighting problem, especially in scenes with more complex shading models and descriptions. 
A major method used to accelerate rendering for design purposes is to .x the camera position and create 
a deep-framebuffer im­age. A deep-framebuffer image contains all the information needed to rerender the 
scene, assuming only surface properties and light sources are changed. They do this by precomputing all 
the geo­metric information and storing it at each pixel. This precompu­tation makes it unnecessary to 
repeat geometric calculations such as tessellating the surface, transforming positions, and removing 
hidden surfaces during rerendering. The oldest technique of this type is orientation coding [3] [25] 
[13], where a normal vector is stored in the framebuffer and the lighting calculation is tabulated in 
the colormap. To change the surface or lighting attributes involves only recomputing the colormap entries. 
The natural extension of this technique is the G-buffer [21]. In the G-buffer, a collection of framebuffers 
is allocated, one per attribute. The information avail­able in these framebuffers is available for shading 
calculations. This technique was considerably extended by Sequin and Smyrl [24]; in their system they 
store a ray tree at each pixel and reevaluate the ray tree to recompute the effects due to changes in 
surface properties or lights. Perhaps the most sophisticated deep-framebuffer system is the one built 
by Briere and Poulin [4]. In their system, visibility structures are stored that accelerate the recomputation 
of shadows and others effects due to re.ection and transparency. As a result their system is able to 
handle a simple moving environment. Unfor­tunately, the data structures they use require a great deal 
of memory, and so their system is impractical for very complex scenes. The lighting system discussed 
in this paper is designed to be used with procedural shaders [19] [6] [12] [26]. Using procedu­ral shaders 
signi.cantly complicates the system design for several reasons. First, the lighting system must be general 
enough to han­dle arbitrary shading programs. Second, pro.les of rendering sys­tems show that 70-90% 
of the time is spent evaluating procedural shaders. Thus, a simple minded deep-framebuffer approach is 
only likely to speed up rendering time by 10-30% (the cost of all other calculations). Guenter et al. 
have described an approach to incrementally eval­uate procedural shaders by using partial evaluation 
and generalized shader specialization[11]. Although this method is very general, it is quite complex. 
Specialized versions of the shaders must be created for all combinations of surface and light parameters. 
In ad­ditional, partial evaluation requires a complex caching scheme that uses a lot of memory and is 
relatively dif.cult to manage. The com­bination of the methods used in this paper are much simpler, and 
as a result much easier to adapt to hardware. Finally, this paper builds on techniques for hardware accelerated 
point rendering, or splatting [16][10][28]. The advantage of point rendering is that it is a forward 
mapping technique that runs very ef.ciently on graphics pipelines. Current widely available com­modity 
graphics cards can render approximately 5 million points per second [7] and in the near future systems 
may be able to render 50 million or more points per second. The disadvantage of point rendering is that 
artifacts such as gaps may occur during the ren­dering process. However, in lighting design the view 
point is .xed, so no resampling is required. Thus point rendering is a very simple way of building a 
high performance rendering engine. 3 Lighting Design Process Sharon Calahan has written an excellent 
overview chapter on cine­matic lighting design in the context of computer graphics [5]. Her chapter describes 
the goals of lighting and how it contributes to sto­rytelling. She also outlines the process used by 
professional light­ing artists. There are two scenarios in which a lighting artist works. In the .rst 
stage, called master lighting, the artist places lights that pro­vide the background lighting for the 
set and props of the sequence. When working on the master lighting, the artist judges the lighting by 
rendering the static scene from a few camera positions that are characteristic of those used during the 
sequence. The second stage, called shot lighting, occurs when the artist is working on an ani­mated sequence 
with a continuous camera position. Shots tend to last between one to ten seconds. In this stage the lighting 
artists concentrates on lighting the characters and telling the story. A lighting artist typically uses 
a key-frame system in which they choose a few key locations in the animation, creates the lighting they 
desire for these frames, and then renders the entire animated sequence to judge the lighting for the 
entire shot. This means that the lighting artist spends most of their time designing the lighting for 
a single image from a .xed viewpoint. Thus, our design decision to optimize our system for a .xed viewpoint 
matches well existing production practices.  4 Relighting Engine In this section we describe the basic 
operation of our lighting de­sign system and our relighting engine. The system has three major stages: 
The .rst stage converts a shot from a given viewpoint to a deep-framebuffer. During this process the 
rendering system is used to partially evaluate the shading. The second stage is the lighting design 
stage. This stage takes as input the deep-framebuffer and outputs a set of lights. This system is built 
upon a hardware-assisted real-time rendering system so that light sources may be interactively moved 
and their properties changed.  In the third stage the set of lights are added to the scene graph and 
the .nal rendering is performed.  4.1 Deep-Framebuffer The .rst step in our system is the generation 
of the deep­framebuffer. Each pixel in the deep-framebuffer represents a visible surface element and 
contains all the geometric and optical informa­tion needed to perform subsequent lighting calculations. 
 Figure 1: Each sample in the deep-framebuffer stores the world space position, surface normal, tangent, 
and bitangent vectors of the visible surface. The geometric information associated with a sample de.nes 
the local coordinate system used for shading a point on the surface. This information includes the position, 
normal, tangent, and bitan­gent vectors, see Figure 1, all represented in world space coordi­nates. For 
relighting purposes, the only optical information needed is the bi-directional re.ection distribution 
function (BRDF). The BRDF is de.ned to be the ratio of re.ected radiance to the differ­ential incoming 
irradiance and represents the percentage of light energy re.ected in a given outgoing direction for each 
incoming di­rection. Figure 2 shows the important direction vectors used in the computation of BRDFs. 
In our approach, each BRDF F is represented as a linear combi­nation of primitive BRDFs Fi multiplied 
by color re.ectances Ci X F (x, y, wi,wo)= Ci(x, y)Fi(wi,wr,s(x, y)) i In our system, the primitive BRDFs 
include the common diffuse and specular (Phong) models, and less common models such as the Cook-Torrance, 
Oren-Nayar, and a model appropriate for materials such as velvet [17], Specular BRDF models are typically 
parame­terized by a single number, the specularity or roughness, that char­acterizes the size of the 
highlight on the surface. Speci.cally, each deep-framebuffer sample stores an array of n BRDF terms; 
each term includes a color re.ectance, the type of the primitive BRDF, and any associated parameters 
associated with that primitive BRDF. Typically, .nal renderings are done with a production high qual­ity 
rendering system. Thus, it is necessary to adapt the rendering system to output a deep-framebuffer; this 
is usually very straight­forward. Because we have targetted our system for cinematic light­ing, we use 
rendering systems that have programmable shading lan­guages. To generate the deep-framebuffers, surface 
shaders that normally output only a .nal color are modi.ed to output the ge­ometric and optical information 
described above. In the case of H L E B Figure 2: The major vectors required for the evaluation of 
the BRDF. N, L,and E represent the surface normal, the incoming direction from the light source, and 
the outgoing direction to the eye, respectively. H is the halfway vector between L and E,and R is the 
re.ection of L about the surface normal N. PIXAR s RenderMan shading language [26], this simply requires 
inserting print commands in the right places. However, for the ex­amples used in this paper, we have 
used a locally written ray tracer as the .nal rendering system. This ray tracer is similar to Render-Man 
in that it uses a shading language for creating different surface and lighting models, but the shading 
language subsystem has been modi.ed to output the required geometric and optical information directly 
to a .le. This system also uses dynamically loaded libraries of C code to evaluate built-in and programmed 
BRDF models, and this same code may be used by a software-only implementation of the relighting system. 
The surface sample points and BRDFs are sent down the graph­ics pipeline to compute lighting. The vertex 
transformation unit is used to transform the world space location of the point into the shading coordinate 
system and the lighting unit is used to evaluate the product of the point s BRDF and the radiance from 
a single light source. Finally, the texturing and compositing features of the frag­ment processing pipeline 
are used to modulate the re.ected light and blend the result into the framebuffer. 4.2 Re.ection Functions 
In order to accelerate rendering we factor the color re.ectance and BRDF into a form suitable for hardware 
acceleration. Deep-framebuffer systems store parameters as images. In our system this image data is converted 
to texture maps. We store each color re.ectance Ci in a texture map in which the location of the color 
component is the location of its image sample (x, y).When the points are fed into the graphics pipeline, 
the texture coordinates of each point are set to (x, y) and the color looked up and then used to modulate 
the color of the lit point. Heidrich and Seidel recently have shown how a physically­based re.ection 
model may be mapped onto the OpenGL graph­ics pipeline. The key idea is to factor the re.ection model 
into a product of simpler lower-dimensional terms. Although the general BRDF is a function of 4 dimensions, 
each term in the factoriza­tion is typically a function of only one or two dimensions. For ex­ample, 
the Cook-Torrance re.ection model is the product of three terms: a micro-facet distribution function 
D(N ·H), a Fresnel term F (H ·L), and a self-shadowing term S(E·N)S(L·N). The micro­facet distribution 
function and Fresnel term are one-dimensional functions and the self-shading term is a two-dimensional 
function (actually this may be simpli.ed into a product of one-dimensional functions). The advantage 
of this factorization is that these lower­dimensional terms may be stored in texture maps. Building upon 
their work, we map each re.ection function term Fi stored in the deep-framebuffer is mapped to a product 
of simpler factors. The terms themselves are associated with different stages of the graphics pipeline; 
BH is a term computed by the host pro­cessor, BL by the lighting unit, and BT by texture lookup unit. 
YYY Fi =( BHi(.i))· ( BLj(.j))· ( BTk(.k)) ijk Using this representation, a product of the form BH ·BL·BT 
may be computed using a single rendering pass. If multiple BH, BL or BT factors are required, then multiple 
passes are used. Note that in general, BH terms are to be avoided if possible, since they do not use 
the graphics hardware. BL factors are computed using the lighting engine and all that is required is 
that the normal be passed with each point and the light position be set properly. BT factors are the 
most complex. To evaluate these terms requires that the texture coordinates of the point be set to the 
appropriate dot product between two lighting vectors. As a preprocess we must also tabulate the BT factor 
and store it as a texture map. Note that on newer graphics hardware with multitexturing units, multiple 
textures may be applied in a single pass and this may reduce the total number of passes. Our current 
system uses the following factorization which is au­tomatically created from the relighting engine based 
on the deep­framebuffer. It is very easy to add additional factorizations or other terms as new re.ection 
models are developed. As future work, it may be possible to compute such a factorization automatically. 
F =CD · BLD(L · N)+CS · BLS(N · H,s)+ CCT · BTFD(L · H,N · H)· BTG(N · L, N · E)+ (1) CLN · BHLN(L · 
N,k)+CEN BLD and BLS are the standard OpenGL diffuse and specular func­tions. Following the factorization 
of the Cook-Torrance re.ection model used by Heidrich and Siedel [14], BTFD stores the product of the 
Fresnel term F(L · H)and the microfacet distribution func­tion D(N ·H),and BTG stores the geometry term 
G(N ·L, N ·E). Finally, BHLN and CEN compute re.ection components used in the Minneart re.ection model 
[18] (the Minneart model is useful for modeling materials such as velvet [17]). The Minneart model is 
a sum of two terms L · Nk and E · Nl.Since L · Nk cannot be evaluated using the lighting unit, we elected 
to compute this on the host (although we could have also used a texture map). CEN rep­resents the function 
E · Nl; Because the viewpoint is .xed, E · N is constant for each sample and may be precomputed and stored 
in a single texture map. Figure 4 shows the the factorization and the resulting passes used for a typical 
scene. Figure 3 shows the pseudo-code for the multi-pass algorithm. 4.3 Light Sources Our system is 
designed to use light sources such as those described by Barzel [2]. The key feature of these light source 
models is the ex­treme .exibility in creating lighting distributions. The basic source model speci.es 
the directional beam distribution and the intensity fall-off with distance from the source. Then this 
spatial lighting dis­tributing is modulated using a set of attenuators. These include pro­cedurally de.ned 
superelliptical blockers, cookie textures, slide textures, noise textures, and projected shadow mattes. 
To implement such lights in real-time we use projective textur­ing algorithms [23]. To use projective 
textures in lighting, texture coordinate matrix is set to projectively transform the world space position 
of the sample into the light coordinate system. The result­ing texture map look up is used to specify 
the color and intensity of the light or the color and alpha of an attenuator. We model a Barzel light 
as an initial distribution and a product of textures Y L(x,.)=Ls(x,.)( Ai(.)) i /* Fb = Framebuffer */ 
 Fb = CCT; BindBTTextures(FD); Fb = Fb * RenderTextureBRDFPoints(BTFD); BindBTTextures(G); Fb = Fb * 
RenderTextureBRDFPoints(BTG); BindTexture(CD); Fb = Fb + RenderHardwareBRDFPoints(BLD); BindTexture(CS); 
Fb = Fb + RenderHardwareBRDFPoints(BLS); BindTexture(CLN); Fb = Fb + RenderHostBRDFPoints(BHLN); BindTexture(CEN); 
Fb = Fb + RenderNoBRDFPoints(); BindProjectiveTexture(); Fb = Fb * RenderProjectiveImage(); RenderShadowMap(); 
Fb = Fb * RenderShadowImage(); If have texture image of previous lights Fb = Fb + ImageOfPreviousLights; 
Figure 3: Psuedo-code for the multi-pass algorithm that corre­sponds to the pipeline shown in .gure 4. 
Here Ls is the beam distribution from a point source and Ai are color attenuation maps as a function 
of direction. This factorization allows very complex lights to be rendered us­ing multiple passes. However, 
an important optimization is to pre­multiply .xed terms together into a single projected texture map 
so that it may be rendered with a single pass. Another useful mode in the design process is to allow 
only a single term to be modi.ed. For example, the user changes the shape or size of a single blocker 
or moves a single attenuator with respect to the light. This mode requires only two passes, one for the 
static terms and one for the dynamic term. The computation of shadows requires two additional passes. 
The algorithm we use takes advantage of hardware z-buffering, depth comparisons, and the SGI OpenGL shadow 
extension. The .rst pass of the shadow algorithm creates a shadow depth map from the viewpoint of the 
light source. In second pass the points are rendered from the eye point, but also transformed into the 
light source coor­dinate system. If the depth of the point is less than the correspond­ing shadow map 
depth, then the point is in shadow and its alpha value is set to zero, otherwise the alpha value is set 
to one. Other approaches for computing shadows are available if this functional­ity is not supported 
by the graphics hardware, but it is assumed that these features will appear in commodity graphics hardware 
within the next year. Finally, we exploit the linearity of the contribution of the indi­vidual lights. 
This property allows us to generate an image of the combined result of the new light source with the 
previously placed lights by adding the image of the new light to the image of pre­viously computed lighting 
effects. In our system, this is done by storing the previously computed image as a texture and adding 
it to the result of the newly computed image in a .nal pass.  4.4 Performance Optimizations In order 
to minimize the total number of primitives processed by the graphics hardware, we cull points that do 
not contribute to a particular re.ection function. This culling is easily performed by checking the color 
re.ectance map: if the color is black (0), then Rendering Lights Time (s) Speedup Geometry + Texturing 
+ Lighting Texturing + Lighting Lighting (sw) Lighting (sw) Lighting (hw) Lighting (hw) Lighting + Shadows 
(hw) 10 10 10 1 10 1 1 121 51 13 1.3 0.5 0.05 0.25 1:1 2.4:1 9.3:1 93:1 242:1 2420:1 484:1 Table 1: 
Time to render the scene in Figure 4. that point is culled. Additionally, if the properties of the point 
are static (this is always the case unless texture coordinates need to be recomputed), we store points 
in a display list to maximize rendering rate, Finally, we can partition the points by object and only 
render­ing a subset of the points that come from a particular object. This is very useful since it speeds 
up the system and since lights are often de.ned object by object. Another major method used to increase 
performance is to re­order the calculation to maximize coherence. For example, SGI s Performer and many 
other scene graph libraries reorder primitives by texture maps; that is, all the primitives that use 
the same texture map are drawn together. We have found that we can signi.cantly speedup rendering by 
reordering the points by the material proper­ties. In one sense this is done by reordering the shading 
calculation so that all diffuse calculations are done in one pass and all specular calculations are done 
in another pass. As a further optimization, we quantize material parameters such as the specular exponent 
to reduce the number of different materials and to reorder rendering so that all points with the same 
properties are rendered together. Since there is substantial cost to switching material properties, this 
is a signi.cant speedup. In the case of the specular exponent, we found applying a logarithmic transformation 
before quantization is also useful.  5 Results We have implemented a simple lighting design system 
using our rendering engine. A detailed discussion of all the features of this system is beyond the scope 
of this paper. In this section we de­scribe various experiments we did to estimate the performance of 
the system and to validate our design decisions. We created a simple test scene typical of a set (see 
Figure 4). All the shaders are programmed in a shading language very similar to the RenderMan shading 
language; this scene contains 104 objects, 2.2 million micro-triangles, and 13 distinct surface shaders. 
Figure 5 presents timings of various stages. We timed how long it took to render the scene using the 
ray tracer and how long it took to render the scene using our deep-framebuffer. We also compared the 
time it took to render the deep-framebuffer using software vs. hardware. All timings were done using 
a Onyx2 with a 195 MHZ MIPS R10000 and an In.niteReality2 Graphics Subsystem. The .nal image size was 
512 by 256 which represents 131,072 samples; a detailed breakdown of the number of points required per 
pass is shown in .gure 4. The shadow timing includes the generation of a shadowmap from 100,000 polygons 
of geometry. There are seven separate passes in .gure 4, not including shadow computations (the two passes 
of BTFD and BTG are merged to­gether in the diagram). The average number of points rendered per pass 
is 48,210. Some results stand out. First, in this scene approximately 42% of the time is spent in geometry 
processing and 58% in shading. Second, of the shading, 25% of the time is spent evaluating the re.ection 
model and 75% of the time is spent evaluating spatially varying textures that control the re.ection model. 
Third, as ex­pected, there is linear speedup in the number of lights; that is, in a scene consisting 
of ten lights, moving one light is exactly ten times faster than rendering all the lights. Finally, the 
hardware version is 27.5 times faster than the software system. Overall the hardware relighting system 
runs 2000 times faster than the software batch renderer. Moreover, the lighting design system runs at 
roughly 20 Hz, fast enough for interactive use. 6 Summary and Discussion We have described and implemented 
a simple rendering engine for interactive lighting design. The system meets our original goals which 
were to accept scene descriptions with programmable shaders from a production rendering system; and to 
allow a light­ing artist to position the light source and modify all the major light properties in real-time 
without compromising image quality. Figure 5 shows examples of lighting created using our system. Although 
our example set is relatively simple, we believe our system will scale to movie-complexity scenes since 
the rendering time only de­pends on the output complexity of the deep-framebuffer. The shadow computation 
is dependent on the input complexity of the scene, while all other computations are based on the out­put 
complexity of the deep-framebuffer. Therefore, the shadow al­gorithm does not scale as well as the other 
computations with in­creases in scene complexity. We are currently exploring new image and point-based 
methods for shadow approximations. The key to our system is adapting deep-framebuffer technology to modern 
graphics hardware. Over the past several years we have built a series of more and more complex software-only 
rendering engines for lighting design with limited success. There is no silver bullet: interactive relighting 
requires lots of .oating point opera­tions and today s graphics hardware delivers more .ops per dollar 
than conventional CPUs. For example, a 1999 single chip graphics accelerator from NVIDIA is rated by 
the manufacturer at 50 gi­ga.ops and costs tens of dollars. (We are currently in the process of porting 
our system from the SGI IR used in this paper to a PC.) In the future the gap between graphics hardware 
and the main pro­cessor is predicted to be even larger, and so our approach will be even more attractive. 
For example, a hardware system capable of rendering 50 million points per second would allow us to do 
more than 20 passes at 20 Hz on a 512 by 256 image. Finally, graphics hardware vendors are introducing 
new features such as multitextur­ing and texture combiners that support advanced shading and even programmablity. 
With hardware assistance, we believe interactive lighting design will soon be as common as interactive 
3d painting.  Acknowledgements We would like to thank Matt Pharr and Craig Kolb for co­developing the 
technique of separating surface texturing from the lighting calculations. Julie Dorsey for discussions 
on lighting de­sign systems. Pixar s RenderMan Graphics R&#38;D group for years of discussions on production 
lighting and rendering systems. Kekoa Proudfoot and Bill Mark for their discussions on real-time pro­grammable 
shading systems and graphics hardware. Sudeep Ran­gaswamy for creating the chair, table and bookshelf 
models in our test images. Reid would like to thank Sharon Calahan for her guid­ance and teaching of 
cinematography and lighting design during the productions A Bug s Life and Toy Story 2. This research 
was sup­ported by NSF contract number CCR-9508579-001, and DARPA contracts DABT63-96-C-0084-P00002 and 
DABT63-95-C-0085-P00006. References [1] Lightscape visualization system. [2] Ronen Barzel. Lighting 
controls for computer cinematography. Journal of Graphics Tools, 2(1):1 20, 1997. [3] J. F. Blinn. Raster 
graphics. In K. Booth, editor, Tutorial: Computer Graphics. IEEE Computer Society, 1979. [4] Normand 
Briere and Pierre Poulin. Hierarchical view-dependent structures for interactive scene manipulation. 
In Computer Graphics Annual Conference Se­ries 1996, pages 83 90. Siggraph, August 1996. [5] Sharon Calahan. 
Storytelling through lighting, a computer perspective. In Tony Apodaca and Larry Gritz, editors, Advanced 
Renderman: Creating CGI for Motion Pictures, pages 337 382. Morgan Kaufman Publishers, San Francisco, 
1999. [6] Robert L. Cook. Shade trees. In Hank Christiansen, editor, Computer Graphics (SIGGRAPH 84 Proceedings), 
volume 18, pages 223 231, July 1984. [7] NVIDIA Corporation. GeForce 256 Graphics Accelerator Speci.cations,De­cember 
1999. [8] J. Dorsey, J. Arvo, and D. Greenberg. Interactive design of complex time­dependent lighting. 
IEEE Computer Graphics and Applications, 15(2):26 36, March 1995. [9] Julie O B. Dorsey, Francois X. 
Sillion, and Donald P. Greenberg. Design and simulation of opera lighting and projection effects. Computer 
Graphics, 25(4):41 50, July 1991. [10] J.P. Grossman and Willian J. Dally. Point sample rendering. In 
George Dret­takis and Nelson Max, editors, Eurographics Rendering Workshop, Eurograph­ics, pages 181 
192, 1998. [11] Brian Guenter, Todd B. Knoblock, and Erik Ruf. Specializing shaders. In Com­puter Graphics 
Annual Conference Series 1995, pages 343 350. Siggraph, Au­gust 1995. [12] Pat Hanrahan and Jim Lawson. 
A language for shading and lighting calculations. In Forest Baskett, editor, Computer Graphics (SIGGRAPH 
90 Proceedings), volume 24, pages 289 298, August 1990. [13] Paul S. Heckbert. Techniques for real-time 
frame buffer animation. In Computer FX 84 Conference, October 1984. [14] Wolfgang Heidrich and Hans-Peter 
Seidel. Realistic, hardware-accelerated shad­ing and lighting. In Alyn Rockwood, editor, Computer Graphics 
(SIGGRAPH 99 Proceedings), pages 171 178, August 1999. [15] John K. Kawai, James S. Painter, and Michael 
F. Cohen. Radioptimization -goal based rendering. In Computer Graphics Annual Conference Series 1993, 
pages 147 154. Siggraph, August 1993. [16] Marc Levoy and Turner Whitted. The use of points as a display 
primitive. Tech­nical Report 85-022, UNC-Chapel Hill Computer Science, 1985. [17] Rong Lu, Jan J. Koenderink, 
and Astrid M. Kappers. Optical properties (bidirec­tional re.ection distribution functions) of velvet. 
In Applied Optics, volume 37, pages 5974 5984, 1998. [18] M. Minnaert. The reciprocity principle in lunar 
photometry. In Astrophys. J., volume 93, pages 403 410, 1941. [19] Ken Perlin. An image synthesizer. 
Computer Graphics, 19(3):287 296, July 1985. [20] Pierre Poulin and A. Fournier. Lights from highlights 
and shadows. In 1992 Symposium on Interactive 3D Graphics, pages 31 38, March 1992. [21] Takafumi Saito 
and Tokiichiro Takahashi. Comprehensible rendering of 3-d shapes. In Computer Graphics Annual Conference 
Series 1990, pages 197 206, August 1990. [22] Chris Schoeneman, Julie Dorsey, Brian Smits, James Arvo, 
and Donald Green­berg. Painting with light. In Computer Graphics Annual Conference Series 1993, pages 
143 146. Siggraph, August 1993. [23] Mark Segal, Carl Korobkin, Rolf van Widenfelt, Jim Foran, and Paul 
Haeberli. Fast shadows and lighting effects using texture mapping. In Computer Graphics Annual Conference 
Series 1992, pages 249 252, July 1992. [24] Carlo H. Sequin and Eliot K. Smyrl. Parameterized ray tracing. 
In Computer Graphics Annual Conference Series 1989, pages 307 314. Siggraph, July 1989. [25] Kenneth 
R. Sloan and Christopher M. Brown. Color map techniques. Computer Graphics and Image Processing, 10:297 
317, 1979. [26] Steve Upstill. The RenderMan Companion. Addison Wesley, 1992. [27] Gregory J. Ward. The 
radiance lighting simulation and rendering system. [28] Lee Westover. Footprint evaluation for volume 
rendering. Computer Graphics, 24(4):367 376, August 1990.  = * BTFD (L.H,N.H) . BTG(L.N,E.N) CFD.BTFD 
(L.H,N.H) . BTG(L.N,E.N) CCT  + = * CD BLD(L.N) CD.BLD(L.N) +  * = CS BLS(N.H,s) CS.BLS(N.H,s) + 
  * = BHLN(L.N,k) CLN.BHLN(L.N,k) CLN + = CEN CEN *  Projected Texture Image Of Projection *  Shadowmap 
Image of Shadows = Final Image Figure 4: This image shows the stages of the multi-pass rendering algorithm. 
The number of points rendered per row are: row 1 = 17,302 points, row 2 = 129,604 points, row 3 = 39,006 
points, row 4 = 10,246 points, row 5 = 10,246 points, row 6 (projective texture) = 131,072 points, row 
7 (shadow map) = 131,072. Figure 5: This is an example of lighting designed using our system. It took 
2 hours to create, contains 60 lights (mostly to simulate inter­re.ection effects), and de.nes an early 
morning mood.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344947</article_id>
		<sort_key>359</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>40</seq_no>
		<title><![CDATA[Relief texture mapping]]></title>
		<page_from>359</page_from>
		<page_to>368</page_to>
		<doi_number>10.1145/344779.344947</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344947</url>
		<abstract>
			<par><![CDATA[<p>We present an extension to texture mapping that supports the representation of 3-D surface details and view motion parallax. The results are correct for viewpoints that are static or moving, far away or nearby. Our approach is very simple: a <italic>relief texture</italic> (texture extended with an orthogonal displacement per texel) is mapped onto a polygon using a two-step process: First, it is converted into an ordinary texture using a surprisingly simple 1-D forward transform. The resulting texture is then mapped onto the polygon using standard texture mapping. The 1-D warping functions work in texture coordinates to handle the parallax and visibility changes that result from the 3-D shape of the displacement surface. The subsequent texture-mapping operation handles the transformation from texture to screen coordinates.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[image-based rendering]]></kw>
			<kw><![CDATA[range images]]></kw>
			<kw><![CDATA[rendering]]></kw>
			<kw><![CDATA[texture mapping]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Range data</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010243</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Appearance and texture representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP36042247</person_id>
				<author_profile_id><![CDATA[81100516478]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Manuel]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Oliveira]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, SUNY at Stony Brook, Stony Brook, NY and University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14081142</person_id>
				<author_profile_id><![CDATA[81100206034]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gary]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bishop]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UNC Department of Computer Science, CB #3175, Sitterson Hall, Chapel Hill, NC and University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39028695</person_id>
				<author_profile_id><![CDATA[81100125718]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McAllister]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UNC Department of Computer Science, CB #3175, Sitterson Hall, Chapel Hill, NC and University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>907242</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Catmull, E. A Subdivision Algorithm for Computer Display of Curved Surfaces. Ph.D. Dissertation, Department of Computer Science, University of Utah, December 1974.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807505</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Catmull, E., Smith, A. 3D Transformations of Images in Scanline Order. Proc. SIGGRAPH 80 (Seattle, Washington, July 14-18, 1980), pp. 279-285.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280832</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Cohen, J., Olano, M., Manocha, D. Appearance-Preserving Simplification. Proc. SIGGRAPIt 98 (Orlando, FL, July 19- 24, 1998), pp. 115-122.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237191</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Debevec, P., Taylor, C., Malik, J. Modeling and Rendering Architecture from Photographs: A hybrid geometry- and image-based approach. Proc. SIGGRAPIt 96 (New Orleans, LA, August 4-9, 1996), pp. 11-20.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>13056</ref_obj_id>
				<ref_obj_pid>13050</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Fant, Karl. A Nonaliasing, Real-Time Spatial Transform Technique. IEEE CG&amp;A, Vol. 6, No 1, January 1986, pp. 71-80.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Fournier, A. Normal Distribution Functions and Multiple Surfaces. Graphics Interface '92 Workshop on Local Illumination. pp. 45-52.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237200</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Gortler, S., et al. The Lumigraph. Proc. SIGGRAPIt 96 (New Orleans, LA, August 4-9, 1996), pp. 43-54.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Grossman, J., Dally, W. Point Sample Rendering. Proceedings of the 9th Eurographics Workshop on Rendering. Vienna, Austria, June 1998. Rendering Techniques '98, Springer-Verlag, pp. 181-192.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237199</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Levoy, M., Hanrahan, P. Light Field Rendering Proc. SIGGRAPIt 96 (New Orleans, LA, August 4-9, 1996), pp. 31-42.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>317262</ref_obj_id>
				<ref_obj_pid>317259</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Max, N. A One-Pass Version of Two-Pass Image Resampling. Journal of Graphics Tools, Vol. 3, No. 1, pp. 33-41.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>269042</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[McMillan, L. An Image-Based Approach to Three- Dimensional Computer Graphics. Ph.D. Dissertation. UNC Computer Science Technical Report TR97-013, April 1997.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>300552</ref_obj_id>
				<ref_obj_pid>300523</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Oliveira, M., Bishop, G. Image-Based Objects. Proceedings of 1999 ACM Symposium on Interactive 3D Graphics. pp. 191-198.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>897903</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Oliveira, M. Relief Texture Mapping. Ph.D. Dissertation. UNC Computer Science Technical Report TR00-009. March 2000. http://www.cs.unc.edu/"dbr/pubs/oliveira-diss/TROO- 009.pdf.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Robertson, P. Fast Perspective Views of Images Using One- Dimensional Operations. IEEE CG&amp;A, vol. 7, pp. 47-56, Feb. 1987.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Sawhney, H. 3D Geometry from Planar Parallax. In IEEE CVPR'94, pages 929-934. IEEE Computer Society, Seattle, Washington, June 1994.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Schaufler, G. Per-Object Image Warping with Layered Impostors. Proceedings of the 9th Eurographics Workshop on Rendering. Vienna, Austria, June 1998. Rendering Techniques '98, Springer-Verlag, pp. 145-156.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280882</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Shade, J., et al. Layered Depth Images. Proc. SIGGRAPH 98 (Orlando, FL, July 19-24, 1998), pp. 231-242.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37433</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Smith, Alvy Ray. Planar 2-Pass Texture Mapping and Warping. Proc. SIGGRAPH 87 (Anaheim, CA, July 27-31, 1987), pp. 263-272.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801126</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Williams, L. Pyramidal Parametrics. Proc. SIGGRAPH 83 (Detroit, MI, July 25-29, 1983), pp. 1-11.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74371</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Wolberg, George. Separable Image Warping with Spatial Lool~p Tables. Proc. SIGGRAPH 89 (Boston, MA, July 31- 4 August, 1989), pp. 369-378.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>248518</ref_obj_id>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Woo, M., et al. OpenGL Programming Guide. 2nd edition. Addison Wesley, 1997.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Relief Texture Mapping Manuel M. Oliveira Gary Bishop David McAllister University of North Carolina 
at Chapel Hill   ABSTRACT We present an extension to texture mapping that supports the representation 
of 3-D surface details and view motion parallax. The results are correct for viewpoints that are static 
or moving, far away or nearby. Our approach is very simple: a relief texture (texture extended with an 
orthogonal displacement per texel) is mapped onto a polygon using a two-step process: First, it is converted 
into an ordinary texture using a surprisingly simple 1-D forward transform. The resulting texture is 
then mapped onto the polygon using standard texture mapping. The 1-D warping functions work in texture 
coordinates to handle the parallax and visibility changes that result from the 3-D shape of the displacement 
surface. The subsequent texture-mapping operation handles the transformation from texture to screen coordinates. 
CR Categories and Subject Descriptors: I.3.3 [Computer Graphics]: Picture/Image Generation I.3.6 [Computer 
Graphics]: Methodologies and Techniques; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism. 
Now at the Computer Science Department SUNY at Stony Brook, Stony Brook, NY, 11794-4400 oliveira@cs.sunysb.edu 
http://www.cs.sunysb.edu/~oliveira UNC Department of Computer Science CB #3175, Sitterson Hall, Chapel 
Hill, NC, 27599-3175 {bishop | davemc}@cs.unc.edu    http://www.cs.unc.edu/~ibr Permission to make 
digital or hard copies of part or all of this work or personal or classroom use is granted without fee 
provided that copies are not made or distributed for profit or commercial advantage and that copies bear 
this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. SIGGRAPH 2000, New Orleans, 
LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 Additional Keywords: Image-Based Rendering, 
Texture Mapping, Range Images, Rendering. 1. INTRODUCTION Texture mapping has long been used to enhance 
the realism of computer-generated images by adding 2-D details to object surfaces [1]. For instance, 
it can be used to correctly simulate a picture on a wall, or the label on a can. Unfortunately, texture 
mapping is not as effective for adding 3-D details to a surface. When seen by a moving observer, the 
absence of parallax reveals the flatness of the surface. Such flatness is also evidenced when the surface 
is observed from an oblique angle (Figure 1). A much more convincing illusion of 3-D surface detail can 
be achieved by using a height field in conjunction with a texture map. A height field is a scalar field 
of distances between surface points and their orthogonal projections onto a plane that forms its algebraic 
basis. Unfortunately, rendering height fields is much more difficult than texture mapping. The planar-projective 
transform of texture mapping has a very convenient inverse formulation. This allows direct computation 
of texture coordinates from screen coordinates, thus allowing efficient implementation as well as accurate 
resampling and filtering. Height-field rendering allows no such inverse formulation directly. Multiple 
samples from the height field may be mapped to the same pixel in the final image. Assuring correct visibility 
requires either a search for the closest surfaces (essentially a ray-tracing strategy) or a direct forward 
mapping [11]. We present an extension to texture mapping for representing three-dimensional surface details 
and view motion parallax. This new approach, called relief texture mapping, results from a factorization 
of the 3-D image-warping equation of McMillan and Bishop into a pre-warp followed by standard texture 
mapping. The pre-warp is applied to images with per-texel displacements and handles only the parallax 
effects resulting from the direction of view and the displacements of texture elements; the subsequent 
texture-mapping operation handles scaling, rotation, and the remaining perspective transformation. The 
pre-warping equations have a very simple 1-D structure that enables the pre-warp to be implemented using 
only 1-D image operations along rows and columns and requires interpolation between only two adjacent 
texels at a time. This allows efficient implementation in software and should allow a simple and efficient 
hardware implementation. The texture-mapping hardware already very common in graphics systems efficiently 
implements the final texture mapping stage of the warp. In recent years, image-based modeling and rendering 
(IBMR) techniques have gained considerable attention in the graphics community because of their potential 
to create very realistic images. We hope to help to bridge the gap between IBMR and conventional polygonal 
rendering techniques by casting a subset of IBMR as an extension of texture mapping. Such a hybrid system 
can offer much of the photo-realistic promise of IBMR while retaining the advantages of polygonal rendering. 
In section 4.4, we present an example of a real environment modeled and rendered using relief texture 
mapping. We demonstrate a software implementation of our method and show that it significantly increases 
the expressive power of conventional texture mapping. Our approach also dramatically reduces the polygonal 
count required to model a scene, while preserving its realistic look. Figure 2 shows the use of our approach 
for the same viewpoint used to create Figure 1. The two scenes used to render these images were modeled 
using the same number of polygons. In the example of Figure 2, each façade and brick wall is represented 
with a single relief texture. Notice the bricks standing out of the wall and the protruding dormers. 
In the original model of the town, each house consists of a few thousand polygons, whereas the corresponding 
relief texture representation uses only seven polygons per house. The new results presented in this paper 
are: An extension to texture mapping that supports view motion parallax (Section 3);  An exact factorization 
of the 3-D image warping equation [11] into a 1-D pre-warp followed by a planar projective mapping (Section 
3.1);  After rotations are factored out, 3-D warps reduce to a 2-D problem, regardless of the coordinate 
systems associated with the source and target images (section 3.1); and  A 1-D image reconstruction 
algorithm that handles an arbitrary number of self-occlusions without requiring extra storage or depth 
comparison (section 3.3.2).   2. RELATED WORK 3-D Image Warping. Three-dimensional image warping [11] 
is a geometric transformation that maps a source image with depth is into a target image it . The geometric 
content of the scene is represented implicitly by combining depth information with a camera model associated 
with the source image. Thus, let x be a point in Euclidean space whose projection on the image plane 
of i has coordinates (u , v ) . The projection of x into an arbitrary s ss target image plane, xX t 
, is given by: -1 - CC x = P Px + P 1( - )d (u , v ) C (1) t tss t s tsss where =x is projective equivalence, 
i.e., the same except for a scalar multiple, Pk is a 3x3 camera matrix associated with image ik , xs 
=[us vs 1]T , C k is the center of projection (COP) of the pinhole camera associated with image ik and 
ds (us , vs ) is the generalized disparity of source pixel (us , vs ) [11]. Equation (1) shows that the 
target image can be obtained by applying a planar perspective transformation to the source image followed 
by a per-pixel shift proportional to ds (us , vs ) in the direction of the epipole1 of the target image. 
Such a factorization is often referred to as plane-plus-parallax in the computer vision literature [15]. 
Texture mapping is a special case of 3-D image warping for which all pixels of the source image share 
a single disparity value [11]. This fact will be exploited in section 3.1 for the derivation of the pre-warping 
equations. Sprites with Depth. Sprites with depth [17] enhance the descriptive power of traditional sprites 
with out-of-plane displacements per pixel. Such a technique is based on the plane­plus-parallax factorization 
[15] mentioned before. In a first step, the displacement information associated with the source image 
is forward mapped using a 2-D transformation to compute an intermediate displacement map. In the second 
pass, each pixel of the desired image is transformed by a homography (planar perspective projection) 
and the resulting coordinates are used to index the displacement map computed in the first pass. The 
retrieved displacement value is then multiplied by the epipole of the target image and added to the result 
of the homography. These new coordinates are used to index the color of the desired pixel. Although such 
an approach may sound similar to ours at first, it differs in some fundamental aspects. Sprites with 
Depth are an approximation to the 3-D image warping process. Our method, on the other hand, is based 
on an exact factorization of the 3-D image warping equation [11], takes advantage of texture mapping 
hardware, uses an efficient image reconstruction strategy and naturally integrates itself with popular 
graphics APIs such as OpenGL [21]. View-dependent Texture Mapping. New views of a scene can be rendered 
by compositing multiple textures based on the observer s viewpoint, which are then mapped onto a polygonal 
model. In [4], a model-based stereo algorithm is used to compute depth maps from pairs of images. Once 
a depth map associated with a particular image has been computed, new views of the scene can be rendered 
using several image-based rendering techniques. 1-D Perspective Projection. Robertson [14] showed how 
hidden­point removal and perspective projection of height images could be performed on scanlines or columns. 
This approach explores the separability of perspective projection into orthogonal components. First, 
the image is rotated to align its lower edge with the lower edge of the viewing window. Then, a horizontal 
compression is applied to each scanline so that all points that may potentially occlude each other 1 
The projection of one camera s center of projection into the image plane of another camera. fall along 
the same column. 1-D vertical perspective projection is applied to the columns of the intermediate image 
in back-to-front order, thus performing hidden-point removal. Finally, 1-D horizontal perspective projection 
is applied to the resulting image, incorporating compensation for the compression applied in the second 
step [14]. Serial Warps. Image operations such as texture mapping and image warping involve transformations 
among pairs of coordinates. Catmull and Smith [2] showed how affine and perspective transformations applied 
to planar surfaces and to bilinear and biquadratic patches can be decomposed into a series of 1-D operations 
over rows and columns. Later, Smith [18] showed that texture mapping onto planar quadric and superquadric 
surfaces, and planar bicubic and biquadratic image warps are two-pass transformable. Serial warps suffer 
from a problem commonly referred to as bottleneck, the collapse of the intermediate image into an area 
much smaller than the final image [2]. Non-injective 2-D mapping may also map multiple samples to the 
same pixel on the screen, a situation known as foldover [2]. The major sources of bottlenecks are image 
rotations and perspective distortions [20]. In combination with rotations, perspective distortions can 
cause the intermediate image to twist, leading to loss of information and introducing severe artifacts 
in the final image [13].  3. RELIEF TEXTURE MAPPING A relief texture is a texture extended with orthogonal 
displacements per texel, and has some interesting properties. For instance, when the viewer is far away 
from the represented surface, it can be rendered as a regular texture. As the viewer approaches the surface, 
the relief texture can be warped before being mapped onto a polygon and, when the viewer is extremely 
close, the relief texture can be rendered as a mesh of micro-polygons. Image-based rendering techniques 
can generate very realistic views by warping images and Equation (1) concisely describes the warping 
process. Ideally, from a conventional rendering point-of­view, the mapping expressed by Equation (1) 
should be factored so to allow conventional texture mapping to be applied after the shift in the direction 
of the epipole. Such an approach is the opposite of the conventional plane-plus-parallax decomposition, 
in the sense that shifts take place prior to the homography (Figure 3), and presents several advantages. 
First, it can benefit from the texture mapping hardware in graphics systems to perform the final transformation 
and filtering. Secondly, the warp can be implemented using 1-D image operations along rows and columns, 
requiring interpolation between only two adjacent texels at a time. This property greatly simplifies 
the tasks of reconstruction and filtering of the intermediate image [5] and should allow a simple and 
efficient hardware implementation. Thirdly, the approach naturally integrates itself with popular graphics 
APIs such as OpenGL [21]. During the warp, texels move only horizontally and vertically in texture space 
by amounts that depend on their orthogonal displacements and on the viewing configuration. The warp Texture 
Pre-warping mapping Final view Figure 3. Relief texture mapping: pre-warping followed by standard texture 
mapping. implements no rotations (which are subsequently performed as part of the conventional texture 
mapping operation) and the resulting serial warps do not suffer from bottlenecks or from image twists. 
Figure 3 shows a flowchart for the relief texture­mapping algorithm resulting from such an ideal factorization. 
Section 3.1 explains how such a factorization is obtained. 3.1 Pre-Warping Equations In order to obtain 
the ideal factorization, one needs to find a pre-warp p so that the composition m o p , where m is a 
standard texture­mapping transformation, is equivalent to the 3-D image warp w . Thus, let (u,v ) = (u 
+.u, v +.v) be the intermediate ii ss coordinates obtained after shifting source pixel (us , vs ) by(.u, 
.v). The equivalence between the composed mapping m o p and w can be expressed as: What coordinates (u, 
v ) should the source pixels (u, v ) have so i i ss that a view of such a flat distorted image on the 
source image plane from the target COP would be identical to a 3-D image warp of the source image onto 
the target image plane? While perspective projection images with depth can be used as source images for 
such a purpose, the use of parallel projection images with depth presents some advantages. For instance, 
they have constant sampling density across the entire image. Also, the perpendicular relationship between 
sampling rays and image plane can be exploited to produce a simple and efficient rendering algorithm 
(Section 4.1). Given a parallel projection camera model (Figure 4), the coordinates of a point x in Euclidean 
space are given by: abf u . sisi si .. s . . ... x =C + abf v = C + P' x' s . sjsj sj .. s . s ss .ab 
f ..displ(u ,v ). . sksk sk .. ss . where vectors a as and bs form a basis for the plane of the source 
image. The lengths of these vectors are the horizontal and vertical sample spacing in the Euclidean space, 
respectively. fs is a unit vector perpendicular to the plane defined by a as and bs , C s is the origin 
of the source image plane, and displ(us , vs) is the orthogonal displacement, or height, associated with 
source pixel (us , vs ) . The reprojected coordinates of x into a target perspective projection camera 
(Figure 5) are given by2 Au + Bv + D + C 'displ(u , v ) ss ss u = (2a) t Iu + Jv + L + K 'displ(u , v 
) ss ss Eus + Fvs + H + G'displ(us ,vs ) v = (2b) t Iu + Jv + L + K 'displ(u ,v ) ss ss h hh hh h where 
A = a ·(b ×c ), B = b ·(b ×c ), C'= f · (b × c ), stt stt stt X XXX aa CC D = ( s - t ) · (b X t × ct 
), E = as · (ct × at ), F = b a s · (ct × at ), aa '= CC G fs · (ct × at ), H = ( s - t ) · (ct × at 
), I= as · (at × bt ),  CC J= bs · (at × bt ), K'= fs · (at × bt ), L= ( s - t ) · (at × bt ) 2 We 
preserved the original notation used in [13] for easy reference by interested readers. fs a . aCs sbs 
 Figure 4. Parallel projection camera model. and ca t is a vector from the target COP to the origin of 
the target image plane (Figure 5). The corresponding texture mapping expressions are obtained from Equations 
(2a) and (2b) by letting displ(us , vs) = 0 for all source pixels. Thus, the problem of finding the desired 
warp can be modeled as Au +Bv + D Au + Bv + D + C'displ(u ,v ) iiss ss =  (3a) Iu + Jv + L Iu + Jv 
+ L + K 'displ(u ,v ) iiss ss Eu + Fv + H Eu + Fv + H + G'displ(u , v ) iiss ss = . (3b) Iu + Jv + L 
Iu + Jv + L + K 'displ(u , v ) iiss ss The pre-warp associated with the ideal factorization is then obtained 
by solving the system above for ui and vi : u + k displ(u ,v ) s 1 ss ui = (4a) 1+ k displ(u ,v ) 3 ss 
v + k displ(u ,v ) s 2 ss vi = (4b) 1+ k displ(u ,v ) 3 ss where k1, k2 and k3 are constants for the 
given configuration of source and target cameras and, together with displ(us , vs ), determine the amount 
of change (.u,.v) in the coordinates of the source texels. A formal proof of the 1-D nature of the pre­warping 
equations can be found in [13]. Such a factorization proves to have many desirable properties. In particular, 
the coordinates of a pixel in the intermediate image can be computed independently from each other, i.e., 
ui does not depend on v and does not depend on u . Also, when svis displ(u , v ) = 0 no computation is 
required. sss The evaluation of Equations (4a) and (4b) can be reduced to two additions, two multiplications 
and three lookup operations by quantizing the displacement values (in a pre-processing step) and storing 
the reciprocal of the denominator of Equation (4a) and the expressions k displ(u, v ) and k displ(u, 
v ) in lookup tables. 1 ss 2 ss We have used a uniform quantization scheme in which a quantized displacement 
is recovered as displ'= min+ qi * qs , where min is the minimum displacement value, qs = (max- min) / 
254 is the quantization step and aa t bt ct . Ct Figure 5. Pinhole camera model [11]. qi =int((displ(us 
, vs ) - min) / qs) is the quantization index. The indices were stored in the alpha channel of the relief 
texture with one value reserved for transparency. In practice, this uniform quantization scheme works 
very well and the results are virtually indistinguishable from those obtained with the actual displacement 
values. Moreover, such a strategy reduces the storage requirements of relief textures to essentially 
the same as conventional textures (the values of min and qs need to be saved) and it also helps to improve 
cache coherence, since the displacement and color data associated with a texel are always used together. 
This scheme uses the alpha channel of source textures only and the alpha channel of the pre-warped textures 
can still be used for antialiasing and transparency. The Coefficients of the Pre-Warping Equations. The 
amount of shift (.u, .v) to be applied to a source texel does not depend on the parameters of the target 
camera except for its COP [13]. Therefore, one can freely specify the parameters a , b and a attct which 
define a temporary target camera used only for the purpose of the pre-warp and which usually differs 
from the virtual camera used for the visualization of the final scene. By appropriately choosing such 
parameters, it is possible to eliminate several of the coefficients in Equations (3a) and (3b) by forcing 
the aa a corresponding scalar triple products to have the form v · (v × w) or aa a w · (v × w) . Such 
a procedure leads to a drastic simplification of the expressions used to compute coefficients k1, k2 
and k3 . For aa . instance, the condition a =aa , b =ßb and c =. (C - C ), t sts t st for nonzero a, 
ß ,. .R , eliminates coefficients B, D, E, H, I and J and is trivially satisfied by letting source and 
target image planes coincide, including their origins and basis vectors (Figure 6). The subscripts of 
all vectors can then be dropped without risk of confusion and the coefficients of Equations (4a) and 
(4b) become f · (b × c) f · (c × a)1 k = , k = and k = a . a · (b × c) a · (b × c) c · f 12 3 a f a C 
C s a b c x Ct Figure 6. Parallel and perspective projection cameras sharing the same image plane (origin, 
a and b a vectors). Occlusion-Compatible Ordering. The COP of a parallel projection image is at infinity. 
Its epipole is the projection of the other camera s COP onto the plane of the parallel projection image. 
By similarity of triangles, whenever two samples fall along the same viewing ray, the one whose projection 
is closer to the epipole is also closer to the viewer (Figure 7). Thus, an occlusion­compatible order 
[11] (essentially a painter s algorithm) for parallel projection images with depth is obtained by warping 
pixels from the borders towards the epipole. warping target image plane  B epipole Figure 7. Triangles 
ABC , A'B'Cand A ' B ' C are similar. Similarity of triangles guarantees that occlusion compatible order 
is achieved by warping from the borders towards the epipole. 3.2 Reconstructing Pre-warped Textures 
Section 3.1 has shown how to determine the coordinates of infinitesimal points in the intermediate image 
from points in the source image. Determining these is only the beginning of the image-warping process. 
The more expensive step is reconstruction and resampling onto the pixel grid of the intermediate image. 
The simplest and most common approaches to reconstruction and resampling are splatting and meshing. Splatting 
requires spreading each input pixel over several output pixels to assure full coverage and proper interpolation. 
Meshing requires rasterizing a quadrilateral for each pixel in the NxN input texture. The special structure 
of our pre-warp equations allows us to implement reconstruction and resampling as a two-pass process 
using 1-D transforms along rows and columns [2]. The reader should make a clear distinction between the 
two steps of our method: pre-warping followed by texture mapping, and the two phases used to implement 
the pre-warping step itself. Such phases consist of a horizontal pass and a vertical pass. 3.2.1 Two-pass 
Reconstruction Assuming that the horizontal pass takes place first, the steps of a two­pass reconstruction 
algorithm are illustrated in Figure 8. Figure 8(a) shows two source texels A and B and their positions 
after the pre-warp (outlined circles). The first texel of each row is moved to its final column (Figure 
8(b)) and, as the subsequent texels are warped, color and final row coordinates are interpolated during 
rasterization (Figure 8(c)). Fractional coordinate values (for both rows and columns) are used for filtering 
purposes in a similar way as described in [5]. Notice that adjacent texels are usually warped to adjacent 
positions and the situation shown in Figure 4-3(c) is used to stress the interpolation scheme. The warp 
may, however, map adjacent texels to relatively distant positions if such texels are at different sides 
of a depth discontinuity. Let texel C be right above texel B after all rows have been warped (Figure 
8(d)). During the vertical pass, texels are moved to their final row coordinates (Figure 8(e)) and colors 
are interpolated (Figure 8(f)). Figure 9 illustrates the stages of the two-pass warp and reconstruction 
for the case of a building façade. Figure 9(a) shows a source relief texture. Figures 9(b) and 9(c) present 
the results of the horizontal and vertical passes, respectively. The final view of the texture-mapped 
polygon, whose borders are shown in red, is presented in Figure 9(d). A pseudocode for a two-pass reconstruction 
algorithm that disregards filtering issues for simplicity is presented in Figure 10. 4.0 4.33 4.66 5.0 
 A B A B  A     A B   (a) (b) (c) (d) (e) (f)  C A B    Figure 8. Warping of one texel. 
(a) Source texels A and B and their final positions after the warp. (b) The first texel of the current 
row is moved to its final column. (c) Next texel is moved to its final column and color and final row 
coordinates are interpolated during rasterization. (d) After all rows have been warped, texel C is adjacent 
to texel B. (e) Along each column, texels are moved to their final rows. (f) Color is interpolated during 
rasterization. There are advantages in computing both coordinates of pre­warped texels in the first step 
of the algorithm. For instance, it avoids nonlinear distortions in the final image that would otherwise 
be introduced if row coordinates were computed during the second pass using interpolated displacement 
values [13]. We have compared the results produced by this algorithm with the results of rendering relief 
textures as meshes of micropolygons. The results are essentially the same in most cases. Improper color 
interpolation may happen across depth discontinuities, where no information about the surface is available, 
and are the major source of artifacts in images produced by two-pass 1-D Horizontal Pass  (a) (b)Vertical 
 Texture Mapping  (d) (c) Figure 9. Stages of the relief texture-mapping algorithm. (a) Source relief 
texture. (b) Image produced by the horizontal pass. (c) Pre-warped texture obtained after the vertical 
pass. (d) Final view, showing the borders of the texture-mapped polygon. reconstruction strategies. In 
practice, however, depth discontinuities are frequently associated with either smooth color changes or 
sharp color transitions matching the discontinuities. In both cases, the results produced by the 1-D 
approach are similar to the ones obtained with a 2-D warp followed by the rasterization of 2-D micropolygons. 
The examples shown in this paper and the accompanying animations were rendered with the two-pass reconstruction 
algorithm described. get Uin, Vin, Cin, Din Unext = Equation_5a(Uin,Din) Vnext = Equation_5b(Vin,Din) 
for (Uout = integer(Uprev+1); Uout = Unext; Uout++) linearly interpolate Cout between Cprev and Cin linearly 
interpolate Vout between Vprev and Vin put Cout, Vout at Uout Uprev=Unext; Vprev=Vnext; Cprev=Cin Figure 
10. Pseudocode for a first-pass left-to-right horizontal warp and resampling of one texel with coordinates 
(U, V), color C and displacement D. No antialiasing computed for simplicity.  3.2.2 Pipelined Reconstruction 
The overwriting of texels during the first pass may cause self­occlusions. Although bottlenecks [2] are 
not an issue during the pre-warp step and, in practice, self-occlusions seem not to introduce noticeable 
artifacts in the pre-warped textures, we present a solution that is capable of handling an arbitrary 
number of foldovers and that does not require depth comparison. It consists of interspersing the horizontal 
and vertical warps and is related to the work described in [10]. As before, assume the horizontal pass 
is completed first and the rows are processed in occlusion-compatible order. As the horizontal warp produces 
each intermediate texel, this is immediately rasterized into the appropriate column. Since each vertical 
warp receives and processes its texels in occlusion-compatible order, correct visibility is preserved 
in the output. Also, because each texel is processed immediately after its generation, no information 
is overwritten and self-occlusions are avoided. The steps of the algorithm are illustrated in Figure 
11, where gray circles represent the texels previously warped to the corresponding columns.   4. MODELING 
Relief textures can be used as modeling primitives by simply instantiating them in a scene in such a 
way that the reprojected surfaces match the surfaces of the objects to be modeled. During the pre-warp, 
however, samples may have their coordinates mapped beyond the limits of the original texture. This corresponds, 
in the final image, to have samples projecting outside the limits of the polygon to be texture­mapped 
(Figure 12 (left)). The occurrence of such situations depends on the viewpoint and on the size of the 
displacements. This is similar to what happens when a light field [9] consisting of a single light slab 
is viewed from oblique angles.  The problem of incomplete views can be overcome if extra perpendicular 
polygons are texture-mapped with the outliers. This situation is illustrated in Figure 12 (center). The 
final result, rendered with an additional sidewall (pentagon), is shown in Figure 12 (right). The details 
of the technique will be explained next, in the context of the more general problem of rendering three-dimensional 
objects from arbitrary viewpoints. 4.1 Object Representation Several researchers have used image-based 
techniques to represent objects [9] [7] [8] [16] [12]. Relief texture mapping can also be used to render 
complex three-dimensional shapes. Figure 13 shows a relief texture representation of an object originally 
modeled with 35,280 polygons. It consists of six relief textures acquired from the faces of the object 
s bounding box. New views of the object can be obtained by pre-warping these textures and mapping the 
resulting images onto the faces of the box. But just warping each relief texture to its original face 
of the box is not enough to produce the desired result. Some samples may project onto other faces, depending 
on the viewpoint (Figure 14). One solution to this problem is to pre-warp adjacent faces to the desired 
ones. The perpendicular orientation between faces allows such mappings to be performed using the same 
pre-warping equations (Equations (4a) and (4b)). The concept will be explained in 2-D. Its generalization 
to 3-D is straightforward. Figure 14 shows a division of the object space into numbered regions. If the 
viewer is in an odd region, the three closest faces are classified as front, left, and right with respect 
to the viewpoint.  Thus, for instance, if the viewer is in region (1), face a is front, face d is left, 
and face b is right. In this case, faces left and right are pre-warped to the image plane of front. Then 
front is pre­warped to its own image plane, overwriting all samples except the ones intended to fill 
holes. If, however, the viewer is in an even region, the two closest faces are classified as left and 
right. For instance, if the viewer is in region (6), face c is left and face d is right. left is pre-warped 
to the image plane of right, then right is pre-warped to its own image plane. Likewise, right is pre-warped 
to the image plane of left, and then left is pre-warped to its own image plane. Notice that at most three 
polygons (in the full 3-D version of the algorithm) need to be displayed. (6)  (5) (4) c (7) d b (3) 
a (8) (1) (2) The perpendicular orientation between adjacent faces can be exploited to pre-warp a face 
to its adjacent image plane as if it were the adjacent face itself. When the viewer is in an odd region, 
the displacement values associated with left and right are converted to column indices for front, while 
their column indices can be used as displacement for front (Figure 15). Thus, left and right can be pre-warped 
to front as if they were front themselves. The even region is similar. aleft left right aright front 
a front viewpoint Figure 15. Height values from left and right become columns for front. Columns from 
left and right become height for front. Figure 16 shows the statue rendered as two texture-mapped quadrilaterals 
(left and right), whose boundaries are shown to the right. The corresponding pre-warped textures are 
shown in Figure 17 and provide a clear illustration of the factorization of the planar perspective, which 
is compensated by the texture map stage of the warp. 4.1.1 Handling Surface Discontinuities Treating 
relief textures as continuous surfaces may not be desirable in some situations. Improper reconstruction 
of originally non-connected surfaces may lead to the occurrence of skins . The assumption about surface 
continuity can be relaxed if surfaces that would otherwise be rendered as skins had been appropriately 
sampled by adjacent relief textures. In this case, Figure 17. Pre-warped textures used to produce Figure 
16. texels belonging to non-connected surfaces should not be interpolated during the pre-warp. A simple 
way to achieve this is to use depth thresholds to identify and mark such discontinuities during a pre-processing 
step. Figure 18 shows the rendering of a rat before and after skin removal. In the accompanying videotape, 
the skins between the façade and the roof of the jeans shop store were removed and the resulting hole 
was seamlessly filled by adding an extra conventionally texture-mapped polygon.  4.2 Correct Occlusion 
The relief texture-mapping algorithm, as described so far, does not handle interpenetrating polygons 
appropriately. Thus, for example, consider intersecting a planar polygon with the bounding box used to 
represent the statue shown in Figure 13. Since the intersection between two polygons defines a straight 
line, the resulting occlusion pattern will not match the perceived depth of the associated relief textures. 
In order to solve this problem, corrected depth values accounting for the perceived off­the-plane displacements 
must be computed (Figure 19). Thus, let x be a point in 3-space associated with texel t, whose coordinates 
in the source texture are (us , vs ) . The Z coordinate of x in camera space when observed from a virtual 
COP C. is given by Z = c + uc + vc + displ(u , v )c x. 1 s 2 s 3 ss 4 aaaaa a where c1 = c · n , c2 
= as · n , c3= b a s · n and c4= f a s · n are constants for a given viewing configuration, n is the 
unit vector normal to the image plane of the virtual camera, c = CCs - CC , and a C C , a , b a and 
f a are the camera parameters associated with sss s the relief texture. Let (uj , vj ) be the coordinates 
of texel t' obtained after pre-warping t . Notice that the perceived depth at t' is Zx and such value 
can be interpolated along rows and columns in the same way as described for color in section 3.2. Alternatively, 
one can compute and interpolate only the difference .z between the actual polygon depth at t' and its 
perceived depth, which can be encoded using a smaller number of bits. Since t' is on the polygon to be 
texture mapped, its Z coordinate in the virtual camera space can be expressed as Z = c + uc + vc . During 
the pre-warp, .z values can be t' 1 j 2 j 3 linearly interpolated along rows and columns. The interpolated 
values can be used to compute the amount by which the depth buffer must be changed to produce correct 
visibility. Figure 19 shows the statue rendered with an interpenetrating polygon seen from different 
distances. In this example, .z was interpolated and the resulting values were quantized using the same 
strategy described in section 3.1 before being used to modulate the depth buffer.  4.3 Multiresolution 
Image pyramids have been long used in computer graphics for antialiasing [19]. Representing relief textures 
using fixed resolution causes a constant amount of work to be carried out during the pre-warp, independently 
of the number of pixels covered on the screen. The use of relief texture pyramids can be used not only 
to reduce aliasing but also to keep the warping cost proportional to the texture contribution to the 
final image. Level i of a relief texture pyramid is constructed by averaging color and depth data associated 
to groups of 2ix2i adjacent texels from the highest resolution relief texture (level zero). The lengths 
of vectors a and b a are doubled from level i to level i + 1 in order to compensate for the halving of 
the number of texels in each dimension, so that the spatial coverage of the relief texture remains unchanged. 
Figure 20 shows a statue rendered using the first four levels of a texture pyramid. Although mip-mapping 
is frequently used in computer graphics to reduce aliasing artifacts introduced by texture minification, 
bilinear interpolation is the preferred image resampling strategy during the texture mapping stage of 
the relief texture-mapping algorithm. It produces sharper images, is less prone to undesirable blurring 
due to polygon orientation and is computationally less expensive than trilinear interpolation. An in-depth 
discussion of this subject can be found in [13].  4.4 Modeling Immersive Environments Relief texture 
mapping can be used not just to represent objects, but complete environments. This is useful for the 
many applications of computer graphics that require immersive virtual environments. Moreover, the relief 
textures used to represent a whole environment can be generated nearly automatically, making this an 
elegant method of representing acquired real scenes. In our experiments, a laser rangefinder and a digital 
camera were used to scan the environment. The resulting data were then projected onto relief textures 
instantiated at user-defined locations (Figure 22 (left)). Registration is naturally enforced by the 
depth information associated with the samples (Figure 22 (right)). Figure 23 shows two renderings of 
a partial model of Sitterson Hall s reading room obtained using the three relief textures depicted in 
Figure 22 and an extra polygon representing the ceiling. Notice the parallax effect that allows the plaque 
on the wall to become visible in the image to the right.  5. RESULTS For a typical 256x256-texel relief 
texture mapped onto a single quadrilateral (e.g., Figure 9) using the two-pass approach described in 
Section 3.2.1, the current software prototype, written in C++, achieves an average frame rate of 9.42 
frames per second. Such measurements were performed on a Pentium II PC running at 400MHz with an Intergraph 
graphics accelerator (Intense 3D RealiZm II VX113A-T) with 16 MB of texture memory and 16MB of frame 
buffer memory. The final view of the surface was displayed on a 512x512-pixel window. The percentage 
of the rendering time spent with pre-warping and resampling, loading pre-warped textures into texture 
memory, and the actual texture mapping operation are shown in Table 1. Notice that, since the pre-warping 
and resampling operations dominated the rendering time, one can expect a considerable speedup from a 
hardware implementation of the algorithm. Also notice that pre-warping cost is independent of the dimensions 
of the output window. When a large number of texels have zero displacement, such as the case of the brick 
texture shown in Figure 21, speed-ups of over 100% were verified in the current software prototype by 
just skipping the unnecessary transformations. Table 1: Percentage of the average rendering time associated 
with the steps of the relief texture-mapping algorithm (one relief texture mapped onto one quadrilateral). 
Pre-warping Loading warped Actual texture Others and textures into mapping resampling texture memory 
operation 94.10% 2.65% 0.066% 3.18%  6. LIMITATIONS A relief texture is a single-layer image representation. 
If multiple layers of surfaces are required (e.g., in the case of objects containing holes), alternative 
representations such as LDI s [17] or image-based objects [12] should probably be preferred. Although 
parallel projection LDIs can be rendered in occlusion compatible order, the existence of multiple samples 
along each ray introduces ambiguity about which samples should be connected, making 1-D interpolation 
and mesh-based reconstruction impractical. In many cases, the rendering of several layers each consisting 
of individual relief textures can be used to achieve similar results. Objects, on the other hand, consist 
of six perpendicular relief textures and such a representation is equivalent to a multi-layer representation 
[13]. In some applications, it may not be possible to constrain the viewpoint from crossing the plane 
of a relief texture. In such a case, the relief texture-mapped polygon will not be rendered even if the 
represented surface may still be visible. In these cases, relief textures should to be rendered as meshes 
of micro-polygons. Ideally, only texels that effectively contribute to some screen fragments should be 
pre-warped. While such a strategy would make optimal use of the pre-warping, in practice this is not 
feasible, since it would require an inverse mapping. Alternatively, one can consider selecting the most 
appropriate level of a relief texture pyramid by using the projected area (in screen space) of the quadrilateral 
to be texture mapped.  7. SUMMARY AND FUTURE WORK We have presented an extension to texture mapping 
that supports the representation of 3-D surface details and view motion parallax. It results from an 
exact factorization of the 3-D image warping equation [11] into a pre-warp followed by conventional texture 
mapping. We have shown that, from a conventional rendering perspective, such a new factorization presents 
several advantages over the conventional plane-plus-parallax factorization. The simple pre-warping functions 
allow surface reconstruction (color and depth) to be performed in 1-D. This property should allow a simple 
and efficient hardware implementation. One important area for investigation is the design of efficient 
hardware implementations for relief texture mapping using our pre-warping functions. Adding this pre-warping 
capability to the texture memory of a graphics accelerator may allow this approach to become as commonly 
used as conventional texture mapping. Automatic acquisition of relief textures from 3-D environments 
is another important area for exploration. Other avenues for exploration involve the use of normal maps 
[6] [3] for view­dependent lighting and the use of relief textures for geometry simplification. Acknowledgements 
We would like to thank Chris Dwyer, Anselmo Lastra, Steve Molnar, Lars Nyland, Jason Smith and Mary Whitton 
for their assistance and suggestions, and the anonymous reviewers for their insightful comments. Special 
thanks go to Frederick P. Brooks, Jr. for his detailed critique of an earlier draft of this paper. Cássio 
Ribeiro designed Relief Town. The UNC IBR group provided the reading room data set. De Espona Infográfica 
provided the other models. This work was sponsored by CNPq/Brazil under Process # 200054/95, DARPA under 
order # E278 and NFS under grant # MIP-9612643.  References [1] Catmull, E. A Subdivision Algorithm 
for Computer Display of Curved Surfaces. Ph.D. Dissertation, Department of Computer Science, University 
of Utah, December 1974. [2] Catmull, E., Smith, A. 3D Transformations of Images in Scanline Order. Proc. 
SIGGRAPH 80 (Seattle, Washington, July 14-18, 1980), pp. 279-285. [3] Cohen, J., Olano, M., Manocha, 
D. Appearance-Preserving Simplification. Proc. SIGGRAPH 98 (Orlando, FL, July 19­24, 1998), pp. 115-122. 
[4] Debevec, P., Taylor, C., Malik, J. Modeling and Rendering Architecture from Photographs: A hybrid 
geometry-and image-based approach. Proc. SIGGRAPH 96 (New Orleans, LA, August 4-9, 1996), pp. 11-20. 
[5] Fant, Karl. A Nonaliasing, Real-Time Spatial Transform Technique. IEEE CG&#38;A, Vol. 6, No 1, January 
1986, pp. 71-80. [6] Fournier, A. Normal Distribution Functions and Multiple Surfaces. Graphics Interface 
92 Workshop on Local Illumination. pp. 45-52. [7] Gortler, S., et al.. The Lumigraph. Proc. SIGGRAPH 
96 (New Orleans, LA, August 4-9, 1996), pp. 43-54. [8] Grossman, J., Dally, W. Point Sample Rendering. 
9th Proceedings of the Eurographics Workshop on Rendering. Vienna, Austria, June 1998. Rendering Techniques 
98, Springer-Verlag, pp. 181-192. [9] Levoy, M., Hanrahan, P. Light Field Rendering Proc. SIGGRAPH 96 
(New Orleans, LA, August 4-9, 1996), pp. 31-42. [10] Max, N. A One-Pass Version of Two-Pass Image Resampling. 
Journal of Graphics Tools, Vol. 3, No. 1, pp. 33-41. [11] McMillan, L. An Image-Based Approach to Three-Dimensional 
Computer Graphics. Ph.D. Dissertation. UNC Computer Science Technical Report TR97-013, April 1997. [12] 
Oliveira, M., Bishop, G. Image-Based Objects. Proceedings of 1999 ACM Symposium on Interactive 3D Graphics. 
pp. 191-198. [13] Oliveira, M. Relief Texture Mapping. Ph.D. Dissertation. UNC Computer Science Technical 
Report TR00-009. March 2000. http://www.cs.unc.edu/~ibr/pubs/oliveira-diss/TR00­009.pdf. [14] Robertson, 
P. Fast Perspective Views of Images Using One-Dimensional Operations. IEEE CG&#38;A, vol. 7, pp. 47-56, 
Feb. 1987. [15] Sawhney, H. 3D Geometry from Planar Parallax. In IEEE CVPR 94, pages 929-934. IEEE Computer 
Society, Seattle, Washington, June 1994. [16] Schaufler, G. Per-Object Image Warping with Layered Impostors. 
Proceedings of the 9th Eurographics Workshop on Rendering. Vienna, Austria, June 1998. Rendering Techniques 
98, Springer-Verlag, pp. 145-156. [17] Shade, J., et al. Layered Depth Images. Proc. SIGGRAPH 98 (Orlando, 
FL, July 19-24, 1998), pp. 231-242. [18] Smith, Alvy Ray. Planar 2-Pass Texture Mapping and Warping. 
Proc. SIGGRAPH 87 (Anaheim, CA, July 27-31, 1987), pp. 263-272. [19] Williams, L. Pyramidal Parametrics. 
Proc. SIGGRAPH 83 (Detroit, MI, July 25-29, 1983), pp. 1-11. [20] Wolberg, George. Separable Image Warping 
with Spatial Lookup Tables. Proc. SIGGRAPH 89 (Boston, MA, July 31­4 August, 1989), pp. 369-378. [21] 
Woo, M., et al. OpenGL Programming Guide. 2nd edition. Addison Wesley, 1997.    
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344951</article_id>
		<sort_key>369</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>41</seq_no>
		<title><![CDATA[Image-based visual hulls]]></title>
		<page_from>369</page_from>
		<page_to>374</page_to>
		<doi_number>10.1145/344779.344951</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344951</url>
		<abstract>
			<par><![CDATA[<p>In this paper, we describe an efficient image-based approach to computing and shading visual hulls from silhouette image data. Our algorithm takes advantage of epipolar geometry and incremental computation to achieve a constant rendering cost per rendered pixel. It does not suffer from the computation complexity, limited resolution, or quantization artifacts of previous volumetric approaches. We demonstrate the use of this algorithm in a real-time virtualized reality application running off a small number of video streams.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[computer vision]]></kw>
			<kw><![CDATA[constructive solid geometry]]></kw>
			<kw><![CDATA[image-based rendering]]></kw>
			<kw><![CDATA[misc. rendering algorithms]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Constructive solid geometry (CSG)**</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP40027493</person_id>
				<author_profile_id><![CDATA[81100458116]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wojciech]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matusik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Laboratory for Computer Science, Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P45948</person_id>
				<author_profile_id><![CDATA[81100210278]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Buehler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Laboratory for Computer Science, Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40022816</person_id>
				<author_profile_id><![CDATA[81100022847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of North Carolina, Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P269895</person_id>
				<author_profile_id><![CDATA[81100259454]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Steven]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Gortler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Division of Engineering and Applied Sciences, Harvard University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14058906</person_id>
				<author_profile_id><![CDATA[81100137780]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Leonard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McMillan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Laboratory for Computer Science, Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>628672</ref_obj_id>
				<ref_obj_pid>628317</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bichsel, M. "Segmenting Simply Connected Moving Objects in a Static Scene." IEEE PAM116, 11 (November 1994), 1138-1142.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>254735</ref_obj_id>
				<ref_obj_pid>254733</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Boyer, E., and M. Berger. "3D Surface Reconstruction Using Occluding Contours." IJCV 22, 3 (1997), 219-233.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166153</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Chen, S. E. and L. Williams. "View Interpolation for Image Synthesis." SIGGRAPH 93, 279-288.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218395</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Chen, S. E. "Quicktime VR- An Image-Based Approach to Virtual Environment Navigation." SIGGRAPH 95, 29-38.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237269</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Curless, B., and M. Levoy. "A Volumetric Method for Building Complex Models from Range Images." SIGGRAPH 96, 303-312.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237191</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Debevec, P., C. Taylor, and J. Malik, "Modeling and Rendering Architecture from Photographs." SIGGRAPH 96, 11-20.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Debevec, P.E., Y. Yu, and G. D. Borshukov, "Efficient View- Dependent Image-based Rendering with Projective Texture Mapping." Proc. of EGRW 1998 (June 1998).]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>924545</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Debevec, P. Modeling and Rendering Architecture from Photographs. Ph.D. Thesis, University of California at Berkeley, Computer Science Division, Berkeley, CA, 1996.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>171658</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Faugeras, O. Three-dimensional Computer Vision: A Geometric Viewpoint. MIT Press, 1993.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>2074247</ref_obj_id>
				<ref_obj_pid>2074226</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Friedman, N. and S. Russel. "Image Segmentation in Video Sequences." Proc 13th Conference on Uncertainty in Artifical Intelligence (1997).]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15898</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Goldfeather, J., J. Hultquist, and H. Fuchs. "Fast Constructive Solid Geometry Display in the Pixel-Powers Graphics System." SIG- GRAPH 86, 107-116.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237200</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Gortler, S. J., R. Grzeszczuk, R. Szeliski, and M. F. Cohen. "The Lumigraph." SIGGRAPH 96, 43-54.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>614766</ref_obj_id>
				<ref_obj_pid>614653</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Kanade, T., P. W. Rander, and P. J. Narayanan. "Virtualized Reality: Constructing Virtual Worlds from Real Scenes." IEEE Multimedia 4, 1 (March 1997), 34-47.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>628563</ref_obj_id>
				<ref_obj_pid>628309</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Laurentini, A. "The Visual Hull Concept for Silhouette Based Image Understanding." IEEE PAMI 16,2 (1994), 150-162.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237199</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Levoy, M. and P. Hanrahan. "Light Field Rendering." SIGGRAPH 96, 31-42.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37422</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Lorensen, W.E., and H. E. Cline. "Marching Cubes: A High Resolution 3D Surface Construction Algorithm." SIGGRAPH 87, 163-169.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218398</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[McMillan, L., and G. Bishop. "Plenoptic Modeling: An Image- Based Rendering System." SIGGRAPH 95, 39-46.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>269042</ref_obj_id>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[McMillan, L. An Image-Based Approach to Three-Dimensional Computer Graphics, Ph.D. Thesis, University of North Carolina at Chapel Hill, Dept. of Computer Science, 1997.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618392</ref_obj_id>
				<ref_obj_pid>616043</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Moezzi, S., D.Y. Kuramura, and R. Jain. "Reality Modeling and Visualization from Multiple Video Sequences." IEEE CG&amp;A 16, 6 (November 1996), 58-63.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>939185</ref_obj_id>
				<ref_obj_pid>938978</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Narayanan, P., P. Rander, and T. Kanade. "Constructing Virtual Worlds using Dense Stereo." Proc. ICCV 1998, 3-10.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>293713</ref_obj_id>
				<ref_obj_pid>293701</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Pollard, S. and S. Hayes. "View Synthesis by Edge Transfer with Applications to the Generation of Immersive Video Objects." Proc. of VRST, November 1998, 91-98.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>31124</ref_obj_id>
				<ref_obj_pid>31123</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Potmesil, M. "Generating Octree Models of 3D Objects from their Silhouettes in a Sequence of Images." CVGIP 40 (1987), 1-29.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>267081</ref_obj_id>
				<ref_obj_pid>266989</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Rander, P. W., P. J. Narayanan and T. Kanade, "Virtualized Reality: Constructing Time Varying Virtual Worlds from Real World Events." Proc. IEEE Visualization 1997, 277-552.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258865</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Rappoport, A., and S. Spitz. "Interactive Boolean Operations for Conceptual Design of 3D solids." SIGGRAPH 97, 269-278.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Roth, S. D. "Ray Casting for Modeling Solids." Computer Graphics and Image Processing, 18 (February 1982), 109-144.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Saito, H. and T. Kanade. "Shape Reconstruction in Projective Grid Space from a Large Number of Images." Proc. of CVPR, (1999).]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>794361</ref_obj_id>
				<ref_obj_pid>794189</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Seitz, S. and C. R. Dyer. "Photorealistic Scene Reconstruction by Voxel Coloring." Proc. of CVPR (1997), 1067-1073.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Seuss, D. "The Cat in the Hat," CBS Television Special (1971).]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>167383</ref_obj_id>
				<ref_obj_pid>171245</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Szeliski, R. "Rapid Octree Construction from Image Sequences." CVGIP: Image Understanding 58, 1 (July 1993), 23-32.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Vedula, S., P. Rander, H. Saito, and T. Kanade. "Modeling, Combining, and Rendering Dynamic Real-World Events from Image Sequences." Proc. 4~h Intl. Conf. on Virtual Systems and Multimedia (Nov 1998).]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Image-Based Visual Hulls Wojciech Matusik* Chris Buehler* Ramesh Raskar Laboratory for Computer Science 
Laboratory for Computer Science Department of Computer Science Massachusetts Institute of Technology 
Massachusetts Institute of Technology University of North Carolina - Chapel Hill Steven J. Gortler Leonard 
McMillan* Division of Engineering and Applied Sciences Laboratory for Computer Science Harvard University 
Massachusetts Institute of Technology Abstract In this paper, we describe an efficient image-based approach 
to computing and shading visual hulls from silhouette image data. Our algorithm takes advantage of epipolar 
geometry and incre­mental computation to achieve a constant rendering cost per rendered pixel. It does 
not suffer from the computation complex­ity, limited resolution, or quantization artifacts of previous 
volumetric approaches. We demonstrate the use of this algorithm in a real-time virtualized reality application 
running off a small number of video streams. Keywords: Computer Vision, Image-Based Rendering, Con­structive 
Solid Geometry, Misc. Rendering Algorithms. 1 Introduction Visualizing and navigating within virtual 
environments composed of both real and synthetic objects has been a long-standing goal of computer graphics. 
The term Virtualized Reality , as popular­ized by Kanade [23], describes a setting where a real-world 
scene is captured by a collection of cameras and then viewed through a virtual camera, as if the scene 
was a synthetic computer graphics environment. In practice, this goal has been difficult to achieve. 
Previous attempts have employed a wide range of computer vision algorithms to extract an explicit geometric 
model of the desired scene. Unfortunately, many computer vision algorithms (e.g. stereo vision, optical 
flow, and shape from shading) are too slow for real-time use. Consequently, most virtualized reality 
systems em­ploy off-line post-processing of acquired video sequences. Furthermore, many computer vision 
algorithms make unrealistic simplifying assumptions (e.g. all surfaces are diffuse) or impose impractical 
restrictions (e.g. objects must have sufficient non­periodic textures) for robust operation. We present 
a new algo­rithm for synthesizing virtual renderings of real-world scenes in real time. Not only is our 
technique fast, it also makes few sim­plifying assumptions and has few restrictions. *(wojciech | cbuehler 
| mcmillan)@graphics.lcs.mit.edu sjg@cs.harvard.edu raskar@cs.unc.edu Permission to make digital or 
hard copies of part or all of this work or personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. SIGGRAPH 2000, New Orleans, 
LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 Figure 1 - The intersection of silhouette cones 
defines an approxi­mate geometric representation of an object called the visual hull. A visual hull has 
several desirable properties: it contains the actual object, and it has consistent silhouettes. Our algorithm 
is based on an approximate geometric repre­sentation of the depicted scene known as the visual hull (see 
Figure 1). A visual hull is constructed by using the visible silhou­ette information from a series of 
reference images to determine a conservative shell that progressively encloses the actual object. Based 
on the principle of calculatus eliminatus [28], the visual hull in some sense carves away regions of 
space where the object is not . The visual hull representation can be constructed by a series of 3D constructive 
solid geometry (CSG) intersections. Previous robust implementations of this algorithm have used fully 
enumer­ated volumetric representations or octrees. These methods typically have large memory requirements 
and thus, tend to be restricted to low-resolution representations. In this paper, we show that one can 
efficiently render the ex­act visual hull without constructing an auxiliary geometric or volumetric representation. 
The algorithm we describe is image based in that all steps of the rendering process are computed in image 
space coordinates of the reference images. We also use the reference images as textures when shading 
the visual hull. To determine reference images that can be used, we compute which reference cameras have 
an unoccluded view of each point on the visual hull. We present an image-based visibility algorithm based 
on epipolar geometry and McMillan's occlusion compatible ordering [18] that allows us to shade the visual 
hull in roughly constant time per output pixel. Using our image-based visual hull (IBVH) algorithm, we 
have created a system that processes live video streams and ren­ders the observed scene from a virtual 
camera's viewpoint in real time. The resulting representation can also be combined with traditional computer 
graphics objects. Background and Previous Work Kanade s virtualized reality system [20] [23] [13] is 
perhaps clos­est in spirit to the rendering system that we envision. Their initial implementations have 
used a collection of cameras in conjunction with multi-baseline stereo techniques to extract models of 
dy­namic scenes. These methods require significant off-line processing, but they are exploring special-purpose 
hardware for this task. Recently, they have begun exploring volume-carving methods, which are closer 
to the approach that we use [26] [30]. Pollard s and Hayes [21] immersive video objects allow rendering 
of real-time scenes by morphing live video streams to simulate three-dimensional camera motion. Their 
representation also uses silhouettes, but in a different manner. They match sil­houette edges across 
pairs of views, and use these correspondences to compute morphs to novel views. This ap­proach has some 
limitations, since silhouette edges are generally not consistent between views. Visual Hull. Many researchers 
have used silhouette infor­mation to distinguish regions of 3D space where an object is and is not present 
[22] [8] [19]. The ultimate result of this carving is a shape called the object s visual hull [14]. A 
visual hull always contains the object. Moreover, it is an equal or tighter fit than the object s convex 
hull. Our algorithm computes a view-dependent, sampled version of an object s visual hull each rendered 
frame. Suppose that some original 3D object is viewed from a set of reference views R. Each reference 
view r has the silhouette sr with interior pixels covered by the object. For view r one creates the cone-like 
volume vhr defined by all the rays starting at the image's point of view pr and passing through these 
interior points on its image plane. It is guaranteed that the actual object must be con­tained in vhr. 
This statement is true for all r; thus, the object must be contained in the volume vhR= r.Rvhr. As the 
size of R goes to infinity, and includes all possible views, vhR converges to a shape known as the visual 
hull vh8 of the original geometry. The visual hull is not guaranteed to be the same as the original object 
since concave surface regions can never be distinguished using silhou­ette information alone. In practice, 
one must construct approximate visual hulls us­ing only a finite number of views. Given the set of views 
R, the approximation vhR is the best conservative geometric description that one can achieve based on 
silhouette information alone (see Figure 1). If a conservative estimate is not required, then alterna­tive 
representations are achievable by fitting higher order surface approximations to the observed data [2]. 
Volume Carving. Computing high-resolution visual hulls can be tricky matter. The intersection of the 
volumes vhr requires some form of CSG. If the silhouettes are described with a polygo­nal mesh, then 
the CSG can be done using polyhedral CSG, but this is very hard to do in a robust manner. A more common 
method used to convert silhouette contours into visual hulls is volume carving [22] [8] [29] [19] [5] 
[27]. This method removes unoccupied regions from an explicit volu­metric representation. All voxels 
falling outside of the projected silhouette cone of a given view are eliminated from the volume. This 
process is repeated for each reference image. The resulting volume is a quantized representation of the 
visual hull according to the given volumetric grid. A major advantage of our view­dependent method is 
that it minimizes artifacts resulting from this quantization. CSG Rendering. A number of algorithms have 
been de­veloped for the fast rendering of CSG models, but most are ill suited for our task. The algorithm 
described by Rappoport [24], Figure 2 Computing the IBVH involves three steps. First, the desired ray 
is projected onto a reference image. Next, the intervals where the projected ray crosses the silhouette 
are determined. Finally, these intervals are lifted back onto the desired ray where they can be intersected 
with intervals from other reference images. requires that each solid be first decomposed to a union of 
convex primitives. This decomposition can prove expensive for compli­cated silhouettes. Similarly, the 
algorithm described in [11] requires a rendering pass for each layer of depth complexity. Our method 
does not require preprocessing the silhouette cones. In fact, there is no explicit data structure used 
to represent the sil­houette volumes other than the reference images. Using ray tracing, one can render 
an object defined by a tree of CSG operations without explicitly computing the resulting solid [25]. 
This is done by considering each ray independently and computing the interval along the ray occupied 
by each object. The CSG operations can then be applied in 1D over the sets of intervals. This approach 
requires computing a 3D ray-solid inter­section. In our system, the solids in question are a special 
class of cone-like shapes with a constant cross section in projection. This special form allows us to 
compute the equivalent of 3D ray inter­sections in 2D using the reference images. Image-Based Rendering. 
Many different image-based rendering techniques have been proposed in recent years [3] [4] [15] [6] [12]. 
One advantage of image-based rendering techniques is their stunning realism, which is largely derived 
from the acquired images they use. However, a common limitation of these methods is an inability to model 
dynamic scenes. This is mainly due to data acquisition difficulties and preprocessing re­quirements. 
Our system generates image-based models in real­time, using the same images to construct the IBHV and 
to shade the final rendering. 3 Visual-Hull Computation Our approach to computing the visual hull has 
two distinct char­acteristics: it is computed in the image space of the reference images and the resulting 
representation is viewpoint dependent. The advantage of performing geometric computations in image space 
is that it eliminates the resampling and quantization artifacts that plague volumetric approaches. We 
limit our sampling to the pixels of the desired image, resulting in a view-dependent visual­hull representation. 
In fact, our IBVH representation is equivalent to computing exact 3D silhouette cone intersections and 
rendering the result with traditional rendering methods. Our technique for computing the visual hull 
is analogous to finding CSG intersections using a ray-casting approach [25]. Given a desired view, we 
compute each viewing ray s intersection with the visual hull. Since computing a visual hull involves 
only intersection operations, we can perform the CSG calculations in any order. Furthermore, in the visual 
hull context, every CSG primitive is a generalized cone (a projective extrusion of a 2D image silhouette). 
Because the cone has a fixed (scaled) cross section, the 3D ray intersections can be reduced to cheaper 
2D ray intersections. As shown in Figure 2 we perform the following steps: 1) We project a 3D viewing 
ray into a reference image. 2) We perform the intersection of the projected ray with the 2D sil­houette. 
These intersections result in a list of intervals along the ray that are interior to the cone s cross-section. 
3) Each interval is then lifted back into 3D using a simple projective mapping, and then intersected 
with the results of the ray-cone intersections from other reference images. A naïve algorithm for computing 
these IBVH ray intersections follows: IBVHisect (intervalImage &#38;d, refImList R){ for each referenceImage 
r in R computeSilhouetteEdges (r) for each pixel p in desiredImage d do p.intervals = {0..inf} for 
each referenceImage r in R for each scanline s in d for each pixel p in s ray3D ry3 = compute3Dray(p,d.camInfo) 
 lineSegment2D l2 = project3Dray(ry3,r.camInfo) intervals int2D = calcIntervals(l2,r.silEdges) intervals 
int3D = liftIntervals(int2D,r.camInfo,ry3) p.intervals = p.intervals ISECT int3D } To analyze the efficiency 
of this algorithm, let n be the num­ber of pixels in a scanline. The number of pixels in the image d 
is O(n2). Let k be the number of reference images. Then, the above algorithm has an asymptotic running 
time O(ikn2), where i is the time complexity of the calcIntervals routine. If we test for the intersection 
of each projected ray with each of the e edges of the silhouette, the running time of calcIntervals is 
O(e). Given that l is the average number of times that a projected ray intersects the silhouette1, the 
number of silhouette edges will be O(ln). Thus, the running time of IBVHisect to compute all of the 2D 
intersections for a desired view is O(lkn3). The performance of this naïve algorithm can be improved 
by taking advantage of incremental computations that are enabled by the epipolar geometry relating the 
reference and desired images. These improvements will allow us to reduce the amortized cost of 1D ray 
intersections to O(l) per desired pixel, resulting in an im­plementation of IBVHisect that takes O(lkn2). 
Given two camera views, a reference view r and a desired view d, we consider the set of planes that share 
the line connect­ing the cameras centers. These planes are called epipolar planes. Each epipolar plane 
projects to a line in each of the two images, called an epipolar line. In each image, all such lines 
intersect at a common point, called the epipole, which is the projection of one of the camera's center 
onto the other camera's view plane [9]. As a scanline of the desired view is traversed, each pixel projects 
to an epipolar line segment in r. These line segments emanate from the epipole edr, the image of d s 
center of projection onto r s image plane (see Figure 3), and trace out a pencil of epipolar lines in 
r. The slopes of these epipolar line segments will either increase or decrease monotonically depending 
on the direc­tion of traversal (Green arc in Figure 3). We take advantage of this monotonicity to compute 
silhouette intersections for the whole scanline incrementally. 1 We assume reference images also have 
O(n2) pixels. r1 Reference Image Figure 3 The pixels of a scanline in the desired image trace out a 
pencil of line segments in the reference image. An ordered tra­versal of the scanline will sweep out 
these segments such that their slope about the epipole varies monotonically. The silhouette contour of 
each reference view is represented as a list of edges enclosing the silhouette s boundary pixels. These 
edges are generated using a 2D variant of the marching cubes approach [16]. Next, we sort the O(nl) contour 
vertices in in­creasing order by the slope of the line connecting each vertex to the epipole. These sorted 
vertex slopes divide the reference image domain into O(nl) bins. Bin Bi has an extent spanning between 
the slopes of the ith and i+1st vertex in the sorted list. In each bin Bi we place all edges that are 
intersected by epipolar lines with a slope falling within the bin s extent2. During IBVHisect as we traverse 
the pixels along a scanline in the desired view, the pro­jected corresponding view rays fan across the 
epipolar pencil in the reference view with either increasing or decreasing slope. Concurrently, we step 
through the list of bins. The appropriate bin for each epipolar line is found and it is intersected with 
the edges in that bin. This procedure is analogous to merging two sorted lists, which can be done in 
a time proportional to the length of the lists (O(nl) in our case). For each scanline in the desired 
image we evaluate n viewing rays. For each viewing ray we compute its intersection with edges in a single 
bin. Each bin contains on average O(l) silhouette edges. Thus, this step takes O(l) time per ray. Simultaneously 
we traverse the sorted set of O(nl) bins as we traverse the scanline. Therefore, one scanline is computed 
in O(nl) time. Over n scanli­nes of the desired image, and over k reference images, this gives a running 
time of O(lkn2). Pseudocode for the improved algorithm follows. IBVHisect (intervalImage &#38;d, refImList 
R){ for each referenceImage r in R computeSilhouetteEdges (r) for each pixel p in desiredImage d do 
 p.intervals = {0..inf} for each referenceImage r in R bins b = constructBins(r.caminfo, r.silEdges, 
d.caminfo) for each scanline s in d incDec order = traversalOrder(r.caminfo,d.caminfo,s) resetBinPositon(b) 
 for each pixel p in s according to order ray3D ry3 = compute3Dray(p,d.camInfo) lineSegment2D l2 = 
project3Dray(ry3,r.camInfo) slope m = ComputeSlope(l2,r.caminfo,d.caminfo) updateBinPosition(b,m) 
intervals int2D = calcIntervals(l2,b.currentbin) intervals int3D = liftIntervals(int2D,r.camInfo,ry3) 
 p.intervals = p.intervals ISECT int3D } 2 Sorting the contour vertices takes O(nl log(nl)) and binning 
takes O(nl2). Sorting and binning over k reference views takes O(knl log(nl)) and O(knl2) correspondingly. 
In our setting, l << n so we view this preproc­essing stage as negligible. It is tempting to apply further 
optimizations to take greater advantage of epipolar constraints. In particular, one might con­sider rectifying 
each reference image with the desired image prior to the ray-silhouette intersections. This would eliminate 
the need to sort, bin, and traverse the silhouette edge lists. However, a call to liftInterval would 
still be required for each pixel, giving the same asymptotic performance as the algorithm presented. 
The disadvantage of rectification is the artifacts introduced by the two resampling stages that it requires. 
The first resampling is applied to the reference silhouette to map it to the rectified frame. The second 
is needed to unrectify the computed intervals of the de­sired view. In the typical stereo case, the artifacts 
of rectification are minimal because of the closeness of the cameras and the similarity of their pose. 
But, when computing visual hulls the reference cameras are positioned more freely. In fact, it is not 
unreasonable for the epipole of a reference camera to fall within the field of view of the desired camera. 
In such a configuration, rectification is degenerate.  4 Visual-Hull Shading The IBVH is shaded using 
the reference images as textures. In order to capture as many view-dependent effects as possible a view-dependent 
texturing strategy is used. At each pixel, the ref­erence-image textures are ranked from "best" to "worst" 
according to the angle between the desired viewing ray and rays to each of the reference images from 
the closest visual hull point along the desired ray. We prefer those reference views with the smallest 
angle [7]. However, we must avoid texturing surface points with an image whose line-of-sight is blocked 
by some other point on the visual hull, regardless of how well aligned that view might be to the desired 
line-of-sight. Therefore, visibility must be consid­ered during the shading process. When the visibility 
of an object is determined using its visual hull instead of its actual geometry, the resulting test is 
conserva­tive erring on the side of declaring potentially visible points as non-visible. We compute visibility 
using the visual hull, VHR, as determined by IBVHisect. This visual hull is represented as inter­vals 
along rays of the desired image d. Pseudocode for our shading algorithm is given below. IBVHshade(intervalImage 
&#38;d, refImList R){ for each pixel p in d do p.best = BIGNUM for each referenceImage r in R do 
for each pixel p in d do ray3D ry3 = compute3Dray(p,d.camInfo) point3 pt3 = front(p.intervals,ry3) 
 double s = angleSimilarity(pt3,ry3,r.camInfo) if isVisible(pt3,r,d) if (s < p.best) point2 pt2 = 
project(pt3,r.camInfo) p.color = sample_color(pt2,r) p.best = s } The front procedure finds the front 
most geometric point of the IBVH seen along the ray. The IBVHshade algorithm has time complexity O(vkn2), 
where v is the cost for computing visibility of a pixel. Once more we can take advantage of the epipolar 
geometry in order to incrementally determine the visibility of points on the visual hull. This reduces 
the amortized cost of computing visibil­ity to O(l) per desired pixel, thus giving an implementation 
of IBVHshade that takes O(lkn2). Consider the visibility problem in flatland as shown in Figure 4. For 
a pixel p, we wish to determine if the front-most point on the visual hull is occluded with respect to 
a particular reference image by any other pixel interval in d. Figure 4 In order to compute the visibility 
of an IBVH sample with respect to a given reference image, a series of IBVH intervals are projected back 
onto the reference image in an occlusion­compatible order. The front-most point of the interval is visible 
if it lies outside of the unions of all preceding intervals. Efficient calculation can proceed as follows. 
For each refer­ence view r, we traverse the desired-view pixels in front-to-back order with respect to 
r (left-to-right in Figure 4). During traversal, we accumulate coverage intervals by projecting the IBVH 
pixel intervals into the reference view, and forming their union. For each front most point, pt3, we 
check to see if its projection in the reference view is already covered by the coverage intervals com­puted 
thus far. If it is covered, then pt3 is occluded from r by the IBVH. Otherwise, pt3 is not occluded from 
r by either the IBVH or the actual (unknown) geometry. visibility2D(intervalFlatlandImage &#38;d, referenceImage 
r){ intervals coverage = <empty> for each pixel p in d do \\front to back in r ray2D ry2 = compute2Dray(p,d.camInfo) 
point2 pt2 = front(p.intervals,ry2); point1D p1 = project(pt2,r.camInfo) if contained(p1,coverage) p.visible[r] 
= false else p.visible[r] = true intervals tmp = prjctIntrvls(p.intervals,ry2,r.camInfo) coverage = coverage 
UNION tmp } This algorithm runs in O(nl), since each pixel is visited once, and containment test and 
unions can be computed in O(l) time. Figure 5 Ideally, the visibility of points in 3D could be computed 
by applying the 2D algorithm along epipolar planes. In the continuous case, 3D visibility calculations 
can be re­duced to a set of 2D calculations within epipolar planes (Figure 5), since all visibility interactions 
occur within such planes. How­ever, the extension of the discrete 2D algorithm to a complete discrete 
3D solution is not trivial, as most of the discrete pixels in our images do not exactly share epipolar 
planes. Consequently, one must be careful in implementing conservative 3D visibility. First, we consider 
each of the intervals stored in d as a solid frustum with square cross section. To determine visibility 
of a (square) pixel p correctly we consider Sp, the set of all possible epipolar planes which touch p. 
There are at least two possible definitions for whether p is visible: (1) p is visible along all planes 
in Sp , (2) p is visible along any plane in Sp. Clearly the first defi­nition results in more pixels 
that are labeled not visible, therefore, it is better suited when using a large number of reference images. 
With a small number of reference images, the second definition is preferred. Implementing efficient exact 
algorithms for these visi­bility definitions is difficult, therefore, we use conservative algorithms; 
if the pixel is truly invisible we never label it as visi­ble. However, the algorithms could label some 
pixel as invisible though it is in fact visible. An algorithm that conservatively computes visibility 
ac­cording to the first definition is performed as follows. We define an epipolar wedge starting from 
the epipole erd in the desired view extending out to a one pixel-width interval on the image bound­ary. 
Depending on the relative camera views, we traverse the wedge either toward or away from the epipole 
[17]. For each pixel in this wedge, we compute visibility with respect to the pixels traversed earlier 
in the wedge using the 2D visibility algorithm. If a pixel is computed as visible then no geometry within 
the wedge could have occluded it in the reference view. We use a set of wedges whose union covers the 
whole image. A pixel may be touched by more than one wedge, in these cases its final visibility is computed 
as the AND of the results obtained from each wedge. The algorithm for the second visibility definition 
works as follows. We do not consider all possible epipolar lines that touch pixel p but only some subset 
of them such that at least one line touches each pixel. One such subset is all the epipolar lines that 
pass through the centers of the image boundary pixels. This par­ticular subset completely covers all 
the pixels in the desired image; denser subsets can also be chosen. The algorithm com­putes visibility2D 
for all epipolar lines in the subset. Visibility for a pixel might be computed more than once (e.g., 
the pixels near the epipole are traversed more often). We OR all ob­tained visibility results. Since 
we compute visibility2D for up to 4n epipolar lines in k reference images the total time complex­ity 
of this algorithm is O(lkn2). In our real-time system we use small number of reference images (typically 
four). Thus, we use the algorithm for the second definition of visibility. The total time complexity 
of our IBVH algorithms is O(lkn2), which allows for efficient rendering of IBVH objects. These algo­rithms 
are well suited to distributed and parallel implementations. We have demonstrated this efficiency with 
a system that computes IBVHs in real time from live video sequences. System Implementation Our system 
uses four calibrated Sony DFW500 FireWire video cameras. We distribute the computation across five computers, 
four that process video and one that assembles the IBVH (see Figure 6). Each camera is attached to a 
600 MHz desktop PC that captures the video frames and performs the following processing steps. First, 
it corrects for radial lens distortion using a lookup table. Then it segments out the foreground object 
using back­ground-subtraction [1] [10]. Finally, the silhouette and texture information are compressed 
and sent over a 100Mb/s network to a central server for IBVH processing. Our server is a quad-processor 
550 MHz PC. We interleave the incoming frame information between the 4 processors to in­crease throughput. 
The server runs the IBVH intersection and shading algorithms. The resulting IBVH objects can be depth­buffer 
composited with an OpenGL background to produce a full scene. In the examples shown, a model of our graphics 
lab made with the Canoma modeling system was used as a background. Figure 7 A plot of the execution 
times for each step of the IBVH rendering algorithm on a single CPU. A typical IBVH might cover approximately 
8000 pixels in a 640 × 480 image and it would exe­cute at greater than 8 frames per second on our 4 CPU 
machine. In Figure 7, the performances of the different stages in the IBVH algorithm are given. For these 
tests, 4 input images with resolutions of 256 × 256 were used. The average number of times that a projected 
ray crosses a silhouette is 6.5. Foreground seg­mentation (done on client) takes about 85 ms. We adjusted 
the field of view of the desired camera, to vary the number of pixels occupied by the object. This graph 
demonstrates the linear growth of our algorithm with respect to the number of output pixels. 6 Conclusions 
and Future Work We have described a new image-based visual-hull rendering algo­rithm and a real-time 
system that uses it. The algorithm is efficient from both theoretical and practical standpoints, and 
the resulting system delivers promising results. The choice of the visual hull for representing scene 
elements has some limitations. In general, the visual hull of an object does not match the object s exact 
geometry. In particular, it cannot represent concave surface regions. This shortcoming is often con­sidered 
fatal when an accurate geometric model is the ultimate goal. In our applications, the visual hull is 
used largely as an im­poster surface onto which textures are mapped. As such, the visual hull provides 
a useful model whose combination of accurate sil­houettes and textures provides surprisingly effective 
renderings that are difficult to distinguish from a more exact model. Our system also requires accurate 
segmentations of each image into foreground and background elements. Methods for accomplishing such segmentations 
include chromakeying and image differenc­ing. These techniques are subject to variations in cameras, 
lighting, and background materials. We plan to investigate techniques for blending between tex­tures 
to produce smoother transitions. Although we get impressive results using just 4 cameras, we plan to 
scale our system up to larger numbers of cameras. Much of the algorithm parallelizes in a straightforward 
manner. With k computers, we expect to achieve O(n2 l log k) time using a binary-tree based structure. 
 Figure 8 - Example IBVH images. The upper images show depth maps of the computed visual hulls. The 
lower images show shaded ren­derings from the same viewpoint. The hull segment connecting the two legs 
results from a segmentation error caused by a shadow. 7 Acknowledgements We would like to thank Kari 
Anne Kjølaas, Annie Choi, Tom Buehler, and Ramy Sadek for their help with this project. We also thank 
DARPA and Intel for supporting this research effort. NSF Infrastructure and NSF CAREER grants provided 
further aid. 8 References [1] Bichsel, M. Segmenting Simply Connected Moving Objects in a Static Scene. 
IEEE PAMI 16, 11 (November 1994), 1138-1142. [2] Boyer, E., and M. Berger. 3D Surface Reconstruction 
Using Oc­cluding Contours. IJCV 22, 3 (1997), 219-233. [3] Chen, S. E. and L. Williams. View Interpolation 
for Image Synthe­sis. SIGGRAPH 93, 279-288. [4] Chen, S. E. Quicktime VR An Image-Based Approach to 
Virtual Environment Navigation. SIGGRAPH 95, 29-38. [5] Curless, B., and M. Levoy. A Volumetric Method 
for Building Complex Models from Range Images. SIGGRAPH 96, 303-312. [6] Debevec, P., C. Taylor, and 
J. Malik, Modeling and Rendering Architecture from Photographs. SIGGRAPH 96, 11-20. [7] Debevec, P.E., 
Y. Yu, and G. D. Borshukov, Efficient View-Dependent Image-based Rendering with Projective Texture Map­ping. 
Proc. of EGRW 1998 (June 1998). [8] Debevec, P. Modeling and Rendering Architecture from Photo­graphs. 
Ph.D. Thesis, University of California at Berkeley, Computer Science Division, Berkeley, CA, 1996. [9] 
Faugeras, O. Three-dimensional Computer Vision: A Geometric Viewpoint. MIT Press, 1993. [10] Friedman, 
N. and S. Russel. Image Segmentation in Video Se­quences. Proc 13th Conference on Uncertainty in Artifical 
Intelligence (1997). [11] Goldfeather, J., J. Hultquist, and H. Fuchs. Fast Constructive Solid Geometry 
Display in the Pixel-Powers Graphics System. SIG-GRAPH 86, 107-116. [12] Gortler, S. J., R. Grzeszczuk, 
R. Szeliski, and M. F. Cohen. The Lumigraph. SIGGRAPH 96, 43-54. [13] Kanade, T., P. W. Rander, and P. 
J. Narayanan. Virtualized Reality: Constructing Virtual Worlds from Real Scenes. IEEE Multimedia 4, 1 
(March 1997), 34-47. [14] Laurentini, A. The Visual Hull Concept for Silhouette Based Image Understanding. 
IEEE PAMI 16,2 (1994), 150-162. [15] Levoy, M. and P. Hanrahan. Light Field Rendering. SIGGRAPH 96, 31-42. 
[16] Lorensen, W.E., and H. E. Cline. Marching Cubes: A High Resolu­tion 3D Surface Construction Algorithm. 
SIGGRAPH 87, 163-169. [17] McMillan, L., and G. Bishop. Plenoptic Modeling: An Image-Based Rendering 
System. SIGGRAPH 95, 39-46. [18] McMillan, L. An Image-Based Approach to Three-Dimensional Computer Graphics, 
Ph.D. Thesis, University of North Carolina at Chapel Hill, Dept. of Computer Science, 1997. [19] Moezzi, 
S., D.Y. Kuramura, and R. Jain. Reality Modeling and Visualization from Multiple Video Sequences. IEEE 
CG&#38;A 16, 6 (November 1996), 58-63. [20] Narayanan, P., P. Rander, and T. Kanade. Constructing Virtual 
Worlds using Dense Stereo. Proc. ICCV 1998, 3-10. [21] Pollard, S. and S. Hayes. View Synthesis by Edge 
Transfer with Applications to the Generation of Immersive Video Objects. Proc. of VRST, November 1998, 
91-98. [22] Potmesil, M. Generating Octree Models of 3D Objects from their Silhouettes in a Sequence 
of Images. CVGIP 40 (1987), 1-29. [23] Rander, P. W., P. J. Narayanan and T. Kanade, Virtualized Reality: 
Constructing Time Varying Virtual Worlds from Real World Events. Proc. IEEE Visualization 1997, 277-552. 
[24] Rappoport, A., and S. Spitz. Interactive Boolean Operations for Conceptual Design of 3D solids. 
SIGGRAPH 97, 269-278. [25] Roth, S. D. Ray Casting for Modeling Solids. Computer Graphics and Image Processing, 
18 (February 1982), 109-144. [26] Saito, H. and T. Kanade. Shape Reconstruction in Projective Grid Space 
from a Large Number of Images. Proc. of CVPR, (1999). [27] Seitz, S. and C. R. Dyer. Photorealistic Scene 
Reconstruction by Voxel Coloring. Proc. of CVPR (1997), 1067-1073. [28] Seuss, D. The Cat in the Hat, 
CBS Television Special (1971). [29] Szeliski, R. Rapid Octree Construction from Image Sequences. CVGIP: 
Image Understanding 58, 1 (July 1993), 23-32. [30] Vedula, S., P. Rander, H. Saito, and T. Kanade. Modeling, 
Com­bining, and Rendering Dynamic Real-World Events from Image Sequences. Proc. 4th Intl. Conf. on Virtual 
Systems and Multimedia (Nov 1998).   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344954</article_id>
		<sort_key>375</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>42</seq_no>
		<title><![CDATA[Efficient image-based methods for rendering soft shadows]]></title>
		<page_from>375</page_from>
		<page_to>384</page_to>
		<doi_number>10.1145/344779.344954</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344954</url>
		<abstract>
			<par><![CDATA[<p>We present two efficient imaged-based approaches for computation and display of high-quality soft shadows from area light sources. Our methods are related to shadow maps and provide the associated benefits. The computation time and memory requirements for adding soft shadows to an image depend on image size and the number of lights, not geometric scene complexity. We also show that because area light sources are localized in space, soft shadow computations are particularly well suited to imaged-based rendering techniques. Our first approach&#8212;<italic>layered attenuation maps</italic>&#8212;achieves interactive rendering rates, but limits sampling flexibility, while our second method&#8212;<italic>coherence-based raytracing</italic> of depth images&#8212;is not interactive, but removes the limitations on sampling and yields high quality images at a fraction of the cost of conventional raytracers. Combining the two algorithms allows for rapid previewing followed by efficient high-quality rendering.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[image-based rendering]]></kw>
			<kw><![CDATA[raytracing]]></kw>
			<kw><![CDATA[shadows]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Reliability</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP40034855</person_id>
				<author_profile_id><![CDATA[81100346089]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Maneesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Agrawala]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14019965</person_id>
				<author_profile_id><![CDATA[81100019585]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ravi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ramamoorthi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Gates Wing 3B-386, Stanford University, Stanford, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40024777</person_id>
				<author_profile_id><![CDATA[81100200189]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Alan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Heirich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Compaq Computer Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P169399</person_id>
				<author_profile_id><![CDATA[81100441528]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Laurent]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Moll]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Compaq Computer Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[L.W. Chang. Combining multiple reference images in an inverse warper. M.eng. thesis, MIT, 1998.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166153</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[S. E. Chen and L. Williams. View interpolation for image synthesis. In SIG- GRAPH 93 proceedings, pages 279-288, 1993.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>154731</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[M. F. Cohen and J. R. Wallace. Radiosity and Realistic image Synthesis. Academic Press, 1993.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37414</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[R.L. Cook, L. Carpenter, and E. Catmull. The Reyes image rendering architecture. In SIGGRAPIt 87proceedings, pages 95-102, 1987.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808590</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[R.L. Cook, T. Porter, and L. Carpenter. Distributed ray tracing. In SIGGRAPIt 84proceedings, pages 137-145, 1984.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563901</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[F. C. Crow. Shadow algorithms for computer graphics. In SIGGRAPH 77proceedings, pages 242-248, 1977.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192207</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[G. Drettakis and E. Fiume. A fast shadow algorithm for area light sources using backprojection. In SIGGRAPH 94 proceedings, pages 223-230, 1994.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280888</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[B. Guo. Progressive radiance evaluation using directional coherence maps. In SIGGRAPH 98 Proceedings, pages 255-266, 1998.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97913</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[R E. Haeberli and K. Akeley. The accumulation buffer: Hardware support for high-quality rendering. In SIGGRAPH 90proceedings, pages 309-318, 1990.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311551</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[D. Hart, R Dutre, and D. R Greenberg. Direct illumination with lazy visibility evaluation. In SIGGRAPIt 99 proceedings, pages 147-154, 1999.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[R Heckbert. Discontinuity meshing for radiosity. In Eurographics Rendering Workshop 92 proceedings, pages 203-226, May 1992.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[R Heckbert and M. Heft. Simulating soft shadows with graphics hardware. Technical Report CMU-CS-97-104, Carnegie Mellon University, 1997.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>319337</ref_obj_id>
				<ref_obj_pid>328712</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[A. Heirich and L. Moll. Scalable distributed visualization using off-the-shelf components. In Symposium on Parallel Visualization and Graphics, pages 55- 60, 1999.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383834</ref_obj_id>
				<ref_obj_pid>2383815</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[B. Keating and N. Max. Shadow penumbras for complex objects by depthdependent filtering of multi-layer depth images. In Eurographics Rendering Workshop 99 proceedings, 1999.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[S. Laveau and O. Faugeras. 3D scene representation as a collection of images and fundamental matrices. Technical Report 2205, INRIA, February 1994.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[D. Lischinski and A. Rappoport. Image-based rendering for non-diffuse synthetic scenes. In Eurographics Rendering Workshop 98proceedings, pages 301- 314, 1998.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>142453</ref_obj_id>
				<ref_obj_pid>142443</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[D. Lischinski, F. Tampieri, and D. R Greenberg. Discontinuity meshing for accurate radiosity. IEEE Computer Graphics and Applications, 12(6):25-39, November 1992.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[R.W. Marcato, Jr. Optimizing an inverse warper. M.eng. thesis, MIT, 1998.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>253292</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[W. R. Mark, L. McMillan, and G. Bishop. Post-rendering 3D warping. In Michael Cohen and David Zeltzer, editors, 1997 Symposium on interactive 3D Graphics, pages 7-16, April 1997.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>269042</ref_obj_id>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[L. McMillan. An image-Based Approach to Three-Dimensional Computer Graphics. Phd thesis, Department of Computer Science, University of North Carolina, 1997.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218398</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[L. McMillan and G. Bishop. Plenoptic modeling: An image-based rendering system. In SIGGRAPH 95 Proceedings, pages 39-46, 1995.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74337</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[F. K. Musgrave, C. E. Kolb, and R. S. Mace. The synthesis and rendering of eroded fractal terrains. In SIGGRAPH 89proceedings, pages 41-50, 1989.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37435</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[W. T. Reeves, D. H. Salesin, and R. L. Cook. Rendering antialiased shadows with depth maps. In SIGGRAPH 87proceedings, pages 283-291, 1987.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134071</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[M. Segal, C. Korobkin, R. van Widenfelt, J. Foran, and P. E. Haeberli. Fast shadows and lighting effects using texture mapping. In SIGGRAPH 92 proceedings, pages 249-252, 1992.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280882</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[J. W. Shade, S. J. Gortler, L. He, and R. Szeliski. Layered depth images. In SIGGRAPH 98proceedings, pages 231-242, 1998.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[C. Soler and F. X. Sillion. Automatic calculation of soft shadow textures for fast, high-quality radiosity. In Eurographics Rendering Workshop 98 proceedings, pages 199-210, 1998.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280927</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[C. Soler and F. X. Sillion. Fast calculation of soft shadow textures using convolution. In SIGGRAPH 98 proceedings, Computer Graphics Proceedings, Annual Conference Series, pages 321-332, 1998.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311552</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[M. Stark, E. Cohen, T. Lyche, and R. F. Riesenfeld. Computing exact shadow irradiance using splines. In SIGGRAPH 99 proceedings, 1999.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192210</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[A.J. Stewart and S. Ghali. Fast computation of shadow boundaries using spatial coherence and backprojection. In SIGGRAPH 94 proceedings, pages 231-238, 1994.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807402</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[L. Williams. Casting curved shadows on curved surfaces. In SIGGRAPH 78 proceedings, pages 270-274, 1978.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>130802</ref_obj_id>
				<ref_obj_pid>130745</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[A. Woo. Graphics Gems iii, chapter The Shadow Depth Map Revisited, pages 338-342. Academic Press, 1992.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617600</ref_obj_id>
				<ref_obj_pid>616014</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[A. Woo, R Poulin, and A. Fournier. A survey of shadow algorithms. IEEE Computer Graphics and Applications, 10(6): 13-32, November 1990.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[H. Zhang. Forward shadow mapping. In Eurographics Rendering Workshop 98 proceedings, pages 131-138, 1998.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Ef.cient Image-Based Methods for Rendering Soft Shadows Maneesh Agrawala Ravi Ramamoorthi Alan Heirich 
Laurent Moll Pixar Animation Studios Stanford University* Compaq Computer Corporation Figure 1: A plant 
rendered using our interactive layered attenuation-map approach (left), rayshade (middle), and our ef.cient 
high-quality coherence-based raytracing approach (right). Note the soft shadows on the leaves. To emphasize 
the soft shadows, this image is rendered without cosine falloff of light intensity. Model courtesy of 
O. Deussen, P. Hanrahan, B. Lintermann, R. Mech, M. Pharr, and P. Prusinkiewicz. Abstract We present 
two ef.cient image-based approaches for computation and display of high-quality soft shadows from area 
light sources. Our methods are related to shadow maps and provide the associ­ated bene.ts. The computation 
time and memory requirements for adding soft shadows to an image depend on image size and the number 
of lights, not geometric scene complexity. We also show that because area light sources are localized 
in space, soft shadow computations are particularly well suited to image-based render­ing techniques. 
Our .rst approach layered attenuation maps achieves interactive rendering rates, but limits sampling 
.exibility, while our second method coherence-based raytracing of depth images is not interactive, but 
removes the limitations on sampling and yields high quality images at a fraction of the cost of conven­tional 
raytracers. Combining the two algorithms allows for rapid previewing followed by ef.cient high-quality 
rendering. CR Categories: I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism Shadowing, 
Raytracing Keywords: Shadows, Raytracing, Image-Based Rendering Introduction Soft shadows from area light 
sources can greatly enhance the vi­sual realism of computer-generated images. However, accurately computing 
penumbrae can be very expensive because it requires determining visibility between every surface point 
and every light. *(maneesh,ravir)@graphics.stanford.edu (Alan.Herich,Laurent.Moll)@compaq.com Address: 
Gates Wing 3B-386, Stanford University, Stanford, CA 94305. Permission to make digital or hard copies 
of part or all of this work or personal or classroom use is granted without fee provided that copies 
are not made or distributed for profit or commercial advantage and that copies bear this notice and the 
full citation on the first page. To copy otherwise, to republish, to post on servers, or to redistribute 
to lists, requires prior specific permission and/or a fee. SIGGRAPH 2000, New Orleans, LA USA The cost 
of many soft shadow algorithms grows with the geomet­ric complexity of the scene. Algorithms such as 
ray tracing [5], and shadow volumes [6], perform visibility calculations in object­space, against a complete 
representation of scene geometry. More­over, some interactive techniques [12, 27] precompute and display 
soft shadow textures for each object in the scene. Such approaches do not scale very well as scene complexity 
increases. Williams [30] has shown that for computing hard shadows from point light sources, a complete 
scene representation is not nec­essary. He performs the visibility calculations in image space, against 
shadow maps image-based representations of scene ge­ometry. Although the shadows may suffer undersampling, 
bias, and aliasing artifacts, the cost of the algorithm is relatively inde­pendent of scene complexity. 
Further, it is possible to implement this method as a post-shading pass that modulates a shadowless rendering 
from the base renderer to include shadows. This makes it simple to add the method to any existing renderer, 
without mod­ifying the base renderer, and does not limit the approach to partic­ular geometric primitives. 
In this paper, we describe two ef.cient image-based techniques for rendering soft shadows that can be 
seen as logical extensions of Williams approach. In both methods, shadows are computed in image space. 
There­fore the time and memory requirements for adding soft shadows to an image are dependent only on 
image complexity and the number of lights, not geometric scene complexity. Neither algorithm com­putes 
per object textures, so texture mapping is not a bottleneck for us. This independence from geometric 
scene complexity allows us to ef.ciently compute soft shadows for large scenes, including those which 
have complex patterns of self-shadowing. We will also show that soft shadows are a particularly good 
ap­plication for image-based rendering approaches. Since area light sources are localized in space, visibility 
changes relatively little across them. The depth complexity of the visible or partially visi­ble scene 
as seen from a light (and stored in our shadow maps) is generally very low. Further, shadow maps rendered 
from the light source sparsely sample surfaces that are oblique to the light source. However, these surfaces 
are less important to sample well, because they are precisely the surfaces that are dimly lit. The contributions 
of this paper are the two algorithms summa­rized below, which represent two ends of a spectrum. &#38;#169; 
ACM 2000 1-58113-208-5/00/07 ...$5.00 Layered Attenuation Maps: Our .rst approach achieves inter­active 
rendering rates but limits sampling .exibility, and can there­fore generate undersampling and banding 
artifacts. We precom­pute a modi.ed layered depth image (LDI) [25] by warping and combining depth maps 
rendered from a set of locations on the light source. The LDI stores both depth information and layer-based 
at­tenuation maps which can be thought of as projective soft shadow textures. During display, the proper 
attenuation is selected from the LDI in real time in software, and is used to modulate normal rendering 
without shadows. The precomputation is performed in a few seconds, and soft shadows are then displayed 
at several frames a second. Since the light source sample positions are chosen a pri­ori, they are correlated 
for each surface location and this correlation can appear as banding in the .nal image. Coherence-Based 
Raytracing: Our second approach removes limitations on sampling and yields high quality images, suitable 
for high resolution prerendered animations, but is not interactive. We precompute shadow maps from a 
few points on the light, often the boundary vertices. To shade a surface point, we trace shadow rays 
through the shadow maps rather than the scene geometry. The shadows rays are decorrelated since they 
are chosen independently for each surface point, and therefore banding is replaced by noise. While the 
general approach to ray tracing depth images is well­known1 [15, 16, 18, 20], we develop several novel 
acceleration techniques for accelerating shadow ray computations. The visible portion of a light source 
tends to change very little for surface points close to one another. We describe a new image­based technique 
for exploiting this coherence when sampling vis­ibility along shadow rays. Our image-based raytracing 
approach with coherence-based sampling produces soft shadows at a fraction of the cost of conventional 
raytracers. While we combine both the image-based ray-tracing and sampling algorithms in a single ren­derer, 
they can be used independently (i.e. a standard geometric ray tracer might incorporate our coherence-based 
sampling method). Our algorithms can be combined in an interactive lighting sys­tem; our fast layered 
attenuation map method can be used to in­teractively set the viewing transformation, and position the 
light source and geometry. Our coherence-based raytracing method can then be used to quickly generate 
.nal high-quality images. This is the approach we took to produce the results in this paper, and we believe 
this approach has many applications to lighting design. The rest of this paper is organized as follows. 
Section 2 reviews previous work on soft shadows. Section 3 presents preliminaries, while Section 4 describes 
our interactive layered attenuation map algorithm. In section 5, we describe coherence-based raytracing 
of depth images. The results are presented in Section 6, and Section 7 discusses future work and conclusions. 
2Previous Work There is a vast literature on shadow algorithms, which we touch on only brie.y. Although 
a decade old, the survey by Woo et al. [32] is still an excellent reference. 2.1 Object-Based Methods 
Soft shadows can be computed using object-space methods such as distributed ray tracing [5] and radiosity 
with discontinuity mesh­ing [11, 17] or backprojection [7, 29]. Stark et al.[28] describe an­alytic methods 
for computing soft shadows. These approaches are computationally intensive and are not suitable for fast 
soft shadow generation for complex scenes. Herf and Heckbert [12] combine a number of shadow images for 
each receiver using an accumulation buffer [9]. The method is 1McMillan [20] calls this inverse warping. 
object-based, and the precomputation time can grow quadratically with the number of objects being shadowed, 
making it impractical for large scenes. Furthermore, a separate (generally large) texture is created 
for each shadowed object. Our layered attenuation map ap­proach is analogous in that we combine a similar 
number of depth images rendered from different points on the light. However, we improve on Herf and Heckbert 
s method by precomputing image­based textures simultaneously for the entire scene. Soler and Sillion 
[27] use convolution on blocker images to compute fast approximate soft shadows. A primary advantage 
of their technique is that sampling artifacts that sometimes occur when averaging hard shadows are avoided. 
A disadvantage of their method is that they cluster geometry in object-space and the clus­ters cannot 
shadow themselves; to correct this for complex objects like plants or trees would require a very large 
number of clusters for the leaves. This increased number of clusters can greatly increase the computation 
time, obviating the bene.ts of the method. Sep­arate textures are needed for each cluster being shadowed, 
which can strain the texture mapping hardware for complex objects. Fur­thermore, robust error control 
and automated clustering algorithms can be complicated to implement [26]. Hart et al. [10] develop a 
view dependent method to accelerate soft shadow computations for a standard ray tracer. They precom­pute 
a blocker list of geometry, stored in object space, for each im­age pixel by tracing a small number (often 
only one) of shadow rays to the light. When a blocker is found, they check if adjacent im­age pixels 
also see the same blocker using a recursive 4-connect .ood-.ll algorithm. The main stage of their algorithm 
.rst projects and clips each blocker to the light source and then computes the ir­radiance from the remaining 
portion of the light. While this method can greatly accelerate shadow computation, it is not well-suited 
for handling large amounts of tiny blocker geometry. As the size of ge­ometric elements decreases, the 
probability that a blocker is missed in the blocker list precomputation phase increases, which can re­sult 
in light leaks. Moreover, the storage of the blocker list and the projection and clipping of each blocker 
against the light source can become very expensive. While our coherence-based sampling bears some similarities 
to this approach, we remove two limita­tions. First, our approach is view independent. From a given set 
of precomputed shadows maps we can generate shadows for any view of the scene. Second, since our algorithm 
is image-based, its cost is independent of scene complexity. Small triangles are not a bottleneck for 
our coherence-based raytracing approach.  2.2 Image-Based Methods Williams shadow map algorithm [30] 
is an image-based alterna­tive to object-space methods. Visibility along a shadow ray is de­termined 
by precomputing a shadow map from the light and then comparing the depth of each pixel in the .nal image 
to the cor­responding depth in the shadow map. Percentage-closer .ltering can be used for antialiasing 
[23] and projective textures [24] can be used for hardware implementation. Forward shadow mapping [33] 
is an alternative implementation when texture-mapping represents a bottleneck for normal rendering of 
the scene. All of these meth­ods render hard shadows from point light sources. Chen and Williams [2] 
describe a simple extension to shadow mapping for rendering soft shadows. They render a few key shadow 
maps at the vertices of the light source and then use view interpo­lation to compute shadow maps for 
each sample location on the interior. To render soft shadows, they simply perform the stan­dard shadow 
map test on each interpolated map to compute av­erage visibility. The view interpolation method suffers 
from two drawbacks. First, the .nal image must be projected into each inter­polated shadow map independently. 
These projections can become expensive since they are required for each view in an interactive session. 
Second, like our layered attenuation map algorithm, band­ing artifacts can appear in the shadows, since 
the light source sam­ple positions are chosen a priori. Lischinski and Rappoport [16] use hierarchical 
raytracing of depth images as one of several image-based techniques for comput­ing secondary rays in 
synthetic scenes. Keating and Max [14] point out that light leaks are a problem with this approach because 
each depth sample is treated independently as a 2D surface unconnected with adjacent samples. They extend 
Lischinski and Rappoport s method by aggregating adjacent depth samples into discrete depth buckets, 
forming relatively large .at surfaces. While this approach reduces light leaks, as the authors point 
out, it can also completely change the scene geometry. It is unclear how such changes affect the .nal 
image. While our raytracing algorithm is also based on that of Lischinski and Rappoport, we reduce the 
light leak problem by reconstructing more accurate surfaces from the depth samples. We also introduce 
several new acceleration techniques that improve the ef.ciency of the hierarchical algorithm, especially 
when using multiple reference shadow maps. Guo [8] accelerates raytracing by using image-space coher­ence 
to reduce the number of primary rays traced without af­fecting the number of shadow rays traced per primary 
ray. Our coherence-based raytracing method exploits visibility coherence among shadow rays to reduce 
the number of shadow rays traced per primary ray. It may be possible to combine our approach with Guo 
s to exploit coherence for both shadow rays and primary rays. Preliminaries Irradiance from an area light 
source on a surface is given by [] Lcos .i cos .l E =VdA (1) pr2 Alight where L is the radiance output 
from the light source, .i is the inci­dent angle, and .l is the angle made with the light normal [3]. 
We are primarily concerned with the change in binary visibility V .In a post-shading approach, the lighting 
term is computed separately from visibility and is often approximated by treating the area light as a 
point light source. We can then independently compute an average visibility that attenuates the shadowless 
rendering. 1 ATT = VdA (2) A A It is also possible to implement both of our methods within the normal 
shading pass of the base renderer and compute equa­tion 1 directly. For simplicity and ef.ciency, our 
layered attenua­tion map algorithm takes the former approach, separating visibil­ity from lighting. For 
high quality results, our coherence-based raytracing algorithm takes the latter approach, directly computing 
equation 1. As in most soft shadow approaches, multiple lights are handled independently. The integral 
in equation 2 is evaluated using quadrature by sam­pling a number of points on the light source. We assume 
there is a mapping from the unit square to the light source such that a uniform sampling of the square 
will uniformly sample the light. To choose N2 sample locations on the light source, we stratify the unit 
square into NxN cells and choose some jittered sample location within each cell. In our layered attenuation 
map approach, the same sam­ple points on the light are used for shadowing each surface point. In contrast, 
our ray tracing algorithm chooses which points to sample on the light separately for each surface point, 
and thereby removes the banding artifacts that can appear in the former approach. Layered Attenuation 
Maps In this section, we describe our algorithm for precomputing and displaying layered attenuation maps. 
The reader will want to refer to the illustrations in .gures 2 thru 6. Light Scene Geometry  Figure 
2: Left: A schematic of the scene used to illustrate the layered attenuation map algorithm. Right: The 
scene without shadows, Figure 3: Images taken from the light source center (leftmost) and the four corners 
(line 3 of the precomputation pseudocode). Figure 4: Images in .gure 3 are warped to the light center 
(line 5 precom­putation pseudocode). On top, we show the .rst layer after warping, and below, the second 
layer. Yellow indicates absence of a second layer i.e. only one layer present. Regions not visible from 
a given light position show up as holes (black) when warped.  Figure 5: Images in .gure 4 are combined 
to form layered attenuation maps (lines 6 and 7 of the pseudocode). From left to right, layer 1, the 
texture on layer 1 (white indicates fully visible), layer 2 (yellow indicates absence of a second layer), 
and the texture on layer 2. Note that the com­pletely occluded green square is not present at all. Figure 
6: Left: A visualization; white denotes the light, yellow: regions with only one layer, red: the .rst 
of two layers, blue: the second of two layers, black: umbral regions behind the last layer in the LDI, 
and magenta: when a point is between two layers (which happens for a very small region of the green square 
at its edges since it is culled from the LDI). Middle: Attenuation map. This modulates the basic image. 
Right: The .nal image. Precomputation: During the precomputation phase, we build the layered attenuation 
maps. This data structure consists of a sep­arate list of layers for each pixel. Each layer stores depth, 
and the attenuation or fraction of the light that is visible from that point. procedure Precompute 1 
foreach light sample li 2 Viewpoint . li 3 Render(SCENE) 4 foreach pixel (x,y) ' 5 (x,y') . WarpCenter(x,y,z(x,y)) 
' 6 Insert((x,y'),z,e) 7 Process Attenuation Maps For each of a number of samples (typically 64) on the 
light, we render an image looking into the scene along the normal to the light at the sample location. 
In line 5 of the pseudocode, we transform the pixel into a central reference frame the view from the 
light s center. For planar light sources, this warp is especially simple, be­ing given by dv dp = - (3) 
z where dp is the vector disparity (change) in pixels, dv is the (known) vector difference in viewing 
positions. z is measured from the viewpoint into the scene, and is the same for both views. In gen­eral, 
a projective transformation is required. In line 6, we insert the transformed pixel into the layered-depth 
image (LDI). In our algorithm, each layer contains a depth value and an integer count. If the transformed 
depth value is already in the LDI (to tolerance e), we simply increment the count of the appropriate 
layer by one. The count corresponds to the number of light samples visible to a point in a given layer 
at a given pixel. If the depth does not exist in the layer list, we add a new layer to the list, setting 
its count to one. Holes, or gaps, can occur when warping image-based represen­tations of large objects 
in line 5. Splatting is often used to com­bat this problem. Since our viewpoints on the light are all 
close to each other (assuming the light is a relatively small .nite object), we adopt a simple strategy. 
For each transformed (fractional) point, the four neighboring (integer) pixels are considered in line 
6 of the pseudocode. To avoid double-counting, we increment the count for a given layer at a given pixel 
at most once for each viewpoint. Note that this splatting can slightly overestimate object size, making 
the shadows appear somewhat lighter. Finally, line 7 computes an attenuation map by dividing the count 
in a layer by the total number of samples used in the outer loop. This corresponds to the fraction of 
the light that is visible. Display: As shown in the pseudocode below, in the display phase of the algorithm, 
the scene is .rst rendered normally with lighting. Note that we do not interfere with the texture path, 
so the normal rendering can include textures. procedure Display 1 RenderWithLightingAndTextures(SCENE) 
2 foreach pixel (x, y) ''' 3 (x , y , z ) . WarpLDI((x, y, z(x, y))) 4 layer . Layer((x ' , y ' ),z ' 
,e) 5 color . color * AttMap((x ' , y ' ),layer) In line 3, each pixel is then projected back to the 
viewpoint at the center of the light source, and is associated with the nearest pixel in the precomputed 
LDI. The appropriate projection matrix is continuously updated during an interactive session. In line 
4, the list of layers at the corresponding pixel in the LDI is traversed and depths are compared with 
the transformed depth from the input pixel using a tolerance (shadow bias) e. If no depth matches, the 
rendered point is not visible from anywhere on the light, and the attenuation applied in the next step 
is 0. In line 5, the base color of the image pixel is modulated by the attenuation map corresponding 
to the layered attenuation map for layer computed in the previous step. Discussion: The time for precomputation 
is proportional to the number of light source samples used, while the time for display is proportional 
to the average depth complexity of the LDI. The pre­computation can be performed quickly because we use 
only fast image warping operations instead of slower raytracing or backpro­jection. The display phase 
can be carried out at interactive rates be­cause the depth complexity of the LDI is very low. Since the 
light samples are close together, and the LDI only stores points visible from somewhere on the light, 
the average number of layers in our representation is signi.cantly less than for an LDI that represents 
the entire scene. As we will see in section 6.1, LDI depth complex­ity increases slowly after the .rst 
few light samples i.e. very few new layers are created in line 6 of the precomputation pseudocode. We 
need to render the scene from many light sample locations to precompute accurate attenuation maps. If 
rendering the scene separately for each sample is expensive, we may instead warp depth images from key 
locations such as the corners of the light. In our implementation, this is not an issue because we use 
standard graph­ics hardware as the base renderer for line 3 precomputation, and line 1 display. However, 
all other parts of our precomputation and display routines are implemented entirely in software. To implement 
the precomputation and display routines in hard­ware would require hardware support for LDI creation 
and lookup. This is somewhat complicated because the depth complexity of LDIs is not .xed a priori. For 
soft shadows, however, the .nal depth complexity of the LDIs tends to be very low. Therefore, it may 
be possible to limit the number of layers and implement the display phase in hardware by combining shadows 
separately computed for each layer either using the accumulation buffer and the SGI shadow map extensions, 
or using a programmable image­compositing framework [13]. 5 Coherence-Based Raytracing The layered attenuation 
map method is suitable for rapid preview­ing because of its fast precomputation phase and its interactive 
dis­play phase whose time complexity is independent of the number of light source samples. However, .nal 
images for applications such as prerendered animation, require high quality antialiased artifact-free 
shadows. To render such images ef.ciently, we have developed a coherence-based raytracing algorithm. 
The algorithm combines two independent image-based meth­ods: a hierarchical raytracing technique and 
a coherence-based sampling technique. We begin by precomputing shadow maps from several locations in 
the scene. Our raytracing algorithm places no restrictions on the position and orientation of the reference 
views; we typically use views from the exterior vertices of the light. To shade a surface point, we compute 
visibility along each shadow ray by tracing it through each shadow map, until either an intersection 
is found or we pass through all the shadow maps. Our coherence­based sampling algorithm reduces the number 
of shadow rays cast to a light source by sampling light source visibility only where changes in visibility 
are most likely. For the layered attenuation map approach, the light source sam­pling is done during 
precomputation. On the other hand, it is done during display in the coherence-based ray tracing method, 
mak­ing the precomputation phase independent of the number of light source samples, and the display time 
proportional to the number of shadow rays traced. 5.1 Raytracing Depth Images Raytracing depth images 
is a well known technique [15, 20] . After a quick summary of the algorithm, we describe several new 
modi­.cations to it, which improve both its accuracy and ef.ciency. To trace a shadow ray against a single 
reference image (shadow map), we can .rst project it onto the reference image plane, and then step along 
this epipolar ray from pixel to pixel, checking for intersections with the scene geometry represented 
as depths in the shadow map. The intersection calculation is performed in two phases. The .rst phase 
is a quick overlap test to determine if in­tersection is possible. As we step along the ray, we maintain 
the epipolar depth interval [Zenter,Zexit] of the shadow ray that spans the current reference pixel. 
If the corresponding reference image depth Zref is inside the epipolar depth interval, the second phase 
of the intersection test is performed to determine the exact point of intersection. Recently, several 
papers [1, 16, 18] have described a hierar­chical version of this raytracing algorithm that is similar 
to ear­lier work on raytracing height .elds [22]. As a pre-process, two quadtrees are constructed from 
the reference image, one storing maximum depth values and one storing minimum depth values. The hierarchical 
algorithm performs the overlap test in a coarse­to-.ne manner using these quadtrees. Thus, the raytracer 
can ef.­ciently skip over large sections of the ray that cannot contain inter­sections. At the leaf-node 
level the exact intersection test is applied as before. Our pseudocode is adapted from [16]: procedure 
Trace(QTreesNode,Ray,Zenter ,Zexit) 1 if(Leaf(QTreesNode)) 2 check for exact intersection 3 else 4 RefIntrvl 
. [QTreesNode(MIN),QTreesNode(MAX)] 5 EpiIntrvl . [Zenter,Zexit] 6 if(OverLap(EpiIntrvl,RefIntrvl)) 7 
foreach non-empty Child of QTreesNode 8 Update(Zenter,Zexit) 9 Trace(Child,Ray,Zenter,Zexit) The exact 
intersection test requires reconstructing a surface from the depth values stored in the reference image. 
Two com­mon reconstruction techniques are triangulation[19] and bilinear interpolation[18, 21]. However, 
both methods impose costly exact intersection checks. A much simpler approach is to assume each reference 
image depth pixel represents a plane, which we call a .oor, that runs parallel to the reference image 
plane. The .oors are connected at their edges with vertical planes we call walls2 . Al­though the .oors-and-walls 
approach may yield a blocky surface reconstruction compared to triangulation or bilinear interpolation, 
in practice we have found that such artifacts are easy to control by generating higher resolution reference 
images. Assuming every pair of adjacent reference pixels is connected with a wall yields incorrect intersections 
at silhouette edges of un­connected objects. We mitigate the problem by assuming adjacent reference image 
pixels are connected only if they differ in depth by less than a user speci.ed gap bias. We check for 
intersections with walls only when adjacent reference pixels are connected. In the next two subsections, 
we describe new methods for ac­celerating the hierarchical traversal and for ef.ciently combining information 
from multiple reference images. 5.1.1 Accelerating the Hierarchical Traversal Hierarchical Connectedness: 
By traversing the epipolar ray in a coarse-to-.ne manner, the hierarchical algorithm can quickly eliminate 
sections of the epipolar ray that cannot possibly intersect scene geometry. However, the only way to 
.nd an intersection is to recurse through the min/max quadtrees all the way to the .nest level and then 
perform the exact intersection test. If it is not nec­essary to determine the exact point of intersection, 
we can modify the algorithm to determine whether or not the ray is blocked before descending to the .nest 
level. When building the min/max quadtrees, we also build a connect­edness quadtree. At the leaf level, 
each pixel is connected to itself. At the next level, we determine whether each group of four adjacent 
pixels form a single surface by checking if their depths fall within the gap bias. We continue computing 
connectedness in this manner all the way to the root of the tree. At any stage in the hierarchical traversal, 
if the epipolar depth interval contains the corresponding reference image min/max interval and the connectedness 
quadtree reports that all four children of the current node are connected, then the ray must intersect 
some geometry within the node. Thus, it is possible to report that the ray is blocked without recursing 
to the .nest level of the quadtree. 2To compute the exact intersection, we .rst check if reference image 
depth Zref , lies within the epipolar depth interval for the leaf node pixel. If so, the ray intersects 
the .oor. At the exiting edge of the pixel we look up the reference depth for the adjacent pixel Zref2 
and if Zexit lies within [Zref ,Zref2], the ray intersects a wall. Linear interpolation can be used to 
.nd the exact point of intersection along the ray if necessary.  Surface (b) Occlusion Intervals Figure 
7: Handling multiple reference images. In (a) reference image R1 s image plane is more perpendicular 
to the ray than R2 s image plane and therefore yields a shorter epipolar ray. We rank R1 higher than 
R2 since it is faster to traverse and more likely to yield an intersection. In (b) the shadow ray starting 
at p1 is a clear miss in R1 since it never passes behind geometry. Since the ray is entirely visible 
in R1 we do not need to trace it in any other reference image and can immediately declare the ray unblocked. 
For the shadow ray starting at p2 only the green occlusion interval found in R1 is traced in R2. Min/Max 
Clipping: The epipolar ray is initially clipped to the viewing frustum of the reference image. We can 
aggressively reduce the length of the epipolar ray by clipping it against the min/max bounds of the current 
node in the min/max quadtrees. By clipping the epipolar ray upon entering the Trace procedure, at ev­ery 
level of the recursion we ensure that the overlap test is always performed against the smallest possible 
piece of the epipolar ray3 . 5.1.2 E.ciently Combining Multiple Reference Images Using a single reference 
image, it is possible to miss intersections. If the desired ray intersects geometry that is not visible 
in the ref­erence view due to occlusion, the intersection can not be found. Using multiple reference 
images that capture different portions of the scene geometry can mitigate this problem. However, multiple 
views generally contain redundant information about the scene as well4 . Our layered attenuation map 
algorithm handles this redun­dancy by creating an LDI that only retains a single sample at each depth. 
A drawback of the LDI approach is that it requires resam­pling the reference images which can reduce 
numerical accuracy. We take a two pronged solution that avoids this resampling. First we rank the reference 
images so the that images most likely to gen­erate an intersection are traversed .rst. We adapt Chang 
s[1] ap­proach of ranking the reference images by the length of the clipped epipolar ray in each image 
(see .gure 7(a)). As soon as an inter­section is found we move on to the next shadow ray. Second, for 
each reference image after the .rst, we only trace portions of the ray that were not visible in the previous 
images. Clear Miss: Shadow ray computation requires traversing the epipolar ray in every reference image, 
until some intersection is found. For blocked shadow rays, the loop over reference images can be exited 
early, as soon as the intersection is found. This type of early exit is sometimes possible for unblocked 
rays as well. If we traverse an epipolar ray in one reference image and .nd no in­tervals in which it 
passes behind occluding geometry, the entire ray was visible in the reference view. We can safely report 
that the shadow ray is not blocked. This type of clear miss allows us to exit the function early, without 
traversing the epipolar ray in any other reference image. Occlusion Intervals: As we traverse the epipolar 
ray in some reference image, the overlap test forces the recursion all the way 3Marcato[18] performs 
this clipping only once at the coarsest level of the recursion. 4Choosing the optimal set of reference 
views to cover the space of a given scene while minimizing redundant information is a dif.cult problem 
which we do not ad­dress in this paper. down to the leaf level each time the ray enters and exits a region 
be­hind occluding geometry, We determine that an intersection did not occur at such silhouette boundaries 
only after we perform the exact intersection test which includes a check for connectedness. As we traverse 
the ray, we store a list of these occlusion intervals in which the ray passes behind occluding geometry. 
These are the only in­tervals that need to be checked in the other reference images5 as shown in .gure 
7(b).  5.2 Sampling the Light Source Stochastic raytracers typically distribute shadow rays indepen­dently 
over the entire light source for each surface point in the scene. Yet, visibility as a function of the 
position of the surface point tends to change slowly. Moreover, object-space rendering al­gorithms such 
as z-buffered scan conversion or REYES [4] shade a single surface at a time, moving from surface point 
to surface point in some continuous order. Thus, the surface points are generally processed in groups 
that lie close to one another. In this section we develop a technique for exploiting this coherence to 
reduce the region of the light source for which we cast shadow rays. As described in section 3, sampling 
the light source requires a mapping from the unit square to the surface of the light source. We consider 
the set of cells stratifying the unit square as a 2D image array covering the light source. The visibility 
image is a binary im­age storing whether or not each light source cell is blocked. The key idea of our 
algorithm is to predict the visibility image for surface point pi based on the light source visibility 
for the previous surface points. The algorithm is described in the following pseudocode: procedure SoftShad 
 1 BlockerPtsList .Ø 2 foreach surface point pi 3 Predict(pi,BlockerPtsList,VisImg,cellsTodo,cellsUsePrev) 
4 cellsDone .Ø 5 while not empty(cellsTodo) 6 lj .cellsTodo.dequeue 7 blockPt = Trace(ray(pi,lj)) 8 Update(BlockerPtsList,blockPt,lj) 
9 if(isBlocked(blockPt) != VisImg[lj]) 10 VisImg[lj] .isBlocked(blockPt) 11 foreach cell lk adjacent 
to lj 12 if(lk in cellsUsePrev) 13 cellsTodo.enqueue(lk ) 14 cellsDone.enqueue(lj ) 15 colori .Shade(VisImg) 
 The BlockerPtsList stores the intersection point for each oc­cluded shadow ray we have traced. Initially 
it is the empty set (line 1) and we insert new blocker points every time we trace a shadow ray (line 
7). Blocker points that are no longer relevant are removed in the Predict procedure (line 3) as described 
in section 5.2.2. Figure 8: Predicted Visibility Image. Gray boxes represent occluded cells, and white 
boxes represent unoccluded cells. Each cell marked with an X is initially placed in the cellsTodo list 
by the Predict procedure. Blues X s represent cells at edges between occluded and unoccluded regions, 
while red X s represent cells at the exterior edges of the light source. Cells that do not contain an 
X are initially placed in the cellsUsePrev list. We generate a predicted visibility image by projecting 
each point in the current BlockerPtsList onto the light source. Assuming a planar light source, the appropriate 
projection matrix is formed using pi as the center of projection and the light source as the im­age. 
In the predict procedure, we also build two lists of light source 5Chang[1] also computes occlusion intervals, 
but uses them to invalidate intersec­tions along the epipolar ray, rather than as an acceleration technique. 
 (a) Missed Blocker (b) Shadow Volume of b (c) Missed Hole Figure 9: Prediction errors. In (a) the blocker 
point b is not seen from surface point pi-1. We predict the light is completely unblocked at pi and never 
trace the ray through b. By increasing the surface sampling density so that pi falls on pe, the edge 
of the shadow volume due to b (shown in(b)), the blocker is found. In (c), a ray starting at pi passes 
through the aligned holes in the blockers. To ensure that this ray is traced, we must increase the light 
source sampling density. cells based on our con.dence of the predicted visibility. We assume that changes 
in visibility are most likely to occur in two places at the boundaries between blocked and unblocked 
regions on the inte­rior of the light source and at the exterior edges of the light source. If the predicted 
visibility for each interior cell is the same as the predicted visibility value of all of its neighboring 
cells, we are con­.dent of the prediction and we add the cell to the cellsUsePrev list. Otherwise, the 
cell is at an edge between blocked and unblocked regions, con.dence is low, so we add the cell to the 
cellsTodo list. Since we cannot examine a complete neighborhood for the cells on the exterior edges of 
the light source, we add all of these cells to the cellsTodo list. This is illustrated in .gure 8. If 
the BlockerPtsList is empty, we put all of the light source cells in the cellsTodo list. The main loop 
of the algorithm (lines 5 14) traces shadow rays for each light source cell in the cellsTodo list. When 
the value of a traced ray differs from the prediction held in the corresponding visibility image cell 
(line 9) we move any adjacent cell (considering all 8 neighbors) in the cellsUsePrev list to the cellsTodo 
list. As we .nd prediction errors, we spread out the lower con.dence values to the neighboring cells 
using an 8-connect .ood-.ll algorithm. The main loop ends when we have processed all cells in the cellsTodo 
list, and we then shade the surface point using the current version of the visibility image. 5.2.1 Prediction 
Errors As shown in .gure 9(a),(c) there are two types of errors that may occur with our prediction technique; 
1) missed blocker: a ray inside a region of the frustum predicted as unblocked is actually blocked and 
2) missed hole: a ray inside a region of the frustum predicted as blocked is unblocked. Both types of 
prediction errors can lead to visible artifacts if the ray containing the blocker or the hole is predicted 
with high con.dence and is not traced. There is a fundamental difference between these two errors. The 
missed blocker error can be diminished by increasing sur­face sampling density. Reducing the missed hole 
error requires increased light sampling. To better understand this difference, we simplify the situation. 
Assume the light source is entirely visible from pi-1 and there is some small object b blocking a ray 
between pi and some point on the interior of the light. The missed blocker error occurs because b is 
completely inside the frustum of pi but outside the frustum of pi-1, as shown in .gure 9(a). Thus, our 
prediction for pi requires that we only trace rays to the edges of the light source and since each of 
these traced rays agrees with our predicted visibility (unblocked), we never trace the ray through b. 
Consider however, the shadow volume due to b and its inter­section with the surface P . There must be 
some point pe on the edge of the intersection that lies between pi and pi-1. Since pe is on the surface 
of the shadow volume, the ray with origin at pe and passing though b must intersect the edge of the light 
source (.gure 9(b)). Since we always trace rays for points on the edge of the light source we correctly 
.nd the blocker b. As long as the surface points are suf.ciently close to one another, this property 
holds regardless of the direction (on the surface) in which we approach pi. There­fore, if we increase 
the surface sampling density, we reduce the missed blocker errors. A similar argument applies when regions 
of the light source are empty rather than the entire light source. Note that, if the distance between 
the current surface point and the previ­ous surface point is larger than a given tolerance, we disregard 
the predicted visibility and trace all the shadow rays. While it may seem a similar argument would apply 
to missed holes, there is a special case when multiple holes align to allow shadow rays to reach the 
interior of the light unoccluded. Suppose as in .gure 9(c), at surface point pi the holes in the two 
blockers align so that a single shadow ray to the interior of the light source is unblocked. For every 
surface point in any neighborhood of pi, every shadow ray to the light is blocked. There is no surface 
point for which a shadow ray to an exterior edge of the light sees the hole. To ensure this ray is traced, 
we must increase light source sampling density. This in turn increases the precision of our pre­dicted 
visibility since our blocker points list samples the surfaces more .nely and we project them onto the 
light more accurately.  5.2.2 Updating the Blocker Points List A drawback of our coherence-based sampling 
approach is that we must store and projectively warp each point in the blocker points list. Storing the 
list requires that the raytracer explicitly compute intersection points. Therefore, we cannot use the 
hierarchical con­nectedness optimization described in section 5.1.1. While warping a single point is 
relatively fast, if the blocker points list is large, the time to warp every point in the list can be 
signi.cant. We have designed several optimizations that limit the size of the blocker points list. We 
remove any blocker point that projects to a point outside the light source. This maintains some locality 
within the list and ensures that it stays relatively small. Often, mul­tiple blocker points will warp 
to the same cell of the light. This is especially true for cells at edges between blocked and unblocked 
regions, since these are exactly the cells that we trace for every sur­face point. Such blocker points 
essentially provide redundant infor­mation and generally lie very close to each other in object space. 
One option is to keep only one of the blocker points that warp to the same light source cell. We generalize 
this approach. In each cell, we maintain a higher resolution grid (typically 3x3) and within each high 
resolution cell we keep only a single blocker point. With these optimizations, the size of the blocker 
points list is at most the light sampling resolution times the high resolution grid reso­lution. In contrast, 
Hart et al. [10] precompute and store blocker lists of geometry for each pixel in the .nal image. Their 
lists can contain redundancies and the size of each list is limited only by im­age resolution and the 
total number of shadow rays traced in their precomputation phase. As a side bene.t, we can use our high 
resolution grid to directly insert cells into the cellsDone list. Before prediction, we compute a jittered 
sample location for each cell of the light source. During the prediction phase, if a blocker point warps 
to a high-resolution cell containing a sample location, we place the corresponding light source cell 
in the cellsDone list. In this case, the blocker point lies close enough to the ray we would trace that 
we can assume the ray is blocked. 6Results Our results are presented in .gures 1, 12, 13, and 14. Each 
image was originally rendered at 512x512 pixels and uses the equivalent of 256 light source samples. 
Some images have been cropped to preserve space, but the timings in .gure 10 are for the entire un­cropped 
image. The light is rectangular for each of these scenes. It lies directly above the center of the geometry 
for the plant and the .ower, and is displaced toward the head of the dragon. We have deliberately shown 
extremely complex examples. The light posi­tion is chosen so that almost all the geometry either casts 
shadows or is itself shadowed. Also, our viewpoints have been picked so that almost the entire geometry 
and shadow region is visible. Soft shadows for these scenes would be expensive to generate by object­space 
methods, including the interactive approaches of Herf and Heckbert [12] or Soler and Sillion [27], since 
the scenes contain a large number of triangles, and complex self-shadowing interac­tions. Further, the 
triangles are typically very small, especially for the dragon and the .ower, making these scenes dif.cult 
to handle by an approach that stores geometric information per pixel such as that of Hart et al. [10]. 
For quality comparisons, we have included images generated by rayshade. Performance: Layered Attenuation 
Maps Our layered at­tenuation map approach is implemented using standard graphics hardware under OpenGL 
as the base renderer. No special features, such as antialiasing, or shadow mapping extensions, are used. 
The hardware is used only for rendering shadowless images; the other parts of our precomputation and 
display phases work entirely in software. The running times for the precomputation phase on an SGI Onyx 
2 In.nite Reality, and with an LDI size of 512x512, are shown in the left of .gure 10. The main operations 
are software warping of rendered images and insertion into the LDI (lines 5 and 6 of the precomputation 
pseudocode), so the running time is largely independent of scene complexity. We see that coarse shadows 
(64 light samples), that may be suitable for most previewing work, can be precomputed in between 5 and 
10 seconds, while higher-quality versions (256 samples) can be precomputed in about half a minute. Regardless 
of light sampling density, images with soft shadows can be displayed interactively at 5-10 frames per 
second, with the ap­propriate attenuation being chosen in real time in software. Performance: Coherence-Based 
Raytracing Our coherence based sampling technique is designed to work with object-space rendering algorithms 
that shade and render the scene surface by surface. We have implemented our algorithm within the shading 
pass of Pixar s PhotoRealistic Renderman which is based on the REYES [4] object-space rendering algorithm. 
In the REYES al­gorithm, geometric primitives are split into micropolygons that are shaded and then scan 
converted to create the .nal image. Microp­olygons are shaded in a coherent manner, in groups that lie 
close to one another both in screen space and on the geometric primi­tive. Thus, lighting tends to change 
slowly from micropolygon to micropolygon. Since the micropolygons are shaded before com­puting their 
visibility, we compute soft shadows for every surface in the viewing frustum regardless of whether it 
is visible or not6 . In contrast, standard raytracing renderers and the post-shading ap­proach used with 
layered attenuation maps only perform the soft shadow computation on visible points. Therefore, direct 
compar­isons between the running times of our coherence-based approach and other methods is dif.cult. 
Running times and speedups for our coherence-based raytrac­ing algorithm are presented in .gure 10. The 
precomputation con­sists of rendering shadow maps at a resolution of 1024x1024 from the four corners 
of the light and constitutes a small fraction of the total running time. Note that the no-acceleration 
column refers to standard hierarchical image-based raytracing without any of our new acceleration techniques. 
Adding our raytracing acceleration techniques alone, without coherence-based sampling, provides a fairly 
consistent speedup of around 2.20x across the three scenes, regardless of light source sampling density. 
Much of this perfor­mance increase is due to the clear miss optimization which allows 6Rendering the 
plant, .ower and dragon in Renderman at 512x512 image resolu­tion requires shading 765134, 1344886 and 
772930 surface samples respectively.   64  4.59 2.15x  3.64x 34.86 2.2 Flower 35109 64 7.4 2.0 
Flower 332 120.29 256 28.1 1.8 1024 71.42 2.14x 3.95x 424.24 1.6 Dragon 77890 64 7.7 1.4 64 3.92 2.28x 
  10.59x 27.57 256 29.4 1.2 Dragon 140 79.86 1.0 1024 61.93 2.27x 15.18x 249.45  Coherence-Based 
Raytracing Scene Precom.. # Light  No Accel RT accel Accel + Coherence Layered Attenuation Maps Time 
(s) Samp. t(1000 s) speedup speedup rays Scene Triangles # Light Precom. 64 8.36 2.14x  7.74x 28.80 
Samp. Time (s) Plant 236 88.74 Plant 5247 64 6.0 1024 131.96 2.16x 10.12x 287.59 256 22.4 Average Depth 
Complexity 3.8 3.6 3.4 3.2 3.0 2.8 2.6 2.4 0             Number of Light Source Samples 
 Figure 10: Left: Precomputation times (in seconds) for our layered attenuation map approach on an SGI 
Onyx2 In.nite Reality. The major operations are image-based warping rendered images and inserting into 
the LDI so running time grows slowly with increased geometric scene complexity. The scenes can be displayed 
at 5-10 frames per second after precomputation is completed. Middle: Performance of Coherence-Based Raytracing 
on a 300 Mhz processor. The no acceleration column refers to hierarchical raytracing without any of our 
acceleration techniques and provides a baseline set of running times. The next two columns show the speedups 
achieved by including our raytracing accelerations and then both the accelerations and the coherence-based 
sampling. The .nal column shows the average number of rays actually traced with coherence-based sampling. 
Right: Increase in average depth complexity over non-empty LDI pixels in our layered attenuation map 
as a function of number of light samples. After the .rst few samples, the complexity increases very slowly. 
The .ower has a lot more very small geometry, and so the .nal average visible depth complexity is higher, 
though even in this case, it is fairly low. This graph is a more meaningful measure of scene complexity 
than simple polygon count; the size of the LDI is proportional to this complexity. the algorithm to ef.ciently 
process many of the unoccluded shadow rays without visiting all four shadow maps. As we increase the 
number of light samples, sizes of the fully blocked and unblocked areas on the light source grow faster 
than the lengths of the edges between them. We exploit this perimeter versus area growth ratio with our 
coherence-based sampling algo­rithm since we initially trace shadow rays only at the edges between blocked 
and unblocked regions. For all three scenes, we see that the speedups increase as we increase the light 
source sampling rate. Similarly, the ratio of average rays traced to the total number of light source 
samples decreases. If we increase the light source sam­pling rate by 4x, the running time of the original 
hierarchical ray­tracer and even that of the version with accelerations, but without coherence-based 
sampling, increase by roughly 4x. When we add coherence-based sampling however, the increase in running 
time is signi.cantly smaller (i.e. the speedup increases as we increase the light source sampling density, 
especially for the plant and dragon). We have observed that adding coherence-based sampling causes no 
noticeable difference in image quality compared to hierarchical raytracing without coherence-based sampling 
(.gure 13). The speedup due to coherence-based sampling is relatively low for the .ower scene. We believe 
this is largely due to the thin (typically much smaller than a pixel) geometry of the branches in the 
head of the .ower. For points in the penumbra region on the ground plane, there are few large regions 
of the light source that are either fully occluded or unoccluded. Since most of the light source is at 
an edge between occluded and unoccluded regions, the coherence-based sampling approach provides little 
bene.t. In con­trast, the coherence-based approach achieves the largest speedups for the dragon scene. 
Although the triangles are still very small, this scene tends to contain larger blocked and unblocked 
regions on the light source. Asymptotically, coherence-based sampling will make the num­ber of rays traced 
proportional to the square-root of the number of light samples, rather than the number of samples (because, 
in the limit, the number of rays traced depends on the length of the perimeter between blocked and unblocked 
regions, not area). In the limit, we therefore expect a 4x increase in light source sampling to cause 
the number of rays actually traced to increase by only 2x, not 4x. At the light source sampling densities 
we ve tested, we see an increase of a little more than 3x.   6.1 Discussion In some respects, soft 
shadow representation is a model application for the use of image-based representations. Depth Complexity: 
Since the light is localized in space, sam­ples on it are close together, and visibility does not change 
signif­icantly between the samples. This means the depth complexity of the completely and partially visible 
portions of the scene, as seen from the light, is very low, as seen in the graph in .gure 10. Note that 
geometry that is occluded from everywhere on the light is com­pletely excluded from our image-based representations. 
Therefore, as compared to an LDI that represents the entire scene, the LDIs in our layered attenuation 
map approach require signi.cantly fewer layers. Furthermore, the complexity of the representation increases 
very slightly after the .rst few samples on the light. In the context of coherence-based raytracing, 
this low complexity means a sparse set of shadow maps suf.ces to produce high-quality results. Sampling: 
Since our shadow maps are rendered from points on the light, surfaces whose normals make large angles 
to that of the light are sampled poorly. Since only a single LDI is used, this is more of an issue for 
layered attenuation maps than for coherence­based raytracing. However, these surfaces will also usually 
be very dimly lit because of cosine falloff in light intensity diminishing the visibility of sampling 
artifacts, as seen in the left of .gure 11. Artifacts: The images produced by both of our algorithms 
look very plausible when seen by themselves, and are also very close to those produced by rayshade. However, 
our layered attenuation map method produces some artifacts: Insuf.cient Depth Sampling: If the LDI samples 
depths of some surfaces insuf.ciently, we will not be able to tell whether a point in the .nal image 
occurs on, above, or below the surface, as seen in the left of .gure 11. This is less of a problem with 
coherence-based raytracing since we do not resample into a single LDI. Therefore, those shadow maps that 
better sample the surfaces in question are used to gen­erate the shadows. Note that since both of our 
algorithms require the use of error tolerances, we cannot use mid-point shadows [31]. Therefore, both 
methods require the user to specify a value for shadow bias.  Insuf.cient Attenuation Map Sampling: 
Even if a surface has constant depth, insuf.cient sampling can cause blocki­ness when the attenuation 
map is magni.ed and reprojected. As shown in the right of .gure 11, simple bilinear .ltering of four 
neighboring attenuation map values analogous to percentage-closer .ltering [23] for each image pixel 
can diminish the visibility of the artifacts. However, the results may still be inaccurate because a 
limited number of samples   Figure 11: Artifacts. Left: Insuf.cient depth sampling. Leftmost is the 
precomputed texture for layer 1 using layered attenuation maps. The sides of the large box are very poorly 
sampled. Next is the attenuation map, which has artifacts on the poorly sampled side. Similar artifacts 
are produced by the SGI (hard) shadow map hardware. However, coherence-based raytracing (third image) 
is able to do a much better job. Including cosine-falloff of light intensity, the .nal image produced 
by our layered attenuation map algorithm is shown rightmost, and the artifacts are considerably diminished. 
Right: Insuf.cient attenuation map sampling. Left is a thumbnail of the scene. The middle shows an extreme 
closeup of the ground which indicates blocky textures from magni.cation and reprojection. In the rightmost 
image, simple bilinear .ltering reduces the perceptibility of the artifacts. Models courtesy of Peter 
Shirley. are available for reconstruction. Since coherence-based ray­tracing does not precompute textures, 
and thereby predeter­mine their resolution and sampling pattern, this is not an is­sue for that method. 
Banding: Since the same light samples are used for all sur­faces, banding may occur as seen in .gure 
15. Note that banding is present in the attenuation map, and can therefore not be removed simply by post-.ltering 
on the attenuation map, similar to that discussed above. In the coherence-based raytracing method, banding 
is replaced by noise since light samples are decorrelated for all surface points. These artifacts are 
somewhat more apparent in high resolution images than at the size of the images in the printed version 
of this paper. To clearly show the artifacts here, we have zoomed very close. Similarly, to show banding, 
we have reduced the number of light source samples in .gure 15 only. As can be seen from the results, 
sampling artifacts are generally not a problem with coherence-based raytracing, so this technique is 
suitable for producing .nal high-quality images. Conclusions and Future Work We have described two ef.cient 
image-based methods for comput­ing soft shadows. These methods can be seen as extensions of an extremely 
popular technique for hard shadows shadow maps and produce results signi.cantly faster than traditional 
approaches. The algorithms can be combined for rapid previewing followed by ef.cient high-quality rendering. 
We have also demonstrated how soft shadows are an ideal application for image-based approaches. As future 
work, we would like to investigate better sampling strate­gies, the use of adaptive biases, and hardware 
implementation of the display phase for layered attenuation maps. Acknowledgements: We are indebted to 
Tom Lokovic for developing the hi­erarchical connectedness acceleration for the raytracing approach and 
for implement­ing an initial version of coherence-based raytracing in Pixar s Photorealistic Render­man. 
Special thanks to Reid Gershbein, Tony Apodaca, Mark VandeWettering and Craig Kolb for initial discussions, 
and to Bill Mark, James Davis, and Pat Hanrahan for reviewing early drafts. The Siggraph reviewers provided 
many helpful comments. References [1] L. W. Chang. Combining multiple reference images in an inverse 
warper. M.eng. thesis, MIT, 1998. [2] S. E. Chen and L. Williams. View interpolation for image synthesis. 
In SIG-GRAPH 93 proceedings, pages 279 288, 1993. [3] M. F. Cohen and J. R. Wallace. Radiosity and Realistic 
Image Synthesis. Aca­demic Press, 1993. [4] R. L. Cook, L. Carpenter, and E. Catmull. The Reyes image 
rendering architec­ture. In SIGGRAPH 87 proceedings, pages 95 102, 1987. [5] R. L. Cook, T. Porter, and 
L. Carpenter. Distributed ray tracing. In SIGGRAPH 84 proceedings, pages 137 145, 1984. [6] F. C. Crow. 
Shadow algorithms for computer graphics. In SIGGRAPH 77 pro­ceedings, pages 242 248, 1977. [7] G. Drettakis 
and E. Fiume. A fast shadow algorithm for area light sources using backprojection. In SIGGRAPH 94 proceedings, 
pages 223 230, 1994. [8] B. Guo. Progressive radiance evaluation using directional coherence maps. In 
SIGGRAPH 98 Proceedings, pages 255 266, 1998. [9] P. E. Haeberli and K. Akeley. The accumulation buffer: 
Hardware support for high-quality rendering. In SIGGRAPH 90 proceedings, pages 309 318, 1990. [10] D. 
Hart, P. Dutre, and D. P. Greenberg. Direct illumination with lazy visibility evaluation. In SIGGRAPH 
99 proceedings, pages 147 154, 1999. [11] P. Heckbert. Discontinuity meshing for radiosity. In Eurographics 
Rendering Workshop 92 proceedings, pages 203 226, May 1992. [12] P. Heckbert and M. Herf. Simulating 
soft shadows with graphics hardware. Tech­nical Report CMU-CS-97-104, Carnegie Mellon University, 1997. 
[13] A. Heirich and L. Moll. Scalable distributed visualization using off-the-shelf components. In Symposium 
on Parallel Visualization and Graphics, pages 55 60, 1999. [14] B. Keating and N. Max. Shadow penumbras 
for complex objects by depth­dependent .ltering of multi-layer depth images. In Eurographics Rendering 
Workshop 99 proceedings, 1999. [15] S. Laveau and O. Faugeras. 3D scene representation as a collection 
of images and fundamental matrices. Technical Report 2205, INRIA, February 1994. [16] D. Lischinski and 
A. Rappoport. Image-based rendering for non-diffuse syn­thetic scenes. In Eurographics Rendering Workshop 
98 proceedings, pages 301 314, 1998. [17] D. Lischinski, F. Tampieri, and D. P. Greenberg. Discontinuity 
meshing for accurate radiosity. IEEE Computer Graphics and Applications, 12(6):25 39, November 1992. 
[18] R.W. Marcato, Jr. Optimizing an inverse warper. M.eng. thesis, MIT, 1998. [19] W. R. Mark, L. McMillan, 
and G. Bishop. Post-rendering 3D warping. In Michael Cohen and David Zeltzer, editors, 1997 Symposium 
on Interactive 3D Graphics, pages 7 16, April 1997. [20] L. McMillan. An Image Based Approach to Three 
Dimensional Computer Graphics. Phd thesis, Department of Computer Science, University of North Carolina, 
1997. [21] L. McMillan and G. Bishop. Plenoptic modeling: An image-based rendering system. In SIGGRAPH 
95 Proceedings, pages 39 46, 1995. [22] F. K. Musgrave, C. E. Kolb, and R. S. Mace. The synthesis and 
rendering of eroded fractal terrains. In SIGGRAPH 89 proceedings, pages 41 50, 1989. [23] W. T. Reeves, 
D. H. Salesin, and R. L. Cook. Rendering antialiased shadows with depth maps. In SIGGRAPH 87 proceedings, 
pages 283 291, 1987. [24] M. Segal, C. Korobkin, R. van Widenfelt, J. Foran, and P. E. Haeberli. Fast 
shad­ows and lighting effects using texture mapping. In SIGGRAPH 92 proceedings, pages 249 252, 1992. 
[25] J. W. Shade, S. J. Gortler, L. He, and R. Szeliski. Layered depth images. In SIGGRAPH 98 proceedings, 
pages 231 242, 1998. [26] C. Soler and F. X. Sillion. Automatic calculation of soft shadow textures for 
fast, high-quality radiosity. In Eurographics Rendering Workshop 98 proceedings, pages 199 210, 1998. 
[27] C. Soler and F. X. Sillion. Fast calculation of soft shadow textures using convo­lution. In SIGGRAPH 
98 proceedings, Computer Graphics Proceedings, Annual Conference Series, pages 321 332, 1998. [28] M. 
Stark, E. Cohen, T. Lyche, and R. F. Riesenfeld. Computing exact shadow irradiance using splines. In 
SIGGRAPH 99 proceedings, 1999. [29] A. J. Stewart and S. Ghali. Fast computation of shadow boundaries 
using spatial coherence and backprojection. In SIGGRAPH 94 proceedings, pages 231 238, 1994. [30] L. 
Williams. Casting curved shadows on curved surfaces. In SIGGRAPH 78 proceedings, pages 270 274, 1978. 
[31] A. Woo. Graphics Gems III, chapter The Shadow Depth Map Revisited, pages 338 342. Academic Press, 
1992. [32] A. Woo, P. Poulin, and A. Fournier. A survey of shadow algorithms. IEEE Computer Graphics 
and Applications, 10(6):13 32, November 1990. [33] H. Zhang. Forward shadow mapping. In Eurographics 
Rendering Workshop 98 proceedings, pages 131 138, 1998.   (a) rayshade (b) layered attenuation maps 
(d) rt with acceleration + sampling 3908 seconds, 88.74 rays on avg. Figure 13: Closeups for the plant 
in Figure 1. The coherence-based raytracing image (d) is almost indistinguishable from that without acceleration 
(c), and both are very close to the image produced by rayshade (a). Our coherence-based method is 8.52 
times faster than the unaccelerated hierarchical raytracer (c). An equal time comparison is provided 
in (e). Note that the times listed are for the entire image, not just the closeups. At the scale of the 
closeup, there are some artifacts for our layered attenuation map approach (b), as indicated by the red 
arrows. However, at normal scales as in Figure 1 these artifacts are less prominent, and are usually 
tolerable for interactive applications.    layered attenuation maps rayshade coherence-based raytracing 
triangles significantly smaller than a pixel. This causes the base hardware renderer for our layered 
attenuation map approach to exhibit serious aliasing artifacts in the "head" of the flower. Nevertheless, 
both methods capture the complex self shadowing effects that cause the bottom of the "head" to be darker 
than the top. Our ground shadows closely match the shadow produced by rayshade. Many other soft shadow 
methods would have significant difficulty in rendering this shadow efficiently and correctly. Note that 
as for the plant we render these images without the cosine falloff of light intensity in order to emphasize 
the shadows. Model courtesy of Deussen et al. layered atten. maps coherence-based rt  rt - 64 correlated 
light samples rt - 64 uncorrelated light samples     
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344958</article_id>
		<sort_key>385</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>43</seq_no>
		<title><![CDATA[Deep shadow maps]]></title>
		<page_from>385</page_from>
		<page_to>392</page_to>
		<doi_number>10.1145/344779.344958</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344958</url>
		<abstract>
			<par><![CDATA[<p>We introduce <italic>deep shadow maps</italic>, a technique that produces fast, high-quality shadows for primitives such as hair, fur, and smoke. Unlike traditional shadow maps, which store a single depth at each pixel, deep shadow maps store a representation of the fractional visibility through a pixel at all possible depths. Deep shadow maps have several advantages. First, they are prefiltered, which allows faster shadow lookups and much smaller memory footprints than regular shadow maps of similar quality. Second, they support shadows from partially transparent surfaces and volumetric objects such as fog. Third, they handle important cases of motion blur at no extra cost. The algorithm is simple to implement and can be added easily to existing renderers as an alternative to ordinary shadow maps.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010395</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image compression</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Reliability</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P283586</person_id>
				<author_profile_id><![CDATA[81100205029]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lokovic]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39084949</person_id>
				<author_profile_id><![CDATA[81100497843]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Veach]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>13027</ref_obj_id>
				<ref_obj_pid>13021</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Paul S. Heckbert. Survey of texture mapping. IEEE Computer Graphics and Applications, 6(11):56-67, November 1986.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74361</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[James T. Kajiya and Timothy L. Kay. Rendering fur with three dimensional textures. In Computer Graphics (SIGGRAPH '89 Proceedings), volume 23, pages 271-280, July 1989.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808594</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[James T. Kajiya and Brian P. Von Herzen. Ray tracing volume densities. In Computer Graphics (SIGGRAPH '84 Proceedings), volume 18, pages 165-174, July 1984.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383834</ref_obj_id>
				<ref_obj_pid>2383815</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Brett Keating and Nelson Max. Shadow penumbras for complex objects by depth-dependent filtering of multi-layer depth images. In Eurographics Rendering Workshop 1999, pages 205-220, New York, June 1999. Springer-Verlag.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>76724</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Marc Levoy. Display of Surfaces from Volume Data. Ph.D. thesis, University of North Carolina at Chapel Hill, 1989.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>275476</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Nelson Max. Hierarchical rendering of trees from precomputed multi-layer Z-buffers. In Eurographics Rendering Workshop 1996, pages 165-174, New York, June 1996.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237265</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Don P. Mitchell. Consequences of stratified sampling in graphics. In SIGGRAPH 96 Proceedings, pages 277-280. Addison Wesley, August 1996.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Darwyn R. Peachey. Texture on demand. Unpublished manuscript, 1990.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37435</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[William T. Reeves, David H. Salesin, and Robert L. Cook. Rendering antialiased shadows with depth maps. In Computer Graphics (SIGGRAPH '87 Proceedings), volume 21, pages 283-291, July 1987.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280882</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Jonathan W. Shade, Steven J. Gortler, Li-wei He, and Richard Szeliski. Layered depth images. In SIGGRAPH 98 Proceedings, pages 231-242. Addison Wesley, July 1998.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807402</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Lance Williams. Casting curved shadows on curved surfaces. Computer Graphics (SIGGRAPH '78 Proceedings), 12(3):270-274, August 1978.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801126</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Lance Williams. Pyramidal parametrics. Computer Graphics (SIGGRAPH '83 Proceedings), 17(3):1-11, July 1983.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617600</ref_obj_id>
				<ref_obj_pid>616014</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Andrew Woo, Pierre Poulin, and Alain Fournier. A survey of shadow algorithms. IEEE Computer Graphics and Applications, 10(6):13-32, November 1990.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344960</article_id>
		<sort_key>393</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>44</seq_no>
		<title><![CDATA[Tangible interaction + graphical interpretation]]></title>
		<subtitle><![CDATA[a new approach to 3D modeling]]></subtitle>
		<page_from>393</page_from>
		<page_to>402</page_to>
		<doi_number>10.1145/344779.344960</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344960</url>
		<abstract>
			<par><![CDATA[<p>Construction toys are a superb medium for geometric models. We argue that such toys, suitably instrumented or sensed, could be the inspiration for a new generation of easy-to-use, tangible modeling systems&#8212;especially if the tangible modeling is combined with graphical-interpretation techniques for enhancing nascent models automatically. The three key technologies needed to realize this idea are embedded computation, vision-based acquisition, and graphical interpretation. We sample these technologies in the context of two novel modeling systems: physical building blocks that self-describe, interpret, and decorate the structures into which they are assembled; and a system for scanning, interpreting, and animating clay figures.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[HCI (Human-Computer Interface)]]></kw>
			<kw><![CDATA[applications]]></kw>
			<kw><![CDATA[embedded computation]]></kw>
			<kw><![CDATA[geometric modeling]]></kw>
			<kw><![CDATA[graphics systems]]></kw>
			<kw><![CDATA[perceptual user interfaces]]></kw>
			<kw><![CDATA[shape recognition]]></kw>
			<kw><![CDATA[tangible user interfaces]]></kw>
			<kw><![CDATA[transmedia]]></kw>
			<kw><![CDATA[user interface hardware]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39027262</person_id>
				<author_profile_id><![CDATA[81100103115]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Anderson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MERL, Mitsubishi Electric Research Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P132239</person_id>
				<author_profile_id><![CDATA[81100482649]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Frankel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MERL, Mitsubishi Electric Research Laboratory and Frankel and Associates, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39045048</person_id>
				<author_profile_id><![CDATA[81100484340]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Joe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marks]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MERL, Mitsubishi Electric Research Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40022963</person_id>
				<author_profile_id><![CDATA[81100035467]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Aseem]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Agarwala]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MERL, Mitsubishi Electric Research Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P220909</person_id>
				<author_profile_id><![CDATA[81100489358]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Beardsley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MERL, Mitsubishi Electric Research Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39024867</person_id>
				<author_profile_id><![CDATA[81100049661]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Jessica]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hodgins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14171919</person_id>
				<author_profile_id><![CDATA[81100491228]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Darren]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Leigh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MERL, Mitsubishi Electric Research Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39078157</person_id>
				<author_profile_id><![CDATA[81100236954]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Kathy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ryall]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Virginia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P74643</person_id>
				<author_profile_id><![CDATA[81100394790]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Eddie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sullivan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MERL, Mitsubishi Electric Research Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P147535</person_id>
				<author_profile_id><![CDATA[81100305912]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>10</seq_no>
				<first_name><![CDATA[Jonathan]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Yedidia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MERL, Mitsubishi Electric Research Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[R. Aish. 3D input for CAAD systems. Computer-Aided Design, 11(2):66-70, Mar. 1979.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[R. Aish and P. Noakes. Architecture without numbers - CAAD based on a 3D modelling system. Computer-Aided Design, 16(6):321-328, Nov. 1984.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[G. Anagnostou, D. Dewey, and A. Patera. Geometry-defining processors for engineering design and analysis. The Visual Computer, 5:304-315, 1989.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[B. G. Baumgart. Geometric modeling for computer vision. Technical Report AIM-249, AI Laboratory, Stanford Univ., Oct. 1974.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[R. Brooks. Model-based 3D interpretations of 2D images. IEEE Trans. on Pattern Analysis and Machine Intelligence, 5(2):140-150, 1983.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[G. Cybenko, A. Bhasin, and K. D. Cohen. Pattern recognition of 3D CAD objects: Towards an electronic Yellow Pages of mechanical parts. Smart Engineering Systems Design, 1:1- 13, 1997.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>60359</ref_obj_id>
				<ref_obj_pid>60356</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[D. Dewey and A. Patera. Geometry-defining processors for partial differential equations. In B. Alder, editor, Special Purpose Computers, pages 67-96. Academic Press, 1988.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>137720</ref_obj_id>
				<ref_obj_pid>137716</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[S. Dickinson, A. Pentland, and A. Rosenfeld. From volumes to views: An approach to 3D object recognition. Computer Vision, Graphics, and Image Processing: Image Understanding, 55(2):130-154, 1992.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>206442</ref_obj_id>
				<ref_obj_pid>206436</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[S. J. Dickinson and D. Metaxas. Integrating qualitative and quantitative shape recovery. Intl. Journal of Computer Vision, 13(3):311-330, 1994.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>199424</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[C. Esposito, W. B. Paley, and J. Ong. Of mice and monkeys: A specialized input device for virtual body animation. In Proc. of Symposium on Interactive 3D Graphics, pages 109-114, 213, Monterey, California, Apr. 1995.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[A. W. Fitzgibbon, G. Cross, and A. Zisserman. Automatic 3D model construction for turn-table sequences. In Proc. of SMILE Workshop, Freiburg, Germany, June 1998.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[J. Frazer. Use of simplified three-dimensional computer input devices to encourage public participation in design. In Proc. of Computer Aided Design 82, pages 143-151. Butterworth Scientific, 1982.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[J. Frazer. An Evolutionary Architecture. Architectural Association, London, 1994. Describes several tangible modelers developed by Frazer's group from 1979 onwards.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[J. Frazer, P. Coates, and J. Frazer. Software and hardware approaches to improving the man-machine interface. In Proc. of the First International IFIP Conf. on Computer Applications in Production and Engineering, pages 1083-94, Amsterdam, Holland, 1983. North Holland.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[J. Frazer, J. Frazer, and P. Frazer. Intelligent physical threedimensional modelling system. In Proc. of Computer Graphics 80, pages 359-370. Online Publications, 1980.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[J. Frazer, J. Frazer, and P. Frazer. New developments in intelligent modelling. In Proc. of Computer Graphics 81, pages 139-154. Online Publications, 1981.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>288152</ref_obj_id>
				<ref_obj_pid>288126</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[N. Gagvani, D. Kenchammana-HoseKote, and D. Silver. Volume animation using the skeleton tree. In Proc. of the IEEE Symposium on Volume Visualization, pages 47-53, Research Triangle Park, NC, Oct. 1998.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>263592</ref_obj_id>
				<ref_obj_pid>263552</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[M. G. Gorbet and M. Orth. Triangles: Design of a physical/digital construction kit. In Proc. of DIS 97, pages 125- 128, Amsterdam, Holland, Mar. 1997. ACM.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>274652</ref_obj_id>
				<ref_obj_pid>274644</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[M.G. Gorbet, M. Orth, and H. Ishii. Triangles: Tangible interface for manipulation and exploration of digital information topography. In Proc. of CHI 98, pages 49-56, Los Angeles, California, Apr. 1998. ACM.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258822</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[J. K. Hodgins and N. S. Pollard. Adapting simulated behaviors for new characters. In Proc. of SIGGRAPH 97, pages 153-162, Los Angeles, California, Aug. 1997.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311602</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[T. Igarashi, S. Matsuoka, and H. Tanaka. Teddy: A sketching interface for 3D freeform design. In Proc. of SIGGRAPH 99, pages 409-416, Los Angeles, California, Aug. 1999.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[S. B. Kang. Quasi-euclidean recovery from unknown but complete orbital motion. Technical Report TR 97-10, Compaq CRL, 1997.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[F. Martin and R. Borovoy. The active LEGO baseplate project, http : / / fredm, www. media .mit. - edu/people/fredm/projects/ab/, 1994.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Ovid. Metamorphoses:X. Rome, 1 AD.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218495</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[R. Pausch, T. Burnette, D. Brockway, and M. E. Weiblen. Navigation and locomotion in virtual worlds via flight into hand-held miniatures. In Proc. of SIGGRAPH 95, pages 399- 400, Los Angeles, California, Aug. 1995.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling. Numerical Recipes in C. Cambridge University Press, 1988.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>6152</ref_obj_id>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[M. H. Raibert. Legged Robots That Balance. MIT Press, Cambridge, 1986.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>289912</ref_obj_id>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[J. C. Russ. The Image Processing Handbook. CRC Press, 1998.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>339363</ref_obj_id>
				<ref_obj_pid>339355</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[S.M. Seitz and C. R. Dyer. Photorealistic scene reconstruction by voxel coloring. International Journal of Computer Vision, 35(2):151-173, 1999.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>80997</ref_obj_id>
				<ref_obj_pid>80983</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[F. Solina and R. Bajcsy. Recovery of parametric models from range images: The case for superquadrics with global deformations. IEEE Trans. on Pattern Analysis and Machine Intelligence, 12(2):131-146, 1990.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[H. Suzuki and H. Kato. AlgoBlock: A tangible programming language -- a tool for collaborative learning. In Proc. of the 4th European Logo Conference, pages 297-303, 1993.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[D. Terzopoulos and K. Fleischer. Deformable models. The Visual Computer, 4(6):306-331, December 1988.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[P. H. Winston. Learning structural descriptions from examples. In P. H. Winston, editor, The Psychology of Computer Vision. McGraw-Hill, New York, 1975.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237238</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[R. C. Zeleznik, K. Herndon, and J. F. Hughes. SKETCH: An interface for sketching 3D scenes. In Proc. of SIGGRAPH 96, pages 163-170, New Orleans, Louisiana, Aug. 1996.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Tangible Interaction + Graphical Interpretation: A New Approach to 3D Modeling David Andersonl, James 
L. Frankell,4, Joe Marksl, Aseem Agarwalal, Paul Beardsleyl, Jessica Hodgins2, Darren Leighl, Kathy Ryall3, 
Eddie Sullivanl, Jonathan S. Yedidial  lMERL Mitsubishi Electric Research Laboratory 2Georgia Institute 
of Technology 3University of Virginia 4Frankel and Associates, Inc. Contact: marks@merl.com Abstract 
Construction toys are a superb medium for creating geometric mod­els. We argue that such toys, suitably 
instrumented or sensed, could be the inspiration for a new generation of easy-to-use, tan­gible modeling 
systems especially if the tangible modeling is combined with graphical-interpretation techniques for 
enhancing nascent models automatically. The three key technologies needed to realize this idea are embedded 
computation, vision-based acqui­sition, and graphical interpretation. We sample these technologies in 
the context of two novel modeling systems: physical building blocks that self-describe, interpret, and 
decorate the structures into which they are assembled; and a system for scanning, interpreting, and animating 
clay .gures. CR Categories: I.3.6 [Computer Graphics]: Methodology and Techniques interaction techniques. 
Keywords: Applications, Geometric Modeling, Graphics Sys­tems, HCI (Human-Computer Interface), Shape 
Recognition, User Interface Hardware. Additional Keywords: Embedded Computation, Tangible User In­terfaces, 
Perceptual User Interfaces, Transmedia. 1 Introduction Artists using standard 3D modeling packages must 
specify pre­cisely the geometric and material properties of the models they create, and therein lies 
much of the complexity and tedium of us­ing those tools. By contrast, children playing with construction 
toys like LegoTM and K nexTM make simple models easily, and use their imaginations to .ll in the details. 
We would like to transform computer-based geometric modeling into that same kind of play­ful, tactile 
experience but without sacri.cing the ability to create the interesting geometric detail and movement 
that make 3D graph­ics and animation compelling. To retain the tactile experience of model manipulation, 
we look to tangible-interface technology; and to create detailed, fully realized models, we use new methods 
for graphically interpreting a nascent model by recognizing and aug­menting its salient features. Permission 
to make digital or hard copies of part or all of this work or personal or classroom use is granted without 
fee provided that copies are not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on 
servers, or to redistribute to lists, requires prior specific permission and/or a fee. SIGGRAPH 2000, 
New Orleans, LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 This combination of tangible interaction 
and graphical interpre­tation is investigated in a pair of case studies. Tangible modeling can be approached 
in two ways: either by directly instrumenting the modeling medium with embedded computation or by using 
ex­ternal sensors to capture the geometry. Our .rst system consists of computational building blocks 
assembled into physical structures that in their aggregate determine and communicate their own geo­metric 
arrangement. A rule-based system interprets these structures as buildings, parses their architectural 
features, then adds geometric detail and decorative enhancements (Figures 1 and 4). Our second system 
uses simple and robust computer vision to capture volumet­ric scans of clay models of such common toy-like 
objects as peo­ple, animals, trees, houses, cars, and boats. A volumetric matching algorithm allows us 
to recognize, interpret, and animate the clay models (Figure 2).  2 Computational Building Blocks The 
vision of a tangible 3D geometric modeling system that uses building blocks with embedded computation 
has been pursued by several groups over the past 20 years. Research on this topic began with the pioneering 
projects of Aish [1, 2] and of Frazer [15, 16, 12, 14, 13], and was renewed more recently by Dewey and 
Patera [7, 3].1 All of these systems take advantage of the idea that completely and robustly determining 
the geometry of a tangible model fol­lows naturally if the model is built from rigidly connected building 
blocks of known size and shape. Recovering 3D geometry is then reduced to the problem of determining 
the identity and connectivity of the blocks and communicating that information to a host com­puter. However, 
these systems differ signi.cantly in the details of their design and implementation. A broad range of 
solutions have been tried for these fundamental engineering problems: .How do blocks connect? Blocks 
that can stack only verti­cally have a low constructive versatility relative to, say, LegoTM blocks (a 
pair of standard 2.4LegoTM blocks can connect in 184 different con.gurations). Simple, symmetrical connectors 
are the key to achieving high constructive versatil­ity. .How are blocks powered? Self-powered blocks 
allow use of simpler connectors but increase the cost, maintenance,  1The AlgoBlock [31] and Triangles 
[18, 19] systems are similar in ar­chitecture to the tangible modelers cited above. However, their target 
ap­plication is visual/tangible programming, not geometric modeling; and both systems enable the general 
description of 2D structure only, not 3D. Two other tangible modeling systems deserve mention. The Active 
LegoTM Baseplate Project at MIT [23] addressed the issue of 3D geometric modeling, but it was only a 
paper design and was never implemented. The MonkeyTM is a posable articulated linkage that is used for 
keyframing and performance capture [10]; it is a successful product.  (a) (b) (c) (d) (e) Figure 1: 
(a) a physical block structure comprising 98 blocks; (b) a close-up of the blocks; (c) a bottom view 
of the circuit board inside each block; and renderings of the virtual model recovered from the structure, 
one literal (d) and one interpreted (e). The literal rendering uses associated shapes and colors to render 
the blocks. The virtual model is augmented automatically for the interpreted rendering. (a) (b) (c) 
(d) (e) Figure 2: (a) a clay model; (b) its volumetric scan, computed from silhouette information; (c) 
the best match for it from a small library of object templates; (d) the constituent parts of the interpreted 
virtual model; and (e) a frame from an automatically generated animation of the virtual model running. 
and complexity of the individual blocks. Externally pow­ered blocks require connectors that handle the 
distribution of power. .How do blocks communicate? The earliest systems used sim­ple electronics to create 
a circuit-switched network in a block structure. Recent systems have used a microcontroller in each block, 
and a variety of message-passing architectures for com­munication. .How is geometry computed? There 
are two general strate­gies for computing connectivity, and thereby geometry. At one extreme the connectivity 
computation can be controlled centrally by the host computer; at the other extreme, it can be organized 
as a distributed computation among the computing elements in the blocks.  The system we have developed 
is just one point in a large space spanned by the dimensions of connection, communication, and computation, 
but it illustrates well the various design and engi­neering issues involved. Its distinctive characteristics 
include the following: .Very simple physical/electrical connection: We have based the physical design 
of our blocks on the popular LegoTM block. Although this choice achieves much greater constructive versatility 
than any previous system, it comes at the price of extremely simple connectors. Our standard block has 
eight plugs on the top, and eight jacks on the bottom. The plugs and jacks have only two conductors each, 
one for power distribution and one for bidirectional signals. .Asynchronous, distributed communication: 
These simple connectors make it impossible to have a common bus link­ing all our blocks (in addition 
to point-to-point connections). The software of many previous systems was simpli.ed by us­ing such a 
bus for global synchronization and communica­tion. All communication in our block structures is based 
on asynchronous message passing between physically connected blocks. .Parallel, distributed computation 
of structure: Our design goal was to build self-describing structures of up to 500 blocks. To complete 
the distributed computation of structure for a 500-block model in a reasonable time we had to ex­ploit 
parallelism, which further complicated an already com­plicated distributed computation. .Automatic detailing: 
A modeling system that makes it easy to create coarse models will be of limited use if the re.nement 
of the models requires learning a complex user interface such as that found in today s animation and 
CAD systems. Au­tomatic detailing based on graphical interpretation can make block structures look less 
blocky and more interesting. As an illustration of what is possible, our system interprets a block structure 
as a building; it identi.es walls, roofs, windows, doors, and other features, and augments them with 
additional geometric and surface detail in various styles to produce more interesting models. 2.1 System 
description For economy and ease of development, we used mostly off­the-shelf components in the construction 
of our building-block prototypes (Figure 1(b)). A block consists of a 100mm (L) . 50mm (W) .25mm (H) 
plastic box that is drilled to accommodate slightly modi.ed DC power connectors. Eight plugs are on top 
of the block, and eight jacks are on the bottom. The dimensions of the box and the locations of the plugs 
and jacks are such that our building blocks .t together like LegoTM blocks. Each connector has just two 
conductors. However, instead of using one for power and one for ground, we use the inner pin as a signal 
line for bidirectional communication, and the outer sleeve for power distribution. Each block is wired 
internally so that connec­tors with power and ground on their outer sleeves, respectively, are arranged 
in an alternating pattern, as shown in Figure 1(c). The po­larity of the connector sleeves marked X is 
different from that of the sleeves marked O. Thus each block has at least one connection to power and 
one to ground in any typical LegoTM-block structure, i.e., one in which no block is connected at only 
one corner or at only diagonally opposite corners. There is no way to tell a priori which connector sleeves 
have power or ground, but this problem is solved by the use of a full-wave bridge recti.er. We chose 
the PIC16F877 microcontroller as the best single-chip solution to the various design problems posed by 
our application. Its features include: a relatively large number of I/O pins (16 are required for communication, 
and it has 33); compact size; low­power, high-speed (20 Mhz) CMOS technology; an 8-bit RISC CPU; 8K .14-bit 
words of FLASH program memory; 368.8­bit bytes of data memory (RAM); 256.8-bit bytes of EEPROM data memory; 
a hardware Universal Synchronous Asynchronous Receiver Transmitter (USART) that we use for debugging; 
and in­terrupt handling. The program and data in each block s microcon­troller are identical except for 
a unique ID number. We left several pads in the periphery of our custom circuit board to accommodate 
such additional transducers and sensors inside a block as speakers, and proximity and touch detectors. 
Alternatively, the board can be trimmed to .t inside a 2.2building block without affecting its basic 
functionality. 2.2 Geometry determination A fully assembled block structure computes its own geometry 
in three phases. When a block is powered on, it immediately en­ters Phase 1 of the geometry-determination 
algorithm. Lacking a global communication bus, the switching on of electrical power is the only source 
of synchronization, which is necessarily approxi­mate because of small delays in power propagation throughout 
a block structure. All 16 signal lines in a block are normally held high by pull-up resistors. Phase 
1 begins with each block pulling its top signal lines (those in the plugs) low. Each block then tests 
its bottom signal lines (those in the jacks) to determine and record which of them have been pulled low 
by some other block. After a short delay to ensure that the approximately synchronized blocks do not 
try to drive shared signal lines simultaneously in both directions, this test is then repeated with the 
roles of top and bottom lines reversed. Thus when Phase 1 is complete, each block has identi.ed in parallel 
which of its lines are connected, i.e., are attached to other blocks, but it does not know the identity 
of these neighboring blocks. After another short delay, each block enters Phase 2 of the algo­rithm during 
which blocks communicate with their neighbors over the connected lines found in Phase 1. At the start 
of Phase 2, each block listens on its connected bottom lines for transmitted pack­ets that contain the 
ID of the transmitting block and the number of the connector over which it is transmitting.2 The receiving 
block records this information with its own ID number and the number of the connector over which it received 
the transmission. The com­bined data form a complete record of a single connection between two blocks. 
When a block has successfully received a transmission on all of its connected bottom lines, it begins 
transmitting on its connected top lines, iterating through them in order. Connectivity information, therefore, 
.ows initially through the block structure from bottom to top, with the potential for signi.cant parallel 
com­munication. After a block has completed the .rst half of Phase 2, it knows to which connector of 
which block each of its own bottom connec­tors is attached. During the second half of Phase 2, the procedure 
is repeated with blocks listening on their top connected lines and transmitting on their bottom connected 
lines. Thus, at the end of Phase 2, each block has acquired and recorded in its database com­plete knowledge 
about all of its connected lines: the IDs of the connected pair of blocks and the connector numbers by 
which they are attached. Each connected line that is processed successfully in Phase 2 is termed valid. 
In Phase 3, the connectivity information determined in Phase 2 is communicated to the host computer through 
the drain, a special block that runs slightly different software and has a serial connec­tion to the 
host computer. In addition to mediating communica­tion between a block structure and the host, the drain 
also supplies power to the blocks and may be attached to any part of the struc­ture. During Phase 3 all 
blocks listen for messages on all of their valid lines. When a request-to-drain message is received, 
a block transmits packets containing all of its connectivity information on its drain connector, the 
one from which it received the request-to­drain message. When the block has successfully completed these 
transmissions, it forwards the request-to-drain message on the .rst of its valid lines, and enters a 
message-forwarding mode. If it re­ceives a packet containing connectivity information, it stores and 
forwards it on its drain connector; if it receives subsequent request­to-drain messages, it responds 
with an already-drained message; when it receives an already-drained or done message, it forwards the 
request-to-drain message on its next valid line or sends a done message on its drain connector when all 
its valid lines have been processed. The .rst request-to-drain message is injected into the structure 
by the drain, and permission to drain then percolates through the block structure in a preorder traversal 
of the blocks. Although this traversal is performed sequentially only one block has permission to drain 
at any point in time the forwarding of messages towards the drain is pipelined, thereby achieving some 
parallelism in this phase as well. At the end of Phase 3 the host computer should have complete connectivity 
information for the block structure. (In fact, it should have redundant information because each connectivity 
datum is re­ported twice, once by each of the two blocks involved. This re­ 2Transmitted packets may 
be missed if the receiving microcontroller is busy when transmission commences. Noise may also corrupt 
a message. Therefore all packets transmitted in Phases 2 and 3 have checksums and are acknowledged, and 
faulty transmissions are retried after an appropriate timeout. dundancy contributes to the robustness 
of the system, but it can be eliminated for greater ef.ciency.) The host also has shape data for each 
block, indexed by ID. These data are recorded when a block is programmed. A straightforward recursive 
procedure ought now to give the 3D structure of the block structure, which can then be used to produce 
a geometric scene description suitable as input to a va­riety of common 3D graphics applications. However, 
occasionally the host does not obtain complete information: due to mechanical stresses in the structure, 
some connections fail (about 0.5% of them on average) such that their connectivity data are not acquired. 
Our geometry-recovery procedure therefore determines the most likely block structure given potentially 
incomplete connectivity data. The block structure in Figure 1(a) contains 98 blocks. The time required 
for this structure to compute its own geometry is 35 sec­onds. The structure in Figure 4(c) contains 
560 blocks and requires 53 minutes for geometry determination. Almost all of the time is spent in Phase 
3 of the algorithm. 2.3 Graphical interpretation For a literal rendering of the block structure, the 
host uses associ­ated values for the rendering attributes of each block, such as shape, color, and texture. 
Figure 1(d) shows a literal rendering of the block structure in Figure 1(a); color and shape values have 
been chosen to mimic the physical blocks. Examples of graphical interpretations are shown in Figures 
1(e) and 4(b) and (d). To produce these enhanced renderings, our system generates a description of a 
block structure as a set of logical ax­ioms, one to assert the existence and location of each block. 
These axioms serve as input to a logic program written in Prolog that can identify architectural features 
of a block structure. For example, the rules in Figure 3 compute which blocks constitute the walls and 
roof of a structure, interpreted as a building.3 Recognized structural ele­ments can be decorated with 
additional geometry and surface detail to enhance the visual appearance of the rendered model. To validate 
the ability of the computer to generate interpreted renderings, we handcrafted a few distinct styles 
that can be applied to block structures automatically. For a fully realized application, we would develop 
more interactive user interfaces for customizing and applying these interpretive styles. We return to 
this point in the concluding section of the paper.  3Clay Using external sensors to capture geometry 
is the alternative tech­nology for supporting tangible modeling. For our second case study, we were inspired 
by the ancient myth of Pygmalion, whose sculpture of a woman was brought to life by Venus [24]. We set 
ourselves the goal of bringing clay models to life automatically. Al­though any practical application 
would divide this task more evenly between user and computer, we tried to fully automate the system in 
order to explore the limits of the technology, just as we did in our previous case study. In the following 
subsections we present the details of the hardware and software used to scan, recognize, interpret, and 
animate 3D clay models. 3In the early 70 s Winston developed a landmark program that could learn about 
simple architectural structures from positive and negative ex­amples of those structures [33]. However, 
the robust recognition of the im­portant structural elements in our block models requires hand-crafted 
rules of much greater complexity than those that could be learned by Winston s approach. % wall/1 .nds 
sets of blocks that form walls. A wall is de.ned to be % a contiguous set of blocks that lie .ush against 
some vertical plane, % and that constitute a given fraction of the structure. wall(WALL BLOCKS) :­ structure 
bbox(X MIN, X MAX, , ,Z MIN, Z MAX), candidate planes(X MIN, X MAX, Z MIN, Z MAX, U, V, W, R), lies .ush 
against(U, V, W, R, PLANE BLOCKS), contiguous subsets(PLANE BLOCKS, PLANE BLOCKS SUBSETS), member(WALL 
BLOCKS, PLANE BLOCKS SUBSETS), big enough(WALL BLOCKS). %wall tops/1 .nds the blocks that are the tops 
of walls. wall tops(WALL TOPS) :-setof(BLOCK, WALL BLOCKS (wall(WALL BLOCKS), member(BLOCK, WALL BLOCKS), 
not overhung(BLOCK, WALL BLOCKS)), WALL TOPS). % roof blocks/1 computes the set of blocks make up the 
roof, which is % de.ned to be those blocks that rest directly or indirectly on the tops of % walls. The 
indirectly resting blocks are computed by grow roof/2. roof blocks(ROOF BLOCKS) :­ .ndall(BLOCK1, (wall 
tops(WT BLOCKS), member(WT BLOCK, WT BLOCKS), on top of(BLOCK1, WT BLOCK)), BASE BLOCKS BAG), setof(BLOCK2, 
member(BLOCK2, BASE BLOCKS BAG), BASE BLOCKS), grow roof(BASE BLOCKS, ROOF BLOCKS). grow roof(NASCENT 
ROOF BLOCKS, FINAL ROOF BLOCKS) :- member(BLOCK1, NASCENT ROOF BLOCKS), on top of(BLOCK2, BLOCK1), not 
member(BLOCK2, NASCENT ROOF BLOCKS), grow roof([BLOCK2 . NASCENT ROOF BLOCKS], FINAL ROOF BLOCKS), !. 
grow roof(ROOF BLOCKS, ROOF BLOCKS). Figure 3: Recognizing the structural elements of a block structure 
by logic programming. 3.1 System description and geometry determination Our scanning system consists 
of a motorized rotary table, a consumer-quality digital camera, a laser striper (optional), and a host 
computer (Figure 5). The camera is calibrated from an image of an object of known dimensions. The clay 
model to be scanned is placed upright and face forward on the rotary table. (It is con­venient for the 
matching process to have models placed in a known orientation. Inferring orientation automatically is 
certainly feasible but seems unnecessary for modeling applications that involve co­operative users.) 
The camera captures an image sequence of the model as it rotates, and a volumetric scan is generated 
from silhou­ettes [4]. This approach worked well on the majority of models we scanned, but when signi.cant 
concavities were present (e.g., the door and windows of the house in Figure 8), the laser striper could 
be used to re.ne the shape of the model.4 The use of silhou­ettes and laser striping is well suited to 
our smooth-surfaced, single­color clay models; however, systems that rely on surface-color vari­ation 
[29] or uncalibrated systems that require a signi.cant number 4Affordable 3D scanning systems that operate 
on the same principles as our laboratory system are now available commercially from Geometrix, Sanyo, 
and others.                                      (a) (b) (c) (d) 
Figure 4: (a) a model of a castle comprising 118 blocks, and (b) an interpreted rendering of it. The 
automatic enhancements in this graphical interpretation include the addition of turrets, roofs, windows, 
archways, a portcullis, and a .agpole in appropriate locations, as well as the selection of suitable 
surface properties and features for all the geometry. The 560-block model in (c) a 12-inch ruler is included 
to show scale was built as a challenging virtual environment for Quake II, the data format for which 
is another output option in our system. Applying the same interpretive style to this larger model to 
get the rendering in (d) requires changing only one numerical parameter indicative of building scale: 
it speci.es the smallest number of blocks in the structure that can constitute a distinct architectural 
feature. of point matches between surface features to be visible in the image sequence [11, 22] would 
likely encounter dif.culty. 3.2 Graphical interpretation The technical novelty in our system lies in 
our approach to model recognition and interpretation, both of which are accomplished by comparing a set 
of parameterized object templates to a scanned clay model. The templates are deformed to match the model, 
and the matching score determines how the model is classi.ed.5 5Brooks ACRONYM system [5] is an early 
example of the use of pa­rameterized models (generalized cylinders) for object recognition. Solina et 
al. [30] describe how to recover parametric models (superquadrics with global deformations) from range 
data by minimizing surface distances, which is similar to our maximization of volumetric overlap. Surveys 
of The templates are articulated linkages in which the links are trun­cated rectangular pyramids, or 
beams. A beam is completely de­.ned by 10 numbers that specify the positions and dimensions of two parallel 
base rectangles. By requiring that the bases always be orthogonal to some major axis, a beam can be ef.ciently 
raster­ized in three dimensions, which is important for the ef.ciency of the matching algorithm described 
below. A linkage is formed by connecting beams at the corners or centers of any of their six faces. Figure 
6 shows representative templates for the 13 object categories currently recognized by our system. For 
each category there are 10 actual templates, created by hand, that differ only in the relative other 
related work on shape recovery and object recognition from image and range data can be found in more 
recent papers, e.g., [8, 9]. Finally, an application closely related to ours, shape-based indexing and 
retrieval of mechanical objects, is discussed in [6]. Figure 5: A low-cost scanning system. proportions 
and orientations of their constituent beams: they pro­vide multiple starting points for the matching 
algorithm, thereby reducing its chance of being confounded by matches that are lo­cally optimal but globally 
inferior. A complete set of templates for a single category is shown in Figure 7. Each biped template 
comprises six beams. Hence there would seem to be 6.10=60parameters available for deforming a tem­plate. 
However, many of the parameters are constrained relative to each other. Some of these constraints derive 
implicitly from the enforced connectedness of beams in an articulated linkage. Others are the result 
of explicit programmer-speci.ed constraints that ap­ply to beam-size parameters, e.g., a constraint that 
the dimensions of the base rectangles for both arm beams be the same. When these constraints are applied, 
the number of free parameters for a biped template reduces to a more manageable 25. By modifying these 
parameters the computer can attempt to deform a template to best match a given clay model. Quantifying 
the notion of a best match in an objective function is the essential problem in this optimization-based 
approach. Our objective function has terms for the following characteristics of a voxel-based match: 
.Superposition: Each voxel occupied by both the rasterized object template and the scanned clay model 
contributes +1to the objective-function score. .Excess: This term penalizes voxels occupied by the raster­ized 
template but not by the clay model. A simple approach would be to add a score of .1for each such voxel. 
A much better idea is to add a score of .r,where ris the distance to the nearest occupied voxel in the 
clay model. This value can be computed for each voxel by an ef.cient two-pass al­gorithm [28]. The advantage 
of this distance-based penalty is that its gradient still gives useful information even when there is 
minimal overlap between the template and clay model. .Deformation: Without some penalty for excessive 
deforma­tion, templates can sometimes achieve excellent superposition and excess scores through absurd 
contortions. Deformation beyond a certain threshold is therefore penalized by an ex­ponential function 
of the distance between the original and current parameter vectors. Dividing the superposition and excess 
terms by the number of occu­pied voxels in the scanned volume normalizes for the volume of the clay model; 
dividing the deformation term by the number of beam vertices normalizes for the complexity of the object 
template. Given this objective function, the matching algorithm is straight­forward. First, the object 
template is adjusted for .t: the template is scaled in all three dimensions so that the beam endpoints 
lie just within the bounding box of the scanned model. (There is no need to normalize for orientation 
because we assume that scanned mod­els have been placed upright and facing forward.) Matching is then 
just a matter of gradient descent using the negative of the objective function above. We use the conjugate-gradient 
method of gradi­ent descent [26], approximating partial derivatives of the objective function by central 
differences. For best results we run the gradient­descent algorithm to quiescence three times in succession: 
.rst, we vary only the location parameters of the object template, then only the size parameters, and 
.nally the location parameters again. We also schedule the relative weights of the objective-function 
terms over the three runs; the superposition and excess terms decrease in signi.cance, and the deformation 
term increases. Good values for the weights were determined empirically for a small subset of the clay 
models, and then applied uniformly to all the models in our experiments. Matching is performed against 
a total of 130 object templates, 10 from each of the 13 categories shown in Figure 6. Figure 8 shows 
the 16 clay models on which we tested our sys­tem. Each model is from one of the 13 object categories 
listed in Figure 6, with some duplication. These categories were based on the objects that .gured most 
often in an informal survey of chil­dren s drawings. The artists who created the models worked inde­pendently 
of the programmer who fashioned the object templates, so that sometimes there are signi.cant differences 
in the artists and programmer s conceptions of a modeled object, e.g., compare the clay model of the 
Insect in Figure 8 with the corresponding object template in Figure 6. Each volumetric scan of a clay 
model was computed from 180 images, taken a uniform 2Æapart. An addi­tional 180 images were taken with 
the laser stripe on, though this additional data improved the scan signi.cantly for only one of the reported 
models (the indented windows and door of the house were found). The scanned volumes were subsampled to 
a resolution of 128.128.128voxels for the purposes of matching. Table 1 lists the top two matches for 
each clay model; Figure 9 il­lustrates the best matches graphically. Matching a single clay model against 
all 130 object templates took an average of 85 minutes on a 200 MHz Pentium Pro PC, and required rasterizing 
about 100,000 object templates; the bulk of the time was spent in the rasterization step. The top match 
was correct for 14 of the 16 clay models. An examination of the two matching errors was instructive. 
Al­though the Insect template deformed to cover the Insect model al­most perfectly, the degree of deformation 
was suf.cient to result in the Quadruped and Chair templates receiving better matching scores. Reducing 
the deformation penalty would cause the Insect template to match best, but would also cause many incorrect 
best matches for the other clay models. The failure of the Car template to be the best match for the 
Car #1 model is due to a limitation of our modeling language for articulated linkages: one beam can at­tach 
to another only at the four corners or center of one of its faces. The offset of Car #1 s wheels are 
such that the Car template cannot deform to cover them very well with its wheel beams constrained to 
attach as they do. When the best-matching template has been found for a given clay model, an interpretation 
step parses the model into its con­stituent parts. For example, if a model is recognized as a biped, 
the match between the clay model and the deformed biped tem­plate is used to identify the model voxels 
that constitute the head, arms, legs, and torso. This voxel classi.cation is based on the short­est distance 
from each voxel to each beam through clay-occupied space. Voxels are then assigned to their closest beams, 
with ties broken by distance to the beams center axes. Once the best match is known, parsing takes about 
a minute. The 14 correctly matched models were all parsed acceptably well. Sample parses are shown  
House Bicycle Bird Bridge Insect Chair Tree Rocket Table Boat Biped Car Quadruped Figure 6: Representative 
templates from 13 categories of toy-like objects.  Figure 7: The 10 templates for the Biped category. 
in Figure 2(d) and Figure 10. With this information we can bring a clay model to (virtual) life. We did 
this automatically for the quadruped shown in Figure 2. The body measurements, masses, and moments of 
inertia were com­puted from the parse of the clay model. These values were passed as input to a control 
and simulation system for a four-legged robot, which adapted an existing control system to the dynamics 
of this particular clay model [20, 27]. The motion data computed by the simulation were then used to 
animate the object template, which in turn was used to animate the scanned volume by moving its vox­els 
in rough concert with the beams of the template to which they were assigned in the parsing phase. However, 
care must be taken to avoid the introduction of tears and cracks in the model as it de­forms. Such unwanted 
artifacts will appear if each voxel maintains position relative to just its associated beam. It is better 
to have all beams in.uence the movement of a voxel in inverse proportion to the square of their distance 
from the voxel; this reduces tears at the junctions of different model regions. (See Figure 2 and the 
com­panion videotape.) Related work on animating volumetric objects is described in [17]. An alternative 
and more general way to bring these models to life is with keyframes speci.ed by the user. Commercial 
animation packages are notoriously complex because of the large number of features that must be provided. 
However, these clay models have been parsed by the computer and, therefore, the internal skeletal structure 
and kinematics are already known. The skeleton should allow the construction of an intuitive animation 
interface for this speci.c character. The user need only specify the motion of the skeleton because the 
internal details of the motion of the clay can be computed automatically using heuristic algorithms as 
was done for the quadruped or a more general physical simulation of clay [32]. Examples of this approach 
are shown on the companion video.  4 Conclusions and future work In our case studies we have investigated 
the combination of tan­gible modeling and graphical interpretation. Tangible modeling  House Bicycle 
Bird Bridge Insect Chair Tree Rocket Table Boat Biped #1 Biped #2 Car #1 Car #2 Quadruped #1 Quadruped 
#2 Figure 8: Examples from the image sequences for the 16 clay models captured by the camera illustrated 
in Figure 5. House Bicycle Bird Bridge Chair Tree Rocket Table Boat Biped #1 Biped #2 Car #2 Quadruped 
#1 Quadruped #2 Figure 9: Best matches illustrated: deformed object templates superimposed on the scanned 
volumetric models.  Bird Car #2 Chair Table Biped #2 Figure 10: Sample parses of .ve clay models. The 
colored regions correspond to speci.c beams in the corresponding best-matching object template. Object 
Templates Clay Models Ranked by Score First Second House House(105) Car(77) Bicycle Bicycle(87) Boat(47) 
Bird Bird(79) Insect(76) Bridge Bridge(115) Insect(104) Insect Q ped(54) Chair(50) Chair Chair(78) Bird(56) 
Tree Tree(82) Biped(71) Rocket Rocket(89) Q ped(87) Table Table(107) Car(102) Boat Boat(92) Bicycle(31) 
Biped #1 Biped(82) Q ped(67) Biped #2 Biped(98) Q ped(76) Car #1 Boat(105) Car(80) Car #2 Car(115) Chair(92) 
Q ped #1 Q ped(86) Insect(55) Q ped #2 Q ped(113) Bicycle(71)  Table 1: The best-matching object templates 
for all the clay models, along with the matching scores. Of the 16 models, 14 were matched correctly; 
the entries for the two erroneous matches are shown in italics. makes it easy for the user to create 
simple geometry quickly; the computer performs the tedious detailing tasks via graphical inter­pretation. 
Combined, these two ideas make for a new, more ac­cessible approach to 3D modeling. As with recent sketch-based 
systems [34, 21], some generality is sacri.ced in return for dra­matically simpler interfaces that are 
accessible to unskilled and un­trained users. However, the modeling systems we have developed are only 
re­search prototypes. To make a truly useful and affordable system, more investigation is required. Our 
current work and future plans include: .Alternative architectures for embedded computation: The most 
signi.cant practical problem with our computational building blocks is that of power supply: the 560-block 
model in Figure 4(c) required a peak current of 8 amps at 13.8 volts to determine its geometry and to 
illustrate algorith­mic progress via the blocks LEDs. To reduce the power re­quirements (and thereby 
the cost) of the blocks, we are look­ing at ways of capturing a block structure using only a bare minimum 
of active components in each block. We are also considering the design of embedded computation architec­tures 
that make use of a broadcast medium. And to realize a more interactive experience, we are exploring hardware 
and software modi.cations that allow for interactive, incremental adjustment of the physical models. 
.Applications ancillary to geometric modeling: We have built blocks with LEDs, speakers, switches, and 
motion sensors that support world-in-miniature interaction metaphors for virtual environments in which 
our miniature worlds are phys­ical rather than virtual [25]. We are also exploring game ap­plications 
that make use of these same sensors and actuators. .Interactive embodiments of graphical interpretation: 
In our case studies, we focused on fully automatic graphical inter­pretation with the goal of understanding 
the limits of such an approach. We plan to study semi-automatic systems in which the computer plays a 
more assistive role. For example, it would be straightforward for our interpretation systems to prompt 
the user for help with ambiguous interpretations (e.g., Is this part of the wall or roof? or Is this 
a quadruped or a chair? ) or with a choice of enhancement (e.g., Do you want a portcullis or a drawbridge 
here? or Do you want the quadruped to run or gallop? ). This kind of user input would do much to address 
problems of brittleness and speed in our prototypes. More interestingly, we could develop mixed-initiative 
sys­tems that make more use of the fully automatic algorithms we have developed. For example, a mixed-initiative 
system might begin by prompting the user for all stylistic and aesthetic choices, but then begin making 
suggestions that are consis­tent with the user s previous selections. Our case studies show that these 
kinds of systems can be built, and hold promise to enable a new paradigm of computer-assisted graphical­interpretation 
applications.  5 Acknowledgments Special thanks to Christopher Marks, whose playful blending of the 
real and virtual inspired this work. We also want to thank Emily Anderson, Lynn Anderson, Bob Bell, Dirk 
Brinkman, Audrey Hod­gins, Clifton Leigh, Neil McKenzie, Egon Pasztor, Hanspeter P.s­ter, Rusty Ventura, 
Dick Waters, Jonathan Wolff, and Bill Yerazu­nis for their last-minute help, encouragement, and advice, 
and Altec Plastics, Flextronics, and Switchcraft for satisfying our unusual re­quirements. And thanks 
to the anonymous SIGGRAPH reviewers for helpful suggestions and comments.  References [1] R. Aish. 3D 
input for CAAD systems. Computer-Aided De­sign, 11(2):66 70, Mar. 1979. [2] R. Aish and P. Noakes. Architecture 
without numbers CAAD based on a 3D modelling system. Computer-Aided Design, 16(6):321 328, Nov. 1984. 
[3] G. Anagnostou, D. Dewey, and A. Patera. Geometry-de.ning processors for engineering design and analysis. 
The Visual Computer, 5:304 315, 1989. [4] B. G. Baumgart. Geometric modeling for computer vision. Technical 
Report AIM-249, AI Laboratory, Stanford Univ., Oct. 1974. [5] R. Brooks. Model-based 3D interpretations 
of 2D images. IEEE Trans. on Pattern Analysis and Machine Intelligence, 5(2):140 150, 1983. [6] G. Cybenko, 
A. Bhasin, and K. D. Cohen. Pattern recognition of 3D CAD objects: Towards an electronic Yellow Pages 
of mechanical parts. Smart Engineering Systems Design,1:1 13, 1997. [7] D. Dewey and A. Patera. Geometry-de.ning 
processors for partial differential equations. In B. Alder, editor, Special Pur­pose Computers, pages 
67 96. Academic Press, 1988. [8] S. Dickinson, A. Pentland, and A. Rosenfeld. From volumes to views: 
An approach to 3D object recognition. Computer Vision, Graphics, and Image Processing: Image Understand­ing, 
55(2):130 154, 1992. [9] S. J. Dickinson and D. Metaxas. Integrating qualitative and quantitative shape 
recovery. Intl. Journal of Computer Vision, 13(3):311 330, 1994. [10] C. Esposito, W. B. Paley, and 
J. Ong. Of mice and monkeys: A specialized input device for virtual body animation. In Proc. of Symposium 
on Interactive 3D Graphics, pages 109 114, 213, Monterey, California, Apr. 1995. [11] A. W. Fitzgibbon, 
G. Cross, and A. Zisserman. Automatic 3D model construction for turn-table sequences. In Proc. of SMILE 
Workshop, Freiburg, Germany, June 1998. [12] J. Frazer. Use of simpli.ed three-dimensional computer input 
devices to encourage public participation in design. In Proc. of Computer Aided Design 82, pages 143 
151. Butterworth Scienti.c, 1982. [13] J. Frazer. An Evolutionary Architecture. Architectural Asso­ciation, 
London, 1994. Describes several tangible modelers developed by Frazer s group from 1979 onwards. [14] 
J. Frazer, P. Coates, and J. Frazer. Software and hardware ap­proaches to improving the man-machine interface. 
In Proc. of the First International IFIP Conf. on Computer Applications in Production and Engineering, 
pages 1083 94, Amsterdam, Holland, 1983. North Holland. [15] J. Frazer, J. Frazer, and P. Frazer. Intelligent 
physical three­dimensional modelling system. In Proc. of Computer Graph­ics 80, pages 359 370. Online 
Publications, 1980. [16] J. Frazer, J. Frazer, and P. Frazer. New developments in in­telligent modelling. 
In Proc. of Computer Graphics 81, pages 139 154. Online Publications, 1981. [17] N. Gagvani, D. Kenchammana-HoseKote, 
and D. Silver. Vol­ume animation using the skeleton tree. In Proc. of the IEEE Symposium on Volume Visualization, 
pages 47 53, Research Triangle Park, NC, Oct. 1998. [18] M. G. Gorbet and M. Orth. Triangles: Design 
of a physi­cal/digital construction kit. In Proc. of DIS 97, pages 125 128, Amsterdam, Holland, Mar. 
1997. ACM. [19] M. G. Gorbet, M. Orth, and H. Ishii. Triangles: Tangible inter­face for manipulation 
and exploration of digital information topography. In Proc. of CHI 98, pages 49 56, Los Angeles, California, 
Apr. 1998. ACM. [20] J. K. Hodgins and N. S. Pollard. Adapting simulated behav­iors for new characters. 
In Proc. of SIGGRAPH 97, pages 153 162, Los Angeles, California, Aug. 1997. [21] T. Igarashi, S. Matsuoka, 
and H. Tanaka. Teddy: A sketching interface for 3D freeform design. In Proc. of SIGGRAPH 99, pages 409 
416, Los Angeles, California, Aug. 1999. [22] S. B. Kang. Quasi-euclidean recovery from unknown but complete 
orbital motion. Technical Report TR 97-10, Com­paq CRL, 1997. [23] F. Martin and R. Borovoy. The active 
LEGO base­plate project. http://fredm.www.media.mit.­edu/people/fredm/projects/ab/, 1994. [24] Ovid. 
Metamorphoses:X. Rome, 1 AD. [25] R. Pausch, T. Burnette, D. Brockway, and M. E. Weiblen. Navigation 
and locomotion in virtual worlds via .ight into hand-held miniatures. In Proc. of SIGGRAPH 95, pages 
399 400, Los Angeles, California, Aug. 1995. [26] W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. 
T. Vet­terling. Numerical Recipes in C. Cambridge University Press, 1988. [27] M. H. Raibert. Legged 
Robots That Balance. MIT Press, Cambridge, 1986. [28] J. C. Russ. The Image Processing Handbook. CRC 
Press, 1998. [29] S. M. Seitz and C. R. Dyer. Photorealistic scene reconstruc­tion by voxel coloring. 
International Journal of Computer Vision, 35(2):151 173, 1999. [30] F. Solina and R. Bajcsy. Recovery 
of parametric models from range images: The case for superquadrics with global defor­mations. IEEE Trans. 
on Pattern Analysis and Machine Intel­ligence, 12(2):131 146, 1990. [31] H. Suzuki and H. Kato. AlgoBlock: 
A tangible programming language a tool for collaborative learning. In Proc. of the 4th European Logo 
Conference, pages 297 303, 1993. [32] D. Terzopoulos and K. Fleischer. Deformable models. The Visual 
Computer, 4(6):306 331, December 1988. [33] P. H. Winston. Learning structural descriptions from exam­ples. 
In P. H. Winston, editor, The Psychology of Computer Vision. McGraw-Hill, New York, 1975. [34] R. C. 
Zeleznik, K. Herndon, and J. F. Hughes. SKETCH: An interface for sketching 3D scenes. In Proc. of SIGGRAPH 
96, pages 163 170, New Orleans, Louisiana, Aug. 1996.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344964</article_id>
		<sort_key>403</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>45</seq_no>
		<title><![CDATA[Accessible animation and customizable graphics via simplicial configuration modeling]]></title>
		<page_from>403</page_from>
		<page_to>410</page_to>
		<doi_number>10.1145/344779.344964</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344964</url>
		<abstract>
			<par><![CDATA[<p>O ur goal is to em bed free-form constraints into a graphical m odel. W ith such constraints a graphic can m aintain its visual integrity&#8212; and break rules tastefully&#8212; while being m anipulated by a casualuser. A typicalparam eterized graphic does notm eet these needs because its configuration space contains nonsense im ages in m uch higher proportion than desirable im ages, and the casual user is apt to ruin the graphic on any attem pt to m odify oranim ate it.</p><p>W e therefore m odel the sm all subset of a given graphic's configuration space that m aps to desirable im ages. In our solution, the basic building block is a sim plicial complex&#8212; the m ost practical data structure able to accom m odate the variety of topologies that can arise. The configuration-space m odel can be built from a cross productofsuch com plexes. W e describe how to define the m apping from this space to the im age space. W e show how to invert that m apping, allow ing the user to m anipulate the im age without understanding the structure of the configuration-space m odel. W e also show how to extend the m apping when the originalparam eterization contains hierarchy, coordinate transform ations,and other non linearities.</p><p>O ur software im plem entation applies sim plicial configuration m odeling to 2D vector graphics.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[WWW applications]]></kw>
			<kw><![CDATA[animation with constraints]]></kw>
			<kw><![CDATA[geometric modeling]]></kw>
			<kw><![CDATA[weird math]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Graphics data structures and data types</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010394</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics file formats</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P283651</person_id>
				<author_profile_id><![CDATA[81100419188]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ngo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Interval Research Corporation, 1801 Page Mill Rd., Palo Alto, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P69460</person_id>
				<author_profile_id><![CDATA[81100506216]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Doug]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cutrell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NetLens, 10975 N. Wolfe Rd., Cupertino, CA and Interval Research Corporation, 1801 Page Mill Rd., Palo Alto, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P138471</person_id>
				<author_profile_id><![CDATA[81100564466]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jenny]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dana]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Interval Research Corporation, 1801 Page Mill Rd., Palo Alto, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P33385</person_id>
				<author_profile_id><![CDATA[81100578171]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Bruce]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Donald]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth Computer Science Dept., 6211 Sudikoff Lab., Rm. 113, Hanover, NH and Interval Research Corporation, 1801 Page Mill Rd., Palo Alto, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P173241</person_id>
				<author_profile_id><![CDATA[81100576482]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Lorie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Loeb]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford Computer Science Dept., Stanford, CA and Interval Research Corporation, 1801 Page Mill Rd., Palo Alto, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P264804</person_id>
				<author_profile_id><![CDATA[81100413887]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Shunhui]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Integral Development Corp., 156 University Ave., Palo Alto, CA and Interval Research Corporation, 1801 Page Mill Rd., Palo Alto, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[A blus Corp. IntelhD raw. Computer Program (1992).]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[A lias wave front Corp. Power Animator 9. Computer Prgoram (1988).]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Autodesk Inc. AutoCAD 2000. Computer Program (2000).]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166153</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Shenchang Eric Chen, Lanoe Williams. View interpolation for in age synthesis. In James T. Kajya, ed., SIG G RAPH 93 Conference Proceedings, Annual Conference Series, pages 279-288. ACM SIG GRAPH, Addison Wesley, August 1993. ISBN 0-201-58889-7.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>156635</ref_obj_id>
				<ref_obj_pid>174462</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Herbert Edelsbrunner, Ernst P. Mucke. Three dimensional alpha shapes. ACM Transactions on Graphics, 13 (1), PP. 43-72 (January 1994). ISSN 0730-0301.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134088</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Michael Gleicher, Andrew witkin. Through-the-lens camera control In Edwin E. Catmull, editor, Computer Graphics (SiGGRAPH 92 Conference Proceedings), Vloum 26, 2 (July 1992), ACM SIGGRAPH New York, 1992, pages 331-340. ISBN 0-201-51585-7.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Gene H .Golub,Charles F.Van Loan. Matrix Computations, P.243. Johns Hopkins University Press, Baltimore, MD (1989).]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166119</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Hugues Hoppe, Tony DeRose, Tom Duchamp, John Mc Donald, Werner S~etz}9. Mesh optimization. In James T. Kajya,ed., SIG GRAPH 93 Conf. Proceedings, Annual Conference series, pages 19-26. ACM SIG GRAPH, Addison Wsley, Aug 1993. ISBN 0-201- 58889-7.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134087</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Michael Kass. C O N D O R : Constraint based data flow. In Edwin C. Catmull, editor, Computer Graphics (SIG GRAPH 92 Conference Proceedings), volume 26, pages 321-330. Addison Wesley, July 1992. ISBN 0-89791-479-1.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>159731</ref_obj_id>
				<ref_obj_pid>159730</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[David Kurknder, Steve Feiner. Infencing constraints from multiple snapshots. ACM Transactions on Graphics, 12 (4), PP. 277-304 (October 1993). ISSN 0730-0301.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618498</ref_obj_id>
				<ref_obj_pid>616050</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Seungyong Lee, George Woberg, Sung Yong Shin. Polymorph: morphing among multiple images. IEEE Computer Graphics and Applications 18(1),pp 58-71 (January-February 1998).]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Steve E. Librande. Example-based character drawing. Mster's thesis Media Arts and Science, MIT (1992)]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122731</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Peter C.Litwinowicz. Inkwell: a 21\2 D amination system. In Thomas W.Sederberg, editor, COmputer Graphics (SIG Graph 91 Conference Proceedings), Volume 25, pages 113-122. Addison Wesley, July 1991.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Meta Creations Corp. Painter 6. Computer Program (1999).]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[james R.M. unkers. Elements of Algebaic Topology. Addsion-Wesley, Reedifng, M.A. (1984).]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Dinesh K. Pai. Least constraint: a framework for the control of complex mechanical systems. In Proc. American Control conf., boston, M.A, 1625-1621 (June 1991).]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311612</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Paul Rademacher view dependent geometry. In Alyn Rockwood, editor, SIG GRAPH 99 Conference Proceedings, Annual Conference Series, pages 439-446. ACM SIG GRAPH, Addison Wesley, AUgust Series, pages 439-446. ACM SIG GRAPH, Addison Wesley, AUgust 1999. ISBN 0-201-48560-5.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Ivan E. Sutherland. Sketchpad: A man machine graphical communication system. Ph D.thesis,Dept of E.MIT (1963).]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37429</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Andrew Witkin, Kurt Fleischer, Alan Barr. Energy Constraints on parameterized models. In Maureen C.Stone, editor, computer Graphics (SIG GRAPH 87 Conference Proceedings), volume 21, pages 225-232. ACM, July 1987. ISBN 0-89791-227-6.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Accessible Animation and Customizable Graphics via Simplicial Configuration Modeling Tom Ngo,* Doug 
Cutrell, Jenny Dana, Bruce Donald, Lorie Loeb,§ and Shunhui Zhu# Interval Research Corporation, 1801 
Page Mill Rd., Palo Alto, CA 94304-1216 {ngo,jdana}@interval.com Abstract Our goal is to embed free-form 
constraints into a graphical model. With such constraints a graphic can maintain its visual integrity 
and break rules tastefully while being manipulated by a casual user. A typical parameterized graphic 
does not meet these needs because its configuration space contains nonsense images in much higher proportion 
than desirable images, and the casual user is apt to ruin the graphic on any attempt to modify or animate 
it. We therefore model the small subset of a given graphic s configuration space that maps to desirable 
images. In our solution, the basic building block is a simplicial complex the most practical data structure 
able to accommodate the variety of topologies that can arise. The configuration-space model can be built 
from a cross product of such complexes. We describe how to define the mapping from this space to the 
image space. We show how to invert that mapping, allowing the user to manipulate the image without understanding 
the structure of the configuration-space model. We also show how to extend the mapping when the original 
parameterization contains hierarchy, coordinate transformations, and other nonlinearities. Our software 
implementation applies simplicial configuration modeling to 2D vector graphics. CR Categories and Subject 
Descriptors: I.3.6 [Computer Graphics]: Methodology and Techniques Graphics Data Structures and Data 
Types, Interaction Techniques; I.3.8 [Computer Graphics]: Applications. Keywords: Animation with constraints, 
geometric modeling, weird math, WWW applications * Corresponding author. Present address: NetLens, 10975 
N. Wolfe Rd., Cupertino, CA 95014, doug@netlens.com. Present address: Dartmouth Computer Science Dept., 
6211 Sudikoff Lab., Rm. 113, Hanover, NH 03755, brd@cs.dartmouth.edu. § Present address: Stanford Computer 
Science Dept., Stanford, CA 94304, lorie@graphics.stanford.edu. # Present address: Integral Development 
Corp., 156 University Ave., Palo Alto, CA 94301, shunhui.zhu@integral.com. Permission to make digital 
or hard copies of part or all of this work or personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. SIGGRAPH 2000, New Orleans, 
LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 1 Introduction 1.1 Goal Constraints often provide 
coherence and creative freedom. That principle has been exploited pervasively in computer graphics since 
the days of Ivan Sutherland s Sketchpad system [18], in forms ranging from user-specified constraints 
such as those found in CAD systems [3] and dataflow systems [9] to constraint-inference engines [10] 
Even bitmap-based tools such as Painter [14] empower the user s creativity by constraining the modifications 
she can make to an image. We are interested in how to re-parameterize graphical models so that they are 
constrained enough to hold their visual form when modified or animated by a casual user. Target applications 
include creativity software, dynamic clip art, cartoons, performance-based avatars, and charts. Most 
existing parameterized representations (e.g., 2D vector graphics, NURBS surfaces, and CSG) are not constraining 
enough to meet our requirements. Consider the spline-based example in Figure 1. In that diagram, the 
blue cube represents the configuration space of the spline. Each configuration is a set of numerical 
values for the parameters of the spline (the coordinates of its control points). Most configurations 
correspond to meaningless scribbles. A rare few generate humanoid shapes. Our task, then, is configuration 
modeling: representing a subset of the configuration space of an existing parameterized graphic. Because 
this subset can have arbitrarily topology, our modeling scheme must not impose any a priori restrictions 
on topology. Moreover, the resulting re-parameterization might not map in any convenient way to a user 
s mental parameterization. We therefore seek to shield the user from knowing the structure of the parameterization 
by allowing for direct manipulation of the graphical object. We refer to such direct manipulation astugging. 
Configuration modeling is a fundamental open problem in computer graphics; the need to represent a subset 
of a graphical object s configuration space is pervasive.1 Although our test implementation is designed 
for 2D spline-based graphics, we expect simplicial configuration modeling to be applicable to other forms 
of parameterized graphics, including morphs, physically based models, and so forth. 1.2 Structure of 
paper Section 2 describes a configuration-modeling pipeline in a qualitative way. Quantitative and other 
algorithmic details are deferred for the appendices. Section 3 discusses how the re­parameterization 
used in configuration modeling can be used in concert with domain-specific techniques, primarily to incorporate 
hierarchy. Section 4 describes selected pieces of 1 The technique might also be applied to objects that 
are not graphical, such as parameterized sounds. content made using our authoring system. Section 5 compares 
our technique with related work, employing the modeling abstractions introduced in earlier sections. 
Section 6 recapitulates our main contributions and lists avenues of future work.  2 Qualitative description 
2.1 Configuration-modeling pipeline Any configuration-modeling technique can be described in terms of 
the modeling pipeline depicted in Figure 2. The rightmost component in that pipeline, the image space, 
represents the space of all possible images that can be rendered on the display device. The middle component, 
the configuration space, represents the freedom in the given parameterized image: a configuration is 
a set of parameter values, one for each parameter. Its dimensions are therefore called configuration 
parameters, or simply parameters. The mapping from the configuration space to the image space is the 
rendering map. It determines how the parameter values in a configuration are interpreted to generate 
an image on the display device; thus, it subsumes all processes normally associated with modeling, rendering, 
and display. Together, these two spaces and the intervening map define a parameterized graphic an object 
whose re-parameterization is the goal of our work. The leftmost component of the pipeline is the state 
space. The mapping from it to the configuration space is the re­parameterization map. The goal in configuration 
modeling is to define a state space and re-parameterization map so that the range of the map is a set 
of desirable configurations. Thus, a state determines2 a configuration under the re­parameterization 
map, and a configuration determines an image under the rendering map. Most computer graphics research, 
including work on modeling, rendering, and display, is concerned with the rendering map. This paper describes 
an approach to defining the re-parameterization map. 2.2 Nature of the configuration parameters As we 
have described it, configuration-modeling tasks usually involve continuous configuration parameters: 
geometry (e.g., 2D or 3D coordinates of control points or lengths of line segments), colors, and even 
physical parameters such as forces and masses in a relaxed mesh. In most such cases, it is appropriate 
to define a re-parameterization map that is a function. A significant twist on configuration modeling 
can arise when continuous configuration parameters represent coordinate transformations. Consider a gedanken 
experiment in which the original parameterized image is a closed 2D spline. The goal in this experiment 
is to build a configuration model that represents a complete human walk cycle in profile, a complete 
crawl cycle, and any gait between walking and crawling. The state space is isomorphic to a square: one 
dimension represents progress in the gait cycle; the other, interpolation between walking and crawling. 
Assuming that a walk cycle covers more horizontal distance than a crawl cycle, the path through the state 
space depicted in Figure 3 produces a net horizontal translation. Thus, the addition of a 2 We will later 
introduce hysteresis into the re-parameterization map, so the word determine is used loosely. global 
horizontal translation is required to avoid representing configurations that differ from each other only 
by translation. Moreover, the horizontal-translation variable is not a function of state: it experiences 
hysteresis. When a discrete parameter is present, one can arrange for the re­parameterization map to 
map patches of the state space to values of the discrete parameter. 2.3 Structure of the state space 
In our solution, the state space is represented by a data structure that is topologically general and 
computationally practical: the simplicial complex [15]. For convenience in expressing certain common 
relationships, we allow for a cross product of simplicial complexes. Combinatorially, a simplicial complex 
S can be specified3 by a set V of symbols, and subsets of V chosen so that none is a subset of any other. 
Each of those subsets is called a simplex: a point, a line segment, a triangle, a tetrahedron, or a higher-order 
simplex, depending on the number of symbols in the subset. The standard embedding of that combinatorial 
object is then the subset L of RV such that every l in L lies in a simplex.4 A vector l lies in a simplex 
s if each of its coordinates lies in the unit interval [0,1], the coordinates sum to unity, and all coordinates 
corresponding to symbols outside the simplex are zero. The coordinates corresponding to symbols inside 
the simplex are called barycentric coordinates. We use the combinatorial representation of a simplex 
as shorthand for its standard embedding. Suppose, for example, that the simplicial complex S1 comprises 
the set of symbols {o,p,q,r} and simplices {{o,p,q},{q,r}}. It consists of a triangle {o,p,q} and a line 
segment {q,r}. Using the symbol ordering (o,p,q,r), the points (3/4,1/8,1/8,0) and (0,0,1/3,2/3) both 
lie in the standard embedding of this simplicial complex because (3/4,1/8,1/8) lies in {o,p,q}, and so 
forth. Thus, a simplicial complex can be used as a data structure for generating interpolation weights; 
the simplices restrict certain groups of weights from being simultaneously non-zero. We define the state 
space Y to be a cross product of k simplicial complexes: Y=S1·S2· ·Sk. We use the term state to denote 
an element of the state space. Thus, a state y lies in the state space Y if it can be expressed as a 
tuple (l1;l2; ;lk) such that each li lies in the corresponding simplicial complex Si. We use the term 
vertex to refer to a corner of the state space, i.e., a state y=(l1;l2; ;lk) in which each li has exactly 
one coordinate equal to 1. Suppose, for example, that a state space is equal to S1·S2, where S1 is as 
defined above, and S2 comprises the set of symbols {u,v,w} and the simplices {{u,v},{v,w}}. Using the 
symbol ordering (o,p,q,r; u,v,w), the points (1/4,1/2,1/4,0; 3/7,4/7,0) and (0,0,1,0; 0,1,0) both lie 
in the state space. The latter is a vertex. We use the term zone to refer to a linear region of the state 
space, 3Only top-level simplices simplices that are not sub-faces of other simplices are specified by 
the user and explicitly represented in the implementation. 4 The notation RV denotes a Euclidean space 
whose |V| dimensions are named according to the symbols in V. i.e., a tuple of simplices, with one simplex 
taken from each factor in the cross product of simplicial complexes. Thus, the state space S1·S2 contains 
four zones. 2.4 Re-parameterization map We use re-parameterization maps of various levels of complexity, 
depending on the nature of the configuration parameters. In its simplest form, a re-parameterization 
map on a k-factor state space is specified by an arbitrary configuration for each vertex in the state 
space, and is k-linear within each zone (linear within each factor simplex). Thus, a re-parameterization 
map for the state space S1·S2 would be specified by twelve independent configurations: one for each symbol 
pair in the cross product {o,p,q,r}·{u,v,w}. Configurations admitted by the map would be of the form 
x = Sa.{o,p,q,r} Sb.{u,v,w} l1[a] l2[b] x[a][b] where square brackets denote indexing by the symbols 
in the set {o,p,q,r} ¨ {u,v,w}, x[a][b] denotes the configuration associated with symbols a and b, and 
the state y=(l1;l2) lies in the state space S1·S2. In our current software implementation, those twelve 
configurations are supplied by an artist with a drawing tool. In other systems they might be automatically 
generated. A re-parameterization map can also take more complicated forms if the configuration space 
contains some discrete parameters, or some parameters control coordinate transformations. In the case 
of a discrete parameter, one can tessellate the state space into regions, each of which maps to a different 
value of the parameter. We have implemented a form of such a discrete map in which the regions are zones 
and the discrete parameter controlled the front-to-back permutation ordering of the primitives in a 2D 
vector graphic. Appendix A.2 describes the case of a continuous parameter that represents a coordinate 
transformation and therefore can undergo hysteresis. 2.5 Forward process: driving We use the term driving 
to describe any process in which a sequence of states, generated either automatically or interactively, 
is used to compute a sequence of configurations, and hence an animation. Driving can be used in many 
ways. Clip motions. A sequence of states can be used to animate any simplicial configuration model whose 
state space is compatible5 with it, and therefore can be used as a clip motion. The term clip motion 
has been used previously in a different way. In Litwinowicz Inkwell system [13], for example, a clip 
motion is a collection of animated Coons patches that can be textured differently to generate different 
characters. By contrast, two simplicial configuration models can use the same clip motion if they have 
isomorphic state spaces, even if they bear no 5 For example, if two simplicial configuration models can 
use the same clip motion if they have state spaces that are isomorphic to each other, even if their appearance 
is radically different. The condition for compatibility is weaker than isomorphism: the clip motion can 
also be used for a model whose state space is a superset of the aforementioned isomorphic state spaces. 
geometric resemblance. Factor synchronization. If two configuration models are authored with state spaces 
whose cross products have one or more factors in common (e.g., S1·S2 and S2·S3), then the barycentric 
coordinates associated with the two instances of the shared factors (e.g., S2) can be synchronized. This 
is useful when two models contain some logical dependency; for example, the position of a drawn shadow 
can be synchronized with the position of a light source. Factor sharing is also critical in abating the 
exponential explosion that can occur when a model contains many degrees of freedom. In a model with hierarchy, 
it is typical for different graphical elements to depend on different but overlapping sets of hierarchical 
levels. Consider a human figure: trousers could depend on the knee and hip degrees of freedom; a T-shirt, 
on the hip and shoulders; and a scarf, on the shoulders and neck. Without factor sharing, each element 
would need to be re­parameterized with a state space that includes factors associated with every joint. 
With it, each element s state space needs to include only the factors on which it depends. Algorithmic 
behavior. Some or all of the factors in a model can be driven algorithmically. We have implemented, for 
instance, a behavior that executes cartoon-like squash and stretch, taking into account the magnitude 
and direction of flight, and impacts with the ground. This behavior can be applied to any model that 
contains a squash-and-stretch factor in its state space that is isomorphic to the one expected by the 
behavior. Because it interfaces with the model at the level of state not geometry it is sufficiently 
general to be applied to both a soft, round beach ball and a rigid block of wood. Moreover, the same 
behavior code can be used either interactively (for performance or play) or offline (for authoring). 
 2.6 Inverse process: tugging Tugging means inverting the re-parameterization map so that a user or some 
exogenous process can manipulate the model through some of its configuration parameters, rather than 
its state. The system answers each requested change in configuration with a change in state that matches 
the request as closely as possible. The configuration parameters being manipulated are often an (x,y) 
pair;6 we refer to them collectively as the tug point. The algorithms for tugging, described in Appendix 
A.1, address two principal challenges. 1. The re-parameterization map on a zone is often ill­conditioned. 
We invert it safely using the Moore-Penrose inverse [7]. 2. At boundaries between zones, the re-parameterization 
map contains discontinuities in the first derivative. Our algorithms handle these discontinuities seamlessly, 
so that inter-zone boundaries and therefore the structure of the state space can be transparent to the 
user.  6 The number of configuration parameters in the tug point could be more or less than two. For 
example, a mixing station or armature could control many parameters simultaneously. In a 3D environment, 
the tug point might often be a (x,y,z) point. Non-geometric parameters such as colors can also be tugged. 
Because configuration parameters are generally more intuitive to use than states, tugging is the primary 
mechanism by which both users and software interact with the graphic. Like driving, tugging can be used 
in many ways. Direct manipulation. The user specifies desired configuration changes by dragging the tug 
point with a pointing device. This style of interaction obviates the need for (but still permits the 
creation of) separate graphical user-interface elements. It is also well suited for perform ance-driven 
animation. Combination with factor sharing. When factor sharing makes objects in a scene mutually dependent, 
tugging can be used to control any of the objects. Simple software control. The graphic is regarded as 
a software object whose tug point is its interface. This technique permits an algorithmic behavior to 
be designed for reusability. We have implemented, for instance, a dynamic behavior that simulates a mass 
in a viscous medium, attached to a user-translatable anchor point through a damped spring. It can be 
used for any graphic with an (x,y) tug point and a 2D translation, regardless of state­space topology. 
Factor locking. Barycentric coordinates associated with one or more of the factors in a state space can 
be constrained not to change during a tug. In our system, the author can arrange for certain factors 
to be locked whenever a given point is tugged. Hybrid driving and tugging. One or more factors can be 
driven algorithmically while others are tugged. This is useful in arranging for a combination of interactivity 
and autonomous behavior.   Hybridizing with domain-specific techniques Configuration modeling is best 
suited for describing relatively free-form interdependencies that are difficult to express algebraically. 
When constraints are more easily described by domain-specific techniques such as articulated-figure kinematics, 
the domain-specific techniques are preferable. In addition, a mechanism to make one model depend on another 
improves model reusability: for example, a model of an eye might be made once and used with many faces. 
One approach to addressing these needs would be to use configuration modeling and an existing domain-specific 
technique independently. For example, to develop a human character in a 2D vector-based system, one might 
use standard forward and inverse kinematics for articulated figures to define a complete skeleton, and 
express fragments of clothing using independent re-parameterized models, each in the local coordinate 
system of a different rod in the skeleton. In this approach, clothing would not automatically deform 
in response to skeletal movements. Another approach would be to incorporate curvilinear interpolation 
into the re-parameterization map, which is linear on each simplex as we have presented it. For instance, 
to mitigate the foreshortening effects characteristic of linear interpolation in Cartesian coordinates, 
one might make the re-parameterization map polynomial or transcendental on each simplex. We have considered 
employing simplicial splines with differential constraints at boundaries. Librande and Poggio have successfully 
developed a curvilinear re-parameterization technique that employs radial basis functions on a hypercube 
[12]. Instead of either approach, we place parameterized, domain­specific coordinate transformations 
in the original model. Thus, the coordinate transformations are executed in the rendering map, but because 
they are parameterized, their behavior can be influenced during re-parameterization. Structural hierarchy 
is permitted in several ways: control points, entire models, and coordinate transformations can depend 
on other coordinate transformations. This technique simultaneously addresses the needs for reusability 
and for domain-specific constraints. In addition, we find that it permits the use of re-parameterization 
to break rules imposed by the domain-specific constraints. To amplify the last point, consider a 2D vector-based 
arm drawn as one spline around two rotational joints. Parts of the spline are represented in a forearm 
coordinate system; others are in an upper-arm coordinate system; yet others are in absolute coordinates. 
Rotating the two joints without re-parameterizing any spline coordinates causes the curve to move roughly 
as the outline of an arm, but with artifacts: the spline folds incorrectly at the elbow. Re-parameterization 
allows the artifact to be removed. In addition, it permits the artist to arrange for the shoulder to 
dislocate artfully by adjusting translational joint parameters7 in the extreme poses. Specifically, one 
might create a state space with topology {{elbow1,elbow2}, {elbow2,elbow3}} · {{shoulder1,shoulder2}, 
{shoulder2,shoulder3}}: simple bilinear interpolation in each of four zones generated from a 3·3 grid 
of example configurations. By arranging for the nine configurations to differ only in their shoulder- 
and elbow-joint parameters, one would obtain the coarse movement described above. The artifact repair 
and joint dislocation would be brought about by adjusting the spline coordinates in each of the nine 
configurations. 4 Results We implemented an authoring system for the re­ parameterization of 2D vector 
graphics via simplicial configuration modeling. We also implemented a number of smaller applications 
in which novice users could manipulate models created in the authoring system. The authoring system and 
applications were written for the Win32 operating system and the Microsoft Foundation Classes. For smooth 
real-time animation, a Pentium-class processor with a clock speed of at least 266 MHz is required. Rendering 
not tugging is the slow step. Figure 4 shows selected content authored in our system. The fern, Figure 
4d, exemplifies free-form constraints. It was authored using twenty-one drawings of the fern in different 
7 Our domain-specific coordinate transformation has four parameters. It represents a translatable rubber 
sheet that stretches along a preferred axis whose orientation is variable. When these transformations 
are chained, the origin of each rubber sheet is expressed in the coordinate system of the preceding transformation 
but the orientation and stretch are in absolute coordinates. positions. Tugging its tip elicits flowing 
undulations. A flattened diagram of its single-factor kite-shaped state space is in Figure 5. Moving 
along the kite tail from its tip to the kite body corresponds to unfurling of the fern from seed to seedling. 
Vertical movements within the kite region correspond roughly to further unfurling; and horizontal movements, 
to graceful swaying from left to right. The beach scene, Figure 4a, demonstrates factor synchronization. 
It has five factors: two for the sun position, one for the shoreline shape, one for the castle shape, 
and one for the motion of the waves. The first four factors are linear chains of three, two, three, and 
four symbols each; the fifth is a single triangle. Each object in the scene depends on a different subset 
of the factors. They are interrelated in several ways; for example, the position of the sun, its reflection, 
and the castle s shadow are mutually dependent and all can be tugged. The Trapeze Guy, Figure 4b, illustrates 
how a simple behavior (the mass-spring dynamical system described in Section 2.6) can be reused [19]. 
The Trapeze Guy s state space is a ring of eight symbols (which represents rotation about the trapeze), 
crossed with a linear chain of five symbols (which represents, roughly, the distance of the character 
s feet from the trapeze). The user­translatable anchor point of the mass-spring system is attached to 
the trapeze; the dynamically controlled mass, to a tug point near the character s feet. His body swings 
about as the user moves the trapeze. In the spirit of free-form constraints, an open red mouth and beads 
of sweat added to some of the forty authored configurations add to the comedy. Our most complex piece 
is the cartoon character model, Figure 4c. It contains extensive factor synchronization and hybridization 
using the modified form of articulated-figure kinematics described in Section 3. Its fourteen separate 
simplicial configuration models represent body parts such as arms, legs, eyes, and eyelids. They share 
subsets of the twelve factors, with the greatest amount of sharing used for axial rotation as well as 
left-right and part-to-part coordination of the eyes. One of the more interesting factors is the one 
that governs the shape of the mouth: it contains four triangles and two tetrahedra, assembled into a 
kite-like structure. Articulation in each limb and the attachment of facial parts to the head are both 
accomplished using the hybrid scheme described in Section 3. In both cases, the visual integrity of the 
cartoon depended critically on the re-parameterization component of the hybrid scheme, without which 
the coordination of body parts is only approximate. Related work In computer graphics, simplicial complexes 
have been used principally for modeling 3D geometry. Hoppe et al. [8] exploited the well-understood topological 
properties of simplicial complexes to regulate changes to mesh representations of surfaces. Edelsbrunner 
[5] used a multi-resolution approach (a­shapes) to track scale-dependent topological changes in the Delaunay 
triangulation of a multi-scale point-data set. We use simplicial complexes to meet a goal much closer 
to that of Librande and Poggio [12], whose work may be described in the language of configuration modeling. 
Viewed from that perspective, their technique prescribes a state space that is a hypercube, and therefore 
cannot represent topological holes. Representational power resides in the re-parameterization map, whose 
form is a superposition of radial basis functions, whereas ours is only piecewise linear. Thus, their 
system is geometrically flexible but in comparison to ours topologically restricted. Holes are important, 
for example, when rotational degrees of freedom are involved. Indeed, recent work by Rademacher on view-dependent 
deformations [17] essentially employed simplicial configuration modeling. Rademacher s work exemplifies 
how an artist s expressiveness can be layered onto geometric relationships through the use of example-based 
modeling. Gleicher and Witkin [6] influenced us to ensure that any simplicial configuration model is, 
without any special effort on the part of the artist, responsive to direct manipulation. Pai [16], among 
others, has applied similar concepts to robot control; his methods are based much more on half-space 
penalty functions. Also related are constraint-based systems that require the user to specify constraints 
explicitly, either algorithmically or through a more graphical interface. Such systems are ideal when 
the desired constraints are relatively easy to state, as in industrial CAD [3], diagramming tools [1], 
and 3D animation tools that permit entry of algebraic constraints [2]. By reducing cognitive burden on 
the user, the work of Kurlander and Feiner on constraint inference [10] challenges our distinction between 
explicit and free-form constraints. Techniques for multi-target interpolation [4,11] are comple­mentary 
to simplicial configuration modeling in the sense that each represents a new class of parameterized models 
to which the structured interpolation implied by simplicial configuration modeling may be applied. In 
fact, the present work is part of a growing interest in structured interpolation between examples, either 
digitally captured or created by an artist. 6 Epilogue We have identified the challenge of configuration 
modeling, which we believe to be an important open problem in computer graphics. To address this challenge, 
we have proposed to use a modeling primitive based on the simplicial complex. This choice leads to topological 
generality. We have shown how to run the maps from state space to image space both forward (by driving) 
and in reverse (by tugging). We have identified a number of ways in which driving and tugging lead to 
economies related to reusability of code and content. We have shown why the need for hysteresis arises 
in configuration modeling and have proposed techniques for obtaining it. We have demonstrated how domain-specific 
coordinate transformations can be used in harmony with configuration modeling in a manner that exploits 
the strengths of both. Today s simplicial configuration models are characterized by low simplex counts 
and labor-intensive authoring much like the first polygonal models in 3D graphics. If configuration modeling 
addresses a genuine need, one might expect to see further developments along lines analogous to the ones 
that have permitted polygonal models to grow in complexity by orders of magnitude. These might include 
semi-automated authoring using high-level primitives and capture of configuration models from video sources. 
Acknowledgments We thank Subutai Ahmad, Neal Bhadkamkar, Frank Brooks, Michele Covell, Frank Crow, Kevin 
Hunter, Yan -Bin Jia, Scott Klemmer, Andrew Kunz, Bud Lassiter, Golan Levin, Joy Mountford, Chris Seguine, 
Malcolm Slaney, and Meg Withgott for their thoughtful contributions to this work. We also thank the reviewers 
for their many useful and constructive suggestions. A Appendices A.1 Tugging algorithms This appendix 
describes how to invert the re-parameterization map. Given a requested change in one or more configuration 
parameters, the goal is to compute a state change that satisfies the request as closely as possible. 
We describe cases in order of increasing complexity. Following Gleicher and Witkin [6], we use local 
solutions instead of global ones to provide temporal continuity in animation. One factor, one zone. In 
a single-factor model that contains one simplex, the re-parameterization map is linear. Let x be a vector 
whose coordinates are the configuration parameters to be changed, and let Dx be the desired change. A 
state is said to improve upon the current state if it maps to a configuration x for which (x Dx > 0. 
Let l be the state in barycentric coordinates. To constrain the barycentric coordinates to sum to unity, 
we use the redundant coordinate system r, defined by li = ri + (1 R)/k, where k is the number of symbols 
in the simplex and R = Siri, summed over all symbols in the simplex. The state change is then Dr = J 
Dx, where the components of the Jacobian J are the partial derivatives of x with respect to r, and the 
symbol denotes Moore-Penrose inversion, which handles rank­deficient matrices by giving special treatment 
to singularities [7]. If the requested state change Dr would cause the state to exit the simplex, travel 
is halted at the simplex boundary. At the simplex boundary, one or more barycentric coordinates are zero. 
The Jacobian computation is repeated with the corresponding symbols in the simplex omitted from the computation, 
i.e., constraining the state to lie in the subface. If a boundary of that subface is encountered, the 
procedure continues in subfaces of decreasing dimensionality until a subface of dimensionality zero is 
encountered. Multiple factors, one zone. When k factors are involved, the re­parameterization map is 
k-linear. Let x and Dx be defined as above, but let each ri in the tuple (r1; r2; ; rk) be the state 
coordinates from factor i. The state change is then [Dr1T|Dr2T| |DrkT]T = [J1|J2| |Jk] Dx, where each 
Jacobian Ji is defined as above with respect to the corresponding ri while keeping rj fixed for all j 
i. In a multilinear map, rectilinear movements produce linear effects, but diagonal movements can produce 
polynomial effects because each Jacobian Ji depends on every rj for all j i. Therefore, local optima 
exist. Factor locking. To lock factor i, i.e., prevent ri from changing during tugging, we merely omit 
ri and Ji from the equation given above. Multiple omissions lock multiple factors. When one factor is 
driven while another is tugged, we interleave steps of driving and tugging. One factor, multiple zones. 
This is the piecewise linear case. In contrast with the linear case, a simplex boundary can be the portal 
to one or more neighboring simplices. When the state arrives at a simplex boundary, we first identify 
neighboring simplices, i.e., ones that share the subface in which the state resides. We remove from consideration 
each neighboring simplex8 that contains no states that improve upon the current state. (Because the re-parameterization 
map is linear on the neighboring simplex, it is sufficient to test each vertex in the neighboring simplex 
that lies outside the current subface.) If zero neighboring simplices remain, tugging proceeds in the 
subface, as in the linear case. If one neighboring simplex remains, tugging proceeds in that simplex.9 
The subcase in which multiple neighboring simplices remain is one for which we have discussed and implemented 
various heuristics, but that we have not yet encountered in practice. We leave as an open problem the 
development of a universally acceptable way either to choose an appropriate neighboring simplex, or to 
control the topology of the state space so that this subcase cannot arise. Multiple factors, multiple 
zones. This is the piecewise multilinear case. It raises additional issues that are also open problems. 
As in the piecewise linear case, the only challenges that are not present in the purely multilinear case 
arise at boundaries between zones. If the state encounters a zone boundary that is a simplex boundary 
in only one of the factors, the decision reduces to the one-factor, multiple­zone case. At a zone boundary 
that is a simplex boundary in more than one factor this case does arise in practice the decision is more 
complicated. The remainder of this section discusses that decision. Recall that in a state space formed 
from the cross product of k simplicial complexes, a zone is a k-tuple of simplices, with one simplex 
taken from each factor. In the situation under consideration, the state lies on a subface in more than 
one of those simplices. Another zone is considered to be a neighbor if at least one of the simplices 
is replaced by a neighboring simplex that shares one of those subfaces. One difficulty arises because 
the re-parameterization map associated with a zone can be polynomial for moves that are not rectilinear. 
The simple test used in the piecewise linear case to determine whether a neighboring simplex should be 
eliminated does not necessarily work. A neighboring zone can contain states that improve upon the current 
state even if none of its vertices do. We have therefore replaced the global test over vertices by a 
local test: we compute the anticipated state change from Dx in each neighboring zone and eliminate any 
neighboring zone from which the state would immediately exit. The second difficulty is that the test 
given above may admit multiple neighboring zones. This particular difficulty has not occurred for us 
in practice, but a universally acceptable solution (or way to guarantee that the situation is never encountered) 
would be desirable for the sake of completeness. A.2 Re-parameterization map with hysteresis This appendix 
shows how to define a re-parameterization map with hysteresis. We have developed re-parameterization 
maps with three differing levels of hysteresis. A conservative map is one for which traversing a closed 
loop in the state space always produces a closed loop in the configuration space. Outside of this appendix, 
all re-parameterization maps described in 8 Moving into such a simplex cannot produce any movement in 
the direction of the requested configuration change. 9 Infinite looping is prevented by eliminating a 
simplex from consideration for the duration of a single tugging step once it has been exited. this paper 
are conservative. A semi-conservative map is one for which traversing a closed loop in the state space 
can be guaranteed to produce a closed loop in the configuration space only if the loop in state space 
crosses no zone boundaries. A non-conservative map is one for which traversing a closed loop in the state 
space can never be guaranteed to produce a closed loop in the configuration space. We describe semi-conservative 
and non-conservative maps. Each of these developments requires to departures from the mechanisms put 
forth outside this appendix. First, we modify the definition of a simplicial complex. A simplicial complex 
is normally defined as a union of simplices in which each simplex is an open set. Instead, we define 
each top-level simplex as a closed set, deliberately creating redundancy at shared subfaces. Under this 
definition, every state on a shared subface has one or more siblings that have the same barycentric coordinates 
but are associated with different top-level simplices. Second, we distinguish between the relative values 
of a configuration parameter (which are present in the range of the map) and the actual values (which 
are supplied to the rendering map and may differ from the relative values). Semi-conservative map. Suppose 
x is a continuous configuration parameter for which we wish to define a semi-conservative map. In each 
zone, we permit the author to specify an arbitrary, relative value of x at every vertex. Thus, a vertex 
can have a different relative value of x in every zone of which it is a member. We define the re­parameterization 
map to be multi-linear within each zone, just as in the conservative case, but possibly discontinuous 
at zone boundaries. As the state moves continuously within a zone, the infinitesimal changes in the relative 
value of x are accumulated into the actual value of x; but when it moves from one zone to another, the 
non-infinitesimal changes in the relative value of x are ignored. A semi-conservative map could be used, 
for example, to control the rotation of a 2D wheel on or near a high-friction 1D surface. The parameters 
of the wheel are (x,y,q), where x and y are the position of the wheel s center and q is its rotation 
angle. For x and y, the re­parameterization map is conservative and defined so that a zone boundary maps 
to a horizontal line just above the surface. For q, the map is semi-conservative. It is defined so that 
the relative value of q is constant in the zone above the surface, and linearly related to x in the zone 
on the surface. The wheel exhibits the physically correct hysteresis that results from rotating only 
when moved horizontally while in contact with the surface. Non-conservative map. Suppose, now, that x 
is a continuous configuration parameter for which we wish to define a non­conservative map. In each zone, 
we permit the author to specify an arbitrary relative-value difference Dx at every edge (ordered vertex 
pair). When the state moves within a zone from barycentric­coordinate vector l to barycentric-coordinate 
vector l , we accumulate into the actual value of x the following quantity: DX(l,l ) = Sv Sv l(v) Dx(v,v 
) l (v ), where each sum is over all vertices in the zone, each Dx(v,v ) is an relative-value difference 
supplied by the author, and l(v) is a scalar quantity extracted from the vector l by multiplying together 
all components of l associated with symbols in the symbol tuple v. As with the semi-conservative case, 
we do not change the actual value of x when crossing a zone boundary. This procedure has a number of 
attractive properties. First, it produces hysteresis. Second, DX(l,l ) is a smooth function of l and 
l . Third, when moving from one vector (v) to another (v ) it produces a change in x equal to the one 
specified by the author, i.e., Dx(v,v ). Fourth, under some circumstances10 we have been able to show 
that a linear path through the state space can be executed in any number of smaller steps without affecting 
the total change in x. Fifth, when the values of Dx(v,v ) are conservative (i.e., Dx(v,v ) + Dx(v ,v 
) = Dx(v,v ) for any v, v , and v ), the procedure reduces to the semi-conservative case. A non-conservative 
map could be used to provide the hysteresis called for in Figure 3.   References [1] Aldus Corp. IntelliDraw. 
Computer Program (1992). [2] Alias|Wavefront Corp. PowerAnimator 9. Computer Program (1998). [3] Autodesk 
Inc. AutoCAD 2000. Computer Program (2000). [4] Shenchang Eric Chen, Lance Williams. View interpolation 
for image synthesis. In James T. Kajiya, ed., SIGGRAPH 93 Conference Proceedings, Annual Conference Series, 
pages 279-288. ACM SIGGRAPH, Addison Wesley, August 1993. ISBN 0-201-58889-7. [5] Herbert Edelsbrunner, 
Ernst P. Mücke. Three-dimensional alpha shapes. ACM Transactions on Graphics, 13 (1), pp. 43-72 (January 
1994). ISSN 0730-0301. [6] Michael Gleicher, Andrew Witkin. Through-the-lens camera control. In Edwin 
E. Catmull, editor, Computer Graphics (SIGGRAPH 92 Conference Proceedings), volume 26, 2 (July 1992), 
ACM SIGGRAPH, New York, 1992, pages 331-340. ISBN 0-201-51585-7. [7] Gene H. Golub, Charles F. Van Loan. 
Matrix Computations, p. 243. Johns Hopkins University Pr ess, Baltimore, MD (1989). [8] Hugues Hoppe, 
Tony DeRose, Tom Duchamp, John McDonald, Werner Stuetzle. Mesh optimization. In James T. Kajiya, ed., 
SIGGRAPH 93 Conf. Proceedings, Annual Conference Series, pages 19-26. ACM SIGGRAPH, Addison Wesley, Aug 
1993. ISBN 0-201­58889-7. [9] Michael Kass. CONDOR: Constraint-based dataflow. In Edwin C. Catmull, editor, 
Computer Graphics (SIGGRAPH 92 Conference Proceedings), volume 26, pages 321-330. Addison Wesley, July 
1992. ISBN 0-89791-479-1. [10] David Kurlander, Steve Feiner. Inferring constraints from multiple snapshots. 
ACM Transactions on Graphics, 12 (4), pp. 277-304 (October 1993). ISSN 0730-0301. [11] Seungyong Lee, 
George Wolberg, Sung Yong Shin. Polymorph: morphing among multiple images. IEEE Computer Graphics and 
Applications, 18(1), pp.58-71 (January-February 1998). [12] Steve E. Librande. Example-based character 
drawing. Master s thesis, Media Arts and Science, MIT (1992). [13] Peter C. Litwinowicz. Inkwell: a 2 
½-D animation system. In Thomas W. Sederberg, editor, Computer Graphics (SIGGRAPH 91 Conference Proceedings), 
volume 25, pages 113-122. Addison Wesley, July 1991. ISBN 0-201-56291-X. [14] MetaCreations Corp. Painter 
6. Computer Program (1999). [15] James R. Munkres. Elements of Algebraic Topology. Addison-Wesley, Reading, 
MA (1984). [16] Dinesh K. Pai. Least constraint: a framework for the control of complex mechanical systems. 
In Proc. American Control Conf., Boston, MA, 1615--1621 (June 1991). [17] Paul Rademacher. View-dependent 
geometry. In Alyn Rockwood, editor, SIGGRAPH 99 Conference Proceedings, Annual Conference Series, pages 
439-446. ACM SIGGRAPH, Addison Wesley, August 1999. ISBN 0-201-48560-5. [18] Ivan E. Sutherland. Sketchpad: 
A man-machine graphical communication system. Ph.D. thesis, Dept. of E.E., MIT (1963). [19] Andrew Witkin, 
Kurt Fleischer, Alan Barr. Energy constraints on parameterized models. In Maureen C. Stone, editor, Computer 
Graphics (SIGGRAPH 87 Conference Proceedings), volume 21, pages 225-232. ACM, July 1987. ISBN 0-89791-227-6. 
10We have shown this property for the case of linear movement in one factor. Figure 2. Modeling Pipeline: 
State space . Configuration space . Image space Figure 1. Configuration space Figure 3. Continuous configuration 
parameters with coordinate transformations: walk vs. crawl 4b. 4a. 4c. Figure 4. Content Samples. a) 
Beach scene with factor sharing b) Trapeze guy with state space diagram c) Cartoon character model d) 
Fern unfolding and bending 4d. Figure 5. State space diagram for fern content  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344969</article_id>
		<sort_key>411</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>46</seq_no>
		<title><![CDATA[Example-based hinting of true type fonts]]></title>
		<page_from>411</page_from>
		<page_to>416</page_to>
		<doi_number>10.1145/344779.344969</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344969</url>
		<abstract>
			<par><![CDATA[<p>Hinting in TrueType is a time-consuming manual process in which a typographer creates a sequence of instructions for better fitting the characters of a font to a grid of pixels. In this paper, we propose a new method for automatically hinting TrueType fonts by transferring hints of one font to another. Given a hinted source font and a target font without hints, our method matches the outlines of corresponding glyphs in each font, and then translates all of the individual hints for each glyph from the source to the target font. It also translates the control value table (CVT) entries, which are used to unify feature sizes across a font. The resulting hinted font already provides a great improvement over the unhinted version. More importantly, the translated hints, which preserve the sound, hand-designed hinting structure of the original font, provide a very good starting point for a professional typographer to complete and fine-tune, saving time and increasing productivity. We demonstrate our approach with examples of automatically hinted fonts at typical display sizes and screen resolutions. We also provide estimates of the time saved by a professional typographer in hinting new fonts using this semi-automatic approach.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[automatic hinting]]></kw>
			<kw><![CDATA[digital typography]]></kw>
			<kw><![CDATA[gridfitting]]></kw>
			<kw><![CDATA[shape matching]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.7.4</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Size and shape</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.5.2</cat_node>
				<descriptor>Feature evaluation and selection</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Shape</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.7.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010257.10010321.10010336</concept_id>
				<concept_desc>CCS->Computing methodologies->Machine learning->Machine learning algorithms->Feature selection</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010510</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document preparation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010242</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Shape representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010249</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Shape inference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10010477</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Publishing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Documentation</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P70068</person_id>
				<author_profile_id><![CDATA[81100437187]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Douglas]]></first_name>
				<middle_name><![CDATA[E.]]></middle_name>
				<last_name><![CDATA[Zongker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Corporation and University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P96566</person_id>
				<author_profile_id><![CDATA[81100412884]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Geraldine]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wade]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P63622</person_id>
				<author_profile_id><![CDATA[81100188207]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Salesin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Corporation and University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Adobe Systems, Inc. Adobe Type 1 Font Format, March 1990.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Sten E Andler. Automatic generation of gridfitting hints for rasterization of outline fonts or graphics. In Proceedings of the International Conference on Electronic Publishing, Document Manipulation, and Typography, pages 221-234, September 1990.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Apple Computer, Inc. The TrueType Font Format Specification, 1990. Version 1.0.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Claude B6trisey. G~n~ration Automatique de Contraintes pour CaractOres Typographiques ~ l'Aide d' un ModOle Topologique. PhD thesis, l~cole Polytechnique F6d6rale de Lausanne, 1993.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37431</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Roger D. Hersch. Character generation under grid constraints. In Proceedings of SIGGRAPH 87, pages 243-252, July 1987.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122726</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Roger D. Hersch and Claude B6trisey. Model-based matching and hinting of fonts. In Proceedings of SIGGRAPH 91, pages 71-80, July 1991.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>726031</ref_obj_id>
				<ref_obj_pid>647506</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Beat Stature. Visual TrueType: A graphical method for authoring font intelligence. In R. D. Hersch, J. Andr6, and H. Brown, editors, Electronic Publishing, Artistic Imaging, and Digital Typography, pages 77-92, March/April 1998.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Example-Based Hinting of TrueType Fonts Douglas E. Zongker1,2 Geraldine Wade1 David H. Salesin1,2 1Microsoft 
Corporation Abstract Hinting in TrueType is a time-consuming manual process in which a typographer creates 
a sequence of instructions for better .tting the characters of a font to a grid of pixels. In this paper, 
we pro­pose a new method for automatically hinting TrueType fonts by transferring hints of one font to 
another. Given a hinted source font and a target font without hints, our method matches the outlines 
of corresponding glyphs in each font, and then translates all of the individual hints for each glyph 
from the source to the target font. It also translates the control value table (CVT) entries, which are 
used to unify feature sizes across a font. The resulting hinted font already provides a great improvement 
over the unhinted version. More importantly, the translated hints, which preserve the sound, hand-designed 
hinting structure of the original font, provide a very good starting point for a professional typographer 
to complete and .ne-tune, saving time and increasing productivity. We demonstrate our approach with examples 
of automatically hinted fonts at typical display sizes and screen resolutions. We also provide estimates 
of the time saved by a professional typographer in hinting new fonts using this semi-automatic approach. 
CR Categories: I.7.4 [Document and Text Processing]: Electronic Publishing Keywords: automatic hinting, 
digital typography, grid.tting, shape matching 1 Introduction The demand for high-quality hinted fonts 
is outstripping the ability of digital typography houses to produce them. Hinting is a painstak­ing manual 
process that can only be done well by a handful of highly skilled professionals. It requires a blend 
of typographical artistry with technological ability. In order to provide a full appre­ciation of the 
hinting problem, we begin here with a review of how digital fonts are scan-converted onto a raster display. 
In digital typography, each character in a font is described by a set of outlines, usually represented 
by splines. When the character is rendered onto a grid of pixels, the outlines are scaled to the de­sired 
size, and then each pixel whose center lies inside of an outline is set to black. When fonts are displayed 
at suf.ciently high reso­lutions this approach works beautifully. But for sizes below about 150 ppem,1 
severe aliasing problems can result when this naive out­line .lling process is applied, especially for 
delicate features such as serifs. Figure 1 shows an example. The left image is generated by the naive 
algorithm. This pixel pattern does not look much like a lowercase a . A simple dropout control mechanism 
added to the .ll algorithm turns on additional pixels to preserve the character s topology, resulting 
in the center image. The right image, though, shows the work of an experienced hinter. The pixel pattern 
has been subtly altered to both improve readability and better preserve the character of the original 
outline. 1Hinters express font sizes in pixels per em,or ppem. This measure counts the number of device 
pixels in the em of the font. In traditional ty­pography, the em of a font was the height of the metal 
block of type. With digital typography, there is no actual metal block to measure, but the digital outlines 
are still expressed in coordinates relative to this hypothetical size. The point size of text refers 
to the size of its em expressed in points (a point is 1/72 of an inch). Thus, 12-point text corresponds 
to 12 ppem on a 72 dpi screen, or 100 ppem on a 600 dpi printer. 2University of Washington Figure 1 
Outline for the Palatino Italic a , along with the pixel pattern gener­ated by rasterizing the outlines 
for display of 18-point text on a 72 dpi device. The left image shows the results of the naive .ll algorithm. 
The middle image shows the result of enabling the rasterizer s dropout control mechanism. The right image 
shows the results of hinting the character by hand.  The hinting process is not just about optimizing 
individual charac­ters. The hinter must balance the needs of a single glyph with the desire for consistency 
across all the characters of a font. It is im­portant, for example, to ensure that all the vertical stems 
of a font are the same number of pixels wide at a given size. If the scal­ing and rounding process produced 
one-pixel-wide stems on some characters and two-pixel-wide stems on others, then a passage of text would 
look blotchy and be dif.cult to read. The goal of the hinter is to produce a smooth transition from very 
high sizes, where merely .lling the outlines suf.ces and hinting is unnecessary, down to lower sizes, 
where legibility must be preserved even when that means a departure from the outlines drawn by the original 
font de­signer. Although the ever-improving resolution of hardcopy devices is be­ginning to approach 
the point at which hinting is not necessary, the technology is not there yet: 10-or 12-point text on 
a 300 or even 600 dpi printer still needs hinting for best results. More importantly, the increasing 
emphasis on reading text on-screen from visions of the paperless of.ce to the emergence and proliferation 
of hand­held computers and eBooks means that more and more text is be­ing viewed on devices in the 72 
100 dpi range. Though resolutions of these displays are improving as well, for the foreseeable future 
hinting will be an absolute necessity in order to provide clear, legi­ble text. Although attempts have 
been made to design automated hinting systems in the past [2, 5], even the best of these produce hints 
that are good, but still not up to the standards of professional typogra­phers. This previous work assumed 
that in order to be useful, an au­tohinter had to be a monolithic, self-contained package: outlines in, 
quality hints out. That is an admirable goal, and it may be achieved someday. However, given the detailed, 
aesthetically-based nature Permission to make digital or hard copies of part or all of this work or personal 
or classroom use is granted without fee provided that copies are not made or distributed for profit or 
commercial advantage and that copies bear this notice and the full citation on the first page. To copy 
otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission 
and/or a fee. SIGGRAPH 2000, New Orleans, LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 of 
the work, we think that it is currently more useful to view the autohinter as one piece of a system that 
includes a human hinter. The subject of this paper, then, is not a tool for automatically gen­erating 
hints so much as a tool for automatically translating hints from one font to another. An important advantage 
of this approach is that it preserves the basic strategy and structure of the original hints, which were 
hand crafted by a professional typographer for each individual glyph of the font. Generally, these translated 
hints provide an excellent starting point for a human to .ne-tune and ad­just. We demonstrate our approach 
with examples of automatically hinted fonts at typical display sizes and screen resolutions. We also 
provide estimates of the time saved by a professional typographer in hinting new fonts using this semi-automatic 
approach.  2 Background There are two major font standards in widespread use today: Type 1 and TrueType. 
Type 1 fonts [1], often called PostScript fonts, were developed by Adobe and are popular in the world 
of pub­lishing. Printing applications were the target when this system was developed, though utilities 
are now available to enable on-screen display of Type 1 fonts. The TrueType format [3], originally devel­oped 
by Apple, was intended to unify type on the screen and on paper, and is used in both the Macintosh and 
Windows operating systems. TrueType has something of a reputation for being of low quality, but this 
is mostly due to the fact that TrueType was always an open standard while Type 1 was not, and so the 
public domain is .ooded with a large number of poorly designed, unhinted True-Type fonts. The TrueType 
standard does contain extensive facilities for high-quality hinting, though, and more and more quality 
fonts are now available in TrueType. Though both formats represent characters as spline-based outlines, 
the hinting styles are radically different. Hinting for Type 1 fonts works by marking sections of the 
outline as corresponding to partic­ular typographic features of the character stems, bowls, counters, 
and so on. It is the job of the rasterizer to take advantage of these hints about the character shape 
to produce the best possible pattern of pixels. This scheme has the advantage that enhancements to the 
rasterizer can produce improvements to all fonts on the system, but means that a designer of digital 
type cannot specify exactly what an outline will look like when rendered at a given size. The TrueType 
font technology takes a different approach. Instead of leaving control over the glyph s .nal appearance 
to the rasterizer, a TrueType font contains explicit instructions about how particular control points 
should be shifted to .t the pixel grid. These instruc­tions take the form of a program in a special, 
TrueType-speci.c bytecode language. Since both the behavior of each instruction and the rasterizing algorithm 
are de.ned in the TrueType standard, the designer of a TrueType font can predict exactly which pixels 
will be turned on for a character at a given size, no matter what the output device is. In TrueType, 
each contour of an outline is speci.ed with a sequence of point positions. (See the outline curves of 
Figure 3 for some ex­amples.) Each point is .agged as either on-curve or off-curve.True-Type de.nes the 
outline as follows: Two successive on-curve points are connected with a straight line segment.  When 
an off-curve point falls between two on-curve points, the three are treated as the control points for 
a quadratic B´ezier seg­ment.  When two adjacent off-curve points appear, the midpoint of the segment 
connecting them is treated as an implicit on-curve point between them, allowing reduction to the case 
above.  The glyph renderer starts by scaling the outlines to a particular size, then executing the attached 
program to shift control points around in a size-speci.c way before .lling the altered outline. By itself, 
this approach cannot produce the necessary consistency among differ­ent characters of a font, or even 
between different parts of the same character, since each action is necessarily local. Global synchro­nization 
of outline alterations is achieved through use of the control value table, or CVT. This is a shared table 
of distances, which can be referenced by instructions in each glyph s program. When the rendering is 
initialized for a given size, the values in the CVT are scaled and rounded to the current grid size. 
Point movements can then be constrained by CVT entries. For instance, a person writ­ing hints for TrueType 
may decide to use CVT 81 to represent, say, the width of vertical black stems in lowercase letters. He 
or she will then write instruction sequences for all appropriate lowercase letters, all referring to 
CVT entry 81, so that all the vertical black stems at a given size will have the same width. The TrueType 
language is an assembly-style stack-based language. The intent of the designers of TrueType was not to 
make typog­raphers learn and write in the TrueType language itself, but rather to facilitate the development 
of high-level languages and tools that generate TrueType code. The Visual TrueType (VTT) package from 
Microsoft [7] is such a tool. VTT provides a high-level language, called VTT Talk, for expressing relationships 
between points. VTT Talk provides statements for expressing the following classes of hints: Link constraints: 
the vertical or horizontal distance between a pair of knots is constrained by an entry in the CVT.  
Dist constraints: the natural vertical or horizontal distance be­tween a pair of knots is maintained, 
so that if one point is moved the other moves in parallel.  Interpolate constraints: a knot s fractional 
distance between two parent knots is maintained.  Anchors: speci.c knots can be rounded to the nearest 
gridline, or  to a gridline speci.ed by a CVT entry. These types of hints are demonstrated visually 
for two characters from the Georgia Roman font in Figure 6. The VTT Talk hints are compiled into a TrueType 
program stored in the font .le. One ad­vantage of working with VTT Talk is that each statement simply 
asserts a relationship between two points, and there is little depen­dence on the order of the statements. 
If one statement is omitted, the meaning of the others is unchanged. In contrast, TrueType assem­bler 
is a sequential language that maintains a fairly complex state. Most instructions in TrueType have side 
effects that modify this state. If we tried to translate the assembler code directly, and were for some 
reason unable to translate a particular instruction for in­stance, due to a suf.ciently large difference 
in the matched glyphs outlines the effects of subsequent instructions could change en­tirely. Our approach 
is primarily motivated by the work of Hersch and Be­trisey [4, 6]. In their method, hints are generated 
for each glyph by matching its outline to a human-constructed generic model of that character s shape 
(for example, a generic uppercase roman B ). The model consists of two representations of the generic 
character shape. The skeleton model builds the character out of solid parts, labeled as stems, bowls, 
serifs, and so on. The contour model is an outline representation of the character, constructed to have 
as few control points as possible while still spanning the space of possible character shapes. The correspondences 
between the two models are known, being speci.ed by hand when the model is built. In their method, the 
outlines of the glyph to be hinted are matched to the corresponding contour model by a fairly complex 
process that takes into account both global and local features. Points are classi.ed by their position 
relative to the baseline, cap-height and x-height lines, and left and right sidebearings. Local features 
distinguishing points are based on the curvature, direction, and orientation of the adjacent curve segments. 
Once the correspondence between the unknown K K L C M N O-P BB Trebuchet Frutiger Figure 2 Features 
used for matching on-curve points. Diamonds indicate cor­ner points; circles indicate smooth points. 
Incoming and outgoing directions are quantized to the eight compass directions, indicated with blue arrows. 
Lo­cal extrema are shown with red triangles. Each point is also marked to indicate whether the segments 
on each side are curved (solid lines) or straight (dashed lines). outline and the model outline is established, 
the known correspon­dence between the model outline and the model skeleton can be used to label parts 
of the unknown outline as belonging to signif­icant features such as stems and serifs. From this labeling 
a set of Type 1-style hints for the new outline can be derived.  3 Method Hersch and Betrisey s work 
requires a manually constructed model in order to link points on the outline with the semantic features 
needed for hinting. Hinting in TrueType does not require an ex­plicit labeling of these features; this 
information is implicitly used by the human typographer when deciding on a hinting strategy for the character, 
but the end result expressed in the font is just a set of relationships, or constraints, between control 
points. These con­straints obviate the need for the skeleton model once we .nd the correspondence between 
a contour model and the outlines of the target glyph, we can immediately produce hints for the target 
out­line without transitively applying a second correspondence. We ve therefore reduced our needs to 
having a contour model with control-point-level hints attached to it. A shortcut now becomes ob­vious: 
use an already hinted TrueType font as the model! This has a number of advantages over using a specialized 
model built ex­pressly for the auto-hinter. First, we already have a wide variety of fonts from which 
to choose as templates. Moreover, choosing a template close to the target font will increase the likelihood 
of a good match and consequently the quality of the resulting hints. This raises the possibility of having 
the template font be selected from the library automatically, or even choosing different template fonts 
for different characters of the target. Another advantage of using real hinted fonts as templates is 
that typographers, rather than com­puter scientists, can build templates using tools they already know; 
furthermore, each typographer can build templates to suit his or her own hinting style. 3.1 Matching 
the outlines Suppose now that we have two glyphs representing a single charac­ter. One, the source outline, 
will be the hinted character that we are taking hints from. The goal is to translate those hints to refer 
to con­trol points on the target outline. In the illustration here we ll show the process of translating 
hints from the lowercase a of Trebuchet to the a of Frutiger. Our algorithm attempts to match up explicit 
on-curve knots using features such as contour direction and the presence of extrema. The on-curve knots 
typically have far more signi.cance to the shape and extents of the contour. Once a match is computed 
between the Figure 3 The .nal match for the two a glyphs. On-curve knots are solid dots; off-curve knots 
are open circles. A N Times New Roman Italic Palatino Italic Figure 4 Results of matching for a more 
complex pair of glyphs. No matches involve off-curve knots in this example, so these knots are not shown. 
 on-curve knots, we attempt to pair up the remaining knots by sim­ply counting the number of off-curve 
knots between each pair of matched on-curve knots. If the numbers are equal, we pair the off­curve knots 
based solely on their order. Only a very small fraction of hints involve these off-curve knots, but we 
want to preserve as many of the source hints as possible. Many glyphs are de.ned by multiple contours, 
but there are no re­strictions on what order the contours are listed in. Therefore, our .rst task is 
to determine which contour goes with which in the two glyphs. We do this be enumerating all the possibilities 
for a one-to­one pairing of the contours. (The hinter rejects input outline pairs with differing numbers 
of contours.) For each pairing we calculate a score as follows. Suppose that the target character is 
scaled and translated so that its bounding box is equal to that of the source character. For each individual 
contour within the characters, we sum together the absolute values of the differences between corre­sponding 
sides of the contour bounding boxes. This value, summed over all the contours gives the score for the 
match, with the lowest value being the best match. While this is a factorial-time algorithm, we have 
not found the running time to be a problem for the Latin character sets we have been using it is rare 
to .nd a character with more than .ve contours. The next step, the heart of the algorithm, is to match 
up the knots on each pair of contours. We begin by identifying a number of fea­tures at each knot, and 
assigning a point score for matching that feature: 1. Each knot has an incoming and an outgoing direction, 
based on the tangents of the curves touching that knot. The direction is quantized to one of eight possibilities, 
corresponding to the eight compass directions. A pair of knots is assigned from 200 to -200 points based 
on the similarity of each direction. For example, a knot with an incoming direction of north, gets 200 
points when matched with another north knot, 100 points for a northeast or northwest match, 0 points 
for east or west , -100 points for southeast or southwest , and -200 points for matching south knot. 
This score is calculated for both in­coming and outgoing direction. 2. Each knot can be .agged as a local 
minimum or maximum in each of the x or y directions. A knot with one of these .ags will contribute 150 
points when matched with a knot with the same .ag, or -150 points when matched to the opposite .ag. A 
knot may not be an extremum at all in a given direction, in which case any match will not produce a score 
for this category. 3. Finally, each knot has a .ag to indicate whether the incom­ing and outgoing lines 
are straight (within some tolerance) or curved. Matching these .ags produces a score of 100 points, but 
not matching them produces no penalty.  Figure 2 shows our two a characters, marked with the features 
used for matching. To generate these matches, we pick an arbitrary starting knot on each contour to be 
matched, and pair these knots. We then go around the source contour, pairing each knot with the knot 
on the target curve whose fractional arc length relative to the starting knot is closest to that of the 
source knot. This generates a match with one pair for each source knot. We can sum the local-feature 
score of each pair to rate the quality of the overall match. We generate a match using each knot on the 
target outline as the starting point. The .ve matches with the highest local-feature scores go on to 
the next stage. In this .nal stage, we attempt to improve the scores of these .ve best matches by small 
perturbations of the pairings. We remove knot pairs with a negative local-feature score, look for matches 
for unpaired source knots, and shift existing pairs to adjacent target knots, all subject to the constraint 
that the match respect the order­ing of knots around the contour: if knot B follows knot A in the source 
contour, then the partner of knot B should not come before the partner of knot A on the target contour. 
Once we ve performed this local improvement on each of the .ve top matches, we select the match with 
the highest .nal score as our .nal match. The results of this matching algorithm are shown in Figure 
3. These heuristics work well for a wide variety of character styles, including roman, bold, and italic 
characters. A matching for a more complex pair of glyphs is shown in Figure 4.  3.2 Hint translation 
Having produced a match between the knots of the source character outline and those of the target outline, 
we re now ready to translate the hints themselves. We parse the source font s VTT Talk hints and copy 
them to the target font, replacing knot numbers as appropriate according to our match. If we do not have 
a match for a knot ref­erenced in a particular statement, we simply copy the source state­ment unchanged, 
but comment it out, to mark it as a place that may need special attention by a person reviewing the font. 
 3.3 CVT translation The CVT is a central feature of the TrueType hinting mechanism, and no TrueType 
autohinting scheme would be complete without addressing it. In VTT Talk, entries of the CVT are used 
via state­ments such as YLink(14,0,87), which says, in effect, move knot 0 up or down so that its vertical 
distance from knot 14 is equal to CVT entry 87. Our matcher allows us to translate the references to 
speci.c knots to their analogues in the new font, and we can cer­tainly keep using the same CVT entry 
numbers as in the original font. The question is, what values do we put in those entries? The old entries 
tell us little, since they represent distances measured in the source font, which may bear little or 
no relation to distances in the target font.  Trebuchet  Frutiger Figure 5 The top row shows characters 
from the font Trebuchet. The typog­rapher has used CVT entry 87 to control the height of round, black 
features in lowercase letters, indicated by the green links between control points. The bottom row shows 
Frutiger, along with the uses of CVT entry 87 as transferred from Trebuchet by our autohinter. Red lines 
indicate where hints were auto­matically discarded because the natural distance between the points was 
too different from the value in the CVT table. glyph references to CVT entry 87 a 75. 143 143 156 156 
164 b 111. 113. 156 156 c 156 156 160 172 d 111. 113. 156 156 e 156 156 193. f 155 156 g 45. 45. 111. 
156 156 178 h 156 Table 1 Some of the references to CVT entry 87 when translating Trebuchet hints to 
Frutiger. For each pair of points whose vertical distance is constrained by this CVT entry, the natural 
distance between the points in the Frutiger glyphs is listed. The value given to entry 87 is the median 
of these natural distances, 156 units in this case. The starred values are outliers. The solution comes 
from recognizing that the major reason the CVT is used is to take a set of distances that are approximately 
the same in the outline, and force them to be exactly the same num­ber of pixels in the rendered bitmap. 
Since the goal is to provide this consistency while changing the outlines as little as possible, the 
CVT entry will generally contain some average value, which is close to all the distances it is going 
to be used to constrain. We can look at all the uses of a particular CVT entry to estimate what its value 
should be. Let s look at how this works on our a character. The person hinting Trebuchet chose to use 
CVT entry 87 to represent the height of round, black features in lowercase characters. Accordingly, most 
of the lowercase letters that have round parts reference CVT 87, as we see in the top row of Figure 5. 
The a glyph alone uses entry 87 six times that is, there are six pairs of knots in the a whose distance 
is constrained by CVT entry 87. Table 1 shows the natural distances between each of these pairs in Trebuchet 
for characters a through h . One pair of points in the a is 75 units apart vertically in the unhinted 
outline, another is 143 units, and so on. To determine the overall value to place in the CVT entry, we 
take the median of all these individual guesses, which in this case is 156 units. The starred numbers 
in the listing indi­cate those uses of the CVT entry where the natural outline distance differs by more 
than 20% from the median value. We label these outliers, and we remove (comment out) the hints corresponding 
to these uses during the translation process, as they usually represent cases where the shape of the 
target character differs enough from that of the source character that the CVT constraint is inappropri­ate. 
These commented-out constraints correspond to the red lines in the lower row of Figure 5. Note that Trebuchet 
has a so-called spectacle g, while Frutiger has a multi-story g. In this case, it is likely that two 
the forms of the g require entirely different hint­ing strategies, since many of the hints of the source 
g are simply not appropriate for the target character shape. These inappropriate Figure 6 A visualization 
of the VTT Talk hints created by a professional hin­ter for two characters of Georgia Roman. Link constraints 
are shown in green, dist constraints in blue, and interpolate constraints in purple. Solid lines in­dicate 
x-direction constraints, while dashed lines indicate y constraints. An­chors are indicated with small 
wings on the anchored knot. success review &#38; manual rate cleanup hinting savings source font target 
font (%) (min.) (min.) (%) Sylfaen Sans Sylfaen Sans Bold 84% 5.9 9.4 37% Georgia Italic Georgia Bold 
Italic 86% 6.7 7.9 15% Georgia Roman Georgia Bold 93% 4.6 7.1 35% Georgia Roman Bodoni 78% 3.3 3.3 0% 
Georgia Roman Calisto 74% 3.0 4.3 30% Georgia Roman Perpetua 76% 1.2 2.7 56% Georgia Roman Revival 82% 
1.3 2.3 43% Table 2 Times for hinting a sample of representative characters, both starting with the 
autohinted font and starting with no hints at all. hints are automatically discarded by the outlier mechanism. 
Only link constraints, which reference the CVT, are eliminated. Other types of hints do not refer to 
the CVT, and so are never discarded as long as there are matches for the points they constrain.   4 
Results Our program takes two TrueType fonts as input: a source font, from which the hints are transferred; 
and a target font, which is hinted by the program. The program takes under a minute to match the outlines, 
translate the hints, and create the new CVT for a 256­character font. Once the target font is hinted, 
it still needs to be re­viewed by hand and corrected by an experienced typographer. Even minor errors 
in the translated hints or CVT can take a considerable amount of time to identify and correct, so the 
translation has to be highly accurate in order to be useful. Figure 7 shows how the set of manually-de.ned 
hints for two glyphs from Georgia Roman, R and a , have been automatically transferred to .ve different 
fonts. Figures 8 and 9 compare the un­hinted versions of Sylfaen Sans Bold and Georgia Bold, respec­tively, 
to the versions hinted automatically, at 16, 17, and 19 ppem, the most commonly used on-screen sizes. 
In these examples it is clear that most of the objectionable artifacts in the unhinted ver­sions have 
already been corrected by the automatic hinting. Note, for instance, the improved O shapes and the much 
more uniform stem weights in both fonts. Still, the autohinted versions are not perfect; note for instance 
where the bowl of the Georgia Bold b has narrowed unacceptably, especially at lower sizes. Imperfections 
like these will need to be corrected by hand. We evaluated our method by using the program to transfer 
hints between three pairs of fonts within the same family (Sylfaen Sans Bold from Sylfaen Sans, Georgia 
Bold from Georgia, and Georgia Bold Italic from Georgia Italic) as well as four target fonts from a source 
font of a different font family (Bodoni, Calisto, Perpetua, and Revival all from Georgia). Table 2 summarizes 
the results of Georgia Bold Georgia Bold Bodoni Bodoni   Figure 7 Visualization of hints transferred 
from the Georgia Roman of Fig­ure 6 to .ve other fonts.  High resolution H0H00000123 fivd boxing wizardsjimedd 
uiikly 19 ppem Unhinted (14 pt @ 96 dpi) Autohinted 17 ppem Unhinted (13 pt @ 96 dpi) Autohinted 16 
ppem Unhinted (12 pt @ 96 dpi) Autohinted High resolution Unhinted 19 ppem (14 pt @ 96 dpi) Autohinted 
 17 ppem Unhinted (13 pt @ 96 dpi) Autohinted 16 ppem Unhinted (12 pt @ 96 dpi) Autohinted   H0H00000123fivdboxingwizads 
jumpds uuikly Figure 8 Sylfaen Sans Bold (top), and Georgia Bold (bottom), each autohinted by transferring 
hints from the corresponding roman typeface. these tests. In each case, just the alphanumeric glyphs 
were hinted. The success rate column gives the percentage of these 62 glyphs in which the transferred 
hints basically worked. More speci.cally, for a successful glyph, the overall appearance of the glyph 
con­formed to the original outline at high sizes (38 ppem and above) without any stretching or distortion, 
whereas below 38 ppem there might be some cleaning up to do, but no major reshaping or rethink­ing of 
the hints. If a glyph did not conform to its original outline at high sizes or required major reshaping 
at low sizes, then it was con­sidered unsuccessful. As can be seen from the table, the hinter had a fairly 
high success rate by this measure, especially when hinting characters within the same font family. The 
next column gives an estimate of the number of minutes re­ quired for an experienced typographer to review 
the results of the autohinter and clean up any problems in the transferred hints. The .gures in this 
column were estimated by performing this process on some 3 to 11 representative glyphs in the target 
font. These same glyphs were also manually hinted by the same typographer and the times required reported 
in the following column. Finally, the right­most column provides an estimate of the overall time savings 
pro­vided by the example-based hinter. Note that the very high success rate of the hinter translates 
into a more moderate overall time savings, since even a perfectly-hinted font requires time to review, 
and since a few small problems in the hints can be time-consuming to correct. Still, these savings are 
sig­ni.cant, considering that a full font of 256 characters can take on the order of 20 40 hours for 
a skilled professional to produce. 5 Conclusion We have adapted the earlier work of Hersch and Betrisey 
on auto­matic hinting through shape matching to create a useful production tool for hinting TrueType 
fonts. Instead of using hand-created tem­plates for each character to be hinted, we use an existing, 
hinted font as the template, allowing the hints of one font to be transferred to another. This translation 
process includes estimation of the con­trol value table entries used to unify feature sizes across a 
font. The matching algorithm, while simple, works well for a wide variety of The hint transfer itself 
is somewhat less successful, owing primarily to the different strategies used in hinting different styles 
of charac­ters (e.g., modern serif vs. oldstyle serif). The technique is already quite useful for transferring 
hints between members of the same family (a roman to a bold, for instance). We expect that transfer­ring 
hints between fonts of different families will become more and more practical as more fonts are hinted 
with the VTT tool, so that the hinter has a larger selection of source fonts to choose from and can pick 
one that is more similar to the target font. An important advantage of our approach over previous autohinters 
is that it preserves the hand-crafted hinting strategy, built by a pro­fessional typographer, in the 
newly hinted font. Thus, the translated hints provide a good starting point and generally require only 
minor cleanup and adjustment. With time, we expect this work to evolve into a highly practical tool for 
speeding the creation of production­quality digital fonts. Acknowledgements We are indebted to Michael 
Duggan, Greg Hitchcock, and Beat Stamm of the Microsoft eBooks group for the many long discus­sions about 
hinting, VTT, TrueType, and typography in general.  References [1] Adobe Systems, Inc. Adobe Type 1 
Font Format, March 1990. [2] Sten F. Andler. Automatic generation of grid.tting hints for rasterization 
of outline fonts or graphics. In Proceedings of the International Conference on Electronic Publishing, 
Document Manipulation, and Typography, pages 221 234, September 1990. [3] Apple Computer, Inc. The TrueType 
Font Format Speci.cation, 1990. Version 1.0. [4] Claude B´etrisey. G´en´eration Automatique de Contraintes 
pour Caract`eres Ty­pographiques `a l Aide d un Mod`Ecole Polytechnique ele Topologique. PhD thesis, 
´ F´ed´erale de Lausanne, 1993. [5] Roger D. Hersch. Character generation under grid constraints. In 
Proceedings of SIGGRAPH 87, pages 243 252, July 1987. [6] RogerD.HerschandClaudeB´etrisey.Model-basedmatchingandhintingoffonts. 
In Proceedings of SIGGRAPH 91, pages 71 80, July 1991. [7] Beat Stamm. Visual TrueType: A graphical method 
for authoring font intelligence. In R. D. Hersch, J. Andr´e, and H. Brown, editors, Electronic Publishing, 
Artistic character shapes, including serifed and italic fonts. Imaging, and Digital Typography, pages 
77 92, March/April 1998. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344972</article_id>
		<sort_key>417</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>47</seq_no>
		<title><![CDATA[Image inpainting]]></title>
		<page_from>417</page_from>
		<page_to>424</page_to>
		<doi_number>10.1145/344779.344972</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344972</url>
		<abstract>
			<par><![CDATA[<p>Inpainting, the technique of modifying an image in an undetectable form, is as ancient as art itself. The goals and applications of inpainting are numerous, from the restoration of damaged paintings and photographs to the removal/replacement of selected objects. In this paper, we introduce a novel algorithm for digital inpainting of still images that attempts to replicate the basic techniques used by professional restorators. After the user selects the regions to be restored, the algorithm automatically fills-in these regions with information surrounding them. The fill-in is done in such a way that isophote lines arriving at the regions' boundaries are completed inside. In contrast with previous approaches, the technique here introduced does not require the user to specify where the novel information comes from. This is automatically done (and in a fast way), thereby allowing to simultaneously fill-in numerous regions containing completely different structures and surrounding backgrounds. In addition, no limitations are imposed on the topology of the region to be inpainted. Applications of this technique include the restoration of old photographs and damaged film; removal of superimposed text like dates, subtitles, or publicity; and the removal of entire objects from the image like microphones or wires in special effects.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[anisotropic diffusion]]></kw>
			<kw><![CDATA[image restoration]]></kw>
			<kw><![CDATA[inpainting]]></kw>
			<kw><![CDATA[isophotes]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>G.2.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.9</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003624</concept_id>
				<concept_desc>CCS->Mathematics of computing->Discrete mathematics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P187768</person_id>
				<author_profile_id><![CDATA[81100649204]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Marcelo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bertalmio]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31048688</person_id>
				<author_profile_id><![CDATA[81100582663]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Guillermo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sapiro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39069997</person_id>
				<author_profile_id><![CDATA[81410591685]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Vincent]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Caselles]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Escola Superior Politecnica, Universitat Pompeu Fabra]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P49850</person_id>
				<author_profile_id><![CDATA[81100236341]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Coloma]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ballester]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Escola Superior Politecnica, Universitat Pompeu Fabra]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[S. Walden. The Ravished Image. St. Martin's Press, New York, 1985.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[G. Emile-Male. The Restorer's Handbook of Easel Painting. Van Nostrand Reinhold, New York, 1976.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[D. King. The Commissar Vanishes. Henry Holt and Company, 1997.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>2322108</ref_obj_id>
				<ref_obj_pid>2319116</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[A.C. Kokaram, R.D. Morris, W.J. Fitzgerald, P.J.W. Rayner. Detection of missing data in image sequences. IEEE Transac-tions on Image Processing 11(4), 1496-1508, 1995.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>2322109</ref_obj_id>
				<ref_obj_pid>2319116</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[A.C. Kokaram, R.D. Morris, W.J. Fitzgerald, P.J.W. Rayner. Interpolation of missing data in image sequences. IEEE Trans-actions on Image Processing 11(4), 1509-1519, 1995.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[C. Braverman. Photoshop retouching handbook. IDG Books Worldwide, 1998.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237264</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[A. Hirani and T. Totsuka. Combining Frequency and spatial domain information for fast interactive image noise removal. Computer Graphics, pp. 269-276, SIGGRAPH 96, 1996.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>851569</ref_obj_id>
				<ref_obj_pid>850924</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[A. Efros and T. Leung, "Texture synthesis by non-parametric sampling," Proc. IEEE International Conference Computer Vi-sion, pp. 1033-1038, Corfu, Greece, September 1999.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218446</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[D.Heeger andJ.Bergen. Pyramid based texture analy-sis/ synthesis. Computer Graphics, pp. 229-238, SIGGRAPH 95, 1995.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[E. Simoncelli and J. Portilla. Texture characterization via joint statistics of wavelet coefficient magnitudes. 5th IEEE Int'l Conf. on Image Processing, Chicago, IL. Oct 4-7, 1998.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563003</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[M. Nitzberg, D. Mumford, and T. Shiota, Filtering, Segmen-tation, and Depth, Springer-Verlag, Berlin, 1993.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[S. Masnou and J.M. Morel. Level-lines based disocclusion. 5th IEEE Int'l Conf. on Image Processing, Chicago, IL. Oct 4-7, 1998.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[C. Kenney and J. Langan. A new image processing primitive: reconstructing images from modified flow fields. University of California Santa Barbara Preprint, 1999.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>78304</ref_obj_id>
				<ref_obj_pid>78302</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[P. Perona and J. Malik Scale-space and edge detection using anisotropic diffusion. IEEE-PAMI 12, pp. 629-639, 1990.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>131147</ref_obj_id>
				<ref_obj_pid>131134</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[L. Alvarez, P.L. Lions, J.M. Morel. Image selective smoothing and edge detection by nonlinear diffusion.SIAMJ.Numer. Anal. 29, pp. 845-866, 1992.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>56815</ref_obj_id>
				<ref_obj_pid>56813</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[S. Osher and J. Sethian. Fronts propagating with curvature dependent speed: algorithms based on Hamilton-Jacobi for-mulations. Journal of Computational Physics, 79:12-49, 1988.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[A. Marquina and S. Osher. Explicit algorithms for a new time dependent model based on level set motion for nonlinear de-bluring and noise removal. UCLA CAM Report 99-5, January 1999.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>142312</ref_obj_id>
				<ref_obj_pid>142273</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[L. Rudin, S. Osher and E. Fatemi. Nonlinear total variation based noise removal algorithms. Physica D, 60, pp. 259-268, 1992.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[S. Osher, personal communication, October 1999.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>235190</ref_obj_id>
				<ref_obj_pid>235172</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[H. K. Zhao, T. Chan, B. Merriman, and S. Osher, "A varia-tional level-set approach to multiphase motion," J. of Compu-tational Physics 127, pp. 179-195, 1996.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[A. Bertozzi The mathematics of moving contact lines in thin liquid films. Notices Amer. Math. Soc., Volume 45, Number 6, pp. 689-697, June/July 1998.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311544</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[J. Tumblin and G. Turk, "LCIS: A boundary hierarchy for detail-preserving contrast reduction," Computer Graphics, pp. 83-90, SIGGRAPH 99, 1999.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[T. Chan and J. Shen, "Mathematical models for local deter-ministic inpaintings," UCLA CAM TR 00-11, March 2000.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[C. Ballester, M. Bertalmio, V. Caselles, G. Sapiro, and J. Verdera, "Filling-in by joint interpolation of vector fields and grey levels," University of Minnesota IMA TR, April 2000.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Image Inpainting Marcelo Bertalmio and Guillermo Sapiro. Vicent Caselles and Coloma Ballester Electrical 
and Computer Engineering, University of Minnesota Escola Superior Politecnica, Universitat Pompeu Fabra 
 Abstract Inpainting, the technique of modifying an image in an undetectable form, is as ancient as art 
itself. The goals and applications of in­painting are numerous, from the restoration of damaged paintings 
and photographs to the removal/replacement of selected objects. In this paper, we introduce a novel algorithm 
for digital inpainting of still images that attempts to replicate the basic techniques used by professional 
restorators. After the user selects the regions to be restored, the algorithm automatically .lls-in these 
regions with in­formation surrounding them. The .ll-in is done in such a way that isophote lines arriving 
at the regions boundaries are completed in­side. In contrast with previous approaches, the technique 
here in­troduced does not require the user to specify where the novel in­formation comes from. This is 
automatically done (and in a fast way), thereby allowing to simultaneously .ll-in numerous regions containing 
completely different structures and surrounding back­grounds. In addition, no limitations are imposed 
on the topology of the region to be inpainted. Applications of this technique include the restoration 
of old photographs and damaged .lm; removal of su­perimposed text like dates, subtitles, or publicity; 
and the removal of entire objects from the image like microphones or wires in spe­cial effects. CR Categories: 
I.3.3 [Computer Graphics]: Picture/Image Generation ; I.3.4 [Computer Graphics]: Graphics Utilities ; 
I.4.4 [Image Processing and Computer Vision]: Restoration ; I.4.9 [Image Processing and Computer Vision]: 
Applications ; Keywords: Image restoration, inpainting, isophotes, anisotropic diffusion.  1 Introduction 
The modi.cation of images in a way that is non-detectable for an observer who does not know the original 
image is a practice as old as artistic creation itself. Medieval artwork started to be restored as early 
as the Renaissance, the motives being often as much to bring medieval pictures up to date as to .ll in 
any gaps [1, 2]. This practice is called retouching or inpainting. The object of inpainting is to reconstitute 
the missing or damaged portions of the work, in order to make it more legible and to restore its unity 
[2]. The need to retouch the image in an unobtrusive way extended naturally from paintings to photography 
and .lm. The purposes remain the same: to revert deterioration (e.g., cracks in photographs or scratches 
and dust spots in .lm), or to add or remove elements *Electrical and Computer Engineering, University 
of Minnesota, Min­neapolis, MN 55455, USA, fmarcelo,guilleg@ece.umn.edu (e.g., removal of stamped date 
and red-eye from photographs, the infamous airbrushing of political enemies [3]). Digital techniques 
are starting to be a widespread way of per­forming inpainting, ranging from attempts to fully automatic 
detec­tion and removal of scratches in .lm [4, 5], all the way to software tools that allow a sophisticated 
but mostly manual process [6]. In this article we introduce a novel algorithm for automatic digi­tal 
inpainting, being its main motivation to replicate the basic tech­niques used by professional restorators. 
At this point, the only user interaction required by the algorithm here introduced is to mark the regions 
to be inpainted. Although a number of techniques ex­ist for the semi-automatic detection of image defects 
(mainly in .lms), addressing this is out of the scope of this paper. Moreover, since the inpainting algorithm 
here presented can be used not just to restore damaged photographs but also to remove undesired ob­jects 
and writings on the image, the regions to be inpainted must be marked by the user, since they depend 
on his/her subjective selec­tion. Here we are concerned on how to .ll-in the regions to be inpainted, 
once they have been selected.1 Marked regions are au­tomatically .lled with the structure of their surrounding, 
in a form that will be explained later in this paper. 2 Related work and our contribution We should 
.rst note that classical image denoising algorithms do not apply to image inpainting. In common image 
enhancement ap­plications, the pixels contain both information about the real data and the noise (e.g., 
image plus noise for additive noise), while in image inpainting, there is no signi.cant information in 
the region to be inpainted. The information is mainly in the regions surrounding the areas to be inpainted. 
There is then a need to develop speci.c techniques to address these problems. Mainly three groups of 
works can be found in the literature re­lated to digital inpainting. The .rst one deals with the restoration 
of .lms, the second one is related to texture synthesis, and the third one, a signi.cantly less studied 
class though very in.uential to the work here presented, is related to disocclusion. Kokaram et al. [5] 
use motion estimation and autoregressive models to interpolate losses in .lms from adjacent frames. The 
basic idea is to copy into the gap the right pixels from neighboring frames. The technique can not be 
applied to still images or to .lms where the regions to be inpainted span many frames. 1In order to study 
the robustness of the algorithm here proposed, and not to be too dependent on the marking of the regions 
to be inpainted, we mark them in a very rough form with any available paintbrush software. Marking these 
regions in the examples reported in this paper just takes a few seconds to a non-expert user. Hirani 
and Totsuka [7] combine frequency and spatial domain in­formation in order to .ll a given region with 
a selected texture. This is a very simple technique that produces incredible good results. On the other 
hand, the algorithm mainly deals with texture synthesis (and not with structured background), and requires 
the user to select the texture to be copied into the region to be inpainted. For images where the region 
to be replaced covers several different structures, the user would need to go through the tremendous 
work of segment­ing them and searching corresponding replacements throughout the picture. Although part 
of this search can be done automatically, this is extremely time consuming and requires the non-trivial 
selection of many critical parameters, e.g., [8]. Other texture synthesis algo­rithms, e.g., [8, 9, 10], 
can be used as well to re-create a pre-selected texture to .ll-in a (square) region to be inpainted. 
In the group of disocclusion algorithms, a pioneering work is described in [11]. The authors presented 
a technique for removing occlusions with the goal of image segmentation.2 The basic idea is to connect 
T-junctions at the same gray-level with elastica minimiz­ing curves. The technique was mainly developed 
for simple images, with only a few objects with constant gray-levels, and will not be applicable for 
the examples with natural images presented later in this paper. Masnou and Morel [12] recently extended 
these ideas, presenting a very inspiring general variational formulation for dis­occlusion and a particular 
practical algorithm (not entirely based on PDE s) implementing some of the ideas in this formulation. 
The algorithm performs inpainting by joining with geodesic curves the points of the isophotes (lines 
of equal gray values) arriving at the boundary of the region to be inpainted. As reported by the authors, 
the regions to be inpainted are limited to having simple topology, e.g., holes are not allowed.3 In addition, 
the angle with which the level lines arrive at the boundary of the inpainted region is not (well) preserved: 
the algorithm uses straight lines to join equal gray value pixels. These drawbacks, which will be exempli.ed 
later in this paper, are solved by our algorithm. On the other hand, we should note that this is the 
closest technique to ours and has motivated in part and inspired our work. 2.1 Our contribution Algorithms 
devised for .lm restoration are not appropriate for our application since they normally work on relatively 
small regions and rely on the existence of information from several frames. On the other hand, algorithms 
based on texture synthesis can .ll large regions, but require the user to specify what texture to put 
where. This is a signi.cant limitation of these approaches, as may be seen in examples presented later 
in this paper, where the region to be inpainted is surrounded by hundreds of different backgrounds, some 
of them being structure and not texture. The technique we propose does not require any user interven­tion, 
once the region to be inpainted has been selected. The algo­rithm is able to simultaneously .ll regions 
surrounded by different backgrounds, without the user specifying what to put where. No assumptions on 
the topology of the region to be inpainted, or on the simplicity of the image, are made. The algorithm 
is devised for inpainting in structured regions (e.g., regions crossing through boundaries), though it 
is not devised to reproduce large textured areas. As we will discuss later, the combination of our proposed 
approach with texture synthesis techniques is the subject of current research. 2Since the region to be 
inpainted can be considered as occluding objects, removing occlusions is analogous to image inpainting. 
3This is not intrinsic to the general variational formulation they propose, only to the speci.c discrete 
implementation they perform.  3 The digital inpainting algorithm 3.1 Fundamentals Let nstand for the 
region to be inpainted, and @nfor its boundary (note once again that no assumption on the topology of 
nis made). Intuitively, the technique we propose will prolong the isophote lines arriving at @n, while 
maintaining the angle of arrival. We pro­ceed drawing from @ninward in this way, while curving the pro­longation 
lines progressively to prevent them from crossing each other. Before presenting the detailed description 
of this technique, let us analyze how experts inpaint. Conservators at the Minneapolis Institute of Arts 
were consulted for this work and made it clear to us that inpainting is a very subjective procedure, 
different for each work of art and for each professional. There is no such thing as the way to solve 
the problem, but the underlying methodology is as follows: (1.) The global picture determines how to 
.ll in the gap, the purpose of inpainting being to restore the unity of the work; (2.) The structure 
of the area surrounding nis continued into the gap, contour lines are drawn via the prolongation of those 
arriving at @n; (3.) The different regions inside n, as de.ned by the contour lines, are .lled with color, 
matching those of @n;and (4.) The small details are painted (e.g. little white spots on an otherwise 
uniformly blue sky): in other words, texture is added. A number of lessons can immediately be learned 
from these ba­sic inpainting rules used by professionals. Our algorithm simulta­neously, and iteratively, 
performs the steps (2.) and (3.) above.4 We progressively shrink the gap nby prolonging inward, in a 
smooth way, the lines arriving at the gap boundary @n. 3.2 The inpainting algorithm We need to translate 
the manual inpainting concepts expressed above into a mathematical and algorithmic language. We proceed 
to do this now, presenting the basic underlying concepts .rst. The implementation details are given in 
the next section. Let I0(i,j):[0,M]x[0,N]!IR, with [0,M]x[0,N]c INxIN, be a discrete 2D gray level image. 
From the descrip­tion of manual inpainting techniques, an iterative algorithm seems a natural choice. 
The digital inpainting procedure will construct a family of images I(i,j,n):[0,M]x[0,N]xIN!IRsuch that 
I(i,j,0)=I0(i,j)and limI(i,j,n)=IR(i,j),where n!1 IR(i,j)is the output of the algorithm (inpainted image). 
Any gen­eral algorithm of that form can be written as In+1(i,j)=(i,j)+itIn(i,j),8(i,j)2n(1) Int where 
the superindex ndenotes the inpainting time n, (i,j)are the pixel coordinates, itis the rate of improvement 
and In(i,j) t stands for the update of the image In(i,j). Note that the evolution equation runs only 
inside n, the region to be inpainted. With this equation, the image In+1(i,j)is an improved version of 
In(i,j), with the improvement given by In(i,j).As nin­ t creases, we achieve a better image. We need 
now to design the update In(i,j). t As suggested by manual inpainting techniques, we need to con­tinue 
the lines arriving at the boundary @nof the region nto be inpainted (see point (2) in Section 3.1). In 
other words, we need to smoothly propagate information from outside ninto n(points (2) and (3) in Section 
3.1). Being Ln(i,j)the information that !n we want to propagate, and ;(i,j)the propagation direction, 
this N means that we must have 4In the discussion section we will argue how both steps can be performed 
separately, and we will also discuss step (4.). We use then a time varying estimation of the isophotes 
direc­tion .eld: for any given point (i,j), the discretized gradient vector rIn(i,j)gives the direction 
of largest spatial change, while its 90 degrees rotation rJIn(i,j)is the direction of smallest spatial 
JIn change, so the vector r(i,j)gives the isophotes direction. Our .eld Jr In  NN(i,j,nWe are using 
a time-varying estimation that is coarse at the begin­ning but progressively achieves the desired continuity 
at @n,in­ !; !; is then given by the time-varying )= (i,j). N(i,j)that would imply to know the direc- 
 Figure 1: Propagation direction as the normal to the signed distance tions of the isophotes from the 
start. to the boundary of the region to be inpainted. Note that the direction .eld is not normalized, 
its norm is the norm of the gradient of In(i,j). This choice helps in the numerical stability of the 
algorithm, and will be discussed in the following subsection. Since we are performing inpainting along 
the isophotes, it is ir­ !;J relevant if rIn(i,j)is obtained as a clockwise or counterclock­ stead of 
a .xed .eld  wise rotation of nIr (i,j). In both cases, the change of In (i,j) Figure 2: Unsuccessful 
choice of the information propagation di­rection. Left: detail of the original image, region to be inpainted 
is in white. Right: restoration. along those directions should be minimum. Recapping, we estimate a 
variation of the smoothness, given by a discretization of the 2D Laplacian in our case, and project this 
variation into the isophotes direction. This projection is used to update the value of the image inside 
the region to be inpainted. To ensure a correct evolution of the direction .eld, a diffusion process 
is interleaved with the image inpainting process described above.6 That is, every few steps (see below), 
we apply a few itera­tions of image diffusion. This diffusion corresponds to the periodi­ n cal curving 
of lines to avoid them from crossing each other, as was (i,j), (2) LN is a measure of the change in the 
informa­ !; !; n In t (i,j)= (i,j). mentioned in Section 3.1. We use anisotropic diffusion, [14, 15], 
in order to achieve this goal without losing sharpness in the recon­ struction. In particular, we apply 
a straightforward discretization of L(i,j) tion Ln(i,j).5 With this equation, we estimate the information 
!; n where  the following continuous-time/continuous-space anisotropic diffu­ !; Ntion. Note that at 
steady state, that is, when the algorithm con­verges, In+1(i,j)=In(i,j)and from (1) and (2) we have that 
(i,j)of our image and compute its change along thedirec- Ln sion equation: @I 2 n e (3) (x,y,t)=ge(x,y)"(x,y,t)jrI(x,y,t)j,8(x,y)LN 
has been propagated in the direction !; !; n n (i,j)=0, meaning exactly that the information L @t (i,j) 
. !;N. where neis a dilation of nwith a ball of radius c, "is the Euclidean being propa- What is left 
now is to express the information Lcurvature of the isophotes of Iand ge(x,y)is a smooth function in 
 !; N. Since we want the propagation to be smooth, Ln(i,j) gated and the direction of propagation nesuch 
that ge(x,y)=0in @ne,and ge(x,y)=1in n(thisisa should way to impose Dirichlet boundary conditions for 
the equation (3)).7 be an image smoothness estimator. For this purpose we may use a simple discrete implementation 
of the Laplacian: Ln(i,j):= In(i,j)+In(i,j)(subscripts represent derivatives in this case). 3.3 Discrete 
scheme and implementation details xxyy Other smoothness estimators might be used, though satisfactory 
re- The only input to our algorithm are the image to be restored and sults were already obtained with 
this very simple selection. the mask that delimits the portion to be inpainted. As a preprocess­ !; 
n L In order to do this we must .rst de.ne what the directionThen, we must compute the change(i,j)of 
this value along  ing step, the whole original image undergoes anisotropic diffusion smoothing. The 
purpose of this is to minimize the in.uence of noise on the estimation of the direction of the isophotes 
arriving at !;forthe2Dinformationpropagationwillbe. Onepossibilityisto N. N !;!; N point (i,j)in nthe 
vector de.ne as the normal to the signed distance to @n, i.e., at each @n. After this, the image enters 
the inpainting loop, where only !; N(i,j)will be normal to the shrinked version of @nto which (i,j)belongs, 
see Figure 1. This choice to the discrete implementation of the inpainting procedure, which is motivated 
by the belief that a propagation normal to the bound-we proceed to describe. Every few iterations, a 
step of anisotropic ary would lead to the continuity of the isophotes at the boundary. diffusion is applied 
(a straightforward, central differences imple-Instead, what happens is that the lines arriving at @ncurve 
in or­ the values inside nare modi.ed. These values change according mentation of (3) is used; for details 
see [14, 15]). This process is repeated until a steady state is achieved. !; N, see Figure 2. This is 
of course not what we expect. Note that the orientation of @nis not intrinsic to the image Let In(i,j)stand 
for each one of the image pixels inside the geometry, since the region to be inpainted is arbitrary. 
region nat the inpainting time n. Then, the discrete inpainting der to align with !;,thebestchoiceforNN 
!; the isophotes directions. This is a bootstrapping problem: hav-by ing the isophotes directions inside 
nis equivalent to having the In+1 inpainted image itself, since we can easily recover the gray level 
 equation borrows from the numerical analysis literature and is givenIf isophotes tend to align withis 
then (i,j )= In (i,j)+itIn t (i,j),8(i,j) 2 n (4) image from its isophote direction .eld ( see the 
discussion section and [13]). 6We can also add the diffusion as an additional term in In(i,j),the t 
results being very similar. ; 5Borrowing notation from continuous mathematics, we could also write 7Other 
.lters, e.g., form mathematical morphology, can be applied as ! Ln(i,j)as rL. well, though we found 
the results obtained with this equation satisfactory. where ! N(i,j,n) N(i,j,n) !;!; (i,j )= L(i,j) 
!; n . jrIn (i,j)j, (5) In t projection of !;,thatis,wecom-N PC (128Mb RAM, 300MHz) under Linux. All 
the examples use (i,j !; n n Ln (i+1,j);Ln(i;1,j),L(i,j+1);Ln (i,j;1)), ):=( (6) LnIn In (i,j)=(i,j)+(i,j), 
(7) xxyy Figure 3: Relation between the (R,G,B)color model and the one n N(i,j,n)(;In(i,j),I(i,j)) 
used in this article,(p,sinc,sin7). yx !; (8) := , N(i,j,n) !; p (In(i,j))2+(In(i,j))2 xy If nis of 
considerable size, a multiresolution approach is used to speed-up the process.10 N(i,j,n) N(i,j,n) !;!; 
L(i,j) !; n fn (9) (i,j )= , . Color images are considered as a set of three images, and the and 8p 
(In)2+(In)2+(In)2+(In)2 , {xbmxfMybmyfM when fn>0 jrIn(i,j)j={p+(+(+(, (In)2In)2In)2In)2 : xbMxfmybMyfmwhen 
fn<0 (10) We .rst compute the 2D smoothness estimation Lin (7) and the N onto the (normalized) vector 
!;!; in (8). Then in (9) we compute fn,the isophote direction  N above described technique is applied 
independently to each one. To avoid the appearance of spurious colors, we use a color model which is 
very similar to the LUV model, with one luminance and two chroma components. See Figure 3.  4 Results 
The CPU time required for inpainting depends on the size of n. In all the color examples here presented, 
the inpainting pro­cess was completed in less than 5 minutes (for the three color planes), using non-optimized 
C++ code running on a PentiumII L pute the change of Lalong the direction of !; images available from 
public databases over the Internet. The main examples here presented, and additional ones, can be seen 
at !;.Finally,wemultiplyN fnby a slope-limited version of the norm of the gradient of the im­http://www.ece.umn.edu/users/marcelo/restoration.html, 
where inIj, in (10). 8 A central differences realization would turn the age, jr addition to the original 
and inpainted images reproduced below, the evolution process can be observed. scheme unstable, and that 
is the reason for using slope-limiters. The subindexes band fdenote backward and forward differences 
re- Figure 4 shows, on the left, a synthetic image with the region to spectively, while the subindexes 
mand Mdenote the minimum or inpaint in white. Here nis large (30 pixels in diameter) and con­ maximum, 
respectively, between the derivative and zero (we have tains a hole. The inpainted reconstruction is 
shown on the right. omitted the space coordinates (i,j)for simplicity); see [16] for de- Notice that 
contours are recovered, joining points from the inner tails. Finally, let us note that the choice of 
a non-normalized .eld and outer boundaries. Also, these reconstructed contours follow N more stable 
numerical scheme; see [17, 18]. !; instead of a normalized version of it allows for a simpler and smoothly 
the direction of the isophotes arriving at @n(the algo­ rithm reported in [12] will fail with this type 
of data). Figure 5 shows a deteriorated B&#38;W image (.rst row) and its re­ construction (second row). 
As in all the examples in this article, the user only supplied the mask image (last row). This mask was 
drawn manually, using a paintbrush-like program. The vari­ ables were set to the values speci.ed in the 
previous section, and the number of iterations Twas set to 3000. When multiresolution Note once again 
that when the inpainting algorithm arrives to steady state, that is, It =0, we have geometrically solved 
r(Smoothness) r . JI=0, meaning that the smoothness is con­ stant along the isophotes. 9 When applying 
equations (4)-(10) to the pixels in the border @n of the region nto be inpainted, known pixels from outside 
this re­ gion are used. That is, conceptually, we compute equations (4)-(10) is not used, the CPU time 
required by the inpainting procedure was e in the region n(an cdilation of n), although we update the 
values approximately 7 minutes. With a 2-level multiresolution scheme, only inside n(that is, (4) is 
applied only inside n). The information only 2 minutes were needed. Observe that details in the nose 
and in the narrow band nis propagated inside n. Propagation of e ; n right eye of the middle girl could 
not be completely restored. This this information, both gray-values and isophotes directions, is fun­is 
in part due to the fact that the mask covers most of the relevant in­damental for the success of the 
algorithm. formation, and there is not much to be done without the use of highIn the restoration loop 
we perform Asteps of inpainting with level prior information (e.g., the fact that it is an eye). These 
minor (4), then Bsteps of diffusion with (3), again Asteps of (4), and errors can be corrected by the 
manual procedures mentioned in the so on. The total number of steps is T. This number may be pre­introduction, 
and still the overall inpainting time would be reduced established, or the algorithm may stop when changes 
in the image by orders of magnitude. This example was tested and showed to be are below a given threshold. 
The values we use are: A=15,B = robust to initial conditions inside the region to be inpainted. 2, at 
speed it=0:1.The value of Tdepends on the size of n. Figure 6 shows a vandalized image and its restoration, 
followed by an example where overimposed text is removed from the image. 8Note that jrIj=jrJIj. 9This 
type of information propagation is related and might be applicable 10We basically use the converged result 
of a lower resolution stage to to velocity .elds extension in level-set techniques [19, 20]. initialize 
the higher one, as classically done in image processing. Figure 4: Synthetic example: nis shown in white. 
Topology is not an issue, and the recovered contours smoothly continue the isophotes. These are typical 
examples where texture synthesis algorithms as those described in the introduction can not be used, since 
the num­ber of different regions to be .lled-in is very large. The next .gure shows the progressive nature 
of the algorithm, several intermediate steps of the inpainting procedure are shown, removing painted 
text over a natural scene. Finally, Figure 8 shows an entertainment application. The bungee cord and 
the knot tying the man s legs have been removed. Given the size of na 2-level multiresolution scheme 
was used. Here it becomes apparent that it is the user who has to supply the algorithm with the masking 
image, since the choice of the region to inpaint is completely subjective.  5 Conclusions and future 
work In this paper we have introduced a novel algorithm for image in­painting that attempts to replicate 
the basic techniques used by pro­fessional restorators. The basic idea is to smoothly propagate infor­mation 
from the surrounding areas in the isophotes direction. The user needs only to provide the region to be 
inpainted, the rest is automatically performed by the algorithm in a few minutes. The inpainted images 
are sharp and without color artifacts. The exam­ples shown suggest a wide range of applications like 
restoration of old photographs and damaged .lm, removal of superimposed text, and removal of objects. 
The results can either be adopted as a .nal restoration or be used to provide an initial point for manual 
restoration, thereby reducing the total restoration time by orders of magnitude. One of the main problems 
with our technique is the reproduction of large textured regions, as can be seen in Figure 9. The algorithm 
here proposed is currently being tested in conjunction with texture synthesis ideas to address this issue. 
We are mainly investigation the combination of this approach with the reaction-diffusion ideas of Kass 
and Witkin and of Turk. An ideal algorithm should be able to automatically switch between textured and 
geometric areas, and select the best suited technique for each region. We would also like to investigate 
how to inpaint from partial degradation. In the example of the old photo for example, ideally the mask 
should not be binary, since some underlying information exists in the degraded areas. The inpainting 
algorithm here presented has been clearly mo­tivated by and has borrowed from the intensive work on the 
use of Partial Differential Equations (PDE s) in image processing and computer vision. When blindly letting 
the grid go to zero, the inpainting technique in equations (4)-(10) naively resembles a third order equation, 
for which too many boundary conditions are im­posed (being all of them essential). Although theoretical 
results for high order equations are available, e.g., [21], and some properties like preservation of 
the image moments can be immediately proved for our corresponding equation (this was done by A. Bertozzi), 
fur­ther formal study of our high order equation is needed (see also [22, 23]). Nevertheless, this suggests 
the investigation of the use of lower, second order, PDE s to address the inpainting problem. We can 
split the inpainting problem into two coupled variational for­mulations, one for the isophotes direction 
(point (2) in Section 3.1) and one for the gray-values, consistent with the estimated directions (point 
(3) in Section 3.1). The corresponding gradient descent .ows will give two coupled second order PDE s 
for which formal results regarding existence and uniqueness of the solutions can be shown. This is reported 
in [24].  Acknowledgments This work started when the authors were visiting the Institute Henri Poincare 
in Paris, France. We thank the organizers of the quarter on Mathematical Questions in Signal and Image 
Processing and the Institute for their hospitality and partial .nancial support. We would like to thank 
Tom Robbins and Elizabeth Buschor from the Upper Midwest Conservation Association for their help in linking 
our work with actual art restoration and conservation. Amy Miller, of Photo-Medic, kindly provided the 
damaged photograph shown in the examples. We also thank Dr. Santiago Betelu, Prof. Stan Osher, Prof. 
Eero Simoncelli, and Prof. Andrea Bertozzi for very interest­ing discussions and feedback. The reviewers 
provided many useful ideas. This work was partially supported by a grant from the Of.ce of Naval Research 
ONR-N00014-97-1-0509, the Of.ce of Naval Research Young Investigator Award, the Presidential Early Career 
Awards for Scientists and Engineers (PECASE), a National Science Foundation CAREER Award, by the National 
Science Foundation Learning and Intelligent Systems Program (LIS), and Universidad de la Republica (Uruguay). 
 References [1] S. Walden. The Ravished Image. St. Martin s Press, New York, 1985. [2] G. Emile-Male. 
The Restorer s Handbook of Easel Painting. Van Nostrand Reinhold, New York, 1976. [3] D. King. The Commissar 
Vanishes. Henry Holt and Company, 1997. [4] A.C. Kokaram, R.D. Morris, W.J. Fitzgerald, P.J.W. Rayner. 
Detection of missing data in image sequences. IEEE Transac­tions on Image Processing 11(4), 1496-1508, 
1995. [5] A.C. Kokaram, R.D. Morris, W.J. Fitzgerald, P.J.W. Rayner. Interpolation of missing data in 
image sequences. IEEE Trans­actions on Image Processing 11(4), 1509-1519, 1995. [6] C. Braverman. Photoshop 
retouching handbook. IDG Books Worldwide, 1998. [7] A. Hirani and T. Totsuka. Combining Frequency and 
spatial domain information for fast interactive image noise removal. Computer Graphics, pp. 269-276, 
SIGGRAPH 96, 1996. [8] A. Efros and T. Leung, Texture synthesis by non-parametric sampling, Proc. IEEE 
International Conference Computer Vi­sion, pp. 1033-1038, Corfu, Greece, September 1999. [9]D. Heegerand 
J. Bergen. Pyramid based texture analy­sis/synthesis. Computer Graphics, pp. 229-238, SIGGRAPH 95, 1995. 
[10] E. Simoncelli and J. Portilla. Texture characterization via joint statistics of wavelet coef.cient 
magnitudes. 5th IEEE Int l Conf. on Image Processing, Chicago, IL. Oct 4-7, 1998. [11] M. Nitzberg, 
D. Mumford, and T. Shiota, Filtering, Segmen­tation, and Depth, Springer-Verlag, Berlin, 1993. [12] 
S. Masnou and J.M. Morel. Level-lines based disocclusion. 5th IEEE Int l Conf. on Image Processing, Chicago, 
IL. Oct 4-7, 1998. [13] C. Kenney and J. Langan. A new image processing primitive: reconstructing images 
from modi.ed .ow .elds. University of California Santa Barbara Preprint, 1999. [14] P. Perona and J. 
Malik Scale-space and edge detection using anisotropic diffusion. IEEE-PAMI 12, pp. 629-639, 1990. [15] 
L. Alvarez, P.L. Lions, J.M. Morel. Image selective smoothing and edge detection by nonlinear diffusion.SIAM 
J. Numer. Anal. 29, pp. 845-866, 1992. [16] S. Osher and J. Sethian. Fronts propagating with curvature 
dependent speed: algorithms based on Hamilton-Jacobi for­mulations. Journal of Computational Physics, 
79:12-49, 1988.  [17] A. Marquina and S. Osher. Explicit algorithms for a new time dependent model based 
on level set motion for nonlinear de­bluring and noise removal. UCLA CAM Report 99-5, January 1999. 
 [18] L. Rudin, S. Osher and E. Fatemi. Nonlinear total variation based noise removal algorithms. Physica 
D, 60, pp. 259-268, 1992. [19] S. Osher, personal communication, October 1999. [20] H. K. Zhao, T. Chan, 
B. Merriman, and S. Osher, A varia­tional level-set approach to multiphase motion, J. of Compu­tational 
Physics 127, pp. 179-195, 1996. [21] A. Bertozzi The mathematics of moving contact lines in thin liquid 
.lms. Notices Amer. Math. Soc., Volume 45, Number 6, pp. 689-697, June/July 1998. [22] J. Tumblin and 
G. Turk, LCIS: A boundary hierarchy for detail-preserving contrast reduction, Computer Graphics, pp. 
83-90, SIGGRAPH 99, 1999. [23] T. Chan and J. Shen, Mathematical models for local deter­ministic inpaintings, 
UCLA CAM TR 00-11, March 2000. [24] C. Ballester, M. Bertalmio, V. Caselles, G. Sapiro, and J. Verdera, 
Filling-in by joint interpolation of vector .elds and grey levels, University of Minnesota IMA TR, April 
2000.  Permission to make digital or hard copies of part or all of this work or personal or classroom 
use is granted without fee provided that copies are not made or distributed for profit or commercial 
advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, 
to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or 
a fee. SIGGRAPH 2000, New Orleans, LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00  Figure 
6: Restoration of a color image and removal of superimposed text. Figure 7: Progressive nature of the 
algorithm. Several intermediate steps of the reconstruction are shown. Figure 8: The bungee cord and 
the knot tying the man s feet have been removed.  Figure 9: Limitations of the algorithm: texture is 
not reproduced. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344976</article_id>
		<sort_key>425</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>48</seq_no>
		<title><![CDATA[Interactive multi-pass programmable shading]]></title>
		<page_from>425</page_from>
		<page_to>432</page_to>
		<doi_number>10.1145/344779.344976</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344976</url>
		<abstract>
			<par><![CDATA[<p>Programmable shading is a common technique for production animation, but interactive programmable shading is not yet widely available. We support interactive programmable shading on virtually any 3D graphics hardware using a scene graph library on top of OpenGL. We treat the OpenGL architecture as a general SIMD computer, and translate the high-level shading description into OpenGL rendering passes. While our system uses OpenGL, the techniques described are applicable to any retained mode interface with appropriate extension mechanisms and hardware API with provisions for recirculating data through the graphics pipeline.</p><p>We present two demonstrations of the method. The first is a constrained shading language that runs on graphics hardware supporting OpenGL 1.2 with a subset of the ARB imaging extensions. We remove the shading language constraints by minimally extending OpenGL. The key extensions are <italic>color range</italic> (supporting extended range and precision data types) and <italic>pixel texture</italic> (using framebuffer values as indices into texture maps). Our second demonstration is a renderer supporting the RenderMan Interface and RenderMan Shading Language on a software implementation of this extended OpenGL. For both languages, our compiler technology can take advantage of extensions and performance characteristics unique to any particular graphics hardware.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[OpenGL]]></kw>
			<kw><![CDATA[graphics hardware]]></kw>
			<kw><![CDATA[graphics systems]]></kw>
			<kw><![CDATA[illumination]]></kw>
			<kw><![CDATA[interactive rendering]]></kw>
			<kw><![CDATA[languages]]></kw>
			<kw><![CDATA[multi-pass rendering]]></kw>
			<kw><![CDATA[non-realistic rendering]]></kw>
			<kw><![CDATA[procedural shading]]></kw>
			<kw><![CDATA[programmable shading]]></kw>
			<kw><![CDATA[rendering]]></kw>
			<kw><![CDATA[texture mapping]]></kw>
			<kw><![CDATA[texture synthesis]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Languages</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003128</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction techniques</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010243</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Appearance and texture representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010542.10011714</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Other architectures->Special purpose systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Languages</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31089841</person_id>
				<author_profile_id><![CDATA[81100454294]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Peercy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SGI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39081319</person_id>
				<author_profile_id><![CDATA[81334487411]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Olano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SGI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39068982</person_id>
				<author_profile_id><![CDATA[81100218746]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Airey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intrinsic Graphics and SGI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P218244</person_id>
				<author_profile_id><![CDATA[81385600212]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[P.]]></first_name>
				<middle_name><![CDATA[Jeffrey]]></middle_name>
				<last_name><![CDATA[Ungar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SGI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BIRCH, P., BLYTHE, D., GRANTHAM, B., JONES. M., SCHAFER, M. SE- GAL, M., AND TANNER, C. An OpenGL++ Specification. SGI, March i997.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BLYTHE, D., GRANTHAM, B., KILGARD, M. J., MCREYNOLDS, T., NEL- SON, S. R., FOWLER, C., HUI, S., AND WOMACK, P. Advanced graphics programming techniques using OpenGL: Course notes. In Proceedings of SIGGRAPH '99 (July 1999).]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BOCK, D. Tech watch: Volume rendering. Computer Graphics Worm 22, 5 (May 1999).]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>165781</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BRIGGS, P. Register Allocation via Graph Coloring. PhD thesis, Rice University, April 1992.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>197972</ref_obj_id>
				<ref_obj_pid>197938</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[CABRAL, B., CAM, N., AND FORAN, J. Accelerated volume rendering and tomographic reconstruction using texture mapping hardware. 1994 Symposium on Volume Visualization (October 1994), 91-98. ISBN 0-89791-741-3.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311553</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[CABRAL, B., OLANO, M., AND NEMEC, P. Reflection space image based rendering. Proceedings of SIGGRAPH 99 (August 1999), 165-170.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808602</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[COOK, R. L. Shade trees. Computer Graphics (Proceedings of SIGGRAPH 84) 18, 3 (July 1984), 223-231. Held in Minneapolis, Minnesota.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>949896</ref_obj_id>
				<ref_obj_pid>949845</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[CORRIE, B., AND MACKERRAS, P. Data shaders. Visualization '93 1993 (1993).]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>949650</ref_obj_id>
				<ref_obj_pid>949607</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[CRAWFIS, R. A., AND ALLISON, M. J. A scientific visualization synthesizer. Visualization '91 (1991), 262-267.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>253308</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[DIEFENBACH, P. J., AND BADLER, N. I. Multi-pass pipeline rendering: Realism for dynamic environments. 1997 Symposium on Interactive 3D Graphics (April 1997), 59-70.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>151642</ref_obj_id>
				<ref_obj_pid>151640</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[FRASER, C. W., HANSON, D. R., AND PROEBSTING, T. A. Engineering a simple, efficient code generator generator. ACM Letters on Programming Languages and Systems 1, 3 (September 1992), 213-226.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>643327</ref_obj_id>
				<ref_obj_pid>643323</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[GRITZ, L., AND HAHN, J. K. BMRT: A global illumination implementation of the RenderMan standard. Journal of Graphics Tools 1, 3 (1996), 29-47.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97913</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[HAEBERLI, P. E., AND AKELEY, K. The accumulation buffer: Hardware support for high-quality rendering. Computer Graphics (Proceedings of SIG- GRAPH 90) 24, 4 (August 1990), 309-318.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97911</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[HANRAHAN, P., AND LAWSON, J. A language for shading and lighting calculations. Computer Graphics (Proceedings of SIGGRAPH 90) 24, 4 (August 1990), 289-298.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>255312</ref_obj_id>
				<ref_obj_pid>255305</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[HARRIS, M. Extending microcode compaction for real architectures. In Proceedings of the 20th annual workshop on Microprogramming (1987), pp. 40- 53.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311575</ref_obj_id>
				<ref_obj_pid>311534</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[HART, J. C., CARR, N., KAMEYA, M., TIBBITTS, S. A., AND COLEMAN, T. J. Antialiased parameterized solid texturing simplified for consumer-level hardware implementation. 1999 SIGGRAPH / Eurographics Workshop on Graphics Hardware (August 1999), 45-53.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311554</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[HEIDRICH, W., AND SEIDEL, H.-P. Realistic, hardware-accelerated shading and lighting. Proceedings of SIGGRAPH 99 (August 1999), 171-178.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>300538</ref_obj_id>
				<ref_obj_pid>300523</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[HEIDRICH, W., WESTERMANN, R., SEIDEL, H.-P., AND ERTL, T. Applications of pixel textures in visualization and realistic image synthesis. 1999 ACM Symposium on Interactive 3D Graphics (April 1999), 127-134. ISBN 1-58113-082-1.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[JAQUAYS, P., AND HOOK, B. Quake 3: Arena shader manual, revision 10. In Game Developer 'S Conference Hardcore Technical Seminar Notes (December 1999), C. Hecker and J. Lander, Eds., Miller Freeman Game Group.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383838</ref_obj_id>
				<ref_obj_pid>2383815</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[KAUTZ, J., AND McCOOL, M. D. Interactive rendering with arbitrary brdfs using separable approximations. Eurographics Rendering Workshop 1999 (June 1999). Held in Granada, Spain.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258769</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[KELLER, A. Instant radiosity. Proceedings of SIGGRAPH97 (August 1997), 49-56.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>555172</ref_obj_id>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[KYLANDER, K., AND KYLANDER, O. S. Gimp: The Official Handbook. The Coriolis Group, 1999.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383822</ref_obj_id>
				<ref_obj_pid>2383815</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[MAX, N., DEUSSEN, O., AND KEATING, B. Hierarchical image-based rendering using texture mapping hardware. Rendering Techniques '99 (Proceedings of the lOth Eurographics Workshop on Rendering) (June 1999), 57-62.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311585</ref_obj_id>
				<ref_obj_pid>311534</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[McCOOL, M. D. AND HEIDRICH, W. Texture shaders. 1999 SIGGRAPH/ Eurographics Workshop on Graphics Hardware (August 1999), 117-126.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[OLANO, M., HART, J. C., HEIDRICH, W., MCCOOL, M., MARK, B., AND PROUDFOOT, K. Approaches for procedural shading on graphics hardware: Course notes. In Proceedings of SIGGRAPH 2000 (July 2000).]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280857</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[OLANO, M., AND LASTRA, A. A shading language on graphics hardware: The PixelFlow shading system. Proceedings of SIGGRAPH 98 (July 1998), 159-168.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[OPENGL ARB. Extension specification documents, http://www.opengl.org- /Documentation/Extensions.html, March 1999.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[PIXAR. The RenderMan Inte~ace Specification: Version 3.1. Pixar Animation Studios, September 1999.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[SEGAL, M., AKELEY, K., FRAZIER, C., AND LEECH, J. The OpenGL Graphics System: A Specification (Version 1.2.1). Silicon Graphics, Inc., 1999.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[SGI TECHNICAL PUBLICATIONS. Cosmo 3D Programmer's Guide. SGI Technical Publications, 1998.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97923</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[SIMS, K. Particle animation and rendering using data parallel computation. Computer Graphics (Proceedings of SIGGRAPH 90) 24, 4 (August 1990), 405-413.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[UPSTILL, S. The RenderMan Companion. Addison-Wesley, 1989.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Interactive Multi-Pass Programmable Shading Mark S. Peercy, Marc Olano, John Airey, P . Jeffrey Ungar 
SGI Abstract Programmable shading is a common technique for production an­imation, but interactive programmable 
shading is not yet widely available. We support interactive programmable shading on vir­tually any 3D 
graphics hardware using a scene graph library on top of OpenGL. We treat the OpenGL architecture as a 
general SIMD computer, and translate the high-level shading description into OpenGL rendering passes. 
While our system uses OpenGL, the techniques described are applicable to any retained mode in­terface 
with appropriate extension mechanisms and hardware API with provisions for recirculating data through 
the graphics pipeline. We present two demonstrations of the method. The .rst is a constrained shading 
language that runs on graphics hardware supporting OpenGL 1.2 with a subset of the ARB imaging exten­sions. 
We remove the shading language constraints by minimally extending OpenGL. The key extensions are color 
range (support­ing extended range and precision data types) and pixel texture (us­ing framebuffer values 
as indices into texture maps). Our second demonstration is a renderer supporting the RenderMan Interface 
and RenderMan Shading Language on a software implementation of this extended OpenGL. For both languages, 
our compiler tech­nology can take advantage of extensions and performance charac­teristics unique to 
any particular graphics hardware. CR categories and subject descriptors: I.3.3 [Computer Graphics]: Picture/Image 
generation; I.3.7 [Image Processing]: En­ hancement. Keywords: Graphics Hardware, Graphics Systems, Illumina­tion, 
Languages, Rendering, Interactive Rendering, Non-Realistic Rendering, Multi-Pass Rendering, Programmable 
Shading, Proce­dural Shading, Texture Synthesis, Texture Mapping, OpenGL. 1 INTRODUCTION Programmable 
shading is a means for specifying the appearance of objects in a synthetic scene. Programs in a special 
purpose lan­guage, known as shaders, describe light source position and emis­sion characteristics, color 
and re.ective properties of surfaces, or transmittance properties of atmospheric media. Conceptually, 
these programs are executed for each point on an object as it is being ren­dered to produce a .nal color 
(and perhaps opacity) as seen from a given viewpoint. Shading languages can be quite general, having 
 Now at Intrinsic Graphics Permission to make digital or hard copies of part or all of this work or 
personal or classroom use is granted without fee provided that copies are not made or distributed for 
profit or commercial advantage and that copies bear this notice and the full citation on the first page. 
To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific 
permission and/or a fee. SIGGRAPH 2000, New Orleans, LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 
constructs familiar from general purpose programming languages such as C, including loops, conditionals, 
and functions. The most common is the RenderMan Shading Language [32]. The power of shading languages 
for describing intricate light­ing and shading computations been widely recognized since Cook s seminal 
shade tree research [7]. Programmable shading has played a fundamental role in digital content creation 
for motion pictures and television for over a decade. The high level of abstraction in programmable shading 
enables artists, storytellers, and their techni­cal collaborators to translate their creative visions 
into images more easily. Shading languages are also used for visualization of scien­ti.c data. Special 
data shaders have been developed to support the depiction of volume data [3, 8], and a texture synthesis 
language has been used for visualizing data .elds on surfaces [9]. Image process­ing scripting languages 
[22, 31] also share much in common with programmable shading. Despite its proven usefulness in software 
rendering, hardware acceleration of programmable shading has remained elusive. Most hardware supports 
a parametric appearance model, such as Phong lighting evaluated per vertex, with one or more texture 
maps ap­plied after Gouraud interpolation of the lighting results [29]. The general computational nature 
of programmable shading, and the un­bounded complexity of shaders, has kept it from being supported widely 
in hardware. This paper describes a methodology to support programmable shading in interactive visual 
computing by compil­ing a shader into multiple passes through graphics hardware. We demonstrate its use 
on current systems with a constrained shading language, and we show how to support general shading languages 
with only two hardware extensions. 1.1 Related Work Interactive programmable shading, with dynamically 
changing shader and scene, was demonstrated on the PixelFlow system [26]. PixelFlow has an array of general 
purpose processors that can ex­ecute arbitrary code at every pixel. Shaders written in a language based 
on RenderMan s are translated into C++ programs with em­bedded machine code directives for the pixel 
processors. An appli­cation accesses shaders through a programmable interface exten­sion to OpenGL. The 
primary disadvantages of this approach are the additional burden it places on the graphics hardware and 
driver software. Every system that supports a built-in programmable in­terface must include powerful 
enough general computing units to execute the programmable shaders. Limitations to these computing units, 
such as a .xed local memory, will either limit the shaders that may be run, have a severe impact on performance, 
or cause the system to revert to multiple passes within the driver. Further, ev­ery such system will 
have a unique shading language compiler as part of the driver software. This is a sophisticated piece 
of software which greatly increases the complexity of the driver. Our approach to programmable shading 
stands in contrast to the programmable hardware method. Its inspiration is a long line of interactive 
algorithms that follow a general theme: treat the graph­ics hardware as a collection of primitive operations 
that can be used to build up a .nal solution in multiple passes. Early examples of this model include 
multi-pass shadows, planar re.ections, highlights on top of texture, depth of .eld, and light maps [2, 
10]. There has been a dramatic surge of research in this area over the past few years. Sophisticated 
appearance computations, which had previously been available only in software renderers, have been mapped 
to generic graphics hardware. For example, lighting per pixel, general bidi­rectional re.ectance distribution 
functions, and bump mapping now run in real-time on hardware that supports none of those effects na­tively 
[6, 17, 20, 24]. Consumer games like ID Software s Quake 3 make extensive use of multi-pass effects [19]. 
Quake 3 recognizes that multi-pass provides a .exible method for surface design and takes the impor­tant 
step of providing a scripting mechanism for rendering passes, including control of OpenGL blending mode, 
alpha test functions, and vertex texture coordinate assignment. In its current form, this scripting language 
does not provide access to all of the OpenGL state necessary to treat OpenGL as a general SIMD machine. 
A team at Stanford has been investigating real-time pro­grammable shading. Their focus is a framework 
and language that explicitly divides operations into those that are executed at the ver­tex processing 
stage in the graphics pipeline and those that are exe­cuted at the fragment processing stage [25]. The 
hardware in all of these cases is being used as a com­puting machine rather than a special purpose accelerator. 
Indeed, graphics hardware has been used to accelerate techniques such as back-projection for tomographic 
reconstruction [5] and radiosity approximations [21]. It is now recognized that some new hardware features, 
such as multi-texture [24, 29], pixel texture [17], and color matrix [23], are particularly valuable 
for supporting these advanced computations interactively. 1.2 Our Contribution In this paper, we embrace 
and extend previous multi-pass tech­niques. We treat the OpenGL architecture as a SIMD computer. OpenGL 
acts as an assembly language for shader execution. The challenge, then, is to convert a shader into an 
ef.cient set of OpenGL rendering passes on a given system. We introduce a com­piler between the application 
and the graphics library that can target shaders to different hardware implementations. This philosophy 
of placing the shading compiler above the graphics API is at the core of our work, and has a number of 
advantages. We believe the number of languages for interactive programmable shading will grow and evolve 
over the next sev­eral years, responding to the unique performance and feature de­mands of different 
application areas. Likewise, hardware will in­crease in performance and many new features will be introduced. 
Our methodology allows the languages, compiler, and hardware to evolve independently because they are 
cleanly decoupled. This paper has three main contributions. First, we formalize the idea of using OpenGL 
as an assembly language into which pro­grammable shaders are translated, and we show how to apply dy­namic 
tree-rewriting compiler technology to optimize the mapping between shading languages and OpenGL (Section 
2). Second, we demonstrate the immediate application of this approach by intro­ducing a constrained shading 
language that runs interactively on most current hardware systems (Section 3). Third, we describe the 
color range and pixel texture OpenGL extensions that are neces­sary and suf.cient to accelerate fully 
general shading languages (Section 4). As a demonstration of the viability of this solution, we present 
a complete RenderMan renderer including full support of the RenderMan Shading Language running on a software 
im-   Pixel Operations (lookup table, Vertex Operations (transforms, color matrix, minmax) tex coord 
generation, lighting)  Figure 1:     plementation of this extended OpenGL. We close the paper with 
a discussion (Section 5) and conclusion (Section 6).  2 THE SHADING FRAMEWORK There is great diversity 
in modern 3D graphics hardware. Each graphics system includes unique features and performance charac­teristics. 
Countering this diversity, all modern graphics hardware also supports the basic features of the OpenGL 
API standard. While it is possible to add shading extensions to graphics hard­ware, OpenGL is powerful 
enough to support shading with no ex­tensions at all. Building programmable shading on top of standard 
OpenGL decouples the hardware and drivers from the language, and enables shading on every existing and 
future OpenGL-based graphics system. A compiler turns shading computations into multiple passes through 
the OpenGL rendering pipeline (Figure 1). This compiler can produce a general set of rendering passes, 
or it can use knowl­edge of the target hardware to pick an optimized set of passes. 2.1 OpenGL as an 
Assembly Language One key observation allows shaders to be translated into multi-pass OpenGL: a single 
rendering pass is also a general SIMD instruction the same operations are performed simultaneously for 
all pixels in an object. At the simplest level, the framebuffer is an accumu­lator, texture or pixel 
buffers serve as per-pixel memory storage, blending provides basic arithmetic operations, lookup tables 
sup­port function evaluation, the alpha test provides a variety of con­ditionals, and the stencil buffer 
allows pixel-level conditional exe­cution. A shader computation is broken into pieces, each of which 
can be evaluated by an OpenGL rendering pass. In this way, we build up a .nal result for all pixels in 
an object (Figure 2). There are typically several ways to map shading operations into OpenGL. We have 
implemented the following: Data Types: Data with the same value for every pixel in an ob­ject are called 
uniform, while data with values that may vary from pixel to pixel are called varying. Uniform data types 
are handled outside the graphics pipeline. The framebuffer retains intermediate varying results. Its 
four channels may hold one quadruple (such as a homogeneous point), one triple (such as a vector, normal, 
point, or color) and one scalar, or four independent scalars. We have made no attempt to handle varying 
data types with more than four chan­nels. The framebuffer channels (and hence independent scalars or 
#include "marble.h" surface marble() { varying color a; uniform string tx; uniform float x; x = 1/2; 
tx = "noisebw.tx"; FB = texture(tx,scale(x,x,x)); repeat(3) { x = x*.5; FB *= .5; FB += texture(tx,scale(x,x,x)); 
} FB = lookup(FB,tab); a = FB; FB = diffuse; FB *= a; FB += environment("env"); }   the components 
of triples and quadruples) can be updated selec­tively on each pass by setting the write-mask with glColorMask. 
Variables: Varying global, local, and temporary variables are transferred from the framebuffer to a named 
texture using glCopyTexSubImage2D, which copies a portion of the frame­buffer into a portion of a texture. 
In our system, these textures can be one channel (intensity) or four channels (RGBA), depending on the 
data type they hold. Variables are used either by drawing a tex­tured copy of the object bounding box 
or by drawing the object ge­ometry using a projective texture. The relative speed of these two methods 
will vary from graphics system to graphics system. In­tensity textures holding scalar variables are expanded 
into all four channels during rasterization and can therefore be restored into any framebuffer channel. 
Arithmetic Operations: Most arithmetic operations are per­formed with framebuffer blending. They have 
two operands: the framebuffer contents and an incoming fragment. The incom­ing fragment may be produced 
either by drawing geometry (ob­ject color, a texture, a stored variable, etc.) or by copying pix­els 
from the framebuffer and through the pixel operations with glCopyPixels. Data can be permuted (swizzled) 
from one framebuffer channel to another or linearly combined more gen­erally using the color matrix during 
a copy. The framebuffer blending mode, set by glBlendEquation, glBlendFunc, and glLogicOp, supports overwriting, 
addition, subtraction, mul­tiplication, bit-wise logical operations, and alpha blending. Unex­tended 
OpenGL does not have a divide blend mode. We handle di­vide using multiplication by the reciprocal. The 
reciprocal is com­puted like other mathematical functions (see below). More com­plicated binary operations 
are reduced to a combination of these primitive operations. For example, a dot product of two vectors 
is a component-wise multiplication followed by a pixel copy with a color matrix that sums the resulting 
three components together. Mathematical and Shader Functions: Mathematical func­tions with a single 
scalar operand (e.g. sin or reciprocal) use color or texture lookup tables during a framebuffer-to-framebuffer 
pixel copy. Functions with more than one operand (e.g. atan2) or a sin­gle vector operand (e.g. normalize 
or color space conversion) are broken down into simpler monadic functions and arithmetic opera­tions, 
each of which can be supported in a pass through the OpenGL pipeline. Some shader functions, such as 
texturing and diffuse or specular lighting, have direct correspondents in OpenGL. Often, complex mathematical 
and shader functions are simply translated to a series of simpler shading language functions. Flow Control: 
Stenciling, set by glStencilFunc and glStencilOp, limits the effect of all operations to only a subset 
of the pixels, with other pixels retaining their original framebuffer values. We use one bit of the stencil 
to identify pixels in the ob­ject, and additional stencil bits to identify subsets of those pixels that 
pass varying conditionals (if-then-else constructs and loops). One stencil bit is devoted to each level 
of nesting. Loops with uni­form control and conditionals with uniform relations do not need a stencil 
bit to control their in.uence because they affect all pixels. A two step process is used to set the stencil 
bit for a varying conditional. First, the relation is computed with normal arithmetic operations, such 
that the result ends up in the alpha channel of the framebuffer. The value is zero where the condition 
is true and one where it is false. Next, a pixel copy is performed with the alpha test enabled (set by 
glAlphaFunc). Only fragments that pass the alpha test are passed on to the stenciling stage of the OpenGL 
pipeline. A stencil bit is set for all of these fragments. The stencil remains unchanged for fragments 
that failed the alpha test. In some cases, the .rst operation in the body of the conditional can occur 
in the same pass that sets the stencil.  The passes corresponding to the different blocks of shader 
code at different nesting levels affect only those pixels that have the proper stencil mask. Because 
we are executing a SIMD compu­tation, it is necessary to evaluate both branches of if-then-else con­structs 
whose relation varies across an object. The stencil compare for the else clause simply uses the complement 
of the stencil bit for the then clause. Similarly, it is necessary to repeat a loop with a varying termination 
condition until all pixels within the object exit the loop. This requires a test that examines all of 
the pixels within the object. We use the minmax function from the ARB imaging extension as we copy the 
alpha channel to determine if any alpha values are non-zero (signifying they still pass the looping condi­tion). 
If so, the loop continues. 2.2 OpenGL Encapsulation We encapsulate OpenGL instructions in three kinds 
of rendering passes: GeomPasses, CopyPasses, and CopyTexPasses. Geom-Passes draw geometry to use vertex, 
rasterization, and fragment operations. CopyPasses copy a subregion of the framebuffer (via glCopyPixels) 
back into the same place in the framebuffer to use pixel, rasterization, and fragment operations. A stencil 
allows the CopyPass to avoid operating on pixels outside the object. Copy-TexPasses copy a subregion 
of the framebuffer into a texture object (via glCopyTexSubImage2D) and also utilize pixel operations. 
There are two subtypes of GeomPass. The .rst draws the object geometry, including normal vectors and 
texture coordinates. The second draws a screen-aligned bounding rectangle that covers the object using 
stenciling to limit the operations to pixels on the ob­ject. Each pass maintains the relevant OpenGL 
state for its path through the pipeline. State changes on drawing are minimized by only setting the state 
in each pass that is not default and immedi­ately restoring that state after the pass. 2.3 Compiling 
to OpenGL The key to supporting interactive programmable shading is a com­piler that translates the shading 
language into OpenGL assembly. This is a CISC-like compiler problem because OpenGL passes are complex 
instructions. The problem is somewhat simpli.ed due to constraints in the language and in OpenGL as an 
instruction set. For example, we do not have to worry about instruction scheduling since there is no 
overlap between rendering passes. Our compiler implementation is guided by a desire to retarget the compiler 
to easily take advantage of unique features and perfor­mance and to pick the best set of passes for each 
target architecture. We also want to be able to support multiple shading languages and adapt as languages 
evolve. To help meet these goals, we built our compiler using an in-house tool inspired by the iburg 
code gen­eration tool [11], though we use it for all phases of compilation. This tool .nds the least-cost 
covering of a tree representation of the shader based on a text .le of patterns. A simple example can 
show how the tree-matching tool op­erates and how it allows us to take advantage of extensions to OpenGL. 
Part of a shader might be matched by a pair of tex­ture lookups, each with a cost of one, or by a single 
multi-texture lookup, also with a cost of one. In this case, multi-texture is cheaper because it has 
a total cost of one instead of two. Using similar matching rules and semantic actions, the compiler can 
make use of fragment lighting, light texture, noise generation, divide or condi­tional blends, or any 
other OpenGL extension [16, 27]. The entire shader is matched at once, giving the set of match­ing rules 
that cover the shader with the least total cost. For exam­ple, the computations surrounding the above 
pair of texture lookups expand the set of possible matching rules. Given operation A, tex­ture lookup 
B, texture lookup C, and operation D, it may be pos­sible to do all of the operations in four separate 
passes (A,B,C,D), to do the surrounding operations separately while combining the texture lookups into 
one multi-texture pass for a total cost of three (A,BC,D), or to combine one computation with each texture 
lookup for a cost of two (AB,CD). By considering the entire shader we can choose the set of matching 
rules with the least overall cost. When we use the tool for .nal OpenGL pass generation, we currently 
use the number of passes as the cost for each matching rule. For performance optimization, the costs 
should correspond to predicted rendering speed, so the cost for a GeomPass would be different from the 
cost for a CopyPass or a CopyTexPass. The pattern matching happens in two phases, labeling and re­ducing. 
Labeling is done bottom-up through the abstract syntax tree, using dynamic programming to .nd the least-cost 
set of pat­tern match rules. Reducing is done top-down, with one semantic action run before the node 
s children are reduced and one after. The iburg-like label/reduce tool proved useful for more than just 
.nal pass selection. We use it for shader syntax checking, constant folding, and even memory allocation 
(although most of the memory allocation algorithm is in the code associated with a small number of rules). 
The ease of changing costs and creating new matching rules allows us to achieve our goal of .exible retargeting 
of the compiler for different hardware and shading languages. 2.4 Scene Graph Support Since objects 
may be rendered multiple times, it is necessary to retain geometry data and to deliver it repeatedly 
to the graphics hardware. In addition, shaders need to be associated with objects to describe their appearances, 
and the shaders and objects need to be translated into OpenGL passes to render an image. Our framework 
supports these operations in a scene graph used by an application through the addition of new scene graph 
containers and new traver­sals. In our implementation, we have extended the Cosmo3D scene graph library 
[30]. Cosmo3D uses a familiar hierarchical scene graph. Internal nodes describe coordinate transformations, 
while the leaves are Shape nodes, each of which contains a list of Geome­try and an Appearance. Traversals 
of the scene graph are known as actions.A DrawAction, for example, is applied to the scene graph to render 
the objects into a window. We have implemented a new appearance class that contains shaders. When included 
in a shape node, this appearance com­pletely describes how to shade the geometry in the shape. The shaders 
may include a list of active light shaders, a displacement shader, a surface shader, and an atmosphere 
shader. In addition, we have implemented a new traversal, known as a ShadeAction.A ShadeAction converts 
a scene graph containing shapes with the new appearance into another Cosmo3D scene graph describing the 
mul­tiple passes for all of the objects in the original scene graph. (The transformation of scene graphs 
is a powerful, general technique that has been proposed to address a variety of problems [1].) The key 
element of the ShadeAction is the shading language compiler that converts the shaders into multiple passes. 
A ShadeAction may treat multiple objects that share the same shader as a single, combined object to minimize 
overhead. A DrawAction applied to this second scene graph renders the .nal image. The scene graph passes 
information to the compiler including the matrix to transform from the object s coordinate system into 
camera space and the screen space footprint for the geometry. The footprint is computed during the ShadeAction 
by projecting a 3D bounding box of the geometry into screen space and computing an axis-aligned 2D bounding 
box of the eight projected points. Only pixels within the 2D bounding box are copied on a CopyPass or 
drawn on the quad-GeomPass to minimize unnecessary data move­ment when shading each object. We provide 
support for debugging at the single-step, pass­by-pass level through special hooks inserted into the 
DrawAction. Each pass is held in an extended Cosmo3D Group node, which in­vokes the debugging hook functions 
when drawn. Each pass is also tagged with the line of source code that generated it, so everything from 
shader source-level debugging to pass-by-pass image dumps is possible. Hooks at the per-pass level also 
let us monitor or es­timate performance. At the coarsest level, we can .nd the number of passes executed, 
but we can also examine each pass to record details like pixels written or time to draw.  3 EXAMPLE: 
INTERACTIVE SL We have developed a constrained shading language, called ISL (for Interactive Shading 
Language) [25] and an ISL compiler to demon­strate our method on current hardware. ISL is similar in 
spirit to the RenderMan Shading Language in that it provides a C-like syntax to specify per-pixel shading 
calculations, and it supports separate light, surface, and atmosphere shaders. Data types include varying 
colors, and uniform .oats, colors, matrices, and strings. Local vari­ables can hold both uniform and 
varying values. Nestable .ow con­trol structures include loops with uniform control, and uniform and 
varying conditionals. There are built-in functions for diffuse and specular lighting, texture mapping, 
projective textures, environment mapping, RGBA one-dimensional lookup tables, and per-pixel ma­  surface 
celtic() { varying color a; FB = diffuse; FB *= color(.5,.2,0.,1.); a = FB; FB = specular(30.); FB 
+= a; FB *= texture("celtic"); a = FB; FB = 1; celtic"); FB *= texture("silk"); FB *= .15; FB += a; 
} distantlight leaves(uniform string map = "leaves", ...) { uniform float tx; uniform float ty; uniform 
float tz;  tx = frame*speedx+phasex; ty = frame*speedy+phasey; tz = frame*speedz+phasez; FB = project(map, 
   scale(sx,sx,sx)*    rotate(0,0,1,rx)*    translate(ax*sin(tx),0,0)*    shadermatrix); 
 FB *= project(map,    scale(sy,sy,sy)*...); } uniform matrix lt = (0,0,0,0,  0,0,0,0,1,1,1,0,0,0,0,1); 
surface bump(uniform string b="";  uniform string tx = "") { uniform matrix m; FB = texture(b); 
m = objectmatrix; m[0][3] = m[1][3] = m[2][3] = 0.; m[3][3] = m[3][0] = m[3][1] = 0.; m[3][2] = 0.; 
m = lt*m*transl    scale(2,2,2); FB = transform(FB,m); FB *= texture(tx); } #include "threshtab.h" 
surface shipRockRot(...) { varying color a, b, c; FB = texture(rot); FB *= .5;      s(.08*frame)); 
FB = lookup(FB,mtab); c = FB; FB = color(1,1, FB *= texture(t1); a = FB; FB = texture(t2); FB *= texture(rot); 
FB = diffuse; FB *= color(.5,.2,0,1); b = FB; FB = specular(30.); FB += b; FB *= texture(t2); FB *= c; 
FB += a;     } #include "swizzle.h" table greentable = { {0,.2,0,1},    {0,.4,0,1) }; surface 
toon(uniform float do = 1.; uniform float edge = .25 ) { FB = environment("park.env"); if (do > .5) 
{ FB += edge; FB =transform(FB,rgba_rrra); FB =lookup(FB,greentable); FB += environment("sun"); } } 
 trix transformations. In addition, ISL supports uniform shader pa­rameters and a set of uniform global 
variables (shader space, object space, time, and frame count). We have intentionally constrained ISL 
in a number of ways. First, we only chose primitive operations and built-in functions that can be executed 
on any hardware supporting base OpenGL 1.2 plus the color matrix extension. Consequently, many current 
hard­ware systems can support ISL. (If the color matrix transformation is eliminated, ISL should run 
anywhere.) This constraint provides the shader writer with insight into how limited precision of current 
commercial hardware may affect the shader. Second, the syntax does not allow varying expressions of expressions, 
which ensures that the compiler does not need to create any temporary storage not already made explicit 
in the shader. As a result, the writer of a shader knows by inspection the worst-case temporary storage 
re­quired by the shading code (although the compiler is free to use less storage, if possible). Third, 
arbitrary texture coordinate computa­tion is not supported. Texture coordinates must come either from 
the geometry or from the standard OpenGL texture coordinate gen­eration methods and texture matrix. One 
consequence of these design constraints is that ISL shad­ing code is largely decoupled from geometry. 
For example, since shader parameters are uniform there is no need to attach them di­rectly to each surface 
description in the scene graph. As a result, ISL and the compiler can migrate from application to application 
and scene graph to scene graph with relative ease. 3.1 Compiler We perform some simple optimizations 
in the parser. For instance, we do limited constant compression by evaluating at parse time all expressions 
that are declared uniform. When parameters or the shader code change, we must reparse the shader. In 
our current sys­tem, we do this every time we perform a ShadeAction. A more so­phisticated compiler, 
such as the one implemented for the Render-Man Shading Language (Section 4) performs these optimizations 
outside the parser. We expand the parse trees for all of the shaders in an appear­ance (light shaders, 
surface shader, and atmosphere shader) into a single tree. This tree is then labeled and reduced using 
the tree matching compiler tool described in Section 2.3. The costs fed into the labeler instruct the 
compiler to minimize the total number of passes, regardless of the relative performance of the different 
kinds of passes. The compiler recognizes and optimizes subexpressions such as a texture, diffuse, or 
specular lighting multiplied by a constant. The compiler also recognizes when a local variable is assigned 
a value that can be executed in a single pass. Rather than executing the pass, storing the result, and 
retrieving it when referenced, the compiler simply replaces the local variable usage with the single 
pass that describes it. 3.2 Demonstration We have implemented a simple viewer on top of the extended 
scene graph to demonstrate ISL running interactively. The viewer sup­ports mouse interaction for rotation 
and translation. Users can also modify shaders interactively in two ways. They can edit shader text .les, 
and their changes are picked up immediately in the viewer. Additionally, they can modify parameters by 
dragging sliders, ro­tating thumb-wheels, or entering text in a control panel. The viewer creates the 
control panel on the .y for any selected shader. Changes to the parameters are seen immediately in the 
window. Examples of the viewer running ISL are given in Figures 2 and 3.  4 EXAMPLE: RENDERMAN SL RenderMan 
is a rendering and scene description interface standard developed in the late 1980s [14, 28, 32]. The 
RenderMan stan­dard includes procedural and bytestream scene description inter­faces. It also de.nes 
the RenderMan Shading Language, which is the de facto standard for programmable shading capability and 
represents a well-de.ned goal for anyone attempting to accelerate programmable shading. The RenderMan 
Shading Language is extremely general, with control structures common to many programming languages, 
rich data types, and an extensive set of built-in operators and geomet­ric, mathematical, lighting, and 
communication functions. The lan­guage originally was designed with hardware acceleration in mind, so 
complicated or user-de.ned data types that would make acceler­ation more dif.cult are not included. It 
is a large but straightforward task to translate the RenderMan Shading Language into multi-pass OpenGL, 
assuming the following two extensions: Extended Range and Precision Data Types: Even the sim­plest RenderMan 
shaders have intermediate computations that re­quire data values to extend beyond the range [0-1], to 
which OpenGL fragment color values are clamped. In addition, they need higher precision than is found 
in current commercial hard­ware. With the color range extension, color data can have an implementation-speci.c 
range to which it is clamped during raster­ization and framebuffer operations (including color interpolation, 
texture mapping, and blending). The framebuffer holds colors of the new type, and the conversion to a 
displayable value happens only upon video scan-out. We have used the color range extension with an IEEE 
single precision .oating point data type or a subset thereof to support the RenderMan Shading Language. 
Pixel Texture: RenderMan allows texture coordinates to be computed procedurally. In this case, texture 
coordinates cannot be expected to change linearly across a geometric primitive, as re­quired in unextended 
OpenGL. This general two-dimensional indi­rection mechanism can be supported with the OpenGL pixel texture 
extension [17, 18, 27]. This extension allows the (possibly .oat­ing point) contents of the framebuffer 
to be used as texture indices when pixels are copied from the framebuffer. The red, green, blue, and 
alpha channels are used as texture coordinates s, t, r, and q, respectively. We use pixel texture not 
only to index two dimen­sional textures but also to index extremely wide one-dimensional textures. These 
wide textures are used as lookup tables for math­ematical functions such as sin, reciprocal, and sqrt. 
These can be simple piecewise linear approximations, starting points for Newton iteration, components 
used to construct the more complex mathe­matical functions, or even direct one-to-one mappings for a 
reduced .oating point format. 4.1 Scene Graph Support The RenderMan Shading Language demands greater 
support from the scene graph library than ISL because geometry and shaders are more tightly coupled. 
Varying parameters can be supplied as four values that correspond to the corners of a surface patch, 
and the parameter over the surface is obtained through bilinear interpola­tion. Alternatively, one parameter 
value may be supplied per con­trol point for a bicubic patch mesh or a NURBS patch, and the parameter 
is interpolated using the same basis functions that de­.ne the surface. We associate a (possibly empty) 
list of named pa­rameters with each surface to hold any parameters provided when the surface is de.ned. 
When the surface geometry is tessellated to form GeoSets (triangle strip sets and fan sets, etc.), its 
parame­ters are transferred to the GeoSets so that they may be referenced  illuminance  noise  http://www.renderman.org 
 and drawn as vertex colors by the passes produced by the compiler. Similarly, a shader may require 
derivatives of surface properties, such as the partial derivatives of the position (dP/du and dP/dv) 
either as global variables or through a differential function such as calculatenormal. A shader may also 
use derivatives of user-supplied parameters. The compiler can request from the scene graph any of these 
quantities evaluated over a surface at the same points used in its tessellation. As with any other parameter, 
they are computed on the host and stored in the vertex colors for the surface. Where possible, lazy evaluation 
ensures that the user does not pay in time or space for this support unless requested. 4.2 Compiler 
Our RenderMan compiler is based on multiple phases of the tree­ matching tool described in Section 2.3. 
The phases include: Parsing: convert source into an internal tree representation. Phase0: detect errors 
Phase1: perform context-sensitive typing (e.g. noise, texture) Phase2: detect and compress uniform expressions 
Phase3: compute difference trees for Derivatives Phase4: determine variable usage and live range information 
Phase5: identify possible OpenGL instruction optimizations Phase6: allocate memory for variables Phase7: 
generate optimized, machine speci.c OpenGL The mapping of RenderMan to OpenGL follows the method­ology 
described in Section 2.1. Texturing and some lighting carry over directly; most math functions are implemented 
with lookup tables; coordinate transformations are implemented with the color matrix; loops with varying 
termination condition are supported with minmax; and many built-in functions (including illuminance, 
solar, and illuminate) are rewritten in terms of simpler operations. Fea­tures whose mapping to OpenGL 
is more sophisticated include: Noise: The RenderMan SL provides band-limited noise primitives that include 
1D, 2D, 3D, and 4D operands and single or multiple component output. We use .oating point arithmetic 
and texture tables to support all of these functions. Derivatives: The RenderMan SL provides access to 
surface­derivative information through functions that include Du, Dv, Deriv, area, and calculatenormal. 
We dedicate a compiler phase to fully implement these functions using a technique similar that described 
by Larry Gritz [12]. A number of optimizations are supported by the compiler. Uniform expressions are 
identi.ed and computed once for all pix­els. If texture coordinates are linear functions of s and t or 
vertex coordinates, they are recognized as a single pass with some com­bination of texture coordinate 
generation and texture matrix. Tex­ture memory utilization is minimized by allocating storage based on 
single-static assignment and live-range analysis [4]. 4.3 Demonstration We have implemented a RenderMan 
renderer, complete with shad­ing language, bytestream, and procedural interfaces on a software implementation 
of OpenGL including color range and pixel tex­ture. We experimented with subsets of IEEE single precision 
.oat­ing point. An interesting example was a 16 bit .oating point format with a sign bit, 10 bits of 
mantissa and 5 bits of exponent. This format was suf.cient for most shaders, but fell short when com­puting 
derivatives and related difference-oriented functions such as calculatenormal. Our software implementation 
supported other OpenGL extensions (cube environment mapping, fragment lighting, light texture, and shadow), 
but they are not strictly neces­sary as they can all be computed using existing features. ISL Image celtic 
leaves bump rot toon MPix Filled 2.8 4.3 1.2 2.2 1.9 Frames/Second 6.8 7.3 9.6 12.5 4.6 RSL Image teapots 
apple print MPix Filled 500 280 144 Table 1:       The RenderMan bytestream interface was implemented 
on top of the RenderMan procedural interface. When data is passed to the procedural interface, it is 
incorporated into a scene graph. Higher order geometric primitives not native to Cosmo3D, such as trimmed 
quadrics and NURBS patches are accommodated by extending the scene graph library with parametric surface 
types, which are tes­sellated just before drawing. At the WorldEnd procedural call, this scene graph 
is rendered using a ShadeAction that invokes the Ren­derMan shading language compiler followed by a DrawAction. 
To establish that the implementation was correct, over 2000 shading language tests, including point-feature 
tests, publicly avail­able shaders, and more sophisticated shaders were written or ob­tained. The results 
of our renderer were compared to Pixar s com­mercially available PhotoRealistic RenderMan renderer. While 
never bit-for-bit accurate, the shading is typically comparable to the eye (with expected differences 
due, for instance, to the noise function). A collection of examples is given in Figure 4. We fo­cused 
primarily on the challenge of mapping the entire language to OpenGL, so there is considerable room for 
further optimization. There are a few notable limitations in our implementation. Displacement shaders 
are implemented, but treated as bump map­ping shaders; surface positions are altered only for the calculation 
of normals, not for rasterization. True displacement would have to happen during object tessellation 
and would have performance similar to displacement mapping in traditional software implemen­tations. 
Transparency is not implemented. It is possible, but re­quires the scene graph to depth-sort potentially 
transparent surfaces. Pixel texture, as it is implemented, does not support texture .lter­ing, which 
can lead to aliasing. Our renderer also does not currently support high quality pixel antialiasing, motion 
blur, and depth of .eld. One could implement all of these through the accumulation buffer as has been 
demonstrated elsewhere [13].  5 DISCUSSION We measured the performance of several of our ISL and RenderMan 
shaders (Table 1). The performance numbers for millions of pixels .lled are conservative estimates since 
we counted all pixels in the object s 2D bounding box even when drawing object geometry that touched 
fewer pixels. 5.1 Drawbacks Our current system has a number of inef.ciencies that impact our performance. 
First, since we do not use deferred shading, we may spend several passes rendering an object that is 
hidden in the .nal image. There are a variety of algorithms that would help (for ex­ample, visibility 
culling at the scene graph level), but we have not implemented any of them. Second, the bounding box 
of objects in screen space is used to de.ne the active pixels for many passes. Consequently pixels within 
the bounding box but not within the object are moved un­necessarily. This taxes one of the most important 
resources in hard­ware: bandwidth to and from memory. Third, we have only included a minimal set of 
optimization rules in our compiler. Many current hardware systems share frame­buffer and texture memory 
bandwidth. On these systems, stor­age and retrieval of intermediate results bears a particularly high 
price. This is a primary motivation for doing as many operations per pass as possible. Our iburg-like 
rule matching works well for the pipeline of simple units found in standard OpenGL, but more complex 
units (as found in some new multitexture extensions, for example) require more powerful compiler technology. 
Two possi­bilities are surveyed by Harris [15]. 5.2 Advantages Our methodology allows research and development 
to proceed in parallel as shading languages, compilers, and hardware indepen­dently evolve. We can take 
advantage of the unique feature and performance needs of different application areas through special­ized 
shading languages. The application does not have to handle the complexities of multipass shading since 
the application interface is a scene graph. This model is a natural extension of most interactive applications, 
which already have a retained mode interface of some sort to enable users to manipulate their data. Applications 
still retain the other advantages of having a scene graph, like occlusion culling and level of detail 
management. As mentioned, we have only implemented a few of the many possible compiler optimizations. 
As the compiler improves, our performance will improve, independent of language or hardware. Finally, 
the rapid pace of graphics hardware development has resulted in systems with a diverse set of features 
and relative feature performance. Our design allows an application to use a shading language on all of 
the systems, and still take advantage of many of their unique characteristics. Hardware vendors do not 
need to create the shading compiler and retained data structures since they operate above the level of 
the drivers. Further, since complex effects can be supported on unextended hardware, designers are free 
to create fast, simple hardware without compromising on capabilities.  6 CONCLUSION We have created 
a software layer between the application and the hardware abstraction layer to translate high-level shading 
descrip­tions into multi-pass OpenGL. We have demonstrated this approach with two examples, a constrained 
shading language that runs inter­actively on current hardware, and a fully general shading language. 
We have also shown that general shading languages, like the Ren­derMan Shading Language, can be implemented 
with only two ad­ditional OpenGL extensions. There is a continuum of possible languages between ISL and 
the RenderMan Shading Language with different levels of func­tionality. We have applied our method to 
two different shading lan­guages in part to demonstrate its generality. There are many avenues of future 
research. New compiler technology can be developed or adapted for programmable shading. There are signi.cant 
optimizations that we are investigating in our compilers. Research is also needed to understand what 
hardware features are best for supporting interactive programmable shading. Finally, given examples like 
the scienti.c visualization constructs described by Craw.s that are not found in the RenderMan shading 
language [9], we believe the wide availability of interactive pro­grammable shading will spur exciting 
developments in new shading languages and new applications for them.  References [1] BIRCH, P., BLYTHE, 
D., GRANTHAM, B., JONES, M., SCHAFER, M., SE-GAL, M., AND TANNER, C. An OpenGL++ Speci.cation. SGI, March 
1997. [2] BLYTHE, D., GRANTHAM, B., KILGARD, M. J., MCREYNOLDS, T., NEL-SON, S. R., FOWLER, C., HUI, 
S., AND WOMACK, P. Advanced graphics programming techniques using OpenGL: Course notes. In Proceedings 
of SIGGRAPH 99 (July 1999). [3] BOCK, D. Tech watch: Volume rendering. Computer Graphics World 22,5 (May 
1999). [4] BRIGGS, P. Register Allocation via Graph Coloring. PhD thesis, Rice Uni­versity, April 1992. 
[5] CABRAL, B., CAM, N., AND FORAN, J. Accelerated volume rendering and tomographic reconstruction using 
texture mapping hardware. 1994 Sympo­sium on Volume Visualization (October 1994), 91 98. ISBN 0-89791-741-3. 
[6] CABRAL, B., OLANO, M., AND NEMEC, P. Re.ection space image based rendering. Proceedings of SIGGRAPH 
99 (August 1999), 165 170. [7] COOK, R. L. Shade trees. Computer Graphics (Proceedings of SIGGRAPH 84) 
18, 3 (July 1984), 223 231. Held in Minneapolis, Minnesota. [8] CORRIE, B., AND MACKERRAS, P. Data shaders. 
Visualization 93 1993 (1993). [9] CRAWFIS, R. A., AND ALLISON, M. J. A scienti.c visualization synthe­sizer. 
Visualization 91 (1991), 262 267. [10] DIEFENBACH, P. J., AND BADLER, N. I. Multi-pass pipeline rendering: 
Re­alism for dynamic environments. 1997 Symposium on Interactive 3D Graph­ics (April 1997), 59 70. [11] 
FRASER, C. W., HANSON, D. R., AND PROEBSTING, T. A. Engineering a simple, ef.cient code generator generator. 
ACM Letters on Programming Languages and Systems 1, 3 (September 1992), 213 226. [12] GRITZ, L., AND 
HAHN, J. K. BMRT: A global illumination implementation of the RenderMan standard. Journal of Graphics 
Tools 1, 3 (1996), 29 47. [13] HAEBERLI, P. E., AND AKELEY, K. The accumulation buffer: Hardware support 
for high-quality rendering. Computer Graphics (Proceedings of SIG-GRAPH 90) 24, 4 (August 1990), 309 
318. [14] HANRAHAN, P., AND LAWSON, J. A language for shading and lighting cal­culations. Computer Graphics 
(Proceedings of SIGGRAPH 90) 24, 4 (August 1990), 289 298. [15] HARRIS, M. Extending microcode compaction 
for real architectures. In Pro­ceedings of the 20th annual workshop on Microprogramming (1987), pp. 40 
53. [16] HART, J. C., CARR, N., KAMEYA, M., TIBBITTS, S. A., AND COLEMAN, T. J. Antialiased parameterized 
solid texturing simpli.ed for consumer-level hardware implementation. 1999 SIGGRAPH / Eurographics Workshop 
on Graphics Hardware (August 1999), 45 53. [17] HEIDRICH, W., AND SEIDEL, H.-P. Realistic,hardware-acceleratedshading 
and lighting. Proceedings of SIGGRAPH 99 (August 1999), 171 178. [18] HEIDRICH, W., WESTERMANN, R., SEIDEL, 
H.-P., AND ERTL, T. Appli­cations of pixel textures in visualization and realistic image synthesis. 1999 
ACM Symposium on Interactive 3D Graphics (April 1999), 127 134. ISBN 1-58113-082-1. [19] JAQUAYS, P., 
AND HOOK, B. Quake 3: Arena shader manual, revision 10. In Game Developer s Conference Hardcore Technical 
Seminar Notes (Decem­ber 1999), C. Hecker and J. Lander, Eds., Miller Freeman Game Group. [20] KAUTZ, 
J., AND MCCOOL, M. D. Interactive rendering with arbitrary brdfs using separable approximations. Eurographics 
Rendering Workshop 1999 (June 1999). Held in Granada, Spain. [21] KELLER, A. Instant radiosity. Proceedings 
of SIGGRAPH 97 (August 1997), 49 56. [22] KYLANDER, K., AND KYLANDER, O. S. Gimp: The Of.cial Handbook. 
The Coriolis Group, 1999. [23] MAX, N., DEUSSEN, O., AND KEATING, B. Hierarchical image-based ren­dering 
using texture mapping hardware. Rendering Techniques 99 (Proceed­ings of the 10th Eurographics Workshop 
on Rendering) (June 1999), 57 62. [24] MCCOOL, M. D., AND HEIDRICH, W. Texture shaders. 1999 SIGGRAPH 
/ Eurographics Workshop on Graphics Hardware (August 1999), 117 126. [25] OLANO, M., HART, J. C., HEIDRICH, 
W., MCCOOL, M., MARK, B., AND PROUDFOOT, K. Approaches for procedural shading on graphics hardware: Course 
notes. In Proceedings of SIGGRAPH 2000 (July 2000). [26] OLANO, M., AND LASTRA, A. A shading language 
on graphics hardware: The PixelFlow shading system. Proceedings of SIGGRAPH 98 (July 1998), 159 168. 
[27] OPENGL ARB. Extension speci.cation documents. http://www.opengl.org­/Documentation/Extensions.html, 
March 1999. [28] PIXAR. The RenderMan Interface Speci.cation: Version 3.1. Pixar Anima­tion Studios, 
September 1999. [29] SEGAL, M., AKELEY, K., FRAZIER, C., AND LEECH, J. The OpenGL Graphics System: A 
Speci.cation (Version 1.2.1). Silicon Graphics, Inc., 1999. [30] SGI TECHNICAL PUBLICATIONS. Cosmo 3D 
Programmer s Guide. SGI Technical Publications, 1998. [31] SIMS, K. Particle animation and rendering 
using data parallel computation. Computer Graphics (Proceedings of SIGGRAPH 90) 24, 4 (August 1990), 
405 413. [32] UPSTILL, S. The RenderMan Companion. Addison-Wesley, 1989. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344979</article_id>
		<sort_key>433</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>49</seq_no>
		<title><![CDATA[The WarpEngine]]></title>
		<subtitle><![CDATA[an architecture for the post-polygonal age]]></subtitle>
		<page_from>433</page_from>
		<page_to>442</page_to>
		<doi_number>10.1145/344779.344979</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344979</url>
		<abstract>
			<par><![CDATA[<p>We present the WarpEngine, an architecture designed for real-time imaged-based rendering of natural scenes from arbitrary viewpoints. The modeling primitives are real-world images with per-pixel depth. Currently they are acquired and stored off-line; in the near future real-time depth-image acquisition will be possible, the WarpEngine is designed to render in immediate mode from such data sources.</p><p>The depth-image resolution is locally adapted by interpolation to match the resolution of the output image. 3D warping can occur either before or after the interpolation; the resulting warped/interpolated samples are forward-mapped into a warp buffer, with the precise locations recorded using an offset. Warping processors are integrated on-chip with the warp buffer, allowing efficient, scalable implementation of very high performance systems. Each chip will be able to process 100 million samples per second and provide 4.8GigaBytes per second of bandwidth to the warp buffer.</p><p>The WarpEngine is significantly less complex than our previous efforts, incorporating only a single ASIC design. Small configurations can be packaged as a PC add-in card, while larger deskside configurations will provide HDTV resolutions at 50 Hz, enabling radical new applications such as 3D television.</p><p>WarpEngine will be highly programmable, facilitating use as a test-bed for experimental IBR algorithms.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[graphics hardware]]></kw>
			<kw><![CDATA[image-based rendering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14132867</person_id>
				<author_profile_id><![CDATA[81100375260]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Voicu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Popescu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill, Sitterson Hall, CB#3175, CS UNC, Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31075577</person_id>
				<author_profile_id><![CDATA[81332497960]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Eyles]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill, Sitterson Hall, CB#3175, CS UNC, Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P20148</person_id>
				<author_profile_id><![CDATA[81100264426]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Anselmo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lastra]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill, Sitterson Hall, CB#3175, CS UNC, Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P149711</person_id>
				<author_profile_id><![CDATA[81100459176]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Joshua]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Steinhurst]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill, Sitterson Hall, CB#3175, CS UNC, Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39035674</person_id>
				<author_profile_id><![CDATA[81100278087]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Nick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[England]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill, Sitterson Hall, CB#3175, CS UNC, Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P168880</person_id>
				<author_profile_id><![CDATA[81100368207]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Lars]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nyland]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill, Sitterson Hall, CB#3175, CS UNC, Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>311574</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Aliaga D. and Lastra A., "Automatic Image Placement to Provide a Guaranteed Frame Rate", Proc. SIGGRAPH '99,307-316 (1999).]]></ref_text>
				<ref_id>Aliaga99</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134003</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Beier T. and Neely S., "Feature-Based Image Metamorphosis", Proc. SIGGRAPH '92, 35-42 (1992).]]></ref_text>
				<ref_id>Beier92</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Beraldin J.-A., Rioux M., Blais F., Domey J., and Coumoyer L., "Registered Range and Intensity Imaging at 10-Mega Samples per Second", Opt. Eng., 31(1): p. 88-94 (1992).]]></ref_text>
				<ref_id>Beraldin92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166153</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Chen S. and Williams L., "View Interpolation for Image Synthesis", Proc. SIGGRAPH '93, 279-288 (1993).]]></ref_text>
				<ref_id>Chen93</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218395</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Chen S., "Quicktime VR - An Image-Based Approach to Virtual Environment Navigation", Proc. SIGGRAPH '95, 29-38 (1995).]]></ref_text>
				<ref_id>Chen95</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[The Cyrax System, in http://www.cyra.com/.]]></ref_text>
				<ref_id>Cyra</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237191</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Debevec P., Taylor C., and Malik J., "Modeling and Rendering Architecture from Photographs: A Hybrid Geometry and Image-Based Approach", Proc. SIGGRAPH '96, 11-20 (1996).]]></ref_text>
				<ref_id>Debevec96</ref_id>
			</ref>
			<ref>
				<ref_obj_id>925048</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Ellsworth D., Polygon Rendering for Interactive Visualization on Multicomputers, PhD thesis, University of North Carolina at Chapel Hill, 1997.]]></ref_text>
				<ref_id>Ellsworth97</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237200</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Gortler S., Grzeszczuk R., Szeliski R., and Cohen M., "The Lumigraph", Proc. SIGGRAPH '96, 43-54 (1996).]]></ref_text>
				<ref_id>Gortler96</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Kanade T., Rander P., Vedula S., and Saito H., "Virtualized Reality: Digitizing a 3D Time-Varying Event As Is and in Real Time", Mixed Reality, Merging Real and Virtual Worlds, Y. Ohta and H. Tamura, Editors. Springer-Verlag. p. 41-57 (1999).]]></ref_text>
				<ref_id>Kanade99</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Scene Modeler, http://www.k2t.com/.]]></ref_text>
				<ref_id>K2T</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237199</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Levoy M. and Hanrahan P., "Light Field Rendering", Proc. SIGGRAPH '96, 31-42 (1996).]]></ref_text>
				<ref_id>Levoy96</ref_id>
			</ref>
			<ref>
				<ref_obj_id>253292</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Mark W., McMillan L., and Bishop G., "Post-Rendering 3D Warping", 1997 Symposium on Interactive 3D Graphics, 7-16 (1997).]]></ref_text>
				<ref_id>Mark97</ref_id>
			</ref>
			<ref>
				<ref_obj_id>929571</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Mark W., Post-Rendering 3D Image Warping: Visibility, Reconstruction, and Performance for Depth-Image Warping, PhD thesis, University of North Carolina at Chapel Hill, 1999.]]></ref_text>
				<ref_id>Mark99</ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383830</ref_obj_id>
				<ref_obj_pid>2383815</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[McAllister, D., Nyland L., Popescu V., Lastra A., McCue C., "Real-Time Rendering of Real-World Environments", Rendering Techniques '99, Proc. Eurographics Workshop on Rendering, 145- 160, (1999).]]></ref_text>
				<ref_id>McAllister99</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218398</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[McMillan L. and Bishop G., "Plenoptic Modeling: An Image-Based Rendering System", Proc. SIGGRAPH '95, 39-46 (1995).]]></ref_text>
				<ref_id>McMillan95</ref_id>
			</ref>
			<ref>
				<ref_obj_id>269042</ref_obj_id>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[McMillan L., An Image-Based Approach to Three- Dimensional Computer Graphics, PhD thesis, University of North Carolina at Chapel Hill, 1997.]]></ref_text>
				<ref_id>McMillan97</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Minolta 3D 1500, in http://www.minolta3d.com/.]]></ref_text>
				<ref_id>Minolta</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134067</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Molnar S., Eyles J., and Poulton J., "PixelFlow: High-speed Rendering using Image Composition", Proc. SIGGRAPH '92, 231- 240(1992).]]></ref_text>
				<ref_id>Molnar92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>182469</ref_obj_id>
				<ref_obj_pid>182466</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Molnar S., Cox M., Ellsworth D., and Fuchs H., "A Sorting Classification of Parallel Rendering", IEEE Computer Graphics and Aplications, 14(4), 23-32 (1994)]]></ref_text>
				<ref_id>Molnar94</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258871</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Montrym J., Baum D., Dignam D., and Migdal C., "InfiniteReality: A Real-Time Graphics System", Proc. SIGGRAPH '97,293-302 (1997).]]></ref_text>
				<ref_id>Montrym97</ref_id>
			</ref>
			<ref>
				<ref_obj_id>199417</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Mueller C., "The Sort-First Rendering Architecture for High- Performance Graphics", 1995 Symposium on Interactive 3D Graphics, 75-84 (1995).]]></ref_text>
				<ref_id>Mueller95</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280857</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Olano M. and Lastra A., "A Shading Language on Graphics Hardware: The PixelFlow Shading System", Proc. SIGGRAPH 98, (1998).]]></ref_text>
				<ref_id>Olano98</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311569</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Regan M., Miller G., Rubin S., and Kogelnik C., "A Real Time Low-Latency Hardware Light-Field Renderer", Proc. SIGGRAPH '99,287-290 (1999).]]></ref_text>
				<ref_id>Regan99</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237196</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Seitz S. and Dyer C., "View Morphing: Synthesizing 3D Metamorphoses Using Image Transforms", Proc. SIGGRAPH '96, 21-30(1996).]]></ref_text>
				<ref_id>Seitz96</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280882</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Shade J., Gortler S., He L., and Szeliski R., "Layered Depth Images", Proc. SIGGRAPH '98, 231-242 (1998).]]></ref_text>
				<ref_id>Shade98</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237274</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Torborg J. and Kajiya J., "Talisman: Commodity Real-time 3D Graphics for the PC",Proc. SIGGRAPH '96,353-364 (1996).]]></ref_text>
				<ref_id>Torborg96</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97919</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Westover L., "Footprint Evaluation for Volume Rendering", Proc. SIGGRAPH '90, 367-376 (1990).]]></ref_text>
				<ref_id>Westover90</ref_id>
			</ref>
			<ref>
				<ref_obj_id>528718</ref_obj_id>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Wolberg G., Digital Image Warping, IEEE Computer Society Press, Los Alamitos California, 1990.]]></ref_text>
				<ref_id>Wolberg90</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The WarpEngine: An Architecture for the Post-Polygonal Age Voicu Popescu, John Eyles, Anselmo Lastra, 
Joshua Steinhurst, Nick England, Lars Nyland University of North Carolina at Chapel Hill ABSTRACT We 
present the WarpEngine, an architecture designed for real­time image-based rendering of natural scenes 
from arbitrary viewpoints. The modeling primitives are real-world images with per-pixel depth. Currently 
they are acquired and stored off-line; in the near future real-time depth-image acquisition will be possible, 
and WarpEngine is designed to render in immediate mode from such data sources. The depth-image resolution 
is locally adapted by interpolation to match the resolution of the output image. 3D warping can occur 
either before or after the interpolation; the resulting warped/interpolated samples are forward-mapped 
into a warp buffer, with the precise locations recorded using an offset. Warping processors are integrated 
on-chip with the warp buffer, allowing efficient, scalable implementation of very high performance systems. 
Each chip will be able to process 100 million samples per second and provide 4.8GigaBytes per second 
 of bandwidth to the warp buffer. The WarpEngine is significantly less complex than our previous efforts, 
incorporating only a single ASIC design. Small configurations can be packaged as a PC add-in card, while 
larger deskside configurations will provide HDTV resolutions at 50 Hz, enabling radical new applications 
such as 3D television. WarpEngine will be highly programmable, facilitating use as a test-bed for experimental 
IBR algorithms. KEYWORDS: Graphics hardware, image-based rendering.  1 INTRODUCTION Research efforts 
in interactive 3D computer-graphics have been targeted at providing high-quality, high-resolution images. 
This goal has proven elusive: renderings that can be mistaken for photographs can usually be obtained 
only by sacrificing interactivity. This problem, and the extreme difficulty of modeling natural environments, 
motivated research on image-based rendering. Although the image-based primitives are novel, conventional 
polygon-based graphics hardware has been used for the rendering. Few attempts have been made to take 
advantage of the new image-based primitives with novel hardware. {popescu, jge, lastra, jsteinhu, nick, 
nyland}@cs.unc.edu Sitterson Hall, CB#3175, CS UNC, Chapel Hill, 27599, NC Permission to make digital 
or hard copies of part or all of this work or personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. SIGGRAPH 2000, New Orleans, 
LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 In this paper, we present the WarpEngine architecture 
for rendering directly from an image-based representation, specifically from images with per-pixel depth 
[McMillan95]. The prototype that we plan to build promises high performance at HDTV resolution, as well 
as extensive programmability to support research in algorithms for image-based rendering. We first review 
related work in image-based rendering (and graphics hardware that uses image-based primitives). Then 
we present the algorithm on which the WarpEngine is based, followed by a detailed architectural description 
of the machine. We close with proposed future work and conclusions. 1.1 Related Work The spectrum of 
image-based approaches ranges from those that exclusively use images to those that re-project acquired 
imagery onto geometric models. [Chen95] employed 360° panoramas, stitched together from overlapping photographs. 
A panorama offers a realistic view of the scene, but the user has a correct 3D perspective only from 
a single location. However, using inexpensive hardware, the user can view, at interactive rates, outdoor 
or indoor scenes that are hard to model as a collection of polygons. Image morphing approaches [Wolberg90, 
Beier92, Chen93, Seitz96] allow some range of motion but the transformation of the reference views to 
the desired views is approximated by interpolation. To maintain a high update rate, the Talisman architecture 
[Torborg96] reused portions of rendered images by re-projecting to a new, nearby, view using a 2D warp. 
The Lumigraph [Gortler96] and Light Field [Levoy96] densely sample light rays to create a ray database. 
Unfortunately, the database of rays grows quite large for bigger viewing volumes. [Regan99] describes 
low-latency rendering hardware for a one­axis light field. At the other end of the spectrum are methods 
based largely on geometry. Texture mapping is the most common way to incorporate images in the scene 
description. The Façade system [Debevec96] represents the scene with an approximate geometric model (semi-automatically 
created) texture-mapped, in a view­ dependent fashion, from photographs. McMillan and Bishop's [McMillan95] 
method is in the middle of the spectrum, representing the scene as a collection of images that in addition 
to color also store depth at each pixel. The desired view is generated by a 3D-warp of the depth images. 
We have chosen image-based rendering by 3D warping (IBRW) as the basis for the WarpEngine. There are 
two reasons for this: (1) the storage and bandwidth requirements are manageable, and (2) laser rangefinders 
[K2T, Beraldin92, Cyra] and other range­acquisition equipment [Kanade99, Minolta] are rapidly improving 
and appearing on the commercial market. Some instruments can even acquire range in real time. Such a 
"depth camera" coupled with the WarpEngine will enable extraordinary applications: a spectator of a live 
event will not be confined to the view of the TV camera; he or she can choose any seat in the arena, 
and even venture onto the stage or court.  1.2 3D Image Warping McMillan and Bishop show in [McMillan95] 
how to compute the desired image coordinates of a depth image sample using the 3D warping equations w 
+ w . u + w . v + w . d (u , v ) 11 121 131 14 11 u = 2 w31 + w . u1 + w . v + w . d (u , v ) 32 33134 
11 (1) w + w . u + w . v + w . d (u , v ) 21 221 231 24 11 v = 2 w + w . u + w . v + w . d (u , v ) 31 
321 331 34 11 where u2, v2 are the desired image coordinates, u1, v1 the original (reference) image coordinates, 
the w s are transformation constants obtained from the reference and desired image camera parameters, 
and d(u1, v1) is the generalized disparity at sample (u1, v1), which is defined as the ratio between 
the distance to the reference image plane and zeye(u1, v 1). The warping equation is equivalent to the 
vertex transformation commonly used in computer graphics, but allows one to take advantage of the regular 
structure of images to perform incremental transformation. The warped coordinates of a sample can be 
computed with six adds, five multiplies1, and one divide. Reconstructing by simply setting a desired 
image pixel to the color of the sample that warps within its boundary results in holes thus not acceptable. 
Also, more than one visible sample can warp to the same pixel, and simply discarding all but one sample 
produces aliasing. Reconstruction is a challenging task when warping images with depth; we analyze it 
in more detail next. 1.3 Reconstruction The 3D warping equation is a forward mapping that takes samples 
from the reference domain and maps them to the destination domain. An inverse mapping (as in conventional 
texturing) would be ideal. Unfortunately there is no analytically computable inverse for 3D warping (there 
is a rather costly search procedure described in [McMillan97]). Reconstruction for IBRW is mainly done 
in one of two ways [Mark97, McMillan97]: with splats or with a polygonal mesh. A splat [Westover90] is 
a representation of the projected shape of the reference sample. The original use for splats was to render 
transparency for volume rendering; thus the splats were blended in front-to-back order. For IBRW, we 
do not want to blend samples that are at different depths. Rather we want to overwrite samples that should 
be hidden and only blend samples that represent the same surface. This is very difficult to do because 
in IBRW we have no information about surfaces. To prevent samples of hidden surfaces from showing through, 
the sizes of the splats are overestimated [Shade98], thus overlapping splats may incorrectly erase visible 
samples, resulting in aliasing. Good reconstruction can be obtained by connecting the samples of the 
reference image into a polygonal mesh. Not all samples should be connected, of course; in Section 2.1 
we present a simple method for detecting depth discontinuities. With meshing, continuity of the surfaces 
is maintained where desired, and hardware acceleration increases performance. On WarpEngine, we connect 
samples, but avoid the overhead of general polygonal rendering. 1 If w34 is non-zero (non-zero translation 
from reference position) one could save a multiply by dividing all w's by w34. 1.4 Why WarpEngine? One 
might ask, why build the WarpEngine if existing graphics hardware can be used for IBRW? One reason is 
performance; another is efficiency. Assume that 1280 by 1024 is the targeted resolution and that on average 
we warp twice the reference samples as the desired resolution. Two triangles need to be rendered for 
every warped sample. The average number of triangles per second to sustain a frame rate of 30 Hz is N 
» 1280 ·1024 · 2 · 2 · 30 » 157Mtris / s Neither high-end systems like PixelFlow [Molnar92] or InfiniteReality 
[Montrym97], nor the rapidly improving PC 3D graphics accelerators, can produce the necessary performance. 
Moreover, we speculate that it will take years for them to reach this sustained level of performance. 
Even then, it will take more hardware than on a machine optimized for IBRW. We believe that WarpEngine 
is more efficient because it takes advantage of the regularity of image-based primitives and of the small 
screen-size of the warped samples. More needs to be said to explain the number of reference-image samples 
required at each frame. This number depends on the scene and on how it is modeled. One must process (on 
average) more than one reference image sample per desired image location because:  there are surfaces 
that are redundantly captured in more than one reference image;  there are surfaces captured in the 
reference images that are not visible in the desired image (depth complexity is greater than one);  
there are surfaces that were better sampled in the reference image than in the desired image, which leads 
to more than one visible sample per desired image pixel.  Two input samples per output pixel is a reasonable 
lower bound; in practice we have found it difficult to use fewer. With real-time depth-image updates 
(immediate mode), the number of samples will be determined by the number, resolution and update rates 
of the cameras. The most viable alternative to IBRW is to simplify triangle meshes in order to reduce 
the polygon count, and thus meet our performance goals with conventional graphics hardware (in fact, 
other members of our team are investigating this approach). However, simplified meshes are less well 
suited to real-time depth updates because of the pre-processing required.   2 RENDERING ALGORITHM We 
wish to treat the depth image as connected (as in the mesh approach) in order to prevent samples of hidden 
surfaces from showing through. The triangles resulting from the mesh method are very small in screen 
space; thus scan conversion time is dominated by setup. Instead of conventional scan conversion, we propose 
simply bilinearly interpolating between connected samples in reference image domain, reducing per-sample 
setup. Figure 1. Once warped, the four neighboring reference-image samples u0v0, u1v0, u1v1, and u0v1 
 u1v1 may form a concave quad. Interpolating in desired-image space u0v0 produces the sub-samples shown 
with little squares, which is different from the projection of the surface on the image plane. When interpolating 
in reference­image space and then warping, the surface projection is approximated better, but at higher 
computational cost. u0v1 The algorithm is: For all adjacent, connected samples Bilinearly interpolate 
color and depth to obtain subsamples Warp resulting subsamples to desired image space Z-composite warped 
subsamples into the warp buffer In order to reduce aliasing, we warp into a sub-pixel resolution warp 
buffer (usually 2x2), then filter to produce the final image in the frame buffer. The sub-samples are 
z-buffered. The interpolation factor (number of subsamples created in each of x and y directions) is 
critical in order to ensure that (1) back surfaces do not show through, and (2) we do not generate too 
many subsamples. We describe the computation of the interpolation factor in more detail in Section 3.3. 
Recall that the reference-image depth information is stored as generalized disparity that is proportional 
to 1/zeye, which is linear in image space. Consequently, if the four neighboring samples are planar, 
the sub-samples resulting from the interpolation are correctly located on the same plane. If the samples 
are not coplanar, the sub-samples define a general bilinear patch. Adjacent patches exhibit C0 continuity. 
An alternative rendering algorithm, to save the cost of warping the sub-samples (dominated by the inverse 
computation, see Equation 1) is to first warp the reference-image samples and then interpolate. This 
still avoids the triangle setup costs. Just as when interpolation is done in reference-image space, if 
the original samples lie on a plane, the sub-samples are also on the plane.  Figure 3. In the warp buffer 
fragment shown, the green lines define the warp buffer locations. The blue lines delimit the output pixels, 
which span four warp buffer locations each. The fine black lines show the virtual subdivision of the 
warp buffer locations corresponding to the two 2-bit offset values. The locations at which these samples 
warped are shown by black squares and are recorded to a precision of 1/8 th of a pixel. There is exactly 
one sample per warp buffer location. The output pixel is reconstructed using a two-pixel wide kernel, 
with a half pixel (one warp buffer location) overlap. The kernel is shown in gray. The 16 color samples 
are weighted according to their position inside the warp buffer location, as modified by the offsets. 
The reconstruction is equivalent to reconstructing from an 8x8 supersampled buffer that is sparsely populated, 
without having to explicitly allocate the dense buffer or to search for the locations that are populated. 
However the similarity between the two methods ends when the four original samples are not coplanar and 
the resulting screen­space quad might be concave (resulting in a "bow tie", see Figure 1). Our simulations 
show that this is a very infrequent case, which usually occurs between silhouette samples that were marked 
as disconnected anyway. 2.1 Determining Connectivity We devised a robust and inexpensive way of detecting 
which samples should not be connected by interpolation, based on the surface curvature (see also [McAllister99]). 
At every reference image sample we compute the second derivative2 of the generalized disparity along 
four directions: E-W, SE-NW, N-S, and SW-NE. If the surface sampled is planar the second derivative is 
exactly zero. If it exceeds a threshold (which is unique per scene) the samples are marked as disconnected 
(see Figure 2). 2.2 Reconstruction Using Offsets We want to render high-quality, antialiased images. 
Conventional jittered supersampling is not an option because of the forward­mapping nature of the warping 
process; warping produces sub­samples that do not correspond exactly with the centers of warp­buffer 
locations. Even with a 2x2 warp buffer, aliasing is greater than we wish. Our proposed alternative is 
to compute the (x, y) location of the warp to a precision higher than that of the warp buffer, and store 
that more precise location as an offset from the corner of the warp 2 difference of neighboring differences 
Offset Viewpoint Rotated Viewpoint Translated Zoom 4x4 1x 8x 1x1 1x (none) 8x Figure 4. Antialiasing 
using offsets. These images were rendered from a depth image of a checkerboard. The left column is just 
rotated, the right also translated. We show both original and 8X zoomed versions. All images used a 2x2 
warp buffer. The upper set was rendered with two bits for each of x and y offset. The lower set used 
no offsets and exhibits more aliasing. buffer location. The offsets are used during reconstruction to 
obtain better filtering and a higher-quality final image. We have found that a 2-bit offset in each of 
x and y (total of 4 bits per warp buffer location) provides good results. This, combined with a 2x2 warp 
buffer, locates the warped subsamples to within one-eight of a pixel (see Figure 3). The results are 
illustrated in Figure 4. sampling locations within a pixel. No matter how the sampling locations are 
chosen, at least two of them are collinear in jittered supersampling and k are collinear when kxk subpixel 
masks are computed. If from one frame to the next all collinear sampling locations move from one side 
to the other of a slowly moving edge, the change in color of the output pixel is too abrupt. Using 2x2 
pixel kernels with 2x2 warpbuffer and 4x4 offsets guarantees 16 intermediate levels (when an edge moves 
slowly enough). We refer the reader to the conference-proceedings videotape and DVD-ROM, which illustrate 
the dynamic antialiasing properties of our algorithm. Also the conference-proceedings CD-ROM includes 
the antialiasing examples. Please look at the images on the CD to see the full effects of the antialiasing. 
Figures 5, 6, and 10 (at the end of the paper) show images of various test scenes rendered with the WarpEngine 
simulator.  3 WarpEngine ARCHITECTURE 3.1 Overview The hardware architecture must provide sufficient 
warping power for all required reference-image samples and sufficient bandwidth to the warp buffer. We 
decided to partition the reference images into 16x16-sample tiles (with a 15x15 payload) and to use these 
as the basic rendering primitive. Tiles provide several important advantages: we can selectively use 
portions of reference images as needed for adequate sampling and coverage of visible surfaces (Section 
3.3); one can easily estimate the screen area a tile transforms to, enabling efficient high-level parallelism 
(Section 3.1.3); Offsets are, of course, not equivalent to higher warp buffer resolution. Although its 
location is recorded more precisely, only one sample is stored at each warp buffer location. In the expected 
case, when the sampling resolution of the desired image is within a factor of two of that of the reference 
images, the 2x2 warp buffer with 4x4 offsets provides a good reconstruction. Outside that interval, other 
reference images should be used. One could increase the resolution of the warp buffer to accommodate 
even bigger sampling mismatches, but this comes at a substantial additional cost, not only in memory, 
but also in warping since more reference image samples must be used. The offset reconstruction also has 
good temporal antialiasing properties. Antialiasing by jittered supersampling or coverage­mask-based 
methods suffer from the problem of collinear tiles are small enough that the same interpolation factor 
can be used for all samples, enabling SIMD low-level parallelism (Section 3.1.1). 3.1.1 Warping and 
Interpolation All the samples of a tile can be warped and interpolated with the same set of instructions 
so a SIMD implementation is, we believe, the most efficient. We opted for an array of simple byte-wide 
processors, similar to the one used in PixelFlow [Molnar92]. For a computation that can be efficiently 
mapped, a SIMD array provides efficient use of silicon, since control is factored out over all the processors. 
A large array of simple processors is more easily programmable than a complex pipelined processor. The 
programmability is necessary for use of the WarpEngine as a research tool. A SIMD array equal in size 
to the reference-image-tile maps very efficiently, since the warping calculation is the same for every 
pixel, with minimal branching required. Nearest neighbor Processing Element (PE) connectivity provides 
each PE with access to the three other samples needed for interpolation. 3.1.2 Warp Buffer The biggest 
design concern was providing sufficient warp buffer bandwidth. We assume the maximum resolution to be 
HDTV (approximately 2K x 1K pixels) and 60 Hz update rate; we assume again that one needs to use at least 
two reference-image samples per output pixel. This implies that at least 240 million reference samples 
per second must be warped. In our simulations, a 2x2 warp buffer resolution required in some cases an 
average interpolation factor of 4x4. Thus, for each warped reference­image pixel, 16 warped samples are 
generated, and the warp buffer must process approximately 4 billion warped samples per second. Each sample 
is about 12 bytes in size (4 bytes RGB; 4 bytes Z-buffer; 4 bytes X and Y values, including offsets). 
Assuming a depth complexity of two, and that 50% of the hidden samples initially pass the Z-comparison 
test, an average of 10 byte accesses is required per warped sample. Thus total warp buffer bandwidth 
is about 40 GigaBytes/sec. To achieve this enormous warp-buffer bandwidth, a very large number of commodity 
DRAMs is required (well over 100); similarly, the warping/interpolation processors would require hundreds 
of pins dedicated to interfacing with the warp-buffer. By placing the warp buffer on-chip, that is, on 
the same ASIC as the processors that generate the warped samples, very wide and fast memory interfaces 
can be used. 3.1.3 Region-Based Rendering With current technology, a single ASIC can provide neither 
sufficient processing power nor sufficient warp buffer memory3. Thus multiple ASICs are required, and 
some form of high-level parallelism must be employed. Partitioning the warp buffer into contiguous screen 
regions with each region assigned to an ASIC (screen-space subdivision) is appealing, because the typical 
16x16-sample tile intersects only one screen region and therefore needs to be processed by a single ASIC 
(tiles that overlap region boundaries are assigned to multiple regions). By contrast, with interleaving, 
each tile would need to be processed by many or all of the ASICs. 3 As silicon technology improves, a 
full-sized warp buffer becomes feasible (on an embedded-DRAM process). For partitioning by screen-space 
subdivision, primitives must be sorted by screen region4. Using tiles as the rendering primitive means 
that sorting is performed on 256 samples at a time; the number of tiles per frame ranges from a few thousand 
to a maximum of a few tens of thousands (depending on screen resolution) so the computational and memory 
burden of sorting is considerably less than for the general polygon-rendering case. By assigning multiple 
screen regions to each ASIC, a smaller number of ASICs is sufficient; however this requires sorting into 
buckets corresponding to screen regions [Ellsworth97], because an ASIC must process all primitives in 
a given region before moving to its next assigned region. The sort first, sort middle, sort last taxonomy 
developed to describe object-parallel polygon-rendering architectures [Molnar94], can also be applied 
to IBR architectures. Sorting by reference-image tiles is sort first from the point of view of reference-image 
samples, since after a tile has been assigned to a screen region, it is known a priori that its sub-samples 
will warp to the desired screen region (those that do not can be discarded, since the tile will be assigned 
to all pertinent regions). In polygon rendering, sort first [Mueller95] is prone to load-balancing difficulties; 
this is not be a problem for IBR, since reference­image tiles and interpolation factors are chosen to 
sample the destination image uniformly. We believe that sort first is an attractive approach for the 
WarpEngine, because it makes scaling of the system relatively painless. Performance is increased by adding 
additional ASICs, and assigning fewer screen regions to each ASIC. Screen-space subdivision requires 
a central processor, perhaps the host, which can perform the tile sorting, or a way of distributing these 
tasks across the multiple ASICs. 3.1.4 Processing Warped Sub-Samples It is straightforward to build 
a region-sized on-chip warp buffer with very high performance. Since each warped sample maps to only 
one location in the warp buffer, the warp buffer can be partitioned, with a sample processor assigned 
to each partition. Very high numbers of samples can be processed by instantiating more sample processors, 
processing simultaneous streams of warped samples. Load-balancing can be achieved by sub-pixel interleaving 
the partitions and providing input FIFOs for the sample processors. The region size is determined by 
the silicon budget for the warp buffer, independently of the number of partitions. The sample processors 
are very simple: they combine a new warped sample with the previous contents of the warp buffer location, 
using a z-compare operation. Since the sample processors memory interface does not cross chip boundaries, 
it can be very wide and very fast; thus the sample processors are not bandwidth limited.  3.2 WarpEngine 
Implementation Our architecture, the WarpEngine, consists of one or more identical Nodes (typically 4 
to 32); each Node consists of an ASIC and a Tile Cache. The ASIC contains: a 16x16 SIMD Warp Array , 
for warping and interpolating reference-image samples; 4 For tiles, this is efficiently done by warping 
the 4 corners, using both the tile s minimum and maximum disparity values; the resulting 8 points define 
the tile s screen-space bounding box. from host Figure 7. Block diagram of the WarpEngine a Region 
Accumulator, which includes a double-buffered warp buffer for a 128x128 screen region and 4 sample processors 
for resolving visibility;  a Reconstruction Buffer, for computing final pixel values;  a Network Interface, 
which connects the Nodes together into a high-bandwidth ring, and provides a connection to the host, 
a connection to each of the Warp Arrays, and a connection to the Tile Cache.  The Tile Cache is a commodity 
DRAM device; it is used for caching both reference-image tiles and instructions. A double­buffered Frame 
Buffer receives the final pixel values from the Nodes for display. The basic operation of the system 
is as follows (see Figure 7): The host determines which reference-image tiles are to be used to compute 
the destination image, and computes the screen-space bounding box for each of these tiles. For each screen 
region, the host maintains a bin; each bin contains pointers to the tiles whose bounding boxes intersect 
that screen region.  For each screen region, the host assigns a Node to be responsible for that screen 
region. The host sends each tile in the region s bin to the Node. (Tiles are cached in each node s Tile 
Cache. If a tile is resident in one of the caches, the host instructs the Network Interface to forward 
it to the appropriate Node. If not, the host must send the tile data to the Node).  Each tile received 
by each Node is loaded into the Warp Array, which performs the warping and interpolation calculations 
for the tile, and forwards the warped samples to the Region Accumulator.  The Region Accumulator collects 
the warped samples into its sub-pixel resolution warp buffer.  After all tiles in the region s bin have 
been processed, the Region Accumulator swaps its buffers and initializes the visibility buffer, in preparation 
for processing the next screen region.  Proceedings of SIGGRAPH 2000 Concurrently with processing the 
next screen region, the Region Accumulator steals memory cycles to send the previous region s data to 
the Reconstruction Buffer. The Reconstruction Buffer computes the final pixel values for the region 
and forwards them to the Frame Buffer.  After all regions have been processed and the final pixel values 
calculated and forwarded to the Frame Buffer, the Frame Buffer swaps buffers.  The system can function 
in retained mode, in which there is a fixed set of reference images describing an environment, or immediate 
mode, in which new reference images are being received on the fly . 3.2.1 Warp Array The Warp Array (see 
Figure 8) consists of 256 processing elements (PEs), arranged as a 16x16-pixel array. Each PE consists 
of a simple byte-wide ALU and 160 bytes of local memory partitioned as: 128 bytes main memory, 16 bytes 
IO Buffer, 16 bytes Sample Buffer. A distributed linear expression evaluator provides values of the linear 
expression Ax+By+C to each PE simultaneously, in byte­serial form (x and y represent the position of 
the PE in the 16x16 array). It is used for very fast computation of the linear part of the numerator 
and denominator of the warp-equation expressions (see Equation 1). Each PE includes a byte-wide connection 
to its neighbor in each dimension. Clock rate for the PE and local memory will be at 300 MHz or more. 
The IO Buffer is used for inputting reference-image tiles (from the Tile Cache or host, via the Network 
Interface) via a 300 MByte/sec interface. The Sample Buffer is used for exporting warped samples to the 
Region Accumulator, over the sample port, via an on-chip 4.8 GigaByte/sec interface. Access to these 
buffers may occur simultaneously with accessing of the main memory by the ALU, so that the next tile 
may be loaded during processing of the current one, and one set of interpolated samples can be computed 
while the previous set is being output to the Region Accumulator. ALU LocalLEE Micro-MemoryCoefficients 
Instructions Address  3.2.2 Region Accumulator The Region Accumulator (Figure 9) consists of a large 
SRAM warp buffer (the Region Buffer) and a set of 4 Sample Processors, which combine warped samples into 
Region Buffer memory. The Region Buffer contains data for a 128x128-screen region, at 2x2 sub-pixel resolution; 
a half-pixel wide boundary is added, to allow reconstruction kernels up to two pixels wide. The Region 
Buffer is partitioned into 4 sections, interleaved 2x2 across the sub-pixel grid. Each word of Region 
Buffer memory is divided into three fields. Two double-buffered fields (the RGB/Offset/Present fields) 
include RGB values, the offsets used for reconstruction, and a present bit (used to avoid z-buffer initialization). 
One buffer is used for accumulating samples for the current region, while the other buffer contains the 
previous region s values for output. The third field contains values that are not required for reconstruction 
and need not be double-buffered. Besides z value, we are reserving space for measures such as the quality 
of each sample [Mark99]. If the z of two samples are similar, the sample processor gives preference to 
the better sample. The quality of the sample is derived differently according to the scene. In the context 
of imperfect registration characteristic to our (and probably all) current depth-image acquisition devices, 
we obtained better results when we consistently chose the samples of one sampling location and used the 
additional samples from other images just to fill in holes. Synthetic data simulates perfect registration 
and the quality of the samples was derived from the interpolation factor of the tile it belonged to: 
the closer the interpolation factor was to 2x2, the higher the quality5. A 128-bit wide memory interface 
provides read/write access to all three buffers in parallel. Each Sample Processor processes a sample 
every two clock cycles; this is the maximum possible rate, since 2 Region Buffer accesses (1 read and 
1 write) are required for each sample. The Sample Processor is pipelined, so that each computation has 
several cycles to execute, while sustaining the rate of a sample every two clock cycles. Thus 4 Sample 
Processors handle an aggregate rate of 2 samples per clock cycle, or 600 million samples/sec at 300 MHz. 
The back buffer outputs samples from the previous region to the Reconstruction Buffer, via a shift path 
that spans all 4 partitions of the Region Accumulator. A small fraction of memory cycles are stolen from 
the Sample Processors, to feed this scan-out path. 3.2.3 Reconstruction buffer The Reconstruction Buffer 
accepts the stream of final warped subsample values from the Region Accumulator, and filters them to 
produce final pixel values for the 128x128 pixel region. The Reconstruction Buffer includes two scan-line-sized 
accumulators, and four simple processors. For each RGB/Offset/Present value, each color component is 
multiplied by a weight from the filtering kernel and added to a sum. Normalization by the sum of weights 
produces the final pixel value, which is output from the ASIC to the Frame Buffer. The filter kernel 
is 2x2 pixels in size, with 4x4 sub-pixel resolution. The two 2-bit offset values select the proper kernel 
element within each sub-pixel. 5 A 2x2 interpolation factor implies destination image sampling close 
to reference image sampling, which is desirable. From Warp Array  3.2.4 Frame Buffer The Frame Buffer 
is a straightforward assembly of commodity DRAMs and programmable parts. It must absorb the full bandwidth 
of the Reconstruction Buffers on all Nodes, so the peak output rate of the Nodes must be tuned to avoid 
over-running the Frame Buffer.  3.3 Host and Software The host is responsible for determining which 
reference-image tiles will be used to compute the current destination image, for sorting the tiles according 
to screen region, and for sending the tiles to the WarpEngine Nodes. The host must also determine the 
interpolation factor for each chosen tile and send instructions to control the warping and interpolation, 
but these instructions are cached in the Tile Caches and should not represent a significant computational 
or bandwidth burden for the host. 3.3.1 Retained Mode Since all reference images are available beforehand, 
and since the depth discontinuities in the reference images do not depend on the desired view, surface 
connectivity is estimated as a pre-process. This frees the Warp Array of an additional task at the price 
of a few additional connectivity bits per reference-image sample. Determining which tiles are needed 
for the current image begins with choosing the tiles that are visible. This is done efficiently by subdividing 
each reference image down to 16x16 tiles in quad­tree fashion and recursively testing whether rectangular 
sub­images of the image are visible. The visibility test itself is identical to the bucket sorting of 
tiles: the 4 corners of the sub­image are warped with the minimum and then the maximum disparity of the 
sub-image. The bounding box of the 8 resulting points is a conservative estimate of the screen area covered 
by the sub-image. If the sub-image is a tile (a leaf in the quad-tree) it is also assigned to the appropriate 
screen region bin(s). Depending on the scene, a large number of tiles can be visible and warping all 
of them is inefficient. Not all visible tiles are needed for the current frame since some tiles sample 
the same surfaces. Choosing among the visible tiles is not a trivial task. First one needs to determine 
which tiles sample the same surfaces and then choose among the several candidate tiles according to a 
quality metric. The algorithm we use approximates each visible tile by two triangles. The triangles are 
transformed, projected and scan­converted according to the desired view. The z-buffer test is fuzzy and 
when two samples are close, the one that belongs to a better tile wins. A better tile is a tile whose 
approximating triangles have a desired-image size closer to 16x16, which implies a reference­image sampling 
close to the destination-image sampling. After all visible tiles are processed the chosen tiles are the 
tiles that have at least one sample left in the tile-choosing buffer. Scene Tiles Overlap factor Interpolation 
Factor Reading room w/o t.c. 6246 1.28 2.3x2.5 w/ t.c. 4863 1.32 2.4x2.6 Eurotown w/o t.c. 15050 1.23 
2.2x2.4 w/ t.c. 6736 1.44 2.6x2.9 Helicopter w/o t.c. 9101 1.47 2.5x3.7 w/ t.c. 4976 1.61 3.3x4.8 Table 
1. Simulation results on three test scenes at VGA resolution (see Color Plates 3-5) with and without 
tile choosing. Table 1 shows the average number of visible tiles in our simulations and the number of 
tiles chosen by the algorithm. It also shows the overlap factor, the average number of regions a tile 
mapped to in our simulations. The number of chosen tiles is high due to the tiles that have depth discontinuities 
and for which the triangle approximation breaks. Such tiles are conservatively chosen since the algorithm 
cannot establish their potential redundancy. We are currently investigating splitting the tiles that 
have depth discontinuities into depth-discontinuity-free tiles, whenever possible. We believe that this 
will reduce the total number of chosen tiles since the algorithm presented above will now eliminate more 
redundant tiles. The overlap factor also will be lower since tiles with depth discontinuities have an 
unnecessarily large screen-space bounding box. The host needs to determine the interpolation factor for 
each chosen tile. The ideal interpolation factor is the minimum value for which surface continuity is 
preserved. We first find the maximum changes in disparity along each direction, then we use them to estimate 
the maximum screen-space distance between two neighboring samples. The maximum one-pixel disparity variation 
is computed as a pre-process, taking into consideration depth discontinuities. Table 1 shows the average 
of the interpolation factors used in the simulations shown on the video. Frame-to-frame coherence can 
be exploited to minimize bandwidth requirements by storing each rendered tile in the Tile Cache of the 
WarpEngine Node that rendered it6. A large 6 Using a 64-MegaBit SDRAM chip as the Tile Cache, each WarpEngine 
node can cache up to 4,096 reference-image tiles (each tile contains, 256 pixels, each with 4 bytes of 
color and connectivity, and 4 bytes of disparity). percentage of these tiles can then be used in rendering 
the same region for the next frame, and many of the remainder can be re­distributed using the Network 
Interface and used by other WarpEngine Nodes for other regions. Only a relatively small percentage of 
the tiles will need to be sent from the host; in fact, with a modest number of reference images, it should 
be possible to cache all the reference-image tiles. A PC's AGP interface should provide plenty of bandwidth 
for sending missing tiles and pointers to cached tiles7 . 3.3.2 Immediate Mode For immediate-mode, frame-to-frame 
coherence cannot be utilized as effectively, since users may wander into areas of the environment that 
have not been previously sampled in reference images, and the environment itself may indeed be in flux 
(persons moving, for example). This means that bandwidth requirements from the host will be much higher. 
In the worst case, it may be necessary on each frame to send every tile from the host to the WarpEngine, 
and to render every tile. Within the next few years, we do not expect real-time depth­image acquisition 
at better than VGA resolution. We can build an immediate-mode system with 20 WarpEngine Nodes that contains 
a full-screen-sized warp buffer; this means that bucket sorting is not required. Similarly, the low-resolution 
yields a manageable amount of data. If one data stream provides 640*480 pixels at 30Hz, this is 36,000 
tiles/sec or 72 megabytes/sec. For an immediate mode system with four such data streams, a single high-end 
PC host with an AGP 4X Interface could handle routing tiles to the WarpEngine Nodes. Silicon technology 
(for the WarpEngine ASIC) and interface technology (for data bandwidth) should scale as depth-image acquisition 
scales. We are also investigating the possibility of decompressing tiles within the Warp Array. Another 
difference is that the PEs will have to compute connectivity information. This is not a serious performance 
loss since the computation required is simple enough: two adds and a compare for each of the four directions 
along which connectivity is estimated; a PE can easily get the disparities of the neighboring samples 
through the closest-neighbor communication paths. Also the host cannot approximate the interpolation 
factor as described in 3.3.1 since the tile information needed cannot be pre-computed. Our solution is 
to let the Warp Array estimate the interpolation factor: after all PEs warp their sample, using the inter-PE 
paths, the Warp Array establishes the maximum distance between consecutive warped samples, in both directions. 
This produces interpolation-factors that are close to ideal, as described in 3.3.1.  3.4 Performance 
Considerations The performance estimates are based on our WarpEngine functional-block-level software 
simulator. The Warp Array performance was measured with a cycle-accurate simulator. The Warp Engine system 
has two basic performance limits: the number of tiles per second that can be warped and interpolated, 
and the number of regions per second for which final pixel values can be reconstructed and forwarded 
to the Frame Buffer. The first defines the maximum achievable rendering rate, while the second defines 
the maximum achievable update rate for a given screen­size. 7 AGP 2X presently supports peak data transfer 
rates of 533 MBytes/sec, with a future 4X extension to 1066 MBytes/sec planned. Actual usable throughputs 
are 50-80% of the peak rate. 3.4.1 Tile Warping/Interpolation Performance We have found that the Warp 
Array will require 1878 cycles to perform a 3D image warp for all samples in a tile, using fixed­point 
arithmetic as described in [Mark99]. If interpolation is done after warping, the interpolated samples 
will be computed and output to the Region Accumulator one sample at a time (over the entire tile). Outputting 
one sample for the entire tile requires 256 clock cycles (one cycle per PE). The time to actually compute 
each interpolated sample from the warped samples will be significantly less. Table 1 indicates that, 
on average, interpolation generates about 8 sub-samples, so about 8 * 256 = 2K cycles are required to 
interpolate and output the warped samples. The Region Accumulator can process up to two samples per clock 
cycle, assuming decent load-balancing, so it is very likely that the one sample per cycle peak output 
rate of the Warp Array can be sustained. The total time per tile is therefore about 4K cycles, or about 
75K tiles per second, per Node. Table 1 shows a typical overlap factor of less than 1.5, so the net performance 
will be 50K tiles per second per Node. Table 1 shows that at VGA resolutions 5K tiles are typically required 
to render a scene; we believe that these numbers extrapolate to higher resolutions. Using these assumptions, 
we computed the following performance numbers for some typical system configurations: Screen size Tiles/ 
frame Nodes Sub­samples/sec Update rate 640x480 5K 3 307 M 30 1280x1024 20K 16 1.6 G 40 2048x1024 32K 
32 3.2 G 50 Table 2. Projected performance of typical system configurations. These numbers show that 
4 Nodes can easily handle VGA output resolution loads and that 32 Nodes make a quite powerful system 
capable of high update rates at HDTV resolution. If interpolation is done before warping (which we do 
not think is necessary), it takes on average 8200 cycles to interpolate and warp the same average number 
of 8 sub-samples. However, there is enough time for the warped sub-samples to be forwarded to the Region 
Accumulator so no additional cycles are needed. This indicates that interpolation and then warping is 
feasible but it requires on average twice as many Nodes for the same performance. 3.4.2 Reconstruction 
Performance The Reconstruction Buffer operates on the back buffer. It requires 64K clock cycles to compute 
final pixel values for a region, which is pipelined with the time to render another region. Only if the 
next region is assigned fewer than 16 tiles (less than it takes to cover the region) will the reconstruction 
time affect performance.  4 FUTURE WORK The programmability and high-performance of the WarpEngine 
will allow us to conduct many experiments. Thus far we have not attempted to generate view dependent 
effects. If provided with the necessary BRDF information, perhaps as a shader program, the Warp Array 
could compute the view dependent color [Olano98]. Similarly, one could experiment with changing the original 
lighting conditions of the reference images. Tile-choosing is a very important and difficult problem, 
similar to the visibility and level-of-detail problems in conventional rendering. Our tile-choosing is 
presently complicated by having to detect and resolve inconsistencies between the samples of the same 
surface seen in several reference images. Dealing separately with view-dependencies will simplify tile-choosing. 
Another challenging problem is encountered at the silhouettes. Since photographs or antialiased renderings 
are used, the color of a silhouette sample is a blend between the color of the front and back surfaces. 
When warping the depth image, this blended color persists on both the front and the back surfaces, which 
are no longer adjacent. To prevent this, we discard the silhouette samples, and rely on other reference 
images to provide replacement samples. For very thin features however, correct samples cannot be found 
in any of the reference images, causing the thin features to disappear. This comes at no surprise since, 
in order to respect the Nyquist sampling-rate criterion, the reference images should sample the scene 
at least twice as densely as the output image. We could, of course, use higher-resolution reference images, 
but practical considerations will usually prevent this. An alternative approach, at least in retained 
mode, is to detect the thin features and model them with tiles from higher resolution images closer to 
the objects. For example, a light pole that projects one-pixel wide in the desired image can be extracted 
from a reference image that sees it as several pixels wide. Detecting the thin features can be done relatively 
easily using the depth-discontinuity detection method described. For efficient tile utilization, the 
reference-image coordinates of the samples can be stored explicitly, which allows packing the pole samples 
on one tile, at the price of more data per tile and slightly longer warping time. An attractive alternative 
use for the WarpEngine architectural ideas is in a hybrid geometry/image-based rendering machine, which 
uses images as impostors to bound the total number of polygons [Aliaga99].  5 CONCLUSIONS The WarpEngine 
is a 3D graphics hardware architecture designed specifically for rendering by warping images with depth. 
It might be argued that warping and then interpolating is equivalent to creating a quadrilateral mesh 
and rendering it on conventional polygon-rendering hardware. There is some truth to this, but the WarpEngine 
ASIC is a quad renderer particularly optimized for this application. This is because the quads formed 
by warping the samples of a reference-image tile are of small and uniform size, and conveniently grouped 
into square arrays, so a SIMD array provides particularly efficient processing. Furthermore, bilinearly 
interpolating between warped samples (a forward-mapping) requires minimal setup costs, unlike conventional 
scan conversion (which is a reverse-mapping), further optimizing the processing of tiny quads. Finally, 
the SIMD array allows flexible programmability, facilitating experimentation with new algorithms. And 
integration of the SIMD array with the on-chip warp buffer obviates bandwidth concerns and the use of 
off-chip memory (except for the frame buffer), and the partitioning facilitates scalability to very high 
performance levels. The WarpEngine will be implemented using a single custom ASIC, replicated as necessary 
to meet the desired resolution and warping performance. We expect the ASIC to measure about 12 mm by 
16 mm when fabricated on a 0.18-micron process, and to run at 300 MHz or higher. A small 4-node system 
could fit on a board inside a PC, while a 32-node system will be in a workstation-sized enclosure. We 
expect to begin layout of the WarpEngine ASIC later this year. Building the WarpEngine will provide us 
with insights applicable not only to IBRW architectures but also to architectures for conventional polygon-based 
rendering, particularly when rendering small polygons. We expect that the WarpEngine, coupled with image-based 
modeling or real-time depth imaging, will render images that look truly photorealistic, leading to a 
dramatically heightened sense of presence for applications like visual simulation and tele-presence, 
and enabling entirely new applications such as 3D TV. ACKNOWLEDGEMENTS We would like to thank Gary Bishop 
and John Poulton for their encouragement at early stages of this work, David McAllister for his important 
contributions to depth-image acquisition, and Henry Fuchs for his useful critique of earlier versions 
of this paper. Special thanks to Mary Whitton for organizing the SIGGRAPH­submission event here at UNC. 
Support was provided by DARPA, order number E278, and NSF grant number MIP-9612643. REFERENCES [Aliaga99] 
Aliaga D. and Lastra A., Automatic Image Placement to Provide a Guaranteed Frame Rate , Proc. SIGGRAPH 
99, 307-316 (1999). [Beier92] Beier T. and Neely S., Feature-Based Image Metamorphosis , Proc. SIGGRAPH 
92, 35-42 (1992). [Beraldin92] Beraldin J.-A., Rioux M., Blais F., Domey J., and Cournoyer L., Registered 
Range and Intensity Imaging at 10-Mega Samples per Second , Opt. Eng., 31(1): p. 88-94 (1992). [Chen93] 
Chen S. and Williams L., View Interpolation for Image Synthesis , Proc. SIGGRAPH 93, 279-288 (1993). 
[Chen95] Chen S., Quicktime VR - An Image-Based Approach to Virtual Environment Navigation , Proc. SIGGRAPH 
95, 29-38 (1995). [Cyra] The Cyrax System, in http://www.cyra.com/. [Debevec96] Debevec P., Taylor C., 
and Malik J., Modeling and Rendering Architecture from Photographs: A Hybrid Geometry and Image-Based 
Approach , Proc. SIGGRAPH 96, 11-20 (1996). [Ellsworth97] Ellsworth D., Polygon Rendering for Interactive 
Visualization on Multicomputers, PhD thesis, University of North Carolina at Chapel Hill, 1997. [Gortler96] 
Gortler S., Grzeszczuk R., Szeliski R., and Cohen M., The Lumigraph , Proc. SIGGRAPH 96, 43-54 (1996). 
[Kanade99] Kanade T., Rander P., Vedula S., and Saito H., Virtualized Reality: Digitizing a 3D Time-Varying 
Event As Is and in Real Time , Mixed Reality, Merging Real and Virtual Worlds, Y. Ohta and H. Tamura, 
Editors. Springer-Verlag. p. 41-57 (1999). [K2T] Scene Modeler, http://www.k2t.com/. [Levoy96] Levoy 
M. and Hanrahan P., Light Field Rendering , Proc. SIGGRAPH 96, 31-42 (1996). [Mark97] Mark W., McMillan 
L., and Bishop G., "Post-Rendering 3D Warping", 1997 Symposium on Interactive 3D Graphics, 7-16 (1997). 
[Mark99] Mark W., Post-Rendering 3D Image Warping: Visibility, Reconstruction, and Performance for Depth-Image 
Warping, PhD thesis, University of North Carolina at Chapel Hill, 1999. [McAllister99] McAllister, D., 
Nyland L., Popescu V., Lastra A., McCue C., "Real-Time Rendering of Real-World Environments", Rendering 
Techniques '99, Proc. Eurographics Workshop on Rendering, 145­160, (1999). [McMillan95] McMillan L. and 
Bishop G., Plenoptic Modeling: An Image-Based Rendering System , Proc. SIGGRAPH 95, 39-46 (1995). [McMillan97] 
McMillan L., An Image-Based Approach to Three-Dimensional Computer Graphics, PhD thesis, University of 
North Carolina at Chapel Hill, 1997. [Minolta] Minolta 3D 1500, in http://www.minolta3d.com/. [Molnar92] 
Molnar S., Eyles J., and Poulton J., PixelFlow: High-speed Rendering using Image Composition , Proc. 
SIGGRAPH 92, 231­240 (1992). [Molnar94] Molnar S., Cox M., Ellsworth D., and Fuchs H., "A Sorting Classification 
of Parallel Rendering", IEEE Computer Graphics and Aplications, 14(4), 23-32 (1994) [Montrym97] Montrym 
J., Baum D., Dignam D., and Migdal C., InfiniteReality: A Real-Time Graphics System , Proc. SIGGRAPH 
97, 293-302 (1997). [Mueller95] Mueller C., The Sort-First Rendering Architecture for High-Performance 
Graphics , 1995 Symposium on Interactive 3D Graphics, 75-84 (1995). [Olano98] Olano M. and Lastra A., 
A Shading Language on Graphics Hardware: The PixelFlow Shading System , Proc. SIGGRAPH 98, (1998). [Regan99] 
Regan M., Miller G., Rubin S., and Kogelnik C., A Real Time Low-Latency Hardware Light-Field Renderer 
, Proc. SIGGRAPH 99, 287-290 (1999). [Seitz96] Seitz S. and Dyer C., View Morphing: Synthesizing 3D Metamorphoses 
Using Image Transforms , Proc. SIGGRAPH 96, 21-30 (1996). [Shade98] Shade J., Gortler S., He L., and 
Szeliski R., Layered Depth Images , Proc. SIGGRAPH 98, 231-242 (1998). [Torborg96] Torborg J. and Kajiya 
J., Talisman: Commodity Real-time 3D Graphics for the PC , Proc. SIGGRAPH 96, 353-364 (1996). [Westover90] 
Westover L., Footprint Evaluation for Volume Rendering , Proc. SIGGRAPH 90, 367-376 (1990). [Wolberg90] 
Wolberg G., Digital Image Warping, IEEE Computer Society Press, Los Alamitos California, 1990.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344981</article_id>
		<sort_key>443</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>50</seq_no>
		<title><![CDATA[Pomegranate]]></title>
		<subtitle><![CDATA[a fully scalable graphics architecture]]></subtitle>
		<page_from>443</page_from>
		<page_to>454</page_to>
		<doi_number>10.1145/344779.344981</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344981</url>
		<abstract>
			<par><![CDATA[<p>Pomegranate is a parallel hardware architecture for polygon rendering that provides scalable input bandwidth, triangle rate, pixel rate, texture memory and display bandwidth while maintaining an immediate-mode interface. The basic unit of scalability is a single graphics pipeline, and up to 64 such units may be combined. Pomegranate's scalability is achieved with a novel &#8220;sort-everywhere&#8221; architecture that distributes work in a balanced fashion at every stage of the pipeline, keeping the amount of work performed by each pipeline uniform as the system scales. Because of the balanced distribution, a scalable network based on high-speed point-to-point links can be used for communicating between the pipelines.</p><p>Pomegranate uses the network to load balance triangle and fragment work independently, to provide a shared texture memory and to provide a scalable display system. The architecture provides one interface per pipeline for issuing ordered, immediate-mode rendering commands and supports a parallel API that allows multiprocessor applications to exactly order drawing commands from each interface. A detailed hardware simulation demonstrates performance on next-generation workloads. Pomegranate operates at 87-99% parallel efficiency with 64 pipelines, for a simulated performance of up to 1.10 billion triangles per second and 21.8 billion pixels per second.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[graphics hardware]]></kw>
			<kw><![CDATA[parallel computing]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Parallel processing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010169</concept_id>
				<concept_desc>CCS->Computing methodologies->Parallel computing methodologies</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31077201</person_id>
				<author_profile_id><![CDATA[81100023271]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Matthew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Eldridge]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P111544</person_id>
				<author_profile_id><![CDATA[81100571859]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Homan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Igehy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15033698</person_id>
				<author_profile_id><![CDATA[81100482576]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Pat]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hanrahan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Freedom 3000 Technical Overview. Technical report, Evans &amp; Sutherland Computer Corporation, October 1992.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Denali Technical Overview. Technical report, Kubota Pacific Computer Inc., March 1993.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>629078</ref_obj_id>
				<ref_obj_pid>628900</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[William J. Dally. Virtual-Channel Flow Control. IEEE Transactions on Parallel and Distributed Systems, pages 194-205, March 1992.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>550183</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Jos~ e Duato, Sudhakar Yalmanchili, and Lionel Ni. Interconnection Networks: an Engineering Approach. IEEE Computer Society Press, 1997.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311583</ref_obj_id>
				<ref_obj_pid>311534</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Homan Igehy, Matthew Eldridge, and Pat Hanrahan. Parallel Texture Caching. 1999 SIGGRAPH / Eurographics Workshop on Graphics Hardware, pages 95-106, August 1999.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>285321</ref_obj_id>
				<ref_obj_pid>285305</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Homan Igehy, Matthew Eldridge, and Kekoa Proudfoot. Prefetching in a Texture Cache Architecture. 1998 SIG- GRAPH / Eurographics Workshop on Graphics Hardware, pages 133-142, August 1998.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280837</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Homan Igehy, Gordon Stoll, and Pat Hanrahan. The Design of a Parallel Graphics Interface. SIGGRAPH 98 Conference Proceedings, pages 141-150, July 1998.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37422</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[William E. Lorensen and Harvey E. Cline. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. Computer Graphics (SIGGRAPH 87 Conference Proceedings), pages 163-169, July 1987.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>182469</ref_obj_id>
				<ref_obj_pid>182466</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Steven Molnar, Michael Cox, David Ellsworth, and Henry Fuchs. A Sorting Classification of Parallel Rendering. IEEE Computer Graphics and Applications, pages 23-32, July 1994.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134067</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Steven Molnar, John Eyles, and John Poulton. PixelFlow: High-Speed Rendering Using Image Composition. Computer Graphics (SIGGRAPH 92 Conference Proceedings), pages 231-240, July 1992.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258871</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[John S. Montrym, Daniel R. Baum, David L. Dignam, and Christopher J. Migdal. InfiniteReality: A Real-Time Graphics System. SIGGRAPH 97 Conference Proceedings, pages 293- 302, August 1997.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311584</ref_obj_id>
				<ref_obj_pid>311534</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Rudrajit Samanta, Jiannan Zheng, Thomas Funkhouser, Kai Li, and Jaswinder Pal Singh. Load Balancing for Multi- Projector Rendering Systems. 1999 SIGGRAPH / Eurographics Workshop on Graphics Hardware, pages 107-116, August 1999.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Pomegranate: A Fully Scalable Graphics Architecture Matthew Eldridge Homan Igehy Pat Hanrahan Stanford 
University* Abstract Pomegranate is a parallel hardware architecture for polygon ren­dering that provides 
scalable input bandwidth, triangle rate, pixel rate, texture memory and display bandwidth while maintaining 
an immediate-mode interface. The basic unit of scalability is a single graphics pipeline, and up to 64 
such units may be com­bined. Pomegranate s scalability is achieved with a novel sort­everywhere architecture 
that distributes work in a balanced fash­ion at every stage of the pipeline, keeping the amount of work 
per­formed by each pipeline uniform as the system scales. Because of the balanced distribution, a scalable 
network based on high-speed point-to-point links can be used for communicating between the pipelines. 
Pomegranate uses the network to load balance triangle and frag­ment work independently, to provide a 
shared texture memory and to provide a scalable display system. The architecture provides one interface 
per pipeline for issuing ordered, immediate-mode render­ing commands and supports a parallel API that 
allows multiproces­sor applications to exactly order drawing commands from each in­terface. A detailed 
hardware simulation demonstrates performance on next-generation workloads. Pomegranate operates at 87 
99% parallel ef.ciency with 64 pipelines, for a simulated performance of up to 1.10 billion triangles 
per second and 21.8 billion pixels per second. CR Categories: I.3.1 [Computer Graphics]: Hardware Architecture 
Parallel Processing Keywords: Graphics Hardware, Parallel Computing 1 Introduction The performance of 
interactive graphics architectures has been im­proving at phenomenal rates over the past few decades. 
Not only have the speed improvements kept up with or exceeded Moore s Law, but each successive generation 
of graphics architecture has expanded the feature set. Despite these great improvements, many applications 
cannot run at interactive rates on modern hardware. Examples include scienti.c visualization of large 
data sets, photo­realistic rendering, low-latency virtual reality, and large-scale dis­play systems. 
A primary goal in graphics research is .nding ways to push this performance envelope, from the details 
of the chip ar­chitecture to the overall system architecture. The past few years have also marked a turning 
point in the his­tory of computer graphics. Two decades ago, interactive 3D graph­ *{eldridge,homan,hanrahan}@graphics.stanford.edu 
 Permission to make digital or hard copies of part or all of this work or personal or classroom use is 
granted without fee provided that copies are not made or distributed for profit or commercial advantage 
and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, 
to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. SIGGRAPH 
2000, New Orleans, LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 Figure 1: Each Pomegranate 
pipeline is composed of .ve stages: geometry (Geom), rasterization (Rast), texture (Tex), fragment (Frag) 
and display (Disp). A network (Net) con­nects the pipelines and a sequencer (Seq) orders their execu­tion 
of multiple graphics streams submitted by the application threads (App). ics systems were found only 
at large institutions. As semiconductor technologies improved, graphics architects found innovative ways 
to place more functionality on fewer chips, and interactive graphics workstations made their way to the 
desktops of engineers. Today, the entire graphics pipeline can be placed on a single chip and sold at 
a mass-market price point. Because of the enormous economies of scale afforded by commoditization, this 
trend has a signi.cant impact on how high-end systems must be built: it is much more cost effective to 
design a single low-end, high-volume system and replicate it in an ef.cient manner in order to create 
high-end, low­volume systems. For example, supercomputers used to be designed with unique, proprietary 
architectures and esoteric technologies. With the commoditization of microprocessors, these designs were 
replaced by highly parallel multiprocessor systems that made use of microprocessor technology. The Pomegranate 
architecture pro­vides a way of scaling the base unit of a single graphics pipeline to create higher 
performance systems. Pomegranate is composed of n graphics pipelines interconnected by a scalable point-to-point 
network, as depicted in .gure 1. Each pipeline accepts standard, immediate-mode OpenGL commands from 
a single context as well as parallel API commands for ordering the drawing commands of the context with 
the drawing commands of other contexts. As with any parallel system, Pomegranate will only operate ef.ciently 
if the load is balanced across its functional units. However, graphics primitives can vary substantially 
in the amount of processing time they require. Furthermore, the amount of work a primitive will require 
is not known a priori. Distribut­ing and balancing this workload in a dynamic fashion while min­imizing 
work replication is a key innovation of the Pomegranate architecture and directly contributes to its 
scalability. A novel se­rial ordering mechanism is used to maintain the order speci.ed by the OpenGL 
command stream, and a novel parallel ordering mech­anism is used to interleave the work of multiple graphics 
contexts. Because the use of broadcast communication is minimized in both input rate pixel rate triangle 
rate texture display bandwidth memory Figure 2: The serial graphics pipeline consists of an applica­tion 
(A), a geometry processor (G), a rasterizer (R), a texture processor (T), a fragment processor (F) and 
a display pro­cessor (D). The units with a direct impact on each scalability measure are underlined. 
the data distribution and the ordering, Pomegranate is able to scale to a high degree of parallelism. 
In addition to scalability, an equally important characteristic of the Pomegranate architecture is its 
compatibility with a modern graphics API. OpenGL has strict ordering semantics, meaning that all graphics 
commands must appear to execute in the order they are speci.ed. For example, two overlapping polygons 
must appear on the screen in the order they were submitted by the user, and a state change command applies 
to all subsequent primitives. This constraint forces any parallel OpenGL hardware architecture to be 
capable of maintaining the serial order speci.ed by the application. This restriction is one of the major 
obstacles to building a scalable OpenGL hardware renderer. As an analogy, C has become the de facto standard 
for programming, and as a result microprocessor ar­chitects focus the bulk of their efforts addressing 
the dif.culties it introduces pointer aliasing, limited instruction-level parallelism, strict order 
of operations, etc. Similarly, we felt it was important to design within the ordering constraints of 
OpenGL. In addition to specifying ordered semantics, OpenGL is an immediate-mode in­terface. Commands 
that are submitted by the application are drawn more or less immediately thereafter. APIs that are built 
around dis­play lists, scene graphs, or frame semantics all provide the oppor­tunity for the hardware 
to gather up a large number of commands and partition them among its parallel units. An immediate-mode 
in­terface does not enable this approach to extracting parallelism, and thus provides a further challenge. 
A fully scalable graphics architecture should provide scalability on the .ve key metrics depicted in 
.gure 2: input rate, triangle rate, rasterization rate, texture memory and display bandwidth. Input 
rate is the rate at which the application can transmit commands (and thus primitives) to the hardware. 
 Triangle rate is the rate at which geometric primitives are as­sembled, transformed, lit, clipped and 
set up for rasterization.  Pixel rate is the rate at which the rasterizer samples primi­tives into fragments, 
the texture processor textures the frag­ments and the fragment processor merges the resultant frag­ments 
into the framebuffer.  Texture memory is the amount of memory available to unique textures.  Display 
bandwidth is the bandwidth available to transmit the framebuffer contents to one or more displays.  
Pomegranate provides near-linear scalability in all .ve metrics while maintaining an ordered, immediate-mode 
API. We motivate our discussion of Pomegranate by suggesting two possible implementations: a scalable 
graphics pipeline and a multi­pipeline chip. A scalable graphics pipeline could be .exibly de­ployed 
at many levels of parallelism, from a single pipeline solu­tion with performance comparable to a modern 
graphics accelerator up to a 64 pipeline accelerator with supercomputer performance. (a) (b) (c) (d) 
Figure 3: Sort-.rst (a) sorts triangles before the geometry stage. Sort-middle (b) sorts triangles between 
geometry and rasterization. Sort-last fragment (c) sorts fragments between rasterization and fragment 
processing. Sort-last image (d) sorts pixels between fragment processing and the display. The texture 
stage has been eliminated in this diagram, but in prac­tice will either be located at the end of the 
rasterization stage or the beginning of the fragment stage. The incremental cost of the Pomegranate pipeline 
over a traditional graphics pipeline is the area required for approximately 1MB of buffering, 256KB for 
supporting 64 contexts, and the area and pins of a high-speed network interface. We estimate that the 
incremen­tal cost of the Pomegranate pipeline is an additional 200 pins and 50mm2 for memory in a modern 
0.18µm process. A network chip, replicated as necessary to interconnect all of the pipelines, would weigh 
in at approximately 1000 pins, which is feasible. Our second possible implementation is a single chip 
with multiple Pomegranate pipelines. In such a chip the Pomegranate architecture would be leveraged as 
a practical method for using the hundreds of millions of transistors which will soon be practical in 
even a consumer-level graphics accelerator. Current graphics accelerators already stretch the capabilities 
of VLSI tools and engineers with their size and complexity. Pomegranate would enable the design of a 
compara­tively smaller pipeline which could then be replicated to consume the available transistor count, 
rather than requiring the design of a huge monolithic pipeline. In this paper, we will .rst brie.y review 
previous work in par­allel graphics architectures. Then, we will give an overview of the Pomegranate 
architecture and the details of its key components, de­scribing how work is distributed in a balanced 
way at each stage of the pipeline to give scalable performance in each of the .ve metrics. Next, we describe 
the serial ordering algorithm that maintains the serial order mandated by a single OpenGL context as 
well as a par­allel ordering algorithm that interleaves work according to the order speci.ed by a parallel 
API. Finally, we present results from a de­tailed hardware simulation that demonstrates Pomegranate s 
scala­bility and compares it to traditional parallel graphics architectures.  2 Background 2.1 Parallel 
Graphics Architectures There are a number of published systems that use parallelism to achieve high performance 
levels. How this parallelism is organized has a direct effect on the scalability of these architectures. 
Mol­nar et al. describe a taxonomy for classifying parallel rendering architectures as sort-.rst, sort-middle 
or sort-last based on where they transition from object-space parallelism to screen-space par­allelism 
[9]. A variation of this taxonomy is illustrated in .gure 3. All of these architectures typically exploit 
parallelism at each of the geometry, rasterization and fragment stages, either in object-space (assigning 
work by primitive) or in screen-space (assigning work by screen location). Historically, while addressing 
scalable triangle rate and pixel rate, most architectures have used a single host inter­face, replicated 
texture memory across the rasterizers, and shared a single bus for display, all of which eventually limit 
the system s capabilities. In a sort-.rst architecture, the screen is subdivided so that each graphics 
pipeline is responsible for a fraction of the pixels. The application processor distributes primitives 
only to the overlap­ping pipelines. Because the overlap computation can be time­consuming, it is usually 
amortized over groups of primitives. The primary advantage of this technique is its ability to use relatively 
standard graphics pipelines as its building block, with only glue logic for the display, and a straightforward 
mechanism of provid­ing ordering. A major challenge of sort-.rst architectures has been the load balancing 
of both triangle work and pixel work. One scheme is to dynamically subdivide the screen into a small 
num­ber of large tiles [12]. These schemes typically require a retained­mode interface with frame semantics 
so that each tile comprises an equal amount of work, and .nding an ef.cient, accurate esti­mator of work 
is challenging. Another scheme is to subdivide the screen into a large number of small regions and either 
statically or dynamically assign the regions. While such schemes work with immediate-mode interfaces, 
minimizing overlap while balancing the load across a wide variety of workloads is dif.cult. As with sort-.rst 
architectures, sort-middle architectures exploit image parallelism by dividing responsibility for primitive 
rasteri­zation and fragment processing in image-space. However, any ge­ometry unit is allowed to process 
any primitive. Thus, a sort must occur between the geometry units and the rasterizers, which are re­sponsible 
for speci.c areas of the screen. Generally, the partitioning of the screen has been done on a very .ne 
granularity. For exam­ple, 2-pixel wide stripes are used on the SGI In.nite Reality [11] to ensure a 
good load balance of pixel work across all rasterizers. While providing excellent load balancing of pixel 
work, these small tiles impose a high cost in redundant triangle work because every triangle is assumed 
to overlap every tile. This broadcast of triangle work sets an upper limit on the triangle rate the system 
can sustain. However, this broadcast mechanism does provide a natural point to return the primitives 
processed by the parallel geometry stage to their speci.ed serial order. Larger tiles have been used 
to remove this broadcast limitation at the cost of large reorder buffers [7]. Minimizing redundant work 
due to primitives overlapping multi­ple tiles while ef.ciently addressing the temporal load imbalances 
of an immediate-mode API is a major challenge for these systems. Unlike the sort-.rst and sort-middle 
architectures, sort-last ar­chitectures exploit object parallelism in both the geometry and ras­terization 
stages. In fragment sorting architectures, any primitive may be given to any geometry unit, and each 
geometry unit dis­tributes its work to a single rasterization unit. The rasterization units then distribute 
the resultant fragments to the speci.c fragment processor responsible for the corresponding region of 
the screen. Because each fragment is communicated only once from a single rasterizer to a single fragment 
processor, no broadcast is involved. The Evans &#38; Sutherland Freedom 3000 [1] and the Kubota De­nali 
[2] are both examples of fragment sorting architectures. The advantage of these architectures is that 
they potentially have greater triangle scalability than sort-middle since each triangle is processed 
by only one geometry and rasterization unit. However, even though a .ne image-space interleaving ensures 
load balancing at the frag­ment processors, there is little or no .exibility to load balance ras­terization 
work. If a few large primitives are submitted, they may push the system signi.cantly out of balance as 
one or a few ras­terizers are given much more work to do than the other rasterizers. This is problematic 
since primitive sizes are not known a priori. A second variation of sort-last architectures are image 
compo­sition architectures such as PixelFlow [10]. Multiple independent graphics pipelines render a fraction 
of the scene into independent context A context B glClear() glBarrierExec(1) draw opaque[1..500] glBarrierExec(1) 
draw transparent[1..500] glSemaphoreV(2) glBarrierExec(1) glSwapBuffers() glBarrierExec(1) draw opaque[501..1000] 
glBarrierExec(1) glSemaphoreP(2) draw transparent[501...1000] glBarrierExec(1)   Figure 4: The scene 
to be rendered consists of 1000 opaque primitives and 1000 transparent primitives. The opaque primi­tives 
are rendered with depth buffering enabled, and the trans­parent primitives are rendered in back to front 
order. The pseudocode uses two contexts to submit this scene in parallel. The .rst barrier ensures that 
the clear performed by context A is complete before context B starts drawing. The second barrier ensures 
that all the opaque primitives are drawn before any transparent primitives. The semaphore pair ensures 
that context A s half of the transparent primitives are drawn .rst. The .nal barrier ensures that all 
drawing is done before the swapbuffers occurs. framebuffers. Then, these framebuffers are composited 
based on color and depth to form a .nal image for display. Image composi­tion architectures are a signi.cant 
departure from the architectures discussed so far because they forfeit ordering altogether in order to 
scale to higher levels of performance. As with fragment sort­ing architectures, large primitives can 
cause signi.cant load imbal­ance in image composition architectures. Furthermore, while the displays 
on the previous architectures could be made scalable using approaches similar to Pomegranate, image composition 
displays are dif.cult to scale robustly. 2.2 Parallel Interface While building internally parallel graphics 
hardware is challeng­ing in its own right, recent graphics accelerators outstrip the abil­ity of the 
host interface to supply them with data (e.g. NVIDIA s GeForce256). Igehy et al. introduced a parallel 
API for graphics to address this bandwidth limitation [7]. The parallel API extends OpenGL with synchronization 
primitives that express ordering re­lationships between two or more graphics contexts that simultane­ously 
submit commands to the hardware. The signi.cance of these primitives is that they do not execute at the 
application level, which allows the application threads to execute past the synchronization primitives 
and continue submitting work. These synchronization commands are then later executed by the graphics 
system. This al­lows the programmer to order the execution of the various contexts without being reduced 
to using a serial interface. The primitives we focus our attention on are barriers and semaphores. A 
barrier synchronizes the execution of multiple graphics con­texts, ensuring that all of the commands 
executed by any of the contexts previous to the barrier have completed before any of the commands subsequent 
to the barrier have any effect. A barrier is de.ned with glBarrierCreate (name, count), which as­sociates 
a graphics barrier that has count contexts participating in it with name. A graphics context enters a 
barrier by calling glBarrierExec(name). A semaphore provides a point-to-point ordering constraint, and 
acts as a shared counter. A semaphore V (or up) operation atomically increments the counter. A semaphore 
P (or down) operation blocks until the counter is greater than zero, and then atomically decrements the 
counter. A semaphore is de.ned with glSemaphoreCreate(name, initialCount), V d by glSemaphoreV(name) 
and P dby glSemaphoreP(name). Figure 4 provides an example of the use of these primitives. Figure 5: 
The Pomegranate pipeline. The bandwidth require­ ments of the communication channels are labeled.  3 
Pomegranate Architecture The Pomegranate architecture is composed of graphics pipelines and a high-speed 
network which connects them. The pipeline, shown in .gure 5, is composed of .ve stages: geometry, raster­ization, 
texture, fragment and display. The geometry stage re­ceives commands from an application; transforms, 
lights and clips the primitives; and sends screen-space primitives to the rasterizer. The rasterizer 
performs rasterization setup on these primitives, and scan converts them into untextured fragments. The 
texturer ap­plies texture to the resultant fragments. The fragment processor receives textured fragments 
from the texturer and merges them with the framebuffer. The display processor reads pixels from the frag­ment 
processor and sends them to a display. The network allows each pipeline of the architecture to communicate 
with all the other pipelines at every stage. For example, each geometry processor can distribute its 
transformed primitives over all the rasterizers. Pomegranate achieves its scalability through a combination 
of a parallel host interface and multiple types of communication be­tween the functional units. Each 
geometry unit has a host interface that may receive graphics commands simultaneously and independently. 
Or­dering constraints between different graphics contexts may be speci.ed by parallel API commands. This 
provides scalabil­ity of input rate. Because the geometry unit is limited by the interface speed, there 
is no purpose in distributing commands from a single interface across multiple geometry units. The application 
must therefore provide a balanced number of tri­angles to each interface. This provides scalability of 
triangle rate.  A virtual network port allows each geometry unit to transmit screen-space primitives 
to any rasterizer. There is no con­straint on this mapping, thus allowing the geometry units to load 
balance triangle work among rasterizers. This provides scalability of triangle rate.  The sequencer, 
shared among all pipelines, determines the in­terleaving of the execution of the primitives emitted by 
each geometry unit. It allows multiple contexts to simultaneously submit commands to the hardware and 
to have their order of execution described by the parallel API. This provides scala­bility of input rate. 
 Each rasterizer scan converts screen-space triangles into un­textured fragments, and then passes them 
to the texturer where they are textured. The geometry units may load balance the amount of pixel work 
sent to each rasterizer in addition to the number of triangles. The geometry units may also subdivide 
large triangles so that their work is distributed over all the ras­terizers. This provides scalability 
of pixel rate.  Textures are distributed in a shared fashion among the pipeline memories, and each texture 
processor has a network port for reading and writing of remote textures. This provides scala­bility of 
texture memory.  Each texture processor has a network port that enables it to route its resultant fragments 
to the appropriate fragment pro­cessor according to screen-space location. This sorting stage performs 
the object-space to image-space sort, and allows the unconstrained distribution of triangles between 
the geom­etry and rasterization stages that balances object-space par­allelism. Fine interleaving of 
the fragment processors load balances screen-space parallelism and provides scalability in pixel rate. 
 Each display unit has a network port that allows it to read pixels from all of the fragment processors 
and output them to its display. This provides scalability of display bandwidth.  The Pomegranate architecture 
faces the same implementation challenges as other parallel graphics hardware: load balancing and ordering. 
Load balancing issues arise every time that work is dis­tributed. The four main distributions of work 
are: primitives to rasterizers by the geometry processors; remote texture memory ac­cesses by the texturers; 
fragments to fragment processors by the texturers; and pixel requests to the fragment processors by the 
dis­play engine. Additionally a balanced number of primitives must be provided to each geometry processor, 
but that is the responsibility of the application programmer. Two distinct ordering issues arise in Pomegranate. 
First, the primitives of a single graphics context will be distributed twice, .rst over the rasterizers, 
and then over the fragment processors. This double distribution results in the work for a single context 
arriv­ing out of order at the fragment processors, where it must be re­ordered. Second, each serial graphics 
context will execute its own commands in order, but it must in turn be interleaved with the other graphics 
contexts to provide parallel execution in accordance with any parallel API commands. In this section, 
we discuss in detail the different stages of the pipeline and their mechanisms for load balancing, and 
defer the discussion of maintaining a correct serial and parallel order and the associated sequencer 
unit until later. 3.1 Network Central to the Pomegranate architecture is a scalable network that supports 
the balanced distribution of work necessary for load bal­ancing and the synchronization communication 
necessary for or­dering. We chose to implement the network as a multi-stage butter­.y, depicted in .gure 
6. A discussion of other candidate networks is beyond this paper, and readers are encouraged to see the 
text by Duato, Yalmanchili and Ni [4] for a deeper discussion of high­performance scalable interconnects. 
 (a) (b) (c) Figure 6: The butter.y network is composed of a single build­ing block (a) which may be 
cascaded into a multi-stage net­work to support an arbitrary number of inputs (b &#38; c), with a number 
of stages that grows logarithmically with the number of inputs. Networks, and butter.ies in particular, 
are notorious for suffer­ing severe performance penalties under imbalanced loads and in­creasing latency 
with increasing utilization. Pomegranate s net­work usage is engineered to be both uniform and latency 
tolerant to avoid these problems. For example, in an n-pipeline system, a ge­ometry unit will send 1/nth 
of its triangles to each rasterizer. This distribution pattern occurs similarly during texture, fragment 
and display communication, and is balanced over very .ne time scales. Furthermore, the algorithms used 
in Pomegranate are designed to be able to tolerate latency through the use of buffering. At the heart 
of a butter.y network is a k × k switch (k =2 for .gure 6). Every cycle, this switch is able to read 
a single quan­tum of data (a .it) from each of its input channels and write a .it to each of its output 
channels. Internally, the switch must arbitrate its outputs according to the requests of incoming packets, 
which are composed of multiple .its. Channels are virtualized to provide multiple virtual channels per 
physical channel to increase the likeli­hood that an output channel will have a packet that needs it 
on every cycle [3]. Virtual channels are critical to large butter.y networks as they increase bandwidth 
ef.ciency from the range of 25% to over 75%. In order to scale a butter.y network beyond the k inputs 
and out­puts of a single switch, an additional stage of switches is introduced. The .rst stage routes 
based on the most signi.cant digits of the des­tination address, and the second stage routes based on 
the least sig­ni.cant digits. An n-interface network may be constructed using log k n stages of n/k switches. 
As the number of interfaces in­creases the aggregate bandwidth available increases linearly, while the 
cost increases as n logk n. The multiple networks of .gure 5 are actually virtualized ports within a 
single uni.ed network. Two messages from the same source to the same destination, e.g. geometry processor 
0 to ras­terizer 3, are guaranteed to arrive in order, but no other ordering guarantees are made by the 
network. A uni.ed network allows the network to be used ef.ciently for all types of traf.c, rather than 
having some networks left idle while other networks are overloaded with traf.c. In order to support the 
expected workload and network inef.ciencies, each channel runs at 10 GB/sec. Each channel is 32 bits 
wide and operates at 2.5 GHz. Each 160-bit .it in our system is transferred over 5 32-bit clocks, and 
thus the switching logic runs at 500 MHz. We use 4 × 4 switches and 16 virtual channels per physi­cal 
channel, each capable of buffering 16 .its. Ignoring contention, each hop through a switch imposes 8 
.its of latency. Packets are constrained to be an integral number of .its, with a 24 bit header in the 
.rst .it, which imposes a small overhead. 3.2 Geometry The geometry unit consists of a DMA engine, a 
transform and light­ing engine, a clip processor and a distribution processor. Each ge­ometry unit supports 
a single hardware context, although the con­text may be virtualized. The DMA engine is responsible for 
transferring blocks of commands across the host interface and transferring them to the transform and 
lighting engine. In our model the host in­terface bandwidth is 1 GB/sec. This is representative of AGP 
4x, a current graphics interface.  The transform and lighting (T&#38;L) engine is a vertex parallel 
vector processor. It transforms, culls and lights the primitives. Clipping is not performed in the T&#38;L 
engine because it in­troduces a potentially large number of new vertices and cor­responding primitives 
which must be correctly ordered in the primitive stream. Deferring this generally infrequent opera­tion 
to a dedicated clip processor greatly simpli.es the T&#38;L engine. The T&#38;L engine has a maximum 
performance of 20 million transformed and lit vertices per second.  The clip processor performs geometric 
clipping for any prim­itives that intersect a clipping plane. Computation of the clip state for each 
vertex is performed by the T&#38;L engine, so the clip processor s fast path has no computation. After 
geomet­ric clipping, the clip processor subdivides large primitives into multiple smaller primitives 
by specifying the primitives mul­tiple times with different rasterization bounding boxes. This subdivision 
ensures that the work of rasterizing a large tri­angle can be distributed over all rasterizers. Large 
primi­tives are detected by the signed area computation of back-face culling and subdivided according 
to a primitive-aligned 64 × 64 stamp.  The distribution processor distributes the clipped and subdi­vided 
primitives to the rasterizers. This is Pomegranate s .rst sort . Because the rasterizers are primitive 
parallel (object­space parallel) rather than fragment parallel (image-space par­allel), the distribution 
processor has the freedom to distribute primitives as it sees .t.  The distribution processors transmit 
individual vertexes with meshing information over the network to the rasterizers. A vertex with 3D texture 
coordinates is 228 bits plus 60 bits for a description of the primitive it is associated with and its 
rasterization bounding box, resulting in 320 bit (2 .it) vertex packets. At 20 Mvert/sec, each distribution 
processor generates 0.8 GB/sec of network traf­.c. The distribution processor generates additional network 
traf.c in two cases. First, large primitives are subdivided to ensure that they present a balanced load 
to all the rasterizers. In such a case the additional network traf.c is unimportant, as the system will 
be ras­terization limited. Second, commands that modify rasterizer state (e.g. the texture environment) 
must be broadcast to all the rasteriz­ers. The distribution processor governs its distribution of work 
under con.icting goals. It would like to give the maximum number of sequential triangles to a single 
rasterizer to minimize the transmis­sion of mesh vertexes multiple times and to maximize the texture 
cache ef.ciency of the rasterizer s associated texture processor. At the same time it must minimize the 
number of triangles and frag­ments given to each rasterizer to load balance the network and allow the 
reordering algorithm, which relies on buffering proportional to the granularity of distribution decisions, 
to be practical. The dis­tribution processor balances these goals by maintaining a count of the number 
of primitives and an estimate of the number of frag­ments sent to the current rasterizer. When either 
of these counts exceeds a limit, the distribution processor starts sending primitives to a new rasterizer. 
While the choice of the next rasterizer to use could be based on feedback from the rasterizers, a simple 
round­robin mechanism with a triangle limit of 16 and a fragment limit of 4096 has proven effective in 
practice. When triangles are small, and thus each rasterizer gets very few fragments, performance is 
geom­etry limited and the resulting inef.ciencies at the texture cache are unimportant. Similarly, when 
triangles are large, and each rasterizer gets few triangles, or perhaps even only a piece of a very large 
tri­angle, the performance is rasterization limited and the inef.ciency of transmitting each vertex multiple 
times is inconsequential. 3.3 Rasterizer The rasterizer scan converts triangles, as well as points and 
lines, into a stream of fragments with color, depth and texture coordi­nates. The rasterizer emits 2 
× 2 fragment quads at 100 MHz and requires 3 cycles for triangle setup, for a peak .ll rate of 400 Mpixel/sec. 
Partially covered quads can reduce the rasterizer sef.­ciency to 100 Mpixel/sec in the worst case. We 
achieve 1.34 to 3.95 fragments per quad for the scenes in this paper. Each rasterizer re­ceives primitives 
from all the geometry processors and receives ex­ecution order instructions from the sequencer (see section 
4). Each of the geometry units maintains its own context, and thus each ras­terizer maintains n contexts, 
one per geometry processor. The frag­ment quads emitted by the rasterizer are in turn textured by the 
tex­ture processor. 3.4 Texture The texture stage consists of two units, the texture processor which 
textures the stream of quads generated by the rasterizer, and the texture access unit which handles texture 
reads and writes. The input to the rasterization stage has already been load balanced by the distribution 
processors in the geometry stage, so each texture processor will receive a balanced number of fragments 
to texture. In order to provide a scalable texture memory, textures are dis­tributed over all the pipeline 
memories in the system. Igehy et al. have demonstrated a prefetching texture cache architecture that 
can tolerate the high and variable amount of latency that a system with remote texture accesses, such 
as Pomegranate, is likely to in­cur [6]. Igehy et al. subsequently showed that this cache architec­ture 
could be used very effectively under many parallel rasteriza­tion schemes, including an object-space 
parallel rasterizer similar to Pomegranate [5]. Based on these results, we distribute our tex­tures according 
to 4 × 4 texel blocks. Texture cache misses to a non-local memory are routed over the network to the 
texture access unit of the appropriate pipeline. The texture access unit reads the requested data and 
returns it to the texture processor, again over the network. A texture cache miss requires that a 160-bit 
texture request be sent over the network, which will be followed by a 640­bit reply, for a total of 800 
bits of network traf.c per 16 texels, or 6.25 bytes per texel. If we assume 1 2 texels of memory bandwidth 
per fragment, our rasterizer requires 4 8 bytes of texture memory bandwidth and 6.25 12.5 bytes of network 
bandwidth per fragment. At 400 Mpixel/sec, this becomes 1.6 3.2 GB/sec of memory band­width and 2.5 5 
GB/sec of network bandwidth. After texturing the fragments, the texture processor routes the fragment 
quads to the appropriate fragment processors. The frag­ment processors .nely interleave responsibility 
for pixel quads on the screen. Thus, while the texture engine has no choice in where it routes fragment 
quads, the load it presents to the network and all of the fragment processors will be very well balanced. 
A quad packet contains 4 fragment colors, 4 corresponding sample masks, the depth of the lower-left fragment, 
the depth slopes in x and y and the location of the quad on the screen. This representation en­codes 
a quad in 241 bits, or 320 bits (2 .its) on the network. Due to network packet size constraints, this 
is only twice the size of an in­dividually encoded fragment, which is transmitted as 1 .it. At 100 Mquad/sec, 
the texture processor sends 4 GB/sec of traf.c to the fragment processors. Just as the distribution processor 
broadcasts rasterization state changes to the rasterizers, the texture processor must also broadcast 
fragment processor state changes. 3.5 Fragment The fragment stage of the pipeline consists of the fragment 
proces­sor itself and its attached memory system. The fragment processor receives fragment quads from 
the texture processor and performs all the per-fragment operations of the OpenGL pipeline, such as depth­buffering 
and blending. The memory system attached to each frag­ment processor is used to store the subset of the 
framebuffer and the texture data owned by this pipeline. The use of fragment quads, in addition to reducing 
network bandwidth, allows ef.cient access to the memory system by group­ing reads and writes into 16-byte 
transactions. Each pixel quad is organized by pixel component rather than by pixel, so, for example, 
all of the depth components are contiguous and may be accessed in a single transaction. This improves 
Pomegranate sef.ciency in the peak performance case of fully covered fragment quads, and when fragment 
quads are only partially covered Pomegranate is already running beneath peak pixel rates, so the loss 
of memory ef.ciency is not as important. The memory system provides 6.4 GB/sec of memory bandwidth. At 
400 Mpixel/sec and 8 to 12 bytes per pixel (a depth read, depth write, and color write), fragment processing 
utilizes 3.2 to 4.8 GB/sec. When combined with texture accesses of 1.6 to 3.2 GB/sec and display accesses 
of 0.5 GB/sec, the memory system bandwidth is overcommitted. Memory access is given preferentially to 
the dis­play processor, since it must always be serviced, then to the frag­ment processor, because it 
must make forward progress for the tex­ture processor to continue making forward progress, and .nally 
the texture access unit. The majority of our results are not memory access limited. Pomegranate statically 
interleaves the framebuffer at a fragment quad granularity across all of the fragment processors. This 
image­space parallel approach has the advantage of providing a near per­fect load balance for most inputs. 
As with the rasterizers, the frag­ment processors maintain the state of n hardware contexts. While the 
rasterizers will see work for a single context from any particu­lar geometry unit, the fragment processor 
will see work for a single context from all the texture processors because the geometry stage s distribution 
processor distributes work for a single context over all the rasterizers. 3.6 Display The display processor 
is responsible for retrieving pixels from the distributed framebuffer memory and outputting them to a 
display. Each pipeline s display processor is capable of driving a single dis­play. The display processor 
sends pipelined requests for pixel data to all of the fragment processors, which in turn send back strips 
of non-adjacent pixels. The display processor reassembles these into horizontal strips for display. Unlike 
the use of the network every­where else in Pomegranate, the display system is very sensitive to latency 
 if pixels arrive late, gaps will appear in the displayed im­age. We address this issue with a combination 
of buffering, which enables the display processor to read ahead several scanlines, and a priority channel 
in the network. Dally has shown that a bounded percentage of the traf.c on a network can be made high 
priority and delivered with guaranteed latency [3]. At a display resolution of 1920 × 1280 and a 72 Hz 
refresh rate, the display bandwidth is 0.5 GB/sec, 5% of Pomegranate s per-pipeline bandwidth.  4 Ordering 
Ordered execution of the application command stream must be maintained everywhere its effects are visible 
to the user. The work distribution algorithms described in the previous section explain Figure 8: Rasterizer 
0 and rasterizer 1 simultaneously process the primitives distributed to them by geometry processor 0. 
After rasterizing primitive 1, rasterizer 0 broadcast a NextR to all the fragment processors, announcing 
that they should now process fragments from rasterizer 1. The texture proces­sors have been omitted for 
clarity. how the Pomegranate architecture scales performance, but the con­straint of ordering was ignored. 
By far the most prevalent place this constraint is exposed is the OpenGL API itself, which is state­ful. 
For example, a glBlendFunccommand modi.es the blend­ing state for all subsequent primitives and no previous 
primitives. Second, many commands (i.e. points, lines, triangles) modify the contents of the framebuffer, 
and these modi.cations must occur in order at each pixel. Finally, changes to the texture memory must 
be ordered. Pomegranate faces two distinct ordering issues. First, the com­mands for a single context 
are distributed over all the rasterizers, which in turn distribute their fragments over all the fragment 
pro­cessors. This double sort means that the original order of the com­mand stream must be communicated 
to the fragment processors to allow them to merge the fragments in the correct order. Second, the operations 
of different contexts must be interleaved in a manner that observes constraints speci.ed by the parallel 
API. 4.1 Serial Ordering The key observation to implementing ordering within a single con­text is that 
every place work is distributed, the ordering of that work must be distributed as well. The .rst distribution 
of work is per­formed by the distribution processor, which distributes blocks of primitives over the 
rasterizers. Every time it stops sending primi­tives to the current rasterizer and starts sending primitives 
to a new rasterizer it emits a NextR command to the current rasterizer, an­nouncing where it will send 
subsequent primitives. Figure 7 shows the operation of this mechanism. These NextRcommands provide a 
linked list of the primitive order across the rasterizers. The ras­terizers in turn broadcast the NextRcommands 
to all the fragment processors. Each rasterizer has dedicated command buffering for each geometry unit, 
so that the commands from different geometry units may be distinguished. The fragment processors each 
have dedicated buffering for re­ceiving commands from each of the rasterizers, as illustrated in .g­ure 
8. Each fragment processor processes commands from a single rasterizer at a time. When a fragment processor 
receives a NextR command, it ceases listening to the current rasterizer and starts lis­tening to the 
speci.ed next rasterizer. This is analogous to follow­ing the linked list of NextRcommands emitted by 
the distribution processor. While a fragment processor will only ever process com­mands from a single 
rasterizer at any point in time, all of the raster­izers can continue to make forward progress and transmit 
fragments to the fragment processors where they will be buffered. The Pomegranate architecture is designed 
with the expectation that the same parts which construct the base units are repeated to create larger, 
more powerful systems. As part of this assumption, the amount of buffering at the input of each fragment 
processor is .xed. However, this buffering is always divided evenly among all the rasterizers, so as 
the number of pipelines increases the buffering available per rasterizer at each fragment processor shrinks. 
How­ever, the increase in the number of pipelines matches this decrease, and the total amount of buffering 
per rasterizer across all fragment processors remains constant. The amount of traf.c generated by NextR 
commands from a geometry unit to a rasterizer is limited. When the scene is triangle limited, one single-.it 
NextRpacket is sent to a rasterizer for ev­ery 16 two-.it vertex packets sent to a rasterizer. This represents 
an overhead of approximately 3%, which remains constant as the sys­tem scales. The NextR messages from 
the rasterizers to the frag­ment processors, on the other hand, represent a potential broadcast in the 
system because each rasterizer must broadcast each NextRit receives to all the fragment processors. Fortunately, 
this broadcast may be avoided by employing a lazy algorithm. Because NextR commands take only a few bits 
to encode, we can include space for a potential NextR command in every fragment quad without in­creasing 
its size in network .its. Because the fragment processors have very .nely interleaved responsibility 
for quads on the screen, chances are that a fragment quad will be sent to the fragment pro­cessor shortly 
after the NextRcommand is observed by the raster­izer. A timeout ensures that a NextR command that is 
waiting to piggyback on a fragment quad is not excessively delayed, prompt­ing the rasterizer to send 
as many outstanding NextR commands as possible in a single network packet. In general, the fragment processors 
operate independently, each processing fragments at its own rate. The exception is when a command observes 
or modi.es shared state beyond that on a single fragment processor, the fragment processors must be syn­chronized. 
Pomegranate uses an internal fragment barrier com­mand, BarrierF, to support this synchronization. For 
example, glFinishhas an implementation similar to this pseudocode: glFinish( ) { BarrierF hardware 
writeback to device driver } The BarrierF ensures that all previous operations by this con­text are 
complete before the writeback signaling completion of the glFinishoccurs. A similar issue arises at the 
rasterizers. If a command modi.es the current texture state, which is shared among the multiple raster­izers, 
it must be executed in the correct serial order with respect to the other commands from that context. 
Pomegranate enforces this constraint with an internal BarrierR command which forces all of the rasterizers 
to synchronize. A texture modi.cation command can be bracketed between BarrierRcommands and thus be made 
atomic within the hardware. For example, glTexImage2Dhas an implementation similar to this pseudocode: 
glTexImage2D( ) { BarrierR texture download BarrierR } The initial BarrierRensures that all previous 
commands for this context are complete on all rasterizers before the texture download starts so that 
the new texture does not appear on any previous prim­itives. The .nal BarrierRensures no subsequent commands 
for this context are executed on any rasterizer before the texture down­load completes so that the old 
texture does not appear on any sub­sequent primitives. 4.2 Parallel Ordering The internal hardware commands 
NextR, BarrierR and BarrierFsuf.ce to support serial ordering semantics. The exten­sion of the hardware 
interface to a parallel API requires additional support. The parallel API requires that some or all of 
the graph­ics resources must be virtualized, and more importantly, subject to preemption and context 
switching. Imagine an application of n +1 graphics contexts running on a system that supports only n 
simul­taneous contexts. If a graphics barrier is executed by these n +1 contexts, at least one of the 
n running contexts must be swapped out to allow the n +1th context to run. Furthermore, the parallel 
API introduces the possibility of deadlock. Imagine an incorrectly written graphics application that 
executes a glSemaphorePthat never receives a corresponding glSemaphoreV. At the very least, the system 
should be able to preempt the deadlocked graphics con­text and reclaim those resources. Resolving the 
preemption prob­lem was one of the most dif.cult challenges of the Pomegranate architecture. One solution 
to the preemption problem is the ability to read back all of the state of a hardware context and then 
restart the con­text at a later time. Although this may seem straightforward, it is a daunting task. 
Because a context may block at any time, the pre­empted state of the hardware is complicated by partially 
processed commands and large partially-.lled FIFOs. As a point of compari­son, microprocessor preemption, 
which has a much more coherent architecture compared to a graphics system, is generally viewed by computer 
architects as a great complication in high-performance microprocessors. A second approach to the preemption 
problem is to resolve the API commands in software, using the preemption resources of the microprocessor. 
With this approach, even though ordering con­straints may be speci.ed to the hardware, every piece of 
work spec­i.ed has been guaranteed by the software to eventually execute. Figure 9 illustrates this approach. 
Each graphics context has an associated submit thread that is responsible for resolving the paral­lel 
API primitives. The application thread communicates with the submit thread via a FIFO, passing pointers 
to blocks of OpenGL commands and directly passing synchronization primitives. If the submit thread sees 
a pointer to a block of OpenGL commands, it passes this directly to the hardware. If the submit thread 
sees a parallel API command, it actually executes the command, possi­bly blocking until the synchronization 
is resolved. This allows the application thread to continue submitting OpenGL commands to the FIFO beyond 
a blocked parallel API command. In addition to executing the parallel API command, the submit thread 
passes the hardware a sequencing command that maintains the order resolved by the execution of the parallel 
API command. The important part of this hardware sequencing command is that even though an order­ing 
is speci.ed, the commands are guaranteed to be able to drain: the hardware sequencing command for a glSemaphorePwill 
not be submitted until the hardware sequencing command for the corre­sponding glSemaphoreV is submitted. 
Thus, a blocked context is blocked entirely in software, and software context switching and resource 
reclamation may occur. In order to keep hardware from constraining the total number of barriers and semaphores 
available to a programmer, the inter-Figure 9: Each graphics context has an associated submit thread 
which is responsible for resolving the parallel API primitives. In this .gure submit thread 0 is blocked 
waiting to resolve a semaphore P that will be released by context 1. Both application threads are continuing 
to submit work, and the hardware is continuing to consume work. nal hardware sequencing mechanism is 
based on a single sequence number per hardware context. Upon executing a glSemaphoreV operation, the 
submit thread increments the hardware context s sequence number by one to indicate a new ordering boundary, 
annotates the semaphore with a (ctx, seq) pair and issues an AdvanceContext (ctx, seq) command to the 
hardware. Upon completing the glSemaphoreP operation, the signaled submit thread removes the corresponding 
(ctx , seq) annotation from the semaphore and issues a AwaitContext (ctx, seq) command to the hardware. 
A similar mechanism is used to implement barriers.1 The sequence numbers are associated with a particular 
hardware context, not with a virtual graphics context, and when a context switch occurs, it is not reset. 
This allows us to express dependen­cies for contexts that have been switched out of the hardware, and 
thus execute an n +1context barrier on n context hardware. Given the AdvanceContext/ AwaitContext commands 
for expressing ordering constraints among contexts, Pomegranate now needs a way of acting on these constraints. 
The sequencer unit provides a central point for resolving these ordering constraints and scheduling the 
hardware. The distribution processors at the end of the geometry stage, each of which is dedicated to 
a single hard­ware context, inform the sequencer when they have work available to be run and what ordering 
constraints apply to that work. The sequencer then chooses a particular order in which to process work 
from the various contexts and broadcasts this sequence to all of the rasterizers, which, along with all 
the subsequent stages of the pipeline, are shared among all the contexts. Whenever a distribution processor 
starts emitting primitives, it sends a Start command to the sequencer to indicate that it has work available 
to be scheduled. In addition, the distribution proces­sor transmits all AdvanceContextand AwaitContextcom­mands 
for its context to the sequencer, which in turn enforces the ordering relationships expressed by these 
commands when making its scheduling decisions. The counterpart of the Startcommand is the Yieldcommand 
which the distribution processors broadcast to all the rasterizers at the end of a block of work. When 
a raster­izer encounters a Yield it reads the next execute command from the sequencer and starts executing 
that context. The Yield com­ 1The .rst n - 1 submit threads to arrive at the barrier execute an AdvanceContextto 
create a sequence point and block. The last context to arrive at the barrier executes an AwaitContexton 
the previous n - 1 contexts, an AdvanceContextto create its own sequence point and then unblocks the 
waiting contexts. The n-1 waiting contexts then each execute an AwaitContexton the nth context s just 
created sequence point, for a total of n AdvanceContexts and n AwaitContexts.   scene March Nurbs Tex3D 
input 4003 volume 1632 patches, 8 passes 2563 volume output 2.5K × 2K 2.5K × 2K 1K × 1K triangles 1.53M 
6.68M 512 fragments 10.4M 5.81M 92.5M Table 1: Benchmark scenes. mand provides context switching points 
to support ordering and to allow the pipelines to be shared. First, if a context waits on an­other context, 
it must then yield to allow the rasterizers to work on other contexts, which will eventually allow this 
context to run again. Second, a context must occasionally yield voluntarily, to al­low the hardware to 
be shared among all the contexts so that a single context does not unfairly monopolize the machine. The 
frequency of these yields is determined by the relationship of the triangle rate of the geometry units 
to the command buffering provided at each rasterizer. In our implementation, a context yields once it 
has sent one NextRcommand to each rasterizer. Because the sequencer is decoupled from the rasterizers, 
it will make scheduling decisions as far in advance as it can, limited only by the available informa­tion 
from the distribution processors and the available buffering for execution commands at the rasterizers. 
 5 Results We have implemented an OpenGL software device driver and hard­ware simulator to verify our 
architecture. Our system supports all of the major functionality of OpenGL. The Pomegranate hardware 
is modeled under an event-driven simulator. The simulator is composed of multiple independent threads 
of execution which communicate with each other via events. Threads advance events and await on events 
to coordinate their execution. The simulator provides a shared global knowledge of time, so threads may 
both wait for other threads to complete a task, as well as simply wait for time to pass, to model clock 
cycles, etc. The simulator is non-preemptive and a particular thread of ex­ecution only ceases execution 
when it explicitly waits for time to pass or waits on a semaphore. Our simulator masquerades as the system 
OpenGL dynamic li­brary on Microsoft Windows NT and SGI IRIX operating systems. Application parallelism 
is supported through additional functions exported by our OpenGL library that allow the creation of user 
threads within our simulator. This simulation methodology allows us to deterministically simulate Pomegranate, 
which aids both de­bugging and analysis of performance. In particular, performance problems can be iteratively 
analyzed by enabling more and more instrumentation in different areas of the hardware, with the con.­dence 
that subsequent runs will behave identically. We analyzed Pomegranate s performance with three applica­tions, 
shown in table 1. The .rst, March, is a parallel implementa­tion of marching cubes [8]. The second, Nurbs, 
is a parallel patch evaluator and renderer. The .nal, Tex3D, is a 3D texture volume renderer. March extracts 
and draws an isosurface from a volume data set. The volume is subdivided into 123 voxel subcubes that 
are processed in parallel by multiple application threads. Each subcube is drawn in back to front order, 
allowing the use of transparency to reveal the internal structure of the volume. The parallel API is 
used to order the subcubes generated by Table 2: Total FIFO sizes for each of functional unit. The entries 
bytes/entry bytes Primitive 4096 120 480K Texture 256 72 18K Fragment 16384 32 512K FIFO size is listed 
as the total number of commands it can contain. each thread in back to front order. Note that while March 
requires a back to front ordering, there are no constraints be­tween cubes which do not occlude each 
other, so substantial inter-context parallelism remains for the hardware. Nurbs uses multiple application 
threads to subdivide a set of patches and submit them to the hardware. We have arti.cially chosen to 
make Nurbs a totally ordered application in order to stress the parallel API. Such a total order could 
be used to support transparency. Each patch is preceded by a semaphore P and followed by a semaphore 
V to totally order it within the work submitted by all the threads. Multiple passes over the data simulate 
a multipass rendering algorithm.  Tex3D is a 3D texture volume renderer. Tex3D draws a set of back to 
front slices through the volume along the viewing axis. Tex3D represents a serial application with very 
high .ll rate demands and low geometry demands, and it is an example of a serial application that can 
successfully drive the hardware at a high degree of parallelism.  We measure Pomegranate s performance 
on these scenes in four ways. First we examine Pomegranate s raw scalability, the speedup we achieved 
as a function of the number of pipelines. Next we examine the load imbalance across the functional units, 
which will determine the best achievable speedup for our parallel system. Then we quantify the network 
bandwidth demanded by the different stages of the pipeline and analyze the lost performance due to net­work 
imbalance. Finally we compare Pomegranate s performance to simulations of sort-.rst, sort-middle and 
sort-last architectures. All of these simulations are based on the parameters outlined in our description 
of the architecture, and the FIFO sizes listed in ta­ble 2. The primitive FIFO is the FIFO at the input 
to the rasterizer, and determines how many primitives a geometry unit can buffer before stalling. The 
texture FIFO is the FIFO that receives texture memory requests and replies and determines how many outstand­ing 
texture memory requests the texture system can have. The .nal major FIFO is the fragment FIFO, which 
is where the fragment pro­cessors receive their commands from the texture processors. The n pipeline 
architecture uses the same FIFOs as the 1 pipeline archi­tecture, but divides them into n pieces. The 
FIFO sizes have been empirically determined. 5.1 Scalability Our .rst measure of parallel performance 
is speedup, presented for our scenes in .gure 10. Nurbs exhibits excellent scalability, de­spite presenting 
a totally ordered set of commands to the hardware. At 64 processors the hardware is operating at 99% 
ef.ciency, with a triangle rate of 1.10 Gtri/sec and a .ll rate of 0.96 Gpixel/sec. The only application 
tuning necessary to achieve this level of per­formance is picking an appropriate granularity of synchronization. 
Because Nurbs submits all of its primitives in a total order, the se­quencer has no available parallel 
work to schedule, and is always completely constrained by the API. This results in only 1 geometry unit 
being schedulable at any point in time, and the other geometry units will only make forward progress 
as long as there is adequate buffering at the rasterizers and fragment processors to receive their commands. 
This requirement is somewhat counterintuitive, as the 64 1102 Mtri/s 0.96 Gpix/s 557 Mtri/s 3.79 Gpix/s 
0.12 Mtri/s 21.8 Gpix/s 32  1 4 8 16 14 8 16 32 pipelines 64 Ideal Nurbs March Tex3D speedup Figure 
10: Pomegranate speedup vs. number of pipelines. usual parallel programming rule is to use the largest 
possible gran­ularity of work. March runs at a peak of 557 Mtri/sec and 3.79 Gpixel/sec in a 64-pipeline 
architecture, a 58× speedup over a single pipeline ar­chitecture. While this scalability is excellent, 
it is substantially less than that of Nurbs. If we examine the granularity of synchroniza­tion, the problem 
becomes apparent. Nurbs executes a semaphore pair for every patch of the model, which corresponds to 
every 512 triangles. March, on the other hand, executes 3 semaphore pairs for every 123 voxel subcube 
of the volume, and the average subcube only contains 38.8 triangles. Thus, the number of synchronization 
primitives executed per triangle is more than an order of magnitude greater than that of Nurbs. Furthermore, 
there is high variance in the number of triangles submitted between semaphores. These ef­fects cause 
March to encounter scalability limitations much sooner than Nurbs despite its much weaker ordering constraints. 
Tex3D runs at 21.8 Gpixel/sec on a 64-pipeline Pomegranate, with a tiny 0.12 Mtri/sec triangle rate, 
a 56× speedup over a single pipeline architecture. Tex3D scales very well, considering that it is a serial 
application. If Tex3D s input primitives were skewed towards smaller triangles it would rapidly become 
limited by the geometry rate of a single interface and execution time would cease improving as we add 
pipelines. 5.2 Load Balance In order to achieve a high parallel ef.ciency, the work performed by the 
hardware must be balanced across the functional units and com­munication must be balanced across the 
network. Table 3 presents the load imbalance for Nurbs on our architecture with 4, 16, and 64 pipelines. 
The load balance is within a few percent for all the functional units. This indicates that Pomegranate 
s methodology for distributing work is providing us with an excellent load balance. By the time Nurbs 
reaches 64 pipelines the network is signi.cantly out of balance. This is an artifact of Nurbs s relatively 
low net­work usage, as it is geometry limited, and the asymmetry of the network traf.c generated by the 
sequence processor, as discussed in section 5.3. The results for March are not shown, but they are qualitatively 
similar. Table 4 shows the load imbalance for Tex3D. Despite all of the application commands arriving 
through a single interface, the sub­sequent rasterization and fragment stages still receive an extremely 
balanced load. The texture load imbalance is the ratio of the most texture requests handled by a pipeline 
to the average. Numbers close to 1 indicate that the shared texture memory is working effec­tively, because 
all of the texture requests are well distributed over the pipelines. Tex3D s network imbalance is becoming 
signi.cant Table 3: Load balance for Nurbs. Each entry in the table presents the minimum/maximum work 
done by any functional unit as a fraction of the average work per functional unit. Ge­ometry work is 
measured in triangles; rasterization and com­position work is measured in fragment quads. The network 
imbalance is measured in bytes of traf.c per pipeline. pipelines 4 16 64 Geometry 1.00/1.00 1.00/1.00 
1.00/1.00 Rasterizer 1.00/1.00 1.00/1.00 0.98/1.02 Fragment 1.00/1.00 0.99/1.01 0.99/1.01 Network 0.98/1.04 
0.97/1.27 0.95/2.63 pipelines 4 16 64 Geometry 0.00/4.00 0.00/16.0 0.00/64.0 Rasterization 1.03/1.00 
1.00/1.00 1.00/1.00 Texture 1.00/1.00 1.00/1.00 0.99/1.00 Fragment 1.00/1.00 1.00/1.00 1.00/1.01 Network 
1.00/1.01 1.00/1.04 0.99/1.15 Table 4: Load balance for Tex3D. Each entry in the table presents the 
minimum/maximum work done by any functional unit , as a fraction of the average work per functional unit. 
Ge­ometry work is measured in triangles; rasterization and com­position work is measured in fragment 
quads. by the time we reach 64 pipelines. This large asymmetry is the re­sult of all of the primitives 
entering through a single interface and being distributed from a single geometry unit. As Pomegranate 
is scaled, the total rasterization speed increases, but the entire geome­try traf.c is borne by a single 
pipeline. 5.3 Network Utilization There are .ve main types of network traf.c in Pomegranate: ge­ometry, 
sequencer, texture, fragment and display. Geometry traf.c is comprised of vertexes transmitted from the 
geometry processor to the rasterizers and the NextR ordering commands, as well as any state commands, 
textures, etc. for the subsequent stages. Se­quencer traf.c is the communication between the distribution 
pro­cessors and the sequencer as well as the sequencer and the raster­izers, and encapsulates all the 
traf.c which allows the hardware to be shared among multiple contexts and the parallel API com­mands. 
Texture traf.c is made up of the texture request and texture reply traf.c generated by each texture processor. 
Fragment traf.c is composed of the quads emitted by the texture processors and sent to the fragment processors. 
Display traf.c is the pixel read requests and replies between the display processors and the fragment 
pro­cessors. The network bandwidth for each traf.c type across our scenes on a 64-pipeline Pomegranate 
is presented in table 5. The sequencer numbers are extremely skewed because there is a single sequencer 
in the system, so all sequencing information from the distribution processors .ows into a single point, 
and all sequencing decisions for the rasterizers .ow back out of that point, which intro­duces a broadcast 
into the system. A future version of Pomegranate will use a low bandwidth broadcast ring connecting all 
the pipelines speci.cally for the distribution of the sequencing information. 5.4 Comparison We compare 
Pomegranate s performance to 4 other parallel graph­ics architectures: Sort-First introduces a communication 
stage between the DMA units and transform &#38; lighting in the geometry processor. The screen is statically 
partitioned in 32 × 32 tiles among the pipelines. The screen-space bounding boxes of blocks of 16 vertexes 
are used to route primitives to pipelines. March Nurbs Tex3D Geometry 0.84/0.85 0.54/0.58 0.01/0.93 Sequence 
0.02/1.06 0.05/2.95 0.00/0.14 Texture 0/ 0 0/ 0 3.00/3.01 Fragment 1.82/1.84 1.19/1.20 3.31/3.32 Total 
2.68/3.71 1.78/4.67 6.33/7.26 Table 5: The network traf.c by type for each of our scenes on a 64-pipeline 
Pomegranate. Each row corresponds to a particular type of traf.c and each pair of numbers is the av­erage/maximum 
amount of traf.c per pipeline of that type in gigabytes per second. These simulations do not include 
a dis­play processor. Sort-Middle Tiled is a sort-middle architecture with the screen statically partitioned 
in 32 × 32 tiles among the rasterizers. Individual primitives are only transmitted to the rasterizers 
whose tiles they overlap. Sort-Middle Interleaved partitions the screen in 2 × 2 tiles to en­ sure rasterization 
and fragment load balancing. Each geome­ try processor broadcasts its primitives to all rasterizers. 
Sort-Last Fragment partitions the screen in 2 × 2 tiles among the fragment processors. Each rasterizer 
is responsible for all the primitives transformed by its corresponding geometry proces­sor. All of these 
architectures are built on top of the Pomegranate simulator, and only differ in how the network is deployed 
to inter­connect the various components. We provide each of these archi­tectures, although not Pomegranate, 
with an ideal network zero latency and in.nite bandwidth to illustrate fundamental differ­ences in 
the work distribution. All of these architectures have been built to support the parallel API and a shared 
texture memory. The ordering mechanisms necessary to support the parallel API are bor­rowed from Pomegranate, 
although they are deployed in different places in the pipeline. Our simulator requires substantial time 
to run over 24 hours for some the 64 pipeline simulations. In order to provide these re­sults across 
all these architectures we were forced to reduce the size of the benchmarks for the remaining simulations. 
Point simulations of the full data sets give us con.dence that the results presented here are quantitatively 
very similar to the results for the full scenes used in the previous sections. Figure 11a shows the performance 
of all of these architectures for the March data set. As March runs, all of the primitives are clustered 
along the isosurface, which results in high screen-space temporal locality. Sort-.rst, which uses coarse-grained 
screen­space parallelism for both geometry and rasterization, is most severely impacted because temporal 
locality causes spatial load imbalances over short periods of time, the length of which are de­termined 
by the amount of FIFOing available. Sort-middle tiled employs object-space parallelism for the geometry 
stage, and be­cause this scene is not rasterization limited, exhibits substantially more scalability 
than sort-.rst, although its limitations are exposed at higher levels of parallelism. Sort-middle interleaved 
behaves much more poorly than sort-middle tiled because it broadcasts tri­angle work to every rasterizer, 
and each rasterizer can process a limited number of triangles per second. Sort-last and Pomegranate both 
scale very well because they rasterize each triangle only once (eliminating redundant work) and use object-space 
parallelism for rasterization (eliminating any issues with temporal locality). The main difference between 
Pomegranate and sort-last, the balancing of fragment work across rasterizers by the geometry processors, 
does not matter here because the triangles are relatively uniformly sized. Nurbs, shown in .gure 11b, 
exhibits much worse scalability for sort-.rst and sort-middle than March, and in fact even slows down 
at high degrees of parallelism. The granularity of work for Nurbs is a patch, which exhibits a great 
degree of temporal locality in screen-space, even greater than March, which explains the per­formance 
at low degrees of parallelism. However, unlike March, Nurbs is a totally ordered application, and when 
combined with architectures that use screen-space parallelism for geometry or ras­terization, the result 
is hardware that performs almost no better than the serial case. As the number of pipelines increases, 
the system is capable of processing more work. However, the amount of FIFO­ing available from each pipeline 
to each tile decreases, reducing the window over which temporal load imbalance may be absorbed. The hump 
in performance at moderate numbers of pipelines is a result of these effects. As with March, sort-last 
and Pomegranate exhibit excellent scalability. Unlike March and Nurbs, Tex3D, shown in .gure 11c, is 
a com­pletely rasterization limited application. The speedup for sort-.rst and sort-middle tiled here 
is limited purely by the rasterization load balance of the entire frame, illustrating that even scenes 
which ap­pear very well balanced in screen-space may suffer large load im­balances due to tiling patterns 
at high degrees of parallelism. Sort­middle interleaved, which was previously limited by its reliance 
on broadcast communication, is now limited by texture cache per­formance, which is severely compromised 
by the use of extremely .ne-grained rasterization. Each triangle is so large in this applica­tion that 
it serializes sort-last at the fragment processor stage: the fragment FIFOs provide elasticity for the 
rasterizers to continue or­dered rasterization on subsequent triangles while the current trian­gle is 
merged with the framebuffer, but when a single large triangle .lls up the entire FIFO this elasticity 
is lost and the rasterizers are serialized. If we greatly increase the buffering at the fragment pro­cessors, 
shown by the sort-last big curve, so that sort-last is no longer serialized by the large primitives, 
the fundamental problem with sort-last is exposed: imbalances in triangle size cause load im­balances 
across the rasterizers. In Tex3D at 64 pipelines, the worst rasterizer has almost twice the work of an 
average rasterizer. Many applications (e.g. architectural walkthroughs) have a few very large polygons 
and exhibit much more severe imbalance in rasterization work than the relatively innocuous Tex3D. Pomegranate 
addresses this fundamental problem by load balancing both the number of triangles and the number of fragments 
across the rasterizers, and exhibits excellent scalability on Tex3D.  6 Discussion Pomegranate was 
designed to support an immediate-mode parallel graphics interface and uses high-speed point-to-point 
communica­tion to load balance work across its pipelines. Our results have demonstrated the quantitative 
impact of these choices, and we will now revisit their qualitative bene.ts and costs. 6.1 OpenGL and 
the Parallel API The decision to support OpenGL, a strict serial API, has proven somewhat complicated 
to implement, but has not resulted in a per­formance impact. In fact, Nurbs, which totally orders the 
submis­sion of its work across all contexts, achieves almost perfectly linear speedup, despite its very 
strong ordering constraints. The expense of supporting ordering is FIFOs which allow the various pipeline 
stages to execute commands in application order. While it may be necessary for the application programmer 
to choose an appropriate granularity of parallelism, particularly in a strongly ordered scene, it is 
not required that the application balance fragment work, only primitive work. This is a desirable feature, 
as in general application programmers have little knowledge of the amount of fragment work that will 
be generated by a primitive, but they are well aware of the number of primitives being submitted. 1000 
 Pomegranate 800 15 Sort-First Sort-Middle Tiled 600 10  Sort-Middle Interleaved   Mtri/s 400 Sort-Last 
 5 Sort-Last Big 200 0 0 0 pipelines pipelines pipelines (a) March (b) Nurbs (c) Tex3D Figure 11: 
Performance of each architecture on each scene. 6.2 Communication References Pomegranate uses a network 
to interconnect all the pipeline stages. This approach to interconnecting a graphics system has become 
much more practical recently due to the advent of high-speed point­to-point signaling, which reduces 
the cost of providing the multiple high bandwidth links necessary in such a network. Nonetheless, we 
expect the cost of the network to dominate the cost of implementing an architecture like Pomegranate. 
Pomegranate only achieves high scalability when it is able to use the network as a point-to-point communication 
mechanism. Ev­ery time broadcast communication is performed, scalability will be lost. However, some 
commands must be broadcast. Commands that modify the state of a particular context (e.g. glBlendFunc) 
must be broadcast to all of the units using that state. The command distribution could potentially be 
implemented lazily, but is still fun­damentally a broadcast communication, which will impose a scala­bility 
limit. Most high performance applications already try to min­imize the frequency of state changes to 
maximize performance. It remains to be seen how the potentially greater cost of state changes in Pomegranate 
would impact its scalability.  7 Conclusions We have introduced Pomegranate, a new fully scalable graphics 
architecture. Simulated results demonstrate performance of up to 1.10 billion triangles per second and 
21.8 billion pixels per second in a 64-way parallel system. Pomegranate uses a high-speed point-to-point 
network to inter­connect its pipeline stages, allowing each pipeline stage to provide a temporally balanced 
work load to each subsequent stage, without requiring broadcast communication. A novel ordering mechanism 
preserves the serial ordered semantics of the API while allowing full parallel execution of the command 
stream. Hardware support for a parallel host interface allows Pomegranate to scale to previ­ously unreached 
levels of performance for immediate-mode appli­cations. 8 Acknowledgments We would like to acknowledge 
the numerous helpful comments the reviewers made that have improved this work. We would like to thank 
Greg Humphreys, John Owens and the rest of the Stanford Graphics Lab for their reviews of this paper 
and their insights. This work was supported by the Fannie and John Hertz Foundation, Intel and DARPA 
contract DABT63-95-C-0085-P00006. [1] Freedom 3000 Technical Overview. Technical report, Evans &#38; 
Sutherland Computer Corporation, October 1992. [2] Denali Technical Overview. Technical report, Kubota 
Paci.c Computer Inc., March 1993. [3] William J. Dally. Virtual Channel Flow Control. IEEE Trans­actions 
on Parallel and Distributed Systems, pages 194 205, March 1992. [4] Jos´e Duato, Sudhakar Yalmanchili, 
and Lionel Ni. Intercon­ nection Networks: an Engineering Approach. IEEE Com­ puter Society Press, 1997. 
[5] Homan Igehy, Matthew Eldridge, and Pat Hanrahan. Parallel Texture Caching. 1999 SIGGRAPH / Eurographics 
Workshop on Graphics Hardware, pages 95 106, August 1999. [6] Homan Igehy, Matthew Eldridge, and Kekoa 
Proudfoot. Prefetching in a Texture Cache Architecture. 1998 SIG-GRAPH / Eurographics Workshop on Graphics 
Hardware, pages 133 142, August 1998. [7] Homan Igehy, Gordon Stoll, and Pat Hanrahan. The Design of 
a Parallel Graphics Interface. SIGGRAPH 98 Conference Proceedings, pages 141 150, July 1998. [8] William 
E. Lorensen and Harvey E. Cline. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. 
Com­puter Graphics (SIGGRAPH 87 Conference Proceedings), pages 163 169, July 1987. [9] Steven Molnar, 
Michael Cox, David Ellsworth, and Henry Fuchs. A Sorting Classi.cation of Parallel Rendering. IEEE Computer 
Graphics and Applications, pages 23 32, July 1994. [10] Steven Molnar, John Eyles, and John Poulton. 
PixelFlow: High-Speed Rendering Using Image Composition. Computer Graphics (SIGGRAPH 92 Conference Proceedings), 
pages 231 240, July 1992. [11] John S. Montrym, Daniel R. Baum, David L. Dignam, and Christopher J. Migdal. 
In.niteReality: A Real-Time Graphics System. SIGGRAPH 97 Conference Proceedings, pages 293 302, August 
1997. [12] Rudrajit Samanta, Jiannan Zheng, Thomas Funkhouser, Kai Li, and Jaswinder Pal Singh. Load 
Balancing for Multi-Projector Rendering Systems. 1999 SIGGRAPH / Eurograph­ics Workshop on Graphics Hardware, 
pages 107 116, August 1999.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344984</article_id>
		<sort_key>455</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>51</seq_no>
		<title><![CDATA[Illuminating micro geometry based on precomputed visibility]]></title>
		<page_from>455</page_from>
		<page_to>464</page_to>
		<doi_number>10.1145/344779.344984</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344984</url>
		<abstract>
			<par><![CDATA[<p>Many researchers have been arguing that geometry, bump maps, and BRDFs present a hierarchy of detail that should be exploited for efficient rendering purposes. In practice however, this is often not possible due to inconsistencies in the illumination for these different levels of detail. For example, while bump map rendering often only considers direct illumination and no shadows, geometry-based rendering and BRDFs will mostly also respect shadowing effects, and in many cases even indirect illumination caused by scattered light.</p><p>In this paper, we present an approach for overcoming these inconsistencies. We introduce an inexpensive method for consistently illuminating height fields and bump maps, as well as simulating BRDFs based on precomputed visibility information. With this information we can achieve a consistent illumination across the levels of detail.</p><p>The method we propose offers significant performance benefits over existing algorithms for computing the light scattering in height fields and for computing a sampled BRDF representation using a virtual gonioreflectometer. The performance can be further improved by utilizing graphics hardware, which then also allows for interactive display.</p><p>Finally, our method also approximates the changes in illumination when the height field, bump map, or BRDF is applied to a surface with a different curvature.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Monte Carlo techniques]]></kw>
			<kw><![CDATA[frame buffer tricks]]></kw>
			<kw><![CDATA[graphics hardware]]></kw>
			<kw><![CDATA[illuminatiion effects]]></kw>
			<kw><![CDATA[reflectance &amp; shading models]]></kw>
			<kw><![CDATA[texture mapping]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Graphics processors</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Bitmap and framebuffer operations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010243</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Appearance and texture representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010389</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics processors</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14221002</person_id>
				<author_profile_id><![CDATA[81100644737]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wolfgang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Heidrich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPI for Computer Science, Im Stadtwald, 66123 Saarbr&#252;cken, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P158063</person_id>
				<author_profile_id><![CDATA[81100197429]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Katja]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Daubert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPI for Computer Science, Im Stadtwald, 66123 Saarbr&#252;cken, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40022735</person_id>
				<author_profile_id><![CDATA[81100016395]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kautz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPI for Computer Science, Im Stadtwald, 66123 Saarbr&#252;cken, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15028898</person_id>
				<author_profile_id><![CDATA[81100315426]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hans-Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seidel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPI for Computer Science, Im Stadtwald, 66123 Saarbr&#252;cken, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>166141</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[B. Becker and N. Max. Smooth transitions between bump rendering algorithms. In Computer Graphics (SIGGRAPH '93 Proceedings), pages 183-190, August 1993.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>507101</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[J. Blinn. Simulation of wrinkled surfaces. In Computer Graphics (SIGGRAPH '78 Proceedings), pages 286-292, August 1978.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37434</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[B. Cabral, N. Max, and R. Springmeyer. Bidirectional reflection fimctions from surface bump maps. In Computer Graphics (SIGGRAPH '87 Proceedings), pages 273-281, July 1987.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808590</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[R. Cook, T. Porter, and L. Carpenter. Distributed ray tracing. In Computer Graphics (SIGGRAPH '84 Proceedings), pages 137-45, July 1984.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>300778</ref_obj_id>
				<ref_obj_pid>300776</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[K. Dana, B. van Ginneken, S. Nayar, and J. Koenderink. Reflectance and texture of real world surfaces. ACM Transactions on Graphics, 18(1): 1-34, January 1999.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280864</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[R Debevec. Rendering synthetic objects into real scenes: Bridging traditional and image-based graphics with global illumination and high dynamic range photography. In Computer Graphics (SIGGRAPH '98 Proceedings), pages 189- 198, July 1998.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[A. Frolov and N. Chentsov. On the calculation of certain integrals dependent on a parameter by the Monte Carlo method. Zh. Vychisl. Mat. Fiz., 2(4): 714 - 717, 1962. (in Russian).]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237200</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[S. Gortler, R. Grzeszczuk, R. Szelinski, and M. Cohen. The Lumigraph. In Computer Graphics (SIGGRAPH '96 Proceedings), pages 43-54, August 1996.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97913</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[R Haeberli and K. Akeley. The accumulation buffer: Hardware support for high-quality rendering. In Computer Graphics (SIGGRAPH '90 Proceedings), pages 309-318, August 1990.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311554</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[W. Heidrich and H.-R Seidel. Realistic, hardware-accelerated shading and lighting. In Computer Graphics (SIGGRAPH '99 Proceedings), August 1999.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>299030</ref_obj_id>
				<ref_obj_pid>299029</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[S. Heinrich. Monte Carlo Complexity of Global Solution of Integral Equations. Journal of Complexity, 14:151-175, 1998.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Silicon Graphics Inc. Pixel Texture Extension, December 1996. Specification document, available from http://www, opengl, org.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15902</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[J. Kajiya. The rendering equation. In Computer Graphics (SIGGRAPH '86 Proceedings), pages 143-150, August 1986.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[J. Kautz, W. Heidrich, and K. Daubert. Bump map shadows for OpenGL rendering. Technical Report MPI-I-2000-4-001, Max-Planck-Institut ffir Informatik, Saarbrficken, Germany, 2000.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>312153</ref_obj_id>
				<ref_obj_pid>311625</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[J. Kautz and M. McCool. Interactive rendering with arbitrary BRDFs using separable approximations. In Rendering Techniques '99 (Proc. of Eurographics Workshop on Rendering), pages 247- 260, June 1999.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258769</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[A. Keller. Instant radiosity. In Computer Graphics (SIG- GRAPH '97 Proceedings), pages 49-56, August 1997.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[A. Keller. Hierarchical monte carlo image synthesis. Mathematics and Computers in Simulation, 2000. preprint available from http://www.uni-kl.de/AG-Heinrich/Alex.html.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237199</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[M. Levoy and R Hanrahan. Light field rendering. In Computer Graphics (SIGGRAPH '96 Proceedings), pages 31-42, August 1996.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[N. Max. Horizon mapping: shadows for bump-mapped surfaces. The Visual Computer, 4(2): 109-117, July 1988.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383841</ref_obj_id>
				<ref_obj_pid>2383815</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[L. Mostefaoui, J.-M. Dischler, and D. Ghazanfarpou. Rendering inhomogeneous surfaces with radiosity. In Rendering Techniques '99 (Proc. of Eurographics Workshop on Rendering), pages 283-292, June 1999.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>614392</ref_obj_id>
				<ref_obj_pid>614269</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[F. Neyret. Modeling, animating, and rendering complex scenes using volumetric textures. IEEE Transactions on Visualization and Computer Graphics, 4(1), January- March 1998.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[NVIDIA Corporation. NVIDIA OpenGL Extension Specifications, October 1999. Available from http://www.nvidia.com.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258873</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[M. Peercy, J. Airey, and B. Cabral. Efficient bump mapping hardware. In Computer Graphics (SIGGRAPH '97 Proceedings), pages 303-306, August 1997.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[M. Segal and K. Akeley. The OpenGL Graphics System: A Specification (Version 1.2), 1998.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[I. Sobol. The use of a~2-distribution for error estimation in the calculation of integrals by the monte carlo method. In U.S.S.R. Computational Mathematics and Mathematical Physics, pages 717-723, 1962.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>731964</ref_obj_id>
				<ref_obj_pid>647651</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[J. Stewart. Hierarchical visibility in terrains. In Rendering Techniques '97 (Proc. of Eurographics Workshop on Rendering), pages 217-228, June 1997.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>614394</ref_obj_id>
				<ref_obj_pid>614269</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[J. Stewart. Fast horizon computation at all points of a terrain with visibility and shading applications. IEEE Transactions on Visualization and Computer Graphics, 4(1):82-93, March 1998.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[J. Stewart. Computing visibility from folded surfaces. Computers and Graphics, 1999. preprint obtained from http://www.dgp.toronto.edu/people/JamesStewart/.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[L. Szirmay-Kalos, T. F6ris, L. Neumann, and B. Csdbfalvi. An Analysis of Quasi-Monte Carlo Integration Applied to the Transillumination Radiosity Method. Computer Graphics Forum (Proc. of Eurographics '97), 16(3):271-282, August 1997.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280860</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[R. Westermann and T. Ertl. Efficiently using graphics hardware in volume rendering applications. In "Computer Graphics (SIGGRAPH '98 Proceedings)", pages 169-178, July 1998.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134075</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[S. Westin, J. Arvo, and K. Torrance. Predicting reflectance functions from complex surfaces. In Computer Graphics (SIGGRAPH '92 Proceedings), pages 255-264, July 1992.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Illuminating Micro Geometry Based on Precomputed Visibility Wolfgang Heidrich* Katja Daubert Jan Kautz 
Hans-Peter Seidel Max-Planck-Institute for Computer Science Abstract Many researchers have been arguing 
that geometry, bump maps, and BRDFs present a hierarchy of detail that should be exploited for ef.cient 
rendering purposes. In practice however, this is often not possible due to inconsistencies in the illumination 
for these differ­ent levels of detail. For example, while bump map rendering often only considers direct 
illumination and no shadows, geometry-based rendering and BRDFs will mostly also respect shadowing effects, 
and in many cases even indirect illumination caused by scattered light. In this paper, we present an 
approach for overcoming these in­consistencies. We introduce an inexpensive method for consistently illuminating 
height .elds and bump maps, as well as simulating BRDFs based on precomputed visibility information. 
With this in­formation we can achieve a consistent illumination across the levels of detail. The method 
we propose offers signi.cant performance bene.ts over existing algorithms for computing the light scattering 
in height .elds and for computing a sampled BRDF representation using a virtual goniore.ectometer. The 
performance can be further im­proved by utilizing graphics hardware, which then also allows for interactive 
display. Finally, our method also approximates the changes in illumina­ tion when the height .eld, bump 
map, or BRDF is applied to a surface with a different curvature. CR Categories: I.3.1 [Computer Graphics]: 
Hardware Architecture Graphics processors; I.3.3 [Computer Graphics]: Picture/Image Generation Bitmap 
and frame buffer operations; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism Color, 
Shading, Shadowing and Texture Keywords: Illumination Effects, Monte Carlo Techniques, Graph­ics Hardware, 
Frame Buffer Tricks, Re.ectance &#38; Shading Models, Texture Mapping  1 Introduction Geometry, bump 
maps and bidirectional re.ectance distribution functions (BRDFs) are often considered different levels 
of detail for the same surface structure. Although there has been a consid­erable amount of work on generating 
smooth transitions between *MPI for Computer Science, Im Stadtwald, 66123 Saarbrücken, Ger­many, {heidrich,daubert,jnkautz,hpseidel}@mpi-sb.mpg.de 
 Permission to make digital or hard copies of part or all of this work or personal or classroom use is 
granted without fee provided that copies are not made or distributed for profit or commercial advantage 
and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, 
to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. SIGGRAPH 
2000, New Orleans, LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 these levels [19, 1], the 
fundamental problem of inconsistent illu­mination algorithms for the representations still remains. For 
example, both simulated and measured BRDFs typically re­spect not only direct illumination, but also 
shadowing and masking effects of the micro geometry, as well as indirect illumination re­sulting from 
light that scatters between the micro surfaces. Geom­etry based representations usually consider direct 
illumination and shadowing/masking, but the indirect illumination is often neglected for performance 
reasons. Similarly, techniques for shadowing [19] and masking [1] in bump maps have been developed, but 
most ap­plications do not use them; techniques for light scattering in bump maps have not been available 
so far. The importance of this indirect, scattered light to the overall appearance is illustrated in 
Figure 1. (a) without scattering (b) with scattering Figure 1: Indirect light in height .elds can have 
a strong impact on the overall appearance. With the advent of new, inexpensive computer graphics hard­ware 
that supports both bump mapping (see [23], [22], and [10] for some possible implementations) and rendering 
with arbitrary BRDFs [15, 10], some newly gained interest in making the transi­tions consistent has evolved. 
In this paper we introduce a single, ef.cient, but high-quality method for illuminating height .eld ge­ometry 
and bump maps, as well as for precomputing BRDFs and even higher dimensional data structures. The method 
is suitable for both ray-tracing and hardware-accelerated rendering. Our algorithm is based on precomputation 
and reuse of visibility information in height .elds, simulates both shadowing and indirect illumination, 
and is able to approximate the illumination as the un­derlying base geometry changes. Thus it is capable 
of consistently illuminating height .eld geometry, bump maps, and BRDFs. The main contributions of this 
paper are: An algorithm for ef.cient computation of indirect light in height .elds, based on precomputed 
visibility information.  An ef.cient representation for shadowing and masking in bump maps.  Hardware-accelerated 
versions of the above algorithms, us­ing a generalization of Monte Carlo algorithms known as the Method 
of Dependent Tests .  Techniques to approximate the changes in illumination when the height .eld is 
applied to a curved surface.  Application of all methods to simulate BRDFs and render other high dimensional 
data structures such as light .elds and bidirectional texture functions (BTFs). Throughout this paper 
we will only deal with height .eld geom­etry, which also means that the simulated BRDFs can only originate 
from height .elds. Furthermore, we assume that the bump heights are relatively small compared to their 
distance from light sources and other objects, so that the only occlusions occurring within a hight .eld 
are caused by parts of the same height .eld, but not by any other geometry in the scene. The remainder 
of this paper is organized as follows: In Section 2 we brie.y discuss related work. Then, we introduce 
our data struc­tures for precomputed visibility and their applications to comput­ing indirect illumination 
in Section 3. We also describe how to make use of graphics hardware for further performance improve­ments 
and interactive viewing (Section 3.3), and introduce new data structures for shadowing and masking in 
Section 4. We then discuss how to adapt both the scattering and the shadow data structures to varying 
base geometry (Section 5), and .nally conclude with some results and a discussion (Section 6). 2 Related 
Work Articles that discuss enhancing the original bump map algorithm [2] represent some of the work most 
closely related to ours. For exam­ple, Max [19] shows how to compute the self-shadowing of bump maps 
with a so-called horizon map . This horizon map describes the horizon for a small number of directions 
(8 in the original pa­per) at each point in the height .eld. During rendering, the shadow test then simply 
determines whether the light direction is above or below the (interpolated) horizon. Stewart [26, 27] 
introduced a hi­erarchical approach to determine the visibility in terrains both for occlusion culling 
and shading. More recently, Stewart used a simi­lar idea to simulate the shadowing in cloth [28]. For 
the shadowing part of our paper, we use a concept similar to the original horizon map, but in a different 
representation of the horizon that allows for a highly ef.cient shadow test, which can also be performed 
with graphics hardware. In addition, we discuss how the data structure can be adapted to different curvatures 
of the underlying base geometry. Masking is in a sense a dual problem to shadowing: where shad­owing 
means that a light ray does not hit a speci.c surface point because it is occluded by some portion of 
the height .eld, mask­ing means that the viewing ray does not hit the point for the same reason. In order 
to incorporate masking in bump maps, Becker and Max [1] introduced redistribution bump mapping, which 
ad­justs the distribution of normals in the bump map with the viewing angle. Another possibility is to 
blend in coarse approximations of the height .eld as a displacement map. In our implementation, we take 
this latter approach, although using the results from Becker and Max should also be possible. Furthermore, 
we can reuse the hori­zon data structures, which we also apply for shadowing, to compute BRDFs. So far, 
there has been little work on computing the indirect il­lumination in height .elds and bump maps. Although 
Mostefaoui et al. [20] do integrate the indirect illumination at micro geometry scale into a global illumination 
simulation, their method relies on both precomputed data (BTFs) and a full geometric description of the 
features. It does not address the problem of precomputing these BTFs, or of computing the illumination 
in bump maps. Several approaches for simulating BRDFs have been proposed in the past. The method by Cabral 
et al. [3] is based on horizon maps, while Becker and Max [1] use the normal distribution in a bump map. 
Our method is most closely related to the one from Westin et al. [31], which is based on ray-tracing. 
At the same time, we borrow the idea of using precomputed visibility information in bump maps from Cabral 
et al. [3], although our data structures are more comprehensive than simple horizon maps in order to 
account for indirect illumination. For the hardware-accelerated variants of our algorithms we re­quire 
some way of rendering bump mapped surfaces. Any of the recently published algorithms (e.g. [10, 22, 23]) 
will be suf.cient. Mathematically, the hardware implementation of indirect illumina­tion uses a generalization 
of Monte Carlo integration known as the Method of Dependent Tests [7]. This method, which is described 
in more detail in Section 3.1, uses the same random sampling pattern for estimating an integral at all 
different points (the indirect illumi­nation in all different points of a height .eld, in our case). 
Several other hardware-based algorithms have implicitly used the Method of Dependent Tests in the past, 
for example most algorithms using the accumulation buffer [9] or the Instant Radiosity algorithm by Keller 
[16]. Another example is the transillumination method [29], an algorithm for global illumination computations, 
which is based on propagating light from all surfaces in one direction. Our method improves on this by 
using precomputed visibility that can be reused for many light paths, and allows for the use of graphics 
hardware. While our algorithms yield a signi.cant performance improve­ment for the generation of single 
images, they are even more attrac­tive for the computation of higher dimensional data structures such 
as BRDFs, light .elds [18, 8], volume representations [21], and spa­tially variant BRDFs or bidirectional 
texture functions (BTFs, [5]), because this allows for a reuse of the precomputed visibility infor­mation. 
Thus, the costly precomputation of visibility can be amor­tized over a larger amount of reuses. For the 
same reason our meth­ods are even more attractive if applied to periodic height .elds. 3 Light Scattering 
in Height Fields To compute the indirect illumination in a height .eld, we have to solve an integral 
equation called the Rendering Equation [13]. This requires integrating over the incident illumination 
in each point of the height .eld, which can, for example, be achieved with Monte Carlo ray-tracing. The 
most expensive part of this integration is typically the visibility computation, which determines the 
surface visible from a given surface point in a certain direction. This is the part that depends on the 
complexity of the scene, while the com­putation of the local interaction of the light with the surface 
has a constant time complexity. In the case of small-scale height .elds that describe the irreg­ularities 
of a surface, we can make two simplifying assumptions. Most importantly, we only deal with cases where 
the visibility in­side the height .eld is completely determined by the height .eld itself, and not by 
any external geometry. This is equivalent to re­questing that no external geometry penetrates the convex 
hull of the height .eld, which is a reasonable assumption for the kind of small surface structures that 
we are targeting. It allows us to precompute the visibility information, and afterwards combine it to 
a variety of different paths, so that the cost of its computation can be amortized over a larger number 
of light paths. Secondly, in the case where we want to use our method to com­pute a BRDF, we request 
that the height .eld geometry is small compared to the remainder of the scene, and therefore any incom­ing 
direct light can be assumed parallel. This is necessary simply because the BRDF by de.nition is a function 
of exactly one incom­ing direction and exactly one outgoing direction. This assumption is not necessary 
for the other levels of detail, i.e. bump maps and displacement maps. The visibility restriction requests 
that no other geometry in the scene will act as an occluder between two points in the height .eld. If 
we now assume the height .eld is attached to a speci.c, .xed base geometry, we can, for a given point 
p on the height .eld, and a given direction dd, precompute whether the ray originating at p in direction 
ddhits some other portion of the height .eld, or not. Furthermore, if it does intersect with the height 
.eld, we can pre­compute the intersection point and store it in a data base. Since this intersection 
point is some point in the same height .eld, it is unambiguously characterized by a 2D texture coordinate. 
 Figure 2: One of the scattering textures Si for the triangular height .eld on the right. Now imagine 
having a set D of N uniformly distributed direc­tions, and having precomputed the visible surface point 
for every direction dd. D and for every grid point in the height .eld tex­ture. If the height .eld is 
periodic, this has to be taken into account for determining this visibility information. Also, for the 
moment we assume that the height .eld is applied to a speci.c base geom­etry, typically a .at surface. 
Section 5 deals with an adaptation to varying base geometries. We store this precomputed data as a set 
of N texture maps Si; for each direction ddi . D there is one 2D texture with two components representing 
the 2D coordinates of the visible point. Each of these textures is parameterized the same way the height 
.eld is, i.e. the 2D texture coordinates directly correspond to height .eld positions p. The texture 
value also corresponds to a point in the height .eld and represents the surface point q that is visible 
from p in direc­tion ddi. Note again, that these visibility textures are only valid for a given, prede.ned 
base geometry to which the height .eld is at­tached. Section 5 will describe how this information can 
be used to illuminate the height .eld when it is attached to other geometries. By chaining together this 
visibility information, we can now gen­erate a multitude of different light paths for computing the indirect 
illumination in the height .eld. This way, it is possible to imple­ment variants of many existing Monte 
Carlo algorithms, using the precomputed data structures instead of on-the-.y visibility compu­tations. 
Below, for example, we outline a simple path tracing algo­rithm that computes the illumination at a given 
surface point, but ignores indirect light from geometry other than the height .eld: radiance( p, dv ) 
{ L:= direct illumination( p ); i:= random number in [1 ...N ]; if(q := Si[p] is valid height field coord.){ 
L:= L+ fr (p, dv, ddi) · cos(. (ddi, dnp))· radiance( q, -ddi ); } return L; } In this algorithm, dnp 
is the bump map normal in point p, and fr (p, dv, ddi) is the BRDF of the height .eld in that point. 
The di­rect illumination in each point is computed using a bump mapping technique. Of course the visibility 
information for direction Si is only known at discrete height .eld grid positions. At other points, we 
can only exactly reconstruct the direct illumination, while the indi­rect light has to be interpolated. 
For example, we can simply use the visibility information of the closest grid point as Si[p]. This nearest-neighbor 
reconstruction of the visibility information corre­sponds to a quantization of texture coordinates, so 
that these always point to grid points of the height .eld. For higher quality, we can also choose a bilinear 
interpolation of the indirect illumination from surrounding grid points. In our implementation, we use 
the nearest­neighbor approach for all secondary intersections by simply quan­tizing the texture coordinates 
encoded in the visibility textures Si. On the other hand, we use the interpolation method for all primary 
intersections to avoid blocking artifacts. Figure 1b shows a result of this method. For more complex 
examples, see Section 6. The simple algorithm above ignores shadowing, but with the technique described 
in Section 4, which is similar to the one in­troduced by Max [19], shadows can also be included. Using 
similar methods, other Monte Carlo algorithms like dis­tribution ray-tracing [4] can also be built on 
top of this visibility information. The advantage of using precomputed visibility for the light scattering 
in height .elds, as described in this section, is that the visibility information is reused for different 
paths. Therefore, the cost of computing it can be amortized over several uses. 3.1 The Method of Dependent 
Tests As mentioned above, we have to solve the Rendering Equation [13] in order to determine the indirect 
illumination in a height .eld. Based on the precomputed visibility information, we solve the Ren­dering 
Equation by Monte Carlo integration of the incident illumi­nation at any given surface point, and obtain 
the re.ected radiance for that point and a given viewing direction. In general, however, we do not only 
want to compute the re­.ected light for a single point on the height .eld, but typically for a large 
number of points. With standard Monte Carlo integration, we would use different, statistically independent 
sample patterns for each of the surface points we are interested in. The Method of Dependent Tests [7] 
is a generalization of Monte Carlo techniques that uses the same sampling pattern for all surface points. 
More speci.cally, we choose the same set of directions for sampling the incident light at all surface 
points. For example, as depicted in Figure 3, for all points p in the height .eld, we collect illumination 
from the same direction ddi. As pointed out by Keller [17], there are several instances in the computer 
graphics literature, where the Method of Dependent Tests has been applied implicitly [9, 16]. For example, 
one of the stan­dard algorithms for the accumulation buffer [9] is a depth-of-.eld effect, which uses 
identical sampling patterns of the lens aperture for all pixels. It has been shown by Sobol [25] that 
the Method of Dependent Tests is an unbiased variant of Monte Carlo integration. Recently, hierarchical 
versions of the Method of Dependent Tests have been proposed [11, 17], but we do not currently make use 
of these results. 3.2 Dependent Test Implementation of Light Scat­tering in Height Fields Based on the 
Method of Dependent Tests, we can rewrite Monte Carlo algorithms as a sequence of SIMD operations that 
operate on the grid cells of the height .eld. Consider the light path in Figure 3. Light hits the height 
.eld from direction dl, scatters at each point in direction -ddi . D, and leaves the surface in the direction 
of the viewer dv. Since all these vectors are constant across the height .eld, the only varying parameters 
are the surface normals. More speci.cally, for the radiance leaving a grid point p in direction dv, the 
important Figure 3: With the Method of Dependent Tests, the different paths for the illumination in all 
surface points are composed of pieces with identical directions. varying parameters are the normal dnp, 
the point q := Si[p] visible from p in direction d i, and the normal dnq in that point. In particular, 
the radiance in direction dv caused by light arriving from direction dl and scattered once in direction 
-d i is given by the following formula. Lo(p, dv)=fr (dnp,d i, dv) < dnp,d i > · () fr (dnq ,dl, -ddi) 
< dnq ,dl > ·Li (q,dl). (1) Usually, the BRDF is written as a 4D function of the incoming and the outgoing 
direction, both given relative to a local coordi­nate frame where the local surface normal coincides 
with the z-axis. In a height .eld setting, however, the viewing and light directions are given in some 
global coordinate system that is not aligned with the local coordinate frame, so that it is .rst necessary 
to perform a transformation between the two frames. To emphasize this fact, we have denoted the BRDF 
as a function of the incoming and out­going direction as well as the surface normal. If we plan to use 
an anisotropic BRDF on the micro geometry level, we would also have to include a reference tangent vector. 
Note that the term in parenthesis is simply the direct illumination of a height .eld with viewing direction 
-ddi, with light arriving from dl. If we precompute this term for all grid points in the height .eld, 
we obtain a texture Ld containing the direct illumination for each surface point. This texture can be 
generated using a bump mapping step where an orthographic camera points down onto the height .eld, but 
-d i is used as the viewing direction for shading purposes. Once we have Ld, the second re.ection is 
just another bump mapping step with dv as the viewing direction and d i as the light direction. This 
time, the incoming radiance is not determined by the intensity of the light source, but rather by the 
content of the Ld texture. For each surface point p we look up the corresponding visible point q = Si 
[p]. The outgoing radiance at q, which is stored in the texture as Ld[q], is at the same time the incoming 
radiance at p. Thus, we have reduced computing the once-scattered light in each point of the height .eld 
to two successive bump mapping op­erations, where the second one requires an additional indirection to 
look up the illumination. We can easily extend this technique to longer paths, and also add in the direct 
term at each scattering point. This is illustrated in the Figure 4. For the total illumination in a height 
.eld, we sum up the contri­butions for several such paths (some 40-100 in most of our scenes). This way, 
we compute the illumination in the complete height .eld at once, using two SIMD-style operations on the 
whole height .eld texture: bump mapping for direct illumination, using two given di­rections for incoming 
and outgoing light, as well as a lookup of the indirect illumination in a texture map using the precomputed 
visibility data in form of the textures Si . This is in itself a performance improvement over the regular 
Monte Carlo algorithms presented before, because the illumination Figure 4: Extending the dependent test 
scattering algorithm to mul­tiple scattering. Each box indicates a texture that is generated with regular 
bump mapping. in one grid cell will contribute to many other points on the surface in the .nal image 
via light scattering. In contrast to standard Monte Carlo, our dependent test approach avoids recomputing 
this contri­bution for each individual pixel. What remains to be done is an ef.cient test of whether 
a given point lies in shadow with respect to the light direction dl. While it is possible to interpolate 
this information directly from the visibility database Si, we can also .nd a more ef.cient, although 
approxi­mate representation, that will be described in Section 4. 3.3 Use of Graphics Hardware In addition 
to the above-mentioned performance improvements we get from the implementation of the Method of Dependent 
Tests in software, we can also utilize graphics hardware for an additional performance gain. In recent 
graphics hardware, both on the work­station and on the consumer level, several new features have been 
introduced that we can make use of. In particular, we assume a standard OpenGL-like graphics pipeline 
[24] with some extensions as described in the following. Firstly, we assume the hardware has some way 
of rendering bump maps. This can either be supported through speci.c exten­sions (e.g. [22]), or through 
the OpenGL imaging subset [24], as described by Heidrich and Seidel [10]. Any kind of bump mapping scheme 
will be suf.cient for our purposes, but the kind of re.ection model available in this bump mapping step 
will determine what re­.ection model we can use to illuminate our hight .eld. Secondly, we will need 
a way of interpreting the components stored in one texture or image as texture coordinates pointing into 
another texture. One way of supporting this is the so-called pixel texture extension [12, 10], which 
performs this operation during transfer of images into the frame buffer, and is currently only avail­able 
on some high-end SGI machines. Alternatively, we can use dependent texture lookups, a variant of multi-texturing, 
that has re­cently been announced by several vendors for the next generation of consumer level hardware 
(the Matrox G400 offers a restricted version of this today). With dependent texturing, we can map two 
or more textures simultaneously onto an object, where the texture coordinates of the second texture are 
obtained from the components of the .rst texture. This is exactly the feature we are looking for. In 
case we have hardware that supports neither of the two, it is quite simple, although not very fast, to 
implement the pixel texture exten­sion in software: the framebuffer is read out to main memory, and each 
pixel is replaced by a value looked up from a texture, using the previous contents of the pixel as texture 
coordinates. Using these two features, dependent texturing and bump map­ping, the implementation of the 
dependent test method as described above is simple. As mentioned in Section 3.2 and depicted in Fig­ure 
3, the scattering of light via two points p and q in the height .eld .rst requires us to compute the 
direct illumination in q. If we do this for all grid points we obtain a texture Ld containing the re­.ected 
light caused by the direct illumination in each point. This texture Ld is generated using the bump mapping 
mechanism the hardware provides. Typically, the hardware will support only dif­fuse and Phong re.ections, 
but if it supports more general models, then these can also be used for our scattering implementation. 
The second re.ection in p is also a bump mapping step (although with different viewing-and light directions), 
but this time the direct illumination from the light source has to be replaced by a per-pixel radiance 
value corresponding to the re.ected radiance of the point q visible from p in the scattering direction. 
We achieve this by bump mapping the surface with a light intensity of 1, and by af­terwards applying 
a pixel-wise multiplication of the value looked up from Ld with the help of dependent texturing. Figure 
5 shows how to conceptually set up a multi-texturing system with dependent textures to achieve this result. 
p q Li Figure 5: For computing the indirect light with the help of graphics hardware, we conceptually 
require a multi-texturing system with dependent texture lookups. This .gure illustrates how this system 
has to be set up. Boxes indicate one of the two textures, while incoming arrows signal texture coordinates 
and outgoing ones mean the resulting color values. The .rst texture is the Si that corresponds to the 
scattering di­rection di . For each point p it yields q, the point visible from p in direction di. The 
second texture Ld contains the re.ected direct light in each point, which acts as an incoming radiance 
at p. By using this hardware approach, we treat the graphics board as a SIMD-like machine which performs 
the desired operations, and computes one light path for each of the grid points at once. As shown in 
Section 6, this use of hardware dramatically increases the performance over the software version to an 
almost interactive rate.  4 Approximate Bump Map Shadows As mentioned in Section 3.1, we can simply 
use scattering informa­tion stored in Si for determining the shadows cast in a height .eld. For example, 
to determine if a given grid point p lies in shadow for some light direction, we could simply .nd the 
closest direction ddi . D, and use texture Si to determine whether p sees another point of the height 
.eld in direction ddi. For a higher quality test, we can precompute a triangulation of all points points 
on the unit sphere corresponding to the unit vectors ddi (since the set of directions is the same for 
all surface points, this is just one triangle mesh for all points on the height .eld). The same triangulation 
will later be used in Section 5 for other purposes. Based on this mesh, we can easily determine the three 
directions ddi that are closest to any given light direction, and then interpolate those directions visibility 
values. This yields a visibility factor between 0 and 1 de.ning a smooth transition between light and 
shadow. Although this approach works, we have also implemented a more approximate method that is better 
suited for hardware implementa­tion and much faster. We start by projecting all the unit vectors for 
the sampling di­rections ddi . D of the upper hemisphere over the shading normal into the tangent plane, 
i.e. we drop the z coordinate of ddi in the lo­cal coordinate frame. Then we .t an ellipse containing 
as many of those 2D points that correspond to unshadowed directions as pos­sible, without containing 
too many shadowed directions. This el­lipse is uniquely determined by its (2D) center point c, a direction 
(ax,ay )T describing the direction of the major axis (the minor axis is then simply (-ay,ax)T ), and 
two radii r1 and r2 , one for the extent along each axis. Figure 6: For the shadow test we precompute 
2D ellipses at each point of the height .eld, by .tting them to the projections of the scattering directions 
into the tangent plane. For the .tting process, we begin with the ellipse represented by the eigenvectors 
of the covariance matrix of all points correspond­ing to unshadowed directions. We then optimize the 
radii with a local optimization method. As an optimization criterion we try to maximize the number of 
light directions inside the ellipse while at the same time minimizing the number of shadowed directions 
in­side it. Once we have computed this ellipse for each grid point in the height .eld, the shadow test 
is simple. The light direction dl is also projected into the tangent plane, and it is checked whether 
the re­sulting 2D point is inside the ellipse (corresponding to a lit point) or not (corresponding to 
a shadowed point). This approach is similar to the one described by Max [19] using horizon maps. Only 
here the horizon map is replaced by a map of ellipses, each uniquely determined by 6 parameters. Both 
the projection and the in-ellipse test can mathematically be expressed very easily. First, the 2D coordinates 
lx and ly have to be transformed into the coordinate system de.ned by the axes of the ellipse: . ax lx 
- cx lx := <, >, (2) ay ly - cy . -ay lx - cx ly := <, > (3) ax ly - cy Afterwards, the test 1 - (l. 
x)2 r2 1 - (l. y )2 r2 2 = 0 (4) has to be performed. To map these computations to graphics hardware, 
we represent the six degrees of freedom for the ellipses as 2 RGB textures. Then the required operations 
to implement Equations 2 through 4 are simple dot products as well as additions and multiplications. 
Both Westermann et al. [30] and Heidrich and Seidel [10] have shown how such operations can be implemented 
on graphics hardware. This is possible using the OpenGL imaging subset [24], available on most contemporary 
workstations, but also using some vendor speci.c extensions, such as the register combiner extension 
from NVIDIA [22]. Depending on the exact graphics hardware avail­able, the implementation details will 
have to vary slightly. Thus, they are omitted from this paper, and we refer the interested reader to 
our technical report [14]. 5 Varying the Base Geometry So far we have only considered the case where 
the height .eld is attached to a base geometry of a .xed, previously known curvature, typically a planar 
object. However, if we plan to use the same height .eld for different geometric objects, the valleys 
in a height .eld widen up or narrow down depending on the local curvature of the object, and the height 
.eld can be locally stretched in a non-uniform fashion. This affects both the casting of shadows and 
the scattering of indirect light. For the shadows, it is obvious that narrower valleys will cause more 
regions to be shadowed, while in wider valleys more regions are lit. For the scattering part, the opposite 
is true. For a point on the bottom of a narrow valley, a large proportion of the solid angle is covered 
by other portions of the height .eld, and therefore the im­pact of indirect light is strong. On the other 
hand, in a wide valley, most of the light will be re.ected back into the environment rather than remaining 
inside the height .eld. In this section we discuss adaptations of the previously described algorithms 
and data structures to the case where the base geometry changes. To this end, we will assume that the 
curvature of this base geometry is small compared to the features in the height .eld. It is then a reasonable 
assumption that the visibility does not change as the surface is bent. This means that two points in 
the height .eld that are mutually visible for a planar base geometry, are also mutually visible in the 
curved case. Obviously, this assumption breaks down for extreme curvatures, but it generally holds for 
small ones. First let us consider the data structures and algorithms for com­puting scattered, indirect 
light. Since we have assumed that no extreme changes in visibility occur, the precomputed visibility 
data i.e. the textures Si are still valid as the underlying geometry changes. However, as depicted in 
Figure 7, some parameters of the illumination change. Firstly, there is no longer a .xed global di­ rection 
ddi corresponding to each texture Si . Rather, the direction changes as a parameter of the curvature 
and of the distance between two mutually visible points, and becomes different for every point on the 
surface. Secondly, the normal (and therefore the angles be­tween the normal and other vectors) changes 
as a function of the same parameters. q Figure 7: The directions ddi change on a per-pixel basis if 
the height .eld is applied to a curved base geometry. The rate of change de­pends on the distance of 
two points from each other. These changes remove the coherence that we used to map the algorithm to graphics 
hardware, since now all directions need to be computed for each individual height .eld point.This requires 
opera­tions that are currently not possible with graphics hardware. On the other hand, the abovementioned 
changes are quite easy to account for in a software renderer. However, there is a third change due to 
the curvature, which af­fects all our Monte Carlo algorithms. The set of directions D used to be a uniform 
sampling of the directional sphere for the case of a given, .xed base geometry. Now, when the height 
.eld is applied to a geometry with slightly changed curvature or a non-uniformly scaled one, the directions 
change as mentioned above. The rate of change depends on the distance of the two mutually visible points. 
Therefore, the directions do not change uniformly, and, as a conse­quence, the sampling of directions 
is no longer uniform. In Monte Carlo terms, this means that the importance of the individual di­rections 
has changed, and that this importance has to be taken into account for the Monte Carlo integration. Different 
light paths can no longer be summed up with equal weight, but have to be weighted by the importance of 
the respective path. This importance has to be computed for every individual point in the height .eld. 
This requires us to develop an estimate for the importance of a given sample direction, which is explained 
in the following. We start by interpreting the unit directions di . D for the original geometry as points 
on the unit sphere, and generate a triangulation of these. Since the sampling of directions is uniform 
in this planar case, the areas of the triangle fans surrounding any direction di will be approximately 
the same for all di, see Figure 8. Figure 8: When a height .eld is applied to a different base geom­etry, 
the importance of the individual directions changes, which is indicated by a change of area of the triangulated 
unit directions on the sphere. Now, if we gradually bend the underlying surface, the points cor­responding 
to the directions will slowly move either towards the horizon or towards the zenith, depending on the 
sign of the curva­ture we apply. Note that a change in visibility means that during this movement the 
triangle mesh folds over at a given point. As mentioned above, we are going to ignore this situation, 
and restrict ourselves to small curvatures which do not cause such visibility changes. In this case, 
the sole effect of the moving points on the unit sphere is that the areas of the triangle fans surrounding 
each di­rection change (see Figure 8). This change of area is an estimate for the change in sampling 
rate, and therefore an estimate for the importance of a particular direction in the curved case. Thus, 
if we apply a height .eld to a curved surface, we weight all light paths by the relative area of the 
triangle fan surrounding the chosen di­rection. Now that we have dealt with the adaptation of the scattering 
data structures, we also have to take care of the shadowing. If we com­pute the shadows directly from 
the Si, as described at the beginning of Section 4, then no changes are required. However, if we are 
us­ing the 2D ellipses introduced at the end of Section 4, then these ellipses have to be adapted to 
the local surface curvature. Starting from the updated scattering directions di , we can .t a different 
ellipse for each point and each surface curvature. How­ever, precomputing and storing this information 
for a lot of differ­ent curvatures is both memory and time consuming. We therefore only precompute a 
total of .ve different ellipses: the original one for zero curvature, one each for a slight positive 
and a slight nega­tive curvature in each of the parametric directions. From this data we can then generate 
a linear approximation of the changes of el­lipse parameters under any given curvature. Again, this only 
works reasonably as long as the radii of curvature are large compared to the height .eld features (i.e. 
as long as the curvatures are small), but for large curvatures we will run into visibility changes anyway. 
 6 Results We have implemented the approaches described in this paper both in software, and for two different 
kinds of graphics hardware. Firstly, we use the SGI Octane, which provides support for pixel textures, 
but does not have advanced features like multi-texturing, which would help us to reduce the number of 
rendering passes. On this platform we have implemented the Phong re.ection model using the normal map 
approach described by Heidrich and Sei­del [10]. Heidrich and Seidel also describe ways of incorporating 
other re.ection models as well as environment maps through the use of pixel textures. We have not made 
use of these results. Secondly, we have used an NVIDIA GeForce 256 with DDR RAM. This graphics board 
supports multi-texturing with very .ex­ible ways of combining the resulting colors for each fragment 
(via NVIDIA s register combiner extension [22]). This allows us to per­form bump mapping with local illumination 
and the Phong model in one pass, and helps us to ef.ciently implement the shadow test. However, the GeForce 
does not support dependent texture lookups, so that the scattering had to be implemented essentially 
using a soft­ware version of the pixel texture extension. The .rst tests we have performed are designed 
to show whether we can use precomputed visibility to consistently illuminate geom­etry and bump maps, 
and also to simulate BRDFs. Figure 9 shows some curved geometry to which the triangular height .eld from 
Fig­ure 2 has been applied. In this height .eld, the faces pointing in one direction are red, and the 
faces pointing in the other are white. The top row of Figure 9 shows the results of applying the geometry 
as a displacement map. On the left side, which does not include scattering but shadowing and masking, 
the separation of the colors becomes apparent, since the top of the geometry is more reddish, while the 
bottom is white. Due to color bleeding, the image in­cluding the scattering term on the right is more 
homogeneous. The bottom row of the .gure shows the geometry with a BRDF that has been computed from the 
same height .eld using graphics hardware. Both the version with and the one without scattering show the 
same kind of behavior as the geometry-based rendering, which illustrates that our technique can be used 
for smooth transitions between levels of detail. Both for the rendering of the geometry-based image and 
for the generation of the BRDF, we .rst had to generate the visibility data, namely the textures Si and 
the ellipse data structures for the shad­ows. The two leftmost columns of Table 1 show the timings for 
this precomputation phase and a number of different height .elds. The memory requirements for the data 
structures are quite low: for the scattering in a 32 × 32 height .eld with 100 sample directions we generate 
100 two-component textures with a size of 32 × 32 × 2 Bytes, which amounts to less than 2 MB of data 
for the whole scat­tering information. The shadowing data structure simply consists of two three-component 
textures, yielding 32 × 32 × 6 = 6144 Bytes. After the data structures are precomputed, we can ef.ciently 
compute images with scattering (100 samples) and shadow­ing/masking from them using either a software 
or a hardware ren­derer. The times for computing the scattering terms in the height .eld are listed in 
the third and fourth column of Table 1. Note that the timings for hardware rendering of small (32 × 32) 
height .elds including a one-time scattering are well below one sec­ond. Thus, we can generate images 
of scattered height .elds at in­teractive frame rates, although not quite fast enough for applications 
like games. Figure 9: A comparison of geometry (top) and BRDF (bottom). Left side: without indirect 
light, right side: with indirect light. Height Field Si Shadows SW HW Triangles (32 × 32) Bricks (128 
× 128) Bumps (32 × 32) 27 1029 109 32 194 65 10 12 8 0.48 2.10 0.48 Table 1: Timings for precomputation 
and rendering of different height .elds in seconds. However, we can use the hardware algorithm to compute 
higher­dimensional data structures, such as light .elds [8, 18] and both space variant and space invariant 
BRDFs. For example, we can generate a light .eld consisting of 32 × 32 images of a height .eld including 
scattering terms in just about 6-8 minutes. As we move to BRDFs, a single BRDF sample is the average 
radiance from a whole image of the height .eld. Thus, if we would like to compute a dense, regular mesh 
of samples for a BRDF, we have to compute a 4-dimensional array of images, and then average the radiance 
of each image. The BTF [5], on the other hand, is a 6-dimensional data structure obtained by omitting 
the averaging step, and storing the images directly. These operations can become prohibitively expensive: 
even for relatively small BRDF resolutions such as 164 , this would take about 7-8 hours. However, as 
other researchers have pointed out before [3, 31], it is not necessary to compute this large number of 
independent samples. Since BRDFs are typically smooth functions, it is suf.cient to compute several hundred 
random samples, and project those into a hierarchical basis such as spherical harmonics. Using our approach, 
this small number of samples can be gen­erated within several minutes. To further improve the performance 
slightly, we can completely get rid of geometry for the computation of BRDF samples, and work in texture 
space. As described in Sec­tions 3.2 and 3.3, the Method of Dependent Tests already operates in texture 
space. Only in the last step, when we want to display the result, we normally have to apply this texture 
to geometry. For the BRDF computation, however, we are only interested in the av­erage of the radiances 
for the visible surface points. Therefore, if we manage to solve the masking problem by some other means, 
we do not have to use geometry at all. The masking problem can be solved by using the same data structures 
as used for the shadow test, only with the viewing direction instead of the light direction. This technique 
was .rst proposed by Cabral et al. [3] for their method of shadowing bump maps. Figure 10 shows some 
more examples for our technique. The bottom left sphere is rendered with a bump map using only direct 
light and our shadow test. The top sphere uses the same bump map, but also includes indirect light re.ected 
from other portions of the bump map up to a path length of 4. Finally, in the bottom right sphere, we 
also include indirect illumination from other parts of the scene, which, in this case, is represented 
as an environment map, similarly to the method described by Debevec [6]. This is imple­mented by querying 
the environment map every time the visibility textures Si indicate that no intersection occurs with the 
height .eld for the given direction. Figure 11 demonstrates the effect of different curvatures of the 
underlying geometry, that other researchers have neglected so far. Note that the red faces receive only 
indirect illumination through scattering from the white faces. We can clearly see the reduced scattering 
in the case where the curved base geometry causes the valleys to widen up, and at the same time we can 
see that more regions are shadowed for this case. Finally, Figures 12 and 13 show some more complicated 
exam­ples. Figure 12 depicts a backyard scene in which every object except for the .oor and the bin has 
been bump-mapped. This im­age took 16.2 minutes to render in a resolution of 640 × 400 pixels. The scene 
has been rendered in software with our methods for shad­owing and scattering. Figure 13 shows an image 
of a terrain model rendered with hardware acceleration and bump shadows in real time ( 20 fps. on a GeForce 
256). 7 Conclusion In this paper, we have described an ef.cient method for illumi­nating height .elds 
and bump maps based on precomputed visi­bility information. The algorithm simulates both self shadowing 
of the height .eld, as well as indirect illumination bouncing off height .eld facets. This allows us 
to use geometry, bump maps, and BRDFs as different levels of detail for a surface structure, and to consistently 
illuminate these three representations. Using the Method of Dependent Tests, which is a generalization 
of Monte Carlo techniques, it is possible to map these methods onto graphics hardware. These techniques 
exploit the new bump map­ping features of recent graphics boards, as well as dependent tex­ture mapping, 
which is currently available only on some high-end systems, but will be a standard feature of the commodity 
hardware shipping at the end of the year. Both the software and the hardware implementation of our algo­rithms 
can be used to ef.ciently precompute BRDFs and higher di­mensional data structures such as BTFs or shift-variant 
BRDFs. Fi­nally, we are also able to approximate the effects of different curva­tures of the underlying 
base geometry, which to some extent change shadowing and light scattering in a height .eld, and therefore 
also affects representations like the BRDF. This is the .rst time in the literature that these effects 
are simulated. There is a potential for extending the techniques described in this paper in several ways. 
First of all, we would like to be able to deal with other geometry than height .elds, since materials 
such as cloth and porous materials cannot be represented in this form. In principle it should be easy 
to extend our algorithms to arbitrary geometry, however. In order to utilize graphics hardware, we have 
to .nd an appropriate 2D parameterization for the object so that all surface points can be represented 
in a texture. The next step could then be to extend the method to participating media, for example to 
simulate sub-surface scattering. It would also be interesting to ex­plore whether it is useful to apply 
the developed methods to com­pute global illumination in macroscopic scenes along the lines of the transillumination 
method [29]. Finally, a hierarchical version of the Method of Dependent Tests has recently been introduced 
by Heinrich [11] and Keller [17]. Heinrich proved that this method is optimal for a certain class of 
functions ful.lling some smoothness criteria, and Keller extended this work to other classes of functions. 
It would be interesting to see if these results can be utilized to further improve the performance of 
our algorithm. 8 Acknowledgments We would like to thank Alexander Keller for pointing out the re­lationship 
between our algorithms and the Method of Dependent Tests. Furthermore, we would like to thank him, Michael 
McCool, and the anonymous reviewers for their valuable comments.  References [1] B. Becker and N. Max. 
Smooth transitions between bump ren­dering algorithms. In Computer Graphics (SIGGRAPH 93 Proceedings), 
pages 183 190, August 1993. [2] J. Blinn. Simulation of wrinkled surfaces. In Computer Graphics (SIGGRAPH 
78 Proceedings), pages 286 292, Au­gust 1978. [3] B. Cabral, N. Max, and R. Springmeyer. Bidirectional 
re.ec­tion functions from surface bump maps. In Computer Graph­ics (SIGGRAPH 87 Proceedings), pages 273 
281, July 1987. [4] R. Cook, T. Porter, and L. Carpenter. Distributed ray tracing. In Computer Graphics 
(SIGGRAPH 84 Proceedings), pages 137 45, July 1984. [5] K. Dana, B. van Ginneken, S. Nayar, and J. Koenderink. 
Re­.ectance and texture of real world surfaces. ACM Transac­tions on Graphics, 18(1):1 34, January 1999. 
[6] P. Debevec. Rendering synthetic objects into real scenes: Bridging traditional and image-based graphics 
with global il­lumination and high dynamic range photography. In Com­puter Graphics (SIGGRAPH 98 Proceedings), 
pages 189 198, July 1998. [7] A. Frolov and N. Chentsov. On the calculation of certain in­tegrals dependent 
on a parameter by the Monte Carlo method. Zh. Vychisl. Mat. Fiz., 2(4):714 717, 1962. (in Russian). 
[8] S. Gortler, R. Grzeszczuk, R. Szelinski, and M. Cohen. The Lumigraph. In Computer Graphics (SIGGRAPH 
96 Proceed­ings), pages 43 54, August 1996. [9] P. Haeberli and K. Akeley. The accumulation buffer: Hard­ware 
support for high-quality rendering. In Computer Graph­ics (SIGGRAPH 90 Proceedings), pages 309 318, August 
1990. [10] W. Heidrich and H.-P. Seidel. Realistic, hardware-accelerated shading and lighting. In Computer 
Graphics (SIGGRAPH 99 Proceedings), August 1999. [11] S. Heinrich. Monte Carlo Complexity of Global Solution 
of Integral Equations. Journal of Complexity, 14:151 175, 1998. [12] Silicon Graphics Inc. Pixel Texture 
Extension, De­cember 1996. Speci.cation document, available from http://www.opengl.org. [13] J. Kajiya. 
The rendering equation. In Computer Graphics (SIGGRAPH 86 Proceedings), pages 143 150, August 1986. [14] 
J. Kautz, W. Heidrich, and K. Daubert. Bump map shadows for OpenGL rendering. Technical Report MPI-I-2000-4-001, 
Max-Planck-Institut für Informatik, Saarbrücken, Germany, 2000. [15] J. Kautz and M. McCool. Interactive 
rendering with arbitrary BRDFs using separable approximations. In Rendering Tech­niques 99 (Proc. of 
Eurographics Workshop on Rendering), pages 247 260, June 1999. [16] A. Keller. Instant radiosity. In 
Computer Graphics (SIG-GRAPH 97 Proceedings), pages 49 56, August 1997. [17] A. Keller. Hierarchical 
monte carlo image synthesis. Mathe­matics and Computers in Simulation, 2000. preprint available from 
http://www.uni-kl.de/AG-Heinrich/Alex.html. [18] M. Levoy and P. Hanrahan. Light .eld rendering. In Com­puter 
Graphics (SIGGRAPH 96 Proceedings), pages 31 42, August 1996. [19] N. Max. Horizon mapping: shadows for 
bump-mapped sur­faces. The Visual Computer, 4(2):109 117, July 1988. [20] L. Mostefaoui, J.-M. Dischler, 
and D. Ghazanfarpou. Ren­dering inhomogeneous surfaces with radiosity. In Rendering Techniques 99 (Proc. 
of Eurographics Workshop on Render­ing), pages 283 292, June 1999. [21] F. Neyret. Modeling, animating, 
and rendering complex scenes using volumetric textures. IEEE Transactions on Vi­sualization and Computer 
Graphics, 4(1), January March 1998. [22] NVIDIA Corporation. NVIDIA OpenGL Extension Speci.ca­tions, 
October 1999. Available from http://www.nvidia.com. [23] M. Peercy, J. Airey, and B. Cabral. Ef.cient 
bump mapping hardware. In Computer Graphics (SIGGRAPH 97 Proceed­ings), pages 303 306, August 1997. [24] 
M. Segal and K. Akeley. The OpenGL Graphics System: A Speci.cation (Version 1.2), 1998. [25] I. Sobol. 
The use of .2 -distribution for error estimation in the calculation of integrals by the monte carlo method. 
In U.S.S.R. Computational Mathematics and Mathematical Physics, pages 717 723, 1962. [26] J. Stewart. 
Hierarchical visibility in terrains. In Rendering Techniques 97 (Proc. of Eurographics Workshop on Render­ing), 
pages 217 228, June 1997. [27] J. Stewart. Fast horizon computation at all points of a terrain with visibility 
and shading applications. IEEE Transactions on Visualization and Computer Graphics, 4(1):82 93, March 
1998. [28] J. Stewart. Computing visibility from folded surfaces. Computers and Graphics, 1999. preprint 
obtained from http://www.dgp.toronto.edu/people/JamesStewart/. [29] L. Szirmay-Kalos, T. Fóris, L. Neumann, 
and B. Csébfalvi. An Analysis of Quasi-Monte Carlo Integration Applied to the Transillumination Radiosity 
Method. Computer Graphics Forum (Proc. of Eurographics 97), 16(3):271 282, August 1997. [30] R. Westermann 
and T. Ertl. Ef.ciently using graphics hard­ware in volume rendering applications. In "Computer Graph­ics 
(SIGGRAPH 98 Proceedings)", pages 169 178, July 1998. [31] S. Westin, J. Arvo, and K. Torrance. Predicting 
re.ectance functions from complex surfaces. In Computer Graphics (SIGGRAPH 92 Proceedings), pages 255 
264, July 1992.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344987</article_id>
		<sort_key>465</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>52</seq_no>
		<title><![CDATA[Lapped textures]]></title>
		<page_from>465</page_from>
		<page_to>470</page_to>
		<doi_number>10.1145/344779.344987</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344987</url>
		<abstract>
			<par><![CDATA[<p>We present for creating texture over an surface mesh using an example 2D texture. The approach is to identify interesting regions (<italic>texture patches</italic>) in the 2D example, and to repeatedly paste them onto the surface until it is completely covered. We call such a collection of overlapping patches a <italic>lapped texture</italic>. It is rendered using compositing operations, either into a traditional global texture map during a preprocess, or directly with the surface at runtime. The runtime compositing approach avoids resampling artifacts and drastically reduces texture memory requirements.</p><p>Through a simple interface, the user specifies a tangential vector field over the surface, providing local control over the texture scale, and for anisotropic textures, the orientation. To paste a texture patch onto the surface, a <italic>surface patch</italic> is grown and parametrized over texture space. Specifically, we optimize the parametrization of each surface patch such that the tangential vector field aligns everywhere with the standard frame of the texture patch. We show that this optimization is solved efficiently as a sparse linear system.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[parametrizations]]></kw>
			<kw><![CDATA[texture mapping]]></kw>
			<kw><![CDATA[texture synthesis]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010243</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Appearance and texture representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39077456</person_id>
				<author_profile_id><![CDATA[81339522758]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Emil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Praun]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P10635</person_id>
				<author_profile_id><![CDATA[81100576882]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Adam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Finkelstein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P112851</person_id>
				<author_profile_id><![CDATA[81100397561]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hugues]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoppe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>122744</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BENNIS, C., VEZIEN, J.-M., IGLESIAS, G., AND GAGALOWICZ, A. Piecewise surface flattening for non-distorted texture mapping. Computer Graphics (Proceedings of SIGGRAPH 91) 25, 4, 237-246.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258882</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BONEY, J. S. D. Multiresolution sampling procedure for analysis and synthesis of texture images. Computer Graphics (Proceedings of SIGGRAPH 97), 361- 368.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>288224</ref_obj_id>
				<ref_obj_pid>288216</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[CIGNONI, P., MONTANI, C., ROCCHINI, C., AND SCOPIGNO, R. A general method for preserving attribute values on simplified meshes. In IEEE Visualization (1998), pp. 59-66.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[DISCHLER, J. M., GHAZANFARPOUR, D., AND FREYDIER, R. Anisotropic solid texture synthesis using orthogonal 2D views. Computer Graphics Forum 17, 3 (1998), 87-96.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>851569</ref_obj_id>
				<ref_obj_pid>850924</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[EFROS, A. A., AND LEUNG, T.K. Texture synthesis by non-parametric sampling. In IEEE international Conference on Computer Vision (Sept. 1999).]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218447</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[FLEISCHER, K., LAIDLAW, D., CURRIN, B., AND BARR, A. Cellular texture generation. Computer Graphics (Proceedings of SIGGRAPH 95), 239-248.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[GHAZANFARPOUR, D., AND DISCHLER, J.-M. Generation of 3D texture using multiple 2D models analysis. Computer Graphics Forum 15, 3 (1996), 311-324.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218446</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[HEEGER, D. J., AND BERGEN, J. R. Pyramid-based texture analysis/synthesis. Computer Graphics (Proceedings of SIGGRAPH 95), 229-238.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280930</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[LEVY, B., AND MALLET, J.-L. Non-distorted texture mapping for sheared triangulated meshes. Computer Graphics (Proceedings of SIGGRAPH 98), 343- 352.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166120</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[MAILLOY, J., YAHIA, H., AND VERROUSY, A. Interactive texture mapping. Computer Graphics (Proceedings of SIGGRAPH 93), 27-34.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>276885</ref_obj_id>
				<ref_obj_pid>276884</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[MILENKOVIC, V. J. Rotational polygon containment and minimum enclosure. Proc. of the 14th Annual Symp. on Computational Geometry, ACM (June 1998).]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311561</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[NEYRET, F., AND CANI, M.-P. Pattern-based texturing revisited. Computer Graphics (Proceedings of SIGGRAPH 99), 235-242.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218458</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[PEDERSEN, H.K. Decorating implicit surfaces. Computer Graphics (Proceedings of SIGGRAPH 95), 291-300.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237268</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[PEDERSEN, H. K. A framework for interactive texturing operations on curved surfaces. Computer Graphics (Proceedings of SIGGRAPH 96), 295-302.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325247</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[PERLIN, K. An image synthesizer. Computer Graphics (Proceedings of SIGGRAPH 85) 19, 3,287-296.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>344935</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[SANDER, P., GU, X., GORTLER, S., HOPPE, H., AND SNYDER, J. Silhouette clipping. Computer Graphics (Proceedings of SIGGRAPH 2000).]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122749</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[TURK, G. Generating textures for arbitrary surfaces using reaction-diffusion. Computer Graphics (Proceedings of SIGGRAPH 91) 25, 4, 289-298.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122750</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[WITKIN, A., AND KASS, M. Reaction-diffusion textures. Computer Graphics (Proceedings of SIGGRAPH 91) 25, 4, 299-308.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237267</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[WORLE~, S.P. A cellular texture basis function. Computer Graphics (Proceedings of SIGGRAPH 96), 291-294.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Xu, Y., Guo, B., AND SHUM, H.-Y. Chaos mosaic: Fast and memory efficient texture synthesis. Tech. Rep. MSR-TR-2000-32, Microsoft Research, 2000.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Lapped Textures Emil Praun Adam Finkelstein Hugues Hoppe Princeton University Microsoft Research http://www.cs.princeton.edu/~{emilp,af} 
http://research.microsoft.com/~hoppe  Figure 1: Four different textures pasted on the bunny model. The 
last picture illustrates changing local orientation and scale on the body. Abstract We present a method 
for creating texture over an arbitrary surface mesh using an example 2D texture. The approach is to identify 
interesting regions (texture patches) in the 2D example, and to repeatedly paste them onto the surface 
until it is completely covered. We call such a collection of overlapping patches a lapped texture. It 
is rendered using compositing operations, either into a traditional global texture map during a preprocess, 
or directly with the surface at runtime. The runtime compositing approach avoids resampling artifacts 
and drastically reduces texture memory requirements. Through a simple interface, the user speci.es a 
tangential vector .eld over the surface, providing local control over the texture scale, and for anisotropic 
textures, the orientation. To paste a texture patch onto the surface, a surface patch is grown and parametrized 
over texture space. Speci.cally, we optimize the parametrization of each surface patch such that the 
tangential vector .eld aligns everywhere with the standard frame of the texture patch. We show that this 
optimization is solved ef.ciently as a sparse linear system. Keywords: Texture synthesis, texture mapping, 
parametrizations. URL: http://www.cs.princeton.edu/gfx/proj/lapped tex 1 Introduction This paper describes 
a method for creating a texture over an arbitrary surface mesh, using a given example 2D texture (Figure 
1). Computer graphics applications often use surface textures to give the illusion of .ne detail without 
explicit geometric modeling. There exist several schemes for synthesizing texture on the 2D plane based 
on example texture. However, these methods cannot be readily extended to cover surfaces of arbitrary 
topology because such surfaces lack continuous parametrizations over the plane. Permission to make digital 
or hard copies of part or all of this work or personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. SIGGRAPH 2000, New Orleans, 
LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 Our approach to this problem relies on the observation 
that even though a global parametrization may not exist, any manifold surface may be locally mapped onto 
the 2D plane. We repeatedly paste small regions of the example texture (texture patches) onto parts of 
the mesh that can be easily mapped (surface patches), until the entire mesh is covered with a series 
of overlapping texture patches, called collectively a lapped texture. The perceptibility of seams is 
reduced by applying alpha-blending at the edges of the pasted texture patches. The user provides local 
control over the orientation and scale of the synthesized texture by specifying a tangential vector .eld 
over the mesh surface. For each paste operation, we form a surface patch on the mesh by growing a region 
homeomorphic to a disc. The surface patch is parametrized into texture space so as to locally align the 
axes of the texture patch with the surface tangential vector .eld. We cast this as an optimization of 
a least squares functional. Unlike other approaches that rely on an explicit fairness functional for 
minimizing texture distortion, the fairness comes from the underlying vector .eld. Our optimization is 
therefore extremely fast, as it only involves solving a sparse linear system. We have tested our method 
by applying more than sixty textures over seven models, and found that it works surprisingly well. Figure 
1 shows a bunny model covered using four different example textures. The .fth bunny has the same texture 
as the fourth, but the tangential vector .eld has been modi.ed on the body. As shown in Figure 7, our 
scheme applies a variety of isotropic and anisotropic textures over complex, organic surfaces in a natural 
fashion. Lapped textures embody the simple idea of texturing a surface with overlapping patches. Ef.cient 
implementation of this idea is straightforward. Precomputation of patch placements takes only minutes, 
and the resulting texture displays in real time. Little human effort is necessary to delineate a texture 
patch and specify texture direction and scale. Because a single texture patch is instantiated many times 
over the mesh, a large surface may be covered using a compact texture footprint. Finally, the method 
extends trivially to bump maps, displacement maps, and other surface appearance .elds. The contributions 
of this paper are: (1) the idea of covering an arbitrary surface using overlapping copies of a texture 
patch, (2) a fast parametric optimization that allows control over the local texture orientation and 
scale, (3) a simple scheme for specifying the vector .eld necessary for this control, and (4) a method 
of rendering the texture in real time through the composition of precomputed, overlapping surface patches. 
 2 Previous work Previous methods for generating texture by example work mainly on images, and are dif.cult 
to extend to surfaces. Conversely, the methods proposed for texturing surfaces are mainly procedural 
in nature, and therefore dif.cult to control. None of these methods provide convenient control over local 
orientation and scale. We address the problem of generating texture by example on complex surfaces, while 
providing control over local orientation and scale. Texture synthesis The problem of synthesizing textures 
in 2D has been studied extensively. Heeger and Bergen [8] perturb a noisy image in order to match the 
histograms of the original image and its steerable pyramid representation with the corresponding histograms 
of the generated image. They report good results with stochastic textures, but cannot produce realistic 
replicas of more structured textures such as bricks. DeBonet [2] synthesizes texture from a wide variety 
of input images by shuf.ing elements in the Laplacian pyramid representation. Recently, Efros and Leung 
[5] proposed a scheme based on non-parametric sampling. They grow the texture one pixel at a time, creating 
for each target pixel a probability distribution based on windows of the original image. The scheme is 
relatively slow, but produces impressive results for a large class of input images. Xu et al. [20] developed 
a 2D texture synthesis scheme based on the random motion of image blocks; their prototype inspired us 
to consider texturing surface meshes using overlapping patches. Many 2D texture synthesis schemes can 
be extended to 3D using solid textures: the color function is de.ned over a volume, and then sampled 
on the surface. Heeger and Bergen [8] and Dischler et al. [4, 7] propose schemes that apply spectral 
and histogram analysis to produce a volume-.lling function. A different class of methods synthesizes 
texture based on a few parameters, instead of by example. Pioneering work in this area by Perlin [15] 
and subsequent extensions by Worley [19] generate a color de.ned over the volume using a noise function. 
In a different approach, reaction-diffusion and biologic evolution can procedurally texture surfaces 
in 3D directly [6, 17, 18]. The main drawback of these methods is the dif.culty of controlling the input 
parameters in order to get the desired visual result. This parameter-to-visual-appearance feedback loop 
is further hindered by the long simulation times necessary to produce an image. Also, these methods explore 
a limited space of possible textures. Of these techniques, perhaps the most suitable for the problem 
we address is that of Fleischer et al. [6]; while their method is designed to produce geometric detail, 
it might be adapted so that each of their cell models carries a texture patch, and gets aligned locally 
to a direction .eld. Neyret and Cani [12] introduce a scheme for texturing a mesh with a given set of 
triangular texture tiles. They partition the mesh into a coarse tiling, where each triangular tile is 
as close to equilateral as possible. Each surface tile is assigned one of the given texture tiles, subject 
to continuity constraints across tile boundaries. The user chooses a priori the number of distinct tile 
boundaries, and creates a set of tiles that match all possible boundary conditions. Surface parametrization 
for texture mapping In seeking a surface parametrization for texture mapping, the primary objective is 
to minimize distortion. The usual strategy is to de.ne an energy functional for the mapping, and to try 
to minimize it. In early work in this area, Bennis et al. [1] .atten a series of user­de.ned patches 
via optimization. Maillot et al. [10] propose as a deformation functional the Green-Lagrange tensor from 
elasticity theory. They discretize the problem by meshing the surface, placing on each mesh edge a spring 
of nonzero rest length. To prevent surface buckling, they also measure squared differences of signed 
face areas. They minimize the energy functional using a nonlinear optimization procedure, which makes 
the method relatively slow. L´evy and Mallet [9] propose a functional that combines orthogo­nality and 
homogeneous spacing of isoparametric curves. Although the resulting functional is nonlinear, it can be 
minimized iteratively as a sequence of linear problems by solving alternately for the s and the t parametric 
coordinates. Pedersen [13, 14] extends texture mapping and cut-and-paste operations to a broader class 
of surfaces (including implicit surfaces). He positions a meshed version of a square domain onto the 
surface and allows the vertices of this regular parametrization to slide over the surface, while minimizing 
the energy of an associated mesh of springs. Arbitrarily-shaped regions of the sliding patch are cut 
and pasted, using curve-drawing on the surface to de.ne alpha masks. The sliding patch is translated, 
rotated, scaled and even warped through the manipulation of the control points de.ning the parametrization. 
This is probably the closest work to our own. The main difference is that Pedersen s system is designed 
to be an interactive paint system while ours is aimed at the automatic texturing of objects. Another 
difference is our use of a tangential vector .eld to guide the texture orientation and scale during the 
parametrization process.  3 Our Approach Our approach consists of identifying a set of broad features 
from the example texture ( texture patches ), and then repeatedly pasting them onto surface patches grown 
on the mesh, until the mesh is completely covered. Here is an overview of our procedure: Cut texture 
patches from input texture (Figure 2a, § 3.1) Specify direction and scale .elds over mesh (Figure 2a, 
§ 3.2) Repeat Select random texture patch T Select random uncovered location L for paste Grow surface 
patch S around L to size of T (§ 3.3) Flatten S over T (§ 3.4) Record paste operation (§ 3.5) Update 
face coverages (§ 3.6) Until the mesh is covered (Figure 2b, § 3.6) For recording the paste operations 
and rendering the .nal model, we propose two completely different approaches which are presented and 
compared in Section 4. 3.1 Creating the texture patches For highly structured textures, the texture 
patch boundaries should avoid cutting across important features, so as to minimize obtrusive seams in 
the resulting lapped texture. For example, in a brick texture, the patch boundary should not intersect 
the bricks but instead follow the grout. The user manually outlines image regions using a commercial 
drawing package. This outlining process is facilitated by gradient-seeking tools such as the edge .nder 
in Microsoft PhotoDraw. The .rst six examples in Figure 7 were created this way. For homogeneous or stochastic 
textures, the outline of the texture patch is less important, so we use a set of prede.ned shapes, such 
as a circle or an irregular splotch . In either case, the texture patch is assigned an alpha mask that 
falls off near the patch boundary (e.g. over a distance of 3 pixels).  3.2 Establishing local orientation 
and scale The desired orientation and scale for the texture are speci.ed over the mesh as a tangential 
vector .eld. Speci.cally, each mesh face is assigned a vector T within its plane (Figure 2). The direction 
of T is the desired the texture up direction, and the magnitude of T is the desired local uniform scaling. 
A simple choice for the tangential vector .eld is to project the global up direction onto the surface, 
and then normalize the resulting tangent vector. More often, the user needs more control (a) Input: mesh, 
vector .eld, texture; (b) Output: covered mesh  Figure 2: Process overview. The inputs are a triangle 
mesh, a tangential vector .eld de.ned on this mesh, and a texture cut into patches. The patches are pasted 
onto the mesh until it is covered. over the vector .eld. With our interface, the user speci.es vectors 
at a few faces. We interpolate vectors at the remaining faces using Gaussian radial basis functions, 
where radius is de.ned as distance over the mesh, as computed using Dijkstra s algorithm. The user has 
control over the spatial extent and weight of each basis function. We convert the tangential vector T 
at each face into a tangential basis (S, T) by using the right direction S = T × N , where N is the unit 
face normal (see Figure 5). The user could alternatively specify this tangential basis directly for local 
control of a full linear transform for the texture, including shearing and non-uniform stretching. However, 
we have found this additional .exibility to be unnecessary in our experiments. For isotropic textures, 
the orientations of the paste operations are unimportant. For such cases, we only specify a scaling .eld 
over the mesh (and in most cases it is a global constant). During the growth of surface patches (described 
in the next section), a local orientation .eld is instead de.ned through propagation. That is, the center 
face of the patch is assigned an arbitrary tangent direction, and the direction of each subsequently 
added face is computed by projecting onto its face plane the average direction of neighboring faces already 
in the patch.  3.3 Growing the surface patch For each paste operation, we grow a surface patch on the 
mesh by successively adding faces, and form an initial parametrization f of the surface patch into texture 
space. The parametrization f : R3 . R2 is a piecewise linear map speci.ed by texture coordinates assigned 
to the surface patch vertices. The growth of the surface patch is guided so that its image through f 
fully covers the texture patch. We next present the details. First, a random point is chosen on a triangle 
face that is not yet fully textured, using the coverage test presented in Section 3.6. (Early in the 
process we give higher priority both to areas of high curvature and to discontinuities in the direction 
.eld where the parametrization is dif.cult with the hope that any distorted regions will be covered 
over later.) The triangle is mapped to texture space such that the chosen point maps to the texture patch 
center, and the face tangential basis (S, T) maps to the texture space standard axes (s , t ). Next, 
the surface patch is grown around this seed face, one (a) Align only patch center (b) Align locally to 
.eld Figure 3: Continuity of the texture direction is improved when the optimization aligns the entire 
surface patch with the vector .eld rather than just its center. circular dependency in the speci.ed texture 
S direction (in red). Second, the patch is only grown over an edge if the edge is still par­tially inside 
the texture patch, since there is no point in growing the surface patch beyond the image region that 
will be pasted. This test is made ef.cient using the polygonal hull representation discussed later in 
this section. Third, patch growth is stopped when distor­tion becomes excessive, which can occur in surface 
areas with high curvature. In these cases where we are unable to extend the surface patch to fully cover 
the texture patch, the pasted texture lacks an alpha falloff across one or more edges. We .nd that the 
few notice­able artifacts from these hard edges are less objectionable than distorted texture. When a 
face is added to the patch, the newly added vertex is assigned an initial parametrization using the heuristic 
in [10]. Speci.cally, for each face that contains the new vertex and an already mapped edge, we predict 
the parametrization of the new vertex by extending the edge with a triangle similar to the face in 3D. 
The new vertex is assigned the centroid of these predictions. Note that for texture pasting, we do not 
prevent the patch from folding or wrapping over itself in texture space. To determine if we need to grow 
the patch over a given edge, we test to see if the edge intersects the interior of a polygonal hull of 
the texture patch. We .rst construct a polygon with vertices at all the boundary pixels (in blue), and 
then we conservatively simplify it, allowing it only to grow (red outline). Sander et al. [16] construct 
conservative approximations of polyhedral surface meshes using progressive hull simpli.cation. We adapt 
their construction to the 2D setting. Simpli.cation is done using a sequence of edges collapses, but 
with the constraint that the resulting vertex lie within the correct half-spaces of the previous model. 
In 3D this involves linear programming, but in 2D it reduces to just the 3 cases illustrated in Figure 
4. The simpli.cation operations are prioritized according to the area they add to the polygon interior. 
Operations that would give rise to self-intersections are disallowed. Once the surface patch stops growing, 
we optimize the map f as discussed in the next section. The optimization may sometimes uncover parts 
of the texture patch. When this occurs, we further grow the patch and optimize again.  dT A f f(A)^ 
f(B) t dS s f(S) f(C) Figure 5: The optimization process minimizes the differences (dS, dT ) between 
the texture coordinate axes (s , t ) and the images (f(S), f(T)) in texture space of the user-speci.ed 
vectors (S, T).  3.4 Optimizing the surface patch parametrization Having formed a surface patch together 
with its initial parametriza­tion f, we optimize fso as to locally match both the orientation and scale 
of the texture with the vector .eld de.ned on the surface. More precisely, we attempt to match the images 
of the surface tan­gent vectors (S, T) with the texture coordinate axes (s , t ). Figure 3 shows the 
importance of aligning the vector .eld over the whole patch, as opposed to just at its center. For each 
mesh face f = {A, B, C}, the up vector T lies within the face plane. We can therefore express it using 
its barycentric coordinates with respect to the vertex positions1: T = aA + ßB + .C , where a+ ß+ .=0. 
Since the map fis linear over the face, the image f(T) is therefore a linear function of the vertex parametrizations 
f(A), f(B), and f(C). As shown in Figure 5, we de.ne the difference vector dT = af(A)+ ßf(B)+ .f(C) - 
t , and we do likewise for the difference vector dS. Our optimization problem is to .nd the vertex parametrizations 
that minimize the least squares functional dS2+ dT 2. f The minimum of this function is unique up to 
a translation. We therefore add a positional constraint to .x the location of the patch center. The exact 
solution to the minimization problem only requires solving a sparse linear system. Since we begin with 
a reasonable approximation of the solution, we use a conjugate­gradient iterative solver, which is faster 
than an explicit solver like Gaussian elimination. Note that the functional does not include any explicit 
fairness term to penalize distortion in the parametrization. Instead, continuity of the parametrization 
across mesh edges relies on the continuity of the user-provided tangential vector .eld. Unlike many local 
edge-spring functionals, our functional does not have local minima when the face orientations .ip, and 
thus avoids buckling artifacts. Finally, the parametrization is well-behaved even though the patch boundary 
is left unconstrained. 3.5 Recording the paste operations The paste operation sends image samples from 
the texture patch onto the surface patch using f-1. Section 4 presents two schemes for recording these 
paste operations. 1Given a set of points P in general position, any vector in the af.ne subspace spanning 
P is uniquely expressed as a linear combination of P, and these barycentric coordinates sum to zero. 
 3.6 Computing face coverages To decide where to apply paste operations (Section 3.3), we need to know 
if a face is already fully covered by texture. We answer this query using a rasterization algorithm. 
After each paste operation, we render all the patch faces in an offscreen buffer, with the parametrization 
from Section 3.4. (In the rare case that the patch overlaps itself in texture space, we compute the coverage 
in several passes, for subsets of non-overlapping faces.) Each face in the patch is rendered using all 
paste operations that overlap it. We use the R and G color channels to store the face ID, and the B channel 
to accumulate the opaque regions of the paste operations. To determine the coverage of a face, we divide 
the number of covered pixels (in the B channel) by the number of pixels in the triangle. For each faces 
that is not fully covered, we remember an uncovered point inside the face, in order to start a future 
paste operation centered there. When all faces are fully covered, we are done pasting.  4 Texture storage 
and rendering We propose two approaches for representing the textured object. The .rst approach constructs 
a traditional surface parametrization using a texture atlas, and pre-renders the lapped texture into 
this atlas. The second and more interesting approach uses the hardware graphics pipeline to composite 
the texture patches at runtime. Rendering with a texture atlas Previous approaches for storing texture 
on meshes use a texture atlas (e.g. [3, 10, 16]). An atlas is a collection of charts that map regions 
of the surface to subsets of a texture unit square, such that no two distinct surface points map to the 
same texture point (see Figure 6b). Ideally, the charts should have low parametric distortion, and should 
have uniform resolution across the mesh. To build an atlas, we use a method similar to Maillot et al. 
[10]. We segment the mesh into regions by bounding each region s space of face normals, .atten each region 
using relaxation, and let the user arrange the .attened pieces. To grow and .atten the surface regions, 
we use the algorithm described in Section 3.3 (for the case of isotropic textures), but with two additional 
constraints. We require the normals of the added faces to be within a certain angle from the one of the 
center face. And, we prevent overlaps by checking for intersections using a spatial hash table. The next 
step is to arrange the chart images inside the unit square. Packing a set of non-convex polygons into 
a given 2D domain is a well-studied problem in computational geometry known as pants packing [11] due 
to its application in the clothing industry. Since the problem is NP-hard, an exact solution cannot generally 
be computed. Heuristic algorithms for arranging on the order of a hundred polygons with no initial layout 
produce signi.cantly worse results than a trained human. Therefore, we let the user manually arrange 
the charts. The atlas is represented using sets of texture coordinates at the mesh vertices. During a 
preprocess, the texture paste operations are composited into the atlas charts. At runtime, the mesh is 
rendered using ordinary texture mapping. (a) Runtime pasting (b) Rendering with a texture atlas Figure 
6: Comparison of our two texture representations. The atlas representation is more portable but may have 
sampling problems.  Figure 7: For the .rst 6 pictures the user speci.ed the texture patch boundary and 
a vector .eld over the mesh. The remaining pictures are examples of isotropic textures, and are generated 
automatically. The two upper dinosaurs are frames from an animation of the tail (see video). Runtime 
pasting of lapped textures Our preferred approach for rendering the lapped texture is to record the parameters 
for each paste operation (texture patch index, list of surface patch faces, and texture coordinates for 
vertices), and to render these surface patches at runtime with alpha blending enabled. With runtime pasting, 
each face of the model is rendered several times, once for each surface patch to which the face belongs. 
This increases the load on the graphics system, in terms of both geometry processing and rasterization. 
To reduce this overhead, during a preprocess we remove for a given paste operation any faces that are 
completely occluded by subsequent paste operations; such faces are detected through a rasterization algorithm 
as in Section 3.6. With this optimization, the average number of times that each face gets rendered ranges 
between 1.5 and 3.2, depending on the texture scale and the model. Graphics systems have begun to support 
multitexturing, whereby the rasterizer can directly evaluate a complicated shading expression involving 
several texture lookups (recently, as many as four). Although not used in our current prototype, this 
multitexturing capability could reduce the number of times each face is rendered. Since our .attening 
process may produce texture coordinates outside the unit square, we use texture coordinate clamping. 
For proper interaction of blending and mipmapping, the texture patch must have a border of transparent 
pixels at all mipmap levels .ner than 2x2. We therefore build the mipmap levels explicitly. Tradeoffs 
of rendering approaches Compared to the atlas, runtime pasting requires little texture memory, since 
it only involves storing the initial texture patches (and often there is only one such patch). As in 
ordinary 2D texture tiling, large amounts of apparent texture can be created with little actual texture 
memory usage. The randomness of our surface patch construction makes repetitiveness of the texture less 
obvious than with ordinary 2D tiling. Runtime pasting offers better visual quality because it does not 
suffer from several problems inherent to a texture atlas (Figure 6): Sampling. The use of an atlas adds 
one more resampling step, thus inherently degrading the texture image quality. The quality is improved 
by increasing the texture resolution, but this further reduces the ratio of apparent texture to texture 
memory usage.  Discontinuities. The tri-linear interpolation .lter used to sample the texture does not 
match exactly at chart boundaries.  Mipmapping problems. At coarser mipmap levels, distinct charts of 
the atlas are wrongly averaged together.  The disadvantages of the runtime pasting approach are the 
following. As discussed above, rendering is likely slower since faces are drawn multiple times (though 
multitexturing may alleviate this). Also, the storage format for the model is somewhat less portable 
since it involves textures with alpha. Finally, each face must be rendered with different textures in 
a speci.c order, and some rendering systems may not guarantee the order in which overlapping polygons 
are drawn. Figure 8: Limitations of our method: (a) Strong low-frequency components, (b) Boundary mismatch, 
(c) Singularity point.  5 Results Several textured meshes are shown in Figure 7. In all the examples, 
the synthesized texture is generated from a single texture patch extracted from the example texture (except 
the brick foot which uses two). It takes the user about 15 minutes to create a non­trivial direction 
.eld for the meshes shown. The growth and parametrization of the patches takes between 20 seconds and 
6 minutes to compute on a 733 MHz Pentium III with a GeForce graphics card. Except where noted, all of 
the examples shown in the paper and the accompanying video tape use runtime pasting rather than the atlas 
approach. The meshes used in this paper average 5000 faces. Since each face is rendered on average two 
or three times, all of the lapped textures shown here display in real time. For homogeneous textures 
we can use a generic texture patch boundary (e.g., the splotch), and switch between different texture 
examples instantaneously at runtime. Figure 8 shows some of the limitations of lapped textures. Patch 
seams become noticeable when the texture patch has strong low frequency components. Seams are also apparent 
when viewing highly structured textures up close. For anisotropic textures, the user-speci.ed vector 
.eld generally has singularity points, since of course one cannot smoothly comb a hairy ball. Sometimes, 
visual artifacts are caused by poor vector .eld sampling near these points due to the presence of large 
faces; we reduce such artifacts by locally subdividing the mesh. 6 Summary and future work We have introduced 
lapped textures, a new approach for covering arbitrary triangle meshes with an example 2D texture. The 
approach is to apply copy/paste operations to cover the surface with overlapping texture patches, using 
alpha blending to hide seams. The paste operation relies on a new, fast, robust .attening scheme that 
simultaneously minimizes distortion of the texture and matches local orientation and scale speci.ed by 
the user. Our scheme proves to be highly practical, allowing the creation of complex textures on meshes 
at a fraction of the user effort required by 3D painting. Lapped textures can be used as a starting point 
for further manual painting (e.g. for unique details such as the mouth and eyes of a bunny). This work 
suggests a number of areas for future investigation: Fine-tuning patch placement. It may be bene.cial 
to .ne­tune the placement of the surface patches so that sharp texture features align across patch boundaries 
(Figure 8b). Initially, we anticipated that this process would be absolutely necessary to make patch 
boundaries unobtrusive; we were pleasantly surprised to .nd that the method works quite well without 
this embellishment. Nonetheless, we still believe it could enhance the results. Greater automation. We 
believe that methods to reduce user interaction would make this system even more practical. For example, 
we have considered automatic texture patch creation, automatic equalization of low-frequency information 
in these patches (Figure 8a), and automatic direction .eld construction using surface curvatures. Other 
texture types. Within the lapped texture framework, we are now exploring several other types of textures, 
including animated, volumetric and view-dependent textures.  Acknowledgements We thank Harry Shum for 
demonstrating a prototype 2D texture synthesis scheme [20] that largely inspired this work. Thanks to 
Viewpoint DataLabs and Stanford University for the surface meshes, and Michael Cohen and Rico Malvar 
for proposing the name lapped textures . Emil Praun was supported in part by a Microsoft Research internship. 
The research of Adam Finkelstein is supported by an NSF CAREER Award and an Alfred P. Sloan Fellowship. 
 References [1] BENNIS, C., V ´ ESIAS, G., AND GAGALOWICZ, A. Piecewise EZIEN, J.-M., IGL´ surface .attening 
for non-distorted texture mapping. Computer Graphics (Proceedings of SIGGRAPH 91) 25,4,237 246. [2] BONET, 
J. S. D. Multiresolution sampling procedure for analysis and synthesis of texture images. Computer Graphics 
(Proceedings of SIGGRAPH 97), 361 368. [3] CIGNONI,P., MONTANI, C., ROCCHINI, C., AND SCOPIGNO,R. A general 
method for preserving attribute values on simpli.ed meshes. In IEEE Visualization (1998), pp. 59 66. 
[4] DISCHLER, J. M., GHAZANFARPOUR, D., AND FREYDIER, R. Anisotropic solid texture synthesis using orthogonal 
2D views. Computer Graphics Forum 17, 3 (1998), 87 96. [5] EFROS, A. A., AND LEUNG, T. K. Texture synthesis 
by non-parametric sampling. In IEEE International Conference on Computer Vision (Sept. 1999). [6] FLEISCHER, 
K., LAIDLAW, D., CURRIN, B., AND BARR, A. Cellular texture generation. Computer Graphics (Proceedings 
of SIGGRAPH 95), 239 248. [7] GHAZANFARPOUR, D., AND DISCHLER, J.-M. Generation of 3D texture using multiple 
2D models analysis. Computer Graphics Forum 15, 3 (1996), 311 324. [8] HEEGER, D. J., AND BERGEN, J. 
R. Pyramid-based texture analysis/synthesis. Computer Graphics (Proceedings of SIGGRAPH 95), 229 238. 
[9] L´ B., AND MALLET, J.-L. Non-distorted texture mapping for sheared EVY, triangulated meshes. Computer 
Graphics (Proceedings of SIGGRAPH 98), 343 352. [10] MAILLOT, J., YAHIA, H., AND VERROUST, A. Interactive 
texture mapping. Computer Graphics (Proceedings of SIGGRAPH 93),27 34. [11] MILENKOVIC, V. J. Rotational 
polygon containment and minimum enclosure. Proc. of the 14th Annual Symp. on Computational Geometry, 
ACM (June 1998). [12] NEYRET,F., AND CANI, M.-P. Pattern-based texturing revisited. Computer Graphics 
(Proceedings of SIGGRAPH 99), 235 242. [13] PEDERSEN, H. K. Decorating implicit surfaces. Computer Graphics 
(Proceedings of SIGGRAPH 95), 291 300. [14] PEDERSEN, H. K. A framework for interactive texturing operations 
on curved surfaces. Computer Graphics (Proceedings of SIGGRAPH 96), 295 302. [15] PERLIN, K. An image 
synthesizer. Computer Graphics (Proceedings of SIGGRAPH 85) 19, 3, 287 296. [16] SANDER,P., GU, X., GORTLER, 
S., HOPPE, H., AND SNYDER, J. Silhouette clipping. Computer Graphics (Proceedings of SIGGRAPH 2000). 
[17] TURK, G. Generating textures for arbitrary surfaces using reaction-diffusion. Computer Graphics 
(Proceedings of SIGGRAPH 91) 25,4,289 298. [18] WITKIN, A., AND KASS, M. Reaction-diffusion textures. 
Computer Graphics (Proceedings of SIGGRAPH 91) 25,4,299 308. [19] WORLEY, S. P. A cellular texture basis 
function. Computer Graphics (Proceedings of SIGGRAPH 96), 291 294. [20] XU,Y., GUO, B., AND SHUM, H.-Y. 
Chaos mosaic: Fast and memory ef.cient texture synthesis. Tech. Rep. MSR-TR-2000-32, Microsoft Research, 
2000.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>344990</article_id>
		<sort_key>471</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>53</seq_no>
		<title><![CDATA[Seamless texture mapping of subdivision surfaces by model pelting and texture blending]]></title>
		<page_from>471</page_from>
		<page_to>478</page_to>
		<doi_number>10.1145/344779.344990</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=344990</url>
		<abstract>
			<par><![CDATA[<p>Subdivision surfaces solve numerous problems related to the geometry of character and animation models. However, unlike on parametrised surfaces there is no natural choice of texture coordinates on subdivision surfaces. Existing algorithms for generating texture coordinates on non-parametrised surfaces often find solutions that are locally acceptable but globally are unsuitable for use by artists wishing to paint textures. In addition, for topological reasons there is not necessarily any choice of assignment of texture coordinates to control points that can satisfactorily be interpolated over the entire surface. We introduce a technique, <italic>pelting</italic>, for finding both optimal and intuitive texture mapping over almost all of an entire subdivision surface and then show how to combine multiple texture mappings together to produce a seamless result.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[curves & surfaces]]></kw>
			<kw><![CDATA[physcially based animation]]></kw>
			<kw><![CDATA[texture mapping]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010243</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Appearance and texture representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P58196</person_id>
				<author_profile_id><![CDATA[81100284377]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Piponi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MVFX, a division of Manex Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP18001553</person_id>
				<author_profile_id><![CDATA[81100409357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[George]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Borshukov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MVFX, a division of Manex Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>134084</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[David Baraff and Andrew Witkin. Dynamic simulation of non-penetrating flexible bodies. In SIGGRAPH '92 Conference Proceedings, Annual Conference Series, pages 303-308. ACM SIGGRAPH, July 1992.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122744</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Chakib Bennis, Jean-Marc Vezien, and Gerard Iglesias. Piecewise surface flattening for non-distorted texture mapping. In SIGGRAPH '91 Conference Proceedings, Annual Conference Series, pages 237-246. ACM SIGGRAPH, July 1991.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[E. Bier and K. Sloan. Two-part texture mapping. IEEE Computer Graphics and Applications, pages 40-53, September 1986.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360353</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[J.F. Blinn and M.E. Newell. Texture and reflection in computer generated images. Communications of the ACM,19,10, pages 542-547, October 1976.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>907242</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[E. Catmull. A subdivision algorithm for the computer display of curved surfaces. PhD thesis, University of Utah, December 1974.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[E. Catmull and J. Clark. Recursively generated b-spline surfaces on arbitrary topological meshes. Computer Aided Design, 10(6):350-355, 1978.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280826</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Tony DeRose, Michael Kass, and Tien Truong. Subdivision surfaces in character animation. In SIGGRAPH '98 Conference Proceedings, Annual Conference Series, pages 85-94. ACM SIGGRAPH, July 1998.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134031</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Helaman Ferguson, Alyn Rockwood, and Jordan Cox. Topological design of sculptured surfaces. In SIGGRAPH '92 Conference Proceedings, Annual Conference Series, pages 149- 156. ACM SIGGRAPH, July 1992.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83821</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[J. Foley and A. van Dam. Computer Graphics: Principles and Practice. Addison-Wesley, 1990.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97911</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Pat Hanrahan. A language for shading and lighting calculations. In SIGGRAPH '90 Conference Proceedings, Annual Conference Series, pages 289-298. ACM SIGGRAPH, August 1990.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97903</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Pat Hanrahan and Paul E. Haeberli. Direct WYSIWYG painting and texturing on 3D shapes. In SIGGRAPH '90 Conference Proceedings, Annual Conference Series, pages 215-223. ACM SIGGRAPH, August 1990.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>13027</ref_obj_id>
				<ref_obj_pid>13021</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[P. S. Heckbert. Survey of texture mapping. IEEE Computer Graphics and Applications, pages 215-223, August 1990.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218458</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Hans K~ling Pedersen. Decorating implicit surfaces. In SIG- GRAPH '95 Conference Proceedings, Annual Conference Series, pages 291-300. ACM SIGGRAPH, August 1995.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237270</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[V. Krishnamurthy and M. Levoy. Fitting smooth surfaces to dense polygon meshes. In SIGGRAPH '96 Conference Proceedings, Annual Conference Series, pages 313-324. ACM SIGGRAPH, aug 1996.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280828</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[A. W. F. Lee, W. Sweldens, P. Schroder, L. Cowsar, and D. Dobkin. Maps: Multiresolution adaptive parameterization of surfaces. In SIGGRAPH '98 Conference Proceedings, Annual Conference Series, pages 95-104. ACM SIGGRAPH, 1998.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192187</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Peter Litwinowicz and Gavin Miller. Efficient techniques for interactive texture placement. In SIGGRAPH '94 Confernce Proceedings, Annual Conference Series, pages 119-122. ACM SIGGRAPH, 1994.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[S.D. Ma and H. Lin. Optimal texture mapping. In EURO- GRAPHICS '88, September 1988.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166120</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Jerome Maillot, Hussein Yahia, and Anne Verroust. Interactivetexture mapping. In SIGGRAPH '93 Conference Proceedings, Annual Conference Series, pages 27-34. ACM SIG- GRAPH, August 1993.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[C. R. F. Maunder. Algebraic Topology. Dover, 1996.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325246</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[D. Peachey. Solid texturing of complex surfaces. Computer Graphics 19(3), pages 253-260, July 1984.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[M. Spivak. A Comprehensive Introduction to Differential Geometry. Publish or Perish, Inc., 1979.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280945</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Jos Stam. Exact evaluation of catmull-clark subdivision surfaces at arbitrary parameter values. In SIGGRAPH '98 Conference Proceedings, Annual Conference Series. ACM SIG- GRAPH, 1998.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SeamlessTexture Mapping of Subdivision Surfaces by Model Pelting and Texture Blending Dan Piponi and 
George Borshukov MVFX, a division of Manex Entertainment Abstract Subdivision surfaces solve numerous 
problems related to the ge­ometry of character and animation models. However, unlike on parametrised 
surfaces there is no natural choice of texture coordi­nates on subdivision surfaces. Existing algorithms 
for generating texture coordinates on non-parametrised surfaces often .nd solu­tions that are locally 
acceptable but globally are unsuitable for use by artists wishing to paint textures. In addition, for 
topological rea­sons there is not necessarily any choice of assignment of texture coordinates to control 
points that can satisfactorily be interpolated over the entire surface. We introduce a technique, pelting, 
for .nd­ing both optimal and intuitive texture mapping over almost all of an entire subdivision surface 
and then show how to combine multiple texture mappings together to produce a seamless result. Keywords: 
Curves &#38; Surfaces, Texture Mapping, Physically Based Animation 1 Introduction Subdivision surfaces 
[5], [6] possess unique advantages over tradi­tionally used NURBS surfaces that make them ideal for animated 
models. Complex models can be modeled with the ef.ciency of polygons and the smoothness of NURBS and 
other spline surfaces. Multi-limbed characters can be created using a single, contigu­ous mesh when using 
subdivision surfaces, something that is, in most cases, not possible using a single NURBS or other type 
of parametrised surface. The most obvious advantage of using a single mesh to de.ne, for example, a humanoid 
model, can be seen during the deformation or rigging process. Keeping closed seams and tangency between 
multiple NURBS patches, trimmed or untrimmed, becomes very dif.cult in areas where multiple joints exert 
in.uence over vertices of these multiple surfaces. With subdivision surfaces, the use of a single polygon 
mesh to de.ne the head, torso, arms, and legs makes skeletal binding and weighting a much simpler and 
more effective process. However, in order to apply surface detail with 2 dimensional tex­ ture maps [4],[12] 
the surface must be parametrised. Spline patches come with a natural parametrisation, but there is no 
such natural Permission to make digital or hard copies of part or all of this work or personal or classroom 
use is granted without fee provided that copies are not made or distributed for profit or commercial 
advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, 
to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or 
a fee. SIGGRAPH 2000, New Orleans, LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 parametrisation 
on subdivision surfaces. In fact, as subdivision sur­faces may have arbitrary topologies, there may in 
fact be no global parametrisation over the entire surface. In this paper we look at an approach to providing 
parametrisations for regions of subdivi­sion surfaces and combining these into one continuous colour 
map on the surface. Much of our discussion also applies to polygonal models however subdivision surfaces 
have some extra complica­tions making them more instructive to consider.  2 Background Traditionally 
the way to represent the geometry of CG creatures is through the use of NURBS [9]. Unfortunately, there 
are many dif.culties with arranging separate NURBS together so that they de.ne a single smooth surface. 
The problems of arranging NURBS to abut against each other with a shared tangent are well known to those 
who have worked with commercial modelling packages. However, even when these problems have been solved 
there are dif.culties with arranging textures to vary continuously over the seams between patches. Renderers 
need to sample texture values over a region in order to interpolate cleanly, anti-alias, and to cal­culate 
quantities like normals for correct lighting of displacement maps. As a result even when the geometry 
of NURBS join cleanly with a common tangent the surface detail can appear to have tears along seams. 
Subdivision surfaces can provide a way to solve all of these problems resulting in texture and displacements 
that are continuous over the entire surface. Subdivision surfaces are constructed by starting with a 
polygonal mesh that forms a manifold,mns . A subdivision process is applied to this mesh to produce a 
new higher resolution mesh msn and then this process is iterated to form meshes msummnlm, e e that better 
ap­proximate the limiting mesh mm [6], [7]. The subdivision scheme is chosen in such a way that under 
reasonable conditions the limit­ing surface is smooth. In this paper we consider only Catmull-Clark surfaces 
but the techniques extend naturally to other subdivision schemes. (For clar­ity of exposition we consider 
mainly Catmull-Clark surfaces de­rived from quadrilateral meshes. After the .rst Catmull-Clark re­.nement 
any polygonal mesh becomes a quadrilateral mesh anyway [22].) Unfortunately, assigning texture coordinates 
to a subdivision sur­face can be a dif.cult problem. Firstly, there is a purely topological problem. 
There is no way to assign texture coordinates to a sphere, for example, in such a way that the assignment 
is continuous and every point is assigned a unique pair of texture coordinates. Even worse -by the Borsuk-Ulam 
theorem [19] it is guaranteed that there are antipodal points on the sphere that are mapped to the same 
tex­ture coordinates. So attempting to map a sphere using one global coordinate system is doomed to catastrophic 
failure. This problem is well known from cartography. Similar results hold for models with topologies 
other than that of the sphere. There are a number of ways to sidestep this problem: Use texture mappings 
with discontinuities Change the topology of the surface maps Use 3d solid textures rather than 2d textures 
Don t seek a global texture map but instead use multiple local textures We consider these options in 
turn. It is dif.cult to ensure that texture mappings with discontinuities look seamless because ren­derers 
require continuous functions in order to calculate normals for bump mapping or sample areas for anti-aliasing 
(this is similar to the aforementioned problem with joining NURBS surfaces). There are a number of approaches 
to changing the topology of the underlying objects. One approach to changing the model is to break it 
into individual pieces that can each be globally texture mapped -unfortunately this gives the problem 
of arranging that there are no seams between pieces. One can arrange for pieces to overlap but now we 
run into many of the same problems found with NURBS and additionally we lose the advantages of being 
able to work with a single model. Another approach can be best illus­trated with a cylinder. A cylinder 
may be formed by rolling a sheet of paper, so that the opposite ends of the paper overlap. However, this 
overwrapping still requires breaking and rebuilding the original geometry and does not generalise easily 
to all topologies. Using a solid texture for the entire model is inef.cient and cum­bersome. It is far 
easier for artists to paint 2d texture maps with familiar painting tools. We chose the fourth approach 
because we found we could work uniformly with surfaces of any topology and always guarantee a .­nal colour 
map that was completely smooth. In addition we found the task of managing multiple overlapping textures 
to be much eas­ier than that of managing overlapping geometries because the for­mer has no impact on 
the work of modellers or animators. (Note that from a mathematical viewpoint these approaches may be 
simi­lar.) We broke the problem down into two stages: Finding a way to texture regions on the model in 
a fashion that is intuitive for artists to work with and Joining together different regions in a completely 
seamless way Assigning texture coordinates to a subdivision surface consists of two parts -assigning 
values to control vertices on the zeroth re.ne­ment of the mesh and a technique for interpolating these 
values over the surface. Therefore, we need to consider interpolation schemes. Stam [22] describes a 
set of basis functions that parametrise patches on the subdivision surface corresponding to each quadri­lateral 
in the zeroth re.nement. This parametrisation de.nes a pair of coordinates on the surface to Tmmee,rpaTmmletr 
. Using these values we may bilinearly interpolate scalar values assigned at the vertices of the quadrilateral 
to de.ne scalar .elds over each patch and hence over the entire surface. Denote the set of functions 
on the sub­division surface obtained in this way by Lf((mn nh . Unfortunately, these .elds are only guaranteed 
to be rn differentiable 1 on the in­terior of each patch. They have discontinuities in the .rst derivative 
along the edges and so texture coordinates chosen from Le((mn ih can result in texture maps that look 
unattractive. (Actually, these discontinuities are not necessarily a problem for single images, but they 
have a tendency to look bad when animated.) According to [7] another technique for assigning texture 
coordi­nates to points on a subdivision surface is to consider scalar values assigned at vertices to 
be coordinates in an extra dimension. For s means continuous. e means the th derivative exists and is 
con­tinuous. Figure 1: The domain of control of n example a vertex at ((xmty mtzh with texture coordinates 
((8m,hh is con­sidered to be a point ((xmny mtz)m8mthh . We then apply the subdivision process in the 
dimensionally extended space -5 dimensional in this example. Each point in the subdivision surface ((xe'mny 
'mnz'm8e'mth 'ih de.nes a point in 3D ((x'tmnye'(mnzu'h with texture coordinates ((8'tmtht'h . Under 
reasonable conditions these .elds are rn over the entire sub­division surface. Denote the set of functions 
produced this way by d ((mm Ohd . Our goal is to produce texture coordinate functions that lie in ((mm 
Oh . It is useful to introduce the idea of the domain of control of a point n on m . This is the set 
of control points that d determine the value atn of a scalar .eld in ((mn Oh . In Figure 1 we illustrate 
the domain of control of an example point -the vertices inside and on the boundary of the red region 
form the domain of control. 3 Texture Mapping Creatures We now turn to the problem of assigning surface 
detail to a sub­division surface. Much of what we discuss applies equally well to polygonal models, however, 
the interpolation scheme that we use for subdivision surface texturing causes some extra complica­tions 
that are not present in polygonal models. Typical methods included projection, solid textures [20] or 
two-part texturing [3]. These methods can be dif.cult to use with complex surfaces, so we chose to use 
2 dimensional texture mapping using texture coordi­nate .elds de.ned over the surface. We now look at 
the problem of choosing how to assign texture coordinates to control vertices. Even on a region that 
has no topological obstruction to being con­tinuously texture mapped there is no natural choice of such 
texture coordinates. This makes the problem of choosing texture coordi­nates for subdivision surfaces 
similar to that for implicit surfaces or for surfaces derived from point coulds [13], [14],[15],[8]. 
One useful characteristic of any technique for assigning texture coordi­nates to a surface is to ensure 
that distances between points on the surface are represented accurately in the distances between the 
cor­responding points in the texture space. (For surfaces with non-zero implicit curvature there will 
always be some distortion of distances [21].) This makes it much easier for artists to work with a 2 
dimen­sional texture because the image they paint closely re.ects how it will look in 3D. It also ensures 
uniformity in the look of the surface and ef.ciency in the storage of texture information. One approach 
to achieving this is to de.ne a function that represents the extent to which distances are distorted 
by the texture mapping function. One can then .nd the mapping that minimises this parametric dis­tortion. 
Ma [17] describes an algorithm that minimises a discrete Figure 2: A pelting frame. The surrounding circle 
is the frame and the dashed lines represent the springs instantiated around the edge of the model and 
along the edges of the model.  approximation to a distortion measure on a grid. There are a num­ber 
of different candidates for a measure of distortion such as the Green-Lagrange deformation tensor [18], 
[2]. An elementary ap­proach is to consider the lengths of the edges of the polygon mesh in texture space 
and make the functional the sum of the squares of the deviations of the lengths of the edges.   x (tL 
e 1 hn (1) ( e n ) L where L is the length of edge e and 1 is the distance between the two points in 
texture space. (Dividing by L provides a certain amount of invariance with respect to the details of 
the tesselation. Splitting an edge into two pieces by adding a new vertex in the mid­ x dle will make 
no difference to the minimum value of .) We can then use standard black box minimisation techniques to 
minimise this function. Unfortunately, this is not necessarily adequate. This is a complex search problem 
with many local minima -especially for complex surfaces. Sometimes buckling [18] can occur during minimisa­tion. 
In addition, minima do not necessarily preserve approximate symmetries in the original model that artists 
involved in a produc­tion desire. Sometimes there are other factors that artists feel are more important 
than minimising distortion including aesthetic as­pects of the resulting texture mappings. In short, 
many of the exist­ing techniques provide mappings that are locally good but globally over the whole model 
the result is not suitable for production work -especially with complex models. We now consider a way 
to deal with all of these issues. There are a number of different types of terms that can be added to 
Equation 1 in order to eliminate problems, for example, terms to prevent faces .ipping [18] or angular 
variation. We use a dif­ferent approach. Equation 1 is formally identical to the total en­ergy of a collection 
of springs that obey Hooke s law except that the dynamics is described only in 2D. Temporarily we will 
work with texture coordinates that lie in a 3D space. The resulting en­ergy can now be minimised by deriving 
equations of motion from Equation 1, adding damping terms, and running a dynamics solver until a steady 
state is achieved [1]. Suppose we have a model that is topologically equivalent to a disk. We can add 
springs to the bound­ary of this disk with the opposing ends of these springs attached to a surrounding 
.xed frame. (See Figure 2 for an example using a hemisphere model.) What this does is ensure that the 
edges of region to be textured Figure 3: An example cut are spread out and it pulls the entire system 
out of local minima. The fact that we have extended the system to 3 dimensions is signif­icant. If the 
orientation of a face in the 2D texture space is opposite to what it should be at the global minimum 
then it will often be caught in a local minimum because in order to .ip it over one or two vertices will 
have to actually pass through other edges in the polygon in order to reach the global minimum and hence 
pass through a higher energy state. In 3D no such .ipping is neces­sary because each face can smoothly 
rotate to its correct orienta­tion. With suitable choice of spring length and stiffness the system quickly 
achieves a state where it is almost .at and now texture co­ordinates may be applied by planar projection. 
There is, of course, another way of looking at this: we are carry­ing out a simulation of a procedure 
traditionally used to stretch out animal hides for tanning. As a result the user can intuitively inter­act 
with the minimisation procedure and if starting with a creature model can arrange to end up with a texture 
that is in the form a pelt. We call this technique pelting. The dynamics model we have just described 
is very elementary. In principle it could be made much more sophisticated -in particu­lar we could add 
extra springs arranged so as to minimise shearing of the texture. However we have found that in practice 
that the strength of the pelting process is that it quickly .nds a global tex­ture mapping that is an 
excellent starting point for a subsequent local re.nement. For this reason the precise details of the 
dynamics used are relatively unimportant. 4 The Pelting Procedure in Detail For a model topologically 
equivalent to a sphere we need only make one cut in order to have a model that is topologically equivalent 
to a disk. Before the cutting procedure is described there is an impor­tant point to make: these cuts 
are for the sake of generating texture coordinates only. After our procedure is described we will show 
how texture coordinates can be transferred back to the original un­cut model in a way that provides smooth 
colour mapping. A cut is a collection of edges that form a connected tree. In our pelting tool the user 
indicates this set of edges and the soft­ware operates on the model by duplicating the edges and vertices 
as in .gure Figure 3. Although we have illustrated point 2, say, as duplicated into distinct points 2a 
and 2b, these two points actually have the same location in 3D space. The cutting process changes the 
connectivity of the faces, edges and vertices only. The edges around the cut form a topological circle 
and we walk around this circle connecting each vertex (via a connecting spring) to a corre­sponding point 
in the frame. We ensure that the stiffness of each springd and the spacing between the connecting points 
on the frame is proportional to the spacing between the other ends of the springs in the original model. 
A spring is then instantiated along each edge in the original model with stiffness proportional to the 
edge length. A mass is placed at each vertex and the outer frame is held .xed. (We chose to use an equal 
mass at each vertex. One could choose the mass of each vertex in such a way that the mass density per 
unit area of the model is approximately constant but in practice we found this unnecessary.) The user 
can then adjust various param­eters such as spring stiffness and frame con.guration and run the simulation 
in order to .nd a pelt that is optimal. For alternative topologies we can use more than one frame but 
the principles are the same. The user can make alterations to the geometry of the frame before or even 
during the minimisation. Our .rst test was with a model for a rat-like creature. (See Figure 4 (a,c).) 
Once the model is stretched out .at the user can make manual adjustments if required and then texture 
coordinates can be applied using planar projection. In addition, further modi.cations may be carried 
out using algorithms more suited to local re.nement. (See Figure 4 (b).) These assigned texture coordinates 
may now be trans­ferred back to the original model. Note that as the vertices in the original cut were 
duplicated there is no unique way to transfer tex­ture coordinates back to the original model at these 
points. We as­sign texture coordinates at these points completely arbitrarily (we see later why this 
is acceptable). At this stage what we have is the original model with texture coordinates assigned to 
all vertices. Unfortunately, in the region of the cut we will have an unsightly seam because (1) we have 
assigned arbitrary texture coordinates to some vertices and (2) we will be interpolating between texture 
co­ordinates that correspond to opposite sides of the .attened model. 5 Blending Textures Fortunately, 
there is a way to deal with this problem. We can split up the surface into a number of pieces, each of 
which is a topo­logical disk, and then generate texture coordinates for each piece. (Again we stress 
that although we are talking about splitting up the model our ultimate goal is to leave the original 
model intact.) If we split up the model into non-overlapping pieces we face the problem that the domain 
of control of points near the boundary may in fact lie in a neighbouring region. So we choose to work 
with a set of overlapping pieces. There is a standard technique used in topology for dealing with functions 
on arbitrary topological surfaces that are de.ned on over­lapping pieces. Suppose we have a manifold 
m and that we can express it as a union of open subsets 2  m L L( U (( is some indexing set.) We choose 
the sets UL so that they are all topologically disks. Suppose we can .nd texture coordinates 8L and hL 
on each L so that any function /L on L can be written LLLULU as /((8mthh (Each U is called a chart and 
the whole collection is called an atlas [18], [19].) Suppose we have a set of continuous functions bLt 
mMn-R such that each bL is zero outside of L and U bL(nhe Lt u  2An open set is one where for every 
point there is a real such that it has a ball of radius around it that is completely contained in the 
set. As we are dealing with compact manifolds we are guaranteed that we can write M as a union of a .nite 
number of sets. [19] (Traditionally such a set of bL is called a partition of unity [19] but we will 
call thebL blend functions.) Given any continuous function / on m the functions bL/ have the property 
 bL// L( and each bL/ is zero outside of L . This gives a way to write any ULL function on m as a sum 
of functions of 8 and h . Conversely, given a collection of continuous functions /L LndR we can construct 
U the function /bL/L L( u and it is guaranteed to be continuous over the entire manifold. This gives 
a way to build a continuous function on the whole manifold out of the individual pieces. More generally 
by choosing blend functions that are themselves r we can build functions that are r out of individual 
pieces. Our problem now is to build blend functions out of the functions d contained in the sets Le((mn 
Oh and ((mn Oh because these are the functions we are able to specify using control points. Although 
it makes sense to use smooth functions for texture co­ordinates -when blending two different but similar 
functions it is acceptable to use functions in Le((mh . This is because, as we will see later, we will 
be blending textures that are approximately equal on the overlap making the discontinuity in the derivative 
of the tex­ture small. We will now consider in detail how to construct blend functions for the pelting 
procedure above. In our scheme we will use two regions: U and U . will cor­ snUs respond to the pelt 
that we have described above and is a region Un that we will call the patch. We will have two sets of 
texture coordi­nates: ((8m,hh and ((8mthh . ((8 ,hh are the coordinates derived by  ssnnss the pelting 
process above and ((8nmthnh are discussed below. In this case we have only two blend functions b(nh andb(nheb(nh 
. sns We discuss .rst how we construct control points for the blend function b because this scheme determines 
how we choose the s patch . Un In any overlap between the pelt and the patch regions we require a polygon 
over which both of the texture maps interpolate correctly because this gives a region over which we may 
smoothly switch from one texture map to another. Consider Figure 5. This repre­sents a region spanning 
both sides of a cut (which is represented by ebh ). As discussed above we assign arbitrary values for 
the pelt texture coordinates to e and h and texture coordinates derived from the planar projection of 
the pelt at p , b , c , d , / , g , h and d (and similarly on the opposite site of the cut). The domain 
of control of the points on the interior of polygonc,hodtd includes e andh so we know that the pelt texture 
coordinates interpolated here will have an arbitrary component and so will not represent the planar projection. 
The polygon closest to the cut that is a good representation is btgnhoc . So in order to make the patch 
as small as possible we make the patch extend out as far as p and / so that on bbgnhoc the patch texture 
coordinates are also good. We now de.ne the blend function to be the function in Le((mn ih that has control 
value 1 at p , b , / , andg and zero on c , d , eh ,, d , h . Over bbgnhoc the blend function can be 
used to smoothly interpolate between the two textures and outside this region either one or the other 
texture mapping will be used. In this way we de.ne a smooth transition between two texture maps by de.ning 
one extra scalar .eld and more importantly: not chang­ing the geometry of the model in any way. In practice 
we design our textures so that the two different texture maps map into textures so that onbbgnhoc they 
approximate each other. When the topology of the faces is more complex than in this dia­gram a more complex 
de.nition is required. Call the set of vertices and edges in the cut rs . De.ne r fores Cm to be the 
loop of  Figure 5: Regions of the blend function edges and vertices that goes completely around r n 
, containing n the smallest area, that does not share any vertices with it. We are now in a position 
to de.ne the patch -it is the region enclosed byreo . Vertices within and including r are given blend 
functon value m and those outside and including reC are given value e . It can be seen that the region 
between r and rC contains no points whose domain of control intersects with the cut. If we choose models 
with all vertices at least trivalent (at least three edges meeting at each vertex) and with every polygon 
a quadrilateral (after one Catmull-Clark subdivision all faces are quadrilateral) then there are guaran­teed 
to be no isolated vertices between r and rC . This means that every vertex between and including r and 
reC can be unambigu­ously assigned blend value m or e by the above method. On the other hand if other 
polygons are present we can interpolate blend values to vertices between r and reC by any reasonable 
scheme ­ e.g. by taking the average of the values at nearest neighbours. We can assign texture coordinates 
to the patch by detaching it from the full model and applying the pelting procedure to it. We then transfer 
the texture coordinates assigned to the patch back onto the full model. This mapping is extended to the 
entire model to produce ((8nm,hnh -assigning texture coordinates arbitrarily to points not in the patch. 
At this stage we now have two global sets of texture coordinates, and a global blending function that 
seamlessly blends between texture maps rendered using these coordinates in such a way that the regions 
with arbitrary texture coordinates are invisible. The procedure for painting the models typically went 
as follows: once texture coordinates have been assigned to the model through the pelt int is roughly 
painted directly in a 3d paint package. The re­sulting low resolution texture map was then transferred 
to a 2d paint package where additional detail was applied. Using the texture co­ordinates derived from 
the pelting process we are able to render the patch region of the model after it has been .attened by 
pelting. As a result of the scheme described above the edges of the patch will be textured correctly. 
The .nal task of the artist is then to paint the small region of the patch in the immediate vicinity 
of the cut. Implementing the texture blending is easily achieved with mod­ern shaders [10] or with multipass 
texturing. Although we have concentrated on the implementation details for subdivision surfaces a simpli.ed 
scheme can be implemented for use with polygon models using linear interpolation of texture coordinates. 
In Figure 5 we mark the region textured solely by the patch tex­ture in red, that using solely the pelt 
texture in green and the region in which blending occurs in yellow. In Figure 6 we show how these regions 
extend over a large portion of our example model. Figure 6: The regions of the blend function over the 
example rat model 6 Results We implemented the system to set up the dynamics using MEL scripting and 
C++ plug-ins within Alias-Wavefront s Maya and the dynamics solver used was the standard one in Maya. 
On an R10000 SGI O2 it took under a minute to .nd a good texture mapping for the 11000 polygon rat model 
and we were able to .nd good texture mappings for models such as the alien creature example with no dif­.culty. 
We used Flesh from Digits n Art software to carry out lo­cal re.nement on the .nal texture mappings and 
the initial 3d model painting. (See Figure 7.) We were able to implement the rendering using Pixar s 
Renderman which supports Catmull-Clark surfaces, multiple scalar attributes at control vertices, and 
both the Le((mn Oh d and ((mh interpolation schemes. The resulting mappings were found to be very suitable 
for work with 2D paint packages such as Adobe Photoshop or with scans of real paint and brush. For the 
alien creature example we added geometric detail to the 6500 poly­gon model by using a displacement map 
painted using the same set of texture coordinates.  7 Conclusions and Future Research We have found 
a way to texture map the entire surface of subdi­vision surfaces without modifying the model. We have 
used a dy­namics solver to .nd optimal texture mappings and have used two techniques to .nd a good global 
solution: solving in 3D instead of 2D and using springs and a frame to impose boundary conditions. We 
have also introduced a scheme for blending smoothly between different texture mappings on a subdivision 
surface. We would like to reimplement the dynamics more directly and ef.ciently (possibly replacing it 
with a more specialised statics solver that could work in real time) so that we can allow users to manipulate 
texture maps directly and easily as if they were rub­ber sheets [16]. We would also like to implement 
the entire blend­ing process in an automatic way so that the user simply paints in 3D on a single model 
and the software automatically updates the (possibly multiple) texture map contributing to each point 
[11]. Although the technique has only been applied in the case of two regions it generalises to multiple 
regions by using multiple blend functions. We would like to .nd good ways to enable the user to de.ne 
such regions with various types of cuts allowing extremely complex topologies to be dealt with ef.ciently. 
One dif.culty is with .nding ways to make the cutting and blend­ing scheme compatible with level of detail 
(LOD) techniques. LOD methods substitute low resolution proxies for models whenever  Figure 4: (a) The 
initial setup for the rat pelting (b) The .nal texture mapping after local re.nement (c) 3 stages during 
the pelting simulation high resolution details are not required. Unfortunately, the patch is itselfa 
high resolution detail and so model simpli.cation tends . to remove it entirely. For very low resolution 
substitutes where the seam is too small to be noticeable we have used pelting combined with the earlier 
mentioned overwrapping approach in production. We have had great success using pelting within a production 
en­vironment. Animators and modelers have found it easy to generate texture mappings and 2D and 3D painters 
have found the mappings extremely well adapted to their needs. We believe the future of character animation 
lies with subdivision surfaces and pelting has played a major role in allowing us to move along this 
path.  8 Acknowledgements Thanks to John Tissavary, Brett Hartshorne, Dan Klem, Mauricio Baiocchi among 
the artists who worked with and constructively criticised our work. A special thanks goes to Devorah 
Petty for both modelling and painting the alien model. Thanks also to Peter Plevritis for testing our 
approach in production.  References [1] David Baraff and Andrew Witkin. Dynamic simulation of non-penetrating 
.exible bodies. In SIGGRAPH 92 Confer­ence Proceedings, Annual Conference Series, pages 303 308. ACM 
SIGGRAPH, July 1992. [2] Chakib Bennis, Jean-Marc V´ezien, and G´erard Igl´esias. Piecewise surface .attening 
for non-distorted texture map­ping. In SIGGRAPH 91 Conference Proceedings, Annual Conference Series, 
pages 237 246. ACM SIGGRAPH, July 1991. [3] E. Bier and K. Sloan. Two-part texture mapping. IEEE Com­puter 
Graphics and Applications, pages 40 53, September 1986. [4] J.F. Blinn and M.E. Newell. Texture and re.ection 
in com­puter generated images. Communications of the ACM,19,10, pages 542 547, October 1976. [5] E. Catmull. 
A subdivision algorithm for the computer display of curved surfaces. PhD thesis, University of Utah, 
December 1974. [6] E. Catmull and J. Clark. Recursively generated b-spline sur­faces on arbitrary topological 
meshes. Computer Aided De­sign, 10(6):350-355, 1978. [7] Tony DeRose, Michael Kass, and Tien Truong. 
Subdivision surfaces in character animation. In SIGGRAPH 98 Confer­ence Proceedings, Annual Conference 
Series, pages 85 94. ACM SIGGRAPH, July 1998. [8] Helaman Ferguson, Alyn Rockwood, and Jordan Cox. Topo­logical 
design of sculptured surfaces. In SIGGRAPH 92 Con­ference Proceedings, Annual Conference Series, pages 
149 156. ACM SIGGRAPH, July 1992. [9] J. Foley and A. van Dam. Computer Graphics: Principles and Practice. 
Addison-Wesley, 1990. [10] Pat Hanrahan. A language for shading and lighting calcula­tions. In SIGGRAPH 
90 Conference Proceedings, Annual Conference Series, pages 289 298. ACM SIGGRAPH, Au­gust 1990. [11] 
Pat Hanrahan and Paul E. Haeberli. Direct WYSIWYG paint­ing and texturing on 3D shapes. In SIGGRAPH 90 
Confer­ence Proceedings, Annual Conference Series, pages 215 223. ACM SIGGRAPH, August 1990. [12] P. 
S. Heckbert. Survey of texture mapping. IEEE Computer Graphics and Applications, pages 215 223, August 
1990. [13] Hans Køhling Pedersen. Decorating implicit surfaces. In SIG-GRAPH 95 Conference Proceedings, 
Annual Conference Se­ries, pages 291 300. ACM SIGGRAPH, August 1995. [14] V. Krishnamurthy and M. Levoy. 
Fitting smooth surfaces to dense polygon meshes. In SIGGRAPH 96 Conference Pro­ceedings, Annual Conference 
Series, pages 313 324. ACM SIGGRAPH, aug 1996. [15] A. W. F. Lee, W. Sweldens, P. Schr¨oder, L. Cowsar, 
and D. Dobkin. Maps: Multiresolution adaptive parameteriza­tion of surfaces. In SIGGRAPH 98 Conference 
Proceedings, Annual Conference Series, pages 95 104. ACM SIGGRAPH, 1998. [16] Peter Litwinowicz and Gavin 
Miller. Ef.cient techniques for interactive texture placement. In SIGGRAPH 94 Confer­ence Proceedings, 
Annual Conference Series, pages 119 122. ACM SIGGRAPH, 1994. [17] S.D. Ma and H. Lin. Optimal texture 
mapping. In EURO-GRAPHICS 88, September 1988. [18] J´er ome Maillot, Hussein Yahia, and Anne Verroust. 
Interac­tive texture mapping. In SIGGRAPH 93 Conference Proceed­ings, Annual Conference Series, pages 
27 34. ACM SIG-GRAPH, August 1993. [19] C. R. F. Maunder. Algebraic Topology. Dover, 1996. [20] D. Peachey. 
Solid texturing of complex surfaces. Computer Graphics 19(3), pages 253 260, July 1984. [21] M. Spivak. 
A Comprehensive Introduction to Differential Ge­ometry. Publish or Perish, Inc., 1979. [22] Jos Stam. 
Exact evaluation of catmull-clark subdivision sur­faces at arbitrary parameter values. In SIGGRAPH 98 
Con­ference Proceedings, Annual Conference Series. ACM SIG-GRAPH, 1998.  Figure 7: (a) Pelt texture 
for rat (b) Seam texture (c) Rendered rat using only pelt texture (note the seam) (d) Rendered rat using 
both textures (e) The .nal rat (f) Pelt texture for alien (g) Rendered alien (6500 polygons)  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>345009</article_id>
		<sort_key>479</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>54</seq_no>
		<title><![CDATA[Fast texture synthesis using tree-structured vector quantization]]></title>
		<page_from>479</page_from>
		<page_to>488</page_to>
		<doi_number>10.1145/344779.345009</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=345009</url>
		<abstract>
			<par><![CDATA[<p>Texture synthesis is important for many applications in computer graphics, vision, and image processing. However, it remains difficult to design an algorithm that is both efficient and capable of generating high quality results. In this paper, we present an efficient algorithm for realistic texture synthesis. The algorithm is easy to use and requires only a sample texture as input. It generates textures with perceived quality equal to or better than those produced by previous techniques, but runs two orders of magnitude faster. This permits us to apply texture synthesis to problems where it has traditionally been considered impractical. In particular, we have applied it to constrained synthesis for image editing and temporal texture generation. Our algorithm is derived from Markov Random Field texture models and generates textures through a deterministic searching process. We accelerate this synthesis process using tree-structured vector quantization.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[compression algorithms]]></kw>
			<kw><![CDATA[image processing]]></kw>
			<kw><![CDATA[texture synthesis]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010395</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image compression</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010243</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Appearance and texture representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14165448</person_id>
				<author_profile_id><![CDATA[81452594229]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Li-Yi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wei]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University, Gates Computer Science Building, Stanford, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15037068</person_id>
				<author_profile_id><![CDATA[81100593780]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Levoy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University, Gates Computer Science Building, Stanford, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>237276</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[A. C. Beers, M. Agrawala, and N. Chaddha. Rendering from compressed textures. Proceedings of SIGGRAPH 96, pages 373-378, August 1996.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[R Brodatz. Textures: A Photographic Album for Artists and Designers. Dover, New York, 1966.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>247</ref_obj_id>
				<ref_obj_pid>245</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[R J. Burt and E. H. Adelson. A multiresolution spline with application to image mosaics. A CM Transactions on Graphics, 2(4):217-236, Oct. 1983.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258882</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[J. S. De Bonet. Multiresolution sampling procedure for analysis and synthesis of texture images. In T. Whitted, editor, SIGGRAPH 97 Conference Proceedings, Annual Conference Series, pages 361-368. ACM SIGGRAPH, Addison Wesley, Aug. 1997.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311560</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[J. Dorsey, A. Edelman, J. Legakis, H. W. Jensen, and H. K. Pedersen. Modeling and rendering of weathered stone. Proceedings of SIGGRAPH 99, pages 225- 234, August 1999.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>851569</ref_obj_id>
				<ref_obj_pid>850924</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[A. Efros and T. Leung. Texture synthesis by non-parametric sampling. In international Conference on Computer Vision, volume 2, pages 1033-8, Sep 1999.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>128857</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[A. Gersho and R. M. Gray. Vector Quantization and Signal Compression. Kluwer Academic Publishers, 1992.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[R. Haralick. Statistical image texture analysis. In Handbook of Pattern Recognition and image Processing, volume 86, pages 247-279. Academic Press, 1986.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218446</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[D. J. Heeger and J. R. Bergen. Pyramid-Based texture analysis/synthesis. In R. Cook, editor, SIGGRAPH 95 Conference Proceedings, Annual Conference Series, pages 229-238. ACM SIGGRAPH, Addison Wesley, Aug. 1995.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237264</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[A.N. Hirani and T. Totsuka. Combining frequency and spatial domain information for fast interactive image noise removal. Computer Graphics, 30(Annual Conference Series):269-276, 1996.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>841730</ref_obj_id>
				<ref_obj_pid>839286</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[H. Igehy and L. Pereira. Image replacement through texture synthesis. In international Conference on image Processing, volume 3, pages 186-189, Oct 1997.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>179432</ref_obj_id>
				<ref_obj_pid>179426</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[H. Iversen and T. Lonnestad. An evaluation of stochastic models for analysis and synthesis of gray scale texture. Pattern Recognition Letters, 15:575-585, 1994.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237270</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[V. Krishnamurthy and M. Levoy. Fitting smooth surfaces to dense polygon meshes. Proceedings of SIGGRAPH 96, pages 313-324, August 1996. ISBN 0-201-94800-1. Held in New Orleans, Louisiana.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>344849</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[M. Levoy, K. Pulli, B. Curless, S. Rusinkiewicz, D. Koller, L. Pereira, M. Ginzton, S. Anderson, J. Davis, J. Ginsberg, J. Shade, and D. Fulk. The Digital Michelangelo Project: 3D scanning of large statues. To appear in Proceedings of SIGGRAPH 2000.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[T. Malzbender and S. Spach. A context sensitive texture nib. In Proceedings of Computer Graphics international, pages 151-163, June 1993.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[MIT Media Lab. Vision texture, http://www-white.media.mit.edu/vismod/- imagery/VisionTexture/vistex.html.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>265189</ref_obj_id>
				<ref_obj_pid>265182</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[S. Nene and S. Nayar. A simple algorithm for nearest neighbor search in high dimensions. IEEE Transactions on Pattern Analysis and Machine intelligence, 19:989-1003, 1997.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>2319557</ref_obj_id>
				<ref_obj_pid>2318961</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[R. Paget and I. Longstaff. Texture synthesis via a noncausal nonparametric multiscale Markov random field. IEEE Transactions on image Processing, 7(6):925- 931, June 1998.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>927216</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[A. C. Popat. Conjoint Probabilistic Subband Modeling. Phi) thesis, Massachusetts Institute of Technology, 1997.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[K. Popat and R. Picard. Novel cluster-based probability model for texture synthesis, classification, and compression. In Visual Communications and image Processing, pages 756-68, 1993.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134071</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[M. Segal, C. Korobkin, R. van Widenfelt, J. Foran, and R E. Haeberli. Fast shadows and lighting effects using texture mapping. Computer Graphics (Proceedings of SIGGRAPH 92), 26(2):249-252, July 1992.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[E. Simoncelli and J. Portilla. Texture characterization via joint statistics of wavelet coefficient magnitudes. In Fifth international Conference on image Processing, volume 1, pages 62-66, Oct. 1998.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218430</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[J. Stam and E. Fiume. Depicting fire and other gaseous phenomena using diffusion processes. Proceedings of SIGGRAPH 95, pages 129-136, August 1995.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[M. Szummer and R. W. Picard. Temporal texture modeling. In international Conference on image Processing, volume 3, pages 823-6, Sep 1996.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>677177</ref_obj_id>
				<ref_obj_pid>646013</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[L. Wei. Deterministic texture analysis and synthesis using tree structure vector quantization. In XII Brazilian Symposium on Computer Graphics and image Processing, pages 207-213, October 1999.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122750</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[A. Witkin and M. Kass. Reaction-diffusion textures. In T. W. Sederberg, editor, Computer Graphics (SIGGRAPH '91 Proceedings), volume 25, pages 299-308, July 1991.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237267</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[S. R Worley. A cellular texture basis function. In H. Rushmeier, editor, SIG- GRAPH 96 Conference Proceedings, Annual Conference Series, pages 291-294. ACM SIGGRAPH, Addison Wesley, Aug. 1996.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>290092</ref_obj_id>
				<ref_obj_pid>290091</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[S. Zhu, Y. Wu, and D. Mumford. Filters, random fields and maximun entropy (FRAME) - towards a unified theory for texture modeling, international Journal of Computer Vision, 27(2):107-126, 1998.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Fast Texture Synthesis using Tree-structured Vector Quantization Li-Yi Wei Marc Levoy  Stanford University 
 Figure 1: Our texture generation process takes an example texture patch (left) and a random noise (middle) 
as input, and modi.es this random noise to make it look like the given example texture. The synthesized 
texture (right) can be of arbitrary size, and is perceived as very similar to the given example. Using 
our algorithm, textures can be generated within seconds, and the synthesized results are always tileable. 
Abstract Texture synthesis is important for many applications in computer graphics, vision, and image 
processing. However, it remains dif.­cult to design an algorithm that is both ef.cient and capable of 
gen­erating high quality results. In this paper, we present an ef.cient algorithm for realistic texture 
synthesis. The algorithm is easy to use and requires only a sample texture as input. It generates tex­tures 
with perceived quality equal to or better than those produced by previous techniques, but runs two orders 
of magnitude faster. This permits us to apply texture synthesis to problems where it has traditionally 
been considered impractical. In particular, we have applied it to constrained synthesis for image editing 
and temporal texture generation. Our algorithm is derived from Markov Random Field texture models and 
generates textures through a determinis­tic searching process. We accelerate this synthesis process using 
tree-structured vector quantization. _ Gates Computer Science Building, Stanford, CA 94305 e Email: liyiweiulevoy}@graphics.stanford.edu 
WWW: http://graphics.stanford.edu/projects/texture/ Permission to make digital or hard copies of part 
or all of this work or personal or classroom use is granted without fee provided that copies are not 
made or distributed for profit or commercial advantage and that copies bear this notice and the full 
citation on the first page. To copy otherwise, to republish, to post on servers, or to redistribute to 
lists, requires prior specific permission and/or a fee. SIGGRAPH 2000, New Orleans, LA USA &#38;#169; 
ACM 2000 1-58113-208-5/00/07 ...$5.00 Keywords: Texture Synthesis, Compression Algorithms, Image Processing 
 1 Introduction Texture is a ubiquitous visual experience. It can describe a wide variety of surface 
characteristics such as terrain, plants, minerals, fur and skin. Since reproducing the visual realism 
of the physi­cal world is a major goal for computer graphics, textures are com­monly employed when rendering 
synthetic images. These textures can be obtained from a variety of sources such as hand-drawn pic­tures 
or scanned photographs. Hand-drawn pictures can be aesthet­ically pleasing, but it is hard to make them 
photo-realistic. Most scanned images, however, are of inadequate size and can lead to visible seams or 
repetition if they are directly used for texture map­ping. Texture synthesis is an alternative way to 
create textures. Be­cause synthetic textures can be made any size, visual repetition is avoided. Texture 
synthesis can also produce tileable images by properly handling the boundary conditions. Potential applications 
of texture synthesis are also broad; some examples are image de­noising, occlusion .ll-in, and compression. 
The goal of texture synthesis can be stated as follows: Given a texture sample, synthesize a new texture 
that, when perceived by a human observer, appears to be generated by the same underlying stochastic process. 
The major challenges are 1) modeling-how to estimate the stochastic process from a given .nite texture 
sample and 2) sampling-how to develop an ef.cient sampling procedure to produce new textures from a given 
model. Both the modeling and sampling parts are essential for the success of texture synthesis: the visual 
.delity of generated textures will depend primarily on the accuracy of the modeling, while the ef.ciency 
of the sampling procedurewill directly determine the computational cost of texture . generation. In this 
paper, we present a very simple algorithm that can ef­.ciently synthesize a wide variety of textures. 
The inputs consist of an example texture patch and a random noise image with size speci.ed by the user 
(Figure 1). The algorithm modi.es this ran­dom noise to make it look like the given example. This technique 
is .exible and easy to use, since only an example texture patch (usu­ally a photograph) is required. 
New textures can be generated with little computation time, and their tileability is guaranteed. The 
al­gorithm is also easy to implement; the two major components are a multiresolution pyramid and a simple 
searching algorithm. The key advantages of this algorithm are quality and speed: the quality of the synthesized 
textures are equal to or better than those generated by previous techniques, while the computation speed 
is two orders of magnitude faster than those approaches that generate comparable results to our algorithm. 
This permits us to apply our algorithm in areas where texture synthesis has traditionally been considered 
too expensive. In particular, we have extended the algo­rithm to constrained synthesis for image editing 
and motion texture synthesis. 1.1 Previous Work Numerous approaches have been proposed for texture analysis 
and synthesis, and an exhaustive survey is beyond the scope of this pa­per. We brie.y review some recent 
and representative works and refer the reader to [8] and [12] for more complete surveys. Physical Simulation: 
It is possible to synthesize certain sur­face textures by directly simulating their physical generation 
pro­cesses. Biological patterns such as fur, scales, and skin can be modeled using reaction diffusion 
[26] and cellular texturing [27]. Some weathering and mineral phenomena can be faithfully repro­duced 
by detailed simulations [5]. These techniques can produce textures directly on 3D meshes so the texture 
mapping distortion problem is avoided. However, different textures are usually gener­ated by very different 
physical processes so these approaches are applicable to only limited classes of textures. Markov Random 
Field and Gibbs Sampling: Many algo­rithms model textures by Markov Random Fields (or in a differ­ent 
mathematical form, Gibbs Sampling), and generate textures by probability sampling [6, 28, 20, 18]. Since 
Markov Random Fields have been proven to be a good approximation for a broad range of textures, these 
algorithms are general and some of them pro­duce good results. A drawback of Markov Random Field sampling, 
though, is that it is computationally expensive: even small texture patches can take hours or days to 
generate. Feature Matching: Some algorithms model textures as a set of features, and generate new images 
by matching the features in an example texture [9, 4, 22]. These algorithms are usually more ef­.cient 
than Markov Random Field algorithms. Heeger and Bergen [9] model textures by matching marginal histograms 
of image pyra­mids. Their technique succeeds on highly stochastic textures but fails on more structured 
ones. De Bonet [4] synthesizes new im­ages by randomizing an input texture sample while preserving the 
cross-scale dependencies. This method works better than [9] on structured textures, but it can produce 
boundary artifacts if the in­put texture is not tileable. Simoncelli and Portilla [22] generate tex­tures 
by matching the joint statistics of the image pyramids. Their method can successfully capture global 
textural structures but fails to preserve local patterns.  Figure 2: How textures differ from images. 
(a) is a general image while (b) is a texture. A movable window with two different positions are drawn 
as black squares in (a) and (b), with the corresponding contents shown below. Different regions of a 
texture are always perceived to be similar (b1,b2), which is not the case for a general image (a1,a2). 
In addition, each pixel in (b) is only related to a small set of neighboring pixels. These two charac­teristics 
are called stationarity and locality, respectively.  1.2 Overview Our goal was to develop an algorithm 
that combines the advantages of previous approaches. We want it to be ef.cient, general, and able to 
produce high quality, tileable textures. It should also be user friendly; i.e., the number of tunable 
input parameters should be minimal. This can be achieved by a careful selection of the tex­ture modeling 
and synthesis procedure. For the texture model, we use Markov Random Fields (MRF) since they have been 
proven to cover the widest variety of useful texture types. To avoid the usual computational expense 
of MRFs, we have developed a syn­thesis procedure which avoids explicit probability construction and 
sampling. Markov Random Field methods model a texture as a realization of a local and stationary random 
process. That is, each pixel of a texture image is characterized by a small set of spatially neighbor­ing 
pixels, and this characterization is the same for all pixels. The intuition behind this model can be 
demonstrated by the following experiment (Figure 2). Imagine that a viewer is given an image, but only 
allowed to observe it through a small movable window. As the window is moved the viewer can observe different 
parts of the image. The image is stationary if, under a proper window size, the observable portion always 
appears similar. The image is local if each pixel is predictable from a small set of neighboring pixels 
and is independent of the rest of the image. Based on these locality and stationarity assumptions, our 
algo­rithm synthesizes a new texture so that it is locally similar to an example texture patch. The new 
texture is generated pixel by pixel, and each pixel is determined so that local similarity is preserved 
between the example texture and the result image. This synthesis procedure, unlike most MRF based algorithms, 
is completely deter­ministic and no explicit probability distribution is constructed. As a result, it 
is ef.cient and amenable to further acceleration. The remainder of the paper is organized as follows. 
In Section 2, we present the algorithm. In Section 3, we demonstrate synthe­sis results and compare them 
with those generated by previous ap­proaches. In Section 4, we propose acceleration techniques. In Sections 
5 and 6, we discuss applications, limitations, and exten­sions.   (a) (b) (c) (d) Figure 3: Single 
resolution texture synthesis. (a) is the input texture and (b)-(d) show different synthesis stages of 
the output image. Pixels in the output image are assigned in a raster scan ordering. The value of each 
output pixel pis determined by comparing its spatial neighborhoodfe apc with all neighborhoods in the 
input texture. The input pixel with the most similar neighborhood will be assigned to the corresponding 
output pixel. Neighborhoods crossing the output image boundaries (shown in (b) and (d)) are handled toroidally, 
as discussed in Section 2.4. Although the output image starts as a random noise, only the last few rows 
and columns of the noise are actually used. For clarity, we present the unused noise pixels as black. 
(b) synthesizing the .rst pixel, (c) synthesizing the middle pixel, (d) synthesizing the last pixel. 
 2 Algorithm Using Markov Random Fields as the texture model, the goal of the synthesis algorithm is 
to generate a new texture so that each lo­cal region of it is similar to another region from the input 
texture. We .rst describe how the algorithm works in a single resolution, and then we extend it using 
a multiresolution pyramid to obtain im­provements in ef.ciency. For easy reference, we list the symbols 
used in Table 1 and summarize the algorithm in Table 2. Symbol Meaning n w Input texture sample n I Output 
texture image Gwn w Gaussian pyramid built from GIn I Gaussian pyramid built from nwGwG An input pixel 
inor nIGIG An output pixel inor o NGn)G Neighborhood around the pixel GGGN(G) th level of pyramid GN(G 
)dL ) )GN(L )i )G Pixel at leveland position of G RxC,ky(2D) neighborhood containing levels, with size 
RxC at the top level G RxCxD,ky3D neighborhood containing levels, with size RxCxD at the top level Table 
1: Table of symbols 2.1 Single Resolution Synthesis n w The algorithm starts with an input texture sample 
and a white n In Inw random noise. We force the random noiseto look like by n I transformingpixel by 
pixel in a raster scan ordering, i.e. from top (a) (b) (c) Figure 4: Synthesis results with different 
neighborhood sizes. The neigh­borhood sizes are (a) 5x5, (b) 7x7, (c) 9x9, respectively. All images shown 
are of size 128x128. Note that as the neighborhood size increases the result­ing texture quality gets 
better. However, the computation cost also increases.  (a) (b) (c) Figure 5: Causality of the neighborhood. 
(a) sample texture (b) synthesis result using a causal neighborhood (c) synthesis result using a noncausal 
neighborhood. Both (b) and (c) are generated from the same random noise using a 9x9 neighborhood. As 
shown, a noncausal neighborhood is unable to generate valid results. to bottom and left to right. Figure 
3 shows a graphical illustration of the synthesis process. Gn I To determine the pixel value at , its 
spatial neighborhood o NGn) (the L-shaped regions in Figure 3) is compared against all ooNGi i)nwGi 
possible neighborhoods from. The input pixel with GGP the most similarooNGi d)is assigned to . We use 
a simple norm (sum of squared difference) to measure the similarity between the neighborhoods. The goal 
of this synthesis process is to ensure that G the newly assigned pixel will maintain as much local similarity 
nwn I betweenandas possible. The same process is repeated for each output pixel until all the pixels 
are determined. This is akin to putting together a jigsaw puzzle: the pieces are the individual pixels 
and the .tness between these pieces is determined by the colors of the surrounding neighborhood pixels. 
 2.2 Neighborhood o NG) Because the set of local neighborhoods is used as the pri­mary model for textures, 
the quality of the synthesized results will depend on its size and shape. Intuitively, the size of the 
neighbor­hoods should be on the scale of the largest regular texture structure; otherwise this structure 
may be lost and the result image will look too random. Figure 4 demonstrates the effect of the neighborhood 
size on the synthesis results. The shape of the neighborhood will directly determine the qual­ n I ity 
of. It must be causal, i.e. the neighborhood can only contain those pixels preceding the current output 
pixel in the raster scan ordering. The reason is to ensure that each output neighborhood o NGn) will 
include only already assigned pixels. For the .rst few nIooNGn) rows and columns of, may contain unassigned 
(noise) pix­els but as the algorithm progresses all the otherooNGn)will be com­pletely valid (containing 
only already assigned pixels). A non­L+1L e Figure 6: A causal multiresolution neighborhood with size 
5x5,2}. The current level of the pyramid is shown at left and the next lower resolution level is shown 
at right. The current output pixel p, marked as X, is located ata e e l e c, where is the current level 
number and a l k cis its coordinate. At this level of the pyramid the image is only partially complete. 
Thus, we must use the preceding pixels in the raster scan ordering (marked as O). The position of the 
parent of the current pixel, located at a+o e. s., is marked as Y. Since the parent s level is complete, 
the neighborhood can contain pixels around Y, marked by Q. When searching for a match for pixel X, the 
neighborhood vector is constructed that includes the O s, Q s, and Y, in scanline order. causal o NGi), 
which always includes unassigned pixels, is unable to nIn w transform to look like (Figure 5). Thus, 
the noise image is only used when generating the .rst few rows and columns of the output image. After 
this, it is ignored.  2.3 Multiresolution Synthesis The single resolution algorithm captures the texture 
structures by using adequately sized neighborhoods. However, for textures con­taining large scale structures 
we have to use large neighborhoods, and large neighborhoods demand more computation. This problem can 
be solved by using a multiresolution image pyramid [3]; com­putation is saved because we can represent 
large scale structures more compactly by a few pixels in a certain lower resolution pyra­mid level. The 
multiresolution synthesis algorithm proceeds as follows. GwGInw Two Gaussian pyramids, and , are .rst 
built from and nIGI , respectively. The algorithm then transforms from lower to higher resolutions, such 
that each higher resolution level is con­structed from the already synthesized lower resolution levels. 
This is similar to the sequence in which a picture is painted: long and thick strokes are placed .rst, 
and details are then added. Within GIN(G) each output pyramid level , the pixels are synthesized in a 
way similar to the single resolution case where the pixels are as­signed in a raster scan ordering. The 
only modi.cation is that for o NGn) the multiresoltion case, each neighborhood contains pixels in the 
current resolution as well as those in the lower resolutions. The similarity between two multiresolution 
neighborhoods is measured by computing the sum of the squared distance of all pixels within them. These 
lower resolution pixels constrain the synthesis process so that the added high frequency details will 
be consistent with the already synthesized low frequency structures. An example of a multiresolution 
neighborhood is shown in Fig­ure 6. It consists of two levels, with sizes 5x5 and 3x3, respectively. 
Within a neighborhood, we choose the sizes of the lower levels so that they are about half the sizes 
of the previous higher resolution G levels. For clarity, we use the symbol RxC,kyto indicate multires­ 
  (a) (b) (c) Figure 7: Synthesis results with the same neighborhood, but different num­bers of pyramid 
levels (a) 1 level, (b) 2 levels, (c) 3 levels. Except for the lowest resolution, which is synthesized 
with a 5x5 single resolution neigh­borhood, each pyramid level is synthesized using the multiresolution 
neigh­borhood shown in Figure 6. Note that as the number of pyramid levels increases, the image quality 
improves. olution neighborhoods which contain levels with size RxC at the top level. Figure 7 shows results 
of multiresolution synthesis with different numbers of pyramid levels. Note that Figure 7 (c), although 
syn- G thesized with a small 5x5,2ymultiresolution neighborhood, looks comparable with Figure 4 (c), 
which was generated with a larger 9x9 single resolution neighborhood. This demonstrates a major ad­vantage 
of multiresolution synthesis: moderately small neighbor­hoods can be used without sacri.cing synthesis 
qualities. 2.4 Edge Handling o NGn) Proper edge handling for near the image boundaries is very important. 
For the synthesis pyramid the edge is treated GIiN(G ) L )i) toroidally. In other words, if denotes the 
pixel at GN(L ))GIGIN(G ) L ))l level and position of pyramid , then GIfN(G )LLandp) oGi)nndp) oo)Go 
, whereand are the num­ GIfN(G) ber of rows and columns, respectively, of . Handling edges toroidally 
is essential to guarantee that the resulting synthetic tex­ture will tile seamlessly. 1 Gw For the input 
pyramid , toroidal neighborhoods typically con- Gwn w tain discontinuities unless is tileable. A reasonable 
edge handler for is to pad it with a re.ected copy of itself. Another solu­ o NGi i)Gw tion is to use 
only those completely inside , and discard those crossing the boundaries. Because a re.ective edge handler 
may introduce discontinuities in the derivative, we adopt the sec­ond solution which uses only interior 
blocks. 2.5 Initialization Natural textures often contain recognizable structures as well as a certain 
amount of randomness. Since our goal is to reproduce re­alistic textures, it is essential that the algorithm 
capture the random aspect of the textures. This notion of randomness can sometimes be achieved by entropy 
maximization [28], but the computational nI cost is prohibitive. Instead, we initialize the output image 
as a white random noise, and gradually modify this noise to look like nw the input texture . This initialization 
step seeds the algorithm with suf.cient entropy, and lets the rest of the synthesis process fo­ n Inw 
cus on the transformation of towards . To make this random noise a better initial guess, we also equalize 
the pyramid histogram GIGw of with respect to [9]. 1The multiresolution algorithm is also essential for 
tileability if a causal neighborhood is used. Since a single resolution causal neighborhood fe apccontains 
only pixels above pin scanline order, the vertical tileability may not be enforced. A multiresolution 
neighborhood, which contains symmetric regions at lower resolution levels, avoids this problem. The initial 
noise affects the synthesis process in the following way. For the single resolution case, neighborhoods 
in the .rst few nI rows and columns of contain noise pixels. These noise pixels introduce uncertainty 
in the neighborhood matching process, caus­ing the boundary pixels to be assigned semi-stochastically 
(How­ever, the searching process is still deterministic. The randomness is caused by the initial noise). 
The rest of the noise pixels are over­written directly during synthesis. For the multiresolution case, 
how­ever, more of the noise pixels contribute to the synthesis process, at least indirectly, since they 
determine the initial value of the lowest GI resolution level of . 2.6 Summary of Algorithm We summarize 
the algorithm in the following pseudocode. n InMnwpGS hGSihepiiis( function TextureSynthesis(, ) n I 
MpGS hGSihepiiis( 1 Initialize(); Gw+Mn w 2 BuildPyramid(); GInMn I 3 BuildPyramid(); GGI 4 foreach level 
from lower to higher resolutions of N(LI)I)GIN(G) 5 loop through all pixels of MGwGIGP) L Ii)iI 6 CFindBestMatch( 
, , ); GIN(G )dLI)dIM 7 )C; n I MGI 8 ReconPyramid( ); n I 9 return ; MGwGIG ) L If)nI function CFindBestMatch( 
, , ) oiInMGIG) G ) L If)nI 1 BuildNeighborhood( ); ooaitwI LMM 2 null; Cnull; N(Lw)w)GwN(G) 3 loop through 
all pixels of odwtMGwG ) L w))nw 4 BuildNeighborhood( , ); odwoIowaitI LoI 5 if Match(, ) Match(, ) oaitwI 
LMaowMGwN(G ) Lw)w 6; C); 7 return C; Table 2: Pseudocode of the Algorithm The architecture of this algorithm 
is .exible; it is composed from several orthogonal components. We list these components as follows and 
discuss the corresponding design choices. Pyramid: The pyramids are built from and reconstructed to images 
using the standard routines BuildPyramid and Recon-Pyramid. Various pyramids can be used for texture 
synthesis; examples are Gaussian pyramids [20], Laplacian pyramids [9], steerable pyramids [9, 22], and 
feature-based pyramids [4]. A Gaussian pyramid, for example, is built by successive .ltering and downsampling 
operations, and each pyramid level, except for the highest resolution, is a blurred and decimated version 
of the original image. Reconstruction of Gaussian pyramids is trivial, since the image is available at 
the highest resolution pyramid level. These different pyramids give different trade-offs between spatial 
and frequency resolutions. In this paper, we choose to use the Gaussian pyramid for its simplicity and 
greater spatial localization (a detailed discussion of this issue can be found in [19]). However, other 
kinds of pyramids can be used instead. Neighborhood: The neighborhood can have arbitrary size and shape; 
the only requirement is that it contains only valid pixels. A noncausal/symmetric neighborhood, for example, 
can be used by extending the original algorithm with two passes (Section 5.1). Synthesis Ordering: A 
raster scan ordering is used in line 5 of the function TextureSynthesis. This, however, can also be extended. 
For example, a spiral ordering can be used for constrained texture synthesis (Section 5.1). The synthesis 
ordering should cooperate with the BuildNeighborhood so that the output neighborhoods contain only valid 
pixels. Searching: An exhaustive searching procedure FindBestMatch is employed to determine the output 
pixel values. Because this is a standard process, various point searching algorithms can be used for 
acceleration. This will be discussed in detail in Section 4.   3 Synthesis Results To test the effectiveness 
of our approach, we have run the algo­rithm on many different images from standard texture sets. Figure 
8 shows examples using the MIT VisTex set [16], which contains real world textures photographed under 
natural lighting conditions. Ad­ditional texture synthesis results are available on our project web­site. 
A visual comparison of our approach with several other algo­rithms is shown in Figure 9. Result (a) is 
generated by Heeger and Bergen s algorithm [9] using a steerable pyramid with 6 orienta­tions. The algorithm 
captures certain random aspects of the texture but fails on the dominating grid-like structures. Result 
(b) is gener­ated by De Bonet s approach [4] where we choose his randomness parameter to make the result 
look best. Though capable of cap­turing more structural patterns than (a), certain boundary artifacts 
are visible. This is because his approach characterizes textures by lower frequency pyramid levels only; 
therefore the lateral relation­ship between pixels at the same level is lost. Result (c) is generated 
by Efros and Leung s algorithm [6]. This technique is based on the Markov Random Field model and is capable 
of generating high quality textures. However, a direct application of their approach can produce non-tileable 
results.2 Result (d) is synthesized using our approach. It is tileable and the image quality is comparable 
with those synthesized directly from MRFs. It took about 8 minutes to generate using a 195 MHz R10000 
processor. However, this is not the maximum possible speed achievable with this algorithm. In the next 
section, we de­scribe modi.cations that accelerate the algorithm greatly.  4 Acceleration Our deterministic 
synthesis procedure avoids the usual computa­tional requirement for sampling from a MRF. However, the 
algo­rithm as described employs exhaustive searching, which makes it slow. Fortunately, acceleration 
is possible. This is achieved by con­ o NGi) sidering neighborhoods as points in a multiple dimensional 
space, and casting the neighborhood matching process as a nearest­point searching problem [17]. The nearest-point 
searching problem in multiple dimensions is p stated as follows: given a setof points and a novel query 
point i in a -dimensional space, .nd a point in the set such that its dis­ ii tance from is lesser than, 
or equal to, the distance of from any other point in the set. Because a large number of such queries 
may p need to be conducted over the same data set , the computational p cost can be reduced if we preprocess 
to create a data structure that allows fast nearest-point queries. Many such data structures have been 
proposed, and we refer the reader to [17] for a more com­plete reference. However, most of these algorithms 
assume generic inputs and do not attempt to take advantage of any special struc­ p tures they may have. 
Popat [20] observed that the setof spatial neighborhoods from a texture can often be characterized well 
by 2Though not stated in the original paper [6], we have found that it is pos­sible to extend their approach 
using multiresolution pyramids and a toroidal neighborhood to make tileable textures.   (a)  (b) 
 (c) (d)  (e) (f)  (g) (h)  (j) (i)  Figure 8: Texture synthesis results. The smaller patches 
(size 128x128) are the input textures, and to their right are synthesized results GGGG (size 200x200). 
Each texture is generated using a 4-level Gaussian pyramid, with neighborhood sizes 3x3,1y, 5x5,2y, 7x7,2y, 
9x9,2y, respectively, from lower to higher resolutions. VisTex textures: (a) Water 0000 (b) Misc 0000 
(c) Metal 0004 (d) Fabric 0015 (e) Terrain 0000 (f) Clouds 0000 (g) Tile 0007 (h) Stone 0002 (i) Flowers 
0000 (j) Leaves 0009.   (a) (b) (c) (d) Figure 9: A comparison of texture synthesis results using different 
algorithms: (a) Heeger and Bergen s method [9] (b) De Bonet s method [4] (c) Efros and Leung s method 
[6] (d) Our method. Only Efros and Leung s algorithm produces results comparable with ours. However, 
our algorithm is two orders of magnitude faster than theirs (Section 4). The sample texture patch has 
size 64x64, and all the result images are of size 192x192. A 9x9 neighborhood is used for (c), and (d) 
is synthesized using the same parameters as indicated in the caption of Figure 8. a clustering probability 
model. Taking advantage of this cluster­ing property, we propose to use tree-structured vector quantization 
(TSVQ, [7]) as the searching algorithm [25]. 4.1 TSVQ Acceleration Tree-structured vector quantization 
(TSVQ) is a common technique for data compression. It takes a set of training vectors as input, and generates 
a binary-tree-structured codebook. The .rst step is to compute the centroid of the set of training vectors 
and use it as the root level codeword. To .nd the children of this root, the centroid and a perturbed 
centroid are chosen as initial child codewords. A generalized Lloyd algorithm [7], consisting of alternations 
between centroid computation and nearest centroid partition, is then used to .nd the locally optimal 
codewords for the two children. The train­ing vectors are divided into two groups based on these codewords 
and the algorithm recurses on each of the subtrees. This process terminates when the number of codewords 
exceeds a pre-selected size or the average coding error is below a certain threshold. The .nal codebook 
is the collection of the leaf level codewords. The tree generated by TSVQ can be used as a data structure 
for ef.cient nearest-point queries. To .nd the nearest point of a given query vector, the tree is traversed 
from the root in a best-.rst order­ing by comparing the query vector with the two children codewords, 
and then follows the one that has a closer codeword. This process is repeated for each visited node until 
a leaf node is reached. The best codeword is then returned as the codeword of that leaf node. Unlike 
full searching, the result codeword may not be the optimal one since only part of the tree is traversed. 
However, the result codeword is usually close to the optimal solution, and the computation is more ef.cient 
than full searching. If the tree is reasonably balanced (this can be enforced in the algorithm), a single 
search with codebook pN(v(p)rp) size can be achieved in time , which is much faster Np) than exhaustive 
searching with linear time complexity . To use TSVQ in our synthesis algorithm, we simply collect the 
ooNGi d) set of neighborhood pixels for each input pixel and treat ooNG) them as a vector of size equal 
to the number of pixels in . Go NGi)GwN(G) We use these vectors yfrom each as the train­ing data, and 
generate the corresponding tree structure codebooks fN(G) . During the synthesis process, the (approximate) 
closest point o NGi)GIiN(G) for each at is found by doing a best-.rst traversal of fN(GN(v(pGr oie ). 
Because this tree traversal has time complexity ) oeGwN(G) (where is the number of pixels of ), the synthesis 
proce­dure can be executed very ef.ciently. Typical textures take seconds to generate; the exact timing 
depends on the input and output image sizes. (a) D103 (b) D11   Exhaustive TSVQ Figure 10: Accelerated 
synthesis using TSVQ. The original Brodatz tex­tures, with size 128x128, are shown in the left column. 
The results gener­ated by exhaustive searching and TSVQ are shown in the middle and right columns, respectively. 
All generated images are of size 200x200. The av­erage running time for exhaustive searching is 360 seconds. 
The average training time for TSVQ is 22 seconds and the average synthesis time is 7.5 seconds.  4.2 
Acceleration Results An example comparing the results of exhaustive searching and TSVQ is shown in Figure 
10. The original image sizes are 128x128 and the resulting image sizes are 200x200. The average running 
time for exhaustive searching is 360 seconds. The average train­ing time for TSVQ is 22 seconds and the 
average synthesis time is 7.5 seconds. The code is implemented in C++ and the timings are measured on 
a 195MHz R10000 processor. As shown in Figure 10, results generated with TSVQ acceleration are roughly 
comparable in quality to those generated from the unaccelerated approach. In some cases, TSVQ will generate 
more blurry images. We .x this by allowing limited backtracking in the tree traversal so that more than 
one leaf node can be visited. The amount of backtracking can be used as a parameter which trades off 
between image quality and computation time. When the number of visited leaf nodes is equal to the codebook 
size, the result will be the same as the exhaustive searching case. One disadvantage of TSVQ acceleration 
is the memory require­(a) (b) (c)  Figure 11: TSVQ acceleration with different codebook sizes. The 
original image size is 64x64 and all these synthesized results are of size 128x128. The number of codewords 
in each case are (a) 64 (b) 512 (c) 4096 (all). (a) (b) Algorithm Training Time Synthesis Time Efros 
and Leung none 1941 seconds Exhaustive Searching none 503 seconds TSVQ acceleration 12 seconds 12 seconds 
 Table 3: A breakdown of running time for the textures shown in Figure 9. The .rst row shows the timing 
of Efros and Leung s algorithm. The second and third rows show the timing of our algorithm, using exhaustive 
searching and TSVQ acceleration, respectively. All the timings were measured using a 195 MHz R10000 processor. 
ment. Because an input pixel can appear in multiple neighborhoods, N( nnmo) a full-sized TSVQ tree can 
consume memory where is o the neighborhood size and is the number of input image pixels. Fortunately, 
textures usually contain repeating structures; therefore we can use codebooks with fewer codewords than 
the input train­ing set. Figure 11 shows textures generated by TSVQ with different codebook sizes. As 
expected the image quality improves when the codebook size increases. However, results generated with 
fewer codewords such as (b) look plausible compared with the full code­book result (c). In our experience 
we can use codebooks less than 10 percent the size of the original training data without noticeable degradation 
of quality of the synthesis results. To further reduce the expense of training, we can also train on 
a subset rather than the entire collection of input neighborhood vectors. Table 3 shows a timing breakdown 
for generating the textures shown in Figure 9. Our unaccelerated algorithm took 503 seconds. The TSVQ 
accelerated algorithm took 12 seconds for training, and another 12 seconds for synthesis. In comparison, 
Efros and Le­ung s algorithm [6] took half an hour to generate the same texture (the time complexity 
of our approach over Efros and Leung s is N(v(p)r o)e N(o)o where is the number of input image pixels). 
Because their algorithm uses a variable sized neighborhood it is dif.cult to accelerate. Our algorithm, 
on the other hand, uses a .xed neighborhood and can be directly accelerated by any point searching algorithm. 
  5 Applications One of the chief advantages of our texture synthesis method is its low computational 
cost. This permits us to explore a variety of applications, in addition to the usual texture mapping 
for graphics, that were previously impractical. Presented here are constrained synthesis for image editing 
and temporal texture generation. 5.1 Constrained Texture Synthesis Photographs, .lms and images often 
contain regions that are in some sense .awed. A .aw can be a scrambled region on a scanned photograph, 
scratches on an old .lm, wires or props in a movie (c) (d) Figure 12: Constrained texture synthesis. 
(a) a texture containing a black region that needs to be .lled in. (b) multiresolution blending [3] with 
an­other texture region will produce boundary artifacts. (c) A direct application of the algorithm in 
Section 2 will produce visible discontinuities at the right and bottom boundaries. (d) A much better 
result can be generated by using a modi.cation of the algorithm with 2 passes. .lm frame, or simply an 
undesirable object in an image. Since the processes causing these .aws are often irreversible, an algorithm 
that can .x these .aws is desirable. For example, Hirani and Tot­suka [10] developed an interactive algorithm 
that .nds translation­ally similar regions for noise removal. Often, the .awed portion is contained within 
a region of texture, and can be replaced by con­strained texture synthesis [6, 11]. Texture replacement 
by constrained synthesis must satisfy two requirements: the synthesized region must look like the surround­ing 
texture, and the boundary between the new and old regions must be invisible. Multiresolution blending 
[3] with another similar tex­ture, shown in Figure 12 (b), will produce visible boundaries for structured 
textures. Better results can be obtained by applying our algorithm in Section 2 over the .awed regions, 
but discontinuities still appear at the right and bottom boundaries as shown in Fig­ure 12 (c). These 
artifacts are caused by the causal neighborhood as well as the raster scan synthesis ordering. To remove 
these boundary artifacts a noncausal (symmetric) neighborhood must be used. However, we have to modify 
the orig­inal algorithm so that only valid (already synthesized) pixels are contained within the symmetric 
neighborhoods; otherwise the al­gorithm will not generate valid results (Figure 5). This can be done 
with a two-pass extension of the original algorithm. Each pass is the same as the original multiresolution 
process, except that a different neighborhood is used. During the .rst pass, the neighborhood con­tains 
only pixels from the lower resolution pyramid levels. Because the synthesis progresses in a lower to 
higher resolution fashion, a symmetric neighborhood can be used without introducing invalid pixels. This 
pass uses the lower resolution information to extrap­olate the higher resolution regions that need to 
be replaced. In the Original Result second pass, a symmetric neighborhood that contains pixels from 
both the current and lower resolutions is used. These two passes alternate for each level of the output 
pyramid. In the accelerated algorithm, the analysis phase is also modi.ed so that two TSVQ trees corresponding 
to these two kinds of neighborhoods are built for each level of the input pyramid. Finally, we also modify 
the synthesis ordering in the following way: instead of the usual raster­scan ordering, pixels in the 
.lled regions are assigned in a spiral fashion. For example, the hole in Figure 12 (a) is replaced from 
outside to inside from the surrounding region until every pixel is assigned (Figure 12 (d)). This spiral 
synthesis ordering removes the directional bias which causes the boundary discontinuities (as in Figure 
12 (c)). With a slight change of the synthesis ordering, the algorithm can be applied to other applications, 
such as the image extrapolation shown in Figure 13. The algorithm could also be extended as an interactive 
tool for image editing or denoising [15]. 5.2 Temporal Texture Synthesis The low cost of our accelerated 
algorithm enables us to consider synthesizing textures of dimension greater than two. An example of 3D 
texture is a temporal texture. Temporal textures are motions with indeterminate extent both in space 
and time. They can describe a wide variety of natural phenomena such as .re, smoke, and .uid motions. 
Since realistic motion synthesis is one of the major goals of computer graphics, a technique that can 
synthesize temporal tex­tures would be useful. Most existing algorithms model temporal textures by direct 
simulation; examples include .uid, gas, and .re [23]. Direct simulations, however, are often expensive 
and only suitable for speci.c kinds of textures; therefore an algorithm that can model general motion 
textures would be advantageous [24]. Temporal textures consist of 3D spatial-temporal volume of mo­tion 
data. If the motion data is local and stationary both in space and time, the texture can be synthesized 
by a 3D extension of our algorithm. This extension can be simply done by replacing various 2D entities 
in the original algorithm, such as images, pyramids, and neighborhoods, with their 3D counterparts. For 
example, the two Gaussian pyramids are constructed by .ltering and downsampling from 3D volumetric data; 
the neighborhoods contain local pixels in both the spatial and temporal dimension. The synthesis progresses 
from lower to higher resolutions, and within each resolution the output is synthesized slice by slice 
along the time domain. Figure 14 shows synthesis results of several typical temporal tex­tures: .re, 
smoke, and ocean waves (animations available on our webpage). The resulting sequences capture the .avor 
of the original motions, and tile both spatially and temporally. This technique is also ef.cient. Accelerated 
by TSVQ, each result frame took about 20 seconds to synthesize. Currently all the textures are generated 
automatically; we plan to extend the algorithm to allow more ex­plicit user controls (such as the distribution 
and intensity of the .re and smoke).  6 Conclusions and Future Work Textures are important for a wide 
variety of applications in com­puter graphics and image processing. On the other hand, they are hard 
to synthesize. The goal of this paper is to provide a practi­cal tool for ef.ciently synthesizing a broad 
range of textures. In­spired by Markov Random Field methods, our algorithm is general: a wide variety 
of textures can be synthesized without any knowl­edge of their physical formation processes. The algorithm 
is also ef.cient: by a proper acceleration using TSVQ, typical textures can be generated within seconds 
on current PCs and workstations. The algorithm is also easy to use: only an example texture patch is 
re­quired. The basic version of our algorithm (Section 2) relates to an ear­lier work by Popat and Picard 
[20] in that a causal neighborhood and raster scan ordering are used for texture synthesis. However, 
instead of constructing explicit probability models, our algorithm uses deterministic searching. This 
approach shares the simplicity of Efros and Leung [6], but uses .x-sized neighborhoods which al­low TSVQ 
acceleration. The fact that such a simple approach works well on many different textures implies that 
there may be compu­tational redundancies in other texture synthesis techniques. This algorithm shares 
some of the same limitations as Markov Random Field approaches: in particular, only local and stationary 
phenom­ena can be represented. Other visual cues such as 3D shape, depth, lighting, or re.ection can 
not be captured by this simple model. Aside from constrained synthesis and temporal textures, nu­merous 
applications of our approach are possible. Other potential applications/extensions are: Multidimensional 
texture: The notion of texture extends natu­rally to multi-dimensional data. One example was presented 
in this paper -motion sequences. The same technique can also be directly applied to solid textures or 
animated solid texture synthesis. We are also trying to extend our algorithm for generating structured 
solid textures from 2D views [9]. Texture compression/decompression: Textures usually contain repeating 
patterns and high frequency information; therefore they are not well compressed by transform-based techniques 
such as JPEG. However, codebook-based compression techniques work well on textures [1]. This suggests 
that textures might be compressable by our synthesis technique. Compression would consist of building 
a codebook, but unlike [1], no code indices would be generated; only the codebook would be transmitted 
and the compression ratio is controlled by the number of codewords. Decompression would consist of texture 
synthesis. This decom­pression step, if accelerated one more order of magnitude over our current software 
implementation, could be usable for real time texture mapping. The advantage of this approach over [1] 
is much greater compression, since only the codebook is transmitted. Motion synthesis/editing: Some motions 
can be ef.ciently modeled as spatial-temporal textures. Others, such as animal or human motion, are too 
highly structured for such a direct approach. However, it might be possible to encode their motion as 
joint angles, and then apply texture analysis-synthesis to the resulting 1D temporal motion signals. 
Modeling geometric details: Models scanned from real world objects often contain texture-like geometric 
details, making the  e sequence is shown on the left, and the corresponding synthesis result is shown 
on the right. A 3-level Gaussian pyramid, with neighborhood sizes 5x5x5,2}, ee 3x3x3,2}, 1x1x1,1}, are 
used for synthesis. The original motion sequences contain 32 frames, and the synthesis results contain 
64 frames. The individual frame sizes are (a) 128x128 (b) 150x112 (c) 150x112. Accelerated by TSVQ, the 
training times are (a) 1875 (b) 2155 (c) 2131 seconds and the synthesis times per frame are (a) 19.78 
(b) 18.78 (c) 20.08 seconds. To save memory, we use only a random 10 percent of the input neighborhood 
vectors to build the (full) codebooks. models expensive to store, transmit or manipulate. These geometric 
details can be represented as displacement maps over a smoother surface representation [13]. The resulting 
displacement maps should be compressable/decompressable as 2D textures using our technique. Taking this 
idea further, missing geometric details, a common problem in many scanning situations [14], could be 
.lled in using our constrained texture synthesis technique. Direct synthesis over meshes: Mapping textures 
onto irregular 3D meshes by projection often causes distortions [21]. These dis­tortions can sometimes 
be .xed by establishing suitable parameter­ization of the mesh, but a more direct approach would be to 
synthe­size the texture directly over the mesh. In principle, this can be done using our technique. However, 
this will require extending ordinary signal processing operations such as .ltering and downsampling to 
irregular 3D meshes.  Acknowledgments We would like to thank Kris Popat and Alyosha Efros for answering 
ques­tions about their texture synthesis works, Phil Hubbard for his help on the writing of the paper, 
and the anonymous reviewers for their comments. The texture thumbnail shown in Figure 1 was acquired 
from Jeremy De Bonet s webpage. Special thanks to members of the Stanford Graphics Group. This research 
was supported by Intel, Interval, and Sony under the Stanford Im­mersive Television Project. References 
[9] D. J. Heeger and J. R. Bergen. Pyramid-Based texture analysis/synthesis. In R. Cook, editor, SIGGRAPH 
95 Conference Proceedings, Annual Conference Series, pages 229 238. ACM SIGGRAPH, Addison Wesley, Aug. 
1995.  [10] A. N. Hirani and T. Totsuka. Combining frequency and spatial domain informa­tion for fast 
interactive image noise removal. Computer Graphics, 30(Annual Conference Series):269 276, 1996. [11] 
H. Igehy and L. Pereira. Image replacement through texture synthesis. In Inter­national Conference on 
Image Processing, volume 3, pages 186 189, Oct 1997. [12] H. Iversen and T. Lonnestad. An evaluation 
of stochastic models for analysis and synthesis of gray scale texture. Pattern Recognition Letters, 15:575 
585, 1994. [13] V. Krishnamurthy and M. Levoy. Fitting smooth surfaces to dense polygon meshes. Proceedings 
of SIGGRAPH 96, pages 313 324, August 1996. ISBN 0-201-94800-1. Held in New Orleans, Louisiana. [14] 
M. Levoy, K. Pulli, B. Curless, S. Rusinkiewicz, D. Koller, L. Pereira, M. Ginz­ton, S. Anderson, J. 
Davis, J. Ginsberg, J. Shade, and D. Fulk. The Digital Michelangelo Project: 3D scanning of large statues. 
To appear in Proceedings of SIGGRAPH 2000. [15] T. Malzbender and S. Spach. A context sensitive texture 
nib. In Proceedings of Computer Graphics International, pages 151 163, June 1993. [16] MIT Media Lab. 
Vision texture. http://www-white.media.mit.edu/vismod/­imagery/VisionTexture/vistex.html. [17] S. Nene 
and S. Nayar. A simple algorithm for nearest neighbor search in high dimensions. IEEE Transactions on 
Pattern Analysis and Machine Intelligence, 19:989 1003, 1997. [18] R. Paget and I. Longstaff. Texture 
synthesis via a noncausal nonparametric mul­tiscale Markov random .eld. IEEE Transactions on Image Processing, 
7(6):925 931, June 1998. [19] A. C. Popat. Conjoint Probabilistic Subband Modeling. PhD thesis, Mas­sachusetts 
Institute of Technology, 1997. [20] K. Popat and R. Picard. Novel cluster-based probability model for 
texture syn­thesis, classi.cation, and compression. In Visual Communications and Image Processing, pages 
756 68, 1993. [21] M. Segal, C. Korobkin, R. van Widenfelt, J. Foran, and P. E. Haeberli. Fast shadows 
and lighting effects using texture mapping. Computer Graphics (Pro­ceedings of SIGGRAPH 92), 26(2):249 
252, July 1992. [1] A. C. Beers, M. Agrawala, and N. Chaddha. Rendering from compressed tex­tures. Proceedings 
of SIGGRAPH 96, pages 373 378, August 1996. [2] P. Brodatz. Textures: A Photographic Album for Artists 
and Designers. Dover, New York, 1966. [3] P. J. Burt and E. H. Adelson. A multiresolution spline with 
application to image mosaics. ACM Transactions on Graphics, 2(4):217 236, Oct. 1983. [4] J. S. De Bonet. 
Multiresolution sampling procedure for analysis and synthesis of texture images. In T. Whitted, editor, 
SIGGRAPH 97 Conference Proceedings, Annual Conference Series, pages 361 368. ACM SIGGRAPH, Addison Wesley, 
Aug. 1997. [5] J. Dorsey, A. Edelman, J. Legakis, H. W. Jensen, and H. K. Pedersen. Modeling and rendering 
of weathered stone. Proceedings of SIGGRAPH 99, pages 225 234, August 1999. [6] A. Efros and T. Leung. 
Texture synthesis by non-parametric sampling. In Inter­national Conference on Computer Vision, volume 
2, pages 1033 8, Sep 1999. [7] A. Gersho and R. M. Gray. Vector Quantization and Signal Compression. 
Kluwer Academic Publishers, 1992. [8] R. Haralick. Statistical image texture analysis. In Handbook of 
Pattern Recogni­tion and Image Processing, volume 86, pages 247 279. Academic Press, 1986. [22] E. Simoncelli 
and J. Portilla. Texture characterization via joint statistics of wavelet coef.cient magnitudes. In Fifth 
International Conference on Image Pro­cessing, volume 1, pages 62 66, Oct. 1998. [23] J. Stam and E. 
Fiume. Depicting .re and other gaseous phenomena using diffu­sion processes. Proceedings of SIGGRAPH 
95, pages 129 136, August 1995. [24] M. Szummer and R. W. Picard. Temporal texture modeling. In International 
Conference on Image Processing, volume 3, pages 823 6, Sep 1996. [25] L. Wei. Deterministic texture analysis 
and synthesis using tree structure vector quantization. In XII Brazilian Symposium on Computer Graphics 
and Image Processing, pages 207 213, October 1999. [26] A. Witkin and M. Kass. Reaction-diffusion textures. 
In T. W. Sederberg, editor, Computer Graphics (SIGGRAPH 91 Proceedings), volume 25, pages 299 308, July 
1991. [27] S. P. Worley. A cellular texture basis function. In H. Rushmeier, editor, SIG-GRAPH 96 Conference 
Proceedings, Annual Conference Series, pages 291 294. ACM SIGGRAPH, Addison Wesley, Aug. 1996. [28] S. 
Zhu, Y. Wu, and D. Mumford. Filters, random .elds and maximun entropy (FRAME) -towards a uni.ed theory 
for texture modeling. International Journal of Computer Vision, 27(2):107 126, 1998.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>345012</article_id>
		<sort_key>489</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>55</seq_no>
		<title><![CDATA[Video textures]]></title>
		<page_from>489</page_from>
		<page_to>498</page_to>
		<doi_number>10.1145/344779.345012</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=345012</url>
		<abstract>
			<par><![CDATA[<p>This paper introduces a new type of medium, called a <italic>video texture</italic>, which has qualities somewhere between those of a photograph and a video. A video texture provides a continuous infinitely varying stream of images. While the individual frames of a video texture may be repeated from time to time, the video sequence as a whole is never repeated exactly. Video textures can be used in place of digital photos to infuse a static image with dynamic qualities and explicit actions. We present techniques for analyzing a video clip to extract its structure, and for synthesizing a new, similar looking video of arbitrary length. We combine video textures with view morphing techniques to obtain <italic>3D video textures</italic>. We also introduce <italic>video-based animation</italic>, in which the synthesis of video textures can be guided by a user through high-level interactive controls. Applications of video textures and their extensions include the display of dynamic scenes on web pages, the creation of dynamic backdrops for special effects and games, and the interactive control of video-based animation.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[animation]]></kw>
			<kw><![CDATA[image-based rendering]]></kw>
			<kw><![CDATA[morphing]]></kw>
			<kw><![CDATA[multimedia]]></kw>
			<kw><![CDATA[natural phenomena]]></kw>
			<kw><![CDATA[texture synthesis]]></kw>
			<kw><![CDATA[video sprites]]></kw>
			<kw><![CDATA[video-based animation]]></kw>
			<kw><![CDATA[video-based rendering]]></kw>
			<kw><![CDATA[view morphing]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Video (e.g., tape, disk, DVI)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.10</cat_node>
				<descriptor>Morphological</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.9</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010242</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Shape representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010243</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Appearance and texture representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010247</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Image segmentation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010248</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Video segmentation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010248</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Video segmentation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010230</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Video summarization</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P21745</person_id>
				<author_profile_id><![CDATA[81100397188]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Arno]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sch&#246;dl]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology and Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15023234</person_id>
				<author_profile_id><![CDATA[81100122769]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Szeliski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P63622</person_id>
				<author_profile_id><![CDATA[81100188207]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Salesin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research and University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39066651</person_id>
				<author_profile_id><![CDATA[81100313702]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Irfan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Essa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Z. Bar-Joseph. Statistical learning of multi-dimensional textures. Master's thesis, The Hebrew University of Jerusalem, June 1999.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>794554</ref_obj_id>
				<ref_obj_pid>794190</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[D. Beymer. Feature correspondence by interleaving shape and texture computations. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'96), pages 921-928, San Francisco, California, June 1996.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258880</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[C. Bregler, M. Covell, and M. Slaney. Video rewrite: Driving visual speech with audio. Computer Graphics (SIG-GRAPH'97), pages 353-360, August 1997.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>247</ref_obj_id>
				<ref_obj_pid>245</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[P. J. Burt and E. H. Adelson. A multiresolution spline with applications to image mosaics. ACM Transactions on Graphics, 2(4):217-236, October 1983.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218395</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[S. E. Chen. QuickTime VR - an image-based approach to virtual environment navigation. Computer Graphics (SIG- GRAPH'95), pages 29-38, August 1995.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258882</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[J. De Bonet. Multiresolution sampling procedure for analysis and synthesis of texture images. Computer Graphics (SIG- GRAPH'97), pages 361-368, August 1997.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>312148</ref_obj_id>
				<ref_obj_pid>311625</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[P. Debevec et al., editors. Image-Based Modeling, Rendering, and Lighting, SIGGRAPH'99 Course 39, August 1999.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237191</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[P. E. Debevec, C. J. Taylor, and J. Malik. Modeling and rendering architecture from photographs: A hybrid geometry- and image-based approach. Computer Graphics (SIGGRAPH'96), pages 11-20, August 1996.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>851569</ref_obj_id>
				<ref_obj_pid>850924</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[A. A. Efros and T. K. Leung. Texture synthesis by nonparametric sampling. In Seventh International Conference on Computer Vision (ICCV'99), pages 1033-1038, Kerkyra, Greece, September 1999.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237266</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[A. Finkelstein, C. E. Jacobs, and D. H. Salesin. Multiresolution video. Proceedings of SIGGRAPH 96, pages 281-290, August 1996. ISBN 0-201-94800-1. Held in New Orleans, Louisiana.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122721</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[W. T. Freeman, E. H. Adelson, and D. J. Heeger. Motion without movement. Computer Graphics (Proceedings of SIG- GRAPH 91), 25(4):27-30, July 1991.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218446</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[D. J. Heeger and J. R. Bergen. Pyramid-based texture analysis/synthesis. Proceedings of SIGGRAPH 95, pages 229-238, August 1995.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Workshop on Image-Based Modeling and Rendering, Stanford University, March 1998. http://graphics.stanford.edu/workshops/ibr98/.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218454</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[C. E. Jacobs, A. Finkelstein, and D. H. Salesin. Fast multiresolution image querying. Proceedings of SIGGRAPH 95, pages 277-286, August 1995.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1622748</ref_obj_id>
				<ref_obj_pid>1622737</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement learning: A survey. Journal of Artificial Intelligence Research, 4, 1996.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>614766</ref_obj_id>
				<ref_obj_pid>614653</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[T. Kanade, P. W. Rander, and P. J. Narayanan. Virtualized reality: constructing virtual worlds from real scenes. IEEE MultiMedia Magazine, 1(1):34-47, Jan-March 1997.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237199</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[M. Levoy and P. Hanrahan. Light field rendering. In Computer Graphics Proceedings, Annual Conference Series, pages 31- 42, Proc. SIGGRAPH'96 (New Orleans), August 1996. ACM SIGGRAPH.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218398</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[L. McMillan and G. Bishop. Plenoptic modeling: An image-based rendering system. Computer Graphics (SIG- GRAPH'95), pages 39-46, August 1995.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618392</ref_obj_id>
				<ref_obj_pid>616043</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[S. Moezzi et al. Reality modeling and visualization from multiple video sequences. IEEE Computer Graphics and Applications, 16(6):58-63, November 1996.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[S. A. Niyogi and E. H. Adelson. Analyzing and recognizing walking figures in xyt. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'94), pages 469-474, Seattle, Washington, June 1994.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280825</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[F. Pighin, J. Hecker, D. Lischinski, D. H. Salesin, and R. Szeliski. Synthesizing realistic facial expressions from photographs. In Computer Graphics (SIGGRAPH'98) Proceedings, pages 75-84, Orlando, July 1998. ACM SIGGRAPH.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>261682</ref_obj_id>
				<ref_obj_pid>261678</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[R. Polana and R. C. Nelson. Detection and recognition of periodic, nonrigid motion. International Journal of Computer Vision, 23(3):261-282, 1997.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>836811</ref_obj_id>
				<ref_obj_pid>521384</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[S. Pollard et al. View synthesis by trinocular edge matching and transfer. In British Machine Vision Conference (BMVC98), Southampton, England, September 1998.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237196</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[S. M. Seitz and C. M. Dyer. View morphing. In Computer Graphics Proceedings, Annual Conference Series, pages 21- 30, Proc. SIGGRAPH'96 (New Orleans), August 1996. ACM SIGGRAPH.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280289</ref_obj_id>
				<ref_obj_pid>280286</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[S. M. Seitz and C. R. Dyer. View invariant analysis of cyclic motion. International Journal of Computer Vision, 25(3):231- 251, December 1997.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280882</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[J. Shade, S. Gortler, L.-W. He, and R. Szeliski. Layered depth images. In Computer Graphics (SIGGRAPH'98) Proceedings, pages 231-242, Orlando, July 1998. ACM SIGGRAPH.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237209</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[J. Shade, D. Lischinski, D. Salesin, T. DeRose, and J. Snyder. Hierarchical images caching for accelerated walkthroughs of complex environments. In Computer Graphics (SIG-GRAPH'96) Proceedings, pages 75-82, Proc. SIGGRAPH'96 (New Orleans), August 1996. ACM SIGGRAPH.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[B. Shaw. Light of other days. In Other Days, Other Eyes. Ace Books, NewYork, 1972. (also published in Analog and various sci-fi anthologies).]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>338201</ref_obj_id>
				<ref_obj_pid>338200</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[H.-Y. Shum and R. Szeliski. Construction of panoramic mosaics with global and local alignment. International Journal of Computer Vision, 36(2):101-130, February 2000.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237263</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[A. R. Smith and J. F. Blinn. Blue screen matting. In Computer Graphics Proceedings, Annual Conference Series, pages 259- 268, Proc. SIGGRAPH'96 (NewOrleans), August 1996. ACM SIGGRAPH.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258861</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[R. Szeliski and H.-Y. Shum. Creating full view panoramic image mosaics and texture-mapped models. In Computer Graphics (SIGGRAPH'97) Proceedings, pages 251-258, Los Angees, August 1997. ACM SIGGRAPH.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237274</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[J. Torborg and J. T. Kajiya. Talisman: Commodity realtime 3D graphics for the PC. In Computer Graphics Proceedings, Annual Conference Series, pages 353-363, Proc. SIGGRAPH'96 (New Orleans), August 1996. ACM SIGGRAPH.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Video Textures Arno Sch¨odl1,2 Richard Szeliski2 David H. Salesin2,3 Irfan Essa1 1Georgia Institute 
of Technology 2Microsoft Research 3University of Washington Abstract This paper introduces a new type 
of medium, called a video texture, which has qualities somewhere between those of a photograph and a 
video. A video texture provides a continuous in.nitely varying stream of images. While the individual 
frames of a video texture may be repeated from time to time, the video sequence as a whole is never repeated 
exactly. Video textures can be used in place of digital photos to infuse a static image with dynamic 
qualities and explicit action. We present techniques for analyzing a video clip to extract its structure, 
and for synthesizing a new, similar looking video of arbitrary length. We combine video textures with 
view morphing techniques to obtain 3D video textures. We also introduce video­based animation, in which 
the synthesis of video textures can be guided by a user through high-level interactive controls. Applica­tions 
of video textures and their extensions include the display of dynamic scenes on web pages, the creation 
of dynamic backdrops for special effects and games, and the interactive control of video-based animation. 
CR Categories and Subject Descriptors: H.5.1 [Information Interfaces]: Multimedia Information Systems 
video I.3.3 [Computer Graphics]: Picture/Image Generation display algorithms I.4.9 [Image Processing 
and Computer Vision]: Applications Keywords: Animation, image-based rendering, morphing, multimedia, 
natural phe­nomena, texture synthesis, video-based rendering, video-based animation, video sprites, view 
morphing.  1 Introduction A picture is worth a thousand words. And yet, there are many phe­nomena, 
both natural and man-made, that are not adequately cap­tured by a single static photo. A waterfall, a 
.ickering .ame, a .ag .apping in the breeze each of these phenomena has an inherently dynamic quality 
that a single image simply cannot portray. The obvious alternative to static photography is video. But 
video has its own drawbacks. If we want to store video on a computer or some other storage device, we 
are forced to use a video clip of .nite duration. Hence, the video has a beginning, a middle, and an 
end. The video becomes a very speci.c embodiment of a very speci.c period of time. Although it captures 
the time-varying behavior of the phenomenon at hand, the video lacks the timeless quality of the photograph. 
In this work, we propose a new type of medium, which is in many ways intermediate between a photograph 
and a video. This new Permission to make digital or hard copies of part or all of this work or personal 
or classroom use is granted without fee provided that copies are not made or distributed for profit or 
commercial advantage and that copies bear this notice and the full citation on the first page. To copy 
otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission 
and/or a fee. SIGGRAPH 2000, New Orleans, LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 medium, 
which we call a video texture, provides a continuous, in­.nitely varying stream of video images. (We 
use the term video texture because of the strong analogy to image textures, which usually repeat visual 
patterns in similar, quasi-periodic ways.) The video texture is synthesized from a .nite set of images 
by randomly rearranging (and possibly blending) original frames from a source video. Video textures occupy 
an interesting niche between the static and the dynamic realm. Whenever a photo is displayed on a computer 
screen, a video texture might be used instead to infuse the image with dynamic qualities. For example, 
a web page advertising a scenic destination could use a video texture of a beach with palm trees blowing 
in the wind rather than a static photograph. Or an actor could provide a dynamic head shot with continuous 
movement on his home page.Video textures could also .nd application as dynamic backdrops or foreground 
elements for scenes composited from live and synthetic elements, for example, in computer games. The 
basic concept of a video texture can be extended in several different ways to further increase its applicability. 
For backward compatibility with existing video players and web browsers, .nite duration video loops can 
be created to play continuously without any visible discontinuities. The original video can be split 
into in­dependently moving regions, and each region can be analyzed and rendered independently. We can 
also use computer vision techniques to separate objects from the background and represent them as video 
sprites, which can be rendered at arbitrary image locations. Multi­ple video sprites or video texture 
regions can be combined into a complex scene. Video textures can also be combined with stereo matching 
and view morphing techniques to produce three-dimensional video textures that can be rendered from continually 
varying viewpoints. Most interesting, perhaps, is the ability to put video textures under in­teractive 
control to drive them at a high level in real time. For instance, by interactively specifying a preferred 
segment within a source video, a jogger can be made to speed up and slow down according to the position 
of an interactive slider. Alternatively, an existing video clip can be shortened or lengthened by removing 
or adding video texture in the middle. We call these forms of high-level control video-based animation. 
Creating video textures and applying them in all of these ways re­quires solving a number of problems. 
The .rst dif.culty is in locating potential transition points in the video sequences, i.e., places where 
the video can be looped back on itself in a minimally obtrusive way.A second challenge is in .nding a 
sequence of transitions that respects the global structure of the video. Even though a given transition 
may, itself, have minimal artifacts, it could lead to a portion of the video from which there is no graceful 
exit, and therefore be a poor transition to take. A third challenge is in smoothing visual discon­tinuities 
at the transitions we solve this problem using morphing techniques. A fourth problem is in automatically 
factoring video frames into different regions that can be analyzed and synthesized independently. Furthermore, 
the various extensions described above involve new, additional challenges: creating good, .xed-length 
cy­cles; separating video texture elements from their backgrounds so that they can be used as video sprites; 
applying view morphing to video imagery; and generalizing the transition metrics to incorporate real-time 
user input. In some cases, our solutions to these problems are (in retrospect) quite simple. We thus 
feel that the primary contribution of this paper may lie not so much in the technical solutions to each 
of these indi­vidual problems per se, but rather in the overall paradigm of reusing video frames to create 
video textures, video sprites, and video-based animation. 1.1 Related work Increasingly, computer graphics 
is turning toward image-based mod­eling and rendering techniques [7, 13], where images captured from 
a scene or object are used as an integral part of the rendering pro­cess, sometime obviating the need 
for geometry altogether [5]. As Debevec points out [13], this trend parallels the one that occurred in 
music synthesis a decade ago, when sample-based synthesis re­placed more algorithmic approaches like 
frequency modulation. To date, image-based rendering techniques have mostly been applied to still scenes 
such as architecture [8, 18], although they have also been used to cache and accelerate the renderings 
produced by con­ventional graphics hardware [27, 32]. Our work generalizes image-based rendering to the 
temporal do­main. It can thus be thought of as a kind of video-based render­ing. A similar idea has been 
used in video games, in which hand­generated video loops have been created to simulate natural phenom­ena 
like .re or water. However, there has been little previous work on automatically generating motion by 
reusing captured video. Proba­bly the work most closely related to our own is Video Rewrite [3], in which 
video sequences of a person s mouth are extracted from a training sequence of the person speaking and 
then reordered in order to match the phoneme sequence of a new audio track. Related 3D view interpolation 
techniques have also been applied to multiple video streams in the Virtualized Reality [16] and Immersive 
Video [19] projects. Pollard et al. [23] introduced the term video sprite for applying such techniques 
to an alpha-matted region of the video rather than to the whole image. Finkelstein et al. [10] also used 
alpha-matted video elements in their earlier multiresolution video work, which they called video clip-art. 
Video textures can also be thought of as a temporal extension of 2D image texture synthesis. The multiscale-sampling 
techniques used for texture synthesis [6, 9, 12] have in fact been directly extended by Bar-Joseph into 
the space-time domain [1]. Bar-Joseph s work focuses on texture-type motions with limited semantic content 
such as .re or water close-ups, which are well modeled by the hierarchy of .lter responses used for texture 
synthesis. Our approach can deal with these kinds of phenomena, but also deals with much more struc­tured 
motions such as repetitive human actions. In this context, some of our analysis resembles earlier work 
in .nding cycles in repetitive motions and rectifying these cycles into truly periodic sequences [20, 
22, 25]. Our work also has some similarity to the motion with­out movement technique [11], in which patterns 
in an image appear to move continuously without changing their positions. 1.2 System overview Given 
a small amount of training video (our input video clip), how do we generate an in.nite amount of similar 
looking video? The general approach we follow in this paper is to .nd places in the original video where 
a transition can be made to some other place in the video clip without introducing noticeable discontinuities. 
Our system is thus organized into three major components (Figure 1). The .rst component of the system 
analyzes the input video to .nd the good transition points, and stores these in a small data table that 
 Input video clip 8 Video Figure 1 System overview diagram. An input video clip is fed into the Analysis 
component, which .nds good transition points where the video can be looped back on itself. These transitions 
(the Video structure) are fed to one of two Synthesis components: either Random Play, which sequences 
the transitions stochastically; or Generate Loops, which .nds a set of transitions that together create 
a single overall video loop of a given length. The Rendering component takes the generated sequence of 
frames, together with the original video clip, and produces either an in.nite video texture sequence, 
or a video loop that can be played inde.nitely by a standard Video Player in loop mode. becomes part 
of the video texture representation. This analysis com­ponent may also optionally trim away parts of 
the input video that are not needed, or segment the original video into independently mov­ing pieces, 
in order to more easily analyze (and .nd the repetition in) these individual regions. The second component 
of our system synthesizes new video from the analyzed video clip, by deciding in what order to play (or 
shuf.e) the original video frames (or pieces thereof). We have developed two different approaches to 
perform this sequencing. The .rst approach is random play, which uses a Monte-Carlo (stochastic) technique 
to decide which frame should be played after a given frame, using the table of frame-to-frame similarities 
computed by the analysis algorithm. The second approach selects a small number of transitions to take 
in such a way that the video is guaranteed to loop after a speci.ed number of frames. The resulting video 
loop can then be played by a conventional video player in loop mode. Once the set of frames to be played 
has been selected, the render­ing component puts together the frames (or frame pieces) in a way that 
is visually pleasing. This process may be as simple as just dis­playing or outputting the original video 
frames, or it may involve cross-fading or morphing across transitions and/or blending together independently 
moving regions. The remainder of this paper describes, in more detail, the represen­tation used to capture 
the structure of video textures (Section 2), our process for extracting this representation from source 
video (Sec­tion 3), and for synthesizing the video texture (Section 4). The ren­dering algorithms used 
to composite video sprites together and to smooth over visual discontinuities are described next (Section 
5). The discussion of our basic results (Section 6) is followed by a description of some further extensions 
(Section 7). These include the extraction and rendering of video sprites, changing viewpoints using image-based 
rendering techniques, and the creation of video­based animation. The video clips associated with these 
results can be viewed on the CD-ROM, DVD, and Video Conference Proceedings. We conclude with a discussion 
of the potential of video textures and some ideas for future research.  2 Representation Our video 
textures are essentially Markov processes, with each state corresponding to a single video frame, and 
the probabilities corre­sponding to the likelihood of transitions from one frame to another. In practice, 
we have found two alternate (and equivalent) representa­tions to be useful for storing these video textures. 
One is as a matrix of probabilities (Figure 3), in which each element Pij of the matrix describes the 
probability of transitioning from frame i to frame j. The other is as a set of explicit links (Figure 
6) from one frame i to another j, along with an associated probability. The .rst representa­tion is advantageous 
when the matrix is dense, as the indices do not need to be stored explicitly. However, in most cases 
the set of allow­able transitions is relatively sparse, and so the second representation is preferred. 
In many cases, better results can be achieved by splitting the original video into regions and computing 
a video texture for each region separately. As discussed in more detail in Section 7.3, we sometimes 
also segment the video into different video sprite elements, each with its own af.liated alpha channel 
and compute a video texture for each sprite separately. In this case, additional information is stored 
along with the links to describe how the relative position of the sprite is changed as the link is crossed. 
 3 Analysis: Extracting the video texture The .rst step in creating a video texture from an input video 
sequence is to compute some measure of similarity between all pairs of frames in the input sequence. 
In our current implementation, we use L2 distance, since it is simple and works well in practice. Before 
computing these distances, we often equalize the brightness in the image sequence (based on some background 
portions that do not change) in order to remove visual discontinuities that would otherwise appear when 
jumping between different parts of the input video. If the camera also has a small amount of jitter (e.g., 
from being handheld or shot in high wind conditions), we run video stabilization software over the sequence. 
Once the frame-to-frame distances have been computed, we store them in the matrix Dij = Ii -Ij 2, (1) 
which denotes the L2 distance between each pair of images Ii and Ij. During the new video synthesis, 
the basic idea will be to create transitions from frame i to frame j anytime the successor of i is similar 
to j that is, whenever Di+1, j is small. A simple way to do this is to map these distances to probabilities 
through an exponential function, Pij . exp (-Di+1, j /s). (2) All the probabilities for a given row of 
P are normalized so that j Pij = 1. At run time, the next frame to display after frame i is selected 
according to the distribution of Pij. The s parameter controls the mapping between L2 distance and relative 
probability of taking a given transition. Smaller values of s emphasize just the very best transitions, 
while larger values of sallow for greater variety at the cost of poorer transitions. We typically (but 
not always) set s to a small multiple of the average (non-zero) Dij values, so that the likelihood of 
making a transition at a given frame is fairly low. 3.1 Preserving dynamics Of course, video textures 
need to preserve more than just similarity across frames: the dynamics of motion need to be preserved 
as well. Consider, for example, a swinging pendulum (Figure 2). Each frame i j1 j2 Figure 2 Finding 
good transitions in a pendulum sequence. Frame i in the top row matches both frames j1 and j2 of the 
bottom row very closely. However, of these two possibilities, only frame j2 comes from a sequence with 
the correct dynamics. The two possibilities are disambiguated by considering the sequence of frames surrounding 
i, j1, and j2. Frames i - 1, i, and i + 1 match j2 - 1, j2, and j2 +1butnot j1 - 1, j1, and j1 +1. of 
the left-to-right swing will have a corresponding frame in the right-to-left swing that looks very similar 
(indicated by the blue arrow in Figure 2). However, transitioning from frame i in the left­to-right swing 
to a frame that looks very similar to i + 1 in the right-to-left swing will create an abrupt and unacceptable 
change in the pendulum s motion. One possible way to overcome this problem might be to match veloc­ities 
(e.g., using optical .ow computed at each frame), in addition to matching the visual similarity between 
frames. However, .ow computations can be quite brittle (they can be almost arbitrary in the absence of 
texture), so we have opted for the following simpler alternative. We solve the problem of preserving 
dynamics by requiring that for a frame to be classi.ed as similar to some other frame, not only the frames 
themselves, but also temporally adjacent frames within some weighted window must be similar to each other. 
In other words, we match subsequences instead of individual frames. Such a sub­sequence match can be 
achieved by .ltering the difference matrix with a diagonal kernel with weights [w-m, ..., wm-1], m-1 
D; .ij = wk Di+k, j+k . (3) k=-m In practice, we use m = 1 or 2, corresponding to a 2-or 4-tap .lter, 
with binomial weights. (Making the .lter even-length allows the decision to transition from some frame 
i to some other frame j to be determined as much by the similarity of i and j-1 as by the similarity 
of i + 1 and j, removing any asymmetry in this decision.) After .ltering and computing the probabilities 
from the .ltered difference matrix, the undesired transitions no longer have high probability. Figure 
3 shows this behavior using two-dimensional images of the Dij and Pij tables for the pendulum sequence 
of Figure 2. (These images bear some resemblance to those found in earlier papers on the analysis of 
periodic motion [20, 22, 25].) Here, the new probabil­ities P;ij ij are computed from the dynamics-preserving 
distances D;in the same way as Pij were computed from Dij (in equation (2)). In the original un.ltered 
tables, the periodic nature of the pendulum is readily visible, as is the tendency to match both forward 
and back­ward swings. After .ltering, only swings in the same direction are matched. (The bright knots 
are where the pendulum pauses at the ends of its swing, and hence has more self-similarity.) The accom­panying 
video clips show how false jumps are eliminated. D; Dij ij P; Pij ij Figure 3 Un.ltered and .ltered 
distance matrix and transition probabilities for the clock pendulum sequence. While the .ltering has 
only a moderate effect on the distance table, the effect becomes pronounced when passed through the exponential 
function. The .ltered images are slightly smaller because the .lter kernel has to .t completely into 
the matrix and thus frames near the beginning and the end are thrown away. Figure 4 First and last frame 
of clock sequence with dead-end. A hand moves into the .eld of view at the end. If only instantaneous 
transition costs are used, the video texture will get stuck in the last frame. 3.2 Avoiding dead ends 
and anticipating the future The decision rule we have described so far looks only at the local cost of 
taking a given transition. It tries to match the appearance and dynamics in the two frames, but gives 
no consideration to whether the transition might, for example, lead to some portion of the video from 
which there is no graceful exit a dead end, in effect (Fig­ure 4). Much better results can be achieved 
by planning ahead by trying to predict the anticipated (increased) future cost of choosing a given transition, 
given the future transitions that such a move might necessitate. More precisely, let D;; ij be the anticipated 
future cost of a transition from frame i - 1 to frame j, i.e., a cost that re.ects the expected average 
cost of future transitions. We de.ne D;; ij by summing over all future anticipated costs, D;;P;; =(D;+ 
ajkD;;(4) ij ij) p jk . k Here, p is a constant used to control the tradeoff between taking multiple 
good (low-cost) transitions versus a single, poorer one. (Higher p favors multiple good transitions; 
lower p favors a single poorer one.) The constant ais used to control the relative weight of future transitions 
in the metric. For convergence, we must choose 0 <a<1 (in practice, we use 0. 99 = a= 0. 999). The probabil- 
 P;; P; ij ij Figure 5 Probability matrices for clock sequence with dead end. The original probability 
matrix causes the player to run to the end and get stuck. The new matrix based on future costs causes 
the system to jump out early, before getting stuck in the dead end. ities P;;, jk are de.ned as before, 
but using D;; instead of D; P;; ij . exp (-Di;;+1, j/s). (5) The pair of equations (4) and (5) can be 
solved using a simple it­erative algorithm, i.e., by alternating the evaluation of (4) and (5). Unfortunately, 
this algorithm is slow to converge. A faster variant on equation (4) can be derived by making the fol­lowing 
observation. As s. 0, the P;; jk in equation (5) will tend to 1 for the best transition, and 0 otherwise. 
We can therefore replace equation (4) with D;;D;; =(D;+ amin (6) ij ij) p jk . k This equation is known 
in the reinforcement learning community as Q-learning [15]. It corresponds to .nding the best possible 
continu­ation (path) through a graph with associated costs on edges, and has been proven to always converge. 
We can further increase the computational ef.ciency of the algorithm by being selective about which rows 
in D;;are updated at each ij step. Heuristically speaking, the lowest cost path often involves a transition 
from a frame near the end of the sequence, and the cost of this transition has to be propagated forward. 
We initialize with D;; ij ) p and de.ne ij =(D; D;; mj = min jk . (7) k Iterating from the last row to 
the .rst, we alternately compute D;; =(D;+ amj (8) ij ij) p and update the corresponding mj entries using 
equation (7). We re­peat these sweeps from back to front until the matrix entries stabi­lize. Figure 
5 shows the probability tables before and after applying the future cost computation. The original probability 
matrix causes the player to run to the end and get stuck. The new matrix based on future costs causes 
the system to jump out early, before getting stuck in the dead end. 3.3 Pruning the transitions While 
the above techniques can be used to produce perfectly good video textures, it is often desirable to prune 
the set of allowable transitions, both to save on storage space, and to improve the quality of the resulting 
video (suppressing non-optimal transitions). We have examined two pruning paradigms: 1. Select only local 
maxima in the transition matrix for a given source and/or destination frame. 2. Set all probabilities 
below some threshold to zero. The .rst strategy .nds just the sweet spots in the matrix of possi­ble 
transitions between frames, since often a whole neighborhood of frames has good and very similar transitions 
to some other neighbor­hood of frames, and only the best such transition needs to be kept. This can be 
combined with the second strategy, which is applied after the .rst. Both strategies are generally applied 
after the future cost computation has been done. In the case of video loops, which are described in Section 
4.1, we use a slightly different pruning strategy. For video loops, we would like to .nd sequences of 
frames that can be played continuously with low average cost, de.ned as the sum of all the transition 
costs D; ij, divided by the total length of the sequence. It is straightforward to show that the average 
cost of a sequence of transitions is just the weighted average of the average costs of the transitions. 
Thus, for video loops, after pruning all transitions that are not local minima in the distance matrix, 
we compute the average cost for each transition, and keep only the best few (typically around 20).  
4 Synthesis: Sequencing the video texture Once the analysis stage has identi.ed good transitions for 
the video texture, we need to decide in what order to play the video frames. For this synthesis stage, 
we have developed two different algorithms: random play and video loops. Random play is very simple to 
describe. The video texture is begun at any point before the last non-zero-probability transition. After 
displaying frame i, the next frame j is selected according to Pij. Note that usually, Pi, i+1 is the 
largest probability, since D;ii = 0 (however, this is not necessarily true when using the anticipated 
future cost D;; ij , which is how the system avoids dead ends). This simple Monte-Carlo approach creates 
video textures that never repeat exactly and is useful in situations in which the video texture can be 
created on the .y from the source material. When a conventional digital video player is used to show 
video textures, it is necessary to create video loops that do in fact repeat with a .xed period. In this 
case the video texture can be played in standard loop mode by such a player. Generating such loops with 
the highest possible quality is actually a rather dif.cult problem, to which we devote the rest of this 
section. 4.1 Video loops Consider a loop with a single transition i . j, from source frame i to destination 
frame j, which we call a primitive loop. In order for the single transition to create a (non-trivial) 
cycle we must have i = j. Thus, the range of this loop is [j, i]. The cost of this loop is the .ltered 
distance between the two frames D; ij. One or more primitive loops can be combined to create additional 
cyclic sequences, called compound loops. To add one (primitive or compound) loop to another, their ranges 
must overlap. Otherwise, there is no way to play the .rst loop after the second has played. The resulting 
compound loop has a range that is the union of the ranges of the two original loops, and a length and 
cost that is the sum of the original lengths and costs. Compound loops may contain several repeated instances 
of the same primitive loop, and can thus be represented by a multiset, where the ordering of the loops 
is not important. Forward transitions i . j, where i +1 < j, can be added into a cycle as well. Although 
we have an algorithm that ef.ciently checks whether a multiset of forward and backward jumps is playable, 
the dynamic programming algorithm described below, which .nds the D(5) C(4) A(2) B(3)  1 2 3 4 5 6 7 
8 length A(2) B(3) C(4) D(5) 1 B(3) 2 B2(6) D(5) 3 B3(9) C(4) 4 B4(12) D2(10) 5 A(2) B5(15) CD(9) 
CD(9) 6 AB(5) AB(5) C2(8) D3(15) . . . Figure 6 Dynamic programming table for .nding optimal loops. Each 
entry lists the best compound loop of a given length that includes the primitive loop listed at the top 
of the column. Total costs are shown in parentheses. lowest cost compound loop of a given length, does 
not work with for­ward jumps, and we currently have no suitable extension. Our algo­rithm for creating 
compound loops of minimal average cost therefore considers only backward transitions (transitions i . 
j with i = j). In the remainder of this section we present the two algorithms we need to generate optimal 
loops that is, video loops with minimal cost for a given sequence length. The .rst algorithm selects 
a set of transitions that will be used to construct the video loop. The second algorithm orders these 
transitions in a legal fashion that is, in an order that can be played without any additional transitions. 
 4.2 Selecting the set of transitions The most straightforward way to .nd the best compound loop of a 
given length L is to enumerate all multisets of transitions of total length L, to select the legal ones 
(the compound loops whose ranges form a continuous set), and to keep the best one. Unfortunately, this 
process is exponential in the number of transitions considered. Instead, we use a dynamic programming 
algorithm. Our algorithm constructs a table of L rows, where L is the maximum loop length being considered, 
and N columns, where N is the number of tran­sitions, or primitive loops, being considered (see Figure 
6). The algorithm builds a list of the best compound loop of a given length that contains at least one 
instance of the primitive loop listed at the top of the column. Each cell in the table lists the transitions 
in the compound loop and the compound loop s total cost. The algorithm works by walking through the table, 
updating cells one row at a time. For each cell, it examines all compound loops of shorter length in 
that same column, and tries to combine them with compound loops from columns whose primitive loops have 
ranges that overlap that of the column being considered. (This assures that the created compound loops 
are actually playable, since the ranges of the constituent compound loops must overlap.) For example, 
the entry in row 5 column C is obtained by combining the entry in row 3 column C with the entry in row 
2 column D, which is possible since primitive loops C and D have ranges that overlap and have lengths 
that sum to 5. The combination with the lowest total cost becomes the new entry. For each of the LN cells 
examined, the algorithm must combine at most L - 1 compound loops from its column with at most N - 1 
entries from the other columns. The total computational complexity of the algorithm is therefore O(L2N2), 
with a space complexity of O(LN). Note that the full descriptions of the compound loops need not be stored 
during the computation phase: only backpointers to the originating cells (constituent compound loops) 
are needed. 4.3 Scheduling the primitive loops After .nding the list of primitive loops in the lowest 
cost compound loop, the transitions have to be scheduled in some order so that they form a valid compound 
loop. This is done as follows (we use the scheduling of {ABCD} in this example): 1. Schedule the transition 
that starts at the very end of the sequence as the very .rst transition to be taken. This would be A 
in our example. 2. The removal of this transition i . j may break the remaining primitive loops into 
one or more sets of continuous ranges. In our example, the removal of A breaks the remaining loops into 
two continuous-range sets {C, D} and {B}. Frame j is always contained in the .rst such set and we schedule 
next any transition from this set whose source frame occurs after j. In our example, C is the only transition 
that meets these criteria. 3. Repeat the previous step, removing transitions i . j until there are no 
more primitive loops left in the .rst range. In our example, D would be removed next by this repeated 
step. 4. Schedule any primitive loop in each of the following disjoint ranges, using the algorithm beginning 
at step 2. In our example, B is the only primitive loop left. 5. Continue with step 2 until all primitive 
loops are removed.  In our example, the loops are scheduled in the order A, C, D, B. The computational 
complexity of this algorithm is quadratic in the number of transitions in the compound loop. The scheduling 
algo­rithm can either be run in a deterministic fashion (e.g., taking the .rst legal transition encountered), 
or in a stochastic fashion (randomly selecting from the legally available transitions). The latter variant, 
which utilizes transitions with precisely the same frequency as in the compound loop, is an alternative 
to the Monte-Carlo sequencing algorithm presented earlier.  5 Rendering Although we favor transitions 
that introduce only small discontinu­ities in the motion, there are cases where no unnoticeable transitions 
are available in the sequence. This section describes techniques for disguising discontinuities in the 
video texture, and for blending in­dependently analyzed regions together. Instead of simply jumping from 
one frame to another when a transi­tion is made, the images of the sequence before and after the transi­tion 
can be blended together with standard cross-fading: frames from the sequence near the source of the transition 
are linearly faded out as the frames from the sequence near the destination are faded in. The fade is 
positioned so that it is halfway complete where the transition was scheduled. Although cross-fading of 
the transitions avoids abrupt image changes, it temporarily blurs the image if there is a misalignment 
be­tween the frames. The transition from sharp to blurry and back again is sometimes noticeable. In some 
situations, this problem can be ad­dressed by taking very frequent transitions so that several frames 
are always being cross-faded together, maintaining a more or less constant level of blur. together at 
a time. The algorithm computes a weighted average of all frames participating in a multi-way fade, B(x, 
y)= ai Ii(x, y), (9) i where the blending weights ai are derived from the shifted weighting kernels associated 
with each participating frame, normalized such that i ai =1. Another approach to reducing the blurriness 
of the transitions is to morph the two sequences together, so that common features in the two sets of 
frames are aligned. The method we use is based on the de-ghosting algorithm described by Shum and Szeliski 
[29] and is also related to automatic morphing techniques [2]. To perform the de-ghosting, we .rst compute 
the optical .ow be­tween each frame Ii participating in the multi-way morph and a reference frame IR 
(the reference frame is the one that would have been displayed in the absence of morphing or cross-fading). 
For every pixel in IR, we .nd a consensus position for that pixel by tak­ing a weighted average of its 
corresponding positions in all of the frames Ii (including IR). Finally, we use a regular inverse warping 
algorithm to resample the images such that all pixels end up at their consensus positions. We then blend 
these images together. When the video texture consists of several independently ana­lyzed regions, the 
rendering algorithm blends these regions together smoothly. We use the feathering approach commonly used 
for image mosaics [31], where the contribution of a given region (our analysis regions are typically 
overlapping) tapers gradually towards its edge. 6 Basic results The accompanying video clips (available 
on the CD-ROM, DVD, and Video Conference Proceedings) demonstrate several different video textures produced 
by the methods described so far. Here, we summarize these basic results; in the next section, we develop 
some extensions to the basic algorithms and show some additional results. Candle .ame. A 33-second video 
of a can­ dle .ame was turned into four different video textures: one random play texture; and three 
different video loops, each con­taining three different primitive loops. One of the video loops repeats 
every 426 frames. The other two repeat every 241 frames; these each use the same set of three primi­tive 
loops, but are scheduled in a different order. In the .gure at right, the position of the frame currently 
being displayed in the original video clip is denoted by the red bar. The red curves show the possible 
transitions from one frame in the original video clip to another, used by the random play texture. Clock. 
This example shows the ne­cessity for both the preservation of dynamics and the future cost com­putation. 
The input video sequence shows a clock with a swinging pen­dulum. Without considering dynam­ics, a forward-swinging 
pendulum is likely to match equally well with a backward-swinging frame, causing unnatural jumps in the 
motion. Adding in the temporal .ltering solves this problem. At the end of the input video, a hand moves 
into the frame. Without the future cost computation, the video texture will reach a dead end, from which 
no transition to the earlier video will work without a visual jump.  Our implementation of the cross-fading 
algorithm supports multi-The future cost computation solves this problem by increasing the way cross-fades, 
i.e., more than two subsequences can be blended probability of a transition before the hand comes into 
frame. Flag.A 38-second video of a .ying .ag was cycli.ed using the lowest average cost loop contained 
in the video. Video textures were created using no fading, cross-fading, and morphing. Cross-fading improves 
the quality of the transition, at the cost of a small amount of blurring. Morphing works even better 
at re­moving the jump without introducing blur, even though the align­ment is one stripe off the geometrically 
correct alignment.The wrong alignment that causes a fold to magically disappear during transition is 
almost invisible to the unsuspecting observer. Camp.re. A 10-second video of a camp.re was cycli.ed using 
a single transition. The transition is hardly visible without crossfading, but crossfading over four 
frames hides it entirely. Although the con­.guration of the .ames never repli­cates even approximately, 
the tran­sition is well hidden by the high temporal .icker.  Portrait. A 25-second video of a woman 
posing for a portrait was turned into a random-play video texture with 20 transitions. Al­though the 
frames across the tran­sitions are already quite similar, the morphing performs a subtle align­ment of 
details, such as the eye po­sitions, which hides the transitions almost entirely. Such video textures 
could be useful in replacing the static portraits that often appear on web pages.  Waterfall. This example 
of a wa­terfall works less well. The origi­nal 5 minute video sequence never repeats itself, and yet, 
unlike the camp.re, there is a great deal of continuity between the frames, making it dif.cult to .nd 
any un­noticeable transitions. Our best re­sult was obtained by selecting a 6­second source clip, and 
using cross-fading with frequent transitions so that averaging is always performed across multiple subsequences 
at once.Although the resulting video texture is blurrier than the orig­inal video clip, the resulting 
imagery is still fairly convincing as a waterfall.  Blowing grass. Here is another ex­ample that does 
not work well as a video texture. Like the waterfall sequence, the original 43-second video of blowing 
grass never re­peats itself. Unlike the waterfall se­quence, blurring several frames to­gether does not 
produce acceptable results. Our automatic morphing also fails to .nd accurate corre­spondences in the 
video frames. The best we could do was to cross­fade the transitions (using a 4-second clip as the source), 
which creates occasional (and objectionable) blurring as the video texture is played.  7 Extensions 
 In this section, we present several extensions to the basic idea of video textures: sound synthesis, 
in which an audio track is re­rendered along with the video texture; three-dimensional video tex­tures, 
in which view interpolation techniques are applied to sim­ulate 3D motion; motion factorization, in which 
the video frames are factored into separate parts that are analyzed and synthesized independently; and 
video-based animation, in which video texture is modi.ed under interactive control. 7.1 Sound synthesis 
 Adding sound to video textures is relatively straightforward. We simply take the sound samples associated 
with each frame and play them back with the video frames selected to be rendered.To mask any popping 
effects, we use the same multi-way cross-fading algorithm described in Section 5. The resulting sound 
tracks, at least in the videos for which we have tried this (Waterfall and Bon.re), sound very natural. 
 7.2 Three-dimensional video textures Video textures can be combined with traditional image-based ren­dering 
algorithms such as view interpolation [5, 18, 24] to obtain three-dimensional video textures. These are 
similar to the 3D video­based characters demonstrated in several video-based view interpo­lation systems 
[16, 19, 23], except that they are based on synthetic video textures instead of captured video clips. 
3D Portrait.We created a three-dimensional video texture from three videos of a smiling woman, taken 
simultaneously from three dif­ferent viewing angles about 20 degrees apart. We used the center camera 
to extract and synthesize the video texture, and the .rst still from each camera to estimate a 3D depth 
map, shown here. (As an alternative, we could have used some other 3D image-based modeling technique 
[21].) We then masked out the background us­ing background subtraction (a clear shot of the background 
was taken before .lming began). To generate each new frame in the 3D video ani­mation, we mapped a portion 
of the video texture onto the 3D surface, rendered it from a novel view­point, and then combined it with 
the .at image of the background warped to the correct location, using the algorithm described in [26]. 
 7.3 Motion factorization For certain kinds of more complex scenes, we can divide the original video 
into independently moving parts, and analyze each one sep­arately. This kind of motion factorization 
decreases the number of frame samples necessary to synthesize an interesting video texture. Interdependencies 
between different parts of the synthesized frames could later be added with supplemental constraints. 
The simplest form of motion factorization is to divide the frame into independent regions of motion, 
either manually or automatically. Swings. In this example, the video of two children on swings is man­ually 
divided into two halves: one for each swing. These parts are analyzed and synthesized indepen­dently, 
then recombined into the .­nal video texture. The overall video texture is signi.cantly superior to the 
best video texture that could be generated using the entire video frame. Balloons. For this example, 
we developed an automatic segmen­tation algorithm that separates the original video stream into regions 
that move independently. We .rst compute the variance of each pixel across time, threshold this image 
to obtain connected regions of mo­tion, and use connected component labeling followed by a morphological 
dilation to obtain the .ve re­gion labels (shown as color regions in this still). The independent regions 
are then analyzed and synthesized separately, and then re­combined using feathering. Motion factorization 
can be further extended to extract independent video sprites from a video sequence. For instance, we 
can use back­ground subtraction or blue-screen matting [30] to identify connected portions of the video 
image that change with time and to extract these portions as foreground elements with alpha. To create 
a video sprite, we .rst factor out the positional information by placing the element s centroid at the 
origin in each frame. We call this registering the ele­ment. We also store the velocity of the sprite 
at each frame, de.ned as the difference in the unregistered elements centroid positions. In the analysis 
phase the distance between frames is computed as a linear combination of the registered elements colors, 
alphas, and moving directions and speeds. The synthesis phase is performed by utilizing the optimal transitions 
computed by the analysis and adding back in the stored velocities across the transitions. Fish. We used 
background subtrac­tion to create a video sprite of a .sh, starting from 5 minutes of video of a .sh 
in a tank. Unfortunately, .sh are uncooperative test subjects who frequently visit the walls of the .sh 
tank, where they are hard to extract from the scene because of re.ec­tions in the glass. We therefore 
used as source material only those pieces of video where the .sh is swimming freely. (This requires generalizing 
the future cost computation to handle the possibility of multiple dead ends, but is otherwise straightforward.) 
 Using this technique, the .sh swims freely in two-dimensional space. Ideally, we would like to constrain 
its motion for example, to the boundaries of a .sh tank. The next section describes approaches to this 
problem.  7.4 Video-based animation Instead of using visual smoothness as the only criterion for generat­ing 
video, we can also add some user-controlled terms to the error function in order to in.uence the selection 
of frames. The simplest form of such user control is to interactively select the set of frames S in the 
sequence that are used for synthesis. In this case, we perform the analysis phase as before, optionally 
pruning the list of transitions. In the synthesis stage, however, we recompute the probabilities of the 
transitions, using a modi.ed form of equation (5), which takes into account the distance from the destination 
of the transition to the set of user-speci.ed frames S: Pij . exp (-(Di;;+1,j /s + w distance(j, S))) 
(10) Here, w trades off the weight of the user-control against the smooth­ness of the transitions. Runner. 
We took 3 minutes of video of a runner on a treadmill, starting at a slow jog and then grad­ually speeding 
up to a fast run. As the user moves a slider selecting a certain region of the video (the black region 
of the slider in the .g­ure), the synthesis attempts to select frames that remain within that region, 
while at the same time using only fairly smooth transitions to jump forward or backward in time. The 
user can therefore control the speed of the runner by moving the slider back and forth, and the runner 
makes natural-looking transitions between the different gaits. We expect it will be possible to extend 
this type of parametric motion control to other types of movements as well, thereby allowing much greater 
directorial control in the post-processing phase of video pro­duction. Watering can. As another exam­ple, 
we took a 15-second clip of a watering can pouring water into a birdbath. The central portion of this 
video, which shows the wa­ter pouring as a continuous stream, makes a very good video texture. We can 
therefore shorten or extend the pouring sequence by using the same technique as we did for the runner, 
only advancing the slider automatically at a faster or slower speed. Thus, the same mechanism can be 
used to achieve a natural­looking time compression or dilation in a video sequence. Mouse-controlled 
.sh. Instead of di­rectly specifying a preferred range of frames, we can select frames based on other 
criteria. For example, in or­der to interactively guide the path of the .sh presented earlier with a 
mouse, we could give preference to frames in which the .sh s video sprite has a certain desired velocity 
vector. In particular, if x is the current position of the .sh, y the desired position of the .sh (say 
the mouse location), and vi the velocity at frame i, then we can use the following distance function: 
D; ij = w1 Ii -Ij 2+ w2 E(vi, vj )+ w3 E(y -x, vj) (11) where w1, w2, w3 are user-speci.ed weights, 
Ii -Ij 2 is a modi.ed image distance metric that takes into account the difference in the two image sprites 
alpha channels, and E(v, v;) is a velocity error function . In our current implementation, E is proportional 
to the ; angle between v and v. In the runner example (Equation 10), in order to achieve interactive 
performance we added the extra error term to D;; ij directly, instead of adding the term to D; ij and 
re-running the precomputed future cost computation. It turns out that this technique does not work so 
well for directed movement: the system has trouble .nding good sequences on the .y that will avoid later 
bad transitions. To do this right, a larger-scale anticipated future cost computation is required. We 
therefore compute the future cost D;;ij using the tech­ ij from D;niques described in Section 3.2. Unfortunately, 
we have to do this precomputation for all possible values of E. In practice, we perform the precomputation 
for a set of eight different directions and dis­cretize the user input to one of these directions on 
the .y, choosing the precomputed probability table accordingly. Fish tank. The .nal example we show is 
a complete .sh tank, populated with arti.cial .sh sprites. The tank includes two sets of bubbles, two 
inde­pendently swaying plants, and a small number of independently moving .sh. The .sh can also be scripted 
to follow a path (here, the SIGGRAPH 2000 logo), using the same techniques described for the mouse-controlled 
.sh.  8 Discussion and future work In his 1966 science-.ction short story, Light of Other Days, Bob 
Shaw describes a material called slow glass, which traps photons coming into it, and emits them a year 
or two later [28]. Slow glass can be exposed in scenic locations (such as a woodland lake) and then placed 
on people s walls, where it gives a three-dimensional illusion of having a scenic view of your own. Video 
textures (which were partially inspired by this story) share some of the characteristic of slow glass, 
but also differ in important ways. Like slow glass, they are an attempt to capture the inherent dynamic 
characteristics of a scene. (Using a video camera array to capture a time-varying light .eld [17] would 
be another approach, but capturing enough data to play back for a year would be pro­hibitively expensive.) 
Video textures attempt to capture the inherent characteristics of a dynamic scene or event, without necessarily 
cap­turing all of the stochastically-varying detail inherent in a particular segment of time. Video textures 
also have the potential to give the artist creative con­trol over the appearance of the dynamic events 
they are depicting. By capturing a wide variety of similar looking video that is periodic or quasi-periodic, 
the user can then select which portions to use, and blend smoothly between different parameter settings. 
The video texture analysis and synthesis software takes care of making these transitions smooth and creating 
segments of the desired duration. How well do video textures work? For motions that are smooth and repetitive 
or quasi-repetitive, such as the kids on the swing, the can­dle .ame, the swaying balloons, the runner, 
and the smiling woman, the illusion works quite well. For complex stochastic phenomena with little discernible 
structure, like the water pouring out of the can, it also works well. We run into trouble when the phenomena 
are complex but also highly structured, like the grass blowing in the wind (we have also thus far failed 
at creating a convincing video texture for waves on a beach). Other highly structured phenomena like 
full-body human motion will also likely fail, unless we start using some higher-level motion and structure 
analysis. Our work suggests a number of important areas for future work: Better distance metrics. To 
create video textures, we need a distance metric that reliably quanti.es the perceived discontinuity 
of a frame transition. For most of our examples we used a simple L2 distance between images. Finding 
better features and distance functions will be crucial for improving the quality of video textures and 
for in­creasing their applicability. We have some initial promising results applying a wavelet-based 
distance metric [14] to some of our se­quences. We have also improved the metric for the .sh example 
by modeling it as a linear combination of several features and learning the coef.cients from hand-labeled 
training transitions. Better blending. To suppress residual visual discontinuities, we are currently 
using blending and morphing. We would like to explore techniques that allow for blending and morphing 
separately in dif­ferent frequency bands both in space and time, perhaps using mul­tiresolution splining 
techniques [4]. Maintaining variety. A signi.cant problem with generating long (in.nite) sequences of 
video from the same set of frames is that, after a while, the algorithm will .nd some optimal paths and 
more or less play the same series of frames over and over again. This requires that in addition to s, 
which controls randomness, we de.ne a parameter that penalizes a lack of variety in the generated sequences. 
Such a parameter would enforce that most (if not all) of the frames of the given input sequence are sometimes 
played and probabilistically vary the generated order of frames. Better tools for creative control. Another 
important area of future research will be the addition of more creative control over video tex­tures. 
An alternative to interactively controlling the parameters in a video animation would be to specify control 
points or keyframes as in conventional keyframe animation. For this, we need to develop op­timization 
techniques that generate smoothly playing video textures that obey user-supplied constraints. Better 
video animation control would enable us to generate complex scenes such as crowds; the animation controller 
could also be enhanced to include behavioral aspects such as .ocking. While many areas remain to be explored, 
we believe that video tex­tures provide an interesting new medium with many potential appli­cations, 
ranging from simple video portraits to realistic video syn­thesis. Video textures are just one example 
of the more general class of techniques we call video-based rendering. By re-using real-world video footage 
(in a manner analogous to image-based rendering), we can achieve a degree of photorealism and naturalness 
hard to match with traditional computer graphics techniques. We hope that this work will spur further 
research in this .eld, and that video textures, along with video-based rendering in general, will ultimately 
become an essential part of the repertoire of computer graphics techniques. References [1] Z. Bar-Joseph. 
Statistical learning of multi-dimensional tex­tures. Master s thesis, The Hebrew University of Jerusalem, 
June 1999. [2] D. Beymer. Feature correspondence by interleaving shape and texture computations. In IEEE 
Computer Society Confer­ence on Computer Vision and Pattern Recognition (CVPR 96), pages 921 928, San 
Francisco, California, June 1996. [3] C. Bregler, M. Covell, and M. Slaney. Video rewrite: Driv­ing visual 
speech with audio. Computer Graphics (SIG-GRAPH 97), pages 353 360, August 1997. [4] P. J. Burt and E. 
H. Adelson. A multiresolution spline with ap­plications to image mosaics. ACM Transactions on Graphics, 
2(4):217 236, October 1983. [5] S. E. Chen. QuickTime VR an image-based approach to virtual environment 
navigation. Computer Graphics (SIG-GRAPH 95), pages 29 38, August 1995. [6] J. De Bonet. Multiresolution 
sampling procedure for analysis and synthesis of texture images. Computer Graphics (SIG-GRAPH 97), pages 
361 368, August 1997. [7] P. Debevec et al., editors. Image-Based Modeling, Rendering, and Lighting, 
SIGGRAPH 99 Course 39, August 1999. [8] P. E. Debevec, C. J. Taylor, and J. Malik. Modeling and ren­dering 
architecture from photographs:A hybrid geometry-and image-based approach. Computer Graphics (SIGGRAPH 
96), pages 11 20, August 1996. [9] A. A. Efros and T. K. Leung. Texture synthesis by non­parametric sampling. 
In Seventh International Conference on Computer Vision (ICCV 99), pages 1033 1038, Kerkyra, Greece, September 
1999. [10] A. Finkelstein, C. E. Jacobs, and D. H. Salesin. Multiresolution video. Proceedings of SIGGRAPH 
96, pages 281 290, August 1996. ISBN 0-201-94800-1. Held in New Orleans, Louisiana. [11] W. T. Freeman, 
E. H. Adelson, and D. J. Heeger. Motion without movement. Computer Graphics (Proceedings of SIG-GRAPH 
91), 25(4):27 30, July 1991. [12] D. J. Heeger and J. R. Bergen. Pyramid-based texture analy­sis/synthesis. 
Proceedings of SIGGRAPH 95, pages 229 238, August 1995. [13] Workshop on Image-Based Modeling dering, 
Stanford University, March http://graphics.stanford.edu/workshops/ibr98/. and Ren­1998. [14] C. E. Jacobs, 
A. Finkelstein, and D. H. Salesin. Fast multireso­lution image querying. Proceedings of SIGGRAPH 95, 
pages 277 286, August 1995. [15] L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforce­ment learning: 
A survey. Journal of Arti.cial Intelligence Re­search, 4, 1996. [16] T. Kanade, P. W. Rander, and P. 
J. Narayanan. Virtualized reality: constructing virtual worlds from real scenes. IEEE MultiMedia Magazine, 
1(1):34 47, Jan-March 1997. [17] M. Levoy and P. Hanrahan. Light .eld rendering. In Computer Graphics 
Proceedings, Annual Conference Series, pages 31 42, Proc. SIGGRAPH 96 (New Orleans), August 1996. ACM 
SIGGRAPH. [18] L. McMillan and G. Bishop. Plenoptic modeling: An image-based rendering system. Computer 
Graphics (SIG-GRAPH 95), pages 39 46, August 1995. [19] S. Moezzi et al. Reality modeling and visualization 
from mul­tiple video sequences. IEEE Computer Graphics and Appli­cations, 16(6):58 63, November 1996. 
[20] S. A. Niyogi and E. H. Adelson. Analyzing and recognizing walking .gures in xyt. In IEEE Computer 
Society Confer­ence on Computer Vision and Pattern Recognition (CVPR 94), pages 469 474, Seattle, Washington, 
June 1994. [21] F. Pighin, J. Hecker, D. Lischinski, D. H. Salesin, and R. Szeliski. Synthesizing realistic 
facial expressions from pho­tographs. In Computer Graphics (SIGGRAPH 98) Proceed­ings, pages 75 84, Orlando, 
July 1998. ACM SIGGRAPH. [22] R. Polana and R. C. Nelson. Detection and recognition of periodic, nonrigid 
motion. International Journal of Computer Vision, 23(3):261 282, 1997. [23] S. Pollard et al. View synthesis 
by trinocular edge matching and transfer. In British MachineVision Conference (BMVC98), Southampton, 
England, September 1998. [24] S. M. Seitz and C. M. Dyer. View morphing. In Computer Graphics Proceedings, 
Annual Conference Series, pages 21 30, Proc. SIGGRAPH 96 (New Orleans), August 1996. ACM SIGGRAPH. [25] 
S. M. Seitz and C. R. Dyer. View invariant analysis of cyclic motion. International Journal of Computer 
Vision, 25(3):231 251, December 1997. [26] J. Shade, S. Gortler, L.-W. He, and R. Szeliski. Layered depth 
images. In Computer Graphics (SIGGRAPH 98) Proceedings, pages 231 242, Orlando, July 1998. ACM SIGGRAPH. 
[27] J. Shade, D. Lischinski, D. Salesin, T. DeRose, and J. Sny­der. Hierarchical images caching for 
accelerated walkthroughs of complex environments. In Computer Graphics (SIG-GRAPH 96) Proceedings, pages 
75 82, Proc. SIGGRAPH 96 (New Orleans), August 1996. ACM SIGGRAPH. [28] B. Shaw. Light of other days. 
In Other Days, Other Eyes. Ace Books,NewYork,1972. (alsopublishedin Analog and various sci-. anthologies). 
[29] H.-Y. Shum and R. Szeliski. Construction of panoramic mo­saics with global and local alignment. 
International Journal of Computer Vision, 36(2):101 130, February 2000. [30] A. R. Smith and J. F. Blinn. 
Blue screen matting. In Computer Graphics Proceedings, Annual Conference Series, pages 259 268, Proc. 
SIGGRAPH 96 (New Orleans),August 1996.ACM SIGGRAPH. [31] R. Szeliski and H.-Y. Shum. Creating full view 
panoramic im­age mosaics and texture-mapped models. In Computer Graph­ics (SIGGRAPH 97) Proceedings, 
pages 251 258, Los Ange­les, August 1997. ACM SIGGRAPH. [32] J. Torborg and J. T. Kajiya. Talisman: Commodity 
realtime 3D graphics for the PC. In Computer Graphics Proceedings, An­nual Conference Series, pages 353 
363, Proc. SIGGRAPH 96 (New Orleans), August 1996. ACM SIGGRAPH. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>345022</article_id>
		<sort_key>499</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>56</seq_no>
		<title><![CDATA[Escherization]]></title>
		<page_from>499</page_from>
		<page_to>510</page_to>
		<doi_number>10.1145/344779.345022</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=345022</url>
		<abstract>
			<par><![CDATA[<p>This paper introduces and presents a solution to the &#8220;Escherization&#8221; problem: given a closed figure in the plane, find a new closed figure that is similar to the original and tiles the plane. Our solution works by using a simulated annealer to optimize over a parameterization of the &#8220;isohedral&#8221; tilings, a class of tilings that us flexible enough to encompass nearly all of Escher's own tilings, and yet simple enough to be encoded and explored by a computer. We also describe a representation for isohedral tilings that allows for highly interactive viewing and rendering. We demonstrate the use of these tools&#8212;along with several additional techniques for adding decorations to tilings&#8212;with a variety of original ornamental designs.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Escher]]></kw>
			<kw><![CDATA[morphing]]></kw>
			<kw><![CDATA[optimization]]></kw>
			<kw><![CDATA[simulated annealing]]></kw>
			<kw><![CDATA[tesselations]]></kw>
			<kw><![CDATA[tilings]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>J.6</cat_node>
				<descriptor>Computer-aided design (CAD)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Fine arts</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010472.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Architecture (buildings)->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010439.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010470</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Fine arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14064111</person_id>
				<author_profile_id><![CDATA[81100155157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Craig]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Kaplan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P63622</person_id>
				<author_profile_id><![CDATA[81100188207]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Salesin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington and Microsoft Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1138362</ref_obj_id>
				<ref_obj_pid>563732</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Howard Alexander. The computer/plotter and the 17 ornamental design types. Proceedings of SIGGRAPH'75, pages 160-167, 1975.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>105491</ref_obj_id>
				<ref_obj_pid>105488</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[E.M. Arkin, L. P. Chew, D. P. Huttenlocher, K. Kedem, and J. S. B. Mitchell. An efficiently computable metric for comparing polygonal shapes. PAMI(13), pages 209-216, 1991.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134003</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Thaddeus Beier and Shawn Neely. Feature-based image metamorphosis. Proceedings of SIGGRAPH'92, pages 35-42, 1992.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[William W. Chow. Automatic generation of interlocking shapes. Computer Graphics and Image Processing, 9:333-353, 1979.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[William W. Chow. Interlocking shapes in art and engineering. Computer Aided Design, 12:29-34, 1980.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Andrew Crompton. Grotesque geometry, http://dspace, dial .pipex. com/crompton/Home, shtml.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Douglas J. Dunham. Creating hyperbolic escher patterns. In H.S.M. Coxeter et al., editor, M.C. Escher: Art and Science, pages 241-247. Elsevier Science Publishers B.V., 1986.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>2295780</ref_obj_id>
				<ref_obj_pid>616080</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Andrew Glassner. Frieze groups. IEEE Computer Graphics and Applications, 16(3):78-83, May 1996.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618531</ref_obj_id>
				<ref_obj_pid>616052</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Andrew Glassner. Andrew glassner's notebook: Aperiodic tiling. IEEE Computer Graphics &amp; Applications, 18(3):83-90, May- June 1998. ISSN 0272- 1716.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618551</ref_obj_id>
				<ref_obj_pid>616053</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Andrew Glassner. Andrew glassner's notebook: Penrose tiling. IEEE Computer Graphics &amp; Applications, 18(4), July - August 1998. ISSN 0272-1716.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>19304</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Branko Grfinbaum and G. C. Shephard. Tilings and Patterns. W. H. Freeman, 1987.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[GTK--. http ://gtkmm. sourceforge, net.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>536264</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Douglas Hofstadter. Metamagical Themas." Questing for the Essence of Mind and Pattern. Bantam Books, 1986.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Daniel H. Huson and Olaf Delgado Friedrichs. Reptiles. ftp-//ftp. uni - biele feld. de/pub/math/t i i ing/rept i les/.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Hans Van Lemmen. Tiles." 1000 Years of Architectural Decoration. Harry N. Abrams, Inc., 1993.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Raph Levien. libart, http://www, levien, com/libart/.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>148286</ref_obj_id>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. Numerical recipes in c: The art of scientific computing (2nd ed.). 1992. ISBN 0-521-43108-5. Held in Cambridge.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Doris Schattschneider. M.C. Escher: Visions of Symmetry. W.H. Freeman, 1990.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[The GIMP toolkit, http ://www. gtk. org.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[M.C. Escher (tran. Karin Ford). Escher on Escher: Exploring the Infinite. Henry N. Abrams, Inc., 1989.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280948</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Michael T. Wong, Douglas E. Zongker, and David H. Salesin. Computergenerated floral ornament. Proceedings ofSIGGRAPIt'98, pages 423-434, 1998.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Escherization Craig S. Kaplan1 1University of Washington Abstract This paper introduces and presents 
a solution to the Escherization problem: given a closed .gure in the plane, .nd a new closed .g­ure that 
is similar to the original and tiles the plane. Our solution works by using a simulated annealer to optimize 
over a parameter­ization of the isohedral tilings, a class of tilings that is .exible enough to encompass 
nearly all of Escher s own tilings, and yet simple enough to be encoded and explored by a computer. We 
also describe a representation for isohedral tilings that allows for highly interactive viewing and rendering. 
We demonstrate the use of these tools along with several additional techniques for adding decora­tions 
to tilings with a variety of original ornamental designs. CR Categories: I.3.5 [Computational Geometry 
and Object Modeling]: Geometric al­gorithms, languages and systems; I.3.8 [Computer Graphics]: Applications; 
J.5 [Arts and Humanities]: Fine arts; J.6 [Computer-Aided Engineering]: Computer-aided de­sign (CAD). 
Keywords: Tilings, tesselations, morphing, optimization, simulated annealing, Escher Introduction Tilings 
are as old as civilization. Our ancestors earliest experience with tilings probably arose out of the 
quest for regularity in the construction of walls, .oors, and ceilings. This regularity could at once 
simplify the task of construction and lend a sense of order and uniformity to the objects being constructed. 
Historical uses of ornamental tilings abound; numerous examples from as early as the twelfth century 
survive today [15]. Perhaps the most renowned example is the Alhambra palace in Granada, Spain. The Moors 
who built the Alhambra became masters of geometric ornament, covering every surface of the palace with 
intricate tilings of astonishing beauty. By the time the Dutch graphic artist M.C. Escher began studying 
the reg­ular division of the plane in the .rst half of the twentieth century, tiling as an art form had 
passed mostly into history, to be replaced by the grow­ing development of a systematic math­ematical 
theory. Escher was deeply in­spired by the interlocking geometric forms of the Moors but felt it a pity 
that they were forbidden by their reli­gion from depicting real-world objects in their art [20]. He undertook 
as a per­sonal quest the reinvention of geomet­ric art, substituting easily-recognized motifs such as 
animal forms for the pu­rity of the Moorish rosettes and poly­gons. Escher arrived at each of his interlocking 
animal forms after a great deal of tinkering and manipulation. Over the years, he became more pro.cient 
at inventing new arrangements of motifs, develop­ing his own layman s theory of tilings to track the 
ground he had covered and suggest new directions for exploration. He managed over his career to produce 
a notebook with more than a hundred of these ingenious, playful designs [18].  Taking our inspiration 
from Escher and his elegant work, we at­tempt to solve the following problem in this paper: Problem ( 
ESCHERIZATION ): Given a closed plane .gure S (the goal shape ), .nd a new closed .gure T such that: 
1. T is as close as possible to S;and 2. copies of T .t together to form a tiling of the plane.  This 
problem is tricky in that for a suf.ciently large perturbation of the goal shape, it is always possible 
to .nd a tiling in a trivial sense. (Let T , for example, be a square.) We need to formalize the measure 
of closeness in such a way that it both preserves the essence of the goal shape S and at the same time 
produces new shapes T that are known to tile. This paper presents a solution to the Escherization problem 
that is able to .nd reasonable-looking tiles for many real-world shapes Permission to make digital or 
hard copies of part or all of this work or personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. SIGGRAPH 2000, New Orleans, 
LA USA &#38;#169; ACM 2000 1-58113-208-5/00/07 ...$5.00 (see, for example, the Escherized version of 
Escher s own self­portrait, shown in Figure 2). Creating such tilings requires solving a number of subproblems, 
which we discuss in this paper. The .rst dif.culty is in selecting a set of tiling types that are both 
simple enough to be encoded and manipulated by a computer, and .exible enough to express most of the 
ornamental designs we would like to create. A second problem is in .nding consistent and complete pa­rameterizations 
for these tilings that is, parameterizations that are always guaranteed to produce correct tilings of 
a given type ( con­sistent ) and that are furthermore capable of producing all tilings of that type ( 
complete ). Though fundamental to the analysis of pat­terns and tilings, to our knowledge this problem 
has never before been addressed for the types of tilings we consider. A third prob­lem is in choosing 
a good measure of closeness. A fourth challenge is in designing an optimizer to search over all possible 
tiling types, their parameterizations, and tile shapes in order to .nd a good ap­proximation to the goal 
tile. A .fth problem is in creating a repre­sentation for these tilings that allows for highly interactive 
viewing and editing. A .nal problem is in decorating and rendering the re­sulting tiles. Unlike most 
research projects in computer graphics, this one is mo­tivated more by intellectual curiosity than by 
practical import. Nev­ertheless, a solution to the Escherization problem does have certain applications 
in the real world. Tilings are of course useful as .oor and wall coverings. In manufacturing, the outlines 
of tiles can be cut repeatedly out of stone using a process known as water-jet cut­ting. Automatically-designed 
tilings could just as easily be carved out of wood or even sewn into a quilt. This suggests a further 
ap­plication, proposed by Chow [5]: a tiling program could be used to lay out copies of a part to be 
cut out a sheet of some material. If the copies are arranged in a tiling, they can be cut from the sheet 
without creating any waste material (except around the outer edges of the sheet). 1.1 Related Work Several 
authors have explored the possibility of creating ornament in various forms by computer. A paper at the 
second annual SIG-GRAPH conference featured a system for drawing .gures con­strained to the seventeen 
planar symmetry groups [1]. More re­cently, Glassner examined the synthesis of frieze patterns [8] and 
aperiodic tilings [9, 10], which can be used for generating orna­ments for bands and for the 2-D plane, 
respectively. Wong et al. in­vestigated algorithms for computer-generated .oral ornament [21] and surveyed 
other previous work in creating these kinds of orna­mental designs. In addition, software created speci.cally 
for allowing users to con­struct tilings of the plane has been around for at least twenty years. Chow 
had a very successful FORTRAN program [4] that let the user input the portion of the tile that is independent, 
i.e., not ex­pressed in terms of some other portion of the tile. The program then .lled in the remaining 
part of the tile and replicated it in the plane. Reptiles [14], by Huson and Friedrichs, is a complex 
system that understands a large class of mathematically-interesting tilings. Reptiles has since been 
expanded into Funtiles, an even more so­phisticated tool that can create tilings in non-Euclidean geometries. 
Lee s TesselMania! is a marvelous program for giving children an understanding of symmetry and tilings. 
A number of individuals are actively designing new Escher-like tilings, aided by illustration software. 
Crompton [6] has compiled an extensive list of recent contributions to tesselation-based art. Still, 
none of these earlier efforts attempt in any way to .nd tilings automatically whose tiles approximate 
a particular goal shape, the work we describe here. 1.2 Overview We begin with background on the mathematical 
theory of tilings, leading into a description of the isohedral tilings (Section 2), on which the rest 
of this work is based. We then address each of the remaining subproblems in turn: parameterizing the 
isohedral tilings (Section 3); developing a measure of closeness between two tiles (Section 4); designing 
an optimizer for .nding the best tiles (Sec­tion 5); representing the resulting tilings for ef.cient 
editing and viewing (Section 6); and decorating and rendering the tiles (Sec­tion 7). We end with a discussion 
of our results (Section 8) and ideas for future work (Section 9).  2 Mathematical theory of tilings 
In this section, we present background on only the parts of tiling theory necessary to understand the 
research work presented in the rest of this paper. Readers seeking a more in-depth analysis of tilings 
should consult the highly accessible treatise on tiling theory, Gr¨unbaum and Shephard s Tilings and 
Patterns [11]. 2.1 Tilings A tiling of the plane is a collection of shapes, called tiles, that cover 
the plane without any gaps or overlaps. That is, every point in the plane is contained in at least one 
tile, and the intersection of any two tiles is a set with zero area (we regard tiles as closed sets, 
and allow them to intersect along their boundaries). Given certain natural analytic restrictions on the 
shapes of tiles [11, sec 3.2], the intersection of any set of tiles will either be empty, a point, or 
a simple curve. When the intersection is a curve, we call that curve a tiling edge. When the intersection 
is a point, in which case that point will necessarily be a meeting place of at least three tiles, we 
call that point a tiling vertex. Every tile can be decomposed, based on intersections with its neigh­bours, 
into a sequence of tiling vertices joined by tiling edges. These must be distinguished from the vertices 
and edges of the tiles (if the tiles are in fact polygons), which we will call shape vertices and shape 
edges, respectively, to differentiate them from their tiling counterparts. Although the fea­tures of 
the tiling occupy the same positions as the features of the tiles, they may break down differ­ently. 
For the blue tile in the tiling on the right, A is a shape vertex  but not a tiling vertex, B is a tiling 
vertex but not a shape vertex, and C is both a tiling vertex and a shape vertex. We will also make use 
of the tiling polygon, the poly­gon formed by joining the tiling vertices that lie on a given tile, shown 
here as a red dashed line. This polygon is important in de­scribing the structure of the tiling. In many 
of the tilings we see every day on walls and streets, the tiles all have the same shape. If any given 
tile in a tiling is congruent to any other through a rigid motion of the plane, we say that the tiling 
is monohedral. Similarly, a k-hedral tiling is one in which every tile is congruent to one of k different 
prototiles. When k =2,we also use the term dihedral to describe the tiling. 2.2 Isohedral tilings A 
symmetry of a .gure in the plane is a rigid motion of the plane that maps the .gure onto itself. Every 
.gure in the plane necessarily has an associated set of symmetries, even if it is just the trivial set 
containing the identity motion. It is easy to see that the symme­tries of a .gure have a natural group 
structure under composition of Figure 3 Both of these tilings are monohedral, but the one on the left 
is iso­hedral and the one on the right is not. The re.ection that maps tile A onto tile B is not a symmetry 
of the tiling on the right. IH55 IH61 IH71 Figure 4 An isohedral tiling type imposes a set of adjacency 
constraints on the tiling edges of a tile. When the bottom edge of the square deforms into the dashed 
line, the other edges must respond in some way to preserve the tiling. The six resulting tiles here are 
from six different isohedral types. rigid motions. The set of symmetries of a .gure is therefore called 
the symmetry group of that .gure. If the symmetry group of a .g­ure contains linearly independent translations, 
we call that .gure periodic. For two congruent tiles A and B in a tiling, there will be some rigid motion 
of the plane that carries one onto the other (there may in fact be several). A somewhat special case 
occurs when the rigid motion is also a symmetry of the tiling. In this case, when A and B are brought 
into correspondence, the rest of the tiling will map onto itself as well. We then say that A and B are 
transitively equivalent. Transitive equivalence is an equivalence relation that partitions the tiles 
into transitivity classes. When a tiling has only one transitiv­ity class, we call the tiling isohedral. 
More generally, a k-isohedral tiling has k transitivity classes. An isohedral tiling is one in which 
a single prototile can cover the entire plane through repeated appli­cation of rigid motions from the 
tiling s symmetry group. Note that an isohedral tiling must be monohedral, though the converse is not 
true [11, p. 31], as Figure 3 illustrates. We use the isohedral tilings as a mathematical basis for our 
explo­ration of computer generation of ornamental tilings. They achieve a satisfying balance between 
.exibility and convenience. On the one hand, they are capable of representing a wide subjective range 
of tilings. Of all the monohedral tilings in Escher s notebook, only one is not isohedral (the exception 
is based on a special mono-but not isohedral tiling .rst shown to Escher by Roger Penrose). More­over, 
in Escher s dihedral tilings, tiles of each of the two motifs can be paired up to form a single supertile 
that tiles that plane isohedrally. On the other hand, the isohedral tilings can be clas­si.ed into a 
small number of symbolically-encoded families (the following subsections give more details about this 
classi.cation). It is therefore fairly easy to create a system to manipulate and render them. 2.3 Isohedral 
families By de.nition, an isohedral tiling is bound by a set of geometric constraints: congruences between 
tiles must be symmetries of the tiling. Gr¨unbaum and Shephard showed that those geometric con­straints 
can be equated with a set of combinatoric constraints ex­pressing the adjacency relationship between 
edges of a tile. They proved that these constraints yield a division of the isohedral tilings into precisely 
93 distinct types or families,1 referred to individually as IH1, . . . , IH93 and collectively as IH 
[11, sec. 6.2]. Each family encodes information about how a tile s shape is constrained by the adjacencies 
it is forced to maintain with its neighbours. A defor­mation in a tiling edge is counterbalanced by deformations 
in other edges; which edges respond and in what way is dependent on the tiling type, as shown in Figure 
4. Isohedral tilings have the property that if you list the valence of each tiling vertex as you move 
around any given tile, the list will be consistent across all tiles in the tiling. This list is fundamental 
to the topological structure of the tiling and is called its topological type. For example, the topological 
type of IH16, shown on the left, is 36 , since there are 6 different tiling vertices around each tile, 
each of valence 3. Every isohedral tiling belongs to one of eleven different topological types [11, sec. 
2.7]. In any periodic tiling, it is pos­sible to identify a collection of tiles that together cover the 
plane using only the translations from the tiling s symmetry group. Any such collection that is connected 
and minimal in size is called a translational unit of the tiling. Within a translational unit, all tiles 
must have different orientations, which are referred to as the aspects of the tiling. IH16 has three 
aspects, shown in varying shades of blue above. These three tiles comprise one possible translational 
unit, with translation vectors T1 and T2. 2.4 Incidence symbols The adjacency constraints between the 
tiling edges of a tile are sum­marized by an incidence symbol. Given a rendering of a tiling, the incidence 
symbol can be constructed in a straightforward way. Figure 5 Five steps in the derivation of a tiling 
s incidence symbol. Figure 5 shows .ve steps in the derivation of the incidence sym­bol for our sample 
tiling. To obtain the .rst part of the incidence symbol, pick an arbitrary tiling edge as a starting 
point, assign that edge a single-letter name, and draw an arrow pointing counterclock­wise around the 
tile (step 1). We then copy the edge s label to all other edges of the tile related to it through a symmetry 
of the tiling (step 2). Should the edge get mapped to itself with a reversal of di­rection, it is given 
a double-headed arrow and becomes undirected. We then proceed counterclockwise around the tile to the 
next un­labeled edge (if there is one) and repeat the process (step 3). The .rst half of the symbol is 
obtained by reading off the assigned edge 1In tiling theory, seemingly arbitrary numbers like 93 are 
not uncom­mon; enumerations of families of tilings tend to have sets of constraints that collapse certain 
cases and fracture others. IH1 IH64 IH58 IH17 Figure 6 Examples (from left to right) of J, U, S and 
I edges. In each case, the tiling edge with the given shape is highlighted in red. names (step 4). A 
directed edge is superscripted with a sign indi­cating the coherence of its arrow with the traversal 
direction. Here, a plus sign is used for a counterclockwise arrow and a minus sign for a clockwise arrow. 
The second half of an incidence symbol records how, for each dif­ferent name, a tiling edge of that name 
is related to the correspond­ing edge of the tile adjacent to it. To derive this part of the symbol, 
we copy the labeling of the tile to its neighbours (step 5). Then, for each unique edge letter assigned 
in the .rst step, we write down the edge letter adjacent to it in the tiling. If the original edge was 
directed, we also write down a plus or minus sign, depending on whether edge direction is respectively 
preserved or reversed across the edge. A minus sign is used if the arrows on the two sides of an edge 
are pointing in the same direction and a plus sign is used otherwise. For the running example, the incidence 
symbol turns out ++ --- + to be [ab+ ccb- a ; acb+]. Note that the incidence symbol is not unique; edges 
can be renamed and a different starting point can be chosen. But it can easily be checked whether two 
incidence symbols refer to the same isohedral type. Every isohedral type is fully described in terms 
of a topological type and an incidence symbol. Enumerating all possible topologi­cal types and incidence 
symbols and then eliminating the ones that do not result in valid tilings or that are trivial renamings 
of other symbols leads to the classi.cation given by Gr¨unbaum and Shep­hard. 2.5 Tile shapes Within 
a single isohedral type, tilings are distinguished from each other by their shapes, consisting of the 
positions of the tiling ver­tices and the shapes of the curves that join them. In the next section, we 
will address the question of .nding, for each isohedral type, a parameterization of the tiling vertices 
that yields all and only those tiling polygons compatible with the type. To our knowledge, this problem 
has not been previously explored. On the other hand, the constraints on the shapes of tiling edges are 
simple to describe. Although the underlying choice of how to rep­resent a curve is left open, the tiling 
s symmetries imply a large reduction in the tiling edges degrees of freedom. These constraints can be 
extracted directly from the tiling s incidence symbol. We enumerate four cases for the structure of a 
tiling edge. For each case, Figure 6 gives a tiling with such an edge. If some directed edge is adjacent 
to itself without a .ip, then a tile s neighbour across that edge is adjacent through a half-turn. This 
ro­tation forces the edge shape to itself be symmetric through a half­turn about its centre. We call 
such an edge an S edge as a visual mnemonic. Only half of an S edge is free; the other half must com­plete 
the rotational symmetry. An undirected edge must look the same starting from either end, meaning it must 
have a line of mirror symmetry through its mid­point. If the edge is adjacent to an edge other than itself, 
it is free to take on any curve with this mirror symmetry. We call it a U edge. Again, only half of a 
U edge is free. If an undirected edge is adjacent to itself, or if a directed edge is adjacent to itself 
with a change in sign, that edge must have both S symmetry and U symmetry. The only shape that has both 
is a straight line, leading us to call such an edge an I edge. The remaining case is when a directed 
edge is adjacent to some other directed edge. Such an edge is free to take on any shape, and wecall ita 
J edge. Note also that if an edge x is adjacent to an edge y,then x and y have the same shape (even though 
they have different names). In this case, we need only specify one tiling edge, since the other is entirely 
constrained to it. Thus, the tiling edges of IH16 can be summarized by one curve: the shape of the edge 
labeled b. Edges labeled a are I edges and have no degrees of freedom, and edges labeled c are constrained 
to b.  3 Parameterizing the isohedral tilings Like the shape vertices, tiling vertices cannot move independently 
of each other. Moving one tiling vertex forces the others to move to preserve the tiling. The exact nature 
of this movement depends on the tiling type in question. The incidence symbol for a tiling type implies 
a set of constraints on the tiling polygon s edge lengths and interior angles. Any tile of that type 
will have a tiling polygon that obeys those constraints. If we hope to build a generative model of isohedral 
tilings, it is not suf.cient to merely recognize the constraints on the shape vertices: we need a way 
to explicitly navigate the space of legal tiling poly­gons. For each isohedral type we need a parameterization 
of the tiling vertices for tilings of that type. The parameterization should be complete, in the sense 
that for every legal con.guration of tiling vertices, there is a set of parameters that generates that 
con.gura­tion. We also require it to be consistent, in the sense that every set of parameters generates 
legal tiling vertices. To our knowledge, no tiling vertex parameterizations have ever been given for 
IH. They represent a nontrivial extension to the table of information about IH found in Gr¨unbaum and 
Shephard. We have developed a set of consistent and complete parameteriza­tions for the isohedral types 
(of course, the history of tiling theory has experienced its share of imperfect analyses [11, Sec. 6.6]). 
They were derived by determining angle and length constraints from the incidence symbols and parameterizing 
the unconstrained degrees of freedom. In some cases, parameterizations are shared between tiling types: 
nine tiling types have squares as tiling polygons (im­plying a parameterization with zero parameters), 
and seven have parallelograms (implying two parameters). These easy parameter­izations are balanced by 
tiling types with one-of-a-kind structure that can take some thought to derive. In all, the 93 isohedral 
types require 45 different parameterizations. Diagrams of the parameter­izations appear in full in Figures 
9 and 10. To give the .avor of these parameterizations, here is a sketch of the derivation for our running 
example, IH16 (see Figure 7). We begin by placing at least enough tiles to completely surround one central 
tile, and marking up the tiles with the labels from the tiling s inci­dence symbol. Now consider the 
situation at tiling vertex A.This vertex is surrounded by three copies of the same angle from three different 
tiles, namely . FAB, the angle between the a edges. It fol­lows that the tiling polygon must have a 120. 
angle at that vertex. The same observation applies to vertices C and E. Thus, .FAB, .BCD,and .DEF are 
all 120. isosceles triangles. Because these isosceles triangles can be constructed given only the edge 
op­posite the 120. angle, the tiling polygon depends entirely on the skeleton triangle .BDF. Furthermore, 
the incidence symbol re­veals a line of bilateral symmetry in the tile across AD, forcing .BDF to be 
isosceles. The only degrees of freedom left in the Figure 7 The diagram used to establish a tiling vertex 
parameterization for IH16. For simplicity, the arrows indicating edge direction have been left out of 
the diagram. tiling polygon are the lengths of AD and BF .However,asdis­cussed in the next section, 
the shape comparison metric that we would like to use is independent of scale. We can factor out the 
de­ pendence on scale by .xing ||BF || =1 and keeping just a single parameter: v0 =||AD||. Figure 8 shows 
tilings of type IH16 that can result from different values of this single parameter. v0 = 1 v0 =1 v0 
= v2v0 =2 23 Figure 8 Some examples of IH16 with different values for the single param­eter in its tiling 
vertex parameterization.  4 The shape metric The Escherization problem raises the dif.cult question 
of how to compare two shapes. An answer should be in the form of a metric that would take two outlines 
and return a nonnegative real number; zero would mean that the outlines are identical, and higher posi­tive 
values would denote shapes that are increasingly dissimilar. To simplify the rest of the Escherization 
algorithm, we would also like the metric to be insensitive to rigid motion or scale of either of the 
shapes. Fortunately, such metrics have been developed by computer vision researchers. We use the metric 
created by Arkin et al. for comparing polygons [2]. Their metric represents the input polygons as turning 
functions, functions that map fraction of arc length in a polygon to the angle of the polygon at that 
point. Turning functions are nat­urally translation and scale independent. Translation of a turning function 
corresponds to rotation of the polygon and movement of the point where the measurement of arc length 
begins. They com­pute the minimal L2 distance between all translations of the two turning functions by 
proving that only a small number of such trans­lations need to be checked. Their algorithm is ef.cient 
and has a predictable run time: O(n 2 log n) in the total number of vertices n. The algorithm also corresponds 
fairly well to a subjective notion of the distance be­tween two shapes. It is limited in its ability 
to cope with varying levels of detail across the shapes (which is a form of what they call non-uniform 
noise ), but it is acceptable for our purposes. We use the polygon comparison metric for both polygons 
and sub­division curves. In the case of subdivision curves, we .rst approx­imate the curve as a polygon 
with a large number of vertices and then make a call to the same routine. 5 Optimizing over the space 
of tilings Armed with a set of tilings (the isohedral tiles), parameterizations over those tilings, and 
a good shape metric, we are now ready to address the problem of building an optimizer that can search 
over the space of those tilings to .nd an instance whose tiles are close to the goal shape. Our optimizer 
is based on simulated annealing. It works roughly as follows: function FINDOPTIMALTILING (GOALSHAPE , 
FAMILIES ): INSTANCES . CREATEINSTANCES (FAMILIES ) while ||INSTANCES || > 1do for each i in INSTANCES 
do ANNEAL(i, GOALSHAPE )  end for INSTANCES . PRUNE(INSTANCES ) end while return CONTENTS (INSTANCES 
) end function  The optimizer takes as input a goal shape and a set of isohedral fam­ilies in which 
to search for an optimal tiling. The optimizer begins by creating a set of multiple instances of tilings 
from each isohe­dral family. It then calls a re-entrant simulated annealing procedure to improve each 
one of these instances. (This ANNEAL() proce­dure is discussed in more detail below.) After each of the 
instances has been optimized to some degree, the instances are evaluated ac­cording to the shape metric, 
and the worst ones are removed. The annealing is continued on the remaining instances. This iterative 
process of alternately pruning the search space and then improving the remaining instances is repeated 
until just a single tiling instance is left. This tiling is returned as the optimal tiling. The annealer 
is a re-entrant procedure, which works roughly like this: procedure ANNEAL(TILING , GOALSHAPE ): for 
j =1to N do while T>Tmin do OPTIMIZETILING (TILING , GOALSHAPE ,T) T . REDUCE(T)  end while SMOOTHEDGESHAPES 
(TILING ) SPLITEDGESHAPES (TILING ) (T, Tmin ). UPDATESCHEDULE (T, Tmin )  end for suspend  end procedure 
 The annealer takes a given tiling instance and a goal shape as in­put. It loops for a constant number 
of iterations to improve the tiling and then exits, maintaining its state, so that upon re-entry it can 
continue from where it left off, in the same cooling sched­ule. Within each iteration of the outer loop, 
the procedure takes a number of cooling steps, reducing the temperature at each step. Within this inner 
loop, it makes a call to a procedure that we have termed OPTIMIZETILING(). This procedure implements 
the mul­tidimensional minimization by simulated annealing combined with the downhill simplex method, 
as described by Press et al. [17]. The procedure attempts to improve all of the parameters of the tiling, 
including the parameterizations of the tiling vertices (discussed in Section 3) and the positions of 
the shape vertices of the tile. The procedure always accepts a downhill step (one that improves the tiling 
instance) and sometime accepts an uphill step, with proba­bility depending on the temperature T . Once 
the temperature has v 2 v3  v 5 v 3 v 3 v2 v2 v4 v 1 v3 v 1  v 1 v3 v 1 v3 v 1 v0 v0 v 2 v0 
v 2 v0 v 2 v0 IH01, IH08 IH02 IH03 IH04 IH05   IH06 IH07 IH09 IH10, IH11, IH18, IH19, IH20 IH12, 
IH17 IH13 IH14 IH15 IH16 IH21 IH22, IH25 IH23, IH24 IH26 IH27 IH28   IH29 IH30 IH31, IH32 IH33, IH34, 
IH35, IH36, IH37 IH38, IH39, IH40 v 3  v0 v 0 v 1 v2 v 1 v1 v 1 v 1 v0 v 1 v0 v 0 v0  IH41, IH42, 
IH43, IH47,. IH44 IH45 IH46 IH48, IH52, IH60, IH64,. IH50, IH57, IH58 IH65, IH66, IH72 v0 v2 v 1 v0 
 v 1 v 1 v 0 v0 v2 v 1 v 0  v 2 v0  IH49, IH67 IH51 IH53 IH54 IH55, IH61, IH62, IH63, IH70, . H71, 
IH73, IH75, IH76 Figure 9 The complete set of tiling vertex parameterizations for the isohedral tilings, 
part one. In each tile, the edge marked with a red line is the .rst edge in the tiling type s incidence 
symbol. When that .rst edge is directed, the red line has an arrowhead. Labelled dotted lines represent 
parameter values, and are horizontal or vertical (with the exception of one guide line in the diagram 
for IH30). Since the diagrams are scale independent, distances that do not depend on parameters can be 
taken to have unit length. Tile edges cut with the same number of short lines have the same length, and 
edges cut with chevrons are additionally parallel. A single arc, a small square, and a double arc at 
vertices represent 60. , 90.,and 120. angles, respectively. v 0 v1  IH84, IH85 IH87, IH88, IH89, 
IH90, IH92, IH93 IH91 Figure 10 The complete set of tiling vertex parameterizations for the isohedral 
tilings, part two.  cooled to some minimum temperature Tmin, we exit this inner loop. At this stage, 
we run through the vertices of the tile and remove any vertices that are nearly collinear with their 
neighbors, thereby elim­inating any unnecessary degrees of freedom that may have been introduced into 
the tiling instance. Next, we subdivide each edge of the tiling, essentially doubling the number of variables 
over which we optimize in the next stage. Finally, we restart the cooling sched­ule, generally with some 
slightly lower temperatures T and Tmin . One additional part of the optimization, which is not shown 
in the pseudocode and which is optional, is to automatically convert the vertices of the tiles into control 
points for B-spline subdivision curves after a certain stage in the optimization. We can then ad­ditionally 
optimize over weights on each vertex that control the smoothness of the curve near that point. Our use 
of simulated annealing is subject to the usual practicalities. First, the success of the optimization 
for a single instance of a single tiling type depends on the initial shape of the tiling polygon and 
the initial positions of the shape vertices. We therefore generally start with multiple instances for 
each tiling type. As with any simulating annealing algorithm, the choice of cooling schedule can also 
make a difference. We use a very simple approach where the tempera­ture T is multiplied by a factor of 
fafter every N iterations, with T =0.1, N = 250,and f =0.9 to start. When the temperature reaches 5% 
of its initial value (Tmin =0.05T), the optimization resets, lowering the starting and minimum temperatures 
by a factor of 0.6, increasing the number of iterations N by a factor of 1.2,and reducing the temperature 
multiplier f by a factor of 0.1.We did not spend a lot of time optimizing this cooling schedule, so other 
reasonable choices would probably work equally well or better.  Representation of isohedral tilings 
 We have developed a computer representation of isohedral tilings that allows us to express our Escherization 
algorithm ef.ciently and naturally. The key is to factor out the constraints on the tile imposed by adjacencies 
and internal symmetries, and to store only the mini­mal set of free parameters that encode the tile shape. 
We break down the information associated with a tile into two com­ponents: the tiling template and the 
tile instance. The tiling template contains information about a tiling type in general. The tile instance 
refers to a template and contains a set of parameters for the tiling vertex parameterization, along with 
the minimal set of information required to reproduce the edge shapes. We .rst describe each of template 
IH16 {  topology 3 6 [1] symbol [a+b+c+c-b-a-;a-c+b+] [2] colouring 3 (1 2 3) (1 2 3) (1 2 3) [3] aspects 
3 [4] rules [5] aspect 2 1 [6] aspect 3 6 [7] translate T1 1,4 [8] translate T2 1,2 [9] } Figure 11 The 
tiling type information stored for IH16 these components in detail, and then show how they can be used 
to support ef.cient editing and viewing in an interactive system. 6.1 Tiling templates Tiling templates 
are computed once ahead of time, and stored in a master .le that is read in when the tile library is 
initialized. Fig­ure 11 shows a sample entry from the template .le. The complete set of templates is 
available on the proceedings CD-ROM. Along with the topological type and incidence symbol (lines 1 and 
2), we store additional static information that increases the ef.­ciency and functionality of our system. 
First, we add a colouring .eld (line 3) that provides a default rule for .lling the interiors of tiles 
with colours. An n-colouring of a tiling is a set of symbols {c1, ..., cn}, together with a function 
f that assigns a colour ci to each tile in the tiling. A perfect colour­ing is a colouring that respects 
the tiling s symmetry in the sense that symmetries act as permutations of the colours. Every perfect 
colouring of an isohedral tiling can be conveniently encoded as an assignment of different colours to 
the different aspects in a single translational unit, along with an assignment of different colour per­mutations 
to each of the two translation vectors. The colouring .eld in the template gives, in order, the number 
of colours, the assign­ment of colours to aspects, and the permutations of the assignment associated 
with the two translation vectors. This encoding can ex­press a superset of the perfect colourings. In 
the case shown here, the permutations are both the identity. (In all of his drawings, Es­cher was careful 
to ensure that no two adjacent tiles ever shared the same colour. He also used the minimum number of 
colours neces­sary to satisfy this condition. The default colourings we provide in our tiling templates 
have both of these properties.) Another line of the template (line 4) speci.es the number of aspects 
in the tiling, in this case, 3. The rules section (lines 5 through 9) gives a collection of rules that, 
when applied to a tiling polygon, yield transform matrices for all the aspects of a translational unit, 
as well as the two transla­tion vectors. These transforms cannot be computed ahead of time, as they depend 
on the tiling polygon. Each rule is expressed as a sequence of hops across edges, starting from the .rst 
aspect in the translational unit at the origin. Aspect 1 is always given the identity matrix as its transform, 
and the other aspect transforms are computed from it. In this example, the .rst rule (line 6) says that 
the transform for creating aspect 2 from the .rst aspect is just the transform that creates the symmetry 
across edge 1 of the .rst aspect in the tiling that is, a re.ection about the .rst edge, labelled a+, 
in the incidence symbol. Similarly, the sec­ond rule (line 7) says that the transform for creating aspect 
3 from the .rst aspect is the transform that creates the symmetry across edge 6 of the .rst aspect in 
the tiling here, a re.ection about the edge labelled a-. Sometimes, more than a single hop is required. 
For instance, the rule aspect 2 1,2,3 would specify a se­quence of hops: .rst, across edge 1 of the .rst 
aspect in the tiling, then across edge 2 of the .rst aspect s neighboring tile, then across edge 3 of 
that neighbor s neighbor. The two translation vectors are speci.ed in the same way. Thus, following across 
edge 1 of the .rst aspect in the tiling, then across edge 4 of the .rst aspect s neighboring tile, gives 
the translation vector T1. One piece of per-tiling-type information missing from the template .le is 
the set of tiling vertex parameterizations. The parameteriza­tions are more easily described in code 
than in a table-driven for­mat, and are embedded in the source code, each as a C++ class. A Python .le 
that implements the parameterizations is available on the CD-ROM. 6.2 Tile instances The tile is stored 
as a set of parameters for the tiling vertex parame­terization, along with a hierarchical model whose 
leaves are funda­mental edge shapes the portions of the tiling edges that cannot be further decomposed 
by symmetries. The fundamental edge shapes are simply stored as arrays of points. Each fundamental edge 
shape implicitly begins at (0, 0) and ends at (1, 0). By default, the points are interpreted as a sequence 
of line segments, but to increase the aesthetic appeal of our tilings we have implemented the ability 
to treat them as control points for a subdivision curve. As a further enhancement, each control point 
has an associated weight. The higher the weight, the more subdivision steps will go by before that point 
is averaged with its neighbours. In effect, the weight controls the sharpness of the curve near the control 
point, with maximum weight yielding a sharp corner that interpolates the control point. To rebuild the 
tile shape, we apply the parameterization to obtain the positions of the tiling vertices, and transform 
the edge shapes into place between them. There are at most three levels of transformation between a funda­mental 
edge shape and a point on the outline of the tile. The .rst level takes into account the symmetries of 
U and S edges. Half of the U or S edge comes directly from the fundamental edge. The other half is derived 
from the .rst half as needed through rotation or re.ection. J edges are passed unmodi.ed through this 
level, and since I edges are immutable, all tiles share a single system-wide copy of an I edge. At the 
next level up, we recognize that edges with different names in the incidence symbol may still have related 
shapes. In IH16, for example, the edge named b+is adjacent to c+, forcing the two edge shapes to be congruent. 
In this case, the two edges share the same shape passed up from the level below. Finally, the topmost 
level maps the unit interval to an edge of the tiling polygon; this mapping will move an edge shape from 
its nor­malized coordinate system into a portion of the tile s outline. At this level, all edges with 
the same name in the incidence symbol share a lower-level shape object. Speci.c tiles are stored in tile 
.les, which are simply XML docu­ments. 6.3 Interactive tools To provide a convenient interface to the 
Escherization algorithm, and to explore the mathematical and aesthetic properties of iso­hedral tilings 
in general, we have constructed several graphical tools on top of the tile library and optimizer, using 
the free toolkits GTK+ [19] and GTK--[12]. The simplest of these tools is a utility for tracing goal 
shapes from images. An image can be loaded into a viewer where the user can trace an outline of an image 
by hand. The outline can then be saved and passed to the optimizer. The more sophisticated tool is a 
rich viewer and editor for tile .les. The editor is highly responsive, running at interactive rates on 
an off-the-shelf Linux system with no graphics acceleration. Be­cause of the deep sharing of information 
in the tile representation, when a part of the tile is edited, the system provides immediate feedback 
by showing all parts of the tile (and tiling) that are affected by the change. When subdivision-based 
edges are enabled, we provide a novel gauge-based interface for editing weights on control points. The 
gauge  pops up at the vertex location and is set with a radial motion. Setting weights integrates very 
comfortably with the general process of editing the vertices. 6.4 Filling a region with tiles The most 
basic drawing operation for a tiling is to .ll a region of the plane with copies of the tile. Beginning 
with a tile in its local coordinate system and a viewing region, we need to .nd the rigid motions to 
apply to the tile that replicate it across the region. To .nd these motions, we project the viewing region 
s corners into the coordinate system formed by the tiling s translation vectors, de­rived from the template 
s rules. In that coordinate system, the trans­lational units become lattice squares; the lattice squares 
that inter­sect the projection of the viewing region are the ones that need to be drawn. For each needed 
translation, we place a tile relative to the rigid motion formed by composing the translation with each 
of the aspect transforms in turn.  7 Decorations and rendering The output of the core Escherization 
algorithm is a geometric de­scription of a tile, not a .nished ornamental design. To complete the Escherization 
process, we need to surround the core algorithm with tools to add decorations to tiles and create high-quality 
renderings of the results. We have explored the use of both vector-based and image-based decorations 
and rendering styles. A tile maintains a set of markings, sequences of weighted subdi­vision control 
points with various drawing attributes. Markings can be open, closed or .lled, polygonal or subdivided, 
and have variable line thickness, line colour and .ll colour. The line and .ll colours 0 seconds 60 
seconds 120 seconds 180 seconds 240 seconds 300 seconds goal shape 0.608 0.505 0.363 0.116 0.0443 0.0380 
IH42 IH24 IH1 IH1 IH1 IH1  0 seconds 150 seconds 300 seconds 450 seconds 600 seconds 750 seconds goal 
shape 0.444 0.366 0.314 0.263 0.257 0.257 IH83 IH53 IH6 IH6 IH6 IH6 Figure 12 Timelines for two sample 
Escherization runs. Each step shows the current best tile in the system (in red) overlaid on the goal 
shape. The caption indicates the elapsed time, the score for that tile, and its isohedral type. The second 
goal shape is the penguin from Figure 15(c). can also be mapped according to the tiling s colouring. 
The mark­ings can be created and edited from within the interactive tool, and are stored in the tile 
.le as fragments of XML. The editor can also render a tiling decorated with markings as PostScript. It 
may also be desirable to .ll the interior of a tile with image-based markings. We have implemented an 
image-based tiling renderer us­ing libart, a freely-available image manipulation library [16]. The renderer 
takes a tile .le and a set of images to serve as backdrops. For each tile in a region, it starts with 
the image backdrop for that tile s colour, applies a transparent wash of the tile colour, rasterizes 
the markings, draws an outline, and transforms the composited tile into its position in the .nal rendering. 
The natural choice for an image-based marking is the interior of the goal shape in the image that was 
originally traced. Using the correspondence provided by the polygon comparison metric, we do a Beier-Neely 
style image warp [3] to deform the interior of the goal shape in the source image into the interior of 
the Escherized tile shape. When the deformation is not too great, we end up with an attractive tiling 
out of motifs that resemble the original image. When the automatically-determined correspondence produces 
too much distortion (which can happen when the goal shape and tile shape differ in level of detail), 
it can be edited by hand to create a better match. To further increase the appeal of an image-based rendering, 
we ap­ply various painterly effects to the warped tile image before replica­tion. This post-processing 
step gives the artist creative control over the appearance of the .nal tiling, and can bring the result 
closer to the informal hand-drawn style of Escher s notebook drawings. 8Results We have used our Escherization 
implementation and decoration tools to produce a number of ornamental tilings from various sources of 
imagery. Figure 12 shows snapshots from two sample runs of the Escherizer. The goal shape in the .rst 
run is a simple test polygon, part of a series used to verify and tune the optimizer. The second goal 
shape is a more typical outline traced from an image. The more compli­cated shape takes longer to run, 
and the convergence is not quite as complete (as should be expected from a real-world outline). Figure 
13 A comparison between the tile returned by the optimizer and the same tile with user modi.cations. 
Note also that the second tile has subdivi­sion enabled. Figure 13 shows the tile result produced by 
the optimizer for a teapot image, followed by the tile after a small amount of hand­tweaking in the interactive 
editor. Even when manual intervention improved the overall appeal of a tiling, Escherization did the 
hard work of determining how to make the goal shape .t together with itself in the .rst place. The edits 
shown here took a minute or two to perform and were fairly typical of our experience in creating tilings 
in this fashion. The remaining results can be seen in Figures 14 and 15. Figures 15 (d) is rendered as 
line art, and the remaining examples use the image-based renderer. In all cases, the optimizer generated 
a tile shape that was then modi.ed slightly in the editor. The source im­age was warped into the tile 
shape, and copies of the warped image were recoloured and edited to make the .nal rendering. The user 
intervention was primarily to exert creative control, and rarely to guide the optimization process. On 
some occasions, it was helpful to watch the optimizer discover a tiling type suitable for a given goal 
shape, then stop and restart it with many tilings of that type, resulting in a narrower and deeper search. 
9 Discussion Most outlines are not tiles. For just about any goal shape, an Escher­izer will have to 
produce an approximation, and a better Escherizer will produce a closer approximation. A perfect Escherizer 
would determine the smallest distance over all possible tile shapes, and return the tiling that achieves 
that bound. Our imperfect optimizer, by contrast, coarsely samples the space of isohedral tilings in 
a di­rected fashion and returns the best sample it .nds. Consequently, there are seemingly easy cases, 
such as the one in Figure 16 that our algorithm cannot successfully Escherize. (a) Dogs; Dogs Everywhere 
(IH4) (b) Pigs in 2-Space (IH3) (c) Tea-sselation (IH28) (d) Twisted Sisters (IH86) Figure 14 Some examples 
of Escherized images and the tilings they generate. Hamm the pig appears courtesy of Disney/Pixar. (a) 
Sketchy Dogs (IH6) (b) A Plague of Frogs (IH6) (c) Tux-ture mapping (IH6) (d) Bubbles the Cat (IH1) 
 In practice, our Escherization system performs well on convex or nearly convex shapes. The shapes that 
tend to fail are the ones with long, complicated edges between the tiling vertices. It is dif.cult for 
the optimizer to come up with just right the sequence of vertex adjustments to push a tendril of detail 
out, especially when con­strained by the no non-uniform noise condition of the metric. Furthermore, in 
our shape comparison metric, the importance of a section of outline is directly proportional to its fraction 
of the perimeter of the goal shape, even if from our own perspective out­lines may obey different measures 
of signi.cance. For example, the precise pro.le edge of a face in silhouette, descending along eyes, 
nose, and mouth, is much more important to us than the hairline. But to the current shape metric these 
might be relatively insigni.­cant details. It would be valuable to investigate an extension to the polygon 
comparison metric wherein a section of outline could be assigned a measure of importance, a weight controlling 
which parts of the polygon should match more closely. Moreover, although Escher s tiles are almost always 
immediately recognizable as particular kinds of animals, they generally bear lit­tle actual resemblance 
to a real image: they are more like conven­tionalizations, or cartoons. Our optimizer does not understand 
the shapes it is manipulating, so it has no way to deform them while preserving their essential recognizability. 
It must instead rely on a purely geometric notion of proximity. All this being said, the Escherizer we 
have built performs remark­ably well on many different shapes for which no tiling is obvious. Who would 
have guessed that a teapot could tile the plane? We cer­tainly couldn t. Even when the optimizer fails 
to .nd an ideal tiling, it often .nds a tiling that is close enough that it is easily converted into 
an acceptable result. Thus, it allows us to work in much the same way that Escher did, only with a very 
close starting point and more helpful interactive tools. This research suggests many future directions, 
including general­izing our algorithms to handle multihedral and aperiodic tilings, parquet deformations 
[13, Chap. 10], or tilings over non-Euclidean domains, such as the hyperbolic plane [7]. Another intriguing 
idea is to allow some .exibility in the goal shape as well. For instance, instead of a 2D shape, we might 
use a 3D (and potentially parame­terized) model and attempt to automatically discover a camera po­sition 
from which the view of the model is most easily Escherized. Finally, along the lines of creating Escher 
tilings automatically is the problem of automatic conventionalization : somehow creating not just the 
tile boundaries, but the line-art graphical decorations that go inside the tilings, more or less automatically 
from a refer­ence image.  Acknowledgments Many people contributed to the development of this research. 
Michael Noth and Jeremy Buhler collaborated on FuTile, the class project that ultimately led to our ongoing 
research in tilings. Branko Gr¨unbaum motivated the use of isohedral tilings as a powerful and manageable 
system for ornamental design. Douglas Zongker sat in on many early discussions. Michael Cohen, Rick Szeliski 
and John Hughes participated in discussions and provided valuable feedback and insight that kept the 
project moving forward. Mike Ernst pointed us at the papers that led us to the polygon compari­son metric. 
Dan Huttenlocher assisted us with the metric (and made his source code available). Tony DeRose .lled 
in details on the im­plementation of subdivision curves. Zoran Popovi´c helped with the taming of the 
continuous simulated annealing algorithm. Finally, Victor Ostromoukhov provided helpful feedback on a 
draft of this paper. This research was supported in part through industrial grants from Intel, Microsoft 
and Pixar. References [1] Howard Alexander. The computer/plotter and the 17 ornamental design types. 
Proceedings of SIGGRAPH 75, pages 160 167, 1975. [2] E.M.Arkin,L.P.Chew, D. P. Huttenlocher, K. Kedem, 
and J.S. B.Mitchell. An ef.ciently computable metric for comparing polygonal shapes. PAMI(13), pages 
209 216, 1991. [3] Thaddeus Beier and Shawn Neely. Feature-based image metamorphosis. Pro­ceedings of 
SIGGRAPH 92, pages 35 42, 1992. [4] William W. Chow. Automatic generation of interlocking shapes. Computer 
Graphics and Image Processing, 9:333 353, 1979. [5] William W. Chow. Interlocking shapes in art and engineering. 
Computer Aided Design, 12:29 34, 1980. [6] Andrew Crompton. Grotesque geometry. http://dspace.dial.pipex. 
com/crompton/Home.shtml. [7] Douglas J. Dunham. Creating hyperbolic escher patterns. In H.S.M. Coxeter 
et al., editor, M.C. Escher: Art and Science, pages 241 247. Elsevier Science Publishers B.V., 1986. 
[8] Andrew Glassner. Frieze groups. IEEE Computer Graphics and Applications, 16(3):78 83, May 1996. [9] 
Andrew Glassner. Andrew glassner s notebook: Aperiodic tiling. IEEE Com­puter Graphics &#38; Applications, 
18(3):83 90, May June 1998. ISSN 0272­1716. [10] Andrew Glassner. Andrew glassner s notebook: Penrose 
tiling. IEEE Computer Graphics &#38; Applications, 18(4), July August 1998. ISSN 0272-1716. [11] Branko 
Gr¨unbaum and G. C. Shephard. Tilings and Patterns. W. H. Freeman, 1987. [12] GTK--. http://gtkmm.sourceforge.net. 
[13] Douglas Hofstadter. Metamagical Themas: Questing for the Essence of Mind and Pattern. Bantam Books, 
1986. [14] Daniel H. Huson and Olaf Delgado Friedrichs. Reptiles. ftp://ftp. uni-bielefeld.de/pub/math/tiling/reptiles/. 
[15] Hans Van Lemmen. Tiles: 1000 Years of Architectural Decoration. Harry N. Abrams, Inc., 1993. [16] 
Raph Levien. libart. http://www.levien.com/libart/. [17] William H. Press, Saul A. Teukolsky, William 
T. Vetterling, and Brian P. Flan­nery. Numerical recipes in c: The art of scienti.c computing (2nd ed.). 
1992. ISBN 0-521-43108-5. Held in Cambridge. [18] Doris Schattschneider. M.C. Escher: Visions of Symmetry. 
W.H. Freeman, 1990. [19] The GIMP toolkit. http://www.gtk.org. [20] M.C. Escher (tran. Karin Ford). Escher 
on Escher: Exploring the In.nite.Henry N. Abrams, Inc., 1989. [21] Michael T. Wong, Douglas E. Zongker, 
and David H. Salesin. Computer generated .oral ornament. Proceedings of SIGGRAPH 98, pages 423 434, 1998. 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>345073</article_id>
		<sort_key>511</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>57</seq_no>
		<title><![CDATA[Shadows for cel animation]]></title>
		<page_from>511</page_from>
		<page_to>516</page_to>
		<doi_number>10.1145/344779.345073</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=345073</url>
		<abstract>
			<par><![CDATA[<p>We present a semi-automatic method for creating shadow mattes in cel animation. In conventional cel animation, shadows are drawn by hand, in order to provide visual cues about the spatial relationships and forms of characters in the scene. Our system creates shadow mattes based on hand-drawn characters, given high-level guidance from the user about depths of various objects. The method employs a scheme for &#8220;inflating&#8221; a 3D figure based on hand-drawn art. It provides simple tools for adjusting object depths, coupled with an intuitive interface by which the user specifies object shapes and relative positions in a scene. Our system obviates the tedium of drawing shadow mattes by hand, and provides control over complex shadows falling over interesting shapes.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[NPR]]></kw>
			<kw><![CDATA[cel animation]]></kw>
			<kw><![CDATA[inflation]]></kw>
			<kw><![CDATA[shadows]]></kw>
			<kw><![CDATA[sketching]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P170397</person_id>
				<author_profile_id><![CDATA[81332521169]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lena]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Petrovi&#263;]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P32615</person_id>
				<author_profile_id><![CDATA[81100428357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fujito]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31100535</person_id>
				<author_profile_id><![CDATA[81100005753]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Lance]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Williams]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Feature Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P10635</person_id>
				<author_profile_id><![CDATA[81100576882]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Adam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Finkelstein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>280949</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[CORRI~A, W. T., JENSEN, R. J., THAYER, C. E., AND FINKELSTEIN, A. Texture mapping for cel animation. Computer Graphics (Proceedings of SIGGRAPH 98), 435-446.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218417</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[FEKETE, J., BIZOUARN, E., COURNARIE, E., GALAS, T., AND TAILLEFER, F. TicTacToon: A paperless system for professional 2-D animation. Computer Graphics (Proceedings of SIGGRAPH 95), 79-90.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[GORNOWlCZ, G., AND WILLIAMS, L. Snap to it! Automatic 3D object and silhouette registration. Sketches and Applications, SIGGRAPH 2000.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311602</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[IGARASHI, T., MATSUOKA, S., AND TANAKA, H. Teddy: A sketching interface for 3D freeform design. Computer Graphics (Proceedings of SIGGRAPH 99), 409-4 16.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[KERSTEN, D., MAMASSIAN, P., AND KNILL, D. C. Moving cast shadows induce apparent motion in depth. Perception 26, 2 (1997), 171-192. Also see: http://vision.psych.umn.edu/www/kersten-lab/demos/shadows.html.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311612</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[RADEMACHER, P. View-dependent geometry. Computer Graphics (Proceedings of SIGGRAPH 99), 439-446.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[ROBERTSON, B. Disney lets CAPS out of the bag. Computer Graphics World (July 1994), 58-64.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[ROBERTSON, B. Mixed media. Computer Graphics World (Dec. 1998), 32-35.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[ROBERTSON, B. Deep background. Computer Graphics World (July 1999), 50-51.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166135</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[SCHOENEMAN, C., DORSEY, J., SMITS, B., ARVO, J., AND GREENBERG, D. Painting with light. Computer Graphics (Proceedings of SIGGRAPH 93), 143-146.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192191</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[SHANTZlS, M. A. A model for efficient and flexible image computing. Computer Graphics (Proceedings of SIGGRAPH 94), 147-154.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[THOMAS, F., AND JOHNSTON, O. Disney Animation: The Illusion of Life. Walt Disney Productions, New York, 1981.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[UNIVERSAL STUDIOS / DREAMWORKS. The Prince of Egypt. Movie, 1999.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[VAN OVERVELD, K., AND WYVlLL, B. Polygon inflation for animated models: A method for the extrusion of arbitrary polygon meshes. Journal of Vision and Computer Animation 18 (1997), 3-16.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806813</ref_obj_id>
				<ref_obj_pid>800224</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[WALLACE, B. A. Merging and transformation of raster images for cartoon animation. Computer Graphics (Proceedings of SIGGRAPH 81), 253-262.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[WALT DISNEY PRODUCTIONS. Tarzan. Movie, 1999.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617750</ref_obj_id>
				<ref_obj_pid>616023</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[WANGER, L., FERWERDA, J., AND GREENBERG, D. Perceiving spatial relationships in computer-generated images. IEEE Computer Graphics and Applications, 12, 3 (1992), 44-58.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[WARNER BROTHERS. The Iron Giant. Movie, 1999.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>312112</ref_obj_id>
				<ref_obj_pid>311625</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[WILLIAMS, L. 3D rendering effects for 2D animation. Sketches and Applications, SIGGRAPH 1999.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[WILLIAMS, L. Shading in two dimensions. Proceedings of Graphics Interface 91, 143-151.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>897162</ref_obj_id>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[WILLIAMS, L. R. Topological reconstruction of a smooth manifold-solid from its occluding contour. Tech. Rep. 94-04, University of Massachusetts, Amherst, MA, 1994.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258859</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[WOOD, D. N., FINKELSTEIN, A., HUGHES, J. F., THAYER, C. E., AND SALESIN, D. H. Multiperspective panoramas for cel animation. Computer Graphics (Proceedings of SIC- GRAPH 97), 243-250.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237238</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[ZELEZNIK, R. C., HERNDON, K. P., AND HUGHES, J. F. SKETCH: An interface for sketching 3D scenes. Computer Graphics (Proceedings of SIGGRAPH 96), 163-170.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>247375</ref_obj_id>
				<ref_obj_pid>247372</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[ZHU, S., AND YUILLE, A. FORMS: A flexible object recognition and modelling system. International Journal of Computer Vision 20, 3 (1996), 187-212.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Shadows for Cel Animation Lena Petrovi´c . Brian Fujito . Lance Williams . Adam Finkelstein . . Princeton 
University . Disney Feature Animation  Abstract We present a semi-automatic method for creating shadow 
mattes in cel animation. In conventional cel animation, shadows are drawn by hand, in order to provide 
visual cues about the spatial relationships and forms of characters in the scene. Our system creates 
shadow mattes based on hand-drawn characters, given high-level guidance from the user about depths of 
various objects. The method employs a scheme for in.ating a 3D .gure based on hand-drawn art. It provides 
simple tools for adjusting object depths, coupled with an intuitive interface by which the user speci.es 
object shapes and relative positions in a scene. Our system obviates the tedium of drawing shadow mattes 
by hand, and provides control over complex shadows falling over interesting shapes. Keywords: Shadows, 
cel animation, in.ation, sketching, NPR. URL: http://www.cs.princeton.edu/gfx/proj/cel shadows 1 Introduction 
Shadows provide important visual cues for depth, shape, contact, movement, and lighting in our perception 
of the world [5, 17]. In cel animation, a moving .gure and background scenery are illustrated in different 
layers with different styles, and therefore shadows play an especially crucial role by integrating the 
character into the background. According to Thomas and Johnston, two of Disney s most renowned animators, 
shadows were used in cel animation even from the very early days because they anchored the .gure to the 
ground. Without some kind of contact with the background, the characters seemed to .oat around, walking 
on air, no matter how much weight had been animated into their movements. [12] Traditionally, shadow 
mattes have been drawn by hand, and while modern digital image manipulation tools provide simple operations 
that assist in the creation of mattes, the process is still largely manual. In this paper, we present 
a semi-automatic method of creating shadow mattes from the hand-drawn outlines of moving .gures. The 
process requires relatively little effort per frame, and generates plausible shadows cast by complex 
shapes over interesting background scenery such as walls, stairs and statues. In rare cases where the 
shadow itself becomes a focal point for the viewer s attention, the shadow mattes should be drawn by 
hand, because they embody an artistic interpretation of the scene. In our work, we are addressing the 
rest of the shadow mattes shadows that serve to anchor the character to the ground, enhance the form 
of the .gure, or suggest lighting or mood. These represent the Permission to make digital or hard copies 
of part or all of this work or personal or classroom use is granted without fee provided that copies 
are not made or distributed for profit or commercial advantage and that copies bear this notice and the 
full citation on the first page. To copy otherwise, to republish, to post on servers, or to redistribute 
to lists, requires prior specific permission and/or a fee. SIGGRAPH 2000, New Orleans, LA USA &#38;#169; 
ACM 2000 1-58113-208-5/00/07 ...$5.00  Figure 1: An example frame. (a) hand-drawn line art (b) shadow 
mattes created by our system (c) composited frame. majority of the shadow mattes in cel animation, and 
the work of creating them is considerable. Unfortunately, it is not yet possible to fully automate the 
process of creating shadow mattes based on the line art; understanding the shapes suggested by the line 
art is tantamount to solving the computer vision problem, and is subject to ambiguities in interpretation. 
Thus, our system requires a small amount of user input less effort than would be required to draw the 
mattes by hand. Once the user has set up the scene, it is easy to alter the lighting conditions to produce 
very different kinds of shadows. The bene.ts of such a system are a reduction in effort, an increase 
in control and .exibility, and the ability to create plausible shadow mattes even for complex character-scene 
interactions. At a high level, our process works as follows. We begin with hand-drawn line art created 
by a traditional animator (Figure 1a), as well as hand-painted scenery created by a background artist. 
The user sketches over features in the painted background to establish the camera, ground plane, and 
background objects. Using character mattes integral to the compositing stage of the normal cel animation 
pipeline, we automatically in.ate a 3D mesh for the character. The user speci.es the depth for the character 
in the scene, as well as light positions. Next, based on the lights, the 3D character, and the background 
objects, the computer renders three types of shadow mattes for the character: tone mattes indicate both 
self-shadowing and shadows of other objects on the character (Figure 1b, blue); contact shadow mattes 
emphasize contact between the character and the ground (green); and cast shadow mattes specify shadows 
cast by the character onto the background scenery (red). Finally, we composite these mattes into the 
scene (Figure 1c) as part of the conventional cel animation pipeline. The contributions of this work 
are: (1) application of in.ation algorithms to create frame-by-frame 3D models from hand-drawn animation; 
(2) tools that allow the user to manipulate the in.ated models of the artwork in three dimensions while 
preserving their image-space silhouettes; and (3) an intuitive user interface for describing shapes and 
relative 3D positions of both static and animated objects in the scene. The remainder of this paper is 
organized as follows. In Section 2 we review related work. Section 3 describes the details of our process. 
In Section 4 we demonstrate the results of our working system. Section 5 concludes with observations 
and proposed areas of future work. Figure 2: Specifying background. (a) marking features in artwork 
(b) 3D scene in wireframe (c) .at-shaded side-view of 3D scene.  2 Related Work A variety of previous 
efforts have applied computer graphics in cel animation. Researchers have automated the image processing 
and compositing aspects of cel animation [2, 7, 11, 15], in which the shadow mattes created by our system 
could replace hand-drawn mattes. For the cel animation industry, 3D techniques are becoming increasingly 
prominent in production. For example, in Disney s Tarzan [16], jungle backgrounds were built and painted 
in 3D with Deep Canvas, to give extra depth to the scene and allow camera .y-throughs [9]. For The Prince 
of Egypt [13], DreamWorks used Exposure, allowing them to .y cameras through a 3D scene with sequential 
background shots speci.ed by manipulating a series of painted 2D background cards [8]. 3D methods have 
also been used for cel animation in research contexts, for example in the design of multiperspective 
panoramas (static background paintings that are appropriate for a moving camera [22]) or view-dependent 
geometry (3D models that change shape depending on view direction [6]). Previously, we showed how to 
apply texture to a hand-animated character by warping a 3D model to match the line art [1]. While this 
method could be adapted for creating plausible shadow mattes, the process requires too much human effort 
for this application; it would be easier generally to draw the shadow mattes by hand. Since shading and 
tones enhance our understanding of 3D shape, a number of cel animation projects have applied 3D computer 
graphics for shading. In the extreme, a character (for example, the giant in The Iron Giant [18]) is 
modeled and animated entirely in 3D and then rendered with a cartoon shader. Most characters in cel animation 
are not designed in 3D, and therefore to invoke 3D shading techniques, one must form some kind of 3D 
representation of the 2D artwork. This project grew out of previous work [19, 20] where we used pixel-based 
in.ation schemes to automatically build 3D representations from 2D art, and then rendered tones and shadows 
with strictly 2D interactions. In this paper, we employ a 3D in.ation technique, and develop technology 
that allows us to interactively stage a scene with 3D shadow interactions. We adapt the in.ation algorithm 
originally proposed by van Overveld and Wyvill [14], as implemented in a simpler form in the Teddy system 
of Igarashi et al. [4]. We use Teddy because it generates 3D forms that yield plausible tones and cast 
shadows. However, we modify their method to account for a perspective camera, requiring that the .gure 
aligns with the artwork on its silhouette, as seen from the camera. The more signi.cant departure from 
the Teddy system is that we build the character up in multiple layers, rather than extruding limbs from 
the main body. In our application, there are two advantages to the layer-based approach: it is more consistent 
with the cel animation pipeline, and it guarantees that the silhouette of each layer in the .gure matches 
the line art. Finally, our strategy for constructing the 3D scene is largely inspired by the SKETCH system 
of Zeleznik et al. [23]. Figure 3: Building a three-layered character. (a) character mattes (b) middle-layer 
in.ation (c) 3D character seen from side view.  3 The Process Here we describe the process of creating 
shadow mattes based on hand-drawn art. Section 3.1 addresses construction of background scenery based 
on a few simple gestures by the user. Section 3.2 describes in.ation of 3D characters from line art, 
and Section 3.3 presents tools for adjusting the relative depths of the 3D .gures in the scene. Finally, 
Section 3.4 describes setting up lights, rendering shadow mattes, and compositing them into the artwork. 
 3.1 Constructing Background Scenery The .rst stage of the process of creating the scene is to construct 
a background. In order to establish the relationship between the camera and the scene, we begin with 
several assumptions: a .xed .eld of view and aspect ratio for the camera (in our tests, 83. wide with 
an aspect of 4:3), and known camera roll and ground plane tilt (both are upright). These assumptions 
work for a broad class of scenes, and may be easily modi.ed by the user to work for other scenes. Next, 
in order to establish the pitch of the camera relative to the ground plane, the user sketches over the 
background art a pair of parallel lines in the ground plane, for example the cyan lines in Figure 2a. 
In a perspective image, all parallel lines on the ground plane that intersect in the image plane will 
intersect on the horizon. Thus, even if the horizon is not visible in the scene we can .nd it by intersecting 
the parallel lines given by the user. The height h of the horizon relative to the center of the image 
determines1 the pitch f of the camera by the simple relationship: f =arctan (h/d) where d is the distance 
from the camera to the image plane (which we set arbitrarily to 1). The only remaining camera parameters 
are its yaw and its height above the ground plane. Since at this stage there are no objects in the scene 
other than the ground plane, the yaw is arbitrary and we set it to zero. The camera height establishes 
a scale for the scene that is also arbitrary. For example, in Figure 1 we see a man in a room, but whether 
this room is the size of a bread box or a warehouse has no impact on the size or shape of his shadow 
projected onto the image plane. Thus, we arbitrarily choose the height of the camera, only taking care 
that the entire image plane is guaranteed to be above the ground plane. Now we establish a coordinate 
system for the scene: we take the origin to be the center of the image plane, the x and y axes to be, 
respectively, the horizontal and vertical axes of the image, and the -z axis to be the camera look direction. 
Taking inspiration from the SKETCH system [23], we construct objects in the scene relative to the ground 
plane using simple gestures. Walls are built perpendicular to the ground by specifying the line of intersection 
with the ground plane. The user can create more complicated objects consisting of multiple polygonal 
faces 1This method fails for cameras that are looking straight up or straight down, and in these cases 
we would ask the user to set the pitch by hand.  Figure 4: In.ating a 3D .gure from 2D line art using 
perspective. Left: orthographic extrusion. Right: perspective extrusion. (such as stairs and boxes) by 
specifying two polylines on the object, starting with ground contact points. The object is constructed 
so that it conforms to the sketched lines and its neighboring polygonal faces are perpendicular. Figure 
2 shows the user-identi.ed features in the image: cyan for the ground lines, yellow for the wall line, 
and violet for the stairs. Smooth, organic 3D background objects, such as the statue in Figure 2, are 
built using the character in.ation and placement methods described in Sections 3.2 and 3.3.  3.2 In.ating 
3D Models Having speci.ed the background, the next step is to in.ate the line art to form meshes representing 
the character in 3D. Our goal is to create 3D .gures that cast plausible shadows under various lighting 
conditions. First, we convert the line art into character mattes bitmaps that de.ne regions of the image 
covered by the character. This process is performed as a normal aspect of the cel animation pipeline, 
because these mattes are used for .lling and clipping when the character is composited into the scene. 
For further control in our system, we often divide the character mattes into multiple layers, so that 
each may be placed at a different depth. These separate layers help to cast plausible shadows under lighting 
conditions that reveal the geometry of the character. For the character in Figure 3, the arm (in.ated 
from the yellow matte) could cast a shadow from an overhead light onto the leg (magenta). Layer mattes 
may need to be extended past visible boundaries to cast reasonable shadows. For example, a matte for 
the man s right arm in Figure 1 would extend into his body, with the body-arm boundary being preserved 
by placing the arm behind the body (as described in Section 3.3). Converting line art into character 
mattes must be a manual process, because it requires a visual interpretation of the scene. However, it 
is fairly easy to perform using digital image editing tools such as Adobe Photoshop. In some cases, these 
layer mattes may already be available to us, as animation houses may split characters into multiple layers 
for separate animation.2 Next we in.ate each layer in the character matte to form rotund 3D shapes; for 
example the shape shown in Figure 3b is in.ated from the cyan region in Figure 3a. Our in.ation scheme 
is based on the Teddy system of Igarashi et al. [4], which .nds the chordal axis of a closed curve, lifts 
it out of the plane, and lofts a surface between the curve and its axis. Many other in.ation methods 
might be used at this stage. We chose Teddy because it is simple and fast, and produces smooth, bulky 
shapes that yield reasonable cast shadows and tone mattes. As input to Igarashi s algorithm, each layer 
in the character mattes should be described by a simple, closed, 2D polyline, i.e. there should be no 
holes. We use an automatic tracing tool to convert the matte to a closed curve, and then resample it 
as a polyline. Teddy then elevates a surface whose intersection with the image plane is the input polyline. 
2This is more common in television animation where individual layers tend to be reused, but less common 
in feature animation. Figure 5: Adjusting depth. Left: ball grows as it moves back to contact point. 
Right: Depth-shear preserves perspective outline. If we were using an orthographic camera, we would be 
done. However, with a perspective camera, this scheme will not produce a 3D shape whose silhouette is 
exactly aligned with the original matte. Therefore, we adjust the resulting shape as follows. Every point 
p on the lofted surface is a signed distance d from its corresponding point p0 in the image plane of 
the orthographic ' camera (Figure 4). For the perspective camera, we .nd the point pon the ray from the 
camera to p0: ' p= p0 + dv where v is the normalized vector from the camera to p0. By sending every point 
p in the Teddy-lofted surface through this transform, the silhouette of the resulting shape conforms 
exactly to the matte. 3.3 Specifying Depth Now that we have in.ated the layers of the character from 
the image plane, we need to give them depth in the 3D world while preserving their image plane projection. 
We provide two projection-preserving depth adjustments: depth-translation and depth-shear. While one 
could implement more sophisticated projection-preserving warps, we designed these methods to minimize 
user interaction. Depth-translation moves the .gure out of the image plane to varying depths in the scene; 
we maintain its image plane projection using a uniform scale about the camera center. The user speci.es 
the translation in one of two ways. First, she can mark a contact point in the image plane, and the system 
pushes the object back into the scene until the marked point reaches the ground plane, as shown in Figure 
5a. Second, the user can push the object towards or away from the camera via a manipulator, watching 
the object move in relation to other objects in the scene as well as an approximate shadow cast by an 
overhead light onto the ground plane. We also allow the user to control the relative depth across a single 
object via a projection-preserving depth-shear, which provides .ne control over how shadows will be cast 
by this .gure. For example, in Figure 3c, the arm has been sheared so that the hand is closer to the 
image plane (to the left in this pose) than the rest of the arm. Nonetheless, the image-plane projection 
of this object must remain unchanged. The user speci.es the shear via a manipulator that provides an 
axis and an angle .. Our algorithm works as follows. For every point p on the unsheared object (light 
blue in Figure 5b), we calculate ps, the point that would result from sending p through the conventional 
shear by .. We take our .nal point p' to be the point on the ray extending from the camera through p 
that has the same z value (depth) as ps. Depth adjustment is generally speci.ed separately for all layers. 
However, for any layer this information may be keyframed across time. For example, by specifying a contact 
point in two frames of the animation of the ball shown in Figure 7 we are able to implicitly set depths 
for the ball in all of the other frames. Likewise, the contact points for the stomping man are keyframed, 
even though his body remains at approximately constant depth. Figure 6: Specifying lights. From top 
to bottom: directional light, point light, gazebo gobo. 3.4 Lights ... Camera ... Action! Now that we 
have constructed a background set and positioned a 3D character in the scene, setting up lights and rendering 
shadow mattes is straightforward. As shown in Figure 6, we obtain different shadow effects by using directional 
lights, point lights, or by using a gobo. 3 Within a range of lighting subjectively consistent with the 
painted background, lights may even be animated to provide .ne control over where a shadow falls in the 
scene. When creating the sequence shown in Figure 8, we animated the light position to cause the shadow 
to fall on the legs of the statue early in the clip and extend up the stairs later. To render the mattes 
for tones and cast shadows, we use a standard ray tracer with custom shaders. For example, for on­character 
tone mattes we have implemented a cartoon shader that thresholds all colors to either white or black 
based on a diffuse lighting calculation. Due to sampling and approximation errors in 3D mesh construction, 
it is possible that the 3D .gure used to render tone mattes does not exactly align with the line art, 
causing a small gap between the tone matte and the line art. In these cases, 3A gobo , referred to in 
live action .lm as a cuckaloris or cookie , is a device for projecting a shadow pattern (such the foliage 
in Figure 9, or in this case a gazebo) onto a scene. Figure 7: Bouncing ball casts shadow in desert 
scene. we apply morphological dilation to the tone matte, and then clip it to the original character 
matte. To accommodate this operation, we actually render a separate tone matte for each layer in the 
.gure and perform dilation and clipping on each independently before combining them. We render contact 
shadows in two passes. First, we place an orthographic camera in the ground plane, looking up at the 
character, and use a far clipping plane that is just above the ground. With this camera, we capture an 
image of parts of the character that nearly touch the ground. Second, we re-project the image onto the 
ground plane, and view it from the regular camera for the scene, giving us contact shadows. Final dilation 
of the resulting matte ensures that it emerges from beneath the character. Now we are ready to composite 
the frame. Tone mattes modulate the character s color, while mattes for cast shadows and contact shadows 
darken the background painting. In some cases, we blur the mattes to suggest softer shadows. Finally, 
we composite the shadowed character over the shadowed background, and then add the line art.  4 Experimental 
Results In this section, we describe the animations that we created using our system, as well as the 
time and effort for building these sequences.4 The animations are shown on the video proceedings. Figure 
7 shows two frames from a 33-frame sequence of a bouncing ball. We built a ground plane (from the lines 
of the road) and in.ated each ball mesh as a single, animated layer. Since parts of the ball go off-frame 
in the last .ve frames, we completed the character mattes (extending out of the frame) to get reasonable 
shadows. We speci.ed the depth of the bouncing ball using ball-to-ground contact points in two frames, 
and interpolated and extrapolated depths for all other frames. 4Our system is implemented on a 400 MHz 
Pentium II PC as a series of plug-ins for Alias|Wavefront Maya. For matte editing and compositing we 
use Adobe Photoshop, Softimage Eddie and NothingReal Shake.  Figure 8: Stomping man in action, with 
two different backgrounds. Figure 8 shows three frames of a stomping man one in a hallway and two near 
some stairs. For both scenes, we built the stomping man from a 16-frame, hand-drawn walk cycle. For each 
of the 16 frames, we split up the character into three or four layers (depending on the visibility of 
the far arm) and speci.ed different relative depths and shears in each layer. Our system then built offset 
copies of these meshes to form 10 stomp cycles (160 frames). In the hallway scene, we placed a near-horizontal 
directional light that casts a long shadow breaking up the far wall. In the staircase scene, we subtly 
animated a point light, as described in Section 3.4. In Figure 9 we show three test frames from a work 
in progress of an old man carring a .ower. The upper two frames illustrate varying the light direction. 
Because of the style of the artwork, these frames do not use tone mattes. The bottom frame shows the 
body of the man and the ground receiving a shadow from a tree gobo. The bulk of the human effort involved 
in our system consists of the following: (1) specifying the background 1 or 2 minutes, even for complex 
scenes such as the stair scene; (2) creating character mattes 2 minutes per layer per frame;5 (3) specifying 
depth information for each layer roughly 30 seconds per layer 5For the stomping man, it took 2 hours 
to build mattes for a 16-frame cycle with 4 layers. For the 9 other cycles, character mattes were simply 
offset with no human effort. Figure 9: Man with .ower. The bottom image uses a tree gobo. per frame;6 
(4) specifying lights under 1 minute. The human effort required to specify layer mattes and depth information 
may be substantial. However, several factors mitigate this cost. First, layer segmentation is only required 
insofar as the lighting and geometry demand it. Second, the task of layering and depth speci.cation requires 
minimal artistic interpretation, and might be relegated to junior staff. Third, specifying depth using 
our interface is simple, so that even if the layers frequently change depth across frames, the work of 
adjusting layers is faster, simpler, and more easily adapted than drawing shadows by hand. Finally, once 
the scene has been established, changing the lighting is easy. Thus, in general, our system requires 
relatively little human effort, and allows users to cast interesting shadows that would otherwise be 
quite tedious to draw by hand. The most computationally expensive aspect of our system is ray tracing 
shadow mattes, each of which takes roughly 45 seconds at 640×480.7 This time is characteristic for ray-traced 
shadows; other rendering regimes (e.g. depth buffer shadows or shadows cast by area lights) could be 
expected to vary in computational expense. 6For the entire ball sequence, the depths were speci.ed in 
1 minute using only two contact points. For the stomping man, setting depths took about 30 minutes for 
16 frames; the remaining 144 frames were cycled. 7Computing shadows took less than 2 hours for the ball 
(33 frames, 3 mattes) and 12 hours for the stomping man (160 frames, 6 mattes).  5 Conclusion and Future 
Work This paper presents a method for semi-automatic creation of shadow mattes for cel animation. The 
process reduces human effort normally required for painting shadow mattes. The system yields plausible 
shadows, even for complex characters and scenes. We show examples with dramatic lighting directional 
lights and point lights casting very oblique shadows, and even gobos in order to demonstrate the effectiveness 
of the method with such lights. Setting up the scene and generating shadow mattes is relatively easy. 
Furthermore, once this task is accomplished it is trivial to subsequently change the lighting conditions 
and experiment with different effects. Thus, animators and directors can easily adjust the lighting (within 
a range consistent with the underlying painted artwork), in contrast to the traditional method wherein 
the shadow artist would redraw all of the mattes. While the shadows in our examples are reasonable, there 
are some characters or background objects for which our in.ation technique would be inappropriate. Either 
hand-drawn shadows or alternate modeling methods would address these problem cases. This work suggests 
a number of areas for future investigation: Applying computer vision techniques for understanding art. 
The most time-consuming aspect of our system is creation of layered character mattes. Applying computer 
vision techniques (e.g. [21, 24]) to automate layer speci.cation would facilitate this process. Ultimately, 
tracking processes which adapt 3D models to drawn animation sequences [3] offer the potential of highly 
re.ned shadows and rendering effects. Automatic light placement. Artists and directors accustomed to 
traditional hand-drawn techniques may be bene.t from automatic light placement, based on either the background 
painting and its simple 3D model, or on crude hand-drawn samples (in the spirit of Schoeneman et al. 
[10]). Shadow simpli.cation. In traditional animation, hand-drawn shadows are often abstract rather than 
realistic. We would like to be able to simplify shadows, perhaps as a post-process in this system.  
 Acknowledgements Many of the ideas in this paper were advanced at DreamWorks by Galen Gornowicz, Saty 
Raghavachary, and Gigi Yates. We are extremely grateful to Grady Klein and Rob Jensen for creating the 
hand-drawn characters and background scenery shown here. Thanks to Rick Szeliski and Mike Salisbury for 
helpful discussions about constructing the scene. This work was supported by an NSF CAREER Award and 
an Alfred P. Sloan Fellowship. References [1] CORR AND EA,W. T., JENSEN,R. J., THAYER,C. E., FINKELSTEIN, 
A. Texture mapping for cel animation. Com­puter Graphics (Proceedings of SIGGRAPH 98), 435 446. [2] FEKETE,J., 
BIZOUARN,E., COURNARIE,E., GALAS,T., AND TAILLEFER,F. TicTacToon: Apaperlesssystemforpro­fessional 2-D 
animation. Computer Graphics (Proceedings of SIGGRAPH 95), 79 90. [3] GORNOWICZ,G., ANDWILLIAMS,L.Snaptoit!Automatic 
3D object and silhouette registration. Sketches and Applica­tions, SIGGRAPH 2000. [4] IGARASHI,T., MATSUOKA,S., 
AND TANAKA, H. Teddy: A sketching interface for 3D freeform design. Computer Graphics (Proceedings of 
SIGGRAPH 99), 409 416. [5] KERSTEN,D., MAMASSIAN,P., AND KNILL,D.C. Moving cast shadows induce apparent 
motion in depth. Perception 26, 2 (1997), 171 192. Also see: http://vision.psych.umn.edu/www/kersten-lab/demos/shadows.html. 
[6] RADEMACHER, P. View-dependent geometry. Computer Graphics (Proceedings of SIGGRAPH 99), 439 446. 
[7] ROBERTSON, B. Disney lets CAPS out of the bag. Computer Graphics World (July 1994), 58 64. [8] ROBERTSON, 
B. Mixed media. Computer Graphics World (Dec. 1998), 32 35. [9] ROBERTSON, B. Deep background. Computer 
Graphics World (July 1999), 50 51. [10] SCHOENEMAN,C., DORSEY,J., SMITS,B., ARVO,J., AND GREENBERG, D. 
Painting with light. Computer Graphics (Proceedings of SIGGRAPH 93), 143 146. [11] SHANTZIS, M. A. A 
model for ef.cient and .exible image computing. Computer Graphics (Proceedings of SIGGRAPH 94), 147 154. 
[12] THOMAS,F., AND JOHNSTON,O. Disney Animation: The Illusion of Life. Walt Disney Productions, New 
York, 1981. [13] UNIVERSAL STUDIOS /DREAMWORKS. ThePrinceof Egypt. Movie, 1999. [14] VAN OVERVELD,K., 
AND WYVILL, B. Polygon in.ation for animated models: A method for the extrusion of arbitrary polygon 
meshes. Journal of Vision and Computer Animation 18 (1997), 3 16. [15] WALLACE, B. A. Merging and transformation 
of raster im­ages for cartoon animation. Computer Graphics (Proceedings of SIGGRAPH 81), 253 262. [16] 
WALT DISNEY PRODUCTIONS. Tarzan. Movie, 1999. [17] WANGER,L., FERWERDA,J., AND GREENBERG,D. Per­ceiving 
spatial relationships in computer-generated images. IEEE Computer Graphics and Applications, 12, 3 (1992), 
44 58. [18] WARNER BROTHERS. The Iron Giant. Movie, 1999. [19] WILLIAMS, L. 3D rendering effects for 
2D animation. Sketches and Applications, SIGGRAPH 1999. [20] WILLIAMS, L. Shading in two dimensions. 
Proceedings of Graphics Interface 91, 143 151. [21] WILLIAMS, L. R. Topological reconstruction of a smooth 
manifold-solid from its occluding contour. Tech. Rep. 94-04, University of Massachusetts, Amherst, MA, 
1994. [22] WOOD,D.N., FINKELSTEIN,A., HUGHES,J.F., THAYER, C. E., AND SALESIN, D. H. Multiperspective 
panoramas for cel animation. Computer Graphics (Proceedings of SIG-GRAPH 97), 243 250. [23] ZELEZNIK,R. 
C., HERNDON,K.P., AND HUGHES,J.F. SKETCH: An interface for sketching 3D scenes. Computer Graphics (Proceedings 
of SIGGRAPH 96), 163 170. [24] ZHU,S., AND YUILLE, A. FORMS: A .exible object recognition and modelling 
system. International Journal of Computer Vision 20, 3 (1996), 187 212.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>345074</article_id>
		<sort_key>517</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>58</seq_no>
		<title><![CDATA[Illustrating smooth surfaces]]></title>
		<page_from>517</page_from>
		<page_to>526</page_to>
		<doi_number>10.1145/344779.345074</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=345074</url>
		<abstract>
			<par><![CDATA[<p>We present a new set of algorithms for line-art rendering of smooth surfaces. We introduce an efficient, deterministic algorithm for finding silhouettes based on geometric duality, and an algorithm for segmenting the silhouette curves into smooth parts with constant visibility. These methods can be used to find all silhouettes in real time in software. We present an automatic method for generating hatch marks in order to convey surface shape. We demonstrate these algorithms with a drawing style inspired by <italic>A Topological Picturebook</italic> by G. Francis.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[direction fields]]></kw>
			<kw><![CDATA[hatching]]></kw>
			<kw><![CDATA[non-photorealistic rendering]]></kw>
			<kw><![CDATA[pen-and-ink illustration]]></kw>
			<kw><![CDATA[silhouettes]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14018548</person_id>
				<author_profile_id><![CDATA[81100015154]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Aaron]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hertzmann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[New York University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39037916</person_id>
				<author_profile_id><![CDATA[81100328351]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Denis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zorin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[New York University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{1} I. A. Babenko. Singularities of the projection of piecewise-linear surfaces in rspan3. <i>Vestnik Moskov. Univ. Ser. I Mat. Mekh.</i>, 1991(2): 72-75.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{2} Thomas F. Banchoff and Ockle Johnson. The normal Euler class and singularities of projections for polyhedral surfaces in 4-space. <i>Topology</i>, 37(2): 419-439, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>826471</ref_obj_id>
				<ref_obj_pid>826028</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{3} Fabien Benichou and Gershon Elber. Output sensitive extraction of silhouettes from polygonal geometry. <i>Pacific Graphics '99</i>, October 1999. Held in Seoul, Korea.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{4} William M. Boothby. <i>An Introduction to Differentiable Manifolds and Riemannian Geometry</i>. Academic Press, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280949</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{5} Wagner Toledo Corr&#234;a, Robert J. Jensen, Craig E. Thayer, and Adam Finkelstein. Texture Mapping for Cel Animation. In <i>SIGGRAPH 98 Conference Proceedings</i>, pages 435-446, July 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>281913</ref_obj_id>
				<ref_obj_pid>281388</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{6} Cassidy Curtis. Loose and Sketchy Animation. In <i>SIGGRAPH 98: Conference Abstracts and Applications</i>, page 317, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{7} Philippe Decaudin. Cartoon-Looking Rendering of 3D-Scenes. Technical Report 2919, INRIA, June 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>951115</ref_obj_id>
				<ref_obj_pid>951087</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{8} Thierry Delmarcelle and Lambertus Hesselink. The topology of symmetric, second-order tensor fields. In <i>Visualization '94</i>, pages 140-147, October 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>351683</ref_obj_id>
				<ref_obj_pid>351631</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{9} Oliver Deussen, J&#246;rg Harnel, Andreas Raab, Stefan Schlechtweg, and Thomas Strothotte. An Illustration Technique Using Hardware-Based Intersections. <i>Graphics Interface '99</i>, pages 175-182, June 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218440</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[{10} Matthias Eck, Tony DeRose, Tom Duchamp, Hugues Hoppe, Michael Lounsbery, and Werner Stuetzle. Multiresolution Analysis of Arbitrary Meshes. In <i>Computer Graphics Proceedings</i>, Annual Conference Series, pages 173-182. ACM Siggraph, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614310</ref_obj_id>
				<ref_obj_pid>614259</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[{11} Gershon Elber. Line art rendering via a coverage of isoparametric curves. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 1(3): 231-239, September 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614393</ref_obj_id>
				<ref_obj_pid>614269</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[{12} Gershon Elber. Line Art Illustrations of Parametric and Implicit Forms. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 4(1), January - March 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[{13} Gershon Elber. Interactive line art rendering of freeform surfaces. <i>Computer Graphics Forum</i>, 18(3): 1-12, September 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97890</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[{14} Gershon Elber and Elaine Cohen. Hidden Curve Removal for Free Form Surfaces. In <i>Computer Graphics (SIGGRAPH '90 Proceedings)</i>, volume 24, pages 95-104, August 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[{15} George K. Francis. <i>A Topological Picturebook</i>. Springer-Verlag, New York, 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[{16} George K. Francis. The Etruscan Venus. In P. Concus, R. Finn, and D. A. Hoffman, editors, <i>Geometric Analysis and Computer Graphics</i>, pages 67-77. 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[{17} Amy Gooch. Interactive Non-Photorealistic Technical Illustration. Master's thesis, University of Utah, December 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>300526</ref_obj_id>
				<ref_obj_pid>300523</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[{18} Bruce Gooch, Peter-Pike J. Sloan, Amy Gooch, Peter Shirley, and Richard Riesenfeld. Interactive Technical Illustration. In <i>Proc. 1999 ACM Symposium on Interactive 3D Graphics</i>, April 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[{19} Aaron Hertzmann. Introduction to 3D Non-Photorealistic Rendering: Silhouettes and Outlines. In Stuart Green, editor, <i>Non-Photorealistic Rendering</i>, SIGGRAPH Course Notes. 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192233</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[{20} Hugues Hoppe, Tony DeRose, Tom Duchamp, Mark Halstead, Huber Jin, John McDonald, Jean Schweitzer, and Werner Stuetzle. Piecewise smooth surface reconstruction. In <i>Computer Graphics Proceedings</i>, Annual Conference Series, pages 295-302. ACM Siggraph, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258796</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[{21} Victoria L. Interrante. Illustrating Surface Shape in Volume Data via Principal Direction-Driven 3D Line Integral Convolution. In <i>SIGGRAPH 97 Conference Proceedings</i>, pages 109-116, August 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[{22} Bruno Jobard and Wilfrid Lefer. Creating evenly-spaced streamlines of arbitrary density. In <i>Proc. of 8th Eurographics Workshop on Visualization in Scientific Computing</i>, pages 45-55, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[{23} Lutz Kettner and Emo Welzl. Contour Edge Analysis for Polyhedron Projections. In W. Strasser, R. Klein, and R. Rau, editors, <i>Geometric Modeling: Theory and Practice</i>, pages 379-394. Springer Verlag, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[{24} Pascal Mamassian and Michael S. Landy. Observer biases in the 3D interpretation of line drawings. <i>Vision Research</i>, (38): 2817-2832, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258894</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[{25} Lee Markosian, Michael A. Kowalski, Samuel J. Trychin, Lubomir D. Bourdev, Daniel Goldstein, and John F. Hughes. Real-Time Nonphotorealistic Rendering. In <i>SIGGRAPH 97 Conference Proceedings</i>, pages 415-420, August 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[{26} Kimon Nikola&#239;des. <i>The Natural Way to Draw</i>. Houghton Miffin, Boston, 1975.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237268</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[{27} Hans K&#248;hling Pedersen. A Framework for Interactive Texturing on Curved Surfaces. <i>Proceedings of SIGGRAPH 96</i>, pages 295-302, August 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>300539</ref_obj_id>
				<ref_obj_pid>300523</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[{28} Ramesh Raskar and Michael Cohen. Image Precision Silhouette Edges. In <i>Proc. 1999 ACM Symposium on Interactive 3D Graphics</i>, April 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[{29} Ulrich Reif. A degree estimate for polynomial subdivision surfaces of higher regularity. <i>Proc. Amer. Math. Soc.</i>, 124: 2167-2174, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97901</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[{30} Takafumi Saito and Tokiichiro Takahashi. Comprehensible Rendering of 3-D Shapes. In Forest Baskett, editor, <i>Computer Graphics (SIGGRAPH '90 Proceedings) </i>, volume 24, pages 197-206, August 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258890</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[{31} Michael P. Salisbury, Michael T. Wong, John F. Hughes, and David H. Salesin. Orientable Textures for Image-Based Pen-and-Ink Illustration. In <i>SIGGRAPH 97 Conference Proceedings</i>, pages 401-406, August 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[{32} Kent A. Stevens. Inferring shape from contours across surfaces. In Alex P. Pentland, editor, <i>From Pixels to Predicates</i>, pages 93-110. 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237285</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[{33} Greg Turk and David Banks. Image-Guided Streamline Placement. In <i>SIGGRAPH 96 Conference Proceedings</i>, pages 453-460, August 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[{34} J. Chal Vinson. <i>Thomas Nast: Political Cartoonist</i>. University of Georgia Press, Atlanta, 1967.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[{35} Hassler Whitney. On singularities of mappings of euclidean spaces. I. Mappings of the plane into the plane. <i>Ann. of Math. (2)</i>, 62: 374-410, 1955.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237287</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[{36} Georges Winkenbach and David H. Salesin. Rendering Parametric Surfaces in Pen and Ink. In <i>SIGGRAPH 96 Conference Proceedings</i>, pages 469-476, August 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>279236</ref_obj_id>
				<ref_obj_pid>279232</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[{37} Ciyou Zhu, Richard H. Byrd, Peihuang Lu, and Jorge Nocedal. Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound-constrained optimization. <i>ACM Trans. Math. Software</i>, 23(4): 550-560, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[{38} D. Zorin. Constructing curvature-continuous surfaces by blending. in preparation.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Illustrating smooth surfaces Aaron Hertzmann Denis Zorin New York University Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers or to redistribute to lists, requires prior specific permission and/or a fee. Abstract We 
present a new set of algorithms for line-art rendering of smooth surfaces. We introduce an ef.cient, 
deterministic algorithm for .nding silhouettes based on geometric duality, and an algorithm for segmenting 
the silhouette curves into smooth parts with con­stant visibility. These methods can be used to .nd all 
silhouettes in real time in software. We present an automatic method for generat­ing hatch marks in order 
to convey surface shape. We demonstrate these algorithms with a drawing style inspired by A Topological 
Picturebook by G. Francis. CR Categories and Subject Descriptors: I.3.3 [Computer Graphics]: Pic­ture/Image 
Generation Display algorithms. Additional Keywords: Non-photorealistic rendering, silhouettes, pen-and-ink 
illus­tration, hatching, direction .elds. 1 Introduction Line art is one of the most common illustration 
styles. Line drawing styles can be found in many contexts, such as cartoons, technical il­lustration, 
architectural design and medical atlases. These drawings often communicate information more ef.ciently 
and precisely than photographs. Line art is easy to reproduce, compresses well and, if represented in 
vector form, is resolution-independent. Many different styles of line art exist; the unifying feature 
of these styles is that the images are constructed from uniformly col­ored lines. The simplest is the 
style of silhouette drawing, which consists only of silhouettes and images of sharp creases and object 
boundaries. This style is often suf.cient in engineering and archi­tectural contexts, where most shapes 
are constructed out of simple geometric components, such as boxes, spheres and cylinders. This style 
of rendering captures only geometry and completely ignores texture, lighting and shadows. On the other 
end of the spectrum is the pen-and-ink illustration style. In pen-and-ink illustrations, variable-density 
hatching and complex hatch patterns convey infor­mation about shape, texture and lighting. While silhouette 
drawing is suf.cient to convey information about simple objects, it is of­ten insuf.cient for depicting 
objects that are complex or free-form. From many points of view, a smooth object may have no visible 
silhouette lines, aside from the outer silhouette (Figure 8), and all the information inside the silhouette 
is lost. In these cases, can be added to indicate the shape of the surface. The primary goal of our work 
was to develop rendering techniques for automatic generation of line-art illustrations of piecewise-smooth 
free-form surfaces. When using conventional photorealistic rendering techniques (e.g. Z-buffer or ray 
tracing) Figure 1: Illustrations of the Cupid mesh. one can typically replace a smooth surface with a 
polygonal approx­imation, and thus reduce the problem to that of rendering polygo­nal meshes. This no 
longer true when our goal is to generate line drawings. Some differential quantities associated with 
the smooth surface must be recovered in order to generate visually pleasing hatch directions and topologically 
correct silhouette lines. Some of the problems that occur when a smooth surface is replaced by its polygonal 
approximation are discussed in greater detail in Sec­tion 4. In this paper we address two general problems: 
computing sil­houette curves of smooth surfaces, and generating smooth direction .elds on surfaces that 
are suitable for hatching. The algorithms that we have developed can be used to implement a number of 
non-photorealistic rendering techniques. Our main focus is on a particular rendering style, which aims 
to communicate all essential information about the shape of the surface with a limited amount of hatching. 
Contributions. Algorithms. To support rendering of smooth sur­faces, we have developed a number of novel 
algorithms including: An ef.cient, deterministic algorithm for detecting silhouettes; (Section 4.3). 
In addition to non-photorealistic applications, this method can be used to accelerate computation of 
shadow volumes.  An algorithm for cusp detection and segmentation of silhouette curves into smooth parts 
with constant visibility (Section 4.2).  An algorithm for computing smooth direction .elds on surfaces, 
suitable for use in hatching (Section 5). These .elds have a wide range of uses, ranging from high-quality 
pen-and-ink rendering to interactive illustration and hatching.  An important feature of our approach 
is that any polygonal mesh can serve as input; the smooth surface that we render is inferred from the 
mesh. We do not assume an explicitly speci.ed parame­ terization, which make our approach more general 
than previously developed techniques. Rendering style. We have developed a new non-photorealistic ren­dering 
style based on the techniques of Francis [15], and in.uenced by the cartoons of Thomas Nast [34] and 
others. The rules for drawing in this style are described in Section 6. 2 Previous Work The methods 
used in nonphotorealistic rendering can be separated into two groups: image-space and object-space. The 
image-based approach is general and simple; however, it is not particularly suit­able for generating 
concise line drawings of untextured smooth sur­faces. Image-based techniques are presented in [5, 30, 
7, 18, 6, 28]; these algorithms exploit graphics hardware to produce image preci­sion silhouette images. 
Our technique is an object space method; it directly uses the 3D representation of objects, rather than 
their im­ages. Winkenbach and Salesin [36] describe a method for produc­ing appealing pen-and-ink renderings 
of smooth surfaces. Paramet­ric lines on NURBS patches were used to determine the hatch direc­tions and 
silhouette lines were computed using polyhedral approx­imation to the surface. Their main technical focus 
is on using the hatch density to render complex texture and lighting effects. Their system relied on 
a surface parameterization to produce hatch di­rections; however, such a parameterization does not exist 
for many types of surfaces, and can often be a poor indicator of shape when it does exist. Elber [12, 
13] and Interrante [21] used principal cur­vature directions for hatching. Curvatures generally provide 
good hatch directions, but cannot be reliably or uniquely computed at many points on a surface. Our system 
makes use of the principle curvature directions, and uses an optimization technique to .ll in the hatching 
.eld where it is poorly-de.ned. Deussen et al. [9] use intersections of the surfaces with planes; while 
being quite .exible, this approach requires segmentation of the surface into parts, where different groups 
of planes are used; the plane orientations computed using skeletons relate only indirectly to the local 
surface properties. Our work also draws on techniques developed for vector .eld vi­sualization [8, 22]. 
It should be noted that relatively little work has been done on generating .elds on surfaces as opposed 
to visualiza­tion of existing .elds. Elber [12, 13] discuses the relative merits of some commonly-used 
hatching .elds (principle curvature direc­tions, .eld of tangents to the isoparametric lines, the gradient 
.eld of the brightness). Silhouette detection is an important component of many non­photorealistic rendering 
systems. Markosian et al. [25] presented a randomized algorithm for locating silhouettes; this system 
is fast but does not guarantee that all silhouettes will be found. Gooch et al. [18] and Benichou and 
Elber [3] proposed the use of a Gauss map to ef.ciently locate all object silhouettes under orthographic 
projection. In this paper, we present a new method for silhouette detection that is fast, deterministic, 
and applicable to both ortho­graphic and perspective projection. Our method for computing the silhouette 
lines of free-form sur­faces is closely related to the work of [14, 17] in computing silhou­ettes for 
NURBS surfaces. 3 Overview In this section we present a general overview of our algorithms. Surface 
representation. The input data for our system is a polyg­onal mesh that approximates a smooth surface. 
Polygonal meshes remain the most common and .exible form for approximating sur­faces. However, information 
about differential quantities (normals, curvatures, etc.) associated with the original surface is lost. 
We need a way to estimate these quantities and compute, if necessary, .ner approximations to the original 
smooth surface. This can be Figure 2: Klein bottle. Lighting and hatch directions are chosen to convey 
surface shape. Undercuts and Mach bands near the hole and the self-intersection enhance contrast. done 
if we choose a method that allows us to construct a smooth surface from an approximating arbitrary polygonal 
mesh, and easily compute the associated differential quantities (normals, curvatures, etc.). We use piecewise-smooth 
subdivision, similar to the algorithms presented in [20], with an important modi.cation (Appendix A) 
to make the curvature well-de.ned and nonzero at extraordinary ver­tices. However, other ways of de.ning 
smooth surfaces based on polygonal meshes can be used, provided that all the necessary quan­tities can 
be computed. Algorithms. Our rendering technique has three main stages: com­putation of a direction .eld 
on the surface, computation of the sil­houette lines and generation of hatch lines. Hatch direction .eld. 
This stage de.nes a view-independent .eld on the surface that can be used later to generate hatches. 
Rather than de.ning two separate directional .elds, we de.ne a single cross .eld (Section 5) for hatches 
and cross-hatches. The main steps of our algorithm are: smooth the surface if necessary; compute an initial 
approximation to the .eld in areas of the surface where it is well de.ned, initialize the directions 
arbitrarily elsewhere; optimize the directions in places where the cross .eld was not well de.ned. Silhouette 
curve computation. We compute the curves in several steps (Section 4): compute boundary, self-intersection 
and crease curves, as well as boundaries of .at areas; compute silhouette curves as zero-crossings of 
the dot product of the normal with the view direction; .nd cusps, determine visibility, and segment the 
sil­houette curves into smooth pieces. Hatch generation. Our hatch generation algorithms follow some 
of the rules described by Francis [15] (Section 6). The surface is divided into four levels of brightness 
with corresponding levels of hatching: highlights and Mach bands (no hatching), midtones (sin­gle hatching), 
shadowed regions (cross-hatching), and undercuts (dense cross-hatching). Line thickness varies within 
each region according to the lighting. Undercuts and Mach bands are used to increase contrast where objects 
overlap. Lights are placed at the view position or to the side of the object. The hatching algorithm 
covers all hatch regions with cross-hatches, then removes hatches from the single hatch regions as necessary. 
 4 Computing Silhouette Drawings In this section we describe algorithms for generating the simplest 
line drawings of smooth surfaces, which we call silhouette draw­ings. A silhouette drawing includes only 
the images of the most visually important curves on the surface: boundaries, creases, sil­houette lines 
and self-intersection lines. Finding intersections of smooth surfaces is a complex problem, which we 
do not address in the paper. We .nd self-intersections of a mesh approximating the surface and assume 
that self-intersection lines of the mesh approx­imate the self-intersection curves of the surface suf.ciently 
well. Boundary curves and creases are explicitly represented in the sur­face; thus, we focus our attention 
on the problem of computing the silhouette lines. We will refer to the creases, boundaries and self­intersection 
curves as feature curves. Before proceeding, we recall several de.nitions1. First, we de­.ne more precisely 
what we mean by a piecewise-smooth surface. A piecewise-smooth surface can be thought of as a .nite union 
of a number of smooth surfaces with boundaries. A smooth embed­ded surface is a subset M of R3 such that 
for any point p of this subset there is a neighborhood U(p)=Balle(p)n M and a C1­continuous nondegenerate 
one-to-one map F(u, v)from a domain D in R2 onto U(p). The domain D can be taken to be an open disk for 
interior points, and a half-disk (including the diameter, but excluding the circular boundary) for smooth 
boundary points. It follows from the de.nition that the normal Fu × Fv is de.ned and is nonzero everywhere 
on the surface. The direction of Fu × Fv at any point of the surface is independent, up to a sign, of 
the local parameterization F and is denoted n(p). The silhouette set for the smooth surface is the set 
of points p of the surface such that (n(p)· (p - c))=0, where c is the view­point. The silhouette is 
in general a union of .at areas on the sur­face, curves and points. We isolate .at areas and consider 
them separately. Isolated silhouette points are unstable, and are not rele­vant for our purposes. For 
a surface that does not contain .at areas and is C2, the silhouette for a general position of the viewpoint 
can be shown to consist of C1 non-intersecting curves (silhouette curves). An important role in our constructions 
is played by the curvature of the surface. More speci.cally, we are interested in principal cur­vatures 
and principal curvature directions. The two principal curva­tures at a point p are maximal and minimal 
curvatures of the curves obtained by intersecting the surface with a plane passing through p and containing 
the normal to the surface. The principal curvature directions are the tangents to the curves for which 
the maximum and minimum are obtained; these directions are always orthogonal and lie in the tangent plane 
to the surface. The formulas expressing these quantities in terms of the derivatives of F are standard 
and can be found, for example, in [4]. The most important property of the principal curvatures that we 
use can be formulated as follows: if a surface has principal curvatures 1 and 2, and the unit vectors 
along principal directions and the normal are used to de.ne an or­thonormal coordinate system (r, s, 
t), with r and s parameterizing the tangent plane then locally the surface is the graph of a function 
over the tangent plane 2 222 t = 1r + 2s +o(r +s ) (1) It follows that principal curvatures and principal 
curvature direc­tions locally de.ne the best approximating quadratic surface. 4.1 Silhouettes of Meshes 
and Smooth Surfaces The simplest approach to computing the silhouette curves would be to replace the 
smooth surface with its triangulation and .nd the silhouette edges of the triangular mesh. However, there 
are sig­ni.cant differences between the silhouettes of smooth surfaces and their approximating polygonal 
meshes (Figure 4). For polygonal meshes, complex cusps (Figure 3), where several silhouette chains meet, 
are stable, that is, do not disappear when the viewpoint is perturbed. Singularities of projections of 
polyhedra were studied 1We do not state rigorous mathematical de.nitions in complete detail; an interested 
reader can .nd them in most standard differential geometry texts. in considerable detail (see a recent 
paper [2] for pointers); a simple classi.cation in the two-dimensional case, which does not appear to 
be explicitly described elsewhere, can be found in [1]. For smooth surfaces, the only type of stable 
singularity is a simple cusp, as it was shown in the classic paper by Whitney [35]. As a consequence, 
silhouette curves on smooth surfaces are either closed loops, or start and end on feature lines, while 
on polygonal surfaces they may in­tersect, and their topology is more complex. Moreover, we observe 
complex polygonal simple smooth cusp cusp Figure 3: Left: Complex cusps are stable on polygonal meshes. 
Right: Only simple cusps are stable on smooth surfaces. that no matter how .ne the triangulation is, 
the topology of the sil­houette of a polygonal approximation to the surface is likely to be signi.cantly 
different from that of the smooth surface itself. This does not present a problem for .xed-resolution 
images: if the dis­tance between the projected silhouette of the mesh and the projected silhouette of 
the smooth surface is less than a pixel, the topological details cannot be distinguished. However, if 
we do want to generate resolution-independent images capturing the essential features of the silhouette 
of the smooth surface correctly, or apply line styles to the silhouette curves, the polygonal approximation 
cannot be used. (Figure 4). Similar observations were made in [5]. To preserve the essential topological 
properties of silhouettes, we compute the silhouette curves using an approach similar to the one used 
in [14, 17] for spline surfaces. Recall that the silhouette set of a surface is the zero set of the function 
g(p)=(n(p)· (p - c)) de.ned on the surface. The idea is to compute an approximation to this function 
and .nd its zero set. For each vertex p of the polyg­onal approximation, we compute the true surface 
normal and g(p) at the vertex. Then the approximation to the function g(p)is de­.ned by linear interpolation 
of the values of the function. As the resulting function is piecewise-linear, the zero set will consist 
of line segments inside each triangle of the polygonal approximation. Moreover, we can easily enforce 
the general position assumption by picking arbitrarily the sign of the function g(p)at vertices where 
it happens to be exactly zero. As a result, the line segments of the zero set connect points in the interior 
of the edges of the mesh, and form either closed loops or non-intersecting chains connecting points on 
the feature lines (Figure 4), similar in structure to the ac­tual silhouette curves. We may miss narrow 
areas on the surface where the sign is different from surrounding areas. It is easy to see, however, 
that the silhouette curves we obtain by our method will have the same topology as the silhouette curves 
of some surface obtained by a small perturbation of the original. This means that we are guaranteed to 
have a plausible image of a surface, but it may not accurately re.ect features of size on the order of 
the size of a triangle of the approximating mesh in some cases. The silhouette algorithm is described 
in greater detail in [19].  4.2 Cusp Detection While the silhouette curves on the surface do not have 
singularities in a general position, the projected silhouette curves in the image plane do; there is 
a single stable singularity type, aside from termi­nating points at feature lines: a simple cusp (Figure 
3). The most straightforward way to detect these singularities is to examine the (b) (c) (d) (e) Figure 
4: (a) Silhouette edges of a polygonal approximation pro­duce jagged silhouette curves. (b) Our method 
produces smooth silhouette curves by inferring information about the smooth surface from the polygonal 
mesh. (c) The same curves shown from another viewpoint and overlayed. (d) A complex cusp occurs in the 
polyg­onal approximation when the surface is nearly parallel to the view direction. This does not occur 
in the smooth silhouette curve. (e) Smooth line drawing of the smiling torus. The red box shows the location 
of the curves in (a)-(c). tangents of the silhouette curves; cusps are the points where the tan­gent 
is parallel to the view direction. However, this approach is not numerically reliable, especially if 
the silhouette curves are approx­imated by polylines. We propose a new, numerically more robust way to 
.nd the cusps, using the following geometric observations. Consider a silhouette point p with principal 
curvature directions w1 and w2 and principal curvatures 1 and 2. Let c be the viewpoint; since p is the 
silhouette point, the viewing direction v = c - p is in the tangent plane. Let [c1,c2, 0]be the compo­nents 
of c with respect to the coordinates (r, s, t)associated with the principal curvature directions, computed 
by c1 =(v · w1)and c2 =(v·w2). As we have observed, p is a cusp when the tangent to the silhouette at 
p is parallel to the viewing direction v. The tangent to the silhouette can easily be expressed in terms 
of curvature. Ap­proximation (1) yields the following approximation to the normals in a small neighborhood 
near p: n(r, s)=[-2 1r, -2 2s, 1]. The equation of the 2nd order approximation to the silhouette curve 
is an implicit quadratic equation, g(r, s)=(n(r, s)· v(r, s))=0, where v(r, s)is the viewing direction 
c - p(r, s)=[c1 - r, c2 - s, - 1r 2 - 2s 2]. We calculate the vector perpendicular to the silhouette 
at p as Vg(0, 0) = [-2 1c1, -2 2c2]. The resulting condition for the viewing direction to be parallel 
to the silhouette tangent (or, equivalently, perpendicular to Vg(0, 0)) to the view­ing direction is 
1c12 + 2c22 =0. Therefore, we can de.ne a parameterization-independent scalar function on the surface 
which we call the cusp function: C(p)= 1 (v · w1)2 + 2 (v · w2)2 where all quantities are evaluated at 
point p. This function has the following important property: cusps are contained in the intersec­tion 
set of the two families of curves: one obtained as the zero set of the function g(p), the other as the 
zero set of the cusp function C(p) (Figure 5). The zero set of C(p)can be approximated in the same way 
as the zero set of g(p); each triangle of the polygo­nal mesh may contain a single line segment approximating 
the zero set of C(p)and another approximating the zero set of g(p). This allows us to compute approximate 
cusp locations robustly, without introducing many spurious cusps, and at the same time using rela­tively 
coarse polygonal approximations to the smooth surface.  Figure 5: Left: Cusps are found as intersections 
of zero sets of two functions de.ned on the surface, the dot product of the normal with the viewing direction 
and the cusp function. The silhouette curve is shown in blue, the cusp zero set in red. Right: The same 
curves; view from a viewpoint different from the one that was used to compute the curves.  4.3 Fast 
Silhouette Detection In the previous section, we have presented an algorithm for con­structing approximations 
to the silhouette curves which, when im­plemented in the simplest way, requires complete traversal of 
the mesh. Such a traversal is unnecessary; typically, only a small per­centage of mesh faces contain 
silhouettes [25, 23]. For polygo­nal meshes, a number of fast techniques were developed that allow one 
to avoid complete traversal. A stochastic algorithm was pro­posed in [25]. A deterministic algorithm 
based on the Gauss map was proposed in [3, 18], but is restricted to orthographic projection. We present 
a new deterministic algorithm for accelerated location of silhouettes, which works for both orthographic 
and perspective projection. This algorithm is equally suitable for .nding silhouettes de.ned as zero 
sets, and for .nding silhouette edges of polygonal meshes. Our algorithm is based on the concept of dual 
surfaces. The points of the dual surface M' are the images of the tangent planes to a surface M under 
a duality map, which maps each plane Ax +By +Cz +D =0to the homogeneous point [A, B, C, D]. More explicitly, 
M' can be obtained by mapping each point of M to a homogeneous point N =[n1,n2,n3, -(p · n)], where n 
=[n1,n2,n3, 0]is the unit normal at p. Note that the inverse is also true: each plane in the dual space 
corresponds to a point in the primal space. Let C =[c1,c2,c3,c4]be our viewpoint in the homogeneous form. 
Then the silhouette of the surface consists of all points p for which C is in the tangent plane at that 
point. For perspective projection, this means that (C · N)=(c - p)· n =0. For orthographic projection, 
the homogeneous formula is the same: (C · N)=(c · n)=0, where c is interpreted as the view direction. 
Our algorithm is based on the following observation: the image of the silhouette set of the surface with 
respect to the viewpoint C un­der the duality map is the intersection of the plane (C · x)=0, with the 
dual surface. This fact allows us to reduce the problem of .nding the silhouette to the problem of intersecting 
a plane with a surface (Figure 7), for which many space-partition-based accelera­tion techniques are 
available. However, an additional complication is introduced by the fact that some points of the dual 
surface may be at in.nity. This does not allow us to consider only the .nite part of the projective space, 
which can be identi.ed with R3. However we can identify the whole 3D projective space with points of 
the unit hypersphere S3, or, equivalently, of the boundary of a hypercube, in four-dimensional space. 
As four-dimensional space is somewhat dif.cult to visualize, we show the idea of the algorithm on a 2D 
example in Figure 6. In the 2D case, the problem is to compute all silhouette points on a curve, that 
is, the points for which the tangent line contains the viewpoint. While the geometric background is somewhat 
abstract, the actual algorithm is quite simple. The input to the algorithm is a polygonal mesh, with 
normals speci.ed at vertices, if we are computing sil­houettes using zero-crossings. The normals are 
not necessary if we are locating the silhouette edges of the polygonal mesh. There are two parts to the 
algorithm: initialization of the spatial partition and Figure 6: Left: Using a dual curve to .nd silhouette 
points. The .gure shows a curve in the plane z =1 and its dual on a sphere. The blue arrow is the vector 
cfrom the origin in 3D to the viewpoint in the plane, the blue circle is the intersection of the plane 
passing through the origin perpendicular to cwith the unit sphere. The red points are a silhouette point 
and its dual. The silhouette point can be found by intersecting the blue circle with the dual curve and 
retriev­ing corresponding point on the original curve. Right: Reducing the intersection problem to planar 
subproblems. The upper hemisphere containing the dual curve is projected on the surface of cube and at 
most 5 (in this case 3) planar curve-line intersection problems are solved on the faces. viewpoint 1 
silhouette 2 point 3 dual project curve curve 1 2 3 Figure 7: Silhouette lines under the duality map 
correspond to the intersection curve of a plane with the dual surface. Top: Torus shown from camera and 
side views. Bottom: The eight 3D faces of the hypercube, seven of which contain portions of the dual 
surface. The viewpoint dual is shown as a blue plane. Silhouettes occur at the intersection of the dual 
plane with the dual surface. intersection of the dual surface with the plane corresponding to the viewpoint. 
The second part is fairly standard, so we focus on the .rst part. Step 1: For each vertex p with normal 
n, we compute the dual position N =[n1,n2,n3,-(p · n)]. The dual positions de.ne the dual mesh which 
has different vertex positions but the same connectivity. Step 2: Normalize each dual position N using 
lo-norm, that is, divide by max(|N1|,|N2|,|N3|,|N4|). After division, at least one of the components 
Ni, i =1..4, becomes 1 or -1. The resulting four-dimensional point is on the surface of the unit hypercube. 
The three-dimensional face of the cube on which the vertex is located is determined by the index and 
sign of the maximal component. Step 3: Each triangle of the dual mesh is assigned to a list for every 
three-dimensional face in which it has a vertex. Step 4: An octtree is constructed for each three-dimensional 
face, and the triangles assigned to this face are placed into the octtree. The second step of the algorithm, 
which is repeated for each frame, uses the octtree to .nd the silhouette edges for a given cam­era position 
by intersecting the dual plane with the dual surface. We have implemented an interactive silhouette viewer 
based on the dual space method. In our tests, silhouette tests were performed on twice as many triangles 
as there were actual triangles contain­ing silhouettes, suggesting that performance is roughly linear 
in the number of silhouette triangles. This represents a substantial speedup over traversing the entire 
mesh. Silhouette edge detec­tion and visibility calculations on the three-times subdivided Venus model 
(.90,000 triangles) can be performed at approximately 17 frames per second on a 225 MHz SGI Octane, without 
using graph­ics hardware, which is similar to the performance of the nondeter­ministic algorithm of [25]. 
 4.4 Visibility Before computing visibility, we separate the silhouette curves into segments. Visibility 
is determined for each segment. The follow­ing points are used to separate segments: cusps, silhouette-feature 
joints, and inverse images of silhouette-feature and silhouette­silhouette intersections in image space. 
Visibility can change only at these points, thus each segment is either completely visible or invisible. 
Determining visibility is fundamentally dif.cult for smooth sur­faces, because it cannot be inferred 
precisely from visibility of the approximating mesh. Our algorithm can only guarantee that the correct 
visibility will be produced if the mesh is suf.ciently .ne, us­ing a theoretically-estimated required 
degree of re.nement. How­ever, the estimate is too conservative and dif.cult to compute to be practical; 
in our implementation, we re.ne the mesh to a .xed subdivision level. Our visibility algorithm is based 
on the following observation: at any area on the surface, the rate of change of the normal is bounded 
by the maximal directional curvature. For a suf.ciently .ne triangu­lation, one can guarantee that for 
any triangle for which (n·(p-c)) changes sign, there is a silhouette edge of the polygonal approxi­mation 
adjacent to a vertex of the triangle. We use the visibility of these edges to compute visibility of the 
silhouette curves. The visibility of the silhouette edges can be determined using known techniques (e.g. 
[25]). For each curve we .nd visibility of all nearby silhouette edges (which is not necessarily consistent) 
and use the visibility of the majority of the edges to determine visibility of the chain. It is pos­sible 
to show that this method will produce correct visibility for suf.ciently .ne meshes in the following 
sense: there is a smooth surface for which the precise projection has the same topology as the one computed 
by our method. In practice, we have found that the algorithm performs well even without extra re.nement 
near the silhouettes, provided that the orig­inal mesh is suf.ciently close to the surface. An ef.cient 
algorithm with better-de.ned properties would be useful.  5 Direction Fields on Surfaces Fields on 
surfaces. To generate hatches, we need to choose sev­eral direction .elds on visible parts of the surface. 
The direction .elds are different from the more commonly used vector .elds: un­like a vector .eld, a 
direction .eld does not have a magnitude and does not distinguish between the two possible orientations. 
The .elds can either be de.ned directly in the image plane as in [31], or de.ned on the surface and then 
projected. The advan­tage of the former method is that the .eld needs to be de.ned and continuous only 
in each separate area of the image. However, it is somewhat more dif.cult to use the information about 
the shape of the objects when constructing the .eld, and the .eld must be re­computed for each image. 
We choose to generate the .eld on the surface .rst. A number of different .elds on surfaces have been 
used to de.ne hatching directions. The most commonly-used .eld is probably the .eld of isoparametric 
lines; this method has obvious limitations, (a) (b) (c) (d) (e) (f) Figure 8: Direction .elds on the 
Venus. (a) Silhouettes alone do not convey the interior shape of the surface. (b) Raw principle curvature 
directions produce an overly-complex hatching pattern. (c) Smooth cross .eld produced by optimization. 
Reliable principal curvature directions are left unchanged. Optimization is initialized by the principal 
curvatures. (d) Hatching with the smooth cross .eld. (e) Very smooth cross .eld produced by optimizing 
all directions. (f) Hatching from the very smooth .eld. as the parameterization may be very far from 
isometric, and is not appropriate for surfaces lacking a good natural parameterization, such as subdivision 
surfaces and implicit surfaces. The successes and failures of this approach provide valuable clues for 
construction of .elds for hatching. The most natural geometric candidate is the pair of principal cur­ 
vature direction .elds [13, 21]. corresponding to the minimal and maximal curvatures2. We will refer 
to the integral lines of these .elds as curvature lines. These .elds do not depend on param­ eterization, 
capture important geometric features, and are consis­ tent with the most common two-directional hatching 
pattern. How­ ever, they suffer from a number of disadvantages. All umbilical points (points with coinciding 
principal curvatures) are singular­ ities, which means that the .elds are not de.ned anywhere on a sphere 
and have arbitrarily complex structure on surfaces obtained by small perturbations of a sphere. On .at 
areas (when both cur­ vatures are very small) the .elds are likely to result in a far more complex pattern 
than the one that would be used by a human. Other candidates include isophotes (lines of constant brightness) 
and the gradient .eld of the distance to silhouette or feature lines [25, 12]. Both are suitable for 
hatching in a narrow band near silhouettes or feature lines, but typically do not adequately cap­ ture 
shape further from silhouettes, nor are they suitable for cross­ hatching. Our approach is based on several 
observations about successes and failures of existing methods, as well as hatching techniques used by 
artists. Cylindric surfaces. Surface geometry is rendered best by princi­pal curvature directions on 
cylindrical surfaces, that is, surfaces for which one of the principal curvatures is zero (all points 
of the sur­face are parabolic). This fact is quite remarkable: psychophysical studies con.rm that even 
a few parallel curves can create a strong impression of a cylindrical surface with curves interpreted 
as prin­cipal curvature lines [32, 24]. Another important observation is that for cylinders the principal 
curvature lines are also geodesics, which is not necessarily true in general. Hatching following the 
principal curvature directions fails when the ratio of principal curvatures is close to one. Deussen 
et al. [9] uses intersections of the surface with planes to obtain hatch directions; the resulting curves 
are likely to be locally close to geodesics on slowly varying surfaces.  Isometric parameterizations. 
Isoparameteric lines work well as curvature directions when a parameterization exists and is close  
2It is possible to show that for a surface in general position, these .elds are always globally de.ned, 
excluding a set of isolated singularities. to isometric, i.e. minimizes the metric distortion as described 
in, for example, [10, 27]. In this case, parametric lines are close to geodesics. Isoparametric lines 
were used by [36, 11]. Artistic examples. We observe that artists tend to use relatively straight hatch 
lines, even when the surface has wrinkles. Smaller details are conveyed by varying the density and the 
number of hatch directions (Figure 9). Figure 9: Almost all hatches in this cartoon by Thomas Nast curve 
only slightly, while capturing the overall shape of the surface. Note that the hatches often appear to 
follow a cylinder approximating the surface. Small details of the geometry are rendered using variations 
in hatch density. These observations lead to the following simple requirements for hatching .elds: in 
areas where the surface is close to parabolic, the .eld should be close to principal curvature directions; 
on the whole surface, the integral curves of the .eld should be close to geodesic. In addition, if the 
surface has small details, the .eld should be gen­erated using a smoothed version of the surface. Cross 
.elds. While it is usually possible to generate two global direction .elds for the two main hatch directions, 
we have ob­served that this is undesirable in general. There are two reasons for this: .rst, if we would 
like to illustrate nonorientable surfaces, such .elds may not exist. Second, and more importantly, there 
are natural cross-hatching patterns that cannot be decomposed into two smooth .elds even locally (Figure 
10). Thus, we consider cross .elds, that is, maps de.ned on the surface, assigning an unordered pair 
of perpendicular directions to each point. Constructing Hatching Fields. Our algorithm is based on the 
considerations above and proceeds in steps. Figure 10: A cross-hatching pattern produced by our system 
on a smooth corner. This pattern cannot be decomposed into two orthogonal smooth .elds near the corner 
singularity. The ana­lytic expression for a similar .eld in the plane is v1(r, e)= [cos(e/4), sin(e/4)]; 
v2(r, e)=[- sin(e/4), cos(e/4)]. This .eld is continuous and smooth only if we do not distinguish be­tween 
v1 and v2. Step 1. Optionally, create a smoothed copy of the original mesh. The copy is used to compute 
the .eld. The amount of smoothing is chosen by the user, with regard to the smoothness of the original 
mesh, and the scale of geometric detail the user wishes to capture in the image. For example, no smoothing 
might be necessary for a close-up view of a small part of a surface, while substantial smooth­ing may 
be necessary to produce good images from a general view; in practice we seldom found this to be necessary. 
Step 2. Identify areas of the surface which are suf.ciently close to parabolic, that is, the ratio of 
minimal to maximal curvature is high, and at least one curvature is large enough to be computed reliably. 
Additionally, we mark as unreliable any vertex for which the aver­age cross .eld energy of its incident 
edges exceeds a threshold, in order to allow optimization of vertices that begin singular. Step 3. Initialize 
the .eld over the whole surface by computing principal curvature directions. If there are no quasi-parabolic 
areas, user input is required to initialize the .eld. Step 4. Fix the .eld in quasi-parabolic areas and 
optimize the .eld on the rest of the vertices, which were marked as unreliable. This step is of primary 
importance and we describe it in greater detail. Our optimization procedure is based on the observation 
that we would like the integral lines of our .eld to be close to geodesics. We use a similar, but not 
identical, requirement that the .eld is as close to constant as possible. Minimizing the angles between 
the world-space directions at adjacent vertices of the mesh is possible, but requires constrained optimization 
to keep the directions in the tangent planes. We use a different idea, based on establishing a correspondence 
between the tangent planes at different points of the surface, which, in some sense, corresponds to the 
minimal possible motion of the tangent plane as we move from one point to another. Then we only need 
to minimize the change of the .eld with respect to the corresponding directions in the tangent planes. 
 Figure 11: Moving vectors along geodesics. Given two suf.ciently close points p1 and p2 on a smooth 
sur­face, a natural way to map the tangent plane at p1 to the tan­gent plane at p2 is to transport vectors 
along the geodesics (Fig­ure 11); for suf.ciently close points there is a unique geodesic .(t), t =0..1, 
connecting these points. This is done by mapping a unit vector u1 in the tangent plane at p1 to a unit 
vector u2 in the tan­gent plane at p2, such that the angle between u1 and the tangent to the geodesic 
. ' (0) is the same as the angle between u2 and . ' (1). In discrete case, for adjacent vertices of the 
approximating mesh vi and vj , we approximate the tangents to the geodesic by the projec­tions of the 
edge (vi, vj ) into the tangent planes at the vertices. Let the directions of these projections be tij 
and tji. Then a rigid trans­formation Tij between the tangent planes is uniquely de.ned if we require 
that tij maps to tji and that the transformation preserves orientation. Then for any pair of tangent 
unit vectors wi and wj at vi and vj respectively, we can use ~Tij wi - wj ~ to measure the difference 
between directions. One can show that the value of this expression is the same as ~Tjiwj -wi~. To measure 
the difference between the values of the cross .eld at two points, we choose a unit tangent vector for 
each point. The vectors are chosen along the di­rections of the cross .eld. There are four possible choices 
at each point. We choose a pair of unit vectors for which the difference is minimal. We now explicitly 
specify the energy functional. The cross .eld is described by a single angle ei for each vertex vi, which 
is the angle between a .xed tangent direction ti, and one of the directions of the cross .eld; we do 
not impose any limita­tions on the value of ei, and there are in.nitely many choices for ei differing 
by n./2 that result in the same cross .eld. Let .ij be the direction of the projection of the edge (vi, 
vj ) into the tangent plane at vi. Using this choice of coordi­nates, one can show that the quantity 
~Tij wi - wj ~ is equal to mink2 - 2 cos ((ei - .ij ) - (ej - .ji)+ k./2). Minimiza­tion of this quantity 
is equivalent to minimization of E(i, j)= mink (- cos ((ei - .ij ) - (ej - .ji)+ k./2)), which is not 
dif­ferentiable. We observe, however, that E0(i, j)= -8E(i, j)4 + 8E(i, j)2 - 1 is just - cos 4 ((ei 
- .ij ) - (ej - .ji)), and is a ' monotonic function of E(i, j) on [2/2..1], the range of possi­ble values 
of E(i, j). Thus, instead of minimizing E(i, j), we can minimize E0(i, j). We arrive at the following 
simple energy: = -cos 4 ((ei - .ij ) - (ej - .ji)) E.eld all edges (vi, vj ) which does not require 
any constraints on the variables ei. Note that the values .ij are constant. Due to the simple form of 
the functional, it can be minimized quite quickly. We use a variant of the BFGS conjugate gradient algorithm 
described in [37] to per­form minimization. For irregularly-sampled meshes, the energy may also be weighted 
in inverse proportion to edge length. We have not found this to be necessary for the meshes used in this 
paper. The result of the optimization depends on the threshold chosen to determine which vertices are 
considered unreliable; in the extreme cases, all vertices are marked as unreliable and the whole .eld 
is op­timized, or all vertices are marked as reliable and the .eld remains unoptimized. Figure 8 shows 
the results for several thresholds.  6 Rendering Style 6.1 Style Rules Our rendering style is based 
to some extent on the rules described by G. Francis in A Topological picturebook [15], which are in turn 
based on Nikola¨ides rules for drawing drapes [26]. We have also used our own observations of various 
illustrations in similar styles. We begin our style description by de.ning undercuts and folds. A visible 
projected silhouette curve separates two areas of the image: one containing the image of the part of 
the surface on which the curve is located, the other empty or containing the image of a dif­ferent part 
of the surface. We call the former area a fold. If the (a) (b) (c) Figure 12: Hatching rules shown on 
drapes. (a) There are 3 main discrete hatch densities: highlights, midtones, and shadows, corre­sponding 
to 0, 1, and 2 directions of hatches. (b) Undercuts. (c) Mach bands. Undercuts and Mach bands increase 
contrast where surfaces overlap. latter area contains the image of a part of the surface, we call it 
an undercut. We use the following rules, illustrated in Figure 12. The surface is separated into four 
levels of hatching: high­lights and Mach bands (no hatching), midtones (single hatching), shadowed regions 
(cross-hatching), and undercuts (dense cross­hatching). Inside each area, the hatch density stays approximately 
uniform. The choice of the number of hatch directions used at a particular area of the surface is guided 
by the lighting and the fol­lowing rules:  If there is an undercut, on the other side of the silhouette 
from a fold, a thin area along the silhouette on the fold side is not hatched ( Mach band effect ). 
 Undercuts are densely hatched.  Hatches are approximately straight; a hatch is terminated if its length 
exceeds a maximum, or if its direction deviates from the original by more than a .xed angle.  Optionally, 
hatch thickness within each density level can be made inversely proportional to lighting; the resulting 
effect is rather sub­tle, and is visible only when the hatches are relatively thick.  6.2 Hatch Placement 
The hatching procedure has several user-tunable parameters: basic hatch density speci.ed in image space; 
the hatch density for under­ cuts; the threshold for highlights (the areas which receive no hatch­ ing); 
the threshold that separates single hatch regions from cross hatch regions; the maximum hatch length; 
the maximum deviation of hatches from the initial direction in world space. Varying these parameters 
has a considerable effect both on the appearance of the images and on the time required by the algorithm. 
Threshold values are usually chosen to divide the object more or less evenly between different hatching 
levels. Once we have a hatching .eld, we can illustrate the surface by placing hatches along the .eld. 
We .rst de.ne three intensity re­ gions over the surface: no hatching (highlights and Mach bands), single 
hatching (midtones), and cross hatching (shadowed regions). Furthermore, some highlight and hatch regions 
may be marked as undercut regions. The hatching algorithm is as follows: 1. Identify Mach bands and undercuts. 
 2. Cover the single and cross hatch regions with cross hatches, and add extra hatches to undercut regions. 
 3. Remove cross-hatches in the single hatch regions, leaving only one direction of hatches.  6.3 Identifying 
Mach Bands and Undercuts In order to identify Mach bands and undercuts, we step along each silhouette 
and boundary curve. A ray test near each curve point is used to determine if the fold overlaps another 
surface. Undercuts and Mach bands are indicated in a 2D grid, by marking every grid cell within a small 
distance of the fold on the near side of the surface as a Mach band, and by marking grid cells on the 
far side of the surface within a larger distance as undercuts. (This is the same 2D grid as used for 
hatching in the next section.) 6.4 Cross-hatching We begin by creating evenly-spaced cross-hatches on 
a surface. We adapt Jobard and Lefers method for creating evenly-spaced stream­ lines of a 2D vector 
.eld [22]. The hatching algorithm allows us to place evenly-spaced hatches on the surface in a single 
pass over the surface. Our algorithm takes two parameters: a desired hatch separation distance dsep, 
and a test factor dtest . The separation distance in­ dicates the desired image-space hatch density; 
a smaller separation distance is used for undercuts. The algorithm creates a queue of surface curves, 
initially containing the critical curves (silhouettes, boundaries, creases, and self-intersections). 
While the queue is not empty, we remove the front curve from the queue and seed new hatches along it 
at points evenly-spaced in the image. Seeding cre­ ates a new hatch on the surface by tracing the directions 
of the cross-hatching .eld. Since the cross .eld is invariant to 90 degree rotations, at each step the 
hatch follows the one of four possible directions which has the smallest angle with the previous direction. 
Hatches are seeded perpendicular to all curves. Hatches are also seeded parallel to other hatches, at 
a distance dsep from the curve. A hatch continues along the surface until it terminates in a critical 
curve, until the world-space hatch direction deviates from the ini­ tial hatch direction by more than 
a constant, or until it comes near a parallel hatch. This latter condition occurs when the endpoint of 
the hatch p1 is near a point p2 on another hatch, such that the following conditions are met: ||p1 - 
p2|| <dtest dsep, measured in image space.  A straight line drawn between the two points in image space 
does not intersect the projection of any visible critical curves. In other words, hatches do not interfere 
when they are not nearby on the surface.  The world space tangents of the two hatch curves are parallel, 
i.e. the angle between them is less than 45 degrees, after projection to the tangent plane at p1.  The 
search for nearby hatches is performed by placing all hatches in a 2D grid with grid spacing equal to 
dsep. This ensures that at most nine grid cells must be searched to detect if there are hatches nearby 
the one being traced. 6.5 Hatch Reduction Once we have cross-hatched all hatch regions, we remove hatches 
from the single hatch regions until they contain no cross-hatches. By removing hatches instead of directly 
placing single a hatch di­ rection, we avoid the dif.culty inherent in producing a consistent vector 
.eld on the surface. Our algorithm implicitly segments the visible single-hatch regions into locally-consistent 
single hatching .elds. This allows us to take advantage of the known view direction and the limited extent 
of these regions. The reduction algorithm examines every hatch on the surface and deletes any hatch that 
is perpendicular to another hatch. In particu­ lar, a hatch is deleted if it contains a point p1 nearby 
a point p2 on another hatch such that: p1 and p2 lie within the single hatch region.  ||p1 - p2|| <2dsep, 
measured in image space.  A straight line drawn between the two points in image space does not intersect 
any visible critical curve.  The world space tangents of the two hatch curves are perpendic­ular, i.e. 
the angle between them is greater than 45 degrees after projection to the tangent plane at p1.  Deleting 
a hatch entails clipping it to the cross-hatch region; the part of the hatch that lies within the cross-hatch 
region is left un­touched. The order in which hatches are traversed is important; a na¨ive traversal 
order will usually leave the single hatch region uneven and inconsistent. We perform a breadth-.rst traversal 
to prevent this. A queue is initialized with a hatch curve. While the queue is not empty, the front curve 
is removed from the queue. If it is perpendicular to another curve in the single hatch region, then the 
curve is deleted, and all parallel neighbors of the hatch that have not been visited are added to the 
queue. When the queue is empty, a hatch that has not yet been visited is added to the queue, if any remain. 
The tests for perpendicular is as described above; the angle condition is reversed for the parallel test. 
 7 Results and Conclusions Most of the illustrations in this paper were created using our system. Figures 
1, 8 demonstrate the results for relatively .ne meshes that de.ne surfaces with complex geometry. Figures 
2 and 13 show the results of using our system to illustrate several mathematical surfaces. The time required 
to create an illustration varies greatly; while silhouette drawings can be computed interactively, and 
the .eld op­timization takes very little time, hatching is still time-consuming, and can take from seconds 
to minutes, depending on hatch density and complexity of the model. Also, for each model the parame­ters 
of the algorithms (thresholds for hatching, position of the light sources, hatch density) have to be 
carefully chosen; Future work. As we have already mentioned, improvements should be made to the silhouette 
visibility algorithm. Performance was not our goal for the hatching algorithm. It is clear that sub­stantial 
speedups are possible. While the quality of .elds generated by our algorithms is quite good, it would 
be desirable to reduce the number of parameters that may be tuned. A more fundamental problem is the 
lack of control over the the number, type and placement of singularities of the generated .eld. As most 
surfaces of interest have low genus, the number of singu­larities can be very small for most surfaces.3 
However, the user currently has little control over their placement and additional sup­port must be provided. 
Furthermore, the hatch reduction algorithm could be made more robust to irregular cross-hatching patterns, 
and the hatching could be improved reduce hatching artifacts, perhaps by employing the optimization technique 
of Turk and Banks [33]. 3The relation between the numbers of singularities of different types is determined 
by the analogs of Euler formula; such formulas are known for vector and tensor .elds; obtaining classi.cation 
of singularities and a for­mula of this type for the cross .elds described in the paper is an interesting 
mathematical problem. (a) (b) (c) (d)  Figure 13: Several surfaces generated using G. Francis generaliza­tion 
of Ap´ery s Romboy homotopy [16]. (a) Boy surface; (b) Ida ; (c) Roman surface; (d) Etruscan Venus. Acknowledgments 
Our special thanks go to Jianbo Peng who implemented the dual surface silhouette detection algorithm. 
We are grateful to Pat Han­rahan, who suggested this research topic to us. We thank the anony­mous reviewers 
for their comments. Chris Stolte participated in the project in its early stages. This research was supported 
in part by the NSF grant DGE­9454173 and the NYU Center for Advanced Technology. A C2-surfaces based 
on subdivision Commonly used subdivision surfaces, such as variants of Loop sub­division, produce either 
surfaces with curvatures that do not con­verge or have zero curvature at extraordinary vertices. There 
are fundamental reasons for this [29]. This property is rather undesir­able, if we would like to compute 
silhouette curves, as it means either .at points or singular behavior near extraordinary points. We have 
developed a surface representation based on subdivision that produces surfaces that are everywhere C2, 
do not have zero cur­vature at extraordinary vertices, and agree arbitrarily well with the limit surfaces 
produced by subdivision. This representation is de­scribed elsewhere [38]. However, for our purposes 
it is suf.cient to have a way to compute curvatures for the surface associated with a mesh, and it is 
not necessary to have a complete surface evaluation algorithm. The curvature computation that we propose 
is based on ideas from subdivision and is compatible with the curvature computations for subdivision 
surfaces in the regular case. Consider a vertex v of the initial mesh of valence k.We will regard a part 
of the smooth surface corresponding to the 1­neighborhood of v as parameterized over a regular k-gon 
in the plane. Introduce the polar coordinates (r, .) in the plane, with u = r cos . and v = r sin .. 
then the second-order approxima­tion to the surface can be written as a0 +(a11 sin . + a12 cos .)r +(a20 
+ a21 sin 2. + a22 cos 2.)r 2 A simple calculation shows that the least squares .t to k +1 points of 
the 1-neighborhood p0 ...pk assumed to be values at (sin(2.i/k), cos(2.i/k)), i =0..k. with p0 in the 
center, leads to 1 a0 = p0; a20 = -p0 + pi k 2 2.i 2 2.i a11 = k pi sin k ; a12 = k pi cos k i i 24.i 
24.i a21 = pi sin ; a22 = pi cos kkk k i i Note that the formulas for a11 and a21 coincide with the stan­dard 
formulas for the tangents to the Loop subdivision surface, and a20,a21,a22, with appropriate variable 
changes, produce second derivatives in the regular case. To make our calculations compati­ limit ble 
with the Loop surface, we replace a0 = p0 with a0 = p0 , the limit position of the control point p0. 
As a result, we obtain a set of simple rules for computing the coef.cients of an approximat­ing quadratic 
surface, which, after appropriate change of variables can be used to compute curvatures and is compatible 
with the Loop subdivision rules. In [38], we show that one can construct a C2 sur­face which has precisely 
these curvatures at the vertices. A similar construction works for the boundary case. We should note 
that for valences k =3, 4, the coef.cients of the quadric are not indepen­dent, and thus not all possible 
local behaviors can be approximated well. Given known partial derivatives Fu, Fv, Fuu, Fuv, Fvv of the 
local parameterization of the surface, the principal curvature direc­tions and magnitudes can be computed 
as eigenvalues and eigen­vectors of the following matrix: EF LM (2) F GMN where E =(Fu · Fu), F =(Fv 
· Fu), G =(Fv · Fv), L = (Fuu · n), M =(Fuv · n), N =(Fvv · n).  References [1] I. A. Babenko. Singularities 
of the projection of piecewise-linear surfaces in rspan3. Vestnik Moskov. Univ. Ser. I Mat. Mekh., 1991(2):72 
75. [2] Thomas F. Banchoff and Ockle Johnson. The normal Euler class and singulari­ties of projections 
for polyhedral surfaces in 4-space. Topology, 37(2):419 439, 1998. [3] Fabien Benichou and Gershon Elber. 
Output sensitive extraction of silhouettes from polygonal geometry. Paci.c Graphics 99, October 1999. 
Held in Seoul, Korea. [4] William M. Boothby. An Introduction to Differentiable Manifolds and Rieman­nian 
Geometry. Academic Press, 1986. [5] Wagner Toledo Corr ea, Robert J. Jensen, Craig E. Thayer, and Adam 
Finkelstein. Texture Mapping for Cel Animation. In SIGGRAPH 98 Conference Proceedings, pages 435 446, 
July 1998. [6] Cassidy Curtis. Loose and Sketchy Animation. In SIGGRAPH 98: Conference Abstracts and 
Applications, page 317, 1998. [7] Philippe Decaudin. Cartoon-Looking Rendering of 3D-Scenes. Technical 
Report 2919, INRIA, June 1996. [8] Thierry Delmarcelle and Lambertus Hesselink. The topology of symmetric, 
second-order tensor .elds. In Visualization 94, pages 140 147, October 1994. [9] Oliver Deussen, J¨org 
Harnel, Andreas Raab, Stefan Schlechtweg, and Thomas Strothotte. An Illustration Technique Using Hardware-Based 
Intersections. Graphics Interface 99, pages 175 182, June 1999. [10] Matthias Eck, Tony DeRose, Tom Duchamp, 
Hugues Hoppe, Michael Louns­bery, and Werner Stuetzle. Multiresolution Analysis of Arbitrary Meshes. 
In Computer Graphics Proceedings, Annual Conference Series, pages 173 182. ACM Siggraph, 1995. [11] Gershon 
Elber. Line art rendering via a coverage of isoparametric curves. IEEE Transactions on Visualization 
and Computer Graphics, 1(3):231 239, Septem­ber 1995. [12] Gershon Elber. Line Art Illustrations of Parametric 
and Implicit Forms. IEEE Transactions on Visualization and Computer Graphics, 4(1), January March 1998. 
[13] Gershon Elber. Interactive line art rendering of freeform surfaces. Computer Graphics Forum, 18(3):1 
12, September 1999. [14] Gershon Elber and Elaine Cohen. Hidden Curve Removal for Free Form Sur­faces. 
In Computer Graphics (SIGGRAPH 90 Proceedings), volume 24, pages 95 104, August 1990. [15] George K. 
Francis. A Topological Picturebook. Springer-Verlag, New York, 1987. [16] George K. Francis. The Etruscan 
Venus. In P. Concus, R. Finn, and D. A. Hoffman, editors, Geometric Analysis and Computer Graphics, pages 
67 77. 1991. [17] Amy Gooch. Interactive Non-Photorealistic Technical Illustration. Master s the­sis, 
University of Utah, December 1998. [18] Bruce Gooch, Peter-Pike J. Sloan, Amy Gooch, Peter Shirley, and 
Richard Riesenfeld. Interactive Technical Illustration. In Proc. 1999 ACM Symposium on Interactive 3D 
Graphics, April 1999. [19] Aaron Hertzmann. Introduction to 3D Non-Photorealistic Rendering: Silhou­ettes 
and Outlines. In Stuart Green, editor, Non-Photorealistic Rendering, SIG-GRAPH Course Notes. 1999. [20] 
Hugues Hoppe, Tony DeRose, Tom Duchamp, Mark Halstead, Huber Jin, John McDonald, Jean Schweitzer, and 
Werner Stuetzle. Piecewise smooth surface reconstruction. In Computer Graphics Proceedings, Annual Conference 
Series, pages 295 302. ACM Siggraph, 1994. [21] Victoria L. Interrante. Illustrating Surface Shape in 
Volume Data via Principal Direction-Driven 3D Line Integral Convolution. In SIGGRAPH 97 Conference Proceedings, 
pages 109 116, August 1997. [22] Bruno Jobard and Wilfrid Lefer. Creating evenly-spaced streamlines of 
arbitrary density. In Proc. of 8th Eurographics Workshop on Visualization in Scienti.c Computing, pages 
45 55, 1997. [23] Lutz Kettner and Emo Welzl. Contour Edge Analysis for Polyhedron Projections. In W. 
Strasser, R. Klein, and R. Rau, editors, Geometric Modeling: Theory and Practice, pages 379 394. Springer 
Verlag, 1997. [24] Pascal Mamassian and Michael S. Landy. Observer biases in the 3D interpreta­tion of 
line drawings. Vision Research, (38):2817 2832, 1998. [25] Lee Markosian, Michael A. Kowalski, Samuel 
J. Trychin, Lubomir D. Bourdev, Daniel Goldstein, and John F. Hughes. Real-Time Nonphotorealistic Rendering. 
In SIGGRAPH 97 Conference Proceedings, pages 415 420, August 1997. [26] Kimon Nikola¨ides. The Natural 
Way to Draw. Houghton Mif.n, Boston, 1975. [27] Hans Køhling Pedersen. A Framework for Interactive Texturing 
on Curved Sur­faces. Proceedings of SIGGRAPH 96, pages 295 302, August 1996. [28] Ramesh Raskar and Michael 
Cohen. Image Precision Silhouette Edges. In Proc. 1999 ACM Symposium on Interactive 3D Graphics, April 
1999. [29] Ulrich Reif. A degree estimate for polynomial subdivision surfaces of higher regularity. Proc. 
Amer. Math. Soc., 124:2167 2174, 1996. [30] Takafumi Saito and Tokiichiro Takahashi. Comprehensible Rendering 
of 3-D Shapes. In Forest Baskett, editor, Computer Graphics (SIGGRAPH 90 Pro­ceedings), volume 24, pages 
197 206, August 1990. [31] Michael P. Salisbury, Michael T. Wong, John F. Hughes, and David H. Salesin. 
Orientable Textures for Image-Based Pen-and-Ink Illustration. In SIGGRAPH 97 Conference Proceedings, 
pages 401 406, August 1997. [32] Kent A. Stevens. Inferring shape from contours across surfaces. In Alex 
P. Pentland, editor, From Pixels to Predicates, pages 93 110. 1986. [33] Greg Turk and David Banks. Image-Guided 
Streamline Placement. In SIG-GRAPH 96 Conference Proceedings, pages 453 460, August 1996. [34] J. Chal 
Vinson. Thomas Nast: Political Cartoonist. University of Georgia Press, Atlanta, 1967. [35] Hassler Whitney. 
On singularities of mappings of euclidean spaces. I. Mappings of the plane into the plane. Ann. of Math. 
(2), 62:374 410, 1955. [36] Georges Winkenbach and David H. Salesin. Rendering Parametric Surfaces in 
Pen and Ink. In SIGGRAPH 96 Conference Proceedings, pages 469 476, August 1996. [37] Ciyou Zhu, Richard 
H. Byrd, Peihuang Lu, and Jorge Nocedal. Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale 
bound-constrained optimization. ACM Trans. Math. Software, 23(4):550 560, 1997. [38] D. Zorin. Constructing 
curvature-continuous surfaces by blending. in prepara­tion. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>345075</article_id>
		<sort_key>527</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2000</article_publication_date>
		<seq_no>59</seq_no>
		<title><![CDATA[Non-photorealistic virtual environments]]></title>
		<page_from>527</page_from>
		<page_to>534</page_to>
		<doi_number>10.1145/344779.345075</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=345075</url>
		<abstract>
			<par><![CDATA[<p>We describe a system for non-photorealistic rendering (NPR) of virtual environments. In real time, it synthesizes imagery of architectural interiors using stroke-based textures. We address the four main challenges of such a system &#8212; interactivity, visual detail, controlled stroke size, and frame-to-frame coherence &#8212; through image based rendering (IBR) methods. In a preprocessing stage, we capture photos of a real or synthetic environment, map the photos to a coarse model of the environment, and run a series of NPR filters to generate textures. At runtime, the system re-renders the NPR textures over the geometry of the coarse model, and it adds dark lines that emphasize creases and silhouettes. We provide a method for constructing non-photorealistic textures from photographs that largely avoids seams in the resulting imagery. We also offer a new construction, <italic>art-maps</italic>, to control stroke size across the images. Finally, we show a working system that provides an immersive experience rendered in a variety of NPR styles.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[image-based rendering]]></kw>
			<kw><![CDATA[interactive virtual environments]]></kw>
			<kw><![CDATA[non-photorealistic rendering]]></kw>
			<kw><![CDATA[texture mapping]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P15532</person_id>
				<author_profile_id><![CDATA[81100142117]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Allison]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Klein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14168357</person_id>
				<author_profile_id><![CDATA[81100480929]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wilmot]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P197596</person_id>
				<author_profile_id><![CDATA[81100017967]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Kazhdan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P295932</person_id>
				<author_profile_id><![CDATA[81100085540]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Wagner]]></first_name>
				<middle_name><![CDATA[T.]]></middle_name>
				<last_name><![CDATA[Corr&#234;a]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P10635</person_id>
				<author_profile_id><![CDATA[81100576882]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Adam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Finkelstein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P280650</person_id>
				<author_profile_id><![CDATA[81100182132]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Funkhouser]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>340929</ref_obj_id>
				<ref_obj_pid>340916</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Buck, I., Finkelstein, A., Jacobs, C., Klein, A., Salesin, D. H., Seims, J., Szeliski, R., and Toyama, K. Performance-driven hand-drawn animation. Proceedings of NPAR 2000 (June 2000).]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218395</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Chen, S. E. Quicktime VR - An image-based approach to virtual en-vironment navigation. Computer Graphics (SIGGRAPH 95), 29-38.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166153</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Chen, S. E., and Williams, L. View interpolation for image synthesis. Computer Graphics (SIGGRAPH 93), 279-288.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258896</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Curtis, C. J., Anderson, S. E., Seims, J. E., Fleischer, K. W., and Salesin, D. H. Computer-generated watercolor. Computer Graphics (SIGGRAPH 97), 421-430.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>345406</ref_obj_id>
				<ref_obj_pid>345370</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Debevec, P. Image-based modeling, rendering, and lighting. Course #35, SIGGRAPH 2000 Course Notes (July 2000).]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237191</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Debevec, P. E., Taylor, C. J., and Malik, J. Modeling and rendering architecture from photographs: A hybrid geometry- and image-based approach. Computer Graphics (SIGGRAPH 96), 11-20.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Debevec, P. E., Yu, Y., and Borshukov, G. D. Efficient view-dependent image-based rendering with projective texture-mapping. Eurographics Rendering Workshop (June 1998), 105-116.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>851569</ref_obj_id>
				<ref_obj_pid>850924</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Efros, A. A., and Leung, T. K. Texture synthesis by non-parametric sampling. IEEE International Conference on Computer Vision (1999).]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Funkhouser, T. A. A visibility algorithm for hybrid geometry- and image-based modeling and rendering. Computers and Graphics, Special Issue on Visibility (1999).]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Funkhouser, T. A., Teller, S. J., Sequin, C. H., and Khorramabadi, D. The UC Berkeley system for interactive visualization of large architectural models. Presence 5, 1 (January 1996).]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280950</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Gooch, A., Gooch, B., Shirley, P., and Cohen, E. A non-photorealistic lighting model for automatic technical illustration. Computer Graph-ics (SIGGRAPH 98), 447-452.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237200</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Gortler, S. J., Grzeszczuk, R., Szeliski, R., and Cohen, M. F. The lumigraph. Computer Graphics (SIGGRAPH 96), 43-54.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97902</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Haeberli, P. E. Paint by numbers: Abstract image representations. Computer Graphics (SIGGRAPH 90), 207-214.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>13027</ref_obj_id>
				<ref_obj_pid>13021</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Heckbert, P. Survey of texture mapping. IEEE Computer Graphics and Applications (Nov. 1986).]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280951</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Hertzmann, A. Painterly rendering with curved brush strokes of multiple sizes. Computer Graphics (SIGGRAPH 98), 453-460.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Hewlett Packard. HP PEX Texture Mapping, www.hp.com/mhm/WhitePapers/PEXtureMapping/PEXtureMapping.html.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258854</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Horry, Y., ichi Anjyo, K., and Arai, K. Tour into the picture: Using a spidery mesh interface to make animation from a single image. Computer Graphics (SIGGRAPH 97), 225-232.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311607</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Kowalski, M. A., Markosian, L., Northrup, J. D., Bourdev, L., Barzel, R., Holden, L. S., and Hughes, J. Art-based rendering of fur, grass, and trees. Computer Graphics (SIGGRAPH 99), 433-438.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237199</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Levoy, M., and Hanrahan, P. Light field rendering. Computer Graphics (SIGGRAPH 96), 31-42.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258893</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Litwinowicz, P. Processing images and video for an impressionist effect. Computer Graphics (SIGGRAPH 97), 407-414.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Manocha, D. Interactive walkthroughs of large geometric databases. Course #18, SIGGRAPH 2000 Course Notes (July 2000).]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258894</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Markosian, L., Kowalski, M. A., Trychin, S. J., Bourdev, L. D., Gold-stein, D., and Hughes, J. F. Real-time nonphotorealistic rendering. Computer Graphics (SIGGRAPH 97), 415-420.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218398</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[McMillan, L., and Bishop, G. Plenoptic modeling: An image-based rendering system. Computer Graphics (SIGGRAPH 95), 39-46.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237288</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Meier, B. J. Painterly rendering for animation. Computer Graphics (SIGGRAPH 96), 477-484.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Mizuno, S., Okada, M., and ichiro Toriwaki, J. Virtual sculpting and virtual woodcut printing. The Visual Computer 14, 2 (1998), 39-51.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>311604</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Ostromoukhov, V. Digital facial engraving. Computer Graphics (SIGGRAPH 99), 417-424.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122741</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Saito, T., and Takahashi, T. NC machining with G-buffer method. Computer Graphics (SIGGRAPH 91), 207-216.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258890</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Salisbury, M. P., Wong, M. T., Hughes, J. F., and Salesin, D. H. Ori-entable textures for image-based pen-and-ink illustration. Computer Graphics (SIGGRAPH 97), 401-406.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280882</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Shade, J. W., Gortler, S. J., wei He, L., and Szeliski, R. Layered depth images. Computer Graphics (SIGGRAPH 98), 231-242.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Tsai, R. Y. A Versatile Camera Calibration Technique for High-Accuracy 3D Machine Vision Metrology Using Off-the-Shelf TV Cameras and Lenses. IEEE Journal of Robotics and Automation 3, 4 (Aug. 1987), 323-344.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801126</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Williams, L. Pyramidal parametrics. Computer Graphics (SIG-GRAPH 83), 1-11.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192184</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Winkenbach, G., and Salesin, D. H. Computer-generated pen-and-ink illustration. Computer Graphics (SIGGRAPH 94), 91-100.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237287</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Winkenbach, G., and Salesin, D. H. Rendering parametric surfaces in pen and ink. Computer Graphics (SIGGRAPH 96), 469-476.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280948</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Wong, M. T., Zongker, D. E., and Salesin, D. H. Computer-generated floral ornament. Computer Graphics (SIGGRAPH 98), 423-434.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258859</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Wood, D. N., Finkelstein, A., Hughes, J. F., Thayer, C. E., and Salesin, D. H. Multiperspective panoramas for cel animation. Computer Graphics (SIGGRAPH 97), 243-250.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Non-Photorealistic Virtual Environments* Allison W. Klein Wilmot Li Michael M. Kazhdan Wagner T. Corr 
ea Adam Finkelstein Thomas A. Funkhouser Princeton University Abstract We describe a system for non-photorealistic 
rendering (NPR) of virtual environments. In real time, it synthesizes imagery of architectural interiors 
using stroke-based textures. We address the four main challenges of such a system interactivity, visual 
detail, controlled stroke size, and frame-to-frame coherence through image based rendering (IBR) methods. 
In a preprocessing stage, we capture photos of a real or synthetic environment, map the photos to a coarse 
model of the environment, and run a series of NPR .lters to generate textures. At runtime, the system 
re-renders the NPR textures over the geometry of the coarse model, and it adds dark lines that emphasize 
creases and silhouettes. We provide a method for constructing non-photorealistic textures from photographs 
that largely avoids seams in the resulting imagery. We also offer a new construction, art-maps, to control 
stroke size across the images. Finally, we show a working system that provides an immersive experience 
rendered in a variety of NPR styles. Keywords: Non-photorealistic rendering, image-based rendering, texture 
mapping, interactive virtual environments. 1 Introduction Virtual environments allow us to explore an 
ancient historical site, visit a new home with a real estate agent, or .y through the twisting corridors 
of a space station in pursuit of alien prey. They simulate the visual experience of immersion in a 3D 
environment by rendering images of a computer model as seen from an observer viewpoint moving under interactive 
control by the user. If the rendered images are visually compelling, and they are refreshed quickly enough, 
the user feels a sense of presence in a virtual world, enabling applications in education, computer-aided 
design, electronic commerce, and entertainment. While research in virtual environments has traditionally 
striven for photorealism, for many applications there are advantages to non-photorealistic rendering 
(NPR). Artistic expression can often convey a speci.c mood (e.g. cheerful or dreary) dif.cult to imbue 
in a synthetic, photorealistic scene. Furthermore, through abstraction and careful elision of detail, 
NPR imagery can focus the viewer s attention on important information while downplaying extraneous or 
unimportant features. An NPR scene can also suggest additional semantic information, such as a quality 
of un.nishedness that *http://www.cs.princeton.edu/gfx/proj/NPRVE Permission to make digital or hard 
copies of part or all of this work or personal or classroom use is granted without fee provided that 
copies are not made or distributed for profit or commercial advantage and that copies bear this notice 
and the full citation on the first page. To copy otherwise, to republish, to post on servers, or to redistribute 
to lists, requires prior specific permission and/or a fee. SIGGRAPH 2000, New Orleans, LA USA &#38;#169; 
ACM 2000 1-58113-208-5/00/07 ...$5.00 Figure 1: A non-photorealistic virtual environment. may be desirable 
when, for example, an architect shows a client a partially-completed design. Finally, an NPR look is 
often more engaging than the prototypical stark, pristine computer graphics rendering. The goal of our 
work is to develop a system for real-time NPR virtual environments (Figure 1). The challenges for such 
a system are four-fold: interactivity, visual detail, controlled stroke size, and frame-to-frame coherence. 
First, virtual environments demand interactive frame rates, whereas NPR methods typically require seconds 
or minutes to generate a single frame. Second, visual details and complex lighting effects (e.g. indirect 
illumination and shadows) provide helpful cues for comprehension of virtual environments, and yet construction 
of detailed geometric models and simulation of global illumination present challenges for a large virtual 
environment. Third, NPR strokes must be rendered within an appropriate range of sizes; strokes that are 
too small are invisible, while strokes that are too large appear unnatural. Finally, frame-to­frame coherence 
among strokes is crucial for an interactive NPR system to avoid a noisy, .ickery effect in the imagery. 
We address these challenges with image-based rendering (IBR). In general, IBR yields visually complex 
scenery and ef.cient rendering rates by employing photographs or pre-rendered images of the scene to 
provide visual detail. Not surprisingly, by using a hybrid NPR/IBR approach we are able to reap the bene.ts 
of both technologies: an aesthetic rendering of the scene, and visual complexity from a simple model. 
More subtly, each technology addresses the major drawbacks of the other. IBR allows us to render artistic 
imagery with complex lighting effects and geometric detail at interactive frame rates while maintaining 
frame-to-frame coherence. On the .ipside, non-photorealistic rendering appeases many of the artifacts 
due to under-sampling in IBR, both by visually masking them and by reducing the viewer s expectation 
of realism. At a high level, our system proceeds in three steps as shown in Figure 2. First, during off-line 
preprocessing, we construct an IBR model of a scene from a set of photographs or rendered images. Second, 
during another preprocessing step, we .lter samples of the IBR model to give them a non-photorealistic 
look. The result is a non-photorealistic image-based representation (NPIBR) for use in interactive walkthroughs. 
Finally, during subsequent on-line sessions, the NPIBR model is resampled for novel viewpoints to reconstruct 
NPR images for display. IBR NPIBR New Figure 2: Overview of our approach. This approach addresses many 
of the challenges in rendering NPR images of virtual environments in real-time. First, by ex­ecuting 
the most expensive computations during off-line prepro­cessing, our system achieves interactive frame 
rates at run-time. Second, by capturing complex lighting effects and geometric de­tail in photographic 
images, our system produces images with vi­sual richness not attainable by previous NPR rendering systems. 
Third, with appropriate representation, pre.ltering, and resampling methods, IBR allows us to control 
NPR stroke size in the projected imagery. Fourth, by utilizing the same NPR imagery for many sim­ilar 
camera viewpoints rather than creating new sets of strokes for each view, our system acquires frame-to-frame 
coherence. More­over, by abstracting NPR processing into a .ltering operation on an image-based representation, 
our architecture supports a number of NPR styles within a common framework. This feature gives us aesthetic 
.exibility, as the same IBR model can be used to produce interactive walkthroughs in different NPR styles. 
In this paper, we investigate issues in implementing this hybrid NPR/IBR approach for interactive NPR 
walkthroughs. The speci.c technical contributions of our work are: (1) a method for construct­ing non-photorealistic 
textures from photographs that largely avoids seams in images rendered from arbitrary viewpoints, and 
(2) a mul­tiresolution representation for non-photorealistic textures (called art-maps) that works with 
conventional mip-mapping hardware to render images with controlled stroke size. These methods are incor­porated 
into a working prototype system that supports interactive walkthroughs of visually complex virtual environments 
rendered in many stroke-based NPR styles. The remainder of this paper is organized as follows. In Section 
2 we review background information and related work. Sections 3-5 address the main issues in constructing, 
.ltering, and resampling a hybrid NPR/IBR representation. Section 6 presents results of experiments with 
our working prototype system, while Section 7 contains a brief conclusion and discussion of areas for 
future work.  2 Related Work The traditional strategy for immersive virtual environments is to render 
detailed sets of 3D polygons with appropriate lighting effects as the camera moves through the model 
[21]. With this approach, the primary challenge is constructing a digital representation for a complex, 
visually rich, real-world environment. Despite recent advances in interactive modeling tools, laser-based 
range-.nders, computer vision techniques, and global illumination algorithms, it remains extremely dif.cult 
to construct compelling models with detailed 3D geometry, accurate material re.ectance properties, and 
realistic global illumination effects. Even with tools to create an attractive, credible geometric model, 
it must still be rendered at interactive frame rates, limiting the number of polygons and shading algorithms 
that can be used. With such constraints, the resulting imagery usually looks very plastic and polygonal, 
despite setting user expectations for photorealism. In contrast, image-based modeling and rendering methods 
rep­resent a virtual environment by its radiance distribution without re­lying upon a model of geometry, 
lighting, and re.ectance proper­ties [5]. An IBR system usually takes images (photographs) of a static 
scene as input and constructs a sample-based representation of the plenoptic function, which can be resampled 
to render photo­realistic images for novel viewpoints. The important advantages of this approach are 
that photorealistic images can be generated with­out constructing a detailed 3D model or simulating global 
illumi­nation, and the rendering time for novel images is independent of a scene s geometric complexity. 
The primary dif.culty is storing and resampling a high-resolution representation of the plenoptic func­tion 
for a complex virtual environment [23]. If the radiance dis­tribution is under-sampled, images generated 
during a walkthrough contain noticeable aliasing or blurring artifacts, which are disturb­ing when the 
user expects photorealism. In recent years, a few researchers have turned their attention away from photorealism 
and towards developing non-photorealistic rendering techniques in a variety of styles and simulated media, 
such as impressionist painting [13, 15, 20, 24], pen and ink [28, 33], technical illustration [11, 27], 
ornamentation [34], engraving [25, 26], watercolor [4], and the style of Dr. Seuss [18]. Much of this 
work has focused on creating still images either from photographs, from computer-rendered reference images, 
or directly from 3D models, with varying degrees of user-direction. One of our goals is to make our system 
work in conjunction with any of these technologies (particularly those that are more automated) to yield 
virtual environments in many different styles. Several stroke-based NPR systems have explored time-changing 
imagery, confronting the challenge of frame-to-frame coherence with varying success. Winkenbach et al. 
[32] and later Cur­tis et al. [4] observed that applying NPR techniques designed for still images to 
time-changing sequences yields .ickery, jittery, noisy animations because strokes appear and disappear 
too quickly. Meier [24] adapted Haeberli s paint by numbers scheme [13] in such a way that paint strokes 
track features in a 3D model to pro­vide frame-to-frame coherence in painterly animation. Litwinow­icz 
[20] achieved a similar effect on video sequences using op­tical .ow methods to af.x paint strokes to 
objects in the scene. Markosian [22] found that silhouettes on rotating 3D objects change slowly enough 
to give frame-to-frame coherence for strokes drawn on the silhouette edges. We exploit this property 
when drawing lines on creases and silhouettes at run-time. Kowalski et al. [18] extends these methods 
by attaching non-photorealistic graftals to the 3D geometry of a scene, while seeking to enforce coherence 
among the graftals between frames. The bulk of the coherence in our system comes from reprojection of 
non-photorealistic imagery, so the strokes drawn for neighboring frames are generally slowly­changing. 
Several other researchers, for example Horry et al. [17], Wood et al. [35], and Buck et al. [1], have 
built hybrid NPR/IBR systems where hand-drawn art is re-rendered for different views. In this spirit 
our system could also incorporate hand-drawn art, al­though the drawing task might be arduous as a single 
scene involves many reference images. In this paper, we present a system for real-time, NPR virtual environments. 
Rather than attempting to answer the question how would van Gogh or Chagall paint a movie? we propose 
solutions to some technical issues facing an artist wishing to use NPR styles in a virtual environment 
system. Two visual metaphors represent the extremes in a spectrum of aesthetics one could choose for 
an artistic immersive experience. On one extreme, we could imagine that an artist painted over the walls 
of the model. In this case, the visual effect is that as the user navigates the environment the detailed 
stroke work is more or less apparent depending on her distance from the various surfaces she can see. 
In the other extreme, we could imagine that as the user navigates the environment in real-time, a photograph 
of what is seen is captured, and an artist instantaneously paints a picture based on the photograph. 
In this case, the visual effect suffers from either .ickering strokes (lack of frame-to-frame coherence) 
or the shower door effect (the illusion that the paintings are somehow embedded in a sheet of glass in 
front of the viewer). Our goal is to .nd a compromise between these two visual metaphors: we would like 
the stroke coherence to be on the surfaces of the scene rather than in the image plane, but we would 
like the stroke size to be roughly what would have been selected for the image plane rather than what 
would have been chosen for the walls. The dif.cult challenge is to achieve this goal while rendering 
images at interactive rates. We investigate a hybrid NPR/IBR approach. Broadly speaking, the two main 
issues we address are: 1) constructing an IBR representation suitable for NPR imagery, and 2) developing 
a IBR pre.ltering method to enable rendering of novel NPR images with controllable stroke-size and frame-to-frame 
coherence in a real­time walkthrough system. These issues are the topics of the following two sections. 
 3 Image-Based Representation The .rst issue in implementing a system based on our hybrid NPR/IBR approach 
is to choose an image-based representation suitable for storing and resampling non-photorealistic imagery. 
Of course, numerous IBR representations have been described in the literature (see [5] for a survey); 
and, in principle, any of them could store NPR image samples of a virtual environment. However, not all 
IBR representations are equally well-suited for NPR walkthroughs. Speci.cally, an IBR method for interactive 
walkthroughs should have the following properties: A1) Arbitrary viewpoints: The image reconstruction 
method should be able to generate images for arbitrary novel view­points within the interior of the virtual 
environment. This property implies a 5D representation of the plenoptic function capable of resolving 
inter-object occlusions. It also implies a pre.ltered multiresolution representation from which novel 
views can be rendered ef.ciently from any distance without aliasing. A2) Practical storage: The image-based 
representation should be small enough to .t within the capacity of common long­term storage devices (e.g., 
CD-ROMs), and the working set required for rendering any novel view should be small enough to .t within 
the memory of desktop computers. This property suggests methods for compressing image samples and managing 
multi-level storage hierarchies in real-time. A3) Ef.cient rendering: The rendering algorithm should 
be very fast so that high-quality images can be generated at interactive frame rates. This property suggests 
a hardware implementation for resampling. Additionally, the following properties are important for IBR 
representations used to store non-photorealistic imagery: B1) Homeomorphic reprojection: The mapping 
of pixel sam­ples onto any image plane should be homeomorphic so that strokes and textures in NPR imagery 
remain intact during im­age reconstruction for novel views. This property ensures that our method can 
work with a wide range of NPR .lters. B2) Predictable reprojection: The reprojected positions of pixel 
samples should be predictable so that the sizes and shapes of strokes in reconstructed NPR images can 
be controlled. This property allows the system to match the sizes and shapes of strokes in NPR images 
to the ones intended by the scene designer. B3) Filter Flexibility: Pixel samples should be stored in 
a form that makes NPR .lters simple and easy to implement so that support for multiple NPR styles is 
practical. This property provides scene designers with the aesthetic .exibility of ex­perimenting with 
a variety of NPR styles for a single scene. We have considered several IBR representations. QuickTime 
VR [2] is perhaps the most common commercial form of IBR, and its cylindrical panoramic images could 
easily be used to create NPR imagery with our approach. For instance, each panoramic image could be run 
through an off-the-shelf NPR image processing .lter, and the results could be input to a QuickTime VR 
run-time viewer to produce an immersive NPR experience. While this method may be appropriate for some 
applications, it cannot be used for smooth, interactive walkthroughs, since QuickTime VR supports only 
a discrete set of viewpoints, and it would require a lot of storage to represent the interior of a complex 
environment, thereby violating properties A1 and A2 above. Other IBR methods allow greater freedom of 
motion. However, in doing so, they usually rely upon more complicated resampling methods, which makes 
reconstruction of NPR strokes dif.cult for arbitrary viewpoints. As a simple example, consider adding 
cross­hatch strokes to an image with color and depth values for each pixel. As novel images are reconstructed 
from this representation, individual pixels with different depths get reprojected differently according 
to their .ow .elds; and, consequently, the cross-hatch stroke pattern present in the original depth image 
disintegrates for most views. This problem is due to a violation of property B1, which is typical of 
most view-dependent IBR representations, including cylindrical panorama with depth [23], layered depth 
images [29], light .elds [19], Lumigraphs [12], interpolated views [3], etc. Our approach, based on textures, 
relies upon a hybrid geometry­and image-based representation. Radiance samples acquired from photographs 
are used to create textures describing the visual com­plexity of the scene, while a coarse 3D polygonal 
model is used to reason about the coverage, resolution, discontinuities, coherence, and projections of 
radiance samples for any given view. This ap­proach satis.es all of the properties listed above. In particular, 
sur­face textures are a very compact form for the 5D plenoptic function, as inter-object occlusions are 
implicit in the hidden surface rela­tionships between polygons of the coarse 3D model ( A1 ). Also, storage 
and rendering can take advantage of the plethora of previ­ous work in texture mapping [14], including 
multi-scale pre.ltering methods ( A1 ), texture compression and paging algorithms ( A2 ), and texture 
rendering hardware implementations ( A3 ), which are available in most commodity PC graphics accelerators 
today. Textures are especially well-suited for NPR imagery, as the map­ping from the texture sample space 
to the view plane is simply a 2D projective warp, which is both homeomorphic ( B1 ) and pre­dictable 
( B2 ). As a consequence, our system can control the sizes and shapes of rendered strokes in reconstructed 
images by pre­.ltering NPR textures during a preprocessing step to compensate for the predictable distortions 
introduced by the projective mapping (the details of this method appear in the following section). Finally, 
we note that textures provide a simple and convenient representa­tion for NPR .ltering, as any combination 
of numerous commonly available image processing tools can be used to add NPR effects to texture imagery 
( B3 ). For instance, most of the NPR styles shown in this paper were created with .lters in Adobe Photoshop. 
 ................ ................ ................ ................ ................ ................ 
................ ................ ................ ................ ................ ................ 
................ ................ ................ ................ ................ ................ 
................ ................ ................ ................ ................ ................ 
  (a) Build coarse 3D model (b) Capture photographs (c) Map photographs (d) Compute coverage (e) Group 
texture (f) Generate art-maps (g) Run time walkthrough (h) Draw lines Figure 3: Our process. Steps (a) 
through (f) happen as pre-processing, enabling interactive frame rates at run-time in steps (g) and (h). 
Our speci.c method for constructing textures from images pro­ceeds as shown in Figure 3a-d. First, we 
construct a coarsely­detailed polygonal model using an interactive modeling tool (Fig­ure 3a). To ensure 
proper visibility calculations in later stages, the model should have the property that occlusion relationships 
be­tween polygons in the model match the occlusion relationships be­tween the corresponding objects in 
the environment. Second, we capture images of the environment with a real or synthetic camera and calibrate 
them using Tsai s method [30] (Figure 3b). Third, we map the images onto the surfaces of the polygonal 
model using a beam tracing method [9] (Figure 3c). The net result is a coverage map in which each polygon 
is partitioned into a set of convex faces corresponding to regions covered by different combinations 
of cap­tured images (Figure 3d). Fourth, we select a representative image for each face to form a view-independent 
texture map, primarily favoring normal views over oblique views, and secondarily favor­ing images taken 
from cameras closer to the surface. Finally, we .ll faces not covered by any image with a texture hole-.lling 
algo­rithm similar to the one described by Efros and Leung [8]. Note that view-dependent texture maps 
could be supported with our method by blending images from cameras at multiple discrete viewpoints (as 
in [6, 7]). However, we observe that NPR .ltering removes most view-dependent visual cues, and blending 
reduces texture clarity, and thus we choose view-independence over blending in our cur­rent system. 
 4 Non-Photorealistic Filtering The second step in our process is to apply NPR .lters to texture imagery. 
Sections 4.1 and 4.2 address the two major concerns relating to NPR .ltering: avoiding visible seams 
and controlling the stroke size in the rendered images. 4.1 Seams Our goal is to enable processing of 
IBR textures with many different NPR .lters. Some NPR .lters might add artistic strokes (e.g., pen and 
ink ), others might blur or warp the imagery (e.g., ink blot ), and still others might change the average 
luminance (e.g., impressionist ) based on the pixels in the input texture. In all these cases, seams 
may appear in novel images anywhere two textures processed by an NPR .lter independently are reprojected 
onto adjacent areas of the novel image plane. As a consequence, we must be careful about how to apply 
NPR .lters so as to minimize noticeable resampling artifacts in rendered images. The problem is best 
illustrated with an example. The simplest way to process textures would be to apply an NPR .lter to each 
of the captured photographic images, and then map the resulting NPR images onto the surfaces of the 3D 
model as projective textures (as in [6, 7]). Unfortunately, this photo-based approach causes noticeable 
artifacts in reconstructed NPR images. For instance, Figure 4a shows a sample image reconstructed from 
photographic textures processed with a ink blot .lter in Photoshop. Since each photographic texture is 
.ltered independently and undergoes a different projective warp onto the image plane, there are noticeable 
seams along boundaries of faces where the average luminance varies ( A ) and where the sizes and shapes 
of NPR strokes change abruptly ( B ). Also, since this particular NPR .lter resamples the photographic 
images with a large convolution kernel, colors from occluding surfaces bleed across silhouette edges 
and map onto occluded surfaces, leaving streaks along occlusion boundaries in the reconstructed image 
( C ). We can avoid many of these artifacts by executing the NPR .lter on textures constructed for each 
surface, rather than for each photographic image. This approach ensures that most neighboring pixels 
in reprojected images are .ltered at the same scale, and it avoids spreading colors from one surface 
to another across silhouette edges. Ideally, we would avoid all seams by creating a single texture image 
with a homeomorphic map to the image plane for every potential viewpoint. Unfortunately, this ideal approach 
is not generally possible, as it would require unfolding the surfaces of 3D model onto a 2D plane without 
overlaps. Instead, our approach is to construct a single texture image for each connected set of coplanar 
faces (Figure 3e), and then we execute the NPR .lter on the whole texture as one image (Figure 4b). This 
method moves all potential seams due to NPR .ltering to the polyhedral edges of the 3D model, a place 
where seams are less objectionable and can be masked by lines drawn over the textured imagery.  a) NPR 
photo textures b) NPR surface textures Figure 5: Scene rendered with art-maps. Figure 4: Applying NPR 
.lters to surface textures avoids seams and warped The stroke size remains roughly constant strokes in 
reconstructed images. across the image.  4.2 Art Maps This section addresses the problem of placing 
strokes into the textures in such a way that we have control over stroke size in the .nal image. Our 
challenge is a fundamental tension between frame­to-frame coherence and stroke size appropriate for the 
image plane. As the user moves toward a surface, the strokes on that surface must change in order to 
maintain an appropriate size in the image plane. Unfortunately, this means that we must either slowly 
blend from one set of strokes to another set, or suffer from a pop when they all change at once. Preferring 
the former effect, our compromise is to choose slowly-changing strokes, with some amount of blurring 
as they change, and to allow stroke size to vary somewhat with a range of sizes nearly appropriate for 
the viewing plane. Our solution relies on the observation that the stroke size prob­lem is analogous 
to choice of .lter for projected imagery in pho­torealistic environments using conventional texture mapping. 
As the user navigates a photorealistic environment, the goal of texture mapping hardware is to select 
for every pixel p a .lter f for the texture such that the size of f varies with the size of the texture 
space pre-image of p. Likewise, our goal is to place each stroke s in the texture such that as the user 
navigates the environment, the relative sizes of s and f in texture space stay constant. Thus, our strategy 
for management of stroke size is to leverage the great deal of work on pre-.ltering imagery for texture 
mapping, most notably mip-maps [31]). We use a construction that we call art-maps. The key idea is to 
apply strokes to each level of the mip-map, knowing that it is suitable for projection to the screen 
at a particular size. Figure 6 shows an example. To create this mip-map hierarchy, we simply .lter the 
photorealistic images as in normal mip-mapping, but then apply an NPR .lter to each level independently. 
The strokes at each level of the mip-map hierarchy vary in size in powers of two relative to the whole 
image, just as pre-.ltered mip-map levels vary the .lter kernel size. Thus, when conventional texture 
mapping hardware selects a level of the mip-map hierarchy from which to sample a pixel, it will automatically 
choose a pixel from a set of strokes of the appropriate size. Furthermore, as it blends between levels 
of the mip-map hierarchy, it will likewise blend between strokes of appropriate size. So the effect is 
that strokes remain af.xed to the surfaces in the scene, but as the user navigates through the environment, 
the strokes have roughly constant size in the image plane, as shown for example in Figure 5. Note that 
at locations marked D and E the stroke size is roughly the same. (In contrast, without art-maps, the 
strokes in these locations varies with the distance between the surface and the camera, as can be seen 
in Figure 4.) As the user moves toward a wall, the strokes shown for that wall will slowly blend from 
the strokes in one mip-map level to the next to maintain roughly constant image-space size. As the viewer 
moves, there is frame­to-frame coherence in the mip-map level chosen for the wall, and therefore there 
is visual coherence in the strokes. We suffer some amount of blending of strokes, because the mip-map 
level is generally non-integer; but we prefer this to either popping or lack of control over stroke size. 
The bene.ts of art-maps are that they are very simple to implement, and that they permit interactivity 
by relegating expensive NPR .ltering to a preprocess and by exploiting texture mapping hardware for sampling 
at runtime. A known problem for conventional mip-maps is that for very oblique polygons the mip-map is 
forced to choose between aliasing and blurring for one or both of the principle directions [14]. This 
problem is due to a round .lter kernel in image space projected to a very oblong shape in texture space, 
which forces the use of a kernel that is either correctly sized in its long direction (giving aliasing 
in the short direction) or correctly sized in its short direction (giving blurring in the long direction). 
This .lter problem manifests itself as stretched strokes when art-maps are applied (Figure 7a). A number 
of solutions to this problem have been proposed [14] art­maps will work with any of them that stores 
multiple pre.ltered  (a) art-maps only (b) with rip-maps (c) varying strokes Figure 7: Art maps using 
generalizations of mip-maps. versions of a texture (e.g., for different perspective warps). We have experimented 
with a generalization of mip-maps, called rip­maps [16]. As shown in Figure 8, rip-maps contain a cascading 
series of pre-.ltered, off-angle images of the texture. An obliquely­projected texture may select one 
of the off-axis images from the rip-map; in the case of rip-maps with art-maps, the stroke shape will 
be corrected, as shown in Figure 7b. Our prototype renders this scene by recursively dividing textured 
polygons, selecting among rip-map textures in the subdivided regions. This method allows interactive 
control over stroke sizes in different areas of the image plane, as illustrated in Figure 7c; in this 
example, we use small strokes in the upper part of the image, and smoothly vary stroke size down to large 
strokes at the bottom of the image. Unfortunately, our current software implementation of rip-mapping 
is too slow for real-time rendering of complex scenes, and thus we use art-maps with conventional mip-mapping 
for our interactive walkthrough system. We note that it might still be possible to control the sizes 
of rendered strokes on a per-surface basis using various texture mapping parameters (e.g., LOD bias) 
that guide the selection of mip-map levels.  5 Interactive Walkthrough System During the run-time phase, 
we simulate the experience of moving through a non-photorealistic environment by drawing surfaces of 
the coarse 3D model rendered with their art-map textures as the user moves a simulated viewpoint interactively. 
Our run-time system loads all art-map levels for all surfaces into texture memory at startup. Then, for 
every novel viewpoint, it draws surfaces of the 3D model with standard texture mip­mapping hardware using 
the pre-loaded art-maps (as described in Section 4). The rendering process is fast, and it produces images 
with relatively high frame-to-frame coherence and nearly constant size NPR strokes, as blending between 
art-map levels is performed in texture mapping hardware on a per-pixel basis according to estimated projected 
areas. To facilitate management of texture memory, we break up large textures into tiles before loading 
them into texture memory, and we execute view frustum culling and occlusion culling algorithms to compute 
a potentially visible set of surface tiles to render for every novel viewpoint [10]. These methods help 
keep the working set of texture data relatively small and coherent from frame-to-frame, and thus we can 
rely upon standard OpenGL methods to manage texture swapping when the total texture size exceeds texture 
memory. Figure 8: Art-maps can be applied to other, more generalized mip­mapping techniques such as 
RIP-maps. Our hybrid geometry-and image-based approach allows us not only to render NPR textured surfaces, 
but also to augment the re­sulting images with additional visual information. For example, we sometimes 
apply photorealistic textures to an object in order to differentiate that object from others in the scene. 
We also use run-time geometric rendering to highlight interesting features of the environment. For instance, 
we draw wavy lines over silhouette edges and creases at the intersections of non-coplanar polygons, which 
helps mask objectionable artifacts due to seams and unnatu­rally hard edges at polygon boundaries. In 
our implementation, the lines are drawn as a 2D triangle strip following a sinusoidal back­bone along 
the 2D projection of each visible edge in the 3D model. Since the frequency of the sine function is based 
on screen space distances, all of the lines drawn have a consistent waviness, re­gardless of their orientation 
relative to the viewer. The lines help to clarify the geometry of the environment, especially when the 
NPR .lter used is very noisy or produces low contrast textures. See Fig­ure 3h for an example.  6 Experimental 
Results We have implemented the methods described in the preceding sections in C++ on Silicon Graphics/Irix 
and PC Windows/NT computers and incorporated them into an interactive system for walkthroughs of non-photorealistic 
virtual environments. To test the viability of our methods, we have performed exper­iments with several 
virtual environments rendered with different NPR styles. Tables 1 and 2 show statistics logged during 
our pro­cess for three of these environments, two of which are synthetic ( Museum and Gallery ) and one 
of which is a real building cap­tured with photographs ( Building ). All times were measured on a Silicon 
Graphics Onyx2 with a 195MHz R10000 CPU and In.nite-Reality graphics. Examining the timing results in 
Table 2, we see that the pre­processing steps of our method can require several hours in all. Yet, we 
reap great bene.t from this off-line computation. The re­ Model Number of Surface area Number Number 
Number of Total MBs Total MBs name polygons (inches2) of photos of faces textures of textures of art-maps 
Gallery 192 2,574,400 46 414 73 82 109 Museum 76 421,520 93 282 42 104 138 Building 201 931,681 18 815 
114 118 157 Table 1: Quantitative descriptions of test environments and preprocessing results. Model 
Preprocessing Run-time Capture Calibrate Map Create Hole Create Run Total Draw Draw Total name photos 
photos photos textures .lling art-maps NPR .lter preprocessing images lines per frame Gallery 1m 40s 
 0.4s 3m 30s 2h 02m 10m 30m 2h 47m 0.017s 0.025s 0.042s Museum 1m 52s  0.8s 2m 53s 3h 34m 8m 40m 4h 
26m 0.017s 0.014s 0.031s Building 2h 2h 5.8s 4m 22s 3h 40m 14m 50m 8h 48m 0.056s 0.037s 0.093s Table 
2: Timing results for each stage of our process. sult is visually compelling imagery rendered at interactive 
frame rates with high frame-to-frame coherence during run-time. Aver­age frame refresh times measured 
during interactive walkthroughs of each model are shown in the right-most column of Table 2. The corresponding 
frame rates range from 11 to 32 frames per second, which are adequate to provide a convincing illusion 
of presence as the user moves interactively through a non-photorealistic environ­ment. Another result 
is the demonstration of our system s .exibility in supporting interactive walkthroughs in many NPR styles. 
Fig­ures 9a-c show screen shots of the walkthrough program with the Museum environment after processing 
with different NPR .lters. Creating each new set of NPR textures took around 40 minutes of preprocessing 
time, as only the last step of the preprocess ( run NPR .lter ) had to be re-done for each one. Then, 
the run-time program could immediately provide interactive walkthroughs in the new style. Figures 9d-f 
show images of the Building environment rendered in a watercolor style from different viewpoints. Each 
im­age took less than 1/10th of a second to generate. Notice how the size of the strokes in all the images 
remains relatively constant, even for surfaces at different distances from the viewer. The primary limitation 
on the complexity of virtual environ­ments and the resolution of imagery rendered with our system is 
the capacity of graphics hardware texture memory. In order to maintain interactive frame rates, all texture 
data for every rendered image must .t into the texture cache on the graphics accelerator (64MB in our 
tests). As a result, the number of surfaces in the virtual environ­ment and the resolution of captured 
textures must be chosen judi­ciously. So far, we have generally constructed group textures with each 
texel corresponding to a 2 by 2 inch region of a surface, and we decompose group textures into 512 by 
512 pixel tiles that can be loaded and removed in the texture cache independently. With these resolutions, 
our test environments require between 109MB and 157MB of texture data with art-maps (see the right-most 
col­umn of Table 1), of which far less than 64MB is required to ren­der an image for any single novel 
viewpoint (due to view frustum culling and occlusion culling). In our experiments, we .nd that the standard 
OpenGL implementation of texture memory manage­ment is able to swap these textures fast enough for interactive 
walk­throughs, at least on a Silicon Graphics Onyx2 with In.niteReality graphics. While the frame rate 
is not perfectly constant (there are occasionally hiccups due to texture cache faults), the frame rate 
is usually between 10 and 30 frames per second yielding an inter­active experience for the user. More 
sophisticated texture manage­ment and compression methods could be used to address this issue in future 
work.  7 Conclusion This paper describes a system for real-time walkthroughs of non­photorealistic virtual 
environments. It tackles the four main chal­lenges of such a system interactivity, visual detail, controlled 
stroke size, and frame-to-frame coherence through image-based rendering of non-photorealistic imagery. 
The key idea is that an image-based representation can be constructed off-line through a sequence of 
image capture and .ltering steps that enable ef.cient reconstruction of visually detailed images from 
arbitrary view­points in any non-photorealistic style. The technical contributions of this work include 
a method for constructing NPR textures that avoids seams in novel images and a multiscale texture representa­tion 
(art-maps) that provides control over the size of strokes during interactive rendering. This work suggests 
a number of areas for future investigation: Augmenting the scene with geometry-based elements. Real-time 
NPR rendering of simple geometric objects in the scene perhaps architectural accents such as a plant 
or a chair rendered in the NPR styles of Gooch et al. [11] or Kowalski et al. [18] would enhance the 
sense of immersion while not greatly slowing our system. View-dependent rendering. We have observed that 
many view­dependent geometric and lighting effects are visually masked by non-photorealistic rendering 
(see Section 3). Nonetheless, view­dependent texture mapping (e.g. [6, 7]) offers an opportunity to capture 
these effects for even better .delity to the environment. Better stroke coherence. As mentioned in Section 
4.2, runtime blending between neighboring levels of the mip-map hierarchy causes visual blending between 
strokes in the art-maps. It may be possible to achieve better coherence between neighboring levels of 
the mip-maps, most likely by designing customized NPR .lters that deliberately assign strokes in multiple 
levels of the art-maps at once. The desired visual effect might be that strokes grow and eventually split 
apart, rather than fading in, as the user approaches a surface.  Acknowledgements The authors would 
like to thank several people for their assistance with this project: Lee Markosian taught us how to draw 
wavy lines quickly using triangle strips; Reg Wilson provided the implementa­tion of Tsai s camera calibration 
algorithm; and John Hughes pro­vided helpful discussion. This work was supported in part by Alfred P. 
Sloan Foundation Fellowships awarded to Adam Finkelstein and Thomas Funkhouser, an NSF CAREER grant for 
Adam Finkelstein, and generous gifts from Microsoft Corporation.  (a) Museum, drybrush (b) Museum, pastel 
(c) Museum, van Gogh  References [1] Buck, I., Finkelstein, A., Jacobs, C., Klein, A., Salesin, D. H., 
Seims, J., Szeliski, R., and Toyama, K. Performance-driven hand-drawn animation. Proceedings of NPAR 
2000 (June 2000). [2] Chen, S. E. Quicktime VR -An image-based approach to virtual en­vironment navigation. 
Computer Graphics (SIGGRAPH 95), 29 38. [3] Chen, S. E., and Williams, L. View interpolation for image 
synthesis. Computer Graphics (SIGGRAPH 93), 279 288. [4] Curtis, C. J., Anderson, S. E., Seims, J. E., 
Fleischer, K. W., and Salesin, D. H. Computer-generated watercolor. Computer Graphics (SIGGRAPH 97), 
421 430. [5] Debevec, P. Image-based modeling, rendering, and lighting. Course #35, SIGGRAPH 2000 Course 
Notes (July 2000). [6] Debevec, P. E., Taylor, C. J., and Malik, J. Modeling and rendering architecture 
from photographs: A hybrid geometry-and image-based approach. Computer Graphics (SIGGRAPH 96), 11 20. 
[7] Debevec, P. E., Yu, Y., and Borshukov, G. D. Ef.cient view­dependent image-based rendering with projective 
texture-mapping. Eurographics Rendering Workshop (June 1998), 105 116. [8] Efros, A. A., and Leung, T. 
K. Texture synthesis by non-parametric sampling. IEEE International Conference on Computer Vision (1999). 
[9] Funkhouser, T. A. A visibility algorithm for hybrid geometry-and image-based modeling and rendering. 
Computers and Graphics, Special Issue on Visibility (1999). [10] Funkhouser, T. A., Teller, S. J., Sequin, 
C. H., and Khorramabadi, D. The UC Berkeley system for interactive visualization of large architectural 
models. Presence 5, 1 (January 1996). [11] Gooch, A., Gooch, B., Shirley, P., and Cohen, E. A non-photorealistic 
lighting model for automatic technical illustration. Computer Graph­ics (SIGGRAPH 98), 447 452. [12] 
Gortler, S. J., Grzeszczuk, R., Szeliski, R., and Cohen, M. F. The lumigraph. Computer Graphics (SIGGRAPH 
96), 43 54. [13] Haeberli, P. E. Paint by numbers: Abstract image representations. Computer Graphics 
(SIGGRAPH 90), 207 214. [14] Heckbert, P. Survey of texture mapping. IEEE Computer Graphics and Applications 
(Nov. 1986). [15] Hertzmann, A. Painterly rendering with curved brush strokes of multiple sizes. Computer 
Graphics (SIGGRAPH 98), 453 460. [16] Hewlett Packard. HP PEX Texture Mapping, www.hp.com/mhm/WhitePapers/PEXtureMapping/PEXtureMapping.html. 
[17] Horry, Y., ichi Anjyo, K., and Arai, K. Tour into the picture: Using a spidery mesh interface to 
make animation from a single image. Computer Graphics (SIGGRAPH 97), 225 232. [18] Kowalski, M. A., Markosian, 
L., Northrup, J. D., Bourdev, L., Barzel, R., Holden, L. S., and Hughes, J. Art-based rendering of fur, 
grass, and trees. Computer Graphics (SIGGRAPH 99), 433 438. [19] Levoy, M., and Hanrahan, P. Light .eld 
rendering. Computer Graphics (SIGGRAPH 96), 31 42. [20] Litwinowicz, P. Processing images and video for 
an impressionist effect. Computer Graphics (SIGGRAPH 97), 407 414. [21] Manocha, D. Interactive walkthroughs 
of large geometric databases. Course #18, SIGGRAPH 2000 Course Notes (July 2000). [22] Markosian, L., 
Kowalski, M. A., Trychin, S. J., Bourdev, L. D., Gold­stein, D., and Hughes, J. F. Real-time nonphotorealistic 
rendering. Computer Graphics (SIGGRAPH 97), 415 420. [23] McMillan, L., and Bishop, G. Plenoptic modeling: 
An image-based rendering system. Computer Graphics (SIGGRAPH 95), 39 46. [24] Meier, B. J. Painterly 
rendering for animation. Computer Graphics (SIGGRAPH 96), 477 484. [25] Mizuno, S., Okada, M., and ichiro 
Toriwaki, J. Virtual sculpting and virtual woodcut printing. The Visual Computer 14, 2 (1998), 39 51. 
[26] Ostromoukhov, V. Digital facial engraving. Computer Graphics (SIGGRAPH 99), 417 424. [27] Saito, 
T., and Takahashi, T. NC machining with G-buffer method. Computer Graphics (SIGGRAPH 91), 207 216. [28] 
Salisbury, M. P., Wong, M. T., Hughes, J. F., and Salesin, D. H. Ori­entable textures for image-based 
pen-and-ink illustration. Computer Graphics (SIGGRAPH 97), 401 406. [29] Shade, J. W., Gortler, S. J., 
wei He, L., and Szeliski, R. Layered depth images. Computer Graphics (SIGGRAPH 98), 231 242. [30] Tsai, 
R. Y. A Versatile Camera Calibration Technique for High-Accuracy 3D Machine Vision Metrology Using Off-the-Shelf 
TV Cameras and Lenses. IEEE Journal of Robotics and Automation 3, 4 (Aug. 1987), 323 344. [31] Williams, 
L. Pyramidal parametrics. Computer Graphics (SIG-GRAPH 83), 1 11. [32] Winkenbach, G., and Salesin, D. 
H. Computer-generated pen-and-ink illustration. Computer Graphics (SIGGRAPH 94), 91 100. [33] Winkenbach, 
G., and Salesin, D. H. Rendering parametric surfaces in pen and ink. Computer Graphics (SIGGRAPH 96), 
469 476. [34] Wong, M. T., Zongker, D. E., and Salesin, D. H. Computer-generated .oral ornament. Computer 
Graphics (SIGGRAPH 98), 423 434. [35] Wood, D. N., Finkelstein, A., Hughes, J. F., Thayer, C. E., and 
Salesin, D. H. Multiperspective panoramas for cel animation. Computer Graphics (SIGGRAPH 97), 243 250. 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2000</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
</content>
</proceeding>
