<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>08/03/2009</start_date>
		<end_date>08/07/2009</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[New Orleans]]></city>
		<state>Louisiana</state>
		<country></country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>1667239</proc_id>
	<acronym>SIGGRAPH '09</acronym>
	<proc_desc>ACM SIGGRAPH 2009 Courses</proc_desc>
	<conference_number>2009</conference_number>
	<proc_class>conference</proc_class>
	<proc_title></proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>2009</copyright_year>
	<publication_date>08-03-2009</publication_date>
	<pages>4249</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
</proceeding_rec>
<content>
	<article_rec>
		<article_id>1667240</article_id>
		<sort_key>10</sort_key>
		<display_label>Article No.</display_label>
		<display_no>1</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Acquisition of optically complex objects and phenomena]]></title>
		<page_from>1</page_from>
		<page_to>158</page_to>
		<doi_number>10.1145/1667239.1667240</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1667240</url>
		<abstract>
			<par><![CDATA[<p>Standard range-scanning techniques work well for approximately Lambertian reflectors, but large classes of objects can currently not be scanned robustly. Specular and refractive objects pose challenges to range scanning because the surface cannot be observed directly. Translucent objects exhibit significant effects of global light transport, while volumetric phenomena like fire, smoke, and plasma effects do not have a proper surface. Recent research has led to the development of feasible and surprisingly accurate scanning approaches for these types of objects. This course introduces the major theoretical findings, practical setups, and experimental results for digitization of optically complex objects and phenomena.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1797100</person_id>
				<author_profile_id><![CDATA[81331495034]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ivo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ihrke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797101</person_id>
				<author_profile_id><![CDATA[81100644737]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wolfgang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Heidrich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SIGGRAPH Course Acquisition of Optically Complex Objects and Phenomena Abstract Standard range scanning 
techniques work well for approximately Lambertian re.ectors, but large classes of objects can currently 
not be scanned robustly. Specular and refractive objects pose challenges to range scanning because the 
surface cannot be observed directly. Translucent objects exhibit signi.cant ef­fects of global light 
transport, while volumetric phenomena like .re, smoke and plasma e.ects do not have a proper surface. 
Recent research has led to the development of feasible and surprisingly accurate scanning approaches 
for these types of objects. This course introduces the major theoretical .ndings, practical setups and 
experimental results for the digitization of optically complex objects and phenomena. These notes were 
frozen well before the conference and the presenters slides were not in the .nal state at that time. 
It is likely that there will be minor changes before the conference, An updated copy of these notes can 
be found at http://www.cs.ubc.ca/ ivoihrke/Tutorials/S09 ocop scanning course.pdf Biographies Ivo Ihrke 
is currently a postdoctoral research fellow at the University of British Columbia, supported by the Humboldt-Foundation, 
Germany. He re­ceived a MS degree in Scienti.c Computing from the Royal Institute of Tech­nology (KTH), 
Stockholm, Sweden in 2002 and a Ph.D. in computer science from Saarland University in 2007. His dissertation 
focused on reconstruction meth­ods for transparent, dynamic phenomena, like .re, smoke, and .uid .ows. 
His main research interest are the modeling of forward and inverse light transport processes and the 
development of e.cient numerical solvers for these problems. Wolfgang Heidrich Associate Professor Wolfgang 
Heidrich holds the Dolby Research Chair in Computer Science at the University of British Columbia. He 
received a PhD in Computer Science from the University of Erlangen in 1999, and then worked as a Research 
Associate in the Computer Graphics Group of 1 the Max-Planck-Institute for Computer Science in Saarbr¨ucken, 
Germany, be­fore joining UBC in 2000. Heidrich s research interests lie at the intersection of computer 
graphics, computer vision, imaging, and optics. In particular, he has worked on High Dynamic Range imaging 
and display, image-based mod­eling, measuring, and rendering, geometry acquisition, GPU-based rendering, 
and global illumination. Heidrich has written over 80 refereed publications on these subjects and has 
served on numerous program committees. He was the program co-chair for Graphics Hardware 2002, Graphics 
Interface 2004, and the Eurographics Symposium on Rendering, 2006. Syllabus Introduction and Motivation 
(2 min) Ivo Ihrke Overview of the course, presentation of covered topics, motivation Specular Object 
Acquisition (25 min) Ivo Ihrke Shape from Distortion - Measurement principle, imaging assumptions - 
Experimental setup - Ambiguity of Shape from Distortion approaches - Exemplary implementation &#38; results 
- Discussion  Shape from Direct Ray Measurements - Measurement principle, experimental setup - Exemplary 
implementation &#38; results - Discussion of reconstruction accuracy  Shape from Specularities - Measurement 
principle, imaging assumptions - Experimental setup - Examples, sub-surface scattering objects  Refractive 
Object Acquisition (40 min) Wolfgang Heidrich Single Refraction Events, Surface Triangulation - Stereoscopic 
shape from distortion - Water surface reconstruction - Experimental setup, results  Generalization to 
Multiple Refraction/Re.ection Events - Light path triangulation, concept of light paths - Introduction 
of measurement principle - Classi.cation scheme for reconstruction techniques - Tractable reconstruction 
problems  Scatter Trace Photography / Shape from Re.ectance - Measurement principle, constraints, error 
measure - Experimental setup - Results  Tomography - Refractive index matching for avoiding refraction 
- Tomographic reconstruction principle - Setup and calibration, practical considerations for refrac­ 
 2 tive index matching Acquisition of Volumetric Object Descriptions (35 min) Ivo Ihrke Fluorescent 
Immersion Range Scanning - Measurement principle, experimental setup - Calibration issues - Direct, all-surface 
scanning of glass objects - Accuracy and Discussion  Smoke Acquisition -Direct Measurement - Laser sheet 
scanning - Laser line scanning - Structured light, compressive sensing  Flame Acquisition -Sparse View 
Tomography - Measurement principle and setups - Basics of Algebraic Reconstruction Techniques (ART) - 
Sparse view: visual hull restricted tomography - Sparse view: density sheet decomposition  Astronomical 
Objects -Single View Tomography  Outlook and Discussion (3 min) Wolfgang Heidrich Outlook, future research 
challenges, identi.cation of important unsolved problems 3 Acquisition of Optically Complex Objects 
and Phenomena Ivo Ihrke Wolfgang Heidrich University of British Columbia   Real-World Object Acquisition 
/ Digitization  Tools e.g.:  laser range scanning  structured light  time of flight scanners  
 images courtesy of Marc Levoy  Real-World Object Acquisition of optically challenging objects  many 
objects can only be observed indirectly!  mirroring or refractive objects  passive techniques observe 
background  active light gets deflected to background   volumetric objects/phenomena do not have a 
surface  Based on reflected light . reflection must be visible and identifiable  Examples of failure 
cases photograph laser range scan  transparency subsurface scattering dark materials  Classification 
of Object Types   Object Types covered in this course   Specular objects  Object Types covered in 
this course  Specular objects  subsurface scattering objects   Object Types covered in this course 
 subsurface scattering objects  refractive objects   Object Types covered in this course  refractive 
objects  Volumetric phenomena   Object Types covered in this course Volumetric phenomena  Overview 
 Part I Specular Objects ( Ivo Ihrke )  shape from distortion  direct ray measurements  shape from 
specularities   Part II Refractive Objects ( Wolfgang Heidrich )  shape from distortion  light 
path triangulation  shape from reflectance  tomography   Part III Volumetric Phenomena ( Ivo Ihrke 
)  direct volume sampling  tomographic methods   Outlook  shape from distortion  shape by direct 
ray measurements  shape from specularities  principle of shape from distortion measurements  similar 
to environment monitor in fixed position matting [Zongker et al. 1999]  camera and monitor in calibrated 
positions  patterns encode positions on background plane     patterns shown on monitor  pattern 
object camera  pattern correspondences camera  if camera is orthographic and pattern is distant normals 
are determined uniquely correspondences camera  [Tarini et al. 2005]  specular reflection results 
in 1D ambiguity between depth and surface normal measured Shape from Distortion -Example depth [Tarini 
et al. 2005] map acquistion reflected environment normal geometry setup patterns matte map 1D ambiguity 
is resolved via belief propagation initial guess for depth  compute normals  until convergence compute 
depth Shape from Distortion -Example [Tarini et al. 2005] color reflected pattern environment photograph 
of real object attenuation map matting with same reflected image recovered geometry   scan of a mirror 
sphere ( Ø 60mm )  error in the range of 0.05 mm  error distribution on a sphere segment   need 
initial guess  convergence properties unknown  inherent problem 1D depth-normal ambiguity  can be 
resolved with orthographic views (tele lens)  distant illumination   not a practical setup [Kutulakos 
&#38; Steger 2005], [Bonfort et al. 2006] depth normal ambiguity can be resolved by measuring two points 
on the reflected ray calibrated requires two calibrated plane positions  special case of light path 
triangulation  [Kutulakos &#38; Steger 2005] camera  specular object   reflected patterns are imaged 
with environment matting techniques  two 3D points Q and Q describe exitant ray  1 2 intersection point 
between camera ray and exitant ray uniquely determines surface point and normal  [Kutulakos &#38; Steger 
2005]  laboratory scene  [Bonfort et al. 2006]  real-world example measurement of a car wind shield 
   very accurate method for mirroring objects  [Kutulakos &#38; Steger 2005]  measurement of a planar 
front surface mirror  size 130 x 230 mm, distance 1.5 m  all points are reconstructed independently 
 accuracy of reconstruction: 0.644 mm RMS  angular error of normals: 0.182 degrees   [Bonfort et 
al. 2006]  planar object Ø 80 mm  reconstruction accuracy:  98% of points within 0.2 mm  64% of points 
within 0.1 mm    Shape from Specularities Shape from Specularities Principle: observe specular highlights 
under known observation and illumination direction assume parallel viewing rays and illumination Shape 
from Specularities Principle: highlight is visible where angle . is the same for illumination and viewing 
direction  normal n uniquely defined for these points  Shape from Specularities Principle: highlight 
is visible where angle . is the same for illumination and viewing direction  normal n uniquely defined 
for these points  Shape from Specularities Principle:  highlight is visible where angle . is the same 
for illumination and viewing direction  normal n uniquely defined for these points   Principle:  
highlight is visible where angle . is the same for illumination and viewing direction  normal n uniquely 
defined for these points  surface normals can be sampled by changing the light (or viewing) direction 
 practical setup   [Chen et al. 2006] camera  high power LED (5W Luxeon Star)  mirror spheres to 
determine incident light direction   material examples  chocolate jelly leather skin Mesostructure 
from Specularity [Chen et al. 2006]  sampling is performed manually, with visual feedback  scan results 
in normal maps  chocolate jelly leather  normal fields are integrated to recover surface  normal 
is related to height field gradient  [Frankot and Chellappa 1988], [Agrawala et al. 2003] leather chocolate 
jelly skin  S c a n n i n g T r a n s p a r e n t O b j e c t s S I G G R A P H C o u r s e 2 0 0 9 
 Wolfgang Heidrich The University of British Columbia  R e f r a c t i v e O b j e c t s A d d i t 
i o n a l C h a l l e n g e s f o r R e f r a c t i v e O b j e c t s  R e f r a c t e d r a y d i r 
e c t i o n d e p e n d s o n m a t e r i a l  Of t e n u n k n o w n  T o t a l i n t e r n a l 
r e f l e c t i o n  R e f r a c t i v e l i g h t p a t h s u s u a l l y c o n t a i n 2 + s u r 
f a c e i n t e r a c t i o n s   L i g h t e n t e r i n g a n d l e a v i n g m e d i u m  S i 
m p l e C a s e : S i n g l e R e f r a c t i o n L i g h t P a t h s M o r r i s &#38; K u t o l a 
k o s , I C C V 2 0 0 5  G o a l : r e c o n s t r u c t w a t e r s u r f a c e ( p o s i t i o n &#38; 
n o r m a l )  A s s u m p t i o n s :  C a l i b r a t e d c a m e r a s  C a l i b r a t e d 
p a t t e r n ( v i s i b l e t h r o u g h l i q u i d ,    S e t u p   R e f r a c t i v e S 
t e r e o C o n s t r a i n t  K e y i n s i g h t :  P r o j e c t i o n p a t h s t h a t c r o 
s s a t a n i n t e r f a c e i m p o s e j o i n t c o n s t r a i n t s B a s i c P r o c e d u r 
e :  L e f t p o i n t - t o - p o i n t c o r r e s p o n d e n c e s &#38; d i s t a n c e h y p o 
t h e s i s l e a d t o p o s i t i o n , n o r m a l  R i g h t p 2 p c o r r e s p o n d e n c e 
s , p o s i t i o n , n o r m a l u s e d f o r v e r i f i c a t i o n   R e s u l t s C a m e r 
a 1 C a m e r a 2  R e s u l t s R e c o n s t r u c t e d S u r f a c e  R e s u l t s R e c o 
n s t r u c t e d N o r m a l s  R e s u l t s   D o e s T h i s G e n e r a l i z e t o M u l t 
i p l e L i g h t I n t e r a c t i o n s ?  s p e c u l a r t r a n s m i s s i v e s p e c u l a r 
r e f l e c t i v e K u t u l a k o s &#38; S t e e g e r , I C C V 2 0 0 5 :  A t h e o r y o f r 
e f r a c t i v e a n d s p e c u l a r 3 D s h a p e b y l i g h t p a t h t r i a n g u l a t i o n 
  } M k n o w n r e f e r e n c e s u r f a c e s  A s s u m p t i o n [ Z o n g k e r 9 9 ] [ G r 
o s s b 0 1 ] [ F i t z g i b 0 2 ] : k n o w n 1 - 1 c o r r e s p o n d e n c e b t w e e n N × M 
k n o w n c o r r e s p o n d e n ce m a p s p i x e l s &#38; p o i n t s o n r e f e r e n c e s u 
r f a c e  } N c a l i b r a t e d c a m e r a s   L i g h t p a t h : i n f i n i t e s i m a l l 
y - t h i n , p i e c e w i s e - l i n e a r c u r v e  c o n n e c t s p i x e l t o r e f e r e n 
c e p o i n t  h a s v e r t e x a t e a c h s u r f a c e i n t e r s e c t i o n  } K v e r t i 
c e s S c e n e : = 1 o b j e c t s  h o m o g e n e o u s m e d i a ( o p a q u e o r t r a n s p 
a r e n t )  a r b i t r a r y , u n k n o w n 3 D s h a p e  s m o o t h ( i e . s p e c u l a r 
) s u r f a c e   A n a t o m y o f a L i g h t P a t h   L i g h t p a t h : i n f i n i t e s 
i m a l l y - t h i n , p i e c e w i s e - l i n e a r c u r v e  c o n n e c t s p i x e l t o r e 
f e r e n c e p o i n t  h a s v e r t e x a t e a c h s u r f a c e i n t e r s e c t i o n  U n 
k n o w n :  d e p t h d  K n o r m a l s  K - 1 v e r t e x d i s t a n c e s  K n o w n :  
l a s t r a y &#38; 3 D p o i n t o n 1 s t r a y  f o r e a c h v e r t e x , w h e t h e r c a u 
s e d b y r e f r a c t i o n o r r e f l e c t i o n  r e f r a c t i o n i n d i c e s ( o p t i 
o n a l )   M } k n o w n p o i n t s K v e r t i c e s } N c a m e r a s L i g h t n e t w o 
r k c o n s i s t e n t f o r d e p t h h y p o t h e s i s d i f . n o r m a l &#38; d i s t a n c 
e a s s i g n m e n t  c o n s i s t e n t w i t h r e f l e c t i o n &#38; r e f r a c t i o n  
c o n s i s t e n t w i t h t h e N × M c o r r e s p o n d e n c e m a p s  < M , N , K > L i g h 
t - p a t h t r i a n g u l a t i o n A s s i g n d e p t h d t h a t p r o d u c e s c o n s i s t e 
n t l i g h t n e t w o r k  T r i a n g u l a t i o n s T r a c t a b l e t r i a n g u l a t i o 
n : d e p t h s o l u t i o n s i n a 0 - d i m m a n i f o l d T h e o r e m : T h e o n l y t r a c 
t a b l e < M , N , K > l i g h t - p a t h t r i a n g u l a t i o n s a r e g i v e n b e l o w  
T r i a n g u l a t i o n s T r a c t a b l e t r i a n g u l a t i o n : d e p t h s o l u t i o n 
s i n a 0 - d i m m a n i f o l d T h e o r e m : T h e o n l y t r a c t a b l e < M , N , K > l i g 
h t - p a t h t r i a n g u l a t i o n s a r e g i v e n b e l o w  T r i a n g u l a t i o n s T 
r a c t a b l e t r i a n g u l a t i o n : d e p t h s o l u t i o n s i n a 0 - d i m m a n i f o l 
d T h e o r e m : T h e o n l y t r a c t a b l e < M , N , K > l i g h t - p a t h t r i a n g u l a 
t i o n s a r e g i v e n b e l o w 2 1 1  T r i a n g u l a t i o n s T r a c t a b l e t r i 
a n g u l a t i o n : d e p t h s o l u t i o n s i n a 0 - d i m m a n i f o l d T h e o r e m : T h 
e o n l y t r a c t a b l e < M , N , K > l i g h t - p a t h t r i a n g u l a t i o n s a r e g i v 
e n b e l o w  R e s u l t s : < 2 , 3 , 2 > T r i a n g u l a t i o n s   S c a t t e r ­ T r a 
c e P h o t o g r a p h y  S c a t t e r ­ T r a c e P h o t o g r a p h y  S c a t t e r ­ T r a 
c e P h o t o g r a p h y   S c e n e : = 1 o b j e c t s   u n k n o w n s h a p e  s p e c u l 
a r m i c r o f a c e t r e f l e c t i o n / t r a n s m i s s i o n I s o t r o p i c p o i n t l 
i g h t s o u r c e     m o v e s t o e v e r y p o s i t i o n     i n f i x e d 2 D o r 3 D r e 
g i o n       T h e S c a t t e r T r a c e o f a P i x e l       ( x , y ) ( x , y )  S c a tte 
r tr a c e   v a l u e o f s c a tte r tr a c e a t ( x , y ) =            p i x e l i n te n s i ty 
w h e n s o u r c e a t ( x , y )  E x a m p l e S c a t t e r T r a c e s     A n a t o m y o f 
a S c a t t e r T r a c e      2 c o m p o n e n t s  d i r e c t &#38; i n d i r e c t    i d e a 
l s p e c u l a r r e f l e c t i o n     s i n g l e r a y  i n t e n s i t y . a s d i s t a n c 
e  .  A n a t o m y o f a S c a t t e r T r a c e       2 c o m p o n e n t s  d i r e c t &#38; 
i n d i r e c t    m i r r o r r e f l e c t i o n     s i n g l e r a y  i n t e n s i t y . a s 
d i s t a n c e  . n o n ­ i d e a l r e f l e c t i o n  c o n e o f r a y s , a p e x o n      
e x t e r i o r s u r f a c e  d u e t o m i c r o f a c e t    d i s t r i b u t i o n   A n a t 
o m y o f a S c a t t e r T r a c e       2 c o m p o n e n t s  d i r e c t &#38; i n d i r e c t 
    m i r r o r r e f l e c t i o n     s i n g l e r a y  i n t e n s i t y . a s d i s t a n c e 
 . n o n ­ i d e a l r e f l e c t i o n  c o n e o f r a y s , a p e x o n       e x t e r i o r 
s u r f a c e  d u e t o m i c r o f a c e t    d i s t r i b u t i o n  i n d i r e c t l i g h 
t t r a n s p o r t    g e n e r a l r a y b u n d l e     A n a t o m y o f a S c a t t e r T r a 
c e  2 c o m p o n e n t s d i r e c t &#38; i n d i r e c t m i r r o r r e f l e c t i o n  s i 
n g l e r a y  i n t e n s i t y . a s d i s t a n c e . n o n ­ i d e a l r e f l e c t i o n  c o 
n e o f r a y s , a p e x o n e x t e r i o r s u r f a c e  d u e t o m i c r o f a c e t d i s t 
r i b u t i o n  g e n e r a l r a y b u n d l e  i n d i r e c t l i g h t t r a n s p o r t Course 
-Acquisition of Optically Complex Objects and Phenomena  T h e S c a t t e r ­ T r a c e S t e r e o 
M e t r i c   R e c o n s t r u c t i o n R e s u l t s   Q u a n t i t a t i v e r e s u l t s : 
e r r o r w r t g r o u n d ­ t r u t h < 2 m m ( ~ d e p t h r e s o l u t i o n )  R e c o n s t r 
u c t i o n R e s u l t s   T o m o g r a p h y A R a d i c a l l y D i f f e r e n t A p p r o a 
c h T r i f o n o v , B r a d l e y , H e i d r i c h , E G S R 2 0 0 6 :  R e c o n s t r u c t g 
e o m e t r y o f t r a n s p a r e n t o b j e c t s f r o m a n u m b e r o f s e e - t h r o u g h 
i m a g e s   T o m o g r a p h y G i v e n  O b j e c t m a d e f r o m s e m i - t r a n s p a 
r e n t m e d i u m  K n o w n a b s o r p t i o n s a l o n g a d e n s e s e t o f l i g h t r a 
y s t h r o u g h t h e o b j e c t   K n o w n g e o m e t r y o f l i g h t r a y s W e c a n  R 
e c o n s t r u c t d i f f e r e n t i a l a b s o r p t i o n o n a v o x e l g r i d s u p e r i m 
p o s e d o n t h e o b j e c t  R e c o v e r s u r f a c e g e o m e t r y o f t h e o b j e c t 
a s a n i s o - s u r f a c e   V i s i b l e L i g h t a n d T r a n s p a r e n t O b j e c t s 
 K e y d i f f i c u l t y :  R e f r a c t i v e i n d e x o f t r a n s p a r e n t o b j e c t s 
c a u s e s r e f r a c t i o n s a n d s p e c u l a r h i g h l i g h t s  R e f r a c t i o n s m 
e a n t h a t t h e r a y g e o m e t r y i s n o t k n o w n u n l e s s t h e g e o m e t r y i s 
 S p e c u l a r h i g h l i g h t s m a k e i t d i f f i c u l t t o s e p a r a t e r e f l e c t 
e d a n d t r a n s m i t t e d l i g h t  i . e d i f f i c u l t t o c o m p u t e a b s o r p t 
i o n  E x a m p l e : r e f r a c t i o n s a n d s p e c u l a r h i g h l i g h t s   V i s i b 
l e L i g h t a n d T r a n s p a r e n t O b j e c t s I d e a :  I m m e r s e o b j e c t i n a 
f l u i d w i t h s a m e r e f r a c t i v e i n d e x a s o b j e c t  E l i m i n a t e s s p e c 
u l a r i t i e s a n d r e f r a c t i o n R a y g e o m e t r y c a n t h e n b e c a l i b r a t 
e d i n d e p e n d e n t o f t h e o b j e c t  S e t u p   M e t h o d I s s u e s t o s o l v 
e :  W h a t f l u i d t o u s e  C a l i b r a t i o n o f r a y g e o m e t r y  C l e a r g l 
a s s a c q u i s i t i o n  A c t u a l r e c o n s t r u c t i o n   W a t e r 1 . 3 4 A c r y 
l i c P l a s t i c 1 . 5 0 P y re x Gl a s s 1 . 4 7 - 1 . 4 9 Op t i c a l G l a s s 1 . 4 5 - 1 
. 9 6 M i n e r a l Oi l 1 . 4 8 Ol i v e Oi l 1 . 4 7 T u n g Oi l 1 . 5 2 C a s s i a Oi l 1 . 
6 0 P o t a s s i u m T h i o c y a n a t e s o l n . 1 . 3 4 - 1 . 5 2  S o u r c e : B u d w i g 
( E x p e r i m e n t s i n F l u i d s 9 4 )  C a l i b r a t i o n  C a l i b r a t i o n p a t 
t e r n s l o c a t e d o n t w o p a r a l l e l p l a n e s  I n t e r p o l a t i o n g i v e s 
( u / v - s / t ) c o r d . f o r e v e r y p i x e l   C a l i b r a t i o n R e s u l t :  D e 
n s e , t w o - p l a n e p a r a m e t e r i z e d r a y b u n d l e f o r t h e c a m e r a p i x e 
l s   C o l o r e d G l a s s A c q u i s i t i o n :  C o l o r o f g l a s s p r o v i d e s c 
o n t r a s t b e t w e e n c l e a r f l u i d a n d o b j e c t , w h i c h c a n l a t e r b e r e 
c o v e r e d  T a k e o n e ( H D R ) p h o t o f o r e v e r y d e g r e e   T h e d a r k e r 
t h e g l a s s , t h e m o r e e x p o s u r e s a r e r e q u i r e d . T y p i c a l l y : 2 - 3 
  C l e a r G l a s s A c q u i s i t i o n :  F o r c l e a r g l a s s , t h e c o n t r a s t b 
e t w e e n f l u i d a n d o b j e c t i s n o t l a r g e e n o u g h  A d d f o o d c o l o r i 
n g t o f l u i d t o i n c r e a s e c o n t r a s t  M o r e c o l o r p r o v i d e s m o r e c 
o n t r a s t , a l l o w s f o r f i n e r d e t a i l s  H o w e v e r : t o o m u c h c o l o r 
d r a s t i c a l l y i n c r e a s e s r e q u i r e d e x p o s u r e t i m e s !    R e c o n s 
t r u c t i o n A l g o r i t h m F o u r i e r S l i c e R e c o n s t r u c t i o n  F a s t , b 
u t d o e s n o t a p p l y f o r o u r r a y g e o m e t r y  N o s i n g l e c e n t e r o f p r o 
j e c t i o n d u e t o a s t i g m a t i s m c a u s e d b y c y l i n d r i c a l c o n t a i n e r 
 A l g e b r a i c R e c o n s t r u c t i o n  F o l l o w i n g M u e l l e r e t a l ( T r a n s 
. M e d . I m a g . 9 9 )  I t e r a t e :  F o r w a r d p r o j e c t i o n  B a c k p r o j 
e c t i o n &#38; u p d a t e    A l g e b r a i c R e c o n s t r u c t i o n F o r w a r d P r 
o j e c t i o n  V o l u m e r e n d e r i n g s t e p : c o m p u t e c o n t r i b u t i o n o f a 
l l v o x e l s t o a n y g i v e n r a y i n a n i m a g e  U s e K a i s e r - B e s s e l f i l 
t e r f o r h i g h q u a l i t y r e s u l t s  U s i n g t h e s e w e i g h t s , c r e a t e  
 a n i m a g e f r o m t h e c u r r e n t e s t i m a t e o f a b s o r p t i o n s   A l g e b r 
a i c R e c o n s t r u c t i o n B a c k p r o j e c t i o n &#38; U p d a t e  F o r e v e r y r 
a y , c o m p u t e t h e d i f f e r e n c e o f t h e e s t i m a t e t o t h e m e a s u r e m e n 
t  U n i f o r m l y d i s t r i b u t e t h i s e r r o r a l o n g t h e r a y , u s i n g t h e 
s a m e w e i g h t s c o m p u t e d d u r i n g f o r w a r d p r o j e c t i o n   R e s u l t 
s   R e s u l t s - J a r   R e s u l t s - C h e s s P i e c e s 1   R e s u l t s - C h e s s 
P i e c e s 2    C o m p a r i s o n W i t h L a s e r S c a n L a s e r S c a n :  A f t e r s 
p r a y - p a i n t i n g f i g u r e s  M u l t i p l e m e r g e d s c a n s   S e v e r a l h 
o u r s o f p o s t - p r o c e s s i n g T o m o g r a p h y :  M o r e d e t a i l  M a n i f o 
l d m e s h  P r e c i s i o n o f b e t t e r t h a n 0 . 5 m m   S l i d e s f r o m :  N i g 
e l M o r r i s , E r o n S t e g e r , K y r o s K u t u l a k o s  D e r e k B r a d l e y , B o 
r i s l a v T r i f o n o v    do not have a proper surface e.g. fire, smoke, astronomical objects 
 often dynamic  do not have a proper surface e.g. fire, smoke, astronomical objects often dynamic 
 direct volume sampling  laser probing  sampling trade-off (time vs. spatial resolution)   tomographic 
reconstruction  dynamics are difficult  need many view-points for good quality  expensive setup (multiple 
cameras)  numerical inversion of an ill-posed problem    Fluorescent Immersion Range Scanning  Fluorescent 
Immersion Range Scanning   Fluorescent Immersion Range Scanning   Fluorescent Immersion Range Scanning 
  completely transparent objects all surface scan  photo x-ray rendering output mesh pbrt glass shader 
 Fluorescent Immersion Range Scanning   Direct Sampling Techniques Laser Scanning -Smoke different 
sampling strategies  plane sampling line sampling  different spatial position at fixed spatial position 
at different .t different .t  [Hawkins et al. 2005] [Fuchs et al. 2006] Laser Scanning -Smoke different 
sampling strategies  plane sampling line sampling  different spatial position at fixed spatial position 
at different .t different .t  [Hawkins et al. 2005] [Fuchs et al. 2006]  Setup: 3W argon-ion laser 
 mirror galvanometer  cylindrical lens  high-speed camera ~5000 frames per second  . 200 slices per 
scan result in 25 volumes per second [Hawkins et al. 2005]  raw frames captured by scanning system 
 [Hawkins et al. 2005]  similar to Laser Induced Fluorescence (LIF) measurements in fluid imaging 
[Deusch and Dracos 2001]  compensation of movement during acquisition by 3D optical flow [Van Vliet 
et al. 2004]  uncompensated volume compensated volume  Laser Plane Scanning -Smoke additionally, smoke 
albedo and scattering phase function are recovered for rendering purposes  albedo measurement phase 
function measurement method assumptions: negligible scattering and absorption of participating medium 
 speed of movement comparatively slow w.r.t. frame rate  [Hawkins et al. 2005]  [Hawkins et al. 2005] 
 Laser Scanning -Smoke different sampling strategies  plane sampling line sampling  different spatial 
position at fixed spatial position at different .t different .t  [Hawkins et al. 2005] [Fuchs et al. 
2006] Laser Line Scanning -Setup [Fuchs et al. 2006] camera burst grating mask laser [Fuchs et al. 
2006] practical setup 2 lasers ( red 130 mW and blue 30 mW )  2x5x5rays using burst grating  laser 
lines do not super-impose in image plane   [Fuchs et al. 2006]  [Fuchs et al. 2006] scans are dense 
along 1D lines  equi-temporal measurement  problem data interpolation  select data points on line 
 RBF interpolation   [Fuchs et al. 2006] static volume animation  [Fuchs et al. 2006] real world 
results  2 times 5x5 rays  3 frames per second capture speed   Tomographic Techniques  recover 
volumetric object descriptions from camera images without additional equipment  multiple views are necessary 
 similar to medical CT  challenges:  few input views  dynamic nature of fire and smoke requires multi­camera 
setup expensive   flame/density sheet decomposition [Hasinoff 03, Hasinoff 07]  visual hull restricted 
algebraic reconstruction [Ihrke 04, Ihrke 06]  Calibrated, synchronized camera setup  8 cameras, 320 
x 240 @ 15 fps  Issues  Perfect synchronization  Unsaturated images  Very short shutter times noisy 
images  Dark background      Input sequence   Algebraic Reconstruction Technique (ART)  model 
fire as density field F of soot particles  linearized image formation model  [Hasinoff and Kutulakos 
2003] pixel intensity is equal to integral over density field CT-Background (ART) Algebraic Reconstruction 
Technique (ART)  Discretize unknown F using a linear combination of basis functions Fi  cp Fi p Ip 
linear system p = Sa CT-Background (ART) Algebraic Reconstruction Technique (ART)  Discretize unknown 
F using a linear combination of basis functions Fi  cp Fi p Ip linear system p = Sa CT-Background (ART) 
Basis functions pi pixels 1 2 3 4 5 1 1 1 1 1 p invert LS in a 2 1 2 2 2 3 2 4 2 5 least squares sense: 
1 2 3 4 5 3 3 3 3 3 a = ( S S ) S pT T-1  Large number of projections is needed  In case of dynamic 
phenomena many cameras expensive  inconvenient placement   straight forward application of ART with 
few cameras not satisfactory   [Ihrke and Magnor 2004]  Only about 1/10 of the voxels contribute 
 Remove voxels that do not contribute from linear system  Complexity of inversion is significantly 
reduced  [Ihrke and Magnor 2004]  [Ihrke and Magnor 2004]  Visual Hull Restricted Tomography  [Ihrke 
and Magnor 2005] Reconstruction and de gof me ayig intuitive explanation: moving sheet-like structures 
give a good impression of flame movement [Hasinoff and Kutulakos 2003]  Problem formulation pairs 
of input images are rectified  basis is defined in each epipolar slice separately  solution is coherent 
because of image continuity  [Hasinoff and Kutulakos 2003]  spatially most coherent basis functions 
that are photo-consistent with two input views  two possibilities D, D  I [Hasinoff and Kutulakos 
2003] 2  Density Sheets Every pair (I1, I2) define 2 unique basis functions D, D that are: photo-consistent 
 non-zero only along a discrete &#38; monotonic curve  their convex combination is photo-consistent 
 wD+ (1 w)D Construction similar to dynamic time warping (DTW) D I1  Input images Flame Sheet surface 
[Hasinoff and Kutulakos 2003]  The Decomposed Flame Sheet Basis 1 - ww single density sheets are not 
expressive enough generate new basis functions by decomposing original basis functions 1. Decompose images 
into parts, with same relative weights 2. Create opposite Flame Sheets between corres-ponding parts 
 3. Take their convex combination  += w [Hasinoff and Kutulakos 2003] 1 - w  convex problem is 
solved using quadratic programming  results from two views:   Astronomical Objects  Planetary Nebulae 
planetary nebulae axi-symmetric orthographic view  single view 2D tomography problem  [Magnor et 
al. 2004]  Planetary Nebulae -Results   first usable reconstruction algorithms  no convergence of 
methods  still specialized techniques are necessary to deal with optically complex materials  applications: 
 modeling for computer graphics  industrial quality inspection  preservation of historical artifacts 
  first usable reconstruction algorithms  no convergence of methods  still specialized techniques 
are necessary to deal with optically complex materials  applications:  modeling for computer graphics 
 industrial quality inspection  preservation of historical artifacts   holy grail: mixed scenes with 
unknown object properties   Thank you for your attention Acknowledgements: Kiriakos Kutulakos, Sam 
Hasinoff, Eron Steger, Nigel Morris, Marcus Magnor, Andrei Lintu, Matthias Hullin, Martin Fuchs, Hendrik 
Lensch, Christian Fuchs, Tongbo Chen, Michael Goesele, Holger Theisel, Hans-Peter Seidel, Tim Hawkins, 
Per Einarsson, Paul Debevec, Alex Reche, Ignacio Martin, George Drettakis, Shuntaro Yamazaki, Masaaki 
Mochimaru, Takeo Kanade, Thomas Bonfort, Peter Sturm, Pau Gargallo and Marco Tarini  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1667241</article_id>
		<sort_key>20</sort_key>
		<display_label>Article No.</display_label>
		<display_no>2</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Advanced illumination techniques for GPU-based volume raycasting]]></title>
		<page_from>1</page_from>
		<page_to>166</page_to>
		<doi_number>10.1145/1667239.1667241</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1667241</url>
		<abstract>
			<par><![CDATA[<p>Volume raycasting techniques are important for both visual arts and visualization. They allow efficient generation of visual effects and visualization of scientific data obtained by tomography or numerical simulation. Volume-rendering techniques are also effective for direct rendering of implicit surfaces used for soft-body animation and constructive solid geometry. The focus of this course is on volumetric illumination techniques that approximate physically based light transport in participating media. Such techniques include interactive implementation of soft and hard shadows, ambient occlusion, and simple Monte Carlo-based approaches to global illumination, including translucency and scattering.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Intensity, color, photometry, and thresholding</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Graphics processors</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010389</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics processors</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1797102</person_id>
				<author_profile_id><![CDATA[81100644757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Markus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hadwiger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[VRVis Research Center, Vienna, Austria]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797103</person_id>
				<author_profile_id><![CDATA[81100015118]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Patric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ljung]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Siemens Corporate Research, Princeton]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797104</person_id>
				<author_profile_id><![CDATA[81435606946]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Christof]]></first_name>
				<middle_name><![CDATA[Rezk]]></middle_name>
				<last_name><![CDATA[Salama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Siegen, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797105</person_id>
				<author_profile_id><![CDATA[81319500253]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Timo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ropinski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of M&#252;nster, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Michael D. Adams. <i>The JPEG-2000 Still Image Compression Standard</i>. ISO/IEC (ITU-T SG8), September 2001. JTC 1/SC 29/WG 1: N 2412.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383747</ref_obj_id>
				<ref_obj_pid>383745</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Chandrit Bajaj, Insung Ihm, and Sanghun Park. 3D RGB image compression for interactive applications. <i>ACM Transactions on Graphics</i>, 20(1):10--38, January 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kevin M. Beason, Josh Grant, David C. Banks, Brad Futch, and M. Yousuff Hussaini. Pre-computed illumination for isosurfaces. In <i>VDA '94: Proceedings of the conference on Visualization and Data Analysis '06 (SPIE Vol. 6060)</i>, pages 1--11, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>288149</ref_obj_id>
				<ref_obj_pid>288126</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Uwe Behrens and Ralf Ratering. Adding shadows to a texture-based volume renderer. In <i>VVS '98: Proceedings of the 1998 IEEE symposium on Volume visualization</i>, pages 39--46. ACM Press, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2386437</ref_obj_id>
				<ref_obj_pid>2386410</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Johanna Beyer, Markus Hadwiger, Torsten M&#246;ller, and Laura Fritz. Smooth Mixed-Resolution GPU Volume Rendering. In <i>IEEE/EG International Symposium on Volume and Point-Based Graphics</i>, pages 163--170, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>584118</ref_obj_id>
				<ref_obj_pid>584110</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Praveen Bhaniramka and Yves Demange. OpenGL Volumizer: A Toolkit for High Quality Volume Rendering of Large Data Sets. In <i>Proceedings IEEE Visualization 2002</i>, pages 45--53, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[J. F. Blinn. Jim blinn's corner: Image compositing--theory. <i>IEEE Computer Graphics and Applications</i>, 14(5), 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Imma Boada, Isabel Navazo, and Roberto Scopigno. Multiresolution volume visualization with a texture-based octree. <i>The Visual Computer</i>, 17:185--197, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[A. R. Calderbank, Ingrid Daubechies, Wim Sweldens, and Boon-Lock Yeo. Wavelet transforms that map integers to integers. Technical report, Department of Mathematics, Princeton University, August 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>844182</ref_obj_id>
				<ref_obj_pid>844174</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[N. Carr, J. Hall, and J. Hart. GPU Algorithms for Radiosity and Subsurface Scattering. In <i>Proc. Graphics Hardware</i>, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>844182</ref_obj_id>
				<ref_obj_pid>844174</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Nathan A. Carr, Jesse D. Hall, and John C. Hart. GPU algorithms for radiosity and subsurface scattering. In <i>HWWS '03: Proceedings of the conference on Graphics Hardware '03</i>, pages 51--59. Eurographics Association, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>288241</ref_obj_id>
				<ref_obj_pid>288216</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Yi-Jen Chiang, Cl&#225;udio T. Silva, and William J. Schroeder. Interactive out-of-core isosurface extraction. In <i>Proceedings of IEEE Visualization '98</i>, pages 167--174, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>288241</ref_obj_id>
				<ref_obj_pid>288216</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Yi-Jen Chiang, Cludio T. Silva, and Willam J. Schroeder. Interactive out-of-core isosurface extraction. In <i>Proceedings IEEE Visualization 1998</i>, pages 167--174, 530, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[M. Colbert and J. K&#345;iv&#225;nek. <i>GPU Gems 3</i>, chapter GPU-Based Importance Sampling, pages 459--475. Addison-Wesley, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>267068</ref_obj_id>
				<ref_obj_pid>266989</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Michael Cox and David Ellsworth. Application-controlled demand paging for out-of-core visualization. In <i>Proceedings IEEE Visualization 1997</i>, pages 235--244, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>563901</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Franklin C. Crow. Shadow algorithms for computer graphics. In <i>SIGGRAPH '77: Proceedings of the 4th annual conference on Computer graphics and interactive techniques</i>, pages 242--248. ACM Press, 1977.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808600</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Franklin C. Crow. Summed-area tables for texture mapping. In <i>Proceedings SIGGRAPH '84</i>, volume 18, pages 207--212, 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1111428</ref_obj_id>
				<ref_obj_pid>1111411</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Carsten Dachsbacher and Marc Stamminger. Splatting indirect illumination. In <i>I3D '06: Proceedings of the 2006 symposium on Interactive 3D graphics and games</i>, pages 93--100, New York, NY, USA, 2006. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>130655</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Ingrid Daubechies. <i>Ten Lectures on Wavelets</i>. Society for Industrial and Applied Mathematics, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Philippe Desgranges and Klaus Engel. US patent application 2007/0013696 A1: Fast ambient occlusion for direct volume rendering, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Philippe Desgranges, Klaus Engel, and Gianluca Paladini. Gradient-free shading: A new method for realistic interactive volume rendering. In <i>VMV '05: Proceedings of the international fall workshop on Vision, Modeling, and Visualization</i>, pages 209--216, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073308</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[C. Donner and H. W. Jensen. Light Diffusion in Multi-Layered Translucent Materials. In <i>Proc. ACM SIGGRAPH</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378484</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[R. A. Drebin, L. Carpenter, and P. Hanrahan. Volume rendering. In <i>Proceedings of SIGGRAPH '88</i>, pages 65--74, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>551861</ref_obj_id>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[D. Ebert, F. K. Musgrave, D. Peachey, K. Perlin, and S. Worley. <i>Texturing and Modeling: A Procedural Approach</i>. Academic Press, July 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1177246</ref_obj_id>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Klaus Engel, Markus Hadwiger, Joe Kniss, Christof Rezk-Salama, and Daniel Weiskopf. <i>Real-Time Volume Graphics</i>. AK Peters, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>561541</ref_obj_id>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[James D. Foley, Richard L. Phillips, John F. Hughes, Andries van Dam, and Steven K. Feiner. <i>Introduction to Computer Graphics</i>. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Jinzhu Gao, Jian Huang, C. Ryan Johnson, and Scott Atchley. Distributed data management for large volume visualization. In <i>Proceedings IEEE Visualization 2005</i>, pages 183--189. IEEE, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1081489</ref_obj_id>
				<ref_obj_pid>1081432</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Jinzhu Gao, Jian Huang, Han-Wei Shen, and James Arthur Kohl. Visibility culling using plenoptic opacity functions for large volume visualization. In <i>Proceedings IEEE Visualization 2003</i>, pages 341--348. IEEE, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1034442</ref_obj_id>
				<ref_obj_pid>1032664</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Jinzhu Gao, Han-Wei Shen, Jian Huang, and James Arthur Kohl. Visibility culling for time-varying volume rendering using temporal occlusion coherence. In <i>Proceedings IEEE Visualization 2004</i>, pages 147--154. IEEE, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1039017</ref_obj_id>
				<ref_obj_pid>1038266</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[S. Grimm, S. Bruckner, A. Kanitsar, and E. Gr&#246;ller. Memory efficient acceleration structures and techniques for cpu-based volume raycasting of large data. In <i>Proceedings IEEE/SIGGRAPH Symposium on Volume Visualization and Graphics</i>, pages 1--8, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1039017</ref_obj_id>
				<ref_obj_pid>1038266</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[S&#246;ren Grimm, Stefan Bruckner, Armin Kanitsar, and Eduard Gr&#246;ller. Memory efficient acceleration structures and techniques for CPU-based volume raycasting of large data. In <i>Proceedings IEEE Volume Visualization and Graphics Symposium</i>, pages 1--8, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[S&#246;ren Grimm, Stefan Bruckner, Armin Kanitsar, and Eduard Gr&#246;ller. A refined data addressing and processing scheme to accelerate volume raycasting. <i>Computers and Graphics</i>, 28:719--729, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>601726</ref_obj_id>
				<ref_obj_pid>601671</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Stefan Guthe and Wolfgang Stra&#223;er. Real-time decompression and visualization of animated volume data. In <i>Proceedings IEEE Visualization 2001</i>, pages 349--356, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Stefan Guthe and Wolfgang Strasser. Advanced techniques for high quality multiresolution volume rendering. In <i>Computers&amp;Graphics</i>, volume 28, pages 51--58. Elsevier Science, February 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>602106</ref_obj_id>
				<ref_obj_pid>602099</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Stefan Guthe, Michael Wand, Julius Gonser, and Wolfgang Stra&#223;er. Interactive rendering of large volume data sets. In <i>Proceedings IEEE Visualization 2002</i>, pages 53--60, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Attila Gyulassy, Lars Linsen, and Bernd Hamann. Time- and space-efficient error calculation for multiresolution direct volume rendering. In <i>Mathematical Foundations of Scientific Visualization, Computer Graphics, and Massive Data Exploration</i>. Springer-Verlag, Heidelberg, Germany, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[M. Hadwiger, C. Sigg, H. Scharsach, K. B&#252;hler, and M. Gross. Real-time ray-casting and advanced shading of discrete isosurfaces. In <i>Proceedings of Eurographics 2005</i>, pages 303--312, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[M. Hadwiger, C. Sigg, H. Scharsach, K. B&#252;hler, and M. Gross. Real-Time Ray-Casting and Advanced Shading of Discrete Isosurfaces. In <i>Proceedings of Eurographics</i>, pages 303--312, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1283908</ref_obj_id>
				<ref_obj_pid>1283900</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Markus Hadwiger, Andrea Kratz, Christian Sigg, and Katja B&#252;hler. Gpu-accelerated deep shadow maps for direct volume rendering. In <i>GH '06: Proceedings of the 21st ACM SIGGRAPH/Eurographics symposium on Graphics hardware</i>, pages 49--52, New York, NY, USA, 2006. ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311554</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[W. Heidrich and H.-P. Seidel. Realistic, Hardware-accellerated Shading and Lighting. In <i>Proc. ACM SIGGRAPH</i>, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[L. Henyey and J. Greenstein. Diffuse radiation in the galaxy. <i>Astrophysical Journal</i>, pages p. 70--83, 93.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2386503</ref_obj_id>
				<ref_obj_pid>2386501</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[Frida Hernell, Patric Ljung, and Anders Ynnerman. Efficient ambient and emissive tissue illumination using local occlusion in multiresolution volume rendering. In <i>Proceedings Eurographics/IEEE-VGTC Symposium on Volume Graphics</i>. Eurographics/IEEE, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2386428</ref_obj_id>
				<ref_obj_pid>2386410</ref_obj_pid>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[Frida Hernell, Patric Ljung, and Anders Ynnerman. Interactive Global Light Propagation in Direct Volume Rendering using Local Piecewise Integration. In <i>IEEE/EG International Symposium on Volume and Point-Based Graphics</i>, pages 105--112, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2386497</ref_obj_id>
				<ref_obj_pid>2386472</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[W. Hong, F. Qiu, and A. Kaufman. Gpu-based object-order raycasting for large datasets. In <i>Proceedings of Volume Graphics 2005</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[Insung Ihm and Sanghun Park. Wavelet-based 3d compression scheme for interactive visualization of very large volume data. <i>Computer Graphics Forum</i>, 18(1):3--15, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383319</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[Henrik Wann Jensen, Stephen R. Marschner, Marc Levoy, and Pat Hanrahan. A Practical Model for Subsurface Light Transport. In <i>Proceedings of ACM SIGGRAPH</i>, pages 511--518, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[Ralf K&#228;hler, John Wise, Tom Abel, and Hans-Christian Hege. Gpu-assisted raycasting for cosmological adaptive mesg refinement simulations. In <i>Proceedings Eurographics/IEEE Workshop on Volume Graphics 2006</i>, pages 103--110, 144, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74364</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[D. Kalra and A. H. Barr. Guaranteed ray intersections with implicit surfaces. In <i>Proceedings of SIGGRAPH '89</i>, pages 297--306, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[A. Kaufman. Voxels as a Computational Representation of Geometry. In <i>The Computational Representation of Geometry. SIGGRAPH '94 Course Notes</i>, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732282</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[Tae-Yong Kim and Ulrich Neumann. Opacity shadow maps. In <i>Proceedings of the 12th Eurographics Workshop on Rendering Techniques</i>, pages 177--182, London, UK, 2001. Springer-Verlag.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614529</ref_obj_id>
				<ref_obj_pid>614287</ref_obj_pid>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[Joe Kniss, Gordon Kindlmann, and Charles Hansen. Multidimensional transfer functions for interactive volume rendering. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 8(3):270--285, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>602114</ref_obj_id>
				<ref_obj_pid>602099</ref_obj_pid>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[Joe Kniss, Simon Premoze, Charles Hansen, and David Ebert. Interactive translucent volume rendering and procedural modeling. In <i>VIS '02: Proceedings of the conference on Visualization '02</i>, pages 109--116. IEEE Computer Society, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>776766</ref_obj_id>
				<ref_obj_pid>776751</ref_obj_pid>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[Joe Kniss, Simon Premoze, Charles Hansen, Peter Shirley, and Allen McPherson. A model for volume lighting and modeling. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 9(2):150--162, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>569048</ref_obj_id>
				<ref_obj_pid>569046</ref_obj_pid>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[M. Kraus and T. Ertl. Adaptive texture maps. In <i>Proceedings of Graphics Hardware 2002</i>, pages 7--15, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1081482</ref_obj_id>
				<ref_obj_pid>1081432</ref_obj_pid>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[J. Kr&#252;ger and R. Westermann. Acceleration techniques for GPU-based volume rendering. In <i>Proceedings IEEE Visualization 2003</i>, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>319432</ref_obj_id>
				<ref_obj_pid>319351</ref_obj_pid>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[Eric C. LaMar, Bernd Hamann, and Kenneth I. Joy. Multiresolution techniques for interactive texture-based volume visualization. In <i>Proceedings IEEE Visualization 1999</i>, pages 355--362, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[Eric C. LaMar, Bernd Hamann, and Kenneth I. Joy. Efficient error calculation for multiresolution texture-based volume visualization. In Gerald Farin, Bernd Hamann, and Hans Hagen, editors, <i>Hierachical and Geometrical Methods in Scientific Visualization</i>, pages 51--62. Springer-Verlag, Heidelberg, Germany, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[Michael S. Langer and Heinrich H. B&#252;lthoff. Depth discrimination from shading under diffuse lighting. <i>Perception</i>, 29(6):649--660, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[H. Lensch, M. Goesele, P. Bekaert, J. Kautz, M. Magnor, J. Lang, and H.-P. Seidel. Interactive rendering of translucent objects. <i>Computer Graphics Forum</i>, 22(2), 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>44652</ref_obj_id>
				<ref_obj_pid>44650</ref_obj_pid>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[M. Levoy. Display of surfaces from volume data. <i>IEEE Computer Graphics and Applications</i>, 8(3):29--37, May 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[F. Link, M. Koenig, and H.-O. Peitgen. Multi-Resolution Volume Rendering with per Object Shading. In <i>Proceedings of Vision, Modeling and Visualization</i>, pages 185--191, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614330</ref_obj_id>
				<ref_obj_pid>614261</ref_obj_pid>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[Yarden Livnat, Han-Wei Shen, and Christopher R. Johnson. A near optimal isosurface extraction algorithm using the span space. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 2:73--84, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[Patric Ljung. Adaptive sampling in single pass, GPU-based raycasting of multiresolution volumes. In <i>Proceedings Eurographics/IEEE Workshop on Volume Graphics 2006</i>, pages 39--46, 134, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[Patric Ljung. <i>Efficient Methods for Direct Volume Rendering of Large Data Sets</i>. PhD thesis, Link&#246;ping University, Sweden, 2006. Link&#246;ping studies in science and technology. Dissertations no. 1043.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2384836</ref_obj_id>
				<ref_obj_pid>2384796</ref_obj_pid>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[Patric Ljung, Claes Lundstr&#246;m, and Anders Ynnerman. Multiresolution interblock interpolation in direct volume rendering. In <i>Proceedings Eurographics/IEEE Symposium on Visualization 2006</i>, pages 259--266, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1039013</ref_obj_id>
				<ref_obj_pid>1038266</ref_obj_pid>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[Patric Ljung, Claes Lundstr&#246;m, Anders Ynnerman, and Ken Museth. Transfer function based adaptive decompression for volume rendering of large medical data sets. In <i>Proceedings IEEE Volume Visualization and Graphics Symposium 2004</i>, pages 25--32, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1187802</ref_obj_id>
				<ref_obj_pid>1187627</ref_obj_pid>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[Patric Ljung, Calle Winskog, Anders Perssson, Claes Lundstr&#246;m, and Anders Ynnerman. Full body virtual autopsies using a state-of-the-art volume rendering pipeline. <i>IEEE Transactions on Visualization and Computer Graphics (Proceedings Visualization/Information Visualization 2006</i>), 12:869--876, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344958</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[Tom Lokovic and Eric Veach. Deep shadow maps. In <i>SIGGRAPH '00: Proceedings of the 27th annual conference on Computer graphics and interactive techniques</i>, pages 385--392, New York, NY, USA, 2000. ACM Press/Addison-Wesley Publishing Co.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>601712</ref_obj_id>
				<ref_obj_pid>601671</ref_obj_pid>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[Eric B. Lum, Kwan-Liu Ma, and John Clyne. Texture hardware assisted rendering of time-varying volume data. In <i>Proceedings IEEE Visualization 2001</i>, pages 263--270, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614530</ref_obj_id>
				<ref_obj_pid>614287</ref_obj_pid>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[Eric B. Lum, Kwan-Liu Ma, and John Clyne. A hardware-assisted scalable solution for interactive volume rendering of time-varying data. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 8:286--298, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1176168</ref_obj_id>
				<ref_obj_pid>1175891</ref_obj_pid>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[Claes Lundstr&#246;m, Patric Ljung, and Anders Ynnerman. Local histograms for design of transfer functions in direct volume rendering. <i>Transactions on Visualization and Computer Graphics</i>, 12(6):1570--1579, Nov.-Dec. 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[Gerd Marmitt, Heiko Friedrich, and Philipp Slusallek. Interactive Volume Rendering with Ray Tracing. In <i>Eurographics State of the Art Reports</i>, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614298</ref_obj_id>
				<ref_obj_pid>614258</ref_obj_pid>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[Nelson Max. Optical models for direct volume rendering. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 1(2):99--108, June 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614298</ref_obj_id>
				<ref_obj_pid>614258</ref_obj_pid>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[Nelson Max. Optical models for direct volume rendering. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 1(2):99--108, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2386435</ref_obj_id>
				<ref_obj_pid>2386410</ref_obj_pid>
				<ref_seq_no>75</ref_seq_no>
				<ref_text><![CDATA[J&#246;rg Mensmann, Timo Ropinski, and Klaus Hinrichs. Accelerating Volume Raycasting using Occlusion Frustum. In <i>IEEE/EG International Symposium on Volume and Point-Based Graphics</i>, pages 147--154, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>76</ref_seq_no>
				<ref_text><![CDATA[Ky Giang Nguyen and Dietmar Saupe. Rapid high quality compression of volume data for visualization. <i>Computer Graphics Forum</i>, 20(3), 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614435</ref_obj_id>
				<ref_obj_pid>614275</ref_obj_pid>
				<ref_seq_no>77</ref_seq_no>
				<ref_text><![CDATA[Steven Parker, Michael Parker, Yarden Livnat, Peter-Pike Sloan, Charles Hansen, and Peter Shirley. Interactive ray tracing for volume visualization. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 5(3):238--250, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>288266</ref_obj_id>
				<ref_obj_pid>288216</ref_obj_pid>
				<ref_seq_no>78</ref_seq_no>
				<ref_text><![CDATA[Steven Parker, Peter Shirley, Yarden Livnat, Charles Hansen, and Peter-Pike Sloan. Interactive ray tracing for isosurface rendering. In <i>Proceedings of IEEE Visualization '98</i>. IEEE-CS, ACM, October 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>789820</ref_obj_id>
				<ref_obj_pid>789088</ref_obj_pid>
				<ref_seq_no>79</ref_seq_no>
				<ref_text><![CDATA[A. Patra and M. D. Wang. Volumetric medical image compression and reconstruction for interactive visualization in surgical planning. In <i>Proceedings Data Compression Conference 2003</i>, page 442, March 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2386420</ref_obj_id>
				<ref_obj_pid>2386410</ref_obj_pid>
				<ref_seq_no>80</ref_seq_no>
				<ref_text><![CDATA[Eric Penner and Ross Mitchell. Isosurface Ambient Occlusion and Soft Shadows with Filterable Occlusion Maps. In <i>IEEE/EG International Symposium on Volume and Point-Based Graphics</i>, pages 57--64, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>975275</ref_obj_id>
				<ref_seq_no>81</ref_seq_no>
				<ref_text><![CDATA[Matt Pharr and Greg Humphries. <i>Physically Based Rendering</i>. Morgan Kauffman, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37435</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>82</ref_seq_no>
				<ref_text><![CDATA[William T. Reeves, David H. Salesin, and Robert L. Cook. Rendering antialiased shadows with depth maps. In <i>SIGGRAPH '87: Proceedings of the 14th annual conference on Computer graphics and interactive techniques</i>, pages 283--291. ACM Press, 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>769948</ref_obj_id>
				<ref_obj_pid>769922</ref_obj_pid>
				<ref_seq_no>83</ref_seq_no>
				<ref_text><![CDATA[S. Roettger, S. Guthe, D. Weiskopf, and T. Ertl. Smart hardware-accelerated volume rendering. In <i>Procceedings of EG/IEEE TCVG Symposium on Visualization VisSym '03</i>, pages 231--238, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>84</ref_seq_no>
				<ref_text><![CDATA[Timo Ropinski, Jens Kasten, and Klaus H. Hinrichs. Efficient Shadows for GPU-based Volume Raycasting. In <i>Proceedings of the 16th International Conference in Central Europe on Computer Graphics, Visualization (WSCG08)</i>, pages 17--24, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>85</ref_seq_no>
				<ref_text><![CDATA[Timo Ropinski, Jennis Meyer-Spradow, Stefan Diepenbrock, J&#246;rg Mensmann, and Klaus H. Hinrichs. Interactive Volume Rendering with Dynamic Ambient Occlusion and Color Bleeding. <i>Computer Graphics Forum (Eurographics 2008)</i>, 27(2):567--576, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2384104</ref_obj_id>
				<ref_obj_pid>2384060</ref_obj_pid>
				<ref_seq_no>86</ref_seq_no>
				<ref_text><![CDATA[Stefan R&#246;ttger, Michael Bauer, and Marc Stamminger. Spatialized transfer functions. In <i>Euro Vis</i>, pages 271--278, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2386429</ref_obj_id>
				<ref_obj_pid>2386410</ref_obj_pid>
				<ref_seq_no>87</ref_seq_no>
				<ref_text><![CDATA[Marc Ruiz, Imma Boada, Ivan Viola, Stefan Bruckner, Miquel Feixas, and Mateu Sbert. Obscurance-based Volume Rendering Framework. In <i>IEEE/EG International Symposium on Volume and Point-Based Graphics</i>, pages 113--120, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1338590</ref_obj_id>
				<ref_obj_pid>1338439</ref_obj_pid>
				<ref_seq_no>88</ref_seq_no>
				<ref_text><![CDATA[C. Rezk Salama. GPU-Based Monte-Carlo Volume Raycasting. In <i>Proc. Pacific Graphics</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1080426</ref_obj_id>
				<ref_obj_pid>1080402</ref_obj_pid>
				<ref_seq_no>89</ref_seq_no>
				<ref_text><![CDATA[Mirko Sattler, Ralf Sarlette, Thomas M&#252;cken, and Reinhard Klein. Exploitation of human shadow perception for fast shadow rendering. In <i>APGV '05: Proceedings of the 2nd symposium on Applied perception in graphics and visualization</i>, pages 131--134. ACM Press, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2384845</ref_obj_id>
				<ref_obj_pid>2384796</ref_obj_pid>
				<ref_seq_no>90</ref_seq_no>
				<ref_text><![CDATA[H. Scharsach, M. Hadwiger, A. Neubauer, S. Wolfsberger, and K. B&#252;hler. Perspective Isosurface and Direct Volume Rendering for Virtual Endoscopy Applications. In <i>Proceedings of Eurovis '06</i>, pages 315--323, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>91</ref_seq_no>
				<ref_text><![CDATA[Henning Scharsach. Advanced GPU raycasting. In <i>Proceedings of the 9th Central European Seminar on Computer Graphics</i>, May 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1081483</ref_obj_id>
				<ref_obj_pid>1081432</ref_obj_pid>
				<ref_seq_no>92</ref_seq_no>
				<ref_text><![CDATA[Jens Schneider and R&#252;diger Westermann. Compression domain volume rendering. In <i>Proceedings IEEE Visualization 2003</i>, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882281</ref_obj_id>
				<ref_obj_pid>1201775</ref_obj_pid>
				<ref_seq_no>93</ref_seq_no>
				<ref_text><![CDATA[Peter-Pike Sloan, Jesse Hall, John Hart, and John Snyder. Clustered principal components for precomputed radiance transfer. In <i>SIGGRAPH '03: ACM SIGGRAPH 2003 Papers</i>, pages 382--391. ACM Press, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073335</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>94</ref_seq_no>
				<ref_text><![CDATA[Peter-Pike Sloan, Ben Luna, and John Snyder. Local, deformable precomputed radiance transfer. In <i>SIGGRAPH '05: ACM SIGGRAPH 2005 Papers</i>, pages 1216--1224. ACM Press, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>95</ref_seq_no>
				<ref_text><![CDATA[Irwin Edward Sobel. <i>Camera models and machine perception</i>. PhD thesis, Stanford University, Stanford, CA, USA, 1970.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>197949</ref_obj_id>
				<ref_obj_pid>197938</ref_obj_pid>
				<ref_seq_no>96</ref_seq_no>
				<ref_text><![CDATA[Lisa M. Sobierajski and Arie E. Kaufman. Volumetric ray tracing. In <i>VVS '94: Proceedings of the 1994 symposium on Volume Visualization '94</i>, pages 11--18. ACM Press, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2386498</ref_obj_id>
				<ref_obj_pid>2386472</ref_obj_pid>
				<ref_seq_no>97</ref_seq_no>
				<ref_text><![CDATA[S. Stegmaier, M. Strengert, T. Klein, and T. Ertl. A simple and flexible volume rendering framework for graphics-hardware--based raycasting. In <i>Proceedings of the International Workshop on Volume Graphics '05</i>, pages 187--195, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1081491</ref_obj_id>
				<ref_obj_pid>1081432</ref_obj_pid>
				<ref_seq_no>98</ref_seq_no>
				<ref_text><![CDATA[A. James Stewart. Vicinity shading for enhanced perception of volumetric data. In <i>VIS '03: Proceedings of the 14th IEEE Visualization 2003 (VIS'03)</i>, page 47. IEEE Computer Society, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>99</ref_seq_no>
				<ref_text><![CDATA[Wim Sweldens. The lifting scheme: A custom-design construction of biorthogonal wavelets. <i>Journal of Applied and Computational Harmonic Analysis</i>, (3):186--200, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>100</ref_seq_no>
				<ref_text><![CDATA[Martin Vetterli and Didier LeGall. Perfect reconstruction FIR filter banks: some properties and factorizations. <i>IEEE Transactions on Acoustics, Speech, and Signal Processing</i>, 37(7):1057--1071, July 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>101</ref_seq_no>
				<ref_text><![CDATA[Joachim E. Vollrath, Tobias Schafhitzel, and Thomas Ertl. Employing complex GPU data structures for the interactive visualization of adaptive mesh refinement data. In <i>Proceedings Eurographics/IEEE Workshop on Volume Graphics 2006</i>, pages 55--58, 136, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1080030</ref_obj_id>
				<ref_obj_pid>1079840</ref_obj_pid>
				<ref_seq_no>102</ref_seq_no>
				<ref_text><![CDATA[Ingo Wald, Heiko Friedrich, Gerd Marmitt, and Hans-Peter Seidel. Faster isosurface ray tracing using implicit kd-trees. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 11(5):562--572, 2005. Member-Philipp Slusallek.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>581899</ref_obj_id>
				<ref_obj_pid>581896</ref_obj_pid>
				<ref_seq_no>103</ref_seq_no>
				<ref_text><![CDATA[Ingo Wald, Thomas Kollig, Carsten Benthin, Alexander Keller, and Philipp Slusallek. Interactive global illumination using fast ray tracing. In <i>EGRW '02: Proceedings of the 13th Eurographics workshop on Rendering</i>, pages 15--24, Aire-la-Ville, Switzerland, Switzerland, 2002. Eurographics Association.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>353889</ref_obj_id>
				<ref_obj_pid>353888</ref_obj_pid>
				<ref_seq_no>104</ref_seq_no>
				<ref_text><![CDATA[M. Weiler, R. Westermann, C. Hansen, K. Zimmerman, and T. Ertl. Level-Of-Detail Volume Rendering via 3D Textures. In <i>Proceedings of IEEE Symposium on Volume Visualization</i>, pages 7--13, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>353889</ref_obj_id>
				<ref_obj_pid>353888</ref_obj_pid>
				<ref_seq_no>105</ref_seq_no>
				<ref_text><![CDATA[Manfred Weiler, R&#252;diger Westermann, Chuck Hansen, Kurt Zimmerman, and Thomas Ertl. Level--of--detail volume rendering via 3d textures. In <i>Proceedings IEEE Volume Visualization and Graphics Symposium 2000</i>, pages 7--13. ACM Press, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>197963</ref_obj_id>
				<ref_obj_pid>197938</ref_obj_pid>
				<ref_seq_no>106</ref_seq_no>
				<ref_text><![CDATA[R&#252;diger Westermann. A multiresolution framework for volume rendering. In <i>1994 Symposium on Volume Visualization</i>, October 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1071874</ref_obj_id>
				<ref_obj_pid>1071866</ref_obj_pid>
				<ref_seq_no>107</ref_seq_no>
				<ref_text><![CDATA[G. Wetekam, D. Staneker, U. Kanus, and M. Wand. A hardware architecture for multi-resolution volume rendering. In <i>Proceedings ACM SIGGRAPH/Eurographics Conference on Graphics Hardware</i>, pages 45--51, New York, NY, USA, 2005. ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>130882</ref_obj_id>
				<ref_obj_pid>130881</ref_obj_pid>
				<ref_seq_no>108</ref_seq_no>
				<ref_text><![CDATA[Jane Wilhelms and Allen Van Gelder. Octrees for faster isosurface generation. <i>ACM Transactions on Graphics</i>, 11:201--227, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>807402</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>109</ref_seq_no>
				<ref_text><![CDATA[Lance Williams. Casting curved shadows on curved surfaces. In <i>SIGGRAPH '78: Proceedings of the 5th annual conference on Computer graphics and interactive techniques</i>, pages 270--274. ACM Press, 1978.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>288186</ref_obj_id>
				<ref_obj_pid>288126</ref_obj_pid>
				<ref_seq_no>110</ref_seq_no>
				<ref_text><![CDATA[C. M. Wittenbrink, T. Malzbender, and M. E. Goss. Opacity-weighted color interpolation for volume sampling. In <i>Proceedings of IEEE Symposium on Volume Visualization</i>, pages 135--142, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1110751</ref_obj_id>
				<ref_obj_pid>1110642</ref_obj_pid>
				<ref_seq_no>111</ref_seq_no>
				<ref_text><![CDATA[Chris Wyman, Steven Parker, Charles Hansen, and Peter Shirley. Interactive display of isosurfaces with global illumination. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 12(2):186--196, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614293</ref_obj_id>
				<ref_obj_pid>614257</ref_obj_pid>
				<ref_seq_no>112</ref_seq_no>
				<ref_text><![CDATA[Boon-Lock Yeo and Bede Liu. Volume rendering of DCT-based compressed 3d scalar data. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 1:29--43, March 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1252510</ref_obj_id>
				<ref_obj_pid>1251973</ref_obj_pid>
				<ref_seq_no>113</ref_seq_no>
				<ref_text><![CDATA[C. Zhang, D. Xue, and R. Crawfis. Light propagation for mixed polygonal and volumetric data. In <i>CGI '05: Proceedings of the Computer Graphics International 2005</i>, pages 249--256, Washington, DC, USA, 2005. IEEE Computer Society.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>776765</ref_obj_id>
				<ref_obj_pid>776751</ref_obj_pid>
				<ref_seq_no>114</ref_seq_no>
				<ref_text><![CDATA[Caixia Zhang and Roger Crawfis. Shadows and soft shadows with participating media using splatting. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 9(2):139--149, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Course Notes Advanced Illumination Techniques for GPU-Based Volume Raycasting Markus Hadwiger VRVis 
Research Center, Vienna, Austria Patric Ljung Siemens Corporate Research, Princeton, USA Christof Rezk 
Salama University of Siegen, Germany Timo Ropinski University of M¨unster, Germany  Advanced Illumination 
Techniques for GPU Volume Raycasting Abstract Volume raycasting techniques are important for both visual 
arts and visualization. They allow an e.cient generation of visual e.ects and the visualization of scienti.c 
data obtained by tomography or numeri­cal simulation. Thankstotheir .exibility,expertsagreethatGPU-based 
raycastingis the state-of-the art techniqueforinteractive volume render­ing. It will most likely replace 
existing slice-based techniques in the near future. Volume rendering techniques are also e.ective for 
the direct ren­dering ofimplicit surfaces used for softbody animation and constructive solid geometry. 
The lecture starts o. with an in-depth introduction to the concepts behind GPU-based ray-casting to provide 
a common base for the fol­lowing parts. The focus of this course is on advanced illumination tech­niques 
which approximate thephysically-based light transport more con­vincingly. Such techniques include interactive 
implementation of soft and hard shadows, ambient occlusion and simple Monte-Carlo based ap­proaches to 
global illumination including translucency and scattering. With the proposed techniques, users are able 
to interactively create convincing images from volumetric data whose visual quality goes far beyond traditional 
approaches. The optical properties in participating media arede.ned using thephasefunction. Many approximations 
to the physicallybasedlighttransportappliedfor rendering naturalphenomena such as clouds or smoke assume 
a rather homogenous phase function model. For rendering volumetric scans on the other handdi.erentphase 
function models are required to account for both surface-like structures and fuzzy boundaries in the 
data. Using volume rendering techniques, artists who create medical visualization for science magazines 
may now work ontomographicscansdirectly, withoutthenecessity tofallback to creating polygonal models 
of anatomical structures. Course participants shouldhave a workingknowledge in computer graph­ics, basic 
programming skills. They should be familiar with graphics hardware and shading languages. We will assume 
a basic knowledge re­garding volume data as well as interactive volume rendering techniques. Prerequisites 
Intermediate The course targets the steadily growing number of devel­opers who create specialized implementations 
of volume rendering tech­niques on state-of-the-art graphics hardware, regardless of whether they are 
working in visual arts or scienti.c visualization. Level of Di.culty Contact Christof Rezk Salama Timo 
Ropinski (course organizer) Visualization and Computer Computer Graphics Group Graphics Research Group, 
University of Siegen University of M¨unster H¨olderlinstr. 3 Einsteinstr. 62 57068 Siegen, Germany 48149 
M¨unster, Germany email: rezk@fb12.uni-siegen.de Markus Hadwiger Patric Ljung VRVis Research Center for 
Siemens Corporate Research Virtual Reality and Visualization Imaging &#38; Visualization Department Donau-City-Straße 
1 755 College Road East A-1220 Vienna, Austria Princeton, NJ 08540 email: msh@vrvis.at email: patric.ljung@siemens.com 
 Lecturers Markus Hadwiger VRVis Research Center for Virtual Reality and Visualization Donau-City-Strasse 
1, A-1220 Vienna, Austria email: msh@vrvis.at Markus Hadwiger is a senior researcher at the VRVis Research 
Center in Vienna, Austria. He received his Ph.D. in computer science from the Vienna University of Technology 
in 2004, and has been a researcher at VRVis since 2000, working in the areas of visualization, volumerendering, 
andgeneralGPU techniques. Hehasbeeninvolvedin several courses and tutorials about volume rendering and 
visualization atACMSIGGRAPH,IEEEVisualization, andEurographics. Heis a co­author ofthebook Real-TimeVolumeGraphicspublishedbyAKPeters. 
Patric Ljung Department of Imaging and Visualization Siemens Corporate Research 755 College Road East 
Princeton, NJ 08540, U.S.A. email: patric.ljung@siemens.com Patric Ljung joined in 2007Siemens CorporateResearch 
inPrince­ton, NJ, where he works as a Research Scientist in the Imaging Ar­chitectures group. He received 
2006 his PhD in Scienti.c Visualization from Link¨oping University, Sweden and graduated with honors 
in 2000 his MS in Information Technology from Link¨oping University. Between 1989and1995he worked as 
a software engineer with embedded and tele­com systems involving software architectures, graphical user 
interfaces, voice-mail systems, communication protocols, network and interprocess communication, compilers. 
Dr. Ljung has published several papers in international conferences and journals including IEEE Visualization, 
Eurographics conferences, IEEE TVCG and others, on volume rendering of large medical data sets, GPU-based 
raycasting of multiresolution data sets. One important focus areahasbeenVirtualAutopsiesforforensicpathology. 
His current research interest is in advanced illumination and shading techniques, software architectures 
for extensible graphics, and management and rendering of large medical data sets. Timo Ropinski Visualization 
andComputerGraphicsResearchGroup(VisCG) University of M¨unster Einsteinstr. 62 48149 M¨unster, Germany 
email: ropinski@math.uni-muenster.de Timo Ropinski is a postdoctoral researcher working in the .eld of 
medical volume visualization. After receiving his PhD in 2004 from the University of M¨unster, he became 
a project leader within the collabo­rative research center SFB 656, a cooperation between researchers 
from medicine, mathematics, chemistry, physics and computer science. His researchisfocused oninteractive 
aspectsin medical volume visualization with the goal to make these techniques more accessible. He is 
initiator of the Voreen open sourceproject(www.voreen.org), in whicha .exible volume rendering framework 
is developed. The results of his scienti.c work have been published in various international conferences 
including Eurographics, IEEE Visualization, IEEE VR, VMV and others. Christof Rezk Salama Computergraphik 
und Multimediasysteme, University of Siegen, H¨olderlinstr. 3, 57068 Siegen, Germany phone: +49 271-740-3315 
fax: +49 271-740-3337 email: rezk@fb12.uni-siegen.de Christof Rezk-Salama has received a PhD from the 
University of Erlangen-Nuremberg as a scholarship holder of the graduate college 3D Image Analysis and 
Synthesis. He has worked as a research engineer for the R&#38;D department of Siemens Medical Solutions. 
Since October 2003 heis working asanassistantprofessor attheComputerGraphicsGroup of the University of 
Siegen, Germany. The results of his research have been presented at international con­ferences, including 
ACM SIGGRAPH, IEEE Visualization, Eurograph­ics, MICCAI and Graphics Hardware. He is regularly holding 
lectures, courses and seminars on computergraphics, scienti.c visualization, char­acter animation and 
graphics programming. He has gained practical experience in applying computer graphics to several scienti.c 
projects in medicine, geology and archaeology. Christof Rezk-Salama has released the award winning open-source 
project OpenQVis and is a co-author of the book Real-Time Volume Graphics. Detailed information about 
this research projects are available at: http://www.cg.informatik.uni-siegen.de/People/Rezk http://www.real-time-volume-graphics.org 
http://openqvis.sourceforge.net  Course Syllabus The half-day course will consist of four di.erent 
blocks. From a didactic point of view, each block will loosely build upon the information pro­vided in 
previous blocks with growing complexity and increasing level of di.culty. The schedule is only tentative, 
since at the time of writing these notes, the .nal time slots have not yet been allocated by the organizers. 
MORNING Introduction and Basics [45 min] (M. Hadwiger) Introduction and Basics Application Areas for 
Volume Rendering Bene.ts and Drawbacks of Ray-Casting GPU-based Volume Ray-Casting Space Leaping and 
Early Ray Termination Memory Management Multiresolution LOD and Adaptive sampling 1:45pm 2.30pm Light 
Interaction [45 min] (T. Ropinski) Light Transport and Illumination Models Local Volume Illumination 
 Specular Re.ection through Ray-Tracing Soft vs. Hard Shadows Semi-Transparent Shadows with Deep Shadow 
Maps 2:30pm 3:15pm BREAK [15 min] 3:15pm 3:30pm 3:30pm 4:15pm Ambient Occlusion [45 min] (P. Ljung) 
 Ambient Occlusion for Isosurfaces Deep Shadow Maps Local Ambient Occlusion (DVR) Dynamic Ambient 
Occlusion (DVR) 4:15pm­5:15pm Scattering [60 min] (C. Rezk-Salama) Single and Multiple Scattering Transparency 
and Translucency Monte-Carlo integration GPU-Based Importance Sampling GPU-Based Monte-Carlo Volume 
Raycasting Scattering with Deep Shadow Maps 5:15pm­5:30pm Discussion, Questions and Answers [15min] 
(all speakers)  Contents I GPU-Based Ray Casting 1 1 Introduction 2 1.1 VolumeData ........................ 
3 1.2 DirectVolumeRendering .................. 4 1.2.1 OpticalModels ................... 5 1.2.2 The 
Volume Rendering Integral . . . . . . . . . . 6 1.2.3 Ray Casting ..................... 8 1.2.4 AlphaBlending 
................... 10 2 GPU-based Volume Ray Casting 12 2.1 BasicRay Casting ..................... 
13 2.2 Object-Order Empty Space Skipping . . . . . . . . . . . 15 2.3 AdvancedRay CastingPipeline . 
. . . . . . . . . . . . . 17 2.3.1 Culling and Brick Boundary Rasterization . . . . 20 2.3.2 Geometry 
Intersection ............... 23 2.4 IsosurfaceRay Casting ................... 24 2.4.1 AdaptiveSampling 
................. 25 2.4.2 IntersectionRe.nement . . . . . . . . . . . . . . 28 2.5 Memory Management 
.................... 28  2.6 Mixed-Resolution Volume Rendering . . . . . . . . . . . 30 2.6.1 Volume 
Subdivision for Texture Packing . . . . . 31 2.6.2 Mixed-Resolution Texture Packing . . . . . . . . 32 
2.6.3 AddressTranslation ................ 33  2.7 Multiresolution LOD and Adaptive sampling . . . . 
. . . 35 2.7.1 Octree-based Multiresolution Representation . . . 35 2.7.2 Block Properties and Acceleration 
Structures . . 36 2.7.3 Hierarchical Multiresolution Representations . . . 37 2.8 Level-of-DetailManagement 
................ 38  2.8.1 View-Dependent Approaches . . . . . . . . . . . . 39 2.8.2 Data Error Based 
Approaches . . . . . . . . . . . 39 2.8.3 Transfer Function Based Approaches . . . . . . . 40  2.9 
Encoding,Decoding andStorage . . . . . . . . . . . . . . 40 2.9.1 Transform and Compression Based Techniques 
. . 41 2.9.2 Out-of-Core Data Management Techniques . . . . 43 2.9.3 Flat blocking Multiresolution Representation 
. . . 44  2.10 Sampling of Multiresolution Volumes . . . . . . . . . . . 46 2.10.1 NearestBlockSampling 
. . . . . . . . . . . . . . 47 2.10.2 Interblock Interpolation Sampling . . . . . . . . . 48 2.10.3 Interblock 
Interpolation Results . . . . . . . . . . 50 2.11Raycasting ontheGPU .................. 51  2.11.1 Adaptive 
Object-Space Sampling . . . . . . . . . 51 2.11.2FlatBlocking Summary . . . . . . . . . . . . . . 53 
II Light Interaction 55 3 Light Transport and Illumination Models 56 3.1 Phong Illumination ..................... 
56 3.2 GradientComputation ................... 59 3.3 Specular Re.ections through Ray-Tracing . . . 
. . . . . 61 4 Shadows 68 4.1 Softvs. HardShadows ................... 68 4.2 Semi-Transparent Shadows 
with Deep Shadow Maps . . 70 III Ambient Occlusion 77 5 Ambient Occlusion for Isosurfaces 78 6 Ambient 
Occlusion for Direct Volume Rendering 80 6.1 LocalAmbientOcclusion.................. 80 6.1.1 Emissive 
Tissues and Local Ambient Occlusion . 82 6.1.2 Integrating Multiresolution Volumes . . . . . . . 82 6.1.3 
Adding Global Light Propagation . . . . . . . . . 84 6.2 DynamicAmbientOcclusion................ 85 
  6.2.1 Local Histogram Generation . . . . . . . . . . . . 87 IV Volume Scattering 99 7 Scattering 
E.ects 100 7.1 PhysicalBackground .................... 100 7.2 Scattering .......................... 
101 7.3 SingleScattering ...................... 101 7.4 Indirect Illumination and Multiple Scattering 
. . . . . . 103 7.4.1 IndirectLight .................... 103 7.4.2 Transparency and Translucency . . 
. . . . . . . . 104 7.4.3 PhaseFunctions. . . . . . . . . . . . . . . . . . . 105 7.4.4 Scattering at 
Transparent Surfaces . . . . . . . . 106  7.5 APracticalPhaseFunctionModel . . . . . . . . . . . . 108 
7.6 FurtherReading . . . . . . . . . . . . . . . . . . . . . . . 109 8 Monte-Carlo Intergration 110 8.1 
NumericalIntegration ................... 110 8.1.1 Blind Monte-Carlo Integration . . . . . . . . . . 
. 110 8.2 When Does Monte-Carlo Integration Make Sense? . . . . 112 8.3 ImportanceSampling . . . . . 
. . . . . . . . . . . . . . . 114 8.4 GPU-based Importance Sampling . . . . . . . . . . . . . 116 8.4.1 
Focussing of Uniform Distribution . . . . . . . . . 116 8.4.2 Sampling of Re.ection MIP-Maps . . . . 
. . . . . 118  8.5 FurtherReading . . . . . . . . . . . . . . . . . . . . . . . 121 9 GPU-Based Monte-Carlo 
Volume Raycasting 123 9.1 Monte-Carlo Techniques for Isosurfaces . . . . . . . . . . 123 9.2 Isosurfaces 
with Shift-Variant or Anisotropic BRDFs . . 125 9.2.1 FirstHitPass .................... 125 9.2.2 DeferredShading 
Pass ............... 128 9.2.3 Deferred Ambient Occlusion Pass . . . . . . . . . 130 9.3 VolumeScattering 
..................... 133    9.3.1 Heuristic Simpli.cations . . . . . . . . . . . . . . 137 10 Light 
Map Approaches 140 Course Notes Advanced Illumination Techniques for GPU Volume Raycasting  GPU-Based 
Ray Casting Markus Hadwiger VRVis Research Center, Vienna, Austria Patric Ljung Siemens Corporate Research, 
Princeton, USA Christof Rezk Salama University of Siegen, Germany Timo Ropinski University of M¨unster, 
Germany  Introduction In traditional modeling, 3D objects are created using surface representa­tionssuch 
aspolygonal meshes,NURBSpatchesorsubdivision surfaces. Inthetraditional modelingparadigm,visualproperties 
of surfaces, such as color, roughness and re.ectance, are modeled by means of a shading algorithm, which 
might be as simple as the Phong model or as complex as a fully-featured shift-variant anisotropic BRDF. 
Since light transport is evaluated only at points on the surface, these methods usually lack the ability 
to account for light interaction which is taking place in the atmosphere or in the interior of an object. 
Contrary to surface rendering, volume rendering[60,23] describes a wide range of techniques for generating 
images from three-dimensional scalar data. These techniques are originally motivated by scienti.c visu­alization, 
where volume data is acquired by measurement or numerical simulation of natural phenomena. Typical examples 
are medical data of the interior of the human body obtained by computed tomography (CT) or magnetic resonanceimaging(MRI). 
Other examples are com­putational .uid dynamics (CFD), geological and seismic data, as well as abstract 
mathematical data such as 3D probability distributions of pseudo random numbers. With the evolution of 
e.cient volume rendering techniques, volumet­ric data is becoming more and more important also for visual 
arts and computer games. Volume data is ideal to describe fuzzy objects, such as .uids, gases and natural 
phenomena like clouds, fog, and .re. Many artistsand researchershavegenerated volumedatasynthetically 
tosup­plement surface models,i.e.,procedurally[24], whichis especially useful for rendering high-quality 
special e.ects. Although volumetric data are more di.cult to visualize than sur­faces,itisboth worthwhile 
and rewarding torenderthemastruly three­dimensional entities without falling back to 2D subsets. 1.1 
Volume Data A discrete volume data set can be thought of as a simple three­dimensional array of cubic 
elements (voxels1)[49], each representing a unit of space(Figure1.1). Althoughimagining voxels as tiny 
cubesis easy and mighthelp to vi­sualize theimmediate vicinity ofindividual voxels, itis more appropriate 
to identify each voxel with a sample obtained at a single in.nitesimally small point from a continuous 
three-dimensional signal f(x). IR with x . IR3 . (1.1) Provided that the continuous signal is band-limited 
with a cut-o.­frequency .s, sampling theory allows the exact reconstruction, if the signal is evenly 
sampled at more than twice the cut-o.-frequency, i.e., theNyquist rate. However, there are two majorproblems 
whichprohibit the ideal reconstruction of sampled volume data in practice. Ideal reconstruction according 
to sampling theory requires the con­volution of the samplepoints with a sinc function(Figure 1.2a) in 
the spatialdomain. For the one-dimensional case, the sincfunction is: sin(px) sinc(x)= . (1.2) px The 
three-dimensional version of this function is simply obtained by tensor-product. Note that this function 
has in.nite extent. Thus, for an exact reconstruction of the original signal at an arbi­traryposition 
all thesamplingpointsmustbeconsidered, not only 1volume elements those in a local neighborhood. This 
turns out to be computation­ally intractable in practice. Real-life data in general does not represent 
a band-limited signal. Any sharp boundary between di.erent materials represents a step function which 
has in.nite extent in the frequency domain. Sam­pling and reconstruction of a signal which is not band-limited 
will produce aliasing artifacts. In order to reconstruct a continuous signal from an array of voxels 
in practice, the ideal 3D sinc .lter is usually replaced by either a box .l­ter(Figure1.2a) oratent .lter(Figure1.2b). 
Thebox .ltercalculates nearest-neighbor interpolation, which results in sharp discontinuities be­tween 
neighboring cells and a rather blocky appearance. Trilinear in­terpolation, which is achieved by convolution 
with a 3D tent .lter, rep­resents a good trade-o. between computational cost and smoothness of the output 
signal. 1.2 Direct Volume Rendering In comparison to the indirect methods, which try to extract a surface 
descriptionfrom the volumedatain apreprocessing step,direct methods displaythe voxeldatabyevaluating 
an optical model whichdescribeshow the volume emits, re.ects, scatters, absorbsand occludeslight[73]. 
The scalar value is virtually mapped to physical quantities which describe light interaction at the respective 
point in 3D space. This mapping is often called classi.cation andis usuallyperformedby means of a transfer 
function. The physical quantities are then used for images synthesis. Di.erent optical models for direct 
volume rendering are described in section 1.2.1. During image synthesis, the light propagation is computed 
by inte­grating light interaction e.ects along viewing rays based on the optical model. The corresponding 
integral is known as the volume rendering integral, which is described in section 1.2.2. Naturally, under 
real-world conditions this integral is solved numerically. Furthermore, the volume can be shaded according 
to the illumination from external light sources. 1.2.1 Optical Models Almost every direct volume rendering 
algorithm regards the volume as adistribution oflight-emittingparticles of a certaindensity. Theseden­sities 
are more or less directly mapped to RGBA quadruplets for com­positing along viewing rays. Thisprocedure,however,is 
motivatedby a physically-based optical model. The most important optical models for direct volume rendering 
are described in a survey paper by Nelson Max [73], and we only brie.y summarize these models here: 
Absorption only. The volume is assumed to consist of cold, perfectly black particles that absorb all 
the light that impinges on them. They do not emit, or scatter light.  Emission only. The volume is assumed 
to consist of particles that only emit light, but do not absorb any, since the absorption is negligible. 
 Absorption plus emission. This optical modelisthemost com­mon one in direct volume rendering. Particles 
emit light, and oc­clude, i.e., absorb, incoming light. However, there is no scattering or indirect illumination. 
 Scattering and shading/shadowing. This modelincludes scat­tering ofilluminationthatisexternal toavoxel. 
Lightthatisscat­tered can either be assumed to impinge unimpeded from a distant light source, or it can 
be shadowed by particles between the light and the voxel under consideration.  Multiple scattering. 
This sophisticated model includes support for incident light that has already been scattered by multiple 
par­ticles before it is scattered toward the eye.  The volume renderingintegral describedin thefollowing 
section assumes the simple emission-absorption optical model. More sophisticatedmodels including shadowing 
and self-shadowing, and single and multiple scat­tering are covered in later parts of these notes. Figure 
1.3 illustrates GPU-based ray casting with the emission­absorption model with and without shading, as 
well as a combination with semi-transparent isosurface rendering. Figure 1.4 illustrates the addition 
of shadows, i.e., the (partial) occlusion of impinging external light via the absorption occuring within 
the volume. 1.2.2 The Volume Rendering Integral Everyphysically-based volume rendering algorithm evaluates 
the volume rendering integral in one way or the other, even if viewing rays are not employed explicitly 
by the algorithm. The most basic, but also most .exbile, volume rendering algorithm is ray casting, which 
is introduced in Section 1.2.3. It might be considered as the most direct numerical method for evaluating 
this integral. More details are covered later on, but for this section it su.ces to view ray casting 
as a process that, for eachpixel in theimage to render, casts a single rayfrom the eye through the pixel 
s center into the volume, and integrates the optical properties obtained from the encountered volume 
densities along the ray. Note that this general description assumes both the volume and the mapping 
to optical properties to be continuous. In practice, of course, the volume data are discrete, and the 
evaluation of the integral is ap­proximated numerically. In combination with several additional simpli­.cations, 
the integral is usually substituted by a Riemann sum. We denote a ray cast into the volume by x(t), andparameterizeitby 
thedistance t from the eye. The scalar value corresponding to aposition () alongthe rayisdenotedby sx(t).Ifweemploytheemission-absorption 
model, the volume rendering equation integrates absorption coe.cients .(s) (accounting for the absorption 
of light), and emissive colors c(s) (accounting for radiant energy actively emitted) along a ray. To 
keep the equations simple, we denote emission c and absorption coe.cients . as function of the eye distance 
t instead of the scalar value s: (()) (()) c(t):= csx(t)and .(t):= .sx(t)(1.3) Figure 1.5 illustrates 
the idea of emission and absorption. An amount of radiant energy, which is emitted at a distance t = 
d along the viewing rayis continuously absorbed along thedistance d untilit reaches the eye. ' This means 
that only aportion cof the original radiant energy c emitted  Figure 1.5: An amount of radiant energy 
emitted at t = d is partially absorbed along the distance d. at t = d will eventually reach the eye. 
If there is a constant absorption . = const along the ray, c ' amounts to '-.d c = c · e. (1.4) However, 
if the absorption . is not constant along the ray, but itself dependent ontheposition,the amount of radiant 
energy c ' reaching the eye mustbe computedbyintegrating the absorption coe.cient along the distance 
d: c ' = c · e - d 0 .( t)d t . (1.5) The integral over the absorption coe.cients in the exponent, d2 
t(d1, d2)= .( t)d t (1.6) d1 is also calledthe opticaldepth. In this simple example,however,lightwas 
only emitted at a singlepoint along the ray. If we want to determine the total amount of radiant energy 
C reaching the eye from this direction, we must take into account the emitted radiant energy from all 
possible positions t along the ray: 8 -t(0,t) dt C =c(t)· e (1.7) 0 Inpractice, thisintegralis evaluated 
numerically through eitherfront-to­back orback-to-front compositing(i.e., alphablending) of samplesalong 
the ray, which is most easily illustrated in the method of ray casting. Ray casting usually employs front-to-back 
compositing. 1.2.3 Ray Casting Ray casting [60] is an image-order direct volume rendering algorithm, 
which uses straight-forward numerical evaluation of the volume render­ingintegral(Equation1.7). For eachpixel 
oftheimage, a single ray2 is cast into the scene. At equi-spaced intervals along the ray, the dis­crete 
volumedata are resampled, usually using tri-linearinterpolation as reconstruction .lter. Thatis,foreachresamplinglocation,thescalarval­ues 
of eight neighboring voxels are weighted according to their distance to the actuallocationfor which adata 
valueis needed. After resampling, the scalar data value is mapped to optical properties via a lookup 
ta­ble, whichyields anRGBAquadrupletthat subsumesthecorresponding emission and absorption coe.cients[60] 
forthislocation. The solution of the volume renderingintegralis then approximated via alphablending in 
either front-to-back or back-to-front order, where usually the former is used in ray casting. The optical 
depth t (Equation 1.6), which is the cumulative absorp­tion up to a certainposition x(t)along the ray, 
can be approximated by a Riemann sum lt/.tJ L t(0,t) t (0,t)= .(i · .t).t (1.8) i=0 with .t denoting 
the distance between successive resampling locations. The summation in the exponent can immediately be 
substituted by a multiplication of exponentiation terms: lt/.tJ I -t (0,t) -.(i·.t).t e = e (1.9) i=0 
Now, we can introduce the opacity A, which is well-known from tradi­tional alpha blending, by de.ning 
-.(i·.t).t Ai =1- e (1.10) and rewriting Equation 1.9 as: lt/dJ I -t (0,t) e = (1- Aj) (1.11) i=0 This 
allows the opacity Ai to be used as an approximation for the ab­sorption of the i-th ray segment,instead 
of absorption at a singlepoint. Similarly, the emitted color of the i-th ray segment can be approxi­mated 
by: Ci = c(i · .t).t (1.12) 2assuming super-sampling is not used for anti-aliasing Having approximated 
both the emissions and absorptions along a ray, we can now state the approximate evaluation of the volume 
rendering integral as: (denoting the number of samples by n = lT/dtJ) ni-1 LI C = Ci (1- Ai) (1.13) 
i=0 j=0 Equation1.13 canbe evaluatediteratively byperforming alpha blending in either front-to-back or 
back-to-front order. 1.2.4 Alpha Blending Equation1.13canbe computediterativelyinfront-to-back orderby 
step­ping i from 1 to n: Ci ' = Ci'-1 +(1 - Ai'-1)Ci (1.14) A ' = A ' +(1 - A ' )Ai (1.15) ii-1 i-1 New 
values Ci ' and A ' i are calculated from the color Ci and opacity Ai at the current location i, and 
the composited color Ci'-1 and opacity A ' i-1 from the previous location i - 1. The starting condition 
is C0 ' = 0 and A ' 0 =0. Note that in all blending equations, we are using opacity-weighted colors [110], 
which are also known as associated colors [7]. An opacity­weighted color is a color that has been pre-multiplied 
by its associated opacity. This is a very convenient notation, and especially important for interpolation 
purposes. It can be shown that interpolating color and opacity separately leads to artifacts, whereas 
interpolating opacity­weighted colors achieves correct results[110]. The following alternative iterative 
formulation evaluates Equa­tion 1.13 in back-to-front order by stepping i from n - 1 to 0: Ci ' = Ci 
+(1- Ai)Ci' +1 (1.16) A new value Ci ' is calculated from the color Ci and opacity Ai at the cur­rent 
location i, and the composite color Ci' +1 from the previous location i +1. The starting condition is 
Cn ' =0. Note that front-to-back compositing requires tracking alpha values, whereas back-to-front compositing 
does not. However, while this was a problem for hardware implementations several years ago, in current 
single-passimplementations ofGPU ray casting thisisnot aproblem at all. In multi-pass implementations, 
destination alpha mustbe supported by the frame bu.er for tracking the accumulation of opacity, i.e., 
an alpha value must be stored in the frame bu.er, and it must be possible to use it as a multiplication 
factor in blending operations. The major advantage offront-to-back compositing is an optimization called 
early ray termination, where the progression along a ray is termi­nated as soon as the cumulative alpha 
value reaches 1.0, or a su.ciently close value. In currentGPU architectures, thisis very easy toimplement 
by simply terminating the ray casting loop as soon as the accumulated alpha value exceeds a speci.ed 
threshold.  GPU-based Volume Ray Casting The basic idea of GPU-based ray casting is to store the entire 
vol­ume in a single 3D texture, and drive a fragment program that casts rays into the volume. Each pixel/fragment 
corresponds to a single ray x(t, x, y)= c+t d(x, y)in volume coordinates. Here, the normalized di­rection 
vector d(x, y)can either be computed from the camera position c and the screen space coordinates (x, 
y) of the pixel, or be obtained via rasterization [55]. In this section, we will use the approach build­ing 
on rasterization since it allows for very simple but e.cient empty space skipping, which is described 
in later sections. The range of depths [tstart(x, y),texit(x, y)]from where a rayenters the volume to 
where a ray exits the volumeis computedperframein a setup stagebefore the actual ray casting fragment 
program is executed. In the simplest case, tstart, or the corresponding 3D volume coordinates, are obtained 
by rasteriz­ing the front faces of the volume bounding box with the corresponding distance to the camera. 
Rendering the back faces of the bounding box yields the depths texit, or the corresponding 3D volume 
coordinates, of each ray exiting the volume.  Figure2.1illustrates this raysetup using rasterization. 
Asillustrated in Figure 2.2, ray entry positions are determined by the front faces of the volumeboundingbox(showninblue), 
and ray exitpositionsbyits backfaces(showningreen), respectively. Ray castingisperformedby sampling the 
space in-between, usually by using a constant sampling rate. On current GPUs, a single rendering pass 
and ray casting loop in the fragment program can be employed for casting through the volume in front-to-back 
order, building on the images illustrated in Figure 2.1 for ray setup, which yield exactly the setup 
positions needed by the ray caster(f0 -f4, and l0 -l4 in Figure 2.2). 2.1 Basic Ray Casting Figure2.3illustratesbasic 
raycasting with ray setup using rasterization. It consists of four principal stages: 1. Frontfacegeneration:Renderthefrontfacesof 
thevolumebound­ing boxto atexture(Figure2.1(left)). 2. Directiontexturegeneration:Renderthebackfaces 
of the volume bounding box (Figure 2.1 (center)), while subtracting the previ­ously generated coordinates 
of the front faces and storing the re­sulting ray vectors as normalized vectors in RGB, as well as their 
lengths in A, of a separate RGBA direction texture (Figure 2.1 (right)).  3. Ray casting: Get the starting 
position from the front face image and cast along the viewing vector until the rayhasleft the volume. 
Exiting the volume is determined by using the previously stored vector lengths. 4. Blending:Blendtheraycasting 
resulttothescreen,e.g.,composite it with the background.  Theonly expensive stageof thisalgorithmistheactual 
ray castingloop, which iteratively steps through the volume, sampling the 3D volume texture using tri-linear 
interpolation, applies the transfer function, and performs compositing. Ray setup via rasterization is 
several orders of magnitude faster with negligible performance impact, and thus no bot­tleneck. The .nalblending 
stageisnegligibleintermsofperformanceas well, or can even be skipped entirely if the ray casting pass 
is executed directly on the .nal output bu.er.  2.2 Object-Order Empty Space Skipping When we consider 
Figure 2.2, and imagine that the actually visible part of the volume does not .ll up the entire bounding 
box, we see that a lot of empty space will be sampled during ray casting if rays are started on the front 
faces of the volume bounding box. However, if we subdivide the volume into smaller blocks and determine 
for each of these blocks whether it is empty or not, we can rasterize the front faces of these smaller 
blocks instead of the entire bounding box. This can simply be achieved by rasterizing front and back 
faces of smaller blocks, resulting in ray setup images as shown in Figure 2.4, which already more closely 
resemblethevisiblepart ofthevolume(inthecaseofthis .gure,ahuman skull and spine). This is illustrated 
in Figure 2.5, where both the ray entry positions(f0 -f2)as well asthe ray exitpositions(l0 -l4)have 
been modi.ed via this rasterization of block bounding faces to be inside the volume bounding box and 
closer to the visible part of the volume. Figure 2.6 illustrates a potential performance problem of this 
ap­proach, whichoccurs when raysgrazethe volume early on,butdo nothit avisiblepart right away(right-hand 
sideof the .gure). Inthiscase,alot of empty space maybe traversed. However, this case usually occurs 
only for a smallnumber of rays, and canbehandledby combining object-order empty space skipping with regular(image-order) 
empty space skipping, i.e.,decidingin the raycastingfragmentprogram to skipindividual sam­ples or advancing 
the samplingposition along the ray by several samples at once. These two approaches for empty space skipping 
complement each other. Object-order empty space skippingisextremelyfast(with negligible overhead compared 
with no empty space skipping), it employs very simple andfast rasterization, and the ray castingfragmentprogram 
does not need to be modi.ed at all. It, however, in principle cannot skip all empty space. Image-order 
empty space skipping, on the other hand, either requires multiple ray castingpasses or mustperformchecks 
on essentially a per-sample basis, and thus is much more expensive. It, however, can skip additional 
empty space that would otherwise be sam­pled. Figure 2.7illustrates another example of object-order empty 
space skipping via rasterization of tight-.tting bounding geometry.   Figure 2.5 illustrates another 
important issue of ray setup, which is the handling of rays when the view point is inside the volume. 
Rays r3 and r4 in this .gure cannot be started on front faces of bounding geometry, because they have 
to start inside it, i.e., on the near plane of the viewfrustum(positions n3 and n4). The next section 
describes an advanced ray castingpipelinethat correctly handlesthis case, as well as the intersection 
of the volume with opaque geometry, e.g., navigational markers or tools in medical interventions. 2.3 
Advanced Ray Casting Pipeline This section describes an advanced ray casting pipeline that combines object-order 
and image-order stages in order to .nd a balance between thetwo, andleveragetheparallelprocessing of 
modernGPUs[90]. For culling of irrelevant subvolumes, a regular grid of min-max values for bricks of 
size 83 is stored along with the volume. Ray casting itself is performedin a single renderingpassin order 
to avoidthe setup overhead of casting eachbrick separately[44]. The .rst step of thealgorithmculls bricks 
on the CPU and generates a bit array that determines whether a brick is active or inactive. This bit 
array contains the state of bricks with respect to the active parts of the volume, where a brick is active 
whenit contains samplesthat aremapped toopacitiesgreaterthanzero by the transfer function and inactive 
otherwise. Intheobject-order stageontheGPU,thebit arrayisused toraster­ize brick boundary faces in several 
rendering passes. The result of these rendering passes are two images that drive the subsequent ray casting 
stage. The .rst image, the ray start position image, contains the volume coordinatepositions where ray 
casting should startfor eachpixel. Coor­dinates are storedintheRGBcomponents, andthe alpha(A) component 
is one when a ray should be started, and zero when no ray should be started. The second image, the ray 
length image contains the direction vectors for ray casting in the RGB components and the length of each 
rayinthealphacomponent. Notethatthedirectionvectors could easily be computed in the fragmentprogramfrom 
the cameraposition and the ray start positions as well. However, the ray length must be rendered into 
an image that is separate from the ray start positions due to read­writedependencies, which canthenalsobeusedforstoring 
thedirection vectors that are needed for ray length computation anyway. The main steps of our ray casting 
approach for each pixel are: 1. Compute the initial ray start position on the near clipping plane of 
the current viewport. When the start position is in an inactive brick, calculate the nearest intersection 
point with the boundary faces of active bricks, in order to skip empty space. The result is stored in 
the ray start position image. 2. Compute the ray length until the last intersection point with boundary 
faces of bricks that are active. The result is stored in the ray length image. 3. Optionally render 
opaque polygonal geometry and overwrite the raylength image where thedistancebetween the ray startposition 
and the geometry position is less than the stored ray length. 4. Cast from the start position stored 
in the ray start position image along the direction vector until the accumulated opacity reaches a speci.ed 
threshold(early ray termination)or the ray length given by the ray length image is exceeded. The result 
of ray casting is stored in a separate compositing bu.er. 5. Blend the ray casting compositing bu.er 
on top of the polygonal geometry.  The two main acceleration schemes exploited here are object-order 
empty space skipping and early ray termination. For the former, view­independent culling of bricks and 
rasterization of their boundary faces are employed, whereas the latter is handled during ray casting. 
2.3.1 Culling and Brick Boundary Rasterization Each brick in the subdivision of the volume is either 
inactive or active with respect to the transfer function. In order to determine ray start positionsand 
raylengths, weemploy rasterizationof theboundaryfaces between active and inactive bricks, which is illustrated 
in Figure 2.5. To handle brick culling e.ciently, the minimum and maximum voxel values of each brick 
are stored along with the volume, which are compared at run-time with the transfer function. A brick 
can be safely discarded when the opacity is always zero between those two values, which can be determined 
veryquickly using summed areatables[30]. Rasterizing the boundary faces between active and inactive bricks 
results in object-order empty space skipping. It prunes the rays used in the ray casting pass and implicitly 
excludes most inactive bricks. Note, however, that this approach does not exclude all empty space from 
ray casting, which can be seen for ray r3 in Figure 2.5(left). This is a trade­o. that enables ray casting 
without any per-brick setup overhead and works extremely well in practice. The border between active 
and inactive bricks de.nes a surface that can be rendered as standard OpenGL geometry with the corresponding 
positionin volume coordinates encodedintheRGBcolors. Allvertices of brick bounding geometry are constantly 
kept in video memory. Only an additional index array referencing the vertices of active boundary faces 
have to be updated every time the transfer function changes. As long as the near clipping plane does 
not intersect the bounding geometry, rays can always be started at the brick boundary front faces. However, 
if such anintersection occurs, it willproduceholesin thefront-facinggeometry, which resultsin some rays 
notbeing started at all, and others started atincorrectpositions. Figure2.9illustratesthisproblem. In 
an endoscopic view, we constantlyface this situation, so rays typically needtobe started atthe near clippingplane, 
whichis showninFigure2.5 in the case of points n2 -n4. To avoid casting through empty space, rays should 
not be started at the nearclippingplaneif thestartingpositionisinaninactivebrickbut at the next intersection 
with active boundary faces, such as rays r0 and r1 in Figure 2.5. These rays are started at f0 and f1, 
instead of being starting at n0 and n1. We achieve this by drawing the near clipping plane .rst and the 
front faces afterwards, which ensures that whenever there are no front faces to start from, the position 
of the near clipping plane will be taken. However, since the non-convex bounding geometry often leads 
to multiple front faces for a single pixel, the next front face is used when the .rst front face is clipped, 
which results in incorrect ray start positions. The solution is to detect when a ray intersects a back 
face before the .rst front face that is not clipped. The basic steps to obtain the ray start position 
image are as follows: 1. Disable depth bu.ering. Rasterize the entire near clipping plane into the color 
bu.er. Set the alpha channel to zero everywhere. 2. Enable depth bu.ering. Disable writing to the RGB 
components of the color bu.er. Rasterize the nearest back faces of all active bricks into the depth bu.er, 
e.g., by using a depth test of GL LESS. Setthe alpha channelto one wherefragments aregenerated. 3. Enable 
writing to the RGB components of the color bu.er. Ras­terize the nearest front faces of all activebricks, 
e.g.,by onceagain using a depth test of GL LESS. Set the alpha channel to one where fragments are generated. 
 This ensures that all possible combinations shown in Figure 2.5 (left) are handled correctly. Rasterizing 
the nearest front faces makes sure that all near plane positions in inactive bricks will be overwritten 
by startpositions on activebricksthat arefarther away(rays r0 and r1). Rasterizing the nearest back faces 
before the front faces ensures that nearplanepositionsinside activeblocks will notbeoverwrittenbyfront 
facesthat arefarther away(rays r2 and r3). Brick geometry that is nearer than the near clipping plane 
is auto­matically clipped by the graphics subsystem. After that, the ray length image can be computed, 
which .rst of all means .nding the last in­tersection points of rays with the bounding geometry. The 
basic steps are:  1. Rasterize the farthest back faces, e.g., by using a depth test of GL GREATER. 
 2. During this rasterization, sample the ray start position image and subtractitfromthebackpositions 
obtained via rasterization ofthe back faces. This yields the ray vectors and the ray lengths from start 
to end position. 3. Multiply all ray lengths with the alpha channel of the ray start positionimage(whichis 
either1 or0).  These steps can allbeperformedinthe samefragmentprogram. Drawing the back faces of the 
bounding geometry results in the last intersection points of rays and active brick geometry, which are 
denoted as li in Fig­ure 2.5. Subtracting end positions from start positions yields the ray vectors, 
which can then be normalized and stored in the RGB compo­nents of the ray length image together with 
the ray lengths in the alpha channel. Note that the alpha channel of the ray length image has con­sistently 
be set to zero where a ray should not be started at all, which is exploited in the ray casting pass. 
 2.3.2 Geometry Intersection Many applications, e.g., virtual endoscopy, require both volumetric and 
polygonal data to be present in the same scene. Naturally, intersections of the volume and geometry have 
to achieve a correct visibility order, and in many cases looking at the intersections of the geometry 
and the isosurface is the reason for rendering geometry in the .rst place. Also, partsthatdonotcontributetothe 
.nalimagebecausethey areoccluded by geometry should not perform ray casting at all. An easy way to achieve 
this is to terminate rays once they hit a polygonal object by modifying the raylength image accordingly. 
This isillustrated in Figure 2.11. Ofcourse, raylengths should onlybe modi.edif apolygonal object is 
closer to the view point than the initial ray length. This problem can  Figure 2.12: Modifying ray endpositionsprevents 
rendering occludedparts of the volume (left). Blending the result of ray casting on top of the opaque 
geometry thenyieldsthe correct result(right). again be solved by using the depth test. After rendering 
the back faces of active/inactive brick boundaries withtheirrespectivedepth values(anddepthtest setto 
GL GREATER), the intersecting geometry is rendered to the same bu.er, with the cor­responding volume 
coordinates encoded in the color channel. With the depth test reversed to GL LESS, only those parts will 
be drawn that are closer to the viewpoint than theinitial raylengths. This approach mod­i.es ray casting 
such that it results in an image that looks as if it was intersected with an invisible object. Blending 
this image on top of the actual geometry in the last pass of the algorithm results in a rendering with 
correct intersections and visibility order.  2.4 Isosurface Ray Casting This section describes a special 
case of volume ray casting for rendering isosurfaces, which is also known as .rst-hit ray casting. In 
order to fa­cilitateobject-order empty space skipping withoutper-sample overhead, we maintain min-max 
values of a regular subdivision of the volume into smallblocks, e.g.,with43 or83 voxelsperblock. These 
blocksdo not ac­tually re-arrange the volume. For each block, a min-max value is simply stored in an 
additional structure for culling. If the whole volume does not .t in GPU memory, however, a second level 
of coarser bricks can be maintained, whichisdescribedinlater sections on memory management. Whenever 
the isovalue changes, blocks are culled against it using their min-maxinformationand a rangequery[12], 
whichdeterminestheir ac­tive status. The view-independent geometry of active block bounding faces that 
are adjacent to inactive blocks is kept in GPU memory for fast rendering.  In order to obtain ray start 
depths tstart(x, y), the front faces of the blockboundinggeometry are rendered with their correspondingdistance 
to the camera. Thefront-mostpoints of rayintersections are retainedby enabling a correspondingdepthtest(e.g., 
GL LESS). For obtaining ray exit depths texit(x, y)we rasterize the back faces with an inverted depth 
test that keeps only the farthest points (e.g., GL GREATER). Figure 2.6 shows thatthis approachdoes not 
excludeinactiveblocksfromthe search range if they are enclosed by active blocks with respect to the current 
viewing direction. The corresponding samples are skipped on a per­sample basis early in the ray casting 
loop. However, most rays hit the isosurface soon after being started and are terminated quickly (yellow 
points in Figure 2.6, left). Only a small number of rays on the outer side of the isosurface silhouette 
are traced for a larger distance until they hit the exit position of the block bounding geometry (red 
points in Figure 2.6, left). The right side of Figure 2.6 illustrates the worst case scenario, where 
rays are started close to the view point, miss the corresponding part of the isosurface, and sample inactive 
blocks with image-order empty space skipping until they enter another part of the isosurface bounding 
geometry and are terminated or exit without any intersection. In order to minimize the performance impact 
when the distancefrom raystart to exit or terminationislarge, we use an adaptive strategy for adjusting 
the distance between successive samples along a ray. 2.4.1 Adaptive Sampling Inorderto .nd thepositionofintersectionforeach 
ray,thescalarfunc­tionis reconstructed atdiscrete samplingpositions xi(x, y)= c+tid(x, y) for increasing 
values of ti in [tstart,texit]. The intersection is detected when the .rst sample lies behind the isosurface, 
e.g., when the sample value is smaller than the isovalue. Note that in general the exact inter­section 
occurs somewhere between two successive samples. Due to this discrete sampling, it is possible that an 
intersection is missed entirely when the segment between two successive samples crosses the isosurface 
twice. This is mainly a problem for rays near the silhouette. Guaran­teedintersections evenforthin sheets 
arepossibleif thegradientlength isbounded by some value L [48]. Notethatfordistance .elds, L is equal 
to 1. For some sample value f, it is known that the intersection at iso­value . cannot occur for any 
point closer than h = |f - .|/L. Yet, h canbecome arbitrarily small neartheisosurface, which wouldlead 
toan in.nite number of samples for guaranteed intersections. We use adaptive sampling to improve intersection 
detection. The actual intersection position of an intersection that has been detected is then further 
re.ned using the approach described in Section 2.4.2. We have found that completely adaptive sampling 
rates are not well suited forimplementations ongraphicshardware. Thesearchitectures use mul­tiplepipelines 
where small tiles of neighboringpixels are scan-converted inparallel using the same texture cache. With 
completely adaptive sam­pling rate, the sampling positions of neighboring pixels diverge during parallel 
execution, leading to under-utilization of the cache. Therefore, we use only two di.erent discrete sampling 
rates. The base sampling rate r0 is speci.ed directly by the user where 1.0 corresponds to a single voxel. 
It is the main tradeo. between speed and minimal sheet thick­ness with guaranteed intersections. In order 
to improve the quality of silhouettes(seeFigure2.14), we use a second maximum sampling rate r1 as a constant 
multiple of r0: r1 = nr0. We are currently using n = 8 in our system. However, we are not detecting silhouettes 
explicitly at this stage,becauseit wouldbetoocostly. Instead, weautomaticallyincrease the sampling ratefrom 
r0 to r1 when the current sample s value is closer to the isovalue . by a small threshold d. In our current 
implementation, d is set by the user as a quality parameter, which is especially easy for distance .elds 
where the gradient magnitude is 1.0 everywhere. In this case, a constant d can be used for all data sets, 
whereas for CT scans it has to be set according to the data.   2.4.2 Intersection Re.nement Once a 
ray segment containing an intersection has been detected, the next stagedetermines an accurateintersectionposition 
using aniterative bisection procedure. In one iteration, we .rst compute an approximate intersection 
position assuming a linear .eld within the segment. Given the sample values f at positions x for the 
near and far ends of the segment, the new sample position is . - fnear xnew =(xfar - xnear)+ xnear (2.1) 
ffar - fnear Then the value fnew isfetched at thispoint and compared to theisovalue .. Depending on the 
result, we update the ray segment with either the front or the back sub-segment. If the new point lies 
in front of the isosurface(e.g. fnew >.), we setxnear to xnew, otherwise we set xfar to xnew and repeat. 
Oftena .xed numberofiterationsteps,e.g.,foursteps, is enough for obtaining high-quality intersection 
positions.  2.5 Memory Management Volume sizes are increasing rapidly, and can easily exceed the available 
amount of GPU on-board memory. However, large parts of many types of volumes are often mapped to optical 
properties such that they are completely transparent, e.g., the air around a medical or industrial CT 
scan. In order todecouple the amount of memory thatis actually needed to render a given volume, i.e., 
the working set required for rendering it, from the overall volume size, a variety of memory management 
schemes such as bricking and, additionally, multi-resolution schemes, can be em­ployed. We .rst consider 
the conceptually simple case of rendering iso­surfaces, which, however, almost directly extends to the 
case of direct volume rendering with arbitrary transfer functions. Foranypossibleisovalue, many of theblocksdonot 
containanypart of the isosurface. In addition to improving rendering performance by skipping emptyblocks, 
thisfact can alsobe usedfor reducingthe e.ective memoryfootprint of relevantparts ofthe volume signi.cantly. 
Whenever theisovalue changes,the corresponding rangequery alsodeterminesthe active status ofbricks of 
coarser resolution, e.g.,323 voxels. Thesebricks re-arrange the volume and include neighbor samples to 
allow .ltering without complicatedlook-ups attheboundaries,i.e., abrick of resolution n3 is storedwith 
size(n+1)3 [54]. This overheadisinverselyproportional to the brick size, which is the reason for using 
two levels of subdivision. Smallblocks .ttheisosurfacetightlyforemptyspaceskipping andlarger bricks avoid 
excessive storage overhead for memory management. In order to decouple the volume size from restrictions 
imposed by GPUs on volume resolution(e.g., 5123 onNVIDIAGeForce6) and avail­able video memory(e.g.,512MB), 
wecanperformray castingdirectly  Figure 2.17: Hierarchicalbricking(top row) vs. .atbricking(bottomrow). 
Culled bricks are marked in white. on a re-arranged brick structure. Similar to the idea of adaptive 
texture maps[54],wemaintainanadditionallow-resolution .oatingpointrefer­ence texture(e.g., 163 for a 
5123 volume with 323 bricks) storing texture coordinate o.sets of bricks in a single brick cache texture 
that is always residentinGPU memory(e.g., a512x512x256texture). However,both the reference and the brick 
cache texture are maintained dynamically and notgeneratedin apre-process[54]. Figure2.16illustrates the 
use of the reference and brick cache textures. Note that since no gradient re­construction or shading 
isperformedduring ray casting, no complicated neighborlook-ups are required at this stage. When theisovalue 
changes, bricks that potentially contain a part of the isosurface are downloaded into the brick cache 
texture. Inactive bricks are removed with a simple LRU(least recently used) strategy whentheir storage 
spaceis required for active bricks. Bricks that are currently not resident in the cache tex­ture are 
specially marked at the corresponding position in the reference texture (shown as white squares in Figure 
2.16). During ray casting, samples in such bricks are simply skipped. 2.6 Mixed-Resolution Volume Rendering 
Most multi-resolution volumerendering methodsarebased onhierarchi­cal bricking schemes where the brick 
size in voxels is kept constant from level to level, and the spatial extent of bricks increases from 
high to low resolution until a singlebrick coversthe entire volume(Figure2.17,top row). Conversely, .atbricking 
schemes(Figure2.17,bottomrow) keep the spatial extent of bricks constant and successively decrease the 
brick size in voxels. A major advantage of .at bricking schemes is that the culling rate is much higher, 
illustrated by the number of white bricks in Figure2.17,because thegranularityof culling stays constantirrespective 
of actual brick resolutions. This not only reduces the required texture memory, as more bricks can be 
culled, but also allows for a much more .ne-grainedLOD orfragmentprogram selectionperbrick[61]. However, 
.at multi-resolution techniques have a bigger memory overhead when samples are replicated at brick boundaries, 
because for decreasing brick sizes the overhead of duplicated voxels increases. This overhead can be 
removedby avoiding sampleduplication[65],trading o.runtime .lter­ing cost for memory savings. We employ 
.at multi-resolution bricking with sample duplication, but reduce the run-time overhead signi.cantly 
by using hardware .ltering and only warping the texture coordinates of samples where necessary[5]. 2.6.1 
Volume Subdivision for Texture Packing The original volume is subdivided into equally-sized bricks of 
size n3 in a pre-process, where n is a power of two, e.g., n = 32. During this sub­division, the minimum 
and maximum value in each brick are stored for culling later at run time, and lower-resolution versions 
of each brick are constructed. For the latter we compute the value of the new sample at the center of 
eight surrounding higher-resolution samples as their aver­age, but higher-order .lters could also be 
used. We limit the number of resolution levels to minimize the overhead of duplicated boundary vox­els, 
and also to allow tightpacking oflow-resolutionbricks in the storage space reservedforhigh-resolutionbricks(Section2.6.2). 
Bydefault we use only two resolution levels, e.g., 323 bricks with a downsampled reso­lution of 163 . 
For fast texture .ltering during rendering, voxels at brick boundaries are duplicated. In principle, 
duplication at one side su.ces for thispurpose[104], e.g., storing(32+1)3 bricks. However, in thehigh­resolutionlevel 
weduplicate atboth sides,becausethespaceforasingle (32+2)3 brickprovides storagefor eight(16 +1)3 bricks. 
Coinciden­tally, this often even does not impose additional memory overhead. The brick cache texture(Section2.6.2) 
alwayshaspower-of-twodimensions forperformance reasons, and a cache of size5123, for example, can hold 
the same number of 343 and 333 bricks. Although this approach is not fully scalable, it is very simple 
and a good trade-o. that is not as restrictive as it might seem. Because culling is very e.cient in a 
.at scheme, fewer bricks need to be resident in GPU memory. Even without culling, if the size of the 
brick cache textureis512x512x1024(256 megavoxels),forexample, and tworesolu­tionlevels are used(brick 
storage size343), 15x15x30 bricks .t into the cache. This yields a possible data set size of about 1.7 
giga voxels, e.g., 960x960x1920, if all bricks actually need to .t into the cache. Due to culling, the 
real data set size can typically be much larger. Additionally, for very large data three levels could 
be used. For example, increasing the allocated spacefor eachbrickfrom(32+2)3 to(32+4)3, both 163 and 
83 bricks can be packed tightly, including boundary duplication for .ltering. Using threelevels with 
storagefor(32+4)3 bricks, 14x14x28 brickswould .tintothecache,yielding adataset sizeof10.7gigavoxels, 
e.g., 1792x1792x3584, and more when bricks are culled. 2.6.2 Mixed-Resolution Texture Packing Forrendering, 
alist of activebricksisdetermined viaculling,using,e.g., the transfer function or iso value, and clipping 
plane positions to deter­mine non-transparent bricks that need to be resident in GPU memory. The goal 
is to pack all active bricks into a single 3D brick cache tex­ture(Figure2.18, right). Inthebeginning, 
all cache spaceis allocated for high-resolution bricks. If the number of active bricks after culling 
exceeds the allocated number, individual bricks are chosen to be repre­sented at lower resolution. In 
this case, the e.ective number of bricks in the cache is increased by successively mapping high-resolution 
bricks in the cache to eight low-resolution bricks each, until the required over­all number of bricks 
is available. This is possible because the storage allocation for bricks has been chosen in such a way 
that exactly eight low-resolution bricks .t into the storage space of a single high-resolution brick, 
including duplication of boundary voxels, as described in the pre­vious section. After thelistof activebricks 
along with the corresponding resolutions hasbeen computed, thelayout of the cache texture and mapping 
ofbrick storage space in the cache to actual volume bricks can be updated ac­cordingly, which results 
in an essentially arbitrary mixture of resolution levels in the cache. The actual brick data are then 
downloaded into their corresponding locations using, e.g., glTexSubImage3D(). During rendering, a small 
3D layout texture is used for address translation be­tween virtual volume space and physical cache texture 
coordinates (Figure 2.18, topleft), which is described in the next section. 2.6.3 Address Translation 
A major advantage of thetexturepacking schemedescribedhereisthat address translation can be done in an 
identical manner irrespective of whether di.erent resolution levels are mixed. Each brick in virtual 
vol­ume space always has constant spatial extent and maps to exactly one brick in physical cache space. 
Virtual addresses in volume space, in [0, 1], corresponding to the volume s bounding box, are translated 
to physical texturecoordinatesinthebrick cachetexture, alsoin[0, 1], corresponding to the full cache 
texture size, via a lookup in a small 3D layout texture with one texel per brick in the volume. This 
layout tex­ture encodes(x, y, z)address translation information in the RGB color channels, and a multi-resolution 
scale value in the A channel, respec­tively. A volume space coordinate xx,y,z . [0, 1]3 is translated 
to cache texture coordinates x ' . [0, 1]3 in the fragment program as: x,y,z ' x = xx,y,z · bscalex,y,z 
· tw (2.2) x,y,z + tx,y,z, where tx,y,z,w is the RGBA-tuple from the layout texture corresponding to 
volume coordinate xx,y,z, and bscale is a constant fragment program parameter containing a global scale 
factor for matching the di.erent coordinate spaces of the volume and the cache. When .lling the layout 
layout texture virtual volume cache texture Figure 2.18: Mixed-resolution texturepacking and address 
translationfrom virtual volume space to physical cache texture space via the layout texture. Resolution 
levels are mixed by packing low-res bricks tightly into high-res bricks. texture, the former is computed 
as: () tx,y,z = bx,y,z ' · bres x,y,z ' - ox,y,z + tw/csizex,y,z (2.3) tw =1.0, (2.4) for a high-resolution 
brick, where b ' is the position of the brick in the cache(0,1, ...), bres ' isthestorageresolutionof 
thebrick, e.g.,343, and csize is the cache texture size in texels to produce texture coordinates in the 
[0, 1] range. For a low-resolution brick, this is computed with tw =0.5. The o.set ox,y,z is computed 
as: ox,y,z = bx,y,z · bresx,y,z · tw, (2.5) where b isthepositionofthebrickinthevolume(0,1, ...),and 
bres is the brick resolution in the volume, e.g., 323 . The global scale factor bscale is computed as: 
bscalex,y,z = vsizex,y,z/csizex,y,z, (2.6) where vsize is the size of the volume in voxels.  2.7 Multiresolution 
LOD and Adaptive sampling 2.7.1 Octree-based Multiresolution Representation Thelinear storage scheme 
described abovehas apoordatalocalityprop­erty. The lookup of neighboring voxels is frequent and it is 
only along the x-axis that this translates to access of neighboring memorylocations. Theimpact of cache-misses 
in rendering andprocessingis signi.cant and often causes a scheme to ultimately fail if not well addressed. 
Blocking of the volume is therefore generally e.cient and signi.cantly improves the cache hit-rate. The 
size of a block is typically derived from the size of the level 1 and 2 caches. Grimm et al. [32, 31] 
.nds that a block size of 32, B = (32, 32, 32), is the most e.cient for their block-based raycaster. 
Parker et al.[78] useasmallerblock size, B =(4, 4, 4),for aparalleliso­surface1 renderer. This block 
size matches the size of the L1 cache line onSGI super-computers(SGIOnyx2&#38;SGIOrigin2000). In numerous 
publications it is indicated that blocking by 16 or 32 is an optimal size for many block related processing 
tasks.2 The addressing of blocks and samples within blocks is straightfor­ward, but introducing a block 
map structure allows for arbitrary place­ment ofblocks andpacking in memory with unused blocksbeingignored 
and thus saving memory space. The introduction of blocking results in an additionallevel of complexityforblockboundaryhandling, 
especially for the cases when a sample is requested in a neighboring block that has beenignored. Twostrategiescanbeexplored 
todeal with this. The .rst requires the access of neighboringblocks. Grimm et al.[32],for example, propose 
a scheme based on table lookups for neighboring samples that avoids conditionalbranchesin the code. The 
second strategyisbased on self-containedblocks and requires the replication of neighboring samples. The 
overhead for sample replication is less than 20% for block sizes of 16 and up. Additionalbasicdata conversions 
mayalsobe applied, such as remap­ping the value range and conversion to 8-or 16-bit integers, that is 
data types that directly map to native GPU types. Blocking improves mem­orylocalityfor software-based 
rendering andprocessing. Skipping empty 1An iso-surface, S, is an implicit surface de.ned as S = { p 
| s(p)= C }, where C is a constant. 2In two-dimensional blocking, or tiling, the equivalent size is 64× 
64, also being thedefaulttile sizeinJPEG-2000[1]. blocksusuallyhasasigni.cant e.ect onthedatasizeand 
renderingper­formance. The concept of anemptyblock,however, needstobeclari.ed and de.ned, which is an 
integral part of the next section. 2.7.2 Block Properties and Acceleration Structures In order to reveal 
any embedded entities within a volume it is obvious that some samples must be rendered transparent and 
other samples ren­dered semi-transparent or opaque. This is achieved through the use of a TransferFunction(TF).For 
ablocking scheme, asdescribed above,the meaning of an empty block is a block that has all its voxels 
classi.ed as completely transparent. Naturally, such a block could be discarded in the rendering process 
and thus improve the performance. Since the goal is to reduce the amount of data in the pipeline it is 
essential that empty blocks can be predicted without access to all samples in a block. Meta-data for 
such predictions is collected during preprocessing, and preferably without knowledge of speci.c TF settings. 
TheTFusuallyde.nes one or more regionsinthe scalar range as non­transparent and,forthe rendering ofiso-surfaces, 
either narrowpeaks are de.ned or special iso-surface renderers are used. It is therefore natural that 
ideas from iso-surface extraction acceleration schemes have been applied. Thegoalofthese schemes are 
to minimize theprocessing sothat only cellsintersecting theiso-surface areconsidered. Wilhelms&#38;Gelder 
[108] create a tree of min/max values. The tree is created bottom up and starts with the cells, cubes 
of8 voxels. Livnat et al.[62] extend this approach and introduce the span-space. For iso-surface rendering, 
a leaf in the tree is included if the iso-value is within the range spanned by the minimumand maximum 
valueof thecell. Parkeret al.[78] usealimited two-level tree and .nd that su.cient in their software 
implementation of an iso-surface raycaster. For arbitrary TF settings, the min/max scheme is generally 
overly conservative and may classify emptyblocks as non-empty. Summed-Area Tables[17] oftheTF opacity 
are usedbyScharsach[91] todetermine the blocks content by taking the di.erence of the table entries for 
the minimum and maximumblock values. Thelowgranularity of the min/­max approach is addressed by Grimm 
et al. [31] who, instead, use a binary vector to identify block content. The scalar range is quantized 
into32 uniformregionsand thebit-vectorindicatesthepresence of sam­ples within the correspondingrange. 
A similar approachis takenbyGao et al. [27] but they use a larger vector, matching the size of their 
TF table(256 entries). Figure 2.19: Hierarchical blocking with subsampling. Downsampling is achieved 
by removing every evensample[56] orby asymmetricodd-sized .lter[105]. 2.7.3 Hierarchical Multiresolution 
Representations Simply skipping empty blocks might not reduce the volume size su.­ciently, the total 
size of the remaining non-empty blocks may still be above the available memory size. A strategy is then 
to apply techniques that vary the resolution in di.erent parts of the volume, so di.erent blocks in the 
volume have di.erent resolutions. This Level-of-Detail (LOD)approach enables a more graceful adaptation 
to limited memory and processing resources. The most common scheme is to create a hierarchical representation 
of the volume by recursive downsampling of the original volume. Since each lower resolution level is 
1/8 the size of the previous, the additional amount of memory required for this pyramid is less than 
14.3%. The created hierarchies may di.er depending on the selected downsampling scheme. Figure 2.19 illustrates 
three levels of an hierarchy created using subsampling, every second samplebeing removed. This schemeis 
usedby LaMar et al.[56] andBoada et al.[8], amongst others. Weiler et al.[105] also use this placement 
of samples but employ a quadratic spline kernel in the downsampling .lter since they argue that subsampling 
is a poor approximation. The positions of the downsampled values, however, do require some attention. 
Thepositioningindicatedin .gure2.19 skewstherepresented domain. A more appropriate placing of a downsampled 
value is in the center of the higher resolution values it represents. This placement is illustrated in 
.gure 2.20 and is also a placement supported by average downsampling. Figure 2.20: Hierarchical blocking 
with average downsampling. In order to be able to select di.erent resolution levels in di.erent parts 
of the volume blocking is suitable for hierarchical representations as well. The block size, in terms 
of number of samples, is usually kept equal at each resolution level and the block grids are indicated 
by wide, blue lines in the .gures 2.19 and 2.20. Blocks at lower resolutions cover increasingly large 
spatial extents of the volume. These multiresolution hierarchies thus provide supporting data structures 
for LOD selection. Methods to determine an appropriate level of detail are discussed in the following 
section.  2.8 Level-of-Detail Management Itis not su.cient to onlydetermine if ablockis empty or not. 
The mul­tiresolution representations described above require additional and dif­ferent techniques that 
also candetermine resolutionlevels for theblocks. This section reviews techniques and approaches for 
LOD selection that have been suggested in the literature. These approaches can be classi­.ed into: view 
dependent and region-of-interest, data error, and transfer function based techniques. It is, furthermore, 
common to combine sev­eral of these measures in di.erent con.gurations. The following sections will, 
however, review them individually. The conceptualprincipleforhierarchicalLOD selectionissimilarfor all 
approaches. The selection starts by evaluating one or more measures for a root node. If the resolution 
of a block, a node in the hierarchy, is foundadequate then the traversal stops andthe selectionprocessisdone. 
If the resolution needs to be increased the block is either immediately replaced by all its children 
or a subset of the children is added. The latter approach will remove the parent node when all its children 
have been added. If the amount of data to use is limited, this constraint is checked at every step and 
the LOD selection is stopped when the limit is reached. 2.8.1 View-Dependent Approaches View-dependent 
techniques seek to determine the LOD selection based on measures like distance to viewer and projected 
screen-space size of voxels. Region-of-interest methods work similarly to distance to viewer measures. 
Using full resolution blocks when viewing entire volumes can be suboptimal. When a single pixel covers 
multiple voxels it may result in aliasing artefacts. Reducing the resolution of the underlying sampled 
data(pre.ltering) is,infact, standardingraphicsrenderinginstead of supersampling. It is referred to as 
mipmapping3 in the graphics litera­ture. Distanceto viewer approaches are usedin[56,105,35,6],forinstance. 
A block is re.ned if the projected voxel size is larger than one pixel on the screen, for example. The 
distance to viewer or region-of-interest can furthermore be used to weight some other measure, like a 
data error measure, by dividing that measure by the distance. 2.8.2 Data Error Based Approaches Representing 
a block in a volume with a lower resolution version may naturally introduce errors when the volume is 
sampled compared with using the full resolution. A measure of this error, for instance the Root-Mean-Square-Error(RMSE), 
expressesthe amount of errorintroduced. When selecting a LOD for the multiresolution hierarchy, the block 
with thehighestdata error shouldbe replaced with ahigher resolution version. Repeating this procedure 
until the memory budget is reached will then select alevel-of-detailfor the volume that minimizes thedata 
error. This measure onlydepends onthedataand canthereforebecomputedinthe preprocessing step. This approach 
is used in Boada et al. [8], who also take into account the e.ect of linear interpolation in the lower 
resolution version. In ad­dition, a user-de.ned minimum error threshold is used to certify that the represented 
data correspond to a certain quality. Guthe et al. [35] 3Mip is an abbreviation of the Latin multum in 
parvo many things in a small place. also take this approach, using the L2-norm, but combine it with 
view­dependent measures, namelydistance-to-viewer andprojected voxel size. 2.8.3 Transfer Function Based 
Approaches The shortcoming of data error approaches lies in the mapping of data samples through the TF. 
The content of the TF is arbitrary and conse­quently the data error is a poor measure if it is used for 
volume render­ing. Determining the content of a block in the TF domain has a higher relevance since this 
will a.ect thequality of the rendered image more di­rectly. The notion of a block s TF content is explored 
below and several schemes forTF contentprediction are reviewed. The challenge, however, istopredicttherequiredLODforeachblock 
without accessing thedata beforehand. The completedistribution of sample valueswithin ablockisahighly 
accurate description of the block content, losing only spatial distribu­tion. Such a description could, 
however, easily result in meta-data of signi.cant sizes, potentially larger than the block data itself. 
LaMar et al. [57] therefore introduce frequency tables to express the frequency of speci.cdata errors(di.erences) 
and compute anintensity errorfor a greyscaleTFas an approximationtothe currentTF.Guthe et al. [34]in­stead 
use a more compact representation of the maximum deviation in a small number ofbins,for which the maximum 
errorinRGB-channels are computed separately. A combined approach of these two, using smaller binned frequency 
tables, is presented by Gyulassy et al. [36]. Gao et al. [27]use abit-vectorto representthepresence of 
valuesin a block. The block vector is gated against RGB bit-vectors of the TF. If the di.erence of two 
such products, compared with a lower resolution blocklevel, isless than a userde.ned threshold then thelower 
resolution blockcanbe choseninstead. Asimilar approach using aquantizedbinary histogramispresentedin[31] 
butis not reportedtobe usedforLOD selection.  2.9 Encoding, Decoding and Storage In section 2.7.3 a 
conceptual view of multiresolution hierarchies was de­scribed. As mentioned, the amountofdatais not reducedbythisprocess, 
rather increased. When the amount of data in the hierarchy can not be handled in core memory, additional 
techniques are required. Data com­pression is one viable approach and, speci.cally, lossy compression 
can signi.cantly reduce the amount of data, at the cost of a loss of .delity. Another approach is to 
rely on out-of-core storage of the volume hierar­chy and selectively load requested portions of the data. 
A combination of these techniques is also possible. Some of the well-known approaches are described in 
the following sections. 2.9.1 Transform and Compression Based Tech­niques Following thesuccess ofimagecompressiontechniques,itisnatural 
that such techniques be transferred to volumetric data sets. Usually a trans­form is applied to the data 
and it is the coe.cients from the transform that are stored. The underlying idea for the transform is 
to make data compression more e.cient. Applying a compression technique on the coe.cients, such as entropy 
coding, then yields a higher degree of com­pression compared to compressing the original data. The following 
sec­tions review two transforms that are commonforimage compression and have been used for volume data. 
Basic concepts of compression are also presented. 2.9.1.1 Discrete Cosine Transform TheDiscreteCosineTransform(DCT) 
is well establishedinimage and video coding standards, such asJPEG andMPEG, andthere existhighly optimized 
algorithmsand codetoperformthistransform, with atypical block size of 8. Relatively few researchers have 
applied this transform to volumetricdatasets although someexamples exist[112,79,69]. Lum et al. [70]apply 
theDCT for time-resolved data. Instead of computing the inverseDCT,itis replacedbyatexturelookup sincetheDCT 
coe.cients are dynamically quantized and packed into a single byte. 2.9.1.2 Multiresolution Analysis 
 Wavelets The wavelet transform has gained a wide acceptance and has been em­bracedin many applicationdomains, 
speci.callyin theJPEG-2000 stan­dard,describedbyAdams[1]. A signi.cant amount of work on volume data 
compressionhas employed wavelet transforms. Being a multiresolu­tion analysis framework it is well suited 
for the multiresolution handling of volumedata. Several wavelets exist,buttheHaar andLeGall(a.k.a. 5/3) 
integer transforms, supporting lossless encoding [9, 100], and the Daubechies9/7[19] arethe most common 
and canbe e.cientlyimple­mented using thelifting scheme[99]. Conceptually, the transform appliedon a1D 
signalproduces two sub­band signals as output, one describing low frequency content and the other describing 
high frequency content. Recursive application of the transform on the low frequency output produces multiple 
sub-band de­scriptions until a lowest resolution level is reached. Once the multiband analysis is done, 
the inverse transform can be applied in an arbitrary number of steps until the original full resolution 
has been reconstructed. For every step, the resolution of the volume is increased, doubled along each 
dimension. It is furthermore possible to apply the inverse trans­form selectively and retrieve higher 
resolution in selected subregions of the volume. This approach is taken by Ihm &#38; Park [45], Nguyen 
&#38; Saupe[76] andBajaj et al.[2]. Their workisprimarily concerned with e.cient random access of compressed 
wavelet data and caching of fully reconstructed parts. Awavelettransform can alsobe applied onblocksindividually. 
Guthe et al. [35] collect eight adjacent blocks and apply the transform once on the combined block. The 
low frequency sub-band then represents a downsampled version. The high frequency sub-band, the detail, 
is com­pressedand stored separately. Theprocedureisthen repeatedrecursively on eachlevel until a singleblock 
remains. Ahierarchyofdetaildata,high frequency coe.cients, is thus constructed and can be selectively 
used to reconstruct a multiresolution level-of-detail selection. A hardware sup­ported application of 
this technique, using a dedicated FPGA-board, is presentedby Wetekam et al.[107]. 2.9.1.3 Data Compression 
Applying transforms to volume data does not reduce the amount of data. Instead, it frequently increases 
the size of the data. The Haar and LeGall integer wavelet bases, for example, require an additional bit 
perdimension andlevel. Nevertheless, astheentropy of thetransformed signal is generally reduced, compared 
with the original signal, a com­pression scheme yields a higher compression ratio for the transformed 
signal. Lossless compression is, however, quite limited in its ability to reduce thedata size. In manypractical 
situations with noisydata, ratios above 3:1 are rare. Even this small reduction canbe valuablebut should 
be considered against the increased computational demand of decoding the compressed data stream and applying 
the inverse transform. Signi.cant data reduction can be achieved, however, if lossy com­pression is allowed. 
Quantization of the coe.cients is commonly used and can be combined with thresholding to remove small 
coe.cients. Hopefully this results in many long sequences of zero values that can be compactly represented 
using run-length encoding. There exist a wide range of quantization and compression methods presented 
in the liter­ature and several of these are used in the context of volume data com­pression [106, 45, 
2, 33, 70]. Vector quantization of coe.cient vectors is also applied[92,69]. For reasonabledistortion, 
causing minor visual degradation, compression ratios of30:1 are achieved[35]. It is also reasonable to 
allow the encoding stage to take a signi.cant processing time if the results are improved, it is the 
performance of the decoding stage that is critical.  2.9.2 Out-of-Core Data Management Techniques Computer 
systems already employ multiple memory level systems, com­monlytwolevels of cache are employed tobu.erdatafrom 
main memory. Since the cache memoryisfaster, thishelps to reducedata accesslatency for portions of the 
data already in the cache. Extending caching tech­niques to include an additional layer, in the form 
of disk storage or on a network resource, is therefore quite natural and bene.cial. A data set can be 
signi.cantly larger than core memory but those subsets of the data to which frequent access is made can 
be held in core making these accesses much faster. Indeed, several techniques exist in operating systems 
that exploit this concept, for instance memory-mapped .les. The semantic di.erence between general purpose 
demand-paging is that an application may know signi.cantly more about data access pat­terns than a general 
low level scheme could. Cox &#38; Ellsworth [15] present application controlled demand-paging techniques 
and compare those with general operating system mechanisms, showing signi.cant improvements for application-controlled 
management. Their work is ap­pliedtoComputationalFluidDynamics(CFD) data. Another example is the data 
querying techniques for iso-surface extraction presented by Chiang et al. [13]. TheOpenGLVolumizertoolkit,presentedby 
Bhani­ramka&#38;Demange[6], also supports management oflarge volumes and volume roaming on SGI graphics 
systems. Volume roaming provides scanning through the volume, with a high-resolution region of interest, 
and large blocks, B = (64, 64, 64), are loaded from high-performance disk systems on demand. Distributed 
rendering approaches also make use of out-of-core data management ideas, the capacity of each rendering 
node is limited and Figure 2.21: Flat multiresolutionblocking. Spatialposition and sizeis constantfor 
allblocks(blue squares). The resolutionof eachblockisarbitrary andindependent of the resolution level 
of neighboring blocks. data transport is costly. Predictive measures are required to ensure that the 
renderingloadisbalancedbetween the nodes. Thisis another aspect where application controlled data management 
is preferred, since the access latency can be reduced or hidden. Examples of this approach are presentedin 
severalpapersby Gao et al.[28,29,27]. 2.9.3 Flat blocking Multiresolution Representation In .atblocking 
thesamplesontheuniformgrid arecentered onthegrid cells instead of on the cell vertices. This sample placement 
is shown in .gure2.21(gridinblack and samplesin red). This cell-centered sample placement is compatible 
with OpenGLs sampling location for the pixels in aframebu.er andtexelsinthetextures. Blockdatais also 
cellcentered on the block grid, as indicated by the blue block grid. Furthermore, a multiresolution representation 
is created individually foreachblock, eitherby awavelettransform[66] orby averagedown­sampling[65,63, 
67]. This scheme is referred to as a .at multiresolution blocking, or .at blocking, since no global hierarchy 
is created. The spa­tial extent of a block is constant and the blocks spatial extents do not growwithreduced 
resolutionlevel. Thekeyadvantagesofthe .at scheme can be summarized by: A uniform addressing scheme 
is supported.  Thegranularityofthelevel-of-detail selectionremains .ne-grained.   Hierarchical blocking 
Flat blocking  Level 0 Level 1 Level 2 Level 3 Arbitrary resolution di.erences between neighboring 
blocks can be supported since a block is independent of its neighbors.  The resolution of a block is 
not restricted to be in powers-of-two.  A heuristic analysis shows that .at blocking provides a higher 
memory e.ciency than a corresponding cut through a hierarchical scheme, see table 2.1.  The disadvantage 
with this .ne-grained .at blocking is that the number of blocks is constant. Hierarchical schemes scale 
in this respect with reduced memory budget. On the other hand, since there are no hierar­chical dependencies 
it is trivial to exploitparallelismin manyprocessing tasks for .at multiresolution data. Figure 2.22 
shows an illustrative comparison between hierarchical and .at multiresolution blocking. In this example 
the LOD is selected so that a block that intersects the boundary of the embedded object must have full 
resolution, while the blocks on the interior should be at the second lowest resolution, level 1. Blocks 
on the exterior are to be ignored, level 0. The LOD selection is indicated with level speci.c Table 2.1: 
Memory e.ciency for multiresolution volumes. The LOD selectioncriteriausedin .gure2.22 areapplied. Hierarchy1 
corresponds toaLOD selectionwhereall children alwaysreplaceaparent. Hierarchy 2 is more .exible and allows 
partial usage, as shown in .gure 2.22. coloring. Two strategies for block re.nement are used in the 
literature, as discussed in section 2.7.3. The .rst is to completely replace a block with allits children. 
The second allowspartial usage of ablock with only the requiredhigher resolutionblocksbeing added and 
apart of thelower resolution block can still be used. These are referred to as Hierarchy 1 and 2 in table 
2.1 which shows memory e.ciency for both hierarchical LOD selection schemes and the .at scheme. Full 
resolution blocks have 8× 8 =64 samplesand theoriginalimagesizeis64× 64. The achieved data reduction 
using .at blocking is 2.8:1, compared to 1.6:1 and 1.8:1 for the two hierarchical LOD selection approaches. 
 2.10 Sampling of Multiresolution Volumes The .at multiresolution structureprovides a uniform addressing 
scheme for access to blocks and samples. The volume range is de.ned by the number of blocks, Nx,Ny,Nz, 
along each dimension. The block index, .,isthen easily retrieved astheintegerpart of aposition, p, within 
the volume. The remainder of the position then de.nes the intrablock local coordinate, p . = frac(p). 
The block index map, holding the block size, s, and the location, q, of the block in the packed volume, 
is then used to compute the coordinate for the sample to take. The coordinate range for .at block addressing 
is de.ned in units of blocks. Taking the integer partof a coordinate thenyields theblockindex and the 
remainderyields the local, intrablock coordinate. This scheme is in e.ect virtualizing the volume address 
domain. The block index is used to lookup a block s d3 Block 3 *) y Size 2 Block 1 Size 1 ) d1 Figure 
2.23: Illustration ofblock neighborsin 2D.Theblocks are assigneddi.erent resolutions and the samples 
are indicated by the red dots. The distance between the sampleboundaries(dashedlinesin red) and theblocks 
spatialboundaries(blue lines) are denoted by di, for each block, i. The shaded area between block centers 
* indicates the domain for local interblock coordinate, p . Edge weights, ei,j , are de.ned along the 
edges between adjacent blocks. scale and location in the packed volume. Since blocks in the packed volume 
are rarely neighbors in the spatial domain, special care has to be taken in the sampling of a block. 
Furthermore, the scale of a block is arbitrary which alsohasimplicationsforblock sampling. Anillustration 
of a neighborhood of four blocks is shown in .gure 2.23. 2.10.1 Nearest Block Sampling The .rst approach 
toblock samplingistorestrictthesampling toaccess only data within the current block, suitably expressed 
in terms of oper­ations on the intrablock coordinate, p . . The valid coordinatedomain for intrablock 
samples is indicated by squares of red, dashed lines in .gure 2.23. The inset from theblocks spatialboundariesisindicated 
by di, for each block, i. The restricted sample location, p . C, is then de.ned as p = Cd (p .), (2.7) 
C1-d where Cßa(x)clamps the value, or element values for vectors, of x to the interval[a, ß]. This form 
of sampling has been relabeled Nearest Block (NB)samplinginpaper[67],but wasintroduced as no interblock 
inter­polation in paper [65]. The GPU-based raycaster introduced in paper [63] also used NB sampling 
only. It is evident that block artefacts will arise, but these are not always visible anditis notedbelow 
that they are most apparent for thin, iso-surface like TF settings. 2.10.2 Interblock Interpolation 
Sampling To overcome these block artefacts, an interblock interpolation technique wasdeveloped andintroducedinpaper[65]. 
Previoustechniques rely on sample replication andpaddingbetweentheblocks[56,105,35]. Repli­cation,however, 
counteractsthedata reductioninthepipeline and may also distort samples, where a block has higher resolution 
than its neigh­bor,in orderto reduceinterpolationdiscontinuities. Speci.callyforlower resolutionblocks, 
andimplicitlyforhigherdata reduction, thedata over­headbecomesincreasingly large,asshownin .gure[65]:1b. 
InterblockInterpolation(II) removesthe needfor sample replication andis a scheme fordirect interpolation 
between blocks of arbitrary reso­lution,including non-power-of-twosizes. TheprincipleforII samplingis 
to take a sample from each of the immediate closest neighboring blocks usingNB sampling and compute a 
sample valueby a normalized weighted sum. The domain for interblock interpolation in a neighborhood is 
indi­cated by the shaded area between the block centers in .gure 2.23. The interblock local coordinate, 
p * =frac(p +0.5)- 0.5,has its origin at the intersection of the adjacent blocks. Theblockweight, .b,for 
each oftheblocksis computed usingindivid­ual edge weights, ei,j, for the edges between two facing block 
neighbors centers, blocks i and j, as illustrated in 2D in .gure 2.23 by the grey dotted box edges between 
block centers. In .gure 2.24 four di.erent edge weight schemes are shown, including the NB sampling mode. 
The Maximum Distance scheme provides the smoothest interpolation, but is also the most sensitive to poor 
choice of level-of-detail since the .lter kernel has the widest weighted support. Consider, for instance, 
the case where an iso-surface intersects a block outside its sample boundary. If a neighboring block 
is chosen at a low level-of-detail, the surface would bendout towards thelow resolutionblock. Therefore 
theblock valuedis­tributions used for LOD selection includes one layer of samples around each block. 
The 2D version of the II sample scheme is then succinctly de.ned by the following normalized sum: 4 b=1 
.b.b . =, (2.8) 4 b=1 .b where .b is an NB sample from block b and the block weights, .b, are Block 
tance Split tance .E Figure 2.24: A comparison of the interpolation methods on a slightly rotated linear 
gradient. The original image was a 256×256 greyscale image (8×8 blocks) being reconstructed using random 
levels between 1×1and thefull resolution,32×32. Thebottom row shows color mapped images of the pixel-wise 
errors using the CIEL* u * v * .E color di.erence. The images have been cropped. de.ned as .1 = (1- e1,2)· 
(1- e1,3), .2 = e1,2 · (1- e2,4), .3 = (1- e3,4)· e1,3, .4 = e3,4 · e2,4. The corresponding 3D variant 
and de.nitions of the edge weight func­tions are presented in paper [65]. All edge weight functions described 
therein resultininterpolations equivalent totrilinearinterpolation when­ever the neighboring blocks are 
all of equal resolution. It is also shown that the method constitutes a C0 continuous function. 2.10.2.1 
Sample Placement Discussion The placement of sample points within a block depends on the align­ment of 
the superimposed block grid and on the scheme being used to derivelowerresolutionblocks. Theplacementusedin 
.gure2.23 canbe motivatedfor average valuedownsampling and some wavelet transforms, such as theHaar and 
theLeGall5/3 wavelets[9,100]. Other approaches for lower resolution representations might suggest di.erent 
placements. Sample replication techniques use a di.erent sample placement scheme a) SkinwithNBsampling(1.1 
b) SkinwithIIsampling(0.1/ fps) 0.4 fps)  Figure2.25: Comparison between Nearest Block andInterblock 
Interpo­lation sampling. Images are rendered with texture slicingin a1024x1024 viewport. The opaque surfaceinimage(a) 
clearly showsblock artefacts while these are di.cult to perceive for the softer TF setting in image (c). 
Interblock interpolation (b &#38; d) removes these artefacts. The II examples show single/dual pass framerates. 
that suggests downsampling by skipping every second sample, arguably a less suitable approach.  2.10.3 
Interblock Interpolation Results The quality of the interblock interpolation sampling is shown in .g­ure 
2.24. A slightly rotated gradient is used to evaluate the di.erent edge weight functions and the bottom 
row shows the pixel-wise error in CIEL* u * v * color space. A .nal comparison between interblock interpolation 
and nearest block sampling is presented in .gure 2.25. The top row, with a TF de.ning an opaque iso-surface, 
clearly shows block artefacts without II sampling. The softer TF setting for the bottom row shows less 
perceiv­able artefacts. The multiresolution interblock interpolation scheme was initially de­ployed in 
a renderer based on texture slicing. The increased compu­tational requirements for II sampling causes 
a signi.cant performance reduction, about a factor of 8 10. It is, however, possible to render each slice 
in two passes. The .rst pass samples the interior domain and dis­cards the fragment for samples outside 
the sample bounding box. The second pass then .lls in the samples outside sample boundaries by ex-ploitingearlyZ-termination4 
. The costofthe more expensiveII sampling shader is thus signi.cantly reduced and the performance hit 
is lowered to a factor of only 3 4, compared to NB sampling only. In this case it is important that the 
blocks at the lowest resolution are sampled entirely with NB sampling since theblocks with a single average 
value constitute a degenerate case with no interior intrablock sampling domain.  2.11 Raycasting on 
the GPU The sampling techniques for .at multiresolution representations pre­sentedinprevious sections 
aredescribed using texture slicing techniques, but a more direct raycasting approach can be implemented 
on modern GPUs. The workpresented theredoes notpresentincreasedperformance for GPU-based raycasting over 
texture slicing techniques, but the ren­dering quality is improved and more complex rendering techniques 
are supported. In this section several techniques are described that signif­icantly improve the rendering 
performance for GPU-based raycasting, asintroducedinpaper[63]. Exploiting the .exibility ofprogrammable 
GPUs, severaladvanced classi.cationtechniques combining multipleTFs into single-pass raycasting, are 
brie.y described. Full details are to be found in[67]. 2.11.1 Adaptive Object-Space Sampling As can be 
seen in images of volumetric data, there are often large areas of empty space and, for multiresolution 
volumes, blocks at less than full resolution. Obviously rendering performance could be increased if these 
parts were sampled more sparsely or skipped entirely. This has been the goal of many research e.orts 
and several schemes have been proposedinvolving frame-to-frame andspatial coherence as well as using 
acceleration data structures such as octrees. These approaches have, in general, been mostly binary decisions, 
either a region is skipped or it is sampled at full density. 4Early Z-termination prevents the execution 
of a fragment program if the depth test fails. Table2.2: Renderingperformance of the adaptive object-space 
sampling in a 1024×1024 viewport. Data reductions are given for each data set. TS refers to texture slicing 
and is comparable with full single-pass ray­casting. Numbers specify FPS and (speed-up vs. full sampling). 
The last column indicates the expected theoretical speed-up. Adaptive stepping along the ray wasintroducedinpaper[63] 
where theblock meta-data, the resolution level for eachblock, is used to adjust the density of samples. 
The LOD selection is, in fact, a .ne-grained acceleration data structure with a more continuous adaptationdirective, 
speci.cally when not rendering thin iso-surfaces only. The theoretical improvement through this scheme 
is a performance gain in proportion to the cube-root of the data reduction factor since the sampling 
is only reduced in one dimension, along the rays. Special care has to be taken when stepping across block 
boundaries. In a naive approach, stepping is dictated by the resolution of the cur­rent block being sampled. 
A sample close to the block boundary could thenpotentially advancethe ray deep into a neighboring blockhaving 
a signi.cantly higher resolution, resulting in an undersampled region. The scheme is therefore modi.ed 
to limit the stepping at block boundaries. Inpaper[67] the raycaster wasfurtherdevelopedto supportinterblock 
interpolation sampling, in this case the stepping is limited according to the minimum number of steps 
among all eight neighboring blocks. Adaptive object-space sampling signi.cantly improves the perfor­mance 
ofGPU-based raycasting while maintaining ahigh visualquality. From table 2.2 the speed-up is slightly 
surpassing the expected theoret­ical speed-up (last column). The naive stepping approach shows even higher 
gains, clearly indicating that undesired undersampling occurs. The images in .gure 2.26 show the result 
of full, limited and naive sam­plingdensity. The naive stepping shows obvious undersampling artefacts 
(.g. 2.26c). Applying II sampling reduces the performance and attempts have a)Full sampling(7.6 b) Limited 
sampling c)Naivesampling(29.8 fps) (19.2 fps) fps) been made to counter this e.ect by conditionally 
executing II sampling in the shader. Due to the SIMD nature of fragment processing units, processing 
many fragments in parallel, these e.orts are often counter­e.ective in thatfragmentsfrequently do notbehave 
uniformly over su.­ciently largetiles. Whilethisis anissueinthe currentGPUgeneration, the execution granularity 
is gradually being re.ned. 2.11.2 Flat Blocking Summary The images in .gure 2.27 capture several signi.cant 
contributions of the .atblocking scheme,includinginterblockinterpolationand adaptive sampling. They show 
the graceful degradation of reducing the volume data size to meet aprogressively decreased memorybudget. 
Image2.27c uses only 2.7 MB of a 864 MB data set. Image 2.27a renders all blocks which contribute to 
the image at full resolution, referred to as virtu­ally lossless. The .ne-grained blocking scheme e.ciently 
reduces the data size to 144 MB, a ratio of 6:1, without any loss of image quality. a)6:1,Virtuallyloss-b)80:1,Mediumreduc-c)315:1,Extremere­less 
tion duction Figure 2.27: Transfer function based data reduction. This data set is .rst used in paper[66] 
whichintroducedperceptualTF-basedlevel-of-detail selectionand .at multiresolution blocking. They are 
here re-rendered using the .nal version of my GPU-based raycasting renderer. Data reductions are maintained 
in all stages of the rendering pipeline, including texture memory on the GPU. Together with out-of-core 
data management this pipeline supports the rendering of data sets substan­tiallylarger than available 
core memory. Thisdata reductionis achieved withoutincluding compression techniques, that could reducedata 
sizein the .rst stagesof thepipeline. Maintaining accesstofull resolutiondata at originalprecision is 
essential in the medical domain and thepresented pipeline supports the use of full resolution data, given 
that the needed memory resources are available. Course Notes Advanced Illumination Techniques for GPU 
Volume Raycasting  Light Interaction Markus Hadwiger VRVis Research Center, Vienna, Austria Patric 
Ljung Siemens Corporate Research, Princeton, USA Christof Rezk Salama University of Siegen, Germany 
 Timo Ropinski University of M¨unster, Germany  Light Transport and Illumination Models In this chapter 
we discuss how light interactions can be computed for each voxel. Besides a brief overview of the Phong 
illumination with a focus on volume rendering, we will explain how to extend a raycaster to support shadowing 
as well as ray-traced re.ections. The way light qualities change, when travelingthroughparticipatingmediais 
explained in the last chapter of this course. 3.1 Phong Illumination The purpose of an illumination model, 
in both polygonal and volume rendering, is the simulation of real-world light interactions. During this 
simulation real-worldphenomena as re.ection and shadowing have tobe incorporated. To improve the performance, 
often the simpli.ed Phong illumination model is used, to reduce the O(n2)complexity, inherent to computing 
global illumination phenomena. By using this illumination model, only directilluminationis computed,i.e.,illumination 
notin.u­enced by otherparts of the scene. Thus not every otherpart of the scene has to be considered 
when computing the illumination for the current object and thus the complexity is reduced to O(n). Also 
within volume rendering the Phong illumination model is often used to simulate light re.ections. In the 
following we brie.y describe its usage. We are not giving an entire introduction to the Phong model, 
and refer to [26] for further reading. For simplicity we assume that a volumetric data set is represented 
by a scalar intensity function which assigns to each point x a scalar value f(x). In this context, the 
Phong computation is dependent of the following entities: Position of the current voxel x,  Gradient 
\t(f(x))at the current voxel position,  /** * Returns the diffuse term, considering the  * currently 
set OpenGL lighting parameters. *  * @param kd The diffuse color to be used.  * Usually this is fetched 
from the transfer  * function.  * @param G The computed gradient.  * @param L The normalized light 
vector. */  vec3 getDiffuseColor(in vec3 kd, in vec3 G, in vec3 L) { float GdotL = max(dot(G, L), 0.0); 
return kd * lightParams.diffuse.rgb * GdotL; } Listing 3.1: A simple GLSL shader, which computes the 
contribution of di.use lighting. Voxel color as assigned through the transfer function, and  Position 
of the light source.  To further simplify things, we assume for the following, that we have only one 
idealized point light source. We can compute the re.ection occurring at the current voxel as a sum of 
three di.erent illumination types: di.use re.ection, specular re.ection and ambient lighting. Di.use 
re.ections can be simulated by incorporating the Lambertian law, which states that the re.ected light 
intensity is relative to the an­gle of incidence. Thus we can compute the di.use re.ection Id for the 
current voxelby considering itsnormalizedgradient |. t(f(x))| and the normalized light vector L as follows: 
Id(x)= Ld,in · kd · max(|. t(f(x))|· L, 0). (3.1) Thus, we can modulate the incoming di.use lighting 
Ld,in based on its incident angle and the current voxel color kd. To achieve this e.ect within a volume 
raycastertheGLSLfragment shader showninListing3.1 can be used. In order to incorporate specular re.ections, 
e.g., when dealing with specular materials as for instance metal acquired with a micro CT scan­ner, also 
the specular colorfor a voxelhastobe computed. In contrast to di.use re.ections, specular re.ections 
also depend on the viewing angle. Therefore the normalized view vector V is implicitly used to modulate 
the incoming specular lighting Ls,in and the voxel s specular color ks: Is(x)= Ls,in · ks · max(|. t(f(x))|· 
H, 0)a . (3.2) /** * Returns the specular term, considering the  * currently set OpenGL lighting parameters. 
*  * @param ks The specular color to be used.  * @param G The computed gradient.  * @param L The normalized 
light vector.  * @param V The normalized view vector. */  vec3 getSpecularColor(in vec3 ks, in vec3 
N, in vec3 L, in vec3 V) { vec3 H = normalize(V + L); float GdotH = pow(max(dot(G, H), 0.0), matParams.shininess); 
return ks * lightParams.specular.rgb * GdotH; } Listing 3.2: A simple GLSL shader, which computes the 
contribution of specular lighting. a is used to in.uence the shape of the highlight seen on surface-like 
structures. A rather large a results in a small sharp highlight, while a smaller a results in a bigger 
smoother highlight. As it can be seen, V is not used directly, but the degree of re.ection depends on 
the normalized half-way vector H, which can be computed as follows: V + L H = . (3.3) 2 For adetailedderivation 
of this equation,please also refer to[26]. To integrate specular re.ections into a volume renderer, the 
GLSL shader shown in Listing 3.2 can be used. Please note, that since the half-way vector H is normalized, 
we can neglect the division by 2. As stated abovethePhongilluminationmodelisalocalillumination model, 
which considers onlydirectillumination,i.e.,illumination coming from the light source directly, without 
being in.uenced by other objects. However, to approximate also indirect illumination e.ects, usually 
an ambienttermis used. Thisproceedingensures, that voxels withgradients pointing away from the light 
source do not appear pitch black. Since this ambient term does not incorporate any spatial information, 
it is the easiest to compute: Ia(x)= La,in · ka, (3.4) by only considering the ambient light La,in and 
the voxels ambient color ka. The GLSL shader shown in Listing 3.3 can be used to incorpo­rate the e.ect 
within a GPU-based raycaster. Now, we know how to compute the three contributing illumination /** * 
Returns the ambient term, considering the  * currently set OpenGL lighting parameters. *  * @param 
ka The ambient color to be used.  * Usually this is fetched from the transfer  * function. */  vec3 
getAmbientColor(in vec3 ka) { return ka * lightParams.ambient.rgb; } Listing 3.3: A simple GLSL shader, 
which computes the contribution of ambient lighting. types: di.use, specular and ambient. Listing 3.4 
shows how to combine them and thus realize the Phong illumination model. To alsoincorporate attenuationbasedonthedistance 
d ofthe current voxel to the light source, the computed light intensity can be modulated before returning 
it as shown in Listing 3.5. It requires a function for computing the attenuation, which is shown in Listing 
3.6. However, thisdistancebased weightingdoes notincorporate any vox­els possibly occluding the way to 
the light source, which would result in shadowing. To solve this issue, shadowing techniques need to 
be incor­porated as described in Section 4 of this chapter. So far, we have only considered a single 
light source. To extend the Phong illumination model to multiple light sources we can simply compute 
the contribution of each light source and add them up. This summation becomes possible, due to the additive 
nature of light energy. 3.2 Gradient Computation As we have seen the gradient \t(f(x)) is important 
for both, di.use and specular re.ections. Thequality of the computedgradientsis crucialfor the visualquality 
of the rendering. Especially whendealing with ahighdegree of specular­ity, it is important that the gradients 
change smoothly along a surface. InFigure3.1 threegradient calculationmethods areshown side-by-side: 
forwarddi.erences, centraldi.erencesandSobelgradients[95]. Asex­pected, among these Sobel gradients result 
in the smoothest surfaces. This becomes directly visible when using the gradients for specular re-.ections(seeFigure3.6). 
While the visualdi.erenceisquitebig when rendering arti.cial data sets, it is not as prominent when visualizing 
 Course : Advanced Illumination Techniques for GPU Volume Raycasting Listing 3.4: This GLSL shader realizes 
Phong shading by exploiting the previously introduced shaders. shadedColor *= getAttenuation(d); Listing 
3.5: After computing the attenuation factor, it can be used to modulate the shaded color. Listing 3.6: 
Distance-based attenuation can be computed with a con­stant, a linear or a quadratic attenuation factor. 
 Figure 3.1: Visual comparison of di.erent gradient computation methods when rendering a synthetic volumedata 
set. Fromleft to right: forwarddi.erence, central di.erence and Sobel gradients. real-world data sets 
with smooth intensity changes. However, gradient .ltering can be applied(see Figure 3.2) tofurther improve 
the rendering quality. Due to its relatively high computing costs 26 additional voxel lookups are needed 
 the Sobel operator is often applied in a prepro­cessing phase. This has the drawback, that the currently 
set transfer functiondoesnothave any e.ect onthecomputedgradients. Therefore, often forward and central 
di.erences are used, when rendering perfor­mance is important. As shown in Listing 3.7 computing forward 
di.er­ence gradients requires only three extra voxel lookups. 3.3 Specular Re.ections through Ray-Tracing 
Now, that we know how to compute the gradients, we can use them as normals in various ways to improve 
the illumination. For instance, the gradients canbe usedto realize a volumetric raytracer. One approachfor 
GPU-based raycasting considering only .rst order rays to be traversed hasbeenproposedbyStegmaier et al.[97]. 
With their approach they are able to simulate mirror-like re.ections and refraction. By casting a ray 
and computing its de.ection, when a re.ection or refraction occurs. To simulatethe mirror-like re.ection,theyperformanenvironmenttexture 
lookup. An alternative approach to trace rays of higher order, would be to use additional entry and exit 
points. This can be achieved by exploiting /** * Calculate the gradient based on the A channel  * using 
forward differences. */  vec3 calcGradient(sampler3D volume, vec3 voxPos, float t, vec3 dir) { vec3 
gradient; float v = texture1D(transferFunc , textureLookup3D(volume, volumeParameters, voxPos).a); float 
v0 = texture1D(transferFunc , textureLookup3D(volume, volumeParameters, voxPos + vec3(offset.x, 0.0, 
0.0)).a); float v1 = texture1D(transferFunc , textureLookup3D(volume, volumeParameters, voxPos + vec3(0, 
offset.y, 0)).a); float v2 = texture1D(transferFunc , textureLookup3D(volume, volumeParameters, voxPos 
+ vec3(0, 0, offset.z)).a); gradient = vec3(v -v0, v -v1, v -v2); return gradient; } Listing3.7: For 
computingforwarddi.erencegradients only3 additional voxel lookups are required. By also fetching the 
transfer function, dy­namic gradient changes are considered. the work.ow shown in Figure 3.3. Inthe .rst 
stagetheinitial unmodi.ed entry and exitpointsarecom­puted as described in the .rst chapter of these 
course notes. By using thesepointsthe .rst orderrayscanbecasttogenerateoneintermediate imageaswell astoconstructa 
.rsthitpointtexture,i.e.,thepositions involumespacewherearay .rstencountersavisiblemedium(seeFig­ure3.4(right)). 
Whiletheintermediateimageis cachedin ordertobe abletoblenditinthelast stage,the .rsthitpointsareused 
astheentry points in the next recursion step. To increase the foot print of the voxels encountered at 
the .rst hit point,the .rsthitpointcomputationcanbealteredby sampling onestep further into the volume. 
This ensures that the encountered medium is su.ciently penetrated and thus more clearly visible in the 
intermediate image. The exit points for the next recursion step can be computed in the second stage, 
as shown in Figure 3.3. Based on the entry position p and the direction d of a higher order ray r, the 
intersection between r and the bounding box of the volume can be computed. This is done by .rstperforming 
an intersectionpoint test for r and the six sideplanes of the bounding box. When knowing the normal N 
and the distance to the origin D of such a side plane, we can express each point x on the plane by the 
equation: (N · x)- D = 0. By substituting the parameter form of the ray x = p +t · d into this equation, 
the following parameter value Figure 3.2: Gradient .ltering improves the visual quality especially for 
synthetic datasets: centraldi.erencegradients(left),Sobelgradients(right). for the intersection point 
can be obtained: D - (p · N) t ' = . (3.5) d · N This parameter value needs to be computed for each of 
the six side planes, before choosing the smallest positive value tmin among these. After this computation, 
the entry and exit points can be forwarded to the subsequent raycaster. When all raycasters have .nished 
rendering, the .nal image can be computed by blending all available intermediate images in stage 4 as 
shown in Figure 3.3. As mentioned above, to compute the ray direction for rays re.ected on a specular 
surface, the gradient \t(f(x)) of this surface is consid­ered. Similar to the computation of the specular 
re.ection in the Phong illumination model as described in Section 3.1, the outgoing ray can be computed 
by considering the incoming ray and \t(f(x)). In cases where a refraction occurs, the incoming ray is 
bent at the surface based on Snell s law. Snell s law is used in physics to describe the behavior of 
a ray hitting an refractive interface, e.g., the surface of a glass-like object. It expresses the relationship 
between the angle of incidence f and the angle of refraction . and says that the ratio of the sines of 
f and . is a constant depending on the present media: sinf n2 = (3.6) sin. n1 Thus, to compute the bending 
angle the refraction indices of the adjacent media, for which the refraction should be computed, has 
to be Figure 3.3: Ray tracing can be achieved by using four subsequent stages. Initially theunmodi.ed 
entry and exitpointsarecomputedforthe .rst orderrays(stage1). Afterwards,foreach recursivestep,theexitpointsarecalculatedbased 
onthe .rst hitpointsof thepreviousstep(stage2). Araycastercomputesintermediateimages based on the generated 
entry and exit points (stage 3). Finally all intermediate imagesareblendedintothe .nalimage(stage4). 
known. When assuming that these indices are n1 for the medium which is left by the ray, and n2 for the 
medium which is entered by the ray, we can compute the bending angle . based on the incoming angle f 
as follows: 2 cos(.)=1-n1 · (1- (cos(f))2). (3.7) n2 Thus, we can compute thedirection of theleaving 
ray Avec asfollows: n1 n1 Avec =Evec +| cos(f)|- cos(.)|\ t(f(x))|, (3.8) n2n2 where Evec is a vector 
representing the incoming ray. In general total re.ection has to be considered in cases the incoming 
rayhitstheobject underavery.at angle,i.e.,norefractionbut specular re.ection occurs. The critical angle 
for which total re.ection occurs can be computed as follows: fcrit = sin-1( n2). (3.9) n1 Figure 3.4: 
A synthetic Cornell box data set rendered with refraction and specular re.ection(left) andits .rsthitpoints(right). 
Thus, when the incident angle f exceeds fcrit, a specular re.ection ray has to be computed instead of 
a refractive one. The result of a ray tracer implemented using the concepts described aboveis showninFigure3.4(left). 
Asit canbe seeninFigure3.5 some scenes require only a rather low recursion depth to render them more 
realistically. Again, by using di.erent gradient calculation methods, di.erent im­age qualities can be 
achieved as shown in Figure 3.6. During theblending of theintermediateimagesin stage4, a mapping functioncanbeused, 
which expressesthespecularity of certainintensity ranges, andthusin.uencethe contribution ofhigher order 
raysto apixels color. Alternatively,byusing an automatic approach, the specularity can be set proportional 
to the length of \t(f(x)). It should be mentioned that this is not an exact measure, since the gradient 
length determines onlytheintensity changes of neighboring voxels. However, when ahigher intensity range 
occurs, one of the materials must have a higher intensity and may thus be assumed as being more specular. 
 Figure 3.6: Specular re.ections can be computed by incorporating forward di.er­encegradients(left) 
andSobelgradients(right).  Shadows Ithasbeen shown thatthe usedlightingmodelhas a majorimpact onthe 
spatial comprehension[58]. Forinstance, shadows serveasanimportant depth cue[89]. Inthis section we wouldliketodiscuss 
sometechniques for integrating shadows into a GPU-based volume raycaster. While we will focus on volume 
rendering, it should also be mentioned that some researchers address the combination of polygonal and 
volumetric data when consideringlightinteractions[113]. 4.1 Soft vs. Hard Shadows Variousshadowalgorithmshavebeendevelopedinthe 
.eld of computer graphics. Crowhasproposed a shadow volumetechniqueforgenerating shadowsforscenes containingpolygonaldata[16]. 
To compute shadows of an object, its silhouette is extracted and extruded in direction of the light rays 
in order to generate shadow polygons which form the shadow volume of an object. During rendering each 
object is tested whether it lies inside or outside a shadow volume, and thus it can be determined whether 
the object is shadowed. Due to the polygonal nature of this algorithm, it is not suited for volume rendering. 
Another common ap­proachforgenerating shadows when renderingpolygonaldatais shadow mapping whichhasbeenpresentedin1978byWilliams[109]. 
Shadow mapping is an image-based approach which exploits an additional ren­dering pass in which the scene 
is rendered from the light source s point of view in order to determine the structures closest to the 
light source (seeFigure4.1). With thisknowledge, afragment-based shadowtest can beintroducedinthemainrenderingpass,i.e., 
eachfragmentistested whether it is further away from the light source than the corresponding texel in 
the shadow map. In contrast to the shadowing approaches for slice-based volume ren­dering [4, 52], only 
little has been done in order to integrate shadows into GPU-based raycasting. However, due to the similar 
ray paradigm, it should be noted that shadows have been integrated in volume ray­tracing systems[103,102]. 
An overview of these techniques canbefound in[72]. In analogy to the polygonal shadow mapping approach, 
for volume rendering also a depth map is needed in order to store light source visi­bility. Therefore 
.rst hit positions as seen from the light source can be computed and used to compute thelight sourcedistance(seeFigure4.2). 
When rendering polygonal models a depth value is properly de.ned. In contrast, volume rendering does 
not provide depth values in general. Therefore a shadow threshold value has to be used. With this threshold 
the intensity values representing opaque geometry can be changed, and with it the content of the generated 
shadow map. The example shadow map shown in Figure 4.2 has the same resolution as the viewport, i.e., 
512× 512 pixel, which allows to generate shadows without introducing aliasing artifacts(seeFigure4.7). 
One bene.t of shadow mapping is that soft shadows can be approxi­matedby exploitingpercentagecloser .ltering[82]. 
Thisisdemonstrated in Figure 4.3, where the Visible Human torso data set is rendered with both,hard shadows(left) 
andfake soft shadows(right). Furthermore, when using shadow mapping the combination withpolygonal models 
can be supported quite easily, since the geometry can also be represented within the shadow map.  4.2 
Semi-Transparent Shadows with Deep Shadow Maps While shadow mappingallows very e.cient shadows on afragmentbasis, 
itdoes notsupport semi-transparent occluders often occurin volume ren­dering. In order to address semi-transparent 
structures, opacity shadow maps serve as a stack of shadow maps, which store alpha values instead ofdepth 
valuesin each shadow map[50]. Another more compact repre­sentationfor semi-transparent occluders aredeep 
shadow maps[68]. The used data structure consists also of a stack of textures, but in contrast to the 
opacity shadow maps, an approximation to the shadowfunctionis storedinthesetextures(seeFigure4.4). Thusitispossibleto 
approxi­mate shadowsby usingfewerhardware resources. Deep shadow mapping has .rstbeenappliedtovolumerendering 
by Hadwigeretal.[39]. Whilethe originaldeep shadow map approach[68] storesthe overall light intensity 
in each layer, in volume rendering it is advantageous to store the absorption given by the accumulated 
alpha value in analogy to the volume rendering integral. Thus, for each shadow ray, the alpha functionis 
analyzed,i.e.,thefunctiondescribingthe absorption, and approximate it by using linear functions. To have 
a termination criterion for the fragment shader which per­forms the processing, the depth interval covered 
by each layer can be restricted. However, when it is determined that the currently analyzed voxels cannot 
be approximated su.ciently by a linear function, smaller depth intervals are considered. Thus, the approximation 
works as fol­lows. Initially,the .rsthitpointforeachshadowrayiscomputed,similar as done for the shadow 
mapping described above. Next, the distance to thelight sourceof the .rsthitpoint and thealphavalueforthisposition 
are stored within the .rst layer of the deep shadow map. Since we are at the .rst hit position, the alpha 
value usually equals zero when the shadow threshold is set accordingly. Starting from this .rst hit point, 
we traverse each shadow ray and check iteratively wether the samples encountered so far can be approximated 
by a linear function. When processing a sample, where this approximation would not be su.cient, the distance 
of the previous sample to the light source as well as the accumulated alpha value at the previous sample 
are stored in the next layer of the deep shadow map. This is repeated until all eight layers of the deep 
shadow map have been created. To demonstrate the possibilities of deep shadow maps, we again use the 
simple volumetric Cornell box scene consisting of 128 × 128 × 128 voxel(see Figure4.5). Theblueball, 
which is set tobe semi-transparent by modifying the transfer function, casts a transparent shadow on 
the back wall. The di.erence to the shadow of the opaque purple box can be noticed, especially when looking 
at the shadow borders. For gener­ating the shown images, a deep shadow map consisting of eight layers 
has been used. A color coded version of the layers is also shown in Figure 4.5 (bottom). To generate 
the layers rapidly, current graphics hardware can be exploited. By using multiple render targets eight 
layers could be computed in a single rendering pass. However, using only four rendering targets and creating 
eight deep shadow layers by performing simple channel splitting allows a more e.cient storage. Therefore, 
the light source distance and the alpha value are stored in two successive channels, i.e., R and G as 
well as B and A. Thus, two shadowlayers can be represented by using only one RGBA texture. However, for 
illustra­tion purposes, we wrote these values into the R and G channels when generating thepictures showninFigure4.5(bottom). 
 Whengenerating thedeep shadowmaps, anerrorvalueisintroduced todetermine wether the currently analyzed 
samples canbe approximated by a linear function. In analogy to the original deep shadow mapping technique 
[68], this error value constrains the variance of the approxi­mation. Thiscanbedoneby adding(resp. subtracting) 
theerrorvalue at each sample s position. When the alpha function does not lie within the range given 
by the error value anymore, a new segment to be ap­proximated by a linear function is started. The e.ect 
of choosing a too small error value is shown in Figure 4.6. As it can be seen, a too small error value 
results in a too close approximation, and the eight layers are not su.cient anymore to represent shadow 
rays having a higher depth complexity. Thus especially in regions, were the two occluders both in­tersect 
a shadow ray, shadow artifacts appear. Obviously this drawback can be avoided by introducing additional 
shadow layers. This would allow a more precise approximation of the shadow function, but would also resultindecreased 
renderingperformance since additional rendering passes are required. A comparison of di.erent shadow 
computation methods is shown in Figure4.7andFigure4.8. Asitcanbe seendeep shadow mapsintroduce artifacts 
when thin occluder structures arepresent. The shadows ofthese structures show transparency e.ects although 
the objects are opaque. This results from the fact that an approximation of the alphafunction is exploited. 
Especially thin structures may diminish when approximating over too long depth intervals.  Course Notes 
Advanced Illumination Techniques for GPU Volume Raycasting  Ambient Occlusion Markus Hadwiger VRVis 
Research Center, Vienna, Austria Patric Ljung Siemens Corporate Research, Princeton, USA Christof Rezk 
Salama University of Siegen, Germany Timo Ropinski University of M¨unster, Germany  Ambient Occlusion 
for Isosurfaces Ambient Occlusion refers to techniques to simulate global lighting by estimating the 
visibility of a global ambient omnidirectional illuminant from a primitive, be it a polygon vertex, surface 
location or voxel in a volume. In this section we will outline various methods based on vol­umetric models 
or data sets. We have divided the algorithms into two classes; isosurface based volume rendering and 
full volume rendering, in­corporating semi-transparent samples and translucency. In both cases direct 
volume rendering is considered, that is, no intermediate geome­try is created for isosurfaces. The remainder 
of this chapter describes techniques for direct isosurface volume rendering. In the past many methods 
based on the theory of light transfer have beendeveloped to enableinteractivedi.useinterre.ectionsforpolygonal 
models[93,11]. Most ofthesephysically motivated approaches arebased onpre-computing illumination for 
all vertices and storingitin an appro­priate data structure which is accessed during rendering. Thus 
these algorithms support interactive modi.cation of light and camera param­eters as well as some material 
parameters. However, the application to deformablegeometryisconstrained and requiresanewpre-computation 
in most cases [94]. To address these limitations, approximations have been proposed, which are not physically 
motivated, but lead to visually convincing results. In contrast to polygonal models the structure repre­sented 
by a volumetric data set depends on the rendering parameters, which include the transfer function as 
well as thresholding parameters, since these renderingparameters canbe used to omit certain voxelsfrom 
being displayed. For instance, by only changing the transfer function or the thresholding, a medical 
volume data set can be visually mapped to manydistinct structures, e.g.,the skeleton only,the muscles 
only or the skeleton with surrounding tissue. Obviously this structural variance has a strong impact 
on the light interaction between the structures of such adata set, renderingthe currentlyknown surface-basedillumination 
techniques insu.cient for interactive volume illumination. Since trans­fer function as well as thresholding 
are altered frequently it should be possible toperform these changesinteractively[51]. Another challengeis 
the fact that volume rendering requires to compute light interactions for several samples along a viewing 
ray and thus introduces a higher level of complexity compared to computing light interactions only once 
for each fragment as is necessary when rendering opaque polygonal data. Vicinity Shading [98] simulates 
illumination of isosurfaces by tak­ing into account neighboring voxels. In a pre-computation the vicinity 
of each voxel is analyzed and the resulting value, which represents the occlusion of the voxel, is stored 
in a shading texture which can be ac­cessed during rendering. Vicinity Shading requires a new preprocessing 
when changing the rendering parameters, and it does not support color bleeding. Desgranges andEngeldescribe 
aless expensive approximation of am­bientlightthanVicinity Shading[20]. They combine ambient occlusion 
volumes from di.erent .ltered volumes into a composite occlusion vol­ume. While pre-processing time is 
greatly reduced the ambient occlu­sion volume still must be recomputed whenever the transfer function 
is changed. Wyman et al. have presented a technique to pre-compute or lazily compute global illumination 
for interactive rendering of isosurfaces ex­tracted from volumetric data sets [111]. They support the 
simulation of direct lighting, shadows and interre.ections by storing pre-computed global illumination 
in an additional volume to allow viewpoint, lighting and isovalue changes. Beason et al. present a method 
which addition­ally can represent translucency and caustics but supports static lighting only[3]. Forthatpurposethey 
extractdi.erentisosurfacesfrom a vol­umetric data set, illuminate them with a path tracer and store the 
re­sults in a new volume data set. During rendering they can interactively change the isovalue and access 
the pre-computed illumination. Penner andMitchell recentlyproposed a methodbased onhistograms to classify 
the visibility around a voxel [80]. It bears some similarities with the techniqueby Ropinski et al.[85],describedlater. 
All these surface illumination models are only applicable to isosur­facesrepresenting asingleintensity 
withinthedataset,butdonot allow to consider multiple surfaces correspondingtodi.erentintensities. Hence 
itis notpossible to represent the entire volumedata set, whereas varying intensities are one of the major 
advantages overpolygonal models, e.g., when representing di.erent types of tissue. Ambient Occlusion 
for Direct Volume Rendering Desgranges and Engel describe a less expensive approximation of ambi­ent 
light than Vicinity Shading [20]. They combine ambient occlusion volumes from di.erent .ltered volumes 
into a composite occlusion vol­ume. While pre-processing time is greatly reduced the ambient occlu­sion 
volume still must be recomputed whenever the transfer function is changed. 6.1 Local Ambient Occlusion 
Recently Hernell et al. have proposed a method for computing ambient and emissive tissueillumination 
e.ciently[42]. Thistechniqueisbased on shooting rays in several directions from each non-transparent 
voxel in the data set. The rays are terminated at predi.ned radial boundary making this technique a local 
approach. Since these rays are generated for each non-transparent voxel in the data set it bene.ts from 
multires­olution level-of-detail data reduction. Empty space is ignored and low resolution regions requirelessprocessing. 
This techniquehasbeenimple­mented using the multiresolutiontechniquesby Ljung et al.[63,65,67] In many 
applications of volume rendering itis notdesired to create a realisticglobalillumination where objects 
canbefully shadowed. Alocal modelisthereforeproposed, where alimited spherical neighborhood,O, around 
each processed voxel is evaluated. To consider semi-transparent materialsthelocal ambient occlusion model 
needstogobeyondthe all­or-nothing approach used in previous work. The proposed technique includessemi-transparent 
objectsand supportsasmoothfading of shad­ows as occluding objects occupy more or less of the local neighborhood. 
The scheme is based on sampling the locally occluding material along multiple rays originating from the 
voxel at the center of O. The incident light intensity, Ik(x), arriving at a voxel location, x, from 
one ray direction, k, is given by the equation: RO s wk Ik(x)= exp(- t(u)du)ds (6.1) a RO - a a where 
a is an initial o.set from the voxel along the ray and RO is the radius of the spherical support. A ray 
is also associated with a weight, wk, and thus enables directional weighting of the ambient light. The 
attenuation oflight contribution along the rayis estimated using the the opticaldepth, theintegral of 
transferfunctiondensities, t(s). Numerical evaluation of the integral in equation 6.1 is obtained as: 
Mm-1 LI wk Ik(x)= (1- ai) (6.2) M m=0 i=0 in afront-to-backcompositing scheme, where M isthe number ofsamples 
along the ray and ai is the sample s opacity according the the current transfer function. Lightiscontributed 
at each samplepoint, normalized tosumto one. This ensures that shadows from an occluding object increase 
smoothly as alargerfraction of the raypenetrates the occluding object. This e.ect canbeseenin .gure6.3(top-right). 
Thelocal ambientocclusionfora voxel, I(x), is then given by the sum of all incident light rays Ik 1 K 
L I(x) = K Ik(x). (6.3) k Figure6.3(bottom-rightimage), shows64 rays. Thedirectionsforthe rays are createdby 
subdividing atetrahedron,icosahedron or octahedron to di.erent levels, depending on how many rays are 
desired. Estimation of the volumetric LocalAmbient Occlusion(LAO)isper­formed as an initial step of the 
rendering pipeline. The resulting light intensity of each voxel is stored in an intensity map which is 
used in the .nal volume rendering to illuminate each sample. To further control the e.ect of the ambient 
occlusion the .nal value canbe adjusted according to agammafunction. In addition, an ambient biascanbespeci.edtoadjusttheamount 
of minimumlighting. The .nal luminance, I ' (x), is computed bythe following equation. I ' (x)=(Ibias 
+ I(x)). (6.4) 6.1.1 Emissive Tissues and Local Ambient Occlu­sion Having established a framework for 
volumetric local ambient occlusion we can proceed to incorporate emissive components from the transfer 
function. This simulates singlelightscattering ofluminous objects within the volume. Equation 6.1 is 
simply modi.ed to also incorporate colored light emission, cE. RO 1+cE(s) s Ik(x)= wk exp(- t(u)du)ds 
(6.5) a RO - a a The emittance of a sample point along a ray within O can therefore a.ect the intensity 
and color of point x. The emissive parameter is included during the .nal ray-casting as well, further 
enhancing the e.ect of luminosity. 6.1.2 Integrating Multiresolution Volumes Mapping from volume coordinates 
to packed coordinates is straightfor­ward usingtheforwardindextexture mentioned above,describedin more 
detailin[65]. The reverse mappingpresents aminor challenge, sincethe location ofa voxelinthe volume mustbedetermined. 
Potentiallyablock couldbe represented by a single voxel in thepacked texture which would require areverseindexmap 
of thesame size asthepacked textureitself. This is avoided by ignoring reverse mapping of blocks below 
a certain minimum resolution level. By ignoring blocks with the two lowest resolution levels, 13 and 
23 , the size of the reverse index map can be reduced by a factor of 64. All empty blocks are assigned 
the lowest resolution or ignored entirely and the remaining blocks are assigned higher resolutions, with 
a minimum of 43 voxels. The reverse map can then hold a block s location in volume coordinates(V) forall 
non-empty blocks. Figure6.1illustratesboth the forward and reverse mapping schemes. Neighboring blocks 
in the packed texture are not always located closely in the original volume. Since linear interpolation 
is used be­tween texels this implies that interpolation also occurs between block boundaries which then 
leads to artifacts. A distance d is therefore used to clamp the coordinates so that interpolation only 
occurs within each block [65]. In the local occlusion calculation interpolation of samples between blocks 
is not considered. 6.1.2.1 Multiresolution Ambient Occlusion Pipeline TheLAOcomputationisdrivenbytheprocessing 
of each voxelinthe vol­ume. Since a multiresolution volume management is applied the gained data reduction 
from the LOD selection also implies a speed-up of the LAO processing. With increased data reduction the 
LAO processing is also reduced, the LAO illumination is computed with the same data reduction as the 
LOD selection of the volume. The processing of each voxel can, in turn, be further accelerated by adapting 
the sampling rate depending on the voxel s block resolution level. This per-fragment pro­cessing pipeline 
is presented next. Thepipelineis executedforall voxelsin aslice of thepacked volume texture in parallel. 
A 3D texture is created to hold both the scalar values of the volume and the colored illumination of 
each voxel. The entire volume is then processed by mapping all the slices, one-by-one, as a rendering 
target of a framebu.er object. The fragment pipeline is initiated by rendering a large quad over the 
entire framebu.er, where each pixel maps to one voxel in the mapped 3D texture. This approach is e.cientbut 
requires that thehardware support the attaching of a slice of a3D texture to aframebu.er, as availablein 
thelatestGPUhardware generation, e.g. NVIDIA GF8800. For each voxel a fragment program is initiated. 
An outline of the per-fragment processing is shown in 6.2. The fragment location is used to lookup the 
corresponding voxel location in the volume through the reverse index texture, P.V. In addition a per-slice 
constant z-position is provided to .nd the correct 3D location. The pipeline then iterates over all the 
incident light directions in O. The number of rays, K, to sample can be dynamically con.gured and the 
ray directions are stored in a texture together with the directional weight, wk. The samplingalong a 
ray starts at a user-de.ned o.set, a. In addition the samplingdensity, d, and radius, RO, ofO canbe speci.ed. 
Theinitial o.set avoids unnecessary self-occlusion for voxels on the boundary of highly opaque regions. 
The sampling density is adjusted for voxels in lower resolution blocks. A voxel belonging to a block 
of resolution level, ., relative to the highest resolution level, increases the sample distance, the 
step length, by a factor of 2. . Thus, the sampling rate stays .xed relative to the sampling density 
of the underlying volume data. The immediate lower resolution will have twice the sampling distance. 
When the sampling rate changes the opacities and colors have to be corrected as well, this is done on 
the .y using the opacity correction formula: amod =1.0- (1.0- aorig)2.d . (6.6)  6.1.3 Adding Global 
Light Propagation Hernell et al. [43] recently extended the Local Ambient Occlusion ap­proach described 
above to include a Global Light propagation and .rst order scattering e.ects. Below we describe this 
approach. Figure 6.7 shows a schematic overview of the technique. The methodprogressesin several steps 
to simulatethelight transport inthe volume and, ateach step, capturesthe mainphysical contributions. 
In the .rst step opacities are composited into a global shadow volume foragivenlight sourceand transferfunction(TF) 
settings. Thecalcula­tionisbased onpiecewiseintegrationtechniques ontheGPU and sparse representations 
ofintermediate results. The method thenproceeds toin­clude .rst orderscattering of thegloballight arriving 
at each voxelbyin­tegration of scatteredlightin alocal spherical neighborhood around each voxel. This 
step is similartothelocal ambient occlusion(LAO) method describedin[42],which enhancesperception oflocal 
shapesandtissue properties. In the last step a single pass ray-casting approach is used to render the 
.nal image. The resulting application enables updates of lightposition and transferfunctions while maintaining 
interactiveframe rates yet simulating a realistic light model.  6.2 Dynamic Ambient Occlusion In this 
section we cover recent techniquesbased onRopinski et al.[85] which realizes dynamic ambient occlusion 
as well as an approximation to color bleeding when rendering volumetric data sets. The method is independent 
of the currently applied transfer function as well as the thresholding and therefore represents ambient 
occlusion as well as color bleeding like e.ects for all combinations of structures contained in a  Figure 
6.4: Emissive illumination e.ects. Left to right: Emissive light captured in the ray-tracing stage, emitted 
light illuminating the surrounding material, and both e.ects combined. Data reduction A 323 2563 8.9:1 
284 552 14.8:1 178 515 22.1:1 121 403 35.2:1 81 365 B 233 145 96 62 C 68 68 48 46 Piecewise segment length(voxels) 
4 8 16 32 323 261 284 331 436 643 267 297 339 439 A 1283 439 373 380 463 2563 1428 552 641 862 Table 
6.1: Performance, in milliseconds (ms), for di.erent levels of il­lumination updates((A),(B) and(C)) 
versus varyingdata reductions (lefttable). Thepiecewise segments are8 voxelslong. Inthe righttable the 
performance is shown for di.erent sizes of the SVR versus varying lengths ofthelocalpiecewise segmentsforlight 
update(A).Thedata reductionis8.9:1. Measurementsinboth tables areperformedfora vol­ume of 5123 voxels, 
rendered in a 1024x1024 window. The same volume, TF and rendering settings are used in .g. 6.10, using 
a step length of 16 voxels. volume data set, which can be extracted interactively by changing these renderingparameters. 
However, thepresented rendering algorithmis not basedonthephysics oflightpropagation,butprovides a visually 
convinc­ing approximation. It canbe applied todirect volume rendering(DVR) as well as isosurface shading 
techniques, and for the latter the isovalue can be changed interactively. Rendering time is kept low, 
since the pro­posed technique requires only little overhead compared to the solution of the standard 
volume rendering integral combined with the applica­tion of strictly local illumination. Besides the 
transfer function and the thresholding, lighting as well as the camera parameters can be changed interactively. 
Furthermore, in contrast to frequently used surface-based illumination models our techniquedoes notnecessarily 
require agradient calculation and is therefore also applicable to homogeneous regions. In the pre-computation 
we ensure that we analyze and store the en­vironment of each voxel x in such a way that we are able to 
compute an environmental color Eenv approximating the in.uence of the voxels neighborhood during rendering 
interactively. In order to support in­teractive modi.cation of the transfer function and the thresholding, 
the computation is performed independently of these rendering parameters. The consecutive steps needed 
are shown in Figure 6.12 and are further explained in the following subsections. 6.2.1 Local Histogram 
Generation To approximate the environmental color Eenv for a given voxel x, we exploit itslocalhistogram 
LH(x). Localhistogramshave alsobeen used in other areas of volume rendering[86,71]. For our approachlocalhis­tograms 
are adequate, because indirect illumination can be calculated properly for a given point by considering 
close objects only [18]. All voxels x lying in a sphere Sr(x) with radius r centered around x con­tribute 
to the local histogram LH(x), weighted basedon their distance to x (see step1 in Figure 6.12). Thus assuming 
that f(x). [0, 2b], with b .{8, 12, 16} being the bit depth of the data set, LH assigns to each x an 
n-tuple, with n =2b: LH(x)=(LH0(x), . . . , LHn-1(x)), with (6.7) L |x - x | LHk(x)= fdist · g(f( x),k). 
(6.8) dmin x .Sr(x) x . =x dmin denotes the minimal distance between any two di.erent voxels in the data 
set. fdist = d1 2 is used to achieve a distance based weighting and takes into account that energy falls 
o. as the inverse square of the distance. g(i, k)is used to group the intensity values appropriately: 
1, if i = k g(i, k)= (6.9) 0 , otherwise. LHk(x)representsthein.uence voxels ofintensity k in the neighbor­hood 
of x have on x. In contrast to other techniques we can not consider the attenuation of light which results 
from voxels lying between the cur­rent voxels and its neighbors, because we discard the spatial locations. 
Since we are only interested in the relative distribution of LH(x), we normalize the valuesin each LH(x)with 
respect to the number of voxels lying in the sphere Sr(x)with radius r centered around x. To capture 
the neighborhood of an object in scenes consisting of polygons, often ray casting is exploited which 
involves sampling that may in.uence the image quality. Since volume data sets are already a discretized 
representation, ray casting and thus possible sampling arti­facts should be avoided. We use a simple 
method which captures the vicinity of voxel x by iterating over all voxels x in its neighborhood and 
adding their contribution to LH(x) as de.ned by equation (6.8). By de.nition LH(x) does not contain any 
spatial information except the distance based weighting fdist inherently capturing the degree of in.u­ence 
of the voxels in the neighborhood. In order to capture also direc­tional information, we subdivide Sr(x) 
based on the gradient at x into two hemispheres. Instead of one local histogram for Sr(x), we compute 
two local histograms, one for the forward facing hemispherical region Hf(x)and one for the backward facing 
hemispherical region Hb(x)(see Figure 6.12). As already mentioned, instead of capturing the light interactions 
be­tween all voxels of adata set, we consider only the vicinityde.nedby the radius r. Obviously r is 
data set dependent, but cangenerally be chosen rather small in comparison to the number of voxels n. 
This reduces the complexity from O(n2)operations to O(r3 · n). a) Di.use Illumination b) Local Ambient 
Occlusion, 32 rays c) Close-up of vesselsin(a) d) Close-up of vesselsin(b) Figure6.5: Exampleimages 
showingthe enhanced3Dstructure made clearthrough the LAO method. (a)Di.use Illumination (b)LAO with 
illuminat-(c) LAO with emissive ing material ray-tracing Figure 6.6: Example images showing the enhanced 
information from the emissive materials. The bullet and fragments are clearly visible in the abdomen. 
The e.ect of the LAO in revealing the bone structure is also very clear. (a) (b) (c) (d) (e) Figure 
6.7: Opacityfor alocal segmentis integratedin(a) andglobal opacityisin­tegratedin(b). Asimplemethod tocompute 
Id is toperform adirectinterpolation from ag, asin(c),however, abetter approximationis obtained,in(d),by 
utilizing thepiecewiseintegrationas aninitial step. Thequality of nearby shadowcontribu­tions is thereby 
increased. In-scattering of the initial intensity approximation, Is, is illustrated in(e). (a)Opacity 
integration (b)Opacity computed (c)Pixel-wise error, for each point to the with piecewise .E light source 
(5110 ms). integrations (545 ms). Figure 6.8: A comparison of illumination integrated for each point 
to the light source, in (a), with intensities computed with our approximation using piecewise integration, 
in (b). The volume and the SVR are 5123 voxels, the view-port is 1024x1024pixels and no in-scatteringis 
considered. A color-coded errorimage that shows the pixel-wise error, .E, is provided in (c). The errors 
that appear are small(.ERMS = 0.19 and .E6=6%), and mainly appear at sharp edges and thin structures. 
 =0, RO =16 =0, RO =48 =0.2, RO =16 =0.2, RO = 48 (a)Ibias (b)Ibias (c)Ibias (d)Ibias Figure 6.9: Light 
is integrated, for a CT scan of a carp, with varying settings for Ibias and RO. Theshadowsbecomeslessdistinct 
with alargeradius(denotedin units of voxels) of the scattering sphere, O. Ibias adjusts the minimum intensity 
of each voxel. a) 163.1 b) 83.1 c) 43.1 d) 23.1 Figure 6.10: Block artifacts appear due to reduced size 
of the shadow volume representation. The original volume size is 5123 with a data reduction of 8.9:1 
and the SVR is computed for(a)323 (b)643 (c)1283 and(d) 2563 voxels. Jagged edges appearin(a) and(b), 
as canbe seeninthe close ups. Also,band artifacts arise at the side of thehead. Computationtimesfortheseimagesareasshownintable6.1. 
The length used for the piecewise segments is 16 voxels.  Figure 6.12: Work.ow: Inthe .rstpreprocessing 
stagealocalhistogramisgener­ated for each of the n voxels in order to capture the distribution of intensities 
in its environment(1). Then the n local histograms are sorted into m clusters(m<n) through a vectorquantization(vq). 
To acceleratethe vectorquantization,it oper­ates onpackedhistograms. Thepackingisbased on thehistogram 
of the volumetric data set. After the clustering is .nished the packed local histograms are replaced 
by their unpacked counterpartsduring the matching,before computing new cluster representatives(2). During 
rendering thelocalhistogramsrepresenting the clusters are modulated with the transfer function to determine 
an environmental color for each voxel(3). (a)Blinn-Phong (b)our technique Figure 6.13: Ahanddata set(244 
× 124 × 257 voxel) rendered using our surface shading technique(r=24, nc=2048)in comparison to Blinn-Phong 
shading. Notice the shading di.erences in the obscured areas. Figure 6.14: Allfourimagesshowthe sameCornellboxdataset(r=32, 
nc=1024). Forthe rightcolumnthetransferfunctionhasbeenchangedin such a way thatthe blue spheredisappears. 
Thelower row shows theimages withdi.useinterre.ections only(left) andmaterialparameterssettosimulatehighlydi.usesurfacesandan 
additional glow e.ect (right). The images have been rendered interactively by changing the transfer function 
resp. the glow mapping. Notice the color bleeding on the objects, whichdisappearsfor the blue sphere 
when removingit by modifying the transfer function. (a)Blinn-Phong (b)our technique Figure 6.15: Ourinteractive 
volume rendering method(r=20, nc=2048) applied to theVisibleHumanheaddata set(192 × 192 × 110 voxel). 
Both hemispheres, in direction ofthegradientandthe oppositedirection, are consideredduring rendering, 
and thus in contrast toBlinn-Phong subsurface scattering e.ects as well as ambient occlusion in the inner 
parts of the auricle. Course Notes Advanced Illumination Techniques for GPU Volume Raycasting  Volume 
Scattering Markus Hadwiger VRVis Research Center, Vienna, Austria Patric Ljung Siemens Corporate Research, 
Princeton, USA Christof Rezk Salama University of Siegen, Germany Timo Ropinski University of M¨unster, 
Germany  Scattering E.ects 7.1 Physical Background Before we examinehowto model realisticlightpropagationintranspar­ent 
and translucent media,let usstartby looking atthephysical theory of light transport. Inphysics, light 
has been described by three di.erent models: Geometric Optics: Geometric optics consider rays of light. 
Rays are perpendicular to the wavefronts of the actual optical waves. They canbe thought of asthepaths 
oflightparticles(photons). Photons will get re.ected or refracted at the interface between two media 
withdi.erent refractiveindex. Geometric opticsisthe usualwayto describe light transport in computer graphics. 
It fails, however, to accountfor some optical e.ects such asdi.raction andpolarization. Wave Optics: 
Wave optics consider light as electromagnetic radia­tion, as described by Maxwell s Equations. This model 
calculates the amplitude and phase of an electromagnetic wave as is passes through optical systems. It 
can account for di.raction, interfer­ence, andpolarization e.ects, as well as aberrations and other com­plexe.ects. 
Incomputergraphicsthismodelisrarely used mainly because thebene.tshardlyoutweighits computational complexity. 
Quantum Optics: Quantum optics is the fundamental theory of light transport, which accounts for both 
particle and wave characteris­tics of light. It is the only explanation for the photoelectic e.ect as 
noted by Albert Einstein. Although computer graphics mainly deals with geometric optics, there are manyin.uences 
from the other models. TheMonte-Carlointe­gration schemes described in this chapterfor example maybe 
considered a technique which accounts for the probabilistic characteristics of the motions of photons 
as derived by quantum mechanics. 7.2 Scattering For surface rendering, light transportis usually assumed 
to takeplace in the vacuum. The interaction between light and matter is restricted to the surfaces of 
objects. In the vacuum, light travels unimpededly along straightlines(rays). Photons are re.ected at 
surfaces, or refracted at the interface between dissimilar optical media. Scattering is thephysical process 
whichforceslighttodeviatefromitsstraighttrajectory. There­.ection of light at a surface point is thus 
a scattering event. Depending onthe materialproperties of the surface,incidentphotons are scattered in 
di.erent directions and thereby change their energy and frequency, which possibly causes a change in 
color. For rough surfaces the direc­tionin whichindividualphotons are scattered vary within a considerably 
higher range(di.use of Lambertian re.ection) compared with shiny sur­faces(specularre.ection). Thisisthereasonwhy 
we observe material types with a di.erent appearance. 7.3 Single Scattering To explain how scattering 
events are modelled in computer graphics, let s start with something familiar. Simple surface illumination 
may be achieved in computer graphics using single scattering events. Here, Figure 7.1: Simple single 
scattering as modelled by the Phong local illumination model with a single point light source. From left 
to right: In a perfect mirror, incidentlight(green arrow) is re.ected aboutthe surface normal(blue arrow) 
into thedirection ofperfect re.ection(red arrow). With Lambertian re.ection, incident light is scattered 
equally in all directions. With specular re.ection light is scattered inside a specular lobe around thedirection 
ofperfect re.ection. a photon coming from a light source is scattered only once before it reaches the 
eye. In terms of ray-casting we may say that the ray changes its direction only once. Single scattering 
is closely related to local illu­mination. The only di.erence is that local illumination does not include 
shadows, while single scattering may account for occluded light sources as well. In Figure 7.1, single 
scattering events are modelled using the well­known Phong illumination model. For the di.use illumination 
term in­cident light is scattered equally across the entire hemisphere centered about the surface normal. 
For the specular term, scattering is restricted to a specularlobe centered aboutthedirection ofperfect 
re.ection. More complex materialproperties may be modelled using the bidirectional re­.ectance distribution 
function(BRDF). The BRDF fr(x, ., .i,.o) at a single surface point x is a function that de.nes how light 
is re.ected at an opaque surface. The function takes an incoming light direction, .i, an outgoing direction, 
.o and a wavelength . and returns the ratio of re.ected radiance exiting along .o to the di.erential 
irradiance incident from direction .i. The wave­length parameter . is usually omitted. In this case, 
the BRDF returns a chromatic RGB value f r(x,.i,.o). In many applications, and especially real-time applications, 
the local illumination model is used in combination with point light sources, i.e. light is coming from 
exactly one or a few discrete directions. Local illu­mination with aBRDF and a singlepointlight source 
canbeformulated as a rendering equation: Li L(x,.o)= f r(x,.i,.o)cos .i , (7.1) pr2 where L(.o)is the 
radiance travelling from the surface point into direc­tion .o. Li is the emitted radiance of the light 
source, r is its distance from the surface point, and .i is the angle between the surface normal and 
the incoming direction .i. In a more realistic scenario, however, incident light may come from all directions 
on the positive hemisphere O+ centered about the surface normal. The observed radiance L(.o)must thus 
be calculated by inte­gratingtheincoming radiancefrom alldirections .i over thehemisphere: L(x,.o)= f 
r(x,.i,.o)cos .i L(x,.i)d.i. (7.2) O+ In mathematical terms, the rendering equation is a Fredholm equation 
of the second kind, which in general cannot be solved analytically. Nu­ merical solutions exist only 
if the BRDF is conservative, i.e. . .o, x, : | f r(x,.i,.o)| d.i = 1. (7.3) .+ To be physically plausible, 
BRDFs must also be positive de.nite and reciprocal. In chapter 8 we will see how the integral can be 
solved nu­merically usingMonte-Carlointegrationtechniques. TheBRDFnotation can also be modi.ed to account 
for transparent materials. 7.4 Indirect Illumination and Multiple Scattering In computer graphics, single 
scattering accounts for light emitted from a light source directly onto a surface and re.ected unimpededly 
into the observer s eye. Incident light L(.i)in Equation 7.3 thus comes directly from alight source. 
Togenerate more realisticimages, we must account for both indirect light and multiple scattering events. 
Both concepts are closely related, they only di.er in their scale. Indirect illumination means that light 
is re.ected multiple times by di.erent objects in the scene. Multiple scattering refers to the probabilistic 
characteristics of a scattering event causedby aphotonbeing re.ected multiple times within an object. 
7.4.1 Indirect Light The traditional (deterministic) ray tracing technique as shown in Fig­ure 7.2,left 
only accounts for a very special type of indirect illumination: Perfect mirror re.ections and perfect 
refractions in transparent objects. Ray tracing assumes that the path of a photon after emission from 
a light source is deterministic. The ray is traced by accounting for mirror re.ections and refractionuntilithitsapoint 
onasurfacewithdi.use or glossy characteristics. Atthispoint alocalBRDF model(such asBlinn, Phong, etc) 
is evaluated. In comparison, Figure 7.2, right shows a more realistic illumination scenario, which accounts 
for indirect illumination. In the left image the ceiling is completely black. If you look at the ceiling 
you will notice that the left part is rendered slightly red and the right part slightly blue. Thisisbecausephotonsthat 
reach theleftpart of theceiling were mostlikely re.ected from the red wall. This typical indirectillumination 
e.ect is called color bleeding. Another e.ect of indirect illumination is the caustic,thefocussedlightpattern 
causedby thetransparent sphere.  7.4.2 Transparency and Translucency Transparent materials are clear, 
so you can see through them. In con­trast,translucentmaterials(such asfrostedglass,clouds,milk etc.) 
al­lowlighttopassthrough them only di.usely. Since ray-tracing relies on deterministic scattering events, 
it cannot account for translucent media, which requires the consideration of the probabilistic path of 
photons. The ray tracing technique thus neglects many possible paths that light cantakeformthelight sourcetotheeye. 
Ingeneral,thesameistruefor GPU-raycasting of volume data. We assume that the light travels along straight 
lines. Figure 7.3 illustrates the di.erence between transparent and translu­cent material. The .gureshowsaCT 
scanof theUTCTGiantSalaman­der head rendered with di.erent tissue properties. The internal struc­tured 
of a transparent object are clearly visible,possiblydistorteddue to refraction (left). Depending on the 
opticalproperties(phasefunction), light is scattered in translucent materials (middle andright). Ingeneral, 
the exit point of a single photon is non-deterministic in translucent ob­jects. In Chapter 9, we will 
see how such images can generated using Monte-Carlo integration techniques which account for the probabilistic 
nature of light.  7.4.3 Phase Functions Up until now, we have only talked about scattering events at 
surfaces, which canbedescribedby aBRDF(Equation7.3). Inside oftranslu­cent objects andparticipating media, 
scattering eventsareconsidered to potentially happen at every point inside this object. In this case, 
the BRDF fr is replaced by the phase function h, and incoming radiance is integrated over the entire 
sphere O: Lo(x,.o)= h(x,.i,.o)Li(x,.i)d.i. (7.4) O Thephasefunctiondescribesthe scattering characteristics 
of thepartic­ipating medium. Note that the cosine termfromEquation7.3is omitted in Equation 7.4, since 
the phase function directly operates on radiance values rather than di.erential irradiance like the BRDF. 
The most popular phase function models are Henyey-Greenstein, Schlick,Mie andRayleigh(see[41,25,81]). 
The Henyey-Greenstein phase function is a simpli.ed model for ra­diation in the galaxy. It describes 
the probability of a scattering events which changes the direction of a photon by a given angle. Compared 
with other models, the Henyey-Greenstein model does not incorporate any wavelength dependency. Nevertheless, 
it may be used to realisti­cally model the optical properties of many natural phenomena, such as fog 
and clouds. The probability of a photon changing its direction of motion by an angle of . is approximated 
by the Henyey-Greenstein model as 1- g2 G(., g) (7.5) = . 32 (1 + g2 - 2g cos.) The parameter g 
. [-1, 1] describes the anisotropy of the scattering event. A value g = 0 denotes that light is scattered 
equally in all direc­tions. A positive value of g increases the probability of forward scatter­ing. Accordingly, 
with a negative value backward scattering will become more likely. If g = 1, a photon will always pass 
through the point unaf­fectedly. If g = -1it willdeterministically be re.ectedinto thedirection it came 
from. Figure 7.4 shows the Henyey Greenstein phase function for di.erent anisotropy parameters g. We 
can use the probability distribution from the Henyey-Greenstein model to construct a phase function according 
to Equation 7.4 1- g2 h HG(x,.i,.o) (7.6) = 4p (1 + g2 - 2g (.i · .o)) 32 with cos. =(.i · .o). 7.4.4 
Scattering at Transparent Surfaces Thephasefunction modelisidealfordescribing scattering events inpar­ticipating 
media, where the probability of a scattering events depends only on the angle between the incoming and 
outgoing direction, regard­less of their explicit orientation (rotation invariance). In contrast, the 
BRDF at agivenpointdepends onthe orientation of the normal vector of the surface element. If we want 
to render images from volume data obtained by tomographic scans, however, we are not only interested 
in the scattering events in the homogenous regions, but also in the surfaces contained in the volume 
data. In this case we need a phased function whichhas an orientation,justlikethe normal vectorintheBRDF. 
An alternative to using the phase function here is to supplement the BRDF by a transmissive term. For 
semi-transparent surfaces, scat­tering events are still considered to happen only at surface boundaries, 
but light can be transmitted through transparent or translucent mate­rials. The BRDF in Equation 7.3 
is supplemented by a bidirectional transmittance distribution function(BTDF) f t de.ned on the opposite 
hemisphere. Both the BRDF and the BTDF are often considered to­gether as a singlebidirectional scattering 
distributionfunction(BSDF). The BSDF f (x,.i,.o)leads to a rendering equation according to Lo(x,.o)= 
f (x,.i,.o)Li(x,.i)|(n · .i)| d.i, (7.7) O and can easily be convertedtothephasefunction notation(Eq.7.4) 
by incorporating the cosine term: h BSDF(x,.i,.o)= f (x,.i,.o)|(n · .i)|. (7.8) For volume rendering 
in practice, this means that we may account for surfaces inside the data, e.g. by looking at the gradient 
magnitude. If the gradient magnitude exceeds a given threshold ., we can use a BSDF to model the scattering 
events using the normal vector of the isosurface atthatpoint(which coincides withthe normalizedgradient 
vector). In rather homogeneous regions, where the gradient magnitude is small, we model rotation-invariant 
scattering events according to the Henyey-Greensteinphasefunction. Foragivenscalar .eld s(x), suchas 
a tomographic scan of an object, a suitable phase function would thus be Vs(x) h BSDF(x,.i,.o) with 
n = |Vs(x)|, if |Vs(x)| >. h(x,.i,.o)= h HG(x,.i,.o), otherwise Thefreeparameters of the model(e.g the 
anisotropy g in h HG)may be obtained as a function of the scalar value s(x) as well using a transfer 
function. Now we have seen the equations we need to solve to build scattering into our GPU-based volume 
ray-caster. In the following chapter, we will see how we can solve these integrals using Monte-Carlo 
techniques.  7.5 A Practical Phase Function Model In an example in Chapter 9.3 we will use a simple 
phenomenological phasefunction model, whichis equaltotheBSDF at speci.edisosurfaces and otherwise contains 
a simple forward peak or a Henyey-Greenstein term as described above. The parameters of this phase function 
model are derived from the underlying scalar .eld s(x). To keep the model controllable by the user, we 
restrict scattering events to happen at a .xed set of isosurfaces. Between these isosurfaces the ray 
direction does not change, but attenuation may still happen. At the speci.ed isosurfaces, thegradient 
magnitude Vs(x)isguaran­teedtobe non-zero. Thegradientvectoris normalized andits orientation is adjusted 
to match the viewingdirection. Asin mostillumination mod­els we assume the viewing vector v to point 
towards the eye position. Vs(x) if Vs(x)· v = 0 .Vs(x). g(x)= (7.9) Vs(x) - if Vs(x)· v < 0 .Vs(x). Our 
phenomonological BSDF is illustrated in Figure 7.5. The re.ec­tive part fr is equal to the specular and 
di.use term of the Phong local illumination model, fr = fdi. + fspec (7.10) fdi.(v . .i)= kd (n · .i) 
(7.11) fspec(v . .i)= ks (r · .i)s (7.12) with r =2n(n · v)- v. (7.13) The transmissive part scatters 
the transmitted light in an additional Phong lobe centered around the negative viewing vector -v in case 
of non-refractive transmission, ft(v . .i)= kt (-v · .i)q. (7.14) For refractive transmission, the Phong 
lobe is centered around the re­fractedraydirection t (Figure7.5, right). Inthis casethe refractedvector 
t is calculated according toSnell slaw and replaces -v inEquation7.14. Figure 7.3 shows the in.uence 
of the exponent q for the transmission lobe. The resulting BSDF modelhas5parameterstoadjustforeach spec­i.ed 
isosurface: the di.use, specular and transmissive material coe.­cients kd, ks and kt and the exponents 
s and q for the specular and transmissive lobe. 7.6 Further Reading Material properties of translucent 
surfaces, such as skin or paper, are often modeled using the bidirectional surface scattering re.ectance 
dis­tribution function (BSSRDF), which require two surface locations to be speci.ed. Apractical modelhasbeenproposedby 
Jensen et al.[46]. Donner etal.[22]have supplemented this modelfor multi-layered translu­cent materials. 
Interactive rendering technique for translucent surfaces havebeenpresentedby Lensch et al.[59] andCarr 
et al.[10].  Monte-Carlo Intergration Intheprevious section wehave seenthatin ordertogeneratephysically 
plausible images, we need to solve Equation 7.4. In order to obtain the radiance at a given point x in 
a given direction .o, we need to inte­grates the incident radiance over the sphere around the point, 
weighted by the phase function. I have also noted, that this integral cannot be solved analyticallyingeneral. 
We thus need to use numericalintegration schemes. 8.1 Numerical Integration The most popular numerical 
integration scheme is the Riemann sum. Let shave alook at a simple one-dimensional integral of afunction 
f(x), b I = f(x)dx. (8.1) a To solve thisintegral numerically using aRiemann sum, we approximate the 
function f(x) by a piecewise constant function and calculate the area of the rectangularblocks as shown 
inFigure8.1. Theheights of the blocks are obtained by sampling the function in n equidistant intervals 
.x = b-na . An approximation to the integral is thus obtained by b n-1 L b - a b - a I = f(x)dx  f(xi) 
with xi = i · (8.2) a i=0 n n The approximation error of this sum depends on the number of samples n. 
If n approaches in.nity, the approximation error will become zero. 8.1.1 Blind Monte-Carlo Integration 
What happens if we modify the Riemann sum, such that the function is sampled at randomized locations, 
as shown in Figure 8.1 right?. Let Figure 8.1: A function f(x) (left) may be integrated by a Riemann 
sum, which approximates the function using piecewise constant blocks (middle). Alternatively, blind Monte-Carlo 
integration will sampling (right) samples the volume at random positions. us assume that the samples 
are taken with a uniform probability distri­bution, which means that the probability of each sample position 
xi is constant. We will use the sameformula(Eq.8.2) to estimate theinte­gral, however, the sample position 
x . [a, b] now is a random variable with a uniform probability dendisty p(x): n-1 L b - a 1 (I) = f(x) 
with p(x)= (8.3) nb - a i=0 To prove that such the estimator (I) is useful, we must show that it is unbiased, 
i.e that the expectation value of (I) is equal to the exact solution of the integral: n-1 L b - a E[(I)]= 
E[ f(x) ]= n i=0 n-1 b - a L = E[f(x)]= n i=0 b - a n-1 L b = f(x)p(x)dx = n i=0 a b - a n-1 L b 1 = 
n i=0 a f(x)b - a dx = 1 n-1 L b = f(x)dx = n i=0 a b = f(x)dx = I (8.4) a We have now shown that the 
estimator (I) is unbiased and converges against the integral sought-after. This means that if the number 
of sam­ples n approaches in.nity, the estimation (I) will converge against the exact value of the integral, 
just like the Riemann sum. The practical meaning of this statement, however, is limited, because we have 
not yet investigated what happens if we use a .nite number of samples n. Ifyou compareFigure8.1middle 
and left,yourintuitionis right, that the stochastic estimatoris not asgood as theRiemann sum. Indeed, 
one canshowthatforagrowing number of samples n, the approximation er­ror of theRiemann sumdecreases muchfaster 
than the error of stochasic estimator (I). The Riemann sum converges faster to the exact solution. The 
estimator (I) we have just seen is called a blind Monte-Carlo estimator. In the following section we 
will see that there are advantages of the blind Monte-Carlo estimator compared to the Riemann sum. The 
estimatoris called blindbecauseitdoes not make any assumptions about the function f(x) to be integrated. 
In Section 8.3 we will also see how the convergence of the blind Monte-Carlo estimator can be improved 
by using a-priori information about the function f(x)  8.2 When Does Monte-Carlo Integration Make Sense? 
In the previous section we have seen, that the blind Monte-Carlo es­timator requires much more samples 
to achieve the same accuracy as the Riemann sum. Nevertheless, there are cases, especially in computer 
graphics, where the Monte-Carlo estimator is more e.cient than the Riemann sum. The blind Monte-Carlo 
estimator has two important ad­vantages: Noise instead of aliasing: If the function f(x) contains high 
frequencies, such as the pattern in Figure 8.2 top row, equidistant sampling will inevitably lead to 
aliasing artifacts as can be seen in the right column of the .gure. Even if the number of samples is 
increased by a factor of 16, aliasing will still be strongly visible (bottom left). Stochastic sampling 
may have a higher approxima­tion error, nevertheless theimages are visually morepleasing, since the human 
eye is less sensitive to noise than to aliasing artifacts.  Independence of any grid structure: The 
second bene.t of stochastic sampling in general is its independence of a grid struc­ture. This is best 
explained using an example: Let us assume we  are going to integrate a three-dimensional function instead 
of the one-dimensional one in our example. We may try to integrate the function using a Riemann sum with 
10 × 10× 10 =1000 samples. Now if we .nd out that the appoximation error is still too large, we need 
to increase the number of samples. The next possibility we have is to use 11 × 10× 10 samples, so we 
will have to evaluate 100 more samples in order to increase the accuracy. Since stochas­tic sampling 
does not require any sampling grid, we can increase the samples in steps of 1. Stochastic sampling is 
thus advanta­geous if the function to be integrated is high-dimensional and the evaluation of a sample 
is rather expensive. In Figure 8.2 we have already seen the visual bene.t of blind Monte-Carlo sampling. 
Stochasic sampling, however, may be further improved by integrating partial knowledge about the function 
to be integrated intothe estimationprocess. We willdiscussthisinthefollowing section which deals with 
importance sampling. 8.3 Importance Sampling In our simple exampleinSection8.1.1we used a uniformprobabilityden­sityfunction(PDF) 
fordetermining the samplepositions x (see Equa­tion 8.3). Monte-Carlo integration, however, may be performed 
with an arbitrary PDF p(x). The general Monte-Carlo integration formula is: L 1 n-1 f(x) (I) = (8.5) 
np(x) i=0 We can easily show that this is a useful estimator again by calculating its expectation value: 
L 1 n-1 f(x) E[(I)]= E[ ]= np(x) i=0 L 1 n-1 b f(x) = p(x)dx = n a p(x) i=0 n-1 L b 1 = f(x)dx = n i=0 
a b = f(x)dx = I (8.6) a Now, the question is: Can we .nd a PDF p(x) which increases the convergence 
of the estimator (I)? It turns out that the convergence of the Monte-Carlo estimator is optimal, if theprobability 
density function p(x)is a multiple ofthefunction f(x)itself. Inpractice this makes sense: We need to 
place more samples at positions x where the function f(x) is large, and less samples where the function 
is small. This way we will focus the computational load to where the largest contribution to the integral 
is expected. However, we cannot use the function f(x)directly as a PDF because it needs to be normalized: 
b f(x)dx p(x) = c · f(x) and p(x)dx = 1 (8.7) a This leads us to: 1 1 c = b = I (8.8) a You might notice 
the .aw: Calculating the constant c to normalize the ideal PDF would require us to know the sought-after 
integral I in ad­ Figure 8.3: A function f(x) with a sharp and narrow peak (left). Equidistant sampling 
of the Riemann sum might easily miss the peak. Based on a-priori infor­mation about the function f(x), 
Monte-Carlo integration will place manysamples at locations close to the peak. vance. Nevertheless, even 
if we have only partial knowledge of the func­tion to be integrated, we can signi.cantly improve the 
convergence of the estimator. Now, let us .nally go back to our rendering equation: Lo(x,.o)= h(x,.i,.o)Li(x,.i)d.i. 
O Although we do not know the integral completely, we have knowledge about the phase function h(x,.i,.o). 
We should thus shoot many rays into directions .i where the phase function h is high, and less samples 
where the phase function is low. Also, if we know anything about the lightingenvironment, we should shoot 
many raysintodirections .i where L(.i) is expected to be large. This concept is known as importance sampling 
and signi.cantly increases the convergence of the Monte-Carlo estimator. Now you may see another important 
bene.t of Monte-Carlo integra­tion over theRiemann sum: Try tointegrate afunction f(x)whichhas a known 
very sharp and narrowpeak(Figure8.3). SinceRiemannintegra­tion does not take into account a-priori information 
about the function f(x), its equidistant sampling might easily miss the narrow peak com­pletely. If the 
location of the peak is known in advance, Monte-Carlo integration may place many samples at location 
near the peak and ap­proximate the .at regions with fewer samples. With Riemann integra­tion, the boxes 
in Figure 8.3 all have equal width. With Monte-Carlo estimation (see Eq. 8.5), the width wi of a box 
in Figure 8.3, right, is depending on the probability of the sample: 1 wi = (8.9) n · p(xi) This means, 
that the more likely a samples position xi is, the more narrow is the box representing the sample. 8.4 
GPU-based Importance Sampling In this section we arehaving alook atdi.erentimplementation ofimpor­tance 
sampling on theGPU. In Chapter 9 we will seepractical examples which apply these strategies for GPU-based 
volume ray-casting. 8.4.1 Focussing of Uniform Distribution Afast and simpleGPU-based techniqueforimportance 
sampling, which is straight-forward to implement, has been used in [88]. Here, pre­computed random directions 
uniformly distributed on the unit sphere are used as basis. Simple but e.ective strategies to omit regions 
with only little contribution according to the phase function or BSDF are employed. To avoid the necessity 
to account for di.erent probability distribu­tions within a shader program, this approach restricts itself 
to uniform distributions. In Section 8.4.2 we will see that the solid angle .i of a sampledepends onitsprobability,justliketheboxesinFigure8.3 
right. Uniform samples are admittedly not the optimal sampling schemes, but they allow us to remove p(x)from 
the sum in Equation 8.5 and re­placethe weightedsumby a simple averagefor e.ciency. InSection8.4.2 we 
will see a GPU-based implementation of importance sampling which deals with di.erent pdfs in the shader. 
8.4.1.1 Rejection Sampling For afast access to randomized direction vectors from within afragment shader, 
a set of random value triplets representing points uniformly dis­tributed on the unit sphere is pre-computed. 
We generate such vectors by the use of rejection sampling: We obtain triplets rS of uniformly dis­tributed 
random valuesinthe range of[-1, 1]. We discard all vectors with a magnitude larger than 1 and normalize 
the remaining vectors to unit length. The pre-computed random vectors are stored in a 2D and 3D texture. 
By sampling the texture at runtime, we can generate samples uniformly distributed on the unit sphere. 
 8.4.1.2 Focussing The randomdirections obtainedfrom the texture candirectlybe used to sample the phase 
function. For di.use, surface-like re.ection, however, itis necessary to restrict the randomdirections 
to ahemisphere centered around a given unit vector n. We can easily generate such samples by negating 
all random vectors outside the given hemisphere, rH(n) = sgn(n · rS)rS, (8.10) with sgn being the signum 
function. For e.ciently sampling a specular Phong lobe, we need to focus the sampling directions to a 
narrow cone centered around a given direction of re.ection. A simple way of focussing ray directions 
is to compute a weighted sum of the hemispherical random samples rH and thedirection of perfect re.ection 
h: rP(h)= a · rH(h) +(1 - a)h. rP(h) rP(h) = (8.11) . rP(h). The scalar weight a determines the maximum 
cone angle of scattering around the direction h. A value a =1 means scattering in all directions onthehemisphere, 
while a value of a =0resultsinthe(non-randomized) ray direction perfectly focused into direction h. Todetermine 
an appropriate value of a for agiven specular exponent s, we calculate the maximum re.ection angle . 
max, at which the specular term falls below a user-speci.ed threshold T (say 0.1), . max(s) = max{. | 
cos(.)s > T }. (8.12) Solving this equation yields v . max = arccos(s T ). (8.13) Figure8.4illustrates 
the relationshipbetween thefocus weight a and the angle .. The maximum angle between a hemispherical 
sample rH and the re.ection direction h is p 2. The interpolation according toEqua­tion 8.11 moves the 
point along the dotted line and the normalization raises to interpolated point back to the hemisphere. 
From Figure 8.4, it is easy to derive a relationship between a and the maximum angle . max by - p)1+tan(. 
max a = 4(8.14) 2 The three sampling techniques outlined in this section should be suf­.cient to e.ectively 
increase the convergence of the Monte-Carlo esti­mator. Importance sampling requires knowledge about 
the scattering distribution at the surfaces. Which sampling strategy to use depends, of course, on the 
phase function model. The drawback of this technique is that the threshold introduced in Equation 8.12 
results in a Monte-Carlo estimator which is no longer un­biased. This means that the estimator will not 
converge against the correct solution of the integral. Instead a small error is introduced by truncating 
the specular lobes. Nevertheless, the described technique al­lows us to e.ciently generate images at 
high visual quality, as will be shown in Chapter 9. A mathematically more accurate solution will be discussed 
in the following section.  8.4.2 Sampling of Re.ection MIP-Maps An e.cient implementation of importance 
sampling has been published in volume3 oftheGPUGems[14]. Thisimplementation utilized area­preservingparameterizations 
ofdi.erentBRDF models[81]. 8.4.2.1 Area-Preserving Parameterizations The Phong specular lobe centered 
around the re.ection vector r, for ex­ample, maybe writteninpolar coordinates(f = azimuth, . = elevation): 
fspec(.i,fi)= cos n .i sin .i. (8.15) du = r d. dv = r sin. df dA = = du dv r 2 sin . d. df d. = dA r2 
= sin. d. df  The additional sine term is introduced by changing from solid angle parameterization of 
the hemisphere to polar angle parameterization as outlined in Figure 8.5: d. = sin. d. df (8.16) To calculate 
the probability density function for sampling the specular lobe, we need to normalize the lobe to integrate 
to one: cosn .i sin.i p(.i,fi)= 2p p/2 00 cosn .i sin .i d. df (n +1) = cos n .i sin.i (8.17) 2p Notethatthisfunctionisindependent 
of theazimuth angle f because of rotational symmetry. We can thus split the PDF into two independent 
equations for f and .: p(.i) = (n +1)cosn .x sin .i (8.18) 1 p(fi) = (8.19) 2p Inpractice, we caneasilygenerate 
randomvariables . . [0, 1] with a uniformdistribution(e.g. using the rand() function in C/C++). Note 
that Equation 8.19 is already a uniform distribution within the range [0, 2p]. Now we need amapping fromauniformdistributiontothePDF 
in Equation 8.18. We calculate the inverse of the cumulative PDF: .i P (.i)= p(.)d. 0 (n+1) .i = cos(8.20) 
 1 ) P -1(.) = arccos.(n+1(8.21) Now, if we have a pair of uniformly distributed random variables (.1,.2). 
[0, 1]× [0, 1], we can generate a random direction: 1 ( n+1) .i = arccos .1 (8.22) fi =2p.2 (8.23) We 
can then convert the polar coordinates back to direction vectors in cartesian coordinates. Since these 
samples are notuniformlydistributedlikein thefocussing approach, we need totakeinto account the solid 
angle .i for eachsample: 1 .i = . (8.24) n · p(.i,fi) Compared to thefocussing approach, we willhave 
to weight each sample byits corresponding solid angle: Wedivide each sample(.i,fi) by its probability 
p(.i,fi)before averaging the samples. 8.4.2.2 Re.ection MIP-Map Inthe approachdescribedin[14], a mip-mappedparabolic 
environment map is used to store the incident light averaged within di.erent solid angles. The MIP-levels 
of the re.ection map contain pre-.ltered envi­ronment maps[40]forapre-de.ned rangeof solid angles. TheMIP-level 
for a texture sample is then calculated as a function of its solid angle (Eq. 8.24). This restricts the 
technique to local illumination of surfaces or isosurfaces. Furthermore, .ltering of environment maps 
is restricted to BRDFs with a rotational symmetric specular lobe. 8.4.2.3 Filtering Environment Maps 
The .ltering of an environment map, however, can also be performed using Monte-Carlointegration. Wecaneasilygeneratea 
.ltered version Figure 8.6: Examples of .ltered environment maps: Original environment map (left). Filteredversion 
with a specular exponent s =100((middle)). Irradiance map created by .ltering over the entire hemisphere 
(right). (Shader code in Listing 8.1) of an environment map by rendering a screen-spaced quad (with the 
correct normal vectors speci.ed at the vertices) into the six faces of a cubemap(render-to-texture). 
The .lteringisperformedbythefragment shader shown in Listing 8.1. This shader uses the focussing technique 
from Section 8.4.1. For a given specular exponent s, the parameter focus must be calculated according 
to Equation 8.13. The shader reads from an arbitrary environment cube map and a pre-de.ned 3D texture 
containing unit random directions calculated using rejection sampling. Examples of .ltered environment 
maps are shown in Figure 8.6.  8.5 Further Reading In this chapter we have covered the theory of Monte-Carlo 
integration to an extent which is su.cient to understand the rest of the course. More information on 
Monte-Carlo integration can be found in the book by Pharr and Humphries[81], or in the SIGGRAPH 2002 
course notes on Advanced Global Illumination by Philip Dutr´e, Kavita Bala and Philippe Bekaert. 122 
Course : Advanced Illumination Techniques for GPU Volume Raycasting 1 #define NUMSAMPLES (200.0) 2 3 
float4 main( 4 float2 uv : TEXCOORD0, 5 float3 normalIn : NORMAL, 6 uniform float4 randSeed, 7 uniform 
float focus, 8 uniform sampler3D noiseTex, 9 uniform samplerCUBE envMap 10 ) : COLOR 11 { 12 float3 N 
= normalize(normalIn); 13 float4 sample = 0.0; 14 15 for(float i = 0; i < NUMSAMPLES; ++i) { 16 17 // 
calculate a randomized 3D texture coordinate 18 float offset = randSeed.a * i/NUMSAMPLES; 19 float3 randUV 
= (NormalIn + offset*randSeed.xyz); 20 21 // sample a precalculated 3D noise texture 22 // to obtain 
a randomized sampling direction 23 float3 randDir = expand(tex3D(noiseTex,randUV)); 24 randDir = normalize(randDir); 
25 26 // invert direction of back-facing 27 float cosTheta = dot(randDir,N); 28 if cosTheta < 0.0) { 
29 randDir = -randDir; 30 cosTheta = -cosTheta ; 31 } 32 33 // focus the direction to specular lobe 34 
randDir = normalize(lerp(randDir,N,focus)); 35 36 // sample the environment cube 37 sample += cosTheta 
* texCUBE(envMap,randDir); 38 } 39 // average all samples 40 sample /= NUMSAMPLES; 41 42 return sample; 
43 44 } Listing 8.1: Cg fragment shader for .ltering of environment maps. The shader is intended to 
read from an environment cube map and render into the faces of a re.ection cube map.  GPU-Based Monte-Carlo 
Volume Raycasting 9.1 Monte-Carlo Techniques for Isosur­faces In the .rst part of these course notes 
we have seen the basic implemen­tation of GPU-based ray-casting. For each pixel it casts a ray into the 
volume and samples the volume on a straightline. Ifthe volumeis stored in a single 3D texture, however, 
we have the freedom to modify the ray direction to accountfor multiple scattering events with respect 
to agiven phase function. We then of course need to cast multiple rays per pixel. As an initial implementation, 
we may modify the fragment program for GPU-raycasting to calculate the .rst directional derivative of 
the scalar .eld along the viewing ray using central di.erences. Let s(x)be the scalar .eld, and xi,i 
.{0, 1,...m} be the sample positions along the viewing ray(i =0 is closest to the eye): ds(x) s(xi+1)- 
s(xi-1) = Vs(x)· v (9.1) dv xi+1 - xi-1 If the magnitude of the .rst derivative is larger than a speci.ed 
threshold, we assume a surface scattering event according to the phase function model described inSection 
7.5. Weprocess the scattering event by calculating a randomized direction using importance sampling and 
scatter the ray into this direction. The user-speci.ed threshold restricts strongdirectional changes 
toisosurfaces with ahighgradient magnitude, while rays may pass forward through rather homogenous regions 
of the volume. We restart the fragment program for GPU-based raycasting multiple times with di.erent 
random values. The program samples the volume at equidistant positions along the ray and integrate the 
phase function while the ray travels through the volume. When the viewing ray leaves the volume s bounding 
box, the incident radiance is sampled from an environment cube map. The accumulated radiance RGB triplet 
is .nally written into the frame bu.er and averaged with the previous passes. This straight forward implementation 
has the problem that many calculations,such asdeterminationof the .rstscattering event,areper­formed 
again and again in successive passes. To improve this, we use a multi-pass rendering technique to reuse 
as much information aspossible. The di.erent rendering passes are described in the following sections. 
 First Hit Pass: The .rst pass calculates the intersection of the viewing rays with the .rst isosurface. 
It is reused in subsequent passes to start viewing rays directly at the relevant position.  Local Illumination 
Pass: The local illumination pass utilizes Monte-Carlo integration techniques to calculate surface shading 
with shift-variant BRDFs.  AmbientOcclusionPass: The ambientocclusionpass calculates self-shadowing 
of the isosurface.  Scattering Pass: The scattering pass accounts for translucency and transparency. 
  The .rstthreepassestogethermay beusedtogeneratehigh-quality renditionsofisosurfaces, such astheonedisplayedinFigure9.1 
atinter­active frame rates. Thesepasses will be explained indetailin thefollow­ing Section. The scattering 
pass is more computationally expensive and willbedescribedinSection9.3. It maybepreviewed atinteractive 
rates with a reduced quality (more noise due to fewer samples). Rendering the scattering pass in .nal 
quality will take up to a few seconds. 9.2 Isosurfaces with Shift-Variant or Anisotropic BRDFs The process 
of rendering high-quality shaded isosurfaces can be divided into three di.erent tasks: 1. Extraction 
of the isosurface. This includes the determination of position, normal direction and other surface properties 
obtained from the scalar .eld. 2. Localilluminationof thesurface. Thisincludesevaluationof shift­variant 
BRDFs at the surface. 3. Shadow calculation. We will use Monte-Carlo integration to calcu­late ambient 
occlusion.  Each task is performed in a separate rendering pass as outlined in the following sections: 
9.2.1 First Hit Pass In a .rst rendering pass the front faces of the bounding box are raster­ized. This 
.rst pass simultaneously renders into two .oating-point o.­screen bu.ers using multiple-render-targets. 
The basic fragment shader is shown in Listing 9.1. The shader contains a loop that successively samples 
the volume along the viewing ray. The user has speci.ed the scalar values of an isosurface. While sampling 
the volume along the ray, we continuously check if the speci.ed isosurfaces was intersected. (In the 
sample code, we assume that the scalar value of the .rst sample is below the isovalue) If the .rst isosurface 
is hit, the shader breaks out of the loop. The 3D texture coordinate of the intersection point is written 
tothe .rstrendertarget. Thegradientmagnitudeiscalculated,and the vector is then normalized. Gradient 
direction and magnitude are stored 126 Course : Advanced Illumination Techniques for GPU Volume Raycasting 
 1 uniform sampler3D VolumeTexture : TEXTURE0; 2 uniform float isoValue; 3 uniform float4x4 World2Tex; 
4 struct COLOROUT { 6 float4 value0 : COLOR0; 7 float4 value1 : COLOR1; 8 }; 9 COLOROUT main(float3 uvw 
: TEXCOORD0, 11 float3 vecCameraPos : TEXCOORD1, 12 float3 vecRayPos : TEXCOORD2) 13 { 14 COLOROUT retval; 
 16 float4 firstHit = 0..xxxx; 17 float4 gradient = 0..xxxx; 18 // ray direction 19 float3 dirRay = normalize(vecRayPos-vecCameraPos); 
 21 for(int j=0;j < MAX NUM SAMPLES; j++) { 22 // sample volume at ray position 23 float3 uvw = mul(World2Tex,float4(vecRayPos,1.0)); 
24 float sample = tex3D(VolumeTexture, uvw).r; // check for hit with isosurface 26 if (sample > isoValue) 
{ 27 // calculate gradient 28 float3 grad = getGradient(uvw); 29 gradient.a = length(grad); gradient.rgb 
= normalize(grad); 31 32 firstHit.rgb = uvw; 33 break; 34 } // check if ray exits the volume without 
intersection 36 if ((uvw.x < 0.0) || (uvw.y < 0.0) || (uvw.z < 0.0) || 37 (uvw.x > 1.0) || (uvw.y > 
1.0) || (uvw.z > 1.0)) { 38 break; 39 } // proceed one step along the ray 41 vecRayPos += STEPSIZE*dirRay; 
42 } 43 44 retval.value0 = firstHit; retval.value1 = gradient; 46 47 } Listing 9.1: A basic Cg fragment 
shader for determining the .rst hit with a speci.ed isosurface. A possible implementation of the function 
getGradient in line 28 can be found in Listing 9.2. 1 float3 getGradient(float3 rayPos) { 2 3 float3 
grad; 4 grad.x = tex3D(VolumeTexture,rayPos + float3(EPSILON,0.0,0.0)).r - 5 tex3D(VolumeTexture,rayPos 
-float3(EPSILON,0.0,0.0)).r; 6 grad.y = tex3D(VolumeTexture,rayPos + float3(0.0,EPSILON,0.0)).r - 7 tex3D(VolumeTexture,rayPos 
-float3(0.0,EPSILON,0.0)).r; 8 grad.z = tex3D(VolumeTexture,rayPos + float3(0.0,0.0,EPSILON)).r - 9 tex3D(VolumeTexture,rayPos 
-float3(0.0,0.0,EPSILON)).r; 10 frad /= (2.0*EPSILON); 11 return grad; 12 } Listing 9.2:Cg shaderfunctionfor 
estimating thegradient vector of the volume using central di.erences. 1 // interval bisection 2 float3 
start = vecRayPos-STEPSIZE*vecStep; 3 float3 end = vecRayPos; 4 5 for(int b = 0; b < 10; ++b) { 6 float3 
testPos = (start+end)/2.0; 7 float3 uvw = mul(World2Tex,float4(testPos,1.0)); 8 if (tex3D(VolumeTexture, 
uvw).a < isoValue) { 9 start = testPos; 10 } else { 11 end = testPos; 12 } 13 } 14 15 vecRayPos = (start+end)/2.0; 
16 uvw = mul(World2Tex,float4(vecRayPos,1.0)); 17 Listing9.3:Cg shaderfunctionforimproving theaccuracy 
of theisosur­face detection using interval bisection. as a .oating-point RGBA quadruplet in the second 
render target and the fragment program terminates. The gradient vector of the scalar .eld is always perpendicular 
to the isosurface. It may be estimated numerically using .nite di.erences scheme. Listing 9.2 shows a 
working implementation. The function getGradient obtains six additional texture samples around the inter­section 
point to estimate the gradient vector using central di.erences. Foradetailedderivation of thegradient 
estimation scheme, seeChapter 5.3.1 in[25]. To improve accuracy of the isosurface detection, a few iterations 
of interval bisection may be performed to come closer to the exact inter­section point, as suggested 
in [38]. If the .rst sample larger than the isovalue is found, we go half a stepsize back and see if 
this sample is already larger than the isovalue. We proceed this way several times, halving the stepsize 
with each iteration. A code sample for interval bi­section is shown in Listing 9.3. It may be inserted 
in Listing 9.1 right after line 21. Thecontentsof thetworendertargetsforthe .rst-hitpassareshown in 
Figure 9.2. Successive rendering passes start the ray integration di­rectly at the intersection point 
with the .rst isosurface by reading the 3D texture coordinate determined in the .rst pass. 9.2.2 Deferred 
Shading Pass Given the contents of the two rendering targets from the .rst hit pass, the surface shading 
for the isosurface may be calculated by reading the surface normal(the normalizedgradient vector) fromthe 
second render target and evaluating a local illumination model, such as Phong with point light sources. 
If we want to account for environment light (e.g. stored in a cube map) we need to di.erentiate between 
shift-invariant BRDFwhose re.ectancebehaviour can easilybeprecomputed, and shift­invariant BRDF which 
require Monte-Carlo integration to calculate the scattering events at run-time. A shift invariant BRDF 
is a function f (x,.i,.o)(see Equation 7.2) which does not depend on the position x, i.e. the re.ectance 
properties ACM SIGGRAPH 2009 1 #define NUMSAMPLES (50.0) 2 3 float4 main( 4 float2 screenPos : TEXCOORD0, 
5 uniform float3 cameraPos, 6 uniform float4 randSeed, 7 uniform sampler2D firstHitPos, 8 uniform sampler2D 
firstHitGrad, 9 uniform samplerCUBE irradianceMap, uniform samplerCUBE environmentMap, 11 uniform sampler3D 
noiseTex 12 ) : COLOR 13 { 14 // read the information from previous pass 15 float4 rayPos = tex2D(firstHitPos, 
screenPos); 16 float4 gradient = tex2D(firstHitGrad, screenPos); 17 // focus parameter is the gradient 
magnitude in this example 18 float focus = gradient.w; 19 float3 V = normalize(rayPos.xyz-cameraPos); 
21 float3 N = normalize(gradient); 22 float3 R = reflect(V,N); 23 24 float3 diffuse = texCUBE(irradianceMap, 
N); 25 float3 specular = 0.0; 26 27 for(float i = 0; i < NUMSAMPLES; ++i) { 28 29 // calculate a randomized 
3D texture coordinate float offset = randSeed.a * i/NUMSAMPLES; 31 float3 randUV = (rayPos + offset*randSeed.xyz); 
32 // sample a precalculated 3D noise texture 33 // to obtain a randomized sampling direction 34 float3 
randDir = expand(tex3D(noiseTex,randUV)); 35 randDir = normalize(randDir); 36 // invert direction of 
back-facing 37 float cosTheta = dot(randDir,N); 38 if cosTheta < 0.0) { 39 randDir = -randDir; cosTheta 
= -cosTheta ; 41 } 42 // focus the direction to specular lobe 43 randDir = normalize(lerp(randDir,R,focus)); 
44 // sample the environment cube 45 specular += cosTheta * texCUBE(environmentMap,randDir); 46 } 47 
// average all samples 48 specular /= NUMSAMPLES; 49 return diffuse + specular; 51 52 } Listing 9.4: 
Cg shader function fordeferred shading using a shift-variant Phong BRDF. The di.use term is sampled from 
apre-.ltered irradiance map. The specular term is calculated using stochastic sampling of an environment 
map are contantfor the entire surface. Toimplement a shift-invariantBRDF, we can use an irradiance map 
for the di.use term and a pre-.ltered re.ection map forthe specularterm(seeSection8.4.2.3), assuming 
that the specularlobehas rotational symmetry. For a shift-variantBRDF,the size of the specularlobe variesfordi.erent 
surfacepoints x. For a shift­invariant, but anisotropicBRDF, the shape of the specular lobedepends on 
the local orientation. In these cases, only the di.use term may be pre-.ltered, the specular term cannotbe 
obtainedbya singlepre-.ltered re.ection map. We therefore modify Listing 8.1 to perform the Monte-Carlo 
integration directly on the surface. An example implementation is shown in Listing 9.4. The di.use term 
is simply looked up in an irradiance cube map using the normal direction as texture coordinate. The specular 
exponent represented by thefocusparameter(seeEq.8.11) inthis exampleis settothegradient magnitude, which 
resultsin a shift variant specular term. TheMonte-Carlointegrationisbasicallythe same as in Listing 8.4.2.3. 
The only di.erence is that the specular lobe is focussed onto the re.ection vector R instead of the normal 
N. The result of the deferred shading pass is shown in Figure 9.3, left. 9.2.3 Deferred Ambient Occlusion 
Pass Using the results from the .rst hit pass, we can as well calculate a de­ferred ambient occlusion 
pass for the isosurface. The fragment shader is ACM SIGGRAPH 2009 1 #define NUM SAMPLES (32.0) 2 #define 
FACTOR RAYSTEP (3.0) 3 #define MAX NUM RAY STEPS (10.0) 4 5 float4 main( 6 float2 screenPos : TEXCOORD0, 
7 uniform float isoValue, 8 uniform float4 randSeed, 9 uniform sampler2D firstHitPos, uniform sampler2D 
firstHitGrad, 11 uniform sampler3D volumeTex, 12 uniform sampler3D noiseTex 13 ) : COLOR 14 { 15 // read 
the information from previous pass 16 float4 rayPos = tex2D(firstHitPos, screenPos); 17 float4 gradient 
= tex2D(firstHitGrad, screenPos); 18 19 if (gradient.w < EPSILON) return 0..xxxx; 21 float ambOcc = NUM 
SAMPLES; 22 23 float3 pos; 24 float3 dir; 25 float3 dirStep; 26 27 for(float i = 0; i < NUM SAMPLES; 
i+=1.0) { 28 29 // calculate a randomized sampling direction float offset = randSeed.a * i/NUMSAMPLES; 
31 float3 randUVW = (rayPos + offset*randSeed.xyz); 32 float3 randDir = expand(tex3D(noiseTex,randUVW)); 
33 if (dot(randDir,N) < 0.0) randDir = -randDir; 34 35 // we do not really need to sample the full 180 
degrees 36 randDir = normalize(randDir + 0.1*N); 37 // we sample with a larger stepsize for efficiency 
38 dirStep = randDir * FACTOR RAYSTEP * STEPSIZE; 39 // ensure the first sample point is outside 41 pos 
= rayPos + dirStep; 42 43 for(float s = 0; s < MAX NUM RAY STEPS; s+=1.0) { 44 float value = tex3D(volumeTexture, 
pos).r; 45 if (value > isoValue) { 46 ambOcc -= 1.0; 47 break; 48 } 49 pos += dirStep; } 51 } 52 53 return 
ambOcc.xxx; 54 } 55 Listing9.5:Cg shaderfunctionfordeferredisosurfaceambient occlusion with Monte-Carlo 
sampling shown in Listing 9.5. In this example ambient occlusion is calculated in the local environment 
of the isosurface point. Starting from the surface point, random rays are shot across the hemisphere. 
For each ray, we check if the isosurface is hit again. The intensity value is the fraction of rays which 
do not hit the isosurface again. The result of the deferred ambient occlusion pass is shown in Figure 
9.3, right. The deferred shading pass and the ambient occlusion pass may be multiplied together for each 
pixel, resulting in a high-quality rendition of the isosurface. In the example imagein Figure9.4 showing 
the skin of theUTCTVeiledChameleonCT scan, the shininess ofthe surfaceispro­portional to the gradient 
magnitude at each surface point. In this data set, high gradients of the isosurface are mainly caused 
by bone struc­turesbeing closetothe skin surface. Ifhighest renderingperformanceis required, the number 
of samples for both the deferred shading pass and the ambient occlusionpass mightbe reduced, which still 
resultsingood quality images as shown in Figure 9.5.  9.3 Volume Scattering In chapter7.4 wehave seendi.erentphasefunction 
models which canbe build into our GPU-based volume ray-caster. For volumetric scans, we want todi.erentiatebetween 
scattering at surfaces and scattering within rather homogeneous regions of the volume. This means that 
we approx­imate the directional derivative along the ray using .nite di.erences and change the behaviour 
of the phase function accordingly. While such an implementation is not very di.cult, in practice, the 
visual appearance of this technique is hard to control by the user. We therefore suggest to directly 
specify a set of isovalues representing the isosurfaces at which the BSDF should be evaluated. Inbetween 
these isosurfaces aHenyey-Greensteinphase function or a simple forwardpeek with attenuation will be used. 
Again,using theresultsof the .rsthitpassfromtheprevioussection, we now cast rays from the .rst isosurface 
into the interior of the object. Thefragment shaderfor such a scatteringpass willbebasically the same 
as Listing 9.1. The only di.erence is that we start the rays directly at the .rstisosurface(reading thestartingpointsfromthe 
.rstpass) and check for intersection of multiple iso surfaces simultaneously. The code sample in Listing 
9.6 shows a function which checks for multiple isosurface intersections. In this example we have speci.ed 
four di.erent isovalues si,i .{1, 2, 3, 4}. These values are written in ascend­ing order( si <s i+1)into 
the vector components of a float4 value. The function computeIsoIndex takes a sample s and returns a 
float4 value which contains a 1 in the respective vector component i, if the sample is inbetween si and 
si+1, and0 otherwise. Iftheisovalues si for example are isoValues =(0.1, 0.3, 0.5, 0.7) and the sample 
value s =0.4, this func­tion will return a vector isoIndex =(0.0, 1.0, 0.0, 0.0). To determine an isosurface 
intersection for successive samples si along the ray, we need to compare the isoIndex of the current 
sample to the previous sample. If the dot product between both isoIndices is zero, we have found an intersection 
with an isosurface. If there is no isosurface intersection we may proceed the sampling in one of the 
following ways: ACM SIGGRAPH 2009 1 float4 computeIsoIndex(float sample) { 2 3 float4 a; 4 5 a.x = (sample 
>= isoValues.x)? 1.0 : 0.0; 6 a.y = (sample >= isoValues.y)? 1.0 : 0.0; 7 a.z = (sample >= isoValues.z)? 
1.0 : 0.0; 8 a.w = (sample >= isoValues.w)? 1.0 : 0.0; 9 10 return a -float4(a.yzw,0.0); 11 12 } Listing 
9.6: Cg shader function for calculating the isoIndex, which is used to check for intersection with four 
isosurfaces simultaneously.  process the next sample along the ray without changing the direc­tion of 
the ray(no scattering). This refersto aHenyey-Greenstein phase function with an anisotropy parameter 
g = 1. (Figure 9.6, top left)  process the next sample along the ray without changing the ray direction, 
but attenuate the radiance value. This refers to an ab­sorption of light energy by the volume (Figure 
9.6, bottom left) The radiance along the ray is multiplied by an attenuation factor t . [0, 1], similar 
to alpha blending. The attenuation factor may be a either a constant(e.g. t =0.99) or a function of the 
scalar .eld(implemented using a1D texturelookup).  scatter the next sample by changing the ray direction 
randomly. (Figure 9.6, top right) This refers to a Henyey-Greenstein phase function with an anisotropy 
value g . [0, 1[. The lower the anisotropy value g, the slower the convergence will be. We will need 
a high number of samples to calculate isotropic scattering at every sample point. In practice it is thus 
useful to restrict the anisotropy to values close to 1.  scattering scattering inbetween scattering 
no scattering at isosurfaces only isosurfaces only everywhere combineboth scattering and attenuation(Figure9.6, 
bottom right). Westartatransmissiveray atthehitpointwith the .rstisosurface. Thedirectionis scattered 
within aPhonglobe around the negative view­ingdirection orthe refracted vector(seeFigure7.5). Wetracethis 
ray (with HGscattering and attenuation) until it hits the second isosurface. At this second hit point, 
the gradient vector is estimated and the ray is re.ected randomly into the hemisphere centered around 
the gradient direction, and so on. A visual comparison of the di.erent scattering e.ects is shown in 
Equation9.7. Inthisexample,theopticalpropertieswerespeci.ed such that theinternal surface structures 
of the volume still remain noticeable. Isosurface scattering alone will render in this example at about 
0.3 0.5 secondsperframeingoodquality. When reducingthe number of samples such that theimagebecomes noisebutqualityis 
still acceptable, aframe rate of about 3 frames per second may be achieved. Ifscatteringisperformed with 
every ray step, a random value mustbe fetched from the noise texture at every step which signi.cantly 
increases the bandwidth load. The example images with scattering everywhere in the volume will render 
at about 1 seconds per frame in good quality. The di.erence between the di.erent scattering e.ect in 
many cases is only marginal,sothebene.t ofimplementing scattering eventsinhomo­geneous regions should 
be evaluated in any individual case. 9.3.1 Heuristic Simpli.cations To improve the performance of the 
Monte-Carlo path tracing approach, we may apply some heuristics for simpli.cation. I point out , however, 
that the techniques described in the following are not mathematically correct since they will lead to 
a bias in the Monte-Carlo estimator. If we cast rays starting from the eye, the .rst few scattering events 
are the most important ones with respect to the .nal color. Scattering events which occur later along 
the path of the ray, are less important. With this heuristic in mind, we may simplify our implementation: 
We may decide to scatter the viewing ray 2 or 3 times at isosurfaces, and then sample the ray with attenuation 
onlybut withoutfurther scattering untilitleaves the volume. This strategyavoids raysbeing re.ected again 
and again until their contribution becomes zero due to attenuation. Another simpli.cation is based on 
the assumption that the attenu­ation which is accumulated from the eye point to the a hit point with 
an isosurface is an appropriate estimate for the attenuation from the hit point back to the outside. 
Hence, we can square the accumulated attenuation and directly sample the environment map in the re.ected 
direction without tracing the ray any further. Although less accurate, this technique is much faster 
because we can sample the environment map many times atthe secondhitpointtodirectly estimatedi.use and 
specular re.ection. An example image using this technique is displayed in Figure 9.9. Our experimentshave 
shown that thereislittle visibledi.erenceifwe compare images generated by the more accurate and the faster 
method. We expect however, that this is due to the onion-like structure of iso­surfaces and should not 
be generalized to arbitrary surfaces inside the volume.   Light Map Approaches In combination withShadowVolumes 
orDeep ShadowMaps(See chap­ter shadows in part 2 of these course notes), alternative implementa­tions 
of scattering arepossible. These approaches,however, are restricted to point light sources or directional 
lights. A shadow volume is an additional voxel data set which contains in­formation about the incident 
light at each voxel. It can be calculated slice-by-slice from a given point light source using a modi.ed 
shadow map approach. The shadow volume covers the same space as the scalar .eld, and its slice planes 
are oriented perpendicular to the light direc­tion. The resolution ofthe shadow volume maybelower thanthe 
original scalar volume. To generate a shadow volume, its slices are processed in front-to-back order 
from the viewpoint of the light source. Each voxel of a slice to be processed is projected onto the previous 
slice(exceptforthe .rst slicewhich usestheincidentilluminationof the light source directly). For point 
lights the position of the light source is thecenter ofprojection(perspective). Fordirectionallightsourcesthe 
projectiondirectionis theinvertedlightdirection(parallelprojection). Each slice voxel reads the incident 
light from the previous slice. The incidentlightisattenuatedbytheabsorptionofthescalar .eldgenerated 
by an opacity transfer function.  We can integrate scattering e.ects into the shadow volume during generation 
by sampling the previous slice multiple times per voxel, at locations randomly scattered around the originalprojected 
sampleposi­tion. Thedistance of the randompointsform the original samplingloca­tionis controlled by aphasefunction 
model, such asHenyey-Greenstein. During raycasting theincident lightfrom the shadow volumeis added to 
the emission termduring ray traversal. An exampleimage of the shadow volume is shown in Figure 10.2. 
A similar approach may be used with a deep shadow map instead of a shadow volume. Inthis case,thedeep 
shadow map(which usually has a high resolution to account for .ne details) is resampled on a lower uniform 
voxel grid. The low resolution for the light map is su.cient since illumination caused by multiple scattering 
e.ects is relatively low frequent. The resampled volumeisthenprocessed exactly the same way as the shadow 
volume. An example image of the combination of deep shadow map and low resolution light map is shown 
in Figure 10.3. While being very e.cient, there are some points which should be noted about the light 
map approaches: Like shadow volumes, scattering light maps are restricted to single point light sources 
 The light map needs to be recomputed whenever the position or direction of the light source changes 
 Thelight map needstoberecomputed wheneverthetransferfunc­tion changes  The approachfor creating thelight 
map asdescribed aboveis more a soft shadow volume than a true scattering light map. In the al­gorithm 
described above and outlined in Figure 10.1, light is scat­tered only in slicing direction, so the scattering 
is most prominent for volumeslitfrombehind(forward scattering). Fortrue subsur­face scattering, more 
than one pass must be used to generate the 3D light map, to account for scattering events in all directions 
in­cluding backward scattering. Though this may be done easily, it will slow down the light map creation 
signi.cantly.   Bibliography [1] MichaelD.Adams. TheJPEG-2000StillImageCompressionStan­dard. ISO/IEC(ITU-T 
SG8), September 2001. JTC1/SC29/WG 1: N 2412. [2] Chandrit Bajaj, Insung Ihm, and Sanghun Park. 3D RGB 
image compression for interactive applications. ACM Transactions on Graphics, 20(1):10 38, January 2001. 
[3] Kevin M. Beason, Josh Grant, David C. Banks, Brad Futch, and M.Yousu.Hussaini. Pre-computedilluminationforisosurfaces. 
In VDA 94: Proceedings of the conference on Visualization and Data Analysis 06(SPIEVol.6060),pages 1 
11, 2006. [4] Uwe Behrens and Ralf Ratering. Adding shadows to a texture­based volume renderer. In VVS 
98: Proceedings of the 1998 IEEE symposium on Volume visualization, pages 39 46. ACM Press, 1998. [5] 
Johanna Beyer, Markus Hadwiger, Torsten M¨oller, and Laura Fritz. Smooth Mixed-Resolution GPU Volume 
Rendering. In IEEE/EG International Symposium on Volume and Point-Based Graphics, pages 163 170, 2008. 
[6] Praveen Bhaniramka and Yves Demange. OpenGL Volumizer: A Toolkit for High Quality Volume Rendering 
of Large Data Sets. In Proceedings IEEE Visualization 2002, pages 45 53, 2002. [7] J. F. Blinn. Jim blinn 
s corner: Image compositing theory. IEEE Computer Graphics and Applications, 14(5), 1994. [8] Imma Boada, 
Isabel Navazo, and Roberto Scopigno. Multiresolu­tion volume visualization with a texture-based octree. 
The Visual Computer, 17:185 197, 2001. [9] A. R. Calderbank, Ingrid Daubechies, Wim Sweldens, and Boon-LockYeo. 
Wavelet transforms that mapintegers tointegers. Tech­nical report, Department of Mathematics, Princeton 
University, August 1996. [10] N. Carr, J. Hall, and J. Hart. GPU Algorithms for Radiosity and Subsurface 
Scattering. In Proc. Graphics Hardware, 2003. [11] NathanA.Carr,JesseD.Hall,andJohnC.Hart.GPUalgorithms 
for radiosity and subsurface scattering. In HWWS 03: Proceed­ings of the conference on Graphics Hardware 
03, pages 51 59. Eurographics Association, 2003. [12] Yi-Jen Chiang, Cl´audio T. Silva, and William J. 
Schroeder. In­teractive out-of-core isosurface extraction. In Proceedings of IEEE Visualization 98, pages 
167 174, 1998. [13] Yi-Jen Chiang, Cludio T. Silva, and Willam J. Schroeder. In­teractive out-of-core 
isosurface extraction. In Proceedings IEEE Visualization 1998,pages 167 174,530, 1998. [14] M. Colbert 
and J. K.riv´anek. GPU Gems 3, chapter GPU-Based Importance Sampling, pages 459 475. Addison-Wesley, 
2007. [15] Michael Cox and David Ellsworth. Application-controlled demand paging for out-of-core visualization. 
In Proceedings IEEE Visual­ization 1997, pages 235 244, 1997. [16] Franklin C. Crow. Shadow algorithms 
for computer graphics. In SIGGRAPH 77: Proceedings of the 4th annual conference on Computer graphics 
and interactive techniques, pages 242 248. ACM Press, 1977. [17] Franklin C. Crow. Summed-area tables 
for texture mapping. In Proceedings SIGGRAPH 84, volume 18, pages 207 212, 1984. [18] Carsten Dachsbacher 
and Marc Stamminger. Splatting indirect illumination. In I3D 06: Proceedings of the 2006 symposium on 
Interactive 3D graphics and games, pages 93 100, New York, NY, USA, 2006. ACM. [19] Ingrid Daubechies. 
Ten Lectures on Wavelets. Society for Indus­trial and Applied Mathematics, 1992. [20] Philippe Desgranges 
and Klaus Engel. US patent application 2007/0013696 A1: Fast ambient occlusion for direct volume ren­dering, 
2007. [21] Philippe Desgranges, Klaus Engel, and Gianluca Paladini. Gradient-free shading: A new method 
for realistic interactive vol­ume rendering. In VMV 05: Proceedings of the international fall workshop 
on Vision, Modeling, and Visualization, pages 209 216, 2005. [22] C. Donner and H. W. Jensen. Light Di.usion 
in Multi-Layered Translucent Materials. In Proc. ACM SIGGRAPH, 2005. [23] R. A. Drebin, L. Carpenter, 
and P. Hanrahan. Volume rendering. In Proceedings of SIGGRAPH 88, pages 65 74, 1988. [24] D. Ebert, F. 
K. Musgrave, D. Peachey, K. Perlin, and S. Worley. Texturing and Modeling: A Procedural Approach. Academic 
Press, July 1998. [25] Klaus Engel, Markus Hadwiger, Joe Kniss, Christof Rezk-Salama, and Daniel Weiskopf. 
Real-Time Volume Graphics. AK Peters, 2006. [26] James D. Foley, Richard L. Phillips, John F. Hughes, 
Andries van Dam, and Steven K. Feiner. Introduction to Computer Graphics. Addison-WesleyLongmanPublishingCo.,Inc.,Boston,MA,USA, 
1994. [27] Jinzhu Gao, Jian Huang, C. Ryan Johnson, and Scott Atchley. Distributed data management for 
large volume visualization. In Proceedings IEEE Visualization 2005,pages 183 189. IEEE, 2005. [28] Jinzhu 
Gao, Jian Huang, Han-Wei Shen, and James Arthur Kohl. Visibility culling usingplenoptic opacityfunctionsforlarge 
volume visualization. In Proceedings IEEE Visualization 2003,pages 341 348. IEEE, 2003. [29] Jinzhu Gao, 
Han-Wei Shen, Jian Huang, and James Arthur Kohl. Visibility culling for time-varying volume rendering 
using tempo­ral occlusion coherence. In Proceedings IEEE Visualization 2004, pages 147 154. IEEE, 2004. 
[30] S. Grimm, S. Bruckner, A. Kanitsar, and E. Gr¨oller. Memory e.­cient acceleration structures and 
techniques for cpu-based volume raycasting of large data. In Proceedings IEEE/SIGGRAPH Sym­posium on 
Volume Visualization and Graphics, pages 1 8, 2004. [31] S¨oren Grimm, Stefan Bruckner, Armin Kanitsar, 
and Eduard Gr¨oller. Memory e.cient acceleration structures and techniques for CPU-based volume raycasting 
of large data. In Proceedings IEEE Volume Visualization and Graphics Symposium, pages 1 8, 2004. [32] 
S¨oren Grimm, Stefan Bruckner, Armin Kanitsar, and Eduard Gr¨oller. A re.ned data addressing and processing 
scheme to ac­celerate volume raycasting. Computers and Graphics, 28:719 729, 2004. [33] StefanGutheandWolfgangStraßer.Real-timedecompression 
and visualization of animated volume data. In Proceedings IEEE Visu­alization 2001, pages 349 356, 2001. 
[34] Stefan Guthe and Wolfgang Strasser. Advanced techniques for high quality multiresolution volume 
rendering. In Computers &#38; Graphics, volume28,pages51 58.ElsevierScience,February2004. [35] Stefan 
Guthe, Michael Wand, Julius Gonser, and Wolfgang Straßer. Interactive rendering of large volume data 
sets. In Pro­ceedings IEEE Visualization 2002, pages 53 60, 2002. [36] Attila Gyulassy, Lars Linsen, 
and Bernd Hamann. Time-and space-e.cient error calculation for multiresolution direct volume rendering. 
In Mathematical Foundations of Scienti.c Visualiza­tion,ComputerGraphics, andMassiveDataExploration.Springer-Verlag, 
Heidelberg, Germany, 2006. [37] M. Hadwiger, C. Sigg, H. Scharsach, K. B¨uhler, and M. Gross. Real-time 
ray-casting and advanced shading ofdiscreteisosurfaces. In Proceedings of Eurographics 2005, pages 303 
312, 2005. [38] M. Hadwiger, C. Sigg, H. Scharsach, K. B¨uhler, and M. Gross. Real-Time Ray-Casting and 
Advanced Shading of Discrete Isosur­faces. In Proceedings of Eurographics, pages 303 312, 2005. [39] 
Markus Hadwiger, Andrea Kratz, Christian Sigg, and Katja B¨uhler. Gpu-accelerated deep shadow maps for 
direct volume ren­dering. In GH 06: Proceedings of the 21st ACM SIGGRAPH/Eu­rographics symposium on Graphics 
hardware, pages 49 52, New York, NY, USA, 2006. ACM Press. [40] W. Heidrich and H.-P. Seidel. Realistic, 
Hardware-accellerated Shading and Lighting. In Proc. ACM SIGGRAPH, 1999. [41] L. Henyey and J. Greenstein. 
Di.use radiation in the galaxy. As­trophysical Journal, pages p. 70 83, 93. [42] Frida Hernell, Patric 
Ljung, and Anders Ynnerman. E.cient am­bient and emissive tissue illumination using local occlusion in 
mul­tiresolution volume rendering. In Proceedings Eurographics/IEEE-VGTC Symposium on Volume Graphics. 
Eurographics/IEEE, 2007. [43] Frida Hernell, Patric Ljung, and Anders Ynnerman. Interactive GlobalLightPropagationinDirectVolumeRendering 
usingLocal Piecewise Integration. In IEEE/EG International Symposium on Volume and Point-Based Graphics, 
pages 105 112, 2008. [44] W.Hong,F.Qiu, andA.Kaufman. Gpu-based object-order ray­castingforlargedatasets. 
In Proceedings ofVolumeGraphics2005, 2005. [45] Insung Ihm and Sanghun Park. Wavelet-based 3d compression 
scheme for interactive visualization of very large volume data. Computer Graphics Forum, 18(1):3 15, 
1999. [46] HenrikWannJensen,StephenR.Marschner,MarcLevoy, andPat Hanrahan. A Practical Model for Subsurface 
Light Transport. In Proceedings of ACM SIGGRAPH, pages 511 518, 2001. [47] Ralf K¨ahler, John Wise, Tom 
Abel, and Hans-Christian Hege. Gpu-assisted raycastingfor cosmological adaptive mesg re.nement simulations. 
In Proceedings Eurographics/IEEE Workshop on Vol­ume Graphics 2006,pages 103 110,144, 2006. [48] D. Kalra 
and A. H. Barr. Guaranteed ray intersections with im­plicit surfaces. In Proceedings of SIGGRAPH 89,pages297 
306, 1989. [49] A. Kaufman. Voxels as a Computational Representation of Ge­ometry. In The Computational 
Representation of Geometry. SIG-GRAPH 94 Course Notes, 1994. [50] Tae-Yong Kim and Ulrich Neumann. Opacity 
shadow maps. In Proceedings ofthe12thEurographicsWorkshop onRenderingTech­niques, pages 177 182, London, 
UK, 2001. Springer-Verlag. [51] Joe Kniss, Gordon Kindlmann, and Charles Hansen. Multidimen­sional transfer 
functions for interactive volume rendering. IEEE Transactions on Visualization and Computer Graphics, 
8(3):270 285, 2002. [52] Joe Kniss, Simon Premoze, Charles Hansen, and David Ebert. In­teractive translucent 
volume rendering and procedural modeling. In VIS 02: Proceedings of the conference on Visualization 02, 
pages 109 116. IEEE Computer Society, 2002. [53] Joe Kniss, Simon Premoze, Charles Hansen, Peter Shirley, 
and Allen McPherson. A model for volume lighting and model­ing. IEEE Transactions on Visualization and 
Computer Graphics, 9(2):150 162, 2003. [54] M. Kraus and T. Ertl. Adaptive texture maps. In Proceedings 
of Graphics Hardware 2002, pages 7 15, 2002. [55] J. Kr¨uger and R. Westermann. Acceleration techniques 
for GPU­based volume rendering. In Proceedings IEEE Visualization 2003, 2003. [56] Eric C. LaMar, Bernd 
Hamann, and Kenneth I. Joy. Multiresolu­tion techniques for interactive texture-based volume visualization. 
In Proceedings IEEE Visualization 1999, pages 355 362, 1999. [57] EricC.LaMar,BerndHamann,andKennethI.Joy.E.cient 
error calculation for multiresolution texture-based volume visualization. In Gerald Farin, Bernd Hamann, 
and Hans Hagen, editors, Hier­achical and Geometrical Methods in Scienti.c Visualization,pages 51 62. 
Springer-Verlag, Heidelberg, Germany, 2003. [58] MichaelS.Langer andHeinrichH.B¨ultho..Depthdiscrimination 
from shading under di.use lighting. Perception, 29(6):649 660, 2000. [59] H. Lensch, M. Goesele, P. Bekaert, 
J. Kautz, M. Magnor, J. Lang, andH.-P.Seidel.Interactive rendering oftranslucent objects. Com­puter Graphics 
Forum, 22(2), 2003. [60] M. Levoy. Display of surfaces from volume data. IEEE Computer Graphics and Applications, 
8(3):29 37, May 1988. [61] F. Link, M. Koenig, and H.-O. Peitgen. Multi-Resolution Volume Rendering with 
per Object Shading. In Proceedings of Vision, Modeling and Visualization, pages 185 191, 2006. [62] Yarden 
Livnat, Han-Wei Shen, and Christopher R. Johnson. A near optimal isosurface extraction algorithm using 
the span space. IEEE Transactions on Visualization and Computer Graph­ics, 2:73 84, 1996. [63] Patric 
Ljung. Adaptive sampling in single pass, GPU-based ray­casting of multiresolution volumes. In Proceedings 
Eurographic-s/IEEE Workshop on Volume Graphics 2006, pages 39 46,134, 2006. [64] Patric Ljung. E.cient 
Methods for Direct Volume Rendering of Large Data Sets. PhD thesis, Link¨oping University, Sweden, 2006. 
Link¨oping studiesin science andtechnology.Dissertations no.1043. [65] Patric Ljung, Claes Lundstr¨om, 
and Anders Ynnerman. Multires­olution interblock interpolation in direct volume rendering. In ProceedingsEurographics/IEEESymposium 
onVisualization2006, pages 259 266, 2006. [66] Patric Ljung, Claes Lundstr¨om, Anders Ynnerman, and Ken 
Museth. Transfer function based adaptive decompression for vol­ume rendering of large medical data sets. 
In Proceedings IEEE Volume Visualization and Graphics Symposium 2004, pages 25 32, 2004. [67] Patric 
Ljung, Calle Winskog, Anders Perssson, Claes Lundstr¨om, and Anders Ynnerman. Full body virtual autopsies 
using a state­of-the-art volume rendering pipeline. IEEE Transactions on Vi­sualization andComputerGraphics(ProceedingsVisualization/In­formation 
Visualization 2006), 12:869 876, 2006. [68] Tom Lokovic and Eric Veach. Deep shadow maps. In SIGGRAPH 
00: Proceedings of the27th annual conference onComputergraph­ics andinteractive techniques,pages385 392,NewYork,NY,USA, 
2000. ACM Press/Addison-Wesley Publishing Co. [69] Eric B. Lum, Kwan-Liu Ma, and John Clyne. Texture 
hardware assisted rendering of time-varying volume data. In Proceedings IEEE Visualization 2001, pages 
263 270, 2001. [70] Eric B. Lum, Kwan-Liu Ma, and John Clyne. A hardware-assisted scalable solution for 
interactive volume rendering of time-varying data. IEEETransactions onVisualization andComputerGraphics, 
8:286 298, 2002. [71] Claes Lundstr¨om, Patric Ljung, and Anders Ynnerman. Local histograms for design 
of transfer functions in direct volume ren­dering. Transactions on Visualization and Computer Graphics, 
12(6):1570 1579, Nov.-Dec. 2006. [72] Gerd Marmitt, Heiko Friedrich, and Philipp Slusallek. Interactive 
Volume Rendering with Ray Tracing. In Eurographics State of the Art Reports, 2006. [73] Nelson Max. Optical 
models for direct volume rendering. IEEE Transactions on Visualization and Computer Graphics, 1(2):99 
108, June 1995. [74] Nelson Max. Optical models for direct volume rendering. IEEE Transactions on Visualization 
and Computer Graphics, 1(2):99 108, 1995. [75] J¨orgMensmann,TimoRopinski,andKlausHinrichs.Accelerating 
Volume Raycasting using Occlusion Frustum. In IEEE/EG Inter­national Symposium on Volume and Point-Based 
Graphics, pages 147 154, 2008. [76] Ky Giang Nguyen and Dietmar Saupe. Rapid high quality com­pression 
of volume data for visualization. Computer Graphics Fo­rum, 20(3), 2001. [77] Steven Parker, Michael 
Parker, Yarden Livnat, Peter-Pike Sloan, Charles Hansen, and Peter Shirley. Interactive ray tracing for 
vol­ume visualization. IEEE Transactions on Visualization and Com­puter Graphics, 5(3):238 250, 1999. 
[78] Steven Parker, Peter Shirley, Yarden Livnat, Charles Hansen, and Peter-Pike Sloan. Interactive ray 
tracing for isosurface rendering. InProceedingsofIEEEVisualization 98.IEEE-CS,ACM,October 1998. [79] 
A. Patra and M.D. Wang. Volumetric medical image compression and reconstruction for interactive visualization 
in surgical plan­ning. In Proceedings Data Compression Conference 2003, page 442, March 2003. [80] Eric 
Penner and Ross Mitchell. Isosurface Ambient Occlusion and Soft Shadows with Filterable Occlusion Maps. 
In IEEE/EG Inter­national Symposium on Volume and Point-Based Graphics, pages 57 64, 2008. [81] Matt 
Pharr and Greg Humphries. Physically Based Rendering. Morgan Kau.man, 2004. [82] William T. Reeves, David 
H. Salesin, and Robert L. Cook. Ren­dering antialiased shadows with depth maps. In SIGGRAPH 87: Proceedings 
of the 14th annual conference on Computer graphics and interactive techniques, pages 283 291. ACM Press, 
1987. [83] S. Roettger, S. Guthe, D. Weiskopf, and T. Ertl. Smart hardware­accelerated volume rendering. 
In Procceedings of EG/IEEE TCVG Symposium on Visualization VisSym 03, pages 231 238, 2003. [84] Timo 
Ropinski, Jens Kasten, and Klaus H. Hinrichs. E.cient Shadows for GPU-based Volume Raycasting. In Proceedings 
of the 16th International Conference in Central Europe on Computer Graphics, Visualization(WSCG08), pages 
17 24, 2008. [85] Timo Ropinski, Jennis Meyer-Spradow, Stefan Diepenbrock, J¨org Mensmann, and Klaus 
H. Hinrichs. Interactive Volume Rendering with Dynamic Ambient Occlusion and Color Bleeding. Computer 
Graphics Forum(Eurographics 2008), 27(2):567 576, 2008. [86] StefanR¨ottger,MichaelBauer,andMarcStamminger.Spatialized 
transfer functions. In EuroVis, pages 271 278, 2005. [87] Marc Ruiz, Imma Boada, Ivan Viola, Stefan Bruckner, 
Miquel Feixas, and Mateu Sbert. Obscurance-based Volume Rendering Framework. In IEEE/EG International 
Symposium on Volume and Point-Based Graphics, pages 113 120, 2008. [88] C. Rezk Salama. GPU-Based Monte-Carlo 
Volume Raycasting. In Proc. Paci.c Graphics, 2007. [89] MirkoSattler,RalfSarlette,ThomasM¨ucken,andReinhardKlein. 
Exploitation of human shadow perception for fast shadow render­ing. In APGV 05: Proceedings of the 2nd 
symposium on Ap­pliedperceptioningraphics andvisualization,pages131 134.ACM Press, 2005. [90] H. Scharsach, 
M. Hadwiger, A. Neubauer, S. Wolfsberger, and K. B¨uhler. Perspective Isosurface and Direct Volume Rendering 
forVirtualEndoscopy Applications. In Proceedings of Eurovis 06, pages 315 323, 2006. [91] Henning Scharsach. 
Advanced GPU raycasting. In Proceedings of the 9th Central European Seminar on Computer Graphics, May 
2005. [92] Jens Schneider and R¨udiger Westermann. Compression domain volume rendering. In Proceedings 
IEEE Visualization 2003, 2003. [93] Peter-Pike Sloan, Jesse Hall, John Hart, and John Snyder. Clus­tered 
principal components for precomputed radiance transfer. In SIGGRAPH 03: ACM SIGGRAPH 2003 Papers, pages 
382 391. ACM Press, 2003. [94] Peter-Pike Sloan, Ben Luna, and John Snyder. Local, deformable precomputed 
radiance transfer. In SIGGRAPH 05: ACM SIG-GRAPH 2005 Papers, pages 1216 1224. ACM Press, 2005. [95] 
IrwinEdwardSobel. Camera models and machineperception. PhD thesis, Stanford University, Stanford, CA, 
USA, 1970. [96] Lisa M. Sobierajski and Arie E. Kaufman. Volumetric ray trac­ing. In VVS 94: Proceedings 
of the 1994 symposium on Volume Visualization 94, pages 11 18. ACM Press, 1994. [97] S. Stegmaier, M. 
Strengert, T. Klein, and T. Ertl. A simple and .exible volume rendering framework for graphics-hardware 
based raycasting. In Proceedings of the International Workshop on Vol­ume Graphics 05, pages 187 195, 
2005. [98] A. James Stewart. Vicinity shading for enhanced perception of volumetric data. In VIS 03: 
Proceedings of the 14th IEEE Visu­alization 2003(VIS 03), page 47. IEEE Computer Society, 2003. [99] 
Wim Sweldens. The lifting scheme: A custom-design construction of biorthogonal wavelets. Journal of Applied 
and Computational Harmonic Analysis,(3):186 200, 1996. [100] MartinVetterliandDidierLeGall.Perfect reconstructionFIR.lter 
banks: some properties and factorizations. IEEE Transactions on Acoustics, Speech, and Signal Processing, 
37(7):1057 1071, July 1989. [101] Joachim E. Vollrath, Tobias Schafhitzel, and Thomas Ertl. Em­ploying 
complex GPUdata structures for theinteractive visualiza­tion of adaptive mesh re.nement data. In Proceedings 
Eurograph-ics/IEEE Workshop on Volume Graphics 2006, pages 55 58,136, 2006. [102] IngoWald,HeikoFriedrich,GerdMarmitt,andHans-PeterSeidel. 
Faster isosurface ray tracing using implicit kd-trees. IEEE Trans­actions on Visualization and Computer 
Graphics, 11(5):562 572, 2005. Member-Philipp Slusallek. [103] Ingo Wald, Thomas Kollig, Carsten Benthin, 
Alexander Keller, and Philipp Slusallek. Interactive global illumination using fast ray tracing. In EGRW 
02: Proceedings of the 13th Eurographics workshop on Rendering, pages 15 24, Aire-la-Ville, Switzerland, 
Switzerland, 2002. Eurographics Association. [104] M. Weiler, R. Westermann, C. Hansen, K. Zimmerman, 
and T. Ertl. Level-Of-Detail Volume Rendering via 3D Textures. In Proceedings of IEEE Symposium on Volume 
Visualization, pages 7 13, 2000.  [105] Manfred Weiler, R¨udiger Westermann, Chuck Hansen, Kurt Zim­merman, 
and Thomas Ertl. Level of detail volume rendering via 3dtextures. InProceedingsIEEEVolumeVisualization 
andGraph­ics Symposium 2000, pages 7 13. ACM Press, 2000. [106] R¨udiger Westermann. A multiresolution 
framework for volume rendering. In 1994 Symposium on Volume Visualization, October 1994. [107] G. Wetekam, 
D. Staneker, U. Kanus, and M. Wand. A hard­ware architecture for multi-resolution volume rendering. In 
Pro­ceedings ACM SIGGRAPH/Eurographics Conference on Graphics Hardware, pages 45 51, New York, NY, USA, 
2005. ACM Press. [108] Jane Wilhelms and Allen Van Gelder. Octrees for faster isosurface generation. 
ACM Transactions on Graphics, 11:201 227, 1992. [109] Lance Williams. Casting curved shadows on curved 
surfaces. In SIGGRAPH 78: Proceedings of the 5th annual conference on Computer graphics and interactive 
techniques, pages 270 274. ACM Press, 1978. [110] C. M. Wittenbrink, T. Malzbender, and M. E. Goss. Opacity­weighted 
colorinterpolationfor volume sampling. In Proceedings of IEEE Symposium on Volume Visualization, pages 
135 142, 1998. [111] Chris Wyman, Steven Parker, Charles Hansen, and Peter Shirley. Interactive display 
of isosurfaces with global illumination. IEEE Transactions on Visualization and Computer Graphics, 12(2):186 
196, 2006. [112] Boon-Lock Yeo and Bede Liu. Volume rendering of DCT-based compressed 3d scalar data. 
IEEE Transactions on Visualization and Computer Graphics, 1:29 43, March 1995. [113] C. Zhang, D. Xue, 
and R. Craw.s. Light propagation for mixed polygonal and volumetric data. In CGI 05: Proceedings of the 
Computer Graphics International 2005, pages 249 256, Washing­ton, DC, USA, 2005. IEEE Computer Society. 
[114] Caixia Zhang and Roger Craw.s. Shadows and soft shadows with participating media using splatting. 
IEEE Transactions on Visu­alization and Computer Graphics, 9(2):139 149, 2003. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1667242</article_id>
		<sort_key>30</sort_key>
		<display_label>Article No.</display_label>
		<display_no>3</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Advanced material appearance modeling]]></title>
		<page_from>1</page_from>
		<page_to>134</page_to>
		<doi_number>10.1145/1667239.1667242</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1667242</url>
		<abstract>
			<par><![CDATA[<p>For many years, appearance models in computer graphics focused on general models for reflectance functions coupled with texture maps. Recently, it has been recognized that even very common materials such as hair, skin, fabric, and rusting metal require more sophisticated models to appear realistic. This course begins with a brief review of basic reflectance models and the use of texture maps. It describes common approaches in advanced material models such as combining the effects of layers, groups of particles, and/or fibers. It surveys the detailed models needed for a wide range of materials such as plants, hair, skin, plants, inks, gems, and automotive paints, and summarizes modeling of complex appearance due to aging and weathering processes.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor>Modeling methodologies</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010342.10010343</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis->Modeling methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1797106</person_id>
				<author_profile_id><![CDATA[81100369597]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Julie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dorsey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yale University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797107</person_id>
				<author_profile_id><![CDATA[81100255828]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Holly]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rushmeier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yale University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>882430</ref_obj_id>
				<ref_obj_pid>882404</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{Adabala <i>et al.</i>, 2003} Neeharika Adabala, Guangzheng Fei, and Nadia Magnenat-Thalmann. Visualization of woven cloth. In Philip Dutr&#233;, Frank Suykens, Per H. Christensen, and Daniel Cohen-Or, editors, <i>Proceedings of the 14th Eurographics workshop on Rendering</i>, pages 178--185, Leuven, Belgium, 2003. Eurographics Association.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>826602</ref_obj_id>
				<ref_obj_pid>826030</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{Aoki <i>et al.</i>, 2002} K. Aoki, Ngo Hai Dong, T. Kaneko, and S. Kuriyama. Physically-based simulation of cracks on drying 3d solid. In <i>10th Pacific Graphics Conference on Computer Graphics and Applications</i>, pages 467--468, Beijing China, Oct 2002. IEEE.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>358644</ref_obj_id>
				<ref_obj_pid>358636</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{Ashikhmin and Shirley, 2000} Michael Ashikhmin and Peter Shirley. An anisotropic Phong BRDF model. <i>Journal of Graphic Tools</i>, 5(2):25--32, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{Baranoski and Rokne, 1997} G. V. G. Baranoski and J. G. Rokne. An algorithmic reflectance and transmittance model for plant tissue. <i>Computer Graphics Forum</i>, 16(3):141--150, August 1997. ISSN 1067--7055.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{Baranoski and Rokne, 2001} Gladimir V. G. Baranoski and Jon G. Rokne. Efficiently simulating scattering of light by leaves. <i>The Visual Computer</i>, 17(8):491--505, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2381386</ref_obj_id>
				<ref_obj_pid>2381384</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{Beardall <i>et al.</i>, 2007} M. Beardall, M. Farley, D. Ouderkirk, J. Smith, M. Jones, and P. Egbert. Goblins by spheroidal weathering. In <i>Eurographics workshop on natural phenomena</i>, pages 7--14, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{Blazej <i>et al.</i>, 1989} A. A. Blazej, J. Galatik, Z. Galatik, Z. Krul, and M. Mladek. <i>Atlas of Microscopic Structures of Fur Skins 1</i>. Elsevier, New York, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>563893</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{Blinn, 1977} James F. Blinn. Models of light reflection for computer synthesized pictures. In <i>Proceedings of the 4th annual conference on Computer graphics and interactive techniques</i>, pages 192--198. ACM Press, 1977.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325249</ref_obj_id>
				<ref_obj_pid>325334</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{Bloomenthal, 1985} Jules Bloomenthal. Modeling the mighty maple. In <i>Proceedings of the 12th annual conference on Computer graphics and interactive techniques</i>, pages 305--311. ACM Press, 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[{Boissieux <i>et al.</i>, 2000} L. Boissieux, G. Kiss, N. Magnenat-Thalmann, and P. Kalra. Simulation of skin aging and wrinkles with cosmetics insight. <i>Computer Animation and Simulation 2000</i>, pages 15--27, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[{Bosch <i>et al.</i>, 2004} C. Bosch, X. Pueyo, S. M&#233;rillou, and D. Ghazanfarpour. A physically-based model for rendering realistic scratches. <i>Computer Graphics Forum</i>, 23(3):361--370, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[{Buchanan, 1998} John W. Buchanan. Simulating wood using a voxel approach. <i>Computer Graphics Forum</i>, 17(3):105--112, 1998. ISSN 1067--7055.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[{Chang and Shih, 2000} Yao-Xun Chang and Zen-Chung Shih. Physically-based patination for underground objects. <i>Computer Graphics Forum</i>, 19(3), August 2000. ISSN 1067--7055.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[{Chang and Shih, 2003} Yao-Xun Chang and Zen-Chung Shih. The synthesis of rust in seawater. <i>The Visual Computer</i>, 19(1):50--66, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566628</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[{Chen <i>et al.</i>, 2002} Yanyun Chen, Yingqing Xu, Baining Guo, and Heung-Yeung Shum. Modeling and rendering of realistic feathers. In <i>Proceedings of the 29th annual conference on Computer graphics and interactive techniques</i>, pages 630--636. ACM Press, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073321</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[{Chen <i>et al.</i>, 2005} Y. Chen, L. Xia, T. T. Wong, X. Tong, H. Bao, B. Guo, and H. Y. Shum. Visual simulation of weathering by &#947;-ton tracing. <i>Proceedings of ACM SIGGRAPH 2005</i>, 24(3):1127--1133, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617946</ref_obj_id>
				<ref_obj_pid>616034</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[{chi Hsu and tsin Wong, 1995} Siu chi Hsu and Tien tsin Wong. Simulating dust accumulation. <i>IEEE Comput. Graph. Appl.</i>, 15(1):18--22, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073221</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[{Chu and Tai, 2005} Nelson S.-H. Chu and Chiew-Lan Tai. Moxi: real-time ink dispersion in absorbent paper. <i>ACM Trans. Graph.</i>, 24(3):504--511, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[{Cockshott <i>et al.</i>, 1992} T. Cockshott, J. Patterson, and D. England. Modelling the texture of paint. <i>Computer Graphics Forum</i>, 11(3):C217--C226, C476, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>357293</ref_obj_id>
				<ref_obj_pid>357290</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[{Cook and Torrance, 1982} R. L. Cook and K. E. Torrance. A reflectance model for computer graphics. <i>ACM Transactions on Graphics</i>, 1(1):7--24, January 1982.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258896</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[{Curtis <i>et al.</i>, 1997} Cassidy J. Curtis, Sean E. Anderson, Joshua E. Seims, Kurt W. Fleischer, and David H. Salesin. Computer-generated watercolor. In <i>Proceedings of the 24th annual conference on Computer graphics and interactive techniques</i>, pages 421--430. ACM Press/Addison-Wesley Publishing Co., 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[{Dai <i>et al.</i>, 1995} Wen-Kai Dai, Zen-Chung Shih, and Ruei-Chuan Chang. Synthesizing feather textures in galliformes. <i>Computer Graphics Forum</i>, 14(3):407--420, August 1995. ISSN 1067--7055.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344855</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[{Debevec <i>et al.</i>, 2000} Paul Debevec, Tim Hawkins, Chris Tchou, Haarm-Pieter Duiker, Westley Sarokin, and Mark Sagar. Acquiring the reflectance field of a human face. In <i>Proceedings of the 27th annual conference on Computer graphics and interactive techniques</i>, pages 145--156. ACM Press/Addison-Wesley Publishing Co., 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[{Desbenoit <i>et al.</i>, 2004} Brett Desbenoit, Eric Galin, and Samir Akkouche. Simulating and modeling lichen growth. <i>Computer Graphics Forum</i>, 23(3):341--350, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237278</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[{Dorsey and Hanrahan, 1996} Julie Dorsey and Pat Hanrahan. Modeling and rendering of metallic patinas. In <i>Proceedings of the 23rd annual conference on Computer graphics and interactive techniques</i>, pages 387--396. ACM Press, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311560</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[{Dorsey <i>et al.</i>, 1999} Julie Dorsey, Alan Edelman, Henrik Wann Jensen, Justin Legakis, and Hans Kohling Pedersen. Modeling and rendering of weathered stone. In <i>Proceedings of the 26th annual conference on Computer graphics and interactive techniques</i>, pages 225--234. ACM Press/Addison-Wesley Publishing Co., 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[{Dumont-B&#232;cle <i>et al.</i>, 2001} P. Dumont-B&#232;cle, A. Kemeny, S. Michelin, and D. Arqu&#232;s. Multi-texturing approach for paint appearance simulation on virtual vehicles. <i>Proceedings of the Driving Simulation Conference 2001</i>, 213, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>781008</ref_obj_id>
				<ref_obj_pid>780986</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[{Eric Paquette, 2001} George Drettakis Eric Paquette, Pierre Poulin. Surface aging by impacts. In <i>Graphics Interface 2001</i>, pages 175--182, June 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[{Ershov <i>et al.</i>, 2001} Sergey Ershov, Konstantin Kolchin, and Karol Myszkowski. Rendering pearlescent appearance based on paint-composition modelling. <i>Computer Graphics Forum</i>, 20(3), 2001. ISSN 1067--7055.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[{Fowler <i>et al.</i>, 1989} Deborah R. Fowler, James Hanan, and Przemyslaw Prusinkiewicz. Modelling spiral phyllotaxis. <i>Computers and Graphics</i>, 13(3):291--296, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134093</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[{Fowler <i>et al.</i>, 1992} Deborah R. Fowler, Przemyslaw Prusinkiewicz, and Johannes Battjes. A collision-based model of spiral phyllotaxis. In <i>Proceedings of the 19th annual conference on Computer graphics and interactive techniques</i>, pages 361--368. ACM Press, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>965540</ref_obj_id>
				<ref_obj_pid>965400</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[{Franzke and Deussen, 2003} Oliver Franzke and Oliver Deussen. Rendering plant leaves faithfully. In <i>Proceedings of the SIGGRAPH 2003 conference on Sketches&amp;applications</i>, pages 1--1. ACM Press, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1053686</ref_obj_id>
				<ref_obj_pid>1053554</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[{Fuchs <i>et al.</i>, 2005} Martin Fuchs, Hendrik Lensch, and Hans-Peter Seidel. Reflectance from images: A model-based approach for human faces. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 11(3):296--305, 2005. Member-Volker Blanz.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1162438</ref_obj_id>
				<ref_obj_pid>1162435</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[{Fuhrer <i>et al.</i>, } Martin Fuhrer, Henrik Wann Jensen, and Przemyslaw Prusinkiewicz. Modeling hairy plants. <i>Graphical Models</i>, 68(4), July.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882438</ref_obj_id>
				<ref_obj_pid>882404</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[{Georghiades, 2003} Athinodoros S. Georghiades. Recovering 3-d shape and reflectance from a small number of photographs. In <i>EGRW '03: Proceedings of the 14th Eurographics workshop on Rendering</i>, pages 230--240, Aire-la-Ville, Switzerland, Switzerland, 2003. Eurographics Association.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>883422</ref_obj_id>
				<ref_obj_pid>882473</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[{Gobron and Chiba, 2001a} S. Gobron and N. Chiba. Simulation of peeling using 3d-surface cellular automata. In <i>9th Pacific Graphics Conference on Computer Graphics and Applications</i>, pages 338--347, Tokyo Japan, Oct 2001. IEEE.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[{Gobron and Chiba, 2001b} St&#233;phane Gobron and Norishige Chiba. Crack pattern simulation based on 3d surface cellular automata. <i>The Visual Computer</i>, 17(5):287--309, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614315</ref_obj_id>
				<ref_obj_pid>614260</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[{Groeller <i>et al.</i>, 1995} Eduard Groeller, Rene T. Rau, and Wolfgang Strasser. Modeling and visualization of knitwear. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 1(4):302--310, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141952</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[{Gu <i>et al.</i>, 2006} Jinwei Gu, Chien-I Tu, Ravi Ramamoorthi, Peter Belhumeur, Wojciech Matusik, and Shree Nayar. Time-varying surface appearance: acquisition, modeling and rendering. <i>ACM Trans. Graph.</i>, 25(3):762--771, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[{G&#252;nther <i>et al.</i>, 2005} Johannes G&#252;nther, Tongbo Chen, Michael Goesele, Ingo Wald, and Hans-Peter Seidel. Efficient acquisition and realistic rendering of car paint. In G&#252;nther Greiner, Joachim Hornegger, Heinrich Niemann, and Marc Stamminger, editors, <i>Proceedings of 10th International Fall Workshop - Vision, Modeling, and Visualization (VMV) 2005</i>, pages 487--494. Akademische Verlagsgesellschaft Aka GmbH, November 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[{Gutierrez <i>et al.</i>, 2008} D. Gutierrez, F. J. Seron, A. Munoz, and O. Anson. Visualizing Underwater Ocean Optics. In <i>Computer Graphics Forum</i>, volume 27, pages 547--556. Blackwell Publishing Ltd, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015708</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[{Guy and Soler, 2004} Stephane Guy and Cyril Soler. Graphics gems revisited: fast and physically-based rendering of gemstones. <i>ACM Trans. Graph.</i>, 23(3):231--238, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166139</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[{Hanrahan and Krueger, 1993} Pat Hanrahan and Wolfgang Krueger. Reflection from layered surfaces due to subsurface scattering. In <i>Proceedings of the 20th annual conference on Computer graphics and interactive techniques</i>, pages 165--174. ACM Press, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1185719</ref_obj_id>
				<ref_obj_pid>1185657</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[{Hiebert <i>et al.</i>, 2006} Brad Hiebert, Jubin Dave, Tae-Yong Kim, Ivan Neulander, Hans Rijpkema, and Will Telford. The chronicles of Narnia: the lion, the crowds and rhythm and hues. In <i>SIGGRAPH '06: ACM SIGGRAPH 2006 Courses</i>, page 1, New York, NY, USA, 2006. ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[{Hirota <i>et al.</i>, 1998} Koichi Hirota, Yasuyuki Tanoue, and Toyohisa Kaneko. Generation of crack patterns with a physical model. <i>The Visual Computer</i>, 14(3):126--137, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[{Hirota <i>et al.</i>, 2000} Koichi Hirota, Yasuyuki Tanoue, and Toyohisa Kaneko. Simulation of three-dimensional cracks. <i>The Visual Computer</i>, 16(7):371--378, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383840</ref_obj_id>
				<ref_obj_pid>2383815</ref_obj_pid>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[{Jensen <i>et al.</i>, 1999} Henrik Wann Jensen, Justin Legakis, and Julie Dorsey. Rendering of wet material. In Dani Lischinski and Greg Ward Larson, editors, <i>Rendering Techniques '99</i>, Eurographics, pages 273--282. Springer-Verlag Wien New York, 1999. Proc. 10th Eurographics Rendering Workshop, Granada, Spain, June 21--23, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383319</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[{Jensen <i>et al.</i>, 2001} Henrik Wann Jensen, Stephen R. Marschner, Marc Levoy, and Pat Hanrahan. A practical model for subsurface light transport. In <i>Proceedings of the 28th annual conference on Computer graphics and interactive techniques</i>, pages 511--518. ACM Press, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74361</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[{Kajiya and Kay, 1989} J. T. Kajiya and T. L. Kay. Rendering fur with three dimensional textures. In <i>Proceedings of the 16th annual conference on Computer graphics and interactive techniques</i>, pages 271--280. ACM Press, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>959207</ref_obj_id>
				<ref_obj_pid>959196</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[{Koenderink and Pont, 2003} J. Koenderink and S. Pont. The secret of velvety skin. <i>Machine Vision and Applications</i>, 14(4):260--268, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[{Koudelka, 2004} M. L. Koudelka. <i>Capture, analysis and synthesis of textured surfaces with variation in illumination, viewpoint, and time</i>. Yale University New Haven, CT, USA, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[{Krishnaswamy and Baranoski, 2004} A. Krishnaswamy and G. V. G. Baranoski. A Biophysically-Based Spectral Model of Light Interaction with Human Skin. <i>Computer Graphics Forum</i>, 23(3):331--340, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258801</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[{Lafortune <i>et al.</i>, 1997} Eric P. F. Lafortune, Sing-Choong Foo, Kenneth E. Torrance, and Donald P. Greenberg. Non-linear approximation of reflectance functions. In <i>Proceedings of the 24th annual conference on Computer graphics and interactive techniques</i>, pages 117--126. ACM Press/Addison-Wesley Publishing Co., 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[{Lam and Baranoski, 2006} Michael W. Y. Lam and Gladimir V. G. Baranoski. A predictive light transport model for the human iris. <i>Computer Graphics Forum</i>, 25(3):359--368, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>581911</ref_obj_id>
				<ref_obj_pid>581896</ref_obj_pid>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[{Lefebvre and Neyret, 2002} Sylvain Lefebvre and Fabrice Neyret. Synthesizing bark. In P. Debevec and S. Gibson, editors, <i>13th Eurographics Workshop on Rendering</i>, Pisa, Italy, 2002. Eurographics Association.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[{Lefebvre and Poulin, 2000} Laurent Lefebvre and Pierre Poulin. Analysis and synthesis of structural textures. In <i>Graphics Interface 2000</i>, pages 77--86, May 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>950715</ref_obj_id>
				<ref_obj_pid>950627</ref_obj_pid>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[{Lefohn <i>et al.</i>, 2003} Aaron Lefohn, Brian Budge, Peter Shirley, Richard Caruso, and Erik Reinhard. An ocularist's approach to human iris synthesis. <i>IEEE Comput Graphics Appl</i>, 23(6):70--75, November/December 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344958</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[{Lokovic and Veach, 2000} Tom Lokovic and Eric Veach. Deep shadow maps. In <i>SIGGRAPH '00: Proceedings of the 27th annual conference on Computer graphics and interactive techniques</i>, pages 385--392, New York, NY, USA, 2000. ACM Press/Addison-Wesley Publishing Co.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2381358</ref_obj_id>
				<ref_obj_pid>2381356</ref_obj_pid>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[{Lu <i>et al.</i>, 2005} Jianye Lu, Athinodoros S. Georghiades, Holly Rushmeier, Julie Dorsey, and Chen Xu. Synthesis of material drying history: Phenomenon modeling, transferring and rendering. In <i>proceedings of Eurographics Workshop on Natural Phenomena</i>, pages 7--16, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1189765</ref_obj_id>
				<ref_obj_pid>1189762</ref_obj_pid>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[{Lu <i>et al.</i>, 2007} Jianye Lu, Athinodoros S. Georghiades, Andreas Glaser, Hongzhi Wu, Li-Yi Wei, Baining Guo, Julie Dorsey, and Holly Rushmeier. Context-aware textures. <i>ACM Trans. Graph.</i>, 26(1):3, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732140</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[{Marschner <i>et al.</i>, 2000} Stephen R. Marschner, Brian K. Guenter, and Sashi Raghupathy. Modeling and rendering for realistic facial animation. In <i>Proceedings of the Eurographics Workshop on Rendering Techniques 2000</i>, pages 231--242, London, UK, 2000. Springer-Verlag.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882345</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[{Marschner <i>et al.</i>, 2003} Stephen R. Marschner, Henrik Wann Jensen, Mike Cammarano, Steve Worley, and Pat Hanrahan. Light scattering from human hair fibers. <i>ACM Trans. Graph.</i>, 22(3):780--791, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073254</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[{Marschner <i>et al.</i>, 2005} Stephen R. Marschner, Stephen H. Westin, Adam Arbree, and Jonathan T. Moon. Measuring and modeling the appearance of finished wood. <i>ACM Trans. Graph.</i>, 24(3):727--734, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1363577</ref_obj_id>
				<ref_obj_pid>1363373</ref_obj_pid>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[{M&#233;rillou and Ghazanfarpour, 2008} S. M&#233;rillou and D. Ghazanfarpour. A survey of aging and weathering phenomena in computer graphics. <i>Computers&amp;Graphics</i>, 32(2):159--174, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614469</ref_obj_id>
				<ref_obj_pid>614280</ref_obj_pid>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[{Merillou <i>et al.</i>, 2000} S. Merillou, J.-M. Dischler, and D. Ghazanfarpour. A BRDF postprocess to integrate porosity on rendered surface. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 6(4):306--318, October 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[{Merillou <i>et al.</i>, 2001a} S. Merillou, J. M. Dischler, and D. Ghazanfarpour. Surface scratches: measuring, modeling and rendering. <i>The Visual Computer</i>, 17(1):30--45, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>781007</ref_obj_id>
				<ref_obj_pid>780986</ref_obj_pid>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[{Merillou <i>et al.</i>, 2001b} Stephane Merillou, Jean-Michel Dischler, and Djamchid Ghazanfarpour. Corrosion: Simulating and rendering. In <i>GI 2001</i>, pages 167--174, June 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141995</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[{Moon and Marschner, 2006} Jonathan T. Moon and Stephen R. Marschner. Simulating multiple scattering in hair using a photon mapping approach. In <i>SIGGRAPH '06: ACM SIGGRAPH 2006 Papers</i>, pages 1067--1074, New York, NY, USA, 2006. ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614382</ref_obj_id>
				<ref_obj_pid>614268</ref_obj_pid>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[{Nagata <i>et al.</i>, 1997} Noriko Nagata, Toshimasa Dobashi, Yoshitsugu Manabe, Teruo Usami, and Seiji Inokuchi. Modeling and Visualization for a Pearl-Quality Evaluation Simulator. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 3(4):307--315, October 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97922</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[{Nakamae <i>et al.</i>, 1990} E. Nakamae, K. Kaneda, T. Okamoto, and T. Nishita. A lighting model aiming at drive simulators. In <i>Proceedings of Siggraph 1990</i>, pages 395--404. ACM SIGGRAPH, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141986</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[{Narasimhan <i>et al.</i>, 2006} S. G. Narasimhan, M. Gupta, C. Donner, R. Ramamoorthi, S. K. Nayar, and H. W. Jensen. Acquiring scattering properties of participating media by dilution. <i>Proceedings of ACM SIGGRAPH 2006</i>, 25(3):1003--1012, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[{Nishita <i>et al.</i>, 1997} T. Nishita, H. Iwasaki, Y. Dobashi, and E. Nakamae. A modeling and rendering method for snow by using metaballs. <i>Computer Graphics Forum</i>, 16(3):357--364, August 1997. ISSN 1067--7055.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192213</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[{Oren and Nayar, 1994} Michael Oren and Shree K. Nayar. Generalization of lambert's reflectance model. In <i>Proceedings of the 21st annual conference on Computer graphics and interactive techniques</i>, pages 239--246. ACM Press, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[{Paquette <i>et al.</i>, 2002} Eric Paquette, Pierre Poulin, and George Drettakis. The simulation of paint cracking and peeling. In <i>Graphics Interface 2002</i>, pages 59--68, May 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>360839</ref_obj_id>
				<ref_obj_pid>360825</ref_obj_pid>
				<ref_seq_no>75</ref_seq_no>
				<ref_text><![CDATA[{Phong, 1975} Bui Tuong Phong. Illumination for computer generated pictures. <i>Commun. ACM</i>, 18(6):311--317, 1975.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378503</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>76</ref_seq_no>
				<ref_text><![CDATA[{Prusinkiewicz <i>et al.</i>, 1988} Przemyslaw Prusinkiewicz, Aristid Lindenmayer, and James Hanan. Development models of herbaceous plants for computer imagery purposes. In <i>SIGGRAPH '88: Proceedings of the 15th annual conference on Computer graphics and interactive techniques</i>, pages 141--150, New York, NY, USA, 1988. ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>77</ref_seq_no>
				<ref_text><![CDATA[{Robertson, 1999} James Robertson. <i>Forensic Examination of Human Hair</i>. CRC Press, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>78</ref_seq_no>
				<ref_text><![CDATA[{Rubin, 1998} Barry Rubin. Tailored Fiber Cross Sections. <i>Advanced Materials</i>, 10(15):1225--1227, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>79</ref_seq_no>
				<ref_text><![CDATA[{Rump <i>et al.</i>, 2008} M. Rump, G. Muller, R. Sarlette, D. Koch, and R. Klein. Photo-realistic rendering of metallic car paint from image-based measurements. In <i>Computer Graphics Forum</i>, volume 27, pages 527--536. Blackwell Publishing Ltd, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>80</ref_seq_no>
				<ref_text><![CDATA[{Shimizu <i>et al.</i>, 2003} Clement Shimizu, Gary W. Meyer, and Joseph P. Wingard. Interactive goniochromatic color design. In <i>Eleventh Color Imaging Conference</i>, pages 16--22, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732287</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>81</ref_seq_no>
				<ref_text><![CDATA[{Stam, 2001} Jos Stam. An illumination model for a skin layer bounded by rough surfaces. In <i>Proceedings of the 12th Eurographics Workshop on Rendering Techniques</i>, pages 39--52, London, UK, 2001. Springer-Verlag.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>82</ref_seq_no>
				<ref_text><![CDATA[{Streit and Heidrich, 2002} L. Streit and W. Heidrich. A biologically-parameterized feather model. <i>Computer Graphics Forum</i>, 21(3):565--565, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>83</ref_seq_no>
				<ref_text><![CDATA[{Sun <i>et al.</i>, 2001} Yinlong Sun, F. David Fracchia, Mark S. Drew, and Thomas W. Calvert. A spectrally based framework for realistic image synthesis. <i>The Visual Computer</i>, 17(7):429--444, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1263313</ref_obj_id>
				<ref_obj_pid>1263130</ref_obj_pid>
				<ref_seq_no>84</ref_seq_no>
				<ref_text><![CDATA[{Sun <i>et al.</i>, 2007} B. Sun, K. Sunkavalli, R. Ramamoorthi, PN Belhumeur, and SK Nayar. Time-Varying BRDFs. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 13(3):595--609, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1122506</ref_obj_id>
				<ref_obj_pid>1122501</ref_obj_pid>
				<ref_seq_no>85</ref_seq_no>
				<ref_text><![CDATA[{Sun, 2006} Yinlong Sun. Rendering biological iridescences with rgb-based renderers. <i>ACM Trans. Graph.</i>, 25(1):100--129, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97908</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>86</ref_seq_no>
				<ref_text><![CDATA[{Takagi <i>et al.</i>, 1990} Atsushi Takagi, Hitoshi Takaoka, Tetsuya Oshima, and Yoshinori Ogata. Accurate rendering technique based on colorimetric conception. In <i>Proceedings of the 17th annual conference on Computer graphics and interactive techniques</i>, pages 263--272. ACM Press, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1501591</ref_obj_id>
				<ref_obj_pid>1501585</ref_obj_pid>
				<ref_seq_no>87</ref_seq_no>
				<ref_text><![CDATA[{Terraz <i>et al.</i>, 2009} O. Terraz, G. Guimberteau, S. M&#233;rillou, D. Plemenos, and D. Ghazanfarpour. 3Gmap L-systems: an application to the modelling of wood. <i>The Visual Computer</i>, 25(2):165--180, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>88</ref_seq_no>
				<ref_text><![CDATA[{Vogelmann, 1993} C. Vogelmann. Plant tissue optics. <i>Annual review of plant physiol. plant mol. biol.</i>, 44:231--251, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>89</ref_seq_no>
				<ref_text><![CDATA[{Walters, 2002} Kenneth A. Walters. <i>Dermatological and Transdermal Formulations</i>. Marcel Dekker Incorporated, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073252</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>90</ref_seq_no>
				<ref_text><![CDATA[{Wang <i>et al.</i>, 2005} Lifeng Wang, Wenle Wang, Julie Dorsey, Xu Yang, Baining Guo, and Heung-Yeung Shum. Real-time rendering of plant leaves. In <i>SIGGRAPH '05: ACM SIGGRAPH 2005 Papers</i>, pages 712--719, New York, NY, USA, 2005. ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141951</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>91</ref_seq_no>
				<ref_text><![CDATA[{Wang <i>et al.</i>, 2006} Jiaping Wang, Xin Tong, Stephen Lin, Minghao Pan, Chao Wang, Hujun Bao, Baining Guo, and Heung-Yeung Shum. Appearance manifolds for modeling time-variant appearance of materials. <i>ACM Trans. Graph.</i>, 25(3):754--761, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134078</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>92</ref_seq_no>
				<ref_text><![CDATA[{Ward, 1992} Gregory J. Ward. Measuring and modeling anisotropic reflection. In <i>Proceedings of the 19th annual conference on Computer graphics and interactive techniques</i>, pages 265--272. ACM Press, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1921278</ref_obj_id>
				<ref_obj_pid>1921264</ref_obj_pid>
				<ref_seq_no>93</ref_seq_no>
				<ref_text><![CDATA[{Weidlich and Wilkie, 2008a} Andrea Weidlich and Alexander Wilkie. Modeling aventurescent gems with procedural textures. In <i>Proceedings of the Spring Conference on Computer Graphics (SCCG)</i>, pages]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1330517</ref_obj_id>
				<ref_obj_pid>1330511</ref_obj_pid>
				<ref_seq_no>94</ref_seq_no>
				<ref_text><![CDATA[{Weidlich and Wilkie, 2008b} Andrea Weidlich and Alexander Wilkie. Realistic rendering of birefringency in uniaxial crystals. <i>ACM Transactions on Graphics</i>, 27(1):6:1--6:12, March 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1555903</ref_obj_id>
				<ref_obj_pid>1555880</ref_obj_pid>
				<ref_seq_no>95</ref_seq_no>
				<ref_text><![CDATA[{Weidlich and Wilkie, 2009} Andrea Weidlich and Alexander Wilkie. Rendering the effect of labradorescence. In <i>Proceedings of Graphics Interface 2009</i>, pages]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>96</ref_seq_no>
				<ref_text><![CDATA[{Weidlich, 2009} Andrea Weidlich. <i>Pseudochromatic Colourisation of Crystals in Predictive Image Synthesis</i>. PhD thesis, Institute of Computer Graphics and Algorithms, Vienna University of Technology, Favoriten-strasse 9--11/186, A-1040 Vienna, Austria, January 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141987</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>97</ref_seq_no>
				<ref_text><![CDATA[{Weyrich <i>et al.</i>, 2006} Tim Weyrich, Wojciech Matusik, Hanspeter Pfister, Bernd Bickel, Craig Donner, Chien Tu, Janet McAndless, Jinho Lee, Addy Ngan, Henrik Wann Jensen, and Markus Gross. Analysis of human faces using a measurement-based skin reflectance model. In <i>SIGGRAPH '06: ACM SIGGRAPH 2006 Papers</i>, pages 1013--1024, New York, NY, USA, 2006. ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2381387</ref_obj_id>
				<ref_obj_pid>2381384</ref_obj_pid>
				<ref_seq_no>98</ref_seq_no>
				<ref_text><![CDATA[{Wojtan <i>et al.</i>, 2007} Chris Wojtan, Mark Carlson, Peter J. Mucha, and Greg Turk. Animating Corrosion and Erosion. In D. Ebert and S. Merillou, editors, <i>Eurographics Workshop on Natural Phenomena</i>, pages 15--22, Prague, Czech Republic, 2007. Eurographics Association.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383303</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>99</ref_seq_no>
				<ref_text><![CDATA[{Xu <i>et al.</i>, 2001} Ying-Qing Xu, Yanyun Chen, Stephen Lin, Hua Zhong, Enhua Wu, Baining Guo, and Heung-Yeung Shum. Photorealistic rendering of knitwear using the lumislice. In <i>Proceedings of the 28th annual conference on Computer graphics and interactive techniques</i>, pages 391--398. ACM Press, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>100</ref_seq_no>
				<ref_text><![CDATA[{Xuey <i>et al.</i>, 2008} S. Xuey, J. Wang, X. Tong, Q. Dai, and B. Guo. Image-based Material Weathering. In <i>Computer Graphics Forum</i>, volume 27, pages 617--626. Blackwell Publishing Ltd, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>101</ref_seq_no>
				<ref_text><![CDATA[{Yuan <i>et al.</i>, 1988} Ying Yuan, Tosiyasu L. Kunii, Naota Inamato, and Lining Sun. Gemstone fire: Adaptive dispersive ray tracing of polyhedrons. <i>The Visual Computer</i>, 4(5):259--70, November 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1263273</ref_obj_id>
				<ref_obj_pid>1263129</ref_obj_pid>
				<ref_seq_no>102</ref_seq_no>
				<ref_text><![CDATA[{Zinke and Weber, 2007} Arno Zinke and Andreas Weber. Light scattering from filaments. <i>IEEE Trans. on Vis. and Comp. Graphics</i>, 13(2):342--356, March/April 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SIGGRAPH 2009 Course Advanced Material Appearance Modeling  Julie Dorsey Holly Rushmeier Yale University 
 { julie.dorsey, holly.rushmeier (at) yale.edu}   Table of Contents Course Description   3 Annotated 
Slides    6 Bibliography    126 Course Description For many years appearance models in computer 
graphics focused on general models for reflectance functions coupled with texture maps. Recently it has 
been recognized that even very common materials such as hair, skin, fabric, and rusting metal require 
more sophisticated models to appear realistic. We will begin by briefly reviewing basic reflectance models 
and the use of texture maps. We then describe common themes in advanced material models that include 
combining the effects of layers, groups of particles and/or fibers. We will survey the detailed models 
necessary needed to model materials such as skin (including pigmentation, pores, subsurface scattering), 
plants (including internal structure) and automotive paints (including color flop and sparkle). We will 
then treat the modeling of complex appearance due to aging and weathering processes. A general taxonomy 
of effects will be presented, as well as methods to simulate and to capture these effects. We close with 
a summary of current trends in material appearance research and a discussion of existing and needed resources. 
Prerequisites Knowledge of basic rendering and reflectance functions. Syllabus/Approximate Schedule 
Introduction: 15 min. Background: 50 min. Specialized Material Models: Common Themes: 20 min Natural 
Materials: 30 min. Manufactured/Processed Materials: 25 min. Aging and Weathering Processes: Taxonomy: 
10 min. Simulation: 20 min. Capture Approaches: 15 min. Current Trends and Needs 40 min. Speakers Julie 
Dorsey is a Professor of Computer Science at Yale University, where she teaches computer graphics. She 
came to Yale in 2002 from MIT, where she held tenured appointments in both the Department of Electrical 
Engineering and Computer Science (EECS) and the School of Architecture. She received undergraduate degrees 
in Architecture and graduate degrees in Computer Science from Cornell University. With architecture as 
a driving application, she has studied a wide range of problems in computer graphics, including sketch-based 
interfaces for early conceptual design, acceleration methods for real-time rendering, and the creation 
of detailed photorealistic renderings. Her contributions also include algorithms for lighting and acoustical 
design and visualization. She is particularly well known for her research in modeling the appearance 
of materials -- for example, she pioneered techniques to model the visual richness of irregular metal 
patinas and eroded stone. Her current research interests include photorealistic image synthesis, material 
and texture models, illustration techniques, and interactive visualization of complex scenes, with an 
application to urban environments. In addition to serving on numerous conference program committees, 
she has served as an associate editor for IEEE Transactions on Visualization and Computer Graphics and 
The Visual Computer, and was Papers Chair for ACM SIGGRAPH 2006. She has received several professional 
awards, including MIT's Edgerton Faculty Achievement Award, a National Science Foundation CAREER Award, 
and an Alfred P. Sloan Foundation Research Fellowship. Holly Rushmeier received the BS, MS and PhD degrees 
in Mechanical Engineering from Cornell University in 1977, 1986 and 1988 respectively. Between receiving 
the BS and returning to graduate school in 1983 she worked as an engineer at the Boeing Commercial Airplane 
Company and at Washington Natural Gas Company (now a part of Puget Sound Energy). In 1988 she joined 
the Mechanical Engineering faculty at Georgia Tech. While there she conducted sponsored research in the 
area of computer graphics image synthesis and taught classes heat transfer and numerical methods at both 
the undergraduate and graduate levels. At the end of 1991 Dr. Rushmeier joined the computing and mathematics 
staff of the National Institute of Standards and Technology, focusing on scientific data visualization. 
 From 1996 to early 2004 Dr. Rushmeier was a research staff member at the IBM T.J. Watson Research Center. 
At IBM she worked on a variety of data visualization problems in applications ranging from engineering 
to finance. She also worked in the area of acquisition of data required for generating realistic computer 
graphics models, including a project to create a digital model of Michelangelo's Florence Pieta, and 
the development of a scanning system to capture shape and appearance data for presenting Egyptian cultural 
artifacts on the World Wide Web. Dr. Rushmeier was Editor-in-Chief of ACM Transactions on Graphics from 
1996-99. She has also served on the editorial board of IEEE Transactions on Visualization and Computer 
Graphics. She is currently on the editorial boards of Computer Graphics Forum, IEEE Computer Graphics 
and Applications, ACM Journal of Computing and Cultural Heritage and ACM Transactions on Applied Perception. 
In 1996 she served as the papers chair for the ACM SIGGRAPH conference, in 1998, 2004 and 2005 as the 
papers co-chair for the IEEE Visualization conference and in 2000 as the papers co­chair for the Eurographics 
Rendering Workshop. She has also served in numerous program committees including multiple years on the 
committees for SIGGRAPH, IEEE Visualization, Eurographics, Eurographics Rendering Workshop, and Graphics 
Interface. Additional information The speakers are (along with François Sillion) the authors of Digital 
Modeling of Material Appearance published by Morgan Kaufmann/Elsevier. Further information about the 
speakers can be found at http://graphics.cs.yale.edu/  In these course notes we present principles 
of defining numerical models to be used in rendering realistic imagery of physical materials. Additional 
information can be found at http://graphics.cs.yale.edu/. The in person presentation of this course varies 
from these notes in the interest of timeliness, and considering the fact that fair use materials can 
not be posted for distribution to non-course attendees. These notes are a revised version of the notes 
for the SIGGRAPH 2008 class on Advanced Material Appearance Models. These notes also draw on the text: 
Digital Modeling of Material Appearance (Morgan-Kaufmann/Elsevier. )  The materials here are rendered 
with models. An artist conceived the shape. A purely artistic approach could be used to digitally paint 
the shades of light and dark on the digital shapes to give the illusion of translucent stone or copper 
metal. However, to generate these images material models are expressed numerically and rendered using 
lighting simulations. That is their appearance the colors, shades of light and dark, were computed, 
rather than being digitally painted on the model.  We define a model as taking a physically measurable 
input and producing a predictive output that can be verified by physical measurement. A model of a material 
makes possible the reliable rendering of the appearance of that material in any geometric and lighting 
conditions. An artistic technique as takes an input which is not necessarily measurable, and produces 
an output that may or may not reproduce the appearance of an object under arbitrary circumstances. Human 
judgment is required to use an artistic technique, and to evaluate its success. Our goal is to make 
predictive images that give a view of a scene or object that is the same as if the person were viewing 
it directly. Material modeling is one aspect of this. We need to consider the object s shape, and the 
light incident on it.  Shape is the large scale form or geometry of the object. The shape is needed 
to place the image of the object correctly with respect to other objects in the scene, to determine which 
other objects are occluded by the object, and what areas are cast into shadow by the object. Fine scale 
geometric variations in the object we define as part of the object s material from the point of view 
of creating digital models in computer graphics. For a close view of a tree branch, a leaf is defined 
by a flat shape, with the number of lobes or points depending on the type of tree. In an aerial photograph, 
a leaf is a small facet in a tree canopy material that covers the terrain. Many methods can be used to 
represent shape. The area of computer­aided geometry is devoted to the study of shape representation, 
and extensive descriptions of representations such as NURBs (non-uniformrational B-splines), triangle 
meshes, subdivision surfaces and implicit surface are documented at length in references such as Farin 
Curves and Surfaces for Computer-Aided Geometric Design: A Practical Code. Academic Press, Inc., 1996. 
Many methods can be used to compute the interreflections of light between objects in an environment. 
These methods, referred to as global illumination methods, include ray tracing, radiosity, photon mapping 
and hybrids of these various approaches. Thorough discussions of these methods can be found in Dutre, 
Bekaert and Bala, Advanced Global Illumination. AK Peters Limited, Wellesley, MA, 2003. For rendering 
appearance, the essential feature of a  global illumination method is that for a given ray direction 
the quantity of light from that direction at a particular point can be efficiently computed. An environment 
consists of a set of objects, each defined by a shape and material description, and at least one light 
source. An infinite number of images could be created of such an environment, and to specify a particular 
image a viewpoint, view direction and view frustum (i.e. field of view) need to be specified. The image 
is formed by projecting the objects in the environment seen through the frustum onto an image plane that 
spans the field of view and is perpendicular to the view direction. In a digital image, the image is 
discretized into pixels, and the display values for that pixel are set by determining the light that 
would arrive at the viewer from the object visible through that pixel. There are three important components 
of a material model that allow us to recognize a material spectral, directional and spatial. We notice 
the color of an object (resulting from the spectral composition of light), its directionality (hazy, 
glossy, shiny,) and small spatial variations (textures formed by light and dark, or little bumps.) Example 
of introducing spectral and directional variations   Introducing spatial variations  Spatially varying 
spectral and directional variations to make this look like a worn,dirty metallic object. Examples of 
directionality beyond directional reflectance The most familiar and basic light scattering is regular 
or mirror-like reflection, as shown in the photo at the top. Light rays reflect into one single direction, 
and that direction forms the same angle to the surface normal as the incident direction, as shown on 
the  lower left. Because the reflected rays stay organized as they were when they left the previous 
objects, a sharp image is formed just as though you were looking directly at the objects. This regular, 
or mirror-like reflection is referred to as pure or ideal specular reflection. Many materials are shiny 
or glossy, but not purely specular. In these materials, incident beams of light are distributed into 
a cone or lobe of directions centered around the specular, or mirror direction. The result of this is 
when you are looking at such materials the light reflected from each point of the surface includes light 
from an a range of surfaces in the environment, instead of just reflecting one point. Instead of seeing 
sharp edges reflected, everything looks blurred. By observing the reflections in the paint can in the 
image, you can see that how blurred things look depends on how close the objects being reflected are 
to the glossy surface. If they are relatively close, the cross section of the cone from which a point 
is reflecting light is relatively small, and lines like that between the yellow and blue surfaces above 
are only blurred a bit. As the objects get further away, the cross section of the cone becomes large, 
and can include entire objects which then do not appear with any detail when reflected in the glossy 
surface.  Objects that appear to have the same pattern of light and dark regardless of how you view 
them (as long as you don t block a significant source of light from the environment as you move to another 
view) are diffuse. An ideal diffuse (also referred to as Lambertian) object reflects an incident beam 
of light as light rays of much lower magnitude in all directions. The light coming from any point on 
the object in any direction is a product of light coming from many different sources in the environment. 
The contribution of each source in the environment varies very slowly from point to point on the object, 
so the amount of light varies slowly from point to point, and In addition to the reflectance that depends 
on material microstructure and chemical composition, the appearance depends on small scale geometric 
structure. Just as some materials are characterized primarily by the spatial variations in reflectance, 
other materials are characterized primarily by their small scale geometric structure. Small is defined 
as orders of magnitude smaller than the overall object. The image above shows a piece of plastic with 
a pattern pressed into it that changes the surface from smooth to bumpy. The small scale geometric structure 
shown here is characteristic of leather material, and this fact is used in the production of physical 
materials to make a plastic look like leather. The variation of light and dark in the image of the plastic 
is not due to spatial changes in reflectance, but to the change of surface orientation caused by the 
small scale geometry. Even small indentations can cause large changes in the surface normal. The surface 
normal, rather than the absolute surface position, determines in which direction incident light will 
be reflected.  Some materials don t just reflect light from the surface, or just transmit the light. 
In some cases light penetrates the material and scatters in the interior. This is referred to as subsurface 
scattering, and can occur in dielectrics, not metals. Under normal room illumination, surfaces which 
allow subsurface scattering often do not look dramatically different from completely opaque surfaces. 
The image on the right though shows an extreme example of illumination. A green laser is directed at 
the top of a small piece of stone. Besides being reflected from the top, the light is scattered in the 
material and is visible emerging from the sides of the stone. Terminology and Mathematical Descriptions 
Key quantities: Radiance L Bidirectional Reflectance Distribution Function (BRDF) fr : An explanation 
of the mathematics of light transport isn t possible in a brief lecture. However, a couple of key points 
are: -- a lot of the notation in light transport is just denoting that quantities vary with color (spectral 
 dependance .), direction (given by angles . and f) and position (x,y) -- there are two quantities that 
are key, but which take some getting used to . One is the quantity of light we want to compute, the radiance 
L. The other is the function telling how a surface scatters light, the BRDF fr. Components of Material 
Model Spectral .: wavelength (color) dependence Directional .,f : direction (Shiny, matte, glossy, hazy) 
Spatial variation x,y: position (texture)   The key quantity we use to define how a surface redirects 
light is the BRDF, which relates incident and reflected radiance for two given directions. The BRDF is 
a distribution function, not a fraction from zero to one. It can take on values from zero to infinity. 
To conserve energy, the integral of the BRDF over all reflected directions must be less then or equal 
to one. Many common reflectance models are named, generally after the people who developed the models. 
NOTE: There are no compliance standards for claiming that a named model is being used, so you can t be 
absolutely sure that giving the same parameters to a particular model in one software package will produce 
the same results in another package.  The directionality of transmission from a smooth surface is a 
bit more complicated that reflection. First, most metals have a high tendency to absorb electromagnetic 
energy, so transmission of visible light is not observed. For dielectrics, the change in the speed of 
light in the material causes a change in the direction. This change in direction is called refraction, 
and is expressed by Snell s Law as shown above. Unlike the direction of reflection, the direction of 
refraction depends on the properties of the materials. Since light is electromagnetic energy, its interaction 
is governed by the properties that quantify the material s interaction with electric and magnetic fields. 
In the solution to Maxwell s equations these properties are expressed as the index of refraction n and 
a coefficient that captures the tendency to absorb electomagnetic waves k . The value of n is the ratio 
of the speed of light in a vacuum to the speed of light in the material. The value of k is zero for dielectrics, 
which do not conduct electricity, and greater than zero for metals, which do. Values of k and n are found 
by measurement and can be looked up in handbooks or online resources. Generally understanding and applying 
the results of the smooth surface solution requires only knowing some rough estimates of typical values 
of these constants for common materials.  In addition to giving directionality, the fraction of light 
reflected can also be calculated from the solution of Maxwell s equations, and the results are referred 
to as the Fresnel equations. For a dielectric, the light that is not reflected from the surface is transmitted. 
For a metal, the light that is not reflected is absorbed. The Fresnel equations give complicated algebraic 
expressions for reflectance, but only straightforward number crunching is needed to evaluate given values 
of . , n and k . Since metals have a high reflectance for all angles, the Fresnel effect is less pronounced. 
Although it is rarely included in visual simulations, metals all tend to look white or gray at grazing 
angles.  Lambertian, or ideal diffuse reflectance is in a sense the opposite of specular reflection. 
Instead of all light being reflected in a single direction, it is reflected in all directions with the 
same radiance. Unlike specular reflection, this is not the result of solving Maxwell s equations for 
some particular surface configuration. It is an approximation of the observed behavior of many materials. 
While real materials usually deviate from Lambertian for angles of view or incidence greater than 60 
degrees, the Lambertian model is used for its computational simplicity. For measurement purposes, some 
materials have been designed that are very close to being to Lambertian, such as Spectralon® from Labsphere 
Inc. Materials can be modeled as a combination of Lambertian and mirror-like reflectance. The material 
can also have spectral values that vary with position. Here a scanned object is shown as white Lambertian 
(upper left), spectrally varying with position (upper right), with mirror-like reflection of the light 
source (lower left), and with mirror-like reflection of the entire environment.   As noted in R. L. 
Cook and K. E. Torrance. A reflectance model for computer graphics. ACM Transactions on Graphics, 1(1):7 
24, January 1982. The color of specularly reflected light is white for dielectics, and the color of the 
material for metals. The color is predicted using the Fresnel equation for a smooth surface. The original 
Phong reflectance model is described in the classic paper: Bui Tuong Phong Illumination for computer 
generated pictures Communications of the ACM, v.18 n.6, p.311-317, June 1975 . It was expressed as reflectance 
function for light intensity, rather than as a BRDF for computing radiance. However, it was inspired 
by physical observation. The effect of the model in rendering a sphere is compared to a photograph of 
a real sphere in the paper. The fuzziness of the specular reflection  is computed as a function of the 
angle a between the reflected direction and the mirror reflection angle: reflectance = .d (cos ..) + 
.s (cos .s)n In contrast to diffuse reflection, the specular component concentrates the reflected light. 
The larger the value of n, the smaller the specular highlights formed by the reflection of the light 
source. The specular lobe in the Phong model is taking into account roughness at a very small scale. 
At a small scale parts of a surface are oriented to reflect into directions that aren t the mirror direction 
for the flat surface. H is the half way vector, the direction a surface normal would need to be pointing 
for a mirror reflection to be visible for a given pair of light L and view V directions. Many reflectance 
models are computed in terms of this half way vector. Blinn-Phong (using .h instead of .s)  Increasing 
p .d+ .s cosp .h  The specular lobe in the Phong model is taking into account roughness at a very small 
scale. At a small scale parts of a surface are oriented to reflect into directions that aren t the mirror 
direction for the flat surface. H is the half way vector, the direction a surface normal would need 
to be pointing for a mirror reflection to be visible for a given pair of light L and view V directions. 
Many reflectance models are computed in terms of this half way vector.  These image show a macroscopic 
example of the spreading effect of a rough surface. For a surface that is somewhat rough at a microscopic 
level, some portions of the surface are oriented in the direction of the halfway vector even when the 
halfway vector isn t the same as the main surface normal. The Ward reflectance model is similar to the 
Phong model except it is expressed in physical terms it expresses the relationship between incident and 
reflectance radiance and conserves energy. Rather than using the cosine to a power, it uses an exponential 
function, parameterized by an average slope, to express the shape of the specular lobe. Furthermore, 
the lobe can be anisotropic by expressing different slopes for different directions on a surface (e.g. 
for a set of grooves the slope is zero along the grooves, and potentially steep perpendicular to the 
grooves).The model can be applied to regular and diffuse transmission through a thin surface. The model 
is fully described in as described in Ward Larson and Shakespeare, Rendering with radiance: the art and 
science of lighting visualization (Morgan Kaufmann, 1998) Since the Ward model is developed in physical 
terms of incident and reflected radiance, it works (by design) in a system that simulates physically 
accurate global illumination. These variations were rendered using the Radiance software system,  http://radsite.lbl.gov/ 
A point to remember is that physically accurate material models only create realistic appearance when 
used in the context of a physically accurate global illumination system. Another detail to note is that 
a small correction to the original model is available in Arne Duer. An Improved Normalization For The 
Ward Reflectance Model. JOURNAL OF GRAPHICS TOOLS, 11(1):51, 2006. Anisotropic reflection has a significant 
impact on appearance, but for a complicated object its effect is only clear when the effect of isotropic, 
or anisotropic reflection with a different orientation is displayed.  The generalized cosine lobe model 
described in Lafortune, Foo,Torrance, and Greenberg Non-linear approximation of reflectance functions 
In Proceedings of the 24th annual conference on Computer graphics and interactive techniques (1997pp. 
117 126.) Gives a different generalization of the Phong model. Like the Ward model, it is formulated 
in physical terms. It conserves energy. Instead of just describing peaks of reflection around the specular 
direction, it allows the definition of lobes (possibly anisotropic) around any axis defined with respect 
to the surface. Important other axes are just off the specular direction, the normal direction and the 
direction of the source (for backscatter). The general form of the reflectance is fr = C(u) (Cxuxvx+Cyuyvy+Czuzvz)n 
where u and v are vectors in the incident and reflected directions, Cx,Cy are coefficients determining 
the direction and shape of the lobe, n defines how narrow it is, and C(u) is a normalizing function to 
insure the function conserves energy. Sets of functions of An example of a BRDF that the Lafortune model 
can represent that previous models could not is generalized diffuse reflectance. In general, even surfaces 
that appear matte or diffuse don t reflect radiance evenly in all directions the reflection may peak 
in the direction of the surface normal and fall off at near grazing viewing angles. The effects shown 
here are found using Cx=Cy=0, Cz=1, n equal to zero, 0.5 and 2 respectively.  The Lafortune model, unlike 
Phong or Ward, also provides a mechanism for defining back scatter. In this case a sum of two Lafortune 
lobes is used. With summing functions, there become a large number of parameters Cx,Cy,Cz and n to be 
defined for specifying reflectance. This makes the model inconvenient for user interfaces. The Lafortune 
model is useful though for fitting masses of measured BRDF data into a compact representation. The Ashikhmin-Shirley 
modification of Phong reflectance (Ashikhmin and Shirley, An Anisotropic Phong BRDF Model Journal of 
Graphic Tools, 5,2, (2000), pp.25-32) has the feature that it includes an explicit term for the Fresnel 
reflectance. The specular reflectance increases as the angle of incidence increases. The diffuse component 
is appropriately reduced at these angles to maintain energy conservation. The formulation also maintains 
reciprocity, and allows for anisotropy. The Fresnel component is computed with Schlick s approximation 
(see Christophe Schlick. A customizable reflectance model for everyday rendering. Rendering Techniques 
93, pages 73 84. ) In the examples shown, the decrease of the diffuse component with view angle relative 
to the ideal diffuse component used in the Ward model can be observed.  In contrast to empirical methods 
that look for convenient functional forms, first principles methods model the interaction with light 
with a mathematical model of material defined at a microscopic scale. The most frequently used first 
 principles models use as a mathematical model a statistical distribution of surface facets to describe 
the details of the boundary between a material and air. The most popular methods model this interaction 
with geometric optics, which requires that the surface being modeled be large with respect to the wavelength 
of light (which is 0.4 to 0.7 microns) Some more complex models use wave optics to capture of the effects 
of diffraction at the surface. First principles models account for the effects that facets can have 
on one another they may block light incident on another facet, making it appear darker, or they may 
block light leaving the facet before it reaches a viewer, again resulting in a darker appearance. Even 
unblocked, the orientation of the facets results in light being scattered in a much different directional 
pattern than from a smooth surface.  Two popular first principles models are Blinn, Models of light 
reflection for computer synthesized pictures, SIGGRAPH 1977, pp. 192-198. and Cook- Torrance, Cook and 
Torrance A reflectance model for computer graphics . ACM Transactionson Graphics 1, 1 (Jan. 1982), 7 
24 They are both based on specular reflections of distributions of facets. The difference between them 
is the distribution of the facets assumed. The principle feature of the Cook-Torrance model is the prediction 
of off specular peaks, that are the consequences of shadowing and masking causing asymmetries. The principle 
feature of the Oren-Nayar model is the prediction of back scattering, that is a consequence of facets 
oriented towards the light source diffusely reflect some light back to the source. The result in each 
case are BRDF functions with lobes in the specular and backscatter directions that have more complicated 
structure than those used in the empirical models. The BRDF for these models is specified by giving parameters 
for the microscopic surface geometry. However, since the microstructure is rarely known, the facet distribution 
parameters are normally treated as parameters similar to n in the Phong and Lafortune models for controlling 
the shape of these complicated distributions.  For nearly smooth surfaces specular and/or diffuse reflectance 
can not be assumed at each facet. The effects of electromagnetic waves interfering with each other need 
to be accounted for. Methods by Kajiya Anisotropic Reflectance Models, SIGGRAPH 1985, pp15-21 and He 
et al. A Comprehensive Physical Model for Light Reflections, SIGGRAPH 91, pp175-186 account for these 
effects that are important for nearly smooth surfaces. Accounting for wave phenomena on irregular surface 
makes for a more complicated model   These images showing the dramatic effect the He-Torrance model 
can have on near smooth surfaces were produced at and are copyrighted by Westin, Li and Torrance, and 
appear in the technical report cited . Recent work in optics and computer vision have re-examined some 
assumptions made in many graphics first principles models, in particular the form of the shadowing term 
and the effect of interreflections in rough surfaces. For further reading consult: J.J. Koenderink, A.J. 
Van Doorn, K.J. Dana, and S. Nayar. Bidirectional Reflection Distribution Function of Thoroughly Pitted 
Surfaces. International Journal of Computer Vision, 31(2):129 144, 1999. H. Ragheb and E.R. Hancock. 
Testing new variants of the Beckmann Kirchhoff model against radiance data. Computer Vision and Image 
Understanding, 102(2):145 168, 2006. Y. Sun. Self shadowing and local illumination of randomly rough 
surfaces. Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer 
Society Conference on, 1. Y. Sun. Statistical ray method for deriving reflection models of rough surfaces. 
Journal of the Optical Society of America A, 24(3):724 744, 2007.  Effects that Require Keeping Track 
of more than just Radiance Polarization  Need to track state of polarization There are some optical 
effects that are important for small classes of materials. One is polarization. General references for 
this include: David C. Tannenbaum, Peter Tannenbaum, and Michael J. Wozny. Polarization and  birefringency 
considerations in rendering. In Proceedings of the 21st annual conference on Computer graphics and interactive 
techniques, pages 221 222. ACM Press, 1994; Alexander Wilkie, Robert F. Tobler, and Werner Purgathofer. 
Combined rendering of polarization and fluorescence effects. In Proceedings of the 12th Eurographics 
Workshop on Rendering, pages 197 204, 2001; Lawrence B. Wolff and David J. Kurlander. Ray tracing with 
polarization parameters. IEEE Comput. Graph. Appl., 10(6):44 55, 1990.  Other Effects Polarization 
scalar values for radiance and BRDF replaced by matrices Wolff and Kurlander IEEE CGA 1990 (complete 
Cook-Torrance model) Tannenbaum SIGGRAPH 1994: birefringent materials  Code available: SCATMECH  http://physics.nist.gov/Divisions/Div844 
/facilities/scatmech/html/  The index of refraction is a function of wavelength, so different wavelengths 
get refracted differently, causing the separation of colors we see.  Another classes of effects is 
interference and diffraction. General references for these phenomena that require modeling the wave nature 
of light include: Brian E. Smits and Gary W. Meyer. Newton s color: Simulating interference phenomena 
in realistic image synthesis. In Kadi Bouatouch and Christian Bouville, editors, Rendering Techniques 
90, Eurographics, pages 185 194. Imprimerie de l universit´e de Rennes, 1990. Proc. 1st Eurographics 
Rendering Workshop, Rennes, France, June 11 13, 1990; Yinlong Sun, F. David Fracchia, ThomasW. Calvert, 
and Mark S. Drew. Deriving spectrum from colors and rendering light interference. IEEE Comput. Graph. 
Appl., 19(4):61 67, 1999.; Jos Stam. Diffraction shaders. In Proceedings of the 26th annual conference 
on Computer graphics and interactive techniques, pages 101 110. ACM Press/Addison-Wesley Publishing Co., 
1999; Yinlong Sun, F. David Fracchia, Mark S. Drew, and Thomas W. Calvert. Rendering iridescent colors 
of optical disks. In Proceedings of the Eurographics Workshop on Rendering Techniques 2000, pages 341 
352, London, UK, 2000. Springer-Verlag.  A different cause of vibrant color is when light reflects 
and transmits through very thin films. When a layer of transmitting material has a thickness on the order 
of the wavelength of light, wave phenomena have to be accounted for. In particular, light waves can 
reiniforce one another or cancel each other out. Whether light waves will cancel or reinforce after 
traveling some distance through a thin film depends on the wavelength. For a given path through the film, 
some wavelengths will be reinforced and some cancelled, resulting in intense colors appearing. Yinlong 
Sun, F. David Fracchia, Thomas W. Calvert, and Mark S. Drew, "Deriving Spectra from Colors and Rendering 
Light Interference," IEEE Computer Graphics and Application, Vol. 19, No. 4, Jul. 1999, pp. 61-67.  
 This image is from : Sun, Y. 2006. Rendering biological iridescences with RGB­based renderers. ACM Trans. 
Graph. 25, 1 (Jan. 2006), 100-129. In this article a simplified model for accurating predicting these 
reflected colors is presented. As mentioned earlier, when roughness is very small, wavelike phenomena 
need to be accounted for in computing reflectance. When there is regular spacing in the small features, 
there can also be the interference effect similar to the thin film effect. The simulated CD image shown 
here is from: Yinlong Sun, F. David Fracchia, Mark S. Drew, and Thomas W. Calvert, "Rendering Iridescent 
Colors of Optical Disks," the 11th EUROGRAPHICS Workshop on Rendering (EGRW), Brno, Czech Republic, June 
2000, pp. 341­  Andrew Glassner. A model of phosphorescence and fluorescence. In 5th Eurographics Rendering 
Workshop, pages 57 68, 1994.; Alexander Wilkie, Robert F. Tobler, and Werner Purgathofer. Combined rendering 
of polarization and fluorescence effects. In Proceedings of the 12th Eurographics Workshop on Rendering, 
pages 197 204, 2001. The difference in these images is the volumetric properties of the atmosphere. 
On the right particles that absorb and scatter light obscure anything in the distance.  For volumes 
the effect of material along a path needs to be considered, rather than a reflectance that encodes what 
happens when a ray hits a particular point on a surface.  As a ray travels through a volume, the amount 
of light may decrease by absorption or by light being scattered out of the ray path. The amount of light 
may increase by light being scattered into the path, or   For more detail on input data for participating 
media, see the SIGGRAPH 95 course notes Input for Participating Media , that are appended at the end 
of this tutorial. Also see the recent paper: D. Gutierrez, F. Seron, O. Anson, A. Muñoz. Visualizing 
underwater ocean optics. Volume effects are also visible in materials where the particles are tightly 
packed into a solid. The result is subsurface scattering . The only difference between the images on 
the left and right is the addition of subsurface scattering on the right. The top images are lit from 
the front, the bottom images from the back.In solids the scattering can essentially be considered isotropic 
in all cases.   Subsurface scattering is characterized by the BSSDF that accounts for light entering 
a material and emerging at a different angle and noticeably different distance from where it entered. 
This was original defined in F. E. Nicodemus, J. C. For densely packed solids, model as diffusion process 
and use dipole approximation. Jensen, H. W., Marschner, S. R., Levoy, M., and Hanrahan, P.. A practical 
model for subsurface light transport. SIGGRAPH '01..  The same subsurface scattering parameters will 
result in different appearance depending on the thickness of the material. For the same density of material, 
the ratio of scattering to absorption in the material gives a different appearance. The lighting in these 
three images is the same. Reference for computing subsurface scattering: Jensen, H. W., Marschner, S. 
R., Levoy, M., and Hanrahan, P. 2001. A practical model for subsurface light transport. In Proceedings 
of the 28th Annual Conference on Computer Graphics and interactive Techniques SIGGRAPH '01. ACM Press, 
New York, NY, 511-518. There have been **many** papers that build on this technique to enable rendering 
of subsurface scattering in real time.   A basic mechanism for representing spatial variations that 
characterize a material is to use images mapped to the surface. Each pixel in the map may store simply 
a diffuse color, or a BRDF, a normal, a displacement, or a BTF (a virtual BRDF that includes the effect 
of small scale geometry). The mapping is done by storing coordinate of a location in an image with each 
vertex used to define a geometric model. Mapping to a geometry requires that the geometry be parameterized 
(i.e. a two dimensional coordinate system must be defined on the surface), a topic which is studied extensively 
in computer aided geometric design. Parameterization is one of the topics considered in the course in 
SIGGRAPH 2005 14. Discrete Differential Geometry: Grinspun Desbrun, Schröder ( in ACM Digital Library) 
 These two renderings were made with Radiance with procedural textures rather than image maps to define 
spatial variations on the surface (e.g. see D. Ebert, Ed. Texturing and Modeling: A Procedural Approach, 
Third Edition. Morgan Kaufmann, San Francisco, CA, 2002.) The same spatial frequency has different visual 
impact depending on whether the fraction of light is modulated, or the direction of the surface normals. 
 An alternative to analytic models of reflectance, is to create the small scale microstructure, and simulate 
its scattering effects by shooting rays at it and saving the results in a data structure designed specifically 
for BRDF. Two examples of this are: Gondek, Meyer, and Newman, Wavelength dependent reflectance functions 
In Proceedings of the 21st annual conference on Computer graphics and interactive techniques (1994), 
ACM Press, pp. 213 220. And Westin, Arvo and Torrance, Predicting reflectance functions from complex 
Surfaces In Proceedings of the 19th annual conference on Computer graphics and interactive techniques 
(1992), ACM Press, pp. 255 264. An advantage of simulation is that it can be used to explore the effects 
of subsurface structure, and effects of interference in thin surface layers. Gondek et al used a simulation 
of thin titanium dioxide mica flakes to simulate an irridescent paint such as   Just a reflectance 
model combined with a spatially varying texture isn t adequate to model all materials. In cases where 
this approach is adequate, it can be difficult to find the right parameters to use. A wide range of models 
have been developed for specialized materials. Some common themes in these models that have evolved 
are developing small scale geometric models, defining layers of materials and using measured or captured 
data. Many materials are composed of bundles of long thin fibers. The appearance of the bulk material 
is modeled by first account for reflection and transmission from individual strands. Hair, textiles and 
finished wood are all examples of materials modeled based on the light interactions of individual fibers. 
  Either naturally or by design many materials have the appearance of sparkles small flecks of material 
that have a high specular reflectance. A challenge is to model where the sparkles appear in a way that 
is consistent frame to frame in animated sequences. Materials in which sparkles appear include automotive 
paint, man-made carpet fibers and snow. Many materials have sense of depth because they are composed 
of multiple layers of material that transmit and reflect light. This effect occurs in materials as diverse 
as paints and the human eye.  With inexpensive digital cameras 3. SPECIALIZED MATERIAL MODELS now widely 
available, many Common themes Natural Materials material models are built around Manufactured/Processed 
Materials data that can readily acquired. See SIGGRAPH 2009 Courses: Acquisition of Optically Complex 
Objects and Phenomena Captured Data Tuesday, 4 August | 8:30 AM - 10:15 AM | Auditorium A and Build 
Your Own 3D Scanner: 3D Photography for Beginners Wednesday, 5 August | 8:30 AM - 12:15 PM | Room 260-262 
We review specialized models that 3. SPECIALIZED MATERIAL MODELS have been developed by organizing Common 
themes Natural Materials them into natural and manufactured Manufactured/Processed Materials materials. 
People/Animals Hair and Fur Skin Eyes -Birda and other Non-mammals Plants Leaves Wood Porous and Wet 
Rocks  3. SPECIALIZED MATERIAL MODELS Common themes Natural Materials Manufactured/Processed Materials 
Hair and Fur: Same stuff, hair on people, fur on other animals. People: vellus and terminal hairs. 
 An individual hair consists of a core medulla, the cortex and exterior cuticle. The cuticle consists 
of lapped cells on the outside of the hair. Coloration: granules of the melanin pigment, either eumelanins 
or pheomelanins . Hair with no pigment granules appears white. There are two different types of hair 
 vellus hairs and terminal hairs. Terminal hairs are those typically found on the scalp. Vellus hairs 
are unpigmented narrow (4 micron diameter), short (1mm) hairs that grow nearly all over the body. Some 
stats: Scalp hairs on humans are 50 micron to 90 micron diameter, may be circular or ellipsoidal, with 
curlier hair more ellipsoidal in cross section. Beard and moustache (rather than scalp) hair may have 
triangular cross section. Eyelashes, are 20 to 120 micron in diameter. A person has about 175 to 300 
terminal hairs per cm2, for a total of on the order of 100,000 hairs on the typical human scalp. Foxes 
and rabbits: average diameter 20 to 30 micron, and approximately 4000 hairs per cm2 Goats and badgers 
:average diameter 70 to 80 micron and approximately 100 to 200 hairs per cm2). General References on 
types of hair and fur: James Robertson. Forensic Examination of Human Hair. CRC Press, 1999. A. A. Blazej, 
J. Galatik, Z. Galatik, Z. Krul, and M. Mladek. Atlas of Microscopic Structures of Fur Skins 1. Elsevier, 
New York, 1989. Database of fur: http://www.furskin.cz/ 3. SPECIALIZED MATERIAL MODELS Common themes 
Natural Materials Manufactured/Processed Materials Where the common themes come in: Terminal Hair: Fibers 
Vellus Hair: Layer  A volumetric approach for rendering fur was presented in: J. T. Kajiya and T. L. 
Kay. Rendering fur with three dimensional textures. In Proceedings of the 16th annual conference on Computer 
graphics and interactive techniques, pages 271 280. ACM Press, 1989. This is different from pure volume 
rendering because the structure of individual strands must still be visible.  The projected density 
accounts for the number and size of hairs, the frame bundle accounts for their orientation.   Example 
views from the original Kajiya and Kay paper.  An example of modeling with strands (rather than texels): 
Aslan in Narnia Brad Hiebert, Jubin Dave, Tae-Yong Kim, Ivan Neulander, Hans Rijpkema, and Will Telford. 
The chronicles of Narnia: the lion,  the crowds and rhythm and hues. In SIGGRAPH 06: ACMSIGGRAPH 2006 
Courses, page 1, New York, NY, USA, 2006. ACM Press. A more detailed reflectance mode for individual 
strands was presented in Stephen R. Marschner , Henrik Wann Jensen , Mike Cammarano , Steve Worley , 
Pat Hanrahan, Light scattering from human hair fibers, ACM Transactions on Graphics (TOG), v.22 n.3, 
July 2003 Hair growth results in oriented scales on the hair. The orientation of the scales relative 
to the centerline of the hair changes the reflectance distribution relative to what would be expected 
from a smooth cylinder.    The different possible paths through the strand result in different reflectance 
lobes. A comparison shows that the more detailed model predicts effects like secondary highlights Effects 
of shadows and interreflections are studied in Jonathan T. Moon and Stephen R. Marschner. Simulating 
multiple scattering in hair using a photon mapping approach. In SIGGRAPH 06: ACM SIGGRAPH 2006 Papers, 
pages 1067 1074, New York, NY, Tom Lokovic and Eric Veach. Deep shadow maps. In SIGGRAPH 00: Proceedings 
of the 27th annual conference on Computer graphics and interactive techniques, pages 385 392, New York, 
NY, USA, 2000. ACM Press/Addison-Wesley Publishing Co.  Multiple scattering is particularly critical 
for blond hair. 3. SPECIALIZED MATERIAL MODELS Common themes Natural Materials Manufactured/Processed 
Materials  Vellus Hairs Machine Vision and Applications (2003) 14: 260 268 The secret of velvety skin 
Jan Koenderink, Sylvia Pont Machine Vision andApplications, 14(4):260 268, 2003. Machine Vision and Applications 
(2003) 14: 260 268 The secret of velvety skin Jan Koenderink, Sylvia Pont Asperity scattering: Thin layer 
(. small) of thin (few particles per unit volume) participating medium formed by tiny hairs or dust. 
 The scattering from short vellus hairs on the rest of the body is similar to the scattering of fuzz 
on a peach. A simple function for simulating this effect, which softens the look of a surface is described 
in J. Koenderink and S. Pont. The secret of velvety skin.  The fuzz on a peach is another example of 
asperity scattering. Rather than seeing individual small fibers, we see the effect of the hazy layer, 
and white edges on the silhouette of the peach when it is backlit. Asperity scattering can be modeled 
with a simple expression, with limits on the values of the angles so that the function doesn t go to 
infinity.  Human skin: -- varies in thickness varying from 0.1 to more than 0.5 cm. -- has three layers 
: The epidermis is the thin outside layer, that includes the exterior layer of dead cells (the stratum 
cornuem). The dermis is thicker, and includes the vessels that carry blood. The hypodermis connects 
the skin to the rest of the body. A general reference: Kenneth A. Walters. Dermatological and Transdermal 
Formulations. Marcel Dekker Incorporated, 2002. 3. SPECIALIZED MATERIAL MODELS Common themes Natural 
Materials Manufactured/Processed Materials Skin Where the common themes come in: --Detailed geometry 
modeling --Layers --Captured data   The need to account for subsurface scattering in skin models was 
first noted in: Pat Hanrahan and Wolfgang Krueger. Reflection from layered surfaces due to subsurface 
scattering. In Proceedings of the 20th annual conference on Computer graphics and interactive techniques, 
pages 165 174. ACM Press, 1993. And more recent methods have estimated a subsurface scattering model 
from measurements: Tim Weyrich, Wojciech Matusik, Hanspeter Pfister, Bernd Bickel, Craig Donner, Chien 
Tu, Janet McAndless, Jinho Lee, Addy Ngan, HenrikWann Jensen, andMarkus Gross. Analysis of human faces 
using a measurement-based skin reflectance model. In SIGGRAPH 06: ACM SIGGRAPH 2006 Papers, pages 1013 
1024, New York, NY, USA, 2006. ACM Press. Diagram of scattering in subsurface layers and resulting rendering, 
from scattering.  Surface reflectance models  Skin Model Components --subsurface scattering model 
--surface reflectance model (such as Cook-Torrance) --a model of small scale spatial variations in the 
spectral reflectance ---geometry variations of the skin surface. have been fit to captured data including 
fitting the Lafortune model used in: Stephen R. Marschner, Brian K. Guenter, and Sashi Raghupathy. Modeling 
and rendering for realistic facial animation. In Proceedings of the Eurographics Workshop on Rendering 
Techniques 2000, pages 231 242, London, UK, 2000. Springer-Verlag. And Cook-Torrance model used in: Athinodoros 
S. Georghiades. Recovering 3-d shape and reflectance from a small number of photographs. In EGRW 03: 
Proceedings of the 14th Eurographics workshop on Rendering, pages 230 240, Aire-la-Ville, Switzerland, 
Switzerland, 2003. Eurographics Association.  Domes: Measure skin BRDF and facial shape simultaneously 
"Recovering 3-D Shape and Reflectance From a Small Number of Photographs," Eurographics Symposium on 
Rendering, June 2003, pp. 230-240, 315.  Skin Model Components --subsurface scattering model --surface 
reflectance model (such as Cook-Torrance) --a model of small scale spatial variations in the spectral 
reflectance ---geometry variations of the skin surface. Spatial coloring variations are due to freckles 
and age spots, and temporary effects such as blushing. A full first principles model of skin including 
prediction of color due to detailed composition including blood flow is the BioSpec Model: A. Krishnaswamy 
and G.V.G. Baranoski. A Biophysically-Based Spectral Model of Light Interaction with Human Skin. Computer 
Graphics Forum, 23(3):331 340, 2004.   Plots from Krishnaswamy and Baranoski, 2004 for the spectral 
data for various skin model components. Images from Krishnaswam and Baranoski, 2004  Details of pores 
from molding compound used on skin: Stephen R. Marschner, Brian K. Guenter, and Sashi Raghupathy. Modeling 
and rendering for realistic facial animation. In Proceedings of the Eurographics Workshop on Rendering 
Techniques 2000, pages 231 242, London, UK, 2000. Springer-Verlag. Generic wrinkle patterns are modeled 
in: L. Boissieux, G. Kiss, N. Magnenat-Thalmann, and P. Kalra. Simulation of skinaging and wrinkles 
with cosmetics insight. Computer Animation and Simulation 2000, pages 15 27, 2000. Applying wrinkles 
from scanned data: lovinskiy et al., 2006] Aleksey Golovinskiy, Wojciech Matusik, Hanspeter Pfister, 
Szymon Rusinkiewicz, and Thomas Funkhouser. A statistical model for synthesis of detailed facial geometry. 
In SIGGRAPH 06: ACM SIGGRAPH 2006 Papers, pages 1025 1034, New York, NY, USA, 2006. ACM Press.  Using 
a molding compound to capture skin geometry Real-time, Photo-realistic, Physically Based Rendering of 
Fine Scale Human Skin Structure A. Haro, B. Guenter, and I. Essa, Proceedings 12th Eurographics Workshop 
on Rendering, London, England, June 2001  Using multiple dipoles In thin layers: Light Diffusion in 
Multi-Layered Translucent Materials - Craig Donner Henrik Wann Jensen Proceedings of ACM SIGGRAPH 2005, 
An example of combining multiple layers to form a model for skin. Source of excellent discussion of 
detailed modeling and parameter tuning for skin for real time rendering: http://developer.download.nvidia.com/presentations/2007/gdc/Advanced_Skin.pdf 
Comprehensive surveys:: "The Appearance of Human Skin: A Survey," T. Igarashi, K. Nishino, and S. K. 
Nayar, Foundations and Trends in Computer Graphics and Vision, Vol.3, No.1, pp.1-95, 2007. Baranoski, 
G. V. and Krishnaswamy, A. 2008. Light interaction with human skin: from believable images to predictable 
models. In ACM SIGGRAPH ASIA 2008 Courses (Singapore, December 10 ­13, 2008). SIGGRAPH Asia '08. (in 
ACM Digital Library) Relevant SIGGRAPH 2009 Course: The Digital Emily Project: Photoreal Facial Modeling 
and Animation Thursday, 6 August | 1:45 PM -3:30 PM | Auditorium C 3. SPECIALIZED MATERIAL MODELS Common 
themes Natural Materials Manufactured/Processed Materials  Eyes Common theme: Layers Eyes have a complex 
appearance due to a complex layered structure. An approximation of this structure based on the manufacture 
of artificial eyes is given by: Aaron Lefohn, Brian Budge, Peter Shirley, Richard Caruso, and Erik Reinhard. 
An ocularist s approach to human iris synthesis. IEEE Comput Graphics Appl, 23(6):70 75, November/ December 
2003. A detailed biological model is described in: Michael W.Y. Lam and Gladimir V.G. Baranoski. A predictive 
light transport model for the human iris. Computer Graphics Forum, 25(3):359 368, 2006.   Simple geometric 
model of eye. Textures for various layers that would be combined in artificial eye that together give 
eyes depth.   Lam and Baranoski present a biologically detailed model of the eye.  The biologically 
accurate model can predict eye appearance.   Detailed geometry is important in the appearance of all 
animals. The details of feathers need to be modeled to render birds.A feather has a main shaft, or rachis. 
At the bottom of the shaft, the calamus, there is nothing attached. The vanes of the feather are formed 
by barbs that are attached to the rachis. Barbs are interlocked with one another by small barbules that 
branch off them. The vanes may be fuzzy, or plumulaceous, such as the lower section shown here or stiff, 
or pennaceous, as in the upper section. A variety of methods have been proposed in graphics to model 
this structure: Wen-Kai Dai, Zen-Chung Shih, and Ruei-Chuan Chang. Synthesizing feather textures in galliformes. 
Computer Graphics Forum, 14(3):407 420, August 1995. Yanyun Chen, Yingqing Xu, Baining Guo, and Heung-Yeung 
Shum. Modeling and rendering of realistic feathers. In Proceedings of the 29th annual conference on Computer 
graphics and interactive techniques, pages 630 636. L. Streit and W. Heidrich. A biologically-parameterized 
feather model. Computer Graphics Forum, 21(3):565 565, 2002. . 3. SPECIALIZED MATERIAL MODELS Common 
themes Natural Materials Manufactured/Processed Materials Birds and other Non-Mammals Common themes --Detailed 
geometry modeling  3. SPECIALIZED MATERIAL MODELS  Birds Structural Color: Applicable to feathers: 
Yinlong Sun. Rendering biological iridescences with rgb­based renderers. ACMTrans. Graph., 25(1):100 
129, 2006. Other parts of birds: R.O. Prum and R. Torres. Structural colouration of avian skin convergent 
evolution of coherently scattering dermal collagen arrays. Journal of Experimental Biology, 206(14):2409 
2429, 2003. 3. SPECIALIZED MATERIAL MODELS Other Non-Mammals Simple Voronoi texture generated in Cinema4D 
For other animals, most methods attempt to mimic patterns of color and small scale geometry. In particular 
cellular textures based on: Steven Worley. A cellular texture basis function. In SIGGRAPH 96: Proceedings 
of the 23rd annual conference on Computer graphics and interactive techniques, pages 291 294, A method 
of generating of anisotropic cellular textures with examples on reptiles: Itoh, Kazunori Miyata, and 
Kenji Shimada. Generating organic textures with controlled anisotropy and directionality. IEEE Comput. 
Graph. Appl., 23(3):38 45, 2003.  Physically detailed models of leaves are given in: G. V. G. Baranoski 
and J. G. Rokne. An algorithmic reflectance and transmittance model for plant tissue. Computer Graphics 
Forum, 16(3):141 150, August 1997. ISSN 1067-7055. of light by leaves. The Visual Computer, 17(8):491 
505, 2001. Lifeng Wang, Wenle Wang, Julie Dorsey, Xu Yang, Baining Guo, and Heung-Yeung Shum. Real-time 
rendering of plant leaves. In SIGGRAPH 05: ACM SIGGRAPH 2005 Papers, pages 712 719, New York, NY, USA, 
2005. ACM Press. The detailed layers of plant tissue can be modeled as thin layers of scattering participating 
media The observations can be encoded in terms of models for reflectance and transmittance.     
    Sample output from appearance modeling  Another feature of plants is that they may be covered 
by systems of small hairs. Fuhrer et al. present a method for placing and rendering such hairs.  While 
it is a single material, wood can have a wide variety of appearance, as noted in these photographs of 
natural and finished wood. Lefebre and Poulin presented a general procedural model for fitting the structure 
of color variation in wood.  Results from Lefebvre and Poulin s work, synthesizing a new piece of wood 
using parameters estimated for the model from a real piece of wood. Light incident on porous materials 
undergoes interreflections within pores causing darkening:S.Merillou, J.- M. Dischler, and D. Ghazanfarpour. 
A BRDF postprocess to integrate porosity on rendered surface. IEEE Transactions on Visualization and 
Computer Graphics, 6(4):306 318, October 2000.  Pores may not be visible, but can be seen in a microscopic 
view  This figure from Merillou et al. shows the multiple reflections/absorptions that result in darkening. 
  From Merillou et al., comparisons to physical images. 3. SPECIALIZED MATERIAL MODELS Common themes 
Natural Materials Manufactured/Processed Materials Wet materials Common Theme:layers captured data 
 A top smooth layer results in specular reflection, the index of refraction of water results in total 
internal reflection, and so more absorption and material darkening. More scattering within the material 
causes more saturated color.   Forward scattering and index of refraction of water result in coherent 
transmission through a thin material.  Observations of geometric variation of wet materials.  Increased 
saturation of material captured with digital photography.   Evaporation and flow in the material contribute 
to drying. This means ambient occlusion (for evaporation) and distance to edge (flow in material) are 
significant parameters for estimating the drying pattern. A wetness map is estimated for an object photographed 
at different times as it dries. Experimental Procedure: Data Analysis  Functions can be fit to the 
drying time as a function of distance to edge and the ambient occlusion (or the accessibility)  The 
parameters for these functions are stored as look up maps.  Examples of using the model compared to 
ground truth.  Applying the model to a synthetic example.   Nishita et al. presented a model of snow 
as a participating medium with embedded particles for sparkle 3. SPECIALIZED MATERIAL MODELS Common 
themes Natural Materials Manufactured/Processed Materials  Water Natural waters are colored by scattering 
from suspended particles  We often see particles or other material suspended in water --especially in 
natural settings such as ponds, rivers, lakes and oceans. These effects are discussed in more depth in 
the SIGGRAPH 09 course: Scattering, Thursday, 6 August | 1:45 PM - 5:30 PM | Room 265­266 Approaches: 
First Principles Modeling of Scattering Measurement of Scattering Properties Dilute milk Fat globules 
in suspension  A detailed description of unique underwater scattering effects, including fluorescence 
and Raman scattering is given in: D. Gutierrez, F. Seron, O. Anson, and A. Munoz, Visualizing underwater 
ocean optics, Computer Graphics Forum (EUROGRAPHICS 2008), 27, pp. 547--556. Measurement of suspended 
particles is described in: Srinivasa G. Narasimhan , Mohit Gupta , Craig Donner , Ravi Ramamoorthi , 
Shree K. Nayar , Henrik Wann Jensen, Acquiring scattering properties of participating media by dilution, 
ACM Transactions on Graphics (TOG), v.25 n.3, July 2006  Modelling rocks requires modeling macroscopic 
3D structure.  2D textures are inconvenient for modeling truly 3D materials such as rock/stone.  Solid 
textures allow the separate modeling of material geometry and object geometry. Volume 26 (2007), number 
1 pp. 66 79 COMPUTER GRAPHICS forum Modeling and Rendering of Heterogeneous Granular Materials: Granite 
Application Romain Souli´e, St´ephane M´erillou, Olivier Romain, Terraz Djamchid and Romain Ghazanfarpour 
Modeling process of heterogeneous granular materials using 3DVorono¨ diagrams to obtain a full representation 
of their structure. Rendering process based on photon mapping to simulate subsurface scattering inside 
these materials, presented in  Figures from Soulie et al. modeling granite: the geometry is modelled 
using a 3D Voronoi diagram, and subsurface scattering is modeled for the individual granite components 
of mica, feldspar and quarts.  Figure from Soulie et al., modeling the material 3D structure allows 
modeling of geometry that is consistent with the material.   Texture synthesis such as Heeger and Bergen 
s spectral analysis fail for materials made of discrete particles embedded in a matrix. Stereological 
Techniques for Solid Textures Jagnow, Dorsey, Rushmeier,SIGGRAPH 2004 Stereology is used in the construction 
industry to estimate rock distributions.   Stereology is also used in biology to estimate 3D distributions 
fom 2D slices The distribution of 2D diameters in a slice is related to , but not the same as the 3D 
distribution.  Recent research Volumetric materials Recovering Sphere Distributions How many profiles 
of the smallest size? NA (1) K11 NV (1) K12 NV (2) K13 NV (3) K14NV (4) Kij = Probability that particle 
NV(j) exhibits profile NA(i) Recovering Sphere Distributions Putting it all together  For particles 
that aren t spheres, the relation between 2D and 3D needs to be calculated Recent research Volumetric 
materials   Other Particle Types We cannot classify arbitrary particles by d/dmax Instead, we choose 
to use A / Amax Algorithm inputs: Approach: Collect statistics for 2D profiles and 3D particle Recent 
research Volumetric materials Profile Statistics Segment input image to obtain profile densities NA. 
 Bin profiles according to their area, A/ Amax Recent research Volumetric materials Recovering Particle 
Distributions Just like before, NA = HKNV 1 -1 K NA Solving for the particle densities, NV = H Use 
NV to populate a synthetic volume. Color using mean particle colors from the input image Recent research 
Volumetric materials  Recovering Noise How can we replicate the noisy appearance of the input? Some 
noise is needed, or results look too pristine The noise residual is less structured and responds well 
to Heeger &#38; Bergen s method     Jagnow et al., ACM Transactions on Applied Perception, 2008 There 
is no single way to estimate particle shape from a single slice. A psychophysical test is needed to find 
which estimation technique is best. Recent research Volumetric materials Applied Perception: Do Psychophysical 
Experiment Which is the slice through the same material shown in the center?  An example of one screen 
view from the psychophysical test. Results of test across different particle shapes. Related issues: 
General 3D Texture Synthesis Perception of Materials This study of touches on two areas synthesizing 
3D materials and perceptual experiments . Subsequent work on 3D texture synthesis can be found in: Solid 
texture synthesis from 2d exemplars, Kopf, J. and Fu, C. and Cohen-Or, D. and Deussen, O. and Lischinski, 
D. and Wong, T., ACM transactions on graphics 26(3), 2007. and Lazy Solid Texture Synthesis, Dong, Y. 
and Lefebvre, S. and Tong, X. and Drettakis, G., Computer Graphics Forum, 27(4), pp. 1165 1174, 2008. 
 Studies on the perception of materials: Ganesh Ramanarayanan , James Ferwerda , Bruce Walter Kavita 
Bala, Visual equivalence: towards a new standard for image fidelity, ACM Transactions on Graphics (TOG), 
v.26 n.3, July 2007 Peter Vangorp , Jurgen Laurijssen , Philip Dutré, The influence of shape on the 
perception of material reflectance, ACM Transactions on Graphics (TOG), v.26 n.3, July 2007 Ganesh Ramanarayanan 
, Kavita Bala , James A. Ferwerda, Perception of complex aggregates, ACM Transactions on Graphics (TOG), 
v.27 n.3, August 2008  3. SPECIALIZED MATERIAL MODELS Common themes Natural Materials Manufactured/Processed 
Materials Finished Wood Textiles Fibers Threads Knitted Woven Automotive Paint Artistic Paint Gems 
 Finished wood can have a lustrous appearance that results from the internal orientation of the wood 
fibers, combined with the reflection and transmission from the smoothed finished top surface. Marschner 
et al. modeled this reflectance as a Gaussian function g that depends on parameters Psi that depend 
on the orientation of the wood fibers and the surface normal. Stephen R. Marschner, Stephen H. Westin, 
Adam Arbree, and Jonathan T. Moon. Measuring and modeling the appearance of finished wood. ACM Trans. 
Graph., 24(3):727 734, 2005.   Figure from Marschner et al 2005, similar to wet surfaces, smooth surface 
from finish layer produces specular refection. Similar to hair analysis, orientation of wood fibers influences 
orientation of reflection.  From Marschner et al.2005, full model for finished wood. 3. SPECIALIZED 
MATERIAL MODELS Common themes Natural Materials Manufactured/Processed Materials Textiles: Individual 
threads Knitted Materials Woven Materials Common theme: Detailed geometry  3. SPECIALIZED MATERIAL 
MODELS Common themes Natural Materials Manufactured/Processed Materials Textiles: Individual threads 
Knitted Materials Woven Materials 36:6388 6392, 1997. The structure of individual threads may be designed 
to give particular optical effects, such as looking a different color from different view angles. B. 
Rubin, H. Kobsa, and S. M. Shearer. Prediction and verification of an iridescent synthetic fiber. Appl. 
Opt., 3. SPECIALIZED MATERIAL MODELS Common themes Natural Materials Manufactured/Processed Materials 
Textiles: Individual threads Knitted Materials  Woven Materials Specialized volumetric structures have 
been proposed to model the fuzzy nature of knitwear: Eduard Groeller, Rene T. Rau, and Wolfgang Strasser. 
Modeling and visualization of knitwear. IEEE Transactions on Visualization and Computer Graphics, 1(4):302 
310, 1995. Yarn is loose, fluffy grouping of fibers: Ying-Qing Xu, Yanyun Chen, Stephen Lin, Hua Zhong, 
Enhua Wu, Baining Guo, and Heung-Yeung Shum. Photorealistic rendering of knitwear using the lumislice. 
In Proceedings of the 28th annual conference on Computer graphics and interactive techniques, pages 391 
398. ACM Press, 2001. Xu et al. model yarn as volume of fibers formed by twisting a scatter diagram in 
2D along a 3D axis.  From Xu et al. 2001, a full knitted fabric is formed from the volumetric model 
applied along a pattern of knitted stiches.  The reflectance of individual threads, effect of light 
going through threads and weaving patterns need to be accounted for in realistically rendering woven 
materials Neeharika Adabala, Guangzheng Fei, and Nadia Magnenat-Thalmann. Visualization of woven cloth. 
Proceedings of the 14th Eurographics workshop on Rendering, Leuven, Belgium, 2003. pages 178 185.  
Figure from Adabla et al. showing complete process for modeling woven cloth. Neeharika Adabala, Guangzheng 
Fei, and Nadia Magnenat-Thalmann. Visualization of woven cloth. Proceedings of the 14th Eurographics 
workshop on Rendering, Leuven, Belgium, 2003. pages 178 185. Figure showing side by side images of photographs 
of real cloth and the simulations performed by Adabala et al.  Slide 14 3. SPECIALIZED MATERIAL MODELS 
Common themes Natural Materials Manufactured/Processed Materials  Automotive Paint A first principles 
model for automotive paint, including the depth effect given by multiple layers and sparkles caused by 
reflections off small particles is given in: Sergey Ershov, Konstantin Kolchin, and Karol Myszkowski. 
Rendering pearlescent appearance based on paint-composition modelling. Computer Graphics Forum, 20(3), 
2001. Sergey Ershov, Roman Durikovic, Konstantin Kolchin, and Karol Myszkowski. Reverse engineering approach 
to appearance-based design of metallic and pearlescent paints. Vis. Comput., 20(8-9):586 600, 2004. Roman 
Durikovic and William L. Martens. Simulation of sparkling and depth effect in paints. In SCCG 03: Proceedings 
of the 19th spring conference on Computer graphics, pages 193 198, New York, NY, USA, 2003. ACM Press 
The change of color with angle for metallic car paints is modeled in a system for automotive finish design 
in: Gary Meyer, Clement Shimizu, Alan Eggly, David Fischer, Jim King, and Allan Rodriguez.Computer aided 
design of automotive finishes. In Proceedings of 10th Congress of the International Colour Association, 
pages 685 688, 2005.  ¨ -Johannes G¨unther, Tongbo Chen, Michael Goesele, Ingo Wald, and Hans-Peter 
Seidel. Efficient acquisition and realistic rendering of car paint. In G¨unther Greiner, Joachim Hornegger, 
Heinrich Niemann, and Marc Stamminger, editors, Proceedings of 10th International Fall Workshop -Vision, 
Modeling, and Visualization (VMV) 2005, pages 487 494. Using captured data to fit a multilobed Cook Torrance 
Model An interface for designing the change in paint color with view. Figure from Guenther et all using 
a paint model based on data capture.   Figure from Ershov showing the detailed paint structure used 
in a first principles model. Figures from Ershov et al. 2001 showing results  Artistic paints, particularly 
water colors, are modeled as particulates carried by a fluid modeled with either a 3. SPECIALIZED MATERIAL 
MODELS shallow fluid model: Common themes Natural Materials Cassidy J. Curtis, Sean E. Manufactured/Processed 
Materials Anderson, Joshua E. Seims, KurtW. Fleischer,  Artistic Paints  and David H. Salesin. Computer-generated 
watercolor. In Proceedings of the 24th annual conference on Computer graphics and interactive techniques, 
pages 421 430. ACM Press/Addison- Wesley Publishing Co., 1997. Or a Lattice-Boltzmann model: Nelson 
S.-H. Chu and Chiew-Lan Tai. Moxi: real-time ink dispersion in absorbent paper. ACM Trans. Graph., 24(3):504 
511, 2005. 3. SPECIALIZED MATERIAL MODELS Common themes Natural Materials Manufactured/Processed Materials 
  GEMS Some geometric detailed modeling, Make use of advanced optics effects  Rendering by Yinlong 
Sun showing colors in a diamond produced by accounting for dispersion. Rendering gems with aterism or 
chatoyancy Shigeki Yokoi, Kosuke Kurashige and Jun-ichiro Toriwaki The Visual Computer, Volume 2, Number 
5 / September, 1986 Bright patterns in gems like cat s eyes and sapphires These are caused by ellipsoidal 
inclusions of material in the gem, and are accounted for using a volume rendering method.   Figure 
from from Guy and Soler. For many types of gems polarization effects must be taken into account.  Pseudochromatic 
Color of Crystals Dispersion  Asterism and Chatoyance  Aventurescence  Opalescence  Labradorescence. 
 Chatoyance modeled and rendered by Weidlich  Recently Andrea Weidlich has done a comprehensive study 
of the structural colors in gems and precious stones, that encompasses the phenomena just noted, as well 
as others. A detailed analysis is avialable in her dissertation: Pseudochromatic Colourisation of Crystals 
in Predictive Image Synthesis , TU Wien, 2009. http://www.cg.tuwien.ac.at/research/pub lications/2009/weidlich-2009-thesis/ 
 Pseudochromatic Color of Crystals Aventurescence Aventurescence: glittering crystalline metallic inclusions 
 WEIDLICH, A. and WILKIE, A. 2008: Modeling Aventurescent Gems with Procedural Textures. In: Proceedings 
of the Spring Conference on Computer Graphics (SCCG), pp. 1 8. ACM Modeled and rendered by Weidlich 
  Pseudochromatic Color of Crystals Opalescence diffraction phenomena  Labradorescence interference 
from multiple pairs of thin layers  IEEE TR VISUALIZ GRAPHIC DECEMBER 19 Modeling for a Pearl-Noriko 
Na Yoshitsugu Teruo Usami  See Pseudochromatic Colourisation of Crystals in Predictive Image Synthesis 
, TU Wien, 2009 for multiple images of each effect, both photographed and simulated. .. Andrea Weidlich, 
Alexander Wilkie. Rendering the Effect of Labradorescenc In Proceedings of Graphics Interface 2009, May 
2009.  Pearls are produced organically, unlike the other gems discussed. These figures are from Nagata 
et al. 1997, showing the results of modeling pearls using different layers to model diffuse reflectance, 
interference effects, specular reflection, and spatial variations. 4. AGING AND WEATHERING PROCESSES 
 Taxonomy Simulation Methods Capture Approaches Taxonomy Chemical Reactions Mechanical Processes  
The area of aging and weathering processes is critical in modeling natural variations in materials. Many 
very detailed models have been developed. A general view of the area is given by organizing the detailed 
models into three general categories. -Chemical Reactions like rusting or patination - Mechanical Processes 
like paint crackling and peeling -Biological Growth like algae, moss or mold growing Biological Growth 
 4. AGING AND WEATHERING PROCESSES Taxonomy Simulation Methods Capture Approaches Metallic patinas 
naturalartificial natural artificial  Metals often develop a characteristic patina over time. The patination 
process, which develops in a series of thin surface layers, is due to the chemical alteration of a surface 
and results in changes in color. Patination may be the result of deliberately applied craft processes 
or natural corrosion. The flow of water is one of the most pervasive and important natural forces involved 
in the weathering of materials producing a distinctive set of patterns of washes and stains. These photographs 
show the weathering of various buildings. Water may wash dirt from some areas and clean them; in other 
areas dirt and other materials are deposited, creating stains. The result is a visually rich set of patterns 
that are difficult to model with most texturing techniques. 4. AGING AND WEATHERING PROCESSES Taxonomy 
  Simulation Methods Capture Approaches   Stephane Merillou, Jean-Michel Dischler, and Djamchid Ghazanfarpour. 
Corrosion: Simulating and rendering. In GI 2001, pages 167 174, June 2001. , Julie Dorsey, Alan Edelman, 
Henrik Wann Jensen, Justin Legakis, and Hans Kohling Pedersen. Modeling and rendering of weathered stone. 
In Proceedings of the 26th annual conference on Computer graphics and interactive techniques, pages 225 
234. ACM Press/Addison-Wesley Publishing Co., 1999. Koichi Hirota, Yasuyuki Tanoue, and Toyohisa Kaneko. 
Generation of crack patterns with a physical model. The Visual Computer, 14(3):126 137, 1998. Koichi 
Hirota, Yasuyuki Tanoue, and Toyohisa Kaneko. Simulation of three-dimensional cracks. The Visual Computer, 
16(7):371 378, 2000. 4. AGING AND WEATHERING PROCESSES Taxonomy Simulation Methods Capture Approaches 
Oxides &#38; water Erosion 4. AGING AND WEATHERING PROCESSES Taxonomy Simulation Methods Cracking 4. 
AGING AND WEATHERING PROCESSES Taxonomy Simulation Methods Capture Approaches Peeling 4. AGING AND WEATHERING 
PROCESSES Taxonomy Simulation Methods Capture Approaches Scratching  S. Gobron and N. Chiba. Simulation 
of peeling using 3d­surface cellular automata. In 9th Pacific Graphics Conference on Computer Graphics 
and Applications, pages 338 347, Tokyo Japan, Oct 2001. IEEE.  Eric Paquette, Pierre Poulin, and George 
Drettakis. The simulation of paint cracking and peeling. In Graphics Interface 2002, pages 59 68, May 
2002. S. Merillou, J.M. Dischler, and D. Ghazanfarpour. Surface scratches: measuring, modeling and rendering. 
The Visual Computer, 17(1):30 45, 2001. C. Bosch, X. Pueyo, S. M´erillou, and D. Ghazanfarpour. A physically­based 
model for rendering realistic scratches. Computer Graphics Forum, 23(3):361 370, 2004. 4. AGING AND 
WEATHERING PROCESSES Taxonomy Simulation Methods Capture Approaches  George Drettakis Eric Paquette, 
Pierre Poulin. Surface aging by impacts. In Graphics Interface 2001, pages 175 182, June 2001.  Siu-Chi 
Hsu and Tien-Tsin Wong. Simulating dust accumulation. IEEE Comput Graphics Appl, 15(1):18 22, January 
1995. Brett Desbenoit, Eric Galin, and Samir Akkouche. Simulating and modeling lichen growth. Computer 
Graphics Forum, 23(3):341 350, 2004. 4. AGING AND WEATHERING PROCESSES 4. AGING AND WEATHERING PROCESSES 
Taxonomy Simulation Methods Capture Approaches  Lichen Growth  Summary table from Jianye Lu, Athinodoros 
S. Georghiades, Andreas Glaser, Hongzhi Wu, Li-YiWei, Baining Guo, Julie Dorsey, and Holly Rushmeier. 
Context-aware textures. ACM Trans. Graph., 26(1):3, 2007. Another recent survey is A survey of aging 
and weathering phenomena in computer graphic, S Merillou and D. Ghazanfarpour, Computers &#38; Graphics 
32 (2) pp 159 174, 2008. A general simulation method for many effects: Y. Chen, L. Xia, T.T. Wong, X. 
Tong, H. Bao and B. Guo et al., Visual simulation of weathering by gamma-ton tracing, ACM Transactions 
on Graphics 24 (3) (2005), pp. 1127 1133. 4. AGING AND WEATHERING PROCESSES Taxonomy Simulation Methods 
 Capture Approaches Capture in Context --appearance ---agent causing change --geometry Observing and 
Transferring Material Histories  First-principles simulations are time-consuming or impossible  New 
approach:  Capture time variations from real shapes, transfer them to generate synthetic objects  
Shapes can be rendered at different times in their histories  While simulating aging effects produces 
good results, first­principles simulations are time consuming or, in some cases, impossible because the 
underlying physics and chemistry are not completely understood. Rather than simulating aging effects, 
time variations from real shapes can be captured. This example shows effects captured from a copper bowl. 
We applied an artificial patination treatment to the bowl over a two week period and captured the shape 
and texture variations a frequent time intervals.  4. AGING AND WEATHERING PROCESSES Taxonomy Simulation 
Methods Capture Approaches  Time Varying Additional efforts have capture time series of BRDF data Jianye 
Lu, Athinodoros S. Georghiades, Andreas Glaser, Hongzhi Wu, Li-YiWei, Baining Guo, Julie Dorsey, and 
Holly Rushmeier. Context-aware textures. ACM Trans. Graph., 26(1):3, 2007. Additional time varying capture: 
 Jinwei Gu, Chien-I Tu, Ravi Ramamoorthi, Peter Belhumeur, Wojciech Matusik, and Shree Nayar. Time-varying 
surface appearance: acquisition, modeling and rendering. ACM Trans. Graph., 25(3):762 771, 2006. Sun, 
B.; Kalyan Sunkavalli; Ravi Ramamoorthi; Belhumeur, P.N.; Nayar, S.K., "Time-Varying BRDFs," Visualization 
and Computer Graphics, IEEE Transactions on , vol.13, no.3, pp.595-609, May-June 2007  4. AGING AND 
WEATHERING PROCESSES Taxonomy Simulation Methods  Capture Approaches From Single Example  In some cases 
weathering effects can be captured by taking data from a single image of a material that has been marked 
with fully aged and fully new regions. Jiaping Wang, Xin Tong, Stephen Lin, Minghao Pan, Chao Wang, Hujun 
Bao, Baining Guo, and Heung-Yeung Shum. Appearance manifolds for modeling time-variant appearance of 
materials. ACM Trans. Graph., 25(3):754 761, 2006. Su Xue, Jiaping Wang, Xin Tong, Qionghai Dai, Baining 
Guo Image-based Material Weathering. Computer Graphics Forum 27( 2):617-626, 2008. 5. CURRENT TRENDS 
AND NEEDS TRENDS Design of Physical Materials Editable Captured Data Simulated Microstructure Aging Inverse 
Aging Shape and Appearance NEEDS INTERFACES DATA!!!    Design of Physical Materials Paints Milled 
Facets The trends discussed here are really mini (micro?) trends some instances of new work that may 
grow into sub areas of material appearance research. The needs in materials continue to be large unsolved 
problems interfaces for the convenient specification of materials, and data for the full range of materials 
we see everyday. Digital modeling of material appearance isn t just for image generation. Computer simulation 
can be used to formulate paints and to design surface microstructure to attain desired effects. Paint 
design: Ershov, S., Durikovic, R., Kolchin, K., and Myszkowski, K. 2004. Reverse engineering approach 
to appearance-based design of metallic and pearlescent paints. Vis. Comput. 20, 8-9 (Nov. 2004), 586-600. 
 Gary Meyer Computer Graphic Tools for Automotive Paint Engineering in Book Title - Service Life Prediction 
of Polymeric Materials, pp 273-282, 2009. Milled Facets: Tim Weyrich ,Pieter Peers, Wojciech Matusik 
and Szymon Rusinkiewicz , Fabricating Microgeometry for Custom Surface Reflectance SIGGRAPH 2009 (ACM 
TOG).   Editable captured data SVBRDF s (Spatially Varying BRDF) Subsurface scattering (BSSRDF) Captured 
data is of limited usefulness if a user can not edit it to suit a new purpose: Lawrence, J., Ben-Artzi, 
A., DeCoro, C., Matusik, W., Pfister, H., Ramamoorthi, R., and Rusinkiewicz, S. 2006. Inverse shade trees 
for non-parametric material representation and editing. ACM Trans. Graph. 25, 3 (Jul. 2006), Ying Song, 
Xin Tong, Fabio Pellacini, Pieter Peers, SubEdit: A Representation for Editing Measured Heterogeneous 
Subsurface Scattering SIGGRAPH 2009, (ACM TOG)   Inverse aging If we can simulate material aging, 
can we de-age models of existing objects? Within images Virtual painting restoration The availability 
of high end computing using clusters of machines coupled with progress in understanding surface microstructure 
makes first principles simulation of coatings and their change with time feasible for appearance applications. 
Surface structure changes and binders weaken. Surface color changes as binders yellow with exposure to 
sunlight. Hunt, F.Y., Galler M.A., and Martin J.W., Microstructure of Weathered Paint and Its Relation 
to Gloss Loss: Computer Simulation and Modelling, J. COAT. TECHNOL., 70, No. 880, 45 (1998). Jenny Faucheu, 
Kurt A. Wood, Li-Piin Sung, Jonathan W. Martin, Relating gloss loss to topographical features of a PVDF 
coating, Journal of Coatings Technology and Research 3(1) pages 29- 39, 2006. De-aging or cleaning objects 
in an image is demonstrated in: Su Xue, Jiaping Wang, Xin Tong, Qionghai Dai, Baining Gu, Image-based 
Material Weathering Computer Graphics Forum ( 27) 2, 617-626, 2008. Using a combination of spectral imaging 
and pigment mapping, RIT researchers have successfully rejuvenated paintings such as van Gogh s Starry 
night Y. Zhao, Image Segmentation and Pigment Mapping of Cultural Shape and Appearance Materials take 
on characteristic shapes Characteristic shapes particularly visible in weathered materials From Dorsey 
et al. 1999 Heritage Based on Spectral Imaging, Ph.D. Dssertation, R.I.T., Rochester, NY, 2008. http://www.art­si.org/PDFs/Processing/Zhao­PhD2008.pdf 
 As noted by Adelson, materials take on particular shapes, that is, we don t expect to see cream with 
sharp facets, or fluffy metals. E. Adelson, "On seeing stuff: The perception of materials by humans and 
machines," Proceedings of the SPIE 4299, pp. 1--12, 2001. The relationship between shape and material 
is particularly apparent in aged materials. Papers that deal with shape change during aging processes 
include: Beardal M, Farley M, Ouderkirk D, Smith J, Rheimschussel C, Jones M, et al. Goblins by spheroidal 
weathering. In: Eurographics workshop on natural phenomena, 2007. Wojtan C., Carlson M., MuchaP. J., 
Turk G.: Animating corrosion and erosion. In: Eurographics workshop on natural phenomena, 2007. 5. CURRENT 
TRENDS AND NEEDS TRENDS Design of Physical Materials Editable Captured Data Simulated Microstructure 
Aging Inverse Aging Shape and Appearance NEEDS INTERFACES DATA!!!  Most interfaces in use in modeling 
packages still require user to understand details of computer graphics representations, rather than allowing 
natural specification of material appearance.  DATA Isolated databases exist in a variety of formats: 
 Rutgers Skin database,  Columbia Databases: BTF ,Time Varying materials  Bonn BTF  Matusik BRDF 
 CornellMeasurements  NIST/NEFDS  Lightworks  Yale Context Aware Textures  Can we build a materials 
search engine? Yale Context Aware Textures database, http://graphics.cs.yale.edu/CAT Rutgers Skin database, 
http : //www.caip.rutgers.edu/rutgerste xture/  Columbia Databases: BTF http : //www1.cs.columbia.edu/CAV 
E/projects/btf/, Time Varying materials: http : //www1.cs.columbia.edu/CAV E//databases/staf/staf.php 
 Bonn BTF data base http : //btf.cs.uni - bonn.de/  Matusik BRDF: http : //www.merl.com/brdf/  CornellMeasurements 
http : //www.graphics.cornell.edu/onlin e/measurements/reflectance/  NIST/NEFDS http : //math.nist.gov/ 
FHunt/appearance/nefds.html  Lightworksmaterials for a variety of building supplymanufacturers http 
: //www.lightworks- user.com/ (proprietary) Currently available data is in many diverse formats   
  Bibliography Advanced Material Appearance Models General BRDF s, BSSRDF s, and Textures [Phong, 
1975],[Ward, 1992],[Lafortune et al., 1997],[Ashikhmin and Shirley, 2000],[Blinn, 1977],[Cook and Torrance, 
1982], [Oren and Nayar, 1994], [Jensen et al., 2001]  Specialized Material Models Natural Materials 
 Organic * skin: wrinkles and pores, pigmentation, subsurface scattering, surface oils [Walters, 2002], 
[Marschner et al., 2000],[Hanrahan and Krueger, 1993] [Stam, 2001] [Krishnaswamy and Baranoski, 2004], 
[De­bevec et al., 2000], [Georghiades, 2003] [Fuchs et al., 2005] [Weyrich et al., 2006] [Boissieux et 
al., 2000], [Koenderink and Pont, 2003] * hair and fur: scattering from individual strands, interre.ections, 
self-shadowing, [Robertson, 1999], [Blazej et al., 1989],[Kajiya and Kay, 1989],[Hiebert et al., 2006], 
[Marschner et al., 2003],[Zinke and Weber, 2007],[Lokovic and Veach, 2000], [Moon and Marschner, 2006] 
 * eyes: pigmentation, creating sense of depth, [Lefohn et al., 2003],[Lam and Baranoski, 2006] * plants: 
internal structure, pigments, hairy features, growth patterns, including leaves, bark, wood, [Bloomenthal, 
1985], [Prusinkiewicz et al., 1988],[Vogelmann, 1993], [Baranoski and Rokne, 1997],[Bara­noski and Rokne, 
2001], [Franzke and Deussen, 2003], [Wang et al., 2005], [Fowler et al., 1989; 1992], [Fuhrer et al., 
], [Lefebvre and Neyret, 2002], [Buchanan, 1998], [Lefebvre and Poulin, 2000], [Terraz et al., 2009] 
 * birds: structure of feathers, forming coats, [Dai et al., 1995], [Streit and Heidrich, 2002] [Chen 
et al., 2002] * insects: iridescence, [Sun, 2006]  Natural Materials Inorganic * porous : modi.cation 
of re.ectance, [Merillou et al., 2000] * wet: darkening and transparency effects for water or oil permeated 
materials, [Jensen et al., 1999], [Nakamae et al., 1990] * snow:internal scattering, visual particles, 
[Nishita et al., 1997] * water: scattering from suspended particles, [Gutierrez et al., 2008], [Narasimhan 
et al., 2006]  Manufactured/Processed Materials * .nished wood: sense of depth, [Marschner et al., 
2005]  * textiles: individual .bers, threads and formation of fabrics, [Rubin, 1998],[Groeller et al., 
1995],[Xu et al., 2001],[Adabala et al., 2003] * automotive paint: color .op, depth, sparkle, orange 
peel, [Takagi et al., 1990], [G¨unther et al., 2005], [Dumont-B`ecle et al., 2001],[Shimizu et al., 2003], 
[Ershov et al., 2001] [Rump et al., 2008] * artistic paints and inks: pigmentation and .ow effects, 
[Cockshott et al., 1992],[1997], [Chu and Tai, 2005] * gems: light phenomena requiring simulation of 
wave effects, [Yuan et al., 1988] , [Sun et al., 2001],[Guy and Soler, 2004],[Nagata et al., 1997], [Weidlich 
and Wilkie, 2008b],[Weidlich and Wilkie, 2008a], [Weidlich and Wilkie, 2009],[Weidlich, 2009]   Aging 
and Weathering Processes  Simulation Methods: * patination: [Dorsey and Hanrahan, 1996],[Chang and 
Shih, 2000] * rust: [Merillou et al., 2001b], [Chang and Shih, 2003]  * erosion: [Dorsey et al., 1999],[Wojtan 
et al., 2007][Beardall et al., 2007] * cracking: [Hirota et al., 1998],[Hirota et al., 2000], [Gobron 
and Chiba, 2001b] ,[Aoki et al., 2002][Pa­quette et al., 2002] * peeling: [Gobron and Chiba, 2001a] 
 * scratching:[Merillou et al., 2001a], [Bosch et al., 2004] * denting and impacts: [Eric Paquette, 
2001] * dust accumulation: [chi Hsu and tsin Wong, 1995] * lichen growth: [Desbenoit et al., 2004] 
 * general effects and survey: [Chen et al., 2005],[M´erillou and Ghazanfarpour, 2008]  Capture Approaches: 
Methods that use photography and/or 3D scanning to capture instances of aging effects that can be generalized 
* capturing context [Lu et al., 2005],[Lu et al., 2007]  * temporal variation of BTF s and BRDF s [Koudelka, 
2004] [Gu et al., 2006], [Sun et al., 2007] * models from single examples of materials: [Wang et al., 
2006] [Xuey et al., 2008]   [Adabala et al., 2003] Neeharika Adabala, Guangzheng Fei, and Nadia Magnenat-Thalmann. 
Visualization of woven cloth. In Philip Dutr´e, Frank Suykens, Per H. Christensen, and Daniel Cohen-Or, 
editors, Proceedings of the 14th Eurographics workshop on Rendering, pages 178 185, Leuven, Belgium, 
2003. Eurographics Association. [Aoki et al., 2002] K. Aoki, Ngo Hai Dong, T. Kaneko, and S. Kuriyama. 
Physically-based simulation of cracks on drying 3d solid. In 10th Paci.c Graphics Conference on Computer 
Graphics and Applications, pages 467 468, Beijing China, Oct 2002. IEEE. [Ashikhmin and Shirley, 2000] 
Michael Ashikhmin and Peter Shirley. An anisotropic Phong BRDF model. Journal of Graphic Tools, 5(2):25 
32, 2000. [Baranoski and Rokne, 1997] G. V. G. Baranoski and J. G. Rokne. An algorithmic re.ectance and 
transmittance model for plant tissue. Computer Graphics Forum, 16(3):141 150, August 1997. ISSN 1067-7055. 
[Baranoski and Rokne, 2001] Gladimir V. G. Baranoski and Jon G. Rokne. Ef.ciently simulating scattering 
of light by leaves. The Visual Computer, 17(8):491 505, 2001. [Beardall et al., 2007] M. Beardall, M. 
Farley, D. Ouderkirk, J. Smith, M. Jones, and P. Egbert. Goblins by spheroidal weathering. In Eurographics 
workshop on natural phenomena, pages 7 14, 2007. [Blazej et al., 1989] A. A. Blazej, J. Galatik, Z. Galatik, 
Z. Krul, and M. Mladek. Atlas of Microscopic Struc­tures of Fur Skins 1. Elsevier, New York, 1989. [Blinn, 
1977] James F. Blinn. Models of light re.ection for computer synthesized pictures. In Proceedings of 
the 4th annual conference on Computer graphics and interactive techniques, pages 192 198. ACM Press, 
1977. [Bloomenthal, 1985] Jules Bloomenthal. Modeling the mighty maple. In Proceedings of the 12th annual 
conference on Computer graphics and interactive techniques, pages 305 311. ACM Press, 1985. [Boissieux 
et al., 2000] L. Boissieux, G. Kiss, N. Magnenat-Thalmann, and P. Kalra. Simulation of skin aging and 
wrinkles with cosmetics insight. Computer Animation and Simulation 2000, pages 15 27, 2000. [Bosch et 
al., 2004] C. Bosch, X. Pueyo, S. M´erillou, and D. Ghazanfarpour. A physically-based model for rendering 
realistic scratches. Computer Graphics Forum, 23(3):361 370, 2004. [Buchanan, 1998] John W. Buchanan. 
Simulating wood using a voxel approach. Computer Graphics Forum, 17(3):105 112, 1998. ISSN 1067-7055. 
[Chang and Shih, 2000] Yao-Xun Chang and Zen-Chung Shih. Physically-based patination for underground 
objects. Computer Graphics Forum, 19(3), August 2000. ISSN 1067-7055. [Chang and Shih, 2003] Yao-Xun 
Chang and Zen-Chung Shih. The synthesis of rust in seawater. The Visual Computer, 19(1):50 66, 2003. 
[Chen et al., 2002] Yanyun Chen, Yingqing Xu, Baining Guo, and Heung-Yeung Shum. Modeling and render­ing 
of realistic feathers. In Proceedings of the 29th annual conference on Computer graphics and interactive 
techniques, pages 630 636. ACM Press, 2002. [Chen et al., 2005] Y. Chen, L. Xia, T.T. Wong, X. Tong, 
H. Bao, B. Guo, and H.Y. Shum. Visual simulation of weathering by . -ton tracing. Proceedings of ACM 
SIGGRAPH 2005, 24(3):1127 1133, 2005. [chi Hsu and tsin Wong, 1995] Siu chi Hsu and Tien tsin Wong. Simulating 
dust accumulation. IEEE Comput. Graph. Appl., 15(1):18 22, 1995. [Chu and Tai, 2005] Nelson S.-H. Chu 
and Chiew-Lan Tai. Moxi: real-time ink dispersion in absorbent paper. ACM Trans. Graph., 24(3):504 511, 
2005. [Cockshott et al., 1992] T. Cockshott, J. Patterson, and D. England. Modelling the texture of paint. 
Computer Graphics Forum, 11(3):C217 C226, C476, 1992. [Cook and Torrance, 1982] R. L. Cook and K. E. 
Torrance. A re.ectance model for computer graphics. ACM Transactions on Graphics, 1(1):7 24, January 
1982. [Curtis et al., 1997] Cassidy J. Curtis, Sean E. Anderson, Joshua E. Seims, Kurt W. Fleischer, 
and David H. Salesin. Computer-generated watercolor. In Proceedings of the 24th annual conference on 
Computer graph­ics and interactive techniques, pages 421 430. ACM Press/Addison-Wesley Publishing Co., 
1997.  [Dai et al., 1995] Wen-Kai Dai, Zen-Chung Shih, and Ruei-Chuan Chang. Synthesizing feather textures 
in galliformes. Computer Graphics Forum, 14(3):407 420, August 1995. ISSN 1067-7055. [Debevec et al., 
2000] Paul Debevec, Tim Hawkins, Chris Tchou, Haarm-Pieter Duiker, Westley Sarokin, and Mark Sagar. Acquiring 
the re.ectance .eld of a human face. In Proceedings of the 27th annual conference on Computer graphics 
and interactive techniques, pages 145 156. ACM Press/Addison-Wesley Publishing Co., 2000. [Desbenoit 
et al., 2004] Brett Desbenoit, Eric Galin, and Samir Akkouche. Simulating and modeling lichen growth. 
Computer Graphics Forum, 23(3):341 350, 2004. [Dorsey and Hanrahan, 1996] Julie Dorsey and Pat Hanrahan. 
Modeling and rendering of metallic patinas. In Proceedings of the 23rd annual conference on Computer 
graphics and interactive techniques, pages 387 396. ACM Press, 1996. [Dorsey et al., 1999] Julie Dorsey, 
Alan Edelman, Henrik Wann Jensen, Justin Legakis, and Hans Kohling Pedersen. Modeling and rendering of 
weathered stone. In Proceedings of the 26th annual conference on Computer graphics and interactive techniques, 
pages 225 234. ACM Press/Addison-Wesley Publishing Co., 1999. [Dumont-B`ecle et al., 2001] P. Dumont-B`ecle, 
A. Kemeny, S. Michelin, and D. Arqu`es. Multi-texturing ap­proach for paint appearance simulation on 
virtual vehicles. Proceedings of the Driving Simulation Conference 2001, 213, 2001. [Eric Paquette, 2001] 
George Drettakis Eric Paquette, Pierre Poulin. Surface aging by impacts. In Graphics Interface 2001, 
pages 175 182, June 2001. [Ershov et al., 2001] Sergey Ershov, Konstantin Kolchin, and Karol Myszkowski. 
Rendering pearlescent ap­pearance based on paint-composition modelling. Computer Graphics Forum, 20(3), 
2001. ISSN 1067-7055. [Fowler et al., 1989] Deborah R. Fowler, James Hanan, and Przemyslaw Prusinkiewicz. 
Modelling spiral phyl­lotaxis. Computers and Graphics, 13(3):291 296, 1989. [Fowler et al., 1992] Deborah 
R. Fowler, Przemyslaw Prusinkiewicz, and Johannes Battjes. A collision-based model of spiral phyllotaxis. 
In Proceedings of the 19th annual conference on Computer graphics and inter­active techniques, pages 
361 368. ACM Press, 1992. [Franzke and Deussen, 2003] Oliver Franzke and Oliver Deussen. Rendering plant 
leaves faithfully. In Pro­ceedings of the SIGGRAPH 2003 conference on Sketches &#38; applications, pages 
1 1. ACM Press, 2003. [Fuchs et al., 2005] Martin Fuchs, Hendrik Lensch, and Hans-Peter Seidel. Re.ectance 
from images: A model­based approach for human faces. IEEE Transactions on Visualization and Computer 
Graphics, 11(3):296 305, 2005. Member-Volker Blanz. [Fuhrer et al., ] Martin Fuhrer, Henrik Wann Jensen, 
and Przemyslaw Prusinkiewicz. Modeling hairy plants. Graphical Models, 68(4), July. [Georghiades, 2003] 
Athinodoros S. Georghiades. Recovering 3-d shape and re.ectance from a small number of photographs. In 
EGRW 03: Proceedings of the 14th Eurographics workshop on Rendering, pages 230 240, Aire-la-Ville, Switzerland, 
Switzerland, 2003. Eurographics Association. [Gobron and Chiba, 2001a] S. Gobron and N. Chiba. Simulation 
of peeling using 3d-surface cellular automata. In 9th Paci.c Graphics Conference on Computer Graphics 
and Applications, pages 338 347, Tokyo Japan, Oct 2001. IEEE. [Gobron and Chiba, 2001b] St´ephane Gobron 
and Norishige Chiba. Crack pattern simulation based on 3d sur­face cellular automata. The Visual Computer, 
17(5):287 309, 2001. [Groeller et al., 1995] Eduard Groeller, Rene T. Rau, and Wolfgang Strasser. Modeling 
and visualization of knitwear. IEEE Transactions on Visualization and Computer Graphics, 1(4):302 310, 
1995. [Gu et al., 2006] Jinwei Gu, Chien-I Tu, Ravi Ramamoorthi, Peter Belhumeur, Wojciech Matusik, and 
Shree Nayar. Time-varying surface appearance: acquisition, modeling and rendering. ACM Trans. Graph., 
25(3):762 771, 2006.  [G¨unther et al., 2005] Johannes G¨unther, Tongbo Chen, Michael Goesele, Ingo 
Wald, and Hans-Peter Seidel. Ef.cient acquisition and realistic rendering of car paint. In G¨unther Greiner, 
Joachim Hornegger, Hein­rich Niemann, and Marc Stamminger, editors, Proceedings of 10th International 
Fall Workshop -Vision, Modeling, and Visualization (VMV) 2005, pages 487 494. Akademische Verlagsgesellschaft 
Aka GmbH, November 2005. [Gutierrez et al., 2008] D. Gutierrez, F.J. Seron, A. Munoz, and O. Anson. Visualizing 
Underwater Ocean Optics. In Computer Graphics Forum, volume 27, pages 547 556. Blackwell Publishing Ltd, 
2008. [Guy and Soler, 2004] Stephane Guy and Cyril Soler. Graphics gems revisited: fast and physically-based 
ren­dering of gemstones. ACM Trans. Graph., 23(3):231 238, 2004. [Hanrahan and Krueger, 1993] Pat Hanrahan 
and Wolfgang Krueger. Re.ection from layered surfaces due to subsurface scattering. In Proceedings of 
the 20th annual conference on Computer graphics and interactive techniques, pages 165 174. ACM Press, 
1993. [Hiebert et al., 2006] Brad Hiebert, Jubin Dave, Tae-Yong Kim, Ivan Neulander, Hans Rijpkema, and 
Will Telford. The chronicles of Narnia: the lion, the crowds and rhythm and hues. In SIGGRAPH 06: ACM 
SIGGRAPH 2006 Courses, page 1, New York, NY, USA, 2006. ACM Press. [Hirota et al., 1998] Koichi Hirota, 
Yasuyuki Tanoue, and Toyohisa Kaneko. Generation of crack patterns with a physical model. The Visual 
Computer, 14(3):126 137, 1998. [Hirota et al., 2000] Koichi Hirota, Yasuyuki Tanoue, and Toyohisa Kaneko. 
Simulation of three-dimensional cracks. The Visual Computer, 16(7):371 378, 2000. [Jensen et al., 1999] 
Henrik Wann Jensen, Justin Legakis, and Julie Dorsey. Rendering of wet material. In Dani Lischinski and 
Greg Ward Larson, editors, Rendering Techniques 99, Eurographics, pages 273 282. Springer-Verlag Wien 
New York, 1999. Proc. 10th Eurographics Rendering Workshop, Granada, Spain, June 21 23, 1999. [Jensen 
et al., 2001] Henrik Wann Jensen, Stephen R. Marschner, Marc Levoy, and Pat Hanrahan. A practical model 
for subsurface light transport. In Proceedings of the 28th annual conference on Computer graphics and 
interactive techniques, pages 511 518. ACM Press, 2001. [Kajiya and Kay, 1989] J. T. Kajiya and T. L. 
Kay. Rendering fur with three dimensional textures. In Proceed­ings of the 16th annual conference on 
Computer graphics and interactive techniques, pages 271 280. ACM Press, 1989. [Koenderink and Pont, 2003] 
J. Koenderink and S. Pont. The secret of velvety skin. Machine Vision and Applications, 14(4):260 268, 
2003. [Koudelka, 2004] M.L. Koudelka. Capture, analysis and synthesis of textured surfaces with variation 
in illu­mination, viewpoint, and time. Yale University New Haven, CT, USA, 2004. [Krishnaswamy and Baranoski, 
2004] A. Krishnaswamy and G.V.G. Baranoski. A Biophysically-Based Spec­tral Model of Light Interaction 
with Human Skin. Computer Graphics Forum, 23(3):331 340, 2004. [Lafortune et al., 1997] Eric P. F. Lafortune, 
Sing-Choong Foo, Kenneth E. Torrance, and Donald P. Green­berg. Non-linear approximation of re.ectance 
functions. In Proceedings of the 24th annual conference on Computer graphics and interactive techniques, 
pages 117 126. ACM Press/Addison-Wesley Publishing Co., 1997. [Lam and Baranoski, 2006] Michael W.Y. 
Lam and Gladimir V.G. Baranoski. A predictive light transport model for the human iris. Computer Graphics 
Forum, 25(3):359 368, 2006. [Lefebvre and Neyret, 2002] Sylvain Lefebvre and Fabrice Neyret. Synthesizing 
bark. In P. Debevec and S. Gibson, editors, 13th Eurographics Workshop on Rendering, Pisa, Italy, 2002. 
Eurographics Association. [Lefebvre and Poulin, 2000] Laurent Lefebvre and Pierre Poulin. Analysis and 
synthesis of structural textures. In Graphics Interface 2000, pages 77 86, May 2000. [Lefohn et al., 
2003] Aaron Lefohn, Brian Budge, Peter Shirley, Richard Caruso, and Erik Reinhard. An ocu­ larist s approach 
to human iris synthesis. IEEE Comput Graphics Appl, 23(6):70 75, November/December 2003.  [Lokovic and 
Veach, 2000] Tom Lokovic and Eric Veach. Deep shadow maps. In SIGGRAPH 00: Proceedings of the 27th annual 
conference on Computer graphics and interactive techniques, pages 385 392, New York, NY, USA, 2000. ACM 
Press/Addison-Wesley Publishing Co. [Lu et al., 2005] Jianye Lu, Athinodoros S. Georghiades, Holly Rushmeier, 
Julie Dorsey, and Chen Xu. Syn­thesis of material drying history: Phenomenon modeling, transferring and 
rendering. In proceedings of Eurographics Workshop on Natural Phenomena, pages 7 16, 2005. [Lu et al., 
2007] Jianye Lu, Athinodoros S. Georghiades, Andreas Glaser, Hongzhi Wu, Li-Yi Wei, Baining Guo, Julie 
Dorsey, and Holly Rushmeier. Context-aware textures. ACM Trans. Graph., 26(1):3, 2007. [Marschner et 
al., 2000] Stephen R. Marschner, Brian K. Guenter, and Sashi Raghupathy. Modeling and ren­dering for 
realistic facial animation. In Proceedings of the Eurographics Workshop on Rendering Techniques 2000, 
pages 231 242, London, UK, 2000. Springer-Verlag. [Marschner et al., 2003] Stephen R. Marschner, Henrik 
Wann Jensen, Mike Cammarano, Steve Worley, and Pat Hanrahan. Light scattering from human hair .bers. 
ACM Trans. Graph., 22(3):780 791, 2003. [Marschner et al., 2005] Stephen R. Marschner, Stephen H. Westin, 
Adam Arbree, and Jonathan T. Moon. Mea­suring and modeling the appearance of .nished wood. ACM Trans. 
Graph., 24(3):727 734, 2005. [M´erillou and Ghazanfarpour, 2008] S. M´erillou and D. Ghazanfarpour. A 
survey of aging and weathering phenomena in computer graphics. Computers &#38; Graphics, 32(2):159 174, 
2008. [Merillou et al., 2000] S. Merillou, J.-M. Dischler, and D. Ghazanfarpour. A BRDF postprocess to 
integrate porosity on rendered surface. IEEE Transactions on Visualization and Computer Graphics, 6(4):306 
318, October 2000. [Merillou et al., 2001a] S. Merillou, J.M. Dischler, and D. Ghazanfarpour. Surface 
scratches: measuring, mod­eling and rendering. The Visual Computer, 17(1):30 45, 2001. [Merillou et 
al., 2001b] Stephane Merillou, Jean-Michel Dischler, and Djamchid Ghazanfarpour. Corrosion: Simulating 
and rendering. In GI 2001, pages 167 174, June 2001. [Moon and Marschner, 2006] Jonathan T. Moon and 
Stephen R. Marschner. Simulating multiple scattering in hair using a photon mapping approach. In SIGGRAPH 
06: ACM SIGGRAPH 2006 Papers, pages 1067 1074, New York, NY, USA, 2006. ACM Press. [Nagata et al., 1997] 
Noriko Nagata, Toshimasa Dobashi, Yoshitsugu Manabe, Teruo Usami, and Seiji Inokuchi. Modeling and Visualization 
for a Pearl-Quality Evaluation Simulator. IEEE Transactions on Visualization and Computer Graphics, 3(4):307 
315, October 1997. [Nakamae et al., 1990] E. Nakamae, K. Kaneda, T. Okamoto, and T. Nishita. A lighting 
model aiming at drive simulators. In Proceedings of Siggraph 1990, pages 395 404. ACM SIGGRAPH, 1990. 
[Narasimhan et al., 2006] S.G. Narasimhan, M. Gupta, C. Donner, R. Ramamoorthi, S.K. Nayar, and H.W. 
Jensen. Acquiring scattering properties of participating media by dilution. Proceedings of ACM SIGGRAPH 
2006, 25(3):1003 1012, 2006. [Nishita et al., 1997] T. Nishita, H. Iwasaki, Y. Dobashi, and E. Nakamae. 
A modeling and rendering method for snow by using metaballs. Computer Graphics Forum, 16(3):357 364, 
August 1997. ISSN 1067-7055. [Oren and Nayar, 1994] Michael Oren and Shree K. Nayar. Generalization of 
lambert s re.ectance model. In Proceedings of the 21st annual conference on Computer graphics and interactive 
techniques, pages 239 246. ACM Press, 1994. [Paquette et al., 2002] Eric Paquette, Pierre Poulin, and 
George Drettakis. The simulation of paint cracking and peeling. In Graphics Interface 2002, pages 59 
68, May 2002. [Phong, 1975] Bui Tuong Phong. Illumination for computer generated pictures. Commun. ACM, 
18(6):311 317, 1975. [Prusinkiewicz et al., 1988] Przemyslaw Prusinkiewicz, Aristid Lindenmayer, and 
James Hanan. Development models of herbaceous plants for computer imagery purposes. In SIGGRAPH 88: Proceedings 
of the 15th annual conference on Computer graphics and interactive techniques, pages 141 150, New York, 
NY, USA, 1988. ACM Press.  [Robertson, 1999] James Robertson. Forensic Examination of Human Hair. CRC 
Press, 1999. [Rubin, 1998] Barry Rubin. Tailored Fiber Cross Sections. Advanced Materials, 10(15):1225 
1227, 1998. [Rump et al., 2008] M. Rump, G. Muller, R. Sarlette, D. Koch, and R. Klein. Photo-realistic 
rendering of metallic car paint from image-based measurements. In Computer Graphics Forum, volume 27, 
pages 527 536. Blackwell Publishing Ltd, 2008. [Shimizu et al., 2003] Clement Shimizu, Gary W. Meyer, 
and Joseph P. Wingard. Interactive goniochromatic color design. In Eleventh Color Imaging Conference, 
pages 16 22, 2003. [Stam, 2001] Jos Stam. An illumination model for a skin layer bounded by rough surfaces. 
In Proceedings of the 12th Eurographics Workshop on Rendering Techniques, pages 39 52, London, UK, 2001. 
Springer-Verlag. [Streit and Heidrich, 2002] L. Streit and W. Heidrich. A biologically-parameterized 
feather model. Computer Graphics Forum, 21(3):565 565, 2002. [Sun et al., 2001] Yinlong Sun, F. David 
Fracchia, Mark S. Drew, and Thomas W. Calvert. A spectrally based framework for realistic image synthesis. 
The Visual Computer, 17(7):429 444, 2001. [Sun et al., 2007] B. Sun, K. Sunkavalli, R. Ramamoorthi, PN 
Belhumeur, and SK Nayar. Time-Varying BRDFs. IEEE Transactions on Visualization and Computer Graphics, 
13(3):595 609, 2007. [Sun, 2006] Yinlong Sun. Rendering biological iridescences with rgb-based renderers. 
ACM Trans. Graph., 25(1):100 129, 2006. [Takagi et al., 1990] Atsushi Takagi, Hitoshi Takaoka, Tetsuya 
Oshima, and Yoshinori Ogata. Accurate render­ing technique based on colorimetric conception. In Proceedings 
of the 17th annual conference on Computer graphics and interactive techniques, pages 263 272. ACM Press, 
1990. [Terraz et al., 2009] O. Terraz, G. Guimberteau, S. M´erillou, D. Plemenos, and D. Ghazanfarpour. 
3Gmap L-systems: an application to the modelling of wood. The Visual Computer, 25(2):165 180, 2009. [Vogelmann, 
1993] C. Vogelmann. Plant tissue optics. Annual review of plant physiol. plant mol. biol., 44:231 251, 
1993. [Walters, 2002] Kenneth A. Walters. Dermatological and Transdermal Formulations. Marcel Dekker 
Incorpo­rated, 2002. [Wang et al., 2005] Lifeng Wang, Wenle Wang, Julie Dorsey, Xu Yang, Baining Guo, 
and Heung-Yeung Shum. Real-time rendering of plant leaves. In SIGGRAPH 05: ACM SIGGRAPH 2005 Papers, 
pages 712 719, New York, NY, USA, 2005. ACM Press. [Wang et al., 2006] Jiaping Wang, Xin Tong, Stephen 
Lin, Minghao Pan, Chao Wang, Hujun Bao, Baining Guo, and Heung-Yeung Shum. Appearance manifolds for modeling 
time-variant appearance of materials. ACM Trans. Graph., 25(3):754 761, 2006. [Ward, 1992] Gregory J. 
Ward. Measuring and modeling anisotropic re.ection. In Proceedings of the 19th annual conference on Computer 
graphics and interactive techniques, pages 265 272. ACM Press, 1992. [Weidlich and Wilkie, 2008a] Andrea 
Weidlich and Alexander Wilkie. Modeling aventurescent gems with pro­cedural textures. In Proceedings 
of the Spring Conference on Computer Graphics (SCCG), pages [Weidlich and Wilkie, 2008b] Andrea Weidlich 
and Alexander Wilkie. Realistic rendering of birefringency in uniaxial crystals. ACM Transactions on 
Graphics, 27(1):6:1 6:12, March 2008. [Weidlich and Wilkie, 2009] Andrea Weidlich and Alexander Wilkie. 
Rendering the effect of labradorescence. In Proceedings of Graphics Interface 2009, pages [Weidlich, 
2009] Andrea Weidlich. Pseudochromatic Colourisation of Crystals in Predictive Image Synthesis. PhD thesis, 
Institute of Computer Graphics and Algorithms, Vienna University of Technology, Favoriten­strasse 9-11/186, 
A-1040 Vienna, Austria, January 2009. [Weyrich et al., 2006] Tim Weyrich, Wojciech Matusik, Hanspeter 
P.ster, Bernd Bickel, Craig Donner, Chien Tu, Janet McAndless, Jinho Lee, Addy Ngan, Henrik Wann Jensen, 
and Markus Gross. Analysis of human faces using a measurement-based skin re.ectance model. In SIGGRAPH 
06: ACM SIGGRAPH 2006 Papers, pages 1013 1024, New York, NY, USA, 2006. ACM Press.  [Wojtan et al., 
2007] Chris Wojtan, Mark Carlson, Peter J. Mucha, and Greg Turk. Animating Corrosion and Erosion. In 
D. Ebert and S. Merillou, editors, Eurographics Workshop on Natural Phenomena, pages 15 22, Prague, Czech 
Republic, 2007. Eurographics Association. [Xu et al., 2001] Ying-Qing Xu, Yanyun Chen, Stephen Lin, Hua 
Zhong, Enhua Wu, Baining Guo, and Heung-Yeung Shum. Photorealistic rendering of knitwear using the lumislice. 
In Proceedings of the 28th annual conference on Computer graphics and interactive techniques, pages 391 
398. ACM Press, 2001. [Xuey et al., 2008] S. Xuey, J. Wang, X. Tong, Q. Dai, and B. Guo. Image-based 
Material Weathering. In Computer Graphics Forum, volume 27, pages 617 626. Blackwell Publishing Ltd, 
2008. [Yuan et al., 1988] Ying Yuan, Tosiyasu L. Kunii, Naota Inamato, and Lining Sun. Gemstone .re: 
Adaptive dispersive ray tracing of polyhedrons. The Visual Computer, 4(5):259 70, November 1988. [Zinke 
and Weber, 2007] Arno Zinke and Andreas Weber. Light scattering from .laments. IEEE Trans. on Vis. and 
Comp. Graphics, 13(2):342 356, March/April 2007.   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1667243</article_id>
		<sort_key>40</sort_key>
		<display_label>Article No.</display_label>
		<display_no>4</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Advances in real-time rendering in 3D graphics and games I]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1667239.1667243</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1667243</url>
		<abstract>
			<par><![CDATA[<p>Advances in real-time graphics research and the increasing power of mainstream GPUs has generated an explosion of innovative algorithms suitable for rendering complex virtual worlds at interactive rates. Every year, the latest video games display a vast variety of sophisticated algorithms that power ground-breaking 3D rendering and push the visual boundaries and interactive experience of rich environments.</p> <p>This course covers a series of topics on the best practices and techniques prevalent in state-of-the-art rendering in several award-winning games, and describes innovative and practical 3D rendering research breaktrhoughs that will be used in the games of tomorrow. The course is designed for technical practitioners and developers of graphics engines for visualization, games, or effects rendering. Presented techniques are applicable in real-time and off-line domains. Attendees will acquire a number of highly optimized algorithms in various areas of real-time rendering.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1797108</person_id>
				<author_profile_id><![CDATA[81332531213]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Natalya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tatarchuk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1667244</article_id>
		<sort_key>50</sort_key>
		<display_label>Article No.</display_label>
		<display_no>5</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Advances in real-time rendering in 3D graphics and games II]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1667239.1667244</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1667244</url>
		<abstract>
			<par><![CDATA[<p>This course is a continuation of Advances in Real-Time Rendering in 3D Graphics and Games I.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1797109</person_id>
				<author_profile_id><![CDATA[81332531213]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Natalya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tatarchuk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1667245</article_id>
		<sort_key>60</sort_key>
		<display_label>Article No.</display_label>
		<display_no>6</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[An introduction to shader-based OpenGL programming]]></title>
		<page_from>1</page_from>
		<page_to>152</page_to>
		<doi_number>10.1145/1667239.1667245</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1667245</url>
		<abstract>
			<par><![CDATA[<p>OpenGL is the most widely available application programming interface (API) for creating applications in almost every area of computer graphics including research, scientific visualization, entertainment and visual effects, computer-aided design, interactive gaming, and many more. Over the past decade, OpenGL has evolved to a large API with multiple, sometimes incompatible, versions. Recent versions of OpenGL have become shader-based, and the original fixed-function pipeline may not be available.</p> <p>This course provides an accelerated introduction to creating applications using these recent versions of OpenGL API. It introduces the most recent version of OpenGL, in which an application must provide vertex and fragment shaders and cannot rely on a fixed-function pipeline. Consequently, this course is a complete rewrite of the OpenGL course that has been taught at the annual SIGGRAPH conference for over 10 years.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Software support</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>D.3.2</cat_node>
				<descriptor>Specialized application languages</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011050.10011023</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Context specific languages->Specialized application languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011074</concept_id>
				<concept_desc>CCS->Software and its engineering->Software creation and management</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1797110</person_id>
				<author_profile_id><![CDATA[81100366265]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ed]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Angel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of New Mexico]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797111</person_id>
				<author_profile_id><![CDATA[81322506343]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dave]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shreiner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ARM, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 An Introduction to Shader-Based OpenGL Programming  Ed Angel University of New Mexico angel@cs.unm.edu 
 Dave Shreiner ARM, Inc. shreiner@siggraph.org With contributions from Evan Hart (NVIDIA) and Bill 
Licea­Kane (AMD) SIGGRAPH 2009 An Introduction to Shader-Based OpenGL Programming Table of Contents 
Notes Title Page................................................................................................................................................ 
1 What Is OpenGL and What Can It Do for Me? ....................................................................................... 
2 Course Ground-rules .............................................................................................................................. 
3 Syllabus .................................................................................................................................................. 
4 Getting Started ....................................................................................................................................... 
5 The Graphics Pipeline ............................................................................................................................ 
6 Steps in Pipeline .................................................................................................................................... 
7 Graphic Pipeline Varieties ...................................................................................................................... 
8 OpenGL Versions .................................................................................................................................. 
9 Developing an OpenGL 3.1 Application .............................................................................................. 
10 OpenGL and Related APIs ................................................................................................................... 
11 General Structure of an OpenGL Program ........................................................................................... 
12 The Simplest OpenGL Program ........................................................................................................... 
13 The Simplest OpenGL Program (cont d) ............................................................................................. 
14 Drawing a Triangle - A More Realistic Example .................................................................................. 
15 Loading Vertex Data ............................................................................................................................ 
16 Initializing Shaders .............................................................................................................................. 
17 Drawing our Triangle ........................................................................................................................... 
18 The Simplest Vertex Shader ................................................................................................................. 
19 The Simplest Fragment Shader ............................................................................................................ 
20 Working With Objects .......................................................................................................................... 
21 Representing Geometry ....................................................................................................................... 
22 OpenGL s Geometric Primitives ........................................................................................................... 
23 Vertex Attributes .................................................................................................................................. 
24 Vertex Arrays ....................................................................................................................................... 
25 Steps for Creating Buffer Objects ........................................................................................................ 
26 Storing Vertex Attributes ....................................................................................................................... 
27 Storing Vertex Attributes (cont d) .......................................................................................................... 
28 Turning on Vertex Arrays ................................................................................................................... 
29 Drawing Geometric Primitives .............................................................................................................. 
30 Drawing Geometric Primitives .............................................................................................................. 
31 Shaders and GLSL .............................................................................................................................. 
32 Where the work gets done ................................................................................................................... 
33 Vertex Shader Execution ..................................................................................................................... 
34 Fragment Shader Execution ................................................................................................................ 
35 OpenGL Shading Language ................................................................................................................. 
36 Types and Qualifiers ............................................................................................................................ 
37 Constructors ......................................................................................................................................... 
38 Components ......................................................................................................................................... 
39 Vector Matrix Operations ...................................................................................................................... 
40 Functions .............................................................................................................................................. 
41 Built-In Variables................................................................................................................................... 
42 Built-in Functions .................................................................................................................................. 
43 Simple Example ................................................................................................................................... 
44 SIGGRAPH 2009 An Introduction to Shader-Based OpenGL Programming Application Callbacks .......................................................................................................................... 
45 Vertex Shader ...................................................................................................................................... 
46 Fragment Shader ................................................................................................................................. 
47 Creating a Shader Program.................................................................................................................. 
48 Shader Compilation (Part 1) ................................................................................................................. 
49 Shader Compilation (Part 2) ................................................................................................................. 
50 Shader Program Linking (Part 1) .......................................................................................................... 
51 Shader Program Linking (Part 2) .......................................................................................................... 
52 Using Shaders in an Application........................................................................................................... 
53 Associating Shader Variables and Data ............................................................................................... 
54 Determining Shader Variable Locations .............................................................................................. 
55 Initializing Uniform Variable Values ...................................................................................................... 
56 Transformations ................................................................................................................................... 
57 Transformations ................................................................................................................................... 
58 Matrix Storage in OpenGL ................................................................................................................... 
59 Camera Analogy ................................................................................................................................... 
60 What Transformations Do in OpenGL ................................................................................................. 
61 Specifying What You Can See ............................................................................................................. 
62 Specifying What You Can See (cont d) ................................................................................................ 
63 Specifying What You Can See (cont d) ................................................................................................ 
64 The Viewport ........................................................................................................................................ 
65 Viewing Transformations ...................................................................................................................... 
66 Creating the LookAt Matrix ................................................................................................................... 
67 Translation ............................................................................................................................................ 
68 Scale ..................................................................................................................................................... 
69 Rotation ................................................................................................................................................ 
70 Rotation (cont d) ................................................................................................................................... 
71 Don t Worry! ......................................................................................................................................... 
72 Double Buffering ................................................................................................................................... 
73 Animation Using Double Buffering........................................................................................................ 
74 Depth Buffering and Hidden Surface Removal .................................................................................... 
75 Depth Buffering Using OpenGL ............................................................................................................ 
76 Lighting ................................................................................................................................................ 
77 Lighting Principles ................................................................................................................................ 
78 OpenGL Shading .................................................................................................................................. 
79 The Modified Phong Model................................................................................................................... 
80 How OpenGL Simulates Lights ............................................................................................................ 
81 Surface Normals ................................................................................................................................... 
82 Material Properties ............................................................................................................................... 
83 Light Sources ........................................................................................................................................ 
84 Light Material Tutorial ........................................................................................................................... 
85 Texture Mapping .................................................................................................................................. 
86 Texture Mapping ................................................................................................................................... 
87 Applying Textures I .............................................................................................................................. 
88 Texture Objects .................................................................................................................................... 
89 Texture Objects (cont'd.) ...................................................................................................................... 
90 Specifying a Texture Image .................................................................................................................. 
91 Mapping a Texture ................................................................................................................................ 
92 Texture Tutorial ..................................................................................................................................... 
93 SIGGRAPH 2009 An Introduction to Shader-Based OpenGL Programming Texture Units ....................................................................................................................................... 
94 Sampling a Texture ............................................................................................................................. 
95 Texture Parameters ............................................................................................................................. 
96 Filter Modes .......................................................................................................................................... 
97 Mipmapped Textures ........................................................................................................................... 
98 Wrapping Mode .................................................................................................................................... 
99 Application Examples ........................................................................................................................ 
100 Shader Examples ............................................................................................................................... 
101 Height Fields ....................................................................................................................................... 
102 Displaying a Height Field .................................................................................................................... 
103 Time varying vertex shader ................................................................................................................ 
104 Mesh Display ...................................................................................................................................... 
105 Adding Lighting ................................................................................................................................... 
106 Mesh Shader ...................................................................................................................................... 
107 Mesh Shader (cont d) ........................................................................................................................ 
108 Mesh Shader (cont d) ........................................................................................................................ 
109 Shaded Mesh ...................................................................................................................................... 
110 Cartoon Shader .................................................................................................................................. 
111 Cartoon Shader ................................................................................................................................. 
112 Adding a Silhouette Edge ................................................................................................................... 
113 Smoothing........................................................................................................................................... 
114 Fragment Shader Examples ............................................................................................................... 
115 Per-Fragment Cartoon Vertex Shader .............................................................................................. 
116 Cartoon Fragment Shader ................................................................................................................. 
117 Cartoon Fragment Shader Result....................................................................................................... 
118 Reflection Map .................................................................................................................................... 
119 Reflection Map Vertex Shader ........................................................................................................... 
120 Reflection Map Fragment Shader ...................................................................................................... 
121 Reflection mapped teapot ................................................................................................................... 
122 Bump Mapping.................................................................................................................................... 
123 Bump Map Example ........................................................................................................................... 
124 Thanks! ............................................................................................................................................... 
125 Course Resources ............................................................................................................................. 
126 On-Line Resources ............................................................................................................................. 
127 Books .................................................................................................................................................. 
128  Program Listings triangle.cxx .......................................................................................................................................... 
130 color-draw-arrays.cxx ......................................................................................................................... 
133 AOS-color-draw-arrays.cxx ................................................................................................................ 
138 LoadProgram.h ................................................................................................................................... 
146 LoadProgram.c ................................................................................................................................... 
147  OpenGL is a library for doing computer graphics. By using it, you can create interactive applications 
that render high-quality color images composed of 3D geometric objects and images. OpenGL is window and 
operating system independent. As such, the part of your application which does rendering is platform 
independent. However, in order for OpenGL to be able to render, it needs a window to draw into. Generally,y, 
this is controlled byy the windowingg syystem on whatever platform you are working on. OpenGL 3.1 is 
the first version of OpenGL not to be backwards compatible with previous version. This version removed 
a lot of older-style functionality, like the immediate-mode rendering interface (e.g., glBegin() / glEnd()rendering), 
the matrix transformation stack, and vertex lighting, to name a few features. The idea was to move all 
applications to shader-based rendering, which can better leverage GPUs, and removed the less efficient 
methods and functions to help application developer s get the best performance from their hharddware. 
Additionalllly, shhadders provide consididerablbly more flflexibility iin how we Additi id ibilit h do 
graphics. We re no longer constrained by the features implemented in the core of OpenGL, but can completely 
control how we render. For the purposes of this course, we don t assume you know anything about OpenGL, 
just that you know a little about computer graphics. We ll take you step-by-step in constructing an application 
that renders 3D, moving, shaded objects.  Here we show a simplified version of the OpenGL pipeline, 
which specifically processes geometry. All OpenGL programs use this pipeline, and with OpenGL Version 
3.1, you get to configure the operation of the pipeline by writing shaders which are executed by the 
various stages of the pipeline. You can see that a vertex shader is used by the vertex processing phase, 
which accepts vertices as its input, processes them, and passes them into the next pphase of the pipeline. 
pp The rasterizer is the part of the pipeline that determines which pixels on the screen should be colored 
in based on the geometric primitive that the vertex processing phase finished with. The output of the 
rasterizer are fragments, which are passed into the fragment processing phase of the pipeline. Each fragment 
references a location in the window, which the fragment processing phase determines the color forphase 
determines the color for. The fragment processing phase takes a fragment shader which is executed for 
ever fragment from the input geometric primitive. The fragment shader s job is to shade (compute the 
color of) a pixel and then pass it into the fragment testing pipeline to determine if the color should 
be displayed in the window (i.e., written into the framebuffer). The framebuffer contains the final image 
after all of the geometric primitives are processed and shaded. Attributes such as colors, texture coordinates 
and lighting normals determine how a geometric object is displayed. Geometry processing includes coordinate 
transformations, per-vertex lighting, clipping and primitive assembly. The rasterizer generates fragments 
that correspond to pixel locations interior to the geometric objects that haven t be clipped. Fragments 
are potential pixels can appear on the display but may not due to later processing potential pixels can 
appear on the display but may not due to later processing such as hidden surface removal. The rasterizer 
must interpolate vertex attributes to determine values such as texture coordinates for each fragment. 
Fragment processing produces a color for each fragment and includes hidden surface removal, per fragment 
lighting, and application of textures. This course concentrates on the programmable version of OpenGL. 
 Most of the fixed-function pipeline functions were marked as deprecated in 3.0. This removed much of 
the default functionality and state. These functions were removed from 3.1 although they are available 
in an ARB extension that will likely be supported on most implementations. In OpenGL 3.1, almost all 
data is stored in objects, which is memory that OpenGL manages on your behalf. You can think of if roughly 
liked calling malloc(), where you specify how much memory you need. Once you ve allocated your objects, 
you ll need to load it with data, like vertex attributes. It s also quite likely you ll use textures, 
which is also data you ll pass toOpenGL to manage, and use to draw your scene. Next, you ll need to configure 
how the OpenGL pipeline should process datadata. You do this by specifying vertex and fragment shaders 
which are small You do this by specifying vertex and fragment shaders, which are small programs written 
in the OpenGL Shading Language (commonly called GLSL ). These shaders are compiled and linked like any 
compiled program. However, the entire compilation process is done by calling functions inside of your 
application. For the purposes of this class, we use an open-source library for managing windows named 
Freeglut. It allows you to easily port the same source code to any of the popular operating systems: 
Microsoft Windows, Apple s Mac OS X, and Linux. Each of the different operating systems has a specific 
library that is used to enable OpenGL rendering within its windowing system. For instance, you would 
use Microsoft Windows methods to open a window, and then use WGL the specific OpenGL windowing interface 
on Microsoft Windows to modify that window to be able to use OpenGL. Likewise, Mac OS X there s AGL 
which serves the same purpose, as does GLX on Linux for the X Window System. The above diagram illustrates 
the relationships of the various libraries and window system components. Generally, applications which 
require more user interface support will use a library designed to support those types of features (i.e., 
buttons, menu and scroll bars, etc.) such as Motif or the Win32 API. Prototype applications, or ones 
which do not require all the bells and whistles of a full GUI may choose to use GLUT instead because 
of its whistles of a full GUI, may choose to use GLUT instead because of its simplified programming model 
and window system independence. The OpenGL Utility Library (GLU) contained many helpful functions that 
were constructed from GL including quadrics, some additional viewing functions, and tessellation code. 
However, most GLU functions require OpenGL functions that have been removed from OpenGL 3.1 so thilib 
ill l b fl( lth h hfi ld bbl this library will no longer be useful (although much of it could probably 
be recreated without the deprecated functions). OpenGL was primarily designed to be able to draw high-quality 
images fast enough so that an application could draw many of them a second, and provide the user with 
an interactive application, where each frame could be customized by input from the user. The general 
flow of an interactive application, including OpenGL applications is: 1 Configure and open a window suitable 
for drawing OpenGL into 1. Configure and open a window suitable for drawing OpenGL into. 2. Initialize 
any OpenGL state that you will need to use throughout the application. 3. Process any events that the 
user might have entered. These could include pressing a key on the keyboard, moving the mouse, or even 
movingg or resizingg the apppp lication s window. 4. Draw your 3D image using OpenGL with values that 
may have been entered from the user s actions, or other data that the program has available to it.  
 This slide contains the program statements for the routine of a C program that uses OpenGL and GLUT. 
For the most part, all of the programs you will see today, and indeed may of the programs available as 
examples of OpenGL programming that use GLUT will look very similar to this program. All GLUT-based OpenGL 
programs begin with configuring the GLUT window that ggets oppened. Next, in the routine init() (detailed 
on the following slide), global OpenGL state is configured. By global , we mean state that will be left 
on for the duration of the application. By setting that state once, we can make our OpenGL applications 
run as efficiently as possible. As we shall later, the shaders must be read, compiled, and linked. AftAfter 
iinitiitiali lization, we set up our GLUTGLUT callback f k functitions, whihichh are i llb routines that 
you write to have OpenGL draw objects and other operations. Callback functions, if you re not familiar 
with them, make it easy to have a generic library (like GLUT), that can easily be configured by providing 
a few routines of your own construction. Finally, as with all interactive programs, the event loop is 
entered. For GLUT-based programs, this is done by calling glutMainLoop(). As glutMainLoop() never exits 
(it is essentially an infinite loop), any program statements that follow glutMainLoop() will never be 
executed. Here is the remainder of the program. While it s not very impressive, it s the fundamental 
structure of every OpenGL program. We use the init() routine configure settings that aren t going to 
change across the execution of a program. In this case, we show our first OpenGL routine, glClearColor(), 
which specifies the color that the window will show when it s cleared. NextNext, we set up the we set 
up the glutReshapeFunc()glutReshapeFunc() callback which we name callback, which we name reshape() (you 
ll see a pattern here .). In this case, we merely reconfigure the viewport based on the values the windowing 
system provide to us (which GLUT passes on for our use). We ll discuss the viewport and its importance 
later. Finally, we specify the glutReshapeFunc() callback display() hihi lld ll f d i Fhiil liti d 
which is were we ll do all of our drawing. For this simple application, we do to rendering operations: 
1. we clear the window using the OpenGL routine glClear(). You ll notice that it takes a parameter that 
specifies we want to clear the color buffer. We ll see that there are other buffers for our use in a 
bit. 2 finally we swap the buffers by calling glutSwapBuffers() WeWe ll talk ll talk 2. finally, we 
swap the buffers by calling glutSwapBuffers(). more about double-buffered rendering later as well.  
 The first example we demonstrated wasn t too visually captivating. Here we ll show what s required to 
actually draw something. This example shows loading object data, initializing the required shaders, and 
rendering. Every application leverages these steps just with more data and complex shaders. We ll discuss 
the specifics of the process in more detail as we proceed. Here we see one method of specifying the 
data we use to represent objects. We ll cover this method, and the others, in the next section. Here 
are examples of the types of shaders that OpenGL requires. The variable vShader contains the source to 
our vertex shader. This is about the simplest vertex shader you can have. Similarly, the variable fShader 
contains the source of our fragment shader which determines the color of the pixel. Here, we set the 
color of all the pixels to a constant color. We pass the shaders into a helper routine we ve written 
for the course named LoadProggram(), which takes a vertex shader and a fragment shader,, (),g and creates 
a GLSL program from them. We ll show the internals of LoadProgram() later, but assuming that there aren 
t errors in your shaders (they re really small programs that get compiled, and can have errors LoadProgram() 
will report them to you if there s a problem), it will create a usable shader program we ll use later. 
The last line retrieves information about a variable in (in this case the The last line retrieves information 
about a variable in (in this case, the variable vPos ) that we ll need later when it comes time render. 
For the curious, all variables labeled in in a vertex shader are assigned a location index that we need 
to initialize to pass data into. We ll see how we use those values when we draw on the next slide. Here 
is the entirety of our routine for drawing. As you ve seen, we first clear the window by calling glClear(). 
Next, we enable the shader program that we initialized in init()previously. After this, all data is processed 
by that program until we turn it off, or switch to using another shader program. Next, we tell OpenGL 
where to find the data for our object, and use the location value location value vPosvPos that we retrieved 
in init() to associate our data with that we retrieved in init() to associate our data with a variable 
in a shader. After making the association, we tell OpenGL that it should use the data for that connection. 
That might seem redundant, but it s really an option for flexibility. Depending on how you will want 
to draw something, you may find it simpler to load lots of data for an object, and switch between subsets 
of that data as you draw different versions of it. We ll show you how you might do that in just a bityou 
how you might do that in just a bit. We draw the object by calling glDrawArrays(), in this case specifying 
that we d like to draw triangles. It s just one method of drawing objects. We ll see other methods in 
the next section. Finally, we swap the buffers, just like you had seen previously. After the buffer swap, 
our completed image is shown in the screen. The vertex shader must output the vertex s position for 
the rasterizer. Here we use the special variable gl_Position. A more realistic vertex shader would also 
do a coordinate transformation since the input position would likely be in an application-defined coordinate 
system (e.g., model coordinates) while gl_Position must be in clip coordinates. In addition, most vertex 
shaders would also process other per-vertex attributes such as colors. OpenGL differentiates between 
pixels, which are the final pixel colors stored in the framebuffer, and fragments which are candidate 
pixels, but still need to have some more work done to them. A fragment s color is computed by a fragment 
shader. Inside of the fragment shader, you are provided with some generated information (like the fragment 
s framebuffer location as computed by the rasterizer), as well as any information yyou ppass into the 
shader througgh variables.  We describe geometric primitives by their vertices, which are either a single 
point, the endpoints of a line, or the corners of a triangle. A vertex is a set of four floating-point 
values, which we term a homogenous coordinate. The homogeneity relates to mathematics we use for processing 
the subject of a future slide. For a vertex, the x-, y-, and z-coordinates describe the vertex s location 
in 3D sppace with resppect to an origgin ((of yyour choosing)g) . The w-coordinate is generally just 
set to the value 1.0. It s role will become clearer a little farther on. We group vertices together to 
form our geometric primitives. OpenGL is a low-level rendering library, and as such, doesn t know how 
to draw too many things. In fact, it really only knows how to draw single points, line segments (which 
might be connected to one another), and triangles (again, which may share boundaries between two triangles). 
Above are the seven primitives that OpenGL knows how to render. These primitives specify how OpenGL should 
group your processed vertices to form a rendered shape. Your job is to use collections of these shapes 
to form geometric objects , ththroughh a process calledd modeling. ll dli For those of you with familiar 
with pervious versions of OpenGL, there were ten geometric primitives. OpenGL Version 3.1 removed support 
GL_QUADS, GL_QUAD_STRIP, and GL_POLYGONs, as they can easily be rendered using the above primitives (a 
quad is two triangles; a quad strip is just a triangle strip; and a polygon just a triangle fan). A 
vertex is a bundle of data that is collectively presented to you in a vertex shader. You determine the 
data in the bundle by specifying its vertex attributes. Attributes can be any type of data; floating-point 
or integer; scalar, vectors, or matrices; or even user-defined structures, and it s up to your vertex 
shader to process the data, and set up the values required for the fragment shader. Some vertex attributes 
have pparticular uses like sppecifyyingg the location of a vertex in space, or its color which are used 
in classic computer graphics computations, while other data are values that only have meaning to you, 
and you ll process in your shader for your own purposes. Vertices need to be stored in vertex arrays, 
which are just vectors of vertex data. They, like all data in OpenGL, are stored in buffers managed by 
OpenGL called buffer objects. You can think of buffer objects like dynamic memory allocated by the GPU 
it s a bucket of bytes that you dump data into, and then you tell OpenGL how to interpret those bytes 
as data values. Buffer objects specifically vertex buffer objects (commonly called VBOs) are used required 
for holding vertex attribute data. Creating and loading data into a VBO is a simple process of generating 
a unique buffer name, binding to the buffer, and loading the data. At this point, the data in the buffer 
hasn t been mapped to anything useful for OpenGL. We ll need to do a little extra work to get that to 
occur. There are numerous different typypes of buffers that we ll encounter. GL_VERTEX_ARRAY is one specifically 
for holding vertex attribute data. All of our sample programs will contain variations on this theme, 
depending on the types data that is required by the object. We need to tell OpenGL where to find the 
relevant data inside of a buffer object, and the routine glVertexAttribPointer() is used to provide the 
meta-data required for OpenGL. There are two ways to organize vertex attribute data: as contiguous arrays 
of attributes (commonly called a structure of arrays ), or as an array of bundles of vertex data (the 
array of structures method). Here we demonstrate the contigguous array method, providingg three sets 
of y ,p data for each vertex: v , c , and tc . Each call to glVertexAttribPointer() provides information 
about the number of components (e.g., three for v ), the associated type for each element, whether integer 
values should be mapped into a floating-point value in the range [0, 1], the number of bytes between 
successive elements (here 0 is a special value indicating that the data is tightly packed, and normal 
pointer incrementing [as you would do in C programs, for example] is sufficient). The program color-draw-arrays.c 
demonstrates setting up vertex attributes in this manner. By comparison to the previous slide demonstrating 
contiguous arrays of data, here we tell OpenGL that all of the vertex attributes for a vertex are contiguous 
in memory the array of structures approach. In this case, the only differences in our glVertexAttribPointer() 
calls is that the distance to find successive elements is the size of the structure , and the differing 
offsets required to tell OpenGL where in the buffer to find the initial data values. Generally speaking, 
this method may provide better performance than the previous method due to memory caching in modern computer 
systems. The program AOS-color-draw-arrays.c (where the AOS is a mnemonic for Arrays-of-structures) demonstrates 
storing data in this manner. The final step in preparing you data for processing by OpenGL (i.e., sending 
it down for rendering) is to specify which vertex attributes you d like issued to the graphics pipeline. 
While this might seem superfluous, it allows you to specify multiple collections of data, and choose 
which ones you d like to use at any given time. Each of the attributes that we enable must be associated 
with an in variable of the currentlyy bound vertex shader. As yyou migght recall from our triangle.cxx 
example, each of those vertex attribute locations was retrieved from the compiled shader by calling glGetAttribLocation(). 
We discuss this call in the shader section. In order to initiate the rendering of primitives, you need 
to issue a drawing routine. While there are many routines for this in OpenGL, we ll discuss the most 
fundamental ones. The simplest routine is glDrawArrays(), to which you specify what type of graphics 
primitive you want to draw (e.g., here we re rending a triangle strip), which vertex in the enabled vertex 
attribute arrays to start with, and how many vertices to send. This is the simpplest wayy of renderingg 
g geometryy in OppenGL Version 3.1. You merely need to store you vertex data in sequence, and then glDrawArrays() 
takes care of the rest. However, in some cases, this won t be the most memory efficient method of doing 
things. Many geometric objects share vertices between geometric primitives, and with this method, you 
need to replicate the data once for each vertex. We ll see a more flexible, in terms of memory storage 
and access in the next slides. The programs color-draw-arrays.cxx and AOS-color-draw­arrays.cxx demonstrate 
using glDrawArrays() with the different methods of storing data in vertex attribute arrays. The other 
very common way to rendering geometry is by using indexed primitives, where access to vertices is done 
by providing a list of indices into the array of vertex values. The routine glDrawElements() does this. 
As you can see, it takes which primitive type to render, and a count of how many vertices should be rendered, 
just like glDrawArrays(). However, you also pass in an array of values representing the vertex indices 
to be processed, including a token describing the type of that array (GL_UNSIGNED_BYTE, GL UNSIGNED SHORT 
GL UNSIGNED INT d)GL_UNSIGNED_SHORT, GL_UNSIGNED_INT are accepted). The program draw-elements.cxx demonstrates 
using glDrawElements() with multiple vertex attribute streams.  Vertex data (positions, colors, and 
texture coordinates, for example) are loaded onto GPU. When application issues a drawing command, each 
vertex generates an execution of the vertex shader. The vertex shader outputs a position and other per 
vertex variables that will be interpolated by the rasterizer. Uniform variables are set by the application 
and remain unchanged through Uniform variables are set by the application and remain unchanged through 
an execution of glDrawElements(). Each fragment generated by the rasterizer starts the execution of 
a fragments shader. Vertex attributes from the vertex shader are interpolated by the rasterizer to provide 
per-fragment values. For example, we can set a texture coordinate for each vertex in the application 
and put these data into vertex arrays. The vertex processor can process these values (or just pass them 
on) and output them as input to the rasterizer which will interpolate a value for each fragment. Specifications 
are at: http://www.khronos.org/registry/ ( http://www.opengl.org/registry/ and http://www.khronos.org/registry/gles/ 
) Here we some of the basic types and qualifiers used in GLSL. Since GLSL is very close to C (and C++ 
in some instances), we don t dwell on the language constructs. Compound types, like vectors and matrices, 
require construction. They re not arrays like in C they re more like classes in C++. Both for creating 
one of these types, as well as converting between them (e.g., a collection of vectors can be used to 
initialize a matrix) is required in GLSL. In addition to being able to use vectors like arrays in C 
(i.e., index into the using a []-type construction) , alternate access methods similar to accessing fields 
in a structure are available. The multiple forms enhance readability of code. Use rgba for color, xyxw 
for positions, stpg for textures but there is no semantic distinctions; you could just as easily use 
xyzw for colors as the rgba swizzlers. Note that operations are overloaded so both vA and Av are valid 
but the results are different. Overloading, restriction is that function prototypes are at global (or 
outside global) scope. In earlier versions of OpenGL, almost all OpenGL state variables were available 
to shaders as built in variables. However most have been deprecated. radians, degrees, sin, cos, tan, 
asin, acos, atan, atan pow, exp2, log2, sqrt, inversesqrt abs, sign, floor, ceil, fract, mod, min, max, 
clamp mix, step, smoothstep length, distance, dot, cross, normalize, faceforward, reflect lessThan, lessThanEqual, 
greaterThan, greaterThanEqual, equal, notEqual, any, all Since we have yet to cover transformations 
and shading, we ll specify vertices in clip coordinates. If all the vertices are in cube centered the 
origin with corners at (-1,-1,-1) and (1,1,1), none of the geometry will be clipped out. In this example 
we will place all the vertices in the plane z = 0 and color all fragments with the same color. Once 
the vertex array is created as part of initialization the application simply redraws the array as fast 
as possible through the idle callback. Note that this example is not very efficient since we are sending 
the same data to the GPU each time rather than storing it there as a vertex buffer object. GLUT_ELAPSED_TIME 
is the elapsed time in milliseconds. It is put into an application variable timeParam which is aligned 
with the time variable in the shader. We will see the details in a few slides. MVP is the model-view 
projection matrix which converts from vertex positions from model coordinates to clip coordinates. In 
this example it is computed in the application. In general, we would compute colors for each fragment 
using a lighting model. Colors are specified in RGBA space where A is the opacity.  const GLchar* vSource[] 
= { in vec4 vPos; in vec4 vPos;"uniform float time; "void main() "{ " gl_Position.x = cos(time)*vPos.x-sin(time)*vPos.y; 
" gl_Position.y = sin(time)*vPos.x+cos(time)*vPos.y; " gl_Position.zw = vPos.zw; "}"}; const GLchar* 
fSource[] = { out vec4 fragColor; "void main() "{"" fragColor = vec4( 1.0, 0.0, 0.0, 1.0 ); "} " }; GLuintvShader, 
fShader; /* create program and shader objects */ vShader = glCreateShader(GL_VERTEX_SHADER);fShader = 
glCreateShader(GL_FRAGMENT_SHADER); glCompileShader(vShader); glCompileShader(fShader); /* check for 
errors here */    As you ve seen us use multiple times, when we need to initialize data for a shader, 
we first have to find that variables associated index so that we can load data into its location. Given 
the two types of variables we need to initialize vertex attributes and uniforms each of the respective 
commands above retrieve the index for us. Using these routines require the that the shader has been linked, 
as that s when OpenGL assigns indices to variables. GLint timeParam = glGetUniformLocation(program, 
"time"); /* time in vertex shader */ GLint loc = glGetAttribLocation(program, "vPosition"); /* vertex 
locations in vertex shader */ glEnableVertexAttribArray(loc); glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, 
0,0); In draw() callback we send the time by glUniform1f(timeParam, 0.001*glutGet(GLUT_ELAPSED_TIME)); 
The vertex data is in the buffer object /* Triangle Data Triangle Data *// / GLfloat vVertices[][3] = 
{ {0.0f, -0.5f, 0.0f}, {0.5f, 0.0f, 0.0f}, {0.0f, 0.5f, 0.0f} }; 56 GLubyte indices[] = {0, 1, 2}; 
 Transformations are fundamental to computer graphics, and OpenGL leverages them quite heavily. The most 
common use is for manipulating the position and orientation of 3D objects, but they also find applications 
in modifying colors, and other vector quantities used in graphics. For OpenGL, transformations are represented 
by 4 × 4 matrices. These matrices are applied in shaders, and can either be static (e.g., hard coded 
in a shader s source code), or loaded dynamically by the application into a shader variable. OpenGL 
matrices are generally considered to be column-major, which causes some trepidation for C programmers, 
as it s exactly the opposite indexing of what we re all used to. There are multiple solutions to this 
issue: You can orient your values in your code to match what OpenGL requires  Most routines that load 
matrix values in OpenGL have a transpose parameter to say that the data should be transposed when loaded 
 You can transpose matrices in your shader using the transpose() method  Finally, you can specify that 
a matrix is row_major in the shader, however, this causes the matrix to truly be considered row major, 
and not merely transposed, and as such you need to reverse the order of the multiplicands in your vector-matrix 
and matrix-matrix multiplications.  OpenGL uses a synthetic camera model for specifying how a set of 
3D objects are viewed. The model effectively uses three conceptual parts, each of which is represented 
by a matrix: The projection transformation which specifies how much of the world is visible. It s like 
setting the field-of-view on a camera lens  The viewing transformation which controls the placement 
and orientation of the camera in the world orientation of the camera in the world  Finally, modeling 
transformations are used to control the placement and orientation of objects in the world  The pprocessingg 
reqquired for convertingg a vertex from 3D sppace into a 2D window coordinate is done by the transform 
stage of the graphics pipeline. The operations in that stage are illustrated above. The purple boxes 
represent a matrix multiplication operation. In graphics, all of our matrices are 4×4 matrices (they 
re homogenous, hence the reason for homogenous coordinates). When we want to draw an geometric object 
like a chair for instance we When we want to draw an geometric object, like a chair for instance, we 
first determine all of the vertices that we want to associate with the chair. Next, we determine how 
those vertices should be grouped to form geometric primitives, and the order we re going to send them 
to the graphics subsystem. This process is called modeling. Quite often, we ll model an object in its 
own little 3D coordinate system. When we want to add that object into the scene we re developing, we 
need to determine its world coordinates. We do this byy pg specifying a modeling transformation, which 
tells the system how to move from one coordinate system to another. Modeling transformations, in combination 
with viewing transforms, which dictate where the viewing frustum is in world coordinates, are the first 
transformation that a vertex goes through. Next, the projection transform is applied which maps the vertex 
into another space called clip coordinates, applied which maps the vertex into another space called clip 
coordinates, which is where clipping occurs. After clipping, we divide by the w value of the vertex, 
which is modified by projection. This division operation is what allows the farther-objects-being-smaller 
activity. The transformed, clipped coordinates are then mapped into the window. Another essential part 
of the graphics processing is setting up how much of Another essential part of the graphics processing 
is setting up how much of the world we can see. We construct a viewing frustum, which defines the chunk 
of 3-space that we can see. There are two types of views: a perspective view, which you re familiar with 
as it s how your eye works, is used to generate frames that match your view of reality things farther 
from your appear smaller. This is the type of view used for video games, simulations, and most graphics 
applications in general. The other view, orthographic, is used principally for engineering and design 
situations, where relative lengths and angles need to be preserved. For a perspective, we locate the 
eye at the apex of the frustum pyramid. We can see any objects which are between the two planes perpendicular 
to eye (they re called the near and far clipping planes, respectively). Any vertices between near and 
far, and inside the four planes that connect them will be rendered Otherwise InIn some somerendered. 
those vertices are clipped out and discarded Otherwise, those vertices are clipped out and discarded. 
cases a primitive will be entirely outside of the view, and the system will discard it for that frame. 
Other primitives might intersect the frustum, which we clip such that the part of them that s outside 
is discarded and we create new vertices for the modified primitive. While the system can easily determine 
which primitive are inside the frustum, it s wasteful of system bandwidth to have lots of primitives 
discarded in this manner. We utilize a technique named culling to determine exactly which primitives 
need to be sent to the graphics processor, and send only those primitives to maximize its efficiency. 
 Viewing frusta are usually specified by six parameters, which are combined to set the six clipping planes 
of the viewing frustum. OpenGL sets up the viewing frustum in eye coordinates, where the eye is located 
at the origin, and looking down the negative z-axis. From that the clipping planes of the frustum are 
configured. There are two types of projections, as we ll see on the following page. There are two types 
of projections that are generally used in computer graphics: orthographic projections which are generally 
used in science and engineering applications those that require angels to be preserved during rendering 
perspective projections which mimic how the eye works objects seem to decrease in size the farther from 
the viewer that are decrease in size the farther from the viewer that are Here we show how the matrices 
are constructed from the six values we described on the previous page: the near and far clipping planes; 
and the left, right, top, and bottom values used to compute the planes for the types of projections. 
In either case, the near and far clipping planes represent the front (where the imaging plane, or where 
the image is projected to) and b back k clilippiing planes llocated allong ththe liline of siight. Th 
th l l d f ht The other values are used to compute the box enclosing the viewable region. For orthographic 
projections, this really defined a parallelepiped in 3D, where for perspective projections, it forms 
a pyramid with the eye located at the apex (which is coincident with the eye-space coordinate space origin). 
Don t fret about constructing those matrices, we have helper routines that take care of making them for 
you The viewport is the part of the window you can draw into. Generally, it s a subset of the pixels 
inside of the window, and is controlled by calling the glViewport() routine, which takes the lower-left 
corner in window coordinates, and the width and height of the viewport in pixels. Any time you find a 
width and height in OpenGL, you should also consider their ratio, usually termed the aspect ratio, and 
is the ratio of the width to the heigght. In order have ggeometric objjects look rigght like havinggqsquares 
look square and sphere look round and not oblong various aspect ratios need to match. In particular, 
the aspect ratio of the viewport should match the viewport of the projection transformation. A common 
way to navigate a scene is as if you re flying. The LookAt() routine is one that we ve implemented based 
on a routine named gluLookAt() from OpenGL s old utility library. It takes three sets of parameters: 
where your eye is (eye*); a point you re looking towards (look*), and a vector in the local coordinate 
system representing which direction is up in the scene. Please see our Transform.h file for implementation 
details. Here we create an ortho-normal basis (a fancy name for three vectors that are perpendicular 
to each other, and each have unit length). Those three vectors form the necessary rotation to orientate 
our world scene to eye coordinates. The final operation in completing the LookAt() function s transform 
is to translate to the eye s position . Please see the implementation in Transform.h for more details. 
   We know all that matrix math could be intimidating if you re not too familiar with linear algebra. 
We ve written a header file, Transform.h, that takes implements all of those routines, manages the row 
and column ordering of the matrices, and makes it very easy to integrate them with your shaders. For 
example, you might want to use a perspective projection matrix in shader. Here s how you d do that with 
our code (this assumes your shader has a variable named P for the projection matrix)): pj projIndex = 
glGetUniformLocation( program, P ); Matrix p = Perspective( 120.0, aspectRatio, zNear, zFar ); glUniformMatrix4fv( 
projIndex, 1, /* transpose matrix? */ GL_FALSE, p ); Double buffer is a technique for tricking the 
eye into seeing smooth animation of rendered scenes. The color buffer is usually divided into two equal 
halves, called the front buffer and the back buffer. The front buffer is displayed while the application 
renders into the back buffer. When the application completes rendering to the back buffer, it requests 
the graphics display hardware to swap the roles of the buffers,, causingg the back buffer to now be dispplayyed,, 
and the pprevious front buffer to become the new back buffer. Requesting double buffering in GLUT is 
simple. Adding GLUT_DOUBLE to your glutInitDisplayMode() call will cause your window to be double buffered. 
When your application is finished rendering its current frame, and wants to swap the front and back buffers, 
the glutSwapBuffers()call will request the windowing system to update the window s color buffers. Depth 
buffering is a technique to determine which primitives in your scene are occluded by other primitives. 
As each pixel in a primitive is rasterized, its distance from the eyepoint (depth value), is compared 
with the values stored in the depth buffer. If the pixel s depth value is less than the stored value, 
the pixel s depth value is written to the depth buffer, and its color is written to the color buffer. 
The deppth buffer alggorithm is: if ( pixel->z < depthBuffer(x,y)->z ) { depthBuffer(x,y)->z = pixel->z; 
colorBuffer(x,y)->color = pixel->color; } OpenGL depth values range from [0 0 1 0] with 1 0 being OpenGL 
depth values range from [0.0, 1.0], with 1.0 being essentially infinitely far from the eyepoint. Generally, 
the depth buffer is cleared to 1.0 at the start of a frame. Enabling depth testing in OpenGL is very 
straightforward. A depth buffer must be requested for your window, once again using the glutInitDisplayMode(), 
and the GLUT_DEPTH bit. Once the window is created, the depth test is enabled using glEnable( GL_DEPTH_TEST 
).  Lighting is an important technique in computer graphics. Without lighting, objects tend to look 
like they are made out of plastic. OpenGL divides lighting into three parts: material properties, light 
properties and global lighting parameters. OpenGL can use the shade at one vertex to shade an entire 
polygon (constant shading) or interpolated the shades at the vertices across the polygon (smooth shading), 
the default. The orientation of a surface is specified by the normal at each point. For a flat polygon 
the normal is constant over the polygon. Because normals are specified by the application program and 
can be changed between the specification of vertices, when we shade a polygon it can appear to be curved. 
 Classically, OpenGL lighting was based on the Phong lighting model. At each vertex in the primitive, 
a color is computed using that primitives material properties along with the light settings. The color 
for the vertex is computed by adding four computed colors for the final vertex color. The four contributors 
to the vertex color are: Ambient is color of the object from all the undirected light in a scene.  
Diffuse is the base color of the object under current lighting. There must be a light shining on the 
object to get a diffuse contribution.  Specular is the contribution of the shiny highlights on the object. 
 Emission is the contribution added in if the object emits light  Emission is the contribution added 
in if the object emits light (i.e., glows)  Since we need to do this in shaders, we ll discuss this 
model as it provides a nice description, but please realize there are more accurate lighting models available 
you could implement in your shaders. The lighting normal tells OpenGL how the object reflects light 
around a vertex. If you imagine that there is a small mirror at the vertex, the lighting normal describes 
how the mirror is oriented, and consequently how light is reflected. Usually, lighting normals are provided 
with a vertex (they might also be generated in the vertex shader, like for algebraic shapes, for exampp)le). 
The mathematics of lighting also rely on the fact it s a normalized vector, which has a length of one, 
to generate correct results. If your normals don t have unit length being passed into a shader, you can 
use the normalize() routine within your shader to make the vector have unit length. Material properties 
describe the color and surface properties of a material (dull, shiny, etc.). OpenGL supports material 
properties for both the front and back of objects, as described by their vertex winding. Generally, 
lights have a matching set of color properties to the material properties, i.e., diffuse, specular, etc. 
components. In addition and more importantly lights have positions and types. The classic OpenGL lighting 
model includes two types of lights: directional and point lights. Directional lights are like the sun, 
where all the light rays are parallel and coming from the same direction. Conversely, point lights are 
like light bulbs, they emit light spherically from a point in space. This tutorial is based on fixed-function 
OpenGL, but provides a useful and descriptive context for discussing lighting computations in OpenGL. 
With shaders, all of the computation that s done for lighting must be done in the shader, including specifying 
variables for the different types of properties and such.  Textures are images that can be pasted on 
geometry and can be one, two, or three dimensional. By convention, the coordinates of the image are s, 
t, r and q. Thus for the two dimensional image above, a point in the image is given by its (s, t) values 
with (0, 0) in the lower-left corner and (1, 1) in the top-right corner. A texture map for a two-dimensional 
geometric object in (x, y, z) world coordinates mapps a ppoint in ((s,, t) space to a corresppondingg 
p point )p on the screen. In the simplest approach, we must perform these three steps. Textures reside 
in texture memory. When we assign an image to a texture it is copied from processor memory to texture 
memory where pixels are formatted differently. Texture coordinates are actually part of the state as 
are other vertex attributes such as color and normals. As with colors, OpenGL interpolates texture inside 
geometric objectsinterpolates texture inside geometric objects. Because textures are really discrete 
and of limited extent, texture mapping is subject to aliasing errors that can be controlled through filtering. 
 The first step in creating texture objects is to have OpenGL reserve some indices for your objects. 
glGenTextures() will request n texture ids and return those values back to you in texIds. To begin defining 
a texture object, you call glBindTexture() with the id of the object you want to create. The target is 
one of GL_TEXTURE_{123}D(). All texturing calls become part of the object until the next gglBindTexture() 
is called. () To have OpenGL use a particular texture object, call glBindTexture() with the target and 
id of the object you want to be active. To delete texture objects, use glDeleteTextures().  Specifying 
the texels for a texture is done using the glTexImage{123}D() call. This will transfer the texels in 
CPU memory to OpenGL, where they will be processed and converted into an internal format. The level parameter 
is used for defining how OpenGL should use this image when mapping texels to pixels. Generally, you ll 
set the level to 0,, unless you are usingg a texturing techniqque called mippmapppping, yg g, which we 
will discuss in the next section. When you want to map a texture onto a geometric primitive, you need 
to provide texture coordinates. Just like a vertex s positional data, or lighting normal, texture coordinates 
are just another (or part of a set of) vertex attributes. Valid texture coordinates are between 0 and 
1, for each texture dimension, and the default texture coordinate is (0, 0, 0, 1). Like our other tutorials, 
this one reflects using the older fixed­function methods of doing texture mapping. However the concepts 
are still relevant and hopefully illustrative. During rendering, you can sample multiple textures and 
apply them to a single geometric object. This is a handy technique as often, you ll have a texture map 
for the surfaces appearance, another one for controlling the surfaces roughness (these are commonly called 
gloss maps think of being a map of where a surface has shown wear or is still shiny and reflective), 
still more for light maps, or transparency maps. Each texture map needs to be associated with its own 
texture unit, which is capable of storing a single (perhaps mipmapped) texture. To switch between different 
texture units, call glActiveTexture()passing the token GL_TEXTUREn, where n runs form zero to one less 
than the number of supported texture units. Inside of your shader, you ll access a particular texture 
unit using a sampler. Samplers are uniform variables, whose type must match the type of texture bound 
to that texture unit. The available types of texture samplers are: sampler1D, sampler2D, sampler3D, samplerCube 
(and a few more advanced versions). When you load a texture, the call and type of texture (e.g., calling 
glTexture2D() with GL_TEXTURE_2D) specifies which type of sampler is capable of accessing that texture. 
In this case, you d use a sampler2D to access the texture. The final issue is that you need to associate 
the texture unit s number (the n you specified when calling glActiveTexture()) with the sampler. Being 
a uniform, you do that by loading a uniform variable with the texture unit s number. For example, in 
the shader, say we declare uniform sampler2D tex; . If we ve loaded a two-dimensional texture into texture 
unit four, we d need to do the following: GLuint texLoc = glGetUniformLocation( program, tex ); glUniform1i( 
texLoc, 4 ); glUniform1i( texLoc, 4 ); After doing that, we can access the texture from our shader. 
From within a shader, to access a texture you need a suitable sampler and the appropriate texture coordinates. 
With those elements, it s as simple as calling texture2D() (or the appropriate call to match the type 
of texture stored in the texture unit). The value returned will be a four-component color that s been 
processed by the texture retrieval system (including filtering and wrapping the texture). Textures and 
the objects being textured are rarely the same size ( in pixels ). Filter modes determine the methods 
used by how texels should be expanded ( magnification ), or shrunk ( minification ) to match a pixel 
s size. An additional technique, called mipmapping is a special instance of a minification filter. Wrapp 
modes determine how to pprocess texture coordinates outside of the [0,1] range. The available modes are: 
GL_CLAMP -clamp any values outside the range to closest valid value, causing the edges of the texture 
to be smeared across the primitive GL_REPEAT -use onlth ly the ffracti tionall partt of th f the ttextture 
coordinatte, di causing the texture to repeat across an object. Filter modes control how pixels are 
minified or magnified. Generally a color is computed using the nearest texel or by a linear average of 
several texels. The filter type, above is one of GL_TEXTURE_MIN_FILTER or GL_TEXTURE_MAG_FILTER. The 
mode is one of GL_NEAREST, GL_LINEAR, or special modes for mipmapping Mipmapping modes are used for minification 
only Mipmapping modes are used for minification only, and for mipmapping. and can have values of: GL_NEAREST_MIPMAP_NEAREST 
GL_NEAREST_MIPMAP_LINEAR GL_LINEAR_MIPMAP_NEAREST GL LINEAR MIPMAP LINEAR GL_LINEAR_MIPMAP_LINEAR Full 
coverage of mipmap texture filters is outside the scope of this course. As primitives become smaller 
in screen space, a texture may appear to shimmer as the minification filters creates rougher approximations. 
Mipmapping is an attempt to reduce the shimmer effect by creating several approximations of the original 
image at lower resolutions. Each mipmap level should have an image which is one-half the height and width 
of the previous level, to a minimum of one texel in either dimension. For example, level 0 could be 32 
x 8 texels. Then p, level 1 would be 16 x 4; level 2 would be 8 x 2; level 3, 4 x 1; level 4, 2 x 1, 
and finally, level 5, 1 x 1. Wrap mode determines what should happen if a texture coordinate lies outside 
of the [0,1] range. If the GL_REPEAT wrap mode is used, for texture coordinate values less than zero 
or greater than one, the integer is ignored and only the fractional value is used. If the GL_CLAMP wrap 
mode is used, the texture value at the extreme (either 0 or 1) is used.   This is the shader from 
are earlier example with the addition of the modelviewprojection matrix to allow for viewing and transformation 
from model coordinates. Shading is left to the fragment shader. To achieve this output all the fragment 
shader can be as simple as our trivial example that colors each fragment with the same color. void main() 
{ gl_FragColor = vec4 ( 0.0, 0.0, 0.0, 1.0 ); } } Such a simple coloring will not be adequate if we 
are to fill the polygons. If we don t add lighting, we will see a solid black mesh and won t be able 
to see shape. Details of lighting model are not important to here. The model includes the standard modified 
Phong diffuse and specular terms without distance. Note that we do the lighting in eye coordinates and 
therefore must compute the eye position in this frame. All the light and material properties are set 
in the application and sent to the shader. time: same as in previous example modelViewProjection matrix: 
product of modelview and projection matrices modelViewMatrix: need to convert normal to eye coordinates 
normalMatrx: inverse transpose of the upper left 3 x 3 part of the model normalMatrx: inverse transpose 
of the upper left 3 x 3 part of the model view matrix needed to preserve angle between normal and light 
source when going to object coordinates lightSourcePosition: in eye coordinates specularLightProduct, 
diffuseLightProduct: vectors of product of component of light sources and material reflectivity mNormal: 
normal in object coordinates Output position computed as before Computation of shades using Blinn-Phong 
model without ambient term and distance terms  In this example, we use some of the standard diffuse 
computation to find the cosine of the angle between the light vector and the normal vector. Its value 
determines whether we color with red or yellow. A silhouette edge is computed as in the previous example. 
 The idea is that if the angle between the eye vector and the surface normal is small, we are near an 
edge. But we do even better if we use a fragment shader  Basic job of the vertex shader is to: Compute 
position in clip coordinates Pass normal, light and eye vectors to the rasterizer   The rasterizer 
interpolates both the texture coordinates and reflection vector to get the respective values for the 
fragment shader. Note that all the texture definitions and parameters are in the application program. 
 Details are a little complex Need lighting model Usually do computations in a local frame that changes 
for each fragment Put code in an appendix Single rectangle with moving light source. Bump map is derived 
from a texture map with which is a step function.   The OpenGL Programming Guide often referred to 
as the Red Book due to the color of its cover discusses all aspects of OpenGL programming, discussing 
all of the features of OpenGL in detail. Mark Kilgard s OpenGL Programming for the X Window System, is 
the Green Book , and Ron Fosner s OpenGL Programming for Microsoft Windows, which has a white cover is 
sometimes called the Alppha Book. The OppenGL Shadingg Langguagge,,yby Randi Rost,, Barthold Litchenbelt, 
and John Kessenich, is the Orange Book. All of the OpenGL programming series books, along with Interactive 
Computer Graphics: A top-down approach with OpenGL, OpenGL: A Primer, and OpenGL Distilled are published 
by Addison Wesley Publishers. SIGGRAPH 2009 An Introduction to Shader-Based OpenGL Programming  Program Listing  
triangle.cxx ////////////////////////////////////////////////////////////////////////////// // // triangle.cxx -draw 
a single triangle in normalized-device coordinates // #include <stdlib.h> #include <GL/glew.h> #ifdef 
MACOSX #include <GLUT/glut.h> #else #include <GL/freeglut.h> #endif #include "LoadProgram.h" #define 
BUFFER_OFFSET( offset ) ((GLvoid*) offset) GLuint buffer; GLuint vPos; GLuint program; //---------------------------------------------------------------------------­ 
void init() { // // ---Load vertex data --­// GLfloat vertices[][4] = { { -0.75, -0.5, 0.0, 1.0 }, { 
0.75, -0.5, 0.0, 1.0 }, { 0.0, 0.75, 0.0, 1.0 } }; glGenBuffers( 1, &#38;buffer ); glBindBuffer( GL_ARRAY_BUFFER, 
buffer ); glBufferData( GL_ARRAY_BUFFER, sizeof(vertices), vertices, GL_STATIC_DRAW ); // // ---Load 
shaders --­// #if GLSL_VERSION == 130 const char* vShader = { "#version 130\n" "" "in vec4 vPos;" "" 
"void main() {" " gl_Position = vPos;" "}" }; const char* fShader = { "#version 130\n" 60 "" "out vec4 
fColor;" "" "void main() {" " fColor = vec4( 1, 1, 0, 1 );" 65 "}" }; #else const char* vShader = { 
"attribute vec4 vPos;" 70 "" "void main() {" " gl_Position = vPos;" "}" }; 75 const char* fShader = 
{ "void main() {" " gl_FragColor = vec4( 1, 1, 0, 1 );" "}" 80 }; #endif // SHADER_VERSION == 130 program 
= LoadProgram( vShader, fShader ); 85 vPos = glGetAttribLocation( program, "vPos" ); glClearColor( 0.0, 
0.0, 1.0, 1.0 ); } 90 //---------------------------------------------------------------------------­ 
void display() 95 { glClear( GL_COLOR_BUFFER_BIT ); glUseProgram( program ); 100 glBindBuffer( GL_ARRAY_BUFFER, 
buffer ); glVertexAttribPointer( vPos, 4, GL_FLOAT, GL_FALSE, 0, BUFFER_OFFSET(0) ); glEnableVertexAttribArray( 
vPos ); glDrawArrays( GL_TRIANGLES, 0, 3 ); 105 glutSwapBuffers(); } //---------------------------------------------------------------------------­ 
110 void reshape( int width, int height ) { glViewport( 0, 0, width, height ); } 115 //---------------------------------------------------------------------------­ 
void keyboard( unsigned char key, int x, int y ) 120 { switch( key ) { case 033: // Escape Key exit( 
EXIT_SUCCESS ); break; 125 } glutPostRedisplay(); } 130 //---------------------------------------------------------------------------­ 
int main( int argc, char* argv[] ) { 135 glutInit( &#38;argc, argv ); glutInitDisplayMode( GLUT_RGBA 
| GLUT_DOUBLE ); // glutInitContextVersion( 3, 0 ); glutCreateWindow( argv[0] ); 140 glewInit(); init(); 
glutDisplayFunc( display ); 145 glutReshapeFunc( reshape ); glutKeyboardFunc( keyboard ); glutMainLoop(); 
} 150 color­draw­arrays.cxx ////////////////////////////////////////////////////////////////////////////// 
// // color-draw-arrays.cxx -draw a cube using glDrawArrays() with two // vertex attribute arrays enabled: 
one for positions, and the other // for colors. // #include <stdlib.h> #include <GL/glew.h> #ifdef MACOSX 
#include <GLUT/glut.h> #else #include <GL/freeglut.h> #endif #include "LoadProgram.h" #define BUFFER_OFFSET( 
offset ) ((GLvoid*) offset) GLuint vbo; GLuint program; GLuint vPos, vColor; GLintptr colorOffset; 
 //---------------------------------------------------------------------------­ void init() { GLfloat 
vertices[] = { 1.0, 0.0, 0.0, /* Index 4 */ 1.0, 0.0, 1.0, /* Index 5 */ 1.0, 1.0, 1.0, /* Index 7 */ 
 1.0, 0.0, 0.0, /* Index 4 */ 1.0, 1.0, 1.0, /* Index 7 */ 1.0, 1.0, 0.0, /* Index 6 */ 0.0, 0.0, 0.0, 
/* Index 0 */ 0.0, 1.0, 0.0, /* Index 2 */ 0.0, 1.0, 1.0, /* Index 3 */ 0.0, 0.0, 0.0, /* Index 0 */ 
0.0, 1.0, 1.0, /* Index 3 */ 0.0, 0.0, 1.0, /* Index 1 */ 0.0, 1.0, 0.0, /* Index 2 */ 1.0, 1.0, 0.0, 
/* Index 6 */ 1.0, 1.0, 1.0, /* Index 7 */ 0.0, 1.0, 0.0, /* Index 2 */ 1.0, 1.0, 1.0, /* Index 7 */ 
0.0, 1.0, 1.0, /* Index 3 */ 0.0, 0.0, 0.0, /* Index 0 */ 0.0, 0.0, 1.0, /* Index 1 */ 60 1.0, 0.0, 
1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, /* Index 5 */ /* Index 0 */ /* Index 5 */ /* Index 
4 */ 65 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, /* Index 0 */ /* Index 4 */ /* Index 6 */ 70 0.0, 
0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, /* Index 0 */ /* Index 6 */ /* Index 2 */ 75 0.0, 0.0, 1.0, 0.0, 
1.0, 1.0, 1.0, 1.0, 1.0, /* Index 1 */ /* Index 3 */ /* Index 7 */ 80 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 
0.0, 1.0, }; /* Index 1 */ /* Index 7 */ /* Index 5 */ 85 GLfloat colors[] = { 1.0, 0.0, 0.0, /* Index 
4 */ 1.0, 0.0, 1.0, /* Index 5 */ 1.0, 1.0, 1.0, /* Index 7 */ 90 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 
1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, /* Index 4 */ /* Index 7 */ /* Index 6 */ /* Index 
0 */ /* Index 2 */ /* Index 3 */ 95 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, /* Index 0 */ /* Index 
3 */ /* Index 1 */ 100 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, /* Index 2 */ /* Index 6 */ /* Index 
7 */ 105 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, /* Index 2 */ /* Index 7 */ /* Index 3 */ 110 0.0, 
0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, /* Index 0 */ /* 
Index 1 */ /* Index 5 */ /* Index 0 */ /* Index 5 */ /* Index 4 */ 115 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 
1.0, 1.0, 0.0, /* Index 0 */ /* Index 4 */ /* Index 6 */ 0.0, 0.0, 0.0, /* Index 0 */ 120 1.0, 1.0, 
0.0, /* Index 6 */ 0.0, 1.0, 0.0, /* Index 2 */ 0.0, 0.0, 1.0, /* Index 1 */ 0.0, 1.0, 1.0, /* Index 
3 */ 125 1.0, 1.0, 1.0, /* Index 7 */ 0.0, 0.0, 1.0, /* Index 1 */ 1.0, 1.0, 1.0, /* Index 7 */ 1.0, 
0.0, 1.0, /* Index 5 */ 130 }; // // ---Load vertex data --­// 135 glGenBuffers( 1, &#38;vbo ); glBindBuffer( 
GL_ARRAY_BUFFER, vbo ); glBufferData( GL_ARRAY_BUFFER, sizeof(vertices) + sizeof(colors), NULL, GL_STATIC_DRAW 
); 140 colorOffset = sizeof(vertices ); glBufferSubData( GL_ARRAY_BUFFER, 0, sizeof(vertices), vertices 
); glBufferSubData( GL_ARRAY_BUFFER, colorOffset, sizeof(colors), colors ); 145 // // ---Load shaders 
--­// 150 #if SHADER_VERSION == 130 const char* vShader = { "#version 130\n" "" "in vec3 vPos;" 155 
"in vec3 vColor;" "out vec4 oColor;" "" "void main() {" " oColor = vec4( vColor, 1 );" 160 " gl_Position 
= vec4( vPos, 1 );" "}" }; const char* fShader = { 165 "#version 130\n" "" "in vec4 color;" "out vec4 
fColor;" "" 170 "void main() {" " fColor = color;" "}" }; #else 175 const char* vShader = { "attribute 
vec3 vPos;" "attribute vec3 vColor;" "varying vec4 color;" "" "void main() {" " color = vec4( vColor, 
1 );" " gl_Position = vec4( vPos, 1 );" "}" }; 185 const char* fShader = { "varying vec4 color;" "" 
"void main() {" 190 " gl_FragColor = color;" "}" }; #endif // SHADER_VERSION == 130 195 program = LoadProgram( 
vShader, fShader ); vPos = glGetAttribLocation( program, "vPos" ); vColor = glGetAttribLocation( program, 
"vColor" ); 200 glClearColor( 0.0, 0.0, 1.0, 1.0 ); } //---------------------------------------------------------------------------­ 
205 void display() { glClear( GL_COLOR_BUFFER_BIT ); 210 glUseProgram( program ); glColor3f( 1, 1, 1 
); glBindBuffer( GL_ARRAY_BUFFER, vbo ); 215 glVertexAttribPointer( vPos, 3, GL_FLOAT, GL_FALSE, 0, BUFFER_OFFSET(0) 
); glVertexAttribPointer( vColor, 3, GL_FLOAT, GL_FALSE, 0, BUFFER_OFFSET(colorOffset) ); glEnableVertexAttribArray( 
vPos ); glEnableVertexAttribArray( vColor ); 220 glDrawArrays( GL_TRIANGLES, 0, 36 ); glBindBuffer( 
GL_ARRAY_BUFFER, 0 ); 225 glutSwapBuffers(); } //---------------------------------------------------------------------------­ 
230 void reshape( int width, int height ) { glViewport( 0, 0, width, height ); } 235 //---------------------------------------------------------------------------­ 
void keyboard( unsigned char key, int x, int y ) 240 { switch( key ) { case 033: // Escape Key exit( 
EXIT_SUCCESS ); break; 245 } glutPostRedisplay(); } 250 //---------------------------------------------------------------------------­ 
int main( int argc, char* argv[] ) { 255 glutInit( &#38;argc, argv ); glutInitDisplayMode( GLUT_RGBA 
| GLUT_DOUBLE ); // glutInitContextVersion( 3, 0 ); glutCreateWindow( argv[0] ); 260 glewInit(); init(); 
glutDisplayFunc( display ); 265 glutReshapeFunc( reshape ); glutKeyboardFunc( keyboard ); glutMainLoop(); 
} 270 AOS­color­draw­arrays.cxx ////////////////////////////////////////////////////////////////////////////// 
// // AOS-color-draw-arrays.cxx -demonstrates using the Array-of-structures // (AOS) methods of storing 
data in vertex attributes. The cube is // rendering using the glDrawArrays() method. // #include <stdlib.h> 
#include <GL/glew.h> #ifdef MACOSX #include <GLUT/glut.h> #else #include <GL/freeglut.h> #endif #include 
"LoadProgram.h" #define BUFFER_OFFSET( offset ) ((GLvoid*) offset) GLuint vbo; GLuint program; GLuint 
vPos, vColor; GLintptr vertexOffset; struct VertexData { GLfloat color[3]; GLfloat vertex[3]; VertexData( 
GLfloat x, GLfloat y, GLfloat z ) { color[0] = x; color[1] = y; color[2] = z; vertex[0] = x; vertex[1] 
= y; vertex[2] = z; } }; //---------------------------------------------------------------------------­ 
void init() { VertexData vertexData[] = { VertexData( 1.0, 0.0, 0.0 ), /* Index 4 */ VertexData( 1.0, 
0.0, 1.0 ), /* Index 5 */ VertexData( 1.0, 1.0, 1.0 ), /* Index 7 */ VertexData( 1.0, 0.0, 0.0 ), /* 
Index 4 */ VertexData( 1.0, 1.0, 1.0 ), /* Index 7 */ VertexData( 1.0, 1.0, 0.0 ), /* Index 6 */ VertexData( 
0.0, 0.0, 0.0 ), /* Index 0 */ VertexData( 0.0, 1.0, 0.0 ), /* Index 2 */ VertexData( 0.0, 1.0, 1.0 ), 
/* Index 3 */ VertexData( 0.0, 0.0, 0.0 ), /* Index 0 */ VertexData( 0.0, 1.0, 1.0 ), /* Index 3 */ 
VertexData( 0.0, 0.0, 1.0 ), /* Index 1 */ VertexData( 0.0, 1.0, 0.0 ), /* Index 2 */ 60 VertexData( 
1.0, 1.0, 0.0 ), /* Index 6 */ VertexData( 1.0, 1.0, 1.0 ), /* Index 7 */ VertexData( 0.0, 1.0, 0.0 
), /* Index 2 */ VertexData( 1.0, 1.0, 1.0 ), /* Index 7 */ 65 VertexData( 0.0, 1.0, 1.0 ), /* Index 
3 */ VertexData( 0.0, 0.0, 0.0 ), /* Index 0 */ VertexData( 0.0, 0.0, 1.0 ), /* Index 1 */ VertexData( 
1.0, 0.0, 1.0 ), /* Index 5 */ 70 VertexData( 0.0, 0.0, 0.0 ), /* Index 0 */ VertexData( 1.0, 0.0, 1.0 
), /* Index 5 */ VertexData( 1.0, 0.0, 0.0 ), /* Index 4 */ 75 VertexData( 0.0, 0.0, 0.0 ), /* Index 
0 */ VertexData( 1.0, 0.0, 0.0 ), /* Index 4 */ VertexData( 1.0, 1.0, 0.0 ), /* Index 6 */ VertexData( 
0.0, 0.0, 0.0 ), /* Index 0 */ 80 VertexData( 1.0, 1.0, 0.0 ), /* Index 6 */ VertexData( 0.0, 1.0, 0.0 
), /* Index 2 */ VertexData( 0.0, 0.0, 1.0 ), /* Index 1 */ VertexData( 0.0, 1.0, 1.0 ), /* Index 3 
*/ 85 VertexData( 1.0, 1.0, 1.0 ), /* Index 7 */ VertexData( 0.0, 0.0, 1.0 ), /* Index 1 */ VertexData( 
1.0, 1.0, 1.0 ), /* Index 7 */ VertexData( 1.0, 0.0, 1.0 ), /* Index 5 */ 90 }; vertexOffset = sizeof(vertexData[0].color); 
// 95 // ---Load vertex data --­// glGenBuffers( 1, &#38;vbo ); glBindBuffer( GL_ARRAY_BUFFER, vbo ); 
100 glBufferData( GL_ARRAY_BUFFER, sizeof(vertexData), vertexData, GL_STATIC_DRAW ); // // ---Load shaders 
--­ 105 // #if SHADER_VERSION == 130 const char* vShader = { "#version 130\n" 110 "" "in vec3 vPos;" 
"in vec3 vColor;" "out vec4 oColor;" "" 115 "void main() {" " oColor = vec4( vColor, 1 );" " gl_Position 
= vec4( vPos, 1 );" "}" }; 120 const char* fShader = { "#version 130\n" "" "in vec4 color;" 125 "out 
vec4 fColor;" "" "void main() {" " fColor = color;" "}" 130 }; #else const char* vShader = { "attribute 
vec3 vPos;" "attribute vec3 vColor;" 135 "varying vec4 color;" "" "void main() {" " color = vec4( vColor, 
1 );" " gl_Position = vec4( vPos, 1 );" 140 "}" }; const char* fShader = { "varying vec4 color;" 145 
"" "void main() {" " gl_FragColor = color;" "}" }; 150 #endif // SHADER_VERSION == 130 program = LoadProgram( 
vShader, fShader ); vPos = glGetAttribLocation( program, "vPos" ); 155 vColor = glGetAttribLocation( 
program, "vColor" ); glClearColor( 0.0, 0.0, 1.0, 1.0 ); } 160 //---------------------------------------------------------------------------­ 
void display() { 165 glClear( GL_COLOR_BUFFER_BIT ); glUseProgram( program ); glColor3f( 1, 1, 1 ); 
170 glBindBuffer( GL_ARRAY_BUFFER, vbo ); glVertexAttribPointer( vColor, 3, GL_FLOAT, GL_FALSE, sizeof(VertexData), 
BUFFER_OFFSET(0) ); glVertexAttribPointer( vPos, 3, GL_FLOAT, GL_FALSE, 175 sizeof(VertexData), BUFFER_OFFSET(vertexOffset) 
); glEnableVertexAttribArray( vPos ); glEnableVertexAttribArray( vColor ); glDrawArrays( GL_TRIANGLES, 
0, 36 ); 180 glBindBuffer( GL_ARRAY_BUFFER, 0 ); glutSwapBuffers(); } 185 //---------------------------------------------------------------------------­ 
void reshape( int width, int height ) 190 { glViewport( 0, 0, width, height ); } //---------------------------------------------------------------------------­ 
195 void keyboard( unsigned char key, int x, int y ) { switch( key ) { 200 case 033: // Escape Key exit( 
EXIT_SUCCESS ); break; } 205 glutPostRedisplay(); } //---------------------------------------------------------------------------­ 
210 int main( int argc, char* argv[] ) { glutInit( &#38;argc, argv ); glutInitDisplayMode( GLUT_RGBA 
| GLUT_DOUBLE ); 215 // glutInitContextVersion( 3, 0 ); glutCreateWindow( argv[0] ); glewInit(); 220 
init(); glutDisplayFunc( display ); glutReshapeFunc( reshape ); glutKeyboardFunc( keyboard ); 225 glutMainLoop(); 
}  Draw­elements.cxx ////////////////////////////////////////////////////////////////////////////// 
// // draw-elements.cxx -render a cube using glDrawElements() indexing into // a set of vertex attributes 
stored in array-of-structures (AOS) storage. // #include <stdlib.h> #include <GL/glew.h> #ifdef MACOSX 
#include <GLUT/glut.h> #else #include <GL/freeglut.h> #endif #include "LoadProgram.h" #define BUFFER_OFFSET( 
offset ) ((GLvoid*) offset) enum { Vertices, Indices, NumBuffers }; GLuint buffers[NumBuffers]; GLuint 
program; GLuint vPos, vColor; GLintptr vertexOffset; struct VertexData { GLfloat color[3]; GLfloat 
vertex[3]; VertexData( GLfloat x, GLfloat y, GLfloat z ) { color[0] = x; color[1] = y; color[2] = z; 
vertex[0] = x; vertex[1] = y; vertex[2] = z; } }; //---------------------------------------------------------------------------­ 
void init() { VertexData vertexData[] = { VertexData( 0.0, 0.0, 0.0 ), /* Index 0 */ VertexData( 0.0, 
0.0, 1.0 ), /* Index 1 */ VertexData( 0.0, 1.0, 0.0 ), /* Index 2 */ VertexData( 0.0, 1.0, 1.0 ), /* 
Index 3 */ VertexData( 1.0, 0.0, 0.0 ), /* Index 4 */ VertexData( 1.0, 0.0, 1.0 ), /* Index 5 */ VertexData( 
1.0, 1.0, 0.0 ), /* Index 6 */ VertexData( 1.0, 1.0, 1.0 ), /* Index 7 */ }; GLubyte indices[] = { 4,5,7,// 
+Xface 4, 7, 6, 0, 2, 3, // -X face 0, 3, 1, 2,6,7,// +Yface 2, 7, 3, 60 0, 1, 5, // -Y face 0, 5, 4, 
0,4,6,// +Zface 0, 6, 2, 1, 3, 7, // -Z face 65 1, 7, 5 }; vertexOffset = sizeof(vertexData[0].color); 
70 // // ---Load vertex data --­// glGenBuffers( NumBuffers, buffers ); 75 glBindBuffer( GL_ARRAY_BUFFER, 
buffers[Vertices] ); glBufferData( GL_ARRAY_BUFFER, sizeof(vertexData), vertexData, GL_STATIC_DRAW ); 
 glBindBuffer( GL_ELEMENT_ARRAY_BUFFER, buffers[Indices] ); 80 glBufferData( GL_ELEMENT_ARRAY_BUFFER, 
sizeof(indices), indices, GL_STATIC_DRAW ); // // ---Load shaders --­85 // #if SHADER_VERSION == 130 
const char* vShader = { "#version 130\n" 90 "" "in vec3 vPos;" "in vec3 vColor;" "out vec4 oColor;" 
"" 95 "void main() {" " oColor = vec4( vColor, 1 );" " gl_Position = vec4( vPos, 1 );" "}" }; 100 const 
char* fShader = { "#version 130\n" "" "in vec4 color;" 105 "out vec4 fColor;" "" "void main() {" " fColor 
= color;" "}" 110 }; #else const char* vShader = { "attribute vec3 vPos;" "attribute vec3 vColor;" 115 
"varying vec4 color;" "" "void main() {" " color = vec4( vColor, 1 );" " gl_Position = vec4( vPos, 1 
);" 120 "}" }; const char* fShader = { "varying vec4 color;" 125 "" "void main() {" " gl_FragColor = 
color;" "}" }; 130 #endif // SHADER_VERSION == 130 program = LoadProgram( vShader, fShader ); vPos = 
glGetAttribLocation( program, "vPos" ); 135 vColor = glGetAttribLocation( program, "vColor" ); glClearColor( 
0.0, 0.0, 1.0, 1.0 ); } 140 //---------------------------------------------------------------------------­ 
void display() { 145 glClear( GL_COLOR_BUFFER_BIT ); glUseProgram( program ); glColor3f( 1, 1, 1 ); 
150 glBindBuffer( GL_ARRAY_BUFFER, buffers[Vertices] ); glVertexAttribPointer( vColor, 3, GL_FLOAT, GL_FALSE, 
sizeof(VertexData), BUFFER_OFFSET(0) ); glVertexAttribPointer( vPos, 3, GL_FLOAT, GL_FALSE, 155 sizeof(VertexData), 
BUFFER_OFFSET(vertexOffset) ); glEnableVertexAttribArray( vPos ); glEnableVertexAttribArray( vColor ); 
glBindBuffer( GL_ELEMENT_ARRAY_BUFFER, buffers[Indices] ); 160 glDrawElements( GL_TRIANGLES, 36, GL_UNSIGNED_BYTE, 
BUFFER_OFFSET(0) ); glBindBuffer( GL_ARRAY_BUFFER, 0 ); glBindBuffer( GL_ELEMENT_ARRAY_BUFFER, 0 ); 
165 glutSwapBuffers(); } //---------------------------------------------------------------------------­ 
170 void reshape( int width, int height ) { glViewport( 0, 0, width, height ); } 175 //---------------------------------------------------------------------------­ 
void keyboard( unsigned char key, int x, int y ) 180 { switch( key ) { case 033: // Escape Key exit( 
EXIT_SUCCESS ); break; 185 } glutPostRedisplay(); } 190 //---------------------------------------------------------------------------­ 
int main( int argc, char* argv[] ) { 195 glutInit( &#38;argc, argv ); glutInitDisplayMode( GLUT_RGBA 
| GLUT_DOUBLE ); // glutInitContextVersion( 3, 0 ); glutCreateWindow( argv[0] ); 200 glewInit(); init(); 
glutDisplayFunc( display ); 205 glutReshapeFunc( reshape ); glutKeyboardFunc( keyboard ); glutMainLoop(); 
}  LoadProgram.h ////////////////////////////////////////////////////////////////////////////// // // 
--- LoadProgram.h --­// ////////////////////////////////////////////////////////////////////////////// 
 #ifndef __LOADPROGRAM_H__ #define __LOADPROGRAM_H__ //---------------------------------------------------------------------------­ 
#ifdef __cplusplus extern "C" { #endif /* __cplusplus */ extern GLuint LoadProgram( const char*, const 
char* ); extern GLuint LoadTransformFeedbackShader( const char*, GLsizei, const char** ); #ifdef __cplusplus 
}; #endif /* __cplusplus */ //---------------------------------------------------------------------------­ 
#endif // !__LOADPROGRAM_H__ LoadProgram.c ////////////////////////////////////////////////////////////////////////////// 
// // --- LoadProgram.c --­// ////////////////////////////////////////////////////////////////////////////// 
 #include <stdio.h> #include <stdlib.h> #ifdef MACOS #include <OpenGL/OpenGL.h> #else #include <GL/glew.h> 
#endif #include "CheckError.h" /*--------------------------------------------------------------------------*/ 
GLuint LoadProgram( const char* vShader, const char* fShader ) { GLuint shader, program; GLint completed; 
 program = glCreateProgram(); /*-----------------------------------------------------------------------­** 
** --- Load and compile the vertex shader ---  */ if ( vShader != NULL ) { shader = glCreateShader( 
GL_VERTEX_SHADER ); glShaderSource( shader, 1, &#38;vShader, NULL ); glCompileShader( shader ); glGetShaderiv( 
shader, GL_COMPILE_STATUS, &#38;completed ); if ( !completed ) { GLint len; char* msg; glGetShaderiv( 
shader, GL_INFO_LOG_LENGTH, &#38;len ); msg = (char*) malloc( len ); glGetShaderInfoLog( shader, len, 
&#38;len, msg ); fprintf( stderr, "Vertex shader compilation failure:\n%s\n", msg ); free( msg ); glDeleteProgram( 
program ); exit( EXIT_FAILURE ); } glAttachShader( program, shader ); CheckError(); } /*-----------------------------------------------------------------------­** 
60 ** */ --- Load and compile the fragment shader ---  65 if ( fShader != NULL ) { shader = glCreateShader( 
GL_FRAGMENT_SHADER ); glShaderSource( shader, 1, &#38;fShader, NULL ); glCompileShader( shader ); glGetShaderiv( 
shader, GL_COMPILE_STATUS, &#38;completed ); 70 if ( !completed ) { GLint len; char* msg; 75 glGetShaderiv( 
shader, GL_INFO_LOG_LENGTH, &#38;len ); msg = (char*) malloc( len ); glGetShaderInfoLog( shader, len, 
&#38;len, msg ); fprintf( stderr, "Fragment shader compilation failure:\n%s\n", msg ); free( msg ); 80 
} glDeleteProgram( program ); exit( EXIT_FAILURE ); 85 } glAttachShader( program, shader ); CheckError(); 
90 /*-----------------------------------------------------------------------­** ** --- Link program --- */ 
95 glLinkProgram( program ); glGetProgramiv( program, GL_LINK_STATUS, &#38;completed ); 100 105 if ( 
!completed ) { GLint len; char* msg; glGetProgramiv( program, GL_INFO_LOG_LENGTH, &#38;len ); msg = (char*) 
malloc( len ); glGetProgramInfoLog( program, len, &#38;len, msg ); fprintf( stderr, "Program link failure:\n%s\n", 
msg ); free( msg ); glDeleteProgram( program ); 110 } exit( EXIT_FAILURE ); CheckError(); 115 } return 
program;  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1667246</article_id>
		<sort_key>70</sort_key>
		<display_label>Article No.</display_label>
		<display_no>7</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Beyond programmable shading (parts I and II)]]></title>
		<page_from>1</page_from>
		<page_to>312</page_to>
		<doi_number>10.1145/1667239.1667246</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1667246</url>
		<abstract>
			<par><![CDATA[<p>There are strong indications that the future of interactive graphics programming is a more flexible model than today's OpenGL/Direct3D pipelines. Graphics developers need a basic understanding of how to combine emerging parallel programming techniques and more flexible graphics processors with the traditional interactive rendering pipeline. As the first in a series, this course introduces the trends and directions in this emerging field. Topics include: parallel graphics architectures, parallel programming models for graphics, and game-developer investigations of the use of these new capabilities in future rendering engines.</p> <p>This second course in the series Beyond Programmable Shading presents the state of the art in combining traditional rendering API usage with advanced task- and data-parallel computation to increase the image quality of interactive graphics.</p> <p>Leaders from graphics hardware vendors, game development, and academic research present case studies that show how general parallel computation is being combined with the traditional graphics pipeline to boost image quality and spur new graphics algorithm innovation. Each case study discusses the mix of parallel programming constructs, details of the graphics algorithm, and how the rendering pipeline and computation interact to achieve the technical goals. Presenters also discuss integrating a combination of GPU and CPU techniques for more efficient and flexible algorithms. The focus is on what currently can be done, how it is done, and near-future trends. Topics include: interactive realistic lighting, advanced geometry-processing pipelines, in-frame data structure construction, complex image processing, and rasterization versus ray tracing.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Software support</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011074</concept_id>
				<concept_desc>CCS->Software and its engineering->Software creation and management</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1797112</person_id>
				<author_profile_id><![CDATA[81100460149]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Aaron]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lefohn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797114</person_id>
				<author_profile_id><![CDATA[81100541080]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Houston]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[AMD]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797115</person_id>
				<author_profile_id><![CDATA[81321488831]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Johan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Andersson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DICE]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797116</person_id>
				<author_profile_id><![CDATA[81100618307]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ulf]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Assarsson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chalmers University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797117</person_id>
				<author_profile_id><![CDATA[81458650607]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Cass]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Everitt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Id Software]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797118</person_id>
				<author_profile_id><![CDATA[81100626902]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Kayvon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fatahalian]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797119</person_id>
				<author_profile_id><![CDATA[81332498769]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Tim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Foley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797120</person_id>
				<author_profile_id><![CDATA[81100566224]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Justin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hensley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[AMD]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797121</person_id>
				<author_profile_id><![CDATA[81100065581]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lalonde]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797113</person_id>
				<author_profile_id><![CDATA[81100131290]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>10</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Luebke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>563901</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Crow, F. C. 1977. Shadow algorithms for computer graphics. <i>Computer Graphics (Proceedings of SIGGRAPH 77)</i>, 242--248.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Everitt, C. 2001. Interactive order-independent transparency. <i>Technical report, NVIDIA Corporation, May 2001. Available at</i> http://www.nvidia.com/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1283908</ref_obj_id>
				<ref_obj_pid>1283900</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Hadwiger, M., Kratz, A., Sigg, C., and B&#252;hler, K. 2006. Gpu-accelerated deep shadow maps for direct volume rendering. In <i>GH '06: Proceedings of the 21st ACM SIGGRAPH/Eurographics symposium on Graphics hardware</i>, ACM Press, New York, NY, USA, 49--52.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>366644</ref_obj_id>
				<ref_obj_pid>366622</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Hoare, C. A. R. 1961. Quicksort: Algorithm 64. <i>Communications of the ACM</i>, 321--322.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74361</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Kajiya, J. T., and Kay, T. L. 1989. Rendering fur with three-dimensional textures. <i>Proceedings of SIGGRAPH</i>, 271--280.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732282</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Kim, T. Y., and Neumann, U. 2001. Opacity shadow maps. <i>Proc. of Eurographics Syymposium on Rendering</i>, 177--182.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Liu, B., Wei, L., and Xu, Y. 2006. Multi-layer depth peeling via fragment sort. <i>Microsoft Technical Report, MSR-TR-2006-81</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344958</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Lokovic, T., and Veach, E. 2000. Deep shadow maps. <i>Proceedings of ACM SIGGRAPH</i>, 385--392.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617499</ref_obj_id>
				<ref_obj_pid>616006</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Mammen, A. 1989. Transparency and antialiasing algorithms implemented with the virtual pixel maps technique. <i>IEEE Computer Graphics</i>, 43--45.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882345</ref_obj_id>
				<ref_obj_pid>1201775</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Marschner, S. R., Jensen, H. W., Cammarano, M., Worley, S., and Hanrahan, P. 2003. Light scattering from human hair fibres. <i>Proceedings of ACM SIGGRAPH 2003</i>, 780--791.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Nguyen, H., and Donelly, W. 2005. Hair animation and rendering in the nalu demo. <i>GPU Gems 2</i>, 361--380.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Sintorn, E., and Assarsson, U. 2007. Fast parallel gpusorting using a hybrid algorithm. <i>Workshop on General Purpose Processing on Graphics Processing Units (GPGPU)</i> (October).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>807402</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Williams, L. 1978. Casting curved shadows on curved surfaces. <i>Proceedings of the 5th annual conference on Computer graphics and interactive techniques</i>, 270--274.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Yuksel, C., and Keyser, J. 2007. Deep opacity maps. <i>Technical Report, Department of Computer Science, Texas A&M University</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Bavoil, L., and Meyers, K. 2008. Order independent transparency with dual depth peeling. Tech. rep., nVidia, February.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[E., A. S. Bit twiddling hacks.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1179859</ref_obj_id>
				<ref_obj_pid>1179849</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Eisemann, E., and D&#233;coret, X. 2006. Fast scene voxelization and applications. In <i>SIGGRAPH '06: ACM SIGGRAPH 2006 Sketches</i>, ACM, New York, NY, USA, 8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Everitt, C., 2001. Interactive order-independent transparency. NVIDIA white paper, citeseer.ist.psu.edu/everitt01interactive.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1177246</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Hadwiger, M., Kniss, J. M., Rezk-salama, C., Weiskopf, D., and Engel, K. 2006. <i>Real-time Volume Graphics</i>. A. K. Peters, Ltd., Natick, MA, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1283908</ref_obj_id>
				<ref_obj_pid>1283900</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Hadwiger, M., Kratz, A., Sigg, C., and B&#252;hler, K. 2006. Gpu-accelerated deep shadow maps for direct volume rendering. In <i>GH '06: Proceedings of the 21st ACM SIGGRAPH/Eurographics symposium on Graphics hardware</i>, ACM, New York, NY, USA, 49--52.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74361</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Kajiya, J. T., and Kay, T. L. 1989. Rendering fur with three-dimensional textures. <i>Proceedings of SIGGRAPH</i>, 271--280.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732282</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Kim, T., and Neumann, U. 2001. Opacity shadow maps. In <i>In Rendering Techniques 2001</i>, Springer, 177--182.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Liu, B., Wei, L.-Y., and Xu, Y.-Q. 2006. Multi-layer depth peeling via fragment sort. Tech. rep., Microsoft Reasearch.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344958</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Lokovic, T., and Veach, E. 2000. Deep shadow maps. In <i>SIGGRAPH '00: Proceedings of the 27th annual conference on Computer graphics and interactive techniques</i>, ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 385--392.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882345</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Marschner, S. R., Jensen, H. W., Cammarano, M., Worley, S., and Hanrahan, P. 2003. Light scattering from human hair fibers. <i>ACM Trans. Graph. 22</i>, 3, 780--791.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1186278</ref_obj_id>
				<ref_obj_pid>1186223</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Mertens, T., Kautz, J., Bekaert, P., and Reeth, F. V. 2004. A self-shadow algorithm for dynamic hair using density clustering. In <i>SIGGRAPH '04: ACM SIGGRAPH 2004 Sketches</i>, ACM, New York, NY, USA, 44.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Nguyen, H., and Donelly, W. 2005. Hair animation and rendering in the nalu demo. <i>GPU Gems 2</i>, 361--380.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1342275</ref_obj_id>
				<ref_obj_pid>1342250</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Sintorn, E., and Assarsson, U. 2008. Real-time approximate sorting for self shadowing and transparency in hair rendering. In <i>SI3D '08: Proceedings of the 2008 symposium on Interactive 3D graphics and games</i>, ACM, New York, NY, USA, 157--162.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1081422</ref_obj_id>
				<ref_obj_pid>1081407</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Staneker, D., Bartz, D., and Meissner, M. 2003. Improving occlusion query efficiency with occupancy maps. In <i>PVG '03: Proceedings of the 2003 IEEE Symposium on Parallel and Large-Data Visualization and Graphics</i>, IEEE Computer Society, Washington, DC, USA, 15.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1263263</ref_obj_id>
				<ref_obj_pid>1263129</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Ward, K., Bertails, F., Kim, T.-Y., Marschner, S. R., Cani, M.-P., and Lin, M. 2007. A survey on hair modeling: Styling, simulation, and rendering. <i>IEEE Transactions on Visualization and Computer Graphics (TVCG) 13</i>, 2 (Mar-Apr), 213--34. To appear.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Yuksel, C., and Keyser, J. 2008. Deep opacity maps. <i>Computer Graphics Forum (Proceedings of EUROGRAPHICS 2008) 27</i>, 2, 675--680.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Zinke, A., Sobottka, G., and Weber, A. 2004. Photorealistic rendering of blond hair. In <i>Vision, Modeling, and Visualization (VMV04)</i>, 191--198.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360631</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Zinke, A., Yuksel, C., Weber, A., and Keyser, J. 2008. Dual scattering approximation for fast multiple scattering in hair. <i>ACM Trans. Graph. 27</i>, 3, 1--10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Beyond Programmable Shading SIGGRAPH 2009 Course Organizers: Aaron Lefohn, Intel Mike Houston, AMD 
Course Speakers: Aaron Lefohn Mike Houston Johan Andersson Ulf Assarsson Cass Everitt Kayvon Fatahalian 
Tim Foley Justin Hensley Paul Lalonde David Luebke Intel AMD DICE Chalmers UniversityId SoftwareStanford 
UniversityStanford UniversityAMD IntelNVIDIA Speaker Contact Info: Johan Andersson DICE johan.andersson@dice.com 
Ulf Assarsson Chalmers University uffe@chalmers.se Cass Everitt Id Software cass@idsoftware.com Kayvon 
Fatahalian Stanford University kayvonf@graphics.stanford.edu Tim Foley Stanford University / Intel tfoley@graphics.stanford.edu 
Justin Hensley AMD justin.hensley@amd.com Mike Houston AMD michael.houston@amd.com Paul Lalonde Intel 
paul.lalonde@intel.com Aaron Lefohn Intel aaron.lefohn@intel.com David Luebke NVIDIA dlebke@nvidia.com 
 Beyond Programmable Shading SIGGRAPH 2009 Summary: There are strong indications that the future of 
interactive graphics programming is a model more flexible than today s OpenGL/Direct3D pipelines. As 
such, graphics developers need to have a basic understanding of how to combine emerging parallel programming 
techniques and more flexible graphics processors with the traditional interactive rendering pipeline. 
In this course, leaders from graphics hardware vendors, game development, and academia introduce the 
trends and directions of this emerging field, parallel graphics architectures, parallel programming models 
for graphics, discuss the use of these new capabilities in game rendering engines and academic research 
to boost image quality and spur new graphics algorithm innovation. Intended Audience: We are targeting 
researchers and engineers interested in investigating advanced graphics techniques using parallel programming 
techniques on GPUs and multi-core architectures, as well as graphics and game developers interested in 
integrating these techniques into their applications. Prerequisites: Attendees are expected to have 
experience with a modern graphics API (OpenGL or Direct3D), including basic experience with shaders, 
textures, and framebuffers and/or background with parallel programming languages. Some background with 
parallel programming on CPUs or GPUs is useful but not required as an overview of will be provided in 
the course. Level of difficulty: Intermediate / Advanced NOTE: Several of the talks in the course contain 
cutting-edge, time-sensitive information that cannot be released until SIGGRAPH 2009. The day of the 
course, we will post a web page containing complete copies of all presentations, but product release 
constraints prevent us from presenting the materials for some of the talks sooner. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1667247</article_id>
		<sort_key>80</sort_key>
		<display_label>Article No.</display_label>
		<display_no>8</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Build your own 3D scanner]]></title>
		<subtitle><![CDATA[3D photography for beginners]]></subtitle>
		<page_from>1</page_from>
		<page_to>94</page_to>
		<doi_number>10.1145/1667239.1667247</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1667247</url>
		<abstract>
			<par><![CDATA[<p>Over the last decade, digital photography has entered the mainstream with inexpensive, miniaturized cameras for consumer use. Digital projection is poised to make a similar breakthrough, with a variety of vendors offering small, low-cost projectors. As a result, active imaging is a topic of renewed interest in the computer graphics community. In particular, low-cost homemade 3D scanners are now within reach of students and hobbyists with a modest budget.</p> <p>This course provides a beginner with the necessary mathematics, software, and practical details to leverage projector-camera systems in their own 3D scanning projects. An example-driven approach is used throughout; each new concept is illustrated using a practical scanner implemented with off-the-shelf parts. The course concludes by detailing how these new approaches are used in rapid prototyping, entertainment, cultural heritage, and web-based applications.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010383</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Image processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1797122</person_id>
				<author_profile_id><![CDATA[81319495323]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Douglas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lanman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797123</person_id>
				<author_profile_id><![CDATA[81100305012]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gabriel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Taubin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1409085</ref_obj_id>
				<ref_obj_pid>1457515</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{AIH*08} Atcheson B., Ihrke I., Heidrich W., Tevs A., Bradley D., Magnor M., Seidel H.-P.: Time-resolved 3d capture of non-stationary gas flows. <i>ACM Transactions on Graphics (Proc. SIGGRAPH Asia) 27</i>, 5 (2008), 132. 78]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>132014</ref_obj_id>
				<ref_obj_pid>132013</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{AW92} Adelson T., Wang J.: Single lens stereo with a plenoptic camera. <i>IEEE TPAMI 2</i>, 14 (1992), 99--106. 78]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218462</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{BF95} Bloomenthal J., Ferguson K.: Polygonization of non-manifold implicit surfaces. In <i>SIGGRAPH '95: ACM SIGGRAPH 1995 papers</i> (1995), pp. 309--316. 66]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{BK08} Bradski G., Kaehler A.: <i>Learning OpenCV: Computer Vision with the OpenCV Library</i>. O'Reilly Media, Inc., 2008. 32]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{Bla04} Blais F.: Review of 20 years of range sensor development. <i>Journal of Electronic Imaging 13</i>, 1 (2004), 231--240. 5]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>55285</ref_obj_id>
				<ref_obj_pid>55279</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{Blo88} Bloomenthal J.: Polygonization of Implicit Surfaces. <i>Computer Aided Geometric Design 5</i>, 4 (1988), 341--355. 66]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{Bou} Bouguet J.-Y.: Camera calibration toolbox for matlab. http://www.vision.caltech.edu/bouguetj/calib_doc/. 28, 40, 52]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>930289</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{Bou99} Bouguet J.-Y.: <i>Visual methods for three-dimensional modeling</i>. PhD thesis, California Institute of Technology, 1999. 38, 42]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{BP} Bouguet J.-Y., Perona P.: 3d photography on your desk. http://www.vision.caltech.edu/bouguetj/ICCV98/. 7, 35, 36, 45]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>339361</ref_obj_id>
				<ref_obj_pid>339355</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[{BP99} Bouguet J.-Y., Perona P.: 3d photography using shadows in dual-space geometry. <i>Int. J. Comput. Vision 35</i>, 2 (1999), 129--149. 35, 38, 39, 42]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2295802</ref_obj_id>
				<ref_obj_pid>616074</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[{BRM*02} Bernardini F., Rushmeier H., Martin I. M., Mittleman J., Taubin G.: Building a digital model of michelangelo's florentine piet&#224;. <i>IEEE Computer Graphics and Applications 22</i> (2002), 59--67. 78]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>330299</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[{CG00} Cipolla R., Giblin P.: <i>Visual Motion of Curves and Surfaces</i>. Cambridge University Press, 2000. 4]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[{CMU} CMU IEEE 1394 digital camera driver, version 6.4.5. http://www.cs.cmu.edu/iwan/1394/. 25]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[{Cre} Creaform: Handyscan 3D. http://www.creaform3d.com/en/handyscan3d/products/exascan.aspx. 6]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>267086</ref_obj_id>
				<ref_obj_pid>266989</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[{CS97} Chiang Y., Silva C. T.: I/O Optimal Isosurface Extraction. In <i>IEEE Visualization 1997, Conference Proceedings</i> (1997), pp. 293--300. 64]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882309</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[{CTMS03} Carranza J., Theobalt C., Magnor M. A., Seidel H.-P.: Free-viewpoint video of human actors. <i>ACM Trans. Graph. 22</i>, 3 (2003), 569--577. 4]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360697</ref_obj_id>
				<ref_obj_pid>1399504</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[{dAST*08} de Aguiar E., Stoll C., Theobalt C., Ahmed N., Seidel H.-P., Thrun S.: Performance capture from sparse multi-view video. In <i>SIGGRAPH '08: ACM SIGGRAPH 2008 papers</i> (2008), pp. 1--10. 4]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[{DC01} Davis J., Chen X.: A laser range scanner designed for minimum calibration complexity. In <i>Proceedings of the International Conference on 3-D Digital Imaging and Modeling (3DIM)</i> (2001), p. 91. 70]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>259366</ref_obj_id>
				<ref_obj_pid>259081</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[{Deb97} Debevec P. E.: Facade: modeling and rendering architecture from photographs and the campanile model. In <i>SIGGRAPH '97: ACM SIGGRAPH 97 Visual Proceedings</i> (1997), p. 254. 4]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[{DK91} Doi A., Koide A.: An Efficient Method of Triangulating Equivalued Surfaces by Using Tetrahedral Cells. <i>IEICE Transactions on Communications and Electronics Information Systems E74</i>, 1 (Jan. 1991), 214--224. 63, 64]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[{Edm} Edmund Optics:. http://www.edmundoptics.com. 37]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[{EGPP04} Epstein E., Granger-Pich&#233; M., Poulin P.: Exploiting mirrors in interactive reconstruction with structured light. In <i>Vision, Modeling, and Visualization 2004</i> (2004), pp. 125--132. 77]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>269430</ref_obj_id>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[{Far97} Farid H.: <i>Range Estimation by Optical Differentiation</i>. PhD thesis, University of Pennsylvania, 1997. 4]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2168499</ref_obj_id>
				<ref_obj_pid>2168483</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[{FNdJV06} Forbes K., Nicolls F., de Jager G., Voigt A.: Shape-from-silhouette with two mirrors and an uncalibrated camera. In <i>ECCV 2006</i> (2006), pp. 165--178. 73, 75]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[{FWM98} Ferryman J. M., Worrall A. D., Maybank S. J.: Learning enhanced 3d models for vehicle tracking. In <i>BMVC</i> (1998), pp. 873--882. 4]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237200</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[{GGSC96} Gortler S. J., Grzeszczuk R., Szeliski R., Cohen M. F.: The lumigraph. In <i>SIGGRAPH '96: ACM SIGGRAPH 1996 papers</i> (1996), pp. 43--54. 78]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614319</ref_obj_id>
				<ref_obj_pid>614260</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[{GH95} Gu&#233;ziec A., Hummel R.: Exploiting Triangulated Surface Extraction Using Tetrahedral Decomposition. <i>IEEE Transactions on Visualization and Computer Graphics 1</i>, 4 (1995). 64]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[{GSP06} Greengard A., Schechner Y. Y., Piestun R.: Depth from diffracted rotation. <i>Opt. Lett. 31</i>, 2 (2006), 181--183. 4]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[{HARN06} Hsu S., Acharya S., Rafii A., New R.: Performance of a time-of-flight range camera for intelligent vehicle safety applications. <i>Advanced Microsystems for Automotive Applications</i> (2006). 6]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[{Hec01} Hecht E.: <i>Optics (4th Edition)</i>. Addison Wesley, 2001. 4]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360686</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[{HFI*08} Hullin M. B., Fuchs M., Ihrke I., Seidel H.-P., Lensch H. P. A.: Fluorescent immersion range scanning. <i>ACM Trans. Graph. 27</i>, 3 (2008), 1--10. 78]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[{HVB*07} Hern&#225;ndez C., Vogiatzis G., Brostow G. J., Stenger B., Cipolla R.: Non-rigid photometric stereo with colored lights. In <i>Proc. of the 11th IEEE Intl. Conf. on Comp. Vision (ICCV)</i> (2007). 7]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>373536</ref_obj_id>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[{HZ04} Hartley R. I., Zisserman A.: <i>Multiple View Geometry in Computer Vision</i>, second ed. Cambridge University Press, 2004. 3, 26, 70]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[{ISM84} Inokuchi S., Sato K., Matsuda F.: Range imaging system for 3-d object recognition. In <i>Proceedings of the International Conference on Pattern Recognition</i> (1984), pp. 806--808. 48]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[{IY01} Iddan G. J., Yahav G.: Three-dimensional imaging in the studio and elsewhere. <i>Three-Dimensional Image Capture and Applications IV 4298</i>, 1 (2001), 48--55. 6]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>357884</ref_obj_id>
				<ref_obj_pid>357871</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[{KM00} Kakadiaris I., Metaxas D.: Model-based estimation of 3d human motion. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence 22</i>, 12 (2000), 1453--1459. 4]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[{Las} Laser Design Inc.: Surveyor DT-2000 desktop 3D laser scanner. http://www.laserdesign.com/quick-attachments/hardware/low-res/dt-series. pdf. 6]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>628563</ref_obj_id>
				<ref_obj_pid>628309</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[{Lau94} Laurentini A.: The Visual Hull Concept for Silhouette-Based Image Understanding. <i>IEEE TPAMI 16</i>, 2 (1994), 150--162. 3]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37422</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[{LC87} Lorensen W. L., Cline H. E.: Marching Cubes: A High Resolution 3D Surface Construction Algorithm. In <i>Siggraph'87, Conference Proceedings</i> (1987), ACM Press, pp. 163--169. 63, 66]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1299151</ref_obj_id>
				<ref_obj_pid>1299128</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[{LCT07} Lanman D., Crispell D., Taubin G.: Surround structured lighting for full object scanning. In <i>Proceedings of the International Conference on 3-D Digital Imaging and Modeling (3DIM)</i> (2007), pp. 107--116. 73, 74, 75, 76]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276464</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[{LFDF07} Levin A., Fergus R., Durand F., Freeman W. T.: Image and depth from a conventional camera with a coded aperture. <i>ACM Trans. Graph. 26</i>, 3 (2007), 70. 4]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237199</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[{LH96} Levoy M., Hanrahan P.: Light field rendering. In <i>Proc. of ACM SIGGRAPH</i> (1996), pp. 31--42. 78]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[{LN04} Land M. F., Nilsson D.-E.: <i>Animal Eyes</i>. Oxford University Press, 2004. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344849</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[{LPC*00} Levoy M., Pulli K., Curless B., Rusinkiewicz S., Koller D., Pereira L., Ginzton M., Anderson S., Davis J., Ginsberg J., Shade J., Fulk D.: The digital michelangelo project: 3D scanning of large statues. In <i>Proceedings of ACM SIGGRAPH 2000</i> (2000), pp. 131--144. 78]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1409084</ref_obj_id>
				<ref_obj_pid>1409060</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[{LRAT08} Lanman D., Raskar R., Agrawal A., Taubin G.: Shield fields: modeling and capturing 3d occluders. <i>ACM Trans. Graph. 27</i>, 5 (2008), 1--10. 78]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1667247</ref_obj_id>
				<ref_obj_pid>1667239</ref_obj_pid>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[{LT} Lanman D., Taubin G.: Build your own 3d scanner: 3d photography for beginners (course website). http://mesh.brown.edu/dlanman/scan3d. 37]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[{LVT08} Leotta M. J., Vandergon A., Taubin G.: 3d slit scanning with planar constraints. <i>Computer Graphics Forum 27</i>, 8 (Dec. 2008), 2066--2080. 70, 71, 72]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[{Mat} MathWorks, Inc: Image acquisition toolbox. http://www.mathworks.com/products/imaq/. 25, 46]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344951</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[{MBR*00} Matusik W., Buehler C., Raskar R., Gortler S. J., McMillan L.: Image-based visual hulls. In <i>SIGGRAPH '00: ACM SIGGRAPH 2000 papers</i> (2000), pp. 369--374. 4]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[{Mit} Mitsubishi Electric Corp.: XD300U user manual. http://www.projectorcentral.com/pdf/projector_manual_1921.pdf. 46]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[{MPL04} Marc R. Y., Pollefeys M., Li S.: Improved real-time stereo on commodity graphics hardware. In <i>In IEEE Workshop on Realtime 3D Sensors and Their Use</i> (2004). 3]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[{MSKS05} Ma Y., Soatto S., Kosecka J., Sastry S. S.: <i>An Invitation to 3-D Vision</i>. Springer, 2005. 26, 39]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[{NA06} Nayar S. K., Anand V.: <i>Projection Volumetric Display Using Passive Optical Scatterers</i>. Tech. rep., July 2006. 75, 77]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[{Nex} NextEngine: 3D Scanner HD. https://www. nextengine.com/indexSecure.htm. 6]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[{NLB*05} Ng R., Levoy M., Bredif M., Duval G., Horowitz M., Hanrahan P.: Light field photography with a hand-held plenoptic camera. <i>Tech Report, Stanford University</i> (2005). 78]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>628640</ref_obj_id>
				<ref_obj_pid>628315</ref_obj_pid>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[{NN94} Nayar S. K., Nakagawa Y.: Shape from focus. <i>IEEE Trans. Pattern Anal. Mach. Intell. 16</i>, 8 (1994), 824--831. 4, 78]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[{NN05} Narasimhan S. G., Nayar S.: Structured light methods for underwater imaging: light stripe scanning and photometric stereo. In <i>Proceedings of 2005 MTS/IEEE OCEANS</i> (September 2005), vol. 3, pp. 2610--2617. 78]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[{NNSK08} Narasimhan S. G., Nayar S. K., Sun B., Koppal S. J.: Structured light in scattering media. In <i>SIGGRAPH Asia '08: ACM SIGGRAPH Asia 2008 courses</i> (2008), pp. 1--8. 78]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[{Opea} Open source computer vision library. http://sourceforge.net/projects/opencvlibrary/. 26, 40]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[{Opeb} OpenCV wiki. http://opencv.willowgarage.com/wiki/. 46]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[{OSS*00} Ormoneit D., Sidenbladh H., Sidenbladh H., Black M. J., Hastie T., Fleet D. J.: Learning and tracking human motion using functional analysis. In <i>IEEE Workshop on Human Modeling, Analysis and Synthesis</i> (2000), pp. 2--9. 4]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[{PA82} Posdamer J., Altschuler M.: Surface measurement by space encoded projected beam systems. <i>Computer Graphics and Image Processing 18</i> (1982), 1--17. 47]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[{Poia} Point Grey Research, Inc.: Grasshopper IEEE-1394b digital camera. http://www.ptgrey.com/products/grasshopper/index.asp. 26, 46]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[{Poib} Point Grey Research, Inc.: Using matlab with point grey cameras. http://www.ptgrey.com/support/kb/index.asp?a=4&q=218. 46]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[{Pol} Polhemus: FastSCAN. http://www.polhemus.com/?page=Scanning_Fastscan. 6]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[{Psy} Psychophysics Toolbox:. http://psychtoolbox.org. 31, 33, 47]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732300</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[{RWLB01} Raskar R., Welch G., Low K.-L., Bandyopadhyay D.: Shader lamps: Animating real objects with image-based illumination. In <i>Proceedings of the 12th Eurographics Workshop on Rendering Techniques</i> (2001), Springer-Verlag, pp. 89--102. 4]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>942605</ref_obj_id>
				<ref_obj_pid>942583</ref_obj_pid>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[{SB03} Suffern K. G., Balsys R. J.: Rendering the intersections of implicit surfaces. <i>IEEE Comput. Graph. Appl. 23</i>, 5 (2003), 70--77. 66]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1153518</ref_obj_id>
				<ref_obj_pid>1153170</ref_obj_pid>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[{SCD*06} Seitz S., Curless B., Diebel J., Scharstein D., Szeliski R.: A comparison and evaluation of multi-view stereo reconstruction algorithms. In <i>CVPR 2006</i> (2006). 3, 70]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>946683</ref_obj_id>
				<ref_obj_pid>946247</ref_obj_pid>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[{SH03} Starck J., Hilton A.: Model-based multiple view reconstruction of people. In <i>Proceedings of the Ninth IEEE International Conference on Computer Vision</i> (2003), p. 915. 4]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1149688</ref_obj_id>
				<ref_obj_pid>1149685</ref_obj_pid>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[{SMP05} Svoboda T., Martinec D., Pajdla T.: A convenient multicamera self-calibration for virtual environments. <i>PRESENCE: Teleoperators and Virtual Environments 14</i>, 4 (August 2005), 407--422. 27]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[{SPB04} Salvi J., Pag&#232;s J., Batlle J.: Pattern codification strategies in structured light systems. In <i>Pattern Recognition</i> (April 2004), vol. 37, pp. 827--849. 6, 47]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1187146</ref_obj_id>
				<ref_obj_pid>1187112</ref_obj_pid>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[{ST05} Sibley P. G., Taubin G.: Vectorfield Isosurface-based Recon-struction from Oriented points. In <i>SIGGRAPH'05 Sketch</i> (2005). 67, 68]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>207787</ref_obj_id>
				<ref_obj_pid>207780</ref_obj_pid>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[{Sul95} Sullivan G.: Model-based vision for traffic scenes using the ground-plane constraint. 93--115. 4]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1179918</ref_obj_id>
				<ref_obj_pid>1179849</ref_obj_pid>
				<ref_seq_no>75</ref_seq_no>
				<ref_text><![CDATA[{TBH06} Trifonov B., Bradley D., Heidrich W.: Tomographic reconstruction of transparent objects. In <i>SIGGRAPH '06: ACM SIGGRAPH 2006 Sketches</i> (2006), p. 55. 78]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>76</ref_seq_no>
				<ref_text><![CDATA[{TPG99} Treece G. M., Prager R. W., Gee A. H.: Regularised Marching Tetrahedra: Improved Iso-Surface Extraction. <i>Computers and Graphics 23</i>, 4 (1999), 583--598. 64]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>132018</ref_obj_id>
				<ref_obj_pid>132013</ref_obj_pid>
				<ref_seq_no>77</ref_seq_no>
				<ref_text><![CDATA[{VF92} Vaillant R., Faugeras O. D.: Using extremal boundaries for 3-d object modeling. <i>IEEE Trans. Pattern Anal. Mach. Intell. 14</i>, 2 (1992), 157--173. 3]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1153597</ref_obj_id>
				<ref_obj_pid>1153171</ref_obj_pid>
				<ref_seq_no>78</ref_seq_no>
				<ref_text><![CDATA[{VLS*06} Vaish V., Levoy M., Szeliski R., Zitnick C. L., Kang S. B.: Reconstructing occluded surfaces using synthetic apertures: Stereo, focus and robust measures. In <i>Proc. IEEE Computer Vision and Pattern Recognition</i> (2006), pp. 2331--2338. 78]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276463</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>79</ref_seq_no>
				<ref_text><![CDATA[{VRA*07} Veeraraghavan A., Raskar R., Agrawal R., Mohan A., Tumblin J.: Dappled photography: Mask enhanced cameras for heterodyned light fields and coded aperture refocusing. <i>ACM Trans. Graph. 26</i>, 3 (2007), 69. 78]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>80</ref_seq_no>
				<ref_text><![CDATA[{Wik} Wikipedia: Gray code. http://en.wikipedia.org/wiki/Gray_code. 48]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073259</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>81</ref_seq_no>
				<ref_text><![CDATA[{WJV*05} Wilburn B., Joshi N., Vaish V., Talvala E.-V., Antunez E., Barth A., Adams A., Horowitz M., Levoy M.: High performance imaging using large camera arrays. <i>ACM Trans. Graph. 24</i>, 3 (2005), 765--776. 78]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>293932</ref_obj_id>
				<ref_obj_pid>293931</ref_obj_pid>
				<ref_seq_no>82</ref_seq_no>
				<ref_text><![CDATA[{WN98} Watanabe M., Nayar S. K.: Rational filters for passive depth from defocus. <i>Int. J. Comput. Vision 27</i>, 3 (1998), 203--225. 4]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>93888</ref_obj_id>
				<ref_obj_pid>93871</ref_obj_pid>
				<ref_seq_no>83</ref_seq_no>
				<ref_text><![CDATA[{Woo89} Woodham R. J.: Photometric method for determining surface orientation from multiple images. 513--531. 7]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>84</ref_seq_no>
				<ref_text><![CDATA[{WvO96} Wyvill B., van Overveld K.: Polygonization of Implicit Surfaces with Constructive Solid Geometry. <i>Journal of Shape Modelling 2</i>, 4 (1996), 257--274. 66]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>85</ref_seq_no>
				<ref_text><![CDATA[{ZCS03} Zhang L., Curless B., Seitz S. M.: Spacetime stereo: Shape recovery for dynamic scenes. In <i>IEEE Conference on Computer Vision and Pattern Recognition</i> (June 2003), pp. 367--374. 39]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>86</ref_seq_no>
				<ref_text><![CDATA[{Zha99} Zhang Z.: Flexible camera calibration by viewing a plane from unknown orientations. In <i>International Conference on Computer Vision (ICCV)</i> (1999). 40]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>357025</ref_obj_id>
				<ref_obj_pid>357014</ref_obj_pid>
				<ref_seq_no>87</ref_seq_no>
				<ref_text><![CDATA[{Zha00} Zhang Z.: A flexible new technique for camera calibration. <i>IEEE Trans. Pattern Anal. Mach. Intell. 22</i>, 11 (2000), 1330--1334. 24, 25, 27]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566584</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>88</ref_seq_no>
				<ref_text><![CDATA[{ZPKG02} Zwicker M., Pauly M., Knoll O., Gross M.: Pointshop 3d: an interactive system for point-based surface editing. <i>ACM Trans. Graph. 21</i>, 3 (2002), 322--329. 58]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Build Your Own 3D Scanner: 3D Photography for Beginners  SIGGRAPH 2009 Course Notes Wednesday, August 
5, 2009 Douglas Lanman Gabriel Taubin Brown University Brown University dlanman@brown.edu taubin@brown.edu 
 Abstract Over the last decade digital photography has entered the mainstream with inexpensive, miniaturized 
cameras routinely included in consumer elec­tronics. Digital projection is poised to make a similar impact, 
with a va­riety of vendors offering small form factor, low-cost projectors. As a re­sult, active imaging 
is a topic of renewed interest in the computer graphics community. In particular, low-cost homemade 3D 
scanners are now within reach of students and hobbyists with a modest budget. This course provides a 
beginner with the necessary mathematics, soft­ware, and practical details to leverage projector-camera 
systems in their own 3D scanning projects. An example-driven approach is used through­out, with each 
new concept illustrated using a practical scanner imple­mented with off-the-shelf parts. First, the mathematics 
of triangulation is explained using the intersection of parametric and implicit representations of lines 
and planes in 3D. The particular case of ray-plane triangulation is illustrated using a scanner built 
with a single camera and a modi.ed laser pointer. Camera calibration is explained at this stage to convert 
image mea­surements to geometric quantities. A second example uses a single digital camera, a halogen 
lamp, and a stick. The mathematics of rigid-body trans­formations are covered through this example. Next, 
the details of projector calibration are explained through the development of a classic structured light 
scanning system using a single camera and projector pair. A minimal post-processing pipeline is described 
to convert the point clouds produced by the example scanners to watertight meshes. Key topics covered 
in this section include: surface representations, .le formats, data structures, polygonal meshes, and 
basic smoothing and gap-.lling opera­tions. The course concludes by detailing the use of such models 
in rapid prototyping, entertainment, cultural heritage, and web-based applications. An updated set of 
course notes and software are maintained at http: //mesh.brown.edu/dlanman/scan3d. Prerequisites Attendees 
should have a basic undergraduate-level knowledge of linear al­gebra. While executables are provided 
for beginners, attendees with prior knowledge of Matlab, C/C++, and Java programming will be able to 
di­rectly examine and modify the provided source code. Speaker Biographies Douglas Lanman Brown University 
dlanman@brown.edu http://mesh.brown.edu/dlanman Douglas Lanman is a fourth-year Ph.D. student at Brown 
University. As a graduate student his research has focused on computational photogra­phy, particularly 
in the use of active illumination for 3D reconstruction. He received a B.S. in Applied Physics with Honors 
from Caltech in 2002 and a M.S. in Electrical Engineering from Brown University in 2006. Prior to joining 
Brown, he was an Assistant Research Staff Member at MIT Lincoln Laboratory from 2002-2005. Douglas has 
worked as an intern at Intel, Los Alamos National Laboratory, INRIA Rh one-Alpes, Mitsubishi Electric 
Re­search Laboratories (MERL), and the MIT Media Lab. Gabriel Taubin Brown University taubin@brown.edu 
http://mesh.brown.edu/taubin Gabriel Taubin is an Associate Professor of Engineering and Computer Sci­ence 
at Brown University. He earned a Licenciado en Ciencias Matem´ aticas from the University of Buenos Aires, 
Argentina in 1981 and a Ph.D. in Elec­trical Engineering from Brown University in 1991. He was named 
an IEEE Fellow for his contributions to three-dimensional geometry compression technology and multimedia 
standards, won the Eurographics 2002 G ¨unter Enderle Best Paper Award, and was named an IBM Master Inventor. 
He has authored 58 reviewed book chapters, journal or conference papers, and is a co-inventor of 43 international 
patents. Before joining Brown in the Fall of 2003, he was a Research Staff Member and Manager at the 
IBM T. J. Wat­son Research Center since 1990. During the 2000-2001 academic year he was Visiting Professor 
of Electrical Engineering at Caltech. His main line of research has been related to the development of 
ef.cient, simple, and mathematically sound algorithms to operate on 3D objects represented as polygonal 
meshes, with an emphasis on technologies to enable the use of 3D models for web-based applications. 
 Course Outline First Session: 8:30 am 10:15 am 8:30 All Introduction 8:45 Taubin The Mathematics of 
3D Triangulation 9:05 Lanman 3D Scanning with Swept-Planes 9:30 Lanman Camera and Swept-Plane Light Source 
Calibration 10:00 Taubin Reconstruction and Visualization using Point Clouds Break: 10:15 am 10:30 am 
Second Session: 10:30 am 12:15 pm 10:30 Lanman Structured Lighting 10:45 Lanman Projector Calibration 
and Reconstruction 11:00 Taubin Combining Point Clouds from Multiple Views 11:25 Taubin Surface Reconstruction 
from Point Clouds 11:50 Taubin Elementary Mesh Processing 12:05 All Conclusion / Q &#38; A  Contents 
1 Introduction to 3D Photography 1 1.1 3DScanningTechnology ..................... 2 1.1.1 PassiveMethods 
..................... 2 1.1.2 ActiveMethods ...................... 4 1.2 ConceptsandScannersinthisCourse 
. . . . . . . . . . . . . 7  2 The Mathematics of Triangulation 9 2.1 Perspective Projection and the 
Pinhole Model . . . . . . . . . 9 2.2 GeometricRepresentations.................... 10 2.2.1 PointsandVectors 
.................... 11 2.2.2 Parametric Representation of Lines and Rays . . . . . 11 2.2.3 Parametric 
Representation of Planes . . . . . . . . . . 12 2.2.4 Implicit Representation of Planes . . . . . . . 
. . . . . 13 2.2.5 Implicit Representation of Lines . . . . . . . . . . . . 13 2.3 ReconstructionbyTriangulation 
. . . . . . . . . . . . . . . . 14 2.3.1 Line-PlaneIntersection.................. 14 2.3.2 Line-LineIntersection 
.................. 16 2.4 CoordinateSystems........................ 18 2.4.1 Image Coordinates and 
the Pinhole Camera . . . . . 19 2.4.2 TheIdealPinholeCamera ................ 19  2.4.3 TheGeneralPinholeCamera 
. . . . . . . . . . . . . . 20 2.4.4 LinesfromImagePoints ................. 22 2.4.5 PlanesfromImageLines................. 
22   3 Camera and Projector Calibration 24 3.1 CameraCalibration ........................ 25 3.1.1 
Camera Selection and Interfaces . . . . . . . . . . . . 25 3.1.2 Calibration Methods and Software . . 
. . . . . . . . . 26 3.1.3 CalibrationProcedure .................. 28 3.2 ProjectorCalibration ....................... 
30 3.2.1 Projector Selection and Interfaces . . . . . . . . . . . . 30 3.2.2 Calibration Methods and 
Software . . . . . . . . . . . 31 3.2.3 CalibrationProcedure .................. 32  4 3D Scanning with 
Swept-Planes 35 4.1 DataCapture............................ 35 4.2 VideoProcessing ......................... 
38 4.2.1 Spatial Shadow Edge Localization . . . . . . . . . . . 39 4.2.2 Temporal Shadow Edge Localization 
. . . . . . . . . . 40 4.3 Calibration............................. 40 4.3.1 IntrinsicCalibration ................... 
41 4.3.2 ExtrinsicCalibration ................... 41 4.4 Reconstruction........................... 
42 4.5 Post-processing and Visualization . . . . . . . . . . . . . . . 42  5 Structured Lighting 45 
5.1 DataCapture............................ 45 5.1.1 ScannerHardware .................... 45 5.1.2 
StructuredLightSequences . . . . . . . . . . . . . . . 47 5.2 ImageProcessing ......................... 
49 5.3 Calibration............................. 52 5.4 Reconstruction........................... 53 
 5.5 Post-processing and Visualization . . . . . . . . . . . . . . . 54  6 Surfaces from Point Clouds 
56 6.1 Representation and Visualization of Point Clouds . . . . . . 56 6.1.1 FileFormats ........................ 
57 6.1.2 Visualization ....................... 58 6.2 MergingPointClouds ...................... 58 
 6.2.1 Computing Rigid Body Matching Transformations . 59 6.2.2 The Iterative Closest Point (ICP) Algorithm 
. . . . . . 61  6.3 Surface Reconstruction from Point Clouds . . . . . . . . . . . 62 6.3.1 ContinuousSurfaces 
................... 62 6.3.2 DiscreteSurfaces ..................... 62 6.3.3 Isosurfaces......................... 
63 6.3.4 Isosurface Construction Algorithms . . . . . . . . . . 63 6.3.5 Algorithms to Fit Implicit 
Surfaces to Point Clouds . 67  7 Applications and Emerging Trends 69 7.1 Extending Swept-Planes and 
Structured Light . . . . . . . . 69 7.1.1 3D Slit Scanning with Planar Constraints . . . . . . . 70 7.1.2 
Surround Structured Lighting . . . . . . . . . . . . . . 73  7.2 RecentAdvancesandFurtherReading. . 
. . . . . . . . . . . 77 7.3 Conclusion............................. 79 Bibliography 80 Chapter 1  
 Introduction to 3D Photography Over the last decade digital photography has entered the mainstream with 
inexpensive, miniaturized cameras routinely included in consumer elec­tronics. Digital projection is 
poised to make a similar impact, with a vari­ety of vendors offering small, low-cost projectors. As a 
result, active imag­ing is a topic of renewed interest in the computer graphics community. In particular, 
homemade 3D scanners are now within reach of students and hobbyists with a modest budget. This course 
provides a beginner with the necessary mathematics, soft­ware, and practical details to leverage projector-camera 
systems in their own 3D scanning projects. An example-driven approach is used through­out; each new concept 
is illustrated using a practical scanner implemented with off-the-shelf parts. A minimal post-processing 
pipeline is presented for merging multiple scans to produce watertight meshes. The course con­cludes 
by detailing how these approaches are used in rapid prototyping, entertainment, cultural heritage, and 
web-based applications. These course notes are organized into three primary sections, span­ning theoretical 
concepts, practical construction details, and algorithms for constructing high-quality 3D models. Chapters 
1 and 2 survey the .eld and present the unifying concept of triangulation. Chapters 3 5 document the 
construction of projector-camera systems, swept-plane scanners, and structured lighting, respectively. 
The post-processing pipeline and recent advances are covered in Chapters 6 7. We encourage attendees 
to email the authors with questions or links to their own 3D scanning projects that draw on the course 
material. Revised course notes, updated software, re­cent publications, and similar do-it-yourself projects 
are maintained on the course website at http://mesh.brown.edu/dlanman/scan3d. 1.1 3D Scanning Technology 
Metrology is an ancient and diverse .eld, bridging the gap between mathe­matics and engineering. Efforts 
at measurement standardization were .rst undertaken by the Indus Valley Civilization as early as 2600 
1900 BCE. Even with only crude units, such as the length of human appendages, the development of geometry 
revolutionized the ability to measure distance accurately. Around 240 BCE, Eratosthenes estimated the 
circumference of the Earth from knowledge of the elevation angle of the Sun during the sum­mer solstice 
in Alexandria and Syene. Mathematics and standardization efforts continued to mature through the Renaissance 
(1300 1600 CE) and into the Scienti.c Revolution (1550 1700 CE). However, it was the Indus­trial Revolution 
(1750 1850 CE) which drove metrology to the forefront. As automatized methods of mass production became 
commonplace, ad­vanced measurement technologies ensured interchangeable parts were just that accurate 
copies of the original. Through these historical developments, measurement tools varied with mathematical 
knowledge and practical needs. Early methods required di­rect contact with a surface (e.g., callipers 
and rulers). The pantograph, in­vented in 1603 by Christoph Scheiner, uses a special mechanical linkage 
so movement of a stylus (in contact with the surface) can be precisely du­plicated by a drawing pen. 
The modern coordinate measuring machine (CMM) functions in much the same manner, recording the displacement 
of a probe tip as it slides across a solid surface (see Figure 1.1). While effective, such contact-based 
methods can harm fragile objects and require long pe­riods of time to build an accurate 3D model. Non-contact 
scanners address these limitations by observing, and possibly controlling, the interaction of light with 
the object. 1.1.1 Passive Methods Non-contact optical scanners can be categorized by the degree to which 
controlled illumination is required. Passive scanners do not require di­rect control of any illumination 
source, instead relying entirely on ambi­ent light. Stereoscopic imaging is one of the most widely used 
passive 3D imaging systems, both in biology and engineering. Mirroring the human visual system, stereoscopy 
estimates the position of a 3D scene point by triangulation [LN04]; .rst, the 2D projection of a given 
point is identi.ed in each camera. Using known calibration objects, the imaging properties of each camera 
are estimated, ultimately allowing a single 3D line to be drawn from each camera s center of projection 
through the 3D point. The intersection of these two lines is then used to recover the depth of the point. 
 Trinocular [VF92] and multi-view stereo [HZ04] systems have been in­troduced to improve the accuracy 
and reliability of conventional stereo­scopic systems. However, all such passive triangulation methods 
require correspondences to be found among the various viewpoints. Even for stereo vision, the development 
of matching algorithms remains an open and chal­lenging problem in the .eld [SCD*06]. Today, real-time 
stereoscopic and multi-view systems are emerging, however certain challenges continue to limit their 
widespread adoption [MPL04]. Foremost, .at or periodic tex­ tures prevent robust matching. While machine 
learning methods and prior knowledge are being advanced to solve such problems, multi-view 3D scan­ning 
remains somewhat outside the domain of hobbyists primarily con­cerned with accurate, reliable 3D measurement. 
Many alternative passive methods have been proposed to sidestep the correspondence problem, often times 
relying on more robust computer vi­sion algorithms. Under controlled conditions, such as a known or constant 
background, the external boundaries of foreground objects can be reliably identi.ed. As a result, numerous 
shape-from-silhouette algorithms have emerged. Laurentini [Lau94] considers the case of a .nite number 
of cam­ eras observing a scene. The visual hull is de.ned as the union of the gener­alized viewing cones 
de.ned by each camera s center of projection and the detected silhouette boundaries. Recently, free-viewpoint 
video [CTMS03] systems have applied this algorithm to allow dynamic adjustment of view­point [MBR*00, 
SH03]. Cipolla and Giblin [CG00] consider a differential formulation of the problem, reconstructing depth 
by observing the visual motion of occluding contours (such as silhouettes) as a camera is perturbed. 
Optical imaging systems require a suf.ciently large aperture so that enough light is gathered during 
the available exposure time [Hec01]. Cor­ respondingly, the captured imagery will demonstrate a limited 
depth of .eld; only objects close to the plane of focus will appear in sharp contrast, with distant objects 
blurred together. This effect can be exploited to recover depth, by increasing the aperture diameter 
to further reduce the depth of .eld. Nayar and Nakagawa [NN94] estimate shape-from-focus, collecting 
a focal stack by translating a single element (either the lens, sensor, or ob­ject). A focus measure 
operator [WN98] is then used to identify the plane of best focus, and its corresponding distance from 
the camera. Other passive imaging systems further exploit the depth of .eld by modifying the shape of 
the aperture. Such modi.cations are performed so that the point spread function (PSF) becomes invertible 
and strongly depth-dependent. Levin et al. [LFDF07] and Farid [Far97] use such coded apertures to estimate 
intensity and depth from defocused images. Green­gard et al. [GSP06] modify the aperture to produce a 
PSF whose rotation is a function of scene depth. In a similar vein, shadow moir´e is produced by placing 
a high-frequency grating between the scene and the camera. The resulting interference patterns exhibit 
a series of depth-dependent fringes. While the preceding discussion focused on optical modi.cations for 
3D reconstruction from 2D images, numerous model-based approaches have also emerged. When shape is known 
a priori, then coarse image measure­ments can be used to infer object translation, rotation, and deformation. 
Such methods have been applied to human motion tracking [KM00, OSS*00, dAST*08], vehicle recognition 
[Sul95, FWM98], and human-computer in­ teraction [RWLB01]. Additionally, user-assisted model construction 
has been demonstrated using manual labeling of geometric primitives [Deb97]. 1.1.2 Active Methods Active 
optical scanners overcome the correspondence problem using con­trolled illumination. In comparison to 
non-contact and passive methods, active illumination is often more sensitive to surface material properties. 
Strongly re.ective or translucent objects often violate assumptions made by active optical scanners, 
requiring additional measures to acquire such problematic subjects. For a detailed history of active 
methods, we refer the reader to the survey article by Blais [Bla04]. In this section we discuss some 
key milestones along the way to the scanners we consider in this course. Many active systems attempt 
to solve the correspondence problem by replacing one of the cameras, in a passive stereoscopic system, 
with a con­trollable illumination source. During the 1970s, single-point laser scanning emerged. In this 
scheme, a series of .xed and rotating mirrors are used to raster scan a single laser spot across a surface. 
A digital camera records the motion of this .ying spot . The 2D projection of the spot de.nes, with appropriate 
calibration knowledge, a line connecting the spot and the cam­era s center of projection. The depth is 
recovered by intersecting this line with the line passing from the laser source to the spot, given by 
the known de.ection of the mirrors. As a result, such single-point scanners can be seen as the optical 
equivalent of coordinate measuring machines. As with CMMs, single-point scanning is a painstakingly slow 
process. With the development of low-cost, high-quality CCD arrays in the 1980s, slit scanners emerged 
as a powerful alternative. In this design, a laser pro­jector creates a single planar sheet of light. 
This slit is then mechanically­swept across the surface. As before, the known de.ection of the laser 
source de.nes a 3D plane. The depth is recovered by the intersection of this plane with the set of lines 
passing through the 3D stripe on the surface and the camera s center of projection. Effectively removing 
one dimension of the raster scan, slit scanners re­main a popular solution for rapid shape acquisition. 
A variety of com­mercial products use swept-plane laser scanning, including the Polhemus FastSCAN [Pol], 
the NextEngine [Nex], the SLP 3D laser scanning probes from Laser Design [Las], and the HandyScan line 
of products [Cre]. While effective, slit scanners remain dif.cult to use if moving objects are present 
in the scene. In addition, because of the necessary separation between the light source and camera, certain 
occluded regions cannot be reconstructed. This limitation, while shared by many 3D scanners, requires 
multiple scans to be merged further increasing the data acquisition time. A digital structured light 
projector can be used to eliminate the me­chanical motion required to translate the laser stripe across 
the surface. Na¨ively, the projector could be used to display a single column (or row) of white pixels 
translating against a black background to replicate the per­formance of a slit scanner. However, a simple 
swept-plane sequence does not fully exploit the projector, which is typically capable of displaying ar­bitrary 
24-bit color images. Structured lighting sequences have been de­veloped which allow the projector-camera 
correspondences to be assigned in relatively few frames. In general, the identity of each plane can be 
en­coded spatially (i.e., within a single frame) or temporally (i.e., across multi­ple frames), or with 
a combination of both spatial and temporal encodings. There are bene.ts and drawbacks to each strategy. 
For instance, purely spatial encodings allow a single static pattern to be used for reconstruction, enabling 
dynamic scenes to be captured. Alternatively, purely temporal en­codings are more likely to bene.t from 
redundancy, reducing reconstruc­tion artifacts. We refer the reader to a comprehensive assessment of 
such codes by Salvi et al. [SPB04]. Both slit scanners and structured lighting are ill-suited for scanning 
dy­namic scenes. In addition, due to separation of the light source and cam­era, certain occluded regions 
will not be recovered. In contrast, time-of­.ight range.nders estimate the distance to a surface from 
a single center of projection. These devices exploit the .nite speed of light. A single pulse of light 
is emitted. The elapsed time, between emitting and receiving a pulse, is used to recover the object distance 
(since the speed of light is known). Several economical time-of-.ight depth cameras are now com­mercially 
available, including Canesta s CANESTAVISION [HARN06] and 3DV s Z-Cam [IY01]. However, the depth resolution 
and accuracy of such systems (for static scenes) remain below that of slit scanners and structured lighting. 
Active imaging is a broad .eld; a wide variety of additional schemes have been proposed, typically trading 
system complexity for shape ac­curacy. As with model-based approaches in passive imaging, several ac­tive 
systems achieve robust reconstruction by making certain simplifying assumptions about the topological 
and optical properties of the surface. Woodham [Woo89] introduces photometric stereo, allowing smooth 
sur­ faces to be recovered by observing their shading under at least three (spa­tially disparate) point 
light sources. Hern´andez et al. [HVB*07] further demonstrate a real-time photometric stereo system using 
three colored light sources. Similarly, the complex digital projector required for structured lighting 
can be replaced by one or more printed gratings placed next to the projector and camera. Like shadow 
moir´e systems e, such projection moir´create depth-dependent fringes. However, certain ambiguities remain 
in the reconstruction unless the surface is assumed to be smooth. Active and passive 3D scanning methods 
continue to evolve, with re­cent progress reported annually at various computer graphics and vision conferences, 
including 3-D Digital Imaging and Modeling (3DIM), SIG-GRAPH, Eurographics, CVPR, ECCV, and ICCV. Similar 
advances are also published in the applied optics communities, typically through various SPIE and OSA 
journals. We will survey several promising recent works in Chapter 7.  1.2 Concepts and Scanners in 
this Course This course is grounded in the unifying concept of triangulation. At their core, stereoscopic 
imaging, slit scanning, and structured lighting all at­tempt to recover the shape of 3D objects in the 
same manner. First, the correspondence problem is solved, either by a passive matching algorithm or by 
an active space-labeling approach (e.g., projecting known lines, planes, or other patterns). After establishing 
correspondences across two or more views (e.g., between a pair of cameras or a single projector-camera 
pair), triangulation recovers the scene depth. In stereoscopic and multi­view systems, a point is reconstructed 
by intersecting two or more corre­sponding lines. In slit scanning and structured lighting systems, a 
point is recovered by intersecting corresponding lines and planes. To elucidate the principles of such 
triangulation-based scanners, this course describes how to construct classic slit scanners, as well as 
a struc­tured lighting system. As shown in Figure 1.3, our slit scanner is inspired by the work of Bouguet 
and Perona [BP]. In this design, a wooden stick and halogen lamp replicate the function of a manually-translated 
laser stripe projector, allowing shadow planes to be swept through the scene. The de­tails of its construction 
are presented in Chapter 4. As shown in Figure 1.4, our structured lighting system contains a single 
projector and one or more digital cameras. In Chapter 5, we describe its construction and examine several 
temporally-encoded illumination sequences.  By providing example data sets, open source software, and 
detailed im­plementation notes, we hope to enable beginners and hobbyists to replicate our results. We 
believe the process of building your own 3D scanner is enjoyable and instructive. Along the way, you 
ll likely learn a great deal about the practical use of projector-camera systems, hopefully in a manner 
that supports your own research. To that end, we conclude in Chapter 7 by discussing some of the projects 
that emerged when this course was pre­viously taught at Brown University in 2007 and 2009. We will continue 
to update these notes and the website with links to any do-it-yourself scan­ners or research projects 
undertaken by course attendees. Chapter 2  The Mathematics of Triangulation This course is primarily 
concerned with the estimation of 3D shape by il­luminating the world with certain known patterns, and 
observing the illu­minated objects with cameras. In this chapter we derive models describing this image 
formation process, leading to the development of reconstruction equations allowing the recovery of 3D 
shape by geometric triangulation. We start by introducing the basic concepts in a coordinate-free fash­ion, 
using elementary algebra and the language of analytic geometry (e.g., points, vectors, lines, rays, and 
planes). Coordinates are introduced later, along with relative coordinate systems, to quantify the process 
of image formation in cameras and projectors. 2.1 Perspective Projection and the Pinhole Model A simple 
and popular geometric model for a camera or a projector is the pinhole model, composed of a plane and 
a point external to the plane (see Figure 2.1). We refer to the plane as the image plane, and to the 
point as the center of projection. In a camera, every 3D point (other than the center of projection) 
determines a unique line passing through the center of projec­tion. If this line is not parallel to the 
image plane, then it must intersect the image plane in a single image point. In mathematics, this mapping 
from 3D points to 2D image points is referred to as a perspective projection. Except for the fact that 
light traverses this line in the opposite direction, the geometry of a projector can be described with 
the same model. That is, given a 2D image point in the projector s image plane, there must exist a unique 
line containing this point and the center of projection (since the center of pro­jection cannot belong 
to the image plane). In summary, light travels away from a projector (or towards a camera) along the 
line connecting the 3D scene point with its 2D perspective projection onto the image plane.  2.2 Geometric 
Representations Since light moves along straight lines (in a homogeneous medium such as air), we derive 
3D reconstruction equations from geometric constructions involving the intersection of lines and planes, 
or the approximate intersec­tion of pairs of lines (two lines in 3D may not intersect). Our derivations 
only draw upon elementary algebra and analytic geometry in 3D (e.g., we operate on points, vectors, lines, 
rays, and planes). We use lower case let­ters to denote points p and vectors v. All the vectors will 
be taken as column vectors with real-valued coordinates v .IR3, which we can also regard as matrices 
with three rows and one column v .IR3×1. The length of a vector v is a scalar lvl.IR. We use matrix multiplication 
notation for the inner t product v1v2 .IR of two vectors v1 and v2, which is also a scalar. Here t v.IR1×3 
is a row vector, or a 1 ×3 matrix, resulting from transposing 1 the column vector v1. The value of the 
inner product of the two vectors v1 and v2 is equal to lv1llv2lcos(a), where a is the angle formed by 
the two vectors (0 =a =180.). The 3 ×N matrix resulting from concatenating N vectors v1,...,vN as columns 
is denoted [v1|···|vN ] .IR3×N . The vector product v1 ×v2 .IR3 of the two vectors v1 and v2 is a vector 
perpendicu­lar to both v1 and v2, of length lv1 ×v2l= lv1llv2lsin(a), and direction determined by the 
right hand rule (i.e., such that the determinant of the matrix [v1|v2|v1 × v2] is non-negative). In particular, 
two vectors v1 and v2 are linearly dependent ( i.e., one is a scalar multiple of the other), if and only 
if the vector product v1 × v2 is equal to zero. 2.2.1 Points and Vectors Since vectors form a vector 
space, they can be multiplied by scalars and added to each other. Points, on the other hand, do not form 
a vector space. But vectors and points are related: a point plus a vector p + v is another point, and 
the difference between two points q - p is a vector. If p is a point, . is a scalar, and v is a vector, 
then q = p + .v is another point. In this expression, .v is a vector of length |.|lvl. Multiplying a 
point by a scalar .p is not de.ned, but an af.ne combination of N points .1p1 + ··· + .N pN , with .1 
+ ··· + .N =1, is well de.ned: .1p1 + ··· + .N pN = p1 + .2(p2 - p1)+ ··· + .N (pN - p1) . 2.2.2 Parametric 
Representation of Lines and Rays A line L can be described by specifying one of its points q and a direction 
vector v (see Figure 2.2). Any other point p on the line L can be described as the result of adding a 
scalar multiple .v, of the direction vector v, to the point q (. can be positive, negative, or zero): 
L = {p = q + .v : . . IR} . (2.1) This is the parametric representation of a line, where the scalar . 
is the pa­rameter. Note that this representation is not unique, since q can be replaced by any other 
point on the line L, and v can be replaced by any non-zero scalar multiple of v. However, for each choice 
of q and v, the correspon­dence between parameters . . IR and points p on the line L is one-to-one. 
A ray is half of a line. While in a line the parameter . can take any value, in a ray it is only allowed 
to take non-negative values. R = {p = q + .v : . = 0} In this case, if the point q is changed, a different 
ray results. Since it is unique, the point q is called the origin of the ray. The direction vector v 
can be replaced by any positive scalar multiple, but not by a negative scalar mul­tiple. Replacing the 
direction vector v by a negative scalar multiple results in the opposite ray. By convention in projectors, 
light traverses rays along the direction determined by the direction vector. Conversely in cameras, light 
traverses rays in the direction opposite to the direction vector (i.e., in the direction of decreasing 
.). 2.2.3 Parametric Representation of Planes Similar to how lines are represented in parametric form, 
a plane P can be described in parametric form by specifying one of its points q and two lin­early independent 
direction vectors v1 and v2 (see Figure 2.3). Any other point p on the plane P can be described as the 
result of adding scalar mul­tiples .1v1 and .2v2 of the two vectors to the point q, as follows. P = {p 
= q + .1v1 + .2v2 : .1,.2 . IR} As in the case of lines, this representation is not unique. The point 
q can be replaced by any other point in the plane, and the vectors v1 and v2 can be replaced by any other 
two linearly independent linear combinations of v1 and v2. 2.2.4 Implicit Representation of Planes A 
plane P can also be described in implicit form as the set of zeros of a linear equation in three variables. 
Geometrically, the plane can be described by one of its points q and a normal vector n. A point p belongs 
to the plane P if and only if the vectors p - q and n are orthogonal, such that P = {p : n t(p - q)=0} 
. (2.2) Again, this representation is not unique. The point q can be replaced by any other point in the 
plane, and the normal vector n by any non-zero scalar multiple .n. To convert from the parametric to 
the implicit representation, we can take the normal vector n = v1 × v2 as the vector product of the two 
basis vectors v1 and v2. To convert from implicit to parametric, we need to .nd two linearly independent 
vectors v1 and v2 orthogonal to the normal vector n. In fact, it is suf.cient to .nd one vector v1 orthogonal 
to n. The second vector can be de.ned as v2 = n × v1. In both cases, the same point q from one representation 
can be used in the other. 2.2.5 Implicit Representation of Lines A line L can also be described in implicit 
form as the intersection of two planes, both represented in implicit form, such that tt L = {p : n (p 
- q)= n (p - q)=0}, (2.3) 12 where the two normal vectors n1 and n2 are linearly independent (if n1 an 
n2 are linearly dependent, rather than a line, the two equations describe the same plane). Note that 
when n1 and n2 are linearly independent, the two implicit representations for the planes can be de.ned 
with respect to a common point belonging to both planes, rather than to two different points. Since a 
line can be described as the intersection of many different pairs of planes, this representation is not 
unique. The point q can be replaced by any other point belonging to the intersection of the two planes, 
and the two normal vectors can be replaced by any other pair of linearly independent linear combinations 
of the two vectors. To convert from the parametric representation of Equation 2.1 to the implicit representation 
of Equation 2.3, one needs to .nd two linearly in­ dependent vectors n1 and n2 orthogonal to the direction 
vector v. One way to do so is to .rst .nd one non-zero vector n1 orthogonal to v, and then take n2 as 
the vector product n2 = v × n1 of v and n1. To convert from implicit to parametric, one needs to .nd 
a non-zero vector v orthogonal to both normal vectors n1 and n2. The vector product v = n1 × n2 is one 
such vector, and any other is a scalar multiple of it.  2.3 Reconstruction by Triangulation As will 
be discussed in Chapters 4 and 5, it is common for projected illu­ mination patterns to contain identi.able 
lines or points. Under the pinhole projector model, a projected line creates a plane of light (the unique 
plane containing the line on the image plane and the center of projection), and a projected point creates 
a ray of light (the unique line containing the image point and the center of projection). While the intersection 
of a ray of light with the object being scanned can be considered as a single illuminated point, the 
intersection of a plane of light with the object generally contains many illuminated curved segments 
(see Figure 1.2). Each of these segments is composed of many illuminated points. A single illuminated 
point, visible to the camera, de.nes a camera ray. For now, we assume that the locations and orientations 
of projector and camera are known with respect to the global coordinate system (with procedures for estimating 
these quantities covered in Chapter 3). Under this assumption, the equations of projected planes and 
rays, as well as the equations of camera rays corresponding to illuminated points, are de.ned by parameters 
which can be measured. From these measurements, the lo­cation of illuminated points can be recovered 
by intersecting the planes or rays of light with the camera rays corresponding to the illuminated points. 
Through such procedures the depth ambiguity introduced by pinhole pro­jection can be eliminated, allowing 
recovery of a 3D surface model. 2.3.1 Line-Plane Intersection Computing the intersection of a line and 
a plane is straightforward when the line is represented in parametric form L = {p = qL + .v : . . IR}, 
and the plane is represented in implicit form P = {p : n t(p - qP )=0} . Note that the line and the 
plane may not intersect, in which case we say that the line and the plane are parallel. This is the case 
if the vectors v and t n are orthogonal nv =0. The vectors v and n are also orthogonal when the line 
L is contained in the plane P . Whether or not the point qL belongs to the plane P differentiates one 
case from the other. If the vectors v and n t are not orthogonal nv =0, then the intersection of the 
line and the plane contains exactly one point p. Since this point belongs to the line, it can be written 
as p = qL + .v, for a value . which we need to determine. Since the point also belongs to the plane, 
the value . must satisfy the linear equation n t(p - qp)= n t(.v + qL - qp)=0 , or equivalently nt(qP 
- qL) . = . (2.4) ntv Since we have assumed that the line and the plane are not parallel (i.e., t by 
checking that nv =0 beforehand), this expression is well de.ned. A geometric interpretation of line-plane 
intersection is provided in Figure 2.4.  2.3.2 Line-Line Intersection We consider here the intersection 
of two arbitrary lines L1 and L2, as shown in Figure 2.5. L1 = {p = q1 + .1v1 : .1 . IR} and L2 = {p 
= q2 + .2v2 : .2 . IR} Let us .rst identify the special cases. The vectors v1 and v2 can be lin­early 
dependent (i.e., if one is a scalar multiple of the other) or linearly independent. The two lines are 
parallel if the vectors v1 and v2 are linearly dependent. If, in addition, the vector q2 - q1 can also 
be written as a scalar multiple of v1 or v2, then the lines are identical. Of course, if the lines are 
parallel but not identical, they do not intersect. If v1 and v2 are linearly independent, the two lines 
may or may not intersect. If the two lines intersect, the intersection contains a single point. The necessary 
and suf.cient condition for two lines to intersect, when v1 and v2 are linearly independent, is that 
scalar values .1 and .2 exist so that q1 + .1v1 = q2 + .2v2, or equivalently so that the vector q2 - 
q1 is linearly dependent on v1 and v2. Since two lines may not intersect, we de.ne the approximate intersection 
as the point which is closest to the two lines. More precisely, whether two lines intersect or not, we 
de.ne the approximate intersection as the point p which minimizes the sum of the square distances to 
both lines f(p, .1,.2)= lq1 + .1v1 - pl2 + lq2 + .2v2 - pl2 . As before, we assume v1 and v2 are linearly 
independent, such the approx­imate intersection is a unique point. To prove that the previous statement 
is true, and to determine the value of p, we follow an algebraic approach. The function f(p, .1,.2) is 
a quadratic non-negative de.nite function of .ve variables, the three coordinates of the point p and 
the two scalars .1 and .2. We .rst reduce the problem to the minimization of a different quadratic non-negative 
de.nite function of only two variables .1 and .2. Let p1 = q1 + .1v1 be a point on the line L1, and let 
p2 = q2 + .2v2 be a point on the line L2. De.ne the midpoint p12, of the line segment joining p1 and 
p2, as 11 p12 = p1 +(p2 - p1)= p2 +(p1 - p2) . 22 A necessary condition for the minimizer (p, .1,.2) 
of f is that the partial derivatives of f, with respect to the .ve variables, all vanish at the mini­mizer. 
In particular, the three derivatives with respect to the coordinates of the point p must vanish .f =(p 
- p1)+(p - p2)=0 , .p or equivalently, it is necessary for the minimizer point p to be the midpoint p12 
of the segment joining p1 and p2 (see Figure 2.6). As a result, the problem reduces to the minimization 
of the square dis­tance from a point p1 on line L1 to a point p2 on line L2. Practically, we must now 
minimize the quadratic non-negative de.nite function of two variables .(.1,.2)=2f(p12,.1,.2)= l(q2 + 
.2v2) - (q1 + .1v1)l2 . Note that it is still necessary for the two partial derivatives of ., with re­spect 
to .1 and .2, to be equal to zero at the minimum, as follows. .. t 1 (.1v1 - .2v2 + q1 - q2)= .1lv1l- 
.2v t+vv21 t 1 (q1 - q2)=0 = v ..1 .. (.2v2 - .1v1 + q2 - q1)= .2lv2l2 - .2v t 2v1 + v These provide 
two linear equations in .1 and .2, which can be concisely expressed in matrix form as . .. .. . 2v1 t 
t t (q2 - q1)=0 = v 2 2 ..2 lv1l2 (q2 - q1) tv21 t -v .1 v 1 = . lv2l2 (q1 - q2) t -v .2 v 2 t 
It follows from the linear independence of v1 and v2 that the 2 × 2 matrix on the left hand side is non-singular. 
As a result, the unique solution to the linear system is given by 2v1 .-1 . lv1l2 (q2 - q1) t 1v2 lv2l2 
t -v .1 v 1 = (q1 - q2) t .2 -v v 2 t t t or equivalently .. ... . 1v2 1v2)22v1 1 lv2l2 (q2 - q1) 
t .1 v v 1 . (2.5) = lv1l2 (q1 - q2) t lv1l2lv2l2 - (v .2 v v 2 In conclusion, the approximate 
intersection p can be obtained from the value of either .1 or .2 provided by these expressions.  2.4 
Coordinate Systems So far we have presented a coordinate-free description of triangulation. In practice, 
however, image measurements are recorded in discrete pixel units. In this section we incorporate such 
coordinates into our prior equa­tions, as well as document the various coordinate systems involved. 
2.4.1 Image Coordinates and the Pinhole Camera Consider a pinhole model with center of projection o and 
image plane P = 1212 {p = q + uv1 + uv2 : u,u. IR}. Any 3D point p, not necessarily on 123)t the image 
plane, has coordinates (p,p,prelative to the origin of the world coordinate system. On the image plane, 
the point q and vectors v1 and v2 de.ne a local coordinate system. The image coordinates of a point 12 
p = q + uv1 + uv2 are the parameters u1 and u2, which can be written as 12 a 3D vector u =(u,u, 1). Using 
this notation point p is expressed as .. .. 1 1 p u 2 2 . .. p. =[v1|v2|q] u. 3 p 1 2.4.2 The Ideal 
Pinhole Camera In the ideal pinhole camera shown in Figure 2.7, the center of projection o is at the 
origin of the world coordinate system, with coordinates (0, 0, 0)t, and the point q and the vectors v1 
and v2 are de.ned as .. 100 .. [v1|v2|q]= 010 . 001 Note that not every 3D point has a projection on 
the image plane. Points without a projection are contained in a plane parallel to the image passing through 
the center of projection. An arbitrary 3D point p with coordinates 12 3 (p,p,p3)t belongs to this plane 
if p=0, otherwise it projects onto an image point with the following coordinates. 1 u= p1/p3 2 u= p2/p3 
There are other descriptions for the relation between the coordinates of a point and the image coordinates 
of its projection; for example, the projec­ 123)t tion of a 3D point p with coordinates (p,p,phas image 
coordinates 12 u =(u,u, 1) if, for some scalar . =0, we can write .. .. 11 up 22 . .u. = .p. . (2.6) 
3 1 p 2.4.3 The General Pinhole Camera The center of a general pinhole camera is not necessarily placed 
at the ori­gin of the world coordinate system and may be arbitrarily oriented. How­ever, it does have 
a camera coordinate system attached to the camera, in addi­tion to the world coordinate system (see Figure 
2.8). A 3D point p has world 123 coordinates described by the vector pW =(pW ,pW ,p)t and camera co- 
W 123 ordinates described by the vector pC =(pC,pC ,p)t . These two vectors C are related by a rigid 
body transformation speci.ed by a translation vector T . IR3 and a rotation matrix R . IR3×3, such that 
pC = RpW + T. In camera coordinates, the relation between the 3D point coordinates and the 2D image coordinates 
of the projection is described by the ideal pinhole camera projection (i.e., Equation 2.6), with .u = 
pC . In world coordinates this relation becomes .u = RpW + T. (2.7) The parameters (R, T ), which are 
referred to as the extrinsic parameters of the camera, describe the location and orientation of the camera 
with respect to the world coordinate system. Equation 2.7 assumes that the unit of measurement of lengths 
on the image plane is the same as for world coordinates, that the distance from the center of projection 
to the image plane is equal to one unit of length, and that the origin of the image coordinate system 
has image coordinates u1 =0 2 and u=0. None of these assumptions hold in practice. For example, lengths 
on the image plane are measured in pixel units, and in meters or inches for world coordinates, the distance 
from the center of projection to the image plane can be arbitrary, and the origin of the image coordinates 
is usually on the upper left corner of the image. In addition, the image plane may be tilted with respect 
to the ideal image plane. To compensate for these limitations of the current model, a matrix K . IR3×3 
is introduced in the projection equations to describe intrinsic parameters as follows. .u = K(RpW + T 
) (2.8) The matrix K has the following form .. fs1 fs. o1 2 K = . 0 fs2 o. , 0 01 where f is the focal 
length (i.e., the distance between the center of projection and the image plane). The parameters s1 and 
s2 are the .rst and second coordinate scale parameters, respectively. Note that such scale parameters 
are required since some cameras have non-square pixels. The parameter 1 s. is used to compensate for 
a tilted image plane. Finally, (o,o2)t are the image coordinates of the intersection of the vertical 
line in camera coordi­nates with the image plane. This point is called the image center or principal 
point. Note that all intrinsic parameters embodied in K are independent of the camera pose. They describe 
physical properties related to the mechan­ical and optical design of the camera. Since in general they 
do not change, the matrix K can be estimated once through a calibration procedure and stored (as will 
be described in the following chapter). Afterwards, image plane measurements in pixel units can immediately 
be normalized , by multiplying the measured image coordinate vector by K-1, so that the re­lation between 
a 3D point in world coordinates and 2D image coordinates is described by Equation 2.7. Real cameras also 
display non-linear lens distortion, which is also con­sidered intrinsic. Lens distortion compensation 
must be performed prior to the normalization described above. We will discuss appropriate lens dis­tortion 
models in Chapter 3. 2.4.4 Lines from Image Points 12 As shown in Figure 2.9, an image point with coordinates 
u =(u,u, 1)t de.nes a unique line containing this point and the center of projection. The challenge is 
to .nd the parametric equation of this line, as L = {p = q+.v : . . IR}. Since this line must contain 
the center of projection, the projection of all the points it spans must have the same image coordinates. 
If pW is the vector of world coordinates for a point contained in this line, then world coordinates and 
image coordinates are related by Equation 2.7 such that .u = RpW + T . Since R is a rotation matrix, 
we have R-1 = Rt and we can rewrite the projection equation as pW =(-RtT )+ . (Rt u) . In conclusion, 
the line we are looking for is described by the point q with world coordinates qW = -RtT , which is the 
center of projection, and the vector v with world coordinates vW = Rtu. 2.4.5 Planes from Image Lines 
A straight line on the image plane can be described in either parametric or implicit form, both expressed 
in image coordinates. Let us .rst consider the implicit case. A line on the image plane is described 
by one implicit equation of the image coordinates L = {u : lt u = l1 u 1 + l2 u 2 + l3 =0} , where l 
=(l1,l2,l3)t is a vector with l1 =0 or l2 =0. Using active il­lumination, projector patterns containing 
vertical and horizontal lines are common. Thus, the implicit equation of an horizontal line is LH = {u 
: lt u = u 2 - . =0} , where . is the second coordinate of a point on the line. In this case we can 
take l = (0, 1, -.)t. Similarly, the implicit equation of a vertical line is LV = {u : lt u = u 1 - . 
=0} , where . is now the .rst coordinate of a point on the line. In this case we can take l = (1, 0, 
-.)t . There is a unique plane P containing this line L and the center of projection. For each image 
point with image coordinates u on the line L, the line containing this point and the center of projection 
is contained in P . Let p be a point on the plane P with world coordinates pW projecting onto an image 
point with image coordinates u. Since these two vectors of coordinates satisfy Equation 2.7, for which 
.u = RpW + T , and the vector u satis.es the implicit equation de.ning the line L, we have 0= .lt u = 
lt(RpW + T )=(Rtl)t (pW - (-RtT )) . In conclusion, the implicit representation of plane P , corresponding 
to Equa­tion 2.2 for which P = {p : nt(p - q)=0}, can be obtained with n being the vector with world 
coordinates nW = Rtl and q the point with world coordinates qW = -RtT , which is the center of projection. 
Chapter 3  Camera and Projector Calibration Triangulation is a deceptively simple concept, simply involving 
the pair­wise intersection of 3D lines and planes. Practically, however, one must carefully calibrate 
the various cameras and projectors so the equations of these geometric primitives can be recovered from 
image measurements. In this chapter we lead the reader through the construction and calibration of a 
basic projector-camera system. Through this example, we examine how freely-available calibration packages, 
emerging from the computer vision community, can be leveraged in your own projects. While touching on 
the basic concepts of the underlying algorithms, our primarily goal is to help beginners overcome the 
calibration hurdle . In Section 3.1 we describe how to select, control, and calibrate a dig­ ital camera 
suitable for 3D scanning. The general pinhole camera model presented in Chapter 2 is extended to address 
lens distortion. A simple cal­ ibration procedure using printed checkerboard patterns is presented, fol­lowing 
the established method of Zhang [Zha00]. Typical calibration re­sults, obtained for the cameras used 
in Chapters 4 and 5, are provided as a reference. While well-documented, freely-available camera calibration 
tools have emerged in recent years, community tools for projector calibration have re­ceived signi.cantly 
less attention. In Section 3.2, we describe custom pro­ jector calibration software developed for this 
course. A simple procedure is used, wherein a calibrated camera observes a planar object with both printed 
and projected checkerboards on its surface. Considering the projec­tor as an inverse camera, we describe 
how to estimate the various parame­ters of the projection model from such imagery. We conclude by reviewing 
calibration results for the structured light projector used in Chapter 5. 3.1 Camera Calibration In this 
section we describe both the theory and practice of camera calibra­tion. We begin by brie.y considering 
what cameras are best suiting for building your own 3D scanner. We then present the widely-used calibra­tion 
method originally proposed by Zhang [Zha00]. Finally, we provide step-by-step directions on how to use 
a freely-available MATLAB-based im­plementation of Zhang s method. 3.1.1 Camera Selection and Interfaces 
Selection of the best camera depends on your budget, project goals, and preferred development environment. 
For instance, the two scanners de­scribed in this course place different restrictions on the imaging 
system. The swept-plane scanner in Chapter 4 requires a video camera, although a simple camcorder or 
webcam would be suf.cient. In contrast, the struc­tured lighting system in Chapter 5 can be implemented 
using a still cam­ era. However, the camera must allow computer control of the shutter so image capture 
can be synchronized with image projection. In both cases, the range of cameras are further restricted 
to those that are supported by your development environment. At the time of writing, the accompanying 
software for this course was primarily written in MATLAB. If readers wish to collect their own data sets 
using our software, we recommend obtaining a camera supported by the Image Acquisition Toolbox for MATLAB 
[Mat]. Note that this toolbox sup­ ports products from a variety of vendors, as well as any DCAM-compatible 
FireWire camera or webcam with a Windows Driver Model (WDM) or Video for Windows (VFW) driver. For FireWire 
cameras the toolbox uses the CMU DCAM driver [CMU]. Alternatively, if you select a WDM or VFW camera, 
Microsoft DirectX 9.0 (or higher) must be installed. If you do not have access to any camera meeting 
these constraints, we recommend either purchasing an inexpensive FireWire camera or a high­quality USB 
webcam. While most webcams provide compressed imagery, FireWire cameras typically allow access to raw 
images free of compression artifacts. For those on a tight budget, we recommend the Unibrain Fire-i (available 
for around $100 USD). Although more expensive, we also recom­mend cameras from Point Grey Research. The 
camera interface provided by this vendor is particularly useful if you plan on developing more ad­vanced 
scanners than those presented here. As a point of reference, our scanners were built using a pair of 
Point Grey GRAS-20S4M/C Grasshop­per video cameras. Each camera can capture a 1600×1200 24-bit RGB image 
at up to 30 Hz [Poia]. Outside of MATLAB, a wide variety of camera interfaces are available. However, 
relatively few come with camera calibration software, and even fewer with support for projector calibration. 
One exception, however, is the OpenCV (Open Source Computer Vision) library [Opea]. OpenCV is written 
in C, with wrappers for C# and Python, and consists of optimized implementations of many core computer 
vision algorithms. Video capture and display functions support a wide variety of cameras under multiple 
operating systems, including Windows, Mac OS, and Linux. Note, how­ever, that projector calibration is 
not currently supported in OpenCV. 3.1.2 Calibration Methods and Software Camera Calibration Methods 
Camera calibration requires estimating the parameters of the general pin­hole model presented in Section 
2.4.3. This includes the intrinsic parame­ters, being focal length, principal point, and the scale factors, 
as well as the extrinsic parameters, de.ned by the rotation matrix and translation vector mapping between 
the world and camera coordinate systems. In total, 11 parameters (5 intrinsic and 6 extrinsic) must be 
estimated from a calibra­tion sequence. In practice, a lens distortion model must be estimated as well. 
We recommend the reader review [HZ04, MSKS05] for an in-depth description of camera models and calibration 
methods. At a basic level, camera calibration required recording a sequence of images of a calibration 
object, composed of a unique set of distinguishable features with known 3D displacements. Thus, each 
image of the calibration object provides a set of 2D-to-3D correspondences, mapping image coordi­nates 
to scene points. Na¨ively, one would simply need to optimize over the set of 11 camera model parameters 
so that the set of 2D-to-3D correspon­dences are correctly predicted (i.e., the projection of each known 
3D model feature is close to its measured image coordinates). Many methods have been proposed over the 
years to solve for the cam­era parameters given such correspondences. In particular, the factorized approach 
originally proposed Zhang [Zha00] is widely-adopted in most community-developed tools. In this method, 
a planar checkerboard pat­tern is observed in two or more orientations (see Figure 3.2). From this sequence, 
the intrinsic parameters can be separately solved. Afterwards, a single view of a checkerboard can be 
used to solve for the extrinsic pa­rameters. Given the relative ease of printing 2D patterns, this method 
is commonly used in computer graphics and vision publications. Recommended Software A comprehensive 
list of calibration software is maintained by Bouguet on the toolbox website at http://www.vision.caltech.edu/bouguetj/ 
calib_doc/htmls/links.html. We recommend course attendees use the MATLAB toolbox. Otherwise, OpenCV replicates 
many of its function­alities, while supporting multiple platforms. Although calibrating a small number 
of cameras using these tools is straightforward, calibrating a large network of cameras is a relatively 
recent and challenging problem in the .eld. If your projects lead you in this direction, we suggest the 
Multi-Camera Self-Calibration toolbox [SMP05]. This software takes a unique ap­ proach to calibration; 
rather than using multiple views of a planar calibra­tion object, a standard laser point is simply translated 
through the working volume. Correspondences between the cameras are automatically deter­mined from the 
tracked projection of the laser pointer in each image. We encourage attendees to email us with their 
own preferred tools. We will maintain an up-to-date list on the course website. For the remainder of 
the course notes, we will use the MATLAB toolbox for camera calibration.  3.1.3 Calibration Procedure 
In this section we describe, step-by-step, how to calibrate your camera us­ing the Camera Calibration 
Toolbox for MATLAB. We also recommend re­viewing the detailed documentation and examples provided on 
the toolbox website [Bou]. Speci.cally, new users should work through the .rst cali­ bration example 
and familiarize themselves with the description of model parameters (which differ slightly from the notation 
used in these notes). Begin by installing the toolbox, available for download at http:// www.vision.caltech.edu/bouguetj/calib_doc/. 
Next, construct a checkerboard target. Note that the toolbox comes with a sample checker­board image; 
print this image and af.x it to a rigid object, such as piece of cardboard or textbook cover. Record 
a series of 10 20 images of the checkerboard, varying its position and pose between exposures. Try to 
col­lect images where the checkerboard is visible throughout the image. Using the toolbox is relatively 
straightforward. Begin by adding the toolbox to your MATLAB path by selecting File . Set Path... . Next, 
change the current working directory to one containing your calibration images (or one of our test sequences). 
Type calib at the MATLAB prompt to start. Since we re only using a few images, select Standard (all the 
im­ages are stored in memory) when prompted. To load the images, select Image names and press return, 
then j . Now select Extract grid cor­ners , pass through the prompts without entering any options, and 
then follow the on-screen directions. (Note that the default checkerboard has 30mm×30mm squares). Always 
skip any prompts that appear, unless you are more familiar with the toolbox options. Once you ve .nished 
selecting (a) camera calibration (b) camera lens distortion (b) The resulting fourth-order lens distortion 
model for the camera, where isocontours denote the displacement (in pixels) between an ideal pinhole 
camera image and that collected with the actual lens. corners, choose Calibration , which will run one 
pass though the calibra­tion algorithm. Next, choose Analyze error . Left-click on any outliers you observe, 
then right-click to continue. Repeat the corner selection and cal­ibration steps for any remaining outliers 
(this is a manually-assisted form of bundle adjustment). Once you have an evenly-distributed set of repro­jection 
errors, select Recomp. corners and .nally Calibration . To save your intrinsic calibration, select Save 
. From the previous step you now have an estimate of how pixels can be converted into normalized coordinates 
(and subsequently optical rays in world coordinates, originating at the camera center). Note that this 
pro­cedure estimates both the intrinsic and extrinsic parameters, as well as the parameters of a lens 
distortion model. In following chapters, we will de­scribe the use of various functions within the calibration 
toolbox in more detail. Typical calibration results, illustrating the lens distortion and de­tected checkerboard 
corners, are shown in Figure 3.3. Extrinsic calibration results are shown in Figure 3.7, demonstrating 
that the estimated centers of projection and .elds of view correspond with the physical prototype.  
 3.2 Projector Calibration We now turn our attention to projector calibration. Following the conclu­sions 
of Chapter 2, we model the projector as an inverse camera (i.e., one in which light travels in the opposite 
direction as usual). Under this model, calibration proceeds in a similar manner as with cameras. Rather 
than pho­tographing .xed checkerboards, we project known checkerboard patterns and photograph their distorted 
appearance when re.ected from a diffuse rigid object. This approach has the advantage of being a direct 
extension of Zhang s calibration algorithm for cameras. As a result, much of the soft­ware can be shared 
between camera calibration and projector calibration. 3.2.1 Projector Selection and Interfaces Almost 
any digital projector can be used in your 3D scanning projects, since the operating system will simply 
treat it as an additional display. However, we recommend at least a VGA projector, capable of displaying 
a 640×480 image. For building a structured lighting system, you ll want to purchase a camera with equal 
(or higher) resolution as the projector. Otherwise, the recovered model will be limited to the camera 
resolution. Additionally, those with DVI or HDMI interfaces are preferred for their relative lack of 
analogue to digital conversion artifacts. The technologies used in consumer projectors have matured rapidly 
over the last decade. Early projectors used an LCD-based spatial light mod­ulator and a metal halide 
lamp, whereas recent models incorporate a digital micromirror device (DMD) and LED lighting. Commercial 
offerings vary greatly, spanning large units for conference venues to embedded projectors for mobile 
phones. A variety of technical speci.cations must be considered when choosing the best projector for 
your 3D scanning projects. Varia­tions in throw distance (i.e., where focused images can be formed), 
projec­tor artifacts (i.e., pixelization and distortion), and cost are key factors. Digital projectors 
have a tiered pricing model, with brighter projectors costing signi.cantly more than dimmer ones. At 
the time of writing, a 1024×768 projector can be purchased for around $400 $600 USD. Most models in this 
price bracket have a 1000:1 contrast ratio with an output around 2000 ANSI lumens. Note that this is 
about as bright as a typical 100 W incandescent light bulb. Practically, such projectors are suf.cient 
for projecting a 100 inch (diagonal) image in a well-lit room. For those on a tighter budget, we recommend 
purchasing a hand-held projector. Also known as pocket projectors, these miniaturized devices typically 
use DMD or LCoS technology together with LED lighting. Cur­rent offerings include the 3M MPro, Aiptek 
V10, Aaxatech P1, and Optoma PK101, with prices around $300 USD. While projectors in this class typically 
output only 10 lumens, this is suf.cient to project up to a 50 inch (diago­nal) image (in a darkened 
room). However, we recommend a higher-lumen projector if you plan on scanning large objects in well-lit 
environments. While your system will consider the projector as a second display, your development environment 
may or may not easily support fullscreen dis­play. For instance, MATLAB does not natively support fullscreen 
display (i.e., without window borders or menus). One solution is to use Java dis­play functions, with 
which the MATLAB GUI is built. Code for this ap­proach is available at http://www.mathworks.com/matlabcentral/ 
fileexchange/11112. Unfortunately, we found that this approach only works for the primary display. As 
an alternative, we recommend using the Psychophysics Toolbox [Psy]. While developed for a different applica­ 
tion, this toolbox contains OpenGL wrappers allowing simple and direct fullscreen control of the system 
displays from MATLAB. For details, please see our structured light source code. Finally, for users working 
outside of MATLAB, we recommend controlling projectors through OpenGL. 3.2.2 Calibration Methods and 
Software Projector calibration has received increasing attention, in part driven by the emergence of 
lower-cost digital projectors. As mentioned at several points, a projector is simply the inverse of a 
camera, wherein points on an image plane are mapped to outgoing light rays passing through the center 
of projection. As in Section 3.1.2, a lens distortion model can augment the basic general pinhole model 
presented in Chapter 2. Numerous methods have been proposed for estimating the parameters of this inverse 
camera model. However, community-developed tools are slow to emerge most researchers keeping their tools 
in-house. It is our opinion that both OpenCV and the MATLAB calibration toolbox can be eas­ily modi.ed 
to allow projector calibration. We document our modi.ca­tions to the latter in the following section. 
As noted in the OpenCV text­book [BK08], it is expected that similar modi.cations to OpenCV (possibly 
arising from this course s attendees) will be made available soon. 3.2.3 Calibration Procedure In this 
section we describe, step-by-step, how to calibrate your projector us­ing our software, which is built 
on top of the Camera Calibration Toolbox for MATLAB. Begin by calibrating your camera(s) using the procedure 
out­lined in the previous section. Next, install the toolbox extensions available on the course website 
at http://mesh.brown.edu/dlanman/scan3d. Construct a calibration object similar to those in Figures 3.5 
and 3.6. This object should be a diffuse white planar object, such as foamcore or a painted piece of 
particle board. Printed .ducials, possibly cut from a section of your camera calibration pattern, should 
be af.xed to the surface. One option is to simply paste a section of the checkerboard pattern in one 
corner. In our implementation we place four checkerboard corners at the edges of the cal­ibration object. 
The distances and angles between these points should be recorded. (a) projector calibration (b) projector 
lens distortion A known checkerboard must be projected onto the calibration object. We have provided 
run capture to generate the checkerboard pattern, as well as collect the calibration sequence. As previously 
mentioned, this script controls the projector using the Psychophysics Toolbox [Psy]. A se­ ries of 10 
20 images should be recorded by projecting the checkerboard onto the calibration object. Suitable calibration 
images are shown in Fig­ure 3.5. Note that the printed .ducials must be visible in each image and that 
the projected checkerboard should not obscure them. There are a vari­ety of methods to prevent projected 
and printed checkerboards from inter­fering; one solution is to use color separation (e.g., printed and 
projected checkerboards in red and blue, respectively), however this requires the camera be color calibrated. 
We encourage you to try a variety of options and send us your results for documentation on the course 
website. Your camera calibration images should be stored in the cam subdirec­tory of the provided projector 
calibration package. The calib data.mat .le, produced by running camera calibration, should be stored 
in this di­rectory as well. The projector calibration images should be stored in the proj subdirectory. 
Afterwards, run the camera calibration toolbox by typing calib at the MATLAB prompt (in this directory). 
Since we re only using a few images, select Standard (all the images are stored in memory) when prompted. 
To load the images, select Image names and press return, (a) projector-camera system (b) extrinsic calibration 
then j . Now select Extract grid corners , pass through the prompts without entering any options, and 
then follow the on-screen directions. Al­ways skip any prompts that appear, unless you are more familiar 
with the toolbox options. Note that you should now select the projected checker­board corners, not the 
printed .ducials. The detected corners should then be saved in calib data.mat in the proj subdirectory. 
To complete your calibration, run the run calibration script. Note that you may need to modify which 
projector images are included at the top of the script (de.ned by the useProjImages vector), especially 
if you .nd that the script produces an optimization error message. The .rst time you run the script, 
you will be prompted to select the extrinsic calibration .ducials (i.e., the four printed markers in 
Figure 3.5). Follow any on-screen directions. Once calibration is complete, the script will visualize 
the recov­ered system parameters by plotting the position and .eld of view of the projector and camera 
(see Figure 3.7). Our modi.cations to the calibration toolbox are minimal, reusing much of its functionality. 
We plan on adding a simple GUI to automate the man­ual steps currently needed with our software. Please 
check the course web­site for any updates. In addition, we will we post links to similar software tools 
produced by course attendees. In any case, we hope that the provided software is suf.cient to overcome 
the calibration hurdle in their own 3D scanning projects. Chapter 4  3D Scanning with Swept-Planes 
In this chapter we describe how to build an inexpensive, yet accurate, 3D scanner using household items 
and a digital camera. Speci.cally, we ll de­scribe the implementation of the desktop scanner originally 
proposed by Bouguet and Perona [BP]. As shown in Figure 4.1, our instantiation of this system is composed 
of .ve primary items: a digital camera, a point-like light source, a stick, two planar surfaces, and 
a calibration checkerboard. By waving the stick in front of the light source, the user can cast planar 
shadows into the scene. As we ll demonstrate, the depth at each pixel can then be recovered using simple 
geometric reasoning. In the course of building your own desktop scanner you will need to develop a good 
understanding of camera calibration, Euclidean coordinate transformations, manipulation of implicit and 
parametric parameteriza­tions of lines and planes, and ef.cient numerical methods for solving least­squares 
problems topics that were previously presented in Chapter 2. We encourage the reader to also review the 
original project website [BP] and obtain a copy of the IJCV publication [BP99], both of which will be 
referred to several times throughout this chapter. Also note that the software accom­panying this chapter 
was developed in MATLAB at the time of writing. We encourage the reader to download that version, as 
well as updates, from the course website at http://mesh.brown.edu/dlanman/scan3d. 4.1 Data Capture As 
shown in Figure 4.1, the scanning apparatus is simple to construct and contains relatively few components. 
A pair of blank white foamcore boards are used as planar calibration objects. These boards can be purchased 
at an (a) swept-plane scanning apparatus (b) frame from acquired video sequence Figure 4.1: 3D photography 
using planar shadows. (a) The scanning setup, composed of .ve primary items: a digital camera, a point-like 
light source, a stick, two planar surfaces, and a calibration checkerboard (not shown). Note that the 
light source and camera must be separated so that cast shadow planes and camera rays do not meet at small 
incidence angles. (b) The stick is slowly waved in front of the point light to cast a planar shadow that 
translates from left to right in the scene. The position and orientation of the shadow plane, in the 
world coordinate system, are estimated by ob­serving its position on the planar surfaces. After calibrating 
the camera, a 3D model can be recovered by triangulation of each optical ray by the shadow plane that 
.rst entered the corresponding scene point. art supply store. Any rigid light-colored planar object could 
be substituted, including particle board, acrylic sheets, or even lightweight poster board. At least 
four .ducials, such as the printed checkerboard corners shown in the .gure, should be af.xed to known 
locations on each board. The dis­tance and angle between each .ducial should be measured and recorded 
for later use in the calibration phase. These measurements will allow the position and orientation of 
each board to be estimated in the world coor­dinate system. Finally, the boards should be oriented approximately 
at a right angle to one another. Next, a planar light source must be constructed. In this chapter we 
will follow the method of Bouguet and Perona [BP], in which a point source and a stick are used to cast 
planar shadows. Wooden dowels of varying diameter can be obtained at a hardware store, and the point 
light source can be fashioned from any halogen desk lamp after removing the re.ector. Alternatively, 
a laser stripe scanner could be implemented by replacing the point light source and stick with a modi.ed 
laser pointer. In this case, a cylindrical lens must be af.xed to the exit aperture of the laser pointer, 
creating a low-cost laser stripe projector. Both components can be obtained from Edmund Optics [Edm]. 
For example, a section of a lenticular array or cylindrical Fresnel lens sheet could be used. However, 
in the remainder of this chapter we will focus on the shadow-casting method. Any video camera or webcam 
can be used for image acquisition. The light source and camera should be separated, so that the angle 
between camera rays and cast shadow planes is close to perpendicular (otherwise triangulation will result 
in large errors). Data acquisition is simple. First, an object is placed on the horizontal calibration 
board, and the stick is slowly translated in front of the light source (see Figure 4.1). The stick should 
be waved such that a thin shadow slowly moves across the screen in one di­rection. Each point on the 
object should be shadowed at some point during data acquisition. Note that the camera frame rate will 
determine how fast the stick can be waved. If it is moved too fast, then some pixels will not be shadowed 
in any frame leading to reconstruction artifacts. We have provided several test sequences with our setup, 
which are available on the course website [LT]. As shown in Figures 4.3 4.7, there are a variety of objects 
available, ranging from those with smooth surfaces to those with multiple self-occlusions. As we ll describe 
in the following sec­tions, reconstruction requires accurate estimates of the shadow boundaries. As a 
result, you will .nd that light-colored objects (e.g., the chiquita, frog, and man sequences) will be 
easiest to reconstruct. Since you ll need to es­timate the intrinsic and extrinsic calibration of the 
camera, we ve also pro­vided the calib sequence composed of ten images of a checkerboard with various 
poses. For each sequence we have provided both a high-resolution 1024×768 sequence, as well as a low-resolution 
512×384 sequence for de­velopment. When building your own scanning apparatus, brie.y note some prac­tical 
issues associated with this approach. First, it is important that every pixel be shadowed at some point 
in the sequence. As a result, you must wave the stick slow enough to ensure that this condition holds. 
In addition, the reconstruction method requires reliable estimates of the plane de.ned by the light source 
and the edge of the stick. Ambient illumination must be reduced so that a single planar shadow is cast 
by each edge of the stick. In addition, the light source must be suf.ciently bright to allow the camera 
to operate with minimal gain, otherwise sensor noise will corrupt the .nal re­construction. Finally, 
note that these systems typically use a single halogen desk lamp with the re.ector removed. This ensures 
that the light source is (a) spatial shadow edge localization (b) temporal shadow edge localization 
Figure 4.2: Spatial and temporal shadow edge localization. (a) The shadow edges are determined by .tting 
a line to the set of zero crossings, along each row in the planar regions, of the difference image .I(x, 
y, t). (b) The shadow times (quantized to 32 values here) are determined by .nding the zero-crossings 
of the difference image .I(x, y, t) for each pixel (x, y) as a function of time t. Early to late shadow 
times are shaded from blue to red. suf.ciently point-like to produce abrupt shadow boundaries. 4.2 Video 
Processing Two fundamental quantities must be estimated from a recorded swept­plane video sequence: (1) 
the time that the shadow enters (and/or leaves) each pixel and (2) the spatial position of each leading 
(and/or trailing) shadow edge as a function of time. This section outlines the basic pro­cedures for 
performing these tasks. Additional technical details are pre­sented in Section 2.4 in [BP99] and Section 
6.2.4 in [Bou99]. Our reference implementation is provided in the videoProcessing m-.le. Note that, for 
large stick diameters, the shadow will be thick enough that two dis­tinct edges can be resolved in the 
captured imagery. By tracking both the leading and trailing shadow edges, two independent 3D reconstructions 
are obtained allowing robust outlier rejection and improved model qual­ity. However, in a basic implementation, 
only one shadow edge must be processed using the following methods. In this section we will describe 
calibration of the leading edge, with a similar approach applying for the trailing edge. 4.2.1 Spatial 
Shadow Edge Localization To reconstruct a 3D model, we must know the equation of each shadow plane in 
the world coordinate system. As shown in Figure 4.2, the cast shadow will create four distinct lines 
in the camera image, consisting of a pair of lines on both the horizontal and vertical calibration boards. 
These lines represent the intersection of the 3D shadow planes (both the leading and trailing edges) 
with the calibration boards. Using the notation of Figure 2 in [BP99], we need to estimate the 2D shadow 
lines .h(t) and .v(t) pro­jected on the horizontal and vertical planar regions, respectively. In order 
to perform this and subsequent processing, a spatio-temporal approach can be used. As described in by 
Zhang et al. [ZCS03], this approach tends to pro­ duce better reconstruction results than traditional 
edge detection schemes (e.g., the Canny edge detector [MSKS05]), since it is capable of preserving sharp 
surface discontinuities. Begin by converting the video to grayscale (if a color camera was used), and 
evaluate the maximum and minimum brightness observed at each camera pixel x¯c =(x, y) over the stick-waving 
sequence. Imax(x, y) £ max I(x, y, t) t Imin(x, y) £ min I(x, y, t) t To detect the shadow boundaries, 
choose a per-pixel detection threshold which is the midpoint of the dynamic range observed in each pixel. 
With this threshold, the shadow edge can be localized by the zero crossings of the difference image .I(x, 
y, t) £ I(x, y, t) - Ishadow(x, y), where the shadow threshold image is de.ned to be . Ishadow(x, y) 
£ Imax(x, y)+ Imin(x, y) 2 In practice, you ll need to select an occlusion-free image patch for each 
pla­nar region. Afterwards, a set of sub-pixel shadow edge samples (for each row of the patch) are obtained 
by interpolating the position of the zero­crossings of .I(x, y, t). To produce a .nal estimate of the 
shadow edges .h(t) and .v(t), the best-.t line (in the least-squares sense) must be .t to the set of 
shadow edge samples. The desired output of this step is illus­trated in Figure 4.2(a), where the best-.t 
lines are overlaid on the original image. Keep in mind that you should convert the provided color images 
to grayscale; if you re using MATLAB, the function rgb2gray can be used for this task. 4.2.2 Temporal 
Shadow Edge Localization After calibrating the camera, the previous step will provide all the informa­tion 
necessary to recover the position and orientation of each shadow plane as a function of time in the world 
coordinate system. As we ll describe in Section 4.4, in order to reconstruct the object you ll also need 
to know when each pixel entered the shadowed region. This task can be accomplished in a similar manner 
as spatial localization. Instead of estimating zero-crossing along each row for a .xed frame, the per-pixel 
shadow time is assigned using the zero crossings of the difference image .I(x, y, t) for each pixel (x, 
y) as a function of time t. The desired output of this step is illustrated in Figure 4.2(b), where the 
shadow crossing times are quantized to 32 val­ ues (with blue indicating earlier times and red indicated 
later ones). Note that you may want to include some additional heuristics to reduce false de­tections. 
For instance, dark regions cannot be reliably assigned a shadow time. As a result, you can eliminate 
pixels with insuf.cient contrast (e.g., dark blue regions in the .gure).  4.3 Calibration As described 
in Chapters 2 and 3, intrinsic and extrinsic calibration of the camera is necessary to transfer image 
measurements into the world coor­dinate system. For the swept-plane scanner, we recommend using either 
the Camera Calibration Toolbox for MATLAB [Bou] or the calibration func­ tions within OpenCV [Opea]. 
As previously described, these packages are commonly used within the computer vision community and, at 
their core, implement the widely adopted calibration method originally proposed by Zhang [Zha99]. In 
this scheme, the intrinsic and extrinsic parameters are estimated by viewing several images of a planar 
checkerboard with var­ious poses. In this section we will brie.y review the steps necessary to calibrate 
the camera using the MATLAB toolbox. We recommend reviewing the documentation on the toolbox website 
[Bou] for additional examples; speci.cally, the .rst calibration example and the description of calibration 
parameters are particularly useful to review for new users. 4.3.1 Intrinsic Calibration Begin by adding 
the toolbox to your MATLAB path by selecting File . Set Path... . Next, change the current working directory 
to one of the cal­ibration sequences (e.g., to the calib or calib-lr examples downloaded from the course 
website). Type calib at the MATLAB prompt to start. Since we re only using a few images, select Standard 
(all the images are stored in memory) when prompted. To load the images, select Image names and press 
return, then j . Now select Extract grid corners , pass through the prompts without entering any options, 
and then follow the on-screen directions. (Note that, for the provided examples, a calibration target 
with the default 30mm×30mm squares was used). Always skip any prompts that appear, unless you are more 
familiar with the toolbox options. Once you ve .nished selecting corners, choose Calibration , which 
will run one pass though the calibration algorithm discussed in Chapter 3. Next, choose Analyze error 
. Left-click on any outliers you observe, then right-click to continue. Repeat the corner selection and 
calibration steps for any remain­ing outliers (this is a manually-assisted form of bundle adjustment). 
Once you have an evenly-distributed set of reprojection errors, select Recomp. corners and .nally Calibration 
. To save your intrinsic calibration, select Save . 4.3.2 Extrinsic Calibration From the previous step 
you now have an estimate of how pixels can be converted into normalized coordinates (and subsequently 
optical rays in world coordinates, originating at the camera center). In order to assist you with your 
implementation, we have provided a MATLAB script called extrinsicDemo. As long as the calibration results 
have been saved in the calib and calib-lr directories, this demo will allow you to select four corners 
on the horizontal plane to determine the Euclidean transformation from this ground plane to the camera 
reference frame. (Always start by select­ing the corner in the bottom-left and proceed in a counter-clockwise 
order. For your reference, the corners de.ne a 558.8mm×303.2125mm rectangle in the provided test sequences.) 
In addition, observe that the .nal section of extrinsicDemo uses the utility function pixel2ray to determine 
the optical rays (in camera coordinates), given a set of user-selected pixels.  4.4 Reconstruction At 
this point, the system is fully calibrated. Speci.cally, optical rays pass­ing through the camera s center 
of projection can be expressed in the same world coordinate system as the set of temporally-indexed shadow 
planes. Ray-plane triangulation can now be applied to estimate the per-pixel depth (at least for those 
pixels where the shadow was observed). In terms of Fig­ure 2 in [BP99], the camera calibration is used 
to obtain a parametrization of the ray de.ned by a true object point P and the camera center Oc. Given 
the shadow time for the associated pixel x¯c, one can lookup (and potentially interpolate) the position 
of the shadow plane at this time. The resulting ray­plane intersection will provide an estimate of the 
3D position of the surface point. Repeating this procedure for every pixel will produce a 3D recon­struction. 
For complementary and extended details on the reconstruction process, please consult Sections 2.5 and 
2.6 in [BP99] and Sections 6.2.5 and 6.2.6 in [Bou99]. 4.5 Post-processing and Visualization Once you 
have reconstructed a 3D point cloud, you ll want to visualize the result. Regardless of the environment 
you used to develop your solu­tion, you can write a function to export the recovered points as a VRML 
.le containing a single indexed face set with an empty coordIndex array. Ad­ditionally, a per-vertex 
color can be assigned by sampling the maximum­luminance color, observed over the video sequence. In Chapter 
6 we docu­ ment further post-processing that can be applied, including merging mul­tiple scans and extracting 
watertight meshes. However, the simple colored point clouds produced at this stage can be rendered using 
the Java-based point splatting software provided on the course website. To give you some expectation 
of reconstruction quality, Figures 4.3 4.7 show results obtained with our reference implementation. Note 
that there are several choices you can make in your implementation; some of these may allow you to obtain 
additional points on the surface or increase the reconstruction accuracy. For example, using both the 
leading and trail­ing shadow edges will allow outliers to be rejected (by eliminating points whose estimated 
depth disagrees between the leading vs. trailing shadow edges).   Chapter 5  Structured Lighting 
In this chapter we describe how to build a structured light scanner using one or more digital cameras 
and a single projector. While the desktop scanner [BP] implemented in the previous chapter is inexpensive, 
it has limited practical utility. The scanning process requires manual manipula­tion of the stick, and 
the time required to sweep the shadow plane across the scene limits the system to reconstructing static 
objects. Manual transla­tion can be eliminated by using a digital projector to sequentially display patterns 
(e.g., a single stipe translated over time). Furthermore, various structured light illumination sequences, 
consisting of a series of projected images, can be used to ef.ciently solve for the camera pixel to projector 
column (or row) correspondences. By implementing your own structured light scanner, you will directly 
extending the algorithms and software developed for the swept-plane sys­tems in the previous chapter. 
Reconstruction will again be accomplished using ray-plane triangulation. The key difference is that correspondences 
will now be established by decoding certain structured light sequences. At the time of writing, the software 
accompanying this chapter was devel­oped in MATLAB. We encourage the reader to download that version, 
as well as any updates, from the course website at http://mesh.brown. edu/dlanman/scan3d. 5.1 Data Capture 
5.1.1 Scanner Hardware As shown in Figure 5.1, the scanning apparatus contains one or more digi­ tal 
cameras and a single digital projector. As with the swept-plane systems, the object will eventually be 
reconstructed by ray-plane triangulation, be­tween each camera ray and a plane corresponding to the projector 
column (and/or row) illuminating that point on the surface. As before, the cam­eras and projector should 
be arranged to ensure that no camera ray and projector plane meet at small incidence angles. A diagonal 
placement of the cameras, as shown in the .gure, ensures that both projector rows and columns can be 
used for reconstruction. As brie.y described in Chapter 3, a wide variety of digital cameras and projectors 
can be selected for your implementation. While low-cost webcams will be suf.cient, access to raw imagery 
will eliminate decod­ing errors introduced by compression artifacts. You will want to select a camera 
that is be supported by your preferred development environment. For example, if you plan on using the 
MATLAB Image Acquisition Toolbox, then any DCAM-compatible FireWire camera or webcam with a Windows Driver 
Model (WDM) or Video for Windows (VFW) driver will work [Mat]. If you plan on developing in OpenCV, a 
list compatible cameras is main­tained on the wiki [Opeb]. Almost any digital projector can be used, 
since the operating system will simply treat it as an additional display. As a point of reference, our 
implementation contains a single Mitsubishi XD300U DLP projector and a pair of Point Grey GRAS-20S4M/C 
Grasshop­per video cameras. The projector is capable of displaying 1024×768 24-bit RGB images at 50-85 
Hz [Mit]. The cameras capture 1600×1200 24-bit RGB images at up to 30 Hz [Poia]. Although lower-resolution 
modes can be used if higher frame rates are required. The data capture was implemented in MATLAB. The 
cameras were controlled using custom wrappers for the FlyCapture SDK [Poib], and fullscreen control of 
the projector was achiev­ ing using the Psychophysics Toolbox [Psy] (see Chapter 3). 5.1.2 Structured 
Light Sequences The primary bene.t of introducing the projector is to eliminate the mechan­ical motion 
required in swept-plane scanning systems (e.g., laser striping or the desktop scanner ). Assuming minimal 
lens distortion, the projector can be used to display a single column (or row) of white pixels translating 
against a black background; thus, 1024 (or 768) images would be required to assign the correspondences, 
in our implementation, between camera pix­els and projector columns (or rows). After establishing the 
correspondences and calibrating the system, a 3D point cloud is reconstructed using familiar ray-plane 
triangulation. However, a simple swept-plane sequence does not fully exploit the projector. Since we 
are free to project arbitrary 24-bit color images, one would expect there to exist a sequence of coded 
patterns, be­sides a simple translation of a single stripe, that allow the projector-camera correspondences 
to be assigned in relatively few frames. In general, the identity of each plane can be encoded spatially 
(i.e., within a single frame) or temporally (i.e., across multiple frames), or with a combination of 
both spatial and temporal encodings. There are bene.ts and drawbacks to each strategy. For instance, 
purely spatial encodings allow a single static pat­tern to be used for reconstruction, enabling dynamic 
scenes to be captured. Alternatively, purely temporal encodings are more likely to bene.t from re­dundancy, 
reducing reconstruction artifacts. A comprehensive assessment of such codes is presented by Salvi et 
al. [SPB04]. In this chapter we will focus on purely temporal encodings. While such patterns are not 
well-suited to scanning dynamic scenes, they have the bene.t of being easy to decode and are robust to 
surface texture vari­ation, producing accurate reconstructions for static objects (with the nor­mal prohibition 
of transparent or other problematic materials). A sim­ple binary structured light sequence was .rst proposed 
by Posdamer and Altschuler [PA82] in 1981. As shown in Figure 5.2, the binary encoding consists of a 
sequence of binary images in which each frame is a single bit plane of the binary representation of the 
integer indices for the projector columns (or rows). For example, column 546 in our prototype has a binary 
representation of 1000100010 (ordered from the most to the least signi.cant bit). Similarly, column 546 
of the binary structured light sequence has an identical bit sequence, with each frame displaying the 
next bit. Considering the projector-camera arrangement as a communication sys­tem, then a key question 
immediately arises; what binary sequence is most robust to the known properties of the channel noise 
process? At a basic level, we are concerned with assigning an accurate projector column/row to camera 
pixel correspondence, otherwise triangulation artifacts will lead to large reconstruction errors. Gray 
codes were .rst proposed as one al­ternative to the simple binary encoding by Inokuchi et al. [ISM84] 
in 1984. The re.ected binary code was introduced by Frank Gray in 1947 [Wik]. As shown in Figure 5.3, 
the Gray code can be obtained by re.ecting, in a spe­ ci.c manner, the individual bit-planes of the binary 
encoding. Pseudocode for converting between binary and Gray codes is provided in Table 5.1. For example, 
column 546 in our in our implementation has a Gray code repre­sentation of 1100110011, as given by BIN2GRAY. 
The key property of the Gray code is that two neighboring code words (e.g., neighboring columns in the 
projected sequence) only differ by one bit (i.e., adjacent codes have a Hamming distance of one). As 
a result, the Gray code structured light sequence tends to be more robust to decoding errors than a simple 
binary encoding. In the provided MATLAB code, the m-.le bincode can be used to gen­erate a binary structured 
light sequence. The inputs to this function are the width w and height h of the projected image. The 
output is a sequence of 21log2 wl +21log2 hl +2 uncompressed images. The .rst two images con­sist of 
an all-white and an all-black image, respectively. The next 21log2 wl images contain the bit planes of 
the binary sequence encoding the projec­ (a) binary structured light sequence tor columns, interleaved 
with the binary inverse of each bit plane (to assist in decoding). The last 21log2 hl images contain 
a similar encoding for the projector rows. A similar m-.le named graycode is provided to generate Gray 
code structured light sequences.  5.2 Image Processing The algorithms used to decode the structured 
light sequences described in the previous section are relatively straightforward. For each camera, it 
must be determined whether a given pixel is directly illuminated by the projector in each displayed image. 
If it is illuminated in any given frame, then the corresponding code bit is set high, otherwise it is 
set low. The dec­imal integer index of the corresponding projector column (and/or row) can then be recovered 
by decoding the received bit sequences for each camera pixel. A user-selected intensity threshold is 
used to determine whether a given pixel is illuminated. For instance, 1log2 wl +2 images could be used 
to encode the projector columns, with the additional two images consist­ing of all-white and all-black 
frames. The average intensity of the all-white and all-black frames could be used to assign a per-pixel 
threshold; the in­ BIN2GRAY(B) GRAY2BIN(G) 1 n . length[B] 1 n . length[G] 2 G[1] . B[1] 2 B[1] . G[1] 
3 for i . 2 to n 3 for i . 2 to n 4 do G[i] . B[i - 1] xor B[i] 4 do B[i] . B[i - 1] xor G[i] 5 return 
G 5 return B Table 5.1: Pseudocode for converting between binary and Gray codes. (Left) BIN2GRAY accepts 
an n-bit Boolean array, encoding a decimal in­teger, and returns the Gray code G. (Right) Conversion 
from a Gray to a binary sequence is accomplished using GRAY2BIN. dividual bit planes of the projected 
sequence could then be decoded by comparing the received intensity to the threshold. In practice, a single 
.xed threshold results in decoding artifacts. For instance, certain points on the surface may only receive 
indirect illumina­tion scattered from directly-illuminated points. In certain circumstances the scattered 
light may cause a bit error, in which an unilluminated point appears illuminated due to scattered light. 
Depending on the speci.c struc­tured light sequence, such bit errors may produce signi.cant reconstruction 
errors in the 3D point cloud. One solution is to project each bit plane and its inverse, as was done 
in Section 5.1. While 21log2 wl frames are now required to encode the projector columns, the decoding 
process is less sen­sitive to scattered light, since a variable per-pixel threshold can be used. Speci.cally, 
a bit is determined to be high or low depending on whether a projected bit-plane or its inverse is brighter 
at a given pixel. Typical decod­ing results are shown in Figure 5.4. As with any communication system, 
the design of structured light se­quences must account for anticipated artifacts introduced by the communi­cation 
channel. In a typical projector-camera system decoding artifacts are introduced from a wide variety of 
sources, including projector or camera defocus, scattering of light from the surface, and temporal variation 
in the scene (e.g., varying ambient illumination or a moving object). We have pro­vided a variety of 
data sets for testing your decoding algorithms. In par­ticular, the man sequence has been captured using 
both binary and Gray code structured light sequences. Furthermore, both codes have been ap­plied when 
the projector is focused and defocused at the average depth of the sculpture. We encourage the reader 
to study the decoding artifacts pro­duced under these non-ideal, yet commonly encountered, circumstances. 
 (a) all-white image (b) decoded row indices (c) decoded column indices Figure 5.4: Decoding structured 
light illumination sequences. (a) Camera image captured while projecting an all white frame. Note the 
shadow cast on the background plane, prohibiting reconstruction in this region. (b) Typ­ical decoding 
results for a Gray code structured light sequence, with pro­jector row and camera pixel correspondences 
represented using a jet col­ormap in MATLAB. Points that cannot be assigned a correspondence with a high 
con.dence are shown in black. (c) Similar decoding results for pro­jector column correspondences. The 
support code includes the m-.le bindecode to decode the pro­vided binary structured light sequences. 
This function accepts as input the directory containing the encoded sequences, following the convention 
of the previous section. The output is a pair of unsigned 16-bit grayscale images containing the decoded 
decimal integers corresponding to the pro­jector column and row that illuminated each camera pixel (see 
Figure 5.4). A value of zero indicates a given pixel cannot be assigned a correspon­dence, and the projector 
columns and rows are indexed from one. The m-.le graydecode is also provided to decode Gray code structured 
light sequences. Note that our implementation of the Gray code is shifted to the left, if the number 
of columns (or rows) is not a power of two, such that the projected patterns are symmetric about the 
center column (or row) of the image. The sample script slDisplay can be used to load and visualize the 
provided data sets.  5.3 Calibration As with the swept-plane scanner, calibration is accomplished using 
any of the tools and procedures outlined in Chapter 3. In this section we brie.y review the basic procedures 
for projector-camera calibration. In our im­plementation, we used the Camera Calibration Toolbox for 
MATLAB [Bou] to .rst calibrate the cameras, following the approach used in the previous chapter. An example 
sequence of 15 views of a planar checkerboard pat­tern, composed of 38mm×38mm squares, is provided in 
the accompanying test data for this chapter. The intrinsic and extrinsic camera calibration pa­rameters, 
in the format speci.ed by the toolbox, are also provided. Projector calibration is achieved using our 
extensions to the Camera Calibration Toolbox for MATLAB, as outlined in Chapter 3. As presented, the 
projector is modeled as a pinhole imaging system containing additional lenses that introduce distortion. 
As with our cameras, the projector has an intrinsic model involving the principal point, skew coef.cients, 
scale fac­tors, and focal length. To estimate the projector parameters, a static checkerboard is projected 
onto a diffuse planar pattern with a small number of printed .ducials lo­cated on its surface. In our 
design, we used a piece of foamcore with four printed checkerboard corners. As shown in Figure 5.5, a 
single image of the printed .ducials is used to recover the implicit equation of the calibra­tion plane 
in the camera coordinate system. The 3D coordinate for each projected checkerboard corner is then reconstructed. 
The 2D camera pixel to 3D point correspondences are then used to estimate the intrinsic and extrinsic 
calibration from multiple views of the planar calibration object. A set of 20 example images of the projector 
calibration object are in­cluded with the support code. In these examples, the printed .ducials were 
horizontally separated by 406mm and vertically separated by 335mm. The camera and projector calibration 
obtained using our procedure are also pro­vided; note that the projector intrinsic and extrinsic parameters 
are in the same format as camera calibration outputs from the Camera Calibration Toolbox for MATLAB. 
The provided m-.le slCalib can be used to visual­ize the calibration results. A variety of Matlab utilities 
are provided to assist the reader in imple­menting their own structured light scanner. The m-.le campixel2ray 
converts from camera pixel coordinates to an optical ray expressed in the coordinate system of the .rst 
camera (if more than one camera is used). A similar m-.le projpixel2ray converts from projector pixel 
coordinates to an optical ray expressed in the common coordinate system of the .rst (a) projector-camera 
system (b) extrinsic calibration Figure 5.5: Extrinsic calibration of a projector-camera system. (a) 
A planar calibration object with four printed checkerboard corners is imaged by a camera. A projected 
checkerboard is displayed in the center of the calibra­tion plane. The physical and projected corners 
are manually detected and indicated with red and green circles, respectively. (b) The extrinsic camera 
and projector calibration is visualized using slCalib. Viewing frusta for the cameras are shown in red 
and the viewing frustum for the projector is shown in green. Note that the reconstruction of the .rst 
image of a sin­gle printed checkerboard, used during camera calibration, is shown with a red grid, whereas 
the recovered projected checkerboard is shown in green. Also note that the recovered camera and projector 
frusta correspond to the physical con.guration shown in Figure 5.1. camera. Finally, projcol2plane and 
projrow2plane convert from pro­jected column and row indices, respectively, to an implicit parametrization 
of the plane projected by each projector column and row in the common coordinate system. 5.4 Reconstruction 
The decoded set of camera and projector correspondences can be used to reconstruct a 3D point cloud. 
Several reconstruction schemes can be im­plemented using the sequences described in Section 5.1. The 
projector col­ umn correspondences can be used to reconstruct a point cloud using ray­plane triangulation. 
A second point cloud can be reconstructed using the projector row correspondences. Finally, the projector 
pixel to camera pixel correspondences can be used to reconstruct the point cloud using ray-ray triangulation 
(i.e., by .nding the closest point to the optical rays de.ned by the projector and camera pixels). A 
simple per-point RGB color can be assigned by sampling the color of the all-white camera image for each 
3D point. Reconstruction artifacts can be further reduced by comparing the reconstruction produced by 
each of these schemes. We have provided our own implementation of the reconstruction equa­tions in the 
included m-.le slReconstruct. This function can be used, together with the previously described m-.les, 
to reconstruct a 3D point cloud for any of the provided test sequences. Furthermore, VRML .les are also 
provided for each data set, containing a single indexed face set with an empty coordIndex array and a 
per-vertex color. 5.5 Post-processing and Visualization As with the swept-plane scanner, the structured 
light scanner produces a colored 3D point cloud. Only points that are both imaged by a cam­era and illuminated 
by the projector can be reconstructed. As a result, a complete 3D model of an object would typically 
require merging multiple scans obtained by moving the scanning apparatus or object (e.g., by using a 
turntable). These issues are considered in Chapter 6. We encourage the reader to implement their own 
solution so that measurements from multi­ple cameras, projectors, and 3D point clouds can be merged. 
Typical results produced by or reference implementation are shown in Figure 5.6, with ad­ ditional results 
shown in Figures 5.7 5.10.   Chapter 6  Surfaces from Point Clouds The objects scanned in the previous 
examples are solid, with a well-de.ned boundary surface separating the inside from the outside. Since 
computers have a .nite amount of memory and operations need to be completed in a .nite number of steps, 
algorithms can only be designed to manipulate surfaces described by a .nite number of parameters. Perhaps 
the simplest surface representation with a .nite number of parameters is produced by a .nite sampling 
scheme, where a process systematically chooses a set of points lying on the surface. The triangulation-based 
3D scanners described in previous chapters produce such a .nite sampling scheme. The so-called point 
cloud, a dense collection of surface samples, has become a popular representation in com­puter graphics. 
However, since point clouds do not constitute surfaces, they cannot be used to determine which 3D points 
are inside or outside of the solid object. For many applications, being able to make such a determi­nation 
is critical. For example, without closed bounded surfaces, volumes cannot be measured. Therefore, it 
is important to construct so-called water­tight surfaces from point clouds. In this chapter we consider 
these issues. 6.1 Representation and Visualization of Point Clouds In addition to the 3D point locations, 
the 3D scanning methods described in previous chapters are often able to estimate a color per point, 
as well as a surface normal vector. Some methods are able to measure both color and surface normal, and 
some are able to estimate other parameters which can be used to describe more complex material properties 
used to generate complex renderings. In all these cases the data structure used to represent a point 
cloud in memory is a simple array. A minimum of three values per point are needed to represent the point 
locations. Colors may require one to three more values per point, and normals vectors three additional 
values per point. Other properties may require more values, but in general it is the same number of parameters 
per point that need to be stored. If M is the number of parameters per point and N is the number of points, 
then point cloud can be represented in memory using an array of length NM. 6.1.1 File Formats Storing 
and retrieving arrays from .les is relatively simple, and storing the raw data either in ASCII format 
or in binary format is a valid solution to the problem. However, these solutions may be incompatible 
with many software packages. We want to mention two standards which have support for storing point clouds 
with some auxiliary attributes. Storing Point Clouds as VRML Files The Virtual Reality Modeling Language 
(VRML) is an ISO standard pub­lished in 1997. A VRML .le describes a scene graph comprising a variety 
of nodes. Among geometry nodes, PointSet and IndexedFaceSet are used to store point clouds. The PointSet 
node was designed to store point clouds, but in addition to the 3D coordinates of each point, only col­ors 
can be stored. No other attributes can be stored in this node. In par­ticular, normal vectors cannot 
be recorded. This is a signi.cant limitation, since normal vectors are important both for rendering point 
clouds and for reconstructing watertight surfaces from point clouds. The IndexedFaceSet node was designed 
to store polygon meshes with colors, normals, and/or texture coordinates. In addition to vertex co­ordinates, 
colors and normal vectors can be stored bound to vertices. Even though the IndexedFaceSet node was not 
designed to represent point clouds, the standard allows for this node to have vertex coordinates and 
properties such as colors and/or normals per vertex, but no faces. The standard does not specify how 
such a node should be rendered in a VRML browser, but since they constitute valid VRML .les, they can 
be used to store point clouds. The SFL File Format The SFL .le format was introduced with Pointshop3D 
[ZPKG02] to pro­ vide a versatile .le format to import and export point clouds with color, normal vectors, 
and a radius per vertex describing the local sampling den­sity. A SFL .le is encoded in binary and features 
an extensible set of surfel attributes, data compression, upward and downward compatibility, and transparent 
conversion of surfel attributes, coordinate systems, and color spaces. Pointshop3D is a software system 
for interactive editing of point­based surfaces, developed at the Computer Graphics Lab at ETH Zurich. 
 6.1.2 Visualization A well-established technique to render dense point clouds is point splat­ting. 
Each point is regarded as an oriented disk in 3D, with the orientation determined by the surface normal 
evaluated at each point, and the radius of the disk usually stored as an additional parameter per vertex. 
As a re­sult, each point is rendered as an ellipse. The color is determined by the color stored with 
the point, the direction of the normal vector, and the illu­mination model. The radii are chosen so that 
the ellipses overlap, resulting in the perception of a continuous surface being rendered.  6.2 Merging 
Point Clouds The triangulation-based 3D scanning methods described in previous chap­ters are able to 
produce dense point clouds. However, due to visibility constraints these point clouds may have large 
gaps without samples. In order for a surface point to be reconstructed, it has to be illuminated by a 
projector, and visible by a camera. In addition, the projected patterns needs to illuminate the surface 
transversely for the camera to be able to capture a suf.cient amount of re.ected light. In particular, 
only points on the front-facing side of the object can be reconstructed (i.e., on the same side as the 
projector and camera). Some methods to overcome these limitations are discussed in Chapter 7. However, 
to produce a complete representa­tion, multiple scans taken from various points of view must be integrated 
to produce a point cloud with suf.cient sampling density over the whole visible surface of the object 
being scanned. 6.2.1 Computing Rigid Body Matching Transformations The main challenge to merging multiple 
scans is that each scan is produced with respect to a different coordinate system. As a result, the rigid 
body transformation needed to register one scan with another must be estimated. In some cases the object 
is moved with respect to the scanner under com­puter control. In those cases the transformations needed 
to register the scans are known within a certain level of accuracy. This is the case when the object 
is placed on a computer-controlled turntable or linear translation stage. However, when the object is 
repositioned by hand, the matching transformations are not known and need to be estimated from point 
corre­spondences. We now consider the problem of computing the rigid body transforma­tion q = Rp + T 
to align two shapes from two sets of N points, {p1,...,pN }and {q1,...,qN }. That is, we are looking 
for a rotation matrix R and a translation vector T so that q1 = Rp1 + T ... qN = RpN + T. The two sets 
of points can be chosen interactively or automatically. In either case, being able to compute the matching 
transformation in closed form is a fundamental operation. This registration problem is, in general, not 
solvable due to measure­ment errors. A common approach in such a case is to seek a least-squares solution. 
In this case, we desire a closed-form solution for minimizing the mean squared error N 1 N f(R, T )= 
lRpi + T - qil2 , (6.1) N i=1 over all rotation matrices R and translation vectors T . This yields a 
quadratic function of 12 components in R and T ; however, since R is restricted to be a valid rotation 
matrix, there exist additional constraints on R. Since the variable T is unconstrained, a closed-form 
solution for T , as a function of R, can be found by solving the linear system of equations resulting 
from differentiating the previous expression with respect to T . N 1 .f 1 N =(Rpi + T - qi) . T = q - 
Rp =0 2 .T N i=1 In this expression p and q are the geometric centroids of the two sets of matching points, 
given by NNp = pi q = qi . NN i=1 i=1 Substituting for T in Equation 6.1, we obtain the following equivalent 
error function which depends only on R. N N 1 1 N N i=1 N 1 lR(pi - p) - (qi - q)l2 .(R)= (6.2) 
If we expand this expression we obtain NN N N N Ni=1 i=1 i=1 since lRvl2 = lvl2 for any vector v. As 
the .rst and last terms do not depend on R, maximizing this expression is equivalent to maximizing 1 
 2 1 lpi - pl2 - .(R)= (qi - q)tR(pi - p)+ lqi - ql2 , N N N N N N i=1 1 .(R)= (qi - q)tR(pi 
- p)= trace(RM) , where M is the 3 × 3 matrix N N N i=1 1 (pi - p)(qi - q)t . M = Recall that, for 
any pair of matrices A and B of the same dimensions, trace(AtB)= trace(BAt). We now consider the singular 
value decomposi­tion (SVD) M = U.V t, where U and V are orthogonal 3 × 3 matrices, and . is a diagonal 
3 × 3 matrix with elements d1 = d2 = d3 = 0. Substituting, we .nd trace(RM)= trace(RU.V t)= trace((V 
tRU).) = trace(W .) , where W = V tRU is orthogonal. If we expand this expression, we obtain trace(W 
.) = w11d1 + w22d2 + w33d3 = d1 + d2 + d3 , where W =(wij). The last inequality is true because the components 
of an orthogonal matrix cannot be larger than one. Note that the last inequality is an equality only 
if w11 = w22 = w33 =1, which is only the case when W = I (the identity matrix). It follows that if V 
tU is a rotation matrix, then R = V tU is the minimizer of our original problem. The matrix V tU is an 
orthogonal matrix, but it may not have a negative determinant. In that case, an upper bound for trace(W 
.), with W restricted to have a negative determinant, is achieved for W = J, where . 1 0 0 . J = .0 1 
0 . . 0 0 -1 In this case it follows that the solution to our problem is R = V tJU. 6.2.2 The Iterative 
Closest Point (ICP) Algorithm The Iterative Closest Point (ICP) is an algorithm employed to match two 
surface representations, such as points clouds or polygon meshes. This matching algorithm is used to 
reconstruct 3D surfaces by registering and merging multiple scans. The algorithm is straightforward and 
can be im­plemented in real-time. ICP iteratively estimates the transformation (i.e., translation and 
rotation) between two geometric data sets. The algorithm takes as input two data sets, an initial estimate 
for the transformation, and an additional criterion for stopping the iterations. The output is an im­proved 
estimate of the matching transformation. The algorithm comprises the following steps. 1. Select points 
from the .rst shape. 2. Associate points, by nearest neighbor, with those in the second shape. 3. Estimate 
the closed-form matching transformation using the method derived in the previous section. 4. Transform 
the points using the estimated parameters. 5. Repeat previous steps until the stopping criterion is 
met.  The algorithm can be generalized to solve the problem of registering mul­tiple scans. Each scan 
has an associated rigid body transformation which will register it with respect to the rest of the scans, 
regarded as a single rigid object. An additional external loop must be added to the previous steps to 
pick one transformation to be optimized with each pass, while the others are kept constant either going 
through each of the scans in sequence, or randomizing the choice.  6.3 Surface Reconstruction from Point 
Clouds Watertight surfaces partition space into two disconnected regions so that every line segment joining 
a point in one region to a point in the other must cross the dividing surface. In this section we discuss 
methods to reconstruct watertight surfaces from point clouds. 6.3.1 Continuous Surfaces In mathematics 
surfaces are represented in parametric or implicit form. A parametric surface S = {x(u): u . U} is de.ned 
by a function x : U . IR3 on an open subset U of the plane. An implicit surface is de.ned as a level 
set S = {p . IR3 : f(p)= .} of a continuous function f : V . IR, where V is an open subset in 3D. These 
functions are most often smooth or piece­wise smooth. Implicit surfaces are called watertight because 
they partition space into the two disconnected sets of points, one where f(p) >. and a second where f(p) 
<.. Since the function f is continuous, every line segment joining a point in one region to a point in 
the other must cross the dividing surface. When the boundary surface of a solid object is de­scribed 
by an implicit equation, one of these two sets describes the inside of the object, and the other one 
the outside. Since the implicit function can be evaluated at any point in 3D space, it is also referred 
to as a scalar .eld. On the other hand, parametric surfaces may or may not be water­tight. In general, 
it is dif.cult to determine whether a parametric surface is watertight or not. In addition, implicit 
surfaces are preferred in many applications, such as reverse engineering and interactive shape design, 
be­cause they bound a solid object which can be manufactured; for example, using rapid prototyping technologies 
or numerically-controlled machine tools, such representations can de.ne objects of arbitrary topology. 
As a result, we focus our remaining discussion on implicit surfaces. 6.3.2 Discrete Surfaces A discrete 
surface is de.ned by a .nite number of parameters. We only consider here polygon meshes, and in particular 
those polygon meshes representable as IndexedFaceSet nodes in VRML .les. Polygon meshes are composed 
of geometry and topological connectivity. The geometry in­cludes vertex coordinates, normal vectors, 
and colors (and possibly texture coordinates). The connectivity is represented in various ways. A popu­lar 
representation used in many isosurface algorithms is the polygon soup, where polygon faces are represented 
as loops of vertex coordinate vectors. If two or more faces share a vertex, the vertex coordinates are 
repeated as many times as needed. Another popular representation used in isosurface algorithms is the 
IndexedFaceSet (IFS), describing polygon meshes with simply-connected faces. In this representation the 
geometry is stored as ar­rays of .oating point numbers. In these notes we are primarily concerned with 
the array coord of vertex coordinates, and to a lesser degree with the array normal of face normals. 
The connectivity is described by the total number V of vertices, and F faces, which are stored in the 
coordIndex array as a sequence of loops of vertex indices, demarcated by values of -1. 6.3.3 Isosurfaces 
An isosurface is a polygonal mesh surface representation produced by an isosurface algorithm. An isosurface 
algorithm constructs a polygonal mesh approximation of a smooth implicit surface S = {x : f(x)=0} within 
a bounded three-dimensional volume, from samples of a de.ning function f(x) evaluated on the vertices 
of a volumetric grid. Marching Cubes [LC87] and related algorithms operate on function values provided 
at the vertices of hexahedral grids. Another family of isosurface algorithms operate on functions evaluated 
at the vertices of tetrahedral grids [DK91]. Usually, no additional information about the function is 
provided, and various in­terpolation schemes are used to evaluate the function within grid cells, if 
necessary. The most natural interpolation scheme for tetrahedral meshes is linear interpolation, which 
we also adopt here. 6.3.4 Isosurface Construction Algorithms An isosurface algorithm producing a polygon 
soup output must solve three key problems: (1) determining the quantity and location of isosurface ver­tices 
within each cell, (2) determining how these vertices are connected forming isosurface faces, and (3) 
determining globally consistent face ori­entations. For isosurface algorithms producing IFS output, there 
is a fourth problem to solve: identifying isosurface vertices lying on vertices and edges of the volumetric 
grid. For many visualization applications, the polygon soup representation is suf.cient and acceptable, 
despite the storage over­head. Isosurface vertices lying on vertices and edges of the volumetric grid 
are independently generated multiple times. The main advantage of this approach is that it is highly 
parallelizable. But, since most of these bound­ary vertices are represented at least twice, it is not 
a compact representation. Researchers have proposed various solutions and design decisions (e.g., cell 
types, adaptive grids, topological complexity, interpolant order) to ad­dress these four problems. The 
well-known Marching Cubes (MC) algo­rithm uses a .xed hexahedral grid (i.e., cube cells) with linear 
interpolation to .nd zero-crossings along the edges of the grid. These are the vertices of the isosurface 
mesh. Second, polygonal faces are added connecting these vertices using a table. The crucial observation 
made with MC is that the possible connectivity of triangles in a cell can be computed independently of 
the function samples and stored in a table. Out-of-core extensions, where sequential layers of the volume 
are processed one at a time, are straightfor­ward. Similar tetrahedral-based algorithms [DK91, GH95, 
TPG99], dubbed Marching Tetrahedra (MT), have also been developed (again using linear interpolation). 
Although the cell is simpler, MT requires maintaining a tetrahedral sampling grid. Out-of-core extensions 
require presorted traver­sal schemes, such as in [CS97]. For an unsorted tetrahedral grid, hash tables 
are used to save and retrieve vertices lying on edges of the volumetric grid. As an example of an isosurface 
algorithm, we discuss MT in more detail. Marching Tetrahedra MT operates on the following input data: 
(1) a tetrahedral grid and (2) one piecewise linear function f(x), de.ned by its values at the grid ver­tices. 
Within the tetrahedron with vertices x0,x1,x2,x3 . IR3, the func­ face [2,1,0,-1] [0,3,4,-1] [1,3,4,2,-1] 
[1,5,3,-1] [0,2,5,3,-1] [0,3,5,4,-1] [1,5,2,-1] [2,5,1,-1] [4,5,3,0,-1] [3,5,2,0,-1] [3,5,1,-1] [2,4,3,1,-1] 
[4,3,0,-1] [0,1,2,-1] e edge 0 (0,1) 1 (0,2) 2 (0,3) 3 (1,2) 4 (1,3) 5 (2,3) i (i3i2i1i0) 0 0000 [-1] 
1 0001 2 0010 3 0011 4 0100 5 0101 6 0110 7 0111 8 1000 9 1001 10 1010 11 1011 12 1100 13 1101 14 1110 
15 1111 [-1] Table 6.1: Look-up tables for tetrahedral mesh isosurface evaluation. Note that consistent 
face orientation is encoded within the table. Indices stored in the .rst table reference tetrahedron 
edges, as indicated by the second table of vertex pairs (and further illustrated in Figure 6.1). In this 
case, only edge indices {1, 2, 3, 4} have associated isosurface vertex coordinates, which are shared 
with neighboring cells. tion is linear and can be described in terms of the barycentric coordinates b 
=(b0,b1,b2,b3)t of an arbitrary internal point x = b0 x0+b1 x1+b2 x2+b3 x3 with respect to the four vertices: 
f(x)= b0 f(x0)+ b1 f(x1)+ b2 f(x2)+ b3 f(x3), where b0,b1,b2,b3 = 0 and b0 + b1 + b2 + b3 =1. As illustrated 
in Figure 6.1, the sign of the function at the four grid vertices determines the connectivity (e.g., 
triangle, quadrilateral, or empty) of the output polygo­nal mesh within each tetrahedron. There are actually 
16 = 24 cases, which modulo symmetries and sign changes reduce to only three. Each grid edge, whose end 
vertex values change sign, corresponds to an isosurface mesh vertex. The exact location of the vertex 
along the edge is determined by linear interpolation from the actual function values, but note that the 
16 cases can be precomputed and stored in a table indexed by a 4-bit integer i =(i3i2i1i0), where ij 
=1 if f(xj) > 0 and ij =0, if f(xj) < 0. The full table is shown in Table 6.1. The cases f(xj)=0 are 
singular and require special treatment. For example, the index is i = (0100) = 4 for Figure 6.1(a), and 
i = (1100) = 12 for Figure 6.1(b). Orientation for the isosurface faces, consistent with the orientation 
of the containing tetrahedron, can be ob­tained from connectivity alone (and are encoded in the look-up 
table as shown in Table 6.1). For IFS output it is also necessary to stitch vertices as described above. 
Algorithms to polygonize implicit surfaces [Blo88], where the implicit functions are provided in analytic 
form, are closely related to isosurface algorithms. For example, Bloomenthal and Ferguson [BF95] extract 
non­manifold isosurfaces produced from trimming implicits and parameter­ics using a tetrahedral isosurface 
algorithm. [WvO96] polygonize boolean combinations of skeletal implicits (Boolean Compound Soft Objects), 
ap­plying an iterative solver and face subdivision for placing vertices along feature edges and points. 
Suffern and Balsys [SB03] present an algorithm to polygonize surfaces de.ned by two implicit functions 
provided in ana­lytic form; this same algorithm can compute bi-iso-lines of pairs of implicits for rendering. 
 Isosurface Algorithms on Hexahedral Grids An isosurface algorithm constructs a polygon mesh approximation 
of a level set of a scalar function de.ned in a .nite 3D volume. The function f(p) is usually speci.ed 
by its values fa = f(pa) on a regular grid of three dimensional points G = {pa : a =(a0,a1,a2) . [ n0] 
×[ n1] ×[ n2] } , where [ nj] = {0,...,nj - 1}, and by a method to interpolate in between these values. 
The surface is usually represented as a polygon mesh, and is speci.ed by its isovalue f0. Furthermore, 
the interpolation scheme is as­sumed to be linear along the edges of the grid, so that the isosurface 
cuts each edge in no more than one point. If pa and pß are grid points con­nected by an edge, and fa 
>f0 >fß, the location of the point paß where the isosurface intersects the edge is fa - f0 fß - f0 paß 
= pß + pa . (6.3)fa - fß fß - fa Marching Cubes One of the most popular isosurface extraction algorithms 
is Marching Cubes [LC87]. In this algorithm the points de.ned by the intersection of the iso­surface 
with the edges of the grid are the vertices of the polygon mesh. These vertices are connected forming 
polygon faces according to the fol­lowing procedure. Each set of eight neighboring grid points de.ne 
a small cube called a cell Ca = {pa+ß : ß .{0, 1}3}. Since the function value associated with each of 
the eight corners of a cell may be either above or below the isovalue (isovalues equal to grid function 
values are called singular and should be avoided), there are 28 = 256 pos­sible con.gurations. A polygonization 
of the vertices within each cell, for each one of these con.gurations, is stored in a static look-up 
table. When symmetries are taken into account, the size of the table can be reduced sig­ni.cantly.  
6.3.5 Algorithms to Fit Implicit Surfaces to Point Clouds Let U be a relatively open and simply-connected 
subset of IR3, and f : U . IR a smooth function. The gradient Vf is a vector .eld de.ned on U. Given 
an oriented point cloud, i.e., a .nite set D of point-vector pairs (p, n), where p is an interior point 
of U, and n is a unit length 3D vector, the problem is to .nd a smooth function f so that f(p) 0 and 
V(p) n for every oriented point (p, n) in the data set D. We call the zero iso-level set of such a function 
{p : f(p)=0} a surface .t, or surface reconstruction, for the data set D. We are particularly interested 
in .tting isosurfaces to oriented point points. For the sake of simplicity, we assume that the domain 
is the unit cube U = [0, 1]3, the typical domain of an isosurface de.ned on an hexa­hedral mesh, and 
the isolevel is zero, i.e., the isosurface to be .tted to the data points is {p : f(p)=0}, but of course, 
the argument applies in more general cases. Figure 6.2: Early results of Vector Field Isosurface reconstruction 
from oriented point clouds introduced in [ST05]. Figure 6.2 shows results of surface reconstruction from 
an oriented point cloud using the simple variational formulation presented in [ST05], where oriented 
data points are regarded as samples of the gradient vector .eld of an implicit function, which is estimated 
by minimizing this energy function mm. NN E1(f)= f(pi)2 + .1 lVf(pi) - nil2 + .2 lHf(x)l2 dx , (6.4) 
V i=1 i=1 where f(x) is the implicit function being estimated, Vf(x) is the gradient of f, Hf(x) is the 
Hessian of f(x), (p1,n1),..., (pm,nm) are point-normal data pairs, V is a bounding volume, and .1 and 
.2 are regularization parame­ters. Minimizing this energy requires the solution of a simple large and 
sparse least squares problem. The result is usually unique modulo an addi­tive constant. Given that, 
for rendering or post-processing, isosurfaces are extracted from scalar functions de.ned over regular 
grids (e.g., via March­ing Cubes), it is worth exploring representations of implicit functions de­.ned 
as a regular scalar .elds. Finite difference discretization is used in [ST05], with the volume integral 
resulting in a sum of gradient differences over the edges of the regular grid, yet Equation 6.4 can be 
discretized in many other ways. Chapter 7  Applications and Emerging Trends Previous chapters outlined 
the basics of building custom 3D scanners. In this chapter we turn our attention to late-breaking work, 
promising direc­tions for future research, and a summary of recent projects motivated by the course material. 
We hope course attendees will be inspired to implement and extend some of these more advanced systems, 
using the basic mathe­matics, software, and methodologies we have presented up until this point. 7.1 
Extending Swept-Planes and Structured Light This course was previously taught by the organizers at Brown 
University in 2007 and 2009. On-line archives, complementing these course notes, are available at http://mesh.brown.edu/3dpgp-2007 
and http:// mesh.brown.edu/3dpgp-2009. In this section we brie.y review two course projects developed 
by students. The .rst project can be viewed as a direct extension of 3D slit scanning, similar to the 
device and concepts presented in Chapter 4. Rather than using a single camera, this project ex­ plores 
the bene.ts and limitations of using a stereoscopic camera in tandem with laser striping. The second 
project can be viewed as a direct extension of structured lighting, in fact utilizing the software that 
eventually led to that presented in Chapter 5. Through a combination of planar mirrors and a Fresnel 
lens, a novel imaging condition is achieved allowing a single dig­ital projector and camera to recover 
a complete 3D object model without moving parts. We hope to add more projects to the updated course notes 
as we hear from attendees about their own results. 7.1.1 3D Slit Scanning with Planar Constraints Leotta 
et al. [LVT08] propose 3D slit scanning with planar constraints as a novel 3D point reconstruction algorithm 
for multi-view swept-plane scan­ners. A planarity constraint is proposed, based on the fact that all 
observed points on a projected stripe must lie on the same 3D plane. The plane lin­early parameterizes 
a homography [HZ04] between any pair of images of the laser stripe, which can be recovered from point 
correspondences de­rived from epipolar geometry [SCD*06]. This planarity constraint reduces reconstruction 
outliers and allows for the reconstruction of points seen in only one view. As shown in Figure 7.1, a 
catadioptric stereo rig is constructed to re­ move artifacts from camera synchronization errors and non-uniform 
pro­jection. The same physical setup was originally suggested by Davis and Chen [DC01]. It maintains 
many of the desirable traits of other laser range scanners while eliminating several actuated components 
(e.g., the trans­lation and rotation stages in Figure 1.2), thereby reducing calibration com­ plexity 
and increasing maintainability and scan repeatability. Tracing back­ward from the camera, the optical 
rays .rst encounter a pair of primary mirrors forming a V shape. The rays from the left half of the image 
are re.ected to the left, and those from the right half are re.ected to the right. Next, the rays on 
each side encounter a secondary mirror that re.ects them back toward the center and forward. The viewing 
volumes of the left and right sides of the image intersect near the target object. Each half of the resulting 
image may be considered as a separate camera for image process­ing. The standard camera calibration techniques 
for determining camera position and orientation still apply to each half. Similar to Chapter 4, a user 
scans an object by manually manipulating a hand-held laser line projector. Visual feedback is provided 
at interactive rates in the form of incremental 3D reconstructions, allowing the user to sweep the line 
across any unscanned regions. Once the plane of light has been estimated, there are several ways to reconstruct 
the 3D location of the points. First, consider the non-singular case when a unique plane of light can 
be determined. If a point is visible from only one camera (due to occlusion or indeterminate correspondence), 
it can still be reconstructed by ray-plane intersection. For points visible in both views, it is bene.cial 
to use the data from both views. One approach is to triangulate the points using ray-ray intersection. 
This is the approach taken by Davis and Chen [DC01]. While both views are used, the constraint that the 
resulting 3D point lies on the laser stripe plane is not strictly enforced. Figure 7.1: 3D slit scanning 
with planar constraints. (Top left) The catadiop­tric scanning rig. (Top right) A sample image. (Bottom 
left) Diagram of the scanning system. Note that the camera captures a stereo image, each half originating 
from a virtual camera produced by mirror re.ection. (Bottom right) Working volume and scannable surfaces 
of a T-shaped object. Note that the working volume is the union of the virtual camera pair, excluding 
occluded regions. (Figure from [LVT08].) Leotta et al. [LVT08] examine how such a planarity constraint 
can en­ hance the reconstruction. Their .rst approach involves triangulating a point (using ray-ray intersection) 
and then projecting it onto the closest point on the corresponding 3D laser plane. While such an approach 
combines the stability of triangulation with the constraint of planarity, there is no guar­antee that 
the projected point is the optimal location on the plane. In their second approach, they compute an optimal 
triangulation constrained to the plane. The two projection approaches are compared in Figure 7.2. We 
refer the reader to the paper for additional technical details on the derivation of the optimally projected 
point. Illustrative results are shown in Figure 7.3. Note the bene.t of adding points seen in only one 
view, as well as the result of applying optimal planar projection. (a) laser plane to image homographies 
(b) projection onto the laser plane (b) A 2D view of triangulation and both orthogonal and optimal projection 
onto the laser plane. (Figure from [LVT08].)  (a) catadioptric stereo view (b) triangulated (c) optimal 
(d) all points 7.1.2 Surround Structured Lighting Lanman et al. [LCT07] propose surround structured 
lighting as a novel modi.cation of a traditional single projector-camera structured light sys­tem that 
allows full 360. surface reconstructions, without requiring turnta­bles or multiple scans. As shown in 
Figure 7.4, the basic concept is to il­ luminate the object from all directions with a structured pattern 
consisting of horizontal planes of light, while imaging the object from multiple views using a single 
camera and mirrors. A key bene.t of this design is to ensure that each point on the object surface can 
be assigned an unambiguous Gray code sequence, despite the possibility of being illuminated from multiple 
directions. Traditional structured light projectors, for example those using Gray code sequences, cannot 
be used to simultaneously illuminate an object from all sides (using more than one projector) due to 
interference. If such a con­.guration was used, then there is a high probability that certain points 
would be illuminated by multiple projectors. In such circumstances, multi­ple Gray codes would interfere, 
resulting in erroneous reconstruction due to decoding errors. Rather than using multiple projectors (each 
with a sin­gle center of projection), the proposed surround structured lighting sys­tem uses a single 
orthographic projector and a pair of planar mirrors. As shown from above and in pro.le in Figure 7.4, 
the key components of the proposed scanning system are an orthographic projector, two pla­nar mirrors 
aligned such that their normal vectors are contained within the plane of light created by each projector 
row, and a single high-resolution digital camera. If any structured light pattern consisting of horizontal 
bi­nary stripes is implemented, then the object can be fully illuminated on all sides due to direct and 
re.ected projected light. Furthermore, if the cam­era s .eld of view contains the object and both mirrors, 
then it will record .ve views of the illuminated object: one direct view, two .rst re.ections, and two 
second re.ections [FNdJV06]. By carefully aligning the mirrors so that individual projector rows are 
always re.ected back upon themselves, only a single Gray code sequence will be assigned to each projector 
row ensuring each vertically-space plane in the reconstruction volume receives a unique code. The full 
structured light pattern combined with the .ve views (see Figure 7.5) provides suf.cient information 
for a nearly complete surface reconstruction from a single camera position, following methods similar 
to those in Chapter 5. The required orthographic projector can be implemented using a stan­dard off-the-shelf 
DLP projector and a Fresnel lens, similar to that used by Figure 7.4: Surround structured lighting for 
full object scanning. (Top left) Surround structured lighting prototype. (Top right) The position of 
the real c0 and virtual {c1, c2, c12, c21} cameras with respect to mirrors {M1, M2}. (Middle) After re.ection, 
multiple rays from a single projector row illuminate the object from all sides while remaining co-planar. 
(Bottom) Prototype, from left to right: the .rst-surface planar mirrors, Fresnel lens, high-resolution 
digital camera, and DLP projector. (Figure from [LCT07].)  Figure 7.5: Example of an orthographic Gray 
code pattern and recovered projector rows. (Top left) Scene, as viewed under ambient illumination, for 
use in texture mapping. (Top right) Per-pixel projector row indices recov­ered by decoding the projected 
Gray code sequence (shaded by increasing index, from red to blue). (Bottom left) Fourth projected Gray 
code. (Bottom right) Sixth projected Gray code. (Figure from [LCT07].) Nayar and Anand [NA06] for volumetric 
display. The Fresnel lens converts light rays diverging from the focal point to parallel rays and can 
be man­ufactured in large sizes, while remaining lightweight and inexpensive. Al­though the projector 
can be modeled as a pinhole projector (see Chapters 2 and 3), practically it will have a .nite aperture 
lens and a corresponding .nite depth of .eld. This makes conversion to a perfectly-orthographic set of 
rays impossible with the Fresnel lens, yet an acceptable approximation is still feasible. The planar 
mirrors are positioned, following Forbes et al. [FNdJV06], such that their surface normals are roughly 
72. apart and perpendicular to the projected planes of light. This ensures that .ve equally-spaced views 
are created. The mirrors are mounted on gimbals with .ne tuning knobs in order to facilitate precise 
positioning. Furthermore, .rst-surface mirrors are used to eliminate refraction from the protective layer 
of glass covering the re.ective surface in lower-cost second-surface mirrors (e.g., those one would typically 
buy at a hardware store). Because of the unique design of the scanning system, calibration of the multiple 
components is a non-trivial task. Lanman et al. [LCT07] ad­ dress these issues by developing customized 
calibration procedures di­vided across three stages: (1) con.guration of the orthographic projector, 
(2) alignment of the planar mirrors, and (3) calibration of the camera/mirror system. Key results include 
the use of Gray codes and homographies to cal­ibrate the orthographic imaging system, procedures to ensure 
precise me­chanical alignment of the scanning apparatus, and optimization techniques for calibrating 
catadioptic systems containing a single digital camera and one or more planar mirrors. We refer readers 
to the paper for additional calibration details. Reconstruction proceeds in a similar manner to that 
used in conven­tional structured light scanners. A key modi.cation, however, is the re­quirement that 
each of the .ve sub-images must be assigned to a speci.c real or virtual camera. Afterwards, each optical 
ray is intersected with its associated projector plane (corresponding to an individual orthographic projector 
row) in order to reconstruct a dense 3-D point cloud. Illustrative results are tabulated in Figure 7.6. 
While the current prototype can only scan relatively small volumes, this system has already demonstrated 
practical bene.ts for telecollabora­tion applications, allowing for rapid acquisition of nearly complete 
object models. We hope the reader is inspired to pursue similar non-traditional optical con.gurations 
using their own 3D scanners. To this end, we also recommend reviewing related work by Epstein et al. 
[EGPP04], on incor­ porating planar mirrors with structured lighting, as well as the work of Nayar and 
Anand [NA06] on creating orthographic projectors using simi­ lar con.gurations of Fresnel lenses.  7.2 
Recent Advances and Further Reading 3D scanning remains a very active area of computer graphics and vision 
research. While numerous commercial products are available, few achieve the ease of use, visual .delity, 
and reliability of simple point-and-shoot cameras. As brie.y reviewed in Chapter 1, a myriad collection 
of both passive and active non-contact optical metrology methods have emerged. Many have not withstood 
the test of time, yielding to more .exible, lower­cost alternatives. Some, like 3D slit scanning and 
structured lighting, have become widespread in equal parts due to their performance, as well as their 
conceptual and practical accessibility. In this section we brie.y review late-breaking work and other 
advances that are shaping the .eld of optical 3D scanning. Before continuing, we encourage the reader 
to consider the materials from closely-related SIG-GRAPH 2009 courses. Heidrich and Ihrke present Acquisition 
of Optically Complex Objects and Phenomena, discussing methods to scan problematic ob­jects with translucent 
and specular materials. In a similar vein, Narasimhan et al. present Scattering, a course on imaging 
through participating me­dia. Several additional courses focus on specialized scanning applications; 
Debevec et al. cover face scanning in The Digital Emily Project: Photoreal Facial Modeling and Animation, 
whereas Cain et al. discuss scanning appli­cations in archeology and art history in Computation &#38; 
Cultural Heritage: Fundamentals and Applications. Finally, we recommend Gross Point Based Graphics-State 
of the Art and Recent Advances for a further tutorial on point­based rendering. Applying 3D scanning 
in practical situations requires carefully consid­ering the complexities of the object you are measuring. 
These complex­ities are explored, to great detail, by two pioneering efforts: the Digital Michelangelo 
Project [LPC*00] and the Piet`a Project [BRM*02]. In the for­ mer, the subsurface scattering properties 
of marble were considered; in the later, lightweight commercial laser scanners and photometric stereo 
were deployed to create a portable solution. Building low-cost, reliable systems for rapidly scanning 
such complex objects remains a challenging task. Passive methods continue to evolve. Light .elds [LH96, 
GGSC96], cap­ tured using either camera arrays [WJV*05] or specialized plenoptic cam­ eras [AW92, NLB*05, 
VRA*07], record the spatial and angular variation of the irradiance passing between two planes in the 
world (assuming no at­tenuation within the medium). Vaish et al. [VLS*06] recover depth maps from light 
.elds. Such imagery can be used to synthesize a virtual focal stack, allowing the method of Nayar and 
Nakagawa [NN94] to estimate shape-from-focus. Lanman et al. [LRAT08] extend light .eld capture to al­ 
low single-shot visual hull reconstruction of opaque objects. The limitations of most active scanning 
methods are well-known. Specif­ically, scanning diffuse surfaces is straightforward; however, scenes 
that contain translucence, subsurface scattering, or strong re.ections lead to artifacts and often require 
additional measures to be taken often involv­ing dappling the surface with certain Lambertian powders. 
Scanning such hard-to-scan items has received signi.cant attenuation over the last few years. Hullin 
et al. [HFI*08] immerse transparent objects in a .orescent liq­ uid. This liquid creates an inverse image 
as that produced by a traditional 3D slit scanner, where the laser sheet is visible up to the point of 
contact. In a similar vein, tomographic methods are used to reconstruct transparent objects; .rst, by 
immersion in an index-matching liquid [TBH06], and sec­ ond through the use of background-oriented Schlieren 
imaging [AIH*08]. In addition to scanning objects with complex material properties, ac­tive imaging is 
beginning to be applied to challenging environments with anisotropic participating media. For example, 
in typical underwater imag­ing conditions, visible light is strongly scattered by suspended particu­lates. 
Narasimhan et al. consider laser striping and photometric stereo in underwater imaging [NN05], as well 
as structured light in scattering me­ dia [NNSK08]. Such challenging environments represent the next 
horizon for active 3D imaging. 7.3 Conclusion As low-cost mobile projectors enter the market, we expect 
students and hobbyists to begin incorporating them into their own 3D scanning systems. Such projector-camera 
systems have already received a great deal of atten­tion in recent academic publications. Whether for 
novel human-computer interaction or ad-hoc tiled displays, consumer digital projection is set to revolutionize 
the way we interact with both physical and virtual assets. This course was designed to lower the barrier 
of entry to novices inter­ested in trying 3D scanning in their own projects. Through the course notes, 
on-line materials, and open source software, we have endeavored to elimi­nate the most dif.cult hurdles 
facing beginners. We encourage attendees to email the authors with questions or links to their own 3D 
scanning projects that draw on the course material. Revised course notes, updated software, recent publications, 
and similar do-it-yourself projects are maintained on the course website at http://mesh.brown.edu/dlanman/scan3d. 
We encourage you to take a look and see what your fellow attendees have built for themselves!  Bibliography 
 [AIH*08] ATCHESON B., IHRKE I., HEIDRICH W., TEVS A., BRADLEY D., MAGNOR M., SEIDEL H.-P.: Time-resolved 
3d capture of non-stationary gas .ows. ACM Transactions on Graphics (Proc. SIGGRAPH Asia) 27, 5 (2008), 
132. 78 [AW92] ADELSON T., WANG J.: Single lens stereo with a plenoptic camera. IEEE TPAMI 2, 14 (1992), 
99 106. 78 [BF95] BLOOMENTHAL J., FERGUSON K.: Polygonization of non­ manifold implicit surfaces. In 
SIGGRAPH 95: ACM SIG-GRAPH 1995 papers (1995), pp. 309 316. 66 [BK08] BRADSKI G., KAEHLER A.: Learning 
OpenCV: Computer Vision with the OpenCV Library. O Reilly Media, Inc., 2008. 32 [Bla04] BLAIS F.: Review 
of 20 years of range sensor development. Journal of Electronic Imaging 13, 1 (2004), 231 240. 5 [Blo88] 
BLOOMENTHAL J.: Polygonization of Implicit Surfaces. Com­ puter Aided Geometric Design 5, 4 (1988), 341 
355. 66 [Bou] BOUGUET J.-Y.: Camera calibration toolbox for matlab. http: //www.vision.caltech.edu/bouguetj/calib_doc/. 
28, 40, 52 [Bou99] BOUGUET J.-Y.: Visual methods for three-dimensional modeling. PhD thesis, California 
Institute of Technology, 1999. 38, 42 [BP] BOUGUET J.-Y., PERONA P.: 3d photography on your desk. http://www.vision.caltech.edu/bouguetj/ 
ICCV98/. 7, 35, 36, 45 [BP99] BOUGUET J.-Y., PERONA P.: 3d photography using shadows in dual-space geometry. 
Int. J. Comput. Vision 35, 2 (1999), 129 149. 35, 38, 39, 42 [BRM*02] BERNARDINI F., RUSHMEIER H., MARTIN 
I. M., MITTLEMAN J., TAUBIN G.: Building a digital model of michelangelo s .orentine piet`a. IEEE Computer 
Graphics and Applications 22 (2002), 59 67. 78 [CG00] CIPOLLA R., GIBLIN P.: Visual Motion of Curves 
and Surfaces. Cambridge University Press, 2000. 4 [CMU] CMU IEEE 1394 digital camera driver, version 
6.4.5. http://www.cs.cmu.edu/ iwan/1394/. 25 [Cre] CREAFORM: Handyscan 3D. http://www.creaform3d. com/en/handyscan3d/products/exascan.aspx.6 
[CS97] CHIANG Y., SILVA C. T.: I/O Optimal Isosurface Extraction. In IEEE Visualization 1997, Conference 
Proceedings (1997), pp. 293 300. 64 [CTMS03] CARRANZA J., THEOBALT C., MAGNOR M. A., SEIDEL H.-P.: Free-viewpoint 
video of human actors. ACM Trans. Graph. 22, 3 (2003), 569 577. 4 [dAST*08] DE AGUIAR E., STOLL C., THEOBALT 
C., AHMED N., SEIDEL H.-P., THRUN S.: Performancecapturefromsparsemulti-view video. In SIGGRAPH 08: ACM 
SIGGRAPH 2008 papers (2008), pp. 1 10. 4 [DC01] DAVIS J., CHEN X.: A laser range scanner designed for 
mini­mum calibration complexity. In Proceedings of the International Conference on 3-D Digital Imaging 
and Modeling (3DIM) (2001), p. 91. 70 [Deb97] DEBEVEC P. E.: Facade: modeling and rendering architecture 
from photographs and the campanile model. In SIGGRAPH 97: ACM SIGGRAPH 97 Visual Proceedings (1997), 
p. 254. 4 [DK91] DOI A., KOIDE A.: An Ef.cient Method of Triangulating Equivalued Surfaces by Using Tetrahedral 
Cells. IEICE Transac­tions on Communications and Electronics Information Systems E74, 1 (Jan. 1991), 
214 224. 63, 64 [Edm] EDMUND OPTICS:. http://www.edmundoptics.com. 37 [EGPP04] EPSTEIN E., GRANGER-PICH 
E´ M., POULIN P.: Exploiting mir­rors in interactive reconstruction with structured light. In Vi­sion, 
Modeling, and Visualization 2004 (2004), pp. 125 132. 77 [Far97] FARID H.: Range Estimation by Optical 
Differentiation. PhD the­sis, University of Pennsylvania, 1997. 4 [FNdJV06] FORBES K., NICOLLS F., DE 
JAGER G., VOIGT A.: Shape-from­ silhouette with two mirrors and an uncalibrated camera. ECCV 2006 (2006), 
pp. 165 178. 73, 75 In [FWM98] FERRYMAN J. M., WORRALL A. D., MAYBANK S. J.: Learn­ ing enhanced 3d models 
for vehicle tracking. In BMVC (1998), pp. 873 882. 4 [GGSC96] GORTLER S. J., GRZESZCZUK R., SZELISKI 
R., COHEN M. F.: The lumigraph. In SIGGRAPH 96: ACM SIGGRAPH 1996 pa­ pers (1996), pp. 43 54. 78 [GH95] 
GU ´ EZIEC A., HUMMEL R.: Exploiting Triangulated Surface Extraction Using Tetrahedral Decomposition. 
IEEE Transac­ tions on Visualization and Computer Graphics 1, 4 (1995). 64 [GSP06] GREENGARD A., SCHECHNER 
Y. Y., PIESTUN R.: Depth from diffracted rotation. Opt. Lett. 31, 2 (2006), 181 183. 4 [HARN06] HSU 
S., ACHARYA S., RAFII A., NEW R.: Performance of a time-of-.ight range camera for intelligent vehicle 
safety ap­plications. Advanced Microsystems for Automotive Applications (2006). 6 [Hec01] HECHT E.: Optics 
(4th Edition). Addison Wesley, 2001. 4 [HFI*08] HULLIN M. B., FUCHS M., IHRKE I., SEIDEL H.-P., LENSCH 
H. P. A.: Fluorescent immersion range scanning. ACM Trans. Graph. 27, 3 (2008), 1 10. 78 [HVB*07] HERN 
´ ANDEZ C., VOGIATZIS G., BROSTOW G. J., STENGER B., CIPOLLA R.: Non-rigid photometric stereo with colored 
lights. In Proc. of the 11th IEEE Intl. Conf. on Comp. Vision (ICCV) (2007). 7 [HZ04] HARTLEY R. I., 
ZISSERMAN A.: Multiple View Geometry in Com­ puter Vision, second ed. Cambridge University Press, 2004. 
3, 26, 70 [ISM84] INOKUCHI S., SATO K., MATSUDA F.: Range imaging system for 3-d object recognition. 
In Proceedings of the International Con­ ference on Pattern Recognition (1984), pp. 806 808. 48 [IY01] 
IDDAN G. J., YAHAV G.: Three-dimensional imaging in the studio and elsewhere. Three-Dimensional Image 
Capture and Ap­ plications IV 4298, 1 (2001), 48 55. 6 [KM00] KAKADIARIS I., METAXAS D.: Model-based 
estimation of 3d human motion. IEEE Transactions on Pattern Analysis and Ma­ chine Intelligence 22, 12 
(2000), 1453 1459. 4 [Las] LASER DESIGN INC.: Surveyor DT-2000 desktop 3D laser scanner. http://www.laserdesign.com/ 
quick-attachments/hardware/low-res/dt-series. pdf. 6 [Lau94] LAURENTINI A.: The Visual Hull Concept for 
Silhouette-Based Image Understanding. IEEE TPAMI 16, 2 (1994), 150 162. 3 [LC87] LORENSEN W. L., CLINE 
H. E.: Marching Cubes: A High Resolution 3D Surface Construction Algorithm. In Siggraph 87, Conference 
Proceedings (1987), ACM Press, pp. 163 169. 63, 66 [LCT07] LANMAN D., CRISPELL D., TAUBIN G.: Surround 
structured lighting for full object scanning. In Proceedings of the Interna­ tional Conference on 3-D 
Digital Imaging and Modeling (3DIM) (2007), pp. 107 116. 73, 74, 75, 76 [LFDF07] LEVIN A., FERGUS R., 
DURAND F., FREEMAN W. T.: Image and depth from a conventional camera with a coded aperture. ACM Trans. 
Graph. 26, 3 (2007), 70. 4 [LH96] LEVOY M., HANRAHAN P.: Light .eld rendering. In Proc. of ACM SIGGRAPH 
(1996), pp. 31 42. 78 [LN04] LAND M. F., NILSSON D.-E.: Animal Eyes. Oxford University Press, 2004. 2 
 [LPC*00] LEVOY M., PULLI K., CURLESS B., RUSINKIEWICZ S., KOLLER D., PEREIRA L., GINZTON M., ANDERSON 
S., DAVIS J., GINS-BERG J., SHADE J., FULK D.: The digital michelangelo project: 3D scanning of large 
statues. In Proceedings of ACM SIGGRAPH 2000 (2000), pp. 131 144. 78 [LRAT08] LANMAN D., RASKAR R., AGRAWAL 
A., TAUBIN G.: Shield .elds: modeling and capturing 3d occluders. ACM Trans. Graph. 27, 5 (2008), 1 10. 
78 [LT] LANMAN D., TAUBIN G.: Build your own 3d scanner: 3d photography for beginners (course website). 
http://mesh. brown.edu/dlanman/scan3d. 37 [LVT08] LEOTTA M. J., VANDERGON A., TAUBIN G.: 3d slit scanning 
with planar constraints. Computer Graphics Forum 27, 8 (Dec. 2008), 2066 2080. 70, 71, 72 [Mat] MATHWORKS, 
INC: Image acquisition toolbox. http://www. mathworks.com/products/imaq/. 25, 46 [MBR*00] MATUSIK W., 
BUEHLER C., RASKAR R., GORTLER S. J., MCMILLAN L.: Image-based visual hulls. In SIGGRAPH 00: ACM SIGGRAPH 
2000 papers (2000), pp. 369 374. 4 [Mit] MITSUBISHI ELECTRIC CORP.: XD300U user man­ ual. http://www.projectorcentral.com/pdf/ 
projector_manual_1921.pdf. 46 [MPL04] MARC R. Y., POLLEFEYS M., LI S.: Improved real-time stereo on commodity 
graphics hardware. In In IEEE Workshop on Real­ time 3D Sensors and Their Use (2004). 3 [MSKS05] MA Y., 
SOATTO S., KOSECKA J., SASTRY S. S.: An Invitation to 3-D Vision. Springer, 2005. 26, 39 [NA06] NAYAR 
S. K., ANAND V.: Projection Volumetric Display Using Passive Optical Scatterers. Tech. rep., July 2006. 
75, 77 [Nex] NEXTENGINE: 3D Scanner HD. nextengine.com/indexSecure.htm. 6 https://www. [NLB*05] NG R., 
LEVOY M., BREDIF M., DUVAL G., HOROWITZ M., HANRAHAN P.: Light .eld photography with a hand-held plenoptic 
camera. Tech Report, Stanford University (2005). 78 [NN94] NAYAR S. K., NAKAGAWA Y.: Shape from focus. 
IEEE Trans. Pattern Anal. Mach. Intell. 16, 8 (1994), 824 831. 4, 78 [NN05] NARASIMHAN S. G., NAYAR S.: 
Structured light methods for underwater imaging: light stripe scanning and photometric stereo. In Proceedings 
of 2005 MTS/IEEE OCEANS (September 2005), vol. 3, pp. 2610 2617. 78 [NNSK08] NARASIMHAN S. G., NAYAR 
S. K., SUN B., KOPPAL S. J.: Structured light in scattering media. In SIGGRAPH Asia 08: ACM SIGGRAPH 
Asia 2008 courses (2008), pp. 1 8. 78 [Opea] Open source computer vision library. http:// sourceforge.net/projects/opencvlibrary/. 
26, 40 [Opeb] OpenCV wiki. http://opencv.willowgarage.com/ wiki/. 46 [OSS*00] ORMONEIT D., SIDENBLADH 
H., SIDENBLADH H., BLACK M. J., HASTIE T., FLEET D. J.: Learning and tracking human motion using functional 
analysis. In IEEE Workshop on Human Modeling, Analysis and Synthesis (2000), pp. 2 9. 4 [PA82] POSDAMER 
J., ALTSCHULER M.: Surface measurement by space encoded projected beam systems. Computer Graphics and 
Image Processing 18 (1982), 1 17. 47 [Poia] POINT GREY RESEARCH, INC.: Grasshopper IEEE-1394b digital 
camera. http://www.ptgrey.com/products/ grasshopper/index.asp. 26, 46 [Poib] POINT GREY RESEARCH, INC.: 
Using matlab with point grey cameras. http://www.ptgrey.com/support/kb/ index.asp?a=4&#38;q=218. 46 [Pol] 
POLHEMUS: FastSCAN. http://www.polhemus.com/ ?page=Scanning_Fastscan.6 [Psy] PSYCHOPHYSICS TOOLBOX:. 
http://psychtoolbox.org. 31, 33, 47 [RWLB01] RASKAR R., WELCH G., LOW K.-L., BANDYOPADHYAY D.: Shader 
lamps: Animating real objects with image-based illu­ mination. In Proceedings of the 12th Eurographics 
Workshop on Rendering Techniques (2001), Springer-Verlag, pp. 89 102. 4 [SB03] SUFFERN K. G., BALSYS 
R. J.: Rendering the intersections of implicit surfaces. IEEE Comput. Graph. Appl. 23, 5 (2003), 70 77. 
66 [SCD*06] SEITZ S., CURLESS B., DIEBEL J., SCHARSTEIN D., SZELISKI R.: A comparison and evaluation 
of multi-view stereo recon­ struction algorithms. In CVPR 2006 (2006). 3, 70 [SH03] STARCK J., HILTON 
A.: Model-based multiple view recon­ struction of people. In Proceedings of the Ninth IEEE International 
Conference on Computer Vision (2003), p. 915. 4 [SMP05] SVOBODA T., MARTINEC D., PAJDLA T.: A convenient 
multi­camera self-calibration for virtual environments. PRESENCE: Teleoperators and Virtual Environments 
14, 4 (August 2005), 407 422. 27 [SPB04] SALVI J., PAG ` ES J., BATLLE J.: Pattern codi.cation strategies 
in structured light systems. In Pattern Recognition (April 2004), vol. 37, pp. 827 849. 6, 47 [ST05] 
SIBLEY P. G., TAUBIN G.: Vector.eld Isosurface-based Recon­ struction from Oriented points. In SIGGRAPH 
05 Sketch (2005). 67, 68 [Sul95] SULLIVAN G.: Model-based vision for traf.c scenes using the ground-plane 
constraint. 93 115. 4 [TBH06] TRIFONOV B., BRADLEY D., HEIDRICH W.: Tomographic re­ construction of transparent 
objects. In SIGGRAPH 06: ACM SIGGRAPH 2006 Sketches (2006), p. 55. 78 [TPG99] TREECE G. M., PRAGER R. 
W., GEE A. H.: Regularised March­ ing Tetrahedra: Improved Iso-Surface Extraction. Computers and Graphics 
23, 4 (1999), 583 598. 64 [VF92] VAILLANT R., FAUGERAS O. D.: Using extremal boundaries for 3-d object 
modeling. IEEE Trans. Pattern Anal. Mach. Intell. 14, 2 (1992), 157 173. 3 [VLS*06] VAISH V., LEVOY 
M., SZELISKI R., ZITNICK C. L., KANG S. B.: Reconstructing occluded surfaces using synthetic apertures: 
Stereo, focus and robust measures. In Proc. IEEE Computer Vi­ sion and Pattern Recognition (2006), pp. 
2331 2338. 78 [VRA*07] VEERARAGHAVAN A., RASKAR R., AGRAWAL R., MOHAN A., TUMBLIN J.: Dappled photography: 
Mask enhanced cam­ eras for heterodyned light .elds and coded aperture refocus­ ing. ACM Trans. Graph. 
26, 3 (2007), 69. 78 [Wik] WIKIPEDIA: Gray code. wiki/Gray_code. 48 http://en.wikipedia.org/ [WJV*05] 
WILBURN B., JOSHI N., VAISH V., TALVALA E.-V., ANTUNEZ E., BARTH A., ADAMS A., HOROWITZ M., LEVOY M.: 
High performance imaging using large camera arrays. ACM Trans. Graph. 24, 3 (2005), 765 776. 78 [WN98] 
WATANABE M., NAYAR S. K.: Rational .lters for passive depth from defocus. Int. J. Comput. Vision 27, 
3 (1998), 203 225. 4 [Woo89] WOODHAM R. J.: Photometric method for determining surface orientation from 
multiple images. 513 531. 7 [WvO96] WYVILL B., VAN OVERVELD K.: Polygonization of Implicit Surfaces with 
Constructive Solid Geometry. Journal of Shape Modelling 2, 4 (1996), 257 274. 66 [ZCS03] ZHANG L., CURLESS 
B., SEITZ S. M.: Spacetime stereo: Shape recovery for dynamic scenes. In IEEE Conference on Computer 
Vision and Pattern Recognition (June 2003), pp. 367 374. 39 [Zha99] ZHANG Z.: Flexible camera calibration 
by viewing a plane from unknown orientations. In International Conference on Com­ puter Vision (ICCV) 
(1999). 40 [Zha00] ZHANG Z.: A .exible new technique for camera calibration. IEEE Trans. Pattern Anal. 
Mach. Intell. 22, 11 (2000), 1330 1334. 24, 25, 27 [ZPKG02] ZWICKER M., PAULY M., KNOLL O., GROSS M.: 
Pointshop 3d: an interactive system for point-based surface editing. ACM Trans. Graph. 21, 3 (2002), 
322 329. 58 87 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1667248</article_id>
		<sort_key>90</sort_key>
		<display_label>Article No.</display_label>
		<display_no>9</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Color imaging]]></title>
		<page_from>1</page_from>
		<page_to>239</page_to>
		<doi_number>10.1145/1667239.1667248</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1667248</url>
		<abstract>
			<par><![CDATA[<p>The study of color combines a unique mixture of physics and human visual perception, which in practice makes this seemingly straightforward topic challenging and often misunderstood. This course covers the basics of physical and perceptual processes, and demonstrates how color science can be appropriately applied to various applications in rendering, high-dynamic-range imaging, and image manipulation. It is specifically designed for students and professionals who need an understanding of color science as a sid-line to their own work, especially those who design algorithms in computer graphics, computer vision, and image processing. It provides a basic understanding of how color science supports design of effective algorithms, and often leads to lower storage requirements, enhanced computational complexity, and better visual quality.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1797124</person_id>
				<author_profile_id><![CDATA[81100331006]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Erik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reinhard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797125</person_id>
				<author_profile_id><![CDATA[81100633003]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ward]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797126</person_id>
				<author_profile_id><![CDATA[81100449055]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Garrett]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Johnson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>280864</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Paul Debevec, "Rendering Synthetic Objects into Real Scenes: Bridging Traditional and Image-Based Graphics with Global Illumination and High Dynamic Range Photography," <i>Computer Graphics (Proceedings of ACM Siggraph 98).</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258884</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Paul Debevec, Jitendra Malik, "Recovering High Dynamic Range Radiance Maps from Photographs," <i>Computer Graphics (Proceedings of ACM Siggraph 97).</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Andrew Glassner, "Adaptive Run-Length Encoding," in <i>Graphics Gems II</i>, edited by James Arvo, Academic Press, (1991).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614380</ref_obj_id>
				<ref_obj_pid>614268</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Greg Larson, Holly Rushmeier, Christine Piatko, "A Visibility Matching Tone Reproduction Operator for High Dynamic Range Scenes," <i>IEEE Transactions on Visualization and Computer Graphics</i>, &#60;b&#62;3&#60;/b&#62;, 4, (1997).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280922</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Sumant Pattanaik, James Ferwerda, Mark Fairchild, Don Greenberg, "A Multiscale Model of Adaptation and Spatial Vision for Realistic Image Display," <i>Computer Graphics (Proceedings of Siggraph 98).</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192286</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Greg Ward, "The RADIANCE Lighting Simulation and Rendering System," <i>Computer Graphics (Proceedings of Siggraph 94).</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Greg Ward, "Real Pixels," in <i>Graphics Gems II</i>, edited by James Arvo, Academic Press, (1991).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Gunter Wyszecki, W. S. Stiles, <i>Color Science: Concepts and Methods, Quantitative Data and Formulae</i>, Second Edition, Wiley, (1982).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122729</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[C. Borges. Trichromatic Approximation for Computer Graphics Illumination Models. <i>Proc. Siggraph '91.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Anthony J. Calabria and Mark D. Fairchild. Herding CATs: A Comparison of Linear Chromatic-Adaptation Transforms for CIECAM97s. <i>Proc. 9th Color Imaging Conf.</i>, pp. 174--178, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>300778</ref_obj_id>
				<ref_obj_pid>300776</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Kristin J. Dana, Bram van Ginneken, Shree K. Nayar and Jan J. Koenderink. Reflectance and Texture of Real World Surfaces. <i>ACM TOG</i>, &#60;b&#62;15&#60;/b&#62;(1):1--34, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[G. D. Finlayson and P. Morovic. Is the Sharp Adaptation Transform more plausible than CMCCAT2000? <i>Proc. 9th Color Imaging Conf.</i>, pp. 310--315, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>527570</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Andrew S. Glassner. Principles of Digital Image Synthesis. Morgan Kaufmann, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614380</ref_obj_id>
				<ref_obj_pid>614268</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[G. W. Larson, H. Rushmeier and C. Piatko. A Visibility Matching Tone Reproduction Operator for High Dynamic Range Scenes. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 3(4) (December 1997).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Laurence T. Maloney. Evaluation of Linear Models of Surface Spectral Reflectance with Small Numbers of Parameters. <i>J. Optical Society of America A</i>, &#60;b&#62;3&#60;/b&#62;(10):1673--1683 (October 1986).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[David Marimont and Brian Wandell. Linear Models of Surface and Illuminant Spectra. <i>J. Optical Society of America A</i>, &#60;b&#62;9&#60;/b&#62;(11):1905--1913 (November 1992).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[C. S. McCamy, H. Marcus and J. G. Davidson. A color-rendition chart. <i>J. Applied Photographic Engineering</i>, &#60;b&#62;2&#60;/b&#62;(3):95--99 (summer 1976).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[R. McDonald and K. J. Smith. CIE94 - a new colur-difference formula. <i>Soc. Dyers Col.</i>, &#60;b&#62;111&#60;/b&#62;:376--9, Dec 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>45601</ref_obj_id>
				<ref_obj_pid>45596</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Gary Meyer. Wavelength Selection for Synthetic Image Generation. <i>Computer Vision, Graphics and Image Processing</i>, &#60;b&#62;41&#60;/b&#62;:57--79, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280922</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Sumanta N. Pattanaik, James A. Ferwerda, Mark D. Fairchild and Donald P. Greenberg. A multiscale model of adaptation and spatial vision for realistic image display. <i>Proc. Siggraph '98.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166142</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Mark S. Peercy. Linear color representations for full speed spectral rendering. <i>Proc. Siggraph '93.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>643331</ref_obj_id>
				<ref_obj_pid>643328</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Brian Smits. An RGB to Spectrum Conversion for Reflectances. <i>J. Graphics Tools</i>, &#60;b&#62;4&#60;/b&#62;(4):11--22, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Michael Stokes et al. A Standard Default Color Space for the Internet -- sRGB. Ver. 1.10, November 1996. http://www.w3.org/Graphics/Color/sRGB.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[S. Sueeprasan and R. Luo. Incomplete Chromatic Adaptation under Mixed Illuminations. <i>Proc. 9th Color Imaging Conf.</i>, pp. 316--320, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[S. S&#252;sstrunk, J. Holm and G. D. Finlayson. Chromatic Adaptation Performance of Different <i>RGB</i> Sensors. <i>IS&T/SPIE Electronic Imaging</i>, SPIE &#60;&#60;b&#62;4300&#60;/b&#62;, Jan. 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Greg Ward et al. Materials and Geometry Format. http://radsite.lbl.gov/mgf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Harold B. Westlund and Gary W. Meyer. A BRDF Database Employing the Beard-Maxwell Reflection Model. <i>Graphics Interface 2002.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[G&#252;nter Wyszecki and W. S. Stiles. Color Science: Concepts and Methods, Quantitative Data and Formulae. John Wiley&amp;Sons, New York, 2nd ed., 1982.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[{Chiu 93} K. Chiu, M. Herf, P. Shirley, S. Swamy, C. Wang and K. Zimmerman "Spatially nonuniform scaling functions for high contrast images," <i>Proceedings of Graphics Interface '93</i>, Toronto, Canada, (May 1993).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280864</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[{Debevec 98} Paul Debevec, "Rendering Synthetic Objects into Real Scenes: Bridging Traditional and Image-Based Graphics with Global Illumination and High Dynamic Range Photography," <i>Computer Graphics (Proceedings of ACM Siggraph 98).</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258884</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[{Debevec 97} Paul Debevec, Jitendra Malik, "Recovering High Dynamic Range Radiance Maps from Photographs," <i>Computer Graphics (Proceedings of ACM Siggraph 97).</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237191</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[{Debevec 96} Paul Debevec, Camillo Taylor, Jitendra Malik, "Modeling and Rendering Architecture from Photographs: A hybrid geometry- and image-based approach," <i>Computer Graphics (Proceedings of ACM Siggraph 96).</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[{Glassner 91} Andrew Glassner, "Adaptive Run-Length Encoding," in <i>Graphics Gems II</i>, edited by James Arvo, Academic Press, (1991).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[{IES 93} Illuminating Engineering Society of North America, <i>IES Lighting Handbook, Reference Volume</i>, IESNA (1993).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2319338</ref_obj_id>
				<ref_obj_pid>2318946</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[{Jobson 97} Daniel Jobson, Zia-ur Rahman, and Glenn A. Woodell. "Properties and Performance of a Center/Surround Retinex," <i>IEEE Transactions on Image Processing</i>, Vol. 6, No. 3 (March 1997).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[{Larson 98} Greg Larson, "Overcoming Gamut and Dynamic Range Limitations in Digital Images," Color Imaging Conference, Scottsdale, Arizona (1998).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614380</ref_obj_id>
				<ref_obj_pid>614268</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[{Larson 97} Greg Larson, Holly Rushmeier, Christine Piatko, "A Visibility Matching Tone Reproduction Operator for High Dynamic Range Scenes," <i>IEEE Transactions on Visualization and Computer Graphics</i>, &#60;b&#62;3&#60;/b&#62;, 4, (1997).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280922</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[{Pattanaik 98} Sumant Pattanaik, James Ferwerda, Mark Fairchild, Don Greenberg, "A Multiscale Model of Adaptation and Spatial Vision for Realistic Image Display," <i>Computer Graphics (Proceedings of Siggraph 98).</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2213</ref_obj_id>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[{Rogers 85} David Rogers, <i>Procedural Elements for Computer Graphics</i>, McGraw-Hill, (1985).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218466</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[{Spencer 95} G. Spencer, P. Shirley, K. Zimmerman, and D. Greenberg, "Physically-based glare effects for computer generated images," <i>Computer Graphics (Proceedings of Siggraph 95).</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>48045</ref_obj_id>
				<ref_obj_pid>46165</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[{Stone 88} Maureen Stone, William Cowan, John Beatty, "Color Gamut Mapping and the Printing of Digital Color Images," <i>ACM Transactions on Graphics</i>, 7(3):249--292, (October 1988).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617873</ref_obj_id>
				<ref_obj_pid>616030</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[{Tumblin93} Tumblin, Jack and Holly Rushmeier. "Tone Reproduction for Realistic Images," <i>IEEE Computer Graphics and Applications</i>, November 1993, 13(6).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192286</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[{Ward 94} Greg Ward, "The RADIANCE Lighting Simulation and Rendering System," <i>Computer Graphics (Proceedings of Siggraph 94).</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[{Ward 91} Greg Ward, "Real Pixels," in <i>Graphics Gems II</i>, edited by James Arvo, Academic Press, (1991).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[{Wyszecki 82} Gunter Wyszecki, W. S. Stiles, <i>Color Science: Concepts and Methods, Quantitative Data and Formulae</i>, Second Edition, Wiley, (1982).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1667249</article_id>
		<sort_key>100</sort_key>
		<display_label>Article No.</display_label>
		<display_no>10</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Computation & cultural heritage]]></title>
		<subtitle><![CDATA[fundamentals and applications]]></subtitle>
		<page_from>1</page_from>
		<page_to>183</page_to>
		<doi_number>10.1145/1667239.1667249</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1667249</url>
		<abstract>
			<par><![CDATA[<p>This course surveys several practical techniques advanced by graphics and vision researchers for applications in cultural heritage, archeology, and art history. Topics include: efficient techniques for digital capture of heritage objects, appropriate uses in the heritage field, an end-to-end pipeline for processing archeological reconstructions (with special attention to incorporating archeological data and review throughout the process), how digital techniques are actually used in cultural heritage projects, and an honest evaluation of progress and challenges in this field.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Fine arts</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010470</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Fine arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1797127</person_id>
				<author_profile_id><![CDATA[81100013389]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Benedict]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Brown]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Katholieke Universiteit Leuven]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797128</person_id>
				<author_profile_id><![CDATA[81100506574]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kevin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cain]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INSIGHT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797129</person_id>
				<author_profile_id><![CDATA[81100397059]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Paolo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cignoni]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CNR, Pisa]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797130</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ICT, USC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797131</person_id>
				<author_profile_id><![CDATA[81448592805]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Downing]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[xRes]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797132</person_id>
				<author_profile_id><![CDATA[81408602556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Mashhuda]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Glencross]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Greg Ward, Anyhere]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
  Computation &#38; Cultural Heritage: Fundamentals and Applications  Section I: Cultural Heritage 
Landscapeand Techniques .Introduction .Case Studies .Image Traversal .Efficient Visualization .Applications 
of image-based synthesis .A range scanning and reconstruction pipeline for heritage   Section II: 
Graphics resources for heritage &#38; Heritage lessons for graphics .Low cost software tools (HDRShop, 
MeshLab) .Setting up an effective collaboration and acquisition system .The pitfalls of 3D acquisition 
for heritage .Conclusions and case studies   Presenters .Benedict Brown, Katholieke Universiteit 
Leuven .Kevin Cain, INSIGHT .Paolo Cignoni, CNR, Pisa .Paul Debevec, ICT, USC .Greg Downing, xRes 
 .Mashhuda Glencross (for Greg Ward, Anyhere)   Introduction Kevin Cain  When we refer to 'cultural 
heritage',what do we mean? -What constitutes 'heritage' as a distinct field? -Which projects have been 
successful? -Success in what terms?  Taxonomy of Recent Projects  Many graphics researchers have chosen 
to work with data from heritage objects .Alan Chalmers, University of Warwick .Carlos Esteban, Toshiba 
Research Europe .Deiter Fellner, Fraunhofer Institute forComputer Graphics Research, Germany .Luc Van 
Gool, University of Leuven .Holly Rushmeier, Yale University .Roberto Scopigno, CNR   Graphics, 
Vision and Heritage .ACM SIGGRAPH Campfire:Graphics &#38; Archaeology Snowbird, Utah, May 2000   
 Heritage researchers are finding applications within graphics .Remixing Çatalhöyük Ruth Tringham, 
Anthopology, UC Berkeley    EPOCH EU Framework Project .EPOCH, EU: 2004-2008 .MeshLab Visual Computing 
LabISTI-CNR  .CityEngine Pascal Mueller, ETH  .Archive Mapper for Archaeology    Visual Computing 
Lab, CNR .Digital Minerva 2000-2007 .Digital Cathedral of Pisa .Software for heritage: MeshLab HPTM 
Browser Metro    Non-profit groups .Groups receiving public and private support Public: NSF, EU, 
Public grants Private: Foundations, private donors, museums     Subdomains in Cultural Heritage 
 .Cultural Heritage is an inspecific term, encompassing: Archaeology Anthropology Art History Area 
Studies Visual Studies  .Each discipline has boundaries   Levoy, Pulli, Curless et al, Digital Michelangelo 
Project, 2000  Marc Levoy Marc Levoy Graphics in the service of the humanities  Debevec et al, The 
Parthenon, SIGGRAPH 2004 ET Marc Levoy Graphics in the service of the humanities University of Virginia, 
UCLA, Politecnico di Milano, Rome Reborn, 2008  Efficient traversal of large sets ofimages in cultural 
heritage Greg Downing, Paolo Cignoni   map2.jpg Yosemite Extreme Panoramic Imaging Project Managing 
Large Image Datasets Title_page.jpg  Rockfall in Yosemite can be a hazard 01_Goals.png  Edit this text 
to create a Heading .This subtitle is 24 points .Bullets are green .They have 110% line spacing, 6 
points before/after .Longer bullets in the form of a paragraph are harder to read if there is insufficient 
line spacing. This is the maximum recommended number of lines per slide (seven). .Sub bullets look like 
this   02_stock.png  This slide has a 16:9 media window 03_hazard.png  This slide has a 4:3 media 
window 04_rockfall_map.png  05_rephotography.png  06_gigapixel.png  07_gigapixel.png  08_locations.png 
 09_volunteers.png  09_volunteers.png  11_total_travel.png  12_ZUI.png  13_DEMS.png  14_color_DEM.png 
 15_1m_res.png  16_tracking.png  17_view.png  18_view_and_geometry.png  19_rendering.png  20_slitscan.png 
 21_ortho.png  22_sigg08.png  23_improvements.png  24_photog.png  25_fly1.png  26_fly2.png  27_fly3.png 
 28_fly4.png  29_photosynth.png  30_point_cloud.png  31_camerras.png  32_large_image_datasets.png 
 33_tools.png  background.png result   Simple, efficient visualization of heritage objects Greg 
Ward(presented by Mashhuda Glencross)  Simple, Efficient Visualization of Heritage Objects Greg Ward, 
Dolby Canada  Goals .Produce registered height field with texture . Good-enough .model.for.visual.reconstruction 
  Laptop:Users:gward:Documents:Conferences:sg09:CultHeritageCourse:RockChimney.mp4 Glencross.et.al.,. 
A.Perceptually.Validated Model for Surface Depth Hallucination, .SIGGRAPH.2008  Example from ChichénItzá 
 Laptop:Users:gward:Documents:Conferences:sg09:CultHeritageCourse:VenusSouth.mov  Depth Hallucination 
Method .Capture photo pair with and without flash under diffuse ambient lighting (overcast or sky light) 
 .Flash calibration required once per lens/flash combination  .Process RAW image files to yield depth 
&#38; albedo maps .Surface normal map may be computed with 3 flashes  .Convert to renderable model 
   Image Capture E+J7_f1 Flash E+J7_am No Flash  Initial Image Processing Albedo E+J7_a (Flash -Diffuse)/Flash_calibration 
 Shading E+J7_I Diffuse / Albedo Images are first cropped and aligned, as necessary  Example Flash 
Calibration Set FlashCalib  Final Image Processing E+J7_D Depth derived from shading image  Rendered 
height field with albedo map Data is perfectly aligned at full-resolution with no gaps E+J7_rend  
Deluxe Three-flash Capture System 3flashDevice  GatleyDarkRocks1_am GatleyDarkRocks1_f1 GatleyDarkRocks1_f2 
GatleyDarkRocks1_f3 Diffuse Lighting Flash 3 (lower right) Flash 2 (lower left) Flash 1 (top)  Previous 
Single Flash Result GatleyDarkRocks1_1s  Three Flash Result Rendered w/o Normals GatleyDarkRocks1_3s 
 Three Flash Result with Surface Normals GatleyDarkRocks1_3n  What If Even One Flash Is Too Many? 
 .Sometimes, we cannot use a flash reliably .Distant surfaces (esp. large areas) .Bright conditions 
 .In these cases, we resort to Histogram Matching .Take sample capture of similar surface using flash(es) 
   Simplified Capture w/o Flash Histogram Matching Needs exemplar model Single diffuse-lit photo 
 Match histograms Create rendering   BrickLeaves1_diff BrickLeaves1b BrickLeaves1b_30_160S albedo 
diffuse  Conclusions .Simple capture method provides renderable model .Works on most surfaces that 
can be represented with a height field (i.e., no detached geometry) .Three-flash method produces improved 
results, perceptually indistinguishable from photographs .Histogram matching allows us to hallucinate 
depth and albedo on surfaces too large for effective flash .Need means to hallucinate surface normals 
as well   Acknowledgements .MashhudaGlencrossfor data capture, subject studies, paper writing, and 
brainstorming .Roger Hubboldfor ideas, grant writing and proofing .Caroline Jayfor experimental design 
 .FranchoMelendezand Jun Liufor coding, testing, and animation production .Alan Murtafor testing, proofing 
and moral support .KevenCain&#38; Insight Digitalfor site access, etc. .EPSRCand Dolbyfor funding and 
support   Software Availability aig.cs.man.ac.uk/research/daedalus/hallucination.php Command-line 
and (soon) interactive GUI tool: THANKS!  Applications of image-based synthesis and HDR imaging for 
heritage sites and objects Paul Debevec  Fifteen Minutes  Section II: Graphics resources for heritage 
&#38; Heritage lessons for graphics .Low cost software tools (HDRShop, MeshLab) .Setting up an effective 
collaboration and acquisition system .The pitfalls of 3D acquisition for heritage .Conclusions and 
case studies   Low cost resources for image synthesis andimage-based modeling Paolo Cignoni, Paul 
Debevec  Background 3D acquisition technologies Going low cost Free tools HW and SW  3D scanning technologies 
Many different technologies, just a few examples: Laser or structured light, Triangulation Small/medium 
scale artifacts (statues) Small/medium workspace 20x20 <->150x150 cm, High accuracy (>0.05 mm) Fast 
(1 shot in ~1-2 sec)  Laser, Time of flight Large scale (architectures) Wide workspace (many meters) 
 lower accuracy (~4-10 mm) Slow (1 shot in ~20 min)   cyrax_device minolta_vi-910 3D Scanning, Visual 
Computing Lab 79 NextEngine fig1b.png duomo.png 3D scanning raw output data For the user, same type 
of output data : Range map: 2D map of sampled 3D points (640x480 -> 1024x1024) Can be managed as a point 
cloudor a triangulated surface chunk   80 face_points   face_points_eye  Why processing raw scanned 
data? 81 The acquisition of a single shot(range map) is only a single step in the 3D scanning process, 
since it returns a partial &#38; incompleterepresentation Processing is needed SW tools are an important 
part Of the whole pipeline fig2a.png Why processing raw scanned data? 82 The acquisition of a single 
shot(range map) is only a single step in the 3D scanning process, since it returns a partial &#38; incompleterepresentation 
Processing is needed SW tools are an important part Of the whole pipeline fig2a.png Motivations for 
image based reconstruction Photos are a known medium Low cost In case of failure you still have a nice 
documentation Technology is evolving, but relies on the same input data   Technologies Structure and 
motion 2min Short description of the key concepts Feature matching -> camera positions Dense reconstruction 
-> rangemaps   In practice Take photos Reconstruct 3d information for the photo Process range maps 
  In practice: Taking Photos How to take photos   In practice: processing How to build a nice model 
inside meshlab   A bit of comparison compar 3D Scanning, Visual Computing Lab 88 Result of multi-stereo 
matching(left, Uni.Leuven) vs.triangulation laser scanner(right, limit conditions: 1.4 m, wide lenses) 
 Partial Reconstruction Browsing the photo photosynth Photocloud   Conclusions Pro and cons   
 A pipeline for efficiently reconstructingheritage objects from range data Benedict Brown, Kevin Cain, 
Mark Eakle, Todd Gill  titlepagelr.pdf            0007CBE3 Q Server      C01E64AA: 
Global Non-Rigid Alignment Benedict BrownKatholieke Universiteit Leuven secondpage.pdf       
      0007CBE3 Q Server      C01E64AA: 3-D Scanning Pipeline .Acquisition  several_scans.jpg 
Scanners acquire data from a single viewpoint secondpage.pdf             0007CBE3 Q Server 
      C01E64AA: 3-D Scanning Pipeline .Acquisition .Alignment  several_scans.jpg secondpage.pdf 
            0007CBE3 Q Server      C01E64AA: 3-D Scanning Pipeline .Acquisition .Alignment 
 .Merging  several_scans.jpg secondpage.pdf             0007CBE3 Q Server      C01E64AA: 
Iterative Closest Points [Besl92] .To fit two meshes, need correspondence between points .Assume points 
correspond to closestpoints on other mesh .Compute best fit on a subset of all points   icp.jpg  
 secondpage.pdf             0007CBE3 Q Server      C01E64AA: Iterative Closest Points 
[Besl92] .To fit two meshes, need correspondence between points .Assume points correspond to closestpoints 
on other mesh .Compute best fit on a subset of all points  .If starting point was good, result should 
be better  icp.jpg  secondpage.pdf             0007CBE3 Q Server      C01E64AA: 
Iterative Closest Points [Besl92] .To fit two meshes, need correspondence between points .Assume points 
correspond to closestpoints on other mesh .Compute best fit on a subset of all points  .If starting 
point was good, result should be better .Iterate until fit converges to minimum error   icp.jpg secondpage.pdf 
            0007CBE3 Q Server      C01E64AA: Range Scanning: Calibration Error S:\scanner-david.jpg 
[Levoy00] Courtesy Paul Debevec secondpage.pdf             0007CBE3 Q Server      
C01E64AA: Range Scanning: Calibration Error S:\scanner-david.jpg S:\david_warp_montage_rigid.jpg S:\key_horizontal.png 
[Levoy00] Courtesy Paul Debevec 2 mm 0 mm secondpage.pdf             0007CBE3 Q Server 
     C01E64AA: Range Scanning: Calibration Error S:\scanner-david.jpg S:\david_warp_montage_rigid.jpg 
S:\key_horizontal.png [Levoy00] Courtesy Paul Debevec 2 mm 0 mm drift.png Mechanical Distortion secondpage.pdf 
            0007CBE3 Q Server      C01E64AA: Range Scanning: Calibration Error S:\scanner-david.jpg 
S:\david_warp_montage_rigid.jpg S:\key_horizontal.png [Levoy00] Courtesy Paul Debevec 2 mm 0 mm drift.png 
Mechanical Distortion Sensor precision is increasing faster than mechanical/optical precision secondpage.pdf 
            0007CBE3 Q Server      C01E64AA: Goal: Multi-Scan Non-Rigid Alignment We 
desire an algorithm that will: .Prevent artifacts in merging  hair1_closeup_rigid.jpg secondpage.pdf 
            0007CBE3 Q Server      C01E64AA: Goal: Multi-Scan Non-Rigid Alignment We 
desire an algorithm that will: .Prevent artifacts in merging .Distribute error evenly  hair1_closeup_rigid.jpg 
hair1_closeup_rigid.jpg secondpage.pdf             0007CBE3 Q Server      C01E64AA: 
Goal: Multi-Scan Non-Rigid Alignment We desire an algorithm that will: .Prevent artifacts in merging 
 .Distribute error evenly .Preserve detail without introducing new warp  hair1_closeup_rigid.jpg hair1_closeup_rigid.jpg 
hair1_closeup_rigid.jpg secondpage.pdf             0007CBE3 Q Server      C01E64AA: 
Goal: Multi-Scan Non-Rigid Alignment We desire an algorithm that will: .Prevent artifacts in merging 
 .Distribute error evenly .Preserve detail without introducing new warp .Be practical, efficient, and 
parallelizable for large datasets  hair1_closeup_rigid.jpg hair1_closeup_rigid.jpg hair1_closeup_rigid.jpg 
David's head comprises 1400 scans and 230 million vertices secondpage.pdf             0007CBE3 
Q Server      C01E64AA: Global Alignment Pipeline pipeline.pdf.emf Pairwise Correspondences  secondpage.pdf 
            0007CBE3 Q Server      C01E64AA: Global Alignment Pipeline pipeline.pdf.emf 
Pairwise Correspondences  secondpage.pdf             0007CBE3 Q Server      C01E64AA: 
Global Alignment Pipeline pipeline.pdf.emf Pairwise Correspondences  secondpage.pdf        
     0007CBE3 Q Server      C01E64AA: Global Alignment Pipeline pipeline.pdf.emf Pairwise Correspondences 
Global Feature Positioning  secondpage.pdf             0007CBE3 Q Server      C01E64AA: 
Global Alignment Pipeline pipeline.pdf.emf Pairwise Correspondences Global Feature Positioning  secondpage.pdf 
            0007CBE3 Q Server      C01E64AA: Global Alignment Pipeline pipeline.pdf.emf 
Pairwise Correspondences Global Feature Positioning Optimize Global Positions  secondpage.pdf   
          0007CBE3 Q Server      C01E64AA: Global Alignment Pipeline pipeline.pdf.emf 
Pairwise Correspondences Global Feature Positioning Warp Scans Optimize Global Positions secondpage.pdf 
            0007CBE3 Q Server      C01E64AA: Results: David's Head .1400 range scans 
 .230 million points  Correspondences .78 hours CPU time .1.5 hours wall time  Positioning and Alignment 
.30 minutes CPU time  head_with_points.png  secondpage.pdf             0007CBE3 Q Server 
      C01E64AA: Results: David's Head head_with_points.png  S:\eye_img.jpg S:\pupil_closup_rigid.jpg 
S:\pupil_closup_rigid.jpg Rigid Non-Rigid  secondpage.pdf             0007CBE3 Q Server 
      C01E64AA: Results: David's Head head_with_points.png  S:\pupil_closup_rigid.jpg S:\pupil_closup_rigid.jpg 
Rigid Non-Rigid S:\pupil_closup_rigid.jpg S:\pupil_closup_rigid.jpg S:\pupil_closup_rigid.jpg secondpage.pdf 
            0007CBE3 Q Server      C01E64AA: Results: David's Head head_with_points.png 
 S:\pupil_closup_rigid.jpg S:\pupil_closup_rigid.jpg Rigid Non-Rigid S:\pupil_closup_rigid.jpg S:\pupil_closup_rigid.jpg 
 S:\pupil_closup_rigid.jpg S:\pupil_closup_rigid.jpg secondpage.pdf             0007CBE3 
Q Server      C01E64AA: Results: Awakening head_with_points.png Non-Rigid S:\pupil_closup_rigid.jpg 
1836 scans, 390 million verticesCorrespondences: 51.5 CPU hoursAlignment: 1 CPU hour secondpage.pdf 
            0007CBE3 Q Server      C01E64AA: Results: Awakening head_with_points.png 
Rigid S:\pupil_closup_rigid.jpg 1836 scans, 390 million vertices Correspondences: 51.5 CPU hours Alignment: 
1 CPU hour  secondpage.pdf             0007CBE3 Q Server      C01E64AA: Results: Forma 
Urbis Romae #033 head_with_points.png Rigid 140 scans, 71 million vertices; Correspondences: 48 hours; 
Alignment: 27 minutes secondpage.pdf             0007CBE3 Q Server      C01E64AA: 
Results: Forma Urbis Romae #033 head_with_points.png Rigid 140 scans, 71 million vertices; Correspondences: 
48 hours; Alignment: 27 minutes  secondpage.pdf             0007CBE3 Q Server      
 C01E64AA: Results: Forma Urbis Romae #033 head_with_points.png Non-Rigid 140 scans, 71 million vertices; 
Correspondences: 48 hours; Alignment: 27 minutes  secondpage.pdf             0007CBE3 Q 
Server      C01E64AA: Results: Forma Urbis Romae #033 head_with_points.png Rigid 140 scans, 71 
million vertices; Correspondences: 48 hours; Alignment: 27 minutes  secondpage.pdf         
    0007CBE3 Q Server      C01E64AA: Results: Forma Urbis Romae #033 head_with_points.png Non-Rigid 
140 scans, 71 million vertices; Correspondences: 48 hours; Alignment: 27 minutes   Our proposed reconstruction 
sequence .Evaluate the artifacts under study .Plan your field capture .Perform multi-resolution capture 
 .Range alignment &#38; merging .Surface simplification and normal extraction .Normal and color inpainting 
 .Synthetic reconstruction and rendering   Evaluate the artifacts .What needs to be documented? Geometry 
 Color Reflection models Lighting Sound    Plan field acquisition .For heritage, inexpensiveand 
flexible techniqueswork best .Dozens of options forrange capture exist: Optical triangulation Time 
of flight Stereo imaging Image-based modeling Photometric stereo    Photometric stereo for heritage 
capture  Consider a pixel         Field work notes .Plan for extreme fieldconditions .Consider 
equipmentvibration / movement .Manage data capture .Field backups aresometimes difficult,but not optional 
   freize align.jpg  Range data alignment .Initial local alignment via ICP (Levoy, Curless 1998) 
.Global alignment (Brown 2006)  chac_normal_points.jpg Inspecting global alignment coherence with a 
clipping plane  Isosurface extraction .Merge points into mesh Poisson Reconstruction (Kazhdan, Bolitho, 
Hoppe 2006)   freize normals.jpg  Archaeological intervention and review .Minimize hole filling in 
general Compute fill geometry using tangency along hole boundary Volumetric diffusion (Davis, Marschner, 
Garr, Levoy 2002)   freize fill.jpg  Mesh simplification .Adaptive decimation(below, left) Preserve 
triangle count for areas with high local curvature Aggressively decimate comparatively flat regions 
 .Mesh culling using sensor reprojection(below, right) Data collected at oblique angles is generally 
less usable   freize_clean.jpg chac_geometry.jpg  Transfer surface normals to proxy geometry freize 
a.jpg .Surface restoration is often simpler as a normal painting exercise for meshes with high complexity 
   Base mesh + extracted normals freize b.jpg  Base mesh + normal map inpainting freize c.jpg  Texture 
reconstruction screenshot11.jpg ball court render 2.jpg ball court render 1.jpg  Designing for multi-resolution 
models .Plan to insert detailed models within a simplified scaffold Pay penalties for large models 
only where needed  .Where possible, extract normals from complex geometry  ball court placement.jpg 
ball court render 2.jpg  Low-frequency data, embroidered withhigh-frequency data for detail regions 
osr_struc_top_Render01_jn.jpg osr_struc_top_Render06_jn.jpg  Texture reprojection chac_col02.jpg chac_geometry.jpg 
chac_Norm08_L.jpg chac_uv.jpg .Photo reprojectionis useful whenrepresenting the current state of an artifact 
 .Generate texture atlas for a simplified mesh .Recover camera position for an input set of images 
.Project color from the input set onto the mesh   chac slate.jpg chac_turntable.jpg  Artifact Reconstruction 
chac_restored.jpg stat_chac_renderTEST_BG.png  Lessons learned .Archaeological review iterations take 
time .Comparing range data to existing documentation can be surprising   Frequent pitfalls and deficiencies 
in the use of3D acquisition techniques for CH Paolo Cignoni  What can go wrong Communication CH institutions 
speak a different language Easy to create wrong expectation   Misuse and Pitfalls of the Technicians 
Wrong choice and use of the hardware   Misuse and Pitfalls of the Technicians Wrong data post-processing. 
  Reasons Recap most errors caused by non deep knowledge of the specific CH field Gross ignorance 
 You miss to exploit all the potentialities due to the fact that you ignore something What is the important 
stuff of this object  Lack of respect Data    Misuse and Pitfalls of the Non technicians Expectations 
 Modeled vsacquired mishunderstatement Limits of the technologies Presentation of results   Misuse 
and Pitfalls of the Non technicians Evaluating and assessing   Misuse and Pitfalls of the non technicians 
Presenting and preserving Long term preservation of data Formats, applications? Standards  How to 
present acquired data Nice movies vs true data Both of them is not impossible    Misuse and Pitfalls 
of the Non technicians Provenance   References From CH only London Charter Anastylosis Venice Charter 
   Technical approaches and questions fromthe CH field Benedict Brown  titlepagelr.pdf      
       0007CBE3Q Server      C01E64AA: Planning a Collaboration Benedict J. BrownKatholiekeUniversiteitLeuven 
 secondpage.pdf             0007CBE3Q Server      C01E64AA: Discuss, Discuss, Discuss 
.Learn about existing processes .Newapproach or improve current one? .What can we mimic? .Understand 
respective goals  thera665.jpg thera482.jpg C:\Documents and Settings\tweyrich\Desktop\acquisition-talk\images\scanner_w_fragment.jpg 
C:\Documents and Settings\tweyrich\Desktop\acquisition-talk\images\scanner_w_fragment.jpg AkrotiriExcavations 
Kerkrade Excavations AkrotiriExcavations secondpage.pdf             0007CBE3Q Server  
    C01E64AA: Technical Questions: Who is the user? .Skilled computer experts   Stanford University 
 secondpage.pdf             0007CBE3Q Server      C01E64AA: Technical Questions: Who 
is the user? .Skilled computer experts .Everybody else  C:\Documents and Settings\tweyrich\Desktop\acquisition-talk\images\scanner_w_fragment.jpg 
C:\DOCUME~1\bjbrown\LOCALS~1\Temp\VMwareDnD\dafa0a4e\theranext.png C:\DOCUME~1\bjbrown\LOCALS~1\Temp\VMwareDnD\169c1de9\awakening_face.jpg 
secondpage.pdf             0007CBE3Q Server      C01E64AA: Technical Questions: Required 
Data Quality .Higher quality .higher price .High resolution 3D data is hard to work with .Analysis 
requires high quality .Visualization can cheat   Michelangelo's Awakening [Brown07] Mayan Glyphs at 
ChichénItzá[Glencross08] secondpage.pdf             0007CBE3Q Server      C01E64AA: 
Practical Questions .Project time frames? .Technical vs. "cultural" success .What is "important"? 
.Funding .Equipment .Travel .Salaries  .Publicity   An honest review and look to the future Kevin 
Cain, Paolo Cignoni  Two Sides of Reconstruction: Graphics .For many working in graphics, reconstruction 
is a legitimate thing to do .Reconstruction for objects or sites as they once appeared is a commonplace 
request for graphics work This is a legitimate request from a scientific point of view The typical 
work process is: thesis / antithesis / synthesis Each step in the process is iterative, converging on 
a plausible solution    Evolution of a visual reconstruction  Italica, Spain. Roman period.   
 Two Sides of Reconstruction: Heritage .For many working in 'heritage', reconstruction is not a legitimate 
thing to do .A common objection is the paradox:how do you reconstruct what isn t there?    The current 
trend for employing three-dimensional computer graphics to represent archaeological sites is limited 
because their level of realism cannot be guaranteed.The images that are generated may look realistic, 
but often no attempt had been made to validate their accuracy. In order for the archaeologist to benefit 
from computer-generated representations and use them in a meaningful way, virtual past environments must 
be more than pretty pictures they must accurately simulate all the physical evidence for the site being 
modeled. Kate Devlin, Alan Chalmers, Duncan Brown Predictive lighting and perception inarchaeological 
representations UNESCO "World Heritage in the Digital Age"30th Anniversary Digital Congress. October 
2002  Another reconstruction case                   Temple of Apollo, Delphi, Greece 
  Prospective reconstruction Drettakis, Martinez, 2004  Archaeological Poetry v. Archaeological Science 
.What is multi-disciplinary practice? .How can expectations be calibrated? .What collisions await the 
uninformed?   Questions?  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1667250</article_id>
		<sort_key>110</sort_key>
		<display_label>Article No.</display_label>
		<display_no>11</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Creating new interfaces for musical expression]]></title>
		<subtitle><![CDATA[introduction to NIME]]></subtitle>
		<page_from>1</page_from>
		<page_to>158</page_to>
		<doi_number>10.1145/1667239.1667250</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1667250</url>
		<abstract>
			<par><![CDATA[<p>Advances in digital audio technologies have led to computers playing a role in most music production and performance. Digital technologies offer unprecedented opportunities for creation and manipulation of sound, but the flexibility of these new technologies imply an often-confusing array of choices for instrument designers, composers, and performers.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Audio input/output</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Interaction styles (e.g., commands, menus, forms, direct manipulation)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Performing arts (e.g., dance, music)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317.10003371.10003386.10003389</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval->Specialized information retrieval->Multimedia and multimodal retrieval->Speech / audio search</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010870</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Natural language interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10003247</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Signal processing systems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1797133</person_id>
				<author_profile_id><![CDATA[81327488395]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sidney]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fels]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797134</person_id>
				<author_profile_id><![CDATA[81100493577]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lyons]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ritsumeikan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{Wanderley&amp;Battier, 2000} Wanderley, M. and Battier, M. 2000. Trends in gestural control of music, IRCAM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1201683</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{Miranda&amp;Wanderley, 2006} Miranda, E. R. and Wanderley, M. M. 2006. New digital musical instruments: control and interaction beyond the keyboard. AR Editions.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1245198</ref_obj_id>
				<ref_obj_pid>1245194</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{Wessel&amp;Wright, 2002} Problems and prospects for intimate musical control of computers, Computer Music Journal 26(3): 11--22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085156</ref_obj_id>
				<ref_obj_pid>1085152</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{Wessel&amp;Wright, 2001} Problems and prospects for intimate musical control of computers, In Proceedings of the 2001 Conference on New interfaces For Musical Expression (NIME-01) (Seattle, Washington, April 01 -- 02, 2001).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{Vertegaal et al. 1996} Vertegaal, R. and Ungvary, T. and Kieslinger, M. 1996. Towards a musician's cockpit: Transducers, feedback and musical function, Proceedings of the International Computer Music Conference, pp. 308--311.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{Mulder, 2000} Mulder, A. 2000. Towards a choice of gestural constraints for instrumental performers. In: Trends in gestural control of music, Wanderley, M. and Battier, M., eds., IRCAM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1116035</ref_obj_id>
				<ref_obj_pid>1116026</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{Camurri, 2000} Camurri, A. and Hashimoto, S. and Ricchetti, M. and Ricci, A. and Suzuki, K. and Trocca, R. and Volpe, G. 2000. EyesWeb: Toward gesture and affect recognition in interactive dance and music systems, Computer Music Journal 24(1): 57--69.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{Waiswicz, 1985} Waiswicz, M. 1985. THE HANDS: A Set of Remote MIDI Controllers. Proceedings of the 1985 International Computer Music Conference (ICMC'85) Vancouver Canada, pp 313 -- 318, San Francisco: ICMA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{Marshall, 2009} Marshall, M. T. 2009. Physical Interface Design for Digital Musical Instruments, Ph.D. thesis, McGill University, March. http://www.idmil.org/_media/publications/marshall_phdthesis_final.pdf]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>47678</ref_obj_id>
				<ref_obj_pid>47676</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[{Moore, 1988} Moore, F. R. 1988. The dysfunctions of MIDI, Computer Music Journal 12(1): 19--28.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[{Burzik, 2002} Burzik, A. 2002. Practising in Flow---The Secret of the Masters, Stringendo 24(2): 18--21.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>972351</ref_obj_id>
				<ref_obj_pid>972348</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[{Hunt&amp;Wanderley, 2002} Hunt, A. and Wanderley, M. M. 2002. Mapping performer parameters to synthesis engines. Organised Sound 7: 97--108.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[{Hunt, Wanderley,&amp;Paradis, 2003} Hunt, A. and Wanderley, M. M. and Paradis, M. 2003. The importance of parameter mapping in electronic instrument design, Journal of New Music Research 32(4): 429 -- 440.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[{Rovan et al., 1997} Rovan, J. B., Wanderley, M. M. Dubnov, S. and Depalle, P. 1997. Instrumental Gestural Mapping Strategies as Expressivity Determinants in Computer Music Performance. In: Kansei, The Technology of Emotion. Proceedings of the AIMI International Workshop, A. Camurri, (ed.), Genoa: Associazione di Informatica Musicale Italiana, October 3--4, pp. 68--73.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>972353</ref_obj_id>
				<ref_obj_pid>972348</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[{Arfib et al., 2002} D. Arfib, J. M. Couturier, L. Kessous and V. Verfaille 2002. Strategies of mapping between gesture data and synthesis model parameters using perceptual spaces. Organised Sound 7: 127--144.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>972350</ref_obj_id>
				<ref_obj_pid>972348</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[{Goudeseune, 2002} Goudeseune, C. 2002. Interpolated mappings for musical instruments. Organised Sound 7: 85--96.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[{Wessel, 1979} Wessel, D. 1979. Timbre space as a musical control structure, Computer music journal 3(2): 45 -- 52.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[{Vertegaal&amp;Eaglestone, 1996} R. Vertegaal and B. Eaglestone 1996. Comparison of input devices in an ISEE direct timbre manipulation task, Interacting with Computers, 8(1): 13 -- 30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085742</ref_obj_id>
				<ref_obj_pid>1085714</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[{Lyons et al., 2003 Lyons, M. J. and Haehnel, M. and Tetsutani, N. 2003}. Designing, playing, and performing with a vision-based mouth interface, Proceedings of the 2003 conference on New interfaces for musical expression pp. 116--121.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1142241</ref_obj_id>
				<ref_obj_pid>1142215</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[{Steiner, 2006} H.-C. Steiner. Towards a catalog and software library of mapping methods. Proceedings of the 2006 conference on New interfaces for musical expression, pp. 106--109, Paris, France, 2006. IRCAM Centre Pompidou.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[{Cook, 2004} Cook, P. R. 2004. Remutualizing the musical instrument: Co-design of synthesis algorithms and controllers. Journal of New Music Research, 33(3):315--320.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085885</ref_obj_id>
				<ref_obj_pid>1085884</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[{Fels et al. 2004} Sidney S. Fels and Linda Kaastra and Sachiyo Takahashi and Graeme McCaig. Evolving Tooka: from Experiment to Instrument. 4rd International Conference on New Interfaces for Musical Expression (NIME04). Pages 1--6. May. 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[{Wright and Freed, 1997} Wright, M. and A. Freed 1997. Open Sound Control: A New Protocol for Communicating with Sound Synthesizers. Proceedings of the International Computer Music Conference, Thessaloniki, Hellas, pp. 101--104.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>76932</ref_obj_id>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[{Mathews&amp;Pierce, 1989} Mathews, M.&amp;Pierce, J. (1989). <i>Current Directions in Computer Music Research.</i> The MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>265600</ref_obj_id>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[{Steiglitz, 1996} Steiglitz, K. 1996 Digital Signal Processing Primer, New York, Addison Wesley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>229865</ref_obj_id>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[{Roads, 1976} Roads, C. 1976, The Computer Music Tutorial, Cambridge, MIT Press]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[{McAulay&amp;Quatieri, 1986} McAulay, R. and T. Quatieri. 1986. "Speech Analysis/Synthesis Based on a Sinusoidal Representation." IEEE Trans. Acoust. Speech and Sig. Proc. ASSP-34(4): pp. 744--754.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[{Smith and Serra, 1987} Smith, J. and Serra, X. 1987. "PARSHL: Analysis/Synthesis Program for Non-Harmonic Sounds Based on a Sinusoidal Representation." Proc. International Computer Music Conference, Urbana, pp. 290 -- 297.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[{Dudley, 1939} Dudley, H. 1939, "The Vocoder," Bell Laboratories Record, December.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[{Moorer, 1978} Moorer, A. 1978. "The Use of the Phase Vocoder in Computer Music Applications." Journal of the Audio Engineering Society, 26 (1/2), pp. 42--45.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[{Moorer, 1979} Moorer, A. 1979, "The Use of Linear Prediction of Speech in Computer Music Applications," Journal of the Audio Engineering Society 27(3):134--140.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[{Dolson, 1986} Dolson, M. 1986, "The Phase Vocoder: A Tutorial," Computer Music Journal, 10 (4), pp. 14--27.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[{Makhoul, 1975} Makhoul, J. 1975. "Linear Prediction: A Tutorial Review," Proc. of the IEEE, v 63., pp. 561--580.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[{Chowning, 1973} Chowning, J. 1973, "The Synthesis of Complex Audio Spectra by Means of Frequency Modulation," Journal of the Audio Engineering Society 21(7): pp. 526--534.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[{LeBrun, 1979} LeBrun, M. 1979. "Digital Waveshaping Synthesis," Journal of the Audio Engineering Society, "27(4): 250--266.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[{Adrien, 1988} Adrien, J. 1988. Etude de Structures Complexes Vibrantes, Application-la Synth&#232;se par Modeles Physiques, Doctoral Dissertation. Paris: Universit&#233; Paris VI.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>77671</ref_obj_id>
				<ref_obj_pid>76932</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Wawrzynek, J. 1989. "VLSI Models for Sound Synthesis," in Current Directions in Computer Music Research, M. Mathews and J. Pierce Eds., Cambridge, MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[{Larouche&amp;Meillier 1994} Larouche, J.&amp;J. Meillier 1994. "Multichannel Excitation/Filter Modeling of Percussive Sounds with Application to the Piano," IEEE Trans. Speech and Audio, pp. 329--344.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[{Smith, 1987} Smith, J. 1987. Musical Applications of Digital Waveguides. Stanford University Center For Computer Research in Music and Acoustics. Report STAN-M-39.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[{Karjalainen et al. 1991} Karjalainen, M. Laine, U., Laakso, T. and V. V&#228;lim&#228;ki, 1991. "Transmission Line Modeling and Real-Time Synthesis of String and Wind Instruments," Proc. International Computer Music Conference, Montreal, pp. 293--296]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[{Cook, 1991} Cook, P. 1991. "TBone: An Interactive Waveguide Brass Instrument Synthesis Workbench for the NeXT Machine," Proc. International Computer Music Conference, Montreal, pp. 297--299.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[{Cook, 1991b} Cook, P. 1991b. "LECTOR: An Ecclesiastical Latin Control Language for the SPASM/singer Instrument," Proc. International Computer Music Conference, Montreal, pp. 319--321.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[{Cook, 1992} Cook, P. 1992. "A Meta-Wind-Instrument Physical Model, and a MetaController for Real-Time Performance Control," Proc. International Computer Music Conference, San Jose, pp. 273--276.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[{Cook, 1992b} Cook, P. 1992b. "SPASM: a Real-Time Vocal Tract Physical Model Editor/Controller and Singer: the Companion Software Synthesis System," Computer Music Journal, 17: 1, pp 30--44.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[{McIntyre et al. 1983} McIntyre, M., Schumacher, R. and J. Woodhouse 1983, "On the Oscillations of Musical Instruments," Journal of the Acoustical Society of America, 74(5), pp. 1325--1345.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>131155</ref_obj_id>
				<ref_obj_pid>131150</ref_obj_pid>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[{Roads, 1991} Roads, C. (1991). Asynchronous Granular Synthesis, In G. De Poli, A. Piccialli,&amp;C. Roads (Eds.), Representations of Musical Signals, pp. 143--185. Cambridge: MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[{Gabor, 1947} Gabor, D. (1947). Acoustical Quanta And The Theory Of Hearing. Nature, 159(4044), 591--594.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[{Xenakis, 1971} Iannis Xenakis, Formalized Music: Thought and Mathematics in Composition. Bloomington and London: Indiana University Press, 1971.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[{Truax, 1988} Truax, B. (1988) Real-time granular synthesis with a digital signal processor. Computer Music Journal, 12(2), 14--26.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[{Verplank, 2000} B. Verplank, M. Mathews, R. Shaw, "Scanned Synthesis", "Proceedings of the 2000 International Computer Music Conference", p: 368--371, Berlin, Zannos editor, ICMA, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[STK from Perry Cook's page: http://www.cs.princeton.edu/~prc/NewWork.html#STK Perry R. Cook and Gary P. Scavone, The Synthesis ToolKit (STK), Proc of the ICMC, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[G. Scavone and P. Cook, "RtMIDI, RtAudio, and a Synthesis (STK) Update," Proceedings of the International Computer Music Conference, Barcelona, September, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[Kees van den Doel and Dinesh K. Pai, JASS: A Java Audio Synthesis System for Programmers, Proceedings of the International Conference on Auditory Display, pp. 150--154, 2001, Helsinki.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085915</ref_obj_id>
				<ref_obj_pid>1085884</ref_obj_pid>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[{Wang&amp;Cook, 2004} Wang, G. and Cook, P. R. 2004. On-the-fly programming: using code as an expressive musical instrument. In Proceedings of the 2004 Conference on New interfaces For Musical Expression (Hamamatsu, Shizuoka, Japan, June 03 -- 05, 2004), 138--143.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[{Kimura, 2004} Mari Kimura 2004, Performance at NIME-04, Hamamatsu, Japan.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[{Wynnychuk, 2004} Jordan Wynnychuk 2004, Performance at NIME-04, Hamamatsu, Japan.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085897</ref_obj_id>
				<ref_obj_pid>1085884</ref_obj_pid>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[{D'Arcangelo, 2004} D'Arcangelo, G. 2004, Recycling music, answering back: toward an oral tradition of electronic music, Proceedings of the 2004 conference on New interfaces for musical expression, pp. 55--58.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[{Okamoto, 2004} Hisashi Okamoto, 2004, Performance at NIME-04, Hamamatsu, Japan.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085206</ref_obj_id>
				<ref_obj_pid>1085171</ref_obj_pid>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[{Gadd&amp;Fels, 2002} Gadd, A. and Fels, S. 2002. MetaMuse: metaphors for expressive instruments, Proceedings of the 2002 conference on New interfaces for musical expression, pp. 1--6.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>972352</ref_obj_id>
				<ref_obj_pid>972348</ref_obj_pid>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[{Fels et al., 2003} Fels, S. and Gadd, A. and Mulder, A. 2003. Mapping transparency through metaphor: towards more expressive musical instruments, Organised Sound 7(2): 109--126.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085730</ref_obj_id>
				<ref_obj_pid>1085714</ref_obj_pid>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[{Jorda, 2003} Jorda, S. 2003. Sonigraphical instruments: from FMOL to the reacTable, Proceedings of the 2003 conference on New interfaces for musical expression, pp. 70--76.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[{Jorda et al. 2005} Jorda, S. and Kaltenbrunner, M. and Geiger, G. and Bencina, R. 2005. The reactable*, Proceedings of the International Computer Music Conference (ICMC 2005) pp. 579 -- 582.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[{Mulder&amp;Fels, 1998} Axel G. E. Mulder and S. Sidney Fels (1998). Sound Sculpting: Performing with Virtual Musical Instruments. Proceedings of the Fifth Brazilian Symposium on Computer Music (Belo Horizonte, Minas Gerais, Brazil, 3--5 August 1998, during the 18th Annual Congress of the Brazilian Computer Society, G. Ramalho (ed.)), pp. 151--164.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[{Horio, 2004} Kanta Horio 2004. Performance at NIME-04, Hamamatsu, Japan.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[{Fujii, 2004} Uriko Fujii 2004. Performance at NIME-04, Hamamatsu, Japan.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>972352</ref_obj_id>
				<ref_obj_pid>972348</ref_obj_pid>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[{Fels et al. 2002} Fels, Sidney, Gadd, Ashley, Mulder, Axel (2002). Mapping Transparency through Metaphor: Towards more expressive musical instruments, Organised Sound: Vol. 7, no. 2. Cambridge: Cambridge University Press: 109--126.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[{Mathews&amp;Schloss, 1989} Mathews, M. V. and A. Schloss The Radio Drum as a Synthesizer Controller, Proc. of the 1989 ICMC.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[{Boei et al. 1989} Boie, R. A. et al. Gesture Sensing via Capacitive Moments. Work Project No. 311401-(2099,2399) AT&T Bell Laboratories, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[{Young, 1991} Young, G. (1991), <i>The Sackbut Blues</i>: University of Toronto Press in Canada, and the University of Chicago Press in the USA. Issued also in French under the title: "Blues pour saqueboute: Hugh Le Caine, pionnier de la musique &#233;lectronique." For further information phone +1/613 991-2983. The National Museum of Science and Technology houses an extensive collection of Le Caine's instruments.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085941</ref_obj_id>
				<ref_obj_pid>1085939</ref_obj_pid>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[{Buchla, 2005} Don Buchla. A History of Buchla's Musical Instruments., Proc. of NIME2005, Vancouver, BC, http://www.nime.org/2005/proc/nime2005_001.pdf]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[Paradiso, J., and Gershenfeld, N. Musical Applications of Electric Field Sensing. In Computer Music Journal 21(2) Summer, pp. 69--89. 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085764</ref_obj_id>
				<ref_obj_pid>1085714</ref_obj_pid>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[{Palacio-Quintin, 2003} Palacio-Quintin, Cl&#233;o. The Hyper-Flute. In Proceedings of the New Interfaces for Musical Expression (NIME) Conference. Montreal, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085188</ref_obj_id>
				<ref_obj_pid>1085171</ref_obj_pid>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[{Young, 2001} Young, D., The hyperbowcontroller: Real-time Dynamics measurement of violin performance. In Proc. NIME, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[{Marshall, 2009} Marshall, M., Physical Interface Design for DigitalMusical Instruments. Ph.D. thesis, McGill University, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1588259</ref_obj_id>
				<ref_obj_pid>1586646</ref_obj_pid>
				<ref_seq_no>75</ref_seq_no>
				<ref_text><![CDATA[{Fraser et al. 2008} Helene Fraser and Sidney Fels and Robert Pritchard. Walk the Walk, Talk the Talk. 12th IEEE 2008 International Symposium on Wearable Computing (ISWC2008). Pages 117--118. 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1142282</ref_obj_id>
				<ref_obj_pid>1142215</ref_obj_pid>
				<ref_seq_no>76</ref_seq_no>
				<ref_text><![CDATA[{Pritchard&amp;Fels, 2006} Pritchard, B. and Fels, S. 2006. GRASSP: gesturally-realized audio, speech and song performance. In Proceedings of the 2006 Conference on New interfaces For Musical Expression (Paris, France, June 04 - 08, 2006). New Interfaces For Musical Expression. IRCAM --- Centre Pompidou, Paris, France, 272--276.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2326218</ref_obj_id>
				<ref_obj_pid>2325759</ref_obj_pid>
				<ref_seq_no>77</ref_seq_no>
				<ref_text><![CDATA[{Fels&amp;Hinton, 2998} Sidney S. Fels and Geoffrey E. Hinton. Glove-TalkII: A neural network interface which maps gestures to parallel formant speech synthesizer controls. IEEE Transactions on Neural Networks. Volume 9. No. 1. Pages 205--212. 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085972</ref_obj_id>
				<ref_obj_pid>1085939</ref_obj_pid>
				<ref_seq_no>78</ref_seq_no>
				<ref_text><![CDATA[{Levin&amp;Lieberman, 2005} Levin, G. and Lieberman, Z. "Sounds from Shapes: Audiovisual Performance with Hand Silhouette Contours in "The Manual Input Sessions". Proceedings of NIME '05, Vancouver, BC, Canada. May 26--28, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>79</ref_seq_no>
				<ref_text><![CDATA[{Mulder&amp;Fels, 1998} Axel G. E. Mulder and S. Sidney Fels (1998). Sound Sculpting: Performing with Virtual Musical Instruments. Proceedings of the Fifth Brazilian Symposium on Computer Music (Belo Horizonte, Minas Gerais, Brazil, 3-5 August 1998, during the 18th Annual Congres of the Brazilian Computer Society, G. Ramalho (ed.)), pp. 151--164.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085173</ref_obj_id>
				<ref_obj_pid>1085171</ref_obj_pid>
				<ref_seq_no>80</ref_seq_no>
				<ref_text><![CDATA[{Huott, 2002} Huott, R. 2002. An interface for precise musical control. In Proceedings of the 2002 Conference on New interfaces For Musical Expression (Dublin, Ireland, May 24 - 26, 2002). E. Brazil, Ed., 1--5. http://www.nime.org/2002/proceedings/paper/huott.pdf]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085901</ref_obj_id>
				<ref_obj_pid>1085884</ref_obj_pid>
				<ref_seq_no>81</ref_seq_no>
				<ref_text><![CDATA[{O'Modhrain&amp;Essl, 2004} O'Modhrain, S. and Essl, G. 2004. PebbleBox and CrumbleBag: tactile interfaces for granular synthesis. In Proceedings of the 2004 Conference on New interfaces For Musical Expression (Hamamatsu, Shizuoka, Japan, June 03 - 05, 2004). M. J. Lyons, Ed., 74--79.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085165</ref_obj_id>
				<ref_obj_pid>1085152</ref_obj_pid>
				<ref_seq_no>82</ref_seq_no>
				<ref_text><![CDATA[{Overholt, 2001} Overholt, D. 2001. The MATRIX: a novel controller for musical expression. In Proceedings of the 2001 Conference on New interfaces For Musical Expression (Seattle, Washington, April 01 - 02, 2001).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>83</ref_seq_no>
				<ref_text><![CDATA[{Wang, 2009} Wang, G. 2009. Designing Smule's iPhone Ocarina, In Proceedings of the 2009 Conference on New interfaces For Musical Expression (Pittsburgh, PA, June 4--6, 2009).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085770</ref_obj_id>
				<ref_obj_pid>1085714</ref_obj_pid>
				<ref_seq_no>84</ref_seq_no>
				<ref_text><![CDATA[{Merrill, 2003} Merrill, D. 2003. Head-tracking for gestural and continuous control of parameterized audio effects. In Proceedings of the 2003 Conference on New interfaces For Musical Expression (Montreal, Quebec, Canada, May 22 -- 24, 2003). 218--219. http://www.music.mcgill.ca/musictech/nime/onlineproceedings/Papers/NIME03_Merill.pdf]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>85</ref_seq_no>
				<ref_text><![CDATA[{Kapur et al. 2004} Kapur, A., Tzanetakis, G.,&amp;P. F. Driessen, "Audio-Based Gesture Extraction on the ESitar Controller," In Proceedings of the Conference on Digital Audio Effects, Naples, Italy, October 5--8, 2004. http://soundlab.cs.princeton.edu/research/controllers/esitar/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085186</ref_obj_id>
				<ref_obj_pid>1085171</ref_obj_pid>
				<ref_seq_no>86</ref_seq_no>
				<ref_text><![CDATA[{Vogt et al. 2002} Vogt, F., McCaig, G., Ali, M. A., and Fels, S. 2002. Tongue 'n' Groove: an ultrasound based music controller. In Proceedings of the 2002 Conference on New interfaces For Musical Expression (Dublin, Ireland, May 24 -- 26, 2002). http://www.nime.org/2002/proceedings/paper/vogt.pdf]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085183</ref_obj_id>
				<ref_obj_pid>1085171</ref_obj_pid>
				<ref_seq_no>87</ref_seq_no>
				<ref_text><![CDATA[{Takaka&amp;Knapp, 2002} Tanaka, A. and Knapp, R. B. 2002. Multimodal interaction in music using the Electromyogram and relative position sensing. In Proceedings of the 2002 Conference on New interfaces For Musical Expression (Dublin, Ireland, May 24 -- 26, 2002. http://www.nime.org/2002/proceedings/paper/tenaka.pdf]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>88</ref_seq_no>
				<ref_text><![CDATA[{Knapp&amp;Lusted, 1990} R. Benjamin Knapp and Hugh Lusted - A Bioelectric Controller for Computer Music Applications, Computer Music Journal, Volume 14 No. 1, New Performance Interfaces 1 - Spring 1990; pg 42--47.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085726</ref_obj_id>
				<ref_obj_pid>1085714</ref_obj_pid>
				<ref_seq_no>89</ref_seq_no>
				<ref_text><![CDATA[{Nagashima, 2003} Nagashima, Y. 2003. Bio-sensing systems and bio-feedback systems for interactive media arts. In Proceedings of the 2003 Conference on New interfaces For Musical Expression (Montreal, Quebec, Canada, May 22 -- 24, 2003), 48--53. http://www.nime.org/2003/onlineproceedings/Papers/NIME03_Nagashima.pdf]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085885</ref_obj_id>
				<ref_obj_pid>1085884</ref_obj_pid>
				<ref_seq_no>90</ref_seq_no>
				<ref_text><![CDATA[{Fels et al. 2004} Sidney S. Fels and Linda Kaastra and Sachiyo Takahashi and Graeme McCaig. Evolving Tooka: from Experiment to Instrument. 4rd International Conference on New Interfaces for Musical Expression (NIME04). Pages 1--6. May. 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085200</ref_obj_id>
				<ref_obj_pid>1085171</ref_obj_pid>
				<ref_seq_no>91</ref_seq_no>
				<ref_text><![CDATA[{Fels&amp;Vogt, 2002} Sidney S. Fels and Florian Vogt. Tooka: Explorations of Two Person Instruments. 2nd International Conference on New Interfaces for Musical Expression (NIME02). Pages 116--121. May. 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1086014</ref_obj_id>
				<ref_obj_pid>1085939</ref_obj_pid>
				<ref_seq_no>92</ref_seq_no>
				<ref_text><![CDATA[{Carlile&amp;Hartmann, 2004} Carlile, J. and Hartmann, B. 2004. OROBORO: a collaborative controller with interpersonal haptic feedback. In Proceedings of the 2005 Conference on New interfaces For Musical Expression (Vancouver, Canada, May 26 -- 28, 2005), 250--251.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085228</ref_obj_id>
				<ref_obj_pid>1085171</ref_obj_pid>
				<ref_seq_no>93</ref_seq_no>
				<ref_text><![CDATA[{Chadabe, 2002} Chadabe, J. 2002. The limitations of mapping as a structural descriptive in electronic instruments. In Proceedings of the 2002 Conference on New interfaces For Musical Expression (Dublin, Ireland, May 24 -- 26, 2002).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>94</ref_seq_no>
				<ref_text><![CDATA[{Blaine&amp;Fels, 2003} Tina Blaine and Sidney S. Fels. Collaborative Musical Experiences for Novices. Journal of New Music Research. Volume 32. No. 4. Pages 411--428. Dec. 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>347705</ref_obj_id>
				<ref_obj_pid>347642</ref_obj_pid>
				<ref_seq_no>95</ref_seq_no>
				<ref_text><![CDATA[{Blaine&amp;Perkis, 2000} Tina Blaine and Tim Perkis: The Jam-O-Drum Interactive Music System: A Study in Interaction Design. Symposium on Designing Interactive Systems 2000: 165--173]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>96</ref_seq_no>
				<ref_text><![CDATA[{Paradiso, 1999} Paradiso, J., The Brain Opera Technology: New Instruments and Gestural Sensors for Musical Interaction and Performance. Journal of New Music Research, 1999. 28(2): p. 130--149.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>97</ref_seq_no>
				<ref_text><![CDATA[Hugh Le Caine: Electronic Sackbut: http://www.sciencetech.technomuses.ca/english/collection/music7.cfm Hugh Le Caine info site: http://www.hughlecaine.com]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>98</ref_seq_no>
				<ref_text><![CDATA[Michel Waisvisz wikipage: http://en.wikipedia.org/wiki/Michel_Waisvisz 2005 talk at CHI2005: http://www.chi2005.org/program/prog_closing.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>99</ref_seq_no>
				<ref_text><![CDATA[Lady's Glove, Laetitia Sonami http://www.sonami.net/lady_glove2.htm Video: http://www.sonami.net/Clips/VideoPerf_clips/China-Loose.mov]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>100</ref_seq_no>
				<ref_text><![CDATA[{Tarabella, 2004} Leonello Tarabella, 2004. Performance at NIME-04, Hamamatsu, Japan.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>101</ref_seq_no>
				<ref_text><![CDATA[{Fels&amp;Mase, 1999} Sidney S. Fels and Kenji Mase. Iamascope: A Graphical Musical Instrument. Computers and Graphics. Volume 2. No. 23. Pages 277--286. 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>102</ref_seq_no>
				<ref_text><![CDATA[{Lyons, 2001} Lyons, M. J. 2001. Mouthersizer Video shown at NIME-01 workshop, Seattle, WA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>634250</ref_obj_id>
				<ref_obj_pid>634067</ref_obj_pid>
				<ref_seq_no>103</ref_seq_no>
				<ref_text><![CDATA[{Lyons&amp;Tetsutani, 2001} Lyons, M. J. and Tetsutani, N. 2001, Facing the music: a facial action controlled musical interface, Conference on Human Factors in Computing Systems, (CHI'2001), Extended Abstracts, pp. 309--310.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085742</ref_obj_id>
				<ref_obj_pid>1085714</ref_obj_pid>
				<ref_seq_no>104</ref_seq_no>
				<ref_text><![CDATA[{Lyons et al., 2003} Lyons, M. J. and Haehnel, M. and Tetsutani, N. 2003. Designing, playing, and performing with a vision-based mouth interface, Proceedings of the 2003 conference on New interfaces for musical expression pp. 116--121.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085974</ref_obj_id>
				<ref_obj_pid>1085939</ref_obj_pid>
				<ref_seq_no>105</ref_seq_no>
				<ref_text><![CDATA[{Funk et al. 2005} Funk, M. and Kuwabara, K. and Lyons, M. J. 2005. Sonification of facial actions for musical expression, Proceedings of the 2005 conference on New interfaces for musical expression, pp. 127--131.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1142256</ref_obj_id>
				<ref_obj_pid>1142215</ref_obj_pid>
				<ref_seq_no>106</ref_seq_no>
				<ref_text><![CDATA[{Nishibori&amp;Iwai, 2006} Nishibori, Y. and Iwai, T., 2006. Tenori-On, Proceedings of the 2006 conference on New interfaces for musical expression, pp. 172 -- 175]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085154</ref_obj_id>
				<ref_obj_pid>1085152</ref_obj_pid>
				<ref_seq_no>107</ref_seq_no>
				<ref_text><![CDATA[{Cook, 2001} Cook, P. 2001. Principles for designing computer music controllers, Proceedings of the 2001 conference on New interfaces for musical expression, Seattle WA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>108</ref_seq_no>
				<ref_text><![CDATA[{Cook, 2007} Cook, P. 2007, Keynote Talk at the 2007 Conference on New Interfaces for Musical Expression, New York, NY.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085898</ref_obj_id>
				<ref_obj_pid>1085884</ref_obj_pid>
				<ref_seq_no>109</ref_seq_no>
				<ref_text><![CDATA[{Jorda, 2004} Jorda, S. 2004. Digital instruments and players: part I---efficiency and apprenticeship, Proceedings of the 2004 conference on New interfaces for musical expression pp. 59 -- 63.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085771</ref_obj_id>
				<ref_obj_pid>1085714</ref_obj_pid>
				<ref_seq_no>110</ref_seq_no>
				<ref_text><![CDATA[{Singer, 2003} Singer, E. 2003. Sonic Banana: A novel bend-sensor-based midi controller, Proceedings of the 2003 conference on New interfaces for musical expression, pp. 220 -- 221.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>111</ref_seq_no>
				<ref_text><![CDATA[Vienna Vegetable Orchestra http://www.gemueseorchester.org]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1406583</ref_obj_id>
				<ref_seq_no>112</ref_seq_no>
				<ref_text><![CDATA[{Igoe, 2007} Igoe, T. 2007. Making Things Talk, O'Reilly.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085155</ref_obj_id>
				<ref_obj_pid>1085152</ref_obj_pid>
				<ref_seq_no>113</ref_seq_no>
				<ref_text><![CDATA[{Verplank, Sapp,&amp;Mathews, 2001} Verplank, B. and Sapp, C. and Mathews, M., A Course on Controllers, Proceedings of the 2001 conference on New interfaces for musical expression, Seattle, WA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085936</ref_obj_id>
				<ref_obj_pid>1085884</ref_obj_pid>
				<ref_seq_no>114</ref_seq_no>
				<ref_text><![CDATA[{Lippit, 2004} Lippit, T. M. 2004. Realtime sampling system for the turntablist version 2: 16padjoystickcontroller, Proceedings of the 2004 conference on New interfaces for musical expression, pp. 211--212.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Brief History of NIME New Interfaces for Musical Expression First organized as a workshop of ACM 
CHI 2001 Experience Music Project - Seattle, April, 2001 Lectures/Discussions/Demos/Performances A Brief 
History of NIME NIME-02 - Media Lab Europe, Dublin in May 2002 Conference-scale event with similar format 
to the NIME-01 workshop held annually since 2001   What is NIME about? The Problem: Digital Technology 
&#38; computers involved in nearly all forms of contemporary music  But the computer is not a Musical 
Instrument  Laptop Battle Tokyo sml! Superdeluxe Roppongi 11/2008  How to Play the Computer?  Computers 
offer a wide range of sound and music creation opportunities  How can we create new interfaces to play 
computers in a way that is appropriate to human brains &#38; bodies?   How to Play the Computer? 
This course is all about new human­ computer interfaces for making music NIME Themes Novel controllers 
&#38; interfaces  Performance &#38; composition with new interfaces  Interfaces for collaborative 
performance  Real-time gestural control of music  Interfaces for musical novices &#38; education  
Cognition in Musical Interface Design  Haptic &#38; force feedback in musical control  Artistic, cultural, 
and social impact  Mapping &#38; Digital Instrument Design  Musical Instrument theory: what makes a 
good instrument?  Visual Musical Interfaces  Haptic &#38; Force feedback  Collaborative Musical Interfaces 
 NIME Themes: A Selection  Module C: Aesthetics of NIME  Technological Expressionism  NIME &#38; 
the Music Process  Challenge of Performance  Mapping &#38; the Audience: Transparency  Visual Feedback 
 Interaction Metaphor  Module D: Case Studies NIMEs of our TIMES  photo: Lydia Kavina Module E: Visual 
Interfaces  Imaginary Piano: visual input only  Iamascope: visual input and output  Facial gesture 
musical interfaces: when vision may be your best option  Reactable: vision + (passive) touch, through 
alignment)  Module F: Education and NIME Perry s Principles: Guidelines for the beginner  NIME Curricula 
 Generic model of a musical interface  Role of feedback from the interface  Mapping problem  Module 
A: NIME Theory NIME, DMI, Instrument  musical interface and nime used interchangeably  DMI Digital 
Musical Instrument  DMI &#38; MI may be preferable because a nime will not be new forever  There are 
some important differences with traditional musical instruments  Musical Interface Research Conceptual 
Framework  Wessel &#38; Wright, NIME -01 NIME Generic Model Gestural Input  Based on: Miranda &#38; 
Wanderley (2006) Gesture  Dynamic human action as opposed to static posture (Mulder, 2000)  Sometimes 
also used to describe movements that communicate meaning  Human actions used to generate sound (Miranda 
&#38; Wanderley, 2006)  No physical contact with device  Classifying Gestures Free gestures Physical 
contact with controller Free Gesture Interface  Theremin (1919)  Sound feedback (F2) only  No primary 
tactile or visual feedback (F1)  Have been few virtuosos  Considered difficult to  master  Léon 
Theremin  Theremin lacks significant primary feedback The Hands  Passive F1 STEIM, Amsterdam (Studio 
for Electro-instrumental Music) Video: NIME-03 Feedback  Sound  F2 Tactile*  Kinesthetic F1 Visual** 
 *Includes vibro-tactile feedback due to sound waves on the instrument ** See Module E on Visual Interfaces 
Primary Feedback  M. Marshall, PhD thesis (McGill, 2009) F1 : Visual &#38; Tactile Feedback  See also 
Module C: Aesthetics Nishiburi &#38; Iwai NIME-06 Non-auditory feedback  http://www.ted.com/index.php/talks/evelyn_glennie_shows_how_to_listen.html 
 Musical Interface  Physical interface, with sensors and actuators  Synthesis system  Mapping system 
which connects parameters in the first two  Traditional Musical Instrument  Utaguchi, fingerholes, 
etc are both as gesture interface and in sound generation  Control interface &#38; Sound production 
are not separate  Mapping is determined by acoustics of the instrument, player, and environment  Shakuhachi 
 Digital Music Instrument  Any sound sample can be used  Control interface and sound production mechanism 
are separate  Arbitrary mapping  Polyphonic Digital Sampling Synth DMI Mapping Problem : How to design 
the gesture to sound mapping? Mapping Gestural Sound Control Synthesis Parameters Parameters Musical 
Control Intimacy ... the match between the variety of musically desirable sounds produced and the psycho­physiological 
capabilities of a practiced per­former. Moore (1988) Control Intimacy depends (somehow) upon gesture 
to sound mapping Traditional Musical Instrument High controllability (and potential for control intimacy) 
 Cost: Difficult to learn  Virtuosity ~10000 hours of practice  10 years of serious practice  Flow 
in musical expression Quality of Control Experience of Mapping Intimacy Flow Special contact with 
the instrument  Development of a subtle feeling for sound  Feeling of effortlessness  Playful &#38; 
Free-spirited feeling handling of the material  A. Burzick (2002) Lack of Intimacy in some Digital Musical 
Interfaces  Latency between gesture and sound  Lack of primary feedback  Poor mapping  Dimensionality 
 Complexity  Mapping Strategy  Other aspects  Aspects of the Mapping Problem  The mapping layer 
can be considered as the essence of a musical interface Hunt, Wanderley, and Paradis (2003) Types of 
Mapping  Simple &#38; Complex Mappings  Simple Complex Hunt, Wanderley, and Paradis (2003) Mapping 
Complexity The mapping can be considered as the essence of a DMI Hunt, Wanderley, and Paradis (2003) 
 Yamaha WX-7 Hunt, Wanderley, &#38; Paradis, NIME-02  Timbre  Quality that differentiates two tones 
having equal pitch and loudness  Spectrum of sound frequencies relative to perceived pitch that determines 
the timbre  Imagine the same note played with the same loudness on two different instruments (piano 
and guitar). The notes differ in timbre.  See Module B: Tools - section on sound synthesis Timbre  
Electronic and sound synthesis has opened an incredible range of timbres to musicians and composers 
 Keyboard remains the most popular synthesizer controller  Controllers have not advanced as much as 
sound synthesis methods  See Module B: Tools - section on sound synthesis Sound Synthesis Control is 
Complex  Synthesis control parameters are usually not intuitive  Many synthesis methods are indeed 
non-intuitive  Would prefer to have a control interface which is based on the perceptual qualities 
of timbre spaces  See Module B: Tools - section on sound synthesis Handling Complexity: Three Layer 
Mapping Strategy  Other Mapping Strategies  Interaction Metaphors (see Module C: Aesthetics)  Neural 
Network  Genetic Algorithm  Cognitive Factors based  Interaction Design Problem large scope for a 
variety of approaches Cognitive Factors Based Mouthesizer interface (Module E: Visual Interfaces) Controlling 
a Formant Filter using Mouth Shape [ o ] [ a ] [ i ] Lyons et al., NIME-03 5/16/09 &#38;#169;2009, S. 
Fels and M. Lyons 31 Summary  Generic musical interface model is helpful in understanding what makes 
&#38; breaks a nime  Mapping constitutes the essence of a digital musical instrument  Mapping is not 
straightforward and many design strategies have been tried  Complex mappings can be better than simple 
one­to-one mappings  Studies of mapping and feedback are core research topics of NIME  Module B: So, 
you want to build a NIME Six steps to build a NIME 1. Pick control space 2. Pick sound space 3. Pick 
mapping 4. Connect with software 5. Compose and practice 6. Repeat 5.  1 and 2 often switched. Tools 
to help with steps 1-4. An example: Tooka (Fels et al., 2004) Vibrato Pitch Bend mapping with PureData 
 Octave Sustain Pitch Volume  5/17/09 &#38;#169;2009, S. Fels and M. Lyons 3 sound synthesis Pick your 
control space Plethora of sensors available to measure: motion of body parts position, rotation, velocity 
and acceleration  translation and rotation (torque) forces isometric and isotonic sensors  pressure 
 airflow  proximity  temperature  neurophysiological signals  heart rate  galvanic skin response 
 brain waves  muscle activities   light levels  and more  So many sensors Piezoelectric Sensors 
 Force Sensing Resistors  Accelerometer (Analog Devices ADXL50)  Biopotential Sensors  Microphones 
 Photodetectors  CCDs and CMOS cameras  Electric Field Sensors  RFID  Magnetic trackers (Polhemus, 
Ascension)  and more   Sensors Reach - EMF disturbance  Slide - resistive  TapTile -Force senstive 
resistor  Tilt   electrolytic, single axis (-70-+70 deg) Touch - 0 travel FSR  TouchGlove  several 
touch sensors TouchStrip long touch sensor Turn potentiometer    Connecting sensors Sensor response 
require transduction and digitizing: electrical Sensor voltage Transduction  resistance  impedance 
 optical signal conditioning colour  intensity  magnetic A to D  induced current conversion  field 
direction   mechanical force   Digitizing converting change in resistance into voltage typical 
sensor has variable resistance (Rsensor) Vsrc (+5V) Rsensor A simple voltage Vout to digitizer divider 
circuit R  Vgnd (0V)  Digitizers for Connecting to Computer  Some MIDI synthesizers, i.e., Yamaha 
mu100  Arduino board  Bluetooth module for wireless ICubeX A/D to MIDI Phidgets A/D to USB DAQ boards 
A/D to computer bus  Mapping Sensor to Music  The relationship between the change in the sensor value 
to the sound output is called a mapping  The mapping defines how much effort to learn and play your 
NIME  module 2 discussed different approaches to mapping Last step is to control your sound output: 
 communication protocol  sound synthesizer  Sound output control: communications Musical Instrument 
Digital Interface (MIDI)  electronic instrument standard defined in 1982  specifies;  connectors, 
data rates, electrical properties, etc. 1 message/msec (approx) note on/off, velocity is typical packet 
 control messages to change instrument synthesis   OpenSound control (OSC) (Wright and Freed, 1997) 
  TCP/IP, internet protocol, typically UDP based  faster, low latency, variable packet types  computer 
to computer, computer to hardware   Internal protocols, i.e. DAQ driver  Sound Synthesis Techniques 
Methods sampled  FM synthesis  additive/subtractive  granular  waveform  scan  Sound Synthesizers 
Hardware MIDI synthesizers Corporation, and others Software STK (Cook)  PureData (Pd, Puckette)  JASS 
(van den Doel)  Max/MSP ($$) (cycling74.com)  Chuck (Wang and Cook, 2003)  Supercollider (McCartney, 
1996)  and others  A few practical notes Portable: Batteries can be used to make portable  Wireless 
protocols available for portable  Write pieces for the instrument  Aesthetics are important  Plan 
your checklist for performance  too many things can go wrong with technology Plan your staging can severely 
impact performance of sensors Plan for producing stable versions hard to learn to play if NIME keeps 
changing Summary  Making a NIME is usually easier than playing it (well)  Choose your movement type 
and/or sound space  then work on sensing Put together your input, mapping and output  Now you are 
ready to:  practice, practice, practice and perform  aesthetics principles covered in module 4  Module 
C: Aesthetics of NIME  Technological Expressionism  NIME &#38; the Music Process  Challenge of Performance 
 Mapping &#38; the Audience: Transparency  Visual Feedback  Interaction Metaphor  Technological 
Expressionism  Shock of the New Techno-fetishism  Human-machine relationship Experimentalism  
 Video Clip NIME Favors a Return Process-oriented Music we are in a period of restoring fluidity to 
the musical transformative process of making music more process-oriented again and less artifact- Gideon 
D Arcangelo, NIME-04 oriented. Challenge of Performance Audience may not understand your NIME  Expectations 
may be varied  No musical tradition to fall back on  A demo is not a performance  Hisashi Okamoto, 
NIME-04 The First Sailing with Limber-Row Complicated mapping Simplify OT Complex mapping OO TO OT 
TT OO TO How to achieve OO? Transparency for Performer &#38; Audience (Gadd et al, 2003) Visual Cues 
&#38; Transparency Visual Appearance of Instrument Module E Visualization of Interaction Visual Interfaces 
Visualization of Sound Output Reactable Tenori-on Transparency &#38; Interaction Metaphor SoundSculpting 
(Mulder and Fels, 1998) - two Cybergloves and Trackers - map metaphor of rubber sheet onto sound space 
 - transparent for audience and performer  Transparency Simple &#38; Direct Interface  Contact Mics 
 Magnets  Paper clips  Video Clip Kanta Horio, NIME-04  Aesthetics of Failure  Suspense highlights 
the technological challenge  If there are never difficulties, glitches etc then the limits are not 
being pushed  Technical difficulty delayed this performance, but improved the outcome Summary  Technology 
is increasing the fluidity of musical culture  NIME presents special challenges for performers  Well-designed 
visual feedback can greatly improve mapping transparency for audience and player  Interaction metaphors 
another strategy  Initial failure can enhance eventual success   Original NIMEs Moog version Leon 
Theremin, 1928 senses hand position frequency volume relative to antennae controls frequency and amplitude 
 Clara Rockmore playing  More original NIMEs Hugh Le Caine (1940s) electronic sackbut  sensor keyboard 
 downward and side-to-side  potentiometers   Science Dimension volume 9 issue 6 1977 right hand 
can modulate loudness and pitch  left hand modulates waveform  precursor to the mod-wheel Canada Science 
and Technology Museum Electronic Sackbut  1971 commercial version Buchla s Midi Controllers Thunder 
(1990) 36 touch sensors Lightning 2 (1996) LED based position sensing Marimba Lumina (1999) pads and 
ribbon controllers (strips)  bars are sensitive to proximity, hit location and movement  4 different 
mallets for different effects  5/17/09 &#38;#169;2009, S. Fels and M. Lyons  Buchla 200e Series music 
controllers Modules can be combined: Control and Signal Router  Multi Dimensional Kinesthetic Input 
Port  Midi Decoder/Preset Manager  System Interface Arbitrary Function Generator (2 panel units)  
Complex Waveform Generator  Source of Uncertainty  Quad Function Generator  Frequency Shifter / Balanced 
Modulator  Triple Morphing Filter  Quad Dynamics Manager  There s a lot of NIMEs out there  (Marshall, 
2009) Most are classed in the Alternate category Augmented Instruments Hypercello (Gershenfeld &#38; 
Chung, 1991) -related Hyperbow (Young, 2001)  Augmented Instuments Yamaha Disklavier MIDI controlled 
acoustic piano  solenoid actuators to play keys  records key press  Radio Baton + Disklavier performance 
Jaffe &#38; Schloss, The Seven Wonders of the Ancient World, 1991  Augmented Instruments Hyper-Flute 
(Palacio-Quintin, 2003) standard Boehm flute  sensors:  magnetic field, G# and C# keys  ultrasound 
tracking  mercury tilt switch  pressure sensors (left hand and thumbs)  light sensor  buttons  
 Augmented Instruments  Haptic drum (Berdahl et al., 2008) -Actuator gives feedback to strike -Easy 
to play single-handed drum rolls Alternative Instruments: Using different sensors  (Marshall, 2009) 
 Alternative Instruments how to organize: sensed property (i.e. wind)  player action (i.e. percussion) 
 instrument shape  relationship to body  Hands Only - no contact Lady s Glove (Sonami, 1991+) hall 
effect sensors, microswitches, resistive strips, pressure pad, accelerometer  controlled musical effects 
  Hands Only - no contact The Hands (Waisvisz, 1984) Hands Only - no contact Manual Input Sessions 
(Levin and Leibermann, 2005) camera and OHP SoundSculpting (Mulder and Fels, 1998) GloveTalkII/GRASSP/DIVA, 
(Fels et al., 1994+) cybergloves, tracker, switches  controlled formant synthesizer  and more Hands 
- Physical contact Most typical type of NIME  Ski (Huott, 2002)  fibre optic multitouch pad Tactex 
Inc.  mappings:  playback: linear, polar and angular control modes  percussive  pitch tuning:  
 MIDI controller  upright form factor  Hand -Physical Contact Pebblebox (O Modhrain &#38; Essl, 2004) 
 microphone + stones  granular synthesis  play with stones mixes granules Hand -Physical Contact Crackle 
box (Waisvisz, 1975) analog circuit op-amp with body resistance connected to pins  in the tradition 
of circuit bending  Hand -Physical Contact Lippold Haken s Continuum touch sensitive ­neoprene covered 
  x, y along board  z -pressure   MIDI controller  sound effects  continuous frequency  pitch 
bends    Hand -Physical Contact Matrix (Overholt, 2001) linear optical encoder on pins  sound shaping 
mapping  direct synthesis mode shape specifies harmonic spectrum  granular synthesis mode  gesture 
mode   Breath and Hands iPhone Ocarina (Wang, 2009) touch screen plus microphone  mapped to tones 
for ocarina sounds   Face/Head Control eSitar (Kapur et al, 2004) accelerometer for head tilt experimented 
with volume, duration, and more Mouthesizer (Lyons et al., 2003)  SoFA, (Funk et al., 2005)  Tongue 
n Groove (Vogt et al., 2002)  ultrasound probe to measure tongue movement  Body Miburi from Yamaha, 
1994  bend sensors at arm joints  two buttons/finger and thumb  two pressure sensors/foot  MIDI 
controller   Inside Body Biomuse (Knapp and Lusted, 1990) 8 channel signal amp EMG, EKG, EOG, EEG 
 Tibeten singing bowls (Tanaka and Knapp, 2002)  EMG and position sensing miniBioMuseIII (Nagashima, 
2003) 8 EMG channels mapped to bandpass filters, sinewave generators and FM synthesizers used in BioCosmicStorm-II 
 Collaborative Instruments Tooka (Fels and Vogt, 2002) pressure for breath  buttons for fingers  
bend sensor  touch sensor  two players share breath  coordinate movements  MIDI mapping  5/17/09 
&#38;#169;2009, S. Fels and M. Lyons Collaborative Instruments OROBORO (Carlile &#38; Hartmann, 2005) 
 haptic mirror between hand paddles  4 FSRs/hand  mapped using Pd to:  violins sounds  sampled sounds 
   NIMEs for Novices  Interactive instruments embody all of thenuance, power, and potential ofdeterministic 
instruments, but the waythey function allows for anyone, from themost skilled and musically talentedperformers 
to the most unskilled membersof the large public, to participate in a musical process (Chadabe, 2002) 
 Walk up and play  NIMEs for Novices (Blaine &#38; Fels, 2003) Aptitude Novice Virtuoso Capacity Single 
player Single interface Electronic Bullroarer Iamascope Duet on piano Multiple interfaces Musical Trinkets 
Jazz Ensembles Multiple players Single interface Beatbugs Squeezables Audio Grove Sound Mapping Speaking 
Orbs Jamodrum Mikrophonie I, Tooka Multiple interfaces Augmented Groove Brain Opera Drum Circle Mikrophonie 
II Jam-o-drum (Blaine and Perkis, 2000) 4 player audio/visual interface drum pads sensors with rotation 
sensor around rim Drum circle concept  Various musical games  turn taking  collaboration  Jam-o-drum 
(Blaine and Perkis, 2000)  Brain Opera (Machover et al, 1996) Multiple NIME as part of a larger connected 
set of interaction Forest station  Harmonic driving  Melody easel  Rhythm tree  Gesture wall  Digital 
baton  Audience sensing in performance space  Sensor chair  Brain Opera (Paradiso, 1999) Sensor chair 
 multiple antenae to track hand positions  two antenae for feet  buttons  lights  MIDI mapping 
Brain Opera NIME  Summary  Creating a NIME is easy to do  Creating a good mapping is hard  Playing 
it well takes practice to be a virtuoso  some NIMEs created to be easy to play but not so expressive 
 Without piece, difficult to gain acceptance  Often audience doesn t know what is going on  mapping 
transparency can help Many explorations trying different ways to make music  Imaginary Piano: visual 
input only  Iamascope: visual input and output  Facial gesture musical interfaces: when vision may 
be your best option  Reactable: vision + (passive) touch, through alignment)  Module E: Visual Interfaces 
 Visual Interfaces  Multi-modal interfaces: focus is on relationship between sound and vision  Two 
broad classes:  Vision based interfaces use camera as sensor  Graphical interfaces provide visual 
feedback to performer and audience  Most works combine the two classes Visual Interfaces  F1 : visual 
feedback in the form of aligned graphics Visual Input Only: Imaginary Piano  Leonello Tarabella, NIME-02 
 Video camera with motion-sensitive zone  No primary feedback  Iamascope  This gives a colourful kaleidoscopic 
feedback of part of the player. Gestures are used to trigger harmonious chord progressions and arpeggios. 
  Quite good coordination between sound and graphics  Visual Input &#38; Output Iamascope  Interactive 
multimedia artwork  Participants are put inside a large kaleidoscope  Participants movements also map 
to music  Immersive, interactive, satisfying and fun  Fels &#38; Mase, 1997  Iamascope - video  Facial 
Gesture Musical Interface  Lyons, NIME-01 Mouthesizer  Colour &#38; intensity thresholding Image 
processing operations  Morphological transform &#38; filtering  Connected components + shape analysis 
 Lyons et al., NIME-03 Mouthesizer Video Guitar Effects Controller  Lyons (2001) Mapping: H Cutoff 
Frequency of Resonant Low Pass Filter W Distortion Sonification of Facial Actions (SoFA)  Optical Flow 
triggers samples  Samples mapped to facial zones  Frame is recalibrated with face detection Saccades 
  Funk et al., NIME-05 Sonification of Facial Actions (SoFA)  Funk et al., NIME-05 Reactable  
Video tracking of marked pucks on a table  Projection of visual feedback  Sergi Jordà et al., Universitat 
Pompeu Fabra  first presented at NIME-03  Video Power of Visual Feedback see also Module A (Theory) 
and Module C (Aesthetics)  Yamaha Tenori-on Monome Nishibori &#38; Iwai, NIME-06 Crabtree &#38; Cain 
Summary  Large number of works have used visual input and output as a way to enhance new musical interfaces 
 Have give a very few case studies  General principle is that vision offers a powerful way to capture 
gestural input  Visual output can enhance aesthetics &#38; experience of performer and audience  Module 
F: Education and NIME Perry s Principles: Guidelines for the beginner  NIME Curricula  Some Guidelines: 
Perry s Principles  Rules of thumb for the design of digital musical instruments  Several of the principles 
are heavily subscribed  Principles for Designing Computer Music Controllers P. Cook, NIME-01 Revised: 
Principles for Controlling Computer Music Designers P. Cook, Keynote talk, NIME-07 Perry s Principles 
Human/Artistic Principles P1: Programmability is a curse P2: Smart instruments are often not smart P3: 
Copying an instrument is dumb, leveraging expert technique is smart P4: Some players have spare bandwidth, 
some do not P5: Make a piece, not an instrument or controller P6: Instant music, subtlety later Perry 
s Principles* Technological: P7: Miracle, Industry Designed, Inadequate P8: Batteries, Die (a command 
not an observation) P9: Wires are not that bad (compared to wireless) Misc.: P10: New algorithms suggest 
new controllers P11: New controllers suggest new algorithms P12: Existing Instruments suggest new controllers 
P13: Everyday objects suggest amusing controllers Perry s Principles* New (as of 2007) P14: More can 
be better (but hard)  P15: Music + Engineering is a great Teaching (and Marketing) tool P17: Younger 
students are more fearless P1: Programmability is a curse P2: Smart Instruments are Often Not  Easy 
to add complexity, features, bandwidth  But instruments can quickly become complex, unstable, and difficult 
to learn  It is tempting to A.I. to instruments but this can often be bad design if the player feels 
the instrument too obviously has a mind of its own  P5: Make a piece not a controller P6:Instant Music, 
Subtlety later  Making music is the goal  The ideal new musical interfaces has: Low entry fee with 
no ceiling on virtuosity  Wessel &#38; Wright, NIME-01 P13: Everyday objects suggest controllers that 
are both amusing &#38; good Sonic Banana (E. Singer, NIME-03) Java mug &#38; Fillup Glass (P. Cook, 
NIME-01)  Vienna vegetable orchestra  Vienna vegetable orchestra  melanzaniklappe kuerbis zellerbongos 
 bohnen karottenfloeten gurkophon Vienna vegetable orchestra  http://www.youtube.com/watch?v=hpfYt7vRHuY 
 P15: Music + Engineering is a great Teaching Tool  High student interest  Motivation for learning 
a range of core topics including:  Sensors  HCI  DSP  Math skills  Programming  Networking  Joe 
Paradiso &#38; student (NIME-02) Education and NIME  Sound Synthesis  Sensors, Effectors, Microcontrollers 
 Basic Electronics  Communication Protocols (MIDI, OSC, TCP etc.)  Sound Synthesis and Processing 
 Acoustics  Human-Computer Interaction  Music  Where to study this field? IRCAM, Paris  CCRMA, 
Stanford  CIRMMT, McGill  Princeton, CS &#38; Music  MIT Media Lab  NYU Interactive Telecommunications 
Program  SARC, Queen s, Belfast  Growing field  URLs listed in the References  Specific Learning 
Resources  Miranda &#38; Wanderley (2006)  Igoe (2007)  Roads (1996)  NIME Proceedings  ICMC Proceedings 
 Computer Music Journal  Organized Sound  J. New Music Research   Curricula  beginning graduate 
or senior undergraduate level  Courses tend to be project oriented  Students learn what they need 
 Live performance or Demo is necessary for completion of the course (ITP, CCRMA)  NYU ITP NIME Course 
 Master s program in design &#38; technology attracting students from a wide range of backgrounds  
 Gideon D Arcangelo Hans C. Steiner Jamie Allen Taku Lippit (NIME-04) NYU ITP NIME Course  ITP tradition 
in Phys. Comp.  Course offered to 2nd year students  Students already have exposure to sensors, microcontrollers, 
flash, processing  Formal music background not a prerequisite  Review basics of music tech. as needed 
 Project based each student designs and develops a novel musical controller  Review of musical controller 
literature and discussion of design issues and approaches  Preparation for a recital with new musical 
controllers  NIME Curriculum NIME Curriculum - Topics Historical Survey of Musical Instrument Types 
  Attributes of Musical Expression  Music Theory and Composition  Musical Interface Responsiveness 
 Discrete vs. Continuous Controllers  Gestures and Mapping  Novice and Expert Interfaces  Spectacle 
and Visual Feedback in Performance  Collaborative Interfaces  Substantial resources for learning about 
NIME  Good place to start: Perry s principles  NIME courses are usually project based  Number of universities 
offering programs of study is expanding  Next frontier: high schools, science fairs  Summary Concluding 
Remarks References and Resource Material Module A Theory [Wanderley &#38; Battier, 2000] Wanderley, 
M. and Battier, M. 2000. Trends in gestural control of music, IRCAM. [Miranda &#38; Wanderley, 2006] 
Miranda, E.R. and Wanderley, M.M. 2006. New digital musical instruments: control and interaction beyond 
the keyboard. AR Editions. [Wessel &#38; Wright, 2002] Problems and prospects for intimate musical control 
of computers, Computer Music Journal 26(3): 11 22. [Wessel &#38; Wright, 2001] Problems and prospects 
for intimate musical control of computers, In Proceedings of the 2001 Conference on New interfaces For 
Musical Expression (NIME-01) (Seattle, Washington, April 01 -02, 2001). [Vertegaal et al. 1996] Vertegaal, 
R. and Ungvary, T. and Kieslinger, M.1996. Towards a musician's cockpit: Transducers, feedback and musical 
function, Proceedings of the International Computer Music Conference, pp. 308 311. [Mulder, 2000] Mulder, 
A. 2000. Towards a choice of gestural constraints for instrumental performers. In: Trends in gestural 
control of music, Wanderley, M. and Battier, M., eds., IRCAM. [Camurri, 2000] Camurri, A. and Hashimoto, 
S. and Ricchetti, M. and Ricci, A. and Suzuki, K. and Trocca, R. and Volpe, G. 2000. EyesWeb: Toward 
gesture and affect recognition in interactive dance and music systems, Computer Music Journal 24(1): 
57-69. [Waiswicz, 1985] Waiswicz, M. 1985. THE HANDS: A Set of Remote MIDI Controllers. Proceedings 
of the 1985 International Computer Music Conference (ICMC 85) Vancouver Canada, pp 313 318, San Francisco: 
ICMA. [Marshall, 2009] Marshall, M.T. 2009. Physical Interface Design for Digital Musical Instruments, 
Ph.D. thesis, McGill University, March. http://www.idmil.org/_media/publications/marshall_phdthesis_final.pdf 
 [Moore, 1988] Moore, F.R. 1988. The dysfunctions of MIDI, Computer Music Journal 12(1): 19-28. [Burzik, 
2002] Burzik, A. 2002. Practising in Flow The Secret of the Masters, Stringendo 24(2): 18-21. [Hunt &#38; 
Wanderley, 2002] Hunt, A. and Wanderley, M.M. 2002. Mapping performer parameters to synthesis engines. 
Organised Sound 7: 97-108. [Hunt, Wanderley, &#38; Paradis, 2003] Hunt, A. and Wanderley, M.M. and Paradis, 
M. 2003. The importance of parameter mapping in electronic instrument design, Journal of New Music Research 
32(4): 429 -440. [Rovan et al., 1997] Rovan, J.B., Wanderley, M.M, Dubnov, S. and Depalle, P. 1997. 
Instrumental Gestural Mapping Strategies as Expressivity Determinants in Computer Music Performance. 
In: Kansei, The Technology of Emotion. Proceedings of the AIMI International Workshop, A. Camurri, (ed.), 
Genoa: Associazione di Informatica Musicale Italiana, October 3-4, pp. 68 73. [Arfib et al., 2002] D. 
Arfib, J. M. Couturier, L. Kessous and V. Verfaille 2002. Strategies of mapping between gesture data 
and synthesis model parameters using perceptual spaces. Organised Sound 7: 127-144. [Goudeseune, 2002] 
Goudeseune, C. 2002. Interpolated mappings for musical instruments. Organised Sound 7: 85-96. [Wessel, 
1979] Wessel, D. 1979. Timbre space as a musical control structure, Computer music journal 3(2): 45 
52. [Vertegaal &#38; Eaglestone, 1996] R. Vertegaal and B. Eaglestone 1996. Comparison of input devices 
in an ISEE direct timbre manipulation task, Interacting with Computers, 8(1): 13 30. [Lyons et al., 
2003 Lyons, M.J. and Haehnel, M. and Tetsutani, N. 2003. Designing, playing, and performing with a vision-based 
mouth interface, Proceedings of the 2003 conference on New interfaces for musical expression pp. 116 
121. [Steiner, 2006] H.-C. Steiner. Towards a catalog and software library of mapping methods. Proceedings 
of the 2006 conference on New interfaces for musical expression, pp. 106 109, Paris, France, 2006. IRCAM 
Centre Pompidou. [Cook, 2004] Cook, P. R. 2004. Remutualizing the musical instrument: Co-design of synthesis 
algorithms and controllers. Journal of New Music Research, 33(3):315-320.  Module B Tools of NIME [Fels 
et al. 2004] Sidney S. Fels and Linda Kaastra and Sachiyo Takahashi and Graeme McCaig. Evolving Tooka: 
from Experiment to Instrument. 4rd International Conference on New Interfaces for Musical Expression 
(NIME04). Pages 1-6. May. 2004. [Wright and Freed, 1997] Wright, M. and A. Freed 1997. Open Sound Control: 
A New Protocol for Communicating with Sound Synthesizers. Proceedings of the International Computer Music 
Conference, Thessaloniki, Hellas, pp. 101-104. Sound Synthesis References: General articles: [Mathews 
&#38; Pierce, 1989] Mathews, M. &#38; Pierce, J. (1989). Current Directions in Computer Music Research. 
The MIT Press. [Steiglitz, 1996] Steiglitz, K. 1996 Digital Signal Processing Primer, New York, Addison 
Wesley. [Roads, 1976] Roads, C. 1976, The Computer Music Tutorial, Cambridge, MIT Press Additive synthesis: 
[McAulay &#38; Quatieri, 1986] McAulay, R. and T. Quatieri. 1986. "Speech Analysis/Synthesis Based on 
a Sinusoidal Representation." IEEE Trans. Acoust. Speech and Sig. Proc. ASSP-34(4): pp. 744-754. [Smith 
and Serra, 1987] Smith, J. and Serra, X. 1987. "PARSHL: Analysis/Synthesis Program for Non-Harmonic Sounds 
Based on a Sinusoidal Representation." Proc. International Computer Music Conference, Urbana, pp. 290 
 297. Subtractive Synthesis: [Dudley, 1939] Dudley, H. 1939, "The Vocoder," Bell Laboratories Record, 
December. [Moorer, 1978] Moorer, A. 1978. "The Use of the Phase Vocoder in Computer Music Applications." 
Journal of the Audio Engineering Society, 26 (1/2), pp. 42-45. [Moorer, 1979] Moorer, A. 1979, The Use 
of Linear Prediction of Speech in Computer Music Applications, Journal of the Audio Engineering Society 
27(3):134-140. [Dolson, 1986] Dolson, M. 1986, "The Phase Vocoder: A Tutorial," Computer Music Journal, 
10 (4), pp. 14 -27. [Makhoul, 1975] Makhoul, J. 1975. "Linear Prediction: A Tutorial Review," Proc. of 
the IEEE, v 63., pp. 561-580. FM synthesis: [Chowning, 1973] Chowning, J. 1973, The Synthesis of Complex 
Audio Spectra by Means of Frequency Modulation, Journal of the Audio Engineering Society 21(7): pp. 526-534. 
[LeBrun, 1979] LeBrun, M. 1979. Digital Waveshaping Synthesis, Journal of the Audio Engineering Society, 
27(4): 250-266. Modal Synthesis: [Adrien, 1988] Adrien, J. 1988. Etude de Structures Complexes Vibrantes, 
Application-la Synthèse par Modeles Physiques, Doctoral Dissertation. Paris: Université Paris VI. Wawrzynek, 
J. 1989. VLSI Models for Sound Synthesis, in Current Directions in Computer Music Research, M. Mathews 
and J. Pierce Eds., Cambridge, MIT Press. [Larouche &#38; Meillier 1994] Larouche, J. &#38; J. Meillier 
1994. Multichannel Excitation/ Filter Modeling of Percussive Sounds with Application to the Piano, IEEE 
Trans. Speech and Audio, pp. 329-344. Physical Modeling Approaches: [Smith, 1987] Smith, J. 1987. Musical 
Applications of Digital Waveguides. Stanford University Center For Computer Research in Music and Acoustics. 
Report STAN-M-39. [Karjalainen et al. 1991] Karjalainen, M. Laine, U., Laakso, T. and V. Välimäki, 1991. 
Transmission Line Modeling and Real-Time Synthesis of String and Wind Instruments, Proc. International 
Computer Music Conference, Montreal, pp. 293-296 [Cook, 1991] Cook, P. 1991. TBone: An Interactive Waveguide 
Brass Instrument Synthesis Workbench for the NeXT Machine, Proc. International Computer Music Conference, 
Montreal, pp. 297-299. [Cook, 1991b] Cook, P. 1991b. LECTOR: An Ecclesiastical Latin Control Language 
for the SPASM/singer Instrument, Proc. International Computer Music Conference, Montreal, pp. 319-321. 
[Cook, 1992] Cook, P. 1992. A Meta-Wind-Instrument Physical Model, and a Meta-Controller for Real-Time 
Performance Control, Proc. International Computer Music Conference, San Jose, pp. 273-276. [Cook, 1992b] 
Cook, P. 1992b. "SPASM: a Real-Time Vocal Tract Physical Model Editor/Controller and Singer: the Companion 
Software Synthesis System," Computer Music Journal, 17: 1, pp 30-44. [McIntyre et al. 1983] McIntyre, 
M., Schumacher, R. and J. Woodhouse 1983, On the Oscillations of Musical Instruments, Journal of the 
Acoustical Society of America, 74(5), pp. 1325-1345. Granular Synthesis: [Roads, 1991] Roads, C. (1991). 
Asynchronous Granular Synthesis, In G. De Poli, A. Piccialli, &#38; C. Roads (Eds.), Representations 
of Musical Signals, pp. 143 185. Cambridge: MIT Press. [Gabor, 1947] Gabor, D. (1947). Acoustical Quanta 
And The Theory Of Hearing. Nature, 159(4044), 591 594. [Xenakis, 1971] Iannis Xenakis, Formalized Music: 
Thought and Mathematics in Composition. Bloomington and London: Indiana University Press, 1971. [Truax, 
1988] Truax, B. (1988) Real-time granular synthesis with a digital signal processor. Computer Music Journal, 
12(2), 14-26. Scan Synthesis: [Verplank, 2000] B. Verplank, M. Mathews, R. Shaw, "Scanned Synthesis", 
"Proceedings of the 2000 International Computer Music Conference", p: 368--371, Berlin, Zannos editor, 
ICMA, 2000. Resources: MIDI website: http://www.midi.org/ Sensors and A/D converters: Infusion Systems: 
www.infusionsystems.com Phidgets: www.phidgets.com Arduino: http://www.arduino.cc/ National Instruments: 
http://www.ni.com/dataacquisition/ Digikey: http://www.digikey.com/ Jameco: www.jameco.com Synthesizers: 
STK from Perry Cook s page: http://www.cs.princeton.edu/~prc/NewWork.html#STK Perry R. Cook and Gary 
P. Scavone, The Synthesis ToolKit (STK), Proc of the ICMC, 1999. G. Scavone and P. Cook, RtMIDI, RtAudio, 
and a Synthesis (STK) Update, Proceedings of the International Computer Music Conference, Barcelona, 
September, 2005. PureData: http://puredata.info/ JASS: http://www.cs.ubc.ca/~kvdoel/jass/jass.html Kees 
van den Doel and Dinesh K. Pai, JASS: A Java Audio Synthesis System for Programmers, Proceedings of the 
International Conference on Auditory Display, pp. 150­154, 2001, Helsinki. Max/MSP: http://www.cycling74.com/ 
Chuck: http://chuck.cs.princeton.edu/ [Wang &#38; Cook, 2004] Wang, G. and Cook, P. R. 2004. On-the-fly 
programming: using code as an expressive musical instrument. In Proceedings of the 2004 Conference on 
New interfaces For Musical Expression (Hamamatsu, Shizuoka, Japan, June 03 -05, 2004), 138-143. Supercollider: 
http://supercollider.sourceforge.net// and http://www.audiosynth.com/  Module C Aesthetics [Kimura, 
2004] Mari Kimura 2004, Performance at NIME-04, Hamamatsu, Japan. [Wynnychuk, 2004] Jordan Wynnychuk 
2004, Performance at NIME-04, Hamamatsu, Japan. [D Arcangelo, 2004] D Arcangelo, G. 2004, Recycling 
music, answering back: toward an oral tradition of electronic music, Proceedings of the 2004 conference 
on New interfaces for musical expression, pp. 55-58. [Okamoto, 2004] Hisashi Okamoto, 2004, Performance 
at NIME-04, Hamamatsu, Japan. [Gadd &#38; Fels, 2002] Gadd, A. and Fels, S. 2002. MetaMuse: metaphors 
for expressive instruments, Proceedings of the 2002 conference on New interfaces for musical expression, 
pp. 1-6. [Fels et al., 2003] Fels, S. and Gadd, A. and Mulder, A. 2003. Mapping transparency through 
metaphor: towards more expressive musical instruments, Organised Sound 7(2): 109-126. [Jorda, 2003] 
Jorda, S. 2003. Sonigraphical instruments: from FMOL to the reacTable, Proceedings of the 2003 conference 
on New interfaces for musical expression, pp. 70-76. [Jorda et al. 2005] Jorda, S. and Kaltenbrunner, 
M. and Geiger, G. and Bencina, R. 2005. The reactable*, Proceedings of the International Computer Music 
Conference (ICMC 2005) pp. 579 582. [Mulder &#38; Fels, 1998] Axel G.E. Mulder and S. Sidney Fels (1998). 
Sound Sculpting: Performing with Virtual Musical Instruments. Proceedings of the Fifth Brazilian Symposium 
on Computer Music (Belo Horizonte, Minas Gerais, Brazil, 3-5 August 1998, during the 18th Annual Congress 
of the Brazilian Computer Society, G. Ramalho (ed.)), pp. 151-164. [Horio, 2004] Kanta Horio 2004. Performance 
at NIME-04, Hamamatsu, Japan. [Fujii, 2004] Uriko Fujii 2004. Performance at NIME-04, Hamamatsu, Japan. 
 [Fels et al. 2002] Fels, Sidney, Gadd, Ashley, Mulder, Axel (2002). Mapping Transparency through Metaphor: 
Towards more expressive musical instruments, Organised Sound: Vol. 7, no. 2. Cambridge: Cambridge University 
Press: 109-126.  Module D Case Studies [Mathews &#38; Schloss, 1989] Mathews, M.V. and A. Schloss The 
Radio Drum as a Synthesizer Controller, Proc. of the 1989 ICMC. [Boei et al. 1989] Boie, R.A. et al. 
Gesture Sensing via Capacitive Moments. Work Project No. 311401-(2099,2399) AT&#38;T Bell Laboratories, 
1989. [Young, 1991] Young, G. (1991), The Sackbut Blues: University of Toronto Press in Canada, and the 
University of Chicago Press in the USA. Issued also in French under the title: "Blues pour saqueboute: 
Hugh Le Caine, pionnier de la musique électronique." For further information phone +1/613 991-2983. The 
National Museum of Science and Technology houses an extensive collection of Le Caine's instruments. [Buchla, 
2005] Don Buchla. A History of Buchla's Musical Instruments., Proc. of NIME2005, Vancouver, BC, http://www.nime.org/2005/proc/nime2005_001.pdf 
Paradiso, J., and Gershenfeld, N. Musical Applications of Electric Field Sensing. In Computer Music Journal 
21(2) Summer, pp. 69-89. 1997. [Palacio-Quintin, 2003] Palacio-Quintin, Cléo. The Hyper-Flute. In Proceedings 
of the New Interfaces for Musical Expression (NIME) Conference. Montreal, 2003. [Young, 2001] Young, 
D., The hyperbowcontroller: Real-time Dynamics measurement of violin performance. In Proc. NIME, 2001. 
[Marshall, 2009] Marshall, M., Physical Interface Design for DigitalMusical Instruments. Ph.D. thesis, 
McGill University, 2009. [Fraser et al. 2008] Helene Fraser and Sidney Fels and Robert Pritchard. Walk 
the Walk, Talk the Talk. 12th IEEE 2008 International Symposium on Wearable Computing (ISWC2008). Pages 
117--118. 2008. [Pritchard &#38; Fels, 2006] Pritchard, B. and Fels, S. 2006. GRASSP: gesturally-realized 
audio, speech and song performance. In Proceedings of the 2006 Conference on Newinterfaces For Musical 
Expression (Paris, France, June 04 -08, 2006). New Interfaces For Musical Expression. IRCAM Centre Pompidou, 
Paris, France, 272-276. [Fels &#38; Hinton, 2998] Sidney S. Fels and Geoffrey E. Hinton. Glove-TalkII: 
A neural network interface which maps gestures to parallel formant speech synthesizer controls. IEEE 
Transactions on Neural Networks. Volume 9. No. 1. Pages 205-212. 1998. [Levin &#38; Lieberman, 2005] 
Levin, G. and Lieberman, Z. "Sounds from Shapes: Audiovisual Performance with Hand Silhouette Contours 
in "The Manual Input Sessions". Proceedings of NIME '05, Vancouver, BC, Canada. May 26-28, 2005. [Mulder 
&#38; Fels, 1998] Axel G.E. Mulder and S. Sidney Fels (1998). Sound Sculpting: Performing with Virtual 
Musical Instruments. Proceedings of the Fifth Brazilian Symposium on Computer Music (Belo Horizonte, 
Minas Gerais, Brazil, 3-5 August 1998, during the 18th Annual Congres of the Brazilian Computer Society, 
G. Ramalho (ed.)), pp. 151-164. [Huott, 2002] Huott, R. 2002. An interface for precise musical control. 
In Proceedings of the 2002 Conference on New interfaces For Musical Expression (Dublin, Ireland, May 
24 -26, 2002). E. Brazil, Ed., 1-5. http://www.nime.org/2002/proceedings/paper/huott.pdf [O'Modhrain 
&#38; Essl, 2004] O'Modhrain, S. and Essl, G. 2004. PebbleBox and CrumbleBag: tactile interfaces for 
granular synthesis. In Proceedings of the 2004 Conference on New interfaces For Musical Expression (Hamamatsu, 
Shizuoka, Japan, June 03 -05, 2004). M. J. Lyons, Ed., 74-79. [Overholt, 2001] Overholt, D. 2001. The 
MATRIX: a novel controller for musical expression. In Proceedings of the 2001 Conference on New interfaces 
For Musical Expression (Seattle, Washington, April 01 -02, 2001). [Wang, 2009] Wang, G. 2009. Designing 
Smule's iPhone Ocarina, In Proceedings of the 2009 Conference on New interfaces For Musical Expression 
(Pittsburgh, PA, June 4-6, 2009). [Merrill, 2003] Merrill, D. 2003. Head-tracking for gestural and continuous 
control of parameterized audio effects. In Proceedings of the 2003 Conference on New interfaces For Musical 
Expression (Montreal, Quebec, Canada, May 22 -24, 2003). 218-219. http://www.music.mcgill.ca/musictech/nime/onlineproceedings/Papers/NIME03_Merill.p 
df [Kapur et al. 2004] Kapur, A., Tzanetakis, G., &#38; P.F. Driessen, "Audio-Based Gesture Extraction 
on the ESitar Controller," In Proceedings of the Conference on Digital Audio Effects, Naples, Italy, 
October 5-8, 2004. http://soundlab.cs.princeton.edu/research/controllers/esitar/ [Vogt et al. 2002] Vogt, 
F., McCaig, G., Ali, M. A., and Fels, S. 2002. Tongue 'n' Groove: an ultrasound based music controller. 
In Proceedings of the 2002 Conference on New interfaces For Musical Expression (Dublin, Ireland, May 
24 -26, 2002). http://www.nime.org/2002/proceedings/paper/vogt.pdf [Takaka &#38; Knapp, 2002] Tanaka, 
A. and Knapp, R. B. 2002. Multimodal interaction in music using the Electromyogram and relative position 
sensing. In Proceedings of the 2002 Conference on New interfaces For Musical Expression (Dublin, Ireland, 
May 24 ­26, 2002. http://www.nime.org/2002/proceedings/paper/tenaka.pdf [Knapp &#38; Lusted, 1990] R. 
Benjamin Knapp and Hugh Lusted -A Bioelectric Controller for Computer Music Applications, Computer Music 
Journal, Volume 14 No. 1, New Performance Interfaces 1 -Spring 1990; pg 42-47. [Nagashima, 2003] Nagashima, 
Y. 2003. Bio-sensing systems and bio-feedback systems for interactive media arts. In Proceedings of the 
2003 Conference on New interfaces For Musical Expression (Montreal, Quebec, Canada, May 22 -24, 2003), 
48-53. http://www.nime.org/2003/onlineproceedings/Papers/NIME03_Nagashima.pdf [Fels et al. 2004] Sidney 
S. Fels and Linda Kaastra and Sachiyo Takahashi and Graeme McCaig. Evolving Tooka: from Experiment to 
Instrument. 4rd International Conference on New Interfaces for Musical Expression (NIME04). Pages 1-6. 
May. 2004. [Fels &#38; Vogt, 2002] Sidney S. Fels and Florian Vogt. Tooka: Explorations of Two Person 
Instruments. 2nd International Conference on New Interfaces for Musical Expression (NIME02). Pages 116-121. 
May. 2002. [Carlile &#38; Hartmann, 2004] Carlile, J. and Hartmann, B. 2004. OROBORO: a collaborative 
controller with interpersonal haptic feedback. In Proceedings of the 2005 Conference on New interfaces 
For Musical Expression (Vancouver, Canada, May 26 ­28, 2005), 250-251. [Chadabe, 2002] Chadabe, J. 2002. 
The limitations of mapping as a structural descriptive in electronic instruments. In Proceedings of the 
2002 Conference on New interfaces For Musical Expression (Dublin, Ireland, May 24 -26, 2002). [Blaine 
&#38; Fels, 2003] Tina Blaine and Sidney S. Fels. Collaborative Musical Experiences for Novices. Journal 
of New Music Research. Volume 32. No. 4. Pages 411-428. Dec. 2003. [Blaine &#38; Perkis, 2000] Tina Blaine 
and Tim Perkis: The Jam-O-Drum Interactive Music System: A Study in Interaction Design. Symposium on 
Designing Interactive Systems 2000: 165-173 [Paradiso, 1999] Paradiso, J., The Brain Opera Technology: 
New Instruments and Gestural Sensors for Musical Interaction and Performance. Journal of New Music Research, 
1999. 28(2): p. 130--149. Resources: Theremin: wikipage: http://en.wikipedia.org/wiki/Theremin Oddmusic 
page: http://www.oddmusic.com/theremin/ Theremin enthusiast page: http://theremin.ca/ Where to buy: Moog 
Music -http://www.moogmusic.com/ Clara Rockmore video: http://www.youtube.com/watch?v=pSzTPGlNa5U More 
video of people playing Theremin: http://www.youtube.com/watch?v=h­3lU3bgOgE Hugh Le Caine: Electronic 
Sackbut: http://www.sciencetech.technomuses.ca/english/collection/music7.cfm Hugh Le Caine info site: 
http://www.hughlecaine.com Bucla s instruments: http://www.buchla.com/ Wikipage: http://en.wikipedia.org/wiki/Buchla 
Michel Waisvisz wikipage: http://en.wikipedia.org/wiki/Michel_Waisvisz 2005 talk at CHI2005: http://www.chi2005.org/program/prog_closing.html 
Lady s Glove, Laetitia Sonami http://www.sonami.net/lady_glove2.htm Video: http://www.sonami.net/Clips/VideoPerf_clips/China-Loose.mov 
Pebblebox video: http://www.youtube.com/watch?v=GEJCmrhrBjc http://www.sarc.qub.ac.uk/~somodhrain/palpable/projects.html#enactivemi 
Info on circuit bending: http://en.wikipedia.org/wiki/Circuit_bending Info on Cracklebox: http://www.crackle.org/ 
iPhone musical applications: Smule: http://ocarina.smule.com/ Tooka publications: NIME02: http://www.nime.org/2002/proceedings/paper/fels.pdf 
NIME04: http://www.nime.org/2004/NIME04/paper/NIME04_1A01.pdf OROBORO site: http://regexp.bjoern.org/archives/000159.html 
Jamodrum site: http://www.jamodrum.net/ Brain Opera information: http://park.org/Events/BrainOpera/ Penn 
playing with Brain Opera sensor chair video: http://www.media.mit.edu/~joep/MPEGs/penn.mpg Monome Open 
Hardware Interface http://monome.org  Module E Visual Interfaces [Tarabella, 2004] Leonello Tarabella, 
2004. Performance at NIME-04, Hamamatsu, Japan. [Fels &#38; Mase, 1999] Sidney S. Fels and Kenji Mase. 
Iamascope: A Graphical Musical Instrument. Computers and Graphics. Volume 2. No. 23. Pages 277-286. 1999. 
[Lyons, 2001] Lyons, M.J. 2001. Mouthersizer Video shown at NIME-01 workshop, Seattle, WA. [Lyons &#38; 
Tetsutani, 2001] Lyons, M.J. and Tetsutani, N. 2001, Facing the music: a facial action controlled musical 
interface, Conference on Human Factors in Computing Systems, (CHI 2001), Extended Abstracts, pp. 309 
310. [Lyons et al., 2003] Lyons, M.J. and Haehnel, M. and Tetsutani, N. 2003. Designing, playing, and 
performing with a vision-based mouth interface, Proceedings of the 2003 conference on New interfaces 
for musical expression pp. 116 121. [Funk et al. 2005] Funk, M. and Kuwabara, K. and Lyons, M.J. 2005. 
Sonification of facial actions for musical expression, Proceedings of the 2005 conference on New interfaces 
for musical expression, pp. 127 131. [Nishibori &#38; Iwai, 2006] Nishibori, Y. and Iwai, T., 2006. Tenori-On, 
Proceedings of the 2006 conference on New interfaces for musical expression, pp. 172 175 Module F 
Education [Cook, 2001] Cook, P. 2001. Principles for designing computer music controllers, Proceedings 
of the 2001 conference on New interfaces for musical expression, Seattle WA. [Cook, 2007] Cook, P. 2007, 
Keynote Talk at the 2007 Conference on New Interfaces for Musical Expression, New York, NY. [Jorda, 2004] 
Jorda, S. 2004. Digital instruments and players: part I---efficiency and apprenticeship, Proceedings 
of the 2004 conference on New interfaces for musical expression pp. 59 63. [Singer, 2003] Singer, E. 
2003. Sonic Banana: A novel bend-sensor-based midi controller, Proceedings of the 2003 conference on 
New interfaces for musical expression, pp. 220 221. Vienna Vegetable Orchestra http://www.gemueseorchester.org 
[Igoe, 2007] Igoe, T. 2007. Making Things Talk, O Reilly. [Verplank, Sapp, &#38; Mathews, 2001] Verplank, 
B. and Sapp, C. and Mathews, M., A Course on Controllers, Proceedings of the 2001 conference on New interfaces 
for musical expression, Seattle, WA. [Lippit, 2004] Lippit, T.M. 2004. Realtime sampling system for the 
turntablist version 2: 16padjoystickcontroller, Proceedings of the 2004 conference on New interfaces 
for musical expression, pp. 211 212. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1667251</article_id>
		<sort_key>120</sort_key>
		<display_label>Article No.</display_label>
		<display_no>12</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[The Digital Emily project]]></title>
		<subtitle><![CDATA[photoreal facial modeling and animation]]></subtitle>
		<page_from>1</page_from>
		<page_to>15</page_to>
		<doi_number>10.1145/1667239.1667251</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1667251</url>
		<abstract>
			<par><![CDATA[<p>This course describes how high-resolution face scanning, advanced character rigging, and performance-driven facial animation were combined to create Digital Emily, a believably photorealistic digital actor. Actress Emily O'Brien was scanned in the USC ICT light stage in 35 different facial poses using a new high-resolution face-scanning process capable of capturing geometry and textures down to the level of skin pores and fine wrinkles. These scans were assembled into a rigged digital character, which could then be driven by Image Metrics video-based facial animation technology. The real Emily was captured speaking on a small set, and her movements were used to drive a complete digital face replacement of her character, including its diffuse, specular, and animated displacement maps. HDRI lighting reconstruction techniques were used to reproduce the lighting on her original performance. The most recent results show new real-time animation and rendering research for the Digital Emily character.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor>Modeling methodologies</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010342.10010343</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis->Modeling methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1797135</person_id>
				<author_profile_id><![CDATA[81458649961]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Oleg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Alexander]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797136</person_id>
				<author_profile_id><![CDATA[81491640493]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rogers]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797137</person_id>
				<author_profile_id><![CDATA[81458647529]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lambeth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797138</person_id>
				<author_profile_id><![CDATA[81458642342]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Matt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chiang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797139</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>964604</ref_obj_id>
				<ref_obj_pid>964568</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baker, S., and Matthews, I. 2004. Lucas-kanade 20 years on: A unifying framework. <i>Int. J. Comput. Vision 56</i>, 3, 221--255.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>965470</ref_obj_id>
				<ref_obj_pid>965400</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Borshukov, G., and Lewis, J. P. 2003. Realistic human face rendering for 'The Matrix Reloaded'. In <i>ACM SIGGRAPH 2003 Sketches&amp;Applications.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Chang. 2007. Beowulf. <i>Variety</i> (Nov).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344855</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Debevec, P., Hawkins, T., Tchou, C., Duiker, H.-P., Sarokin, W., and Sagar, M. 2000. Acquiring the reflectance field of a human face. In <i>Proceedings of ACM SIGGRAPH 2000</i>, Computer Graphics Proceedings, Annual Conference Series, 145--156.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Debevec, P., Tchou, C., Gardner, A., Hawkins, T., Poullis, C., Stumpfel, J., Jones, A., Yun, N., Einarsson, P., Lundgren, T., Fajardo, M., and Martinez, P. 2004. Estimating surface reflectance properties of a complex scene under captured natural illumination. Tech. Rep. ICT-TR-06.2004, USC ICT, Marina del Rey, CA, USA, Jun. http://gl.ict.usc.edu/Research/reflectance/Parth-ICTTR-06.2004.pdf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280864</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Debevec, P. 1998. Rendering synthetic objects into real scenes: Bridging traditional and image-based graphics with global illumination and high dynamic range photography. In <i>Proceedings of SIGGRAPH 98</i>, Computer Graphics Proceedings, Annual Conference Series, 189--198.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Ekman, P., and Friesen, W. 1978. <i>Facial Action Coding System: A Technique for the Measurement of Facial Movement.</i> Consulting Psychologists Press, Palo Alto.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882368</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Fleishman, S., Drori, I., and Cohen-Or, D. 2003. Bilateral mesh denoising. <i>ACM Transactions on Graphics 22</i>, 3 (July), 950--953.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1409092</ref_obj_id>
				<ref_obj_pid>1409060</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Ghosh, A., Hawkins, T., Peers, P., Frederiksen, S., and Debevec, P. 2008. Practical modeling and acquisition of layered facial reflectance. <i>ACM Transactions on Graphics 27</i>, 5 (Dec.), 139:1--139:10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280822</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Guenter, B., Grimm, C., Wood, D., Malvar, H., and Pighin, F. 1998. Making faces. In <i>Proceedings of SIGGRAPH 98</i>, Computer Graphics Proceedings, Annual Conference Series, 55--66.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383575</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Hawkins, T., Wenger, A., Tchou, C., Gardner, A., G&#246;ransson, F., and Debevec, P. 2004. Animatable facial reflectance fields. In <i>Rendering Techniques 2004: 15th Eurographics Workshop on Rendering</i>, 309--320.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1198585</ref_obj_id>
				<ref_obj_pid>1198555</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Hyneman, W., Itokazu, H., Williams, L., and Zhao, X. 2005. Human face project. In <i>ACM SIGGRAPH 2005 Course #9: Digital Face Cloning</i>, ACM, New York, NY, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[ICT-Graphics-Laboratory, 2001. Realistic human face scanning and rendering. Web site. http://gl.ict.usc.edu/Research/facescan/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383319</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Jensen, H. W., Marschner, S. R., Levoy, M., and Hanrahan, P. 2001. A practical model for subsurface light transport. In <i>Proceedings of ACM SIGGRAPH 2001</i>, Computer Graphics Proceedings, Annual Conference Series, 511--518.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Kaufman, D. 1999. Photo genesis. <i>WIRED 7</i>, 7 (July). http://www.wired.com/wired/archive/7.07/jester.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Landis, H. 2002. Production-ready global illumination. In <i>Notes for ACM SIGGRAPH 2005 Course #16: RenderMan in Production</i>, ACM, New York, NY, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383873</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Ma, W.-C., Hawkins, T., Peers, P., Chabert, C.-F., Weiss, M., and Debevec, P. 2007. Rapid acquisition of specular and diffuse normal maps from polarized spherical gradient illumination. In <i>Rendering Techniques</i>, 183--194.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1409074</ref_obj_id>
				<ref_obj_pid>1409060</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Ma, W.-C., Jones, A., Chiang, J.-Y., Hawkins, T., Frederiksen, S., Peers, P., Vukovic, M., Ouhyoung, M., and Debevec, P. 2008. Facial performance synthesis using deformation-driven polynomial displacement maps. <i>ACM Transactions on Graphics 27</i>, 5 (Dec.), 121:1--121:10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Ma, W.-C. 2008. <i>A Framework for Capture and Synthesis of High Resolution Facial Geometry and Performance.</i> PhD thesis, National Taiwan University.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Mori, M. 1970. Bukimi no tani (the uncanny valley). <i>Energy 7</i>, 4, 33--35.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073226</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Nehab, D., Rusinkiewicz, S., Davis, J., and Ramamoorthi, R. 2005. Efficiently combining positions and normals for precise 3d geometry. <i>ACM Transactions on Graphics 24</i>, 3 (Aug.), 536--543.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Perlman, S. 2006. Volumetric cinematography: The world no longer flat. <i>Mova White Paper</i> (Oct).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Plantec, P. 2008. The digital eye: Image metrics attempts to leap the uncanny valley. <i>VFXWorld magazine</i> (August). http://www.vfxworld.com/?atype=articles&id=3723.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Robertson, B. 2009. What's old is new again. <i>Computer Graphics World 32</i>, 1 (Jan).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1186371</ref_obj_id>
				<ref_obj_pid>1186223</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Sagar, M., Monos, J., Schmidt, J., Ziegler, D., Foo, S.-C., Scott, R., Stern, J., Waegner, C., Nofz, P., Hawkins, T., and Debevec, P. 2004. Reflectance field rendering of human faces for Spider-man 2. In <i>SIGGRAPH '04: ACM SIGGRAPH 2004 Technical Sketches</i>, ACM, New York, NY, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1340320</ref_obj_id>
				<ref_obj_pid>1340315</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Wang, Y., Gupta, M., Zhang, S., Wang, S., Gu, X., Samaras, D., and Huang, P. 2008. High resolution tracking of non-rigid motion of densely sampled 3d data using harmonic maps. <i>Int. J. Comput. Vision 76</i>, 3, 283--300.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073258</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Wenger, A., Gardner, A., Tchou, C., Unger, J., Hawkins, T., and Debevec, P. 2005. Performance relighting and reflectance transformation with time-multiplexed illumination. <i>ACM Transactions on Graphics 24</i>, 3 (Aug.), 756--764.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Wolff, E. 2003. Creating virtual performers: Disney's human face project. <i>Millimeter magazine</i> (April).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>357025</ref_obj_id>
				<ref_obj_pid>357014</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Zhang, Z. 2000. A flexible new technique for camera calibration. <i>IEEE Trans. Pattern Anal. Mach. Intell. 22</i>, 11, 1330--1334.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Creating a Photoreal Digital Actor: The Digital Emily Project Oleg Alexander Mike Rogers William Lambeth 
Matt Chiang Paul Debevec Image Metrics USC Institute for Creative Technologies Figure 1: The Digital 
Emily Pipeline: Scanning, Scan processing, Rigging &#38; Animation, and Rendering Abstract The Digital 
Emily Project was a 2008 collaboration between facial animation company Image Metrics and the Graphics 
Laboratory at the University of Southern California s Institute for Creative Tech­nologies to achieve 
one of the world s .rst photorealistic digital facial performances. The project leveraged latest-generation 
tech­niques in high-resolution face scanning, character rigging, video­based facial animation, and compositing. 
By building an ani­matable face model whose expressions closely mirror the shapes observed in a rich 
set of facial scans, acquiring realistic skin re­.ectance maps, and faithfully driving the face by video 
of an actual performance, the project rendered a synthetic facial performance which was generally mistaken 
to be a real face. 1 Introduction Creating a photoreal digital actor with computer graphics has been 
a central goal of the .eld for at least thirty years [Parke 1972]. The Digital Emily project undertaken 
by Image Metrics and the Uni­versity of Southern California s Institute for Creative Technologies (USC 
ICT) attempted to achieve an animated, photoreal digital face by bringing together latest-generation 
results in 3D facial capture, modeling, animation, and rendering. The project aimed to cross the "uncanny 
valley" [Mori 1970], producing a computer-generated face which appeared to be a real, relatable, animated 
person. Some of the key technologies employed included a fast high-resolution digital face scanning process 
using the light stage at USC ICT, and the Image Metrics video-based facial animation system. The result 
of the project was by several accounts the .rst public demonstration of a photoreal computer-generated 
face able to convincingly speak and emote in a medium closeup. 1.1 Project Goals The central goal of 
the Digital Emily project was to animate and render a digital face able to address the key requirements 
of a digital actor for feature .lm production: to appear realistic from any view­point, any illumination, 
any expression, and animated to produce any performance. We based the face on a real living person, actress 
Emily O Brien, to allow direct comparison between a real facial performance and a synthesized version 
of it.  2 Previous Efforts at Photoreal Digital Humans A variety of notable efforts have been made 
to create realistic digi­tal actors over the last decade, each leveraging numerous advances in computer 
graphics technology and artistry. In this section, we overview some of these key efforts in order to 
compare and contrast them with the Digital Emily project. The SIGGRAPH 1999 Electronic Theater featured 
"The Jester" [Kaufman 1999], a short animation by Life/FX of a young woman in a dark environment reading 
abstract poetry in jester s cap cover­ing her head and ears. The actress used for the test (and author 
of the poetry) was Jessica Vallot, a dancer of African/Russian decent, whose face was three-dimensionally 
laser scanned and textured us­ing photographic textures stitched together with artistic effort to minimize 
the original shading and specular re.ectance effects, and her performance was recorded with a traditional 
arrangement of motion capture markers. Using an extension of research techniques developed at the University 
of Auckland and MIT, the motion cap­ture dots were used to drive a volumetric .nite element model which 
allowed a high-resolution facial mesh to produce simulated buckling and wrinkling from anisotropic stress 
at a scale signif­icantly more detailed than the original motion capture data was able to record. While 
the skin shading lacked realistic specular re.ectance, and pore detail, the face conveyed realistic motion 
and emotion in signi.cant part due to the skin dynamics model. The lack of noticeable specular re.ectance 
and skin translucency in the shading model was mitigated by the darker color of the actress skin and 
the plausibility that she may have been wearing makeup to cover skin pores. The lack of global illumination 
in the light­ing model was also likely mitigated by the darker skin tone (which would produce less interre.ection 
in facial concavities) and the fact that the performance took place in a dark void under spotlights rather 
than a more natural environment with both direct and ambi­ent illumination. Some bright regions under 
the eyes resulting from specular re.ections in the original face photography resulted in a somewhat odd 
appearance under the eyes, but the emotion content of the performance was clearly communicated and the 
animation, more successfully than not, looked like a real person. The Life/FX process employed in "The 
Jester" was also used in a little-seen test for an as-yet-unmade .lm version of "The Incredible Mr. Limpett", 
at the time to star Jim Carrey as a live-action version of the human-faced .sh originally voiced (and 
resembling) Don Knotts in the 1960 s television series. As for "The Jester", Mr. Car­rey was similarly 
photographed, scanned, modeled and animated, and rendered results of his animated face on a black background 
were notably compelling, especially given the realistic skin wrin­kling around the eyes during extreme 
transitions in expression. The test has not yet been publicly seen. Before the company s demise in the 
early 2000 s, the Life/FX process was used on the SIGGRAPH 2000 Electronic Theater short "Young At Heart", 
featuring Jessica Vallot playing a digitally aged version of the jester character. The Life/FX team consulted 
with cosmetic surgeons to sculpt a clay model of how Ms. Vallot might look in her 70 s, and based the 
elder Jessica s texture maps by editing the texture map from "The Jester" animation. Again, motion capture 
data of Jessica Vallot with a rel­atively dense set of markers was used to drive the elderly version 
of her performance, which was tracked and composited onto plate photography of Jessica shot on an live-action 
set. Despite many successful elements, the test struggled somewhat to meet its more ambitious challenges, 
namely that the face was no longer based on the appearance of a real person, and that the face needed 
to integrate onto a real actor in a real environment providing ample comparisons to reality in frame. 
In addition, the age of the character made the lack of skin pore and .ne crease detail implausible. In 
the end, these issues prevented "Young at Heart" from achieving a fully photore­alistic digital human, 
although the performance certainly conveyed notable emotion and expression. Disney s "Human Face Project" 
was an R&#38;D test for an as-yet­unproduced feature .lm that would show a recognizable older actor encountering 
a younger version of himself -a version already well known to audiences through his feature work [Wolff 
2003; Hyne­man et al. 2005]. Visual effects team member Price Pethel served as the test subject. A facial 
mold of Pethel s face was taken and the resulting cast was scanned using a the Arius 3D high-resolution 
face scanning process, and twenty-two Cyberware scans in various expressions were also acquired. A medium-resolution 
animated fa­cial rig was somewhat laboriously sculpted to mimic the expression scans used as reference. 
A multi-camera facial capture setup was employed to .lm the face under relatively .at cross-polarized 
light­ing. Polarization difference images isolating specular re.ections acquired as in [Debevec et al. 
2000] were used to reveal high­resolution texture detail for the face, and analysis of this texture was 
used to add skin pores and .ne wrinkle detail as displacement maps to the scans. Instead of driving the 
face with facial motion capture dots, a system which performed optical .ow on the performance was used 
to track video of Pethel s performance per pixel and drive the digital version of himself; achieving 
remarkably close matches to the original performance. In addition to making a photoreal ver­sion of Pethel 
at his current age, a younger version of Pethel was artistically constructed based on photographs of 
him in his 20 s. The .nal test was to show the digitally younger Pethel interacting side-by-side with 
real video of Pethel himself. High dynamic range lighting information was captured on set so that the 
digital Pethel could be rendered with image-based lighting [Debevec 1998] to match the on-set illumination 
of the live actor. Final renderings achieved relatively convincing facial motion and lighting in a two­shot 
but less convincing facial re.ectance; a signi.cant problem was that no simulation of the skin s translucency 
[Jensen et al. 2001] had been performed, the effects of which should have been readily apparent in the 
actor s relatively light-colored skin. The Matrix Sequels (2003) used digital actors in several key vi­sual 
effects sequences, most notably for sequences involving Neo (Keanu Reeves) acrobatically .ghting the 
self-replicating Agent Smith (Hugo Weaving), although some scenes were accomplished using prosthetics 
of Weaving s face worn by thinner-faced body doubles. As in Disney s Human Face Project, high-quality 
facial casts of Reeves and Weaving were acquired and scanned with a high-resolution laser scanning system 
(in this case, the XYZRGB system based on technology from the Canadian National Research Council) to 
provide 3D geometry accurate to the level of skin pores and .ne wrinkles. A six-camera high-de.nition 
facial capture rig was used to .lm facial performance clips of the actors under rela­tively .at illumination. 
The six views in the video were used both to animate a facial rig of the actor (in this case, driven 
by markerless optical .ow rather than facial markers) and also, as seen in Mi­crosoft Research s "Making 
Faces" project [Guenter et al. 1998], to provide time-varying texture maps for the dynamic facial ap­pearance. 
Still renderings using image-based lighting [Debevec 1998] and a texture-space approximation to subsurface 
scattering [Borshukov and Lewis 2003] showed notably realistic faces which greatly bene.tted from the 
high-resolution geometric detail tex­ture found in the XYZRGB scans. However, the animated facial performances 
in the .lm were shown in relatively wide shots and exhibited less realistic skin re.ectance, perhaps 
having lost some appearance of geometric detail during normal map .ltering. The animated texture maps, 
however, provided a convincing degree of dynamic shading and albedo changes to the facial performances. 
A tradeoff to the use of video textures is that the facial model could not easily generate novel performances 
unless they too were captured in the complete performance capture setup. Spider Man 2 (2004) built digital 
stunt doubles for villain Doc Ock (Alfred Molina) and hero Spider-Man (Tobey Maguire) using fa­cial re.ectance 
scans in USC ICT s Light Stage 2 device [Sagar et al. 2004]. Each actor was .lmed with four synchronized 
35mm .lm cameras in several facial expressions from 480 lighting di­rections. Colorspace techniques as 
in [Debevec et al. 2000] were used to separate diffuse and specular re.ections. The relightable texture 
information was projected onto a 3D facial rig based on geometry from a traditional laser scan, and illuminated 
variously by HDRI image-based lighting [Debevec 1998] and traditional CG light sources using a custom 
shading algorithm for approximately 40 digital double shots. Using additional cameras, the technique 
was also used to construct digital actors for Superman Returns (2006), Spider Man 3 (2007), and Hancock 
(2008). Due to the extensive re.ectance information collected, the technique yielded realistic facial 
re.ectance for the digital characters, including close­up shots with mild degrees of facial animation, 
especially in Super­man Returns. However, results of the process did not demonstrate emotive facial performances 
in closeup; signi.cant facial animation was shown only in wide shots. Although they were not human characters, 
Gollum and Davy Jones from the Lord of the Rings Sequels (2002, 2003) and Pirates of the Carribbean Sequels 
(2006, 2007) also advanced the level of achievement in digital characters. Key to the success of these 
characters was that real actors (Andy Serkis and Bill Nighy, re­spectively) played the characters in 
the plate photography. This allowed much of the physical, emotional, and photometric effects of the digital 
actor on the .lm s sets and human characters to nat­urally result from real actor s physical presence 
on the set. More importantly, the real actors provided rich sources of reference for the digital actor 
s movements, illumination, and facial performance, all derived from combinations of manual and automated 
techniques. The animation of the .nal characters bene.tted from notably com­plex and expressive facial 
rigs, facial tissue translucency through subsurface scattering [Jensen et al. 2001], and illumination 
using ambient occlusion approximations [Landis 2002] to environmental illumination. Beowulf (2007) used 
a multitude of digital characters for a fully computer-rendered .lm. The performance capture based .lm 
fol­lowing the approach of the 2001 .lm Final Fantasy of construct­ing as detailed characters as possible 
and then driving them with motion capture, keyframe animation, and simulation. Beowulf sub­stantially 
advanced the state of the art in this area by leveraging greater motion capture .delity and performance 
volume, employing more complex lighting simulation, and using better skin shading techniques. While some 
characters were based closely in appear­ance their voice and motion capture actors (e.g. Angelina Jolie 
and Anthony Hopkins), other characters bore little resemblance (e.g. Ray Winstone), requiring additional 
artistic effort to model them. Static renderings of the faces achieved impressive levels of realism, 
although the single-layer subsurface scattering model produced a somewhat "waxy" appearance. Also, the 
artist-driven character creation process did not leverage high-resolution 3D scans of each actor in a 
multitude of facial expressions. According to Variety [Chang 2007], the "digitized .gures in Beowulf 
look eerily close to storefront mannequins ... suspended somewhere between live­action and animation, 
fairy tale and videogame." The Curious Case of Benjamin Button, released Christmas Day 2008, was the 
.rst feature .lm to feature a photoreal human vir­tual character. The .lm s aged version of Brad Pitt 
 seen in the .lm s .rst 52 minutes was created by visual effects studio Digital Domain and "leveraged 
recently developed offerings from Mova, Image Metrics, and the [USC] Institute for Creative Technologies" 
[Robertson 2009]. Silcone maquettes of Brad Pitt as an old man were constructed by Kazuhiro Tsuji at 
Rick Baker s makeup stu­dio and used as the basis for the digital character. Detailed facial re.ectance 
capture was performed for the age 70 maquette in USC ICT s Light Stage 5 device [Wenger et al. 2005] 
from eight angles and 156 lighting directions. Medium-resolution meshes of Brad Pitt s face in a multitude 
of facial expressions were captured us­ing Mova s Contour facial capture system [Perlman 2006], using 
a mottled pattern of glow-in-the-dark makeup to create facial geom­etry from multi-view stereo. Digital 
Domain used special software tools and manual effort to create a rigged digital character whose articulations 
were based closely on the facial shapes captured in the Mova system. Image Metrics video-based facial 
animation sys­tem was used to provide animation curves for the Benjamin facial rigs by analyzing frontal, 
.at-lit video of Brad Pitt performing each scene of the .lm in a studio; these animation curves were 
frequently re.ned by animators at Digital Domain to create the .nal perfor­mances seen in the .lm. Extensive 
HDRI documentation was taken on each .lm set so that advanced Image-Based Lighting techniques based on 
[Debevec 1998] could be used to render the character with matching illumination to the on-set lighting. 
Image-based relight­ing techniques as in [Debevec et al. 2000] were used to simulate the re.ectance of 
the scanned maquette in each illumination envi­ronment as cross-validation of the digital character s 
lighting and skin shaders. The .nal renderings were accurately tracked onto the heads of slight, aged 
actors playing the body of Benjamin in each scene. The quality of the character was universally lauded 
as a breakthrough in computer graphics, winning the .lm an Academy Award for Best Visual Effects. An 
estimate of more than two hun­dred person-years of effort was reportedly spent [Robertson 2009] creating 
the character in this groundbreaking work.  3 Pipeline Overview The major steps executed in the Digital 
Emily project were: 1. Acquiring high-resolution scans of the actor in various facial expressions 2. 
Building a rigged digital character from the scans 3. Video-based facial animation 4. Tracking, Lighting, 
Rendering, and Compositing  The next four sections of this document describe these parts of the project 
in detail.  4 Acquiring high-resolution scans of the actor in various facial expressions Emily O Brien, 
an Emmy-nominated actress from the American daytime drama "The Young and the Restless", was cast for 
the project. After being .lmed seated describing aspects of the Image Metrics facial animation process 
on an informal studio set, Emily came to USC ICT to be scanned in its Light Stage 5 device on the afternoon 
of March 24, 2008. A set of 40 small facial dots were applied to Emily s face with a dark makeup pencil 
to assist with fa­cial modeling. Emily then entered Light Stage 5 for approximately 90 minutes during 
which data for thirty-seven high-resolution facial scans were acquired. Fig. 2 shows Emily in the light 
stage during a scan, with all 156 of its white LED lights turned on.  The light stage scanning process 
used for Digital Emily was de­scribed in [Ma et al. 2007]. In contrast to earlier light stage pro­cesses 
(e.g. [Debevec et al. 2000; Hawkins et al. 2004]), which photograph the face under hundreds of illumination 
directions, this newer capture process requires only .fteen photographs of the face under different lighting 
conditions as seen in Fig. 3 to capture ge­ometry and re.ectance information for a face. The photos are 
taken with a stereo pair of Canon EOS 1D Mark III digital still cameras, and the images are suf.ciently 
few so that they can be captured in the cameras "burst mode" in under three seconds, before any data 
needs to be written to the compact .ash cards. 4.1 Estimating Subsurface and Specular Albedo and Nor­mals 
Most of the images are shot with essentially every light in the light stage turned on, but with different 
gradations of brightness. All of the light stage lights have linear polarizer .lm placed on them, af.xed 
in a special pattern of orientations, which permits the mea­surement of the specular and subsurface re.ectance 
components of the face independently by changing the orientation of a polarizer on the camera. The top 
two rows of Fig. 3 show Emily s face under four spherical gradient illumination conditions and then a 
point-light condition, and all of the images in this row are cross-polarized to eliminate the shine from 
the surface of her skin her specular component. What remains is the skin-colored "subsurface" re.ection, 
often re­ferred to as the "diffuse" component. This is light which scatters within the skin enough to 
become depolarized before re-emerging. Since this light is depolarized, approximately half of this light 
can pass through the horizontal polarizer on the camera. The top right image is lit by a frontal .ash, 
also cross-polarizing out the specular re.ection. The middle row of Fig. 3 shows parallel-polarized images 
of the face, where the polarizer on the camera is rotated vertically so that the specular re.ection returns, 
in double strength compared to the attenuated subsurface re.ection. We can then reveal the specular re.ection 
on its own by subtracting the .rst row of images from the second row, yielding the specular-only images 
shown in Fig. 4. Figure 4: Emily s specular component under the four gradient lighting conditions (all, 
left, top, front) and single frontal .ash con­dition, obtained by subtracting the cross-polarized images 
from the parallel-polarized images. Fig. 5(a) is a closeup of the "diffuse-all" image of Emily. Every 
light in the light stage is turned on to equal intensity, and the po­larizer on the camera is oriented 
to block the specular re.ection from every single one of the polarized LED light sources. Even the highlights 
of the lights in Emily s eyes are eliminated. This is about as .at-lit an image of a person s face as 
can con­ceivably be photographed, and thus it is nearly a perfect image to use as the diffuse texture 
map for the face in building a digital actor. The one issue is that it is affected to some extent by 
self­shadowing and interre.ections, making the concavities around the eyes, under the nose, and between 
the lips appear somewhat darker and more color-saturated than it inherently is. Depending on the (a) 
 (b) (c)   Figure 5: Closeups of Emily s (a) cross-polarized subsurface com­ponent, (b) parallel-polarized 
subsurface plus specular component, and (c) isolated specular component formed by subtracting (a) from 
(b). The black makeup dots on her face are easily removed digitally and help with aligning and corresponding 
her scans. rendering technique chosen, having these effects of occlusion and interre.ection "baked in" 
to the texture map is either a problem or an advantage. For real-time rendering, the effects can add 
real­ism (as if the environmental light were entirely uniform) given that simulating them accurately 
might be computationally prohibitive. If new lighting is being simulated on the face using a more accurate 
global illumination technique, then it is problematic to calculate self-shadowing of a surface whose 
texture map already has self­shadowing present; likewise for interre.ections. In this case, one could 
perform inverse rendering by using the actor s 3D geometry and approximate re.ectance to predict the 
effects of self-shadowing and/or interre.ections, and then divide these effects out of the tex­ture image 
as in [Debevec et al. 2004]. Fig. 5(a) also shows the makeup dots we put on Emily s face which help us 
to align the images in the event there is any drift in her position or expression over the .fteen images; 
they are relatively easy to remove digitally. Emily was extremely good at staying still for the three-second 
scans and many of her datasets required no motion compensation at all. (Faster capture times are already 
pos­sible: 24fps capture of such data using high-speed video cameras is described in [Ma et al. 2008]). 
The shinier image in Fig. 5(b) is also lit by all of the light stage lights, but the orientation of the 
polarizer has been turned 90 de­grees which allows the specular re.ections to return. Her skin exhibits 
a specular sheen, and the re.ections of the lights are now evident in her eyes. In fact, the specular 
re.ection is seen at double the strength of the subsurface (or diffuse) re.ection, since the polar­izer 
on the camera blocks about half of the unpolarized subsurface re.ection. Fig. 5(b) shows the combined 
effect of specular re.ection and subsurface re.ection. For modeling facial re.ectance, we would ideally 
observe the specular re.ection independently. As a useful alternative, we can simply subtract the diffuse-only 
image Fig. 5(a) from this one. Taking the difference between the diffuse-only image and the diffuse-plus-specular 
image yields an image of primarily the specular re.ection of the face as in 5(c). A polarization dif­ference 
process was used previously for facial re.ectance analysis in [Debevec et al. 2000], but only for a single 
point light source and not for the entire sphere of illumination. The image is mostly colorless since 
this light has re.ected specularly off the surface of the skin, rather than entering the skin and having 
its blue and green colors signi.cantly absorbed by skin pigments and blood before re.ecting back out. 
This image provides a useful starting point for building a digital character s specular intensity map, 
or "spec map"; it shows for each pixel the intensity of the specular re.ection at that pixel. However, 
the specular re.ection becomes ampli.ed near grazing angles, such as at the sides of the face according 
to the denominator of Fresnel s equations. We generally model and compensate for this effect using Fresnel 
s equations but also discount regions of the face at extreme grazing angles. The image also includes 
some of the effects of "re.ection occlusion" [Landis 2002]. The sides of the nose and innermost contour 
of the lips appear to have no specular re.ection since self-shadowing prevents the lights from re.ecting 
in these an­gles. This effect can be an asset for real-time rendering, but should be manually painted 
out for of.ine rendering. Recent work [Ghosh et al. 2008] reports that this sort of polariza­tion difference 
image also contains the effects of single scattering, wherein light refracts into the skin but scatters 
exactly once before refracting back toward the camera. Such light can pick up the color of the skin s 
melanocytes, adding some color to the specular image. However, the image is dominated by the specular 
component s .rst­surface re.ection, which allows us to reconstruct high-resolution facial geometry. The 
four difference images of the face s specular re.ection under the gradient illumination patterns (Fig. 
4) let us derive a high­resolution normal map for the face: a map of its local surface orientation vector 
at each pixel. If we examine the intensity of one pixel across this four-image sequence, its brightness 
in the X, Y, and Z images divided by its brightness in the fully-illuminated image uniquely encodes the 
direction of the light stage re.ected in that pixel. From the simple formula in [Ma et al. 2007] involving 
image ratios, we can derive the re.ection vector at each pixel, and from the camera orientations (calibrated 
with the technique of [Zhang 2000]) we also know the view vector. Figure 6: The specular normal map 
for one of Emily s expressions derived from the four specular re.ection images under the gradient illumination 
patterns. Computing the vector halfway between the re.ection vector and the view vector yields a surface 
normal estimate for the face based on the specular re.ection. Fig. 6 shows the face s normal map visualized 
using the common color map where red, green and blue indicate the X, Y, and Z components of the surface 
normal. The normal map contains detail at the level of skin pores and .ne wrin­kles. The point-lit polarization 
difference image (Fig. 4, far right) provides a visual indication of the BRDF s specular lobe shape on 
the nose, forehead, cheeks, and lips. This image provides visual reference for choosing specular roughness 
parameters for the skin shaders. The image can also be used to drive a data-driven specular re.ectance 
model as in [Ghosh et al. 2008].  4.2 Deriving High-Resolution 3D Geometry The last set of images in 
the scanning process (Fig. 3, bottom row) are a set of colored stripe patterns from a video projector 
which allow a stereo correspondence algorithm to robustly compute pixel correspondences between the left 
and right viewpoints of the face. The projector is also cross-polarized so that the stereo pair of images 
consist of only subsurface re.ection and lack specular highlights; such highlights would shift in position 
between the two viewpoints and thus complicate the stereo correspondence process. The patterns form a 
series of color-ramp stripes of different fre­quencies so that a given facial pixel receives a unique 
set of RGB irradiance values over the course of the sequence [Ma 2008]. The .rst projected image, showing 
full-on illumination, is used to di­vide out the cross-polarized facial BRDF from the remaining stripe 
images; this also helps ensure that pixels in one camera have very nearly the same pixel values as in 
the other camera, facilitating correspondence. From these correspondences and the camera cali­bration 
[Zhang 2000], we can triangulate a three-dimensional mesh of the face with vertices at each pixel, applying 
bilateral mesh de­noising [Fleishman et al. 2003] to produce a smooth mesh without adding blur to geometric 
features. However, the surface resolu­tion observable from the diffuse re.ection of skin is limited by 
the scattering of the incident light beneath the skin. As a result, the geometry appears relatively smooth 
and lacks the skin texture detail that we wish to capture in our scans. We add in the skin texture detail 
by embossing the specular normal map onto the 3D mesh. By doing this, a high-resolution version of the 
mesh is created and the vertices of each triangle are allowed to move forward and back until they best 
exhibit the same surface normals as the normal map. The ICT Graphics Lab .rst described this process 
 using diffuse normals estimated in Light Stage 2 in [ICT-Graphics-Laboratory 2001] 2001; more recent 
work in the area includes Nehab et al. [2005].) This creates a notably high­resolution 3D scan, showing 
different skin texture detail clearly observable in different areas of the face (Fig. 7).  4.3 Scanning 
a Multitude of Expressions Emily was captured in thirty-three different facial expressions based loosely 
on Paul Ekman s Facial Action Coding System (FACS) [Ekman and Friesen 1978] as seen in Fig. 8; fourteen 
of these individual scans are shown in Fig. 9. By design, there is a great deal of variety in the shape 
of her skin and the pose of her lips, eyes, and jaw across the scans. Emily was fortunately very good 
at staying still for all of the expressions. Two of the expressions, one with eyes closed and one with 
eyes open, were also scanned from the sides with Emily s face rotated to the left and right as seen in 
the inset .gures of the neutral-mouth-closed and neutral­mouth-open scans. This allowed us to merge together 
a 3D model of the face with geometry stopping just short of her ears to create the complete "master mesh" 
(Fig. 13(a)) and to extrapolate full ear­to-ear geometry for each partial (frontal-only) facial scan 
as in Fig. 16(c).  Figure 8: The thirty-three facial expressions scanned for creating the Digital Emily 
character. We note that building a digital actor from 3D scans of multiple fa­cial expressions is a commonly 
practiced technique; for example, the Animatable Facial Re.ectance Fields project [Hawkins et al. 2004] 
followed this approach in scanning actress Jessica Vallot in approximately forty facial expressions. 
Going further back, visual effects company Industrial Light + Magic acquired several Cyber­ware 3D scans 
of actress Mary Elizabeth Mastrantonio in different expressions to animate the face of the water creature 
in 1989 s The Abyss.  4.4 Phenomena Observed in the Facial Scans The fourteen faces in Fig. 9 show a 
sampling of the high-resolution scans taken of Emily in different facial expressions, and offer an opportunity 
to observe three-dimensional facial dynamics in more detail than has been previously easy to do. A great 
deal of dy­namic behavior can be observed as a face moves, exempli.ed in the detailed images in Fig. 
10. For example, the skin pore detail on the cheek in Fig. 10(a) changes dramatically when Emily pulls 
her mouth the side in Fig. 10(b): the pores signi.cantly elongate and become shallower. When Emily stretches 
her face vertically, small veins pop out from her eyelid Fig. 10(c). (a) (b) (c) (d) (e) (f)   When 
Emily raises her eyebrows, the relatively isotropic skin pores of her forehead transform into rows of 
.ne wrinkles in Fig. 10(d) her skin is too elastic to develop deep furrows. When she scowls in Fig. 
10(e), indentations above her eyebrows appear where her muscles attach beneath the skin. And when she 
winces in Fig. 10(f), the muscles in her forehead bulge out. Examining the progression through Figs. 
10(d,e,f), we see how the bridge of Emily s nose signi.cantly shrinks and expands as the rest of the 
face pushes and pulls tissue into and out of the area. While we may not consciously notice these kinds 
of phenomena when interacting with others, they are present and visible in all faces, and failing to 
reproduce them accurately imperils the realism of a digital character. (a) (b) Figure 7: (a) Facial 
geometry obtained from the diffuse (or subsurface) component of the re.ection. (b) Far more detailed 
facial geometry obtained by embossing the specular normal map onto the diffuse geometry. Figure 9: High-resolution 
3D geometry from fourteen of the thirty-three facial scans. Each mesh is accurate to 0.1mm resolution 
and contains approximately three million polygons. 4.5 Scanning Emily s Teeth Finally, we also scanned 
a plaster cast of Emily s teeth, which re­quired adapting the 3D scanning system to work with greater 
ac­curacy in a smaller scanning volume. Fig. 11(a) shows the cast and Fig 11(b) shows a rendering of 
Emily s digital teeth model; the upper and lower teeth each were the result of merging eight sinusoid-pattern 
structured light scans to form meshes with approx­imately 600,000 polygons.  (a) (b) (c)  5 Building 
the digital character from the scans (a) (b) 5.1 Constructing the Animatable Base Mesh The stitched 
ear-to-ear neutral-expression scan of Emily was remeshed to create a 4,000 polygon animatable mesh as 
seen in Figure 12(a). This drastic polygon reduction from the several mil­lion polygons of the original 
scan was done to make animation of the mesh tractable and to ease corresponding the geometry across scans; 
the geometric skin texture detail would be added back us­ing displacement maps calculated from the high-resolutions 
scans. This was done principally using the commercial product ZBrush to create the facial topology and 
then Autodesk s Maya package to create the interior of the mouth and the eye sockets. Then, UV texture 
coordinates for the neutral animatable mesh were mapped out in Maya, yielding the complete master mesh. 
 8 5.2 Building the Blendshapes Image Metrics originally planned to use the scans captured in the Light 
Stage as artistic reference for building the blendshapes. How­ever, the scans proved to be much more 
useful than just reference. The tiny stabilization dots drawn on Emily s face during the scan­ning session 
were visible in every texture map of every scan. Rather than sculpt the blendshape meshes artistically 
and then project (or "snap") the vertices to the corresponding scans, we used the dots directly to warp 
the neutral animatable mesh into the different ex­pressions. This achieved not only an accurate shape 
for each blend­shape, but also accurate skin movement between blendshapes. The challenge in constructing 
blendshapes from these data was to use the sparse set of stabilization dots to .nd a dense correspon­dence 
between the animatable master mesh and each expression scan. Figure 13 shows an example of three such 
partial frontal scans alongside the animatable master mesh.  (a) (b) (c) (d) Figure 13: (a) Ear-to-ear 
master mesh with (b,c,d) three partial expression scans to be used as blend shapes. Although highly detailed 
and accurate, the expression scans re­quired some pre-processing before appropriate blendshapes could 
be constructed. For example, many meshes have irregular edges with poor triangulation and mesh artifacts 
around the teeth and eye regions. Also, many of the scans contained surface regions not rep­resented 
in the master mesh, such as the surface of the eyes and teeth. These regions should not be corresponded 
with master mesh vertices. Finally, the scans, captured from a frontal stereo camera pair, did not cover 
the full facial region of the animatable master mesh. Figures 14 and 15 show examples of each of these 
issues. These aspects of the data meant that a fully automatic correspon­dence algorithm would be unlikely 
to provide adequate results. The correspondence method we used required some simple man­ual data cleaning 
and annotation to produce data more amenable to automatic processing. For each expression scan, we removed 
regions from both the master mesh and the expression mesh to achieve rough edge consistency. For example, 
we removed the neck region from the expression scans and face sides from the master to produce consistent 
facial coverage. We also removed uncorre­spondable regions from the meshes by deleting vertices; these 
were commonly the teeth and eyeballs regions of the expression scans, as such regions are not present 
in the master mesh. This process also removed most of the mesh artifacts associated with discontinu­ities 
in the expression scans. The 3D locations of the stabilization dots were annotated manually and served 
as a sparse set of known points of correspondence between master mesh and each expression scan. For each 
expression, this sparse correspondence information was used to initialize and stabilize a proprietary 
automatic method of determining a dense correspondence. The method used a 3D spatial location and mesh 
normal agreement measure within a 2D conformal mapping frame (such as in [Wang et al. 2008]) to obtain 
the required dense correspondence. This process resulted in a map­ping of each master mesh vertex to 
a position on the surface of the appropriate expression mesh.  Figure 14: Mesh artifacts and irregular 
edges in the raw Emily scans With the correspondence between our partial, cleaned-up meshes it was possible 
to calculate the shape change required to transform the neutral master mesh to each expression. However, 
a rigid trans­lation/rotation between the master and expression scans was also generally present in the 
data. This rigid transformation must be cal­culated to ensure only motion due to the changing facial 
expression and not residual head motion is represented in each blendshape. To do this, a 3D rigid transformation 
for each expression was cal­culated using a manually selected subset of texture markers. The texture 
markers selected were chosen independently for each ex­pression so as to be the least affected by the 
skin and jaw motion for that expression, and thus provide a good basis for calculating a rigid transformation 
between scans. Once the rigid transformations were calculated and was factored out of each blendshape, 
blendshape deltas for each vertex in the partial master mesh were calculated. This produced partial blendshapes 
as shown in Figure 16(a,b). Any jaw movement had to be subtracted from each blendshape; this step was 
a very important because it eliminated redundancy in the rig. One example was the lipSuck shape where 
both top and bottom lips are sucked inside the mouth. In order to capture the maximum amount of data 
for the lips, this expression was scanned and modeled with the jaw slightly open. After the blendshape 
was (a) (b) (c) Figure 16: (a) A partial master mesh with (b) a partial blendshape expression mesh. 
(c) A complete expression scan, created by .nding correspondences from (b) to (a) to the complete master 
mesh. modeled the jaw movement was subtracted by going negative with the jawOpen shape. A custom algorithm 
was used to map the partial blendshapes onto the full master mesh. The missing parts of the full master 
mesh that were not observed in the expression scan were interpolated using the assumption that the extreme 
edges of the face remained static, which we found to be a reasonable assumption for most expres­sions. 
Where this assumption was not appropriate, the results were artistically corrected to give the desired 
shape. The unknown inter­nal parts of the master mesh, such as the inner mouth and lips, were interpolated 
to move appropriately with the known vertices. Figure 16(c) illustrates the result of this interpolation 
process. The .nal results of the automatic blendshape creation process were then artistically cleaned 
up by a small team of rigging artists to provide a full set of quality-assured expression blend shapes. 
After the blendshape modeling and cleanup process, Image Metrics ended up with approximately 30 "pre-split" 
blendshapes, one blend­shape corresponding to each scan. However, most of the scans were captured with 
"doubled up" facial expressions. For example, the browRaise and chinRaise were captured in the same scan. 
This was done to get through the scanning session quicker and to save processing time. But now the blendshapes 
had to be split up into localized shapes the "post-split" shape set. To do this, we used the Paint Blendshape 
Weights feature in Maya, creating a set of normal­ized "split maps" for each facial region. For example, 
this is how the browRaise blendshape was split up. Three normalized split maps were painted for the browRaise 
blendshape: browRaise_L, browRaise_C, and browRaise_R, roughly following the eyebrow raisers described 
in [Ekman and Friesen 1978]. A custom MEL script was then run which applied each split map to the browRaise 
shape in the Paint Blendshape Weights tool. The left, center, and right browRaise blendshapes were then 
duplicated out. Splitting up the browRaise shapes in this way allowed the animator to con­trol every 
region of the brows independently. However, because the split maps were normalized, turning on all three 
regions together summed to the original pre-split browRaise shape. All the pre­split blendshapes underwent 
this process resulting in a post-split shape set of roughly 75 blendshapes. This shape set gave the Image 
Metrics animators an unprecedented amount of control over each region of the face.  5.3 The Facial Rig 
s User Interface The rig user interface (Fig. 17) was inspired by the facial muscles and their "direction 
of pull". Many of the controls were represented as arrow-shaped NURBS curves, with each arrow representing 
a certain facial muscle. Pulling on the arrows toward its point was equivalent to contracting those muscles. 
Additional animation con­trols were placed in the Maya channel box allowing numerical in­put.   5.4 
Soft Eyes and Sticky Lips The Emily facial rig included two notable special effects: soft eyes and sticky 
lips. Soft eyes is the effect of the rotation of the cornea pushing and tugging the eyelids and skin 
around the eye. The soft eyes setup created for Emily was relatively simple in that each eye had a separate 
blendshape node with four shapes: lookUp, lookDown, lookLeft, and lookRight. The envelope of this blendshape 
node was negated by the blink shape. In other words, whenever the eye blinked, the soft eyes effect was 
turned off. This was done to prevent a con.ict between the blink and lookDown blendshapes. Sticky lips 
is the subtle effect of the lips peeling apart during speech, and has been a part of several high-end 
facial rigs including the Gollum character built at WETA Digital for the Lord of the Rings sequels. The 
Emily rig had a relatively elaborate sticky lips setup involving a series of Maya deformers which provided 
anima­tors a "sticky" control for both corners of the lips.  5.5 Adding Blendshape Displacement Maps 
In order to preserve as much of the scan data as possible, the Emily render had 30 animated displacement 
maps. The displacement maps were extracted using Pixologic s ZBrush software by plac­ing each pre-split 
blendshape on top of its corresponding scan and calculating the difference between the two. Then each 
displace­ment map was cleaned up in Photoshop and divided according to the same normalized split maps 
used to split the blendshapes. This yielded a displacement map for each blendshape in the rig (although 
only a subset of these were used in the .nal renderings). It is im­portant to note that the highest frequency, 
pore-level detail displace­ment map came only from the neutral scan. All the other displace­ment maps 
had a median .lter applied to them to remove any high frequency detail, while still keeping the wrinkles. 
This was done because adding two maps with high frequency detail would result in the pores "doubling 
up" in the render, since the high-resolutions scans had not been aligned to each other at the level of 
skin pores and .ne wrinkles. Therefore, only the neutral displacement map had the pore detail; all the 
other maps had only wrinkles without pores. The displacement map animation was driven directly by the 
corresponding blendshape animation. 10 5.6 Adding the Teeth Plaster casts of Emily s upper and lower 
teeth were made using standard dental casting techniques. The two casts were scanned with a structured 
light scanning system to produce two 600,000 polygon meshes from sixteen merged scans acquired using 
struc­tured lighting patterns based on Gray codes. These teeth scans were manually remeshed to have far 
fewer triangles and smoother topol­ogy. The top and bottom teeth meshes were remeshed to 10,000 polygons 
each as in Fig. 11(c). We then carefully placed the teeth geometry inside the neutral mesh using the 
high-resolution smile scans as reference for the teeth positions. The teeth scanning was done relatively 
late in the facial model con­struction process. Until the teeth were scanned, a generic teeth model was 
used in the animated character. We found this to be signi.cantly less believable than using the model 
of Emily s actual teeth.  6 Video-Based Facial Animation Digital Emily s facial animation was created 
using Image Metrics proprietary video analysis and animation system which allows an­imators to associate 
character poses with a small subset of perfor­mance frames. The analysis of the performance requires 
video from a single standard video camera and is designed to capture all the characteristics of the actor. 
The animation technology then uses the example poses provided by an animator to generate predictions 
of the required character pose for each frame of the performance. The animator can then iteratively re.ne 
this prediction by adding more example poses until the desired animation is achieved. As the process 
is example driven, it presents a natural framework to allow artistic and stylistic interpretation of 
an actor s performance, for example when using a human to drive a cartoon or animal charac­ter. However, 
in this case the technology was used for the purpose of producing a faithful one-to-one reproduction 
of Emily s perfor­mance. The Image Metrics facial animation process has several advantages over traditional 
performance-driven animation techniques which signi.cantly added to the realism of the digital character. 
First, the process is based on video of the actor performing, which provides a great deal of information 
regarding the motion of the actor s eyes and mouth; these are problematic areas to capture faithfully 
with traditional motion capture markers, but are the most important part of the face for communicating 
an actor s performance. Second, the process leverages an appropriate division of labor between the ani­mator 
and the automated algorithms. Because an artist is part of the process, they can ensure that each of 
the key animation poses reads in an emotionally faithful way to the appearance of the actor in the corresponding 
frame of video. This process would be dif.cult to automate, since it involves reading and comparing the 
emotional content of faces with signi.cant detail. Conversely, the trajectories and timing of the facial 
motion between key poses are successfully derived from the automated video analysis; achieving realistic 
tim­ing in photoreal facial animation requires a great deal of skill and time for an animator to achieve 
manually. As a result, the process combines what an animator can do quickly and well with what an automatic 
process can accomplish, yielding facial animation with higher quality and greater ef.ciency than either 
fully manual or fully automatic techniques currently allow.   7 Tracking, Lighting, Rendering, and 
Com­positing Emily s performance was shot from two angles using high­de.nition cameras: a front-on closeup 
for use in the Image Metrics analysis pipeline, and a medium shot in a three-quarter view to provide 
the background plate for the .nal video. The .nal renderings needed to be match-moved to Emily s head 
position in the three-quarter shot. Complicating the process (but representative of many practical production 
scenarios) the cameras were uncalibrated, and there were no markers on Emily s face to assist in tracking. 
Match moving requires sub-pixel accuracy and temporal consistency of pose, otherwise unwanted ".oating" 
effects become easily visible. To achieve this, we developed a manually guided match-moving process. 
We manually set the 3D pose of the animated character on several example frames so that the projected 
location of the model closely matched that of Emily in the video frame. Figure 18 shows the Maya scene 
used to allow 3D pose annotation. Using these frames as templates, we applied a model-based optical .ow 
algorithm [Baker and Matthews 2004] to calculate the required character pose in the intermediate frames. 
To ensure a smooth temporal path within the pose space, we developed a novel weighted combination of 
template frames that smoothly favors the temporally closest set of templates. Any errors were manually 
corrected and added as a new template to derive a new pose-space tracking result until the results no 
longer exhibited artifacts. Since Emily s face would be composited onto live-action back­ground plate 
including real video of her hair, ears, neck, and body, it was imperative for the rendered version of 
Emily s face to look completely convincing. Re.ning the rendering parameters took a rendering artist 
approximately three months to perfect. The render­ing was done in Mental Ray using the Fast Subsurface 
Scattering skin shader. The lighting was based on an high dynamic range light probe image captured on 
the day of the shoot [Debevec 1998] as seen in Figure 19. Many different passes were rendered, includ­ing 
diffuse, specular, matte, and contour passes. In particular, the contour pass (Fig. 20) was very important 
for visually integrating (or "marrying") the different components of facial geometry using different 
amounts of blur in the composite. Otherwise, for example, the line where the eyelid meets the eyeball 
would appear too sharp and thus unrealistic. Compositing was done in the Eyeon Fusion package. Emily 
s .ngers were rotoscoped whenever she moved them in front of her face so they could also obscure her 
digital face. Small paint .xes were done to the .nal renderings around the eye highlights. Two frames 
from a .nal Emily animation are shown in Fig. 21.  8 Reception In conjunction with its premiere at 
the Image Metrics booth in the SIGGRAPH 2008 exhibit hall at the Los Angeles Convention Center, the Digital 
Emily project received widespread attention in the media, most notably by over one million viewers of 
the Digital Emily video on the popular video-sharing web site YouTube. Other notable media coverage included 
The Wall Street Journal, New Scientist, Spiegel, Studio Daily, FX Guide TV, TG Daily, Mahalo Daily, the 
Discovery Channel, Lab TV, and an upcoming Frontline special report by David Kushner. Several reports 
by industry pro­fessionals credited the project with having convincingly crossed the "uncanny valley" 
and being the .rst project to do so: "It is absolutely awesome amazing. I m one of the toughest critics 
of face capture, and even I have to admit, these guys have nailed it. This is the .rst virtual human 
animated sequence that completely bypasses all my subconscious warnings. I get the feeling of Emily as 
a person. All the subtlety is there. This is no hype job, it s the real thing ... I of.cially pronounce 
that Image Metrics has .nally built a bridge across the Uncanny Valley and brought us to the other side." 
 Peter Plantec, " The Digital Eye: Image Metrics Attempts to Leap the Uncanny Valley", VFXWorld, August 
7, 2008 [Plantec 2008] "Emily is a truly monumental achievement, recreating every nu­ance of human facial 
expression, even though what you re actually looking at is the face of a digital actor. Created [by Image 
Metrics] through a partnership with USC s Institute for Creative Technolo­gies (ICT), the team s primary 
objective was to create a completely  convincing, animated computer-generated face, and I think they 
succeeded." Paul Strauss, Technabob.com, August 20, 2008 "When I .rst saw the demo for Project Emily, 
I thought it was just me there on video capture. But then they started removing my skin, it was creepy, 
but I realized that was the Digital Me! I was really amazed." Emily O Brien, Actress 9 Discussion Some 
timeframes and personnel for the Digital Emily Project were: Scanning: one afternoon, 1.5 hours with 
actor in stage, 3 sec­onds per scan, 3 technicians  Scan Processing: 10 days, 37 processed scans, 1 
artist  Rig Construction: 3 months, 75 blendshapes, 1 artist  Animation: 2 weeks, 90 seconds of animation, 
2 animators  Rendering/Compositing: 3 months, 1 artist  Output: 24fps, 1920x1080 pixel resolution 
 A great deal of information was learned and many tools were de­veloped and improved in the context of 
this project, so to apply the process again would likely require signi.cantly fewer resources to achieve 
results of equal or even better quality. Based on the experiences of the Digital Emily project, .ve suf.cient 
(and perhaps necessary) steps for achieving a photoreal digital actor are: 1. Suf.cient facial scanning 
resolution accurate to the level of skin pores and .ne wrinkles, which for Digital Emily was achieved 
with the Light Stage scanning process of [Ma et al. 2007] 2. 3D geometry and appearance data from a 
wide range of facial expressions, also achieved through [Ma et al. 2007] 3. Realistic facial animation, 
such as that which can be captured from a real actor s performance, including detailed motion of the 
eyes and mouth, which was achieved through the semi­automatic Image Metrics video-based facial animation 
process 4. Realistic skin re.ectance including the translucency of the skin, which for digital Emily 
leveraged a subsurface scattering tech­nique based on [Jensen et al. 2001] 5. Accurate lighting integration 
with the actor s environment, which for Digital Emily was done using HDRI capture and image-based lighting 
as described in [Debevec 1998]  9.1 Lessons Learned The experience of creating Digital Emily taught 
us numerous lessons which will be useful in the creation of future digital charac­ters. These lessons 
included: Having consistent surface (u,v) coordinates across the scans which are accurate to skin pore 
level detail would be of sig­ni.cant use in the process; a great deal of the effort involved was forming 
correspondences between the scans.  Although the Image Metrics facial animation system requires no facial 
markers for animation, including at least a few facial markers as the actor performs would make the head 
tracking process much easier.  Giving the digital character their own teeth, accurately scanned and 
placed within the head and jaw, is important for the be­lievability of the character and their resemblance 
to the original person. 10 Real-Time Emily A follow-up effort to the Digital Emily demo at SIGGRAPH 
2008 was "Real-time Emily", an effort to render realistic facial anima­tion and re.ectance in real time 
using a GPU, done in early 2009 for the Consumer Electronics Show (CES) in Las Vegas. The real­time demo 
shows an animation clip of Digital Emily giving a short speech and exhibiting different expressions and 
emotions (Fig. 22). Users can view this animation from any angle and lit by a point light source from 
any direction. The facial deformation model in real-time Emily consists of three parts. The underlying 
geometry is interpolated according to blend­shapes between the various smoothed expression scans represent­ing 
the low frequency characteristics of the facial deformation. Medium-frequency deformation is represented 
by blending dis­placement maps derived from the expression scans. For each frame, an array of blending 
coef.cients is sent to the fragment shader to synthesize a unique displacement map per frame. For the 
top layer we use a single set of hybrid normal maps [Ma et al. 2007] from the neutral scan to give the 
appearance of subsurface scattering, self-occlusion, and bounce light effects within an ef.cient local 
shading model. These various normal maps (red, green, blue, and specular) are stored in tangent space 
and transformed back to world space according to the median frequency displacements and low frequency 
deformed geometry in the fragment shader. The real-time Emily demo is relatively realistic across most 
of the face, and generally reproduces the motion and shading cues seen in the of.ine animation. The demo, 
however, is less realistic around the mouth and eyes. The reason is that detailed dynamic shading ef­fects 
around the eyelids and teeth are not calculated with suf.cient accuracy to appear realistic, and the 
mouth and eyes are the areas of the face which receive the most scrutiny. Creating real-time eyelid, 
eyeball, teeth, and mouth lighting interactions remains an important avenue for future work, as does 
illuminating real-time facial models with environmental (as opposed to point-source) illumination.  
 Acknowledgements The authors wish to thank Steve Caulkin and Kevin Walker for their contributions to 
the Digital Emily project as well as Cyrus Wilson and Graham Fyffe for helping review this report. The 
complete project credits for the Digital Emily project are, for Image Met­rics: Oleg Alexander (Director/Rigger), 
David Barton (Producer), William Lambeth (Render Artist), Cesar Bravo and Matt Onheiber (Facial Animation), 
Emily O Brien (Actress), Christopher Jones (Co-Producer), Peter Busch (Production Manager), Bryan Burger 
(Performance Analysis), Mike Rogers (Computer Vision Engineer), Edwin Gamez (Production Assistant), Steven 
McClellan (Match­mover), Tom Tran (Additional Modeling), Eric Schumacher (Mar­keting), Shannon McPhee 
(Public Relations), Justin Talley (Direc­tor of Photography), Sean Kennedy (Dental Cast), Con.dence Head 
(Music), and for USC Institute for Creative Technologies, Paul Debevec (Senior Supervisor), Matt Chiang 
(Research Program­mer), Alex Ma (Research Programmer), Tim Hawkins (Supervis­ing Researcher), Tom Pereira 
(Producer), Andrew Jones (3D Scan­ning), Abhijeet Ghosh (Re.ectance Modeling), Jay Busch (Techni­cal 
Artist). The authors additionally wish to thank Joe Letteri, Se­bastian Sylwan, Bill Swartout, Cheryl 
Birch, Randy Hill, Randolph Hall, Patrick Davenport, and Mike Starkenburg for their support of this project. 
Portions of this work were sponsored by the Uni­versity of Southern California Of.ce of the Provost and 
the U.S. Army Research, Development, and Engineering Command (RDE-COM). The content of the information 
does not necessarily re.ect the position or the policy of the US Government, and no of.cial endorsement 
should be inferred.  References BAKER, S., AND MATTHEWS, I. 2004. Lucas-kanade 20 years on: A unifying 
framework. Int. J. Comput. Vision 56, 3, 221 255. BORSHUKOV, G., AND LEWIS, J. P. 2003. Realistic human 
face rendering for The Matrix Reloaded . In ACM SIGGRAPH 2003 Sketches &#38; Applications. CHANG. 2007. 
Beowulf. Variety (Nov). DEBEVEC, P., HAWKINS, T., TCHOU, C., DUIKER, H.-P., SAROKIN, W., AND SAGAR, M. 
2000. Acquiring the re­.ectance .eld of a human face. In Proceedings of ACM SIG-GRAPH 2000, Computer 
Graphics Proceedings, Annual Confer­ence Series, 145 156. DEBEVEC, P., TCHOU, C., GARDNER, A., HAWKINS, 
T., POULLIS, C., STUMPFEL, J., JONES, A., YUN, N., EINARS-SON, P., LUNDGREN, T., FAJARDO, M., AND MAR-TINEZ, 
P. 2004. Estimating surface re.ectance proper­ties of a complex scene under captured natural illumination. 
Tech. Rep. ICT-TR-06.2004, USC ICT, Marina del Rey, CA, USA, Jun. http://gl.ict.usc.edu/Research/re.ectance/Parth-ICT­TR-06.2004.pdf. 
DEBEVEC, P. 1998. Rendering synthetic objects into real scenes: Bridging traditional and image-based 
graphics with global il­lumination and high dynamic range photography. In Proceed­ings of SIGGRAPH 98, 
Computer Graphics Proceedings, Annual Conference Series, 189 198. EKMAN, P., AND FRIESEN, W. 1978. Facial 
Action Coding Sys­tem: A Technique for the Measurement of Facial Movement. Consulting Psychologists Press, 
Palo Alto. FLEISHMAN, S., DRORI, I., AND COHEN-OR, D. 2003. Bilateral mesh denoising. ACM Transactions 
on Graphics 22, 3 (July), 950 953. GHOSH, A., HAWKINS, T., PEERS, P., FREDERIKSEN, S., AND DEBEVEC, P. 
2008. Practical modeling and acquisition of lay­ered facial re.ectance. ACM Transactions on Graphics 
27,5 (Dec.), 139:1 139:10. GUENTER, B., GRIMM, C., WOOD, D., MALVAR, H., AND PIGHIN, F. 1998. Making 
faces. In Proceedings of SIGGRAPH 98, Computer Graphics Proceedings, Annual Conference Series, 55 66. 
HAWKINS, T., WENGER, A., TCHOU, C., GARDNER, A., GÖRANSSON, F., AND DEBEVEC, P. 2004. Animatable facial 
re.ectance .elds. In Rendering Techniques 2004: 15th Euro­graphics Workshop on Rendering, 309 320. HYNEMAN, 
W., ITOKAZU, H., WILLIAMS, L., AND ZHAO, X. 2005. Human face project. In ACM SIGGRAPH 2005 Course #9: 
Digital Face Cloning, ACM, New York, NY, USA. ICT-GRAPHICS-LABORATORY, 2001. Realistic hu­ man face 
scanning and rendering. Web site. http://gl.ict.usc.edu/Research/facescan/. JENSEN, H. W., MARSCHNER, 
S. R., LEVOY, M., AND HANRA-HAN, P. 2001. A practical model for subsurface light transport. In Proceedings 
of ACM SIGGRAPH 2001, Computer Graphics Proceedings, Annual Conference Series, 511 518. KAUFMAN, D. 1999. 
Photo genesis. WIRED 7, 7 (July). http://www.wired.com/wired/archive/7.07/jester.html. LANDIS, H. 2002. 
Production-ready global illumination. In Notes for ACM SIGGRAPH 2005 Course #16: RenderMan in Produc­tion, 
ACM, New York, NY, USA. MA, W.-C., HAWKINS, T., PEERS, P., CHABERT, C.-F., WEISS, M., AND DEBEVEC, P. 
2007. Rapid acquisition of specular and diffuse normal maps from polarized spherical gradient illumina­tion. 
In Rendering Techniques, 183 194. MA, W.-C., JONES, A., CHIANG, J.-Y., HAWKINS, T., FRED-ERIKSEN, S., 
PEERS, P., VUKOVIC, M., OUHYOUNG, M., AND DEBEVEC, P. 2008. Facial performance synthesis using deformation-driven 
polynomial displacement maps. ACM Trans­actions on Graphics 27, 5 (Dec.), 121:1 121:10. MA, W.-C. 2008. 
A Framework for Capture and Synthesis of High Resolution Facial Geometry and Performance. PhD thesis, 
Na­tional Taiwan University. MORI, M. 1970. Bukimi no tani (the uncanny valley). Energy 7, 4, 33 35. 
NEHAB, D., RUSINKIEWICZ, S., DAVIS, J., AND RAMAMOOR-THI, R. 2005. Ef.ciently combining positions and 
normals for precise 3d geometry. ACM Transactions on Graphics 24,3 (Aug.), 536 543. PERLMAN, S. 2006. 
Volumetric cinematography: The world no longer .at. Mova White Paper (Oct). PLANTEC, P. 2008. The digital 
eye: Image metrics attempts to leap the uncanny valley. VFXWorld magazine (August). http://www.vfxworld.com/?atype=articles&#38;id=3723. 
ROBERTSON, B. 2009. What s old is new again. Computer Graph­ics World 32, 1 (Jan). 14 SAGAR, M., MONOS, 
J., SCHMIDT, J., ZIEGLER, D., FOO, S.-C., SCOTT, R., STERN, J., WAEGNER, C., NOFZ, P., HAWKINS, T., AND 
DEBEVEC, P. 2004. Re.ectance .eld ren­dering of human faces for ¨. spider-man 2¨In SIGGRAPH 04: ACM SIGGRAPH 
2004 Technical Sketches, ACM, New York, NY, USA. WANG, Y., GUPTA, M., ZHANG, S., WANG, S., GU, X., SAMA-RAS, 
D., AND HUANG, P. 2008. High resolution tracking of non-rigid motion of densely sampled 3d data using 
harmonic maps. Int. J. Comput. Vision 76, 3, 283 300. WENGER, A., GARDNER, A., TCHOU, C., UNGER, J., 
HAWKINS, T., AND DEBEVEC, P. 2005. Performance relighting and re.ectance transformation with time-multiplexed 
illumina­tion. ACM Transactions on Graphics 24, 3 (Aug.), 756 764. WOLFF, E. 2003. Creating virtual performers: 
Disney s human face project. Millimeter magazine (April). ZHANG, Z. 2000. A .exible new technique for 
camera calibration. IEEE Trans. Pattern Anal. Mach. Intell. 22, 11, 1330 1334.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1667252</article_id>
		<sort_key>130</sort_key>
		<display_label>Article No.</display_label>
		<display_no>13</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Efficient substitutes for subdivision surfaces]]></title>
		<page_from>1</page_from>
		<page_to>107</page_to>
		<doi_number>10.1145/1667239.1667252</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1667252</url>
		<abstract>
			<par><![CDATA[<p>Subdivision surfaces provide a compact representation for smooth surfaces that facilitate modeling and animation. They have widespread application in the movie industry, and there's a natural desire to use them also in real-time applications. This course presents theoretical results, implementations, applications, and future research directions. Topics include an introduction to subdivision surfaces, an overview of the surface-evaluation algorithms that are suitable for real-time rendering, implementation of those algorithms on current and next-generation GPUs, and other practical considerations. The course concludes with a section on practical application of these theoretical schemes and GPU implementations to Valve's source game engine and ILM's film production.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Graphics processors</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010389</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics processors</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1797140</person_id>
				<author_profile_id><![CDATA[81314484538]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tianyun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ni]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797141</person_id>
				<author_profile_id><![CDATA[81100654270]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ignacio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Casta&#241;o]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797142</person_id>
				<author_profile_id><![CDATA[81387593705]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[J&#246;rg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Peters]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797143</person_id>
				<author_profile_id><![CDATA[81320492890]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jason]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mitchell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Valve]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797144</person_id>
				<author_profile_id><![CDATA[81100155250]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Philip]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schneider]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light and Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797145</person_id>
				<author_profile_id><![CDATA[81100404274]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Vivek]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Verma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light and Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1409095</ref_obj_id>
				<ref_obj_pid>1409060</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{AB08} Marc Alexa and Tamy Boubekeur. Subdivision Shading. <i>ACM Trans. Graph, Siggraph Asia</i>, 27(5):142, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1409094</ref_obj_id>
				<ref_obj_pid>1409060</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{BA08} Tamy Boubekeur and Marc Alexa. Phong Tessellation. <i>ACM Trans. Graph, Siggraph Asia</i>, 27(5):141, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>35072</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{BBB87} Richard H. Bartels, John C. Beatty, and Brian A. Barsky. <i>An Introduction to Splines for Use in Computer Graphics&amp;Geometric Modeling.</i> Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{Bez77} Pierre E. Bezier. <i>Essai de Definition Numerique des Courbes et des Surfaces Experimentales.</i> Ph.d. thesis, Universite Pierre et Marie Curie, February 1977.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{BG75} R. Barnhill and J. Gregory. Compatible Smooth Interpolation in Triangles. <i>J of Approx. Theory</i>, 15(3):214--225, 1975.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{BRS05} Tamy Boubekeur, Patrick Reuter, and Christophe Schlick. Scalar Tagged PN Triangles. In <i>EUROGRAPHICS 2005 (Short Papers).</i> Eurographics, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>504505</ref_obj_id>
				<ref_obj_pid>504502</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{BS02} J. Bolz and P. Schr&#246;der. Rapid Evaluation of Catmull-Clark Subdivision Surfaces. In <i>Proceedings of the Web3D 2002 Symposium</i>, pages 11--18. ACM Press, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1071882</ref_obj_id>
				<ref_obj_pid>1071866</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{BS05} Tamy Boubekeur and Christophe Schlick. Generic Mesh Refinement on Gpu. In <i>ACM SIGGRAPH/Eurographics Graphics Hardware</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1338600</ref_obj_id>
				<ref_obj_pid>1338439</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{BS07} Tamy Boubekeur and Christophe Schlick. QAS: Real-Time Quadratic Approximation of Subdivision Surfaces. In Marc Alexa, Steven J. Gortler, and Tao Ju, editors, <i>Proceedings of the Pacific Conference on Computer Graphics and Applications, Pacific Graphics 2007, Maui, Hawaii, USA, October 29 -- November 2, 2007</i>, pages 453--456. IEEE Computer Society, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[{BS08} Tamy Boubekeur and Christophe Schlick. A Flexible Kernel for Adaptive Mesh Refinement on GPU. <i>Computer Graphics Forum</i>, 27(1):102--114, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[{Bun05} Michael Bunnell. <i>GPU Gems 2: Programming Techniques for High-Performance Graphics and General-Purpose Computation</i>, chapter Adaptive Tessellation of Subdivision Surfaces With Displacement Mapping. Addison-Wesley, Reading, MA, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[{Cas08a} Ignacio Casta&#241;o. Next-Generation Rendering of Subdivision Surfaces, 2008. SIGGRAPH.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[{Cas08b} Ignacio Casta&#241;o. Tessellation of Subdivision Surfaces in Directx 11, 2008. Gamefest.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[{CC78} Edwin Catmull and James Clark. Recursively Generated B-Spline Surfaces on Arbitrary Topological Meshes. <i>Computer-Aided Design</i>, pages 350--355, 1978.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[{CDM91} A. S. Cavaretta, W. Dahmen, and C. A. Micchelli. Stationary Subdivision. <i>Memoirs of the American Mathematical Society</i>, 93(453):1--186, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>508360</ref_obj_id>
				<ref_obj_pid>508357</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[{CH02} Nathan A. Carr and John C. Hart. Meshed Atlases for Real-Time Procedural Solid Texturing. <i>ACM Trans. Graph.</i>, 21(2):106--131, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1281981</ref_obj_id>
				<ref_obj_pid>1281957</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[{CHCH06} Nathan A. Carr, Jared Hoberock, Keenan Crane, and John C. Hart. Rectangular Multi-chart Geometry Images. In <i>SGP '06: Proceedings of the fourth Eurographics symposium on Geometry processing</i>, pages 181--190, Aire-la-Ville, Switzerland, Switzerland, 2006. Eurographics Association.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280832</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[{COM98} Jonathan Cohen, Marc Olano, and Dinesh Manocha. Appearance-Preserving Simplification. In <i>SIGGRAPH '98: Proceedings of the 25th annual conference on Computer graphics and interactive techniques</i>, pages 115--122, New York, NY, USA, 1998. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808602</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[{Coo84} Robert L. Cook. Shade Trees. <i>SIGGRAPH Comput. Graph.</i>, 18(3):223--231, 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[{dB93} Carl de Boor. On the Evaluation of Box Splines. <i>Numerical Algorithms</i>, 5(1--4):5--23, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141993</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[{DBG+06} Shen Dong, Peer-Timo Bremer, Michael Garland, Valerio Pascucci, and John C. Hart. Spectral Surface Quadrangulation. <i>ACM Trans. Graph.</i>, 25(3):1057--1066, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[{Dia08} Rich Diamant. Autodesk Mudbox: Integration and Use with Autodesk 3ds Max and Autodesk Maya. In <i>Game Developer's Conference</i>, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280826</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[{DKT98} T. DeRose, M. Kass, and T. Truong. Subdivision Surfaces in Character Animation. In <i>SIGGRAPH '98: Proceedings of the 25th annual conference on Computer graphics and interactive techniques</i>, pages 85--94, New York, NY, USA, 1998. ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[{DRS08} C. Dyken, M. Reimers, and J. Seland. Real-Time Gpu Silhouette Refinement Using Adaptively Blended Bezier Patches. In <i>Computer Graphics Forum 27 (1)</i>, pages 1--12, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[{DRSar} Christopher Dyken, Martin Reimers, and Johan Seland. Semi-Uniform Adaptive Patch Tessellation. <i>Computer Graphics Forum</i>, to appear.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[{DS78} D. Doo and M. Sabin. Behaviour of recursive division surfaces near extraordinary points. <i>Computer-Aided Design</i>, 10:356--360, September 1978.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378468</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[{DWS+88} Michael Deering, Stephanie Winner, Bic Schediwy, Chris Duffy, and Neil Hunt. The Triangle Processor and Normal Vector Shader: a VLSI System for High Performance Graphics. In <i>SIGGRAPH '88: Proceedings of the 15th annual conference on Computer graphics and interactive techniques</i>, pages 21--30, New York, NY, USA, 1988. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[{Far97} Gerald Farin. <i>Curves and Surfaces for Computer-Aided Geometric Design: A Practical Guide.</i> Academic Press, pub-ACADEMIC:adr, fourth edition, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[{FH05} Michael S. Floater and Kai Hormann. Surface Parameterization: a Tutorial and Survey. pages 157--186, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[{For03} Tom Forsyth. Practical Displacement Mapping. In <i>Game Developers Conference</i>, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[{For06} Tom Forsyth. Linear-Speed Vertex Cache Optimization. 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[{Gee08} Kev Gee. DirectX 11 Tessellation. In <i>Microsoft GameFest</i>, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>300524</ref_obj_id>
				<ref_obj_pid>300523</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[{GP99} Carlos Gonzalez and J&#246;rg Peters. Localized Hierarchy Surface Splines. In S. N. Spencer J. Rossignac, editor, <i>ACM Symposium on Interactive 3D Graphics</i>, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[{Gre74} J. A. Gregory. <i>Smooth Interpolation Without Twist Constraints</i>, pages 71--88. Academic Press, 1974.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>282923</ref_obj_id>
				<ref_obj_pid>282918</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[{GS85} Leonidas Guibas and Jorge Stolfi. Primitives for the Manipulation of General Subdivisions and the Computation of Voronoi Diagrams. <i>ACM Trans. Graph.</i>, 4(2):74--123, 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[{JH08} Jin Ma Xinguo Liu Leif Kobbelt Hujun Bao Jin Huang, Muyang Zhang. Spectral Quadrangulation with Orientation and Alignment Control. <i>SIGGRAPH Asia</i>, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[{JLW05} Shuangshuang Jin, Robert R. Lewis, and David West. A Comparison of Algorithms for Vertex Normal Computation. <i>The Visual Computer</i>, 21(1-2):71--82, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1507174</ref_obj_id>
				<ref_obj_pid>1507149</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[{KMDZ09} Denis Kovacs, Jason Mitchell, Shanon Drone, and Denis Zorin. Real-time Creased Approximate Subdivision Surfaces. In <i>I3D '09: Proceedings of the 2009 Symposium on Interactive 3D Graphics and Games</i>, pages 155--160, New York, NY, USA, 2009. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1224454</ref_obj_id>
				<ref_obj_pid>1224249</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[{KP07} Kestutis Kar&#269;iauskas and J&#246;rg Peters. Concentric Tessellation Maps and Curvature Continuous Guided Surfaces. <i>Computer Aided Geometric Design</i>, 24(2):99--111, Feb 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1464799</ref_obj_id>
				<ref_obj_pid>1464516</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[{KPN1} Kestutis Kar&#269;iauskas and J&#246;rg Peters. Guided Spline Surfaces. <i>Computer Aided Geometric Design</i>, pages 1--20, 2009 N1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1017161</ref_obj_id>
				<ref_obj_pid>1017156</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[{KPR04} K. Karciauskas, J. Peters, and U. Reif. Shape Characterization of Subdivision Surfaces -- Case Studies. <i>Computer-Aided Geometric Design</i>, 21(6):601--614, july 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[{KSG03} Vladislav Kraevoy, Alla Sheffer, and Craig Gotsman. Matchmaker: Constructing Constrained Texture Maps. 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141939</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[{LB06} Charles Loop and Jim Blinn. Real-time GPU Rendering of Piecewise Algebraic Surfaces. <i>ACM Trans. Graph.</i>, 25(3):664--670, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[{Lee06} Matt Lee. Next-Generation Graphics Programming on XBox 360. In <i>Microsoft GameFest</i>, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344829</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[{LMH00} Aaron Lee, Henry Moreton, and Hugues Hoppe. Displaced Subdivision Surfaces. In <i>SIGGRAPH '00: Proceedings of the 27th annual conference on Computer graphics and interactive techniques</i>, pages 85--94, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[{Loo92} Charles Loop. <i>Generalized B-Spline Surfaces of Arbitrary Topological Type.</i> PhD thesis, University of Washington, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1057454</ref_obj_id>
				<ref_obj_pid>1057432</ref_obj_pid>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[{Loo04} Charles Loop. Second Order Smoothness over Extraordinary Vertices. In <i>Symposium on Geometry Processing</i>, pages 169--178, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1330519</ref_obj_id>
				<ref_obj_pid>1330511</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[{LS08a} Charles Loop and Scott Schaefer. Approximating Catmull-Clark Subdivision Surfaces with Bicubic Patches. <i>ACM Trans. Graph.</i>, 27(1):1--11, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[{LS08b} Charles T. Loop and Scott Schaefer. <i>G</i>
&#60;sup&#62;2&#60;/sup&#62; Tensor Product Splines Over Extraordinary Vertices. <i>Comput. Graph. Forum</i>, 27(5):1373--1382, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1137516</ref_obj_id>
				<ref_obj_pid>1137246</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[{LY06} Gang Lin and Thomas P. Y. Yu. An improved vertex caching scheme for 3d mesh rendering. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 12(4):640--648, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1413964</ref_obj_id>
				<ref_obj_pid>1413957</ref_obj_pid>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[{MHAM08} Jacob Munkberg, Jon Hasselgren, and Tomas Akenine-Mller. Non-Uniform Fractional Tessellation. In <i>ACM SIGGRAPH/Graphics Hardware 2008</i>, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1281671</ref_obj_id>
				<ref_obj_pid>1281500</ref_obj_pid>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[{Mit07} Martin Mittring. Finding Next Gen: CryEngine 2. In <i>SIGGRAPH '07: ACM SIGGRAPH 2007 courses</i>, pages 97--121, New York, NY, USA, 2007. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1338579</ref_obj_id>
				<ref_obj_pid>1338439</ref_obj_pid>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[{MKP07} Ashish Myles, K&#553;stutis Kar&#269;iauskas, and J&#246;rg Peters. Extending Catmull-Clark Subdivision and PCCM with Polar Structures. In <i>PG '07: Proceedings of the 15th Pacific Conference on Computer Graphics and Applications</i>, pages 313--320, Washington, DC, USA, 2007. IEEE Computer Society.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1731318</ref_obj_id>
				<ref_obj_pid>1731309</ref_obj_pid>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[{MNP08} Ashish Myles, Tianyun Ni, and J&#246;rg Peters. Fast Parallel Construction of Smooth Surfaces from Meshes with Tri/Quad/Pent Facets. In <i>Symposium on Geometry Processing, July 2 -- 4, 2008, Copenhagen, Denmark</i>, pages 1--8. Blackwell, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383520</ref_obj_id>
				<ref_obj_pid>383507</ref_obj_pid>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[{Mor01} Henry Moreton. Watertight Tessellation Using Forward Differencing. In <i>HWWS '01: Proceedings of the ACM SIGGRAPH/EUROGRAPHICS workshop on Graphics hardware</i>, pages 25--32, New York, NY, USA, 2001. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531354</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[{MP09} Ashish Myles and J&#246;rg Peters. Bi-3 <i>C</i>
&#60;sup&#62;2&#60;/sup&#62; Polar Subdivision. <i>ACM Transactions on Graphics</i>, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[{My108} Ashish Myles. <i>Curvature-Continuous Bicubic Subdivision Surfaces for Polar Configurations.</i> PhD thesis, University of Florida, December 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[{NYM+08} Tianyun Ni, Young In Yeo, Ashish Myles, Vineet Goel, and J&#246;rg Peters. GPU Smoothing of Quad Meshes. In <i>IEEE International Conference on Shape Modeling and Applications</i>, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344990</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[{PB00} Dan Piponi and George Borshukov. Seamless Texture Mapping of Subdivision Surfaces by Model Pelting and Texture Blending. In <i>SIGGRAPH '00: Proceedings of the 27th annual conference on Computer graphics and interactive techniques</i>, pages 471--478, New York, NY, USA, 2000. ACM Press/Addison-Wesley Publishing Co.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>581820</ref_obj_id>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[{PBP02} H. Prautzsch, W. Boehm, and M. Paluszny. <i>B&#233;zier and B-Spline Techniques.</i> Mathematics and Visualization. Springer-Verlag, Berlin, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1057441</ref_obj_id>
				<ref_obj_pid>1057432</ref_obj_pid>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[{PCK04} Budirijanto Purnomo, Jonathan D. Cohen, and Subodh Kumar. Seamless Texture Atlases. In <i>SGP '04: Proceedings of the 2004 Eurographics/ACM SIGGRAPH symposium on Geometry processing</i>, pages 65--74, New York, NY, USA, 2004. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[{Pet91} J&#246;rg Peters. Smooth Interpolation of a Mesh of Curves. <i>Constructive Approximation</i>, 7:221--247, 1991. Winner of SIAM Student Paper Competition 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>207497</ref_obj_id>
				<ref_obj_pid>207475</ref_obj_pid>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[{Pet95} J&#246;rg Peters. <i>C</i>
&#60;sup&#62;1&#60;/sup&#62;-Surface Splines. <i>SIAM Journal on Numerical Analysis</i>, 32(2):645--666, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[{Pet02} J&#246;rg Peters. Geometric Continuity. In <i>Handbook of Computer Aided Geometric Design</i>, pages 193--229. Elsevier, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[{Pet04} J&#246;rg Peters. Mid-Structures of Subdividable Linear Efficient Function Enclosures Linking Curved and Linear Geometry. In Miriam Lucian and Marian Neamtu, editors, <i>Proceedings of SIAM conference, Seattle, Nov 2003.</i> Nashboro, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[{Pet08} J&#246;rg Peters. PN-Quads. Technical Report 2008-421, Dept CISE, University of Florida, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[{PK09} J&#246;rg Peters and K. Kar&#269;iauskas. An introduction to guided and polar surfacing. In <i>Mathematics of Curves and Surfaces</i>, pages 1--26, 2009. Seventh International Conference on Mathematical Methods for Curves and Surfaces Toensberg, Norway.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[{PR98} J. Peters and U. Reif. The 42 Equivalence Classes of Quadratic Surfaces in Affine N-Space. <i>Computer-Aided Geometric Design</i>, 15:459--473, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1481587</ref_obj_id>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[{PR08} J. Peters and U. Reif. <i>Subdivision Surfaces</i>, volume 3 of <i>Geometry and Computing.</i> Springer-Verlag, New York, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>265261</ref_obj_id>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[{PT97} Les Piegl and Wayne Tiller. <i>The NURBS Book (2nd ed.).</i> Springer-Verlag New York, Inc., New York, NY, USA, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[{PW08} J. Peters and X. Wu. Net-to-Surface Distance of Subdivision Functions. <i>JAT</i>, page xx, 2008. in press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[{SAUK04} Le-Jeng Shiue, Pierre Alliez, Radu Ursu, and Lutz Kettner. A Tutorial on CGAL Polyhedron for Subdivision Algorithms. In <i>2nd CGAL User Workshop</i>, 2004. http://www.cgal.org/Tutorials/Polyhedron/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073304</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[{SJP05} Le-Jeng Shiue, Ian Jones, and J. Peters. A Realtime GPU Subdivision Kernel. In Marcus Gross, editor, <i>Siggraph 2005, Computer Graphics Proceedings</i>, Annual Conference Series, pages 1010--1015. ACM Press / ACM SIGGRAPH / Addison Wesley Longman, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276489</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[{SNB07} Pedro V. Sander, Diego Nehab, and Joshua Barczak. Fast Triangle Reordering for Vertex Locality and Reduced Overdraw. <i>ACM Trans. Graph.</i>, 26(3):89, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1295187</ref_obj_id>
				<ref_obj_pid>1295186</ref_obj_pid>
				<ref_seq_no>75</ref_seq_no>
				<ref_text><![CDATA[{SPR06} Alla Sheffer, Emil Praun, and Kenneth Rose. Mesh Parameterization Methods and Their Applications. <i>Found. Trends. Comput. Graph. Vis.</i>, 2(2):105--171, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>356626</ref_obj_id>
				<ref_obj_pid>356625</ref_obj_pid>
				<ref_seq_no>76</ref_seq_no>
				<ref_text><![CDATA[{SSRS74} Ivan E. Sutherland, Robert F. Sproull, Robert, and A. Schumacker. A Characterization of Ten Hidden-Surface Algorithms. <i>ACM Computing Surveys</i>, 6:1--55, 1974.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97901</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>77</ref_seq_no>
				<ref_text><![CDATA[{ST90} Takafumi Saito and Tokiichiro Takahashi. Comprehensible Rendering of 3D Shapes. <i>SIGGRAPH Comput. Graph.</i>, 24(4):197--206, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280945</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>78</ref_seq_no>
				<ref_text><![CDATA[{Sta98} Jos Stam. Exact Evaluation of Catmull-Clark Subdivision Surfaces at Arbitrary Parameter Values. In M. Cohen, editor, <i>SIGGRAPH 98 Proceedings</i>, pages 395--404. Addison Wesley, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1338580</ref_obj_id>
				<ref_obj_pid>1338439</ref_obj_pid>
				<ref_seq_no>79</ref_seq_no>
				<ref_text><![CDATA[{SW07} S. Schaefer and J. Warren. Exact Evaluation of Non-Polynomial Subdivision Schemes at Rational Parameter Values. In <i>PG '07: 15th Pacific Conference on Computer Graphics and Applications</i>, pages 321--330, Los Alamitos, CA, USA, 2007. IEEE Computer Society.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882295</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>80</ref_seq_no>
				<ref_text><![CDATA[{SZBN03} Thomas W. Sederberg, Jianmin Zheng, Almaz Bakenov, and Ahmad Nasri. T-splines and T-NURCCs. In Jessica Hodgins and John C. Hart, editors, <i>Proceedings of ACM SIGGRAPH 2003</i>, volume 22(3) of <i>ACM Transactions on Graphics</i>, pages 477--484. ACM Press, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1281983</ref_obj_id>
				<ref_obj_pid>1281957</ref_obj_pid>
				<ref_seq_no>81</ref_seq_no>
				<ref_text><![CDATA[{TACSD06} Y. Tong, P. Alliez, D. Cohen-Steiner, and M. Desbrun. Designing Quadrangulations with Discrete Harmonic Forms. <i>Eurographics Symposium on Geometry Processing</i>, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>82</ref_seq_no>
				<ref_text><![CDATA[{Tat07} Natasha Tatarchuk. Real-Time Tessellation on the GPU. In <i>SIGGRAPH Advanced Real-Time Rendering in 3D Graphics and Games Course</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>83</ref_seq_no>
				<ref_text><![CDATA[{Tat08} Andrei Tatarinov. Instanced Tessellation in Directx 10, 2008. GDC, http://developer.nvidia.com/object/gamefest-2008-subdiv.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>84</ref_seq_no>
				<ref_text><![CDATA[{Va107} Michal Valient. Deferred Rendering in Killzone 2. In <i>DEVELOP Conference</i>, Brighton, UK, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>364387</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>85</ref_seq_no>
				<ref_text><![CDATA[{VPBM01} Alex Vlachos, J&#246;rg Peters, Chas Boyd, and Jason Mitchell. Curved PN Triangles. In <i>I3D 2001: Proceedings of the 2001 Symposium on Interactive 3D Graphics</i>, pages 159--166, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>19102</ref_obj_id>
				<ref_obj_pid>19101</ref_obj_pid>
				<ref_seq_no>86</ref_seq_no>
				<ref_text><![CDATA[{vW86} J. van Wijk. Bicubic Patches for Approximating Non-Rectangular Control-Point Meshes. <i>Computer Aided Geometric Design</i>, 3(1):1--13, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>87</ref_seq_no>
				<ref_text><![CDATA[{WP04} X. Wu and J. Peters. Interference Detection for Subdivision Surfaces. <i>Computer Graphics Forum, Eurographics 2004</i>, 23(3):577--585, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1098456</ref_obj_id>
				<ref_obj_pid>1097876</ref_obj_pid>
				<ref_seq_no>88</ref_seq_no>
				<ref_text><![CDATA[{WP05} X. Wu and J. Peters. An Accurate Error Measure for Adaptive Subdivision surfaces. In <i>Proceedings of The International Conference on Shape Modeling and Applications 2005</i>, pages 51--57, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>580358</ref_obj_id>
				<ref_seq_no>89</ref_seq_no>
				<ref_text><![CDATA[{WW02} J. Warren and H. Weimer. <i>Subdivision Methods for Geometric Design.</i> Morgan Kaufmann, New York, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>90</ref_seq_no>
				<ref_text><![CDATA[{YNM+} Young In Yeo, Tianyun Ni, Ashish Myles, Vineet Goel, and J&#246;rg Peters. Parallel Smoothing of Quad Meshes. <i>The Visual Computer</i>, pages x--x. accepted, in press, TVCJ-267.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>253314</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>91</ref_seq_no>
				<ref_text><![CDATA[{ZH97} Hansong Zhang and Kenneth E. Hoff, III. Fast Backface Culling Using Normal Masks. In <i>SI3D '97: Proceedings of the 1997 symposium on Interactive 3D graphics</i>, pages 103--ff., New York, NY, USA, 1997. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>92</ref_seq_no>
				<ref_text><![CDATA[{ZS00} Denis Zorin and Peter Schr&#246;der, editors. <i>Subdivision for Modeling and Animation</i>, Course Notes. ACM SIGGRAPH, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Ef.cient Substitutes for Subdivision Surfaces SIGGRAPH 2009 Course Notes August 5, 2009 Course Organizer 
and Notes Editor: Tianyun Ni, Ignacio Casta no, NVIDIA Corporation Instructors: J¨ org Peters, University 
of Florida Tianyun Ni, Ignacio Casta no, NVIDIA Corporation Jason Mitchell, Valve Philip Schneider, 
Vivek Verma, Industrial Light and Magic  About This Course The goal of this course is to familiarize 
attendees with the practical aspects of subdivision surfaces for which we introduce substitutes for increased 
ef.ciency in real-time applications. The course starts by highlighting the properties that make SubD 
modeling attractive and introduces some re­cent techniques to capture these properties by alternative 
surface representations with a smaller foot-print. We list and compare the new surface representations 
and focus on their implementation on current and next-generation GPUs. Among the advantages and disadvantages 
of each approach, we address crucial practical issues, such as watertight evaluation, creases and corners, 
seamless displacement mapping, cache optimization. Finally and most importantly, Valve and Industrial 
Light Magic will present a few breathtaking practical examples and demonstrate how these ad­vanced techniques 
have been adopted into their gaming and movie production pipelines. Prerequisites Basic knowledge of 
geometric modeling algorithms, in particular subdivision surfaces, and some familiarity with the graphics 
pipeline of modern GPUs. Instructors J¨ org Peters, University of Florida Dr. Jorg Peters is Professor 
of Computer and Information Sciences at University of Florida. He is interested in representing, analyzing 
and computing with geometry. To this end, he has developed new tools for free-form modeling and design 
in spline, B´ezier , subdivision and implicit represen­tations. He obtained his Ph.D. in 1990 in Computer 
Sciences from the University of Wisconsin, Carl de Boor advisor. In 1994, he received a National Young 
Investigator Award. He was tenured at Purdue University in 1997 and moved to the University of Florida 
in 1998 where he became full professor. He also serves as associate editor for the journals CAGD, APNUM 
and ACM ToG and on program committees (see http://www.cise.u..edu/ jorg). Tianyun Ni, NVIDIA Corporation 
Tianyun Ni is a member of NVIDIA s Developer Technology team. She received her Ph.D. from the University 
of Florida in 2008. She is passionate about developing new graphics techniques and helping game developers 
to incorporate these techniques into their games. Her recent work involves in .nding applications for 
Direct3D 11 and developing advanced technologies to harness the computing power of next-generation GPUs, 
especially in the area of hardware tessellation. Her expertise is real-time rendering of higher-order 
surfaces on modern GPUs where she has published a number of papers. Her publications can be found at 
http://www.cise.u..edu/ tni Ignacio Casta no, NVIDIA Corporation Ignacio is an engineer in the Developer 
Technology group at NVIDIA, where he helps game devel­opers adopt the latest technologies and harness 
the computing power of modern GPUs. His latest work is focused on .nding applications that take advantage 
of GPU tessellation. He has applied existing research, developed new algorithms, and built robust tools 
to prototype and experiment with the new hardware tessellation pipeline. Now, he frequently gives talks 
about his work at pub­lic events, and provides guidance to developers on an individual basis. Ignacio 
was editor of the Rendering section of GPU Gems 3. Before joining NVIDIA, he worked for several game 
compa­nies, including Crytek, Relic Entertainment, and Oddworld Inhabitants. Ignacio frequently blogs 
about his work and other life events at http://castano.ludicon.com/blog/ Jason Mitchell, Valve Corporation 
Jason is a Software Engineer at Valve, where he works on real-time graphics techniques across all of 
Valve s projects. Prior to joining Valve, Jason was the lead of the 3D Application Research Group at 
ATI Research for eight years. Jason has published on a variety of topics from higher-order surfaces to 
non-photorealistic rendering and regularly speaks at graphics and game development conferences around 
the world. Jason received a B.S. in Computer Engineering from Case Western Reserve University and an 
M.S. in Electrical Engineering from the University of Cincinnati. Ja­son s previous publications and 
other materials can be found at http://www.pixelmaven.com/jason/ Philip Schneider, Industrial Light 
and Magic Philip is a Senior Software Engineer in the Research and Development division at Industrial 
Light + Magic, where he is the lead of the Geometry, Modeling, and Sculpting Group. Prior to joining 
ILM, he worked at Digital Equipment Corporation, Apple Computer, Digital Domain, and Walt Disney Feature 
Animation; at all but the .rst of these he led groups working in the areas of ge­ometry, modeling, and/or 
physics simulation. He is co-author (along with David Eberly) of the Morgan-Kaufman book Geometric Tools 
for Computer Graphics . Vivek Verma, Industrial Light and Magic Vivek is a Software Engineer in the 
Research and Development division at Industrial Light + Magic, a Lucas.lm Company, where he is a member 
of the Geometry, Modeling, and Sculpting Group. Vivek s background is in computer graphics, scienti.c 
visualization, and computer vision. He obtained his PhD in 2002 from the University of California, Santa 
Cruz. In the past Vivek has worked at the Vision Technologies lab at Sarnoff Corporation and the 3D group 
at Autodesk, Inc. His professional interests include geometric modeling and computer vision.  Contents 
1 Fundamentals of ef.cient substitutes for Catmull-Clark subdivision surfaces 7 1.1 Whydowewantsmoothsurfaces? 
.......................... 7 1.2 Surfacesmoothness .................................. 8 1.3 Fillingthenormalchannel 
.............................. 9 1.4 Evaluationorapproximation? ............................. 10 1.5 
Polynomial patches of degree bi-3 and bi-cubic splines . . . . . . . . . . . . . . . 11 1.6 Subdivisionsurfaces 
................................. 13 1.7 Evaluationofsubdivisionsurfaces .......................... 
15 1.7.1 StandardEvaluation ............................. 15 1.7.2 TabulationofGeneratingFunctions 
..................... 15 1.7.3 Patchselection(ii)ineigenspace ....................... 16 1.7.4 Eigensystemevaluation 
............................ 16  1.8 Canitbedonesimpler?Ef.cientSubstitutes . . . . . . . . . . . . 
. . . . . . . . . 16 1.8.1 Controlpolyhedraandproxysplines ..................... 17 1.8.2 Separategeometryandnormalchannels 
. . . . . . . . . . . . . . . . . . . 17 1.8.3 C1 surfaceconstructions ........................... 19 
 1.9 Ef.ciency ....................................... 20 1.10Higher-qualitysurfaces? ............................... 
20 1.11Summary ....................................... 21  2 Implementation 22 2.1 TheDirect3D11tessellationpipeline 
........................ 22 2.1.1 Overviewofthepipeline ........................... 22 2.1.2 Motivation 
.................................. 26 2.2 Direct3D11ImplementationofPNTriangles . . . . . . . . . . 
. . . . . . . . . . 27 2.3 Approximating Catmull-Clark Subdivision Surfaces on Direct3D 11 Pipeline . 
. . 31 2.3.1 Introduction .................................. 31 2.3.2 PatchConstruction .............................. 
32 2.3.3 SurfaceEvaluation .............................. 36 2.3.4 Displacementmapping ............................ 
41  2.4 Instancedtessellationoncurrenthardware . . . . . . . . . . . . . . . . . . . . . . 41 2.4.1 
InstancinginDirect3D9 ........................... 43 2.4.2 InstancinginDirect3D10 .......................... 
44 2.4.3 Storage of control points in constant or texture memory . . . . . . . . . . . 45 2.4.4 InstancinginOpenGL 
............................ 45  2.4.5 Instanced tessellation with adaptive re.nement . . . . . . . 
. . . . . . . . 46 2.4.6 Emulating the Vertex Shader and the Hull Shader .............. 48 2.4.7 Vertexcacheoptimizations 
.......................... 48 2.5 WatertightTessellation ................................ 51 2.5.1 
WatertightPositionEvaluation ........................ 51 2.5.2 WatertightNormalEvaluation ........................ 
54 2.5.3 WatertightTextureSeams .......................... 54   3 Approximate Subdivision Surfaces 
in Valve s Source Engine 61 3.1 Introduction ...................................... 61 3.2 Motivation 
....................................... 61 3.3 SoftwarePipeline ................................... 
62 3.3.1 NativeTessellation .............................. 63 3.3.2 Performance ................................. 
63 3.4 Creases ........................................ 66 3.5 DisplacementMapping ................................ 
67 3.5.1 WrinkleMapping ............................... 70 3.6 MovingfromPolygonstoSubdivisionSurfaces 
. . . . . . . . . . . . . . . . . . . 70 3.6.1 QualityTangentFrames ........................... 71 3.6.2 
Manageability ................................ 71 3.7 FutureWork ...................................... 
72 3.8 Conclusion ...................................... 73 4 Approximating Subdivision Surfaces in 
ILM s Tool chain 74 4.1 Introduction ...................................... 74 4.2 Motivation ....................................... 
74 4.2.1 Viewpaint ................................... 75 4.2.2 Catmull-ClarkLimitSurfaceEvaluation 
. . . . . . . . . . . . . . . . . . . 81 4.2.3 Convergence ................................. 82 4.3 
DisplacementDisplayImplementation ........................ 83 4.3.1 CPU-sideSubdivisionPipeline ........................ 
83 4.3.2 CPU-sideACCSubdivisionPipeline . . . . . . . . . . . . . . . . . . . . . 85 4.3.3 DisplacementDisplay 
............................ 86 4.4 LimitSurfaceEvaluationImplementation . . . . . . . . . . . . . . 
. . . . . . . . 87 4.5 Results ......................................... 95 4.5.1 DisplacementDisplay 
............................ 95 4.5.2 LimitSurfaceEvaluation ........................... 95 4.6 ConclusionsandFutureWork 
............................. 95 4.7 Acknowledgements .................................. 96 1 Fundamentals 
of ef.cient substitutes for Catmull-Clark subdivision sur­ faces org Peters, University of Florida J¨ 
As real time graphics aspire to movie-quality rendering, higher-order, smooth surface representa­tions 
take center stage. Besides tensor-product splines, Catmull-Clark subdivision has become a widely accepted 
standard whose advantages we now want to replicate in real-time environments. Recently, ef.cient substitutes 
for recursive subdivision have been embraced by the industry. These notes discuss the theory justifying 
the use of ef.cient substitutes for recursive subdivision. (Three other sections discuss their current 
and future support in the graphics pipeline, in the movie pro­duction pipeline and for gaming implementations.) 
Below we therefore explore the motivation and the properties that surfaces and representations should 
satisfy to be used alongside or in place of Catmull-Clark subdivision. 1.1 Why do we want smooth surfaces? 
The ability to have continuously changing normals (comple­mented by creases where we choose to have them) 
is important both artistically and to avoid errors in downstream algorithms. Artistic shape considerations 
require the ability to create smooth surfaces and transitions: sharp turns and sharply changing nor­mals 
do not match our experience of, say, faces and limbs. On the other hand, where the curvature is high 
compared to the sur­roundings, smoothed out creases are often crucial to bring to life an object or an 
animation character. Downstream algorithms convey realism via lighting, silhouettes, and various forms 
of texturing. In particular, the diffuse and specular components of the lighting computation rely on 
well-de.ned directions (normals) n associated with points v of the object.  This is evident in the OpenGL 
lighting model. 1 Downstream algorithms relying on n and v include (click on the hyperrefs if you have 
the electronic version of the notes) are for example Gouraud shading (http://en.wikipedia.org/wiki/Gouraud 
shading)  Phong shading (http://en.wikipedia.org/wiki/Phong shading)  Bump mapping(http://en.wikipedia.org/wiki/Bump 
mapping)  higher resolution near the silhouette (http://en.wikipedia.org/wiki/Silhouette)  Normal mapping 
(http://en.wikipedia.org/wiki/Normal mapping)  Displacement mapping (http://en.wikipedia.org/wiki/Displacement 
mapping)  1.2 Surface smoothness Surfaces that can locally be parameterized over their tangent plane 
are called regular C1 sur­faces (or manifolds). Such surfaces provide a unique normal n at every point 
computable as the cross product of two independent directions t1 and t2 in the tangent plane: n||t1 × 
t2. To characterize smoothness of piecewise surfaces, as they occur in graphics applications, the area 
of geometric modeling (http://www.siam.org/activity/gd) has developed the notion of geometric continuity 
. Essentially, two patches a and b join G1 to form a part of a C1 surface if their (partial) derivatives 
match along a common curve after a change of variables. Formally (see e.g. [Pet02]), the patches a and 
b map a subset 0of R2 to R3 . That is a,b : 0 S R2 . R3 . Let . be a map that suitably connects the domains, 
i.e. changes the variables. We call such a . a (regular) reparameterization . : R2 . R2 . Let E = [0..1] 
× 0 be an edge of 0and Z the non-negative integers and let . denote composition, i.e. the image of the 
function on its right provides the parameters of the function to its left. Patches a and b join Gk if 
there exists a (regular) reparameterization . so that for the parameter restricted to E for i,j . Z,i 
+ j= k, .i .j a .. = .i .j b. (1) 12 12 1The red, green or blue intensity of OpenGL lighting is intensity 
:=emissionm + ambientl · ambientm '( 1 + · spotl · ambientl · ambientm ... k0 + k1 d + k2 d2 lights ) 
shininess ... + max{(p -v) · n, 0}· diffusel · diffusem + max{s · n, 0}· specularl · specularm v vertex 
n normal p light position e eye position where m material l light source l lighting model d ip -vi s 
:= s ' ' := v-p + v-e Is 'I s Iv-pIIv-eI Smooth surfaces must in particular satisfy G1 continuity, i.e. 
(1) for k =1. That is the surfaces need continuity along the common curve and matching transversal derivatives 
(across the edge): .j .j a ..(E)= b(E), 2 a ..(E)= 2 b(E). (2) (Matching derivatives along the boundary 
curve, .1 j a . .(E)= .1 j b(E) already follow from a ..(E)= b(E).) Proofs are therefore usually concerned 
with establishing .2 j a ..(E)= .2 j b(E) for patches with a common boundary curve (segment).) To join 
n patches Gk at a vertex, two additional constraints come into play: (a) the vertex enclosure constraint 
must hold for the normal component of the boundary curves; and (b) the reparameter­izations . must satisfy 
a consistency constraint. Both constraints arise from the periodicity when visiting the patches, respectively 
the reparameterizations surrounding a vertex. For a detailed ex­planation of these constraints and an 
in-depth look at geometric continuity see for example [Pet02]. A complex of G1-connected patches admits 
a C1 manifold structure. G1 constructions differ from the approach of classical differential geometry 
in that they do not require fully de.ned charts. G1 continuity only regulates differential quantities 
along an interface, whereas charts require overlapping domains. 1.3 Filling the normal channel The separation 
of the position and the normal channel in the graphics pipeline makes it possible to substitute for the 
true nor­mal .eld of the surface, a .eld not necessarily orthogonal to the surface. This .eld of directions 
can be used, as in bump mapping, to make a surface less smooth or to make it appear smoother (under lighting 
but not its silhouette) than it truly is. Of course, the geometry and the shape implied by lighting with 
the .eld of directions declared to be the normal .eld will be (slightly) inconsistent. But we may hope 
that this does not attract attention (see PN triangles and ACC patches below). The visual impact of the 
.eld of directions in the normal channel may be so good that a careful designer will have to check surface 
quality partly by examining silhouettes. For polyhedral models, determining vertex normals is an under- 
Figure 2: Normal channel de­.ned separate from the geom­etry (from [VPBM01]). Lin­ear interpolation of 
the normals at the endpoints (top) ignores in.ections in the curve while the quadratic normal construc­tion 
(bottom) can pick up such shape variations. constrained problem and various heuristics, such as averaging 
face normals, can be used to .ll the normal channel (for a comparison see e.g. [JLW05]). Figure 2 illustrates 
that some level of consistency of the normal channel with the true surface geometry is important. Substitutes 
of (a) interpolation (b) approximation Figure 3: Which polygon represents the circle better? subdivision 
surfaces (Section 1.8) therefore typically use more sophisticated approaches to .ll the normal channel. 
1.4 Evaluation or approximation? Due to the pixel resolution, we ultimately render an averaged, linearized 
approximation of sur­faces. As Figure 3 illustrates, exact evaluation followed by piecewise linear completion 
need not be superior to any other approximation where no point lies exactly on the circle. For another 
example in 2D consider the U-shape y := x2 for x . [-1..1]. The line segment that connects (-1,y(-1)) 
to (1,y(1)) is based on exact evaluation at the parameters -1 and 1 but is a much poorer approximation 
(in the max-norm) to the parabola piece than the line segment (-1,1/2) to (1,1/2). On a philosophical 
level, if one ultimately renders a triangulation of the surface, there is no reason to believe that a 
triangulation with exact values at the vertices is a best approximation to the true surface. All we know 
is that the maximal error does not occur at the vertices but in the interior of the approximating triangles. 
The error in the interior of the triangle may be far more than the distance between a control point and 
the surface or a control triangle and the surface. So, while exact evaluation may sound better than approximate 
evaluation, there is often no reason to prefer one to the other. In fact, if we stay with the control 
net of a surface rather than pro­jecting it to the limit, we preserve the full information of the spline 
or subdivision representation. One attempt at quantifying and minimizing this error are mid-structures 
of Subdividable Linear Ef.cient Function Enclosures (slefes) [Pet04]. Mid-structures link the curved 
geometry of the surface to a two-sided (sandwiching) piecewise linear approximation. For a subclass of 
surfaces the approximation is optimal in the max-norm (http://en.wikipedia.org/wiki/Supremum norm). 
 The main justi.cation for positioning points as exactly as possible on a surface is that, when two abutting 
patches are tessellated independently, it is good to agree on a rule that yields the same point in R3 
so that the resulting surface has no holes, i.e. is watertight. Mandating the point to be exactly on 
the surface (and being careful in its computation) is an easy-to-agree-on strategy for a consistent set 
of points. Of course, any other well-known rule of approximation would do as well. 1.5 Polynomial patches 
of degree bi-3 and bi-cubic splines control polygon q1 spline 3 3 2 2 1 1 0 0 -1 -1 AB Figure 4: Univariate 
uniform cubic spline (from [Myl08]). (A) Control points q := [1, 3, 1, 2, -1] (red) and knots t := [-1, 
0, 1, 2, 3, 4, 5, 6, 7] de.ne a cubic spline x(t) as the sum of uniform B-spline bases fl scaled by their 
respective control points (blue, green, magenta, cyan). (B) An equivalent de.nition of the spline is 
as the limit of iterative control polygon re.nement (subdivision). t0 t1 t2 t3 t4 t5 t6 t7 t8 Figure 
5: Commutativity of tensor-product spline subdivision (from [MKP07]). Bi-3 spline subdivision (A) in 
one direction followed by (B) the other, or (C) simultaneous re.nement as in Catmull-Clark. If we want 
to avoid linearization, we need to use quadratic patches at a minimum. Quadratics offer a rich source 
of shapes after all C2 surfaces can locally be well-approximated by them) but smoothly stitching pieces 
together is generally only possible for regular partitions. Moreover, enforcing G1 continuity can force 
.at spots for higher-order saddles, such as a monkey saddle (http://en.wikipedia.org/wiki/Monkey saddle). 
[PR98] lists all classes of quadratic shapes. Many curved objects are therefore modeled with cubic splines 
x(t) := l qlfl(t) as illustrated in Figure 4. Cubic spline curves in B-spline form are available in OpenGL 
as gluNurbsCurve. By tracing out cubic splines in two independent variables (u,v), we obtain a tensor-product 
spline available in OpenGL as gluNurbsSurface. We call the tensor of cubic splines bi-3 spline or bi-cubic 
spline: 33 '' qi,j fi(u)fj (v). (3) i=0 j=0 Bi-3 splines in B-spline form can be evaluated ef.ciently, 
for example by de Boor s algorithm (http://en.wikipedia.org/wiki/De Boor algorithm). Just as for curves, 
each tensor-product spline can be split into its parts by averaging control points. This is the C2 bicubic 
subdivision as illustrated in Figure 5 whose limit is the bi-3 spline patch. An alternative representation 
of a polynomial piece is the Bernstein­ b3 3 b 00 b 30 q00 q10 q20 q33 Bezier form or, shorter, BB-form 
2. Cubic spline curves in B-spline form are available in OpenGL as glMap1. It, too, can be tensored 33 
3! k '' bi,j hi(u)hj (v),hk(t) := (1 -t)3-kt. (4) (3 -k)!k! i=0 j=0 Bi-3 splines in BB-form are available 
in OpenGL as glMap2. Every surface in B-spline form can be represented in BB-form using one patch Figure 
6: Bi-3 in BB-form for every quadrilateral of the B-spline control net. Due to conversion from combinatorial 
symmetry in the positions, there are three types of formulas B-spline coef.cients in B-form to BB-form 
conversion: qij to BB-coef.cients 9b11 := 4q11 + 2(q12 +2q21)+ q22, bij (5) 18b10 := 8p11 +2p12 +2p10 
+4p21 + p20 + p22, 36b00 := 16q11 + 4(q21 + q12 + q01 + q10)+ q22 + q02 + q00 + q20. Conversely, if patches 
in BB-form are arranged in checkerboard form, they can be represented in B-spline form. To obtain the 
simplest representation, we remove knots where the surface is suf.ciently smooth. (If we do this locally 
and carefully keep track of where we removed knots, we 2http://en.wikipedia.org/wiki/Bezier curve Figure 
7: Control point structure of (left) a polynomial bi-3 patch and (right)a Gregory patch. arrive at T-splines 
[SZBN03]). That is, the B-spline form and the BB-form are equally powerful, but one may choose B-splines 
to have fewer coef.cients and built-in smoothness, while the BB­form provides interpolation at the corners. 
Additionally, the BB-form can be generalized to two variables so that the natural domain is a trian­gle, 
i.e. to total degree BB-form 3. Polynomials in BB-form can be evaluated by de Casteljau s algorithm (http://en.wikipedia.org/wiki/De 
Casteljau s algorithm). As a byproduct of evaluation, De Castel­jau s algorithm provides the derivatives 
at the evaluation point from which the normal direction can be obtained by a simple cross product. For 
a detailed exposition of these useful representations see the textbooks [Far97, PBP02]. There are a number 
of classic bi-3 surface constructions [Bez77, vW86, Pet91], but, due to funda­ mental lower bounds, they 
work in general only if we split facets into several polynomial pieces. The C1 bi-3 Gregory patch [Gre74, 
BG75] is a rational surface patch x : [0..1]2 . R3 such that .u.v x = .v .ux can hold at the corners. 
This allows separate de.nition of .rst order derivatives along the two edges emanating from a corner 
point; this can be viewed as splitting certain control points into two (see Figure 7). The resulting 
lack of higher-order smoothness contributed to it not being widely used in geometric design but should 
not be a problem for real time graphics. High evaluation cost and cost of computing normals require careful 
use. 1.6 Subdivision surfaces We sketch here only the basics of subdivision surfaces 4 suf.cient to explain 
their evaluation and approximation. A full account of the mathematical structure of subdivision surfaces 
can be found in [PR08]. The SIGGRAPH course notes [ZS00] of Schr¨oder and Zorin, and the book Subdi­vision 
Methods for Geometric design by Warren and Weimer [WW02] complement the more formal analysis by a collection 
of applications and data structures. See also the generic CGAL implementation [SAUK04]. 3http://wapedia.mobi/en/Bezier 
triangle 4subdivision surfaces (http://en.wikipedia.org/wiki/Subdivision surface) Figure 8: Mesh re.nement 
by the Catmull-Clark algorithm. Algorithmically, subdivision presents itself as a mesh re.nement procedure 
that applies rules to determine (a) the position of new mesh points from old ones and (b) the new connectivity. 
These rules are often represented graphically as weights (summing to one) associated with a local graph 
or stencil that links the old mesh points combined to form one new one: Forming the weighted old mesh 
points yields the new point. On the GPU, recursive subdivision naturally maps to several shader passes 
(see e.g. [SJP05, Bun05]5). Figure 9: Subdivision surfaces consist of a nested sequence of surface rings. 
Alternatively, the weights can be arranged as a row of a subdivision matrix A. This subdivision matrix 
maps a mesh of initial points ql . R3 collected into a vector q to an (m times) re.ned mesh m Am q = 
q. (6) The mesh can have extraordinary points. An extraordinary point is one that has an unusual number 
of direct neighbors n; n is often referred to as the valence of the extraordinary point. For example, 
n =4 is unusual for Catmull-Clark subdivision (see Figure 8). Mathematically, however, a subdivision 
surface is a spline surface with isolated singularities. Each singularity is the limit of one extraordinary 
point under subdivision. In particular, the neighborhood of any such singularity consists of a nested 
sequence of surface rings as illustrated in Figure 9. In the case of Catmull-Clark subdivision, the nested 
surface rings consist of n L-shaped sectors with three bi-3 polynomial pieces each. Let 0 := [0..1]2 
be the unit square. Then each sector () of the mth ring can be associated with a parameter range 21 m 
0- 21 0(see Figures 4.2, 4.3, 4.4, 4.5 of [PR08] for a nice illustration of this natural parameterization 
and the fact that the union of 5http://http.developer.nvidia.com/GPUGems2/gpugems2 chapter07.html rings 
then forms a spline with a central singularity). An alternative parameterization associates () .mn 0-.n0with 
an L-segment, where .n is the subdominant eigenvalue of Afor valence n. 1.7 Evaluation of subdivision 
surfaces Since subdivision surfaces are splines with singularities there are a number of evaluation methods 
that also work near extraordinary points. We list four methods below. 1.7.1 Standard Evaluation (i) determine 
the ring m (by taking the logarithm base 2); (ii) apply m subdivision steps (either by matrix or stencil 
applications);  (iii) interpret the resulting control net at level m as those of the n L-shaped sectors 
in B-spline form; and (iv) evaluate the bi-3 spline (by de Boor s algorithm). While step (ii) seems to 
require recursion, it can be replaced by the (non-recursive) matrix multi­plication (6). This is typically 
the most ef.cient strategy to evaluate a subdivision surface (and it can not be patented ;-) ). It is 
particularly ef.cient when many points on a regular grid are to be evaluated, for example when, for even 
coverage, we want to evaluate 4 times more points in ring m than in ring m -1. It is also most ef.cient 
when the surfaces have adjustable creases [DKT98], i.e. where Catmull-Clark re.nement rules are averaged 
with curve re.nement rules. Some special scenarios, however, invite different evaluation strategies. 
Before settling for a strat­egy, it is good to verify the conditions under which they are appropriate 
and ef.cient. 1.7.2 Tabulation of Generating Functions If the crease ratios are restricted to a few cases 
and the depth of the subdivision is restricted, then we can trade storage for speed by pre-tabulating 
the evaluation. The idea is to write the subdivision surface x locally, in the neighborhood of an extraordinary 
point, as L ' x(u,v,j)= qlbl(u,v,j), (7) l where the ql . R3 are the subdivision input mesh points; each 
bl . R is a generating spline, i.e. a function that we may think of as obtained by applying the rules 
of subdivision considering one coordinate qj of qj and setting all qj =0 except for ql; and the summation 
by l is over all bl that are nonzero at the point (u,v,j) of evaluation; j.{1,2,...,n}denotes one of 
the n sectors of the spline (ring). If, for each valence separately, we pre-tabulate the bl(u,v,j) for 
l =1,...,L then we can look up and combine these values with the subdivision input mesh points ql at 
run-time. When stored as textures, approximate in-between values can be obtained by bi-linear averaging. 
[BS02] 1.7.3 Patch selection (ii) in eigenspace If several but irregularly distributed parameters are 
to be evaluated and if they lie very close to the extraordinary point, it is worth converting the subdivision 
input mesh points ql to eigencoef.cients pl . R3. For this, we need to form the Jordan decomposition 
Am = VJmV-1 (just once for any given subdivision matrix Aof valence n) and set p := V-1 q so that Am 
q = VJm p. (8) If the Jordan matrix Jm is diagonal then the computational effort at run time of step 
(ii) reduces to taking mth powers of its diagonal entries [DS78]. In step (iii) we need to apply V to 
p and can then proceed as before with step (iv) to evaluate a bi-3 spline [Sta98]. Note that this method 
is no more exact than any of the other evaluation methods and that exact evaluation at individual points 
does not mean that a polyhedron based on the values exactly matches the non-linear subdivision limit 
surface. 1.7.4 Eigensystem evaluation For parameters on a grid, Cavaretta et al. showed that, for functions 
satisfying re.nement rela­tions, the exact values on a lattice can be computed by solving an eigenvalue 
problem [CDM91, page 18],[dB93, page 11]. Schaefer and Warren [SW07] apply this approach to irregular 
settings. We note that neither the standard evaluation using (6) nor any of the three approaches just 
listed require recursion or uniform re.nement (with its concomitant high use of memory and possibly of 
CPU GPU bandwidth). However, they do not provide convenient short formulas. 1.8 Can it be done simpler? 
Ef.cient Substitutes A surface construction can provide a substitute for the subdivision algorithm if 
the resulting sur­faces have similar properties. 1.8.1 Control polyhedra and proxy splines The classic 
substitute is to render, at a .nite level of resolution, either the re.ned control poly­hedron or a polyhedron 
obtained by projecting the re.ned control vertices to the limit (using the left eigenvectors of the subdivision 
matrix A). This is based on the fact that the distance between control polyhedron and limit surfaces 
decreases fast. One of the challenges here is to correctly estimate the distance of the (projected) control 
polyhedron to the surface in order to determine the (adaptive) subdivision level that gives suf.cient 
resolution for the application. By character­izing control polyhedra as (the images of) proxy splines 
with the same structure as subdivision surfaces, [PR08, Chapter 8] gives general bounds on this distance 
for all subdivision schemes. Tighter bounds, speci.cally for Catmull-Clark subdivision surfaces can be 
found in [PW08]. Also available is a plug-in by Wu for (pov-)ray tracing based on the bounds in [WP04, 
WP05]. This class of substitutes is only ef.cient, if it can be applied adaptively (see, e.g. [Bun05]). 
1.8.2 Separate geometry and normal channels A second class of substitutes takes advantage of the separation 
of the position and the normal channel in the graphics pipeline. That is, the entries in the normal channel 
are only approximately normal to the (geometry of the) surface. original geometry, re.ned normals To 
create a denser .eld for the normal channel then would be used by Gouraud shading, we can apply subdivision 
(averaging) to the polyhedral normals [AB08]. re.ned geometry, re.ned normals Replacing an input triangle 
with normals speci.ed at its vertices, PN triangles [VPBM01] consist of a total degree 3 geometry patch 
that joins continuously with its neighbor and has a common normal at the vertices. To convey the impression 
of smoothness, a separate quadratic normal patch interpolates the vertex normals (Figure 10). By reducing 
the patch degree to quadratics, trades .exibility of the geometry ordinary polar extraordinary (a) (b) 
(c) Figure 11: Mesh-to-patch conversion. (from [MNP08]) The input mesh (top) is converted to patches 
(bottom) as follows. (a) An ordinary facet is converted to a bi-cubic patch with 16 control points fij 
. (b) Every triangle in polar con.guration becomes a singular bi-cubic patch represented by 13 control 
points .. (c) An extra-ordinary facet with n sides is converted to a Pn-patch de.ned by 6n +1 control 
points shown as .. The Pn-patch is equivalent to nC1-connected degree-4 triangular patches bi , i =0 
...n-1, having cubic outer boundaries. for faster evaluation [BA08] (see also [BS07]). Since the quadratic 
pieces have no in.ections this is particularly useful when the triangulation is already more re.ned. 
For four-sided facets, the corresponding (family of) PN quads was known but not published at the time 
of PN triangles. Just like the triangles, its bi-3 patches are constructed based solely on the points 
v and normals n at the patch vertices so that a patch need not look up the neighbor quads. Better shape 
can be achieved, when the neighbor patch(es) can be accessed. For example, the inner BB coef.cients bij 
can be derived from a bi-3 spline [Pet08]. One can use Equations 5 for the inner coef.cients of type 
b11 and set b10 on an edge between two patches as an average of their closest inner points. A good heuristic 
is to set the corner control points to the Catmull-Clark limit point (with q0 the central control point 
and for l =0,...,n -1 q2l-1 the direct neighbor points and q2l the face neighbor points): n-1 ' n(n + 
5)bCC := (nq00 +4q2l-1 + q2l) . (9) 00 l=0 Up to perturbation of interior control points near extraordinary 
points, (n + 5) bACC := nq11 + 2(q12 +2q21)+ q22, (10) 11 this is how ACC patches [LS08a] are derived 
(see also the Section 2.3 of these lecture notes). (a) (b) (c) Figure 12: Quad/tri/pent polar models 
(from [MNP08]) (a) Axe handle; using a triangle and a pentagon to transition between detailed and coarser 
areas. The axe head (left) features a sharp crease. (b) Polar con.gurations naturally terminate parallel 
feature lines along elongations, like .ngers. (c) smooth surface consisting of bi-cubic patches (yellow), 
polar patches (orange), and p-patches with n =3 (green), n =4 (red), n =5 (gray). 1.8.3 C1 surface constructions 
A third class of substitutes are proper C1 surfaces, i.e. their normals can be computed everywhere (eg 
in the pixel shader) as the cross product of tangents (derivatives obtained as a byproduct of de Casteljau 
s evaluation) without recourse to a separate normal channel. These patches are typically polynomial, 
although a rational construction like Gregory s patch and its triangular equivalent could be used just 
as well. The patch corners and normals can moreover be adjusted to approximate Catmull-Clark limit surfaces. 
Just as the second class, c-patches [YNM+] and the many-sided p(m)-patches [MNP08] (Figures 11 and 12) 
can be constructed and displayed in real time. [MNP08] comes with shader code, allows for (rounded) creases 
and polar con.gurations (see Figure 12(d)) 6 The third class of surface constructions is related to surface 
splines [Pet95] and Loop s construction [Loo92] and 6 One concern is that such creases and polar con.gurations 
result in parametric distortion when texture mapping. Applying the same crease or polar mapping (in R2 
) when looking up texture coordinates, however, shows this concern to be unfounded. localized hierarchical 
surface splines [GP99]. 1.9 Ef.ciency Whether a particular representation or evaluation strategy is time 
and space ef.cient depends on the software/hardware setup. However, we can observe the following in the 
context of GPU rendering. Fixed, .ne triangulations are expensive to transfer to the GPU and require 
animation of each ver­tex. They lack re.nability. Subdivision surfaces approximated by recursive re.nement, 
possibly followed by projection of the control points to their limit, require multiple passes with increasing 
bandwidth and intermediate memory storage. Subdivision surfaces approximated by non-recursive evaluation 
as listed in Section 1.7 requires the inversion of (moderately sized) matrices. These ma­trices need 
to be adapted for different types of creases. Subdivision surfaces approximated by tabulation require 
storage that limits the representable crease con.gurations. The (ef.cient) sub­stitutes listed in Section 
1.8 allow for creases, adaptive evaluation (by instancing or the tessellation engine) and, as low degree 
polynomials, have been created to be both space-ef.cient and time­ef.cient, in their construction as 
well as in their evaluation. ef.ciency space time comment triangulation  .xed resolution recursive subD 
 adaptivity? non-recursive subD creases? tabulation + creases? ef.cient substitutes + + crease., adapt. 
 1.10 Higher-quality surfaces? For high-end design, C1 continuity is not suf.cient. One can feel (and 
sometimes see) the lack of curvature continuity. In fact, Catmull-Clark subdivision does not meet the 
requirements of high­end design: Generically, near extraordinary points, the curvature lines diverge, 
and the surfaces be­comes hyperbolic [KPR04]. Guided surfacing [KP07, KPN1], Loop and Schaefer [Loo04, 
LS08b] and most recently a bi-3 C2 polar subdivision [MP09] promise better shape. Yet, it is not clear 
that real-time or movie applications can bene.t from such high-quality surfaces. Curiously, at least 
formally, displacement mapping, which often increase roughness of the surfaces, formally requires derivatives 
of normals and therefore higher-order continuity. 1.11 Summary Besides the classical rendering of the 
control polyhedron, possibly projected onto the surface, there are two classes of surface constructions 
that can be used as ef.cient substitutes of subdivision sur­faces or as primitives in their own right. 
Both triangular patches and quad patches are available (as well as polar con.gurations) to give the designer 
broad-ranging options and mimic both Catmull-Clark and triangle-based subdivision. The next chapters 
will explain the use of these constructions in more detail and may inspire additional short-cuts and 
innovations (see for example 7), made all the more relevant by the imminent availability of tessellation 
hardware. Acknowledgements This work supported in part by NSF Grant CCF-0728797 7http://castano.ludicon.com/blog/2009/01/07/approximate-subdivision-shading/ 
2 Implementation Tianyun Ni and Ignacio Casta no, NVIDIA In this chapter we start with an overview of 
the Direct3D 11 graphics pipeline, followed by the motivation behind the design of the the pipeline through 
two case studies: PN Triangles and Ap­proximating Catmull-Clark subdivision surfaces. We then show how 
to implement and emulate portions of this pipeline on current hardware for backwards compatibility. Finally, 
we discuss some of the practical implementation details. 2.1 The Direct3D 11 tessellation pipeline Direct3D 
11 is the latest version of Microsoft s graphics API and it provides access to the lat­est features of 
modern graphics hardware. The most notable of those features is support for an extended pipeline that 
enables programmable hardware tessellation. While here we refer to this new graphics pipeline as the 
Direct3D 11 pipeline, we expect the same features will also be exposed under OpenGL. However, we use 
the Direct3D terminology, since the details of the corresponding OpenGL extensions are not publicly available 
yet. 2.1.1 Overview of the pipeline Direct3D 11 extends the Direct3D 10 pipeline with support for pro­grammable 
tessellation. This is accomplished with the addition of three new stages: the Hull Shader, the Tessellator, 
and the Domain Shader (Figure 13). The goal of these new stages is to enable ef.­cient rendering of higher 
order surfaces, such as the approximations to subdivision surfaces described in the previous chapter. 
In fact, one could say that the pipeline was designed with this particular applica­tion in mind. These 
three new stages stand between the Vertex Shader and the Ge­ometry Shader. As their name implies, the 
Hull Shader and Domain Shader are programmable stages, while the Tessellator, although fairly .exible 
and con.gurable, is a .xed function stage. In addition to these new stages, Direct3D 11 also adds a new 
prim­itive type: the patch. Patches are the only primitive types that are supported when the tessellation 
stages are enabled. Patches have an arbitrary number of vertices between 1 and 32, and unlike any of 
the other primitive types, patches do not have any implied topology. That is, a patch with three vertices 
is not necessarily a triangle; it s up to the programmer to write shaders that decide how the patch vertices 
are interpreted. In this setting, a patch is just a disconnected set of vertices. In the tessellation 
pipeline the Vertex Shader is still the .rst programmable stage. Its purpose, however, is reduced to 
transform vertices from object to world space. That is, it allows you to apply animation and deformations 
at a lower frequency. The idea is that by performing animation and simulation on the control mesh it 
will be possible to drastically reduce animation storage, and to implement much more realistic simulation 
algorithms. On the other hand, one also has to take into account that the larger the size of the patch, 
the lower the effectiveness of the post-transform cache. While in triangle meshes the number of transforms 
per vertex is typically between 1.0 and 1.5, when using quad patches composed of an average of 16 vertices, 
the number of transforms per vertex is generally about 6 times higher. In spite of that, performing animation 
in the Vertex Shader is in many cases more ef.cient than computing the animations in a separate pass. 
After the Vertex Shader, the Hull Shader (Figure 14) is invoked for each patch with all its trans­ formed 
vertices. In this regard the Hull Shader is similar to the Geometry Shader since it can perform per primitive 
operations. However, as opposed to the Geometry Shader, the Hull Shader has a .xed output that has to 
be declared in advance. The Hull Shader serves two purposes. One is to compute per edge tessellation 
factors that are provided to the Tessellator stage. The other is to perform computations that are invariant 
for all the vertices that will be generated in the Domain Shader. The most common example is to transform 
the input vertices from one basis to another, which is useful when the input representation is not practical 
for direct evaluation. In order to perform these tasks ef.ciently, the Hull Shader needs to be parallelized 
explicitly. That is, instead of having a single thread per patch compute all control points and tessellation 
factors, the Hull Shader is divided into three parallel phases (Figure 15), and each of these phases 
is composed of a user de.ned number of threads between 1 and 32. These threads cannot communicate between 
each other, but each phase can see the output of the previous phase. That allows you to, for example, 
compute control points in the .rst stage, the Control Point Phase; based on these control points compute 
the edge tessellation factors in the second stage, the Fork Phase; and .nally based on the edge tessellation 
factors compute the interior tessellation factors in the last stage, the Join Phase. To simplify the 
programming of the Hull Shader, the shader code that correspond to the Fork and Join Phases is not provided 
explicitly. Instead, a single shader function to compute all the patch attributes is provided by the 
programmer and the HLSL compiler automatically extracts parallelism from it. The Tessellator is the stage 
in which the data expansion happens. The only input of the Tessellator is the set of edge and interior 
tessellation factors computed in the Hull Shader. The Tessellator generates semi regular tessellation 
pattern for each patch based on these factors. The actual pattern also depends on the user-selected con.guration. 
 The Tessellator supports tessellation in the triangle and quad domains (Figure 16). In both cases the 
Tessellator can generate triangles in clockwise or counter-clockwise order. The generated tessellation 
patterns are symmetric along the edges and support fractional tessellation factors as in [Mor01]. However, 
as opposed to previous implementations, new triangles are not always inserted from the center of the 
tessellation domain, but follow a more uniform pattern that s based on the least signi.cant bit of the 
tessellation factor. That produces smoother transitions between tessellation levels and minimize the 
aspect ratio of the triangles in the transition regions. Fractional tessellation can sometimes result 
in sampling artifacts, because the sampling location is constantly updated as the tessellation factors 
change. These sampling errors sometimes manifest itself as temporal swimming artifacts. In order to avoid 
these problems the Tessellator also supports a power of two tessellation mode that is stationary, that 
is, it has the nice property of not moving vertices around as they are inserted or removed. Finally, 
the Domain Shader (Figure 17) takes the parametric coordinates of the vertices generated by the Tessellator 
and the control points output by the Hull Shader and uses them to evaluate the surface. The Domain Shader 
stage creates one thread for each generated vertex. These threads are similar to Vertex Shader threads; 
they evaluate the surface in parallel and cannot communicate with each other. To evaluate the surface 
in the Domain Shader it s necessary to use a surface representation that is amenable for direct evaluation. 
That is, given a parametric coordinates, it should be possible to evaluate the position and normal of 
the surface at that location. In addition to position and normal, the Domain Shader also interpolates 
texture coordinates and can sample textures in order to apply displacement maps. Instead of directly 
using the parametric coordinates provided by the Tessellator, it s also possible to adjust them with 
perspective correction in the Domain Shader to generate a more uniform triangle distribution in screen-space 
[MHAM08]. In addition to the surface evaluation, the Domain Shader also has other responsibilities that 
would traditionally correspond to the Vertex Shader. That includes projecting the vertex position to 
screen, transforming normals and light vectors to the same space, computing view/eye vectors, etc. Once 
the vertices have been transformed by the Domain Shader, the primitives generated by the Tessellator 
are optionally processed by the Geometry Shader or directly sent to the triangle setup stage for rasterization. 
The combination of programmable and .xed function stages of the tessellation pipeline provides a powerful 
surface rendering model that can be ef.ciently implemented in hardware. We think this pipeline provides 
developers with a great amount of .exibility, and will create a framework for innovation and development 
of alternate evaluation algorithms and new surface representations. 2.1.2 Motivation The rendering of 
the ef.cient substitutes outlined in Chapter 1 can usually be decomposed into two stages. First, the 
input coarse mesh is converted to a set of low degree parametric patches. We refer to this stage as Patch 
Construction. Second, the positions and normals are evaluated at arbitrary locations of the parametric 
domains. We call this stage Surface Evaluation. Since each facet-to-patch conversion is independent of 
the others, each input facet is converted to one (in some cases, few) patches in parallel during Patch 
Construction. This stage maps naturally to the Hull Shader with a facet (and possibly its 1-ring) as 
an input patch primitive. The output patch of the Hull Shader is the converted parametric patch in the 
form of its control points. In the Hull Shader, every control point is computed as the weighted sum of 
the vertices in the patch primitive. Since the control point computations are independent of each other, 
the control point phase invokes multiple threads and computes one control point per thread. Some of the 
interior control points and all tessellation factors are determined by multiple control points, so they 
are computed in Fork Phase and Join Phase. During Surface Evaluation, the position and normal at each 
parametric domain can be evaluated in parallel. We map Surface Evaluation to the Domain Shader and takes 
control points as well as the parametric domain generated by the Tessellator as inputs. Next, we will 
discuss Direct3D 11 implementation in more details through two of the most popular ef.cient substitutes: 
PN Triangles and Approximating Catmull-Clark subdivision surfaces. 2.2 Direct3D 11 Implementation of 
PN Triangles PN Triangles [VPBM01] provide a simple tessellation scheme for triangular meshes. This inter­polation 
scheme replaces input .at triangles with triangular cubic B´ezier patches and quadratic normal variation. 
It can be easily integrated into Direct3D 11 GPU pipeline. To implement PN Triangles, we .rst compute 
10 geometry and 6 normal coef.cients (or known as control points) for each input triangle. This stage 
is called Patch Construction and mapped to the Hull Shader. Afterwards, the positions (and normals) are 
evaluated using geometry (and normal) coef.cients respectively. This stage is mapped to the Domain Shader. 
The discussion of the Direct3D 11 im­plementation focuses on the data .ow between two new shader stage 
and one .xed function unit for tessellation (see Figure 18). The input patch of the Hull Shader is a 
patch that consists of three vertices where each vertex contains its position (Pi) and normal (Ni). The 
Hull Shader converts a triangle into an output patch in the form of two B´ezier patches that de.ne the 
geometry and normal of the surface piece. While a single thread could be used, it s more ef.cient to 
take advantage of the symmetry of the construction by using multiple threads to parallelize the computations. 
We divide the workload (see Figure 19) based on the observation that from one edge pair (Pi,Ni) and (Pi+1,Ni+1) 
in a .at triangle, we can derive one vertex geometry (and normal) coef.cient and two tangent geometry 
(and one tangent normal) coef.cients. The coef.cients computed according to each edge pair are indicated 
in ecliptic circle in Figure 19. In this way, three threads are invoked inside the Hull Shader to compute 
one third of an output patch. Speci.cally, each thread computes three control points for positions (i.e. 
b300,b201,bb102 ), and two normal control points(i.e. n200,n101). The detailed formulae of computing 
b and n are available in [VPBM01]. The HLSL shader code in the Hull Shader is as follows:  [domain("tri")] 
[outputtopology("triangle_cw")] [outputcontrolpoints(3)] [partitioning("fractional_odd")] [patchconstantfunc("HullShaderPatchConstant")] 
HS_CONTROL_POINT HullShaderControlPointPhase( InputPatch<HS_DATA_INPUT, 3> inputPatch, uint tid : SV_OutputControlPointID, 
uint pid : SV_PrimitiveID) { intnext =(1 <<tid) &#38;3; // (tid+ 1)% 3 float3 p1 = inputPatch[tid].position; 
float3 p2 = inputPatch[next].position; float3 n1 = inputPatch[tid].normal; float3 n2 = inputPatch[next].normal; 
 HS_CONTROL_POINT output; // Position control points output.pos1 = (float[3])p1; output.pos2 = (float[3])(2 
* p1 + p2 -dot(p2-p1, n1) * n1); output.pos3 = (float[3])(2 * p2 + p1 -dot(p1-p2, n2) * n2); // Normal 
control points float3 v12 = 4 * dot(p2-p1, n1+n2) / dot(p2-p1, p2-p1); output.nor1 = n1; output.nor2 
= n1 + n2 -v12 * (p2 -p1); // Texture coordinates output.tex = inputPatch[tid].texcoord; return output; 
} The center coef.cient b111 is a function of all vertex and tangent coef.cients. In order to avoid 
redundant computations, we defer the derivation of b111 to patch constant function stage where all other 
control points have been derived. During this stage, we also determine the edge/interior tessellation 
factors according to the computed control points. As described in Section 2.1.1, The Hull Shader is composed 
of multiple phases. The control point phase is parallelized explicitly, while the other phases are parallelized 
automatically. The user only provides a serial function to compute all the patch constant attributes 
. // Patch constant data HS_PATCH_DATA HullShaderPatchConstant( OutputPatch<HS_CONTROL_POINT, 3> controlPoints 
) { HS_PATCH_DATA patch = (HS_PATCH_DATA)0; // Compute the edge tessellation factors for(int i= 0; 
i< 3; i++){ HullShaderCalcTessFactor(patch, controlPoints, i); } // Compute the interior tessellation 
factor patch.inside = max(max(patch.edges[0], patch.edges[1]), patch.edges[2]); // Calculate the center 
control point. for(int i= 0; i< 3; i++){ patch.center += (controlPoints[i].pos2 + controlPoints[i].pos3) 
* 0.5 -controlPoints[i].pos1; } return patch; } The Tessellator unit takes the edge tessellation factors 
of the triangle as an input, and generates a semi-uniform tessellation pattern. The Domain Shader then 
takes the parametric coordinates of the generated vertices and the patch control points and attributes 
computed in the Hull Shader to evaluate both position and normal at each parametric domain. [domain("triangle")] 
DS_DATA_OUTPUT DomainShaderPN(HS_PATCH_DATA patchData, const OutputPatch<HS_CONTROL_POINT, 3> input, 
float3 uvw : SV_DomainLocation) { float u = uvw.x; float v = uvw.y; float w = uvw.z; // Output position 
is a weighted combination of the 9 position control points and the center. float3 pos = input[0].pos1 
* w*w*w + input[1].pos1 * u*u*u + input[2].pos1 * v*v*v + input[0].pos2 * w*w*u + input[0].pos3 * w*u*u 
+ input[1].pos2 * u*u*v + input[1].pos3 * u*v*v + input[2].pos2 * v*v*w + input[2].pos3 * v*w*w + patchData.center 
* u*v*w; // Output normal is weighted combination the 6 normal control points. float3 nor = input[0].nor1 
* w*w + input[1].nor1 * u*u + input[2].nor1 * v*v + input[0].nor2 * w*u + input[1].nor2 * u*v + input[2].nor2 
* v*w; // Project position to screen, transform normal, and interpolate texture coordinates. DS_DATA_OUTPUT 
output; output.position = mul(float4(pos,1), g_mViewProjection); output.normal = mul(float4(normalize(nor),1), 
g_mNormal).xyz; output.texCoord = input[0].tex * w + input[1].tex * u + input[2].tex * v; return output; 
} PN triangles have been extended by [BRS05] to support creases (but not corners) by attaching scalar 
tags to the mesh vertices in order to control the way geometry and normals are interpolated. Among the 
three types of position control points, only tangent coef.cients are modi.ed by these shape controllers. 
The artifacts along the silhouette of meshes can be improved by the method proposed in [DRS08] with the 
spirit of only increasing the geometry complexity where needed. 2.3 Approximating Catmull-Clark Subdivision 
Surfaces on Direct3D 11 Pipeline 2.3.1 Introduction The Catmull-Clark subdivision algorithm [CC78] can 
model surfaces of arbitrary topological type and has become part of standard modeling packages (e.g., 
3DMax, Maya, Softimage, Mirai, Light­wave, etc.). Catmull-Clark subdivision surfaces are widely used 
as modeling primitives in com­puter generated motion pictures, particularly for modeling characters. 
This method begins with a coarse mesh that approximates a 3d model. This coarse mesh is referred as base 
mesh or control mesh. The mesh is re.ned iteratively by inserting new vertices into the mesh, re.ning 
existing point positions, and updating the connectivity. Each re.nement step produces a denser mesh than 
the previous one. The subdivision limit surface is the smooth surface produced from this pro­cess after 
an in.nite number of re.nements. Highly detailed surfaces are generated by applying displacement maps 
to the smooth subdivision limit surfaces. The techniques of implementing exact evaluation of Catmull-Clark 
subdivision surfaces on modern GPUs fall roughly into three categories, each with its own set of advantages 
and disadvantages: Recursive evaluation [Bun05, DKT98, LMH00, SJP05] is perhaps the most intuitive ap­ 
proach, since it most closely models the mathematical de.nition of subdivision. Unfortu­nately it is 
far from the most ef.cient method. Despite heroic efforts by researches such as Shiue [SJP05], who implemented 
Catmull-Clarksubdivision in multiple pixel shader passes using spiral-enumerated mesh fragments to maximize 
parallelism, recursive evaluation is not a particularly good .t for GPU hardware due to large bandwidth 
and storage requirements. Even if it were easier to parallelize, the recursive method is incompatible 
with hardware tessellation. Recursive evaluations split one edge into two at each subdivision step. This 
re.nement produces sampling patterns which are compatible only with binary subdivision, not with future 
tessellation hardware.  Direct evaluation [Sta98]. Stam s algorithm is a better .t for programmable 
tessellation hard­ ware since it directly evaluates subdivision surfaces at arbitrary parameter values. 
However, the performance is unsatisfying for two reasons. Firstly, this algorithm requires branching, 
which reduces SIMD ef.ciency. Secondly, the required projection of control points into the eigen space 
is too complex for large meshes on the GPU. Stam s evaluation is over an order of magnitude more expensive 
than evaluation proposed in [LS08a, NYM+08, MNP08].  Precomputed basis functions [BS02]. One way to 
think about subdivision surface evaluation is as as a linear combination of basis functions with the 
vertices in the base mesh. The basis functions can be pretabulated at uniform samples since they only 
depend on the topology of a quad and its 1-ring. The total number of topological con.gurations is possibly 
a large number. This approach has the advantage of being both more inherently parallel and far more 
 cache friendly. However, it requires extensive preprocessing, and the properties of the input mesh must 
be tightly controlled in order to keep the size of the lookup tables manageable. Many [BS02, Bun05, SJP05, 
Sta98] of these approaches share an additional .aw: the inability to evaluate quads with multiple extraordinary 
vertices on the GPU. Getting rid of such multiple extraordinary quads requires at least one iteration 
of the Catmull-Clark subdivision on the CPU before the mesh is even seen by the GPU. This in turn means 
a four-fold expansion in the number of vertices that must be sent to and stored by the graphics hardware. 
Although subdivision surfaces are popular in Digital Content Creation (DCC) packages and feature .lms, 
their use has been hindered in real-time applications such as games because the exact evalu­ation of 
Catmull-Clark subdivision surfaces on modern GPUs is neither memory nor performance ef.cient. The state 
of the art in current games is to re.ne Catmull-Clark subdivision surfaces of­.ine using DCC application 
such as Maya. The resulting dense meshes do not require further runtime re.nement, but they demand signi.cant 
bus bandwidth, and consume large amounts of video memory. In addition, the of.ine re.nement process requires 
the artist to choose a .xed level of detail (LOD) . At run time, this can result in objects being drawn 
with more triangles than are actually needed. The lack of dynamic LOD support obviously lowers performance. 
The cost of animating a .xed-LOD mesh is even worse because each vertex in the dense mesh needs to be 
updated independently during animation/deformation. The computations are performed at a high frequency. 
If subdivision were deferred until after control mesh animation on the GPU, the computational overhead 
would be greatly reduced. The dif.culty of implementing exact evaluation of Catmull-Clark surfaces ef.ciently 
on the GPU led to interest in ef.cient substitutes for subdivision surfaces. With the advent of Direct3D 
11, a short explicit surface de.nition is desired over recursively de.ned Catmull-Clark subdivision surfaces 
for hardware tessellation. The alternative surface representations [LS08a, NYM+08, MNP08] have been recently 
advocated to be better suited for the highly parallel SIMD nature of modern GPU hardware, and for non-uniform, 
adaptive hardware tessellation. The idea is to replace the in.nite collection of bi-cubic patches with 
a single parametric patch to simplify the surface evaluation. The replacement patches produce smooth 
surface and closely mimic the shape of the Catmull-Clarksubdivision surface. Those surfaces can be entirely 
constructed and evaluated by local parallel operations on the GPU in real-time. 2.3.2 Patch Construction 
In Patch Construction stage, each facet with its 1-ring (Figure 20 ) in a base mesh is converted to parametric 
patch(s) in parallel. The conversion requires the computation of the control points that de.ne a parametric 
patch. We distinguish two types of facets: regular and extraordinary. A regular facet is a quad where 
each vertex has 4 neighbors and is only adjacent to quads. A facet which is not a regular facet is called 
an extraordinary facet. It is well known that a reg­ular facet can be converted to a Bicubic patch using 
standard B-Spline to B´ezier conversion rules[Far97]. Therefore, any two adjacent patches derived from 
regular facets will join C2 and reproduce exact Catmull-Clark surfaces. These regular facets should be 
separated from extraor­dinary facets and use the simpler and optimized way to convert and evaluate them. 
The chal­lenging part is how to convert the extraordinary facets to parametric patches. Generally speaking, 
a desirable conversion scheme should ensure at least G1 continuity across adjacent patches, and should 
closely approximate the equivalent subdivision surface. Of course, cost of evaluation is also an important 
factor. The overall cost of such schemes is usually in.uenced by the num­ber of control points per patch, 
the degree of the patch, and the number of patches. The recent publications[LS08a, NYM+08, MNP08] have 
made important contributions on improving shape quality and lowering evaluation cost. The key ideas of 
these schemes are summarized in the fol­lowing. We encourage readers to refer to the original papers 
for the more detailed conversion rules. The patch construction maps to the Hull Shader based on the observa­tion 
that each control point is a linear combination of the vertices in a facet with its 1-ring and the weights 
contributed from the vertices are always the same for all the patches sharing the same connectivity. 
With patch being entirely constructed in the Hull Shader, the Vertex Shader is freed to perform animation/deformation 
in the same rendering pass. In this approach, we sort all patches based on their connectivity type and 
then for each connectivity type, we pre-compute the weight of each vertex. The weights are stored in 
a stencil texture. The HLSL code is shown below. ACC_CONTROL_POINT SubDToParametricPatchHS( InputPatch<CONTROL_POINT_OUTPUT, 
M> p, uint tid : SV_OutputControlPointID, uint pid : SV_PrimitiveID ) { ACC_CONTROL_POINT output; 
 // the connectivity type ID int topo = topologyIndex[pid]; // The global ordering of the vertex int 
num = vertexCount[pid]; // invoke each thread to compute a control point ControlPointsEvaluation(pid, 
tid, topo, num); return output; } Given a list of the vertices in the 1-ring, we fetch the necessary 
set of weights from the stencil texture using the connectivity id, control point id, and input patch 
id. Each control point is computed independently per thread. To guarantee consistent evaluation of the 
weighted sum of input vertices, a global ordering of the vertex is also required. This approach maps 
well to the SIMD nature of Direct3D GPU pipeline. To optimize the overall performance, we could reduce 
the number of texture fetches since only a subset of the vertices in the 1-ring actually involve in the 
patch construction. A boolean stencil mask is precomputed to indicate which vertex has zero contribution 
and therefore to avoid the corresponding texture fetches. The size of stencil texture relates to the 
complexity of connectivity, and the connectivity can be simpli.ed by restricting the maximum valence 
as well as the number of triangles in the base mesh. Three major Approximating Catmull-Clark Subdivision 
schemes are summarized below:  ACC patches [LS08a] This scheme has been implemented in Valve s source 
engine and ILM s Tool chain (more details are available in Chapter 3 and Chapter 4). Each extraordinary 
quad (one con.guration in Figure 20) is converted to a geometry patch and a pair of tangent patches (see 
Figure 21). The geometry patch is a simple bicubic B´ezier patch. The tangent patches derived directly 
from it can not produce well-de.ned normals along the edge emitting from any vertex that does not have 
4 neighbors. In order to achieve smooth shading in these areas, both the corner vectors (u00, u03, u20, 
u23, v00, v30, v02, v32) and tangent vectors (u01, u02, u21, u22, v10, v20, v12, v22) need to be adjusted 
to satisfy smoothness constraints. Speci.cally, the corner vectors are modi.ed to satisfy the vertex 
enclosure at vertices pi,i =0..3, and interpolate the normal of the Catmull-Clark limit surface. The 
tangent vectors are derived from solving G1 continuity between two adjacent patches. The total control 
points are 40 (16 from the geometry patch and 24 from the tangent patches). The more control points to 
be derived, the more expensive computation in the Hull Shader. Since many of them are redundant, we can 
reduce the number of control points to 32. This is because the interior vectors (u10, u11, u12, u13, 
v01, v11, v21, v31) do not need to be modi.ed and they are derived directly from the geometry patch, 
they are eliminated from output control points. For further control points reduction, Loop and Shafer 
suggested to only compute b00 , b30, b03, b33 in Figure 21 top, 8 corner vectors in Figure 21 bottom, 
and 12 vertices pi, i =0..11 from the facet and its 1­ring (Figure 20). The remaining 12 geometry control 
points and 16 tangent control points are just the function of these output control points. The fewer 
control points output from the Hull Shader however increases the workload in the Domain Shader for computing 
the remaining patch control points. Choosing how many control points to be computed in the Hull Shader 
depends on which shader (the Hull Shader or the domain) is more likely to become the performance bottleneck. 
The ControlPointsEvaluation function using 32 output points in HLSL for this scheme is the following: 
float3 output.pos= float3(0,0,0); float3 output.tan= float3(0,0,0); for (int i=0; i< num; i++) { 
//use index global ordering for //consistent control points evaluation int idx = stencilIndex[pid][i]; 
 //fetch the weight of the tid_th control point //in the patch for position control point int index 
= topo * MAX_CONTROL_POINTS + idx) * 32 + tid; output.pos += p[i] * gStencil.Load(int3((index, 0,0)); 
 //fetch the weight of the tid_th control point //in the patch for tangent control point output.tan 
+= p[i] * gStencil.Load(int3((index + 16, 0,0)); }  C-patches [NYM+08] Instead of using separate channels 
for geometry and normal, each extraor­dinary quad could be converted to a single c-patch. A c-patch is 
a C1 piecewise polynomial patch with cubic boundary and de.ned by 24 control points. The surface generated 
using c-patches has well-de.ned normal everywhere. The corner control point (e.g. b300) is selected to 
interpolate the Catmull-Clark limit surface. The edge points (e.g. b210 and b120) are chosen to be the 
projected points on the tangent plane at the corner vertex (e.g. p0). The interior points b211 and b121 
are derived by the G1 constraint across two adjacent patches. By contrast, the other interior point b112 
is not pinned down by continuity constraints. This extra degree of freedom is used to increase smoothness 
at the center of the c-patch and additionally to closely mimic the shape of Catmull-Clark subdivision 
surfaces. The correspondent ControlPointsEvaluation function follows: float3 output.pos= float3(0,0,0); 
 for (int i=0; i< num; i++) { //use index global ordering for consistent control points evaluation 
 int idx = stencilIndex[pid][i]; //fetch the weight of the tid_th control point in the patch int index 
= topo * MAX_CONTROL_POINTS + idx) * 24 + tid; float weight = gStencil.Load(int3((index, 0,0)); output.pos 
+= p[i] * weight; }  Pm-patches [MNP08] The above mentioned schemes only apply to quadrilateral meshes. 
While quads naturally model the .ow of (parallel) feature lines and are therefore the main facet type 
in many models, triangular facets allow merging lines. Since the tessellator unit supports both quad 
and triangle parameterizations, Pm-patches generalize C-patches to a scheme for a polygonal mesh that 
contains quadrilaterals, triangles, pentagons and polar con.gurations. Each extraordinary n-sided facet 
corresponds to a Pm-patch that consists of n Pm sectors (Figure 23 left), where n =3 for a triangle. 
For a generalized n-sided facet, the corner point (e.g. b300) is not chosen to interpolate the position 
of the Catmull-Clark limit surface except for the case where n =4. 2.3.3 Surface Evaluation ACC patches 
[LS08a] Because normals are not derived from the geometry patch, we need to di­rectly evaluate geometry 
patch and tangent patches separately. A tensor product patch in B´ezier .m .n (m)(n) form of degree m 
by n is de.ned as: S(u,v) := pij ui(1 -u)m-ivj (1 -v)n-j , i=0 j=0 ij where pij is a control point, and 
(u,v) is a barycentric coordinate on the domain of [0,1] × [0,1]. The geometry patch has degree (3,3), 
and each tangent patch is of degree (2,3). It is straight­forward to implement them. For instance, the 
following HLSL shows an example for directly evaluating a geometry patch. float3 EvaluateGeometryPatch(float2 
uv, float3 p[16]) { // The labeling used for p[i] // 01 23 // 45 67 // 89 10 11 // 12 13 14 15 float2B0= 
(1-uv) *(1-uv) *(1-uv); float2B1=3 *(1-uv) *(1-uv) *( uv); float2B2=3 *( uv) *( uv) *(1-uv); float2B3= 
( uv) *( uv) *( uv); float3w0 =(B0.x * p[ 0]+ B1.x * p[1] + B2.x * p[2] +B3.x *p[ 3]) * B0.y; float3w1 
=(B0.x * p[ 4]+ B1.x * p[5] + B2.x * p[6] +B3.x *p[ 7]) * B1.y; float3 w2 = (B0.x * p[ 8] + B1.x * p[ 
9] + B2.x * p[10] + B3.x * p[11]) * B2.y; float3 w3 = (B0.x * p[12] + B1.x * p[13] + B2.x * p[14] + B3.x 
* p[15]) * B3.y; returnw0 +w1 +w2 + w3; } C-patches [NYM+08] and Pm-patches [MNP08] For simplicity, 
we only discuss the evaluation of Pm-patch here as C-patch is a special case of Pm-patch when the facet 
is a quad. A Pm­patch can be alternatively viewed as a set of triangular, total degree 4 patches in Bernstein-Bezier 
form as indicated in grey lines in Figure 23. This alternative representation is used for actual surface 
evaluation. The parametric domain is .rst mapped to one of the triangular patches and then transferred 
to barycentric coordinate (s,t,w) with respect to that triangle. Afterwards the Pm-patch evaluation is 
the same as the evaluation of a quartic triangular B´ezier patch. Because the resulting surfaces has 
well-de.ned normals everywhere, we could apply de Casteljau algorithm to evaluate position and normal 
at the same time to lower the evaluation cost. For a triangular B´ezier () patch with degree n, there 
are m = n+2 control points. Given qi,i =0..m -1 control points, n the triangular B´ezier patch can be 
constructed by the recurrence relation as shown in Figure 24. In each step, take linear interpolation 
based on the input barycentric coordinate (s,t,w). The recursion level goes from 1 to n. For an arbitrary 
degree n triangular patch, the total number of linear interpolation used for both position and normal 
evaluation at one parametric domain is ni+2 +1. The HLSL code for evaluating a Pm sector (or a quartic 
triangular patch) is the i=1 i following: void EvaluatePmSector(float3 stw, float3 q[15], out float3 
pos, out float3 nor) { float3 p[10]; uint k,j; k=0; // The labeling used for q[i] // 14 // 12 13 // 
910 11 // 5678 // 01 234 float s,t,w; s=stw.x; t=stw.y; w=stw.z; // The recursion level 1 [unroll] for 
(j=0; j<4; j++) { p[k++]=s*q[j]+t*q[j+1]+w*q[j+5]; } [unroll] for (j=5; j<8; j++) { p[k++]=s*q[j]+t*q[j+1]+w*q[j+4]; 
} [unroll] for (j=9; j<11; j++) { p[k++]=s*q[j]+t*q[j+1]+w*q[j+3]; } p[9]=s*q[12]+t*q[13]+w*q[14]; // 
The recursion level 2 k=0; [unroll] for (j=0; j<3; j++) { q[k++]=s*p[j]+t*p[j+1]+w*p[j+4]; } [unroll] 
for (j=4; j<6; j++) { q[k++]=s*p[j]+t*p[j+1]+w*p[j+3]; } q[5]=s*p[7]+t*p[8]+w*p[9]; // The recursion 
level 3 [unroll] for (j=0; j<2; j++) { p[j]=s*q[j]+t*q[j+1]+w*q[j+3]; } p[2]=s*q[3]+t*q[4]+w*q[5]; // 
The recursion level 4 pos=s*p[0]+t*p[1]+w*p[2]; nor=normalize(cross(p[0] -p[2], p[1] -p[0])); } Regular 
Patch The regular patch reproduces Catmull-Clark subdivision surfaces and their eval­uation cost is very 
low compared to other patches. Its smoothness feature enables us to apply de Casteljau algorithm on a 
low polynomial degree piece. It only takes 6 times of de Casteljau construction on a cubic curve as follows. 
void DeCasteljauCurve(float u, float3 p0, float3 p1, float3 p2, float3 p3, out float3 p) { float3 q0 
= lerp(p0, p1, u);  float3 q1 = lerp(p1, p2, u); float3 q2 = lerp(p2, p3, u); float3 r0 = lerp(q0, q1, 
u); float3 r1 = lerp(q1, q2, u); p = lerp(r0, r1, u); } void DeCasteljauCurve(float u, float3 p0, float3 
p1, float3 p2, float3 p3, out float3 p, out float3 dp) { float3 q0 = lerp(p0, p1, u); float3 q1 = lerp(p1, 
p2, u); float3 q2 = lerp(p2, p3, u); float3 r0 = lerp(q0, q1, u); float3 r1 = lerp(q1, q2, u); dp =r0 
-r1; p = lerp(r0, r1, u); } void EvaluateRegularPatch(float2 uv, float3 p[16], out float3 pos, out 
float3 nor) { float3 t0, t1, t2, t3; float3 p0, p1, p2, p3; DeCasteljauCurve(uv.x, p[ 0], p[ 1], p[ 
2], p[ 3], p0, t0); DeCasteljauCurve(uv.x, p[ 4], p[ 5], p[ 6], p[ 7], p1, t1); DeCasteljauCurve(uv.x, 
p[ 8], p[ 9], p[10], p[11], p2, t2); DeCasteljauCurve(uv.x, p[12], p[13], p[14], p[15], p3, t3); float3 
du, dv; DeCasteljauCurve(uv.y, p0, p1, p2, p3, pos, dv); DeCasteljauCurve(uv.y, t0, t1, t2, t3, du); 
 nor = normalize(cross(du, dv)); }  In this section, we discussed three major recent schemes that have 
also been considered to be the most ef.cient substitutes for Catmull-Clark subdivision surfaces. The 
generated shape quality by various schemes are compared in Figure 25 and Figure 26. 2.3.4 Displacement 
mapping Displacement Mapping is a technique for adding geometric details to the mesh with a height map. 
It is different from Bump Mapping or Normal Mapping in the sense that it changes the geometry by moving 
vertices often along their normal directions according to the value in the height map. The change of 
real geometry, not just normal for instance in Bump Mapping, permits parallax and self-occlusion. For 
a detailed discussion, read Section 3.5 in Chapter 3 and Section 4.3.3 in Chapter 4. 2.4 Instanced tessellation 
on current hardware While Direct3D 11 hardware is not yet available, it s possible to emulate some of 
its functionality on current generation hardware. This is very useful in order to provide fallback mechanisms 
for GPUs that do not support native tessellation yet. One of the approaches to do that is the so-called 
instanced tessellation. The idea behind this technique is to de.ne a generic re.nement pattern [BS05] 
that is replicated for each face of the input mesh. A generic re.nement pattern is just a regular tessellation 
of the tessellation domain (Figure 27). The index and vertex buffers of this re.nement pattern can be 
pre-computed for each desired tes­sellation level, and the tessellated mesh can be rendered ef.ciently 
replicating the pattern multiple times and using the Vertex Shader to evaluate the surface at each vertex 
of the re.nement pattern. A simple way of doing that is issuing a draw call for each of the re.nement 
patterns. However, that would introduce a signi.cant draw call overhead that would cause the application 
to be CPU bound. In order to avoid that most graphics APIs provide a mechanism to draw instanced geometry. 
When graphics hardware did not have instancing capabilities, it was already possible to use similar ideas 
as a form of geometry compression [For03]. Instead of dynamically instancing the tes­sellation pattern, 
the mesh resulting from statically instancing the re.nement patterns was used. However, by separating 
control point data from the parametric representation of the surface it was possible to encode it very 
ef.ciently. That was done by compressing the vertices down to 32 bits and by storing their barycentric 
coordinates as bytes, the index to the corresponding control point data as a word. While these procedures 
do not allow implementing the entire tessellation pipeline, they provide an ef.cient mechanism to expand 
geometry on the GPU in a similar way to the .xed function tessellator available in Direct3D 11. The main 
limitation is that it only allows rendering patches at the same level of detail, and that the remaining 
stages of the pipeline need to be implemented by other means. We will .rst see how to use instancing 
on some of the existing graphics APIs and then how to work around some of these limitations. 2.4.1 Instancing 
in Direct3D 9 Direct3D 9 was the .rst API to expose the instancing capabilities of the graphics hardware, 
but it was only available on hardware that supported the Vertex Shader model vs 3 0. The goal of this 
feature was to reduce draw call overhead when rendering many nearly identical instances of the same geometry 
that differed in some aspects, such as position, orientation or color. That is achieved de.ning multiple 
vertex streams and changing the access frequency of the streams, so that one is accessed for each vertex 
and the other streams containing the position and color information are accessed for each instance. For 
example, we can de.ne one vertex stream to access the parametric coordinates of the re.ne­ment pattern, 
and another vertex stream to access the control points corresponding to each patch. This approach has 
several problems: in the .rst place, there may not be enough vertex attributes for all the patch control 
points, and in the second place, loading all these attributes through the vertex fetch unit would be 
very expensive. Instead, a better approach is to only add a single per-instance attribute that contains 
an integer Instance ID that can be used to sample the corresponding control points from texture memory. 
In order to achieve that we .rst need to create a vertex declaration that indicates that the parametric 
coordinates and the instance ID are stored in separate streams: const D3DVERTEXELEMENT9 vertexDescription[] 
= { {0, 0, D3DDECLTYPE_FLOAT2, D3DDECLMETHOD_DEFAULT, D3DDECLUSAGE_POSITION, 0}, // UV coordinate 
{1, 0, D3DDECLTYPE_FLOAT1, D3DDECLMETHOD_DEFAULT, D3DDECLUSAGE_TEXCOORD, 0}, // Instance ID D3DDECL_END() 
 }; pDevice->CreateVertexDeclaration(vertexDescription, &#38;pVertexDeclaration); Then, the vertex 
buffer that will be bound to stream 0 is initialized with the parametric coordinates of the desired re.nement 
pattern, while the other (the instance buffer) is .lled with a sequence of integers starting at zero 
and containing as many elements as desired: pDevice->CreateVertexBuffer(MAX_INSTANCES * sizeof(float), 
0, 0, D3DPOOL_MANAGED, &#38;pInstanceBuffer, NULL); float * pInstanceData; pInstanceBuffer->Lock(0, 
0, (void **)&#38;pInstanceData, 0); for (UINT i = 0; i < MAX_INSTANCES; i++) pInstanceData[i] = float(i); 
 pInstanceBuffer->Unlock(); Prior to rendering, the two vertex streams are bound to the device. The 
one that contains the parametric coordinates is read per vertex, while the one that contains the Instance 
IDs is read once per instance. The device method IDirect3DDevice9::SetStreamSourceFreq is used with D3DSTREAMSOURCE 
INDEXEDDATAto specify the number of patches, and with D3DSTREAMSOURCE INSTANCEDATA to specify the frequency 
of the instance data:   // Set up the geometry data stream pDevice->SetStreamSource(0, pVertexBuffer, 
0, sizeof(float) * 2); pDevice->SetStreamSourceFreq(0, D3DSTREAMSOURCE_INDEXEDDATA | patchCount); // 
Set up the instance data stream pDevice->SetStreamSource(1, pInstanceBuffer, 0, sizeof(float)); pDevice->SetStreamSourceFreq(1, 
D3DSTREAMSOURCE_INSTANCEDATA | 1UL); Finally, the draw calls are issued as usual, and as a result, instead 
of rendering a single instance of the re.nement pattern, patchCount instances will be rendered: pDevice->DrawIndexedPrimitive(D3DPT_TRIANGLELIST, 
0, 0, vertexCount, 0, faceCount); In order to actually render a tessellated mesh, the Vertex Shader 
has to load the control points based on the Instance ID of the current patch, and evaluate the surface 
according to the loaded control points and the input parametric coordinates. The resulting shader should 
look as follows: struct AppVertex { float2 uv : POSITION; float id : TEXCOORD; // Instance ID }; 
 MeshVertex TessellationVS(AppVertex input) { float3 pos, nor; EvaluateSurface(input.id, input.uv, 
pos, nor); MeshVertex output; output.pos = mul(float4(pos, 1.0f ), WorldViewProj); output.normal = 
nor; return output; } 2.4.2 Instancing in Direct3D 10 Direct3D 10 provides the same instancing mechanisms 
as Direct3D 9, but in addition to that it also supports automatic generation of Instance IDs. That means 
that in contrast with Direct3D 9, there s no need to do any additional setup. The application simply 
renders the instanced re.nement patterns by using the ID3D10Device::DrawIndexedInstancedmethod as follows: 
pDevice->DrawIndexedInstanced(indexCount, patchCount, 0, 0, 0); In the Vertex Shader, the only required 
change is to declare the vertex Instance ID attribute with the System-Value semantic SV InstanceID: struct 
AppVertex { float2 uv : POSITION;  uint id : SV_InstanceID; }; And the hardware generates the corresponding 
Instance ID attributes automatically. An example that shows how to render a tessellated mesh in Direct3D 
9 and Direct3D 10 is available at the NVIDIA developer website. 2.4.3 Storage of control points in constant 
or texture memory Constant and texture memory are both cached. However, constant memory is optimized 
for uni­form access, that is, it s most ef.cient when a group of threads access the same memory address. 
This usage pattern is ideal for instanced tessellation, since all the vertices that belong to the same 
instance will be accessing the same control points. [DRSar] compared different approaches to store control 
points and concluded that at low tessellation levels both methods are competitive, but that at high tessellation 
levels constant memory is the most ef.cient. On Direct3D 9 the use of constant memory was not practical, 
because the number of constant registers in Vertex Shader model vs 3 0 was limited to 256 vectors and 
there was no way of ef.­ciently updating them. On the other side, in Direct3D 10 it s possible to create 
constant buffers in advance. All the constants within a constant buffer are bound simultaneously, and 
that operation is fairly ef.cient. The size of the constant buffers is also much higher, up to 4096 vectors, 
and multiple constant buffers can be bound simultaneously. However, in many cases that is still too limiting, 
which means that to render large meshes it may be necessary to partition them and render them in multiple 
passes. Texture memory may have slightly lower performance, but doesn t have the same constrains. 2.4.4 
Instancing in OpenGL Traditionally OpenGL had a lower draw call overhead than Direct3D. However, no matter 
how small the overhead, it becomes signi.cant when the number of instances is high enough. For that reason, 
the same instancing features available in Direct3D 9 and Direct3D 10 are also available in OpenGL through 
extensions. The GL ARB draw instanced extension introduces new draw calls that are conceptually equivalent 
to a series of individual draw calls. It also introduces a read-only Vertex Shader instance ID (gl InstanceID) 
variable that initially contains zero, but is incremented in each conceptual draw call. This feature 
is equivalent to the instancing functionality available in Direct3D 10. The GL ARB instanced arrays extension 
rede.nes the entry points of the GL ARB draw instanced extension, so that in addition the instance id 
variable, the Vertex Shader can also use vertex at­tributes as the source of per instance data. This 
feature is equivalent to the instancing functionality available in Direct3D 9.       An OpenGL 
example that shows how to use these extensions to implement instanced tessellation is available at the 
NVIDIA developer website. 2.4.5 Instanced tessellation with adaptive re.nement The instanced tessellation 
techniques described so far only allow rendering tessellated meshes with a .xed level of detail. While 
that may be OK in some cases, in practice it would be desirable to be able to adapt the level of detail 
based on the average area of the patch, the roughness of the displacement map on the patch surface, the 
position of the camera relative to the patch, and other factors. This is easily achievable in the Direct3D 
11 pipeline, because the tessellation levels of the patches are compute programmatically in the Hull 
Shader. When using instanced tessellation it s possible to achieve somewhat similar results, but as we 
will see, it s a bit more complicated. When the factors used to compute the level of detail do not depend 
on the position of the viewer, a simple solution is to discretize the tessellation factors into buckets 
and split the mesh to draw it in multiple draw calls, so that each draw call includes only patches that 
are in the same bucket. A sim­ple strategy is to use buckets whose corresponding tessellation levels 
are in geometric progression, for example: {1,2,4,8,16}. Then the patches within each buckets are rendered 
simultaneously using a different re.nement pattern with the corresponding level of detail. The main problem 
of this approach is that the resulting mesh will have T-Junctions along the edges of patches with different 
tessellation levels. Several solutions have been proposed to solve this problem. One of them is to create 
a matrix of re.nement patterns [BS08] in which the edges of the re.nement patterns have different tessellation 
levels, so that they match up correctly with the adjacent patches (Figure 28). However, the resulting 
matrix has three dimensions and therefore the number of re.nement patterns grows cubically with maximum 
tessellation level. This is a problem not only because of the storage requirements, but also because 
patches using different patterns would have to be rendered in a different draw calls, so the large number 
of patterns would result in performance degradation. The solution proposed by [BS08] is to constrain 
the edge tessellation factors of each patch, so that the corresponding re.nement patterns belong to a 
smaller subset within matrix of re.nement patterns. A more simple approach is provided by [DRSar], where 
T-Junctions are eliminated by using a snap function that moves boundary vertices and collapses triangles 
to make adjacent patches match up correctly (Figure 29).  In many cases the factors used to determine 
the patch tessellation levels cannot be statically pre­computed, that is the case when the factors are 
view dependent or when the mesh is animated. In those cases, it s not possible to precompute the bucket 
that each patch belongs to, but instead the buckets need to be assigned dynamically. [Tat08] and [Cas08a] 
propose the use of one render queue for each bucket. These render queues can be updated easily in the 
CPU, but that requires an expensive CPU readback. [DRSar] proposes the use of CUDA to build the render 
queues entirely in the GPU using one pre.x sums per bucket. 2.4.6 Emulating the Vertex Shader and the 
Hull Shader One of the remaining differences between the Direct3D 11 tessellation pipeline and the instanced 
tessellation pipeline is that instanced tessellation does not provide a mechanism to animate and convert 
convert points, so these tasks need to be emulated in a different way. The most simple approach is to 
simply perform these tasks in the CPU as described in 3.3. How­ ever, [NYM+08] also shows that in Direct3D 
10 it s possible to keep these tasks in the GPU performing animation and conversion of control points 
in multiple rendering passes by using both the Vertex Shader and the Geometry Shader, and relying on 
the Stream-Output Stage to output the results of each pass to the next. 2.4.7 Vertex cache optimizations 
As described in section 2.4, it s possible to emulate native hardware tessellation by rendering a regular 
re.nement pattern multiple times. In order to accomplish that in the most ef.cient way possible it s 
necessary to optimize the layout of this pattern or grid in order to minimize the number of vertex transforms. 
Figure 30 shows that the results are suboptimal with a straightforward way to lay out the vertices of 
a grid. Instead of using the straightforward layout, its possible to traverse the triangles in Morton 
or Hilbert order, which are known to have better cache behavior. Another possibility is to feed the triangles 
to any of the standard mesh optimization algorithms such as K-Cache-Reorder [LY06], Tipsy [SNB07] and 
Forsyth [For06]. All these options are better than not doing anything, but still produce results that 
are far from the optimal. Table 1 shows the results obtained for a 16 × 16 grid and with a FIFO cache 
with 20 entries. ACMR stands for the average cass miss ratio (number of vertex transforms per primitive), 
Method ACMR ATVR Scanline 1.062 1.882 Tipsy 0.885 1.567 NVTriStrip 0.818 1.450 Morton 0.719 1.273 K-Cache-Reorder 
0.771 1.260 Hilbert 0.666 1.239 Forsyth 0.666 1.180 Optimal 0.564 1.000 Table 1: The results on a 16 
× 16 grid and with a FIFO cache with 20 entries and ATVR for average transform to vertex ratio (number 
of vertex transforms per vertex). We can see that even the best algorithms produce almost 20% more vertex 
transforms than what we could achieve in the optimal case. The most important observation is that, for 
every row of triangles, the only vertices that are reused are the vertices that are at the bottom of 
the triangles, and these are the vertices that we would like to have in the cache when rendering the 
next row of triangles. When traversing triangles in scanline order the cache interleaves vertices from 
the .rst and second row. However, we can avoid that by prefetching the .rst row of vertices (Figure 31 
left). That can be done by issuing degenerate triangles. Once the .rst row of vertices is in the cache, 
we can continue adding the triangles in scanline order. The interesting thing now is that the vertices 
that leave the cache are always vertices that are not going to be used anymore (Figure 31 right). In 
general, the minimum cache size to render a W×W grid without transforming any vertex multiple times is 
W +2. The degenerate triangles introduce a small overhead, so it s also desirable to avoid them when 
the cache is suf.ciently large to store two rows of vertices. When the cache is too small you also have 
to split the grid into smaller sections and apply this method to each of them. The following code accomplishes 
that: void gridGen(int x0, int x1, int y0, int y1, int width, int cacheSize) { if (x1 -x0 + 1 < cacheSize) 
{ if (2 * (x1 -x0) + 1 > cacheSize) { for(intx = x0;x < x1;x++) { indices.push_back(x + 0); indices.push_back(x 
+ 0); indices.push_back(x + 1); } } for (inty =y0; y <y1; y++) { for(intx = x0;x < x1;x++) { indices.push_back((width 
+ 1) * (y + 0) + (x + 0)); indices.push_back((width + 1) * (y + 1) + (x + 0)); indices.push_back((width 
+ 1) * (y + 0) + (x + 1)); indices.push_back((width + 1) * (y + 0) + (x + 1)); indices.push_back((width 
+ 1) * (y + 1) + (x + 0)); indices.push_back((width + 1) * (y + 1) + (x + 1)); } } } else { int xm 
= x0 + cacheSize -2; gridGen(x0, xm, y0, y1, width, cacheSize); gridGen(xm, x1, y0, y1, width, cacheSize); 
 } } This may not be the most optimal grid partition, but the method still performs pretty well in those 
cases. Table 2 and Table 3 show the results for a cache with 16 entries and 12 entries respectively. 
In all cases, the proposed algorithm is signi.cantly faster than the other approaches. Method ACMR ATVR 
Scanline 1.062 1.882 Tipsy 0.771 1.260 NVTriStrip 0.775 1.374 Morton 0.750 1.329 K-Cache-Reorder 0.766 
1.356 Hilbert 0.754 1.336 Forsyth 0.699 1.239 Optimal 0.598 1.059 Table 2: Results using the optimal 
grid partition with 20 entries. Table 3: Results using the optimal grid partition with 12 entries Method 
ACMR ATVR Scanline 1.062 1.882 Tipsy 0.758 1.343 NVTriStrip 0.875 1.550 Morton 0.812 1.439 K-Cache-Reorder 
0.807 1.491 Hilbert 0.797 1.412 Forsyth 0.859 1.522 Optimal 0.600 1.062 2.5 Watertight Tessellation 
The Direct3D 11 hardware tessellation of polynomial surfaces starts with a coarse input mesh. Since each 
parametric patch is independently constructed by a facet and its 1-ring neighborhood, the input mesh 
is split into a set of patch primitives to take full advantage of the GPU SIMD (Single Instruction, Multiple 
Data) parallelism. Each patch primitive is .rst converted to a polynomial surface patch and then evaluated 
at parametric domains. Due to independent parallelism of patch evaluation, the surface tessellation is 
subject to round off error and cracking along the boundary of two adjacent tessellated patches. 2.5.1 
Watertight Position Evaluation Position water-tightness means cracks/holes free. The base mesh is .rst 
transformed to control points that de.ne a set of patches in the Hull Shader. the Domain Shader evaluates 
positions and normals of the .nal surface at parametric domains. The positions of all points along the 
shared edge of two adjacent patches should have the same value; otherwise, we will see cracks/holes in 
the mesh.The cracks can be caused by implementation errors. Even if the implementation is completely 
correct, small cracks may appear because of limited .oating point precision. Due to the .oating point 
precision issue in the hardware, addition is not always commutative as it should be. To prevent cracks/holes 
caused by this limitation, we need to take care of the following two phases: Control Points Derivation 
Three types of control points (corner point, edge point and interior point) are derived from a face (either 
quad or triangle) and its 1-ring. The problems come from computing corner points and edge points because 
they are shared by multiple patches. For a closed surface, a corner point is computed n times given it 
is surrounded by n faces. If the computa­tion result differs among adjacent patches, the cracks occur 
on the .nal surface. For example in Figure 32, the corner point v is derived as the point on the Catmull-Clark 
limit surface by taking linear combinations of its neighbors p0,p1,p2,p3,p4 and p5. If the summation 
order is inconsistent among patch constructions for A,B,C, non-commutativity of .oating point addition 
might lead to slightly different value. As a result, the tessellated surface won t be water-tight. The 
similar problem applies to edge points as each edge point is computed twice by adjacent patches. The 
solution is to ensure the consistent ordering of 1-ring. The 1-ring connectivity is usually stored in 
textures. If two adjacent patches are of the same type, the shared control points are computed using 
the same stencil. However, we need to be careful if their patch types are different. For all the schemes 
intro­duced in Section 2.3 the edge points and corner points are always evaluated using the same rules 
regardless of patch type. Only the face/interior points differ. Since only corner and edge points are 
possibly shared, control points evaluation from different patch types do not make difference than from 
two abutting patches of the same type in terms of water-tightness. Patch Evaluation Water-tight patch 
evaluation only needs to be taken care along the shared boundary of two patches. The shared boundary 
in methods[LS08a, NYM+08, MNP08, YNM+] is essentially a cubic curve. Speci.cally, n . . p(u) = ' bi n 
i u i(1 -u)n-i . (11) i=0 Two possible parametric directions along the boundary disagree on the same 
.nal position because a + b + c + d = d+ b + c + a in hardware using IEEE .oating point representations. 
We need to make sure the addition always take the same order. This can be achieved by either consistent 
parametric orientation or symmetric evaluation along the shared edge. The .rst solution works on quadrilateral 
meshes. By sorting all patches in consistent parametric direction along the same edge as shown in Figure 
33 to remove orientation ambiguity. For generic mesh, we apply symmetric evaluation. One example is to 
use symmetric de Casteljau algorithm. Figure 34 illustrates how it works to iteratively evaluate polynomials 
in Bernstein form in 2D. Assume these two adjacent patches are Pa and Pb. The evaluation from patch 
Pa: b10 := ulb00 + urb01, (12) b12 := ulb01 + urb02, b13 := ulb02 + urb03. Switch the orientation and 
now take the evaluation from patch Pb: b10 := urb01 + ulb00, (13) b12 := urb02 + ulb01, b13 := urb03 
+ ulb02. Because A + B = B + A in .oating point representation, both patches lead to the same values 
for b10,b20,b30. The same principle applies to every re.nement step. Therefore, the .nal position on 
the curve is the same from both patches. More description of evaluation using de Casteljau algorithm 
can be found in Section 2.3.3. 2.5.2 Watertight Normal Evaluation When implementing displacement mapping, 
the normal computation at each shared point needs to reach the same value. Otherwise, discontinuities 
might appear in the highly-detailed shapes as the tessellated vertex is displaced to a slightly different 
position because of inconsistent normal .elds. At a shared corner vertex, each patch contributes to a 
pair of tangent, bitangent vectors. Although theoretically each pair should agree with each other, in 
the digital word with .oating point representations no pairs are exactly the same. This issue is also 
raised from shared edges. One solution is to de.ne ownership at each shared vertex and edge. Namely, 
only one patch owns the shared vertex/edge. For normal evaluation at those problematic areas, we apply 
the normal .eld computed from the patch that owns the vertex/edge. This idea is similar to Zippering 
Method, which will be introduced in the next section. 2.5.3 Watertight Texture Seams The other problem 
in watertight displacement mapping is dealing with texture seams. Texture seams are discontinuities in 
the parametrization. These discontinuities are always necessary unless the mesh has the topology of a 
disk, but in general meshes need to be partitioned into charts that are then parameterized independently. 
A Simple Solution A simple solution to this problem is to manually alter the geometry along the seams 
to make it self intersect. While that does not result in watertight surfaces, the resulting holes are 
not visible. The main problem of this approach is that it requires artist intervention, creates open 
meshes, and only works for opaque surfaces; it would be nice to have more robust solutions. Seamless 
Parametrization Methods Fortunately this problem has been studied extensively and seamless parametrization 
methods have been developed. These are automatic parameterizations in which the chart texels are properly 
aligned across boundaries. Displacement maps are generally not painted using traditional image editing 
applications, but created in specialized sculpting tools (such us ZBrush R® MudboxTM) or generated procedurally 
in attribute capture tools like ®and Autodesk RxNormal. That means that it is possible to store displacements 
using automatic parameterizations, since the artist does not need to edit the texture manually. The most 
straightforward seamless parametrization method is the one used by ZBrush, which is very similar to the 
ones proposed in [CH02]. ZBrush maps every face of the mesh to a quad in texture space, so that all edges 
are either vertical or horizontal, and have the same length. This method is very easy to implement, but 
has several problems: It introduces a large number of seams in the mesh, which increments the number 
of vertex transforms.  It does not make ef.cient use of the texture space, because all faces, independently 
of their area, are mapped to quads of the same size.  ZBrush provides an option to group faces into 
larger charts while preserving the edge length and orientation, which helps reducing the number of vertex 
transforms. It also has an option to scale the charts in proportion to their surface area, but that breaks 
the seamlessness. In order to alleviate these problems, another solution is to use rectangular charts 
(instead of single faces) to map them to texture-space quads. That was .rst proposed in [PCK04], where 
rectangular charts are created by .rst clustering polygons into arbitrary sided charts and then partitioning 
them into quadrilaterals using a Catmull-Clark-inspired scheme: An entirely different approach is described 
in [DBG+06], where a quadrangulation is constructed from the optimized Morse-Smale complex of the natural 
harmonics of the surface. The main limitation of this approach is that it only handles closed surfaces, 
and requires manual selection of the eigenfunction to produce charts of the desired size. [JH08] solves 
these problems and also adds support for explicit constraints. These two methods create quadrangulations 
without T-Junctions. Although that may seem a nice property, [CHCH06] shows that its possible to remove 
that constraint and still achieve smooth parametrization, resulting in better parametrization with less 
distortion. Another interesting method is the one described in [TACSD06]. This method is even more .exible; 
instead of de.ning charts before the parametrization, it introduces singularity points in the mesh and 
computes the parametrization globally. Then the mesh can be cut connecting the singularity points arbitrar­ily. 
However, the resulting parametrization exhibit sharp spikes at the singularity points. Other methods 
try to achieve continuity between patches using constraints and parameterizing adjacent patches simultaneously. 
That is for example the case of [KSG03], but it only minimizes the dis­ continuities and does not fully 
remove them. For more information about parametrization methods in general, [FH05] and [SPR06] provide 
an overview of most parametrization methods available to date. Water-tightness and Texture Mapping Precision 
Modern hardware uses a .oating point represen­tation to interpolate texture coordinates. That can cause 
problems, because .oating point values have more precision closer to the origin than farther from it 
[Goldberg91]. As a result, interpola­tion of texture coordinates along an edge closer to the origin will 
produce a different set of samples than interpolation along an edge that is farther from it. This is 
exactly what happens on texture seams and will result in small cracks in the mesh even when using a seamless 
parametrization. When using programmable tessellation hardware as speci.ed by Direct3D 11, interpolation 
is per­formed explicitly in the Domain Shader (or in the Vertex Shader when using instanced tessellation 
on older GPUs). That s what enables the use of higher order interpolation, but it also allows the use 
of .xed point instead of .oating point for interpolation. However, .xed point interpolation alone does 
not solve all the problems. Another problem is that bilinear interpolation of texture samples is not 
symmetric. Sampling at . between two adjacent texels does not produce the same result as sampling at 
1-. when the values of the texels are reversed. This is also true for nearest .ltering, because the result 
of sampling at 0.5 is unde.ned. For this reason, none of the seamless texture mapping algorithms solve 
the water-tightness problem entirely. So, it s necessary to use other methods. Zippering Method A different 
approach is to introduce a triangle strip connecting the vertices along the seam. These strips can be 
generated with the same tessellation unit used to generate the patches, by setting the tessellation level 
equal to 1 in one of the parametric directions. This solves the problem nicely, but requires rendering 
more patches, and introduces additional triangles that in most cases are nearly degenerate. Another interesting 
solution is the zippering method proposed in [Sander03]. The idea is to sample the displacement (or the 
geometry image) on both sides of the seam and to use the average of the two samples. The main problem 
of this approach is that it requires two texture samples along the seams, which means you have to take 
two samples in all cases, or use branching to take an extra sample on the seam vertices only. However, 
the averaging method does not work for corners. Along the edges there are only two possible displacement 
values, one for each side of the seam, but on corners there are more than two. Storing an arbitrary number 
of texture coordinates, and taking an arbitrary number of texture samples would be too expensive. A simple 
solution is to snap the corner texture coordinates to the nearest texel, and make sure that the displacement 
value for that vertex is the same for all patches that meet at that corner. A cheaper solution that 
only requires a single texture sample and handles corners more gracefully is to de.ne patch ownership 
of the seams [Cas08b, Cas08a]. By designating the patch owner for every edge and corner, all patches 
can agree what texture coordinate to use when sampling the displacement at those locations. That means 
that for every edge and for every corner we need to store the texture coordinates of the owner of those 
features. That is a total of 4 texture coordinates per vertex, (16 for quads and 12 for triangles). At 
runtime, only a single texture sample is needed; the corresponding texture coordinate can be selected 
with a simple calculation: // Compute texture coordinate indices (0: interior, 1,2: edges, 3: corner) 
 int idx0 = 2 * (uv.x == 1) + (uv.y == 1); int idx1 = 2 * (uv.y == 1) + (uv.x == 0); int idx2 = 2 * (uv.x 
== 0) + (uv.y == 0); int idx3 = 2 * (uv.y == 0) + (uv.x == 1); // Barycentric interpolation of texture 
coordinates float2 tc = bar.x * texCoord[0][idx0] + bar.y * texCoord[1][idx1] + bar.z * texCoord[2][idx2] 
+ bar.w * texCoord[3][idx3]; In the averaging method we would have to store the texture coordinate of 
every patch that con­tributes to a shared feature. Edges are shared by only two patches, but corners 
can be shared by many patches. By de.ning the ownership of the shared features (corners and edges), we 
only have to store the texture coordinates of the patch that owns the corresponding feature. So, we have: 
 4 texture coordinates for the interior (4).  2 texture coordinates for each edge (8).  1 texture coordinate 
for each corner (4).  Therefore, the total number of texture coordinates per patch is: 4+8+4 = 16. Deciding 
what patch owns a certain edge or corner is done as a pre-process, so that the patch texture coordinates 
can be computed in advance. The way we store these texture coordinates is shown in Figure 37. Each vertex 
has: one interior texture coordinate. (index 0) one edge texture coordinate for each of the edges. 
(index 1 and 2) one corner texture coordinate. (index 3) On the interior, we interpolate the interior 
texture coordinates bi-linearly: float2 tc = bar.x * texCoord[0][0] + bar.y * texCoord[1][0] + bar.z 
* texCoord[2][0] + bar.w * texCoord[3][0]; where bar stands for the barycentric coordinates: bar.x = 
( uv.x) * ( uv.y); bar.y = (1 -uv.x) * ( uv.y); bar.z = (1 -uv.x) * (1 -uv.y); bar.w = ( uv.x) * (1 
-uv.y); On the edges we interpolate the edge texture coordinates linearly: if (uv.y == 1) tc = texCoord[0][1] 
* bar.x + texCoord[1][2] * bar.y; if (uv.y == 0) tc = texCoord[2][1] * bar.z + texCoord[3][2] * bar.w; 
if (uv.x == 1) tc = texCoord[3][1] * bar.w + texCoord[0][2] * bar.x; if (uv.x == 0) tc = texCoord[1][1] 
* bar.y + texCoord[2][2] * bar.z; And at the corners we simply select the appropriate corner texture 
coordinate: if (bar.x == 1) tc = texCoord[0][3]; if (bar.y == 1) tc = texCoord[1][3]; if (bar.z == 1) 
tc = texCoord[2][3]; if (bar.w == 1) tc = texCoord[3][3]; The same thing can be done more ef.ciently 
using a single bilinear interpolation preceded by some predicated assignments: // Interior float2 t0 
= texCoord[0][0]; float2 t1 = texCoord[1][0]; float2 t2 = texCoord[2][0]; float2 t3 = texCoord[3][0]; 
 // Edges if (uv.y == 1) { t0 = texCoord[0][1]; t1 = texCoord[1][2]; } if (uv.y == 0) { t2 = texCoord[2][1]; 
t3 = texCoord[3][2]; } if (uv.x == 1) { t3 = texCoord[3][1]; t0 = texCoord[0][2]; } if (uv.x == 0) { 
t1 = texCoord[1][1]; t2 = texCoord[2][2]; } // Corners if (bar.x == 1) t0 = texCoord[0][3]; if (bar.y 
== 1) t1 = texCoord[1][3]; if (bar.z == 1) t2 = texCoord[2][3]; if (bar.w == 1) t3 = texCoord[3][3]; 
 float2 tc = bar.x * t0 + bar.y * t1 + bar.z * t2 + bar.w * t3; And .nally, the predicated assignments 
can be simpli.ed and replaced by an index calculation: // Compute texture coordinate indices (0: interior, 
1,2: edges, 3: corner) int idx0 = 2 * (uv.x == 1) + (uv.y == 1); int idx1 = 2 * (uv.y == 1) + (uv.x == 
0); int idx2 = 2 * (uv.x == 0) + (uv.y == 0); int idx3 = 2 * (uv.y == 0) + (uv.x == 1); float2 tc = 
bar.x * texCoord[0][idx0] + bar.y * texCoord[1][idx1] + bar.z * texCoord[2][idx2] + bar.w * texCoord[3][idx3]; 
 The same idea also applies to triangles: // Interior float2 t0 = texCoord[0][0]; float2 t1 = texCoord[1][0]; 
float2 t2 = texCoord[2][0]; // Edges if (bar.x == 0) { t1 = texCoord[1][1]; t2 = texCoord[2][2]; } if 
(bar.y == 0) { t2 = texCoord[2][1]; t0 = texCoord[0][2]; } if (bar.z == 0) { t0 = texCoord[0][1]; t1 
= texCoord[1][2]; } // Corners if (bar.x == 1) t0 = texCoord[0][3]; if (bar.y == 1) t1 = texCoord[1][3]; 
if (bar.z == 1) t2 = texCoord[2][3]; float2 tc = bar.x * t0 + bar.y * t1 + bar.z * t2; And the resulting 
code can be optimized the same way: int idx0 = 2 * (bar.z == 0) + (bar.y == 0); int idx1 = 2 * (bar.x 
== 0) + (bar.z == 0); int idx2 = 2 * (bar.y == 0) + (bar.x == 0); float2 tc = bar.x * texCoord[0][idx0] 
+ bar.y * texCoord[1][idx1] + bar.z * texCoord[2][idx2]; Partition of Unity The zippering methods 
produce watertight results independently of the param­eterizations. However, if the parameterizations 
is not seamless or if the features of the displacement map are different on each side of the seam, then 
that will result in sharp discontinuities, not holes, but undesirable creases along the seams. These 
problems can be avoided using a seamless parame­terizations and generating the displacement maps making 
sure that the displacements match along the seams. However, another solution is to use a partition of 
unity as proposed by [PB00]. A parti­ tion unity is a method to combine multiple texture parameterizations 
to produce smooth transitions between them. The idea is to de.ne transition regions around the seams, 
so that on those regions both parameterizations are used to sample the texture and the results are blended 
smoothly. The zippering methods described before are just a special case of a partition of unity in which 
the blend function is just the unit step function. Conclusion There are many different solutions to achieve 
watertightness when sampling of dis­placement maps. We advocate the use of zippering methods, since they 
do not impose any restric­tion on the parameterizations of the mesh and work with arbitrary displacement 
maps. They are easy to implement and do not add much overhead to the shaders, even though they increase 
the number of texture coordinates. Note that even when using zippering methods to guarantee wa­tertightness, 
the use of seamless (or nearly seamless) parameterizations is still valuable, because they eliminate 
any visible crease or discontinuity along the seams. These artifacts can also be avoided by combining 
multiple parameterizations using a partition of unity, but these methods are too expensive to be practical. 
3 Approximate Subdivision Surfaces in Valve s Source Engine Jason Mitchell, Valve 3.1 Introduction At 
Valve, we have invested early in GPU-friendly approximations to displaced Catmull-Clark sub­division 
surfaces with the expectation that this will accelerate our ability to exploit hardware tessel­lation 
as it becomes available in Direct3D 11 hardware [Gee08]. Our software architecture follows the Direct3D 
11 pipeline architecture in order to ease the eventual migration to hardware. As a result, we have been 
able to address implementation details speci.c to mapping Loop and Schae­fer s Approximate Catmull-Clark 
(ACC) [LS08a] scheme to Direct3D 11 such as those discussed in Section 2.3 and have even extended ACC 
to include support for hard creases [KMDZ09]. In this chapter, we will describe our system, including 
the run-time characteristics of two different implementations, extensions necessary to support displacement 
mapping and implications for our authoring pipeline. 3.2 Motivation Higher-order surfaces have long been 
standard in the .lm industry due to their ability to compactly represent high quality smooth surfaces. 
In the real-time space, we anticipate widespread adoption of higher-order surface schemes as GPU compute 
density continues to outstrip memory and mem­ory bandwidth, particularly when it comes to console designs 
which tend to be especially memory constrained. In addition to the pure compute-related motivations for 
moving tessellation and sur­face evaluation onto the GPU, higher-order surfaces have a number of other 
desirable properties. Higher-order surfaces have a natural LOD mechanism, as they can be arbitrarily 
tessellated to trade off quality and performance. We are interested in the ability to author assets which 
allow us to scale both up and down. That is, we want to build a model once and be able to scale it up 
to .lm quality using tessellation and displacement mapping for use in of.ine-rendered movies as well 
as future hardware. Conversely, we want to be able to naturally scale the quality of an asset down to 
meet the needs of real-time rendering on a given system. We expect that such models can be tailored to 
dove­tail with traditional polygon rendering both at runtime and in the art pipeline, which is essential 
when assets must be reused across hardware with varying performance levels and feature support (including 
no higher-order surface support at all). Another nice advantage of higher order surfaces is that their 
use is orthogonal to most shading techniques, including many popular rendering trends such as screen 
space techniques and deferred rendering, which are agnostic to the upstream geom­etry representation 
that populates their image-space inputs[ST90][DWS+88][Mit07][Val07]. For these reasons, we have advocated 
the implementation of tessellation hardware and have chosen to take on the risk of investing in this 
technology prior to hardware availability. 3.3 Software Pipeline We have mapped the Direct3D 11 pipeline 
onto Di­rect3D 9, including instanced and native tessellation codepaths, where the vertex shader and 
hull shader are implemented in software on the CPU and the re­maining stages are executed on the GPU 
as shown in Figure 39. The CPU-side vertex shading operations include skinning, vertex morphing and other 
opera­tions which are appropriate to perform at the control mesh level. Post-transform control mesh vertices 
are then sent to a threaded and SIMD-optimized software hull shader where they are mapped to a set of 
B´ezier patches using our technique. This data is copied asyn­chronously to a .oating point texture map 
in GPU memory for subsequent processing by the GPU. With the control points sitting in a .oating point 
tex­ture, domain points are instantiated with appropriate mesh connectivity on the GPU using either hardware 
Figure 39: DX11 pipeline using instanced &#38; native tessellation on DX9 instancing or ATI s native 
hardware tessellator. Af­ter this on-chip data ampli.cation, the GPU s vertex shader playing the role 
of domain shader evaluates B´ezier patch positions and tangent frames at the newly-generated vertices 
using B´ezier control points fetched from the .oating point texture generated by the CPU-side hull shader. 
In our case, the Catmull-Clark to ACC conversion was done on the CPU, though it can be performed on the 
GPU as is done by the SubD10 sample in the DirectX SDK as well as by [NYM+08]. We can then instantiate 
domain points using either instanced tessellation or native tessellation. As discussed in Section 2.4, 
instanced tessellation is available in shader model 3.0 hardware with vertex texture fetch capabilities 
such as NVIDIA GeForce 8x00 and ATI RADEON HD 2x00 and newer GPUs. Native tessellation is available in 
the XBox 360 as well as ATI s Direct3D 10 class GPUs [Lee06] [Tat07]. 3.3.1 Native Tessellation ATI s 
hardware tessellator instantiates the vertex shader at u,v points in the [0..1]2 domain and provides 
the shader with access to all of the super-primitive data from the input vertices [Lee06] [Tat07]. In 
ATI s terminology, super-primitive refers to the notion that the shader has access to the attributes 
of the vertices de.ning the whole primitive in our case, all four vertices de.ning each quad. This is 
in contrast to a traditional vertex shader which only has access to the attributes of a single vertex, 
with no primitive-level information. The ATI model behaves much like a limited form of domain shader 
in that the shader has access to primitive-level data, albeit much less data than is made available to 
a Direct3D 11 hardware domain shader. In our implementation of native tessellation the domain/vertex 
shader uses this super-primitive data and fetched B´ezier patch data to evaluate patch attributes. The 
remainder of the graphics pipeline is unchanged, so an implementor need only alter exist­ing vertex shaders 
and vertex buffer layouts to take advantage of tessellation. In Valve s Source engine, minimal changes 
were necessary to add this functionality to existing production-tested ver­tex shaders, though we did 
run into some limitations of the Direct3D 9 vertex shader programming model which we will discuss below. 
3.3.2 Performance In our architecture, it has been fairly straightforward to maintain both instanced 
and native tessel­lation codepaths so that we can measure tradeoffs of the two approaches. For example, 
as we will discuss below, we have measured performance advantages in the instancing path. Despite this, 
it is convenient to maintain the native tessellation method, both as a means of cross-checking algorith­mic 
details as well as to gain access to the additional features provided by native tessellation such as 
easy access to .oating point tessellation and separate per-edge LOD functionality necessary to implement 
adaptive subdivision. CPU Performance As discussed in section 3.3, we perform vertex and hull shading 
on the CPU using a software architecture modeled on the Direct3D 11 pipeline. In our measurements, the 
pri­mary bottleneck in the CPU hull shader s conversion from Catmull-Clark to B´ezier patches using ACC 
is the computation of the tangent patches. As mentioned in Section 2.3.2, quad meshes are typically comprised 
of a mix of regular and extraordinary patches, each with different properties. Because of the nature 
of ACC, we can avoid the signi.cant performance cost of computing tangent patches for the regular patches 
of the mesh since they are not required. Hence, the overall perfor­mance is dependent on the mix of valences 
in the mesh being converted, where a mesh consisting of only regular patches could see as much as a twofold 
performance increase over a wholly extraor­dinary mesh. Additionally, we have vectorized the conversion 
math using CPU SIMD operations, lookup tables and loop unrolling, resulting in roughly a 2x speed improvement 
relative to our orig­inal CPU implementation. Further, since hull shader invocations are independent 
of one another in the Direct3D 11 pipeline architecture, it is natural to split this computation across 
CPU multiple cores. Threading the hull shader invocations resulted in an additional 3.58x performance 
improve­ment on 4 cores for meshes between 1000 and 10,000 patches. Naturally, these CPU speedups are 
independent of the chosen GPU data ampli.cation method (instanced or native tessellation). GPU Performance 
On the GPU side of the bus, we can compare the performance of instanced and native tessellation. In Table 
4, we compare instanced and native tessellation performance of the datasets shown in Figure 40 using 
the ATI RADEON 4870 X2, which is capable of running both codepaths. Native Tessellation Instanced Tessellation 
Mesh N=3 N=9 N=15 N=3 N=9 N=15 Car Ship Poly 1344 1245 747 1296 326 160 589 137 65 1550 1196 532 1301 
473 304 846 222 132 Table 4: Performance Comparisons -The Car, Ship and Poly models contain 1164, 5180 
and 10618 quad faces. Performance numbers are in frames per second, measured on an Intel Quad Core Q9450 
2.66GHz and ATI RADEON 4870 X2. N = number of tessellated vertices per control mesh edge. In our tests, 
we have often seen the instanced path perform twice as fast as the native path. As you would expect, 
both techniques perform the same number of texture operations. Due to differences in the hardware interfaces, 
however, the native tessellation shader uses roughly 16% more instruc­tions than the instanced patch 
shader. This minor difference does not explain the 2x performance delta, leading us to conclude that 
the performance difference is not entirely related to shader length, but rather, is related to deeper 
hardware implementation details. Graphics Pipeline State Our measurements indicate that ATI s native 
tessellation path seems to be more impacted by the rest of the pipeline state, notably the complexity 
of the pixel shader and the number of interpolators output from the vertex shading unit to triangle setup. 
The numbers in Table 4 were generated with a vertex shader which outputs two interpolators to a trivial 
pixel shader. If we output .ve interpolators to a 22 instruction pixel shader, we measure a 1.8x to 2x 
per­formance hit when using native tessellation. The instanced tessellation path sees no performance 
penalty with the same change. Given that our production shaders frequently max out the number of vertex 
shader outputs (up to 10 4D vectors in Direct3D 9), the instanced tessellation path has a signi.cant 
performance advantage in practice. Regular vs Extraordinary Patches Through our own tests and using hardware 
analysis tools such as NVPerfHUD, we have determined that both the instanced and native hardware tessellation 
shaders are vertex texture fetch bound. Each invocation of the domain shader performs 30 fetches of packed 
control point data (12 for the control points, and 9 for each of the two tangent patches). For regular 
patches (with all vertices of valence 4), we can avoid fetching the tangent patch control points and 
use the de Casteljau algorithm (described in Section 2.3.3) to compute both positions and normals. This 
saves 18 texture fetches for these patches at the expense of drawing regular and extraordinary patches 
with two API calls rather than one. In this case, we measured a 20% (1.9ms) performance improvement in 
GPU evaluation cost at N = 33 for the rocket frog mesh by splitting evaluation of regular and extraordinary 
patches. In addition, we avoid calculating tangent patches for regular patches when converting from Catmull-Clark 
to ACC in the (CPU) hull shader. In our implementation, this saves an additional 0.26 ms of CPU time 
on the rocket frog. In practice, care should be taken with small meshes (<2000 patches) with few regular 
patches. Separating regular and extraordinary vertices requires two API calls as opposed to just one 
and the CPU-side overhead of this extra API call can outweigh the savings gained by reducing the shader 
load for regular patches. Additionally, we have found it advantageous (and in some cases necessary) to 
keep the use of vertex shader general purpose registers to a minimum, particularly when combining patch 
evaluation with some of the more advanced vertex shaders that we have used in recent games such as Team 
Fortress 2, Portal, Left 4 Dead and the Half-Life 2 series. To reduce the number of GPRs, we reorganized 
the shader code to split the loading and evaluation of the geometry patch and the loading and evaluation 
of the tangent patches, allowing GPRs to be reused between position evaluation and tangent evaluation. 
This made the implementation somewhat awkward, but the Direct3D 9 vertex shader programming model simply 
exhausted its general purpose register bank without such shader massaging. Staying On Chip It is worth 
noting that the Direct3D 11 hardware pipeline is designed to elim­inate the need for the domain shader 
to fetch control point data from memory. That is, once all stages of this approximate Catmull-Clark rendering 
method have migrated to Direct3D 11 GPUs, the transmission of control point data from the hull shader 
to the domain shader will not require the control points to ever reside in off-chip memory. The control 
points will only exist .eetingly on chip, computed at patch granularity by the hull shader and used for 
evaluation at post-tessellated vertex granularity by the domain shader. It is the hope of API, hardware 
and game designers that this drastic reduction in memory traf.c will greatly increase the GPU performance 
of any higher­order surface scheme which is executed on Direct3D 11 hardware. It will be exciting to 
see how this plays out over the coming years. 3.4 Creases Though Microsoft and its hardware partners 
had Loop and Schaefer s ACC technique in mind when developing the Direct3D 11 pipeline, each new stage 
has remained programmable so that developers can customize the functionality to suit their needs in a 
variety of ways. At Valve, the programmability of the Direct3D 11 architecture has allowed us to extended 
ACC to support hard creases [KMDZ09]. This additional functionality has no measurable impact on performance 
and, in fact, the tessellator and domain shader are no different than they are in the original ACC technique. 
While subdivision surfaces allow us to compactly represent smooth geometry, we sometimes wish to incorporate 
hard creases into our art. On the left side of Figure 41, we see an example of a car modeled with Catmull-Clark 
subdivision surfaces and rendered with ACC. While this yields a smooth, high quality result with no visible 
polygonal artifacts, many areas of the car are overly smooth and do not capture the shape that the artist 
intended to convey. On the right side of Fig­ure 41, we see the same car with numerous edges tagged as 
hard creases by the artist and rendered with our system to better convey the intended shape. For example, 
when rendered with regular ACC, the front bumper turns into a cylindrical shape with tapered ends, but 
this shape is rendered as a square extruded along a smooth B-Spline path using our system. The artist 
has even mod­eled two prominent pinches (darts) in the hood of the car by tagging edges which happen 
to be isolated and do not form part of a corner or creased edge loop. In Figure 42, we see a closeup 
of the dashboard of the car, rendered with ACC and with our method. You can easily see a variety of areas 
where the artist used the expressive power of hard creases to convey the desired shape of this mechanical 
object. Even for non-mechanical models such as the Heavy Weapons Guy from Team Fortress 2 shown in Figures 
38 and 43, hard creases are useful for modeling hard edges at areas such as the character s clothing 
and even his .ngernails. 3.5 Displacement Mapping In addition to approximating the Catmull-Clark limit 
surface, it is possible to compactly represent high frequency detail by displacing the vertices from 
the approximate limit surface [Coo84] [LMH00]. We have written an extractor which processes the Catmull-Clark 
control mesh and a separate high­resolution detail mesh to generate a scalar displacement map relative 
to our approximation to the Catmull-Clark subdivision surface [COM98]. Each invocation of the domain 
shader performs 30 fetches of packed control point data and the inclusion of an additional data fetch 
to access our dis­placement map has negligible incremental performance impact. Likewise, the few additional 
ALU operations necessary to displace the vertex from the approximate limit surface are insigni.cant. 
 The barriers to the adoption of displacement mapping are the additional memory burden of storing the 
displacement maps and the tool investment necessary to integrate displacement mapping into the art pipeline. 
There are available commercial tools such as ZBrush, MudBox and others which can output height.eld textures 
suitable for use as displacement maps. In these tools, however, the computation of such height maps is 
performed relative to the Catmull-Clark limit surface of the underlying control mesh. The Catmull-Clark 
limit surface is not what we are rendering, however. We are rendering an approximation made up of bicubic 
patches using a separate normal .eld which makes up for the fact that the geometry patches are not necessarily 
C1 at patch boundaries. As a result, we have written our own displacement map baker which uses the creased 
ACC geometry and normal .elds in the baking process to ensure that the displacement maps are computed 
ren­dered to approximate limit surface. In Figure 44, we see a Vortigaunt character from the game Half-Life 
2 rendered as an approximate Catmull-Clark subdivision surface. In the row of images, we see the smooth 
approximate limit surface shaded with a simple Phong shader, using a normal map to provide some detail 
in the lighting. In the bottom row of images, displacement mapping has been applied to the character 
to add surface detail. Future-proof Assets As you can see in Figure 44, we tend to use displacement 
maps to capture so-called meso-structure details rather than large-scale object structures such as appendages. 
We feel that this is appropriate because these details can safely be LOD d away or even omitted as part 
of the scalability required of interactive games which must ship into a marketplace with widely varying 
GPU capabilities and performance characteristics. So, while players of future Half-Life games may initially 
only see the displacement mapped Vortigaunt in non-interactive movies or on very high-end hardware, we 
anticipate being able to phase in the displacement mapped assets as hardware improves without having 
to go back and rebuild the character again. 3.5.1 Wrinkle Mapping For a number of years, we have been 
employing a technique we call wrinkle mapping to the normal and base color textures of our characters 
faces in order to give the impression of complex surface deformation. During facial animation, an additional 
scalar channel (the wrinkle weight) is accu­mulated per vertex along with our geometric morph target 
deformations. The pixel shader uses this interpolated wrinkle weight to perform a simple blend between 
three normal and color textures: the neutral, compress and stretch maps, as described previously by [Dia08]. 
As you would expect, wrinkle mapping naturally combines with displacement mapping with very little shader 
modi.­cation. In Figure 45, we can see the neutral, compress and stretch poses for the Heavy Weapons 
Guy from Team Fortress 2, including displacements which add .ne-grained dynamic geometry deformations 
at very low cost. 3.6 Moving from Polygons to Subdivision Surfaces Besides improvements in surface smoothness, 
the move from polygonal to subdivision surface models has additional advantages for game developers, 
including the improvement in both tangent frame quality and skin weight management. 3.6.1 Quality Tangent 
Frames One fact that non-game-developers are often surprised to learn is that games generally have inaccu­rate 
tangent frames due to the nature of the current GPU pipeline. Typically, games must skin their precomputed 
tangent frames and, even worse, accumulate morph offsets to their tangent frames. Traditionally, this 
has resulted in awkward lighting artifacts and the need to generally constrain the range of possible 
character performance, particularly in the case of facial animation. Due to the way that the new Direct3D 
11 pipeline stages integrate into the graphics pipeline after vertex ani­mation operations such as skinning 
and morphing, surface normals can now be computed relative to animated control meshes, improving shading 
quality. An example of a high quality normal .eld is shown in Figure 46, which illustrates per-pixel 
normals of the Heavy Weapons Guy from Team Fortress 2. 3.6.2 Manageability At Valve, the modelers who 
have begun the switch from polygonal modeling to subdivision surface modeling have experienced an increase 
in both productivity and surface control. This is primarily due to the low number of control mesh vertices 
that must be managed relative to a polygonal mesh of suf.cient complexity. Naturally, skin weighting 
is much more manageable as there are relatively few vertices in a control mesh. Additionally, the approximate 
limit surface tends to behave predictably when the control mesh is animated. In many cases, achieving 
similar animation results with a purely polygonal model with polygon count comparable to our post-tessellated 
limit surface approximation would be not only impractical but virtually impossible. 3.7 Future Work Obviously, 
a critical next step in our adoption of approximate Catmull-Clark subdivision surfaces is migration to 
Direct3D 11 hardware as it becomes available. Once we understand the perfor­mance characteristics of 
Direct3D 11 GPUs, we can determine how aggressive we should be in our adoption of approximate Catmull-Clark 
subdivision surfaces and how quickly we can realistically ship it to our game customers. At this point, 
we are far enough along in our implementation that we have shipped GPU-rendered approximate Catmull-Clark 
subdivision surfaces in the form of some rendered elements in the animated shorts Meet the Sandvich and 
Meet the Spy. In the future, we intend to address the topic of adaptive subdivision, as this will be 
critical for per­formance and level of detail (LOD) management. Bunnell has demonstrated extremely compelling 
results using adaptive tessellation of displaced Catmull-Clark subdivision surfaces [Bun05]. We anticipate 
that this will be particularly important as we move beyond isolated character and ob­ject meshes and 
into the trickier problem of environment and terrain rendering with approximate subdivision surfaces. 
Given the new programming model introduced in Direct3D 11, we expect that it will be necessary to develop 
new error metrics and schemes for determining the appropriate level of detail for a given primitive or 
primitive edge, particularly when performing displacement mapping. To date, we have intentionally put 
off exploration of adaptive tessellation schemes since they complicate the Direct3D 9 instanced (or ATI 
native) implementation in a way that doesn t remain useful once we have a real GPU hull shader to tessellator 
connection [BS08]. Naturally, it is necessary for an interactive game to integrate any model representation 
with game systems such as real-time collision detection and decal/damage rendering. Some of the same 
func­tionality will be required for integration with other data ampli.cation schemes such as hair render­ing 
and simulations as discussed in the next chapter. In addition to the optimizations described in Section 
3.3.2, we would like to explore culling operations appropriate to a displaced patch repre­ sentation; 
[LMH00] has reported compelling speedups from the use of normal masks [ZH97]. We have integrated our 
technique with the Source engine s skeletal and facial morphing systems as shown in Figure 38 but we 
look forward to exploring additional animation techniques such as .uid simulation, cloth simulation or 
free-form deformation (FFD) of the low-resolution quad mesh. We anticipate having to make changes to 
such simulations based on the fact that we are animating a subdivision surface control mesh rather than 
the .nal polygonal primitives to be displayed. 3.8 Conclusion We are at a critical point in the evolution 
surface representations for real-time graphics and the investment being made by the GPU vendors in their 
Direct3D 11 hardware aims to address mem­ory and memory bandwidth issues by performing programmable data 
ampli.cation on chip. It still remains to be seen, however, whether hardware will live up to its potential 
in this area. Af­ter all, hardware tessellation (PN Triangles) has shipped in mainstream graphics hardware 
before [VPBM01] and has languished due to lack of adoption by tool vendors and game developers. This 
time around, however, there is reason to be optimistic that this is the big switch to a new high qual­ity 
real-time surface representation as we are working with the more established base primitive (Catmull-Clark 
Subdivision Surfaces) running on more performant and programmable hardware units. 4 Approximating Subdivision 
Surfaces in ILM s Tool chain Philip Schneider and Vivek Verma, ILM 4.1 Introduction Industrial Light 
+ Magic (ILM) is like many other animation and effects companies in that we used to rely on NURBS surfaces 
for model representation. And like these other companies, we spent a lot of effort dealing with continuity 
issues at the seams between patches and the general issues in­volved with the topological restrictions 
of NURBS. And again like many of these other companies, some years ago we switched over to using Catmull-Clark 
subdivision surfaces. Catmull-Clark sur­faces free modelers from the restrictions imposed by the patchwork-of-grids 
topology of NURBS surface models, free software engineers from the of dif.culties of maintaining continuity 
between patches, and are well supported in widely used rendering packages such as Pixar s RenderMan R 
®. Recently, however, we have incorporated the use of an approximation to Catmull-Clark subdivi­sion 
surfaces. In this chapter, we discuss the context and motivations that led to our use of that approximation, 
as well as details about how we use it. 4.2 Motivation ILM makes extensive use of displacement maps to 
add surface detail to characters, creatures, and hard-surface models. Figure 47 shows an example of the 
head of the Mulgarath creature from The Spiderwicke Chronicles -on the top is a basic gray plastic shader 
rendering of the limit surface, and on the bottom the displacement map has been added. Displacement, 
along with numerous other rendering effects de.ned by texture maps (color, bump, opacity, specularity, 
etc.), are rendered with custom RenderMan R ®shaders for both the .nal frames of the .lm and for TDs 
and artists to preview work in progress. The use of Catmull-Clark subdivision surfaces makes this a straightforward 
process. Models are typically created and (if needed) animated in any one of several commercial packages. 
ILM s own proprietary Zeno application is used for setting up lighting, materials, textures, etc. and 
for funneling all the data down to RenderMan R ®for rendering. Zeno acts as the hub of ILM s production 
pipeline, and provides a vast range of functionality. ILM s models are typically partitioned into sets 
of faces, each of which is UV-mapped separately and has assigned to it per-partition texture maps; that 
is, the usual atlas of charts approach. So, it s just a simple task of generating the requisite maps, 
shipping them off to RenderMan R ®, and we re done, right? Well, of course the interesting part here 
is how the texture maps are generated, and how this .ts into the artists work.ow and the production pipeline. 
In order to understand the why and how of ILM s use of an approximation to Catmull-Clark sub­division 
surfaces, we have to take a step back and explain one of the tools that s been an important part of ILM 
s pipeline for more than a decade: ILM s Viewpaint. 4.2.1 Viewpaint ILM s Viewpaint was created in 1991-1992 
by John Schlag, Brian Knep, Zoran Kacic-Alesic, and Tom Williams. Its .rst major use was in Jurassic 
Park. Prior to the development of Viewpaint, artists created and modi.ed textures by simply painting 
on the texture image itself using a 2D painting program such as Parallax Software Limited s Matador Paint 
System R ®. Because the sur­faces of models are not generally .at, and even in the case of a good conformal 
mapping, there is some amount of distortion from the .at image to the rendered texture, and in any case 
the process is very indirect: the artist has to use experience and intuition to reverse-engineer the 
distortion to get the paint to go where it should on the model s surface. The Viewpaint scheme allows 
a more direct approach, and its success can be measured by its continued use at ILM, as well as its de­velopers 
being awarded a Scienti.c and Engineering Award from the Academy of Motion Picture Arts and Sciences 
in 1997. The Viewpaint approach allows artists to effectively paint textures directly on the surface 
of a rendered model. However, this is not literally painting in 3D space: at the time of its development, 
display and computation speed would have been a signi.cant impediment. While such direct 3D painting 
tools have been developed since then, the Viewpaint approach offers an advantage in that the artist can 
continue to make full use of all the power and .exibility of full-featured painting and image processing 
tools such as Adobe R®or GIMP. ®PhotoShop R The basic idea behind Viewpaint is this: 1. Create an image 
that s a snapshot of some texture-mapped model. 2. Export that image to the image-editing program of 
your choice, and paint or modify the desired texture. 3. When done, map the painted pixels of that image 
back into the appropriate pixels in the texture map, and re-render the model.  This process is shown 
in Figure 48. The upper left-hand image (a) is a screen snapshot of Zeno, showing the face of Davy Jones 
from Disney s Pirates of the Caribbean, and Zeno s Texture Map Editor displaying the UV mapping. The 
user interactively positions the model to show the region that is to be painted, and then simply hits 
a button to create a snapshot of that image; this is sent to (in this case) GIMP (b). The artist can 
then modify the image (c), and when it s saved, the modi.cations made to that Viewpaint image are applied 
back into the color texture in Zeno using the inverse mapping (d). Of course, this entire process is 
iterated numerous times, with the Viewpaint artist repeatedly positioning the model, painting from this 
angle or that, and so on. This smooth work.ow allows much of the ease of direct 3D painting, but with 
the ability to take advantage of the power of the artist s familiar and powerful image-editing/painting 
tools. Snapshot to GIMP  (a) Paint... (b) (c) Apply inverse (d) The crux of this approach is that we 
need to know what texture map UV coordinate goes with each pixel in the Viewpaint snapshot image, so 
we know where the Viewpaint snapshot pixel values are to be applied in the texture map image. Viewpaint 
mapping can either be a forward or inverse mapping, each having advantages and disadvantages. In inverse 
mapping, we draw the texture coordinates as .at geometry in texture space, using the painted image as 
the texture, and inverting the perspective distortion at each vertex. This approach has the advantage 
of being fast, and the rasterization is handled robustly by the graphics library. Occlusion can be handled 
using a classic shadow buffer, commonly supported in hardware. The trick is to transform the depth into 
the orthographic texture space of the coordinates, so that the depth comparison is meaningful. This however 
has depth aliasing problems similar to using shadow buffers on regular 3D geometry. A forward mapping 
approach is essentially a deep buffer scheme in which any mapping that can be drawn in the same space 
as the painted image itself can be mapped per pixel, by splatting each pixel s paint into the corresponding 
area of its texture. Hybrid approaches are also possible, and different variations have been employed 
at ILM. We are .nally coming closer to our .rst motivation for using an approximation to Catmull-Clark 
surfaces. The observant reader will note that we haven t mentioned displacement maps for a while. Displacement 
maps can be created using painting, ad hoc processes, procedurally, and more recently using programs 
such as Pixelogic s TMZBrush R®MudboxTM, using ®and Autodesk s Rsculpting-like interface, and again these 
can just be passed on to the .nal rendering software as just another texture effect. But what happens 
if the Viewpaint artist is working on painting, say, a color texture on a model with nontrivial displacement? 
Because the Viewpaint rendering does not include the effect of the displacements, the artist has to use 
experience, intuition, and trial-and-error to paint the color texture in a way that accomodates geometry 
they cannot visualize. The tremendously impressive results we see in images like that shown in Figure 
49 are a tribute to the great level of skill in our Viewpaint artists. But, if the displacement is large 
enough, even great skill and experience may not be suf.cient, and in some cases, ILM modelers have had 
to add new topology (i.e. more geometric detail) in the areas of the displacement that is, they add 
enough topology so that some amount of the displacement is represented explicitly in the geometry of 
the model. This, of course, tends to make the models very heavy, thereby defeating some of the advantages 
of using displacement maps in the .rst place, which is the ability to make lighter models by putting 
some of the high-frequency details into the displacement. From both productivity and model footprint 
standpoints, burdening Viewpaint artists with having to imagine where the displacements are, or simply 
adding more topology, are both highly unde­sirable. So, last year, our ILM s R&#38;D Geometry, Modeling, 
and Sculpting Group was approached by artists from ILM s Digital Model Shop (DMS) with a request to enable 
Viewpaint artists to paint on displaced Catmull-Clark surfaces. This was our .rst, and perhaps most signi.cant, 
motivation leading to our use of a Catmull-Clark approximation scheme. We considered some alternative 
solutions that would have only addressed the speci.c issue of allowing the Viewpaint artists to get a 
displaced model image to paint on. If we could get Zeno to display displaced Catmull-Clark subdivision 
surface models in general, then we d have not only the ability to paint color (etc) textures on them, 
but other artists and TDs would be able to see models that were more true to their .nal .lm-rendered 
shape in zeno itself. One often-used feature in Zeno is its ability to capture GL-rendered playback of 
the timeline (like a Maya playblast) -this is highly useful for generating quick turnaround clips of 
animation, simulation runs, etc, or simply generating a video that will run at .lm speed from a scene 
that s too heavy to play interactively at that rate. In that work.ow, the ability to see the displacements 
would be tremendously bene.cial (imagine the bene.t of being able to quickly see what effect displacement 
has on facial animation with shapes, for example). 4.2.2 Catmull-Clark Limit Surface Evaluation The second 
motivation leading to the use of a Catmull-Clark approximation came from the desire to evaluate Catmull-Clark 
subdivision limit surfaces. NURBS models have several characteristics that can offer signi.cant advantages 
over Catmull-Clark subdivision surface models: They have a built-in parameterization.  They have a 
simple closed formula, and commonly used algorithms such as exact evaluation of position, normals, tangents, 
etc is trivial, and operations like ray intersection and closest point queries are relatively straightforward. 
 There is a wealth of literature supporting them (e.g. [PT97] [BBB87]).  Catmull-Clark subdivision 
surfaces offer neither of these. Indeed, while the subdivision surfaces were .rst de.ned in 1978 by Edwin 
Catmull and Jim Clark [CC78], it was not until 1998 that Jos Stam published a practical method for Catmull-Clark 
subdivision surface evaluation in the neighborhood of extraordinary points [Sta98]. Surface evaluation 
of models can be important in a number of places in an animation, special effects, and simulation pipeline; 
for example: Interactive placement of objects on the surface of other objects.  Hairs/fur on a creature 
needs to be constrained to a particular parametric point on a limit surface such that a hair sticks to 
the surface even when the surface deforms.  A simulated .uid s surface may need to spawn particles for 
spray or bubble effects.  In lieu of a good Catmull-Clark subdivision limit surface evaluation library, 
ILM R&#38;D engineers have resorted to various ad hoc workarounds when their tools required points on 
the limit surface. Such approaches typically involved creating a copy of the Catmull-Clark subdivision 
mesh, sub­dividing it in place a few times, triangulating the resulting quads, and then using point-on-triangle 
schemes, ray-triangle intersections, and the like. Because Catmull-Clark subdivision converges on the 
limit surface so rapidly, these techniques can be serviceable, but there are signi.cant drawbacks to 
such approaches: The piecewise linear approximation to the limit surface is just that an approximation, 
and as such can suffer from imprecision and inaccuracy.  For large meshes, or many such meshes, the 
memory footprint can be daunting.  The linear approximation constitutes a cache, and as such has cache 
validity problems (imag­ine one used for a deforming surface). The mechanism for creating and maintaining 
this data adds to code size and complexity, and impacts performance.  An obvious answer to the need 
for Catmull-Clark limit surface evaluation might be to simply implement Stam s algorithm and be done 
with it. But this approach is not without its own problems: It doesn t deal with hard edges or open 
boundaries.  The computational load can be signi.cant in the vicinity of extraordinary vertices, particu­larly 
if the algorithm is invoked many times.  It doesn t address such operations as ray intersection.  4.2.3 
Convergence The need for displaying displaced Catmull-Clark subdivision surfaces in Zeno was long-standing. 
By last year, the need for a good Catmull-Clark limit surface evaluation library went from a would be 
nice to have to we really need this yesterday state: the problems engendered by the ad hoc approaches 
were becoming a signi.cant issue the drawbacks mentioned earlier were becoming signi.cant issues rather 
than just annoying or inelegant. So, we were in the situation of having two high-priority projects, and 
fairly constrained resources. A solution that helped ful.ll both needs would be highly desirable. A few 
years go, one of us (Philip) was talking to Charles Loop about a paper he was working on (later published 
as [LB06]). Charles mentioned that he was working on another project (which was later published as [LS08a]) 
and he d send Philip a draft when it became available. After considering various other options for the 
two ILM projects, we realized that the Loop/Schaefer approximation scheme might offer us a solution to 
both of our seemingly disparate problems. For the Viewpainter s need to paint color (and other effects) 
on displaced geometry, it would suf.ce for Zeno to be capable of displaying displaced Catmull-Clark subdivision 
surface models with the displacement faithfully applied. The Viewpaint system would then simply just 
work ; that the surface shape was due to displacement or to highly detailed dense geometric manipulation 
would be entirely irrelevant. The Loop/Schaefer scheme provided a means to accomplish this display within 
the con.nes of the existing software architecture. The Loop/Schaefer ACC algorithm creates an approximation 
to the Catmull-Clark subdivision limit surface using bicubic B´ezier patches. B´ezier surfaces (and BSpline 
surfaces in general) have those nice characteristics mentioned earlier that make them relatively easy 
to deal with for limit surface evaluation. We realized that if we had a B´ezier approximation to the 
Catmull-Clark limit surface, and could re-cast the limit surface queries on the original model to limit 
surface queries on that B´ezier approximation, we could leverage much of the work we needed to do for 
the dis­placement display and get the limit surface evaluation functionality for a relatively modest 
amount of additional effort. 4.3 Displacement Display Implementation At this point it would be good to 
present some basic issues regarding ILM s models and pipeline that in.uenced our implementation: Models 
are mostly, but not exclusively, comprised of quadrilateral faces.  Models are partitioned into sets 
of faces that are separately UV-mapped and textured.  These individual partitions can have separate 
interactive shaders associated with them.  The visibility of models can be controlled on a per-partition 
basis (that is, some may be hidden while others are displayed).  Extensive use of shapes, enveloping, 
and interactive sculpting/modeling tools has led to quite a lot of code optimization that attempts to 
update only those vertices on a model that differ from the rest or previous positions, and only on partitions 
that are visible.  Models occasionally have geometric discontinuities in the form of hard edges (aka 
creases).  We already have an existing tessellation subsystem that we use for interactively rendering 
Catmull-Clark subdivision surfaces. The issue with displacement that was problematic within this existing 
system was that displacement needed to be applied at the per-vertex level. Because the displace­ment 
texture can contain very high-frequency detail, the vertices displaced must be very close together in 
screen space. If we were simply to tessellate the surface to the level necessary to avoid subsampling 
artifacts, the amount of CPU-side memory would be spectacular, as would be the time needed to transmit 
all of that data to the GPU and the time needed for the GPU to process it. 4.3.1 CPU-side Subdivision 
Pipeline The existing Catmull-Clark rendering system in Zeno (prior to our implementation of the ACC 
algorithm), basically consisted of repeated application of the Catmull-Clark split-then-smooth al­gorithm 
on the mesh, and then sending the resulting quadrilaterals to OpenGL for rendering. At the time this 
was implemented, GL quad strips were a reasonably good choice for output primitives; the Catmull-Clark 
scheme also split faces into quadrilaterals at each step, leading to topologically square grids of quadrilaterals 
for each face. A number of issues made this simple-sounding scheme become fairly complex. In addition 
to the model and pipeline issues we just listed, GL primitives like quad or tri strips require a matching 
topology for all per-vertex data (normals, colors, etc), and must share a single color or texture. This 
means that the original mesh must, for the purposes of rendering as quad strips, be decomposed into contiguous 
regions. So, in this initial decomposition, regions must at least be bounded by UV discontinuities, partition 
boundaries, and hard edges (geometric discontinuities). In addition, extraordinary vertices were used 
to bound regions. The solution to all of these constraints was to decompose the base Catmull-Clark mesh 
into strips of quadrilaterals, whose boundaries were de.ned by any of the various criteria just enumerated. 
Figure 50 shows a schematic of this mesh-stripping scheme. The vertices in red are extraordinary vertices, 
and the color indicates separate mesh partitions, with a UV discontinuity between them shown in green. 
Non-quadrilateral faces (such as faces j and o) were handled by decomposition into separate strips consisting 
of a single quad each. Once we have this decomposition into strips, we can apply the Catmull-Clark split-then-smooth 
algorithm as many times as necessary; this turns each quad in each strip into a topologically square 
grid of quadrilaterals, as shown in Figure 51. This process ensures that each strip can be turned into 
OpenGL quad strips. As a .nal step, we push the vertices in the grids to the limit surface before sending 
them to OpenGL; the reason we do so is this: if all the vertices of the tessellation are always at the 
limit surface, then as we interactively change the subdivision level of the rendered surface the effective 
shape doesn t appear to change much, but rather looks as if details are being .lled in. Level 1 Subdivision 
Level 2 Subdivision Level 3 Subdivision The wish to display or hide the different partitions of a subdivided 
mesh is enabled by keeping track of which partition goes with which output grid. Similarly, if there 
are geometric changes (due to modeling/sculpting operations, deformers, etc.), data structures that map 
from base mesh vertex to output grids allow us to update only the subdivided positions (and normals, 
UVs, colors, etc.) that are affected by the base vertex change; such changes are also restricted only 
to visible partitions. 4.3.2 CPU-side ACC Subdivision Pipeline In the previous section, we described 
the pre-ACC subdivision pipeline. Here, we describe how we utilized part of that existing mechanism to 
implement the ACC approach. In the subdivision pipeline, we used the appropriate Catmull-Clark rules 
(depending on vertex valence, boundary condition, etc.) on the strips to generate the subdivided grids. 
For the ACC approach, we start with the Level 1 subdivision grids: each quad face is converted to a bicubic 
B´ezier patch and two tangent patches according to the ACC algorithm [LS08a] (see Section 2.3). Figure 
52 shows this process -the cyan quadrilateral is used to generate the corresponding B´ezier patch. We 
employ a symmetric computation in the generation of the ACC B´ezier patches and tangent patches in order 
to avoid the problems discussed in Section 2.5: insert reference to section); the infrastructure supporting 
this was already present in the previous subdivision pipeline in order to ensure exact correspondence 
of vertices at strip boundaries. Figure 52: Converting a strip to bicubic B´ezier patches via the ACC 
algorithm. Each B´ezier patch corre­sponds to a single quad in the Level 1 Subdivision grid. 4.3.3 Displacement 
Display The previous sections outlined the architecture of our tessellation scheme that converts a Catmull-Clark 
cage into a collection of bi-cubic B´ezier patches. A straightforward way to render these B´ezier patches 
would be to send all of them to the GPU where they could be tessellated and rendered using the instancing 
scheme outlined in Section 2.4. At ILM, we typically have very large models with large textures and as 
such the B´ezier patch CVs generated for them can t all .t on the available GPU memory. Given the memory 
constraint, we designed our rendering scheme to use constant buffers to transfer the B´ezier patches 
to the GPU in manageable chunks. The data for the B´ezier patches (geometry and tangent .eld CVs) are 
stored in Parameter Buffer objects. The buffers are created and set upfront for a model and at the time 
of rendering they are bound to their uniform parameter buffer variables as needed. When the geome­try 
changes, we update the appropriate buffers. Since the buffers reside on the server side, the data is 
transferred from the CPU to the GPU more ef.ciently. See http://www.opengl.org/registry/specs/NV/parameter_buffer_object.txt 
 for more details on Parameter Buffers. A great advantage of using the ACC algorithm is that the underlying 
Catmull-Clark surface can be represented by a collection of bi-cubic B´ezier patches. The B´ezier patches 
make it easy to modify the surface using a displacement map. The reader would remember that the ACC algorithm 
not only generates a bi-cubic B´ezier approximation to the Catmull-Clark surface but it also generates 
a C2 continuous tangent .eld. This tangent .eld can be employed to give the appearance of a smooth surface. 
However, after a displacement map is applied to the surface, we can no longer use this tangent .eld for 
smoothly shading the surface. A typical work .ow that generated the displacement map (using software 
packages like ZBrush R ®and MudboxTM) can also generate a normal map.The normal map can be generated 
in object space or tangent space depending on the application. The object space tangent map is faster 
to evaluate but cannot be used for deforming surfaces and in such a situation a tangent space tangent 
map is needed. However, we can t always guarantee the availability of a normal map. We use the GPU shader 
described next when a normal map is not available to provide the shading information for a displaced 
surface. Figure 53 illustrates the change in shading after a displacement map has been applied to a 
surface. In the absence of a normal map we approximate the shading of a displaced surface by computing 
the normal at each vertex of the tessellated surface using the surrounding vertices. Figure 54 shows 
the tessellated surface generated from a strip of B´ezier patches. We generate a new normal for each 
vertex in the vertex shader. Given that we send the B´ezier CVs (position, tangent, and UVs) to the GPU 
using a constant buffer, for each vertex that passes through the vertex shader, we can easily determine 
the position of the neighboring vertices. Our approximation algorithm effectively computes an approximate 
faceted shading for the tessellated B´ezier patches. For a given vertex that passes through the vertex 
shader, we determine the position of the other three vertices such that the given vertex is the bottom 
right vertex of a tessellated face of the surface. We have implemented Newell s algorithm [SSRS74] to 
approximate the normal of the plane for the tessellated face. This provides a suf.ciently nice approximation 
for the shading of the displaced surface as can be seen in Figure 55. In certain areas of the displacement 
map where the displaced quad has a very small area, we don t want to use the expensive Newell s algorithm. 
In those areas we use a threshold to determine whether the quad s area is small. If the area is small 
then we just approximate the the shading by the surface normal of the center of the face. We can demonstrate 
this scheme in a more obvious fashion with a pair of images. In Figure 56 we show a Zeno rendering of 
Thimbletack with no displacement, and the same rendered with displacement, at the lowest resolution possible 
-there is a single quadrilateral rendered per B´ezier patch (of which there are four per Level 1 subdivision 
quadrilateral). 4.4 Limit Surface Evaluation Implementation The bicubic B´ezier control points and tangent 
patches that we create for the purposes of rendering can also be used for (approximate, of course) Catmull-Clark 
limit surface evaluation. As stated in Section 4.2.2, computing B´ezier surface points, normals, and 
tangents, given a UV value, is en­tirely trivial; ray intersection, nearest point, and similar calculations 
are relatively straightforward. B´ezier surfaces have the advantage of having a built in parametrization, 
and so an API for a mod­ule/library that provides this sort of functionality can exploit this to great 
advantage: as a trivial example, the location of a hair root on a piecewise B´ezier surface could be 
de.ned unambiguously as a tuple of patch index and UV coordinates. Catmull-Clark subdivision surfaces 
have no built-in parametrization. Of course, for purposes of texture-mapping, we can assign UV coordinates 
to Catmull-Clark control cage vertices, and apply the Catmull-Clark subdivision and smoothing rules to 
them as well. However, we wished to eval­uate surfaces without requiring a UV mapping to be present. 
Also, even if a UV mapping were present, using those UV coordinates associated with the vertices would 
be undesirable. Consider a Catmull-Clark mesh consisting of a single square face, as show in Figure 57. 
If we apply a couple steps of the Catmull-Clark subdivision algorithm, we get the somewhat circular surface 
shown in the interior. Let v1 have the UV value (0,0), v2 the UV value (1,0), and so on. The point pis 
the lower left corner of the limit surface, and thus should correspond to the limit surface evaluated 
at (0,0). Figure 58 shows Zeno s UV map editor (a), with a simple checkerboard texture applied to this 
mesh, and the resulting rendered mesh (b). Because we apply the Catmull-Clark algorithm to the UV coordinates 
for the purposes of rendering, the point pis actually (1/6,1/6) in texture space. So, if we try to make 
the texture UVs do double duty for a positional surface parametrization space and for texture space, 
then pin surface parametrization space is (0,0) while pin UV texture space is (1/6,1/6); or, we use the 
UV texture space directly for surface parametrization and then some portions of parameter space don t 
correspond to any point on the surface (for example, parameter value (0,0)). If a mesh is watertight, 
then these boundary-related issues don t pose a problem, but even then using the UV texture coordinates 
by themselves would be insuf.cient for meshes that have multiple UV charts. So, the problems became: 
  (a) (b) 1. Given only vertex, edge, and/or face indices, how can we unambiguously specify a para­metric 
position on a Catmull-Clark mesh that de.nes a point on the limit surface? 2. How do we map such a speci.cation 
to the data used for the B´ezier approximation (that is, a particular B´ezier patch identifer and a UV 
value on it)?  If every face were a triangle, then barycentric coordinates of each triangle would be 
a convenient way to describe all points on the face; but we needed a solution that would apply to faces 
of arbitrary degree. The solution comes from this observation: in the .rst step of Catmull-Clark subdivision, 
each face of the original base mesh is subdivided into a set of quadrilaterals by splitting each edge 
at its midpoint, inserting a new vertex in the centroid of the face, and connecting the edges appropriately. 
Our Catmull-Clark Mesh implementation has at its heart the quad edge data structure [GS85], with explicit 
vertex, face, and edge abstractions surrounding it. This approach allows us to de.ne, for example, a 
base edge for each face. It also allows us to de.ne convenient iterators, for example (extending the 
previous example) over the edges in a face, which always start at the same edge, and are guaranteed to 
iterate in a counterclockwise order. Because we apply the edge-splitting steps in that deterministic 
iterator s order, we have a one-to-one ordered mapping from a face s edges to each quad generated by 
a split of those edges. While we found this scheme to be convenient, in principle any scheme that can 
uniquely identify these faces and edges can be used. Figure 59 demonstrates this approach. In (a) we 
see a quadrilateral Catmull-Clark mesh face. The edges are labeled e0,...,en-1, in the order in which 
they re split, and the origin of ei is vertex vi. The new vertices vi,i+1 resulting from splitting each 
edge ei, along with the new centroid vertex vC , de.ne a set of four sub-quadrilaterals, as shown in 
(b). We treat each such sub-quadrilateral as a separate parametric domain. The shaded sub-quadrilateral 
in the lower left of (b) has its parametric origin at v0, with u increasing in the direction of e0, and 
v increasing in the direction -e3. The parameters u and v lie between 0 and 1 and vary bilinearly over 
the subdivided regions. A given point p can then be described by the 4-tuple (F,e,u,v) where F is the 
face index, e is the edge index, and u,v are the parameters that vary between 0 and 1; in (b), the tuple 
would be F(e0,u,v). To demonstrate how this approach generalizes to n-gons, consider the pentagon shown 
in (c) and (d); triangular faces are similar. In the ACC algorithm, each of the base mesh s subdivided 
faces sub-quadrilateral Qj is repre­sented by a B´ezier patch Bj , whose corner vertices correspond to 
the vertices of the Qj . For Bj , de.ne the parametric (UV) origin to be the point bi corresponding to 
vi, and whose orientation is de.ned by associating increasing u with ei (that is, in the direction ei 
to ei+1). That is, we make the origin and orientation of the B´ezier patch match that of the face sub-quadrilateral. 
So, any point de.ned by a tuple (F,ei,u,v) corresponds to a point Bi(u,v), and vice versa, as show in 
Figure 60. We have now solved the problems we enumerated earlier. In order to evaluate an approximate 
point on the Catmull-Clark subdivision mesh s limit surface that is de.ned by such a tuple, we simply 
evaluate the correct B´ezier patch at speci.ed (u,v) parameter; for the case of returning the normal 
or tangents as well as the position, we utilize the tangent patches de.ned by the ACC algorithm. Texture 
coordinates are typically assigned to mesh vertices meshes, so we can evaluate them in the same way, 
returning the texture UV values for the point on the limit surface. One of the main motivations for our 
creating this functionality was simply to enable us to evaluate points on Catmull-Clark subdivision limit 
surfaces. But, because we have these underlying B´ezier patches and tangent patches approximating that 
surface, essentially any of the vast family of query, measure, intersection, etc. type algorithms for 
B´ezier patches are available for Catmull-Clark sub­division surfaces. We have implemented such broadly 
useful algorithms such as ray intersection, e2 v3 v2 e3 e1 v3,0 u v v u vC u v (u,v) u v  v0 v1 
v0 v0,1 e0 (a) (b) v3 v0v1 v1 v0,1 e0 (c) (d) (a) (b)  Figure 60: Mapping from a mesh face parametrization 
to a B´ezier patch. distance from ray to surface, and nearest point on surface, to name a few. Here s 
an example of how we can use this scheme: say we wish to intersect a character s cheek with a ray, and 
then place a hair on the intersection point, and constrain it so that it stays put on the scalp as the 
character is animated. We start off by intersecting a ray with the B´ezier patches that we ve created 
to approximate the Catmull-Clark subdivision surface. The intersection computation gives us back the 
patch s tuple (F,e0,u,v) identifying that point, and actual (approximate) limit surface position, normal, 
and tangents. We record this tuple with the data structure for hair in question. Whenever we wish to 
draw that hair, we look up that tuple to .nd the relevant B´ezier patch, and evaluate that patch at the 
given UV value, yielding the new position at which to constrain the hair s origin, as well as the orientation. 
This evaluation is very inexpensive computationally (since it s always a bicubic), and so applying this 
technique for even a fairly large number of hairs is not problematic. The API for these query operations 
is rather unsurprising. Because these queries require that we create the B´ezier patches for the approximation, 
and we wouldn t wish to create them over an over again when many queries are made sequentially, we require 
that the caller bracket the queries with a begin/end pair of methods. Figure 61 shows some example pseudocode 
query method signatures for a Catmull-Clark mesh. void evaluateLimitSurfaceBegin(); void evaluateLimitSurface(const 
Edge *edge, const UV &#38;uv, Point *position, Tangent *dU = 0, Tangent *dV = 0, Color *color = 0, 
UV *mappedUV = 0); void evaluateLimitSurfaceEnd(); void nearestPointOnLimitSurface(const Point &#38;localPoint, 
int &#38;faceIndex, int &#38;edgeIndex, UV &#38;uv, Point *nearestPt, Tangent *dU = 0, Tangent *dV = 
0, Color *color = 0, UV *mappedUV = 0); bool rayLimitSurfaceIntersection(const Line3d &#38;localRay, 
int &#38;faceIndex, int &#38;edgeIndex, UV &#38;uv, Point *nearestPt, Tangent *dU = 0, Tangent *dV = 
0, Color *color = 0, UV *mappedUV = 0); Figure 61: Mesh limit surface query API. Note that the UV &#38;uvargument 
is the parametric location of the query point, in the domain of the sub-quads described earlier, while 
the UV &#38;mappedUVis the return value of the texture coordi­ 94 nates (if any) on the mesh. As you 
can see, from the caller s point of view, the fact that there is a B´ezier approximation being used is 
completely hidden. In the Python interface to Zeno, there are analogous Python functions with essentially 
the same signatures. 4.5 Results 4.5.1 Displacement Display For the display of displaced Catmull-Clark 
surfaces, our goal was to render them with suf.cient .delity to Renderman R ®so as to allow the Viewpaint 
artists to paint textures for color and other effects on displaced surfaces. In Figure 62 the top image 
of Thimbletack was rendered with Renderman R ®, and the bottom image was rendered with Zeno. In Figure 
63 the top image of Mulgarath was rendered with Renderman R ®, and the bottom image was rendered with 
Zeno. 4.5.2 Limit Surface Evaluation Here we show the use of limit surface query and evaluation functionality. 
Figure 64 shows a Catmull-Clark subdivision surface on which we ve placed a few hundred particles rendered 
as small spheres. The tuple-de.ned positions were generated by a particle simulation system to create 
particle emission points on the limit surface of the torus, using a blue noise function. A more sophisticated 
variant of this simple scheme could be used to populate a character with hair, or to emit spray from 
the surface of a .uid simulation, etc. A demonstration of the nearest-point-on-surface query is shown 
in Figure 65. To generate this image, we placed particles in a vector swirl .eld and evaluated their 
simulated positions over time. We sampled the particles positions and used the positions as the sources 
for nearest-point queries on the limit surface of the human model, drawing a curve with the sequences 
of limit surface points. 4.6 Conclusions and Future Work While we have successfully implemented Loop 
and Schaefer s scheme to approximate subdivision surfaces, there are some remaining challenges: The 
existing CPU-side tessellation architecture, which was highly optimized for the graphics cards at the 
time of its development, is no longer optimal: new features in OpenGL and Cg, and new graphics card capabilities 
and capacities mean that a new approach can be under­taken. Some of the model characteristics that led 
to architectural decisions are still present (UV and geometric discontinuities, the occasional non-quadrilateral 
faces, very large models with many large texture maps, etc.), and we need to ef.ciently manage both deforming 
and static models.  Tools that create displacement textures can optionally create a normal map that 
corresponds to that displacement. Considering that one of the algorithmically and computationally com­plex 
aspects of displacement shading is related to shading normals, using both displacement and normal maps 
may allow for both better performance and visual quality.  4.7 Acknowledgements A lot of folks at ILM, 
both in R&#38;D and production, contributed to the Zeno displacement display and limit surface evaluation 
projects. Colette Mullenhoff and Vivek Verma of R&#38;D did the bulk of the implementation of both projects. 
Lana Lan, Michael Koperwas, and Geoff Campbell of ILM s Digital Model Shop helped specify functionality 
and interfaces for the displacement display project, and tirelessly tested our work in progress. Don 
Hatch, Stephen Bowline, and Nick Ras­mussen of ILM R&#38;D contributed to the functional speci.cation 
of the limit surface project, and helped with testing the functionality and re.ning/extending the API. 
Aaron Elder was the Project Manager for this work.  ®, and the bottom image was rendered in Zeno.  
References [AB08] Marc Alexa and Tamy Boubekeur. Subdivision Shading. ACM Trans. Graph, Sig­graph Asia, 
27(5):142, 2008. [BA08] Tamy Boubekeur and Marc Alexa. Phong Tessellation. ACM Trans. Graph, Siggraph 
Asia, 27(5):141, 2008. [BBB87] Richard H. Bartels, John C. Beatty, and Brian A. Barsky. An Introduction 
to Splines for Use in Computer Graphics &#38; Geometric Modeling. Morgan Kaufmann Publishers Inc., San 
Francisco, CA, USA, 1987. [Bez77] Pierre E. Bezier. Essai de De.nition Numerique des Courbes et des Surfaces 
Experi­mentales. Ph.d. thesis, Universite Pierre et Marie Curie, February 1977. [BG75] R. Barnhill and 
J. Gregory. Compatible Smooth Interpolation in Triangles. Approx. Theory, 15(3):214 225, 1975. J of [BRS05] 
Tamy Boubekeur, Patrick Reuter, and Christophe Schlick. Scalar Tagged PN Trian­gles. In EUROGRAPHICS 
2005 (Short Papers). Eurographics, 2005. [BS02] J. Bolz and P. Schr¨oder. Rapid Evaluation of Catmull-Clark 
Subdivision Surfaces. In Proceedings of the Web3D 2002 Symposium, pages 11 18. ACM Press, 2002. [BS05] 
Tamy Boubekeur and Christophe Schlick. Generic Mesh Re.nement on Gpu. ACM SIGGRAPH/Eurographics Graphics 
Hardware, 2005. In [BS07] Tamy Boubekeur and Christophe Schlick. QAS: Real-Time Quadratic Approxima­tion 
of Subdivision Surfaces. In Marc Alexa, Steven J. Gortler, and Tao Ju, editors, Proceedings of the Paci.c 
Conference on Computer Graphics and Applications, Pa­ci.c Graphics 2007, Maui, Hawaii, USA, October 29 
-November 2, 2007, pages 453 456. IEEE Computer Society, 2007. [BS08] Tamy Boubekeur and Christophe Schlick. 
A Flexible Kernel for Adaptive Mesh Re.nement on GPU. Computer Graphics Forum, 27(1):102 114, 2008. [Bun05] 
Michael Bunnell. GPU Gems 2: Programming Techniques for High-Performance Graphics and General-Purpose 
Computation, chapter Adaptive Tessellation of Sub­division Surfaces With Displacement Mapping. Addison-Wesley, 
Reading, MA, 2005. [Cas08a] Ignacio Casta no. GRAPH. Next-Generation Rendering of Subdivision Surfaces, 
2008. SIG­ [Cas08b] Ignacio Casta no. Tessellation of Subdivision Surfaces in Directx 11, 2008. Gamefest. 
 [CC78] Edwin Catmull and James Clark. Recursively Generated B-Spline Surfaces on Arbi­trary Topological 
Meshes. Computer-Aided Design, pages 350 355, 1978. [CDM91] A.S. Cavaretta, W. Dahmen, and C.A. Micchelli. 
Stationary Subdivision. Memoirs of the American Mathematical Society, 93(453):1 186, 1991. [CH02] Nathan 
A. Carr and John C. Hart. Meshed Atlases for Real-Time Procedural Solid Texturing. ACM Trans. Graph., 
21(2):106 131, 2002. [CHCH06] Nathan A. Carr, Jared Hoberock, Keenan Crane, and John C. Hart. Rectangular 
Multi-chart Geometry Images. In SGP 06: Proceedings of the fourth Eurograph­ics symposium on Geometry 
processing, pages 181 190, Aire-la-Ville, Switzerland, Switzerland, 2006. Eurographics Association. [COM98] 
Jonathan Cohen, Marc Olano, and Dinesh Manocha. Appearance-Preserving Simpli­.cation. In SIGGRAPH 98: 
Proceedings of the 25th annual conference on Computer graphics and interactive techniques, pages 115 
122, New York, NY, USA, 1998. ACM. [Coo84] Robert L. Cook. Shade Trees. SIGGRAPH Comput. Graph., 18(3):223 
231, 1984. [dB93] Carl de Boor. On the Evaluation of Box Splines. Numerical Algorithms, 5(1 4):5 23, 
1993. [DBG+06] Shen Dong, Peer-Timo Bremer, Michael Garland, Valerio Pascucci, and John C. Hart. Spectral 
Surface Quadrangulation. ACM Trans. Graph., 25(3):1057 1066, 2006. [Dia08] Rich Diamant. Autodesk Mudbox: 
Integration and Use with Autodesk 3ds Max and Autodesk Maya. In Game Developer s Conference, 2008. [DKT98] 
T. DeRose, M. Kass, and T. Truong. Subdivision Surfaces in Character Animation. In SIGGRAPH 98: Proceedings 
of the 25th annual conference on Computer graphics and interactive techniques, pages 85 94, New York, 
NY, USA, 1998. ACM Press. [DRS08] C. Dyken, M. Reimers, and J. Seland. Real-Time Gpu Silhouette Re.nement 
Using Adaptively Blended Bezier Patches. In Computer Graphics Forum 27 (1), pages 1 12, 2008. [DRSar] 
Christopher Dyken, Martin Reimers, and Johan Seland. Patch Tessellation. Computer Graphics Forum, to 
appear. Semi-Uniform Adaptive [DS78] D. Doo and M. Sabin. Behaviour of recursive division surfaces near 
extraordinary points. Computer-Aided Design, 10:356 360, September 1978. [DWS+88] Michael Deering, Stephanie 
Winner, Bic Schediwy, Chris Duffy, and Neil Hunt. The Triangle Processor and Normal Vector Shader: a 
VLSI System for High Performance Graphics. In SIGGRAPH 88: Proceedings of the 15th annual conference 
on Com­puter graphics and interactive techniques, pages 21 30, New York, NY, USA, 1988. ACM. [Far97] 
Gerald Farin. Curves and Surfaces for Computer-Aided Geometric Design: A Prac­tical Guide. Academic Press, 
pub-ACADEMIC:adr, fourth edition, 1997. [FH05] Michael S. Floater and Kai Hormann. Surface Parameterization: 
a Tutorial and Sur­vey. pages 157 186, 2005. [For03] Tom Forsyth. Practical Displacement Mapping. In 
Game Developers Conference, 2003. [For06] Tom Forsyth. Linear-Speed Vertex Cache Optimization. 2006. 
[Gee08] Kev Gee. DirectX 11 Tessellation. In Microsoft GameFest, 2008. [GP99] Carlos Gonzalez and J¨org 
Peters. Localized Hierarchy Surface Splines. In S.N. Spencer J. Rossignac, editor, ACM Symposium on Interactive 
3D Graphics, 1999. [Gre74] J. A. Gregory. Smooth Interpolation Without Twist Constraints, pages 71 88. 
Aca­demic Press, 1974. [GS85] Leonidas Guibas and Jorge Stol.. Primitives for the Manipulation of General 
Subdi­visions and the Computation of Voronoi Diagrams. ACM Trans. Graph., 4(2):74 123, 1985. [JH08] Jin 
Ma Xinguo Liu Leif Kobbelt Hujun Bao Jin Huang, Muyang Zhang. Spectral Quadrangulation with Orientation 
and Alignment Control. SIGGRAPH Asia, 2008. [JLW05] Shuangshuang Jin, Robert R. Lewis, and David West. 
A Comparison of Algorithms for Vertex Normal Computation. The Visual Computer, 21(1-2):71 82, 2005. 
[KMDZ09] Denis Kovacs, Jason Mitchell, Shanon Drone, and Denis Zorin. Real-time Creased Approximate Subdivision 
Surfaces. In I3D 09: Proceedings of the 2009 Symposium on Interactive 3D Graphics and Games, pages 155 
160, New York, NY, USA, 2009. ACM. [KP07] Ke¸stutis Kar.ciauskas and J¨org Peters. Concentric Tessellation 
Maps and Curvature Continuous Guided Surfaces. Computer Aided Geometric Design, 24(2):99 111, Feb 2007. 
[KPN1] Ke¸stutis Kar.ciauskas and J¨org Peters. Guided Spline Surfaces. Computer Aided Geometric Design, 
pages 1 20, 2009 N1. [KPR04] K. Karciauskas, J. Peters, and U. Reif. Shape Characterization of Subdivision 
Sur­faces Case Studies. Computer-Aided Geometric Design, 21(6):601 614, july 2004. [KSG03] Vladislav 
Kraevoy, Alla Sheffer, and Craig Gotsman. Constrained Texture Maps. 2003. Matchmaker: Constructing [LB06] 
Charles Loop and Jim Blinn. Real-time GPU Rendering of Piecewise Algebraic Sur­faces. ACM Trans. Graph., 
25(3):664 670, 2006. [Lee06] Matt Lee. Next-Generation Graphics Programming on XBox 360. GameFest, 2006. 
In Microsoft [LMH00] Aaron Lee, Henry Moreton, and Hugues Hoppe. Displaced Subdivision Surfaces. In SIGGRAPH 
00: Proceedings of the 27th annual conference on Computer graphics and interactive techniques, pages 
85 94, 2000. [Loo92] Charles Loop. Generalized B-Spline Surfaces of Arbitrary Topological Type. PhD thesis, 
University of Washington, 1992. [Loo04] Charles Loop. Second Order Smoothness over Extraordinary Vertices. 
In Symposium on Geometry Processing, pages 169 178, 2004. [LS08a] Charles Loop and Scott Schaefer. Approximating 
Catmull-Clark Subdivision Sur­faces with Bicubic Patches. ACM Trans. Graph., 27(1):1 11, 2008. [LS08b] 
Charles T. Loop and Scott Schaefer. G2 Tensor Product Splines Over Extraordinary Vertices. Comput. Graph. 
Forum, 27(5):1373 1382, 2008. [LY06] Gang Lin and Thomas P. Y. Yu. An improved vertex caching scheme 
for 3d mesh rendering. IEEE Transactions on Visualization and Computer Graphics, 12(4):640 648, 2006. 
[MHAM08] Jacob Munkberg, Jon Hasselgren, and Tomas Akenine-Mller. Non-Uniform Frac­tional Tessellation. 
In ACM SIGGRAPH/Graphics Hardware 2008, 2008. [Mit07] Martin Mittring. Finding Next Gen: CryEngine 2. 
In SIGGRAPH 07: ACM SIG-GRAPH 2007 courses, pages 97 121, New York, NY, USA, 2007. ACM. [MKP07] Ashish 
Myles, Ke¸stutis Kar.ciauskas, and J¨org Peters. Extending Catmull-Clark Sub­division and PCCM with Polar 
Structures. In PG 07: Proceedings of the 15th Paci.c Conference on Computer Graphics and Applications, 
pages 313 320, Washington, DC, USA, 2007. IEEE Computer Society. [MNP08] Ashish Myles, Tianyun Ni, and 
J¨org Peters. Fast Parallel Construction of Smooth Surfaces from Meshes with Tri/Quad/Pent Facets. In 
Symposium on Geometry Pro­cessing, July 2 -4, 2008, Copenhagen, Denmark, pages 1 8. Blackwell, 2008. 
[Mor01] Henry Moreton. Watertight Tessellation Using Forward Differencing. In HWWS 01: Proceedings of 
the ACM SIGGRAPH/EUROGRAPHICS workshop on Graphics hardware, pages 25 32, New York, NY, USA, 2001. ACM. 
[MP09] Ashish Myles and J¨org Peters. Bi-3 C2 Polar Subdivision. ACM Transactions on Graphics, 2009. 
[Myl08] Ashish Myles. Curvature-Continuous Bicubic Subdivision Surfaces for Polar Con­ .gurations. PhD 
thesis, University of Florida, December 2008. [NYM+08] Tianyun Ni, Young In Yeo, Ashish Myles, Vineet 
Goel, and J¨org Peters. GPU Smoothing of Quad Meshes. In IEEE International Conference on Shape Modeling 
and Applications, 2008. [PB00] Dan Piponi and George Borshukov. Seamless Texture Mapping of Subdivision 
Sur­faces by Model Pelting and Texture Blending. In SIGGRAPH 00: Proceedings of the 27th annual conference 
on Computer graphics and interactive techniques, pages 471 478, New York, NY, USA, 2000. ACM Press/Addison-Wesley 
Publishing Co. [PBP02] H. Prautzsch, W. Boehm, and M. Paluszny. B´ ezier and B-Spline Techniques. Mathe­matics 
and Visualization. Springer-Verlag, Berlin, 2002. [PCK04] Budirijanto Purnomo, Jonathan D. Cohen, and 
Subodh Kumar. Seamless Texture Atlases. In SGP 04: Proceedings of the 2004 Eurographics/ACM SIGGRAPH 
sym­posium on Geometry processing, pages 65 74, New York, NY, USA, 2004. ACM. [Pet91] J¨org Peters. Smooth 
Interpolation of a Mesh of Curves. Constructive Approximation, 7:221 247, 1991. Winner of SIAM Student 
Paper Competition 1989. [Pet95] J¨org Peters. C1-Surface Splines. SIAM Journal on Numerical Analysis, 
32(2):645 666, 1995. [Pet02] J¨org Peters. Geometric Continuity. In Handbook of Computer Aided Geometric 
Design, pages 193 229. Elsevier, 2002. [Pet04] J¨org Peters. Mid-Structures of Subdividable Linear Ef.cient 
Function Enclosures Linking Curved and Linear Geometry. In Miriam Lucian and Marian Neamtu, editors, 
Proceedings of SIAM conference, Seattle, Nov 2003. Nashboro, 2004. [Pet08] J¨org Peters. PN-Quads. Technical 
Report 2008-421, Dept CISE, University of Florida, 2008. [PK09] J¨org Peters and K. Kar.ciauskas. An 
introduction to guided and polar surfacing. In Mathematics of Curves and Surfaces, pages 1 26, 2009. 
Seventh International Con­ference on Mathematical Methods for Curves and Surfaces Toensberg, Norway. 
[PR98] J. Peters and U. Reif. The 42 Equivalence Classes of Quadratic Surfaces in Af.ne N-Space. Computer-Aided 
Geometric Design, 15:459 473, 1998. [PR08] J. Peters and U. Reif. Subdivision Surfaces, volume 3 of Geometry 
and Computing. Springer-Verlag, New York, 2008. [PT97] Les Piegl and Wayne Tiller. The NURBS Book (2nd 
ed.). Springer-Verlag New York, Inc., New York, NY, USA, 1997. [PW08] J. Peters and X. Wu. Net-to-Surface 
Distance of Subdivision Functions. JAT, page xx, 2008. in press. [SAUK04] Le-Jeng Shiue, Pierre Alliez, 
Radu Ursu, and Lutz Kettner. A Tutorial on CGAL Polyhedron for Subdivision Algorithms. In 2nd CGAL User 
Workshop, 2004. http://www.cgal.org/Tutorials/Polyhedron/. [SJP05] Le-Jeng Shiue, Ian Jones, and J. Peters. 
A Realtime GPU Subdivision Kernel. In Marcus Gross, editor, Siggraph 2005, Computer Graphics Proceedings, 
Annual Con­ference Series, pages 1010 1015. ACM Press / ACM SIGGRAPH / Addison Wesley Longman, 2005. 
[SNB07] Pedro V. Sander, Diego Nehab, and Joshua Barczak. Fast Triangle Reordering for Vertex Locality 
and Reduced Overdraw. ACM Trans. Graph., 26(3):89, 2007. [SPR06] Alla Sheffer, Emil Praun, and Kenneth 
Rose. Mesh Parameterization Methods and Their Applications. Found. Trends. Comput. Graph. Vis., 2(2):105 
171, 2006. [SSRS74] Ivan E. Sutherland, Robert F. Sproull, Robert, and A. Schumacker. A Characterization 
of Ten Hidden-Surface Algorithms. ACM Computing Surveys, 6:1 55, 1974. [ST90] Takafumi Saito and Tokiichiro 
Takahashi. Comprehensible Rendering of 3D Shapes. SIGGRAPH Comput. Graph., 24(4):197 206, 1990. [Sta98] 
Jos Stam. Exact Evaluation of Catmull-Clark Subdivision Surfaces at Arbitrary Pa­rameter Values. In M. 
Cohen, editor, SIGGRAPH 98 Proceedings, pages 395 404. Addison Wesley, 1998. [SW07] S. Schaefer and J. 
Warren. Exact Evaluation of Non-Polynomial Subdivision Schemes at Rational Parameter Values. In PG 07: 
15th Paci.c Conference on Com­ puter Graphics and Applications, pages 321 330, Los Alamitos, CA, USA, 
2007. IEEE Computer Society. [SZBN03] Thomas W. Sederberg, Jianmin Zheng, Almaz Bakenov, and Ahmad Nasri. 
T-splines and T-NURCCs. In Jessica Hodgins and John C. Hart, editors, Proceedings of ACM SIGGRAPH 2003, 
volume 22(3) of ACM Transactions on Graphics, pages 477 484. ACM Press, 2003. [TACSD06] Y. Tong, P. 
Alliez, D. Cohen-Steiner, and M. Desbrun. Designing Quadrangulations with Discrete Harmonic Forms. Eurographics 
Symposium on Geometry Processing, 2006. [Tat07] Natasha Tatarchuk. Real-Time Tessellation on the GPU. 
In SIGGRAPH Advanced Real-Time Rendering in 3D Graphics and Games Course, 2007. [Tat08] Andrei Tatarinov. 
Instanced Tessellation in Directx 10, 2008. http://developer.nvidia.com/object/gamefest-2008-subdiv.html. 
GDC, [Val07] Michal Valient. Deferred Rendering in Killzone 2. Brighton, UK, 2007. In DEVELOP Conference, 
[VPBM01] Alex Vlachos, J¨org Peters, Chas Boyd, and Jason Mitchell. Curved PN Triangles. In I3D 2001: 
Proceedings of the 2001 Symposium on Interactive 3D Graphics, pages 159 166, 2001. [vW86] J. van Wijk. 
Bicubic Patches for Approximating Non-Rectangular Control-Point Meshes. Computer Aided Geometric Design, 
3(1):1 13, 1986. [WP04] X. Wu and J. Peters. Interference Detection for Subdivision Surfaces. Graphics 
Forum, Eurographics 2004, 23(3):577 585, 2004. Computer [WP05] X. Wu and J. Peters. An Accurate Error 
Measure for Adaptive Subdivision surfaces. In Proceedings of The International Conference on Shape Modeling 
and Applications 2005, pages 51 57, 2005. [WW02] J. Warren and H. Weimer. Subdivision Methods for Geometric 
Design. Kaufmann, New York, 2002. Morgan [YNM+] Young In Yeo, Tianyun Ni, Ashish Myles, Vineet Goel, 
and J¨org Peters. Parallel Smoothing of Quad Meshes. The Visual Computer, pages x x. accepted, in press, 
TVCJ-267. [ZH97] Hansong Zhang and Kenneth E. Hoff, III. Fast Backface Culling Using Normal Masks. In 
SI3D 97: Proceedings of the 1997 symposium on Interactive 3D graphics, pages 103 ff., New York, NY, USA, 
1997. ACM. [ZS00] Denis Zorin and Peter Schr¨oder, editors. Subdivision for Modeling and Animation, Course 
Notes. ACM SIGGRAPH, 2000.    
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1667253</article_id>
		<sort_key>140</sort_key>
		<display_label>Article No.</display_label>
		<display_no>14</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Interaction]]></title>
		<subtitle><![CDATA[interfaces, algorithms, and applications]]></subtitle>
		<page_from>1</page_from>
		<page_to>66</page_to>
		<doi_number>10.1145/1667239.1667253</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1667253</url>
		<abstract>
			<par><![CDATA[<p>The virtual reality field has recently seen the advent of novel commodity 3D user interfaces that have led to not only a revolution in video game interaction, but also new possibilities for other virtual reality applications. This course provides background on the interfaces and algorithms involved in 3D interaction, and previews the future of research in the field.</p> <p>Topics include: interaction algorithms, their application to animation, interactive manipulation, video games, and general human-computer interaction. Special attention is focused on interfaces and multimodal interaction techniques.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1797146</person_id>
				<author_profile_id><![CDATA[81100035394]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Miguel]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Otaduy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[URJC Madrid]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797147</person_id>
				<author_profile_id><![CDATA[81100444444]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Igarashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797148</person_id>
				<author_profile_id><![CDATA[81100283513]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Joseph]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[LaViola]]></last_name>
				<suffix><![CDATA[Jr.]]></suffix>
				<affiliation><![CDATA[University of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>836002</ref_obj_id>
				<ref_obj_pid>527216</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Adachi, Y., Kumano, T., and Ogino, K. 1995. Intermediate representation for stiff virtual objects. <i>Virtual Reality Annual International Symposium</i>, 203--210.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Adams, R. J., and Hannaford, B. 1998. A two-port framework for the design of unconditionally stable haptic interfaces. <i>Proc. of IEEE/RSJ International Conference on Intelligent Robots and Systems.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Andriot, C. 2002. Advances in virtual prototyping. <i>Clefs CEA Vol. 47, Research and Simulation</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Astley, O. R., and Hayward, V. 1998. Multirate haptic simulation achieved by coupling finite element meshes through norton equivalents. <i>Proc. of IEEE International Conference on Robotics and Automation</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>165682</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Baraff, D. 1992. <i>Dynamic simulation of non-penetrating rigid body simulation</i>. PhD thesis, Cornell University.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>797575</ref_obj_id>
				<ref_obj_pid>795683</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Barbagli, F., Prattichizzo, D., and Salisbury, K. 2003. Dynamic local models for stable multi-contact haptic interaction with deformable objects. <i>Proc. of Haptics Symposium</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1272714</ref_obj_id>
				<ref_obj_pid>1272690</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Barbi&#269;, J., and James, D. L. 2007. Time-critical distributed contact for 6-DoF haptic rendering of adaptively sampled reduced deformable models. <i>Proc. of ACM SIGGRAPH/Eurographics Symposium on Computer Animation</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Bejczy, A., and Salisbury, J. K. 1980. Kinematic coupling between operator and remote manipulator. <i>Advances in Computer Technology Vol. 1</i>, 197--211.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1536527</ref_obj_id>
				<ref_obj_pid>1536513</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Bott, J., Crowley, J., and LaViola, J. 2009. Exploring 3d gestural interfaces for music creation in video games. In <i>Proceedings of The Fourth International Conference on the Foundations of Digital Games 2009</i>, 18--25.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>253301</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Bowman, D. A., and Hodges, L. F. 1997. An evaluation of techniques for grabbing and manipulating remote objects in immersive virtual environments. In <i>SI3D '97: Proceedings of the 1997 symposium on Interactive 3D graphics</i>, ACM, New York, NY, USA, 35-ff.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>836072</ref_obj_id>
				<ref_obj_pid>523977</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Bowman, D. A., Koller, D., and Hodges, L. F. 1997. Travel in immersive virtual environments: An evaluation of viewpoint motion control techniques. In <i>Proceedings of the Virtual Reality Annual International Symposium</i>, 45--52.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618556</ref_obj_id>
				<ref_obj_pid>616054</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Bowman, D. A., Wineman, J., Hodges, L. F., and Allison, D. 1998. Designing animal habitats within an immersive ve. <i>IEEE Comput. Graph. Appl. 18</i>, 5, 9--13.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>993837</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Bowman, D. A., Kruijff, E., LaViola, J. J., and Poupyrev, I. 2004. <i>3D User Interfaces: Theory and Practice</i>. Addison Wesley Longman Publishing Co., Inc., Redwood City, CA, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566623</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Bridson, R., Fedkiw, R., and Anderson, J. 2002. Robust treatment of collisions, contact and friction for cloth animation. In <i>Proc. of ACM SIGGRAPH</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97899</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Brooks, Jr., F. P., Ouh-Young, M., Batter, J. J., and Kilpatrick, P. J. 1990. Project GROPE --- Haptic displays for scientific visualization. In <i>Computer Graphics (SIGGRAPH '90 Proceedings)</i>, F. Baskett, Ed., vol. 24, 177--185.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>232471</ref_obj_id>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Burdea, G. C. 1996. <i>Force and Touch Feedback for Virtual Reality</i>. John Wiley and Sons, Inc.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[&#199;avu&#351;o&#487;lu, M. C., and Tendick, F. 2000. Multirate simulation for high fidelity haptic interaction with deformable objects in virtual environments. <i>Proc. of IEEE International Conference on Robotics and Automation</i>, 2458--2465.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1581092</ref_obj_id>
				<ref_obj_pid>1581073</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Charbonneau, E., Miller, A., Wingrave, C., and LaViola, J. 2009. Understanding visual interfaces for the next generation of dance-based rhythm video games. In <i>To Appear in Sandbox 2009: ACM SIGGRAPH Video Game Proceedings</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Chen, E. 1999. Six degree-of-freedom haptic system for desktop virtual prototyping applications. In <i>Proceedings of the First International Workshop on Virtual Reality and Prototyping</i>, 97--106.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Cirak, F., and West, M. 2005. Decomposition contact response (DCR) for explicit finite element dynamics. <i>International Journal for Numerical Methods in Engineering 64</i>, 8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Colgate, J. E., and Schenkel, G. G. 1994. Passivity of a class of sampled-data systems: Application to haptic interfaces. <i>Proc. of American Control Conference</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Colgate, J. E., Grafing, P. E., Stanley, M. C., and Schenkel, G. 1993. Implementation of stiff virtual walls in force-reflecting interfaces. <i>Virtual Reality Annual International Symposium</i>, 202--207.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>849725</ref_obj_id>
				<ref_obj_pid>846238</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Colgate, J. E., Stanley, M. C., and Brown, J. M. 1995. Issues in the haptic display of tool use. <i>Proc. of IEEE/RSJ International Conference on Intelligent Robots and Systems</i>, pp. 140--145.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>147199</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Conner, B. D., Snibbe, S. S., Herndon, K. P., Robbins, D. C., Zeleznik, R. C., and van Dam, A. 1992. Three-dimensional widgets. In <i>SI3D '92: Proceedings of the 1992 symposium on Interactive 3D graphics</i>, ACM, New York, NY, USA, 183--188.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1086868</ref_obj_id>
				<ref_obj_pid>1086867</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Constantinescu, D., Salcudean, S. E., and Croft, E. A. 2005. Local model of interaction for realistic manipulation of rigid virtual worlds. <i>International Journal of Robotics Research 24</i>, 10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Cottle, R., Pang, J., and Stone, R. 1992. <i>The Linear Complementarity Problem</i>. Academic Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>846322</ref_obj_id>
				<ref_obj_pid>846276</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Davis, J., Agrawala, M., Chuang, E., Popovic, Z., and Salesin, D. 2003. A sketching interface for articulated figure animation. <i>Proc. of ACM SIGGRAPH/Eurographics Symposium on Computer Animation</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1880309</ref_obj_id>
				<ref_obj_pid>1880269</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Duriez, C., Andriot, C., and Kheddar, A. 2004. A multi-threaded approach for deformable/rigid contacts with haptic feedback. <i>Proc. of Haptics Symposium</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1100960</ref_obj_id>
				<ref_obj_pid>1100864</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Duriez, C., Dubois, F., Kheddar, A., and Andriot, C. 2006. Realistic haptic rendering of interacting deformable objects in virtual environments. <i>Proc. of IEEE TVCG 12</i>, 1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Edmond, C., Heskamp, D., Sluis, D., Stredney, D., Wiet, G., Yagel, R., Weghorst, S., Oppenheimer, P., Miller, J., Levin, M., and Rosenberg, L. 1997. Ent endoscopic surgical simulator. <i>Proc. of Medicine Meets VR</i>, 518--528.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Ehmann, S., and Lin, M. C. 2001. Accurate and fast proximity queries between polyhedra using convex surface decomposition. <i>Computer Graphics Forum (Proc. of Eurographics'2001</i>) 20, 3, 500--510.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>168657</ref_obj_id>
				<ref_obj_pid>168642</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Feiner, S., MacIntyre, B., Haupt, M., and Solomon, E. 1993. Windows on the world: 2d windows for 3d augmented reality. In <i>UIST '93: Proceedings of the 6th annual ACM symposium on User interface software and technology</i>, ACM, New York, NY, USA, 145--155.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1103908</ref_obj_id>
				<ref_obj_pid>1103900</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Fisher, B., Fels, S., MacLean, K., Munzner, T., and Rensink, R. 2004. Seeing, hearing and touching: Putting it all together. In <i>ACM SIGGRAPH course notes</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1550299</ref_obj_id>
				<ref_obj_pid>1549825</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Garre, C., and Otaduy, M. A. 2009. Haptic rendering of complex deformations through handle-space force linearization. <i>Proc. of World Haptics Conference.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Goertz, R., and Thompson, R. 1954. Electronically controlled manipulator. <i>Nucleonics</i>, 46--47.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>375232</ref_obj_id>
				<ref_obj_pid>375213</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Gregory, A., Mascarenhas, A., Ehmann, S., Lin, M. C., and Manocha, D. 2000. 6-DoF haptic display of polygonal models. <i>Proc. of IEEE Visualization Conference</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882358</ref_obj_id>
				<ref_obj_pid>1201775</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Guendelman, E., Bridson, R., and Fedkiw, R. 2003. Nonconvex rigid bodies with stacking. <i>ACM Trans. on Graphics (Proc. of ACM SIGGRAPH) 22</i>, 871--878.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Hannaford, B., Ryu, J.-H., and Kim, Y. S. 2002. Stable control of haptics. In <i>Touch in Virtual Environments</i>, M. L. McLaughlin, J. P. Hespanha, and G. S. Sukhatme, Eds. Prentice Hall PTR, Upper Saddle River, NJ, ch. 3, 47--70.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Harders, M., and Szekely, G. 2003. Enhancing human computer interaction in medical segmentation. <i>Proceedings of the IEEE, Special Issue on Multimodal Human Computer Interfaces 91</i>, 9, 1430--1442.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>662252</ref_obj_id>
				<ref_obj_pid>645626</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Hayward, V., and Armstrong, B. 2000. A new computational model of friction applied to haptic rendering. <i>Experimental Robotics VI.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>662097</ref_obj_id>
				<ref_obj_pid>645625</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[Hayward, V., Gregorio, P., Astley, O., Greenish, S., and Doyon, M. 1998. Freedom-7: A high fidelity seven axis haptic device with applications to surgical training. <i>Experimental Robotics</i>, 445--456. Lecture Notes in Control and Information Sciences 232.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[Hill, J. W., and Salisbury, J. K. 1977. Two measures of performance in a peg-in-hole manipulation task with force feedback. <i>Thirteenth Annual Conference on Manual Control, MIT.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[Hogan, N. 1985. Impedance control: An approach to manipulation, part i - theory, part ii - implementation, part iii - applications. <i>Journal of Dynamic Systems, Measurement and Control 107</i>, 1--24.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[Hogan, N. 1986. Multivariable mechanics of the neuromuscular system. <i>IEEE Annual Conference of the Engineering in Medicine and Biology Society</i>, 594--598.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>571999</ref_obj_id>
				<ref_obj_pid>571985</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[Igarashi, T., and Hughes, J. F. 2002. Clothing manipulation. <i>Proc. of UIST.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>502379</ref_obj_id>
				<ref_obj_pid>502348</ref_obj_pid>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[Igarashi, T., and Hugues, J. F. 2001. A suggestive interface for 3D drawing. <i>Proc. of UIST.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>263525</ref_obj_id>
				<ref_obj_pid>263407</ref_obj_pid>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[Igarashi, T., Matsuoka, S., Kawachiya, S., and Tanaka, H. 1997. Interactive Beautification: A technique for rapid geometric design. <i>Proc. of UIST</i>, 105--114.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311602</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[Igarashi, T., Matsuoka, S., and Tanaka, H. 1999. Teddy: A sketching interface for 3D freeform design. <i>Proc. of ACM SIGGRAPH.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073323</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[Igarashi, T., Moscovich, T., and Hughes, J. F. 2005. As-rigid-as-possible shape manipulation. <i>Proc. of ACM SIGGRAPH.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073383</ref_obj_id>
				<ref_obj_pid>1073368</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[Igarashi, T., Moscovich, T., and Hughes, J. F. 2005. Spatial keyframing for performance-driven animation. <i>Proc. of ACM SIGGRAPH/Eurographics Symposium on Computer Animation</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073253</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[Ijiri, T., Okabe, M., Owada, S., and Igarashi, T. 2005. Floral diagrams and inflorescences: Interactive flower modeling using botanical structural constraints. <i>Proceedings of ACM SIGGRAPH.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>897971</ref_obj_id>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[Insko, B., Meehan, M., Whitton, M., and Brooks, F. 2001. Passive haptics significantly enhances virtual environments. Tech. Rep. 01-010, Department of Computer Science, UNC Chapel Hill.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>933178</ref_obj_id>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[Insko, B. 2001. <i>Passive Haptics Significantly Enhance Virtual Environments</i>. PhD thesis, University of North Carolina. Department of Computer Science.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>364380</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[Johnson, D. E., and Cohen, E. 2001. Spatialized normal cone hierarchies. <i>Proc. of ACM Symposium on Interactive 3D Graphics</i>, pp. 129--134.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>797570</ref_obj_id>
				<ref_obj_pid>795683</ref_obj_pid>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[Johnson, D. E., and Willemsen, P. 2003. Six degree of freedom haptic rendering of complex polygonal models. In <i>Proc. of Haptics Symposium</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1880273</ref_obj_id>
				<ref_obj_pid>1880269</ref_obj_pid>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[Johnson, D. E., and Willemsen, P. 2004. Accelerated haptic rendering of polygonal models through local descent. <i>Proc. of Haptics Symposium</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1092793</ref_obj_id>
				<ref_obj_pid>1092712</ref_obj_pid>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[Johnson, D. E., Willemsen, P., and Cohen, E. 2005. 6-dof haptic rendering using spatialized normal cone search. <i>IEEE Transactions on Visualization and Computer Graphics 11</i>, 6, 661--670.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[Karnopp, D. 1985. Computer simulation of stick slip friction in mechanical dynamic systems. <i>Trans. ASME, Journal of Dynamic Systems, Measurement, and Control</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1187631</ref_obj_id>
				<ref_obj_pid>1187619</ref_obj_pid>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[Katzourin, M., Ignatoff, D., Quirk, L., LaViola, J., and Jenkins, O. C. 2006. Swordplay: Innovating game development through vr. <i>IEEE Computer Graphics and Applications 26</i>, 6, 15--19.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073295</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[Kaufman, D. M., Edmunds, T., and Pai, D. K. 2005. Fast frictional dynamics for rigid bodies. <i>Proc. of ACM SIGGRAPH.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1053452</ref_obj_id>
				<ref_obj_pid>1053427</ref_obj_pid>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[Kho, Y., and Garland, M. 2005. Sketching mesh deformations. <i>Proceedings of the ACM Symposium on Interactive 3D Graphics</i>, 147--154.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[Kim, W., and Bejczy, A. 1991. Graphical displays for operator aid in telemanipulation. <i>IEEE International Conference on Systems, Man and Cybernetics</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[Kim, Y. J., Lin, M. C., and Manocha, D. 2002. DEEP: an incremental algorithm for penetration depth computation between convex polytopes. <i>Proc. of IEEE Conference on Robotics and Automation</i>, 921--926.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>942442</ref_obj_id>
				<ref_obj_pid>942439</ref_obj_pid>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[Kim, Y. J., Otaduy, M. A., Lin, M. C., and Manocha, D. 2003. Six-degree-of-freedom haptic rendering using incremental and localized computations. <i>Presence 12</i>, 3, 277--295.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[Klatzky, R. L., and Lederman, S. J. 2003. Touch. In <i>Experimental Psychology</i>, 147--176. Volume 4 in I. B. Weiner (Editor-in-Chief). Handbook of Psychology.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[Larsen, E. 2001. A robot soccer simulator: A case study for rigid body contact. <i>Game Developers Conference</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1447520</ref_obj_id>
				<ref_obj_pid>1447515</ref_obj_pid>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[LaViola Jr., J. J. 2008. Bringing vr and spatial 3d interaction to the masses through video games. <i>IEEE Comput. Graph. Appl. 28</i>, 5, 10--15.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[LaViola, J. 2000. Msvt: A virtual reality-based multimodal scientific visualization tool. In <i>Proceedings of the Third IASTED International Conference on Computer Graphics and Imaging</i>, 1--7.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>375231</ref_obj_id>
				<ref_obj_pid>375213</ref_obj_pid>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[Lawrence, D. A., Lee, C. D., Pao, L. Y., and Novoselov, R. 2000. Shock and vortex visualization using a combined visual/haptic interface. <i>Proc. IEEE Visualization</i>, 131--137.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1449423</ref_obj_id>
				<ref_obj_pid>1449377</ref_obj_pid>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[Lee, J. 2008. Hacking the nintendo wii remote. <i>Pervasive Computing, IEEE 7</i>, 3 (July-Sept.), 39--45.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1386770</ref_obj_id>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[Lin, M. C., and Otaduy, M. A. 2008. <i>Haptic Rendering: Foundations, Algorithms&amp;Applications</i>. AK Peters.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1124800</ref_obj_id>
				<ref_obj_pid>1124772</ref_obj_pid>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[Luk, J., Pasquero, J., Little, S., MacLean, K. E., Hayward, V., and Levesque, V. 2006. Haptics as a solution for mobile interaction challenges: Initial design using a handheld tactile display prototype. <i>Proceedings of ACM Conference on Human Factors in Computing Systems, CHI.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[Mapes, D., and Moshell, M. 1995. A two-handed interface for object manipulation in virtual environments. <i>Presence: Teleoper. Virtual Environ. 4</i>, 4, 403--416.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237284</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[Mark, W., Randolph, S., Finch, M., Van Verth, J., and Taylor II, R. M. 1996. Adding force feedback to graphics systems: Issues and solutions. In <i>SIGGRAPH 96 Conference Proceedings</i>, 447--452.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237284</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>75</ref_seq_no>
				<ref_text><![CDATA[Mark, W., Randolph, S., Finch, M., Van Verth, J., and Taylor II, R. M. 1996. Adding force feedback to graphics systems: Issues and solutions. In <i>SIGGRAPH 96 Conference Proceedings</i>, H. Rushmeier, Ed., Annual Conference Series, 447--452.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>76</ref_seq_no>
				<ref_text><![CDATA[Massie, T. M., and Salisbury, J. K. 1994. The phantom haptic interface: A device for probing virtual objects. <i>Proc. of ASME Haptic Interfaces for Virtual Environment and Teleoperator Systems 1</i>, 295--301.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>77</ref_seq_no>
				<ref_text><![CDATA[McLaughlin, M., Hespanha, J. P., and Sukhatme, G. S. 2002. <i>Touch in Virtual Environments</i>. Prentice Hall.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311600</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>78</ref_seq_no>
				<ref_text><![CDATA[McNeely, W., Puterbaugh, K., and Troy, J. 1999. Six degree-of-freedom haptic rendering using voxel sampling. <i>Proc. of ACM SIGGRAPH</i>, 401--408.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>79</ref_seq_no>
				<ref_text><![CDATA[McNeely, W., Puterbaugh, K., and Troy, J. 2006. Voxel-based 6-dof haptic rendering improvements. <i>Haptics-e 3</i>, 7.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383263</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>80</ref_seq_no>
				<ref_text><![CDATA[Milenkovic, V. J., and Schmidl, H. 2001. Optimization-based animation. <i>SIGGRAPH 01 Conference Proceedings</i>, 37--46.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>81</ref_seq_no>
				<ref_text><![CDATA[Miller, B. E., Colgate, J. E., and Freeman, R. A. 1999. Guaranteed stability of haptic systems with nonlinear virtual environments. <i>IEEE Transactions on Robotics and Automation 16</i>, 6, 712--719.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258747</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>82</ref_seq_no>
				<ref_text><![CDATA[Mine, M. R., Brooks, Jr., F. P., and Sequin, C. H. 1997. Moving objects in space: exploiting proprioception in virtual-environment interaction. In <i>SIGGRAPH '97: Proceedings of the 24th annual conference on Computer graphics and interactive techniques</i>, ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 19--26.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>897820</ref_obj_id>
				<ref_seq_no>83</ref_seq_no>
				<ref_text><![CDATA[Mine, M. 1995. Virtual environment interaction techniques. Tech. rep., UNC Chapel Hill CS Dept.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>91451</ref_obj_id>
				<ref_obj_pid>91385</ref_obj_pid>
				<ref_seq_no>84</ref_seq_no>
				<ref_text><![CDATA[Minsky, M., Ouh-Young, M., Steele, O., Brooks, Jr., F. P., and Behensky, M. 1990. Feeling and seeing: Issues in force display. In <i>Computer Graphics (1990 Symposium on Interactive 3D Graphics)</i>, R. Riesenfeld and C. Sequin, Eds., vol. 24, 235--243.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>924581</ref_obj_id>
				<ref_seq_no>85</ref_seq_no>
				<ref_text><![CDATA[Mirtich, B. V. 1996. <i>Impulse-based Dynamic Simulation of Rigid Body Systems</i>. PhD thesis, University of California, Berkeley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344866</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>86</ref_seq_no>
				<ref_text><![CDATA[Mirtich, B. 2000. Timewarp rigid body simulation. <i>SIGGRAPH 00 Conference Proceedings</i>, 193--200.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276433</ref_obj_id>
				<ref_obj_pid>1275808</ref_obj_pid>
				<ref_seq_no>87</ref_seq_no>
				<ref_text><![CDATA[Mori, Y., and Igarashi, T. 2007. Plushie: An interactive design system for plush toys. <i>Proc. of ACM SIGGRAPH.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1006087</ref_obj_id>
				<ref_obj_pid>1006058</ref_obj_pid>
				<ref_seq_no>88</ref_seq_no>
				<ref_text><![CDATA[M&#252;ller, M., and Gross, M. 2004. Interactive virtual materials. <i>Proc. of Graphics Interface</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>545269</ref_obj_id>
				<ref_obj_pid>545261</ref_obj_pid>
				<ref_seq_no>89</ref_seq_no>
				<ref_text><![CDATA[M&#252;ller, M., Dorsey, J., McMillan, L., Jagnow, R., and Cutler, B. 2002. Stable real-time deformations. <i>Proc. of ACM SIGGRAPH Symposium on Computer Animation.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073324</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>90</ref_seq_no>
				<ref_text><![CDATA[Nealen, A., Sorkine, O., Alexa, M., and Cohen-Or, D. 2005. A sketch-based interface for detail-preserving mesh editing. <i>Proc. of ACM SIGGRAPH.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276429</ref_obj_id>
				<ref_obj_pid>1275808</ref_obj_pid>
				<ref_seq_no>91</ref_seq_no>
				<ref_text><![CDATA[Nealen, A., Igarashi, T., Sorkine, O., and Alexa, M. 2007. FiberMesh: Designing freeform surfaces with 3D curves. <i>Proc. of ACM SIGGRAPH.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>92</ref_seq_no>
				<ref_text><![CDATA[Nelson, D. D., Johnson, D. E., and Cohen, E. 1999. Haptic rendering of surface-to-surface sculpted model interaction. <i>Proc. of ASME Dynamic Systems and Control Division</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1185779</ref_obj_id>
				<ref_obj_pid>1185657</ref_obj_pid>
				<ref_seq_no>93</ref_seq_no>
				<ref_text><![CDATA[Okabe, M., Owada, S., and Igarashi, T. 2005. Interactive design of botanical trees using freehand sketches and example-based editing. <i>Proceedings of Eurographics</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1130435</ref_obj_id>
				<ref_obj_pid>1130237</ref_obj_pid>
				<ref_seq_no>94</ref_seq_no>
				<ref_text><![CDATA[Ortega, M., Redon, S., and Coquillart, S. 2006. A six degree-of-freedom god-object method for haptic display of rigid bodies. <i>Proc. of IEEE Virtual Reality Conference</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1264793</ref_obj_id>
				<ref_obj_pid>1264358</ref_obj_pid>
				<ref_seq_no>95</ref_seq_no>
				<ref_text><![CDATA[Otaduy, M. A., and Gross, M. 2007. Transparent rendering of tool contact with compliant environments. <i>Proc. of World Haptics Conference.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1050044</ref_obj_id>
				<ref_obj_pid>1048934</ref_obj_pid>
				<ref_seq_no>96</ref_seq_no>
				<ref_text><![CDATA[Otaduy, M. A., and Lin, M. C. 2005. Stable and responsive six-degree-of-freedom haptic manipulation using implicit integration. <i>Proc. of World Haptics Conference</i>, 247--256.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2211906</ref_obj_id>
				<ref_obj_pid>2211648</ref_obj_pid>
				<ref_seq_no>97</ref_seq_no>
				<ref_text><![CDATA[Otaduy, M. A., and Lin, M. C. 2006. A modular haptic rendering algorithm for stable and transparent 6-DOF manipulation. <i>IEEE Transactions on Robotics 22</i>, 4, 751--762.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>98</ref_seq_no>
				<ref_text><![CDATA[Otaduy, M. A., Tamstorf, R., Steinemann, D., and Gross, M. 2009. Implicit contact handling for deformable objects. <i>Proc. of Eurographics</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1087347</ref_obj_id>
				<ref_seq_no>99</ref_seq_no>
				<ref_text><![CDATA[Otaduy, M. A. 2004. <i>6-DoF Haptic Rendering Using Contact Levels of Detail and Haptic Textures</i>. PhD thesis, Department of Computer Science, University of North Carolina at Chapel Hill.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>917155</ref_obj_id>
				<ref_seq_no>100</ref_seq_no>
				<ref_text><![CDATA[Ouh-Young, M. 1990. <i>Force Display in Molecular Docking</i>. PhD thesis, University of North Carolina, Computer Science Department.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1757498</ref_obj_id>
				<ref_obj_pid>1757492</ref_obj_pid>
				<ref_seq_no>101</ref_seq_no>
				<ref_text><![CDATA[Owada, S., Nielsen, F., Nakazawa, K., and Igarashi, T. 2003. A sketching interface for modeling the internal structures of 3D shapes. <i>Smart Graphics</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218495</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>102</ref_seq_no>
				<ref_text><![CDATA[Pausch, R., Burnette, T., Brockway, D., and Weiblen, M. E. 1995. Navigation and locomotion in virtual worlds via flight into hand-held miniatures. In <i>SIGGRAPH '95: Proceedings of the 22nd annual conference on Computer graphics and interactive techniques</i>, ACM, New York, NY, USA, 399--400.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>103</ref_seq_no>
				<ref_text><![CDATA[Picinbono, G., Lombardo, J.-C., Delingette, H., and Ayache, N. 2000. Anisotropic elasticity and forces extrapolation to improve realism of surgery simulation. <i>Proc. of IEEE International Conference on Robotics and Automation</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>253303</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>104</ref_seq_no>
				<ref_text><![CDATA[Pierce, J. S., Forsberg, A. S., Conway, M. J., Hong, S., Zeleznik, R. C., and Mine, M. R. 1997. Image plane interaction techniques in 3d immersive environments. In <i>SI3D '97: Proceedings of the 1997 symposium on Interactive 3D graphics</i>, ACM, New York, NY, USA, 39-ff.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237102</ref_obj_id>
				<ref_obj_pid>237091</ref_obj_pid>
				<ref_seq_no>105</ref_seq_no>
				<ref_text><![CDATA[Poupyrev, I., Billinghurst, M., Weghorst, S., and Ichikawa, T. 1996. The go-go interaction technique: non-linear mapping for direct manipulation in vr. In <i>UIST '96: Proceedings of the 9th annual ACM symposium on User interface software and technology</i>, ACM, New York, NY, USA, 79--80.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>503397</ref_obj_id>
				<ref_obj_pid>503376</ref_obj_pid>
				<ref_seq_no>106</ref_seq_no>
				<ref_text><![CDATA[Rekimoto, J. 2002. SmartSkin: An infrastructure for freehand manipulations on interactive surfaces. <i>Proceedings of CHI</i>, 113--120.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258878</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>107</ref_seq_no>
				<ref_text><![CDATA[Ruspini, D., Kolarov, K., and Khatib, O. 1997. The haptic display of complex graphical environments. <i>Proc. of ACM SIGGRAPH</i>, 345--352.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>199426</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>108</ref_seq_no>
				<ref_text><![CDATA[Salisbury, K., Brock, D., Massie, T., Swarup, N., and Zilles, C. 1995. Haptic rendering: Programming touch interaction with virtual objects. In <i>1995 Symposium on Interactive 3D Graphics</i>, P. Hanrahan and J. Winget, Eds., ACM SIGGRAPH, 123--130. ISBN 0-89791-736-7.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>109</ref_seq_no>
				<ref_text><![CDATA[Schmidt, R., Wyvill, B., Sousa, M. C., and Jorge, J. A. 2005. ShapeShop: Sketch-based solid modeling with BlobTrees. <i>Eurographics Workshop on Sketch-based Interfaces and Modeling</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>110</ref_seq_no>
				<ref_text><![CDATA[Shabana, A. A. 1989. <i>Dynamics of Multibody Systems</i>. John Wiley and Sons.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>111</ref_seq_no>
				<ref_text><![CDATA[Shimoga, K. 1992. Finger force and touch feedback issues in dextrous manipulation. <i>NASA-CIRSSE International Conference on Inetelligent Robotic Systems for Space Exploration</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1520648</ref_obj_id>
				<ref_obj_pid>1520340</ref_obj_pid>
				<ref_seq_no>112</ref_seq_no>
				<ref_text><![CDATA[Silva, M. G., and Bowman, D. A. 2009. Body-based interaction for desktop games. In <i>CHI EA '09: Proceedings of the 27th international conference extended abstracts on Human factors in computing systems</i>, ACM, New York, NY, USA, 4249--4254.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>113</ref_seq_no>
				<ref_text><![CDATA[Sirouspour, M. R., DiMaio, S. P., Salcudean, S. E., Abolmaesumi, P., and Jones, C. 2000. Haptic interface control - design issues and experiments with a planar device. <i>Proc. of IEEE International Conference on Robotics and Automation</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>114</ref_seq_no>
				<ref_text><![CDATA[Slater, M., and Usoh, M. 1993. An experimental exploration of presence in virtual environments. Tech. Rep. 689, Department of Computer Science, University College London.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>115</ref_seq_no>
				<ref_text><![CDATA[Stewart, D. E., and Trinkle, J. C. 2000. An implicit time-stepping scheme for rigid body dynamics with coulomb friction. <i>IEEE International Conference on Robotics and Automation</i>, 162--169.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>223938</ref_obj_id>
				<ref_obj_pid>223904</ref_obj_pid>
				<ref_seq_no>116</ref_seq_no>
				<ref_text><![CDATA[Stoakley, R., Conway, M. J., and Pausch, R. 1995. Virtual reality on a wim: interactive worlds in miniature. In <i>CHI '95: Proceedings of the SIGCHI conference on Human factors in computing systems</i>, ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 265--272.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>117</ref_seq_no>
				<ref_text><![CDATA[Sutherland, I. 1965. The ultimate display. <i>Proc. of IFIP</i>, 506--508.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015740</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>118</ref_seq_no>
				<ref_text><![CDATA[Thorne, M., Burke, D., and Panne, M. 2004. Motion Doodles: An interface for sketching character motion. <i>Proc. of ACM SIGGRAPH.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2386274</ref_obj_id>
				<ref_obj_pid>2386249</ref_obj_pid>
				<ref_seq_no>119</ref_seq_no>
				<ref_text><![CDATA[Turquin, E., Cani, M.-P., and Hughes, J. F. 2004. Sketching garments for virtual characters. <i>Eurographics Workshop on Sketch-based Interfaces and Modeling</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>797521</ref_obj_id>
				<ref_obj_pid>795682</ref_obj_pid>
				<ref_seq_no>120</ref_seq_no>
				<ref_text><![CDATA[Unger, B. J., Nicolaidis, A., Berkelman, P. J., Thompson, A., Lederman, S. J., Klatzky, R. L., and Hollis, R. L. 2002. Virtual peg-in-hole performance using a 6-dof magnetic levitation haptic device: Comparison with real forces and with visual guidance alone. <i>Proc. of Haptics Symposium</i>, 263--270.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1081478</ref_obj_id>
				<ref_obj_pid>1081432</ref_obj_pid>
				<ref_seq_no>121</ref_seq_no>
				<ref_text><![CDATA[Wan, M., and McNeely, W. A. 2003. Quasi-static approximation for 6 degrees-of-freedom haptic rendering. <i>Proc. of IEEE Visualization</i>, 257--262.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>215647</ref_obj_id>
				<ref_obj_pid>215585</ref_obj_pid>
				<ref_seq_no>122</ref_seq_no>
				<ref_text><![CDATA[Wloka, M. M., and Greenfield, E. 1995. The virtual tricorder: a uniform interface for virtual reality. In <i>UIST '95: Proceedings of the 8th annual ACM symposium on User interface and software technology</i>, ACM, New York, NY, USA, 39--40.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>123</ref_seq_no>
				<ref_text><![CDATA[Wu, D. 2000. Penalty methods for contact resolution. <i>Game Developers Conference</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237238</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>124</ref_seq_no>
				<ref_text><![CDATA[Zeleznik, R. C., Herndon, K. P., and Hughes, J. F. 1996. SKETCH: An interface for sketching 3D scenes. <i>Proc. of ACM SIGGRAPH.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>125</ref_seq_no>
				<ref_text><![CDATA[Zienkiewicz, O. C., and Taylor, R. L. 1989. <i>The Finite Element Method</i>, 4th ed. McGraw-Hill.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>849727</ref_obj_id>
				<ref_obj_pid>846238</ref_obj_pid>
				<ref_seq_no>126</ref_seq_no>
				<ref_text><![CDATA[Zilles, C., and Salisbury, K. 1995. A constraint-based god object method for haptics display. In <i>Proc. of IEEE/RSJ Int. Conf. on Intelligent Robotics and Systems.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Interaction Interfaces, Algorithms, and Applications SIGGRAPH 2009, Course #6 Friday, August 7 2009 
 8:30 AM -12:15 PM, Auditorium A http://www.gmrv.es/sig09 interaction/ Miguel A. Otaduy Takeo Igarashi 
Joseph J. LaViola Jr. URJC Madrid The University of Tokyo, JST/ERATO University of Central Florida miguel.otaduy@urjc.es 
takeo@acm.org jjl@cs.uc..edu Contents 1 Course Structure 1 2 3D User Interfaces and Video Games 2 2.1 
Introduction............................................... 2 2.1.1 Whatisa3DUI? ........................................ 
2 2.1.2 3DUIsinVideoGames .................................... 2 2.1.3 LectureOutline ......................................... 
3 2.2 3DSpatialInteractionTechniques ................................... 3 2.2.1 Navigation ........................................... 
3 2.2.2 Selection ............................................ 5 2.2.3 Manipulation.......................................... 
7 2.2.4 SystemControl ......................................... 10 2.3 3DSpatialInteractioninVideoGames 
................................. 14 2.3.1 UnderstandingtheWiimote .................................. 
15 2.3.2 CaseStudies .......................................... 18 2.4 Conclusions............................................... 
21 3 Sketch-Based Interfaces for Computer Graphics 22 3.1 2DDrawing-InteractiveBeauti.cation ................................ 
22 3.2 3DShapeModeling........................................... 23 3.2.1 ArchitectureModels ...................................... 
23 3.2.2 Free-formModels ....................................... 23 3.2.3 DeformationTechniques .................................... 
27 3.3 Animations ............................................... 28 3.3.1 ArticulatedPoseControl .................................... 
28 3.3.2 MotionDoodles ........................................ 28 3.3.3 AnimationbyPerformance................................... 
29 3.4 SpecialPurposeEditors......................................... 31 3.4.1 TreeandFlowerModeling 
................................... 31 3.4.2 GarmentDesignandManipulation............................... 
31 3.5 Summary ................................................ 33 4 Haptic Interaction with Virtual 
Environments 34 4.1 Introduction............................................... 34 4.1.1 De.nitions 
........................................... 34 4.1.2 HapticInteractionParadigms.................................. 
35 4.1.3 KinestheticDisplayofToolContact .............................. 36 4.1.4 FromTeleroboticstoHapticRendering 
............................ 36 4.1.5 HapticRenderingforVirtualManipulation .......................... 
37 4.2 StableInteractionAlgorithm ...................................... 38 4.2.1 TeleoperationofaVirtualTool................................. 
38 4.2.2 TasksofHapticRendering ................................... 39 4.2.3 StabilityandControlTheory 
.................................. 40 4.2.4 TheOptimizationProblem ................................... 
41 4.2.5 DirectRenderingAlgorithm .................................. 42 4.2.6 RenderingthroughVirtualCoupling 
.............................. 43 4.3 ComponentsofaRenderingAlgorithm................................. 
45 4.3.1 TheToolModel......................................... 45 4.3.2 CollisionDetection ....................................... 
45 4.3.3 CollisionResponse ....................................... 46 4.4 ModelingtheToolandtheEnvironment 
................................ 46 4.4.1 RigidBodyDynamics ..................................... 46 
 4.4.2 DynamicsofDeformableObjects ............................... 48 4.4.3 ContactConstraints....................................... 
49 4.5 MultirateAlgorithm........................................... 52 4.5.1 Geometric Vs. Algebraic 
Intermediate Representations . . . . . . . . . . . . . . . . . . . . 52 4.5.2 Example 1: Multirate Rendering 
with Penalty Methods . . . . . . . . . . . . . . . . . . . . 53 4.5.3 Example2:MultirateRenderingwithConstraints 
. . . . . . . . . . . . . . . . . . . . . . . 54  1 Course Structure The .eld of virtual reality has 
recently seen the advent of novel commodity 3D user interfaces that have led to not only a revolution 
in video game interaction, but also new possibilities for other virtual reality applications. This course 
provides background on the interfaces and algorithms involved in 3D interaction, and also gives an outlook 
into the research in the .eld. The course will cover interaction algorithms, their application to animation, 
interactive manipulation, video games, and sketch-based interfaces. The course will also pay special 
attention to interfaces and to tactile interaction tech­niques. It is intended to programmers and researchers 
in computer graphics working on interactive applications, such as modeling, virtual reality, videogames, 
etc., interested in particular on intuitive interaction techniques. The course will be divided into three 
large areas: User interfaces and interaction techniques.  Applications involving 3D interaction and 
sketching.  Tactile interaction with virtual environments.  The .rst part of the course will cover 
input and output technologies that make 3D interaction possible. The .eld is well covered in research 
nowadays, but the progress in interaction technology makes designing effective 3D interfaces an evolving 
skill. The .rst part will also pick video games as a particular application case, and describe the use 
of 3D user interfaces in that context. The second part will cover the use of sketch-based interfaces 
in a large number of applications in computer graphics. It will demonstrate how 3D interaction techniques 
have been successfully applied to, e.g., 3D shape modeling, animation, or special purpose editors. The 
third and .nal part of the course will re.ect the importance of tactile interaction on user interfaces. 
They provide important features such as situation awareness and the ability to perform detailed manipulations. 
Effective tactile interaction requires basic knowledge of psychophysics aspects, control theory, and 
physically-based simulation, which are overviewed in the course. 2 3D User Interfaces and Video Games 
2.1 Introduction 3D user interface (3D UI) research has been going on for more than 20 years, with limited 
success in being a part of people s everyday lives. Although 3D UIs have been useful in areas such as 
simulation and training, scienti.c visualization, medicine, and psychological rehabilitation, they have 
not signi.cantly affected mainstream society. Compared to other technologies such as the mouse and WIMP 
(windows, icons, menus, point and click) interfaces, 3D UIs have had little impact. However, we re seeing 
a change in the utility of virtual reality (VR) and 3D UIs, with concepts from these .elds emerging on 
a large, mainstream scale. The video game industry, as it has done for real­time 3D computer graphics 
for the last 15 years, is bringing spatial 3D interaction into people s living rooms through rather simple, 
yet effective technologies such as computer vision and motion sensing. The popularity of devices such 
as the Sony EyeToy and Nintendo Wii have shown that 3D UIs are here to stay and will continue to evolve 
to enhance the power and playability of video games, producing rich, immersive, and engaging user experiences 
[LaViola Jr. 2008]. 2.1.1 What is a 3D UI? A 3D user interface is a UI that involves 3D interaction, 
human computer interaction where the user s tasks are carried out in a 3D spatial context with 3D input 
devices or 2D input devices with direct mappings to 3D. Typically, 3D interaction techniques, methods 
(hardware and software), allowing a user to accomplish a spatial task make up the components of a 3D 
UI [Bowman et al. 2004]. Figure 1 shows example 3D UIs. Figure 1: Two examples of a 3D UI. The image 
on the left shows a user interacting with a world-in-miniature and the image on the right shows a user 
navigating using body-based controls. 2.1.2 3D UIs in Video Games As discussed in Section 2.1.1, 3D 
UIs involve input devices and interaction techniques for effectively controlling highly dynamic 3D computer-generated 
content, and there s no exception when it comes to video games. A 3D video game world can use one of 
three basic approaches to interaction. The .rst maps 2D input and button devices, such as the keyboard 
and mouse, joysticks, and game controllers to game elements in the 3D world. This is the traditional 
approach; it s how people have been interacting with 3D (and 2D) video games since their inception many 
years ago. The second approach simulates the real world using replicas of existing devices or physical 
props. Common examples include steering wheels, light guns, and musical instruments (for example, the 
guitar in Guitar Hero). These devices don t necessarily provide 3D interaction in the game but do provide 
3D input devices that enable more realistic gameplay. The third approach is true spatial 3D tracking 
of the user s motion and gestures, where users interact in and control elements of the 3D gaming world 
with their bodies. This control can come through vision-based devices such as the Sony EyeToy and motion-sensing 
devices such as Nintendo Wii Remotes (Wiimotes). 2.1.3 Lecture Outline In the remainder of this lecture, 
we will .rst examine 3D user interfaces in general terms by looking at common interaction techniques 
that have been developed. In the second part of the lecture, we will discuss 3D UIs in the video game 
domain, with a focus on interfaces and video game prototypes developed using Wiimotes.  2.2 3D Spatial 
Interaction Techniques There are essentially four basic 3D interaction tasks that are found in most complex 
3D applications. Obviously, there are other tasks which are speci.c to an application domain, but these 
basic building blocks can often be combined to let users perform more complex tasks. These tasks include 
navigation, selection, manipulation, and system control. Navigation is the most common VE task, and is 
consists of two components. Travel is the motor component of navigation, and just refers to physical 
movement from place to place. Way.nding is the cognitive or decision-making component of navigation, 
and it asks the questions, where am I? , where do I want to go? , how do I get there? , and so on. Selection 
is simply the speci.cation of an object or a set of objects for some purpose. Manipulation refers to 
the speci.cation of object properties (most often position and orientation, but also other attributes). 
Selection and manipulation are often used together, but selection may be a stand-alone task. For example, 
the user may select an object in order to apply a command such as delete to that object. System control 
is the task of changing the system state or the mode of interaction. This is usually done with some type 
of command to the system (either explicit or implicit). Examples in 2D systems include menus and command-line 
interfaces. It is often the case that a system control technique is composed of the other three tasks 
(e.g. a menu command involves selection), but it s also useful to consider it separately since special 
techniques have been developed for it and it is quite common. In this section, we will review several 
common techniques used to perform these basic tasks. Note that it is beyond the scope of this lecture 
to go into great detail on these techniques or on the concepts of 3D UIs in general. The reader should 
examine 3D User Interfaces: Theory and Practice for a rigorous treatment of the subject [Bowman et al. 
2004]. Also note for this section, we assume (for the techniques where it makes a difference) that y 
is the vertical axis in the world coordinate system. 2.2.1 Navigation The motor component of navigation 
is known as travel (e.g., viewpoint movement). There are several issues to consider when dealing with 
travel in 3D UIs. One such issue is the control of velocity and/or acceleration. There are many methods 
for doing this, including gesture, speech controls, sliders, etc. Another issue is that of world rotation. 
In systems that are only partially spatially surrounding (e.g. a 4-walled CAVE, or a single screen), 
the user must be able to rotate the world or his view of the world in order to navigate. In fully surrounding 
systems (e.g. with an HMD or 6-sided CAVE) this is not necessary since the visuals completely surround 
the user. Next, one must consider whether motion should be constrained in any way, for example by maintaining 
a constant height or by following the terrain. Finally, at the lowest-level, the conditions of input 
must be considered -that is, when and how does motion begin and end (click to start/stop, press to start, 
release to stop, stop automatically at target location, etc.)? Four of the more common 3D travel techniques 
are gaze-directed steering, pointing, map-based travel, and grabbing the air . Gaze-Directed Steering 
Gaze-directed steering is probably the most common 3D travel technique, although the term gaze is really 
misleading. Usually no eye tracking is being performed, so the direction of gaze is inferred from the 
head tracker orientation. This is a simple technique, both to implement and to use, but it is somewhat 
limited in that you cannot look around while moving [Mine 1995]. To implement gaze-directed steering, 
typically a callback function is set up that executes before each frame is ren­dered. Within this callback, 
.rst obtain the head tracker information (usually in the form of a 4x4 matrix). This matrix gives you 
a transformation between the base tracker coordinate system and the head tracker coordinate sys­tem. 
By also considering the transformation between the world coordinate system and the base tracker coordinates 
(if any), you can get the total composite transformation. Now, consider the vector (0, 0, -1) in head 
tracker space (the negative z-axis, which usually points out the front of the tracker). This vector, 
expressed in world coordinates, is the direction you want to move. Normalize this vector, multiply it 
by the speed, and then translate the viewpoint by this amount in world coordinates. Note: current velocity 
is in units/frame. If you want true velocity (units/second), you must keep track of the time between 
frames and then translate the viewpoint by an amount proportional to that time. Pointing Pointing is 
also a steering technique (where the user continuously speci.es the direction of motion). In this case, 
the hand s orientation is used to determine direction. This technique is somewhat harder to learn for 
some users, but is more .exible than gaze-directed steering [Bowman et al. 1997]. Pointing is implemented 
in exactly the same way as gaze-directed steering, except a hand tracker is used instead of the head 
tracker. Figure 2: Dragging a user icon to move to a new location in the world. Map-based Travel The 
map-based travel technique is a target-based technique. The user is represented as an icon on a 2D map 
of the environment. To travel, the user drags this icon to a new position on the map (see Figure 2). 
When the icon is dropped, the system smoothly animates the user from the current location to the new 
location indicated by the icon [Bowman et al. 1998]. To implement this technique, two things must be 
known about the way the map relates to the world. First, we need to know the scale factor, the ratio 
between the map and the virtual world. Second, we need to know which point on the map represents the 
origin of the world coordinate system. We assume here that the map model is originally aligned with the 
world (i.e. the x direction on the map, in its local coordinate system, represents the x direction in 
the world coordinate system). When the user presses the button and is intersecting the user icon on the 
map, then the icon needs to be moved with the stylus each frame. One cannot simply attach the icon to 
the stylus, because we want the icon to remain on the map even if the stylus does not. To do this, we 
.rst .nd the position of the stylus in the map coordinate system. This may require a transformation between 
coordinate systems, since the stylus is not a child of the map. The x and z coordinates of the stylus 
position are the point to which the icon should be moved. We do not cover here what happens if the stylus 
is dragged off the map, but the user icon should stick to the side of the map until the stylus is moved 
back inside the map boundaries, since we don t want the user to move outside the world. When the button 
is released, we need to calculate the desired position of the viewpoint in the world. This position is 
calculated using a transformation from the map coordinate system to the world coordinate system, which 
is detailed here. First, .nd the offset in the map coordinate system from the point corresponding to 
the world origin. Then, divide by the map scale (if the map is 1/100 the size of the world, this corresponds 
to multiplying by 100). This gives us the x and z coordinates of the desired viewpoint position. Since 
the map is 2D, we can t get a y coordinate from it. Therefore, the technique should have some way of 
calculating the desired height at the new viewpoint. In the simplest case, this might be constant. In 
other cases, it might be based on the terrain height at that location or some other factors. Once we 
know the desired viewpoint, we have to set up the animation of the viewpoint. The move vector m represents 
the amount of translation to do each frame (we are assuming a linear path). To .nd m, we subtract the 
desired position from the current position (the total movement required), divide this by the distance 
between the two points (calculated using the distance formula), and multiplied by the desired velocity, 
so that m gives us the amount to move in each dimension each frame. The only remaining calculation is 
the number of frames this movement will take: distance/velocity frames. Note that again velocity is measured 
here in units/frame, not units/second, for simplicity. Grabbing the Air The grabbing the air technique 
uses the metaphor of literally grabbing the world around you (usually empty space), and pulling yourself 
through it using hand gestures [Mapes and Moshell 1995]. This is similar to pulling yourself along a 
rope, except that the rope exists everywhere, and can take you in any direction. To implement the one-handed 
version of this technique (the two-handed version can get complex if rotation and world scaling is also 
supported), when the initial button press is detected, we simply obtain the position of the hand in the 
world coordinate system. Then, every frame until the button is released, get a new hand position, subtract 
it from the old one, and move the objects in the world by this amount. Alternately, you can leave the 
world .xed, and translate the viewpoint by the opposite vector. Before exiting the callback, be sure 
to update the old hand position for use on the next frame. Note it is tempting to implement this technique 
simply by attaching the world to the hand, but this will have the undesirable effect of also rotating 
the world when the hand rotates, which can be quite disorienting. You can also do simple constrained 
motion simply by ignoring one or more of the components of the hand position (e.g. only consider x and 
z to move at a constant height). 2.2.2 Selection 3D selection is the process of accessing one or more 
objects in a 3D virtual world. Note that selection and manip­ulation are intimately related, and that 
several of the techniques described here can also be used for manipulation. There are several common 
issues for the implementation of selection techniques. One of the most basic is how to indicate that 
the selection event should take place (e.g. you are touching the desired object, now you want to pick 
it up). This is usually done via a button press, gesture, or voice command, but it might also be done 
automatically if the system can infer the users intent. One also has to have ef.cient algorithms for 
object intersections for many of these techniques. Well discuss a couple of possibilities. The feedback 
given to the user regarding which object is about to be selected is also very important. Many of the 
techniques require an avatar (virtual representation) for the users hand. Finally, consider keeping a 
list of objects that are selectable, so that a selection technique does not have to test every object 
in the world, increasing ef.ciency. Four common selection techniques include the virtual hand, ray-casting, 
occlusion, and arm extension. The Virtual Hand The most common selection technique is the simple virtual 
hand, which does real-world selec­tion via direct touching of virtual objects. In the absence of haptic 
feedback, this is done by intersecting the virtual hand (which is at the same location as the physical 
hand) with a virtual object. Implementing this technique is simple, provided you have a good intersection/collision 
algorithm. Often, intersec­tions are only performed with axis-aligned bounding boxes or bounding spheres 
rather than with the actual geometry of the objects. Ray-Casting Another common selection technique is 
ray-casting. This technique uses the metaphor of a laser pointer an in.nite ray extending from the virtual 
hand [Mine 1995]. The .rst object intersected along the ray is eligible for selection. This technique 
is ef.cient, based on experimental results, and only requires the user to vary 2 degrees of freedom (pitch 
and yaw of the wrist) rather than the 3 DOFs required by the simple virtual hand and other location-based 
techniques. There are many ways to implement ray-casting. A brute-force approach would calculate the 
parametric equation of the ray, based on the hands position and orientation. First, as in the pointing 
technique for travel, .nd the world coordinate system equivalent of the vector (0, 0, -1). This is the 
direction of the ray. If the hands position is represented by (xh,yh,zh), and the direction vector is 
(xd,yd,zd), then the parametric equations are given by x(t)= xh + xdt (1) y(t)= yh + ydt (2) z(t)= zh 
+ zdt. (3) Only intersections with t> 0 should be considered, since we do not want to count intersections 
behind the hand. It is important to determine whether the actual geometry has been intersected, so .rst 
testing the intersection with the bounding box will result in many cases being trivially rejected. Another 
method might be more ef.cient. In this method, instead of looking at the hand orientation in the world 
coordinate system, we consider the selectable objects to be in the hands coordinate system, by transforming 
their vertices or their bounding boxes. This might seem quite inef.cient, because there is only one hand, 
while there are many polygons in the world. However, we assume we have limited the objects by using a 
selectable objects list. Thus, the intersection test we will describe is much more ef.cient. Once we 
have transformed the vertices or bounding boxes, we drop the z coordinate of each vertex. This maps the 
3D polygon onto a 2D plane (the xy plane in the hand coordinate system). Since the ray is (0, 0, -1) 
in this coordinate system, we can see that in this 2D plane, the ray will intersect the polygon if and 
only if the point (0, 0) is in the polygon. We can easily determine this with an algorithm that counts 
the number of times the edges of the 2D polygon cross the positive x-axis. If there are an odd number 
of crossings, the origin is inside, if even, the origin is outside. Occlusion Techniques Occlusion techniques 
(also called image plane techniques) work in the plane of the im­age; object are selected by covering 
it with the virtual hand so that it is occluded from your point of view [Pierce et al. 1997]. Geometrically, 
this means that a ray is emanating from your eye, going through your .nger, and then intersecting an 
object. These techniques can be implemented in the same ways as the ray-casting technique, since it is 
also using a ray. If you are doing the brute-force ray intersection algorithm, you can simply de.ne the 
rays direction by subtracting the .nger position from the eye position. However, if you are using the 
2nd algorithm, you require an object to de.ne the rays coordinate system. This can be done in two steps. 
First, create an empty object, and place it at the hand position, aligned with the world coordinate system. 
Next, determine how to rotate this object/coordinate system so that it is aligned with the ray direction. 
The angle can be determined using the positions of the eye and hand, and some simple trigonometry. In 
3D, two rotations must be done in general to align the new objects coordinate system with the ray. Arm-Extension 
The arm-extension (e.g., Go-Go) technique is based on the simple virtual hand, but it intro­duces a nonlinear 
mapping between the physical hand and the virtual hand, so that the user s reach is greatly extended 
[Poupyrev et al. 1996]. The graph in Figure 3 shows the mapping between the physical hand distance from 
the body on the x-axis and the virtual hand distance from the body on the y-axis. There are two regions. 
When the physical hand is at a depth less than a threshold D, the one-to-one mapping applies. Outside 
D, a non-linear mapping is applied, so that the farther the user stretches, the faster the virtual hand 
moves away. Figure 3: The nonlinear mapping function used in the Go-Go selection technique. To implement 
Go-Go, we .rst need the concept of the position of the users body. This is needed because we stretch 
our hands out from the center of our body, not from our head (which is usually the position that is tracked). 
We can implement this using an inferred torso position, which is de.ned as a constant offset in the negative 
y direction from the head. A tracker could also be placed on the users torso. Before rendering each frame, 
we get the physical hand position in the world coordinate system, and then calculate its distance from 
the torso object using the distance formula. The virtual hand distance can then be obtained by applying 
the function shown in the graph in Figure 3. d2.3 (starting at D) is a useful function in many environments, 
but the exponent used depends on the size of the environment and the desired accuracy of selection at 
a distance. Once the distance at which to place the virtual hand is known, we need to determine its position. 
The most common implementation is to keep the virtual hand on the ray extending from the torso and going 
through the physical hand. Therefore, if we get a vector between these two points, normalize it, multiply 
it by the distance, then add this vector to the torso point, we obtain the position of the virtual hand. 
Finally, we can use the virtual hand technique for object selection. 2.2.3 Manipulation As we noted 
earlier, manipulation is connected with selection, because an object must be selected before it can be 
manipulated. Thus, one important issue for any manipulation technique is how well it integrates with 
the chosen selection technique. Many techniques, as we have said, do both: e.g. simple virtual hand, 
ray-casting, and go-go. Another issue is that when an object is being manipulated, you should take care 
to disable the selection technique and the feedback you give the user for selection. If this is not done, 
then serious problems can occur if, for example, the user tries to release the currently selected object 
but the system also interprets this as trying to select a new object. Finally, thinking about what happens 
when the object is released is important. Does it remain at its last position, possibly .oating in space? 
Does it snap to a grid? Does it fall via gravity until it contacts something solid? The application requirements 
will determine this choice. Three common manipulation techniques include HOMER, Scaled-World Grab, and 
World-in-Miniature. HOMER The Hand-Centered Object Manipulation Extending Ray-Casting (HOMER) technique 
uses ray-casting for selection and then moves the virtual hand to the object for hand-centered manipulation 
[Bowman and Hodges 1997]. The depth of the object is based on a linear mapping. The initial torso-physical 
hand distance is mapped onto the initial torso-object distance, so that moving the physical hand twice 
as far away also moves the object twice as far away. Also, moving the physical hand all the way back 
to the torso moves the object all the way to the users torso as well. Like Go-Go, HOMER requires a torso 
position, because you want to keep the virtual hand on the ray between the users body (torso) and the 
physical hand. The problem here is that HOMER moves the virtual hand from the physical hand position 
to the object upon selection, and it is not guaranteed that the torso, physical hand, and object will 
all line up at this time. Therefore, we calculate where the virtual hand would be if it were on this 
ray initially, then calculate the offset to the position of the virtual object, and maintain this offset 
throughout manipulation. When an object is selected via ray-casting, .rst detach the virtual hand from 
the hand tracker. This is due to the fact that if it remained attached but the virtual hand model is 
moved away from the physical hand location, a rotation of the physical hand will cause a rotation and 
translation of the virtual hand. Next, move the virtual hand in the world coordinate system to the position 
of the selected object, and attach the object to the virtual hand in the scene graph (again, without 
moving the object in the world coordinate system). To implement the linear depth mapping, we need to 
know the initial distance between the torso and the physical hand dh, and between the torso and the selected 
object do. The ratio do/dh will be the scaling factor. For each frame, we need to set the position and 
orientation of the virtual hand. The selected object is attached to the virtual hand, so it will follow 
along. Setting the orientation is relatively easy. Simply copy the transformation matrix for the hand 
tracker to the virtual hand, so that their orientation matches. To set the position, we need to know 
the correct depth and the correct direction. The depth is found by applying the linear mapping to the 
current physical hand depth. The physical hand distance is simply the distance between it and the torso, 
and we multiply this by the scale factor do/dh to get the virtual hand distance. We then obtain a normalized 
vector between the physical hand and the torso, multiply this vector by the virtual hand distance, and 
add the result to the torso position to obtain the virtual hand position. Scaled-World Grab The scaled-world 
grab technique (see Figure 4) is often used with occlusion selection. The idea is that since you are 
selecting the object in the image plane, you can use the ambiguity of that single image to do some magic. 
When the selection is made, the user is scaled up (or the world is scaled down) so that the virtual hand 
is actually touching the object that it is occluding. If the user does not move (and the graphics are 
not stereo), there is no perceptual difference between the images before and after the scaling [Mine 
et al. 1997]. However, when the user starts to move the object and/or his head, he realizes that he is 
now a giant (or that the world is tiny) and he can manipulate the object directly, just like the simple 
virtual hand. To implement scaled-world grab, correct actions must be performed at the time of selection 
and release. Nothing special needs to be done in between, because the object is simply attached to the 
virtual hand, as in the simple virtual hand technique. At the time of selection, scale the user by the 
ratio (distance from eye to object / distance from eye to hand). This scaling needs to take place with 
the eye as the .xed point, so that the eye does not move, and should be uniform in all three dimensions. 
Finally, attach the virtual object to the virtual hand. At the time of release, the opposite actions 
are done in reverse. Re-attach the object to the world, and scale the user uniformly by the reciprocal 
of the scaling factor, again using the eye as a .xed point. Figure 4: An illustration of the scaled-world 
grab technique.  Figure 5: An example of a WIM. World-in-Miniature The world-in-miniature (WIM) technique 
uses a small dollhouse version of the world to allow the user to do indirect manipulation of the objects 
in the environment (see Figure 5). Each of the objects in the WIM are selectable using the simple virtual 
hand technique, and moving these objects causes the full-scale objects in the world to move in a corresponding 
way [Stoakley et al. 1995]. The WIM can also be used for navigation by including a representation of 
the user, in a way similar to the map-based travel technique, but including the 3rd dimension [Pausch 
et al. 1995]. To implement the WIM technique, .rst create the WIM. Consider this a room with a table 
object in it. The WIM is represented as a scaled down version of the room, and is attached to the virtual 
hand. The table object does not need to be scaled, because it will inherit the scaling from its parent 
(the WIM room). Thus, the table object can simply be copied within the scene graph. When an object in 
the WIM is selected using the simple virtual hand technique, .rst match this object to the corre­sponding 
full-scale object. Keeping a list of pointers to these objects is an ef.cient way to do this step. The 
miniature object is attached to the virtual hand, just as in the simple virtual hand technique. While 
the miniature object is being manipulated, simply copy its position matrix (in its local coordinate system, 
relative to its parent, the WIM) to the position matrix of the full-scale object. Since we want the full-scale 
object to have the same position in the full-scale world coordinate system as the miniature object does 
in the scaled-down WIM coordinate system, this is all that is necessary to move the full-scale object 
correctly. 2.2.4 System Control System control provides a mechanism for users to issue a command to 
either change the mode of interaction or the system state. In order to issue the command, the user has 
to select an item from a set. System control is a wide-ranging topic, and there are many different techniques 
to choose from such as the use of graphical menus, voice commands, gestures, and tool selectors. For 
the most part, these techniques are not dif.cult to implement, since they mostly involve selection. For 
example, virtual menu items might be selected using ray-casting. For all of the techniques, good visual 
feedback is required, since the user needs to know not only what he is selecting, but what will happen 
when he selects it. In this section, we brie.y highlight some of the more common system control techniques. 
 Figure 6: The Virtual Tricorder: an example of a graphical menu with device-centered placement. Graphical 
Menus Graphical menus can be seen as the 3D equivalent of 2D menus. Placement in.uences the access of 
the menu (correct placement can give a strong spatial reference for retrieval), and the effects of possible 
occlusion of the .eld of attention. The paper by Feiner et al. is an important source for placement issues 
[Feiner et al. 1993]. The authors divided placement into surround-.xed, world-.xed and display-.xed windows. 
The subdivision of placement can, however, be made more subtle. World-.xed and surround-.xed windows, 
the term Feiner et al. use to describe menus, can be subdivided into menus which are either freely placed 
into the world, or connected to an object. Display-.xed windows can be renamed, and made more precise, 
by referring to their actual reference frame: the body. Body-centered menus, either head referenced or 
body-referenced, can supply a strong spatial reference frame. One particularly interesting possible effect 
of body-centered menus is eyes-off usage, in which users can perform system control without having to 
look at the menu itself. The last reference frame is the group of device-centered menus. Device-centered 
placement provides the user with a physical reference frame (see Figure 6) [Wloka and Green.eld 1995]. 
A good example is the placement of menus on a responsive workbench, where menus are often placed at the 
border of the display device. We can subdivide graphical menus into hand-oriented menus, converted 2D 
menus, and 3D widgets. One can identify two major groups of hand-oriented menus. 1DOF menus are menus 
which use a circular object on which several items are placed. After initialization, the user can rotate 
his/her hand along one axis until the desired item on the circular object falls within a selection basket. 
User performance is highly dependent on hand and wrist physical movement and the primary rotation axis 
should be carefully chosen. 1DOF menus have been made in several forms, including the ring menu, sundials, 
spiral menus (a spiral formed ring menu), and a rotary tool chooser. The second group of hand-oriented 
menus are hand-held-widgets, in which menus are stored at a body-relative position. The second group 
is the most often applied group of system control interfaces: converted 2D widgets. These widgets basically 
function the same as in desktop environments, although one often has to deal with more DOFs when selecting 
an item in a 2D widget. Popular examples are pull-down menus, pop-up menus, .ying widgets, toolbars and 
sliders. Figure 7: An example of a 3D widget used to scale a geometric object. The .nal group of graphical 
menus is the group known as 3D widgets [Conner et al. 1992]. In a 3D world, widgets often mean moving 
system control functionality into the world or onto objects (see Figure 7). This matches closely with 
the de.nition of widgets given by Conner et al., widgets are the combination of geometry and behavior 
. This can also be thought of as moving the functionality of a menu onto an object. A very important 
issue when using widgets is placement. 3D widgets differ from the previously discussed menu techniques 
(1DOF and converted 2D menus) in the way the available functions are mapped: most often, the functions 
are co-located near an object, thereby forming a highly context-sensitive menu. Voice Commands Voice 
input allows the initialization, selection and issuing of a command. Sometimes, another input stream 
(like a button press) or a speci.c voice command is used to allow the actual activation of voice input 
for system control. The use of voice input as a system control technique can be very powerful: it is 
hands-free and natural. Still, continuous voice input is tiring, and can not be used in every environment. 
Furthermore, the voice recognition engine often has a limited vocabulary. In addition, the user .rst 
needs to learn the voice commands before they can be applied. Problems often occur when applications 
are more complex, and the complete set of voice commands can not be remembered. The structural organization 
of voice commands is invisible to the user: often no visual representation is coupled to the voice command 
in order to see the available commands. In order to prevent mode errors, it is often very important to 
supply the user with some kind of feedback after she has issued a command. This can be achieved by voice 
output, or by the generation of certain sounds. A very interesting way of supporting the user when interacting 
with voice and invisible menu structures can be found in telecommunication: using a telephone to access 
information often poses the same problems to the user as using voice commands in a virtual environment. 
Gestures and Postures When using gestural interaction, we apply a hand-as-tool metaphor: the hand literally 
becomes a tool. When applying gestural interaction, the gesture is both the initialization and the issuing 
of a com­ Figure 8: A user interacting with a dataset for visualizing a .ow .eld around a space shuttle. 
The user simultane­ously manipulates the streamlines with his left hand and the shuttle with his right 
hand while viewing the data in stereo. The user asked for these tools using speech input. mand, just 
as in voice input. When talking about gestural interaction, we refer, in this case, to gestures and postures, 
not to gestural input with pen-and-tablet or similar metaphors. There is a signi.cant difference between 
gestures and postures: postures are static movements (like pinching), whereas gestures include a change 
of position and/or orientation of the hand. A good example of gestures is the usage of sign language. 
Gestural interaction can be a very powerful system control technique. However, one problem with gestural 
inter­action is that the user needs to learn all the gestures. Since the user can normally not remember 
more than about 7 gestures (due to the limited capacity of the working memory), inexperienced users can 
have signi.cant problems with gestural interaction, especially when the application is more complex and 
requires a larger amount of gestures. Users often do not have the luxury of referring to a graphical 
menu when using gestural interaction -the structure underneath the available gestures is completely invisible. 
In order to make gestural interaction easier to use for a less advanced user, strong feedback, like visual 
cues after initiation of a command, might be needed. An example of application that used a gestural system 
control technique is MSVT [LaViola 2000]. This application combined gestures and voice input to create 
a multimodal interface for exploratory scienti.c visualization (see Figure 8). Tools We can identify 
two different kinds of tools, namely physical tools and virtual tools. Physical tools are context-sensitive 
input devices, which are often referred to as props. A prop is a real-world object which is duplicated 
in the virtual world. A physical tool might be space multiplexed (the tool only performs one function) 
or time multiplexed, when the tool performs multiple functions over time (like a normal desktop mouse). 
One accesses a physical tool by simply reaching for it, or by changing the mode on the input device itself. 
Virtual tools are tools which can be best exempli.ed with a toolbelt (see Figure 9). Users wear a virtual 
toolbelt around the waist, from which the user can access speci.c functions by grabbing at particular 
places on belt, as in the real world. Sometimes, functions on a toolbelt are accessed via the same principles 
as used with graphical menus, where one should look at the menu itself. The structure of tools is often 
not complex: as stated before, physical tools are either dedicated devices for one function, or one can 
access several (but not many) functions with one tool. Sometimes, a physical tool is the display medium 
for a graphical menu. In this case, it has to be developed in the same way as graphical menus. Virtual 
tools often use proprioceptive cues for structuring. Figure 9: An example of a virtual toolbelt. The 
user looks down to invoke the belt and can grab tools and use them in the virtual environment. It is 
still unclear how to best design tools for system control. Still, some general design issues can be stated. 
In the case of physical tools, the form of the tool often strongly communicates the function one can 
perform with the device, so take care with the form when developing new props. The form of a tool can 
highly in.uence the directness and familiarity with the device as well. With respect to virtual tools 
which can not be used without looking at them, their representation can be very similar to graphical 
menus. One example of a commonly used physically-based tool is the pen and tablet. Users hold a large 
plastic tablet on which a (traditional) 2D interface is displayed in the virtual world (see Figure 10). 
Users are able to use graphical menu techniques and can move objects with a stylus within a window on 
the tablet. The tablet supplies the user with strong physical cues with respect to the placement of the 
menu, and allows increased performance due to faster selection of menu items [Bowman et al. 1998]. For 
the implementation of this technique, the most crucial thing is the registration (correspondence) between 
the physical and virtual pens and tablets. The tablets, especially, must be the same size and shape so 
that the edge of the physical tablet, which the user can feel, corresponds to the edge of the virtual 
tablet as well. In order to make tracking easy, the origin of the tablet model should be located at the 
point where the tracker is attached to the physical tablet, so that rotations work properly. Even with 
care, its dif.cult to do these things exactly right, so a .nal tip is to include controls within the 
program to tweak the positions and/or orientations of the virtual pen and tablet, so that they can be 
into registration if there s a problem. Another useful function to implement is the ability for the system 
to report (for example when a callback function is called) the position of the stylus tip in the tablet 
coordinate system, rather than in the world or user coordinate systems. This can be used for things like 
the map-based travel technique described earlier. 2.3 3D Spatial Interaction in Video Games In the last 
section, we examined some techniques for performing common tasks with 3D user interfaces in virtual environments. 
We now switch gears and focus on how this type of interface can be used in the video game domain. One 
of the major barriers for researchers and developers to utilize 3D UIs in video games in the past has 
been due to the expense of the hardware (e.g., motion sensing input devices, 3D stereo displays). Although 
there has been some work in this area, such as SwordPlay (see Figure 11), a focus on 3D UI and gameplay 
has been minimal. SwordPlay is a game designed to explore what 3D user interfaces might be appropriate 
in a 3D, immersive gaming environment where two tracked wands are used to invoke a sword and shield and 
a bow and arrow to .ght enemies (players can also cast spells in 3D by drawing with the sword) [Katzourin 
et al. 2006]. Recently, motion sensing components have gotten much cheaper, resulting in the ability 
to have these devices avail­able to the mass market. In particular, with the release of the Wii, the 
latest gaming console from Nintendo, has made one of the most important technological innovations in 
gaming technology with respect to 3D user interfaces. The key innovation of the Wii is its controller, 
the Wiimote. This input device not only acts as a gamepad, but makes games accessible to the casual gamer 
because it can sense 3D motion and also provides researchers, developers, and hobbyists with the ability 
to explore their own ideas because the device can be connected to a standard computer. Although the Wiimote 
is a relatively simple device, its is quite challenging to use given its unique con.guration. Thus, in 
this section we explore the intricacies of the Wiimote and present some examples of how it has been used 
in games research. The ultimate goal of this section is to provide the reader with enough information 
to get started working with Wiimotes for exploring 3D UIs. 2.3.1 Understanding the Wiimote The Nintendo 
Wiimote is an example of a spatially convenient device. It provides spatial data, numerous functions 
and is designed for commodity use. The spatial data of the Wiimote is limited, prohibiting it from extracting 
a full 6 degrees of freedom (DOF). These limitations also result in the device providing certain types 
of information based on how it is held and whether it is stationary or moving. The combination of its 
accelerometers and optical sensor can provide a variety of useful information for creating 3D user interfaces. 
Some of this information comes directly from the sensors, but other pieces of information come from mathematical 
derivations. The Wiimote has a variety of functionality. First, the Wiimote contains Bluetooth for wireless 
transmission of its data to a computer up to 30ft. This enables it to move freely and be readily repositioned 
for its intended uses. Second, the Wiimote has several easily manipulatable buttons for discrete event 
input. Lastly, the Wiimote holds a speaker for audio and rumble feedback. The Wiimote is a commodity 
device. First, the setup of the device involves placing a sensor bar on a surface and connecting to a 
computer through Bluetooth. Second, the Wiimote consumes low amounts of energy, enabling its batteries 
to .t into a small, lightweight, easily held package. Lastly, the mass production of the Wiimote enables 
it to currently retail for 40 US dollars. Frames of Reference Using the Wiimote entails multiple frames 
of reference. Figure 12 shows the Wiimote s frame of reference (FOR) and data reported by the Wiimote 
is in this reference frame. The second is the Earth s FOR. The Earth is important as its gravity is detected 
by the Wiimote s accelerometers (see 3-Axis Accelerometer below). The .nal FOR is the Wiimote s relationship 
to the sensor bar (see The Sensor Bar below). To clarify these FOR, consider the following examples. 
Holding a Wiimote level (+Z up in the Wiimote and Earth s FOR) and facing the sensor bar, which is placed 
in front of the Wiimote, the user moves the Wiimote towards the sensor bar. This results in acceleration 
reported in the Y-axis of both the Earth and Wiimote FOR and the sensor bar reporting a shrinking distance 
between the Wiimote and sensor bar. Now, pointing the Wiimote down towards the earth and again moving 
the Wiimote forward, the acceleration in the Earth s FOR is unchanged in the Z-axis but is in the Z-axis 
of the Wiimote s FOR (the Y-axis in the Earth s FOR). Regarding the sensor bar s FOR, as the Wiimote 
is rotated, it does not see the sensor bar so there is no FOR. In this last example, if we place the 
sensor bar on the .oor and repeated the forward movement, then the sensor bar would report the Wiimote 
moved up in the Y-axis. The utility of each FOR will become apparent in the following sections. The Sensor 
Bar There are two primary spatial inputs that can be utilized with the Wiimote. The .rst input is the 
sensor bar connection (SBC), which results from the Wiimote s infrared optical sensor pointing into the 
distance at a sensor bar, easily identi.ed by its 5 LEDs on each side (see Figure 13). The LEDs have 
a known width between them and the LEDs are spread in a slight arch with the furthest pointed away from 
the center and the closest pointed inwards. This spread helps to improve the SBC s range, effectively 
operating the Wiimote up to 16ft away from the sensor bar. Figure 13: The Wiimote sensor bar has two 
groups of IR LEDs at .xed widths. When the Wiimote is pointed at the sensor bar, it picks up two points 
PL =(xL,yL) and PR =(xR,yR) from the LED arrays. The midpoint between these PL and PR can easily be calculated 
and used as a 2D cursor or pointer on the display. In addition, if the Wiimote is rotated about the Y-axis, 
we can calculate the roll of the device with 1v1 respect to the X-axis using . . roll = arccos x · v 
(4) where x = (1, 0) and v = PL - PR. Combining information from the sensor bar and the Wiimote s optical 
sensor also make it possible to determine how far the Wiimote is from the sensor bar using triangulation. 
The distance d between the sensor bar and Wiimote is calculated using w/2 d = (5) tan(./2) m · wimg w 
= (6) mimg _ mimg =(xL - xR)2 +(yL - yR)2 (7) where . is the optical sensor s viewing angle, m is the 
distance between the sensor bar s left and right LEDs, wimg is the width of the image taken from the 
optical sensor, and mimg is the distance between PL and PR taken from the optical sensor s image. Note 
that ., wimg, and m are all constants. This calculation only works when the Wiimote device is pointing 
directly at the sensor bar (orthogonally). When the Wiimote is off-axis from the sensor bar, more information 
is needed to .nd depth. We can utilize the relative sizes of the points on the optical sensor s image 
to calculate depth in this case. To .nd d in this case, we compute the distances corresponding to the 
the left and right points on the optical sensor s image wL/2 dL = (8) tan(./2) wR/2 dR = (9) tan(./2) 
using wimg · diamLED wL = (10) diamL wimg · diamLED wR = (11) diamR where diamLED is the diameter of 
the actual LED marker from the sensor bar and diamL and diamR are the diameters of the points found on 
the optical sensor s image. With dL and dR, we can then calculate Wiimote s distance to the sensor bar 
as d = d2 +(m/2)2 - 2dL(m/2)2 cos(f) (12) L where d22 - d2 LmR cos(f)= . (13) 2mdL Note that with d, 
dL, and m we can also .nd the angular position of the Wiimote with respect to the sensor bar, calculated 
as d22 - d2 m L a = arccos . (14) mdL 3-Axis Accelerometer The second Wiimote input is the device s 3-axis 
accelerometer. The accelerometer reports acceleration data in the device s x, y and z directions, expressed 
conveniently in g s. This is common of many devices employing 3-axis accelerometers such as the cell 
phones like the iPhone, laptops and camcorders. With this information, the Wiimote is able to sense motion, 
reporting values that are a blend of accelerations exerted by the user and gravity. As the gravity vector 
is constantly oriented towards the Earth (or (0, 0, 1) in Earth s FOR), the gravity vector can be used 
to discover part of the Wiimote s orientation in terms of earth s frame of reference using pitch = arctan 
az (15) ay roll = arctan az . (16) ax Two issues make determining the Wiimote s orientation from the 
accelerometer data a possible but problematic computation. First, yaw in the Earth s FOR cannot be determined 
as the Earth s gravity vector aligns with the Y-axis (the same is true for both pitch and roll when their 
axes aligns with the Earth s FOR). Second, the gravity vector is only known when the Wiimote is held 
steady; otherwise it is confounded by other accelerations. This limits its use as an orientation tracker 
while it is under motion. Determining the acceleration in the Wiimote s frame of reference is problematic 
as well. Unfortunately, the gravity vector confounds the reported acceleration of the Wiimote. So, the 
gravity vector must .rst be removed from the reported acceleration before the acceleration in the Wiimote 
s frame of reference can be obtained. Unfortunately, this requires knowing the orientation of the gravity 
vector which means either: (1) the Wiimote must be at rest, (2) assumptions are made about the Wiimote 
s orientation or (3) orientation is obtained externally by the SBC or a gyroscope (see Gyroscopes below). 
With the orientation, the gravity vector can be subtracted from the current acceleration data to obtain 
the Wiimote s acceleration. Waggling Movements Cheating motions! Or, less fatiguing and comfortable movements, 
depending on your perspective, are a side effect of the limitations of the Wiimote s input. While game 
designers may intend for games to encourage exercise, breaking the stereotype of the lazy video game 
player, the inability to detect actual position change enables users to make only small or limited little 
waggling motions that are interpreted as full movement. The result is boxing games played by tapping 
the Wiimote, tennis games played with wrist .icks and many games winnable by simply moving the device 
around in random directions. While still fun for gamers, this limits the application of the Wiimote for 
use in exercise and health gaming along with 3D UIs unless better hardware and data interpretation methods 
are employed. Gyroscopes Gyroscopes report orientation. While mechanical gyroscopes are typically too 
large and expensive for a Wiimote, MEMS gyroscopes are cheap, low power and fairly accurate. As such, 
a Wiimote with a gyroscope could provide orientation information, alleviating many of the Wiimote s issues. 
This is not just an academic idea, but is currently being employed by Nintendo in the production of the 
Wii Motion Plus add-on to the Wiimote (released by the time of this lecture s printing). The Wii Motion 
Plus is a two-axis gyroscope plug-in to the Wiimote to be released in 2009. With this, Nintendo is attempting 
to achieve a closer match of the Wii s motion to the real world. The accuracy of this device is to be 
determined. Compensation by Story When possible, the easiest means of compensating for limitations in 
input hardware is through the use of story. By this we mean that by careful manipulation of the user 
s tasks and their goals, the shortcomings of the hardware can be avoided. This is commonly used in Wii 
gaming, where accuracy is second to enjoyment and playability. In these cases, the inherent drift can 
be overridden by requiring the user to return to a neutral position, Wiimote orientation can be assumed 
by telling them how to hold the Wiimote and SBCs can be created by requiring the user to point at an 
on-screen button to begin a task. In the same way, story can engage the user, instructing them to freeze 
their hand at a .xed orientation for a static reading the gravity vector. After this, acceleration can 
be transformed into position information in relation to the gravity vector and any drift can be accounted 
for at the end of the trial. For example, the We Cheer game instructs the user to hold the Wiimote like 
they were holding pom-poms, WarioWare instructs how to hold the Wiimote for each mini-game and Wii Sports 
Boxing requires raising the hands in preparation for a .ght. While these are gaming stories , researchers 
commonly guide participants, instructing them to enter start positions before the beginning of a trial. 
For example, in studies of selection techniques, it is common to have users select an object to begin 
a trial. By proper use of story, many of the limitations of the hardware can be successfully hidden from 
the user. 2.3.2 Case Studies The Wiimote has been used in a variety of different ways. Of course, Johnny 
Chung Lee has done a variety of interesting projects and is probably the most famous Wiimote hacker [Lee 
2008]. However, there are many other researchers who are using Wiimotes for research in spatial 3D interfaces. 
Typically, there are two main approaches people use. The .rst is to actually wear the sensor bar and 
mount the Wiimote in some stationary position, using it as a camera. The second approach is to simply 
hold the Wiimote or attach it to the body. This approach makes more use of the accelerometer data than 
the image sensor data. In this section, we highlight some interesting research that focuses on using 
Wiimotes to create better gameplay experiences. One Man Band One Man Band (see Figure 14) is a prototype 
video game for musical expression that uses novel 3D spatial interaction techniques using accelerometer-based 
motion controllers [Bott et al. 2009]. One Man Band provides users with 3D gestural interfaces to control 
both the timing and sound of the music played, with both single and collaborative player modes. The current 
system supports the guitar, violin, bass, drums, trombone, and therein. It also has a multi-instrument 
musical interface called the MIMI. The idea behind the MIMI is that game players might want to quickly 
and easily transition from one musical instrument to another without any mode switching. The MIMI uses 
heuristics recognition and exponential smoothing to detect 5 different instruments. A study was recently 
conducted comparing One Man Band and Wii Music and the results showed users signi.cantly preferred One 
Man Band. RealDance Real Dance (see Figure 15) is a game prototype that is exploring more natural, full 
body interfaces for dancing games [Charbonneau et al. 2009]. In the game, users wear four Wiimotes attached 
to their wrists and ankles using velcro strips. This provides an untethered experience, meaning that 
the user does not need to stand in one place or position. Another goal of Real Dance is to explore how 
to increase the number of recognizable dance movements. The current prototype detects kicks, stomps, 
punches, and static poses. It also employs a visual interface for teaching users to perform the various 
dances. The visual interface provides either a timeline with icons, motion lines, or beat circles as 
well as score feedback with avatars and an animated instructor .gure.  WoW Navigation World of Warcraft 
has a complex interface and work is being done to determine how body­based interaction can be used to 
complement and reduce this complexity [Silva and Bowman 2009]. The idea is to using body based controls 
to of.oad keyboard and mouse navigation, helping players to concentrate on other tasks (see Figure 16). 
In this con.guration, the user wears a modi.ed sensor bar and mounts a Wiimote on the ceiling, using 
the device as a camera. Navigation is based on a leaning metaphor. Starting from a neutral body position, 
a small amount of forward or backward movement makes the character walk, and leaning farther forward 
makes the character run (rate control). There is a dead zone surrounding the neutral point in which the 
character stands still. Leaning to the side rotates the character, with the amount of rotation proportional 
to the distance the player leaned (position control). Note that a foot pedal is used to activate and 
deactivate movement. Preliminary experimentation has shown that body-based interaction in addition to 
keyboard and mouse can help players perform more tasks at the same time and can be especially attractive 
and helpful to new players. Exergaming The focus of this project, developed at Brown University under 
the direction of Chad Jenkins, is on exergaming, an important and up-and-coming research area that examines 
how to build effective exercise-based games. Wiisoccer is a game that uses natural locomotion to move 
players on a soccer .eld. The key innovation is that the IR sensor bar is attached to the users leg (see 
Figure 17) and a Wiimote is used as a camera and placed on the side of the player. The Wiimote detects 
the players running motion as well as kicks and passes.  2.4 Conclusions The divide between video games 
and 3D user interfaces is shrinking. As more affordable, commodity 3D spatial hardware becomes available, 
we will see an increased use of 3D UIs for more expressive gameplay. In this lecture, we have examined 
several techniques that are used for common tasks, such as navigation, selection, manipulation, and system 
control, in 3D virtual environments. These techniques are directly applicable to the video game domain. 
The Wiimote is a powerful input device that is not just for playing games with the Nintendo Wii console. 
It is a great device for exploring 3D spatial interfaces because it is inexpensive compared to other 
tracking systems, it works with a PC, and provides motion and pointing data. Although the device is not 
perfect because of the ambiguity and constrained input problem, it can still be used to implement a variety 
of 3D user interface techniques. The Wiimote continues to evolve and by the time this course has been 
given, Nintendo will have released the Wii Motion Plus, making the Wiimote more expressive, given the 
orientation sensing from its gyroscopes. Note that the Wiimote will never be ideal for implementing the 
3D UI techniques discussed in this lecture until it can sense a true 6 DOF (position and orientation). 
Other commodity devices are on the horizon, such as Sixense s TruMotion device, that will provide this 
capability making the the integration of 3D UIs into video games even easier. Of course, there is still 
a lot of research to be done in .nding the best ways of using these devices and 3D UIs in the video game 
domain. Acknowledgements A special thanks to Doug Bowman, Ernst Kruijff, Ivan Poupyrev, and Chad Wingrave 
for assisting in the develop­ment of the material for this lecture.  3 Sketch-Based Interfaces for Computer 
Graphics Creation and control of three-dimensional (3D) models for computer graphics authoring are still 
very dif.cult be­cause traditional interfaces for 3D modeling programs have their origins in professional 
drafting. The user places vertices in 3D space by specifying x-, y-, and z-coordinates in a three-view 
interface and then creates faces by connecting these vertices. Alternatively, the user starts with a 
simple primitive, such as a sphere or cylinder, and modi.es it by editing individual vertices and edges. 
Many editing tools (e.g., free-form deformation and Boolean operations among solids) are also available 
for designing complicated shapes from simple primitives. Authoring of animation is also dif.cult. In 
the typical user interface, the user speci.es individual key frames, which is very laborious. Although 
these interfaces may be appropriate for trained users designing precise models and animations, they are 
too dif.cult for .rst-time users to quickly generate meaningful models and animations. The sketching 
interface is emerging as an alternative modeling and control method taking its inspiration from the observation 
of human-to-human communication. People quickly sketch their ideas on a paper with a pen for com­munication 
and self-exploration. Sketching interfaces try to introduce this type of .uid interaction into creation 
and control of 3D models. The user simply draws two-dimensional (2D) lines on the screen; the system 
infers additional information from the user input and presents the result. In the case of 3D modeling, 
the system infers missing depth information. In the case of 3D animation, the system infers detailed 
motion from simple input. Two key issues exist in designing effective sketching systems. One is the design 
of algorithms to infer the missing information from the user input. Inference algorithms generally rely 
on special domain knowledge to achieve this. In addition, an in.nite number of possible 3D con.gurations 
match the given 2D input, so the 2D to 3D problem is not well-de.ned in general cases. However, speci.c 
domains impose certain constraints on the possible 3D con.guration and the system can return reasonable 
results by searching the problem space within these constraints. The other is the design of a user interface 
for disambiguation. Sketching is inherently ambiguous and having the computer return the correct answer 
with 100% certainty is generally dif.cult. Therefore, one must allow the user to modify the result or 
request other guesses. This course note introduces several sketching systems for computer graphics authoring. 
Individual systems address speci.c problems and propose speci.c techniques independently, and generalizing 
these systems is therefore dif.­cult. However, they all attempt to address the problem of inferring missing 
information using domain knowledge and solving the ambiguity problem by providing special disambiguation 
interfaces. We hope that readers will obtain some useful information to aid in resolving their own problems 
from these example systems. Please note that this course note is not a comprehensive survey of the .eld. 
The goal is to introduce key ideas behind sketching systems and the majority of this note is based on 
our own work. We focus mainly on sketching interfaces for 3D computer graphics authoring, especially 
shape modeling, deforma­tion, and animation. Many other interesting systems use sketching, such as user-guided 
image editing, but these are beyond the scope of this note. 3.1 2D Drawing -Interactive Beauti.cation 
The Pegasus system [Igarashi et al. 1997] allows the user to construct precise illustrations as shown 
in Fig. 18 without using complicated editing commands. The concept is to automate complicated drawing 
operations by having the computer infer possible geometric constraints and the user s next steps from 
the user s free-form strokes. Interactive beauti.cation receives the user s free stroke input and beauti.es 
it considering possible geometric constraints among line segments by generating multiple alternatives 
to prevent recognition errors. Predictive drawing foretells the user s next drawing operation based on 
the spatial relationships among existing segments on the screen. A prototype system is implemented as 
a JavaTM program, and our preliminary user study showed promising results. One notable feature of this 
system is that it provides a good disambiguation method. When the system shows the beauti.cation result, 
it also presents secondary candidates to the user in addition to the default result (Fig. 19). If the 
default result is not satisfactory, the user can quickly switch to a secondary result with simple clicking. 
We found that single answer fails too often and secondary candidates are essential for making this system 
usable.  Figure 19: Beauti.cation of an input stroke and multiple candidates. 3.2 3D Shape Modeling 
3.2.1 Architecture Models SKETCH: The SKETCH system introduced the gesture-based 3D scene construction 
technique [Zeleznik et al. 1996]. A simple gesture creates a 3D primitive object and places it in a 3D 
scene (Fig. 20). The system calculates the object s position based on the assumption that every object 
in the scene should be on some other object. For example, when the user draws three lines requesting 
a box in the 3D scene, the system puts the box on top of the existing box. In addition, the plate on 
the .oor is automatically lifted in 3D space when the user draws a leg under the plate, without changing 
the 2D appearance in the 2D window. As a result of these implicit placement rules, the user can construct 
3D scenes without using 2D widgets and can concentrate on the task (constructing a 3D scene) instead 
of spending time interacting with nested menus and commands. Chateau: The Chateau system [Igarashi and 
Hugues 2001] introduced an interface for 3D drawings that improves the usability of gestural interfaces 
and augments typical command-based modeling systems. In their suggestive interface, the user gives hints 
about a desired operation to the system by highlighting related geometric components in the scene. The 
system then infers possible operations based on the hints and presents the results of these operations 
as small thumbnails (Fig. 21). The user completes the editing operation simply by clicking on the desired 
thumbnail. The hinting mechanism allows the user to specify geometric relations among graphical components 
in the scene, and the multiple thumbnail suggestions make it possible to de.ne many operations with relatively 
few distinct hint patterns. The suggestive interface system is implemented as a set of suggestion engines 
working in parallel and can be extended easily by adding customized engines. The prototype 3D drawing 
system shows that a suggestive interface can effectively support the construction of various 3D drawings 
(Fig. 21). 3.2.2 Free-form Models Teddy: The Teddy system [Igarashi et al. 1999] is a sketching scheme 
for constructing free-form models. It allows the user to quickly generate interesting 3D free-form models, 
such as creating a teddy bear simply by drawing the silhouette of the desired shape. Fig. 22 shows an 
example modeling sequence in Teddy. The user s strokes are shown in red; everything else is inferred 
and drawn by the system. The user .rst draws the silhouette of the base primitive, and the system generates 
the corresponding 3D geometry. The user then draws a stroke across the model, and the system cuts the 
model at the line. The user can also add parts to the base model by drawing two strokes. Fig. 23 shows 
several 3D models created using the system.  This type of quick sketching system can be useful for nontraditional 
3D graphics applications. First, this tool can be used by nonexperts, including children, who wish to 
play with 3D graphics for fun. Teddy is already used in several commercial video games to permit players 
to create their own characters. Second, using this tool can be a useful way for experts to express their 
ideas quickly in the early design phases. A commercial 3D modeling package already includes an extension 
of Teddy as a plug-in for generating rough sketches. Finally, and most importantly, this tool will be 
useful for communicating 3D concepts face to face. In the classroom, the teacher can quickly draw a model 
of a bacterium and show the cross section to explain the internal structure. In a hospital, a medical 
doctor can draw a model of the stomach and explain the current status of a stomach disease to the patient. 
Fig. 24 shows an example in which a high school teacher used the system to teach the concept of contour 
lines in geography class. The original Teddy system used a single low-polygonal mesh as a surface representation. 
It only supported a surface with spherical topology and lacked the ability to edit individual parts afterward. 
Several other attempts have been made to address these problems using different geometry representations 
(Fig. 25). Owada et al. [2003] developed a system, Vteddy, based on voxel representation to handle topological 
changes. This system is useful for creating a model with holes and loops, such as a heart. Schmidt et 
al. [2005] developed a system called ShapeShop, which represents a model as a smooth integration of blob 
primitives and supports editing of these primitives afterward. For example, the user can adjust the location 
of a body part while preserving smooth connection between the body parts.  FiberMesh: Nealen et al. 
[2007] developed the FiberMesh system to create a model with a fair surface design and allow .exible 
editing (Fig. 26). It also uses polygonal mesh representation, but computes it via optimization that 
minimizes curvature variation. The user .rst creates a rough 3D model using a sketching interface. Unlike 
other sketching systems, the user-drawn strokes remain on the model surface and serve as handles for 
controlling the geometry. The user can add, remove, and deform these control curves easily, as if working 
with a 2D line drawing. The curves can have an arbitrary topology and need not be connected to each other. 
For a given set of curves, the system automatically constructs a smooth surface embedding by applying 
functional optimization. The system provides real-time algorithms for both control curve deformation 
and subsequent surface optimization.   Plushie: Mori et al. [2007] developed a sketch-based modeling 
system for the design of physical plush toys, called Plushie. The systems introduced above were all designed 
for virtual representations and did not consider any physical constraints. In contrast, a plush toy is 
a real physical object and cannot take an arbitrary form. Therefore, the Plushie system considers the 
physical constraints in the modeling process to facilitate the design of real plush toys. The user interface 
is similar to that of the original Teddy system [Igarashi et al. 1999]. The user interactively draws 
a sketch on the screen, and the system automatically generates a 3D plush toy model (Fig. 28 and Fig. 
29). In addition, the system simultaneously generates a 2D cloth pattern corresponding to the 3D geometry 
so that the user can create a physical plush toy by cutting the cloth according to the generated pattern. 
Internally, the system .rst generates a 2D cloth pattern and then runs a simple physical simulation to 
predict the 3D shape of the resulting toy. In this way, even young children can design their own plush 
toys simply by sketching.  Figure 29: The system automatically constructs the 3D geometry and 2D cloth 
pattern for the given user input. 3.2.3 Deformation Techniques Shape deformation is also a fundamental 
operation in shape modeling, which is necessary for both re.ning the static model and creating an animation. 
Traditional deformation methods rely mostly on the individual positioning of control vertices and achieving 
global deformation using these methods is a laborious process. Here, we introduce two complementary methods 
for sketch-based shape deformation. Sketching Skeleton: Kho et al. [2005] presented a method for deforming 
a 3D shape by drawing a skeleton. The user .rst draws a reference stroke on the 3D model and then draws 
a target stroke. The part of the 3D model near the reference stroke is then deformed so that it matches 
the target stroke (Fig. 30). The basic algorithm is similar to standard blend-shape skinning. The system 
computes the local coordinates of mesh vertices relative to the reference stroke and then computes the 
new global coordinates by applying the local coordinates to the target stroke. The authors also presented 
a method for twisting the model around the sketched axis. Sketching Silhouette: Another method for sketch-based 
shape deformation is to sketch the target silhouette [Nealen et al. 2005]. The user draws a curve indicating 
the desired local silhouette and the system deforms the surface near the curve so that the new silhouette 
matches the user-drawn silhouette. They use a detail-preserving shape deformation method in which the 
system simultaneously minimizes distance to the target silhouette and the distortion of local surface 
details. This makes it possible for the user to obtain reasonable results even with a very approximate 
silhouette sketch (Fig. 31, right).  3.3 Animations This section introduces several animation authoring 
methods. Animation is de.ned by a collection of deforming 3D models and specifying individual frames 
is a laborious procedure. The following systems introduce several ways for the user to specify reasonably 
complicated motions using simple input, such as sketching and direct performance. 3.3.1 Articulated Pose 
Control Davis et al. [2003] introduced a system for authoring .gure animation via simple stick .gure 
drawing. The user draws the 2D projection of the desired pose using a few lines and the system automatically 
infers the depth of each joint to construct a 3D representation. The system uses domain knowledge about 
the human body to successfully infer the joint angles. However, reaching a single solution is still dif.cult 
and the system asks the user to select the appropriate solution by presenting multiple possibilities. 
After specifying several key poses, the system then synthesizes an animation sequence by applying optimization. 
 3.3.2 Motion Doodles Thorne et al. [2004] developed a system in which the user sketches a single path 
in 2D space representing the desired trajectory and the system automatically makes an animation sequence, 
such as walking, running, and jumping. The sketched path can also represent textures of the motion. For 
example, the user can specify stomping by a zigzag stroke and a double-.ip jump by drawing a curve with 
a double loop in the middle. They also extended the technique to 3D space. Basic motions are crafted 
by hand in advance and the system adjusts and combines them to produce the .nal motion.  3.3.3 Animation 
by Performance Spatial Key Framing: One way to design a motion is through live demonstration in which 
the user moves the target character in real time and the system records the motion, e.g., a dancing teddy 
bear in front of a video camera. This can be a much more intuitive interface for inexperienced users 
to create an animation. However, moving a character with many joints is dif.cult using a standard input 
device such as a mouse. Demonstrating the motion of each joint one by one is possible, but synchronizing 
individual motions is not easy. The spatial key framing method was proposed to address this problem [Igarashi 
et al. 2005b]. The user .rst sets a group of key poses in the 3D or 2D space (a pose is associated with 
a position in space). The user then moves the cursor in the 3D or 2D space, and the system sets the character 
pose by blending the key poses around the cursor position (Fig. 34). In this way, the user can design 
interesting whole-body character motion, such as juggling and dancing, by recording simple cursor movements. 
 This technique is particularly suitable for de.ning expressive motions, such as a show of joy or sadness. 
The resulting motion is much more alive than motion generated by traditional key framing because the 
operator s natural hand movement appears directly in the motion. However, motion dominated by physical 
factors, such as jumping and running, is better supported by physical movement-based approaches. As-Rigid-as-Possible 
Shape Manipulation: In the real world, one can hold an object such as a teddy bear with two hands and 
freely manipulate it by moving, rotating, stretching, squashing, and bending it. However, this is not 
easy to do on a computer. Standard 2D drawing programs provide poor support for such shape manipulation 
and offer only simple editing operations, such as scaling and rotation. Not only do these operations 
require a complicated combination of separate tools, but the result does not feel like manipulating a 
physical entity. The as-rigid-as-possible shape manipulation technique [Igarashi et al. 2005a] was developed 
to address this problem. Using this method, the user can select arbitrary points as handles on a 2D shape 
and freely manipulate the shape by moving the handles (Fig. 35). The user can relocate the shape by setting 
a single handle and moving it, and can rotate, stretch, and squash the shape using two handles. Furthermore, 
the user can swing the head or stretch an arm simply by setting handles on the corresponding positions. 
The shape deforms naturally in response to the user input, providing the feeling of manipulating a physical 
object. This technique is particularly useful for creating 2D animations. Traditionally, generating 
many slightly different drawings is necessary to create an animation. However, in our system, one can 
create an interesting animation simply by drawing a character and recording the manipulation process. 
Using a multi-touch input device [Rekimoto 2002], the user can directly grab a character using both hands 
and manipulate it to create an animation (Fig. 36). We tested this system with children and found that 
even elementary school students could generate reasonable animations quickly.  3.4 Special Purpose 
Editors Sketching methods rely on domain knowledge to infer missing information and therefore can be 
very useful tools designed for speci.c targets. This section introduces some of those special purpose 
tools. 3.4.1 Tree and Flower Modeling Okabe et al. [2005] presented a system for quickly and easily designing 
3D models of botanical trees using freehand sketches and additional example-based editing operations. 
The system generates a 3D geometry from a 2D sketch based on the assumption that trees spread their branches 
so that the distances between the branches are as large as possible. The user can apply additional gesture-based 
editing operations, such as adding, cutting, and erasing branches. Our system also supports example-based 
editing modes in which many branches and leaves are generated using a manually designed tree as an example. 
Studies of user experiences demonstrated that our interface allows novices to design a variety of reasonably 
natural-looking trees interactively and quickly. Ijiri et al. [2005] reported a system for modeling 
.owers in three dimensions quickly and easily while preserving correct botanical structures. We used 
.oral diagrams and in.orescences, which were developed by botanists to concisely describe the structural 
information of .owers. Floral diagrams represent the layout of .oral components on a single .ower, while 
in.orescences are arrangements of multiple .owers. Based on these concepts, we developed a simple user 
interface that is specially tailored to .ower editing, while retaining a maximum variety of generable 
models. We also provide sketching interfaces to de.ne the geometries of .oral components. Separation 
of structural and geometric editing makes the authoring process more .exible and ef.cient. We found that 
even novice users could easily design various .ower models using our technique. Our system is an example 
of application-customized sketching, illustrating the potential power of a sketching interface that is 
carefully designed for a speci.c application. 3.4.2 Garment Design and Manipulation Another interesting 
application area for sketching interface is garment design and manipulation. Any individual character 
must be dressed in computer graphics applications and garment design is very important. However, as garments 
are made of .exible materials with very complicated geometry, the typical approach is to design a garment 
as a collection of rigid thin plates, which are placed around the body, and then run a physical simulation. 
However, obtaining the desired result is dif.cult using this approach. Here, we introduce two example 
systems addressing this problem. One method is to have the user sketch a garment directly on the character 
and the other is to allow the user to place a given garment on the character by sketching. Turquin et 
al. [2004] presented a sketching interface for dressing a virtual character. The user sketches the silhouette 
of the desired garment on the character and the system automatically generates a garment around the character. 
The system computes the distance .eld around the body of the 3D character and places the garment such 
that the distance between the body and garment in the 3D space is uniform across the body surface. The 
system also allows the user to sketch additional details such as folding. The method of Turquin et al. 
[2004] is mainly for the design of new garments and was not designed speci.cally to quickly test different 
ways of putting a garment on a character. Clothing manipulation techniques were developed to address 
this problem [Igarashi and Hughes 2002]. To put a garment on a character, the user .rst draws free-form 
marks on both the garment and the character, indicating positional correspondence (Fig. 40). The system 
then places the garment on the character so that the marks on the garment match the corresponding marks 
on the character. The system uses a simple relaxation process during placement to prevent stretching 
and squashing, even if the lengths of the corresponding marks are different. The user can place reasonably 
complicated garments on the character with only a few strokes. Once the garment is on the character, 
the user can grab any point of the garment and drag it onto the surface of the character (Fig. 41). Unlike 
standard vertex dragging in which a single vertex is moved and relies on subsequent simulation to move 
the other vertices, this dragging operation directly moves all vertices of the cloth mesh, causing global 
movement. To achieve this, the movement vector of the dragged vertex is propagated to the complete cloth 
mesh along the surface of the character.  3.5 Summary This note introduced several sketching systems 
for 3D modeling and animation design. They begin with the obser­vation of real sketching activities and 
transfer some of the essence of these activities into the computer. They all infer some missing information 
in the simple user input using domain knowledge and also provide a disambiguation interface to modify 
unexpected results. Although speci.c inference algorithms and interaction techniques introduced here 
may not be directly applicable to other problems, we believe that the design approaches behind these 
techniques are general and useful for solving problems in different domains.  4 Haptic Interaction with 
Virtual Environments For a long time, human beings have dreamed of a virtual world where it is possible 
to interact with synthetic entities as if they were real. To date, the advances in computer graphics 
allow us to see virtual objects and avatars, to hear them, to move them, and to touch them. It has been 
shown that the ability to touch virtual objects increases the sense of presence in virtual environments 
[Insko 2001]. Haptic rendering offers important applicability in engineering and medical training tasks. 
This part of the notes focuses on how to empower interactive systems with tactile or haptic feedback. 
First, we describe the various ways in which the haptic modality can be used in the context of interaction. 
Then, we focus on one of the interaction paradigms, i.e., haptic rendering, which allows us to perceive 
contact with a virtual world. Research in the .eld of haptics in the last 35 years has covered many more 
areas than what we have summarised here. [Burdea 1996; McLaughlin et al. 2002; Lin and Otaduy 2008] give 
a general survey of the .eld of haptics, as well as a discussion on current research topics. 4.1 Introduction 
We start by de.ning some terminology, describing the main interaction paradigms, discussing the evolution 
of the research in haptic rendering, and introducing practical applications. 4.1.1 De.nitions The term 
haptic (from the Greek haptesthai, meaning to touch ) is the adjective used to describe something relating 
to or based on the sense of touch. Haptic is to touching as visual is to seeing and as auditory is to 
hearing [Fisher et al. 2004]. As described by Klatzky and Lederman [Klatzky and Lederman 2003], touch 
is one of the main avenues of sensation, and it can be divided into cutaneous, kinesthetic, and haptic 
systems, based on the underlying neural inputs. The cutaneous system employs receptors embedded in the 
skin, while the kinesthetic system employs receptors located in muscles, tendons, and joints. The haptic 
sensory system employs both cutaneous and kinesthetic receptors, but it differs in the sense that it 
is associated with an active procedure. Touch becomes active when the sensory inputs are combined with 
controlled body motion. For example, cutaneous touch becomes active when we explore a surface or grasp 
an object, while kinesthetic touch becomes active when we manipulate an object and touch other objects 
with it. Haptic rendering is de.ned as the process of computing and generating forces in response to 
user interactions with virtual objects [Salisbury et al. 1995]. Several haptic rendering algorithms consider 
the paradigm of touching virtual objects with a single contact point. Rendering algorithms that follow 
this description are called 3-DoF (degree-of­freedom) haptic rendering algorithms, because a point in 
3D has only three DoFs. Other haptic rendering algorithms deal with the problem of rendering the forces 
and torques arising from the interaction of two virtual objects. This problem is called 6-DoF haptic 
rendering, because the grasped object has six DoFs (position and orientation in 3D), and the haptic feedback 
comprises 3D force and torque. When we eat with a fork, write with a pen, or open a lock with a key, 
we are moving an object in 3D, and we feel the interaction with other objects. This is, in essence, 6-DoF 
object manipulation with force-and-torque feedback. Figure 42 shows an example of a user experiencing 
haptic rendering. When we manipulate an object and touch other objects with it, we perceive cutaneous 
feedback as the result of grasping, and kinesthetic feedback as the result of contact between objects. 
 4.1.2 Haptic Interaction Paradigms Haptic human-machine interaction can be perhaps divided into three 
large categories, based on the purpose of this interaction. 1. Visualization of data. In this situation, 
the original data that is perceived is not necessarily in the tactile modality, but it is transformed 
into this modality either because it is high-dimensional data, or because collo­cation with the human 
motor system may be bene.tial. Some examples of haptic visualization of data include the visualization 
of scienti.c data (such as .uid dynamics [Lawrence et al. 2000], see Fig. 43), or visuo-haptic segmentation 
of radiological data [Harders and Szekely 2003] (see Fig. 44). 2. Haptic augmentation of cursor actions. 
In classical human-computer interaction, the cursor is a merely passive item, but haptics may augment 
the perception of the user, thus helping discern various signals. Haptic cursor augmentation is classically 
associated with devices for aiding the visually disabled, but it has recently become of relevance with 
the explosion of tactile screens (see the TouchScreen concept image from Immersion Corporation in Fig. 
45-left) and small-screen hand-held devices [Luk et al. 2006] (see Fig. 45-right). 3. Tactile interaction 
with a virtual world. But, probably, the most common way of haptic interaction is the one that presents 
a virtual world to the user, and allows the user to move an object in this virtual world and perceive 
contact with the environment. This is the paradigm best known as haptic rendering, and the rest of this 
section will focus on it.    4.1.3 Kinesthetic Display of Tool Contact As introduced earlier, haptic 
perception can be divided into two main categories, based on the nature of the mechanore­ceptors that 
are activated: cutaneous perception is related to mechanoreceptors in the skin, while kinesthetic per­ception 
is related to mechanoreceptors in joints, tendons, and muscles. And, the type of contact forces that 
can be perceived can be classi.ed into two types: forces that appear in direct contact between the skin 
and the environment, and forces that appear due to contact between an object manipulated by the user 
(i.e., the tool) and other objects in the environment. Here, we focus mostly on the kinesthetic perception 
of contact forces through a tool, i.e., the perception of contact between a manipulated tool and other 
objects, on our joints, tendons, and muscles. Even when using an intermediate tool, subjects can infer 
medium-and large-scale properties of the objects in the environment as if touching them directly. Moreover, 
the use of a tool becomes a convenient computational model in the design of haptic rendering algorithms, 
and even the computation of direct skin interaction could make use of a tool model for representing e.g. 
.ngers. 4.1.4 From Telerobotics to Haptic Rendering In 1965, Ivan Sutherland [Sutherland 1965] proposed 
a multimodal display that would incorporate haptic feedback into the interaction with virtual worlds. 
Before that, haptic feedback had already been used mainly in two appli­cations: .ight simulators and 
master-slave robotic teleoperation. The early teleoperator systems had mechanical linkages between the 
master and the slave. But, in 1954, Goertz and Thompson [Goertz and Thompson 1954] de­veloped an electrical 
servomechanism that received feedback signals from sensors mounted on the slave and applied forces to 
the master, thus producing haptic feedback. From there, haptic interfaces evolved in multiple directions, 
but there were two major breakthroughs. The .rst breakthrough was the idea of substituting the slave 
robot by a simulated system, in which forces were computed using physically based simulations. The GROPE 
project at the University of North Carolina at Chapel Hill [Brooks, Jr. et al. 1990], lasting from 1967 
to 1990, was the .rst one to address the synthesis of force feedback from simulated interactions. In 
particular, the aim of the project was to perform real-time simulation of 3D molecular-docking forces. 
The second breakthrough was the advent of computer-based Cartesian control for teleoperator systems [Bejczy 
and Salisbury 1980], enabling a separation of the kinematic con.gurations of the master and the slave. 
Later, Cartesian control was applied to the manipulation of simulated slave robots [Kim and Bejczy 1991]. 
Those .rst haptic systems were able to simulate the interaction of simple virtual objects only. Perhaps 
the .rst project to target computation of forces in the interaction with objects with rich geometric 
information was Minsky s Sandpaper [Minsky et al. 1990]. Minsky et al. developed a planar force feedback 
system that allowed the exploration of textures. A few years after Minsky s work, Zilles and Salisbury 
presented an algorithm for 3-DoF haptic rendering of polygonal models [Zilles and Salisbury 1995]. Almost 
in parallel with Zilles and Salisbury s work, Massie and Salisbury [Massie and Salisbury 1994] designed 
the PHANTOM, a stylus-based haptic interface that was later commercialised and has become one of the 
most commonly used force-feedback devices. But in the late 90s, research in haptic rendering revived 
one of the problems that .rst inspired virtual force feedback: 6-DoF haptic rendering or, in other words, 
grasping of a virtual object and synthesis of kinesthetic feedback of the interaction between this object 
and its environment. 4.1.5 Haptic Rendering for Virtual Manipulation Certain professional activities, 
such as training for high-risk operations or pre-production prototype testing, can bene.t greatly from 
simulated reproductions. The .delity of the simulated reproductions depends, among other factors, on 
the similarity of the behaviours of real and virtual objects. In the real world, solid objects cannot 
inter­penetrate. Contact forces can be interpreted mathematically as constraint forces imposed by penetration 
constraints. However, unless penetration constraints are explicitly imposed, virtual objects are free 
to penetrate each other in virtual environments. Indeed, one of the most disconcerting experiences in 
virtual environments is to pass through virtual objects [Insko et al. 2001; Slater and Usoh 1993]. Virtual 
environments require the simulation of non­penetrating rigid body dynamics, and this problem has been 
extensively explored in the robotics and computer graphics literature [Baraff 1992; Mirtich 1996]. It 
has been shown that being able to touch physical replicas of virtual objects (a technique known as passive 
hap­tics [Insko 2001]) increases the sense of presence in virtual environments. This conclusion can probably 
be gen­eralised to the case of synthetic cutaneous feedback of the interaction with virtual objects. 
As reported by Brooks et al. [Brooks, Jr. et al. 1990], kinesthetic feedback radically improved situation 
awareness in virtual 3D molecu­lar docking. Kinesthetic feedback has proved to enhance task performance 
in applications such as telerobotic object assembly [Hill and Salisbury 1977], virtual object assembly 
[Unger et al. 2002], and virtual molecular docking [Ouh-Young 1990]. In particular, task completion time 
is shorter with kinesthetic feedback in docking operations but not in prepositioning operations. To summarise, 
haptic rendering is especially useful in particular examples of training for high-risk operations or 
pre­production prototype testing activities that involve intensive object manipulation and interaction 
with the environ­ment. Such examples include minimally invasive or endoscopic surgery [Edmond et al. 
1997; Hayward et al. 1998] and virtual prototyping for assembly and maintainability assessment [McNeely 
et al. 1999; Chen 1999; Andriot 2002; Wan and McNeely 2003] (Fig. 46). Force feedback becomes particularly 
important and useful in situations with limited visual feedback.  4.2 Stable Interaction Algorithm 
A haptic rendering system couples a discrete subsystem (i.e., the rendering algorithm) and a continuous 
subsystem (i.e., the device and the user). The simulation of a virtual environment using a discrete model, 
together with sampling and the associated time-delay of the communications, induces a perception of a 
virtual environment that is not quite an exact replica of the intended continuous model. The practical 
implication of this difference is that the complete system user-device-algorithm may become unstable 
when it is perfectly stable in a real (not simulated) world. Much of the work on haptic rendering has 
targeted the design of stable haptic rendering algorithms. 4.2.1 Teleoperation of a Virtual Tool Figure 
42 shows an example of haptic rendering of the interaction between two virtual jaws. The user manipulates 
a haptic device, and the rendering algorithm allows him to perceive the contact between the jaws as if 
he were actually holding and moving the upper jaw. In this example, the upper jaw can be regarded as 
a virtual tool, and the lower jaw constitutes the rest of the virtual environment. For virtual contact 
to be perceived as the result of the interaction between a virtual tool and the virtual environment, 
haptic rendering consists of two key tasks: 1. Compute and display the forces resulting from contact 
between the virtual tool and the virtual environment. 2. Compute the con.guration of the virtual tool. 
 A haptic rendering system composes of two sub-systems: one real system (i.e., the user and the haptic 
device), and one virtual system (i.e., the tool and the environment). The tool acts as a virtual counterpart 
of the haptic device. From this perspective, haptic rendering presents a remarkable similarity to master-slave 
teleoperation, a topic well studied in the .eld of robotics. The main difference is that in teleoperation 
both the master and the slave are real physical systems, while in haptic rendering the tool is a virtual 
system. Similarly, haptic rendering shares some of the challenges of teleoperation, namely, the computation 
of transparent teleoperation: i.e., the virtual tool should follow the con.guration of the device, and 
the device should produce forces that match those computed on the tool, without .ltering or instability 
artifacts.  4.2.2 Tasks of Haptic Rendering It becomes clear from the discussion above that a haptic 
rendering problem should address two major computational issues, i.e., .nding the tool con.guration and 
computing contact forces, and it should use information about device con.guration. Moreover, it requires 
knowledge about the environment, and in some cases it modi.es the environ­ment as well. With these aspects 
in mind, one possible general de.nition of the rendering problem could be as follows: Given a con.guration 
of the haptic device H, .nd a con.guration of the tool T that minimizes an objective function f(H-T ), 
subject to environment constraints. Display to the user a force F(H, T ) dependent on the con.gurations 
of the device and the tool. This de.nition assumes a causality precedence where the input variable is 
the con.guration of the haptic device H, and the output variable is the force F. This precedence is known 
as impedance rendering, because the haptic rendering algorithm can be regarded as a programmable mechanical 
impedance [Hogan 1985; Adams and Han­naford 1998], as depicted in Figure 47. In impedance rendering, 
the device control system should provide position information and implement a force control loop. A different 
possibility is admittance rendering, where the haptic rendering system can be regarded as a programmable 
mechanical admittance that computes the desired device con.guration, as the result of input device forces. 
In that case, the device control should provide device forces and implement a position control loop. 
As discussed by [Adams and Hannaford 1998], the two types of rendering systems present dual properties 
and their design can be addressed in a uni.ed manner, therefore here we restrict the exposition to impedance 
rendering. In the de.nition of the haptic rendering problem presented above we have not speci.ed the 
objective function that must be optimized for computing the tool s con.guration nor the function for 
computing output forces. The dif­ferences among various haptic rendering algorithms lie precisely in 
the design of these functions, and are brie.y outlined next. 4.2.3 Stability and Control Theory The 
concept of mechanical impedance extends the notion of electrical impedance and refers to the quotient 
between force and velocity. Hogan [Hogan 1985] introduced the idea of impedance control for contact tasks 
in manipulation. Earlier techniques controlled contact force, robot velocity, or both, but Hogan suggested 
controlling directly the mechanical impedance, which governs the dynamic properties of the system. When 
the end effector of a robot touches a rigid surface, it suffers a drastic change of mechanical impedance, 
from low impedance in free space, to high impedance during contact. This phenomenon imposes serious dif.culties 
on earlier control techniques, inducing instabilities. The function of a haptic device is to display 
the feedback force of a virtual world to a human user. Haptic devices present control challenges very 
similar to those of manipulators for contact tasks. A subsystem is passive if it does not add energy 
to the global system. Passivity is a powerful tool for analysing stability of coupled systems, because 
the coupled system obtained from two passive subsystems is always stable. Colgate and his colleagues 
were the .rst to apply passivity criteria to the analysis of stability in haptic rendering of virtual 
walls [Colgate et al. 1993]. Passivity-based analysis has enabled separate study of the behaviour of 
the human subsystem, the haptic device, and the virtual environment in force-feedback systems. Hogan 
discovered that the human neuromuscular system exhibits externally simple, springlike behaviour [Hogan 
1986]. This .nding implies that the human arm holding a haptic device can be regarded as a passive subsystem, 
and the stability analysis can focus on the haptic device and the virtual environment. Note that human 
limbs are not passive in all conditions, but the bandwidth at which a subject can perform active motions 
is very low compared to the frequencies at which stability problems may arise. Some authors [Shimoga 
1992; Burdea 1996] report that the bandwidth at which humans can perform controlled actions with the 
hand or .ngers is between 5 and 10Hz. On the other hand, sensing bandwidth can be as high as 20 to 30Hz 
for proprioception, 400Hz for tactile sensing, and 5 to 10kHz for roughness perception. Colgate and Schenkel 
[Colgate and Schenkel 1994] observed that the oscillations perceived by a haptic user dur­ing system 
instability are a result of active behaviour of the force-feedback system. This active behaviour is a 
consequence of time delay and loss of information inherent in sampled-data systems, as suggested by others 
be­fore [Brooks, Jr. et al. 1990]. Colgate and Schenkel formulated passivity conditions in haptic rendering 
of a virtual wall. For that analysis, they modelled the virtual wall as a viscoelastic unilateral constraint, 
and they accounted for the continuous dynamics of the haptic device, sampling of the position signal, 
discrete differentiation for obtaining velocity, and a zero-order hold of the output force. They reached 
a suf.cient condition for passivity that relates the stiffness K and damping B of the virtual wall, the 
inherent damping b of the device, and the sampling period T : KT b> + B. (17) 2 After deriving stability 
conditions for rendering virtual walls modelled as unilateral linear constraints, Colgate and his colleagues 
considered more complex environments [Colgate et al. 1995]. A general virtual environment is non-linear, 
and it presents multiple and variable constraints. Their approach enforces a discrete-time passive imple­mentation 
of the virtual environment and sets a multidimensional viscoelastic virtual coupling between the virtual 
environment and the haptic display. In this way, the stability of the system is guaranteed as long as 
the virtual coupling is itself passive, and this condition can be analysed using the same techniques 
as those used for virtual walls [Colgate and Schenkel 1994]. As a result of Colgate s virtual coupling 
[Colgate et al. 1995], the complexity of the problem was shifted towards designing a passive solution 
of virtual world dynamics. As noted by Colgate et al. [Colgate et al. 1995], one possible way to enforce 
passivity in rigid body dynamics simulation is to use implicit integration with penalty methods. Adams 
and Hannaford [Adams and Hannaford 1998] derived stability conditions for coupled systems based on net­work 
theory. They also extended the concept of virtual coupling to admittance-type devices. Miller et al. 
[Miller et al. 1999] extended Colgate s passivity analysis techniques, relaxing the requirement of passive 
virtual environments but enforcing cyclo-passivity of the complete system. Hannaford and his colleagues 
[Hannaford et al. 2002] investi­gated the use of adaptive controllers instead of the traditional .xed-value 
virtual couplings. They designed passivity observers and passivity controllers for dissipating the excess 
of energy generated by the virtual environment. 4.2.4 The Optimization Problem The simplest possible 
option for the objective function is the distance between tool and haptic device, i.e., f = 1H - T 1. 
This means that the haptic rendering algorithm should simply try to place the tool as close as possible 
to the haptic device. Given this objective function, one can incorporate information about the environment 
in multiple ways. In the most simplest way, known as direct rendering and described in more detail in 
the next section, the environment is not accounted for in the optimization, leading to the solution T 
= H. Another possibility is to introduce hard inequality constraints gi(T ) = 0 that model non-penetration 
of the tool in the environment. The highly popular god-object and virtual proxy (Fig. 48) 3-DoF haptic 
rendering algorithms [Zilles and Salisbury 1995; Ruspini et al. 1997] formulate such an optimization 
problem. Non-penetration may also be modeled following the e 2 penalty method, with soft constraints 
that are added to the objective function: f = 1H - T 1 + kig(T ). ii As in general optimization problems, 
there are multiple ways of solving the tool s con.guration. The optimization may be solved until convergence 
on every rendering frame, but it is also possible to perform a .nite number of solver iterations (e.g., 
simply one), leading to a quasi-static solution for every frame. Moreover, one could add inertial behavior 
to the tool, leading to a problem of dynamic simulation. Then, the prob­lem of solving the con.guration 
of the tool can be formulated as a dynamic simulation of a virtual physically-based replica of a real 
object. This approach has been, in fact, followed by several authors (e.g., [McNeely et al. 1999; Otaduy 
and Lin 2006]). Modeling the virtual tool as a dynamic object has shown advantages for the analysis of 
stability of the complete rendering algorithm. If the dynamic simulation can be regarded as a solution 
to an opti­mization problem, where forces represent the gradient of the objective function, then the 
selected type of numerical integration method can be viewed as a different method for solving the optimization 
problem. For example, a simple explicit Euler corresponds to gradient descent, while implicit Euler corresponds 
to a Newton solver. 4.2.5 Direct Rendering Algorithm The overall architecture of direct rendering methods 
is shown in Figure 49. Direct rendering relies on an impedance­type control strategy. First, the con.guration 
of the haptic device is received from the controller, and it is assigned directly to the virtual tool. 
Collision detection is then performed between the virtual tool and the environment. Collision response 
is typically computed as a function of object separation or penetration depth using penalty-based methods. 
Finally, the resulting contact force (and possibly torque) is directly fed back to the device controller. 
 Figure 49: Main Components of a Direct Rendering Algorithm. More formally, and following the discussion 
from Section 4.2.4, direct rendering corresponds to an optimization problem that trivially assigns T 
= H (up to some scale or rigid transformation). This solution answers the second problem in haptic rendering, 
i.e., the computation of the con.guration of the tool. Then, the .rst problem, the computation of forces, 
is formulated as a function of the location of the tool in the virtual environment, F(g, T ). The popularity 
of direct rendering stems obviously from the simplicity of the calculation of the tool s con.guration, 
as there is no need to formulate a complex optimization problem (for example, rigid body dynamics in 
6-DoF rendering). However, the use of penalty methods for force computation has its drawbacks, as penetration 
values may be quite large and visually perceptible, and system instability can arise if the force update 
rate drops below the range of stable values. Throughout the years, direct rendering architectures have 
often been used as a .rst practical approach to haptic rendering. Thus, the .rst 3-DoF haptic rendering 
algorithms computed forces based on potential .elds de.ned in­side the environment. However, as pointed 
out early by [Zilles and Salisbury 1995], this approach may lead to force discontinuities and pop-through 
problems. Direct rendering algorithms are perhaps more popular for 6-DoF rendering, and a number of authors 
have used them, in combination with convex decomposition and hierarchical collision detection [Gregory 
et al. 2000; Ehmann and Lin 2001] (See Fig. 50); with parametric surface representa­tions [Nelson et 
al. 1999]; collision detection hierarchies based on normal cones [Johnson and Cohen 2001; Johnson and 
Willemsen 2003; Johnson and Willemsen 2004; Johnson et al. 2005] (Fig. 52); or together with fast penetration 
depth computation algorithms and contact clustering [Kim et al. 2002; Kim et al. 2003] (Fig. 51). In 
most of these approaches, the emphasis was on fast collision detection or proximity queries, and the 
work can also be combined with the virtual coupling algorithms described next.  4.2.6 Rendering through 
Virtual Coupling Despite the apparent simplicity of direct rendering, the computation of contact and 
display forces may become a complex task from the stability point-of-view. Stability of haptic rendering 
can be answered by studying the range of programmable impedances. With direct rendering and penalty-based 
contact forces, rendering impedance is hardly controllable, leading often to unstable haptic display. 
Stability enforcement can largely be simpli.ed by separating the device and tool con.gurations, and inserting 
in­between a viscoelastic link referred to as virtual coupling [Colgate et al. 1995]. The connection 
of passive subsys­tems through virtual coupling leads to an overall stable system. Figure 53 depicts 
the con.gurations of the device and the tool in a 6-DoF virtual contact scenario using virtual coupling. 
Contact force and torque are transmitted to the user as a function of the translational and rotational 
misalignment between tool and device con.gurations. The most common form of virtual coupling is a viscoelastic 
spring-damper link. Such a virtual coupling was used by [Zilles and Salisbury 1995; Ruspini et al. 1997] 
in the god-object and virtual proxy algorithms for 3-DoF ren­dering. The concept was later extended to 
6-DoF rendering [McNeely et al. 1999], by considering translational and rotational springs. For simplicity, 
here we also group under the name of virtual coupling other approaches that sep­arate tool and device 
con.gurations, such as the four-channel architecture based on teleoperation control designed by [Sirouspour 
et al. 2000], or constraint-aware projections of virtual coupling for 6-DoF rendering [Ortega et al. 
2006] (see Fig. 56). The use of a virtual coupling allows a separate design of the impedance displayed 
to the user (subject to stability criteria), from the impedance (i.e., stiffness) of environment constraints 
acting on the tool. Environment constraints can be of high stiffness, which reduces (or even completely 
eliminates) visible interpenetration problems. On the other hand, virtual coupling algorithms may suffer 
from noticeable and undesirable .ltering effects, in case the update rate of the haptic rendering algorithm 
becomes too low, which highly limits the value of the rendering impedance. Multirate algorithms [Adachi 
et al. 1995; Mark et al. 1996a; Otaduy and Lin 2006] (discussed in more detail in section 4.5) can largely 
increase the transparency of the rendering by allowing stiffer impedances. 4.3 Components of a Rendering 
Algorithm Figure 55: Main Components of a General Impedance-Type Rendering Algorithm. On a high level, 
for an impedance-rendering system, the rendering algorithm must be composed of (i) a solver module that 
determines the con.guration of the tool, and (ii) a collision detection module that de.nes environment 
constraints on the tool. Figure 55 depicts such high-level description of the algorithm. More speci.cally, 
we distinguish the following components of the rendering algorithm, which may vary among speci.c implementations: 
 4.3.1 The Tool Model The number of degrees of freedom (DoFs) of the tool is a variable that to a large 
extent depends on the application, and should be minimized as much as possible to reduce the complexity 
of the rendering. Hence, in many modeling applications, it suf.ces to describe the tool as a point with 
three DoFs (translation in 3D), in medical applications, it is often possible to describe it as a ray 
segment with .ve DoFs, and in virtual prototyping, it is often required to describe the tool as a solid 
with six DoFs (translation and rotation in 3D). When the tool is represented as a point, the haptic rendering 
problem is known as three-degree-of-freedom (3-DoF) haptic rendering, and when the tool is represented 
as a rigid body, it is known as six-degree-of-freedom (6-DoF) haptic rendering. In case of modeling the 
tool and/or the environment with solid objects, the haptic rendering algorithm also depends on the material 
properties of these objects. Rigid solids (see section 4.4.1) are limited to six DoFs, while deformable 
solids (see section 4.4.2) may present a large number of DoFs. At present, the dynamic simulation of 
a rigid tool is ef.ciently handled in commodity processors, and the challenge may lie on the complexity 
of the environment and the contact con.guration between tool and environment. Ef.cient dynamic simulation 
of complex deformable objects at haptic rates is, however, an issue that deserves further research. 
4.3.2 Collision Detection As noted in Figure 55, in the context of haptic rendering, collision detection 
is the process that, given a con.guration of the tool, detects potentially violated environment constraints. 
Collision detection can easily become the computa­tional bottleneck of a haptic rendering system with 
geometrically complex objects, and its cost often depends on the con.guration space of the contacts. 
Therefore, we devote much attention to the detection of collisions with a point tool for 3-Dof rendering 
and the detection of collisions with a solid tool for 6-DoF rendering. There are also many variations 
of collision detection depending on the geometric representation of the objects, e.g., polygonal representations, 
parametric surfaces, or objects with high-resolution textures. 4.3.3 Collision Response In algorithms 
where the tool s con.guration is computed through a dynamic simulation, collision response takes the 
environment constraints given by the collision detection module as input, and computes forces acting 
on the tool. Therefore, collision response is tightly related to the formulation of environment constraints 
gi(T ) discussed above. The two commonly used approaches for collision response are penalty methods and 
constraint-based methods (e.g., the one by [Ortega et al. 2006], see Fig. 56), which we introduce later 
in Section 4.4.3. In case of dynamic environ­ments, collision response must be computed on the environment 
objects as well.  4.4 Modeling the Tool and the Environment In this section we pay special attention 
to the optimization problem for computing the tool con.guration, by brie.y introducing some examples: 
a 6-DoF tool solved with rigid body dynamic simulation, deformable objects, and formulation of contact 
constraints. 4.4.1 Rigid Body Dynamics We .rst consider a tool modeled as a rigid body in 3D, which yields 
6 DoFs: 3D translation and rotation. One possibility for solving the con.guration of the tool is to consider 
a dynamic model where the tool is in.uenced by the environment through contact constraint forces, and 
by the user through virtual coupling force and torque. We de.ne the generalized coordinates of the tool 
as q, composed of the position of the center of mass x and a quaternion describing the orientation .. 
We de.ne the velocity vector v by the velocity of the center of mass vx and the angular velocity . expressed 
in the world frame. We denote by F the generalized forces acting on the tool (i.e., force Fx and torque 
T, including gravity, centripetal and Coriolis torque, the action of the virtual coupling, and contact 
forces). Given the mass m and the inertia tensor M of the tool, the dynamics are de.ned by the Newton-Euler 
equations: mv.x = Fx, M..= T +(M.) × .. (18) And the relationship between the generalized coordinates 
and the velocity vector is: x.= vx, ..= G.. (19) The matrix G relates the derivative of the quaternion 
to the angular velocity, and its de.nition may be found in e.g. [Shabana 1989]. The same reference will 
serve for an introduction to rigid body dynamics and the derivation of the Newton-Euler equations. For 
compactness, it is useful to write the equations of motion in general form: Mv.= F, q.= Gv. (20) Time 
Discretization with Implicit Integration Here, we consider time discretization schemes that yield a linear 
update of velocities and positions of the form: Mv(i +1) = .tF , q(i +1) = .t (21) Gv(i + 1) + q(i). 
Note that the updated coordinates q(i + 1) need to be projected afterwards onto the space of valid rotations 
(i.e., unit quaternions). One example of linear update is obtained by using Backward Euler discretization 
with .rst-order approximation of derivatives. As pointed out by [Colgate et al. 1995], implicit integration 
of the differential equations describing the virtual environment can ease the design of a stable haptic 
display. This observation has lead to the design of 6-DoF haptic rendering algorithms with implicit integration 
of the rigid body dynamics of the tool [Otaduy and Lin 2005; Otaduy and Gross 2007]. Implicit integration 
also enhances display transparency by enabling stable simulation of the tool with small mass values. 
Linearized Backward Euler discretization takes a general ODE of the form y.= f(y,t) and applies a discretization 
y(i +1) = y(i)+.tf(y(i + 1),t(i + 1)), with a linear approximation of the derivatives f(y(i + 1),t(i 
+ 1)) f(y(i),t)+ .f (y(i + 1) - y(i)) + .f .t. For time-independent derivatives, which is our case, 
this yields an update .y .t .f rule: y(i +1) = y(i)+.t(I - .t.y )-1f(i). Applying the linearized Backward 
Euler discretization to (20), and discarding the derivative of the inertia tensor, the terms of the update 
rule (21) correspond to: .F 2 .F M = M - .t - .tG, .v .q F = F(i) + 1 .t M - .F .v v(i). (22) As can 
be inferred from the equations, implementation of implicit integration requires the formulation of Jacobians 
.F .F of force equations and . These Jacobians include the term for inertial forces (M.) × ., contact 
forces .q .v (see Section 4.4.3), or virtual coupling (see next). [Otaduy and Lin 2006; Otaduy 2004] 
formulate in detail these Jacobians. Six-DoF Virtual Coupling We pay special attention here to modeling 
viscoelastic virtual coupling for 6-DoF haptic rendering. The tool T will undergo a force and torque 
that move it toward the con.guration H of the haptic device, expressed in coordinates of the virtual 
environment. We assume that the tool undergoes no force when the device s con.guration in the reference 
system of the tool corresponds to a position xc and orientation .c. We refer to this con.guration as 
coupling con.guration. Coupling is often engaged at the center of mass of the tool (i.e., xc =0), but 
this is not necessarily true. Coupling at the center of mass has the advantage that coupling force and 
torque are fully decoupled from each other. Given con.gurations (xT ,.T ) and (xH,.H) for the tool and 
the device, linear coupling stiffness kx and damping bx, the coupling force F on the tool can be de.ned 
as: F = kx(xH - xT - RT xc)+ bx(vH - vT - .T × (RT xc)). (23) The de.nition of the coupling torque requires 
the use of an equivalent axis of rotation u [McNeely et al. 1999]. This axis of rotation can be de.ned 
using the scalar and vector parts (..s and ..u respectively) of the quaternion .. = .H · .-1 · .-1 describing 
the relative coupling orientation between tool and device. Then: c T 1 u =2 · acos(..s) ·· ..u . (24) 
sin(acos(..s)) The coupling torque can then be de.ned using rotational stiffness k. and damping b. as: 
T =(Rxc) × F + k.u + b.(.H - .T ). (25) Multibody Simulation Dynamic simulation of the rigid tool itself 
is not a computationally expensive problem, but the problem becomes considerably more complex if the 
tool interacts with multiple rigid bodies. In fact, the fast and robust computation of multibody contact 
is still a subject of research, paying special attention to stacking or friction [Stewart and Trinkle 
2000; Mirtich 2000; Milenkovic and Schmidl 2001; Guendelman et al. 2003; Kaufman et al. 2005]. Moreover, 
with multibody contact and implicit integration it is not possible to write a decoupled update rule (21) 
for each body. The coupling between bodies may appear (a) in the Jacobians of contact forces when using 
penalty methods, or (b) through additional constraint equations when using Lagrange multipliers (see 
Section 4.4.3). 4.4.2 Dynamics of Deformable Objects There are multiple options for modeling deformable 
objects, and researchers in haptic rendering have often opted for approximate approaches that trade accuracy 
for computational cost. Here we do not aim at covering in depth the modeling of deformable objects, but 
rather highlight through an example the inclusion in the complete rendering algorithm. The works of [M¨c 
and James 2007; Duriez et al. 2006; Garre and Otaduy uller and Gross 2004; Barbi.2009] discuss in more 
detail practical examples of deformable object modeling for haptic rendering, focusing on fast approximate 
models and the handling of contact. The variational formulation of continuum elasticity equations leads 
to elastic forces de.ned as the negative gradient o of elastic energy s ·E dO, where s and E represent 
stress and strain tensors. The various elasticity models differ in O the de.nition of elastic strain 
or the de.nition of the relationship between stress and strain. Given the de.nition of elastic energy, 
one can reach a discrete set of equations following the .nite element method [Zienkiewicz and Taylor 
1989]. Typically, the dynamic motion equations of a deformable body may be written as: M · v.= F - K(q) 
· (q - q0) - D · v, (26) q.= v. (27) where M, D, and K represent, respectively, mass, damping, and stiffness 
matrices. The stiffness matrix captures elastic forces and is, in general, dependent on the current con.guration 
q. At present times, haptic rendering calls for fast methods for modeling elasticity, and a reasonable 
approach is to use the linear Cauchy strain tensor, as well as the linear Hookean relationship between 
stress and strain. Linear strain leads, however, to considerable artifacts under large deformations, 
which can be eliminated by using corotational methods that measure deformations in the unrotated setting 
of each mesh element [M¨uller et al. 2002; M¨uller and Gross 2004]. The use of corotational strain allows 
for stable and robust implicit integration methods while producing a linear update rule for each time 
step. With linearized Backward Euler (described before for rigid bodies), the update rule becomes: Mv(i 
+1) = .tF , (28) q(i +1) = .tv(i + 1) + q(i). (29) The discrete mass matrix M and force vector F become: 
.F .F 2 M = M +.t D - +.tK(q) - , .v .q 1 .F F = F(i)+ M + D - v(i). (30) .t.v References such as [M¨ 
uller and Gross 2004; Duriez et al. 2006; Garre and Otaduy 2009] offer a more detailed dis­cussion on 
the ef.cient simulation of linear elastic models with corotational methods, as well as the implementation 
of virtual coupling with a deformable tool. 4.4.3 Contact Constraints Contact constraints model the 
environment as algebraic equations in the con.guration space of the tool, gi(T ) = 0. A con.guration 
of the tool T0 such that gi(T0)=0 indicates that the tool is exactly in contact with the environment. 
Collision response exerts forces on the tool such that environment constraints are not violated. We will 
look at two speci.c ways of modeling environment contact constraints, penalty-based methods and Lagrange 
multipliers, and we will focus as an example on their application to a rigid tool.  Penalty Method In 
general terms, the penalty method models contact constraints as springs whose elastic energy increases 
with object interpenetration. Penalty forces are computed as the negative gradient of the elastic energy, 
which produces collision response that moves objects toward a non-penetrating con.guration. For simplicity, 
we will consider here linearized point-on-plane contacts. Given a colliding point p of the tool, a general 
contact constraint has the form gi(p) = 0, and after linearization nT (p - p0) = 0, where n = \gi is 
the normal of the constraint (i.e., the normal of the contact plane), and p0 is the contact point on 
the environment. With such a linearized constraint, penetration depth can easily be de.ned as d = -nT 
(p - p0). Penalty energies can be de.ned in multiple ways, but the simplest is to consider a Hookean 
spring, which yields an 1 energy E = kd2, where k is the contact stiffness. Then, the contact penalty 
force becomes F = -\E = -kd\d. 2 It is also common to apply penalty forces when objects become closer 
than a certain tolerance d, which can be easily handled by rede.ning the penetration depth as d = d - 
nT (p - p0). The addition of a tolerance has two major advantages: the possibility of using penalty methods 
in applications that do not allow object interpenetration, and a reduction of the cost of collision detection. 
With the addition of a tolerance, object interpenetration occurs less frequently, and computation of 
penetration depth is notably more costly than computation of separation distance. With a rigid tool, 
the contact point p can be expressed in terms of the rigid body state as p = x + Rr, where x and R are, 
respectively, the position and orientation of the tool, and r is the position of the contact point in 
the tool s reference frame. Then, for the case of a rigid tool, and adding a damping term b, the penalty 
force and torque are: F = -kN(x + Rr - p0) - bN(v + . × (Rr)), T =(Rr) × F. (31) N = nnT is a matrix 
that projects a vector onto the normal of the constraint plane. Penalty-based methods offer several attractive 
properties: the force model is local to each contact and computa­tionally simple, object interpenetration 
is inherently allowed, and the cost of the numerical integration is almost insensitive to the complexity 
of the contact con.guration. This last property makes penalty-based methods well suited for interactive 
applications with .xed time steps, such as haptic rendering. In fact, penalty-based methods have been 
applied in many 6-DoF haptic rendering approaches [McNeely et al. 1999; Kim et al. 2003; Johnson and 
Willemsen 2003; McNeely et al. 2006; Otaduy and Lin 2006; Barbi.c and James 2007]. However, penalty-based 
methods also have some disadvantages. For example, there is no direct control over physical parameters, 
such as the coef.cient of restitution, and friction forces are dif.cult to model, as they require tracking 
contact points and using local methods [Karnopp 1985; Hayward and Armstrong 2000]. But, most importantly, 
geometric discontinuities in the location of contact points and/or normals leads to torque discontinuities, 
as depicted schematically in Figure 59. Different authors have proposed various de.nitions for contact 
points and normals, with various advantages and drawbacks. [McNeely et al. 1999; McNeely et al. 2006; 
Barbi.c and James 2007] sample the objects with a discrete set of points, and de.ne contact points as 
the penetrating subset. [Johnson and Willemsen 2003; Otaduy and Lin 2006], on the other hand, employ 
continuous surface de.nitions, and de.ne contact points as local extrema of the distance function between 
colliding surfaces. Using a .xed discrete set of points allows for increased force continuity, while 
using continuous surface de.nitions allows for the detection of all interpenetrations. With the strict 
de.nition of penalty energy given above, penalty force normals are de.ned as the gradient of penetration 
depth, which is discontinuous on the medial axis of the objects. [McNeely et al. 1999; McNeely et al. 
2006; Barbi. c and James 2007] avoid this problem by de.ning as contact normal the surface normal at 
each penetrating point. This alternative de.nition is continuous in time, but does not guarantee that 
contact forces reduce interpenetration. With penalty-based methods, non-penetration constraints are 
enforced by means of very high contact stiffness, which could yield instability problems if numerical 
integration is executed using fast explicit methods. The use of implicit integration of the tool, as 
described in Section 4.4.1, enhances stability in the presence of high contact stiffness [Wu 2000; Larsen 
2001; Otaduy and Lin 2006; Barbi. c and James 2007]. However, the dynamic equations of the different 
dynamic bodies (see (21) for rigid bodies or (29) for deformable bodies) become then coupled, and a linear 
system must be solved for each contact group. We refer to [Otaduy and Lin 2006] for further details on 
the Jacobians of penalty force and torque for 6-DoF haptic rendering. Lagrange Multipliers The method 
of Lagrange multipliers allows for an exact enforcement of contact constraints g(T ) = 0 by modeling 
workless constraint forces Fc = JT . normal to the constraints. Here we consider multiple constraints 
grouped in a vector g, and their generalized normals are gathered in a matrix JT = \g. Constraint forces 
are added to regular forces of the dynamic equations of a colliding object (e.g., the tool). Then, constraints 
and dynamics are formulated in a joint differential algebraic system of equations. The amount of constraint 
force . is the unknown of the system, and it is solved such that constraints are enforced. Typically, 
contact constraints are nonlinear, but the solution of constrained dynamics systems can be accelerated 
by linearizing the constraints. Given the state q(i) of the tool at a certain time, constraint linearization 
yields g(i + 1) g(i)+.tJ · v(i + 1). This linearization, together with the discretized state update 
equation yields the following system to be solved per simulation frame: F + JT ., M · v(i +1) = .t 1 
J · v(i + 1) =- g(i). (32) .t The addition of constraints for non-sticking forces . = 0,.T g(q)=0 yields 
a linear complementarity problem (LCP) [Cottle et al. 1992], which combines linear equalities and inequalities. 
The problem in equation (32) is a mixed LCP, and can be transformed into a strict LCP through algebraic 
manipulation: M-1 J 1 g(i) - .tJ (33) M-1JT . =- F. .tThe LCP can be solved through various techniques 
[Cottle et al. 1992], and once the Lagrange multipliers . are known, it is possible to update the state 
of the tool. There are other variants of the problem, for example by allowing sticking forces through 
equality constraints, or differentiating the constraints and expressing them on velocities or accelerations. 
Several of these variants of contact constraints with Lagrange multipliers have been employed in practical 
solutions to haptic rendering. The god-object method of [Zilles and Salisbury 1995] can be considered 
as the .rst application of Lagrange multipliers for contact in 3-DoF haptic rendering. The work of [Ortega 
et al. 2006] describes an extension of the god-object method to 6-DoF rendering, and [Duriez et al. 2006] 
formulates in detail frictional contact for haptic rendering of deformable objects. [Garre and Otaduy 
2009] propose haptic rendering between deformable objects using an intermediate representation and the 
constrained dynamics solver from [Otaduy et al. 2009]. Constraint-based methods with Lagrange multipliers 
handle all concurrent contacts in a single computational prob­lem and attempt to .nd contact forces that 
produce physically and geometrically valid motions. As opposed to penalty-based methods, they solve one 
global problem, which allows, for example, for relatively easy inclusion of accurate friction models. 
However, constraint-based are computationally expensive, even for the linearized system (32), and the 
solution of constrained dynamics and the de.nition of constraints (i.e., collision detection) are highly 
intertwined. The full problem of constrained dynamics is highly nonlinear, but there are various time­stepping 
approaches that separate a collision-free dynamics update, collision detection, and collision response, 
for solving locally linear problems [Bridson et al. 2002; Cirak and West 2005]. Fast enforcement of constrained 
motion is, however, still a topic of research in haptic rendering, in particular for rendering deformable 
objects.  4.5 Multirate Algorithm As discussed in Section 4.2.6, rendering algorithms based on virtual 
coupling [Colgate et al. 1995] can serve to easily design stable rendering. However, the complexity of 
tool and environment simulation may require low update rates, which turn into low admissible coupling 
stiffness, and hence low-quality rendering. Independently of the simulation and collision detection methods 
employed, and the mechanical characteristics of the tool or the environment, a common solution for enhancing 
the quality and transparency of haptic rendering is to devise a multirate algorithm (See [Barbagli et 
al. 2003] for stability analysis of multirate algorithms). A slow process computes accurate interaction 
between the tool and the environment, and updates an approximate but simple intermediate representation 
[Adachi et al. 1995]. Then, a fast process synthesizes the forces to be sent to the device, using the 
intermediate representation. There have been two main approaches for designing intermediate representations, 
which we discuss next. 4.5.1 Geometric Vs. Algebraic Intermediate Representations One approach is to 
design a local and/or coarse geometric representation of the tool and/or the environment. A slow thread 
performs a computation of the interaction between the full representations of the tool and the environment, 
and updates the local coarse representation. In parallel, a fast thread computes the interaction between 
tool and environment using the simpli.ed representations. The fast computation involves identifying simpli.ed 
contact con­straints, through collision detection, and computing the rendering forces. Note that this 
approach can be used in the context of both virtual coupling algorithms or direct rendering. The earliest 
example of multirate rendering by [Adachi et al. 1995] computes collision detection between a point tool 
and the environment in the slow thread, approximates the environment as a plane, and then uses the plane 
repre­sentation in the fast thread. A similar approach was followed by [Mark et al. 1996b], with addition 
of plane .ltering between local model updates. Others used meshes of different resolutions coupled through 
Norton equivalents [Ast­ley and Hayward 1998], or local linearized submeshes for approximating high-frequency 
behavior [C¸avus¸o.glu and Tendick 2000]. Recently, [Johnson et al. 2005] have suggested the use of local 
collision detection algorithms for updating the contact constraints in the fast loop. The second approach 
is to design a simpli.ed representation of the collision response model between the tool and the environment. 
The slow thread performs full computation of collision detection and response between tool and environment, 
and updates a simpli.ed version of the collision response model. This model is then used in the fast 
thread for computing collision response for rendering forces to the user. The main difference with the 
geometric approach is that the fast thread does not recompute collision detection for de.ning contact 
constraints. Early approaches to multirate simulation of deformable models considered force extrapolation 
for de.ning the local algebraic model [Picinbono et al. 2000]. There are other recent approaches that 
identify contact constraints in the slow thread, and then use those constraints for force computation 
in the fast thread, for rigid bodies [Ortega et al. 2006], or for deformable bodies [Duriez et al. 2004]. 
Apart from those, we should mention the use of contact constraints for computing a least-squares solution 
to Poisson s restitution hypothesis for rigid bodies [Constantinescu et al. 2005]. Last, the rest of 
this section describes two examples that compute in the slow thread a linear model of the contact response 
between the tool and the environment, and then simply evaluate this linear model in the fast thread, 
for penalty-based methods [Otaduy and Lin 2005; Otaduy and Lin 2006] or for constraint-based methods 
with Lagrange multipliers [Otaduy and Gross 2007]. 4.5.2 Example 1: Multirate Rendering with Penalty 
Methods Figure 60 shows the structure of the rendering algorithm suggested by [Otaduy and Lin 2005; Otaduy 
and Lin 2006]. The visual thread computes collision detection between the tool and the environment, as 
well as collision response using the penalty-based method (See section 4.4.3). Moreover, the equations 
of collision response are linearized, and the linear model is fed to the haptic thread. The haptic thread 
runs at fast haptic update rates, solving for the con.guration of the tool subject to forces computed 
using the linearized contact model. Figure 61 shows one application scenario of the multirate rendering 
algorithm. [Otaduy and Lin 2005; Otaduy and Lin 2006] applied the linearization of penalty-based forces 
to 6-DoF haptic rendering with a rigid tool. Recall equation (31), which describes penalty forces for 
a rigid tool. Assuming that the visual thread computes collision detection for a con.guration (q0, v0) 
of the tool, a penalty-based contact model can be linearized in general as: .Fc .Fc Fc(q, v) Fc(q0, 
v0)+ (q - q0)+ (v - v0). (34) .q .v For more details on the linear approximation for a rigid tool, we 
refer to [Otaduy and Lin 2005; Otaduy and Lin 2006]. An interesting observation of the linearized penalty-based 
method is that it imposes no additional cost if the rendering algorithm computes dynamics of the tool 
with implicit integration. As shown in equation (22), the de.nition of discrete-time inertia requires 
the same Jacobians .F and .F as the linearized contact model. We would like to point .q .v  out that 
these Jacobians are also used in quasi-static methods for solving the con.guration of the tool [Wan and 
McNeely 2003; Barbi. c and James 2007]. 4.5.3 Example 2: Multirate Rendering with Constraints Figure 
62 shows the overall structure of the multirate rendering algorithm presented by [Otaduy and Gross 2007] 
for 6-DoF haptic rendering between a rigid tool and a deformable environment. This algorithm creates 
two instances of the rigid tool manipulated by the user. The visual thread, typically running at a low 
update rate (as low as tens of Hz), performs a full simulation of the visual tool coupled to the haptic 
device and interacting with a deformable environment. The haptic thread, running at a fast update rate 
of typically 1 kHz, performs the simulation of the haptic tool and computes force values to be rendered 
by the haptic device. Collision detection and full constraint­based collision response are only computed 
in the visual thread. At the same time, the parameters of a linear contact model are updated, and fed 
to the haptic thread. This linear model can be evaluated with a .xed, low number of operations, and ensures 
extremely fast update of contact forces in the haptic thread. For penalty-based collision response, [Otaduy 
and Lin 2005] proposed a linearized contact model in the state space of the tool. However, for constraint-based 
collision response, [Otaduy and Gross 2007] proposed a model of contact Jacobian, linearly relating contact 
forces Fc and the rest of the forces F acting on the tool. The linearized model takes the form: .Fc Fc(F 
) Fc(F 0)+ (F - F 0). (35) .F All that needs to be done in the visual thread is to compute the contact 
Jacobian .Fc . .F The LCP formulation (33) for collision response can be compactly rewritten as A.. 
= b.. The resolution of the LCP yields a set of inactive contacts, for which the contact force is zero, 
and a set of active contacts for which the A-1 constraints hold exactly A.,a.a = ba . .a = .,aba. The 
contact force can then be written in terms only of active contacts as Fc = JT A-1 ba. Then, the contact 
Jacobian can be easily formulated as: a .,a .Fc .Fc .ba = · = -.tJT A-1 JaM-1 . (36) a .F .ba .F .,a 
This formulation involves several approximations, such as ignoring the change of active constraints between 
time steps or changes of inertia. Note also that equation (36) should be slightly modi.ed to account 
for a moving or deforming environment, as the state of the tool and the environment are not explicitly 
separated. Multirate algorithms enable programming very high rendering stiffness, under the assumption 
that the contact space changes slowly. This is in fact the case in many situations, especially during 
sliding contact between tool and environment. The approach of multirate rendering using force linearization 
has recently been extended to the contact between a deformable tool and other deformable objects in the 
environment [Garre and Otaduy 2009] (See Fig. 63). The key of this method is to identify a rigid handle 
in the deformable tool, which serves as the space for linearization. With a multirate algorithm, the 
visual thread is simulated using accurate constraint-based contact solvers.  References ADACHI, Y., 
KUMANO, T., AND OGINO, K. 1995. Intermediate representation for stiff virtual objects. Virtual Reality 
Annual International Symposium, 203 210. ADAMS, R. J., AND HANNAFORD, B. 1998. A two-port framework for 
the design of unconditionally stable haptic interfaces. Proc. of IEEE/RSJ International Conference on 
Intelligent Robots and Systems. ANDRIOT, C. 2002. Advances in virtual prototyping. Clefs CEA Vol. 47, 
Research and Simulation. ASTLEY, O. R., AND HAYWARD, V. 1998. Multirate haptic simulation achieved by 
coupling .nite element meshes through norton equivalents. Proc. of IEEE International Conference on Robotics 
and Automation. BARAFF, D. 1992. Dynamic simulation of non-penetrating rigid body simulation. PhD thesis, 
Cornell University. BARBAGLI, F., PRATTICHIZZO, D., AND SALISBURY, K. 2003. Dynamic local models for 
stable multi-contact haptic interaction with deformable objects. Proc. of Haptics Symposium. BARBI C., 
J., AND JAMES, D. L. 2007. Time-critical distributed contact for 6-DoF haptic rendering of adaptively 
sampled reduced deformable models. Proc. of ACM SIGGRAPH/Eurographics Symposium on Computer Anima­tion. 
BEJCZY, A., AND SALISBURY, J. K. 1980. Kinematic coupling between operator and remote manipulator. Ad­vances 
in Computer Technology Vol. 1, 197 211. BOTT, J., CROWLEY, J., AND LAVIOLA, J. 2009. Exploring 3d gestural 
interfaces for music creation in video games. In Proceedings of The Fourth International Conference on 
the Foundations of Digital Games 2009, 18 25. BOWMAN, D. A., AND HODGES, L. F. 1997. An evaluation of 
techniques for grabbing and manipulating remote objects in immersive virtual environments. In SI3D 97: 
Proceedings of the 1997 symposium on Interactive 3D graphics, ACM, New York, NY, USA, 35 ff. BOWMAN, 
D. A., KOLLER, D., AND HODGES, L. F. 1997. Travel in immersive virtual environments: An evaluation of 
viewpoint motion control techniques. In Proceedings of the Virtual Reality Annual International Symposium, 
45 52. BOWMAN, D. A., WINEMAN, J., HODGES, L. F., AND ALLISON, D. 1998. Designing animal habitats within 
an immersive ve. IEEE Comput. Graph. Appl. 18, 5, 9 13. BOWMAN, D. A., KRUIJFF, E., LAVIOLA, J. J., AND 
POUPYREV, I. 2004. 3D User Interfaces: Theory and Practice. Addison Wesley Longman Publishing Co., Inc., 
Redwood City, CA, USA. BRIDSON, R., FEDKIW, R., AND ANDERSON, J. 2002. Robust treatment of collisions, 
contact and friction for cloth animation. In Proc. of ACM SIGGRAPH. BROOKS, JR., F. P., OUH-YOUNG, M., 
BATTER, J. J., AND KILPATRICK, P. J. 1990. Project GROPE Haptic displays for scienti.c visualization. 
In Computer Graphics (SIGGRAPH 90 Proceedings), F. Baskett, Ed., vol. 24, 177 185. BURDEA, G. C. 1996. 
Force and Touch Feedback for Virtual Reality. John Wiley and Sons, Inc. C¸ AVUS¸OGLU., M. C., AND TENDICK, 
F. 2000. Multirate simulation for high .delity haptic interaction with deformable objects in virtual 
environments. Proc. of IEEE International Conference on Robotics and Automation, 2458 2465. CHARBONNEAU, 
E., MILLER, A., WINGRAVE, C., AND LAVIOLA, J. 2009. Understanding visual interfaces for the next generation 
of dance-based rhythm video games. In To Appear in Sandbox 2009: ACM SIGGRAPH Video Game Proceedings. 
CHEN, E. 1999. Six degree-of-freedom haptic system for desktop virtual prototyping applications. In Proceedings 
of the First International Workshop on Virtual Reality and Prototyping, 97 106. CIRAK, F., AND WEST, 
M. 2005. Decomposition contact response (DCR) for explicit .nite element dynamics. International Journal 
for Numerical Methods in Engineering 64, 8. COLGATE, J. E., AND SCHENKEL, G. G. 1994. Passivity of a 
class of sampled-data systems: Application to haptic interfaces. Proc. of American Control Conference. 
COLGATE, J. E., GRAFING, P. E., STANLEY, M. C., AND SCHENKEL, G. 1993. Implementation of stiff virtual 
walls in force-re.ecting interfaces. Virtual Reality Annual International Symposium, 202 207. COLGATE, 
J. E., STANLEY, M. C., AND BROWN, J. M. 1995. Issues in the haptic display of tool use. Proc. of IEEE/RSJ 
International Conference on Intelligent Robots and Systems, pp. 140 145. CONNER, B. D., SNIBBE, S. S., 
HERNDON, K. P., ROBBINS, D. C., ZELEZNIK, R. C., AND VAN DAM, A. 1992. Three-dimensional widgets. In 
SI3D 92: Proceedings of the 1992 symposium on Interactive 3D graphics, ACM, New York, NY, USA, 183 188. 
CONSTANTINESCU, D., SALCUDEAN, S. E., AND CROFT, E. A. 2005. Local model of interaction for realistic 
manipulation of rigid virtual worlds. International Journal of Robotics Research 24, 10. COTTLE, R., 
PANG, J., AND STONE, R. 1992. The Linear Complementarity Problem. Academic Press. DAVIS, J., AGRAWALA, 
M., CHUANG, E., POPOVIC, Z., AND SALESIN, D. 2003. A sketching interface for articulated .gure animation. 
Proc. of ACM SIGGRAPH/Eurographics Symposium on Computer Animation. DURIEZ, C., ANDRIOT, C., AND KHEDDAR, 
A. 2004. A multi-threaded approach for deformable/rigid contacts with haptic feedback. Proc. of Haptics 
Symposium. DURIEZ, C., DUBOIS, F., KHEDDAR, A., AND ANDRIOT, C. 2006. Realistic haptic rendering of interacting 
deformable objects in virtual environments. Proc. of IEEE TVCG 12, 1. EDMOND, C., HESKAMP, D., SLUIS, 
D., STREDNEY, D., WIET, G., YAGEL, R., WEGHORST, S., OPPEN-HEIMER, P., MILLER, J., LEVIN, M., AND ROSENBERG, 
L. 1997. Ent endoscopic surgical simulator. Proc. of Medicine Meets VR, 518 528. EHMANN, S., AND LIN, 
M. C. 2001. Accurate and fast proximity queries between polyhedra using convex surface decomposition. 
Computer Graphics Forum (Proc. of Eurographics 2001) 20, 3, 500 510. FEINER, S., MACINTYRE, B., HAUPT, 
M., AND SOLOMON, E. 1993. Windows on the world: 2d windows for 3d augmented reality. In UIST 93: Proceedings 
of the 6th annual ACM symposium on User interface software and technology, ACM, New York, NY, USA, 145 
155. FISHER, B., FELS, S., MACLEAN, K., MUNZNER, T., AND RENSINK, R. 2004. Seeing, hearing and touching: 
Putting it all together. In ACM SIGGRAPH course notes. GARRE, C., AND OTADUY, M. A. 2009. Haptic rendering 
of complex deformations through handle-space force linearization. Proc. of World Haptics Conference. 
GOERTZ, R., AND THOMPSON, R. 1954. Electronically controlled manipulator. Nucleonics, 46 47. GREGORY, 
A., MASCARENHAS, A., EHMANN, S., LIN, M. C., AND MANOCHA, D. 2000. 6-DoF haptic display of polygonal 
models. Proc. of IEEE Visualization Conference. GUENDELMAN, E., BRIDSON, R., AND FEDKIW, R. 2003. Nonconvex 
rigid bodies with stacking. ACM Trans. on Graphics (Proc. of ACM SIGGRAPH) 22, 871 878. HANNAFORD, B., 
RYU, J.-H., AND KIM, Y. S. 2002. Stable control of haptics. In Touch in Virtual Environments, M. L. McLaughlin, 
J. P. Hespanha, and G. S. Sukhatme, Eds. Prentice Hall PTR, Upper Saddle River, NJ, ch. 3, 47 70. HARDERS, 
M., AND SZEKELY, G. 2003. Enhancing human computer interaction in medical segmentation. Pro­ceedings 
of the IEEE, Special Issue on Multimodal Human Computer Interfaces 91, 9, 1430 1442. HAYWARD, V., AND 
ARMSTRONG, B. 2000. A new computational model of friction applied to haptic rendering. Experimental Robotics 
VI. HAYWARD, V., GREGORIO, P., ASTLEY, O., GREENISH, S., AND DOYON, M. 1998. Freedom-7: A high .delity 
seven axis haptic device with applications to surgical training. Experimental Robotics, 445 456. Lecture 
Notes in Control and Information Sciences 232. HILL, J. W., AND SALISBURY, J. K. 1977. Two measures of 
performance in a peg-in-hole manipulation task with force feedback. Thirteenth Annual Conference on Manual 
Control, MIT. HOGAN, N. 1985. Impedance control: An approach to manipulation, part i -theory, part ii 
-implementation, part iii -applications. Journal of Dynamic Systems, Measurement and Control 107, 1 24. 
HOGAN, N. 1986. Multivariable mechanics of the neuromuscular system. IEEE Annual Conference of the Engi­neering 
in Medicine and Biology Society, 594 598. IGARASHI, T., AND HUGHES, J. F. 2002. Clothing manipulation. 
Proc. of UIST. IGARASHI, T., AND HUGUES, J. F. 2001. A suggestive interface for 3D drawing. Proc. of 
UIST. IGARASHI, T., MATSUOKA, S., KAWACHIYA, S., AND TANAKA, H. 1997. Interactive Beauti.cation: A technique 
for rapid geometric design. Proc. of UIST, 105 114. IGARASHI, T., MATSUOKA, S., AND TANAKA, H. 1999. 
Teddy: A sketching interface for 3D freeform design. Proc. of ACM SIGGRAPH. IGARASHI, T., MOSCOVICH, 
T., AND HUGHES, J. F. 2005. As-rigid-as-possible shape manipulation. Proc. of ACM SIGGRAPH. IGARASHI, 
T., MOSCOVICH, T., AND HUGHES, J. F. 2005. Spatial keyframing for performance-driven animation. Proc. 
of ACM SIGGRAPH/Eurographics Symposium on Computer Animation. IJIRI, T., OKABE, M., OWADA, S., AND IGARASHI, 
T. 2005. Floral diagrams and in.orescences: Interactive .ower modeling using botanical structural constraints. 
Proceedings of ACM SIGGRAPH. INSKO, B., MEEHAN, M., WHITTON, M., AND BROOKS, F. 2001. Passive haptics 
signi.cantly enhances virtual environments. Tech. Rep. 01-010, Department of Computer Science, UNC Chapel 
Hill. INSKO, B. 2001. Passive Haptics Signi.cantly Enhance Virtual Environments. PhD thesis, University 
of North Carolina. Department of Computer Science. JOHNSON, D. E., AND COHEN, E. 2001. Spatialized normal 
cone hierarchies. Proc. of ACM Symposium on Interactive 3D Graphics, pp. 129 134. JOHNSON, D. E., AND 
WILLEMSEN, P. 2003. Six degree of freedom haptic rendering of complex polygonal models. In Proc. of Haptics 
Symposium. JOHNSON, D. E., AND WILLEMSEN, P. 2004. Accelerated haptic rendering of polygonal models through 
local descent. Proc. of Haptics Symposium. JOHNSON, D. E., WILLEMSEN, P., AND COHEN, E. 2005. 6-dof haptic 
rendering using spatialized normal cone search. IEEE Transactions on Visualization and Computer Graphics 
11, 6, 661 670. KARNOPP, D. 1985. Computer simulation of stick slip friction in mechanical dynamic systems. 
Trans. ASME, Journal of Dynamic Systems, Measurement, and Control. KATZOURIN, M., IGNATOFF, D., QUIRK, 
L., LAVIOLA, J., AND JENKINS, O. C. 2006. Swordplay: Innovating game development through vr. IEEE Computer 
Graphics and Applications 26, 6, 15 19. KAUFMAN, D. M., EDMUNDS, T., AND PAI, D. K. 2005. Fast frictional 
dynamics for rigid bodies. Proc. of ACM SIGGRAPH. KHO, Y., AND GARLAND, M. 2005. Sketching mesh deformations. 
Proceedings of the ACM Symposium on Interactive 3D Graphics, 147 154. KIM, W., AND BEJCZY, A. 1991. Graphical 
displays for operator aid in telemanipulation. IEEE International Conference on Systems, Man and Cybernetics. 
KIM, Y. J., LIN, M. C., AND MANOCHA, D. 2002. DEEP: an incremental algorithm for penetration depth computation 
between convex polytopes. Proc. of IEEE Conference on Robotics and Automation, 921 926. KIM, Y. J., OTADUY, 
M. A., LIN, M. C., AND MANOCHA, D. 2003. Six-degree-of-freedom haptic rendering using incremental and 
localized computations. Presence 12, 3, 277 295. KLATZKY, R. L., AND LEDERMAN, S. J. 2003. Touch. In 
Experimental Psychology, 147 176. Volume 4 in I.B. Weiner (Editor-in-Chief). Handbook of Psychology. 
LARSEN, E. 2001. A robot soccer simulator: A case study for rigid body contact. Game Developers Conference. 
LAVIOLA JR., J. J. 2008. Bringing vr and spatial 3d interaction to the masses through video games. IEEE 
Comput. Graph. Appl. 28, 5, 10 15. LAVIOLA, J. 2000. Msvt: A virtual reality-based multimodal scienti.c 
visualization tool. In Proceedings of the Third IASTED International Conference on Computer Graphics 
and Imaging, 1 7. LAWRENCE, D. A., LEE, C. D., PAO, L. Y., AND NOVOSELOV, R. 2000. Shock and vortex visualization 
using a combined visual/haptic interface. Proc. IEEE Visualization, 131 137. LEE, J. 2008. Hacking the 
nintendo wii remote. Pervasive Computing, IEEE 7, 3 (July-Sept.), 39 45. LIN, M. C., AND OTADUY, M. A. 
2008. Haptic Rendering: Foundations, Algorithms &#38; Applications. AK Peters. LUK, J., PASQUERO, J., 
LITTLE, S., MACLEAN, K. E., HAYWARD, V., AND LEVESQUE, V. 2006. Haptics as a solution for mobile interaction 
challenges: Initial design using a handheld tactile display prototype. Proceedings of ACM Conference 
on Human Factors in Computing Systems, CHI. MAPES, D., AND MOSHELL, M. 1995. A two-handed interface for 
object manipulation in virtual environments. Presence: Teleoper. Virtual Environ. 4, 4, 403 416. MARK, 
W., RANDOLPH, S., FINCH, M., VAN VERTH, J., AND TAYLOR II, R. M. 1996. Adding force feedback to graphics 
systems: Issues and solutions. In SIGGRAPH 96 Conference Proceedings, 447 452. MARK, W., RANDOLPH, S., 
FINCH, M., VAN VERTH, J., AND TAYLOR II, R. M. 1996. Adding force feedback to graphics systems: Issues 
and solutions. In SIGGRAPH 96 Conference Proceedings, H. Rushmeier, Ed., Annual Conference Series, 447 
452. MASSIE, T. M., AND SALISBURY, J. K. 1994. The phantom haptic interface: A device for probing virtual 
objects. Proc. of ASME Haptic Interfaces for Virtual Environment and Teleoperator Systems 1, 295 301. 
MCLAUGHLIN, M., HESPANHA, J. P., AND SUKHATME, G. S. 2002. Touch in Virtual Environments. Prentice Hall. 
MCNEELY, W., PUTERBAUGH, K., AND TROY, J. 1999. Six degree-of-freedom haptic rendering using voxel sampling. 
Proc. of ACM SIGGRAPH, 401 408. MCNEELY, W., PUTERBAUGH, K., AND TROY, J. 2006. Voxel-based 6-dof haptic 
rendering improvements. Haptics-e 3, 7. MILENKOVIC, V. J., AND SCHMIDL, H. 2001. Optimization-based animation. 
SIGGRAPH 01 Conference Pro­ceedings, 37 46. MILLER, B. E., COLGATE, J. E., AND FREEMAN, R. A. 1999. Guaranteed 
stability of haptic systems with nonlinear virtual environments. IEEE Transactions on Robotics and Automation 
16, 6, 712 719. MINE, M. R., BROOKS, JR., F. P., AND SEQUIN, C. H. 1997. Moving objects in space: exploiting 
proprioception in virtual-environment interaction. In SIGGRAPH 97: Proceedings of the 24th annual conference 
on Computer graphics and interactive techniques, ACM Press/Addison-Wesley Publishing Co., New York, NY, 
USA, 19 26. MINE, M. 1995. Virtual environment interaction techniques. Tech. rep., UNC Chapel Hill CS 
Dept. MINSKY, M., OUH-YOUNG, M., STEELE, O., BROOKS, JR., F.P., AND BEHENSKY,M.1990.Feelingandseeing: 
Issues in force display. In Computer Graphics (1990 Symposium on Interactive 3D Graphics), R. Riesenfeld 
and C. Sequin, Eds., vol. 24, 235 243. MIRTICH, B. V. 1996. Impulse-based Dynamic Simulation of Rigid 
Body Systems. PhD thesis, University of California, Berkeley. MIRTICH, B. 2000. Timewarp rigid body simulation. 
SIGGRAPH 00 Conference Proceedings, 193 200. MORI, Y., AND IGARASHI, T. 2007. Plushie: An interactive 
design system for plush toys. Proc. of ACM SIG- GRAPH. M¨ ULLER, M., AND GROSS, M. 2004. Interactive 
virtual materials. Proc. of Graphics Interface. M¨ ULLER, M., DORSEY, J., MCMILLAN, L., JAGNOW, R., AND 
CUTLER, B. 2002. Stable real-time deformations. Proc. of ACM SIGGRAPH Symposium on Computer Animation. 
NEALEN, A., SORKINE, O., ALEXA, M., AND COHEN-OR, D. 2005. A sketch-based interface for detail­preserving 
mesh editing. Proc. of ACM SIGGRAPH. NEALEN, A., IGARASHI, T., SORKINE, O., AND ALEXA, M. 2007. FiberMesh: 
Designing freeform surfaces with 3D curves. Proc. of ACM SIGGRAPH. NELSON, D. D., JOHNSON, D. E., AND 
COHEN, E. 1999. Haptic rendering of surface-to-surface sculpted model interaction. Proc. of ASME Dynamic 
Systems and Control Division. OKABE, M., OWADA, S., AND IGARASHI, T. 2005. Interactive design of botanical 
trees using freehand sketches and example-based editing. Proceedings of Eurographics. ORTEGA, M., REDON, 
S., AND COQUILLART, S. 2006. A six degree-of-freedom god-object method for haptic display of rigid bodies. 
Proc. of IEEE Virtual Reality Conference. OTADUY, M. A., AND GROSS, M. 2007. Transparent rendering of 
tool contact with compliant environments. Proc. of World Haptics Conference. OTADUY, M. A., AND LIN, 
M. C. 2005. Stable and responsive six-degree-of-freedom haptic manipulation using implicit integration. 
Proc. of World Haptics Conference, 247 256. OTADUY, M. A., AND LIN, M. C. 2006. A modular haptic rendering 
algorithm for stable and transparent 6-DOF manipulation. IEEE Transactions on Robotics 22, 4, 751 762. 
OTADUY, M. A., TAMSTORF, R., STEINEMANN, D., AND GROSS, M. 2009. Implicit contact handling for deformable 
objects. Proc. of Eurographics. OTADUY, M. A. 2004. 6-DoF Haptic Rendering Using Contact Levels of Detail 
and Haptic Textures. PhD thesis, Department of Computer Science, University of North Carolina at Chapel 
Hill. OUH-YOUNG, M. 1990. Force Display in Molecular Docking. PhD thesis, University of North Carolina, 
Computer Science Department. OWADA, S., NIELSEN, F., NAKAZAWA, K., AND IGARASHI, T. 2003. A sketching 
interface for modeling the internal structures of 3D shapes. Smart Graphics. PAUSCH, R., BURNETTE, T., 
BROCKWAY, D., AND WEIBLEN, M. E. 1995. Navigation and locomotion in virtual worlds via .ight into hand-held 
miniatures. In SIGGRAPH 95: Proceedings of the 22nd annual conference on Computer graphics and interactive 
techniques, ACM, New York, NY, USA, 399 400. PICINBONO, G., LOMBARDO, J.-C., DELINGETTE, H., AND AYACHE, 
N. 2000. Anisotropic elasticity and forces extrapolation to improve realism of surgery simulation. Proc. 
of IEEE International Conference on Robotics and Automation. PIERCE, J. S., FORSBERG, A. S., CONWAY, 
M. J., HONG, S., ZELEZNIK, R. C., AND MINE, M. R. 1997. Image plane interaction techniques in 3d immersive 
environments. In SI3D 97: Proceedings of the 1997 symposium on Interactive 3D graphics, ACM, New York, 
NY, USA, 39 ff. POUPYREV, I., BILLINGHURST, M., WEGHORST, S., AND ICHIKAWA, T. 1996. The go-go interaction 
technique: non-linear mapping for direct manipulation in vr. In UIST 96: Proceedings of the 9th annual 
ACM symposium on User interface software and technology, ACM, New York, NY, USA, 79 80. REKIMOTO, J. 
2002. SmartSkin: An infrastructure for freehand manipulations on interactive surfaces. Proceedings of 
CHI, 113 120. RUSPINI, D., KOLAROV, K., AND KHATIB, O. 1997. The haptic display of complex graphical 
environments. Proc. of ACM SIGGRAPH, 345 352. SALISBURY, K., BROCK, D., MASSIE, T., SWARUP, N., AND ZILLES, 
C. 1995. Haptic rendering: Programming touch interaction with virtual objects. In 1995 Symposium on Interactive 
3D Graphics, P. Hanrahan and J. Winget, Eds., ACM SIGGRAPH, 123 130. ISBN 0-89791-736-7. SCHMIDT, R., 
WYVILL, B., SOUSA, M. C., AND JORGE, J. A. 2005. ShapeShop: Sketch-based solid modeling with BlobTrees. 
Eurographics Workshop on Sketch-based Interfaces and Modeling. SHABANA, A. A. 1989. Dynamics of Multibody 
Systems. John Wiley and Sons. SHIMOGA, K. 1992. Finger force and touch feedback issues in dextrous manipulation. 
NASA-CIRSSE International Conference on Inetelligent Robotic Systems for Space Exploration. SILVA, M.G., 
AND BOWMAN,D.A.2009.Body-basedinteractionfordesktopgames.In CHI EA 09: Proceedings of the 27th international 
conference extended abstracts on Human factors in computing systems, ACM, New York, NY, USA, 4249 4254. 
SIROUSPOUR, M. R., DIMAIO, S. P., SALCUDEAN, S. E., ABOLMAESUMI, P., AND JONES, C. 2000. Haptic interface 
control -design issues and experiments with a planar device. Proc. of IEEE International Conference on 
Robotics and Automation. SLATER, M., AND USOH, M. 1993. An experimental exploration of presence in virtual 
environments. Tech. Rep. 689, Department of Computer Science, University College London. STEWART, D. 
E., AND TRINKLE, J. C. 2000. An implicit time-stepping scheme for rigid body dynamics with coulomb friction. 
IEEE International Conference on Robotics and Automation, 162 169. STOAKLEY, R., CONWAY, M. J., AND PAUSCH, 
R. 1995. Virtual reality on a wim: interactive worlds in miniature. In CHI 95: Proceedings of the SIGCHI 
conference on Human factors in computing systems, ACM Press/Addison-Wesley Publishing Co., New York, 
NY, USA, 265 272. SUTHERLAND, I. 1965. The ultimate display. Proc. of IFIP, 506 508. THORNE, M., BURKE, 
D., AND PANNE, M. 2004. Motion Doodles: An interface for sketching character motion. Proc. of ACM SIGGRAPH. 
TURQUIN, E., CANI, M.-P., AND HUGHES, J. F. 2004. Sketching garments for virtual characters. Eurographics 
Workshop on Sketch-based Interfaces and Modeling. UNGER, B. J., NICOLAIDIS, A., BERKELMAN, P. J., THOMPSON, 
A., LEDERMAN, S. J., KLATZKY, R. L., AND HOLLIS, R. L. 2002. Virtual peg-in-hole performance using a 
6-dof magnetic levitation haptic device: Comparison with real forces and with visual guidance alone. 
Proc. of Haptics Symposium, 263 270. WAN, M., AND MCNEELY, W. A. 2003. Quasi-static approximation for 
6 degrees-of-freedom haptic rendering. Proc. of IEEE Visualization, 257 262. WLOKA, M. M., AND GREENFIELD, 
E. 1995. The virtual tricorder: a uniform interface for virtual reality. In UIST 95: Proceedings of the 
8th annual ACM symposium on User interface and software technology, ACM, New York, NY, USA, 39 40. WU, 
D. 2000. Penalty methods for contact resolution. Game Developers Conference. ZELEZNIK, R. C., HERNDON, 
K. P., AND HUGHES, J. F. 1996. SKETCH: An interface for sketching 3D scenes. Proc. of ACM SIGGRAPH. ZIENKIEWICZ, 
O. C., AND TAYLOR, R. L. 1989. The Finite Element Method, 4th ed. McGraw-Hill. ZILLES, C., AND SALISBURY, 
K. 1995. A constraint-based god object method for haptics display. In Proc. of IEEE/RSJ Int. Conf. on 
Intelligent Robotics and Systems.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1667254</article_id>
		<sort_key>150</sort_key>
		<display_label>Article No.</display_label>
		<display_no>15</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[Interactive sound rendering]]></title>
		<page_from>1</page_from>
		<page_to>338</page_to>
		<doi_number>10.1145/1667239.1667254</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1667254</url>
		<abstract>
			<par><![CDATA[<p>An overview of algorithmic and software technologies related to interactive sound rendering. The course lectures cover three main topics: physically based techniques to synthesize sounds generated from colliding objects or liquid sounds, efficient computation of sound propagation paths based on reflection or diffraction paths and converting those paths into audible sound, exploiting the computational capabilities of current multi-core commodity processors for real-time sound propagation and sound rendering for gaming and interactive applications. The presentations include audio demonstrations that show the meaning of various processing components in practice.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Audio input/output</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10003247</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Signal processing systems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317.10003371.10003386.10003389</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval->Specialized information retrieval->Multimedia and multimodal retrieval->Speech / audio search</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010870</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Natural language interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1797149</person_id>
				<author_profile_id><![CDATA[81459640432]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dinesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Manocha]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797150</person_id>
				<author_profile_id><![CDATA[81337488042]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Calamia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797151</person_id>
				<author_profile_id><![CDATA[81452602436]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ming]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797152</person_id>
				<author_profile_id><![CDATA[81100618474]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Dinesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Manocha]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797153</person_id>
				<author_profile_id><![CDATA[81100300190]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Lauri]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Savioja]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797154</person_id>
				<author_profile_id><![CDATA[81100520793]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Nicolas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tsingos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[F. Antonacci, M. Foco, A. Sarti, and S. Tubaro, "Fast modeling of acoustic reflections and diffraction in complex environments using visibility diagrams. In <i>Proc. 12th European Signal Processing Conference (EUSIPCO '04)</i>, pp. 1773--1776, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[P. Calamia, B. Markham, and U. P. Svensson, "Diffraction culling for virtual-acoustic simulations," <i>Acta Acustica united with Acustica, Special Issue on Virtual Acoustics</i>, 94(6), pp. 907--920, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1289147</ref_obj_id>
				<ref_obj_pid>1288980</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[P. Calamia and U. P. Svensson, "Fast time-domain edge-diffraction calculations for interactive acoustic simulations," <i>EURASIP Journal on Advances in Signal Processing, Special Issue on Spatial Sound and Virtual Acoustics</i>, Article ID 63560, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1477384</ref_obj_id>
				<ref_obj_pid>1477066</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[A. Chandak, C. Lauterbach, M. Taylor, Z. Ren, and D. Manocha, "ADFrustum: Adaptive frustum tracing for interactive sound propagation," <i>IEEE Trans. on Visualization and Computer Graphics</i>, 14, pp. 1707--1722, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[R. Kouyoumjian and P. Pathak, "A uniform geometrical theory of diffraction for an edge in a perfectly conducting surface. In <i>Proc. IEEE</i>, vol. 62, pp. 1448--1461, 1974.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[T. Lokki, U. P. Svensson, and L. Savioja, "An efficient auralization of edge diffraction," In <i>Proc. Aud. Engr. Soc. 21st Intl. Conf. on Architectural Acoustics and Sound Reinforcement</i>, pp. 166--172, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[D. Schr&#246;der and A. Pohl, "Real-time hybrid simulation method including edge diffraction," In <i>Proc. EAA Symposium on Auralization</i>, Otaniemi, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[U. P. Svensson, R. I. Fred, and J. Vanderkooy, "An analytic secondary-source model of edge diffraction impulse responses," <i>J. Acoust. Soc. Am.</i>, 106(5), pp. 2331--2344, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[U. P. Svensson and P. Calamia, "Edge-diffraction impulse responses near specular-zone and shadow-zone boundaries," <i>Acta Acustica united with Acustica</i>, 92(4), pp. 501--512, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[M. Taylor, A. Chandak, Z. Ren, C. Lauterbach, and D. Manocha, "Fast edge-diffraction for sound propagation in complex virtual environments," In <i>Proc. EAA Symposium on Auralization</i>, Otaniemi, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[R. Torres, U. P. Svensson, and M. Kleiner, "Computation of edge diffraction for more accurate room acoustics auralization," <i>J. Acoust. Soc. Am.</i>, 109(2), pp. 600--610, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383323</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[N. Tsingos, T. Funkhouser, A. Ngan, and I. Carlbom, "Modeling acoustics in virtual environments using the Uniform Theory of Diffraction," In <i>Proc. ACM Computer Graphics (SIGGRAPH '01)</i>, pp. 545--552, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618914</ref_obj_id>
				<ref_obj_pid>616077</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[N. Tsingos, I. Carlbom, G. Elko, T. Funkhouser, and R. Kubli, "Validation of acoustical simulations in the Bell Labs box," <i>IEEE Computer Graphics and Applications</i>, 22(4), pp. 28--37, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>266779</ref_obj_id>
				<ref_obj_pid>266774</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[N. Tsingos and J.-D. Gascuel, "Soundtracks for computer animation: Sound rendering in dynamic environments with occlusions," In <i>Proc. Graphics Interface97</i>, Kelowna, BC, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[N. Tsingos and J.-D. Gascuel, "Fast rendering of sound occlusion and diffraction effects for virtual acoustic environments," In <i>Proc. 104th Aud. Engr. Soc. Conv.</i>, 1998. Preprint no. 4699.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Fouad, H., Ballas, J., and Hahn, J. Perceptually based scheduling algorithms for real-time synthesis of complex sonic environments. In <i>Proceedings of the International Conference on Auditory Display</i> (Palo Alto, CA, Nov. 2--5). ICAD, 1997, 1--5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280818</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Funkhouser, T., Carlbom, I., Elko, G., Pingali, G., Sondhi, M., and West, J. A beam-tracing approach to acoustic modeling for interactive virtual environments. In <i>Proceedings of the 25th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH)</i> (Orlando, FL, July 19--24). ACM Press, New York, 1998, 21--32; doi.acm.org/10.1145/280814.280818.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Krokstad, A., Strom, S., and Sorsdal, S. Calculating the acoustical room response by the use of a ray tracing technique. <i>Journal of Sound and Vibration 8</i>, 1 (July 1968), 118--125.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1313181</ref_obj_id>
				<ref_obj_pid>1313046</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Lauterbach, C., Chandak, A., and Manocha, D. Interactive sound rendering in complex and dynamic scenes using frustum tracing; gamma.cs.unc.edu/SOUND/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Lauterbach, C., Yoon, S.-E., Tuft, D., and Manocha, D. RT-DEFORM: Interactive ray tracing of dynamic scenes using BVHs. In <i>Proceedings of the IEEE Symposium on Interactive Ray Tracing</i> (Salt Lake City). IEEE Press, 2006, 39--46.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>545290</ref_obj_id>
				<ref_obj_pid>545261</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[O'Brien, J., Shen, C., and Gatchalian, C. Synthesizing sounds from rigid-body simulations. In the <i>ACM SIGGRAPH 2002 Symposium on Computer Animation</i> (San Antonio, TX, July 21--22). ACM Press, New York, 2002, 175--181.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1111429</ref_obj_id>
				<ref_obj_pid>1111411</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Raghuvanshi, N. and Lin, M. Interactive sound synthesis for large-scale environments. In <i>Proceedings of the ACM Symposium on Interactive 3D Graphics and Games</i> (Redwood City, CA, Mar. 14--16). ACM Press, New York, 2006, 101--108.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Sek, A. and Moore, B. Frequency discrimination as a function of frequency, measured in several ways. <i>Journal of the Acoustical Society of America 97</i>, 4 (Apr. 1995), 2479--2486.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383322</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[van den Doel, K., Kry, P., and Pai, D. Foleyautomatic: Physically based sound effects for interactive simulation and animation. In <i>Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH)</i> (Los Angeles, Aug. 12--17). ACM Press, New York, 2001, 537--544.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1246774</ref_obj_id>
				<ref_obj_pid>1246770</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[van den Doel, K. and Pai, D. The sounds of physical shapes. <i>Presence 7</i>, 4 (Aug. 1998), 382--395.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1206075</ref_obj_id>
				<ref_obj_pid>1189762</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Wald, I., Boulos, S., and Shirley, P. Ray tracing deformable scenes using dynamic bounding volume hierarchies. <i>ACM Transactions on Graphics 26</i>, 1 (Jan. 2007).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[I. Bork: Report on the 3rd round robin on room acoustical computer simulation -- Part II: Calculations. Acta Acustica united with Acustica &#60;b&#62;91&#60;/b&#62; (2005) 753--763.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[D. Ouis: Scattering by a barrier in a room. Applied Acoustics &#60;b&#62;56&#60;/b&#62; (1999) 1--24.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[R. R. Torres, U. P. Svensson, M. Kleiner: Computation of edge diffraction for more accurate room acoustics auralization. J. Acoust. Soc. Am. &#60;b&#62;109&#60;/b&#62; (2001) 600--610.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[V. Pulkki, T. Lokki, L. Savioja: Implementation and visualization of edge diffraction with image-source method. Proc. 112nd Audio Engineering Society (AES) Convention, Munich, 2002. preprint no. 5603.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[L. Savioja, J. Huopaniemi, T. Lokki, R. V&#228;&#228;n&#228;nen: Creating interactive virtual acoustic environments. J. Audio Eng. Soc &#60;b&#62;47&#60;/b&#62; (1999) 675--705.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[T. Lokki, J. Hiipakka, L. Savioja: A framework for evaluating virtual acoustic environments. Proc. 110th Audio Engineering Society (AES) Convention, Amsterdam, 2001. preprint no. 5317.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[A. L&#248;vstad, U. P. Svensson: Diffracted sound field from an orchestra pit. Acoustical Science and Technology &#60;b&#62;26&#60;/b&#62; (2005) 237--239.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[T. Lokki, U. P. Svensson, L. Savioja: An efficient auralization of edge diffraction. Proc. Audio Engineering Society 21st International Conference on Architectural Acoustics and Sound Reinforcement, 2002, 166--172.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[N. de Rycker: Theoretical and numerical study of sound diffraction-application to room acoustics auralization. Rapport de Stage D'Option Scientifique &#200;cole Polytechnique, Paris, France, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[R. Torres, N. de Rycker, M. Kleiner: Edge diffraction and surface scattering in concert halls: Physical and perceptual aspects. J. Temporal Design in Architecture and the Environment &#60;b&#62;4&#60;/b&#62; (2004) 52--58.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[N. Tsingos, J.-D. Gascuel: Fast rendering of sound occlusion and diffraction effects for virtual acoustic environments. Proc. Audio Engineering Society 104th Convention, 1998. preprint no. 4699.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[P. Calamia, U. P. Svensson: Edge subdivision for fast diffraction calculations. Proc. 2005 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), 2005, 187--190.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1289147</ref_obj_id>
				<ref_obj_pid>1288980</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[P. Calamia, U. P. Svensson: Fast time-domain edge-diffraction calculations for interactive acoustic simulations. EURASIP Journal on Advances in Signal Processing, Special Issue on Spatial Sound and Virtual Acoustics &#60;b&#62;2007&#60;/b&#62; (2007). Article ID 63560.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383864</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[N. Tsingos, C. Dachsbacher, S. Lefebvre, M. Dellepiane: Instant sound scattering. Proc. of the 18th Eurographics Symposium on Rendering (EGSR), July 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383323</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[N. Tsingos, T. Funkhouser, A. Ngan, I. Carlbom: Modeling acoustics in virtual environments using the Uniform Theory of Diffraction. Proc. SIGGRAPH 2001, 2001, 545--552.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1477384</ref_obj_id>
				<ref_obj_pid>1477066</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[A. Chandak, C. Lauterbach, M. Taylor, Z. Ren, D. Manocha: AD-frustum: Adaptive frustum tracing for interactive sound propagation. IEEE Trans. on Visualization and Computer Graphics &#60;b&#62;14&#60;/b&#62; (2008) 1707--1722.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[F. Antonacci, M. Foco, A. Sarti, S. Tubaro: Fast modeling of acoustic reflections and diffraction in complex environments using visibility diagrams. Proc. 12th European Signal Processing Conference (EUSIPCO '04), 2004, 1773--1776.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[L. Aveneau, E. Andres, M. M&#233;riaux: The discrete tube: A spatial acceleration technique for efficient diffraction computation. Proc. 8th International Conference on Discrete Geometry for Computer Imagery (DGCI '99), 1999, 413--4246.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[U. P. Svensson, R. I. Fred, J. Vanderkooy: An analytic secondary source model of edge diffraction impulse responses. J. Acoust. Soc. Am. &#60;b&#62;106&#60;/b&#62; (1999) 2331--2344.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[M. A. Biot, I. Tolstoy: Formulation of wave propagation in infinite media by normal coordinates with an application to diffraction. J. Acoust. Soc. Am. &#60;b&#62;29&#60;/b&#62; (1957) 381--391.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[H. Medwin: Shadowing by finite noise barriers. J. Acoust. Soc. Am. &#60;b&#62;69&#60;/b&#62; (1981) 1060--1064.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[U. P. Svensson, P. Calamia: Edge-diffraction impulse responses near specular-zone and shadow-zone boundaries. Acta Acustica united with Acustica &#60;b&#62;92&#60;/b&#62; (2006) 501--512.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[R. G. Kouyoumjian, P. H. Pathak: A uniform geometrical theory of diffraction for an edge in a perfectly conducting surface. Proc. IEEE, 1974, 1448--1461.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[U. P. Svensson: Edge Diffration Toolbox for Matlab. 2006. http://www.iet.ntnu.no/~svensson/Matlab.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[F. P. Mechel: Improved mirror source method in room acoustics. J. Sound. Vib. &#60;b&#62;256&#60;/b&#62; (2002) 873--940.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[L. Beranek: Concert and Opera Halls: How They Sound. Acoustical Society of America, Woodbury, NY, USA, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[M. Barron: Auditorium Acoustics and Architectural Design. E and FN Spon, London, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[C. S. Clay, W. A. Kinney: Numerical computations of time-domain diffractions from wedges and reflections from facets. J. Acoust. Soc. Am. &#60;b&#62;83&#60;/b&#62; (1988) 2126--2133.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[T. Lokki, V. Pulkki, P. Calamia: Measurement and modeling of diffraction from an edge of a thin panel. Applied Acoustics &#60;b&#62;69&#60;/b&#62; (2007) 824--832.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[Denon Records: Anechoic orchestral music recording. 1995. Audio CD.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[T. Funkhouser, N. Tsingos, I. Carlbom, G. Elko, J. West, G. Pingali, P. Min, A. Ngan: A beam tracing method for interactive architectural acoustics. J. Acoust. Soc. Am. &#60;b&#62;115&#60;/b&#62; (2004) 739--756.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[J. Borish: Extension of the image model to arbitrary polyhedra. J. Acoust. Soc. Am. &#60;b&#62;75&#60;/b&#62; (1984) 1827--1836.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[P. Calamia, U. P. Svensson: Culling insignificant diffraction components for interactive acoustic simulations. Proc. 19th Intl. Congress on Acoustics (ICA), Madrid, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[U. P. Svensson, P. Calamia: Edge diffraction in computer modeling of room acoustics (A). J. Acoust. Soc. Am. &#60;b&#62;120&#60;/b&#62; (2006) 2998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[P. Min, T. Funkhouser: Priority-driven acoustic modeling for virtual environments. Computer Graphics Forum &#60;b&#62;19&#60;/b&#62; (2000) 179--188.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[R. Yagel, J. Meeker: Priority-driven ray tracing. Journal of Visualization and Computer Animation &#60;b&#62;8&#60;/b&#62; (1997) 17--32.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[J. Vanderkooy, "A simple theory of cabinet edge diffraction," <i>Journal of the Audio Engineering Society</i>, vol. 39, no. 12, pp. 923--933, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[P. Menounou and J. H. You, "Experimental study of the diffracted sound field around jagged edge noise barriers," <i>The Journal of the Acoustical Society of America</i>, vol. 116, no. 5, pp. 2843--2854, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[R. R. Torres, U. P. Svensson, and M. Kleiner, "Computation of edge diffraction for more accurate room acoustics auralization," <i>The Journal of the Acoustical Society of America</i>, vol. 109, no. 2, pp. 600--610, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[T. Funkhouser, N. Tsingos, I. Carlbom, et al., "A beam tracing method for interactive architectural acoustics," <i>The Journal of the Acoustical Society of America</i>, vol. 115, no. 2, pp. 739--756, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[F. Antonacci, M. Foco, A. Sarti, and S. Tubaro, "Fast modeling of acoustic reflections and diffraction in complex environments using visibility diagrams," in <i>Proceedings of 12th European Signal Processing Conference (EUSIPCO '04)</i>, pp. 1773--1776, Vienna, Austria, September 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[R. G. Kouyoumjian and P. H. Pathak, "A uniform geometrical theory of diffraction for an edge in a perfectly conducting surface," <i>Proceedings of the IEEE</i>, vol. 62, pp. 1448--1461, 1974.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[M. A. Biot and I. Tolstoy, "Formulation of wave propagation in infinite media by normal coordinates with an application to diffraction," <i>The Journal of the Acoustical Society of America</i>, vol. 29, no. 3, pp. 381--391, 1957.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[H. Medwin, "Shadowing by finite noise barriers," <i>The Journal of the Acoustical Society of America</i>, vol. 69, no. 4, pp. 1060--1064, 1981.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[U. P. Svensson, R. I. Fred, and J. Vanderkooy, "An analytic secondary source model of edge diffraction impulse responses," <i>The Journal of the Acoustical Society of America</i>, vol. 106, no. 5, pp. 2331--2344, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[P. T. Calamia and U. P. Svensson, "Edge subdivision for fast diffraction calculations," in <i>Proceedings of IEEE Workshop on Applications of Signal Processing to Audio and Acoustics</i>, pp. 187--190, New Paltz, NY, USA, October 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[J. B. Allen and D. A. Berkley, "Image method for efficiently simulating small-room acoustics," <i>The Journal of the Acoustical Society of America</i>, vol. 65, no. 4, pp. 943--950, 1979.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[J. Borish, "Extension of the image model to arbitrary polyhedra," <i>The Journal of the Acoustical Society of America</i>, vol. 75, no. 6, pp. 1827--1836, 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>75</ref_seq_no>
				<ref_text><![CDATA[A. Krokstad, S. Str&#248;m, and S. S&#248;rsdal, "Calculating the acoustical room response by the use of a ray tracing technique," <i>Journal of Sound and Vibration</i>, vol. 8, no. 1, pp. 118--125, 1968.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280818</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>76</ref_seq_no>
				<ref_text><![CDATA[T. Funkhouser, I. Carlbom, G. Elko, G. Pingali, M. Sondhi, and J. West, "A beam tracing approach to acoustic modeling for interactive virtual environments," in <i>Proceedings of the 25th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH '98)</i>, pp. 21--32, Orlando, Fla, USA, July 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>77</ref_seq_no>
				<ref_text><![CDATA[J. B. Keller, "Geometrical theory of diffraction," <i>Journal of the Optical Society of America</i>, vol. 52, no. 2, pp. 116--130, 1962.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383323</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>78</ref_seq_no>
				<ref_text><![CDATA[N. Tsingos, T. Funkhouser, A. Ngan, and I. Carlbom, "Modeling acoustics in virtual environments using the uniform theory of diffraction," in <i>Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH '01)</i>, pp. 545--552, Los Angeles, Calif, USA, August 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>79</ref_seq_no>
				<ref_text><![CDATA[F. Antonacci, M. Foco, A. Sarti, and S. Tubaro, "Accurate and fast audio-realistic rendering of sounds in virtual environments," in <i>Proceedings of 6th IEEE Workshop on Multimedia Signal Processing</i>, pp. 271--274, Siena, Italy, September-October 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>80</ref_seq_no>
				<ref_text><![CDATA[F. Antonacci, M. Foco, A. Sarti, and S. Tubaro, "Real time modeling of acoustic propagation in complex environments," in <i>Proceedings of 7th International Conference on Digital Audio Effects (DAFx '04)</i>, pp. 274--279, Naples, Italy, October 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>81</ref_seq_no>
				<ref_text><![CDATA[H. Medwin, E. Childs, and G. M. Jebsen, "Impulse studies of double diffraction: a discrete Huygens interpretation," <i>The Journal of the Acoustical Society of America</i>, vol. 72, no. 3, pp. 1005--1013, 1982.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>82</ref_seq_no>
				<ref_text><![CDATA[V. Pulkki, T. Lokki, and L. Savioja, "Implementation and visualization of edge diffraction with image-source method," in <i>Proceedings of 112th Audio Engineering Society Convention</i>, Munich, Germany, May 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>83</ref_seq_no>
				<ref_text><![CDATA[P. T. Calamia, U. P. Svensson, and T. Funkhouser, "Integration of edge-diffraction calculations and geometrical-acoustics modeling," in <i>Proceedings of Forum Acusticum</i>, pp. 2499--2504, Budapest, Hungary, August 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>84</ref_seq_no>
				<ref_text><![CDATA[T. Lokki, U. P. Svensson, and L. Savioja, "An efficient auralization of edge diffraction," in <i>Proceedings of the Audio Engineering Society 21st International Conference on Architectural Acoustics and Sound Reinforcement</i>, pp. 166--172, St. Petersburg, Russia, June 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>85</ref_seq_no>
				<ref_text><![CDATA[L. Savioja, J. Huopaniemi, T. Lokki, and R. V&#228;&#228;n&#228;nen, "Creating interactive virtual acoustic environments," <i>Journal of the Audio Engineering Society</i>, vol. 47, no. 9, pp. 675--705, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>86</ref_seq_no>
				<ref_text><![CDATA[L. Savioja, J. Huopaniemi, and T. Lokki, "Auralization applying the parametric room acoustic modeling technique-the DIVA auralization system," in <i>Proceedings of the 8th International Conference on Auditory Display</i>, Kyoto, Japan, July 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>87</ref_seq_no>
				<ref_text><![CDATA[N. de Rycker, "Theoretical and numerical study of sound diffraction-application to room acoustics auralization," Rapport de Stage D'Option Scientifique, &#200;cole Polytechnique, Paris, France, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>88</ref_seq_no>
				<ref_text><![CDATA[R. Torres, N. de Rycker, and M. Kleiner, "Edge diffraction and surface scattering in concert halls: physical and perceptual aspects," <i>Journal of Temporal Design in Architecture and the Environment</i>, vol. 4, pp. 52--58, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>89</ref_seq_no>
				<ref_text><![CDATA[B.-I. Dalenb&#228;ck, <i>CATT-Acoustic v8 Manual</i>, http://www.catt.se/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>90</ref_seq_no>
				<ref_text><![CDATA[C. L. Christensen, <i>ODEON Room Acoustics Program ver. 8 Manual</i>, http://www.odeon.dk.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>266779</ref_obj_id>
				<ref_obj_pid>266774</ref_obj_pid>
				<ref_seq_no>91</ref_seq_no>
				<ref_text><![CDATA[N. Tsingos and J.-D. Gascuel, "Soundtracks for computer animation: sound rendering in dynamic environments with occlusions," in <i>Proceedings of the Conference on Graphics Interface</i>, pp. 9--16, Kelowna, British Columbia, Canada, May 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>92</ref_seq_no>
				<ref_text><![CDATA[N. Tsingos and J.-D. Gascuel, "Fast rendering of sound occlusion and diffraction effects for virtual acoustic environments," in <i>Proceedings of the Audio Engineering Society 104th Convention</i>, Amsterdam, The Netherlands, May 1998, preprint no. 4699.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>93</ref_seq_no>
				<ref_text><![CDATA[U. P. Svensson and P. T. Calamia, "Edge-diffraction impulse responses near specular-zone and shadow-zone boundaries," <i>Acta Acustica united with Acustica</i>, vol. 92, no. 4, pp. 501--512, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>94</ref_seq_no>
				<ref_text><![CDATA[C. S. Clay and W. A. Kinney, "Numerical computations of time-domain diffractions from wedges and reflections from facets," <i>The Journal of the Acoustical Society of America</i>, vol. 83, no. 6, pp. 2126--2133, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>95</ref_seq_no>
				<ref_text><![CDATA[P. J. Davis and P. Rabinowitz, <i>Methods of Numerical Integration</i>, Academic Press, New York, NY, USA, 2nd edition, 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>96</ref_seq_no>
				<ref_text><![CDATA[R. Torres, <i>Studies of edge diffraction and scattering: applications to room acoustics and auralization</i>, Ph.D. thesis, Chalmers University of Technology, G&#246;teborg, Sweden, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>97</ref_seq_no>
				<ref_text><![CDATA[T. Lokki and V. Pulkki, "Measurement and theoretical validation of diffraction from a single edge," in <i>Proceedings of the 18th International Congress on Acoustics (ICA '04)</i>, vol. 2, pp. 929--932, Kyoto, Japan, April 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>98</ref_seq_no>
				<ref_text><![CDATA[A. L&#248;vstad and U. P. Svensson, "Diffracted sound field from an orchestra pit," <i>Acoustical Science and Technology</i>, vol. 26, no. 2, pp. 237--239, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>99</ref_seq_no>
				<ref_text><![CDATA[A. M. J. Davis and R. W. Scharstein, "The complete extension of the Biot-Tolstoy solution to the density contrast wedge with sample calculations," <i>The Journal of the Acoustical Society of America</i>, vol. 101, no. 4, pp. 1821--1835, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>100</ref_seq_no>
				<ref_text><![CDATA[J. C. Novarini and R. S. Keiffer, "Impulse response of a density contrast wedge: practical implementation and some aspects of its diffracted component," <i>Applied Acoustics</i>, vol. 58, no. 2, pp. 195--210, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>101</ref_seq_no>
				<ref_text><![CDATA[U. P. Svensson and P. T. Calamia, "The use of edge diffraction in computational room acoustics," <i>The Journal of the Acoustical Society of America</i>, vol. 120, p. 2998, 2006, (A).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>102</ref_seq_no>
				<ref_text><![CDATA[Alais, D., and Carlile, S. 2005. Synchronizing to real events: subjective audiovisual alignment scales with perceived auditory depth and speed of sound. <i>Proc Natl Acad Sci 102</i>, 6, 2244--7.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>103</ref_seq_no>
				<ref_text><![CDATA[Begault, D. 1999. Auditory and non-auditory factors that potentially influence virtual acoustic imagery. In <i>Proc. AES 16th Int. Conf. on Spatial Sound Reproduction</i>, 13--26.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>104</ref_seq_no>
				<ref_text><![CDATA[Fujisaki, W., Shimojo, S., Kashino, M., and Nishida, S. 2004. Recalibration of audiovisual simultaneity. <i>Nature Neuro-science 7</i>, 7, 773--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>105</ref_seq_no>
				<ref_text><![CDATA[Guski, R., and Troje, N. 2003. Audiovisual phenomenal causality. <i>Perception and Psychophysics 65</i>, 5, 789--800.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>106</ref_seq_no>
				<ref_text><![CDATA[Hormander, L. 1983. <i>The Analysis of Linear Partial Differential Operators I.</i> Springer-Verlag.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>107</ref_seq_no>
				<ref_text><![CDATA[Howell, D. C. 1992. <i>Statistical Methods for Psychology.</i> PWS-Kent.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>108</ref_seq_no>
				<ref_text><![CDATA[ITU. 2001--2003. Method for the subjective assessment of intermediate quality level of coding systems, rec. ITU-R BS.1534--1, http://www.itu.int/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141983</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>109</ref_seq_no>
				<ref_text><![CDATA[James, D. L., Barbic, J., and Pai, D. K. 2006. Precomputed acoustic transfer: Output-sensitive, accurate sound generation for geometrically complex vibration sources. <i>ACM Transactions on Graphics (ACM SIGGRAPH) 25</i>, 3 (July), 987--995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>110</ref_seq_no>
				<ref_text><![CDATA[Larsson, P., V&#228;stfj&#228;ll, D., and Kleiner, M. 2002. Better presence and performance in virtual environments by improved binaural sound rendering. <i>Proc. AES 22nd Intl. Conf. on virtual, synthetic and entertainment audio</i> (June), 31--38.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1230133</ref_obj_id>
				<ref_obj_pid>1230100</ref_obj_pid>
				<ref_seq_no>111</ref_seq_no>
				<ref_text><![CDATA[Moeck, T., Bonneel, N., Tsingos, N., Drettakis, G., Viaud-Delmon, I., and Aloza, D. 2007. Progressive perceptual audio rendering of complex scenes. In <i>ACM SIGGRAPH Symp. on Interactive 3D Graphics and Games (I3D)</i>, 189--196.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>545290</ref_obj_id>
				<ref_obj_pid>545261</ref_obj_pid>
				<ref_seq_no>112</ref_seq_no>
				<ref_text><![CDATA[O'Brien, J. F., Shen, C., and Gatchalian, C. M. 2002. Synthesizing sounds from rigid-body simulations. In <i>ACM SIGGRAPH Symp. on Computer Animation</i>, 175--181.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>294797</ref_obj_id>
				<ref_seq_no>113</ref_seq_no>
				<ref_text><![CDATA[Oppenheim, A. V., Schafer, R. W., and Buck, J. R. 1999. <i>Discrete-Time Signal Processing (2nd edition).</i> Prentice-Hall.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383268</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>114</ref_seq_no>
				<ref_text><![CDATA[Pai, D. K., van den Doel, K., James, D. L., Lang, J., Lloyd, J. E., Richmond, J. L., and Yau, S. H. 2001. Scanning physical interaction behavior of 3d objects. In <i>Proc. ACM SIGGRAPH 2001</i>, 87--96.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>148286</ref_obj_id>
				<ref_seq_no>115</ref_seq_no>
				<ref_text><![CDATA[Press, W. H., Teukolsky, S. A., Vetterling, W. T., and Flannery, B. P. 1992. <i>Numerical recipes in C: The art of scientific computing.</i> Cambridge University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1111429</ref_obj_id>
				<ref_obj_pid>1111411</ref_obj_pid>
				<ref_seq_no>116</ref_seq_no>
				<ref_text><![CDATA[Raghuvanshi, N., and Lin, M. C. 2006. Interactive sound synthesis for large scale environments. In <i>ACM SIGGRAPH Symp. on Interactive 3D Graphics and Games (I3D)</i>, 101--108.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>117</ref_seq_no>
				<ref_text><![CDATA[Rodet, X., and Depalle, P. 1992. Spectral envelopes and inverse FFT synthesis. In <i>Proc. 93rd Conv. AES, San Francisco.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>118</ref_seq_no>
				<ref_text><![CDATA[Sekuler, R., Sekuler, A. B., and Lau, R. 1997. Sound alters visual motion perception. <i>Nature 385</i>, 6614, 308.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>119</ref_seq_no>
				<ref_text><![CDATA[Sugita, Y., and Suzuki, Y. 2003. Audiovisual perception: Implicit estimation of sound-arrival time. <i>Nature 421</i>, 6926, 911.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015710</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>120</ref_seq_no>
				<ref_text><![CDATA[Tsingos, N., Gallo, E., and Drettakis, G. 2004. Perceptual audio rendering of complex virtual environments. <i>ACM Transactions on Graphics (ACM SIGGRAPH) 23</i>, 3 (July), 249--258.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>121</ref_seq_no>
				<ref_text><![CDATA[Tsingos, N. 2005. Scalable perceptual mixing and filtering of audio signals using an augmented spectral representation. In <i>Proc. Int. Conf. on Digital Audio Effects</i>, 277--282.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1246774</ref_obj_id>
				<ref_obj_pid>1246770</ref_obj_pid>
				<ref_seq_no>122</ref_seq_no>
				<ref_text><![CDATA[van den Doel, K., and Pai, D. K. 1998. The sounds of physical shapes. <i>Presence 7</i>, 4, 382--395.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>123</ref_seq_no>
				<ref_text><![CDATA[van den Doel, K., and Pai, D. K. 2003. Modal synthesis for vibrating objects. <i>Audio Anecdotes.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383322</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>124</ref_seq_no>
				<ref_text><![CDATA[van den Doel, K., Kry, P. G., and Pai, D. K. 2001. FoleyAutomatic: physically-based sound effects for interactive simulation and animation. In <i>Proc. ACM SIGGRAPH 2001</i>, 537--544.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>125</ref_seq_no>
				<ref_text><![CDATA[van den Doel, K., Pai, D. K., Adam, T., Kortchmar, L., and Pichora-Fuller, K. 2002. Measurements of perceptual quality of contact sound models. <i>Intl. Conf. on Auditory Display, (ICAD)</i>, 345--349.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1008558</ref_obj_id>
				<ref_obj_pid>1008550</ref_obj_pid>
				<ref_seq_no>126</ref_seq_no>
				<ref_text><![CDATA[van den Doel, K., Knott, D., and Pai, D. K. 2004. Interactive simulation of complex audiovisual scenes. <i>Presence: Teleoperators and Virtual Environments 13</i>, 1, 99--111.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>127</ref_seq_no>
				<ref_text><![CDATA[Z&#246;lzer, U. 2002. <i>Digital Audio Effects (DAFX) chapter 8.</i> Wiley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>128</ref_seq_no>
				<ref_text><![CDATA[Alais, D., and Burr, D. 2004. The ventriloquism effect results from near-optimal bimodal integration. <i>Current Biology 14</i>, 257--262.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>129</ref_seq_no>
				<ref_text><![CDATA[and, C. G. 1993. Methods for quality assessment of low bit-rate audio codecs, proceedings of the 12th aes conference. 97--107.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>130</ref_seq_no>
				<ref_text><![CDATA[Berkhout, A., de Vries, D., and Vogel, P. 1993. Acoustic control by wave field synthesis. <i>J. of the Acoustical Society of America 93</i>, 5 (may), 2764--2778.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>131</ref_seq_no>
				<ref_text><![CDATA[Blauert, J. 1997. <i>Spatial Hearing: The Psychophysics of Human Sound Localization.</i> M.I.T. Press, Cambridge, MA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>132</ref_seq_no>
				<ref_text><![CDATA[Chen, J., Veen, B. V., and Hecox, K. 1995. A spatial feature extraction and regularization model for the head-related transfer function. <i>J. of the Acoustical Society of America 97</i> (Jan.), 439--452.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>133</ref_seq_no>
				<ref_text><![CDATA[Darlington, D., Daudet, L., and Sandler, M. 2002. Digital audio effects in the wavelet domain. In <i>Proceedings of COST-G6 Conference on Digital Audio Effects, DAFX2002, Hamburg, Germany.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>134</ref_seq_no>
				<ref_text><![CDATA[2003. EBU subjective listening tests on low-bitrate audio codecs. <i>Technical report 3296, European Broadcast Union (EBU), Projet Group B/AIM</i> (june).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>135</ref_seq_no>
				<ref_text><![CDATA[Fouad, H., Hahn, J., and Ballas, J. 1997. Perceptually based scheduling algorithms for real-time synthesis of complex sonic environments. <i>proceedings of the 1997 International Conference on Auditory Display (ICAD'97)</i>,.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>136</ref_seq_no>
				<ref_text><![CDATA[Gallo, E., Lemaitre, G., and Tsingos, N. 2005. Prioritizing signals for selective real-time audio processing. In <i>Proc. of ICAD 2005.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1162533</ref_obj_id>
				<ref_obj_pid>1162526</ref_obj_pid>
				<ref_seq_no>137</ref_seq_no>
				<ref_text><![CDATA[Hairston, W., Wallace, M., amd B. E. Stein, J. V., Norris, J., and Schirillo, J. 2003. Visual localization ability influences cross-modal bias. <i>J. Cogn. Neuroscience 15</i>, 20--29.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>138</ref_seq_no>
				<ref_text><![CDATA[Herder, J. 1999. Optimization of sound spatialization resource management through clustering. <i>The Journal of Three Dimensional Images, 3D-Forum Society 13</i>, 3 (Sept.), 59--65.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>139</ref_seq_no>
				<ref_text><![CDATA[Hochbaum, D. S., and Schmoys, D. B. 1985. A best possible heuristic for the ik-center problem. <i>Mathematics of Operations Research 10</i>, 2 (May), 180--184.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>140</ref_seq_no>
				<ref_text><![CDATA[Howell, D. C. 1992. <i>Statistical methods for psychology.</i> PWS-Kent.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>141</ref_seq_no>
				<ref_text><![CDATA[International Telecom. Union. 2001--2003. Method for the subjective assessment of intermediate quality level of coding systems. <i>Recommendation ITU-R</i> BS.1534-1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>297870</ref_obj_id>
				<ref_obj_pid>297843</ref_obj_pid>
				<ref_seq_no>142</ref_seq_no>
				<ref_text><![CDATA[Itti, L., Koch, C., and Niebur, E. 1998. A model of saliency-based visual attention for rapid scene analysis. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence 20</i>, 11 (Nov.), 1254--1259.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>143</ref_seq_no>
				<ref_text><![CDATA[ITU-R. 1994. Methods for subjective assessment of small impairments in audio systems including multichannel sound systems. itu-r bs 1116. Tech. rep.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>144</ref_seq_no>
				<ref_text><![CDATA[Jot, J.-M., and Walsh, M. 2006. Binaural simulation of complex acoustic scenes for interactive audio. <i>In 121th AES Convention, San Francisco, USA. Preprint 6950.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>145</ref_seq_no>
				<ref_text><![CDATA[Jot, J.-M., Larcher, V., and Pernaux, J.-M. 1999. A comparative study of 3D audio encoding and rendering techniques. <i>Proceedings of the AES 16th international conference, Spatial sound reproduction, Rovaniemi, Finland</i> (April).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>146</ref_seq_no>
				<ref_text><![CDATA[Kayser, C., Petkov, C., Lippert, M., and Logothetis, N. 2005. Mechanisms for allocating auditory attention: An auditory saliency map. <i>Current Biology 15</i> (Nov.), 1943--1947.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>147</ref_seq_no>
				<ref_text><![CDATA[Kelly, M., and Tew, A. 2002. The continuity illusion in virtual auditory space. <i>proc. of the 112th AES Conv., Munich, Germany</i> (May).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>148</ref_seq_no>
				<ref_text><![CDATA[Kurniawati, E., Absar, J., George, S., Lau, C. T., and Premkumar, B. 2002. The significance of tonality index and nonlinear psychoacoustics models for masking threshold estimation. In <i>Proceedings of the International Conference on Virtual, Synthetic and Entertainment Audio AES22.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>149</ref_seq_no>
				<ref_text><![CDATA[Lanciani, C. A., and Schafer, R. W. 1997. Psychoacoustically-based processing of MPEG-I layer 1--2 encoded signals. In <i>Proc. IEEE Signal Processing Society 1997 Workshop on Multimedia Signal Processing</i>, 53--58.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1257568</ref_obj_id>
				<ref_obj_pid>1257295</ref_obj_pid>
				<ref_seq_no>150</ref_seq_no>
				<ref_text><![CDATA[Lanciani, C. A., and Schafer, R. W. 1999. Subband-domain filtering of MPEG audio signals. In <i>Proceedings of Intl. Conf. on Acoustics, Speech and Signal Processing</i>, 917--920.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>151</ref_seq_no>
				<ref_text><![CDATA[Larcher, V., Jot, J., Guyard, G., and Warusfel, O. 2000. Study and comparison of efficient methods for 3d audio spatialization based on linear decomposition of HRTF data. <i>Proc. 108th Audio Engineering Society Convention.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>152</ref_seq_no>
				<ref_text><![CDATA[Lewald, J., Ehrenstein, W. H., and Guski, R. 2001. Spatio-temporal constraints for auditory-visual integration. <i>Beh. Brain Research 121</i>, 1--2, 69--79.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>153</ref_seq_no>
				<ref_text><![CDATA[Malham, D., and Myatt, A. 1995. 3D sound spatialization using ambisonic techniques. <i>Computer Music Journal 19</i>, 4, 58--70.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>154</ref_seq_no>
				<ref_text><![CDATA[M&#248;ller, H. 1992. Fundamentals of binaural technology. <i>Applied Acoustics 36</i>, 171--218.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>155</ref_seq_no>
				<ref_text><![CDATA[Painter, E. M., and Spanias, A. S. 2000. Perceptual coding of digital audio. <i>Proceedings of the IEEE 88</i>, 4 (Apr.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>156</ref_seq_no>
				<ref_text><![CDATA[Sarlat, L., Warusfel, O., and Viaud-Delmon, I. 2006. Ventriloquism aftereffects occur in the rear hemisphere. <i>Neuroscience Letters 404</i>, 324--329.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>157</ref_seq_no>
				<ref_text><![CDATA[Stoll, G., and Kozamernik, F. 2000. EBU subjective listening tests on internet audio codecs. <i>EBU TECHNICAL REVIEW</i>, (June).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1052412</ref_obj_id>
				<ref_obj_pid>1052380</ref_obj_pid>
				<ref_seq_no>158</ref_seq_no>
				<ref_text><![CDATA[Touimi, A. B., Emerit, M., and Pernaux, J.-M. 2004. Efficient method for multiple compressed audio streams spatialization. In <i>In Proceeding of ACM 3rd Intl. Conf. on Mobile and Ubiquitous multimedia.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>159</ref_seq_no>
				<ref_text><![CDATA[Touimi, A. B. 2000. A generic framework for filtering in subband domain. In <i>In Proc. of IEEE 9th Wkshp. on Digital Signal Processing, Hunt, Texas, USA.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015710</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>160</ref_seq_no>
				<ref_text><![CDATA[Tsingos, N., Gallo, E., and Drettakis, G. 2004. Perceptual audio rendering of complex virtual environments. <i>Proc. SIGGRAPH'04</i> (August).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>161</ref_seq_no>
				<ref_text><![CDATA[Tsingos, N. 2005. Scalable perceptual mixing and filtering of audio signals using an augmented spectral representation. <i>Proc. of 8th Intl. Conf. on Digital Audio Effects (DAFX'05), Madrid, Spain</i> (Sept.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2386334</ref_obj_id>
				<ref_obj_pid>2386332</ref_obj_pid>
				<ref_seq_no>162</ref_seq_no>
				<ref_text><![CDATA[Wand, M., and Strasser, W. 2004. Multi-resolution sound rendering. In <i>Symp. Point-Based Graphics.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>163</ref_seq_no>
				<ref_text><![CDATA[Z&#246;lzer, U., Ed. 2002. <i>DAFX - Digital Audio Effects.</i> Wiley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1055646</ref_obj_id>
				<ref_obj_pid>1055626</ref_obj_pid>
				<ref_seq_no>164</ref_seq_no>
				<ref_text><![CDATA[Magnus Ekman, Fredrik Warg, and Jim Nilsson, "An in-depth look at computer performance growth," <i>SIGARCH Comput. Archit. News</i>, vol. 33, no. 1, pp. 144--147, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1092257</ref_obj_id>
				<ref_obj_pid>1092229</ref_obj_pid>
				<ref_seq_no>165</ref_seq_no>
				<ref_text><![CDATA[D. Geer, "Taking the graphics processor beyond graphics," <i>Computer</i>, vol. 38, no. 9, pp. 14--16, Sept. 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1413968</ref_obj_id>
				<ref_obj_pid>1413957</ref_obj_pid>
				<ref_seq_no>166</ref_seq_no>
				<ref_text><![CDATA[Avi Bleiweiss, "GPU accelerated pathfinding," in <i>GH '08: Proceedings of the 23rd ACM SIGGRAPH/EUROGRAPHICS symposium on Graphics hardware</i>, Aire-la-Ville, Switzerland, Switzerland, 2008, pp. 65--74, Eurographics Association.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>167</ref_seq_no>
				<ref_text><![CDATA["GPGPU - General-Purpose Computation on Graphics Hardware," Available at http://www.gpgpu.org, accessed May 15, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>168</ref_seq_no>
				<ref_text><![CDATA["GPGPU - Tutorial and Courses.," Available at http://gpgpu.org/tag/tutorials-courses, Accessed May 15, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>169</ref_seq_no>
				<ref_text><![CDATA[John D. Owens, David Luebke, Naga Govindaraju, Mark Harris, Jens Kr&#252;ger, Aaron E. Lefohn, and Timothy J. Purcell, "A survey of general-purpose computation on graphics hardware," <i>Computer Graphics Forum</i>, vol. 26, no. 1, pp. 80--113, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>170</ref_seq_no>
				<ref_text><![CDATA[M. Pharr, Ed., <i>GPU Gems 2: Programming Techniques, Tips and Tricks for Real-Time Graphics, Part IV: General-Purpose Computation on GPUS: A Primer</i>, Addison-Wesley Professional, 2005, Available at http://http.developer.nvidia.com/GPUGems2/gpugems2_part04.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>171</ref_seq_no>
				<ref_text><![CDATA[P. Lapsley, J. Bier, A. Shoham, and E. A. Lee, <i>DSP Processor Fundamentals</i>, IEEE Press, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>172</ref_seq_no>
				<ref_text><![CDATA[J. Eyre and J. Bier, "The evolution of DSP processors," <i>IEEE Signal Processing Magazine</i>, 2000, See also http://www.bdti.com/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>184407</ref_obj_id>
				<ref_seq_no>173</ref_seq_no>
				<ref_text><![CDATA[Durand R. Begault, <i>3D Sound for Virtual Reality and Multimedia</i>, Academic Press Professional, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>174</ref_seq_no>
				<ref_text><![CDATA[L. Savioja, J. Huopaniemi, T. Lokki, and R. V&#228;&#228;n&#228;nen, "Creating interactive virtual acoustic environments," <i>J. Audio Eng. Soc.</i>, vol. 47, no. 9, pp. 675--705, Sept. 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1536767</ref_obj_id>
				<ref_seq_no>175</ref_seq_no>
				<ref_text><![CDATA[D. Shreiner, M. Woo, J. Neider, and T. Davis, <i>OpenGL Programming Guide: The Official Guide to Learning OpenGL, Version 2.1 (6th Edition)</i>, Addison-Wesley, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1251701</ref_obj_id>
				<ref_obj_pid>1251557</ref_obj_pid>
				<ref_seq_no>176</ref_seq_no>
				<ref_text><![CDATA[David Luebke and Greg Humphreys, "How GPUs Work," <i>Computer</i>, vol. 40, no. 2, pp. 96--100, Feb. 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1076523</ref_obj_id>
				<ref_seq_no>177</ref_seq_no>
				<ref_text><![CDATA[R. Rost, <i>OpenGL(R) Shading Language (2nd Edition)</i>, Addison-Wesley, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882362</ref_obj_id>
				<ref_obj_pid>1201775</ref_obj_pid>
				<ref_seq_no>178</ref_seq_no>
				<ref_text><![CDATA[William R. Mark, R. Steven Glanville, Kurt Akeley, and Mark J. Kilgard, "Cg: a system for programming graphics hardware in a c-like language," in <i>SIGGRAPH '03: ACM SIGGRAPH 2003 Papers</i>, New York, NY, USA, 2003, pp. 896--907, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>862247</ref_obj_id>
				<ref_seq_no>179</ref_seq_no>
				<ref_text><![CDATA[Randima Fernando and Mark J. Kilgard, <i>The Cg Tutorial: The Definitive Guide to Programmable Real-Time Graphics</i>, Addison-Wesley Professional, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>180</ref_seq_no>
				<ref_text><![CDATA["Nvidia CUDA," Available at http://www.nvidia.com/cuda, accessed May 15, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>181</ref_seq_no>
				<ref_text><![CDATA[I. Buck and T. Purcell, <i>GPU Gems: Programming Techniques, Tips and Tricks for Real-Time Graphics. Chapter 37: A Toolkit for Computation on GPUs</i>, Addison-Wesley Professional, 2004, Available at http://http.developer.nvidia.com/GPUGems/gpugems_ch37.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015800</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>182</ref_seq_no>
				<ref_text><![CDATA[Ian Buck, Tim Foley, Daniel Horn, Jeremy Sugerman, Kayvon Fatahalian, Mike Houston, and Pat Hanrahan, "Brook for GPUs: stream computing on graphics hardware," <i>ACM Trans. Graph.</i>, vol. 23, no. 3, pp. 777--786, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>183</ref_seq_no>
				<ref_text><![CDATA[John Owens, <i>GPU Gems 2: Programming Techniques, Tips and Tricks for Real-Time Graphics. Chapter 29: Streaming Architectures and Technology Trends</i>, Addison-Wesley Professional, 2005, Available at http://developer.nvidia.com/object/gpu_gems_2_home.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>844191</ref_obj_id>
				<ref_obj_pid>844174</ref_obj_pid>
				<ref_seq_no>184</ref_seq_no>
				<ref_text><![CDATA[Kenneth Moreland and Edward Angel, "The FFT on a GPU," in <i>HWWS '03: Proceedings of the ACM SIGGRAPH/EUROGRAPHICS conference on Graphics hardware</i>, Aire-la-Ville, Switzerland, Switzerland, 2003, pp. 112--119, Eurographics Association.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882363</ref_obj_id>
				<ref_obj_pid>1201775</ref_obj_pid>
				<ref_seq_no>185</ref_seq_no>
				<ref_text><![CDATA[Jens Kr&#252;ger and R&#252;diger Westermann, "Linear algebra operators for gpu implementation of numerical algorithms," in <i>SIGGRAPH '03: ACM SIGGRAPH 2003 Papers</i>, New York, NY, USA, 2003, pp. 908--916, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>186</ref_seq_no>
				<ref_text><![CDATA["GPUmat: A GPU toolbox for MATLAB," Available at http://gp-you.org/, accessed May 15, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>187</ref_seq_no>
				<ref_text><![CDATA["GPUFFTW: High Performance Power-of-two FFT library using graphics processors," Available at http://gamma.cs.unc.edu/GPUFFTW/, accessed May 15, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1316317</ref_obj_id>
				<ref_obj_pid>1316088</ref_obj_pid>
				<ref_seq_no>188</ref_seq_no>
				<ref_text><![CDATA[Naga K. Govindaraju and Dinesh Manocha, "Cache-efficient numerical algorithms using graphics hardware," <i>Parallel Comput.</i>, vol. 33, no. 10--11, pp. 663--684, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>189</ref_seq_no>
				<ref_text><![CDATA[Naga K. Govindaraju, Scott Larsen, Jim Gray, and Dinesh Manocha, "A memory model for scientific algorithms on graphics processors," Nov. 2006, pp. 6--6.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>190</ref_seq_no>
				<ref_text><![CDATA[S. Whalen, "Audio and the Graphics Processing Unit," Available at http://www.node99.org/papers/gpuaudio.pdf, accessed May 15, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>191</ref_seq_no>
				<ref_text><![CDATA[E. Gallo and N. Tsingos, "Efficient 3D audio processing with the GPU," in <i>Proc. ACM Workshop on General Purpose Computing on Graphics Processors (poster), Los Angeles</i>, Aug. 2004, http://www-sop.inria.fr/reves/projects/GPUAudio/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>192</ref_seq_no>
				<ref_text><![CDATA[LiquidSonics, "Reverberate LE GPU Edition," Available at http://www.liquidsonics.com/software.htm, accessed May 15, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>193</ref_seq_no>
				<ref_text><![CDATA[Nils Schneider, "VST Plugin: Convolution Reverb on NVidia GPUs," Available at http://www.nilsschneider.de, accessed May 15, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>194</ref_seq_no>
				<ref_text><![CDATA[Acustica Audio, "Nebula 3 VST Plugin," Available at http://www.acusticaudio.net/, accessed May 15, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>195</ref_seq_no>
				<ref_text><![CDATA["Adobe PixelBender API," Available at http://labs.adobe.com/technologies/pixelbender, accessed May 15, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>196</ref_seq_no>
				<ref_text><![CDATA[J. Blauert, <i>Spatial Hearing: The Psychophysics of Human Sound Localization</i>, M.I.T. Press, Cambridge, MA, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>197</ref_seq_no>
				<ref_text><![CDATA[A. Smirnov and T. Chiueh, "An Implementation of a FIR Filter on a GPU," Available at http://research.alexeysmirnov.name/index.php?area=gr&proj=fog, accessed May 15, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1639617</ref_obj_id>
				<ref_obj_pid>1639601</ref_obj_pid>
				<ref_seq_no>198</ref_seq_no>
				<ref_text><![CDATA[B. Cowan and B. Kapralos, "Real-time GPU-base convolution: A follow-up," in <i>Proc. of the FuturePlay @ GDC Canada Intl. Conf. on the Future of Game Design and Technology</i>, May 12--13 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1497012</ref_obj_id>
				<ref_obj_pid>1496984</ref_obj_pid>
				<ref_seq_no>199</ref_seq_no>
				<ref_text><![CDATA[Brent Cowan and Bill Kapralos, "Spatial sound for video games and virtual environments utilizing real-time GPU-based convolution," in <i>Future Play '08: Proceedings of the 2008 Conference on Future Play</i>, New York, NY, USA, 2008, pp. 166--172, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1536194</ref_obj_id>
				<ref_obj_pid>1536170</ref_obj_pid>
				<ref_seq_no>200</ref_seq_no>
				<ref_text><![CDATA[F. Trebien and M. M. Oliveira, "Realistic real-time sound resynthesis and processing for interactive virtual worlds," <i>The Visual Computer</i>, vol. 25, no. 5--7, pp. 469--477, May 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>201</ref_seq_no>
				<ref_text><![CDATA[F. Trebien and M. M. Oliveira, <i>Real-time Audio Processing on the GPU</i>, pp. 583--604, Charles River Media, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2101039</ref_obj_id>
				<ref_obj_pid>2100998</ref_obj_pid>
				<ref_seq_no>202</ref_seq_no>
				<ref_text><![CDATA[Q. Zhang, L. Ye, and Z. Pan, "Physically-based sound synthesis on GPUs," in <i>Proc. of the 4th Intl. Conf. on Entertainment Computing</i>, Sept. 19--21 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1400965</ref_obj_id>
				<ref_obj_pid>1400885</ref_obj_pid>
				<ref_seq_no>203</ref_seq_no>
				<ref_text><![CDATA[Christoph von Tycowicz and J&#246;rn Loviscach, "A malleable drum," in <i>SIGGRAPH '08: ACM SIGGRAPH 2008 posters</i>, New York, NY, USA, 2008, pp. 1--1, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>204</ref_seq_no>
				<ref_text><![CDATA[N. R&#246;ber, M. Spindler, and M. Masuch, "Waveguide-based Room Acoustics through Graphics Hardware," in <i>Proc. Intl. Computer Music Conf. (ICMC)</i>, Nov. 6-11, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>205</ref_seq_no>
				<ref_text><![CDATA[N. Raghuvanshi, B. Lloyd, and M. C. Lin, "Efficient Numerical Acoustic Simulation on Graphics Processors Using Adaptive Rectangular Decomposition," in <i>Proc. EAA Symp. on Auralization</i>, June 15-17, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1592280</ref_obj_id>
				<ref_obj_pid>1591897</ref_obj_pid>
				<ref_seq_no>206</ref_seq_no>
				<ref_text><![CDATA[Nikunj Raghuvanshi, Rahul Narain, and Ming C. Lin, "Efficient and accurate sound propagation using adaptive rectangular decomposition," <i>IEEE Transactions on Visualization and Computer Graphics</i>, vol. 99, no. 2, 5555.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>207</ref_seq_no>
				<ref_text><![CDATA[Samuel Siltanen, Tapio Lokki, and Lauri Savioja, "Frequency domain acoustic radiance transfer for real-time auralization," <i>Acta Acustica united with Acustica</i>, vol. 95, pp. 106--117(12), January/February 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>844182</ref_obj_id>
				<ref_obj_pid>844174</ref_obj_pid>
				<ref_seq_no>208</ref_seq_no>
				<ref_text><![CDATA[Nathan A. Carr, Jesse D. Hall, and John C. Hart, "GPU algorithms for radiosity and subsurface scattering," in <i>HWWS '03: Proceedings of the ACM SIGGRAPH/EUROGRAPHICS conference on Graphics hardware</i>, Aire-la-Ville, Switzerland, Switzerland, 2003, pp. 51--59, Eurographics Association.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276453</ref_obj_id>
				<ref_obj_pid>1275808</ref_obj_pid>
				<ref_seq_no>209</ref_seq_no>
				<ref_text><![CDATA[Carsten Dachsbacher, Marc Stamminger, George Drettakis, and Fr&#233;do Durand, "Implicit visibility and antiradiance for interactive global illumination," in <i>SIGGRAPH '07: ACM SIGGRAPH 2007 papers</i>, New York, NY, USA, 2007, p. 61, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360636</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>210</ref_seq_no>
				<ref_text><![CDATA[Jaakko Lehtinen, Matthias Zwicker, Emmanuel Turquin, Janne Kontkanen, Fr&#233;do Durand, Fran&#231;ois Sillion, and Timo Aila, "A meshless hierarchical representation for light transport," <i>ACM Trans. Graph.</i>, vol. 27, no. 3, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566640</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>211</ref_seq_no>
				<ref_text><![CDATA[Timothy J. Purcell, Ian Buck, William R. Mark, and Pat Hanrahan, "Ray tracing on programmable graphics hardware," <i>ACM Transactions on Graphics</i>, vol. 21, no. 3, pp. 703--712, July 2002, ISSN 0730-0301 (Proceedings of ACM SIGGRAPH 2002).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>212</ref_seq_no>
				<ref_text><![CDATA[N. R&#246;ber, U. Kaminski, and M. Masuch, "Ray-acoustics using Computer Graphics Technology," in <i>Proc. 10th Intl. Conf. on Digital Audio Effects (DAFx)</i>, Sep. 10-15, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>213</ref_seq_no>
				<ref_text><![CDATA[M. Jedrzejewski and K. Marasek, "Computation of room acoustics using programmable video hardware," in <i>Proc. Computer Vision and Graphics International Conference, ICCVG 2004, Warsaw, Poland</i>, September 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1477384</ref_obj_id>
				<ref_obj_pid>1477066</ref_obj_pid>
				<ref_seq_no>214</ref_seq_no>
				<ref_text><![CDATA[A. Chandak, C. Lauterbach, M. Taylor, Z. Ren, and D. Manocha, "AD-Frustum: Adaptive Frustum Tracing for Interactive Sound Propagation," <i>Visualization and Computer Graphics, IEEE Transactions on</i>, vol. 14, no. 6, pp. 1707--1722, Nov.-Dec. 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>266779</ref_obj_id>
				<ref_obj_pid>266774</ref_obj_pid>
				<ref_seq_no>215</ref_seq_no>
				<ref_text><![CDATA[Nicolas Tsingos and Jean-Dominique Gascuel, "Soundtracks for computer animation: sound rendering in dynamic environments with occlusions," in <i>Proc. of Graphics Interface'97</i>, May 1997, pp. 9--16.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>216</ref_seq_no>
				<ref_text><![CDATA[B. Cowan and B. Kapralos, "Real-time acoustical diffraction modeling using the GPU," in <i>Proc. of the 10th Western Pacific Acoustics Conf.</i>, Sept. 21-23 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>217</ref_seq_no>
				<ref_text><![CDATA[Nicolas Tsingos and Jean-Dominique Gascuel, "Fast rendering of sound occlusion and diffraction effects for virtual acoustic environments," in <i>Proc. 104th Audio Engineering Society Convention, preprint 4699</i>, Amsterdam, Netherlands, May 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383864</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>218</ref_seq_no>
				<ref_text><![CDATA[Nicolas Tsingos, Carsten Dachsbacher, Sylvain Lefebvre, and Matteo Dellepiane, "Instant sound scattering," in <i>Rendering Techniques (Proc. of the Eurographics Symposium on Rendering)</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1053460</ref_obj_id>
				<ref_obj_pid>1053427</ref_obj_pid>
				<ref_seq_no>219</ref_seq_no>
				<ref_text><![CDATA[C. Dachsbacher and M. Stamminger, "Reflective shadow map," <i>Proceedings of I3D'05</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>220</ref_seq_no>
				<ref_text><![CDATA[S. Siltanen, "Geometry reduction in room acoustics modeling," <i>Master Thesis, Helsinki University Of Technology, Department of Computer Science Telecommunications Software and Multimedia Laboratory</i>, September 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>221</ref_seq_no>
				<ref_text><![CDATA[L. M. Wang, J. Rathsam, and S. R. Ryherd, "Interactions of model detail level and scattering coefficients in room acoustic computer simulation," <i>Intl. Symp. on Room Acoustics, a satelite symposium of ICA, Kyoto, Japan</i>, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1008659</ref_obj_id>
				<ref_obj_pid>1008653</ref_obj_pid>
				<ref_seq_no>222</ref_seq_no>
				<ref_text><![CDATA[C. Joslin and N. Magnenat-Thalmann, "Significant facet retreival for real-time 3D sound rendering in complex virtual environments," <i>Proc. of VRTST 2003</i>, October 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37414</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>223</ref_seq_no>
				<ref_text><![CDATA[Robert L. Cook, Loren Carpenter, and Edwin Catmull, "The reyes image rendering architecture," <i>SIGGRAPH Comput. Graph.</i>, vol. 21, no. 4, pp. 95--102, 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280832</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>224</ref_seq_no>
				<ref_text><![CDATA[Jonathan Cohen, Marc Olano, and Dinesh Manocha, "Appearance-preserving simplification," in <i>SIGGRAPH '98: Proceedings of the 25th annual conference on Computer graphics and interactive techniques</i>, New York, NY, USA, 1998, pp. 115--122, ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>225</ref_seq_no>
				<ref_text><![CDATA[Nicolas Tsingos, Carsten Dachsbacher, Sylvain Lefebvre, and Matteo Dellepiane, "Extending geometrical acoustics to highly detailed architectural environments," in <i>19th Intl. Congress on Acoustics</i>, sep 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1006077</ref_obj_id>
				<ref_obj_pid>1006058</ref_obj_pid>
				<ref_seq_no>226</ref_seq_no>
				<ref_text><![CDATA[J. Hirche, A. Ehlert, S. Guthe, and M. Doggett, "Hardware accelerated per-pixel displacement mapping," <i>Proc. of Graphics Interface'04. Canadian Human-Computer Communications Society</i>, pp. 153--158, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1143112</ref_obj_id>
				<ref_obj_pid>1143079</ref_obj_pid>
				<ref_seq_no>227</ref_seq_no>
				<ref_text><![CDATA[Lionel Baboud and Xavier D&#233;coret, "Rendering geometry with relief textures," in <i>Graphics Interface '06</i>, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>228</ref_seq_no>
				<ref_text><![CDATA[U. P. Svensson, R. I. Fred, and J. Vanderkooy, "An analytic secondary source model of edge diffraction impulse responses," <i>J. Acoust. Soc. Am.</i>, vol. 106, pp. 2331--2344, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>229</ref_seq_no>
				<ref_text><![CDATA[U. P. Svensson, P. Calamia, and S. Nakanishi, "Frequency-domain edge diffraction for finite and infinite edges," <i>Acta Acustica united with Acustica</i>, vol. 95, no. 3, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>230</ref_seq_no>
				<ref_text><![CDATA[E. Wenzel, M. Arruda, D. Kistler, and F. Wightman, "Localization using non-individualized head-related transfer functions," <i>J. Acoustical Soc. Am.</i>, vol. 94, no. 1, pp. 111--123, July 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>231</ref_seq_no>
				<ref_text><![CDATA[D. Begault, E. Wenzel, and M. Anderson, "Direct comparison of the impact of head-tracking, reverberation, and individualized head-related transfer functions on the spatial perception of a virtual speech source," <i>J. Audio Eng. Soc.</i>, vol. 49, no. 10, pp. 904--916, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>232</ref_seq_no>
				<ref_text><![CDATA[J. C. Middlebrooks, E. A. Macpherson, and Z. A. Onsan, "Psychophysical customization of directional transfer functions for virtual sound localization," <i>Journal Acoustical Soc. Am.</i>, vol. 108, no. 6, pp. 3088--3091, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>233</ref_seq_no>
				<ref_text><![CDATA[Y. Kahana and P. A. Nelson, "Numerical modelling of the spatial acoustic response of the human pinna," <i>Journal of Sound and Vibration</i>, vol. 292, no. 1-2, pp. 148--178, Apr. 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>234</ref_seq_no>
				<ref_text><![CDATA[B. Katz, "Boundary element method calculation of individual head-related transfer function. part I: Rigid model calculation," <i>Journal Acoustical Soc. Am.</i>, vol. 110, no. 5, pp. 2440--2448, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>235</ref_seq_no>
				<ref_text><![CDATA[Matteo Dellepiane, Nico Pietroni, Nicolas Tsingos, Manuel Asselot, and Roberto Scopigno, "Reconstructing head models from photographs for individualized 3D-audio processing," in <i>Computer Graphics Forum (Special Issue - Proc. Pacific Graphics) 27(7)</i>, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1400941</ref_obj_id>
				<ref_obj_pid>1400885</ref_obj_pid>
				<ref_seq_no>236</ref_seq_no>
				<ref_text><![CDATA[J&#246;rn Loviscach, "GPU-based audio via the VGA port," in <i>SIGGRAPH '08: ACM SIGGRAPH 2008 posters</i>, New York, NY, USA, 2008, pp. 1--1, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2243157</ref_obj_id>
				<ref_obj_pid>2241276</ref_obj_pid>
				<ref_seq_no>237</ref_seq_no>
				<ref_text><![CDATA[Brian Santo and Sally Adee, "Multi-core made simpler," <i>IEEE Spectrum</i>, Jan. 2009, Available at http://www.spectrum.ieee.org/jan09/7129.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360617</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>238</ref_seq_no>
				<ref_text><![CDATA[Larry Seiler, Doug Carmean, Eric Sprangle, Tom Forsyth, Michael Abrash, Pradeep Dubey, Stephen Junkins, Adam Lake, Jeremy Sugerman, Robert Cavin, Roger Espasa, Ed Grochowski, Toni Juan, and Pat Hanrahan, "Larrabee: a many-core x86 architecture for visual computing," <i>ACM Trans. Graph.</i>, vol. 27, no. 3, pp. 1--15, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>239</ref_seq_no>
				<ref_text><![CDATA[M. Pharr, A. Lefohn, C. Kolb, P. Lalonde, T. Foley, and G. Berry, "Programmable Graphics - The Future of Interactive Rendering," Available at http://www.cerlsoundgroup.org/RealTimeMorph/, accessed March 08, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>240</ref_seq_no>
				<ref_text><![CDATA[A. J. Berkhout, D. de Vries, and P. Vogel, "Acoustic control by wave field synthesis," vol. 93, no. 5, pp. 2764--2778, may 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>241</ref_seq_no>
				<ref_text><![CDATA[Adam O'Donovan, Ramani Duraiswami, and Nail A. Gumerov, "Real time capture of audio images and their use with video," Oct. 2007, pp. 10--13.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618732</ref_obj_id>
				<ref_obj_pid>616064</ref_obj_pid>
				<ref_seq_no>242</ref_seq_no>
				<ref_text><![CDATA[M. Monks, B. M. Oh, and J. Dorsey, "Audioptimization: Goal based acoustic design," <i>IEEE Computer Graphics&amp;Applications</i>, pp. 76--91, May 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>243</ref_seq_no>
				<ref_text><![CDATA[Patrick Cardinal, Pierre Dumouchel, Gilles Boulianne, and Michel Comeau, "GPU Accelerated Acoustic Likelihood Computations," in <i>Proc. of INTERSPEECH</i>, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1539908</ref_obj_id>
				<ref_obj_pid>1539065</ref_obj_pid>
				<ref_seq_no>244</ref_seq_no>
				<ref_text><![CDATA[Paul R. Dixon and Tasuku Oonishia and Sadaoki Furuia, "Harnessing graphics processors for the fast computation of acoustic likelihoods in speech recognition," <i>Computer Speech&amp;Language</i>, vol. 23, no. 4, pp. 510--526, Oct. 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>245</ref_seq_no>
				<ref_text><![CDATA[Jike Chong, Youngmin Yi, Arlo Faria, Nadathur Rajagopalan Satish, and Kurt Keutzer, "Data-parallel large vocabulary continuous speech recognition on graphics processors," Tech. Rep. UCB/EECS-2008-69, EECS Department, University of California, Berkeley, May 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>246</ref_seq_no>
				<ref_text><![CDATA[ISO/IEC JTC1/SC29/WG11 IS 14496, "Information Technology-Coding of Multimedia Objects (MPEG-4)," (1999).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>247</ref_seq_no>
				<ref_text><![CDATA[ISO/IEC JTC/SC24 IS 14772-1, "Information Technology---Computer Graphics and Image Processing---The Virtual Reality Modeling Language (VRML97)" (1997 Apr.). URL: http://www.vrml.org/Specifications/VRML97/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>248</ref_seq_no>
				<ref_text><![CDATA[SUN, Inc., "JAVA 3D API Specification 1.1" (1998 Dec.). URL: http://java.sun.com/products/java-media/3D/forDevelopers/j3dguide/j3dTOC.doc.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>249</ref_seq_no>
				<ref_text><![CDATA[W. F. Dale, "A Machine-Independent 3D Positional Sound Application Programmer Interface to Spatial Audio Engines," in <i>Proc. AES 16th Int. Conf. on Spatial Sound Reproduction</i> (Rovaniemi, Finland, 1999 Apr.), pp. 160--171.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>250</ref_seq_no>
				<ref_text><![CDATA[Interactive Audio Special Interest Group. URL: http://www.iasig.org.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>251</ref_seq_no>
				<ref_text><![CDATA[E. Wenzel, "Spatial Sound and Sonification," presented at the International Conference on Auditory Display (ICAD'92). Also in <i>Auditory Display: Sonification, Audification, and Auditory Interface, SFI Studies in the Sciences of Complexity</i>, Proc. XVIII, G. Kramer, Ed. (Addison-Wesley, Reading, MA, 1994).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>184407</ref_obj_id>
				<ref_seq_no>252</ref_seq_no>
				<ref_text><![CDATA[D. Begault, <i>3-D Sound for Virtual Reality and Multimedia</i> (Academic Press, Cambridge, MA, 1994).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>253</ref_seq_no>
				<ref_text><![CDATA[S. Foster, E. Wenzel, and R. Taylor, "Real-Time Synthesis of Complex Acoustic Environments," in <i>Proc. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA'91)</i> (New Paltz, NY, 1991).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>254</ref_seq_no>
				<ref_text><![CDATA[B. Shinn-Cunningham, H. Lehnert, G. Kramer, E. Wenzel, and N. Durlach, "Auditory Displays," in <i>Binaural and Spatial Hearing in Real and Virtual Environments</i>, R. Gilkey and T. Anderson, Eds. (Lawrence Erlbaum, Mahwah, NJ, 1997), pp. 611--663.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>297260</ref_obj_id>
				<ref_obj_pid>297245</ref_obj_pid>
				<ref_seq_no>255</ref_seq_no>
				<ref_text><![CDATA[J.-M. Jot, "Real-Time Spatial Processing of Sounds for Music, Multimedia and Interactive Human-Computer Interfaces," <i>Multimedia Sys.</i> (Special Issue on Audio and Multimedia), vol. 7, no. 1, pp. 55--69 (1999).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>256</ref_seq_no>
				<ref_text><![CDATA[H. Lehnert and J. Blauert, "Principles of Binaural Room Simulation," <i>Appl. Acoust.</i>, vol. 36, pp. 259--291 (1992).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>257</ref_seq_no>
				<ref_text><![CDATA[J. P. Vian and J. Martin, "Binaural Room Acoustics Simulation: Practical Uses and Applications," <i>Appl. Acoust.</i>, vol. 36, pp. 293--305 (1992).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>258</ref_seq_no>
				<ref_text><![CDATA[J. Martin, D. Van Maercke, and J. P. Vian, "Binaural Simulation of Concert Halls: A New Approach for the Binaural Reverberation Process," <i>J. Acoust. Soc. Am.</i>, vol. 94, pp. 3255--3264 (1993).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>259</ref_seq_no>
				<ref_text><![CDATA[J. Huopaniemi, "Virtual Acoustics and 3-D Sound in Multimedia Signal Processing," Ph.D. thesis, Helsinki University of Technology, Laboratory of Acoustics and Audio Signal Processing (1999).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>260</ref_seq_no>
				<ref_text><![CDATA[M. Kleiner, B.-I. Dalenb&#228;ck, and P. Svensson, "Auralization---An Overview," <i>J. Audio Eng. Soc.</i>, vol. 41, pp. 861--875 (1993 Nov.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>261</ref_seq_no>
				<ref_text><![CDATA[A. Krokstad, S. Str&#248;m, and S. S&#248;rsdal, "Calculating the Acoustical Room Response by the Use of a Ray Tracing Technique," <i>J. Sound Vib.</i>, vol. 8, pp. 118--125 (1968).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>262</ref_seq_no>
				<ref_text><![CDATA[A. Kulowski, "Algorithmic Representation of the Ray Tracing Technique," <i>Appl. Acoust.</i>, vol. 18, pp. 449--469 (1985).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>263</ref_seq_no>
				<ref_text><![CDATA[J. B. Allen and D. A. Berkley, "Image Method for Efficiently Simulating Small-Room Acoustics," <i>J. Acoust. Soc. Am.</i>, vol. 65, pp. 943--950 (1979).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>264</ref_seq_no>
				<ref_text><![CDATA[J. Borish, "Extension of the Image Model to Arbitrary Polyhedra," <i>J. Acoust. Soc. Am.</i>, vol. 75, pp. 1827--1836 (1984).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>265</ref_seq_no>
				<ref_text><![CDATA[H. Kuttruff, "Sound Field Prediction in Rooms," in <i>Proc. 15th Int. Congr. on Acoustics (ICA'95)</i>, vol. 2 (Trondheim, Norway, 1995 June), pp. 545--552.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>266</ref_seq_no>
				<ref_text><![CDATA[D. Botteldooren, "Finite-Difference Time-Domain Simulation of Low-Frequency Room Acoustic Problems," <i>J. Ac&#242;ust. Soc. Am.</i>, vol. 98, pp. 3302--3308 (1995).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>267</ref_seq_no>
				<ref_text><![CDATA[L. Savioja, J. Backman, A. J&#228;rvinen, and T. Takala, "Waveguide Mesh Method for Low-Frequency Simulation of Room Acoustics," in <i>Proc. 15th Int. Congr. on Acoustics (ICA'95)</i>, vol. 2 (Trondheim, Norway, 1995 June), pp. 637--640.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>268</ref_seq_no>
				<ref_text><![CDATA[M. R. Schroeder, "Natural-Sounding Artificial Reverberation," <i>J. Audio Eng. Soc.</i>, vol. 10, pp. 219--223 (1962).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>269</ref_seq_no>
				<ref_text><![CDATA[J. A. Moorer, "About This Reverberation Business," <i>Comput. Music J.</i>, vol. 3, pp. 13--28 (1979).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>270</ref_seq_no>
				<ref_text><![CDATA[W. Gardner, "Reverberation Algorithms," in <i>Applications of Digital Signal Processing to Audio and Acoustics</i>, M. Kahrs and K. Brandenburg, Eds. (Kluwer Academic, Boston, MA, 1997), pp. 85--131.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>271</ref_seq_no>
				<ref_text><![CDATA[J. Blauert, <i>Spatial Hearing. The Psychophysics of Human Sound Localization</i>, 2nd ed. (MIT Press, Cambridge, MA, 1997).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>272</ref_seq_no>
				<ref_text><![CDATA[H. M&#248;ller, "Fundamentals of Binaural Technology," <i>Appl. Acoust.</i>, vol. 36, pp. 171--218 (1992).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>273</ref_seq_no>
				<ref_text><![CDATA[G. Kendall, "A 3-D Sound Primer: Directional Hearing and Stereo Reproduction," <i>Comput. Music J.</i>, vol. 19, no. 4, pp. 23--46 (1995 Winter).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>274</ref_seq_no>
				<ref_text><![CDATA[F. L. Wightman and D. J. Kistler, "Headphone Simulation of Free-Field Listening. I: Stimulus Synthesis," <i>J. Acoust. Soc. Am.</i>, vol. 85, pp. 858--867 (1989).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>275</ref_seq_no>
				<ref_text><![CDATA[H. M&#248;ller, M. F. S&#248;rensen, D. Hammersh&#248;i, and C. B. Jensen, "Head-Related Transfer Functions of Human Subjects," <i>J. Audio Eng. Soc.</i>, vol. 43, pp. 300--321 (1995 May).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>276</ref_seq_no>
				<ref_text><![CDATA[G. S. Kendall and W. L. Martens, "Simulating the Cues of Spatial Hearing in Natural Environments," in <i>Proc. 1984 Int. Computer Music Conf.</i> (Paris, France, 1984), pp. 111--125.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>277</ref_seq_no>
				<ref_text><![CDATA[F. Asano, Y. Suzuki, and T. Sone, "Role of Spectral Cues in Median Plane Localization," <i>J. Acoust. Soc. Am.</i>, vol. 88, pp. 159--168 (1990).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>278</ref_seq_no>
				<ref_text><![CDATA[D. J. Kistler and F. L. Wightman, "A Model of Head-Related Transfer Functions Based on Principal Components Analysis and Minimum-Phase Reconstruction," <i>J. Acoust. Soc. Am.</i>, vol. 91, pp. 1637--1647 (1992).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>279</ref_seq_no>
				<ref_text><![CDATA[M. A. Blommer and G. H. Wakefield, "On the Design of Pole-Zero Approximations Using a Logarithmic Error Measure," <i>IEEE Trans. Signal Process.</i>, vol. 42, pp. 3245--3248 (1994 Nov.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>280</ref_seq_no>
				<ref_text><![CDATA[J. Sandvad and D. Hammersh&#248;i, "Binaural Auralization: Comparison of FIR and IIR Filter Representation of HIRs," presented at the 96th Convention of the Audio Engineering Society, <i>J. Audio Eng. Soc. (Abstracts)</i>, vol. 42, p. 395 (1994 May), preprint 3862.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>281</ref_seq_no>
				<ref_text><![CDATA[J. M. Jot, O. Warusfel, and V. Larcher, "Digital Signal Processing Issues in the Context of Binaural and Transaural Sterephony," presented at the 98th Convention of the Audio Engineering Society, <i>J. Audio Eng. Soc. (Abstracts)</i>, vol. 43, p. 396 (1995 May), preprint 3980.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>282</ref_seq_no>
				<ref_text><![CDATA[J. Huopaniemi, N. Zacharov, and M. Karjalainen, "Objective and Subjective Evaluation of Head-Related Transfer Function Filter Design," <i>J. Audio Eng. Soc.</i>, vol. 47, pp. 218--239 (1999 Apr.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>283</ref_seq_no>
				<ref_text><![CDATA[M. Schroeder and B. Atal, "Computer Simulation of Sound Transmission in Rooms," in <i>IEEE Conv. Rec.</i>, pt. 7 (1963), pp. 150--155.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>284</ref_seq_no>
				<ref_text><![CDATA[D. H. Cooper and J. L. Bauck, "Prospects for Transaural Recording," <i>J. Audio Eng. Soc.</i>, vol. 37, pp. 3--19 (1989 Jan./Feb.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>285</ref_seq_no>
				<ref_text><![CDATA[K. B. Rasmussen and P. M. Juhl, "The Effect of Head Shape on Spectral Stereo Theory," <i>J. Audio Eng. Soc.</i>, vol. 41, pp. 135--142 (1993 Mar.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>286</ref_seq_no>
				<ref_text><![CDATA[W. Gardner, "Transaural 3-D Audio," MIT Media Lab Perceptual Computing, Tech. Rep. 342 (1995).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>287</ref_seq_no>
				<ref_text><![CDATA[M. J. Walsh and D. J. Furlong, "Improved Spectral Stereo Head Model," presented at the 99th Convention of the Audio Engineering Society, <i>J. Audio Eng. Soc. (Abstracts)</i>, vol. 43, p. 1093 (1995 Dec.), preprint 4128.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>288</ref_seq_no>
				<ref_text><![CDATA[M. A. Gerzon, "Periphony: With-Height Sound Reproduction," <i>J. Audio Eng. Soc.</i>, vol. 21, pp. 2--10 (1973 Jan./Feb.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>289</ref_seq_no>
				<ref_text><![CDATA[D. Malham and A. Myatt, "3-D Sound Spatialization Using Ambisonic Techniques," <i>Comput. Music J.</i>, vol. 19, no. 4, pp. 58--70 (1995).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>290</ref_seq_no>
				<ref_text><![CDATA[V. Pulkki, "Virtual Sound Source Positioning Using Vector Base Amplitude Panning," <i>J. Audio Eng. Soc.</i>, vol. 45, pp. 456--466 (1997 June).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>291</ref_seq_no>
				<ref_text><![CDATA[T. Takala, R. H&#228;nninen, V. V&#228;lim&#228;ki, L. Savioja, J. Huopaniemi, T. Huotilainen, and M. Karjalainen, "An Integrated System for Virtual Audio Reality," presented at the 100th Convention of the Audio Engineering Society, <i>J. Audio Eng. Soc. (Abstracts)</i>, vol. 44, p. 644 (1996 July/Aug), preprint 4229.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>259178</ref_obj_id>
				<ref_obj_pid>259081</ref_obj_pid>
				<ref_seq_no>292</ref_seq_no>
				<ref_text><![CDATA[DIVA Group: J. Hiipakka, R. H&#228;nninen, T. Ilmonen, H. Napari, T. Lokki, L. Savioja, J. Huopaniemi, M. Karjalainen, T. Tolonen, V. V&#228;lim&#228;ki, S. V&#228;lim&#228;ki, and T. Takala, "Virtual Orchestra Performance," in <i>Visual Proc. of SIGGRAPH'97</i> (Los Angeles, CA, 1997), p. 81, ACM SIGGRAPH.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>972999</ref_obj_id>
				<ref_obj_pid>972993</ref_obj_pid>
				<ref_seq_no>293</ref_seq_no>
				<ref_text><![CDATA[T. Lokki, J. Hiipakka, R. H&#228;nninen, T. Ilmonen, L. Savioja, and T. Takala, "Real-Time Audiovisual Rendering and Contemporary Audiovisual Art," <i>Organised Sound</i>, vol. 3, no. 3 (1999).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134063</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>294</ref_seq_no>
				<ref_text><![CDATA[T. Takala and J. Hahn, "Sound Rendering," Comput. Graphics, SIGGRAPH'92, no. 26, pp. 211--220 (1992).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>295</ref_seq_no>
				<ref_text><![CDATA[J. Hahn, J. Geigel, J. W. Lee, L. Gritz, T. Takala, and S. Mishra, "An Integrated Approach to Sound and Motion," <i>J. Visualiz. and Comput. Animation</i>, vol. 6, no. 2, pp. 109--123 (1995).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>296</ref_seq_no>
				<ref_text><![CDATA[T. Ilmonen, "Tracking Conductor of an Orchestra Using Artificial Neural Networks," Master's thesis, Helsinki University of Technology (1999).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>297</ref_seq_no>
				<ref_text><![CDATA[R. H&#228;nninen, "LibR---An Object-Oriented Software Architecture for Realtime Sound and Kinematics," Licentiate thesis, Helsinki University of Technology (1999).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>298</ref_seq_no>
				<ref_text><![CDATA[ISO/IEC JTC1/SC29/WG11 IS 14496-3 (MPEG-4), "Information Technology---Coding of Audiovisual Objects. Part 3: Audio" (1999).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>299</ref_seq_no>
				<ref_text><![CDATA[K. Brandenburg and M. Bosi, "Overview of MPEG Audio: Current and Future Standards for Low-Bit-Rate Audio Coding," <i>J. Audio Eng. Soc.</i>, vol. 45, pp. 4--21 (1997 Jan./Feb.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>300</ref_seq_no>
				<ref_text><![CDATA[J. O. Smith, "Physical Modeling Synthesis Update," <i>Comput. Music J.</i>, vol. 20, pp. 44--56 (1996 Summer).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>301</ref_seq_no>
				<ref_text><![CDATA[J. Huopaniemi, M. Karjalainen, V. V&#228;lim&#228;ki, and T. Huotilainen, "Virtual Instruments in Virtual Rooms---A Real-Time Binaural Room Simulation Environment for Physical Models of Musical Instruments," in <i>Proc. Int. Computer Music Conf. (ICMC'94)</i> (Aarhus, Denmark, 1994 Sept.), pp. 455--462.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>302</ref_seq_no>
				<ref_text><![CDATA[M. Karjalainen, J. Huopaniemi, and V. V&#228;lim&#228;ki, "Direction-Dependent Physical Modeling of Musical Instruments," in <i>Proc. 15th Int. Congr. on Acoustics (ICA'95)</i> (Trondheim, Norway, 1995 June), pp. 451--454.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>303</ref_seq_no>
				<ref_text><![CDATA[J. Meyer, <i>Acoustics and the Performance of Music</i> (Verlag das Musikinstrument, Frankfurt/Main, Germany, 1978).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>304</ref_seq_no>
				<ref_text><![CDATA[N. H. Fletcher and T. D. Rossing, <i>The Physics of Musical Instruments</i> (Springer, New York, 1991).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>305</ref_seq_no>
				<ref_text><![CDATA[J. Flanagan, "Analog Measurements of Sound Radiation from the Mouth," <i>J. Acoust. Soc. Am.</i>, vol. 32, pp. 1613--1620 (1960 Dec.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>306</ref_seq_no>
				<ref_text><![CDATA[J. Huopaniemi, K. Kettunen, and J. Rahkonen, "Measurement and Modeling Techniques for Directional Sound Radiation from the Mouth," in <i>Proc. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA'99)</i> (New Paltz, NY, 1999 Oct.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>307</ref_seq_no>
				<ref_text><![CDATA[D. H. Cooper, "Calculator Program for Head-Related Transfer Function," <i>J. Audio Eng. Soc. (Personal Calculator Programs)</i>, vol. 30, pp. 34--38 (1982 Jan./Feb.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>308</ref_seq_no>
				<ref_text><![CDATA[W. M. Rabinowitz, J. Maxwell, Y. Shao, and M. Wei, "Sound Localization Cues for a Magnified Head: Implications from Sound Diffraction about a Rigid Sphere," <i>Presence</i>, vol. 2, no. 2, pp. 125--129 (1993).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>309</ref_seq_no>
				<ref_text><![CDATA[R. Duda and W. Martens, "Range-Dependence of the HRTF of a Spherical Head," <i>J. Acoust. Soc. Am.</i>, vol. 104, pp. 3048--3058 (1998 Nov.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>310</ref_seq_no>
				<ref_text><![CDATA[M. R. Schroeder, "Digital Simulation of Sound Transmission in Reverberant Spaces," <i>J. Acoust. Soc. Am.</i>, vol. 47, no. 2, pt. 1, pp. 424--431 (1970).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>311</ref_seq_no>
				<ref_text><![CDATA[M. R. Schroeder, "Computer Models for Concert Hall Acoustics," <i>Am. J. Phys.</i>, vol. 41, pp. 461--471 (1973).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>312</ref_seq_no>
				<ref_text><![CDATA[A. Pietrzyk, "Computer Modeling of the Sound Field in Small Rooms," in <i>Proc. AES 15th Int. Conf. on Audio Acoustics and Small Spaces</i> (Copenhagen, Denmark, 1998 Oct. 31--Nov. 2), pp. 24--31.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>313</ref_seq_no>
				<ref_text><![CDATA[H. Kuttruff, <i>Room Acoustics</i>, 3rd ed. (Elsevier, Essex, UK, 1991).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>314</ref_seq_no>
				<ref_text><![CDATA[R. Lyon and R. DeJong, <i>Theory and Application of Statistical Energy Analysis</i>, 2nd ed. (Butterworth-Heinemann, Newton, MA, 1995).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>315</ref_seq_no>
				<ref_text><![CDATA[D. Jaffe and J. O. Smith, "Extensions of the Karplus-Strong Plucked String Algorithm," <i>Comput. Music J.</i>, vol. 7, no. 2, pp. 56--69 (1983 Summer). reprinted in <i>The Music Machine</i>, C. Roads, Ed. (MIT Press, Cambridge, MA, 1989), pp. 481--494.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>316</ref_seq_no>
				<ref_text><![CDATA[J. O. Smith, "Physical Modeling Using Digital Waveguides," <i>Comput. Music J.</i>, vol. 16, no. 4, pp. 74--87 (1992 Winter).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>973105</ref_obj_id>
				<ref_obj_pid>973103</ref_obj_pid>
				<ref_seq_no>317</ref_seq_no>
				<ref_text><![CDATA[V. V&#228;lim&#228;ki and T. Takala, "Virtual Musical Instruments---Natural Sound Using Physical Models," <i>Organised Sound</i>, vol. 1, no. 2, pp. 75--86 (1996).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>318</ref_seq_no>
				<ref_text><![CDATA[J. O. Smith, "Principles of Digital Waveguide Models of Musical Instruments," in <i>Applications of Digital Signal Processing to Audio and Acoustics</i>, M. Kahrs and K. Brandenburg, Eds. (Kluwer Academic, Boston, MA, 1997), chap. 10, pp. 417--466.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>319</ref_seq_no>
				<ref_text><![CDATA[S. Van Duyne and J. O. Smith, "Physical Modeling with the 2-D Digital Waveguide Mesh," in <i>Proc. Int. Computer Music Conf. (ICMC'93)</i> (Tokyo, Japan, 1993 Sept.), pp. 40--47.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>320</ref_seq_no>
				<ref_text><![CDATA[L. Savioja, M. Karjalainen, and T. Takala, "DSP Formulation of a Finite Difference Method for Room Acoustics Simulation," in <i>Proc. IEEE Nordic Signal Processing Symp. (NORSIG'96)</i> (Espoo, Finland, 1996 Sept.), pp. 455--458.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>321</ref_seq_no>
				<ref_text><![CDATA[S. Van Duyne and J. O. Smith, "The Tetrahedral Digital Waveguide Mesh," in <i>Proc. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA'95)</i> (New Paltz, NY, 1995 Oct.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>322</ref_seq_no>
				<ref_text><![CDATA[F. Fontana and D. Rocchesso, "Physical Modeling of Membranes for Percussion Instruments," <i>Acustica</i> united with <i>Acta Acustica</i>, vol. 84, pp. 529--542 (1998 May/June).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>844506</ref_obj_id>
				<ref_obj_pid>844378</ref_obj_pid>
				<ref_seq_no>323</ref_seq_no>
				<ref_text><![CDATA[L. Savioja and V. V&#228;lim&#228;ki, "Improved Discrete-Time Modeling of Multi-Dimensional Wave Propagation Using the Interpolated Digital Waveguide Mesh," in <i>Proc. Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP'97)</i>, vol. 1 (Munich, Germany, 1997 Apr. 19--24), pp. 459--462.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>324</ref_seq_no>
				<ref_text><![CDATA[L. Savioja and V. V&#228;lim&#228;ki, "Reduction of the Dispersion Error in the Triangular Digital Waveguide Mesh Using Frequency Warping," <i>IEEE Signal Process. Lett.</i>, vol. 6, no. 3, pp. 58--60 (1999 Mar.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1257583</ref_obj_id>
				<ref_obj_pid>1257295</ref_obj_pid>
				<ref_seq_no>325</ref_seq_no>
				<ref_text><![CDATA[L. Savioja and V. V&#228;lim&#228;ki, "Reduction of the Dispersion Error in the Interpolated Digital Waveguide Mesh Using Frequency Warping," in <i>Proc. Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP'99)</i>, vol. 2 (Phoenix, AZ, 1999 Mar. 15--19), pp. 973--976.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>326</ref_seq_no>
				<ref_text><![CDATA[A. Kulowski, "Error Investigation for the Ray Tracing Technique," <i>Appl. Acoust.</i>, vol. 15, pp. 263--274 (1982).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>327</ref_seq_no>
				<ref_text><![CDATA[D. van Maercke and J. Martin, "The Prediction of Echograms and Impulse Responses within the Epidaure Software," <i>Appl. Acoust.</i>, vol. 38, no. 2--4 (Special Issue on Computer Modelling and Auralisation of Sound Fields in Rooms), pp. 93--114 (1993).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>328</ref_seq_no>
				<ref_text><![CDATA[G. M. Naylor, "ODEON---Another Hybrid Room Acoustical Model," <i>Appl. Acoust.</i>, vol. 38, no. 2--4 (Special Issue on Computer Modelling and Auralisation of Sound Fields in Rooms), pp. 131--143 (1993).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>329</ref_seq_no>
				<ref_text><![CDATA[B. M. Gibbs and D. K. Jones, "A Simple Image Method for Calculating the Distribution of Sound Pressure Levels within an Enclosure," <i>Acustica</i>, vol. 26, no. 1, pp. 24--32 (1972).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>330</ref_seq_no>
				<ref_text><![CDATA[H. Lee and B. H. Lee, "An Efficient Algorithm for the Image Model Technique," <i>Appl. Acoust.</i>, vol. 24, pp. 87--115 (1988).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>331</ref_seq_no>
				<ref_text><![CDATA[R. Heinz, "Binaural Room Simulation Based on an Image Source Model with Addition of Statistical Methods to Include the Diffuse Sound Scattering of Walls and to Predict the Reverberant Tail," <i>Appl. Acoust.</i>, vol. 38, no. 2--4 (Special Issue on Computer Modelling and Auralisation of Sound Fields in Rooms), pp. 145--159 (1993).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>332</ref_seq_no>
				<ref_text><![CDATA[D. van Maercke, "Simulation of Sound Fields in Time and Frequency Domain Using a Geometrical Model," in <i>Proc. 12th Int. Congr. on Acoustics (ICA'86)</i>, vol. 2 (Toronto, Ont., Canada, 1986 July), paper E11--7.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>333</ref_seq_no>
				<ref_text><![CDATA[M. Vorl&#228;nder, "Simulation of the Transient and Steady-State Sound Propagation in Rooms Using a New Combined Ray-Tracing/Image-Source Algorithm," <i>J. Acoust. Soc. Am.</i>, vol. 86, pp. 172--178 (1989).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>334</ref_seq_no>
				<ref_text><![CDATA[F. R. Moore, "A General Model for Spatial Processing of Sounds," <i>Comput. Music J.</i>, vol. 7, no. 3, pp. 6--15 (1983 Fall).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>335</ref_seq_no>
				<ref_text><![CDATA[M. Tamminen, "The EXCELL Method for Efficient Geometric Access to Data," <i>Acta Polytechnica Scandinavica, Math. and Comput. Sci. Ser.</i>, no. 34 (1981).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>77589</ref_obj_id>
				<ref_seq_no>336</ref_seq_no>
				<ref_text><![CDATA[H. Samet, <i>The Design and Analysis of Spatial Data Structures</i> (Addison-Wesley, Reading, MA, 1990).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>337</ref_seq_no>
				<ref_text><![CDATA[H. Bass and H. J. Bauer, "Atmospheric Absorption of Sound: Analytical Expressions," <i>J. Acoust. Soc. Am.</i>, vol. 52, pp. 821--825 (1972).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>338</ref_seq_no>
				<ref_text><![CDATA[ISO 9613--1, "Acoustics---Attenuation of Sound during Propagation Outdoors---Part I: Calculation of the Absorption of Sound by the Atmosphere," International Standards Organization, Geneva, Switzerland (1993).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>339</ref_seq_no>
				<ref_text><![CDATA[J. Huopaniemi, L. Savioja, and M. Karjalainen, "Modeling of Reflections and Air Absorption in Acoustical Spaces---A Digital Filter Design Approach," in <i>Proc. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA'97)</i> (New Paltz, NY, 1997 Oct. 19--22).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>340</ref_seq_no>
				<ref_text><![CDATA[G. Naylor and J. Rindel, <i>Odeon Room Acoustics Program, Version 2.5, User Manual</i>, Technical University of Denmark, Acoustics Laboratory, Publ. 49 (1994).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>341</ref_seq_no>
				<ref_text><![CDATA[T. Lahti and H. M&#248;ller, "The Sigyn Hall, Turku---A Concert Hall of Glass," in <i>Proc. Nordic Acoustical Meeting (NAM'96)</i> (Helsinki, Finland, 1996 June), pp. 43--48.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>342</ref_seq_no>
				<ref_text><![CDATA[J.-M. Jot, "Etude et r&#233;alisation d'un spatialisateur de sons par mod&#232;les physique et perceptifs," Ph.D. thesis, Ecole Nationale Sup&#233;rieure des T&#233;l&#233;communications, T&#233;l&#233;com Paris 92 E 019 (1992 Sept.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>343</ref_seq_no>
				<ref_text><![CDATA[M. R. Schroeder, "An Artificial Sterephonic Effect Obtained from a Single Audio Signal," <i>J. Audio Eng. Soc.</i>, vol. 6, pp. 74--79 (1985 Apr.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>344</ref_seq_no>
				<ref_text><![CDATA[J. Stautner and M. Puckette, "Designing Multi-Channel Reverberators," <i>Comput. Music J.</i>, vol. 6, pp. 569--579 (1982).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>345</ref_seq_no>
				<ref_text><![CDATA[R. Vermeulen, "Stereo-Reverberation," <i>J. Audio Eng. Soc.</i>, vol. 6, pp. 124--130 (1958 Apr.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>346</ref_seq_no>
				<ref_text><![CDATA[W. Gardner, "Virtual Acoustic Room," Master's thesis, MIT, Cambridge, MA (1992).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>347</ref_seq_no>
				<ref_text><![CDATA[D. Rocchesso and J. O. Smith, "Circulant and Elliptic Feedback Delay Networks for Artificial Reverberation," <i>IEEE Trans. Speech Audio Process.</i>, vol. 5, pp. 51--63 (1997 Jan.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>348</ref_seq_no>
				<ref_text><![CDATA[R. V&#228;&#228;n&#228;nen, V. V&#228;lim&#228;ki, and J. Huopaniemi, "Efficient and Parametric Reverberator for Room Acoustics Modeling," in <i>Proc. Int. Computer Music Conf. (ICMC'97)</i> (Thessaloniki, Greece, 1997 Sept.), pp. 200--203.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>349</ref_seq_no>
				<ref_text><![CDATA[K. A. Riederer, "Repeatability Analysis of Head-Related Transfer Function Measurements," presented at the 105th Convention of the Audio Engineering Society, <i>J. Audio Eng. Soc. (Abstracts)</i>, vol. 46, p. 1036 (1998 Nov.), preprint 4846.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>350</ref_seq_no>
				<ref_text><![CDATA[W. Gardner and K. Martin, "HRTF Measurements of a KEMAR," <i>J. Acoust. Soc. Am.</i>, vol. 97, pp. 3907--3908 (1995).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>351</ref_seq_no>
				<ref_text><![CDATA[W. Gardner, "3-D Audio Using Loudspeakers," Ph.D. thesis, MIT Media Lab., Cambridge, MA (1997 Sept.). Revised version published by Kluwer Academic, Boston, MA (1998).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>352</ref_seq_no>
				<ref_text><![CDATA[W. Martens, "Principal Components Analysis and Resynthesis of Spectral Cues to Perceived Direction," in <i>Proc. Int. Computer Music Conf. (ICMC'87)</i> (1987), pp. 274--281).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>353</ref_seq_no>
				<ref_text><![CDATA[J. Abel and S. Foster, "Method and Apparatus for Efficient Presentation of High-Quality Three-Dimensional Audio including Ambient Effects," US patent 5,802,180 (1998 Sept.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>354</ref_seq_no>
				<ref_text><![CDATA[S. Mehrgardt and V. Mellert, "Transformation Characteristics of the External Human Ear," <i>J. Acoust. Soc. Am.</i>, vol. 61, pp. 1567--1576 (1977).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>355</ref_seq_no>
				<ref_text><![CDATA[A. Kulkarni, S. K. Isabelle, and H. S. Colburn, "On the Minimum-Phase Approximation of Head-Related Transfer Functions," in <i>Proc. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA'95)</i> (New Paltz, NY, 1995 Oct.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>356</ref_seq_no>
				<ref_text><![CDATA[J. K&#246;ring and A. Schmitz, "Simplifying Cancellation of Cross-Talk for Playback of Head-Related Recordings in a Two-Speaker System," <i>Acustica</i> united with <i>Acta Acustica</i>, vol. 179, pp. 221--232 (19930.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>357</ref_seq_no>
				<ref_text><![CDATA[A. Kulkarni and H. S. Colburn, "Efficient Finite-Impulse-Response Filter Models of the Head-Related Transfer Function," <i>J. Acoust. Soc. Am.</i>, vol. 97, p. 3278 (1995).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>358</ref_seq_no>
				<ref_text><![CDATA[A. Kulkarni and H. S. Colburn, "Infinite-Impulse-Response Filter Models of the Head-Related Transfer Function," <i>J. Acoust. Soc. Am.</i>, vol. 97, p. 3278 (1995).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>359</ref_seq_no>
				<ref_text><![CDATA[J. Huopaniemi and M. Karjalainen, "HRTF Filter Design Based on Auditory Criteria," in <i>Proc. Nordic Acoustical Meeting (NAM'96)</i> (Helsinki, Finland, 1996).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>360</ref_seq_no>
				<ref_text><![CDATA[K. Hartung and A. Raab, "Efficient Modeling of Head-Related Transfer Functions," <i>Acta Informatica</i>, vol. 82 (suppl. 1), S88 (1996).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>361</ref_seq_no>
				<ref_text><![CDATA[J. Mackenzie, J. Huopaniemi, V. V&#228;lim&#228;ki, and I. Kale, "Low-Order Modelling of Head-Related Transfer Functions Using Balanced Model Truncation," <i>IEEE Signal Process. Lett.</i>, vol. 4, no. 2, pp. 39--41 (1997 Feb.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>362</ref_seq_no>
				<ref_text><![CDATA[J. Huopaniemi and M. Karjalainen, "Review of Digital Filter Design and Implementation Methods for 3-D Sound," presented at the 102nd Convention of the Audio Engineering Society, <i>J. Audio Eng. Soc. (Abstracts)</i>, vol. 45, p. 413 (1997 May), preprint 4461.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>844497</ref_obj_id>
				<ref_obj_pid>844378</ref_obj_pid>
				<ref_seq_no>363</ref_seq_no>
				<ref_text><![CDATA[S. Wu and W. Putnam, "Minimum Perceptual Spectral Distance FIR Filter Design," in <i>Proc. Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP'97)</i>, vol. 1 (Los Alamitos, CA, 1997), pp. 447--450. (Institute of Electrical and Electronics Engineers, IEEE Computer Society Press).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>364</ref_seq_no>
				<ref_text><![CDATA[J. Huopaniemi and J. O. Smith, "Spectral and Time-Domain Preprocessing and the Choice of Modeling Error Criteria for Binaural Digital Filters," in <i>Proc. AES 16th Int. Conf. on Spatial Sound Reproduction</i> (Rovaniemi, Finland, 1999 Apr.), pp. 301--312.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1209342</ref_obj_id>
				<ref_seq_no>365</ref_seq_no>
				<ref_text><![CDATA[E. Zwicker and H. Fastl, <i>Psychoacoustics: Facts and Models</i> (Springer, Heidelberg, Germany, 1990).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>366</ref_seq_no>
				<ref_text><![CDATA[B. Moore and B. Glasberg, "Suggested Formulae for Calculating Auditory-Filter Bandwidths and Excitation Patterns," <i>J. Acoust. Soc. Am.</i>, vol. 74, pp. 750--753 (1983 Sept.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>367</ref_seq_no>
				<ref_text><![CDATA[J. O. Smith, "Techniques for Digital Filter Design and System Identification with Application to the Violin," Ph.D. thesis, Stanford University, Stanford, CA (1983 June).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>368</ref_seq_no>
				<ref_text><![CDATA[V. Larcher and J. M. Jot, "Techniques d'interpolation de filtres audionum&#233;riques: Application &#224; la reproduction spatiale des sons sur &#233;couteurs," in <i>Proc. CFA: Congr&#232;s Fran&#231;ais d'Acoustique</i> (1997 Apr.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>369</ref_seq_no>
				<ref_text><![CDATA[M. A. Gerzon, "Panpot Laws for Multispeaker Stereo," presented at the 92nd Convention of the Audio Engineering Society, <i>J. Audio Eng. Soc. (Abstracts)</i>, vol. 40, p. 447 (1992 May), preprint 3309.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2227628</ref_obj_id>
				<ref_obj_pid>2227605</ref_obj_pid>
				<ref_seq_no>370</ref_seq_no>
				<ref_text><![CDATA[V. Pulkki and T. Lokki, "Creating Auditory Displays with Multiple Loudspeakers Using VBAP: A Case Study with DIVA Project," in <i>Proc. Int. Conf. on Auditory Display (ICAD'98)</i> (Glasgow, UK, 1998 Nov. 1--4).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>371</ref_seq_no>
				<ref_text><![CDATA[W. G. Gardner, "Efficient Convolution without Input-Output Delay," <i>J. Audio Eng. Soc.</i>, vol. 43, pp. 127--136 (1994).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>372</ref_seq_no>
				<ref_text><![CDATA[J. Sandvad, "Dynamic Aspects of Auditory Virtual Environments," presented at the 100th Convention of the Audio Engineering Society, <i>J. Audio Eng. Soc. (Abstracts)</i>, vol. 44, p. 644 (1996 July/Aug.), preprint 4226.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>373</ref_seq_no>
				<ref_text><![CDATA[T. I. Laakso, V. V&#228;lim&#228;ki, M. Karjalainen, and U. K. Laine, "Splitting the Unit Delay---Tools for Fractional Delay Filter Design," <i>IEEE Signal Process. Mag.</i>, vol. 13, no. 1, pp. 30--60 (1996 Jan.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>374</ref_seq_no>
				<ref_text><![CDATA[V. V&#228;lim&#228;ki, M. Karjalainen, Z. Janosy, and U. K. Laine, "A Real-Time DSP Implementation of a Flute Model," in <i>Proc. 1992 IEEE Int. Conf. on Acoustics, Speech, and Signal Processing</i>, vol. 2 (San Francisco, CA, 1992 Mar.), pp. 249--252.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>375</ref_seq_no>
				<ref_text><![CDATA[V. V&#228;lim&#228;ki, J. Huopaniemi, M. Karjalainen, and Z. Janosy, "Physical Modeling of Plucked String Instruments with Application to Real-Time Sound Synthesis," <i>J. Audio Eng. Soc.</i>, vol. 44, pp. 331--353 (1996 May).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>376</ref_seq_no>
				<ref_text><![CDATA[E. Wenzel, "Analysis of the Role of Update Rate and System Latency in Interactive Virtual Acoustic Environments," presented at the 103rd Convention of the Audio Engineering Society, <i>J. Audio Eng. Soc. (Abstracts)</i>, vol. 45, pp. 1017, 1018 (1997 Nov.), preprint 4633.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>377</ref_seq_no>
				<ref_text><![CDATA[E. Wenzel, "Effect of Increasing System Latency on Localization of Virtual Sounds," in <i>Proc. AES 16th Int. Conf. on Spatial Sound Reproduction</i> (Rovaniemi, Finland, 1999 Apr.), pp. 42--50.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>378</ref_seq_no>
				<ref_text><![CDATA[Deutsche Telekom, "Investigations on Tolerable Asynchronism between Audio and Video," Doc. 11A/DTAG1, Question ITU-R 35--2/11 (1995 Apr.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>379</ref_seq_no>
				<ref_text><![CDATA[M. P. Hollier and A. N. Rimell, "An Experimental Investigation into Multi-Modal Synchronisation Sensitivity for Perceptual Model Development," presented at the 105th Convention of the Audio Engineering Society, <i>J. Audio Eng. Soc. (Abstracts)</i>, vol. 46, p. 1033 (1998 Nov.), preprint 4790.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>380</ref_seq_no>
				<ref_text><![CDATA[H. M&#246;ller and T. Lahti, "Acoustical Design of the Marienkirche Concert Hall, Neubrandenburg" (Abstract), <i>J. Acoust. Soc. Am.</i>, vol. 105, pp. 928--929 (1999 Feb.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>281934</ref_obj_id>
				<ref_obj_pid>281388</ref_obj_pid>
				<ref_seq_no>381</ref_seq_no>
				<ref_text><![CDATA[T. Takala, E. Rousku, T. Lokki, L. Savioja, J. Huopaniemi, R. V&#228;&#228;n&#228;nen, V. Pulkki, and P. Salminen, "Marienkirche---A Visual and Aural Demonstration Film," in <i>Electronic Art and Animation Catalogue (SIGGRAPH'98)</i> (Orlando, FL, 1998 July 19--24), p. 149. Presented at SIGGRAPH'98 Computer Animation Festival (Electronic Theater).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>382</ref_seq_no>
				<ref_text><![CDATA[L. Beranek, <i>Concert and Opera Halls---How They Sound</i> (Acoustical Society of America, New York, 1996).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>383</ref_seq_no>
				<ref_text><![CDATA[E. M. Wenzel, "What Perception Implies about Implementation of Interactive Virtual Acoustic Environments," presented at the 101st Convention of the Audio Engineering Society, <i>J. Audio Eng. Soc. (Abstracts)</i>, vol. 44, p. 1165 (1996 Dec.), preprint 4353.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>129892</ref_obj_id>
				<ref_obj_pid>129888</ref_obj_pid>
				<ref_seq_no>384</ref_seq_no>
				<ref_text><![CDATA[C. Cruz-Neira, D. Sandin, T. DeFanti, R. Kenyon, and J. Hart, "The Cave---Audio Visual Experience Automatic Virtual Environment," <i>Commun. ACM</i>, vol. 35, no. 6, pp. 64--72 (1992 June).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>385</ref_seq_no>
				<ref_text><![CDATA[ANSI. 1978. American national standard method for the calculation of the absorption of sound by the atmosphere. <i>ANSI S1.26--1978, American Institute of Physics (for Acoustical Society of America), New York.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>386</ref_seq_no>
				<ref_text><![CDATA[Begault, D. R., McClain, B. U., and Anderson, M. R. 2001. Early reflection thresholds for virtual sound sources. In <i>Proc. 2001 Int. Workshop on Spatial Media.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>184407</ref_obj_id>
				<ref_seq_no>387</ref_seq_no>
				<ref_text><![CDATA[Begault, D. R. 1994. <i>3D Sound for Virtual Reality and Multimedia.</i> Academic Press Professional.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>388</ref_seq_no>
				<ref_text><![CDATA[Blauert, J. 1983. <i>Spatial Hearing: The Psychophysics of Human Sound Localization.</i> M.I.T. Press, Cambridge, MA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>389</ref_seq_no>
				<ref_text><![CDATA[Borish, J. 1984. Extension of the image model to arbitrary polyhedra. <i>J. of the Acoustical Society of America 75</i>, 6.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>390</ref_seq_no>
				<ref_text><![CDATA[Brandenburg, K. 1999. mp3 and AAC explained. <i>AES 17th International Conference on High-Quality Audio Coding</i> (Sept.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>391</ref_seq_no>
				<ref_text><![CDATA[Bregman, A. 1990. <i>Auditory Scene Analysis, The perceptual organization of sound.</i> The MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>392</ref_seq_no>
				<ref_text><![CDATA[Chen, J., Veen, B. V., and Hecox, K. 1995. A spatial feature extraction and regularization model for the head-related transfer function. <i>J. of the Acoustical Society of America 97</i> (Jan.), 439--452.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>393</ref_seq_no>
				<ref_text><![CDATA[Chen, H., Wallace, G., Gupta, A., Li, K., Funkhouser, T., and Cook, P. 2002. Experiences with scalability of display walls. <i>Proceedings of the Immersive Projection Technology (IPT) Workshop</i> (Mar.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>394</ref_seq_no>
				<ref_text><![CDATA[Direct Sound 3D, 2004. Direct X homepage, Microsoft&#169;. http://www.microsoft.com/windows/directx/default.asp.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882339</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>395</ref_seq_no>
				<ref_text><![CDATA[Dobashi, Y., Yamamoto, T., and Nishita, T. 2003. Real-time rendering of aerodynamic sound using sound textures based on computational fluid dynamics. <i>ACM Transactions on Graphics 22</i>, 3 (Aug.), 732--740. (Proceedings of ACM SIGGRAPH 2003).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>396</ref_seq_no>
				<ref_text><![CDATA[EAX, 2004. Environmental audio extensions 4.0, Creative&#169;. http://www.soundblaster.com/eaudio.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>397</ref_seq_no>
				<ref_text><![CDATA[Ellis, D. 1992. A perceptual representation of audio. <i>Master's thesis, Massachusets Institute of Technology.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>398</ref_seq_no>
				<ref_text><![CDATA[Faller, C., and Baumgarte, F. 2002. Binaural cue coding applied to audio compression with flexible rendering. In <i>Proc. 113th AES Convention.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>399</ref_seq_no>
				<ref_text><![CDATA[Filipanits, F. 1994. Design and implementation of an auralization system with a spectrum-based temporal processing optimization. <i>Master thesis, Univ. of Miami.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>400</ref_seq_no>
				<ref_text><![CDATA[Fouad, H., Hahn, J., and Ballas, J. 1997. Perceptually based scheduling algorithms for real-time synthesis of complex sonic environments. <i>proceedings of the 1997 International Conference on Auditory Display (ICAD'97), Xerox Palo Alto Research Center, Palo Alto, USA.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>401</ref_seq_no>
				<ref_text><![CDATA[Fouad, H., Ballas, J., and Brock, D. 2000. An extensible toolkit for creating virtual sonic environments. <i>Proceedings of Intl. Conf. on Auditory Display</i> (Atlanta, USA, May 2000).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166149</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>402</ref_seq_no>
				<ref_text><![CDATA[Funkhouser, T., and Sequin, C. 1993. Adaptive display algorithms for interactive frame rates during visualization of complex virtual environments. <i>Computer Graphics (SIGGRAPH '93 proceedings), Los Angeles, CA</i> (August), 247--254.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311590</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>403</ref_seq_no>
				<ref_text><![CDATA[Funkhouser, T., Min, P., and Carlbom, I. 1999. Real-time acoustic modeling for distributed virtual environments. <i>ACM Computer Graphics, SIGGRAPH'99 Proceedings</i> (Aug.), 365--374.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>404</ref_seq_no>
				<ref_text><![CDATA[Gardner, W. 1997. Reverberation algorithms. In <i>Applications of Digital Signal Processing to Audio and Acoustics</i>, M. Kahrs and K. Brandenburg, Eds. Kluwer Academic Publishers, 85--131.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>405</ref_seq_no>
				<ref_text><![CDATA[Grewin, C. 1993. Methods for quality assessment of low bit-rate audio codecs. <i>proceedings of the 12th AES conference</i>, 97--107.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>406</ref_seq_no>
				<ref_text><![CDATA[Herder, J. 1999. Optimization of sound spatialization resource management through clustering. <i>The Journal of Three Dimensional Images, 3D-Forum Society 13</i>, 3 (Sept.), 59--65.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>407</ref_seq_no>
				<ref_text><![CDATA[Herder, J. 1999. Visualization of a clustering algorithm of sound sources based on localization errors. <i>The Journal of Three Dimensional Images, 3D-Forum Society 13</i>, 3 (Sept.), 66--70.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>408</ref_seq_no>
				<ref_text><![CDATA[Hochbaum, D. S., and Schmoys, D. B. 1985. A best possible heuristic for the <i>k</i>-center problem. <i>Mathematics of Operations Research 10</i>, 2 (May), 180--184.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>409</ref_seq_no>
				<ref_text><![CDATA[ITU-R. 1994. Methods for subjective assessment of small impairments in audio systems including multichannel sound systems, ITU-R BS 1116.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>410</ref_seq_no>
				<ref_text><![CDATA[Lagrange, M., and Marchand, S. 2001. Real-time additive synthesis of sound by taking advantage of psychoacoustics. In <i>Proceedings of the COST G-6 Conference on Digital Audio Effects (DAFX-01), Limerick, Ireland, December 6--8.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>411</ref_seq_no>
				<ref_text><![CDATA[Larsson, P., V&#228;stfj&#228;ll, D., and Kleiner, M. 2002. Better presence and performance in virtual environments by improved binaural sound rendering. <i>proceedings of the AES 22nd Intl. Conf. on virtual, synthetic and entertainment audio, Espoo, Finland</i> (June), 31--38.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>412</ref_seq_no>
				<ref_text><![CDATA[Likas, A., Vlassis, N., and Verbeek, J. 2003. The global <i>k</i>-means clustering algorithm. <i>Pattern Recognition 36</i>, 2, 451--461.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>413</ref_seq_no>
				<ref_text><![CDATA[Lokki, T., Gr&#246;hn, M., Savioja, L., and Takala, T. 2000. A case study of auditory navigation in virtual acoustic environments. <i>Proceedings of Intl. Conf. on Auditory Display (ICAD2000).</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>414</ref_seq_no>
				<ref_text><![CDATA[Martens, W. 1987. Principal components analysis and resynthesis of spectral cues to perceived direction. In <i>Proc. Int. Computer Music Conf. (ICMC'87)</i>, 274--281.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>415</ref_seq_no>
				<ref_text><![CDATA[Moore, B. C. J., Glasberg, B., and Baer, T. 1997. A model for the prediction of thresholds, loudness and partial loudness. <i>J. of the Audio Engineering Society 45</i>, 4, 224--240. Software available at http://hearing.psychol.cam.ac.uk/Demos/demos.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>416</ref_seq_no>
				<ref_text><![CDATA[Moore, B. C. 1997. <i>An introduction to the psychology of hearing.</i> Academic Press, 4th edition.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>417</ref_seq_no>
				<ref_text><![CDATA[Painter, E. M., and Spanias, A. S. 1997. A review of algorithms for perceptual coding of digital audio signals. <i>DSP-97.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>418</ref_seq_no>
				<ref_text><![CDATA[Paquette, E., Poulin, P., and Drettakis, G. 1998. A light hierarchy for fast rendering of scenes with many lights. <i>Proceedings of EUROGRAPHICS'98.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>419</ref_seq_no>
				<ref_text><![CDATA[Pierce, A. 1984. <i>Acoustics. An introduction to its physical principles and applications.</i> 3rd edition, American Institute of Physics.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>420</ref_seq_no>
				<ref_text><![CDATA[Savioja, L., Huopaniemi, J., Lokki, T., and V&#228;&#228;n&#228;nen, R. 1999. Creating interactive virtual acoustic environments. <i>J. of the Audio Engineering Society 47</i>, 9 (Sept.), 675--705.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>421</ref_seq_no>
				<ref_text><![CDATA[Sensaura, 2001. ZoomFX, MacroFX, Sensaura&#169;. http://www.sensaura.co.uk.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>422</ref_seq_no>
				<ref_text><![CDATA[SoundBlaster, 2004. Creative Labs Soundblaster&#169;. http://www.soundblaster.com.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>265600</ref_obj_id>
				<ref_seq_no>423</ref_seq_no>
				<ref_text><![CDATA[Steiglitz, K. 1996. <i>A DSP Primer with applications to digital audio and computer music.</i> Addison Wesley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>266779</ref_obj_id>
				<ref_obj_pid>266774</ref_obj_pid>
				<ref_seq_no>424</ref_seq_no>
				<ref_text><![CDATA[Tsingos, N., and Gascuel, J.-D. 1997. Soundtracks for computer animation: sound rendering in dynamic environments with occlusions. <i>Proceedings of Graphics Interface'97</i> (May), 9--16.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383323</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>425</ref_seq_no>
				<ref_text><![CDATA[Tsingos, N., Funkhouser, T., Ngan, A., and Carlbom, I. 2001. Modeling acoustics in virtual environments using the uniform theory of diffraction. <i>ACM Computer Graphics, SIGGRAPH'01 Proceedings</i> (Aug.), 545--552.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>426</ref_seq_no>
				<ref_text><![CDATA[van den Doel, K., Pai, D. K., Adam, T., Kortchmar, L., and Pichora-Fuller, K. 2002. Measurements of perceptual quality of contact sound models. In <i>Proceedings of the International Conference on Auditory Display (ICAD 2002), Kyoto, Japan</i>, 345--349.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1008558</ref_obj_id>
				<ref_obj_pid>1008550</ref_obj_pid>
				<ref_seq_no>427</ref_seq_no>
				<ref_text><![CDATA[van den Doel, K., Knott, D., and Pai, D. K. 2004. Interactive simulation of complex audio-visual scenes. <i>Presence: Teleoperators and Virtual Environments 13</i>, 1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>428</ref_seq_no>
				<ref_text><![CDATA[Vroomen, J., and de Gelder, B. 2004. Perceptual effects of cross-modal stimulation: Ventriloquism and the freezing phenomenon. In <i>Handbook of multisensory processes</i>, G. Calvert, C. Spence, and B. E. Stein, Eds. M.I.T. Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>429</ref_seq_no>
				<ref_text><![CDATA[Wenzel, E., Miller, J., and Abel, J. 2000. A software-based system for interactive spatial sound synthesis. <i>Proceeding of ICAD 2000, Atlanta, USA</i> (April).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1209342</ref_obj_id>
				<ref_seq_no>430</ref_seq_no>
				<ref_text><![CDATA[Zwicker, E., and Fastl, H. 1999. <i>Psychoacoustics: Facts and Models.</i> Springer. Second Upadated Edition.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>431</ref_seq_no>
				<ref_text><![CDATA[M. A. Biot and I. Tolstoy, "Formulation of wave propagation in infinite media by normal coordinates with an application to diffraction," J. Acoust. Soc. Am. &#60;b&#62;29&#60;/b&#62;, 381--391 (1957).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>432</ref_seq_no>
				<ref_text><![CDATA[H. Medwin, "Shadowing by finite noise barriers," J. Acoust. Soc. Am. &#60;b&#62;69&#60;/b&#62;, 1060--64 (1981).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>433</ref_seq_no>
				<ref_text><![CDATA[H. Medwin, E. Childs, and G. M. Jebsen, "Impulse studies of double diffraction: A discrete Huygens interpretation," J. Acoust. Soc. Am. &#60;b&#62;72&#60;/b&#62;, 1005--1013 (1982).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>434</ref_seq_no>
				<ref_text><![CDATA[J. P. Chambers and Y. H. Berthelot, "Time-domain experiments on the diffraction of sound by a step discontinuity," J. Acoust. Soc. Am. &#60;b&#62;96&#60;/b&#62;, 1887--1892 (1994).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>435</ref_seq_no>
				<ref_text><![CDATA[D. Ouis, "Scattering by thin strip-like elements and applications in room acoustics," Dissertation, Report TVBA-1005, Lund University of Technology, Lund, Sweden (1995).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>436</ref_seq_no>
				<ref_text><![CDATA[A. W. Trorey, "Diffractions for arbitrary source-receiver locations," Geophysics &#60;b&#62;42&#60;/b&#62;, 1177--1182 (1977).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>437</ref_seq_no>
				<ref_text><![CDATA[J. R. Berryhill, "Diffraction response for nonzero separation of source and receiver," Geophysics &#60;b&#62;42&#60;/b&#62;, 1158--1176 (1977).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>438</ref_seq_no>
				<ref_text><![CDATA[Y. Sakurai and K. Nagata, "Sound reflections of a rigid plane and of the live end composed by those panels," J. Acoust. Soc. Jpn. (E) &#60;b&#62;2&#60;/b&#62;, 5--14 (1981).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>439</ref_seq_no>
				<ref_text><![CDATA[G. M. Jebsen and H. Medwin, "On the failure of the Kirchhoff assumption in backscatter," J. Acoust. Soc. Am. &#60;b&#62;72&#60;/b&#62;, 1607--11 (1982).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>440</ref_seq_no>
				<ref_text><![CDATA[G. V. Norton, J. C. Novarini, and R. S. Keiffer, "An evaluation of the Kirchhoff approximation in predicting the axial impulse response of hard and soft disks," J. Acoust. Soc. Am. &#60;b&#62;93&#60;/b&#62;, 3049--3056 (1993).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>441</ref_seq_no>
				<ref_text><![CDATA[R. S. Keiffer, J. C. Novarini, and G. V. Norton, "The impulse response of an aperture: Numerical calculations within the framework of the wedge assemblage method," J. Acoust. Soc. Am. &#60;b&#62;95&#60;/b&#62;, 3--12 (1994).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>442</ref_seq_no>
				<ref_text><![CDATA[A. D. Pierce, "Diffraction of sound around corners and over wide barriers," J. Acoust. Soc. Am. &#60;b&#62;55&#60;/b&#62;, 941--955 (1974).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>443</ref_seq_no>
				<ref_text><![CDATA[R. G. Kouyoumjian and P. H. Pathak, "A uniform geometrical theory of diffraction for an edge in a perfectly conducting surface," Proc. IEEE &#60;b&#62;62&#60;/b&#62;, 1448--1461 (1974).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>444</ref_seq_no>
				<ref_text><![CDATA[C. S. Clay and W. A. Kinney, "Numerical computations of time-domain diffractions from wedges and reflections from facets," J. Acoust. Soc. Am. &#60;b&#62;83&#60;/b&#62;, 2126--2133 (1988).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>445</ref_seq_no>
				<ref_text><![CDATA[W. A. Kinney, C. S. Clay, and G. A. Sandness, "Scattering from a corrugated surface: Comparison between experiment, Helmholtz-Kirchhoff theory, and the facet ensemble method," J. Acoust. Soc. Am. &#60;b&#62;73&#60;/b&#62;, 183--194 (1983).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>446</ref_seq_no>
				<ref_text><![CDATA[J. Vanderkooy, "A simple theory of cabinet edge diffraction," J. Aud. Eng. Soc. &#60;b&#62;39&#60;/b&#62;, 923--933 (1991).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>447</ref_seq_no>
				<ref_text><![CDATA[J. J. Bowman and T. B. A. Senior, <i>Electromagnetic and Acoustic Scattering by Simple Shapes</i>, edited by J. J. Bowman, T. B. A. Senior, and P. L. E. Uslenghi (North-Holland, Amsterdam, 1969), Chap. 6.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>448</ref_seq_no>
				<ref_text><![CDATA[T. Kawai, "Sound diffraction by a many sided barrier or pillar," J. Sound Vib. &#60;b&#62;79&#60;/b&#62;, 229--242 (1981).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>449</ref_seq_no>
				<ref_text><![CDATA[T. J. Cox and Y. W. Lam, "Evaluation of methods for predicting the scattering from simple rigid panels," Appl. Acoust. &#60;b&#62;40&#60;/b&#62;, 123--140 (1993).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>450</ref_seq_no>
				<ref_text><![CDATA[B.-I. Dalenb&#228;ck, "Room acoustic prediction based on a unified treatment of diffuse and specular reflection," J. Acoust. Soc. Am. &#60;b&#62;100&#60;/b&#62;, 899--909 (1996).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>451</ref_seq_no>
				<ref_text><![CDATA[Chaigne, A., and Doutaut, V. 1997. Numerical simulations of xylophones. i. time domain modeling of the vibrating bars. <i>J. Acoust. Soc. Am. 101</i>, 1, 539--557.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>452</ref_seq_no>
				<ref_text><![CDATA[Chung, J. Y., Liu, J., and Lin, K. J. 1987. Scheduling real-time, periodic jobs using imprecise results. In <i>Proc. IEEE RTS.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>898855</ref_obj_id>
				<ref_seq_no>453</ref_seq_no>
				<ref_text><![CDATA[Dongarra, J. J. 2005. Performance of various computers using standard linear equations software (linpack benchmark report). Tech. rep., Knoxville, TN, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>131157</ref_obj_id>
				<ref_obj_pid>131150</ref_obj_pid>
				<ref_seq_no>454</ref_seq_no>
				<ref_text><![CDATA[Florens, J. L., and Cadoz, C. 1991. The physical model: modeling and simulating the instrumental universe. In <i>Represenations of Musical Signals</i>, G. D. Poli, A. Piccialli, and C. Roads, Eds. MIT Press, Cambridge, MA, USA, 227--268.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>455</ref_seq_no>
				<ref_text><![CDATA[Fouad, H., Ballas, J., and Hahn, J. 1997. Perceptually based scheduling algorithms for real-time synthesis of complex sonic environments. In <i>Proc. Int. Conf. Auditory Display.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882358</ref_obj_id>
				<ref_obj_pid>1201775</ref_obj_pid>
				<ref_seq_no>456</ref_seq_no>
				<ref_text><![CDATA[Guendelman, E., Bridson, R., and Fedkiw, R. 2003. Nonconvex rigid bodies with stacking. <i>ACM Trans. on Graphics (Proc. of ACM SIGGRAPH) 22</i>, 871--878.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>457</ref_seq_no>
				<ref_text><![CDATA[Kim, Y. J., Lin, M. C., and Manocha, D. 2002. DEEP: an incremental algorithm for penetration depth computation between convex polytopes. <i>Proc. of IEEE Conference on Robotics and Automation</i>, 921--926.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>199436</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>458</ref_seq_no>
				<ref_text><![CDATA[Mirtich, B., and Canny, J. 1995. Impulse-based simulation of rigid bodies. In <i>1995 Symposium on Interactive 3D Graphics</i>, P. Hanrahan and J. Winget, Eds., ACM SIGGRAPH, 181--188. ISBN 0-89791-736-7.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383321</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>459</ref_seq_no>
				<ref_text><![CDATA[O'Brien, J. F., Cook, P. R., and Essl, G. 2001. Synthesizing sounds from physically based motion. In <i>SIGGRAPH '01: Proceedings of the 28th annual conference on Computer graphics and interactive techniques</i>, ACM Press, New York, NY, USA, 529--536.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>545290</ref_obj_id>
				<ref_obj_pid>545261</ref_obj_pid>
				<ref_seq_no>460</ref_seq_no>
				<ref_text><![CDATA[O'Brien, J. F., Shen, C., and Gatchalian, C. M. 2002. Synthesizing sounds from rigid-body simulations. In <i>The ACM SIGGRAPH 2002 Symposium on Computer Animation</i>, ACM Press, 175--181.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>461</ref_seq_no>
				<ref_text><![CDATA[Sek, A., and Moore, B. C. 1995. Frequency discrimination as a function of frequency, measured in several ways. <i>J. Acoust. Soc. Am. 97</i>, 4 (April), 2479--2486.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>462</ref_seq_no>
				<ref_text><![CDATA[van den Doel, K., and Pai, D. K. 1996. Synthesis of shape dependent sounds with physical modeling. In <i>Proceedings of the International Conference on Auditory Displays.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1246774</ref_obj_id>
				<ref_obj_pid>1246770</ref_obj_pid>
				<ref_seq_no>463</ref_seq_no>
				<ref_text><![CDATA[van den Doel, K., and Pai, D. K. 1998. The sounds of physical shapes. <i>Presence 7</i>, 4, 382--395.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383322</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>464</ref_seq_no>
				<ref_text><![CDATA[van den Doel, K., Kry, P. G., and Pai, D. K. 2001. Foleyautomatic: physically-based sound effects for interactive simulation and animation. In <i>SIGGRAPH '01: Proceedings of the 28th annual conference on Computer graphics and interactive techniques</i>, ACM Press, New York, NY, USA, 537--544.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1008558</ref_obj_id>
				<ref_obj_pid>1008550</ref_obj_pid>
				<ref_seq_no>465</ref_seq_no>
				<ref_text><![CDATA[van den Doel, K., Knott, D., and Pai, D. K. 2004. Interactive simulation of complex audiovisual scenes. <i>Presence: Teleoper. Virtual Environ. 13</i>, 1, 99--111.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>466</ref_seq_no>
				<ref_text><![CDATA[Zwicker, E., and Fastl, H. 1990. In <i>Psychoacoustics.</i> Springer-Verlag, Berlin.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>91416</ref_obj_id>
				<ref_obj_pid>91385</ref_obj_pid>
				<ref_seq_no>467</ref_seq_no>
				<ref_text><![CDATA[J. M. Airey, J. H. Rohlf, and F. P. Brooks, Jr. Towards image realism with interactive update rates in complex virtual building environments. In Rich Riesenfeld and Carlo S&#233;quin, editors, <i>Computer Graphics (1990 Symposium on Interactive 3D Graphics)</i>, pages 41--50, March 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>468</ref_seq_no>
				<ref_text><![CDATA[L. Aveneau and M. Meriaux. Rendering polygonal scenes with diffraction account. <i>Seventh International Conference in Central Europe on Computer Graphics and Visualization (Winter School on Computer Graphics)</i>, February 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>469</ref_seq_no>
				<ref_text><![CDATA[L. Aveneau, Y. Pousset, R. Vauzelle, and M. M&#233;riaux. Development and evaluations of physical and computer optimizations for the 3d utd model. <i>AP2000 Millennium Conference on Antennas&amp;Propagation (poster)</i>, April 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>470</ref_seq_no>
				<ref_text><![CDATA[H. L. Bertoni. Coverage prediction for mobile radio systems operating in the 800/900 MHz frequency range. <i>IEEE Transactions on Vehicular Technology (Special Issue on Mobile Radio Propagation)</i>, 37(1), February 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>471</ref_seq_no>
				<ref_text><![CDATA[J. Blauert. <i>Spatial Hearing: The Psychophysics of Human Sound Localization.</i> M.I.T. Press, Cambridge, MA, 1983.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>472</ref_seq_no>
				<ref_text><![CDATA[M. Born and E. Wolf. <i>Principles of Optics.</i> 7th ed., Pergamon Press, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>473</ref_seq_no>
				<ref_text><![CDATA[B.-I. L. Dalenb&#228;ck. Room acoustic prediction based on a unified treatment of diffuse and specular reflection. <i>J. of the Acoustical Soc. of America</i>, 100:899--909, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>921496</ref_obj_id>
				<ref_seq_no>474</ref_seq_no>
				<ref_text><![CDATA[G. Drettakis. <i>Structured Sampling and Reconstruction of Illumination for Image Synthesis.</i> PhD thesis, University of Toronto, January 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>475</ref_seq_no>
				<ref_text><![CDATA[N. I. Durlach and A. S. Mavor. Virtual reality scientific and technological challenges. National Research Council Report, National Academy Press, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>476</ref_seq_no>
				<ref_text><![CDATA[P. Filippi, D. Habault, J. P. Lefevre, and A. Bergassoli. <i>Acoustics, basic physics, theory and methods.</i> Academic Press, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>304911</ref_obj_id>
				<ref_obj_pid>304893</ref_obj_pid>
				<ref_seq_no>477</ref_seq_no>
				<ref_text><![CDATA[S. J. Fortune. Topological beam tracing. In <i>Proc. 15th ACM Symposium on Computational Geometry</i>, pages 59--68, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280818</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>478</ref_seq_no>
				<ref_text><![CDATA[T. Funkhouser, I. Carlbom, G. Elko, G. Pingali, M. Sondhi, and J. West. A beam tracing approach to acoustic modeling for interactive virtual environments. <i>ACM Computer Graphics, Proc. SIGGRAPH98</i>, pages 21--32, July 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311590</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>479</ref_seq_no>
				<ref_text><![CDATA[T. Funkhouser, P. Min, and I. Carlbom. Real-time acoustic modeling for distributed virtual environments. <i>ACM Computer Graphics, Proc. SIGGRAPH99</i>, pages 365--374, August 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>285869</ref_obj_id>
				<ref_seq_no>480</ref_seq_no>
				<ref_text><![CDATA[J. Goodman and J. O'Rourke, editors. <i>Handbook of Discrete and Computational Geometry.</i> CRC Press, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808588</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>481</ref_seq_no>
				<ref_text><![CDATA[P. Heckbert and P. Hanrahan. Beam tracing polygonal objects. <i>Computer Graphics (SIGGRAPH84)</i>, 18(3):119--127, July 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>482</ref_seq_no>
				<ref_text><![CDATA[D. C. Hothersall, S. N. Chandler-Wilde, and M. N. Hajmirzae. Efficiency of single noise barriers. <i>J. of Sound and Vibration</i>, 146(2):303--322, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>483</ref_seq_no>
				<ref_text><![CDATA[C. Huygens. <i>Trait&#233; de la Lumiere.</i> London, Macmillan&amp;Co., 1912.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>484</ref_seq_no>
				<ref_text><![CDATA[P. Jean. A variational approach for the study of outdoor sound propagation and application to railway noise. <i>J. of Sound and Vibration</i>, 212(2):275--294, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>485</ref_seq_no>
				<ref_text><![CDATA[C. B. Jones. A new approach to the 'hidden line' problem. <i>Computer Journal</i>, 14(3):232--237, August 1971.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>486</ref_seq_no>
				<ref_text><![CDATA[T. Kawai. Sound diffraction by a many sided barrier or pillar. <i>J. of Sound and Vibration</i>, 79(2):229--242, 1981.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>487</ref_seq_no>
				<ref_text><![CDATA[J. B. Keller. Geometrical theory of diffraction. <i>J. of the Optical Society of America</i>, 52(2):116--130, 1962.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>488</ref_seq_no>
				<ref_text><![CDATA[S. C. Kim, B. Guarino, T. Willis, V. Erceg, S. Fortune, R. Valenzuela, L. Thomas, J. Ling, and J. Moore. Radio propagation measurements and prediction using three-dimensional ray tracing in urban environments at 908 MHz and 1.9 GHz. <i>IEEE Trans. on Vehicular Technology</i>, 48:931--946, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>489</ref_seq_no>
				<ref_text><![CDATA[M. Kleiner, B. I. Dalenb&#228;k, and P. Svensson. Auralization - an overview. <i>J. of the Audio Engineering Society</i>, 41(11):861--875, November 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>490</ref_seq_no>
				<ref_text><![CDATA[R. G. Kouyoumjian and P. H. Pathak. A uniform geometrical theory of diffraction for an edge in a perfectly conducting surface. <i>Proc. of IEEE</i>, 62:1448--1461, November 1974.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>491</ref_seq_no>
				<ref_text><![CDATA[H. Lehnert and J. Blauert. Principles of binaural room simulation. <i>Applied Acoustics</i>, 36:259--291, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>492</ref_seq_no>
				<ref_text><![CDATA[D. A. McNamara, C. W. I. Pistorius, and J. A. G. Malherbe. <i>Introduction to the Uniform Geometrical Theory of Diffraction.</i> Artech House, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>493</ref_seq_no>
				<ref_text><![CDATA[H. Medwin, E. Childs, and G. Jebsen. Impulse studies of double diffraction: A discrete huygens interpretation. <i>J. Acoust. Soc. Am.</i>, 72:1005--1013, 1982.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>494</ref_seq_no>
				<ref_text><![CDATA[P. Min and T. Funkhouser. Priority-driven acoustic modeling for virtual environments. <i>Proc. Eurographics'2000</i>, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>495</ref_seq_no>
				<ref_text><![CDATA[J. S. B. Mitchell. Geometric shortest paths and network optimization. In J&#246;rg-R&#252;diger Sack and Jorge Urrutia, editors, <i>Handbook of Computational Geometry.</i> Elsevier Science Publishers B. V. North-Holland, Amsterdam, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>496</ref_seq_no>
				<ref_text><![CDATA[B. C. J. Moore. <i>An introduction to the psychology of hearing.</i> Academic Press, 4th ed., 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>497</ref_seq_no>
				<ref_text><![CDATA[A. D. Pierce. <i>Acoustics. An introduction to its physical principles and applications.</i> 3rd ed., American Institute of Physics, 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>148286</ref_obj_id>
				<ref_seq_no>498</ref_seq_no>
				<ref_text><![CDATA[W. Press, S. Teukolsky, W. Vetterling, and B. Flannery. <i>Numerical Recipes in C, 2nd ed.</i> Cambridge University Press, New York, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>234826</ref_obj_id>
				<ref_obj_pid>234821</ref_obj_pid>
				<ref_seq_no>499</ref_seq_no>
				<ref_text><![CDATA[A. Rajkumar, B. F. Naylor, F. Feisullin, and L. Rogers. Predicting RF coverage in large environments using ray-beam tracing and partitioning tree represented geometry. <i>Wireless Networks</i>, 2(2):143--154, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>500</ref_seq_no>
				<ref_text><![CDATA[L. Savioja, J. Huopaniemi, T. Lokki, and R. V&#228;&#228;n&#228;nen. Creating interactive virtual acoustic environments. <i>J. of the Audio Engineering Society</i>, 47(9):675--705, September 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311546</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>501</ref_seq_no>
				<ref_text><![CDATA[J. Stam. Diffraction shaders. <i>ACM Computer Graphics, Proc. SIGGRAPH99</i>, pages 101--110, August 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>502</ref_seq_no>
				<ref_text><![CDATA[U. Stephenson and U. Kristiansen. Pyramidal beam tracing and time dependent radiosity. <i>15th International Congress on Acoustics</i>, pages 657--660, June 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>503</ref_seq_no>
				<ref_text><![CDATA[R. L. Storms. <i>Auditory-Visual Cross-Modal Perception Phenomena.</i> PhD thesis, Naval Postgraduate School, Monterey, California, September 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>504</ref_seq_no>
				<ref_text><![CDATA[U. P. Svensson, R. I. Fred and J. Vanderkooy Analytic secondary source model of edge diffraction impulse responses. <i>J. of the Acoustical Society of America</i>, 106:2331--2344, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134029</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>505</ref_seq_no>
				<ref_text><![CDATA[S. Teller. Computing the antiumbra cast by an area light source. <i>Computer Graphics (SIGGRAPH92)</i>, 26(2):139--148, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>171029</ref_obj_id>
				<ref_seq_no>506</ref_seq_no>
				<ref_text><![CDATA[S. Teller. <i>Visibility Computations in Densely Occuded Polyhedral Environments.</i> PhD thesis, Computer Science Div., Univ. of California, Berkeley, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>507</ref_seq_no>
				<ref_text><![CDATA[R. Torres, P. Svensson and M. Kleiner. Computation of edge diffraction for more accurate room acoustics auralization. <i>J. of the Acoustical Society of America</i>, 109:600--610, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>508</ref_seq_no>
				<ref_text><![CDATA[R. Torres. Studies of Edge Diffraction and Scattering: Applications to Room acoustics and Auralization. PhD thesis, Dept. of Applied Acoustics, Chalmers University of Technology, Sweden, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>266779</ref_obj_id>
				<ref_obj_pid>266774</ref_obj_pid>
				<ref_seq_no>509</ref_seq_no>
				<ref_text><![CDATA[N. Tsingos and J.-D. Gascuel. Soundtracks for computer animation: sound rendering in dynamic environments with occlusions. <i>Proceedings of Graphics Interface'97</i>, pages 9--16, May 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>510</ref_seq_no>
				<ref_text><![CDATA[{AB79} Allen J. B., Berkley D. A.: Image method for efficiently simulating small-room acoustics. <i>The Journal of the Acoustical Society of America 65</i>, 4 (April 1979), 943--950.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>511</ref_seq_no>
				<ref_text><![CDATA[{AMA05} Akenine-M&#246;ller T., Aila T.: Conservative and tiled rasterization using a modified triangle set-up. <i>journal of graphics tools 10</i>, 3 (2005), 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>792949</ref_obj_id>
				<ref_obj_pid>792757</ref_obj_pid>
				<ref_seq_no>512</ref_seq_no>
				<ref_text><![CDATA[{BHS98} Bittner J., Havran V., Slavik P.: Hierarchical visibility culling with occlusion trees. <i>Computer Graphics International, 1998. Proceedings</i> (Jun 1998), 207--219.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383686</ref_obj_id>
				<ref_obj_pid>2383654</ref_obj_pid>
				<ref_seq_no>513</ref_seq_no>
				<ref_text><![CDATA[{BW05} Bittner J., Wonka P.: Fast exact from-region visibility in urban scenes. <i>Eurographics Symposium on Rendering</i> (2005), 223--230.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>514</ref_seq_no>
				<ref_text><![CDATA[{CLT*08} Chandak A., Lauterbach C., Taylor M., Ren Z., Manocha D.: Ad-frustum: Adaptive frustum tracing for interactive sound propagation. In <i>Proc. IEEE Visualization</i> (2008).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2231896</ref_obj_id>
				<ref_obj_pid>2231876</ref_obj_pid>
				<ref_seq_no>515</ref_seq_no>
				<ref_text><![CDATA[{COCSD03} Cohen-Or D., Chrysanthou Y., Silva C., Durand F.: A survey of visibility for walkthrough applications. <i>Visualization and Computer Graphics, IEEE Transactions on 9</i>, 3 (July-Sept. 2003), 412--431.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>253312</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>516</ref_seq_no>
				<ref_text><![CDATA[{CT97} Coorg S., Teller S.: Real-time occlusion culling for models with large occluders. In <i>SI3D '97: Proceedings of the 1997 symposium on Interactive 3D graphics</i> (New York, NY, USA, April 1997), ACM, pp. 83--ff.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566618</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>517</ref_seq_no>
				<ref_text><![CDATA[{DD02} Duguet F., Drettakis G.: Robust epsilon visibility. <i>Proc. of ACM SIGGRAPH</i> (2002), 567--575.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>518</ref_seq_no>
				<ref_text><![CDATA[{Dur99} Durand F.: <i>3D Visibility, Analysis and Applications.</i> PhD thesis, U. Joseph Fourier, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280818</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>519</ref_seq_no>
				<ref_text><![CDATA[{FCE*98} Funkhouser T., Carlbom I., Elko G., Pingali G., Sondhi M., West J.: A beam tracing approach to acoustic modeling for interactive virtual environments. In <i>Proc. of ACM SIGGRAPH</i> (1998), pp. 21--32.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>520</ref_seq_no>
				<ref_text><![CDATA[{Gha01} Ghali S.: A survey of practical object space visibility algorithms. In <i>SIGGRAPH Tutorial Notes</i> (2001).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808588</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>521</ref_seq_no>
				<ref_text><![CDATA[{HH84} Heckbert P. S., Hanrahan P.: Beam tracing polygonal objects. In <i>Proc. of ACM SIGGRAPH</i> (1984), pp. 119--127.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>262847</ref_obj_id>
				<ref_obj_pid>262839</ref_obj_pid>
				<ref_seq_no>522</ref_seq_no>
				<ref_text><![CDATA[{HMC*97} Hudson T., Manocha D., Cohen J., Lin M., Hoff K., Zhang H.: Accelerated occlusion culling using shadow frusta. In <i>Proc. of ACM Symposium on Computational Geometry</i> (1997), pp. 1--10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732124</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>523</ref_seq_no>
				<ref_text><![CDATA[{KCCO00} Koltun V., Chrysanthou Y., Cohen-Or D.: Virtual occluders: An efficient intermediate pvs representation. In <i>Eurpographics Workshop on Rendering</i> (2000), pp. 59--70.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614454</ref_obj_id>
				<ref_obj_pid>614278</ref_obj_pid>
				<ref_seq_no>524</ref_seq_no>
				<ref_text><![CDATA[{KS00} Klosowski J., Silva C.: The prioritized-layered projection algorithm for visible set estimation. <i>IEEE Trans. on Visualization and Computer Graphics 6</i>, 2 (2000), 108--123.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>525</ref_seq_no>
				<ref_text><![CDATA[{Lai06} Laine S.: <i>An Incremental Shaft Subdivision Algorithm for Computing Shadows and Visibility.</i> Master's thesis, Helsinki University of Technology, March 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>526</ref_seq_no>
				<ref_text><![CDATA[{Len93} Lenhert H.: Systematic errors of the ray-tracing algoirthm. <i>Applied Acoustics 38</i> (1993), 207--221.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>199422</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>527</ref_seq_no>
				<ref_text><![CDATA[{LG95} Luebke D., Georges C.: Portals and mirrors: Simple, fast evaluation of potentially visible sets. In <i>ACM Interactive 3D Graphics Conference</i> (Monterey, CA, 1995), pp. 105--108.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882313</ref_obj_id>
				<ref_obj_pid>1201775</ref_obj_pid>
				<ref_seq_no>528</ref_seq_no>
				<ref_text><![CDATA[{LSCO03} Leyvand T., Sorkine O., Cohen-Or D.: Ray space factorization for from-region visibility. <i>Proc. of ACM SIGGRAPH</i> (2003), 595--604.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>529</ref_seq_no>
				<ref_text><![CDATA[{LSLS09} Laine S., Siltanen S., Lokki T., Savioja L.: Accelerated beam tracing algorithm. <i>Applied Acoustic 70</i>, 1 (2009), 172--181.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>530</ref_seq_no>
				<ref_text><![CDATA[{MBW08} Mattausch O., Bittner J., Wimmer M.: Chc++: Coherent hierarchical culling revisted. <i>Proc. of Eurographics</i> (2008), 221--230.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383562</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>531</ref_seq_no>
				<ref_text><![CDATA[{NB04} Nirenstein S., Blake E.: Hardware accelerated visibility preprocessing using adaptive sampling. In <i>Eurographics Workshop on Rendering</i> (2004).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>532</ref_seq_no>
				<ref_text><![CDATA[{Nir03} Nirenstein S.: <i>Fast and Accurate Visibility Preprocessing.</i> PhD thesis, University of Cape Town, South Africa, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>533</ref_seq_no>
				<ref_text><![CDATA[{NRJS03} Navazo I., Rossignac J., Jou J., Sharif R.: Shieldtester: Cell-to-cell visibility test for surface occluderis. In <i>Proc. of Eurographics</i> (2003), pp. 291--302.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383861</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>534</ref_seq_no>
				<ref_text><![CDATA[{ORM07} Overbeck R., Ramamoorthi R., Mark W. R.: A Real-time Beam Tracer with Application to Exact Soft Shadows. In <i>Eurographics Symposium on Rendering</i> (Jun 2007), pp. 85--98.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073329</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>535</ref_seq_no>
				<ref_text><![CDATA[{RSH05} Reshetov A., Soupikov A., Hurley J.: Multi-level ray tracing algorithm. <i>ACM Trans. Graph. 24</i>, 3 (2005), 1176--1185.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>536</ref_seq_no>
				<ref_text><![CDATA[{Sho98} Shoemake K.: Pluecker coordinate tutorial. <i>Ray Tracing News 11</i>, 1 (1998).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>171029</ref_obj_id>
				<ref_seq_no>537</ref_seq_no>
				<ref_text><![CDATA[{Tel92} Teller S. J.: <i>Visibility Computations in Densely Occluded Polyheral Environments.</i> PhD thesis, CS Division, UC Berkeley, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141914</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>538</ref_seq_no>
				<ref_text><![CDATA[{WWZ*06} Wonka P., Wimmer M., Zhou K., Maierhofer S., Hesina G., Reshetov A.: Guided visibility sampling. <i>Proc. of ACM SIGGRAPH</i> (2006), 494--502.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>539</ref_seq_no>
				<ref_text><![CDATA[V. Algazi, R. Duda, and D. Thompson. The CIPIC HRTF Database. In <i>IEEE ASSP Workshop on Applications of Signal Processing to Audio and Acoustics</i>, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>540</ref_seq_no>
				<ref_text><![CDATA[F. Antonacci, M. Foco, A. Sarti, and S. Tubaro. Real time modeling of acoustic propagation in complex environments. In <i>Proc. of 7th International Conference on Digital Audio Effects</i>, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>541</ref_seq_no>
				<ref_text><![CDATA[M. Bertram, E. Deines, J. Mohring, J. Jegorovs, and H. Hagen. Phonon tracing for auralization and visualization of sound. In <i>Proceedings of IEEE Visualization 2005</i>, pages 151--158, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>542</ref_seq_no>
				<ref_text><![CDATA[J. Borish. Extension of the image model to arbitrary polyhedra. <i>Journal of the Acoustical Society of America</i>, 75(6):1827--1836, 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>526855</ref_obj_id>
				<ref_seq_no>543</ref_seq_no>
				<ref_text><![CDATA[C. Brebbia, editor. <i>Computational Acoustics and its Environmental Applications.</i> Transactions of the Wessex Institute, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808585</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>544</ref_seq_no>
				<ref_text><![CDATA[L. Carpenter. The a-buffer, an antialiased hidden surface method. In <i>SIGGRAPH '84: Proceedings of the 11th annual conference on Computer graphics and interactive techniques</i>, pages 103--108, New York, NY, USA, 1984. ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>515316</ref_obj_id>
				<ref_seq_no>545</ref_seq_no>
				<ref_text><![CDATA[P. R. Cook. <i>Real Sound Synthesis for Interactive Applications.</i> A. K. Peters, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>546</ref_seq_no>
				<ref_text><![CDATA[B.-I. Dalenb&#228;ck, P. Svensson, and M. Kleiner. Room acoustic prediction and auralization based on an extended image source model. <i>The Journal of the Acoustical Society of America</i>, 92(4):2346, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1187840</ref_obj_id>
				<ref_obj_pid>1187627</ref_obj_pid>
				<ref_seq_no>547</ref_seq_no>
				<ref_text><![CDATA[E. Deines, M. Bertram, J. Mohring, J. Jegorovs, F. Michel, H. Hagen, and G. Nielson. Comparative visualization for wave-based and geometric acoustics. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 12(5), 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>548</ref_seq_no>
				<ref_text><![CDATA[I. A. Drumm. <i>The Development and Application of an Adaptive Beam Tracing Algorithm to Predict the Acoustics of Auditoria.</i> PhD thesis, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>549</ref_seq_no>
				<ref_text><![CDATA[A. Farina. Ramsete - a new pyramid tracer for medium and large scale acoustic problems. In <i>Proceedings of EURO-NOISE</i>, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280818</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>550</ref_seq_no>
				<ref_text><![CDATA[T. Funkhouser, I. Carlbom, G. Elko, G. Pingali, M. Sondhi, and J. West. A beam tracing approach to acoustic modeling for interactive virtual environments. In <i>Proc. of ACM SIGGRAPH</i>, pages 21--32, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>551</ref_seq_no>
				<ref_text><![CDATA[T. Funkhouser, N. Tsingos, I. Carlbom, G. Elko, M. Sondhi, J. West, G. Pingali, P. Min, and A. Ngan. A beam tracing method for interactive architectural acoustics. <i>Journal of the Acoustical Society of America</i>, 115(2):739--756, February 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>552</ref_seq_no>
				<ref_text><![CDATA[T. Funkhouser, N. Tsingos, and J.-M. Jot. Survey of methods for modeling sound propagation in interactive virtual environment systems. <i>Presence and Teleoperation</i>, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311590</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>553</ref_seq_no>
				<ref_text><![CDATA[T. A. Funkhouser, P. Min, and I. Carlbom. Real-time acoustic modeling for distributed virtual environments. In <i>Proc. of ACM SIGGRAPH</i>, pages 365--374, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1221386</ref_obj_id>
				<ref_obj_pid>1220976</ref_obj_pid>
				<ref_seq_no>554</ref_seq_no>
				<ref_text><![CDATA[M. A. Garcia-Ruiz and J. R. Gutierrez-Pulido. An overview of auditory display to assist comprehension of molecular information. <i>Interact. Comput.</i>, 18(4):853--868, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>555</ref_seq_no>
				<ref_text><![CDATA[J. Genetti and D. Gordon. Ray tracing with adaptive supersampling in object space. In <i>Graphics Interface '93</i>, pages 70--77, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808588</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>556</ref_seq_no>
				<ref_text><![CDATA[P. S. Heckbert and P. Hanrahan. Beam tracing polygonal objects. In <i>Proc. of ACM SIGGRAPH</i>, pages 119--127, 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141983</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>557</ref_seq_no>
				<ref_text><![CDATA[D. L. James, J. Barbic, and D. K. Pai. Precomputed acoustic transfer: output-sensitive, accurate sound generation for geometrically complex vibration sources. In <i>Proc. of ACM SIGGRAPH</i>, pages 987--995, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>558</ref_seq_no>
				<ref_text><![CDATA[D. G. Jon Genetti and G. Williams. Adaptive supersampling in object space using pyramidal rays. <i>Computer Graphics Forum</i>, 17(1):29--54, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1008659</ref_obj_id>
				<ref_obj_pid>1008653</ref_obj_pid>
				<ref_seq_no>559</ref_seq_no>
				<ref_text><![CDATA[C. Joslin and N. Magnetat-Thalmann. Significant facet retrieval for real-time 3d sound rendering. In <i>Proceedings of the ACM VRST</i>, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>560</ref_seq_no>
				<ref_text><![CDATA[B. Kapralos, M. Jenkin, and E. Milios. Acoustic modeling utilizing an acoustic version of phonon mapping. In <i>Proc. of IEEE Workshop on HAVE</i>, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>561</ref_seq_no>
				<ref_text><![CDATA[A. Krokstad, S. Strom, and S. Sorsdal. Calculating the acoustical room response by the use of a ray tracing technique. <i>Journal of Sound and Vibration</i>, 8(1):118--125, July 1968.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>562</ref_seq_no>
				<ref_text><![CDATA[K. Kunz and R. Luebbers. <i>The Finite Difference Time Domain for Electromagnetics.</i> CRC Press, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>563</ref_seq_no>
				<ref_text><![CDATA[K. H. Kuttruff. Auralization of impulse responses modeled on the basis of ray-tracing results. <i>Journal of Audio Engineering Society</i>, 41(11):876--880, November 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>564</ref_seq_no>
				<ref_text><![CDATA[C. Lauterbach, S.-E. Yoon, D. Tuft, and D. Manocha. RT-DEFORM: Interactive Ray Tracing of Dynamic Scenes using BVHs. <i>IEEE Symposium on Interactive Ray Tracing</i>, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>565</ref_seq_no>
				<ref_text><![CDATA[H. Lehnert. Systematic errors of the ray-tracing algorithm. <i>J. Applied Acoustics</i>, 38(2--4):207--221, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>858740</ref_obj_id>
				<ref_obj_pid>1435682</ref_obj_pid>
				<ref_seq_no>566</ref_seq_no>
				<ref_text><![CDATA[R. B. Loftin. Multisensory perception: Beyond the visual in visualization. <i>Computing in Science and Engineering</i>, 05(4):56--58, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618916</ref_obj_id>
				<ref_obj_pid>616077</ref_obj_pid>
				<ref_seq_no>567</ref_seq_no>
				<ref_text><![CDATA[T. Lokki, L. Savioja, R. Vaananen, J. Huopaniemi, and T. Takala. Creating interactive virtual auditory environments. <i>IEEE Computer Graphics and Applications</i>, 22(4):49--57, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>585752</ref_obj_id>
				<ref_obj_pid>585740</ref_obj_pid>
				<ref_seq_no>568</ref_seq_no>
				<ref_text><![CDATA[M. Naef, O. Staadt, and M. Gross. Spatialized audio rendering for immersive virtual environments. In <i>Proceedings of the ACM VRST</i>, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1151863</ref_obj_id>
				<ref_obj_pid>1151855</ref_obj_pid>
				<ref_seq_no>569</ref_seq_no>
				<ref_text><![CDATA[K. V. Nesbitt. Modelling human perception to leverage the reuse of concepts across the multi-sensory design space. In <i>APCCM '06: Proceedings of the 3rd Asia-Pacific conference on Conceptual modelling</i>, pages 65--74, Darlinghurst, Australia, Australia, 2006. Australian Computer Society, Inc.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383321</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>570</ref_seq_no>
				<ref_text><![CDATA[J. F. O'Brien, P. R. Cook, and G. Essl. Synthesizing sounds from physically based motion. In <i>Proc. of ACM SIGGRAPH</i>, pages 529--536, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>571</ref_seq_no>
				<ref_text><![CDATA[T. Otsuru, Y. Uchinoura, R. Tomiku, N. Okamoto, and Y. Takahashi. Basic concept, accuracy and application of large-scale finite element sound field analysis of rooms. In <i>Proc. ICA 2004 (Kyoto)</i>, pages I-479--I-482, April 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>234826</ref_obj_id>
				<ref_obj_pid>234821</ref_obj_pid>
				<ref_seq_no>572</ref_seq_no>
				<ref_text><![CDATA[A. Rajkumar, B. F. Naylor, F. Feisullin, and L. Rogers. Predicting rf coverage in large environments using ray-beam tracing and partitioning tree represented geometry. <i>Wirel. Netw.</i>, 2(2):143--154, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073329</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>573</ref_seq_no>
				<ref_text><![CDATA[A. Reshetov, A. Soupikov, and J. Hurley. Multi-level ray tracing algorithm. <i>ACM Trans. Graph.</i>, 24(3):1176--1185, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>574</ref_seq_no>
				<ref_text><![CDATA[L. Savioja. <i>Modeling Techniques for Virtual Acoustics.</i> PhD thesis, Helsinki University of Technology, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>575</ref_seq_no>
				<ref_text><![CDATA[K. Shoemake. Pluecker coordinate tutorial. <i>Ray Tracing News</i>, 11(1), 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>725538</ref_obj_id>
				<ref_obj_pid>647363</ref_obj_pid>
				<ref_seq_no>576</ref_seq_no>
				<ref_text><![CDATA[S. Smith. Auditory representation of scientific data. In <i>Focus on Scientific Visualization</i>, pages 337--346, London, UK, 1993. Springer-Verlag.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>577</ref_seq_no>
				<ref_text><![CDATA[U. Stephenson. Quantized pyramidal beam tracing - a new algorithm for room acoustics and noise immission prognosis. <i>Acustica - Acta Acustica</i>, 82(3):517--525, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>578</ref_seq_no>
				<ref_text><![CDATA[H. Suzuki and A. S. Mohan. Frustum ray tracing technique for high spatial resolution channel characteristic map. In <i>Radio and Wireless Conference (RAWCON) 98</i>, pages 253--256. IEEE Press, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>579</ref_seq_no>
				<ref_text><![CDATA[R. Tomiku, T. Otsuru, Y. Takahashi, and D. Azuma. A computational investigation on measurements in reverberation rooms by finite element sound field analysis. In <i>Proc. ICA 2004 (Kyoto)</i>, pages II-941--II-942, April 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015710</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>580</ref_seq_no>
				<ref_text><![CDATA[N. Tsingos, E. Gallo, and G. Drettakis. Perceptual audio rendering of complex virtual environments. <i>ACM Trans. Graph.</i>, 23(3):249--258, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1008558</ref_obj_id>
				<ref_obj_pid>1008550</ref_obj_pid>
				<ref_seq_no>581</ref_seq_no>
				<ref_text><![CDATA[K. van den Doel, D. Knott, and D. K. Pai. Interactive simulation of complex audio-visual scenes. <i>Presence: Teleoperators and Virtual Environments</i>, 13(1):99--111, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>582</ref_seq_no>
				<ref_text><![CDATA[I. Wald, C. Benthin, M. Wagner, and P. Slusallek. Interactive rendering with coherent ray tracing. In A. Chalmers and T.-M. Rhyne, editors, <i>Computer Graphics Forum (Proceedings of EUROGRAPHICS 2001)</i>, volume 20, pages 153--164. Blackwell Publishers, Oxford, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1206075</ref_obj_id>
				<ref_obj_pid>1189762</ref_obj_pid>
				<ref_seq_no>583</ref_seq_no>
				<ref_text><![CDATA[I. Wald, S. Boulos, and P. Shirley. Ray Tracing Deformable Scenes using Dynamic Bounding Volume Hierarchies. <i>ACM Transactions on Graphics</i>, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2386334</ref_obj_id>
				<ref_obj_pid>2386332</ref_obj_pid>
				<ref_seq_no>584</ref_seq_no>
				<ref_text><![CDATA[M. Wand and W. Stra&#223;er. Multi-resolution sound rendering. In <i>SPBG'04 Symposium on Point - Based Graphics 2004</i>, pages 3--11, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>585</ref_seq_no>
				<ref_text><![CDATA[L. M. Wang, J. Rathsam, and S. R. Ryherd. Interactions of model detail level and scattering coefficients in room acoustic computer simulation. In <i>International Symposium on Room Acoustics: Design and Science</i>, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>358882</ref_obj_id>
				<ref_obj_pid>358876</ref_obj_pid>
				<ref_seq_no>586</ref_seq_no>
				<ref_text><![CDATA[T. Whitted. An improved illumination model for shaded display. <i>Commun. ACM</i>, 23(6):343--349, 1980.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>587</ref_seq_no>
				<ref_text><![CDATA[J. B. Allen and D. A. Berkley. Image method for efficiently simulating small-room acoustics. <i>The Journal of the Acoustical Society of America</i>, 65(4):943--950, April 1979.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>588</ref_seq_no>
				<ref_text><![CDATA[M. Bertram, E. Deines, J. Mohring, J. Jegorovs, and H. Hagen. Phonon tracing for auralization and visualization of sound. In <i>Proceedings of IEEE Visualization</i>, pages 151--158, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1289147</ref_obj_id>
				<ref_obj_pid>1288980</ref_obj_pid>
				<ref_seq_no>589</ref_seq_no>
				<ref_text><![CDATA[P. Calamia and U. P. Svensson. Fast Time-Domain Edge-Diffraction Calculations for Interactive Acoustic Simulations. <i>EURASIP Journal on Advances in Signal Processing</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>590</ref_seq_no>
				<ref_text><![CDATA[B.-I. Dalenb&#228;ck. Room acoustic prediction based on a unified treatment of diffuse and specular reflection. <i>The Journal of the Acoustical Society of America</i>, 100(2):899--909, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>591</ref_seq_no>
				<ref_text><![CDATA[B.-I. Dalenb&#228;ck and M. Str&#246;mberg. Real time walkthrough auralization - the first year. <i>Proceedings of the Institute of Acoustics</i>, 28(2), 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1187840</ref_obj_id>
				<ref_obj_pid>1187627</ref_obj_pid>
				<ref_seq_no>592</ref_seq_no>
				<ref_text><![CDATA[E. Deines, M. Bertram, J. Mohring, J. Jegorovs, F. Michel, H. Hagen, and G. Nielson. Comparative visualization for wave-based and geometric acoustics. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 12(5), 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>593</ref_seq_no>
				<ref_text><![CDATA[I. A. Drumm and Y. W. Lam. The adaptive beam-tracing algorithm. <i>Journal of the Acoustical Society of America</i>, 107(3):1405--1412, March 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>594</ref_seq_no>
				<ref_text><![CDATA[A. Farina. RAMSETE - a new Pyramid Tracer for medium and large scale acoustic problems. In <i>Proceedings of EURO-NOISE</i>, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>304911</ref_obj_id>
				<ref_obj_pid>304893</ref_obj_pid>
				<ref_seq_no>595</ref_seq_no>
				<ref_text><![CDATA[S. Fortune. Topological beam tracing. In <i>SCG '99: Proceedings of the fifteenth annual symposium on Computational geometry</i>, pages 59--68, New York, NY, USA, 1999. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280818</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>596</ref_seq_no>
				<ref_text><![CDATA[T. Funkhouser, I. Carlbom, G. Elko, G. Pingali, M. Sondhi, and J. West. A beam tracing approach to acoustic modeling for interactive virtual environments. In <i>Proc. of ACM SIGGRAPH</i>, pages 21--32, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>597</ref_seq_no>
				<ref_text><![CDATA[T. Funkhouser, N. Tsingos, and J.-M. Jot. Survey of Methods for Modeling Sound Propagation in Interactive Virtual Environment Systems. <i>Presence and Teleoperation</i>, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>598</ref_seq_no>
				<ref_text><![CDATA[J. Genetti, D. Gordon, and G. Williams. Adaptive supersampling in object space using pyramidal rays. <i>Computer Graphics Forum</i>, 17:29--54, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1008659</ref_obj_id>
				<ref_obj_pid>1008653</ref_obj_pid>
				<ref_seq_no>599</ref_seq_no>
				<ref_text><![CDATA[C. Joslin and N. Magnetat-Thalmann. Significant facet retrieval for real-time 3D sound rendering. In <i>Proceedings of the ACM VRST</i>, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>600</ref_seq_no>
				<ref_text><![CDATA[B. Kapralos, M. Jenkin, and E. Milios. Acoustic Modeling Utilizing an Acoustic Version of Phonon Mapping. In <i>Proc. of IEEE Workshop on HAVE</i>, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>601</ref_seq_no>
				<ref_text><![CDATA[J. B. Keller. Geometrical theory of diffraction. <i>Journal of the Optical Society of America</i>, 52(2):116--130, 1962.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>602</ref_seq_no>
				<ref_text><![CDATA[A. Krokstad, S. Strom, and S. Sorsdal. Calculating the acoustical room response by the use of a ray tracing technique. <i>Journal of Sound and Vibration</i>, 8(1):118--125, July 1968.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>603</ref_seq_no>
				<ref_text><![CDATA[S. Laine, S. Siltanen, T. Lokki, and L. Savioja. Accelerated beam tracing algorithm. <i>Applied Acoustic</i>, 2008. to appear.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>604</ref_seq_no>
				<ref_text><![CDATA[C. Lauterbach, A. Chandak, and D. Manocha. Adaptive sampling for frustum-based sound propagation in complex and dynamic environments. In <i>Proceedings of the 19th International Congress on Acoustics</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1313181</ref_obj_id>
				<ref_obj_pid>1313046</ref_obj_pid>
				<ref_seq_no>605</ref_seq_no>
				<ref_text><![CDATA[C. Lauterbach, A. Chandak, and D. Manocha. Interactive sound propagation in dynamic scenes using frustum tracing. <i>IEEE Trans. on Visualization and Computer Graphics</i>, 13(6):1672--1679, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383501</ref_obj_id>
				<ref_obj_pid>2383465</ref_obj_pid>
				<ref_seq_no>606</ref_seq_no>
				<ref_text><![CDATA[C. Lauterbach, S.-E. Yoon, M. Tang, and D. Manocha. ReduceM: Interactive and Memory Efficient Ray Tracing of Large Models. In <i>Proc. of the Eurographics Symposium on Rendering</i>, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>607</ref_seq_no>
				<ref_text><![CDATA[C. Lauterbach, S.-E. Yoon, D. Tuft, and D. Manocha. RT-DEFORM: Interactive Ray Tracing of Dynamic Scenes using BVHs. <i>IEEE Symposium on Interactive Ray Tracing</i>, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>858740</ref_obj_id>
				<ref_obj_pid>1435682</ref_obj_pid>
				<ref_seq_no>608</ref_seq_no>
				<ref_text><![CDATA[R. B. Loftin. Multisensory perception: Beyond the visual in visualization. <i>Computing in Science and Engineering</i>, 05(4):56--58, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1313183</ref_obj_id>
				<ref_obj_pid>1313046</ref_obj_pid>
				<ref_seq_no>609</ref_seq_no>
				<ref_text><![CDATA[F. Michel, E. Deines, M. Hering-Bertram, C. Garth, and H. Hagen. Listener-based Analysis of Surface Importance for Acoustic Metrics. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 13(6):1680--1687, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383861</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>610</ref_seq_no>
				<ref_text><![CDATA[R. Overbeck, R. Ramamoorthi, and W. R. Mark. A Real-time Beam Tracer with Application to Exact Soft Shadows. In <i>Eurographics Symposium on Rendering</i>, Jun 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>234826</ref_obj_id>
				<ref_obj_pid>234821</ref_obj_pid>
				<ref_seq_no>611</ref_seq_no>
				<ref_text><![CDATA[A. Rajkumar, B. F. Naylor, F. Feisullin, and L. Rogers. Predicting RF coverage in large environments using ray-beam tracing and partitioning tree represented geometry. <i>Wirel. Netw.</i>, 2(2):143--154, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073329</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>612</ref_seq_no>
				<ref_text><![CDATA[A. Reshetov, A. Soupikov, and J. Hurley. Multi-level ray tracing algorithm. <i>ACM Trans. Graph.</i>, 24(3):1176--1185, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>227638</ref_obj_id>
				<ref_obj_pid>227628</ref_obj_pid>
				<ref_seq_no>613</ref_seq_no>
				<ref_text><![CDATA[C. Ronchi, R. Iacono, and P. Paolucci. The "Cubed Sphere": A New Method for the Solution of Partial Differential Equations in Spherical Geometry. <i>Journal of Computational Physics</i>, 124:93--114(22), 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>614</ref_seq_no>
				<ref_text><![CDATA[J. Sandvad. Dynamic aspects of auditory virtual environment. In <i>Audio Engineering Society 100th Convention preprints</i>, page preprint no. 4246, April 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>615</ref_seq_no>
				<ref_text><![CDATA[L. Savioja, J. Huopaniemi, T. Lokki, and R. V&#228;&#228;n&#228;nen. Creating interactive virtual acoustic environments. <i>Journal of the Audio Engineering Society (JAES)</i>, 47(9):675--705, September 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37408</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>616</ref_seq_no>
				<ref_text><![CDATA[M. Shinya, T. Takahashi, and S. Naito. Principles and applications of pencil tracing. <i>Proc. of ACM SIGGRAPH</i>, 21(4):45--54, 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>617</ref_seq_no>
				<ref_text><![CDATA[K. Shoemake. Pl&#252;cker coordinate tutorial. <i>Ray Tracing News</i>, 11(1), 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>618</ref_seq_no>
				<ref_text><![CDATA[S. Siltanen, T. Lokki, S. Kiminki, and L. Savioja. The room acoustic rendering equation. <i>The Journal of the Acoustical Society of America</i>, 122(3):1624--1635, September 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>725538</ref_obj_id>
				<ref_obj_pid>647363</ref_obj_pid>
				<ref_seq_no>619</ref_seq_no>
				<ref_text><![CDATA[S. Smith. Auditory representation of scientific data. In <i>Focus on Scientific Visualization</i>, pages 337--346, London, UK, 1993. Springer-Verlag.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>620</ref_seq_no>
				<ref_text><![CDATA[M. Taylor, C. Lauterbach, A. Chandak, and D. Manocha. Edge Diffraction in Frustum Tracing. Technical report, University of North Carolina at Chapel Hill, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>621</ref_seq_no>
				<ref_text><![CDATA[N. Tsingos. A versatile software architecture for virtual audio simulations. In <i>International Conference on Auditory Display (ICAD)</i>, Espoo, Finland, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383864</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>622</ref_seq_no>
				<ref_text><![CDATA[N. Tsingos, C. Dachsbacher, S. Lefebvre, and M. Dellepiane. Instant sound scattering. In <i>Proceedings of the Eurographics Symposium on Rendering</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383323</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>623</ref_seq_no>
				<ref_text><![CDATA[N. Tsingos, T. Funkhouser, A. Ngan, and I. Carlbom. Modeling acoustics in virtual environments using the uniform theory of diffraction. In <i>Proc. of ACM SIGGRAPH</i>, pages 545--552, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015710</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>624</ref_seq_no>
				<ref_text><![CDATA[N. Tsingos, E. Gallo, and G. Drettakis. Perceptual audio rendering of complex virtual environments. <i>ACM Trans. Graph.</i>, 23(3):249--258, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>625</ref_seq_no>
				<ref_text><![CDATA[I. Wald, W. Mark, J. Gunther, S. Boulos, T. Ize, W. Hunt, S. Parker, and P. Shirley. State of the Art in Ray Tracing Dynamic Scenes. <i>Eurographics State of the Art Reports</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2386334</ref_obj_id>
				<ref_obj_pid>2386332</ref_obj_pid>
				<ref_seq_no>626</ref_seq_no>
				<ref_text><![CDATA[M. Wand and W. Stra&#223;er. Multi-resolution sound rendering. In <i>SPBG'04 Symposium on Point - Based Graphics 2004</i>, pages 3--11, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>627</ref_seq_no>
				<ref_text><![CDATA[J. Warnock. A hidden-surface algorithm for computer generated half-tone pictures. Technical Report TR 4--15, NTIS AD-753 671, Department of Computer Science, University of Utah, 1969.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>628</ref_seq_no>
				<ref_text><![CDATA[E. Wenzel, J. Miller, and J. Abel. A software-based system for interactive sound synthesis. In <i>International Conference on Auditory Display (ICAD)</i>, Atlanta, GA, April 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1278847</ref_obj_id>
				<ref_obj_pid>1278780</ref_obj_pid>
				<ref_seq_no>629</ref_seq_no>
				<ref_text><![CDATA[S.-E. Yoon, S. Curtis, and D. Manocha. Ray Tracing Dynamic Scenes using Selective Restructuring. In <i>Proc. of the Eurographics Symposium on Rendering</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>630</ref_seq_no>
				<ref_text><![CDATA[J. B. Allen and D. A. Berkley, "Image method for efficiently simulating small-room acoustics," <i>The Journal of the Acoustical Society of America</i>, vol. 65, no. 4, pp. 943--950, April 1979.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280818</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>631</ref_seq_no>
				<ref_text><![CDATA[T. Funkhouser, I. Carlbom, G. Elko, G. Pingali, M. Sondhi, and J. West, "A beam tracing approach to acoustic modeling for interactive virtual environments," in <i>SIGGRAPH '98: Proceedings of the 25th annual conference on Computer graphics and interactive techniques</i>, New York, NY, USA, 1998, pp. 21--32, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1313181</ref_obj_id>
				<ref_obj_pid>1313046</ref_obj_pid>
				<ref_seq_no>632</ref_seq_no>
				<ref_text><![CDATA[C. Lauterbach, A. Chandak, and D. Manocha, "Interactive sound propagation in dynamic scenes using frustum tracing," <i>IEEE Transactions on Visualization and Computer Graphics</i>, vol. 13, no. 6, pp. 1672--1679, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>633</ref_seq_no>
				<ref_text><![CDATA[M. Hodgson, "Evidence of diffuse surface reflections in rooms," <i>The Journal of the Acoustical Society of America</i>, vol. 89, no. 2, pp. 765--771, February 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383323</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>634</ref_seq_no>
				<ref_text><![CDATA[N. Tsingos, T. Funkhouser, A. Ngan, and I. Carlbom, "Modeling acoustics in virtual environments using the uniform theory of diffraction," in <i>SIGGRAPH 2001, Computer Graphics Proceedings</i>, 2001, pp. 545--552.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>635</ref_seq_no>
				<ref_text><![CDATA[U. P. Svensson, R. I. Fred, and J. Vanderkooy, "An analytic secondary source model of edge diffraction impulse responses," <i>Acoustical Society of America Journal</i>, vol. 106, pp. 2331--2344, Nov. 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>636</ref_seq_no>
				<ref_text><![CDATA[V. Pulkki, T. Lokki, and L. Savioja, "Implementation and visualization of edge diffraction with image-source method," in <i>The 112th Audio Engineering Society (AES) Convention</i>, 2002, pp. preprint no. 5603+.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>637</ref_seq_no>
				<ref_text><![CDATA[P. T. Calamia and U. P. Svensson, "Edge subdivision for fast diffraction calculations," in <i>Proc. 2005 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA 2005)</i>, October 2005, pp. 187--190.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>638</ref_seq_no>
				<ref_text><![CDATA[R. Kouyoumjian and P. H. Pathak, "A uniform geometrical theory of diffraction for an edge in a perfectly conducting surface," <i>IEEE, Proceedings, vol. 62, Nov. 1974, p. 1448--1461.</i>, 1974.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618914</ref_obj_id>
				<ref_obj_pid>616077</ref_obj_pid>
				<ref_seq_no>639</ref_seq_no>
				<ref_text><![CDATA[N. Tsingos, I. Carlbom, G. Elbo, R. Kubli, and T. Funkhouser, "Validation of acoustical simulations in the "bell labs box"," <i>IEEE Computer Graphics and Applications</i>, vol. 22, no. 4, pp. 28--37, June 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>640</ref_seq_no>
				<ref_text><![CDATA[S. Van Duyne and J. O. Smith, "The 2-d digital waveguide mesh," in <i>Applications of Signal Processing to Audio and Acoustics, 1993. Final Program and Paper Summaries., 1993 IEEE Workshop on</i>, 1993, pp. 177--180.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>641</ref_seq_no>
				<ref_text><![CDATA[D. Botteldooren, "Acoustical finite-difference time-domain simulation in a quasi-cartesian grid," <i>The Journal of the Acoustical Society of America</i>, vol. 95, no. 5, pp. 2313--2319, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1364916</ref_obj_id>
				<ref_obj_pid>1364901</ref_obj_pid>
				<ref_seq_no>642</ref_seq_no>
				<ref_text><![CDATA[N. Raghuvanshi, N. Galoppo, and M. C. Lin, "Accelerated wave-based acoustics simulation," in <i>Symposium on Solid and Physical Modeling</i>, 2008, pp. 91--102.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>643</ref_seq_no>
				<ref_text><![CDATA[J. Borish, "Extension of the image model to arbitrary polyhedra," <i>The Journal of the Acoustical Society of America</i>, vol. 75, pp. 1827--1836, June 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>644</ref_seq_no>
				<ref_text><![CDATA[A. Krokstad, S. Strom, and S. S&#248;rsdal, "Calculating the acoustical room response by the use of a ray tracing technique," in <i>Journal of Sound and Vibration</i>, 1968, vol. 8, pp. 118--125.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>645</ref_seq_no>
				<ref_text><![CDATA[M. A. Biot and I. Tolstoy, "Formulation of wave propagation in infinite media by normal coordinates with an application to diffraction," <i>Journal of the Acoustical Society of America</i>, vol. 29, no. 3, pp. 381--391, March 1957.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>646</ref_seq_no>
				<ref_text><![CDATA[H. Medwin, "Shadowing by finite noise barriers," <i>Journal of the Acoustical Society of America</i>, vol. 69, no. 4, pp. 1060--1064, April 1981.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1289147</ref_obj_id>
				<ref_obj_pid>1288980</ref_obj_pid>
				<ref_seq_no>647</ref_seq_no>
				<ref_text><![CDATA[P. T. Calamia and U. P. Svensson, "Fast time-domain edge-diffraction calculations for interactive acoustic simulations," <i>EURASIP J. Appl. Signal Process.</i>, vol. 2007, no. 1, pp. 186--186, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>648</ref_seq_no>
				<ref_text><![CDATA[F. Antonacci, M. Foco, A. Sarti, and S. Tubaro, "Fast modeling of acoustic reflections and diffraction in complex environments using visibility diagrams," in <i>Proceedings of 12th European Signal Processing Conference (EUSIPCO '04)</i>, September 2004, pp. 1773--1776.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1477384</ref_obj_id>
				<ref_obj_pid>1477066</ref_obj_pid>
				<ref_seq_no>649</ref_seq_no>
				<ref_text><![CDATA[A. Chandak, C. Lauterbach, M. Taylor, Z. Ren, and D. Manocha, "Ad-frustum: Adaptive frustum tracing for interactive sound propagation," <i>IEEE Transactions on Visualization and Computer Graphics</i>, vol. 14, no. 6, pp. 1707--1722, December 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>650</ref_seq_no>
				<ref_text><![CDATA[M. Taylor, A. Chandak, L. Antani, and D. Manocha, "Resound: Interactive sound rendering for dynamic virtual environments," Tech. Rep., University of Chapel Hill, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>651</ref_seq_no>
				<ref_text><![CDATA[A. Chandak, L. Antani, M. Taylor, and D. Manocha, "Fastv: From-point visibility culling on complex models," Tech. Rep., University of Chapel Hill, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>652</ref_seq_no>
				<ref_text><![CDATA[M. Taylor and D. Manocha, "Fast accurate diffraction paths with frustum tracing," Tech. Rep., University of Chapel Hill, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1413373</ref_obj_id>
				<ref_obj_pid>1413370</ref_obj_pid>
				<ref_seq_no>653</ref_seq_no>
				<ref_text><![CDATA[Naga K. Govindaraju, Brandon Lloyd, Yuri Dotsenko, Burton Smith, and John Manferdelli, "High performance discrete fourier transforms on graphics processors," in <i>SC '08: Proceedings of the 2008 ACM/IEEE conference on Supercomputing</i>, Piscataway, NJ, USA, 2008, pp. 1--12, IEEE Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>654</ref_seq_no>
				<ref_text><![CDATA[U. R. Krockstadt, "Calculating the acoustical room response by the use of a ray tracing technique," <i>Journal of Sound Vibration</i>, 1968.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>655</ref_seq_no>
				<ref_text><![CDATA[J. B. Allen and D. A. Berkley, "Image method for efficiently simulating small-room acoustics," <i>J. Acoust. Soc. Am</i>, vol. 65, no. 4, pp. 943--950, 1979.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>656</ref_seq_no>
				<ref_text><![CDATA[J. H. Rindel, "The use of computer modeling in room acoustics," <i>Journal of Vibroengineering</i>, vol. 3, no. 4, pp. 219--224, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>657</ref_seq_no>
				<ref_text><![CDATA[Thomas Funkhouser, Nicolas Tsingos, Ingrid Carlbom, Gary Elko, Mohan Sondhi, James E. West, Gopal Pingali, Patrick Min, and Addy Ngan, "A beam tracing method for interactive architectural acoustics," <i>The Journal of the Acoustical Society of America</i>, vol. 115, no. 2, pp. 739--756, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>658</ref_seq_no>
				<ref_text><![CDATA[F. Antonacci, M. Foco, A. Sarti, and S. Tubaro, "Real time modeling of acoustic propagation in complex environments," <i>Proceedings of 7th International Conference on Digital Audio Effects</i>, pp. 274--279, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1477384</ref_obj_id>
				<ref_obj_pid>1477066</ref_obj_pid>
				<ref_seq_no>659</ref_seq_no>
				<ref_text><![CDATA[Anish Chandak, Christian Lauterbach, Micah Taylor, Zhimin Ren, and Dinesh Manocha, "Ad-frustum: Adaptive frustum tracing for interactive sound propagation," <i>IEEE Transactions on Visualization and Computer Graphics</i>, vol. 14, no. 6, pp. 1707--1722, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>660</ref_seq_no>
				<ref_text><![CDATA[M. Bertram, E. Deines, J. Mohring, J. Jegorovs, and H. Hagen, "Phonon tracing for auralization and visualization of sound," in <i>IEEE Visualization 2005</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>661</ref_seq_no>
				<ref_text><![CDATA[Nicolas Tsingos, <i>Simulating High Quality Dynamic Virtual Sound Fields For Interactive Graphics Applications</i>, Ph.D. thesis, Universite Joseph Fourier Grenoble I, December 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>662</ref_seq_no>
				<ref_text><![CDATA[Murray Hodgson and Eva M. Nosal, "Experimental evaluation of radiosity for room sound-field prediction," <i>The Journal of the Acoustical Society of America</i>, vol. 120, no. 2, pp. 808--819, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383323</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>663</ref_seq_no>
				<ref_text><![CDATA[Nicolas Tsingos, Thomas Funkhouser, Addy Ngan, and Ingrid Carlbom, "Modeling acoustics in virtual environments using the uniform theory of diffraction," in <i>Computer Graphics (SIGGRAPH 2001)</i>, August 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1289147</ref_obj_id>
				<ref_obj_pid>1288980</ref_obj_pid>
				<ref_seq_no>664</ref_seq_no>
				<ref_text><![CDATA[Paul T. Calamia and Peter U. Svensson, "Fast time-domain edge-diffraction calculations for interactive acoustic simulations," <i>EURASIP Journal on Advances in Signal Processing</i>, vol. 2007, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>665</ref_seq_no>
				<ref_text><![CDATA[Micah Taylor, Anish Chandak, Zhimin Ren, Christian Lauterbach, and Dinesh Manocha, "Interactive edge diffraction for sound propagation in complex virtual environments," Tech. Rep., Department of Computer Science, UNC Chapel Hill, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>666</ref_seq_no>
				<ref_text><![CDATA[Mendel Kleiner, Bengt-Inge Dalenb&#228;ck, and Peter Svensson, "Auralization - an overview," <i>JAES</i>, vol. 41, pp. 861--875, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>667</ref_seq_no>
				<ref_text><![CDATA[S. Van Duyne and J. O. Smith, "The 2-d digital waveguide mesh," in <i>Applications of Signal Processing to Audio and Acoustics, 1993. Final Program and Paper Summaries., 1993 IEEE Workshop on</i>, 1993, pp. 177--180.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1289420</ref_obj_id>
				<ref_obj_pid>1289340</ref_obj_pid>
				<ref_seq_no>668</ref_seq_no>
				<ref_text><![CDATA[Matti Karjalainen and Cumhur Erkut, "Digital waveguides versus finite difference structures: equivalence and mixed modeling," <i>EURASIP J. Appl. Signal Process.</i>, vol. 2004, no. 1, pp. 978--989, January 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>669</ref_seq_no>
				<ref_text><![CDATA[L. Savioja, <i>Modeling Techniques for Virtual Acoustics</i>, Doctoral thesis, Helsinki University of Technology, Telecommunications Software and Multimedia Laboratory, Report TML-A3, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>670</ref_seq_no>
				<ref_text><![CDATA[D. Murphy, A. Kelloniemi, J. Mullen, and S. Shelley, "Acoustic modeling using the digital waveguide mesh," <i>Signal Processing Magazine, IEEE</i>, vol. 24, no. 2, pp. 55--66, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>671</ref_seq_no>
				<ref_text><![CDATA[D. Botteldooren, "Finite-difference time-domain simulation of low-frequency room acoustic problems," <i>Acoustical Society of America Journal</i>, vol. 98, pp. 3302--3308, December 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>672</ref_seq_no>
				<ref_text><![CDATA[Shinichi Sakamoto, Takuma Seimiya, and Hideki Tachibana, "Visualization of sound reflection and diffraction using finite difference time domain method," <i>Acoustical Science and Technology</i>, vol. 23, no. 1, pp. 34--39, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>673</ref_seq_no>
				<ref_text><![CDATA[S. Sakamoto, T. Yokota, and H. Tachibana, "Numerical sound field analysis in halls using the finite difference time domain method," in <i>RADS 2004</i>, Awaji, Japan, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>674</ref_seq_no>
				<ref_text><![CDATA[R. Rabenstein, S. Petrausch, A. Sarti, G. De Sanctis, C. Erkut, and M. Karjalainen, "Block-based physical modeling for digital sound synthesis," <i>Signal Processing Magazine, IEEE</i>, vol. 24, no. 2, pp. 42--54, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>675</ref_seq_no>
				<ref_text><![CDATA[Allen Taflove and Susan C. Hagness, <i>Computational Electrodynamics: The Finite-Difference Time-Domain Method, Third Edition</i>, Artech House Publishers, June 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>130635</ref_obj_id>
				<ref_seq_no>676</ref_seq_no>
				<ref_text><![CDATA[Charles Van Loan, <i>Computational Frameworks for the Fast Fourier Transform</i>, Society for Industrial Mathematics, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>677</ref_seq_no>
				<ref_text><![CDATA[Y. S. Rickard, N. K. Georgieva, and Wei-Ping Huang, "Application and optimization of pml abc for the 3-d wave equation in the time domain," <i>Antennas and Propagation, IEEE Transactions on</i>, vol. 51, no. 2, pp. 286--295, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1364916</ref_obj_id>
				<ref_obj_pid>1364901</ref_obj_pid>
				<ref_seq_no>678</ref_seq_no>
				<ref_text><![CDATA[Nikunj Raghuvanshi, Nico Galoppo, and Ming C. Lin, "Accelerated wave-based acoustics simulation," in <i>SPM '08: Proceedings of the 2008 ACM symposium on Solid and physical modeling</i>, New York, NY, USA, 2008, pp. 91--102, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1592280</ref_obj_id>
				<ref_obj_pid>1591897</ref_obj_pid>
				<ref_seq_no>679</ref_seq_no>
				<ref_text><![CDATA[Nikunj Raghuvanshi, Rahul Narain, and Ming C. Lin, "Efficient and accurate sound propagation using adaptive rectangular decomposition," <i>IEEE Transactions on Visualization and Computer Graphics</i>, vol. 99, no. 1, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>680</ref_seq_no>
				<ref_text><![CDATA[Domain decomposition method. http://www.ddm.org.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>681</ref_seq_no>
				<ref_text><![CDATA[Soundscapes in half-life 2, valve corporation. http://developer.valvesoftware.com/wiki/Soundscapes, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>682</ref_seq_no>
				<ref_text><![CDATA[J. B. Allen and D. A. Berkley. Image method for efficiently simulating small-room acoustics. <i>J. Acoust. Soc. Am</i>, 65(4):943--950, 1979.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>683</ref_seq_no>
				<ref_text><![CDATA[F. Antonacci, M. Foco, A. Sarti, and S. Tubaro. Real time modeling of acoustic propagation in complex environments. <i>Proceedings of 7th International Conference on Digital Audio Effects</i>, pages 274--279, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>684</ref_seq_no>
				<ref_text><![CDATA[M. Bertram, E. Deines, J. Mohring, J. Jegorovs, and H. Hagen. Phonon tracing for auralization and visualization of sound. In <i>IEEE Visualization 2005</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>685</ref_seq_no>
				<ref_text><![CDATA[N. Bonneel, G. Drettakis, N. Tsingos, I. V. Delmon, and D. James. Fast modal sounds with scalable frequency-domain synthesis, August 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>686</ref_seq_no>
				<ref_text><![CDATA[D. Botteldooren. Acoustical finite-difference time-domain simulation in a quasi-cartesian grid. <i>The Journal of the Acoustical Society of America</i>, 95(5):2313--2319, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>687</ref_seq_no>
				<ref_text><![CDATA[D. Botteldooren. Finite-difference time-domain simulation of low-frequency room acoustic problems. <i>Acoustical Society of America Journal</i>, 98:3302--3308, December 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>688</ref_seq_no>
				<ref_text><![CDATA[J. P. Boyd. <i>Chebyshev and Fourier Spectral Methods: Second Revised Edition.</i> Dover Publications, December 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1289147</ref_obj_id>
				<ref_obj_pid>1288980</ref_obj_pid>
				<ref_seq_no>689</ref_seq_no>
				<ref_text><![CDATA[P. T. Calamia and P. U. Svensson. Fast time-domain edge-diffraction calculations for interactive acoustic simulations. <i>EURASIP Journal on Advances in Signal Processing</i>, 2007, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>690</ref_seq_no>
				<ref_text><![CDATA[C. A. de Moura. Parallel numerical methods for differential equations - a survey.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2384841</ref_obj_id>
				<ref_obj_pid>2384796</ref_obj_pid>
				<ref_seq_no>691</ref_seq_no>
				<ref_text><![CDATA[E. Deines, F. Michel, M. Bertram, H. Hagen, and G. Nielson. Visualizing the phonon map. In <i>Eurovis</i>, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882339</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>692</ref_seq_no>
				<ref_text><![CDATA[Y. Dobashi, T. Yamamoto, and T. Nishita. Real-time rendering of aerodynamic sound using sound textures based on computational fluid dynamics. <i>ACM Trans. Graph.</i>, 22(3):732--740, July 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>693</ref_seq_no>
				<ref_text><![CDATA[Durlach. Virtual reality scientific and technological challenges. Technical report, National Research Council, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>694</ref_seq_no>
				<ref_text><![CDATA[M. Frigo and S. G. Johnson. The design and implementation of fftw3. <i>Proc. IEEE</i>, 93(2):216--231, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>695</ref_seq_no>
				<ref_text><![CDATA[T. Funkhouser, N. Tsingos, I. Carlbom, G. Elko, M. Sondhi, J. E. West, G. Pingali, P. Min, and A. Ngan. A beam tracing method for interactive architectural acoustics. <i>The Journal of the Acoustical Society of America</i>, 115(2):739--756, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1413373</ref_obj_id>
				<ref_obj_pid>1413370</ref_obj_pid>
				<ref_seq_no>696</ref_seq_no>
				<ref_text><![CDATA[N. K. Govindaraju, B. Lloyd, Y. Dotsenko, B. Smith, and J. Manferdelli. High performance discrete fourier transforms on graphics processors. In <i>SC '08: Proceedings of the 2008 ACM/IEEE conference on Supercomputing</i>, pages 1--12, Piscataway, NJ, USA, 2008. IEEE Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>697</ref_seq_no>
				<ref_text><![CDATA[M. Hodgson and E. M. Nosal. Experimental evaluation of radiosity for room sound-field prediction. <i>The Journal of the Acoustical Society of America</i>, 120(2):808--819, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141983</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>698</ref_seq_no>
				<ref_text><![CDATA[D. L. James, J. Barbic, and D. K. Pai. Precomputed acoustic transfer: output-sensitive, accurate sound generation for geometrically complex vibration sources. <i>ACM Transactions on Graphics</i>, 25(3):987--995, July 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1289420</ref_obj_id>
				<ref_obj_pid>1289340</ref_obj_pid>
				<ref_seq_no>699</ref_seq_no>
				<ref_text><![CDATA[M. Karjalainen and C. Erkut. Digital waveguides versus finite difference structures: equivalence and mixed modeling. <i>EURASIP J. Appl. Signal Process.</i>, 2004(1):978--989, January 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>700</ref_seq_no>
				<ref_text><![CDATA[L. E. Kinsler, A. R. Frey, A. B. Coppens, and J. V. Sanders. <i>Fundamentals of Acoustics.</i> Wiley, December 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>701</ref_seq_no>
				<ref_text><![CDATA[M. Kleiner, B.-I. Dalenb&#228;ck, and P. Svensson. Auralization - an overview. <i>JAES</i>, 41:861--875, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>702</ref_seq_no>
				<ref_text><![CDATA[U. Krockstadt. Calculating the acoustical room response by the use of a ray tracing technique. <i>Journal of Sound Vibration</i>, 1968.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>703</ref_seq_no>
				<ref_text><![CDATA[H. Kuttruff. <i>Room Acoustics.</i> Taylor&amp;Francis, October 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>704</ref_seq_no>
				<ref_text><![CDATA[Q. H. Liu. The pstd algorithm: A time-domain method combining the pseudospectral technique and perfectly matched layers. <i>The Journal of the Acoustical Society of America</i>, 101(5):3182, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>705</ref_seq_no>
				<ref_text><![CDATA[T. Lokki. <i>Physically-based Auralization.</i> PhD thesis, Helsinki University of Technology, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618732</ref_obj_id>
				<ref_obj_pid>616064</ref_obj_pid>
				<ref_seq_no>706</ref_seq_no>
				<ref_text><![CDATA[M. Monks, B. M. Oh, and J. Dorsey. Audioptimization: Goal-based acoustic design. <i>IEEE Computer Graphics and Applications</i>, 20(3):76--91, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>707</ref_seq_no>
				<ref_text><![CDATA[D. Murphy, A. Kelloniemi, J. Mullen, and S. Shelley. Acoustic modeling using the digital waveguide mesh. <i>Signal Processing Magazine, IEEE</i>, 24(2):55--66, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>545290</ref_obj_id>
				<ref_obj_pid>545261</ref_obj_pid>
				<ref_seq_no>708</ref_seq_no>
				<ref_text><![CDATA[J. F. O'Brien, C. Shen, and C. M. Gatchalian. Synthesizing sounds from rigid-body simulations. In <i>SCA '02: Proceedings of the 2002 ACM SIGGRAPH/Eurographics symposium on Computer animation</i>, pages 175--181, New York, NY, USA, 2002. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>709</ref_seq_no>
				<ref_text><![CDATA[R. Petrausch and S. Rabenstein. Simulation of room acoustics via block-based physical modeling with the functional transformation method. <i>Applications of Signal Processing to Audio and Acoustics, 2005. IEEE Workshop on</i>, pages 195--198, 16--19 Oct. 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>710</ref_seq_no>
				<ref_text><![CDATA[A. Quarteroni and A. Valli. <i>Domain Decomposition Methods for Partial Differential Equations.</i> Oxford University Press, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>711</ref_seq_no>
				<ref_text><![CDATA[R. Rabenstein, S. Petrausch, A. Sarti, G. De Sanctis, C. Erkut, and M. Karjalainen. Block-based physical modeling for digital sound synthesis. <i>Signal Processing Magazine, IEEE</i>, 24(2):42--54, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1111429</ref_obj_id>
				<ref_obj_pid>1111411</ref_obj_pid>
				<ref_seq_no>712</ref_seq_no>
				<ref_text><![CDATA[N. Raghuvanshi and M. C. Lin. Interactive sound synthesis for large scale environments. In <i>SI3D '06: Proceedings of the 2006 symposium on Interactive 3D graphics and games</i>, pages 101--108, New York, NY, USA, 2006. ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>713</ref_seq_no>
				<ref_text><![CDATA[Y. S. Rickard, N. K. Georgieva, and W.-P. Huang. Application and optimization of pml abc for the 3-d wave equation in the time domain. <i>Antennas and Propagation, IEEE Transactions on</i>, 51(2):286--295, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>714</ref_seq_no>
				<ref_text><![CDATA[J. H. Rindel. The use of computer modeling in room acoustics.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>715</ref_seq_no>
				<ref_text><![CDATA[H. Sabine. Room acoustics. <i>Audio, Transactions of the IRE Professional Group on</i>, 1(4):4--12, 1953.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>716</ref_seq_no>
				<ref_text><![CDATA[S. Sakamoto, T. Seimiya, and H. Tachibana. Visualization of sound reflection and diffraction using finite difference time domain method. <i>Acoustical Science and Technology</i>, 23(1):34--39, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>717</ref_seq_no>
				<ref_text><![CDATA[S. Sakamoto, A. Ushiyama, and H. Nagatomo. Numerical analysis of sound propagation in rooms using the finite difference time domain method. <i>The Journal of the Acoustical Society of America</i>, 120(5):3008, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>718</ref_seq_no>
				<ref_text><![CDATA[S. Sakamoto, T. Yokota, and H. Tachibana. Numerical sound field analysis in halls using the finite difference time domain method. In <i>RADS 2004</i>, Awaji, Japan, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>719</ref_seq_no>
				<ref_text><![CDATA[L. Savioja. <i>Modeling Techniques for Virtual Acoustics.</i> Doctoral thesis, Helsinki University of Technology, Telecommunications Software and Multimedia Laboratory, Report TML-A3, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>720</ref_seq_no>
				<ref_text><![CDATA[K. L. Shlager and J. B. Schneider. A selective survey of the finite-difference time-domain literature. <i>Antennas and Propagation Magazine, IEEE</i>, 37(4):39--57, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>721</ref_seq_no>
				<ref_text><![CDATA[S. Siltanen. Geometry reduction in room acoustics modeling. Master's thesis, Helsinki University of Technology, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>722</ref_seq_no>
				<ref_text><![CDATA[A. Taflove and S. C. Hagness. <i>Computational Electrodynamics: The Finite-Difference Time-Domain Method, Third Edition.</i> Artech House Publishers, June 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134063</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>723</ref_seq_no>
				<ref_text><![CDATA[T. Takala and J. Hahn. Sound rendering. <i>SIGGRAPH Comput. Graph.</i>, 26(2):211--220, July 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>724</ref_seq_no>
				<ref_text><![CDATA[A. Toselli and O. Widlund. <i>Domain Decomposition Methods.</i> Springer, 1 edition, November 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>725</ref_seq_no>
				<ref_text><![CDATA[N. Tsingos. <i>Simulating High Quality Dynamic Virtual Sound Fields For Interactive Graphics Applications.</i> PhD thesis, Universite Joseph Fourier Grenoble I, December 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383864</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>726</ref_seq_no>
				<ref_text><![CDATA[N. Tsingos, C. Dachsbacher, S. Lefebvre, and M. Dellepiane. Instant sound scattering. In <i>Rendering Techniques (Proceedings of the Eurographics Symposium on Rendering)</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383323</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>727</ref_seq_no>
				<ref_text><![CDATA[N. Tsingos, T. Funkhouser, A. Ngan, and I. Carlbom. Modeling acoustics in virtual environments using the uniform theory of diffraction. In <i>Computer Graphics (SIGGRAPH 2001)</i>, August 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383322</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>728</ref_seq_no>
				<ref_text><![CDATA[K. van den Doel, P. G. Kry, and D. K. Pai. Foleyautomatic: physically-based sound effects for interactive simulation and animation. In <i>SIGGRAPH '01: Proceedings of the 28th annual conference on Computer graphics and interactive techniques</i>, pages 537--544, New York, NY, USA, 2001. ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>729</ref_seq_no>
				<ref_text><![CDATA[S. Van Duyne and J. O. Smith. The 2-d digital waveguide mesh. In <i>Applications of Signal Processing to Audio and Acoustics, 1993. Final Program and Paper Summaries., 1993 IEEE Workshop on</i>, pages 177--180, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>730</ref_seq_no>
				<ref_text><![CDATA[K. Yee. Numerical solution of inital boundary value problems involving maxwell's equations in isotropic media. <i>Antennas and Propagation, IEEE Transactions on {legacy, pre - 1988}</i>, 14(3):302--307, 1966.16]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Interactive Sound Rendering SIGGRAPH COURSE: 2009  http://gamma.cs.unc.edu/SOUND09  Organizer: Dinesh 
Manocha Lecturers (alphabetic order): Paul Calamia, Ming C. Lin, Dinesh Manocha, Lauri Savioja, Nicolas 
Tsingos Abstract: This course covers algorithmic and software technologies for interactive sound rendering. 
The presentation material will be covered at an introductory level and the audience is not expected to 
have any significant background in acoustic simulation. The course lectures cover three main parts: Overview 
of different physically-based techniques to synthesize sounds generated from colliding objects to liquid 
sounds; basic techniques used to turn the computed sound reflection or diffraction paths into audible 
sound; exploit the computational capabilities of current multi-core commodity processors for real-time 
sound propagation and rendering for gaming and interactive applications. The presentations will include 
audio demonstrations that will highlight various processing components in practice. Each instructor has 
a research group that has been working on new technologies related to sound synthesis, sound propagation 
or auralization. Moreover some of the instructors have worked closely with game developers in integrating 
these technologies. The presentation material is rather introductory and audience will get an overview 
of sound rendering as well as some recent research developments. Index of Slides Introduction (Manocha) 
................................................................................................................................ 
4 Auralization (Savioja)................................................................................................................................. 
13 Spatial Auditory Display (Tsingos and Savioja)......................................................................................... 
39 Physics-based Sound Synthesis with a Novel Friction Model (Ren, Yeh and Lin).................................... 
47 Sounding Liquids: Automatic Sound Synthesis from Fluid Simulations (Moss, Yeh, Hong, Lin and Manocha)............................................................................................................................................ 
77 Geometric Sound Propagation (Chandak and Manocha)............................................................................ 
82 Interactive Sound Rendering Simulating Diffraction (Calamia) ........................................................... 
119 Numerical Sound Propagation Using Adaptive Rectangular Decomposition (Raghuvanshi, Narain, Galoppo 
and Lin) ............................................................................................................................. 
138 Acceleration on Many-Cores CPUs and GPUs (Manocha and Savioja) .................................................. 
141 Numerical Acoustics with Adaptive Rectangular Decomposition on the GPU (Raghuvanshi, Lloyd, Govindaraju 
and Lin) ....................................................................................................................... 
145 Integration into Game Engines (Tsingos) ................................................................................................. 
151 Index of Papers N. Raghuvanshi, C. Lauterbach, A. Chandak, D. Manocha and M. Lin, Real-Time Sound 
Synthesis and Propagation for Games, Communications of the ACM, Vo. 50, No. 7, July 2007, pp. 67-73................. 
160 P. Calamia, B. E. Markham and U. P. Svennson, Diffraction Culling for Virtual-Acoustic Simulations, 
ACTA ACUSTICA UNITED WITH ACUSTICA, Vol. 94, 2008, pp. 907-920 .......................................... 
167 P. Calamai and U. P. Svensson, Fast Time-Domain Edge-Diffraction Calculations for Interactive Acoustic 
Simulations, EURASIP Journal on Advances in Signal Processing, ARTICLE ID 63560, 10 pages, 2007 ............................................................................................................................................... 
181 N. Bonneel, G. Drettakis, N. Tsingos, I. Viaud-Delmon and D. James, Fast Modal Sounds with Scalable 
Frequency-Domain Synthesis, Proc. Of ACM SIGGRAPH, 2008, 10 pages.......................................... 
191 T. Moeck, N. Bonneel, N. Tsingos, G. Drettakis, I. Viaud-Delmon and D. Alloza, Progressive Perceptual 
Audio Rendering of Complex Scenes, Proceedings of Symposium on Interactive 3D Graphics, pp. 189­196, 
2007 .................................................................................................................................................. 
201 N. Tsingos, Using Programmable Graphics Hardware for Auralization, Proc. Of the EAA Symposium on 
Auralization, 2009, 10 pages................................................................................................................ 
209 L. Savijoa, J. Huopaniemi, T. Lokki, and R. Vannanen, Creating Interactive Virtual Acoustic Environments, 
Journal of Audio Engineering Society, Vol. 47, No. 9, pp. 675-705, September 1999 .. 219 N. Tsingos, 
E. Gallo, and G. Drettakis, Perceptual Audio Rendering of Complex Virtual Environments, Proc. Of ACM 
SIGGRAPH, 2004, 10 pages ............................................................................................. 
250 U. P. Svensson, R. Fred, and J. Vanderkooy, An analytic secondary source model of edge diffraction 
impulse responses, Journal of Acoustic Society of America, vol. 105, number 5, 14 pages, November 1999 
.......................................................................................................................................................... 
260 N. Raghuvanshi and M. Lin, Interactive Sound Synthesis for Large Scale Environments, Proceedings 
of Symposium on Interactive 3D Graphics, 9 pages, 2006 ........................................................................... 
274 N. Tsingos, T. Funkhouser, A. Ngan, and I. Carlbom, Modeling Acoustics in Virtual Environments, 
Proc. Of ACM SIGGRAPH, 2001, 9 pages............................................................................................... 
283 A. Chandak, L. Antani, M. Taylor, and D. Manocha, FastV: From-point Visibility Culling and Application 
to Sound Rendering, Technical Report TR09-008, Department of Computer Science, UNC Chapel Hill, 2009...................................................................................................................................... 
294 C. Lauterbach, A. Chandak, and D. Manocha, Interactive Sound Rendering in Complex and Dynamic Scenes 
using Frustum Tracing, IEEE Transactions on Visualization and Computer Graphics, volume 13, number 6, 
8 pages, 2007 ........................................................................................................................... 
301 A. Chandak, C. Lauterbach, M. Taylor, Z. Ren and D. Manocha, AD-Frustum: Adaptive Frustum Tracing 
for Interactive Sound Propagation, IEEE Transactions on Visualization and Computer Graphics, volume 14, 
number 6, 8 pages, 2008........................................................................................ 
309 M. Taylor, A. Chandak, Z. Ren, C. Lauterbach, and D. Manocha, Fast Edge-Diffraction for Sound Propagation 
in Complex Virtual Environments, Proceedings of EAA Auralization Symposium, 2009, 6 pages ......................................................................................................................................................... 
317 N. Raghuvanshi, B. Lloyd, N. Govindaraju and M. C. Lin, Efficient Numerical Acoustic Simulation on 
Graphics Processors using Adaptive Rectangular Decomposition, Proceedings of EAA Auralization Symposium, 
2009, 6 pages ........................................................................................................................ 
323 N. Raghuvanshi, R. Narain, and M. C. Lin, Efficient and Accurate Sound Propagation Using Adaptive 
Rectangular Decomposition, IEEE Transactions on Visualization and Computer Graphics, 10 pages, 2009 .......................................................................................................................................................... 
329  Scientific Visualization Modeling Propagation 3D Audio Rendering  Acoustic vs. Graphics Low 
geometric detail vs Acoustic Geometry  Low geometric detail vs. High geometric detail  Modeling  
Acoustic vs. Graphics 343 m/s vs  343 m/s vs. 300,000,000 m/s  20 to 20K Hz vs. RGB  17m to 17cm 
vs. 700 to 400 nm    Acoustic vs. Graphics Compute intensive DSP  Compute intensive DSP vs. addition 
of colors  44.1 KHz vs. 30 Hz  Psychoacoustics vs. Visual psychophysics  Advanced Interfaces  Multi-sensory 
Visualization       Multi sensory Visualization Minority Report (2002) Multi-variate Data Visualization 
 Games  VR Training  VR Training Game (Half-Life 2) Medical Personnel Training   Strict time budget 
for audio simulations simulations  Games are dynamic Moving sound sources Moving listeners Moving scene 
geometry  TradeTrade-off speed with the accuracy ofoff speed with the accuracy of the simulation  Static 
environment effects (assigned to regions in the scene)     8:45am: Sound Rendering (Savioja)) Sound 
Rendering (Savioja9:20am: Interactive Sound Synthesis (Lin) 9:20am: Interactive Sound Synthesis (Lin) 
9:50am: Perceptual Audio Rendering (Tsingos)) Perceptual Audio Rendering (TsingosSound Propagation (Manocha)) 
10:30am: Geometric Sound Propagation (ManochaSimulating Diffraction (Calamia)) 10:50am: Simulating Diffraction 
(Calamia 11:20am: Numeric Sound Propagation (Lin) 11:40am: {Multi,Many}}--Core Accelerations (Manocha 
and Savioja)) 11:40am: {Multi,ManyCore Accelerations (Manochaand SaviojaEngines (Tsingos)) 12:00pm: 
Integration into Game Engines (Tsingos   Auralization, i.e., sound rendering  Impulse response  Basic 
principle + Marienkirche demo  Source signals and modeling of directivity of sources  Modeling from 
perceptual point of view  Dynamic auralization  Evaluation of auralization quality  Spatial sound 
reproduction  Headphones  Loudspeakers  A linear time-invariant system (LTI) can be modeled with an 
impulse response  The output y(t) is the convolution of the input x(t)  Goal 1: room acoustics prediction 
 Static source and receiver Static source and receiver positions  No real-time requirement  Goal 
2: auralization, sound rendering  Possibly moving source(s) and listener, even geometry  Both off-line 
and interactive (real-time) applications  Need of anechoic stimulus signals       (Binaural rendering, 
Lokki, 2002)  - Auralization is the process of rendering audible, by physical or mathematical modeling, 
the sound field of a source in a space, in such a way as to simulate the binaural listening experience 
at a given position in the modeled space. (Kleiner et al. 1993, JAES) -Sound rendering: plausible 3-D 
sound, e.g., in games  3-D model . spatial IR * dry signal = auralization  Goal: Plausible 3-D sound, 
authentic auralization The most intuitive way to study room acoustic predictionThe most intuitive way 
to study room acoustic prediction results Not only for experts  Anechoic stimulus signal  Reproduction 
with binaural or multichannel techniques  Impulse response has to contain also spatial information 
 Input data:  Anechoic stimulus signal(s) ! Anechoic stimulus signal(s) !  Geometry + material data 
 source(s) and receiver(s) locations and orientations  Source(s): omnidirectional, sometimes directional 
 Medium:    Medium: physically-based sound propagation in a room  perceptual models, i.e., artificial 
reverb  Receiver: spatial sound reproduction (binaural or multichannel)  Stimulus  Sound signal synthesis 
 Anechoic recordings  Directivity is a measure of the directional characteristic of a     directional 
characteristic of a sound source. Point sources omnidirectional  frequency dependent directivity characterist 
ics  Line and volume sources  Database of loudspeakers  http://www.clfgroup.org/  In a concert hall 
typical sound source is an orchestra  Anechoic recordings needed  Directivity of instruments also needed 
 We have just completed such recordings  Demo  All recordings with 22 microphones  Recordings are 
publicly available for Academic purposes  Contact: Tapio.Lokki@tkk.fi  http://auralization.tkk .fi 
   Computation vs. Frequency resolution Computation vs. Time resolution (Svensson &#38; Kristiansen 
2002)   Perceptually-based modeling  Impulse response is not computed with a geometry  Impulse response 
is not computed with a geometry A statistical response is applied Psychoacoustical (subjective) parameters 
are applied in tuning the response e.g. reverberation time, clar ity, warmness, spaciousness  Applications: 
music production teleconferencing computer  Applications: music production, teleconferencing, computer 
games...  Physically-based modeling  Sound propagation and reflections of boundaries are Sound propagation 
and reflections of boundaries are modeled based on physics.  Impulse response is predicted based on 
the geometry and its properties depend on surface materials, directivity and position of sound source(s) 
as well as position and orientation of the listener  Applications: prediction of acoustics, concert 
hall design, virtual auditory environments for games and virtual reality applications, education, ... 
    For the direct sound and each image source the followingFor the direct sound and each image source 
the following set of auralization parameters is provided: Distance from the listener  Azimuth and elevation 
angles with respect to the listener  Source orientation with respect to the listener  Reflection data, 
e.g. as a set of filter coefficients which describe the material properties in reflections  Directivity 
 Air absorpption  Distance attenuation  Reflection filters  Listener modeling  Linear system  Commutation 
 Cascading  Dynamic rendering  Propperties of imagge sources are time variant           
      The coefficients of filters are changing all the time  Every single parameter has to be 
interpolated  In delay line pick-ups the fractional delay filters have tobe used to avoid clicks and 
artifacts  Late reverberation is static  Update rate latency  What is the wanted quality?  Assesment 
of quality is possible only by case studies   Assesment of quality is possible only by case studies 
 Objectively:  Acoustical attributes  With auditory modeling  Subjectively:  Listening tests   
 Stimuli: clarinet drum Results clarinet: recording auralization Results drum: recording auralization 
   Reproduce the correct perceived location/direction of a virtual sound source to the ears of the 
listener  Headphone or speaker based.  Binaural stereo Multiple speakers  Natural filtering of the 
ears and torso  Apply a directional filtering to the signal  Apply a directional filtering to the signal 
 Head Related Transfer Functions (HRTFs)  Headphones (binaural)  Speaker pair (transaural)  Modeling 
 Finite element techniques   Finite element techniques Measuring  Dummy-heads  Human listener 
 HRTFs strongly depend on the listener  Morphological differences  Adaptation by scaling in frequency 
domain  Filters separated into two parts:  1. Inter-aural time difference ((ITD))  2. Minimum-phase 
FIR-filter  In movements:  Linear interpolation of ITD  Bilinear interpolation for FIR  Principal 
component analysis    Principal component analysis HRTF is a linear combination of eigenfilters 
 Allows for smooth interpolation  Allows for reducing the number of operations  Cross talk cancellation 
 Hll and Hand Hrr are HRTFsHRTFs  Hare ll rr Hrl and Hlr ?  The common surround sound    Apply 
the proper gain to every speaker to reproduce theApply the proper gain to every speaker to reproduce 
the proper perceived direction in 2D pair of loudspeakers  Spherical harmonics decomposition of the 
pressure field at a given point  1st order spherical harmonics  Sound field can be reproduced from 
4 components  1 omnidirectional and 3 orthogonal figure-of-8  Allows for manipulating the sound-field 
 Rotations, etc.  Reproduce the exact wave-field in the reproduction regions  Use speakers on the 
boundary  Kirchoff integral theorem  Sound field valid everywhere in the room  Heavy resources  In 
practice limited to a planar configuration   T echnique T echnique Setup Setup DSP DSP elev ation elev 
ation imaging imaging Sw eet Sw eet recordi recordi (# chans) spot ng HRTF light (2 ) moderate yeyes 
v .good n/a yeyes T r ansaural light (2 +) moderate yeyes good smallsmall yesyes Am p lit u d e p Panning 
av erag e av erag e (5+) low ye s ye s (3D array ) av erag e av erag e me dium me dium no no A m bis 
onics av erag e (4+) moderate ye s (3D array ) good smallsmall yesyes WF S h eav y (100+ ) high ? v .good 
n/a ?  Comparison  . Binaural systems for desktop use  Includes stereo transaural  Includes stereo 
transaural . Multi-speaker systems for multi-user  Well suited to immersive projection-based VR systems 
  lowpass filters Projection screens act as low--pass filtersVid j i  Vid eo pro j ec titi on cons 
t ra i n t s  . Overall system latency  Less than 100ms is OK . Tracking the user s head  Update 
binaural/transaural filters  Correction of loudspeakers gains ..Room problemsRoom problems  Reflective 
surfaces  Auralization  Direct convolution with full directional impulse responses   Direct convolution 
with full directional impulse responses Computationally too heavy in practice Parametric impulse response 
rendering Early reflections treated separately  Statistic late reverberation   Spatial sound reproduction 
 Headphones: HRTFs  Loudspeakers: VBAP, Ambisonics, Wave Field Synthesis   Thank you for your attention! 
Contact: Lauri.Savioja@tkk.fi htt p p://auralization.tkk.fi                     
                              Work in physics and engineering literature 
since 1917 SSoundd generated b d by resonatiting bbubblbbles Physically-based Models for Liquid Sounds 
(van den Doel, 2005) Spherical bubble model  No fluid simulator coupling  No fluid simulator coupling 
 Hand tune bubble profile  Grid-based methods   AAccurate to grid id resollutiion Bubbles can be 
smaller Slow  Can be two-phase   Shallow Water Equations Si l Simulate water surfface No breaking 
waves Real time  One phase   Explicit bubbles  Generate sound from existing fluid simulation  
  Mdl d db bbbl Model sound generated by bubbles Apply model to two types of fluid simulators Particle-Grid-based 
 Shallow Water Equations Extract bubbles Processes surface Curvature and velocity  Process spherical 
and  non-spherical bubbles Select bubble from distribution  Generate sound  Generate sound  . 
Non spherical bubbles Non-spherical bubbles Decompose into a spherical harmonics     Army Research 
Office   Carolina Development Foundation  Intel Corporation   National Science Foundation  RDECOM 
     Numerical Methods  Solve Helmholtz Wave Equation  Accurate  Compute intensive (fourth power 
of frequency)  Not practical for interactive applications  Methods  Finite Element Methods [Otsuru,2004] 
  Boundary Element Methods [Ciskowski,1993]  Finite Difference Time Domain [Kunz,1993]  Digital Waveguide 
Mesh (DWM) [Savioja,1994]  Domain Decomposition [Raghuvanshi,2008]  Geometric Methods  Ray-Approximation 
of Wave Equation  High-frequency approximation  Fast  Highly dependent on the geometry details  
Methods  Image Source [Borish,1984] [Dalenback,1992]  Ray Tracing [Krokstad,1968] [Kuttruff,1993] 
  Beam Tracing [Funkhouser,1998] [Funkhouser,1999]  Phonon Tracing [Kapralos,2004] [Bertram,2005] 
 Frustum Tracing [Lauterbach,2007] [Chandak,2008]  Acoustic Radiance Transfer [Siltanen,2007] [Siltanen,2009] 
 Shoot geometric primitives from sound source  The could cause:     Direct Contributions Reflection 
Contributions Diffraction Contributions  Very high update rate for direct contributions  ~20-30 Hz 
update rate for higher order contributions  [Berkley,1979] [Borish,1984] [Dalenback,1992]  Input: 
    Input: point sound source point listener scene geometry with acoustic properties Output: pressure 
impulse response (IR)  [Berkley,1979] [Borish,1984] [Dalenback,1992]  Advantages  Advantages  Geometrically 
accurate simulation  No aliasing issues, especially for dynamic scenes  Hybrid approaches are popular 
 Disadvantages  Exponential blow up of virtual image sources  Too slow for dynamic and interactive 
applications  Handles only specular reflection  Geometric sound propagation approach Computes virtual 
image sources recursively from a soundComputes virtual image sources recursively from a sound source 
 Accurately find all the geometric paths from source to the listener Impulse Response (IR) is constructed 
from the contributing pathscontributing paths  The impulse response is a pressure IR which is convolved 
with input dry signal      S1 S2 S5  S3 S4  S1 S2 S5   S3 S4    S1 S2 S5   S3  S1 
   S1 S2 S5   S3 S4  S1    S13 S13 S14  S1  S15  S13 S13 S14  S1 S1    S15  S13 
S13 S14    S1 S15  S13  S13 S14  Two Step Algorithm  1. Computing image sources (From- -point 
Visibilit y )  2. Validating paths from source to the listener    Computi ng image sources  Path 
V a lidation    [Krokstad,1968] [Kulowski,1984]  Input:  Input: spherical sound source spherical 
listener scene geometry with acoustic properties Output: energy impulse response (IR) convert energy 
IR into pressure IR for 3D audio rendering Note: audio signal is a function of pressure [Krokstad,1968] 
[Kulowski,1984]  Advantages  Very Very Fast. Maps well to modern CPU and GPU architectures  Advanced 
field in Computer Graphics  Handles dynamic scenes efficiently  Handles diffuse reflection . Disadvantages 
  Disadvantages  Sampling and aliasing issues  Aggressive acoustic simulation  3D Audio Rendering 
artifacts in dynamic scenarios  Cannot handle diffraction            To compute sound signal 
at a point add sound p pressure of all contributions  Phase angles of pn and pm are different and for 
quite a large number of components   [Kuttruff, 2007]   [Funkhouser,1998] [Funkhouser,1999]  Input: 
  Input: point sound source point listener scene geometry with acoustic properties Output: pressure 
impulse response (IR)  [Funkhouser,1998] [Funkhouser,1999]  Advantages  Geometrically accurate simulation 
 No aliasing issues, especially for dynamic scenes  Has a pre-processing and interactive stages  Can 
handle moving listener  Handles diffraction  Disadvantages  Expensive pre-processing step  Cannot 
handle dynamic sound sources or geometry  Cannot handle diffuse reflection      [Funkhouser,1998] 
      Compute Beam Tree  Node Information  Cell ID  Beam and its apex  Cell boundary these 
images.  Parent node ID  Attenuation  Find cell, C, containing listener (log N)  For each beam in 
C check for listener is inside it  Yes, then a path exist  Attenuation, path length, and direction 
can be computed quickly  Construct path by traversing the beam tree  Compute Impulse Response (IR) 
             Convolve IR with input sound signal  Use t he directional paths to simulate 
3D audio Use the directional paths to simulate 3D audio using HRTFs  Impulse Response (IR) * Sound Signal 
= Output Audio Need to cite the authors of these images.   [Lauterbach,2007] [Chandak,2008]  Input: 
  Input: point sound source point listener scene geometry with acoustic properties Output: pressure 
impulse response (IR)  [Lauterbach,2007] [Chandak,2008]  Advantages    Advantages  Very Very Fast 
  Handles moving sources and listeners  Handles complex and dynamic geometry  Handles diffraction 
 Disadvantages  Could miss some important contributions  Cannot handle diffuse reflection  If receiver 
is inside frustum     Theater 54 .s Factory 174 .s Game 14K .s Sibenik 71K .s City 72K .s SodaHa 
ll 1.5M .s diffraction diffraction NONO NONO NONO NONO YESYES YESYES #frusta 56K 40K 206K 198K 80K 108K 
time (msec) 33 27 273 598 206 373          Calculate path back t o s ource  Calculate path 
back to source  Attenuate path and add to IR  Convolve audio with IR  Out p put final audio sam 
p ple   [Kapralos,2004] [Bertram,2005]  Inspired from Photon Tracing [Jensen,2001] Inspired from 
Photon Tracing [Jensen,2001]  Input: point sound source point listener scene geometry with acoustic 
properties  Output:  Output: energy impulse response (IR)  [Kapralos,2004] [Bertram,2005]  Advantages 
   Advantages  Handles diffuse reflection efficiently  Disadvantages  Compute intensive  Cannot 
handle dynamic source and geometry      Motivation: Why Diffraction?  Simulation Methods  Frequency 
Domain: Uniform Theory of Diffraction (UTD)  Time Domain: Biot-Tolstoy-Medwin Formulation (BTM)  Acceleration 
Techniques  UTD: Frequency Interpolation  BTM: Edgge Subdivision  Both: Path Culling  Implementation 
Example: UTD with Frustum Tracing  Additional Resources  Wavelengths of audible sounds can be comparable 
to (or larger than) object dimensions so diffraction is an important acoustic propagation phenomenon 
 Unlike wave-based simulation techniques, geometrical-acoustics (GA) techniques omit diffraction  Incorrect 
reflection behavior from small surfaces  No propagation around occluders / into shadow zones  Sound-field 
discontinuities at reflection and shadow boundaries  Example: reflection from a faceted arch    
from a faceted arch with and without diffraction Even with low­resolution geometry, GA + diffraction 
yields a continuous sound field Images courtesy of Peter Svensson, NTNU     Uniform Theory of Diffraction 
(UTD)  Keller 62, Kouyoumjian and Pathak 74  Typically used in the frequency domainalthough a time-domain 
formulation exists  Assumptions  Ideal wedge surfaces (perfectly rigid or soft)  High frequency 
 Ifiitl Infinitely llong eddges  Far-field source and receiver  For acoustic simulations see Tsingos 
et al. 01, Antonacci et al. 04, Taylor et al. 09  UTD gives the diffracted pressure as a function of 
incident pressure distance   incident pressure, distance attenuation, and a diffraction coefficient 
 p ()=p () M ·A(r , r )·D ·e-R diff inc SR  rS ·rR k =2p/ ., A = r +rr r + SR Angle of diffraction 
= angle of incidence (.d = .i)  Ray-like paths on a cone of diffraction      jkrR ()=p (M )· 
(, r pR Ar ) diff inc SR . .p+.+.. + .p+.-.. - SR SR cot. .·F(kLa (. +.))+cot. .·F(kLa (. -.)) .RS RS 
2n 2n ... .. .p-.+.. + .p-.-.. - . SR SR +cot..·F ((kLa ((. -.))))+cot..·F((kLa ((. +.)))) RRSS RR SS 
.. 2n 2 . 2 .. 2n .. +8 ± jX -jt2 ± 2 .2pnN -ß. ()=2 FX j Xe .edt a ()= .. 2 .. ß 2cos X .. rS ·rR 
L = sin2 .iN ±=-1, 0,1 r +r SR  Biot-Tolstoy-Medwin (BTM)  Biot and Tolstoy 52, Medwin 81, Biot and 
Tolstoy 52, Medwin 81, Svensson et al. 99  Typically used in the time domain although a frequency-domain 
formulation exists  Assumptions  Ideal wedge surfaces (perfectly rigid or soft) Ideal wedge surfaces 
(perfectly rigid or soft)  Point-source insonification  For acoustic simulations see Torres et al. 
01, Lokki et al. 02, Calamia et al. 07 and 08  Wedge  .W = exterior wedge angle  .. = pp//..WW is 
the wedge index    is the wedge index Source and Receiver: Edge-Aligned Cylindrical Coordinates (r,., 
z)  r = radial distance from the edge  . = angle measured from a face  z = distance along the edge 
 Other  Other m = dist. from source to edge point  l = dist. from receiver to edge point  A = apex 
point, point of shortest  path from S to R through the line containing the edge  n,2 hdiffr ()=- ..n 
. 4 z ßi dz 4p i =1 z ml n,1 sin(..i ) ßi = cosh(..) - cos(..i ) . =pp ±. ±. . ±. ±. i SR  .ml + (z 
- z )(z - z ). - SR .= cosh 1 . . rr . SR .   Four terms in UTD and BTM  When .W > p, two shadow boundaries 
and two reflection boundaries  When .W = p, only reflection boundaries but inter-reflections (order 
2, 3, ) are possible  Each diffraction term is associated with a zone boundary  Geometrical-acoustics 
sound  At the boundaries:field is discontinuous Diffracted field has a sin(.. )0 complimentary discontinuity 
BTM: ßi = i . cosh(..) - cos(..i )1 -1 to compensate ..i . UTD: cot. .=±8 2n ..  Source Reflection 
Shadow Position Boundary Boundary   Position Boundary Boundary Approximations exist to allow for numerically 
robust implementations BTM(S dC l i A ti 06)S il BTM (Svensson and Calamia, Acustica 06): Serial expansion 
around the apex point . .. ... ~ . B 1 zrange 1 zrange h ()n h ()n =- · 0 ·. arctan. ..- arctan. ... 
i 0 i 0 .. 2p B -B B B B 31 . B . 1 . 1 . 3 . . 3 ..  UTD (Kouyoumjian and Pathak 74): Approximation 
 UTD (Kouyoumjian and Pathak 74): Approximation valid in the neighborhood of the zone boundaries  ß=2pnN 
± m (p-e) .p±ß. ± -jp/4 -jp/4 p () cot. .·F (kLa ()ß) ne ( 2 kL sgn e-2kLee ) 2n ..   Reduce computation 
for each diffraction componentt  UTD: Frequency Interpolation  BTM: Edge Subdivision  Reduce the number 
of diffraction components through path culling  components through path culling Shadow Zone  Zone-Boundary 
Proximity  Magnitude of diffraction transfer function typically transfer function typically is smooth 
  Phase typically is ~linear  Compute UTD coefficients at a limited number of frequencies   (e.g. 
octave-band center frequencies 63, 125, 250, Frequency (Hz) , 8k, 16k Hz) and interpolate Magnitude 
(dB re. 1)   Sample-aligned edge segments:f hIR one for each IR sample Pros  Accurate  Good with 
approx for sample n  for sample n0  Cons  Slow to compute  Must be recalculated when S or R moves 
 Even edge segments  Pros  Trivial to compute  Independent of S and R positions  Cons  No explicit 
boundaries for n0 . harder to handle singularity  Requires a scheme for multi-sample distribution  
Hybrid Subdivision  Use a small number of samplle-alilignedd segmentts around the apex point  High 
accuracy for the impulsive (high energy) onset  Easy to use with approximations for h(n0)  Use even 
segments for the rest of the edge  Can be precomputed  Limited recalculation for moving source or receiver 
 35 1.2 m x 1.2 m rigid panels  Interpanel spacing 0.5 m . 5 m above 2 source and 2 receiver   
    5 m above 2 source and 2 receiver positions Evaluate  The number of sample-aligned segments: 
1 10  The size of the even segments: maximum sampple sppan of 40,, 100,, and 300  The numerical integration 
technique  1-Point (midpoint)  3-Point (Simpson s Rule)  5-Point (Compound Simpson s Rule with Romberg 
Extrapolation)  Option 1: For each wedge, compute diffraction only for paths in the shadow zone  Intuition: 
Sound field in the illuminated area around a wedge will be dominated by direct propagation and/or reflections, 
shadow zone will receive limited energy without diffraction  Pro: Allows propagation around obstacles 
 CCon: IIgnores GA discontiinuity at refl flectiion GAdi i boundary  Implementations described in 
Tsingos et al. 01, Antonacci et al. 04, Taylor et al. 09  Option 2: Compute diffraction only when amplitude 
is significant  Intuition: numerically/perceptually significant diffracted paths are those with highest 
amplitude and/or energy, typically those with the receiver close to a zone boundary  Pro: Eliminates 
large discontinuities in the simulated sound field  simulated sound field  Con: Does not allow for 
propagation deep into shadow zones  Implementation described in Calamia et al. 08  Goals  Find propagation 
paths around edges  Render at interactive rates  Allow dynamic sources, receivers, and geometry  
Method           Method  Frustum tracing with dynamic BVH acceleration  Diffraction only 
in the shadow region  Diffraction paths computed with UTD  Mark possible diffdiffractiting eddges 
 Exterior edges  Disconnected edges  Propagate frusta from source through    from source through 
scene    If receiver is inside frustum  Calculate path back to source  Attenuate path with UTD 
coefficient and add to IR  Convolve audio with IR . Output final audio sampleOutput final audio sample 
  Future Work  Direct comparison of UTD and BTM  Numerical accuracy Numerical accuracy  Computation 
time  Subjective Tests  Limited subjective tests of auralization with  diffraction Si  Static scenes 
  Torres et al. JASA 01  Calamia et al. Acustica 08   Dynamic scenes None  P. Calamia, B. Markham, 
and U. P. Svensson, Diffraction culling for virtual-acoustic simulations, Acta Acustica united with Acustica,Special 
Issue on Virtual Acoustics, 94(6), pp. 907 -920, 2008.  P. Calamia and U. P. Svensson, Fast time-domain 
edge-diffractioncalculations for interactive acoustic simulations, EURASIP Journal on Advances in Signal 
Processing, Special Issue on Spatial Sound and Virtual Acoustics, Article ID 63560, 2007.  A. Chandak, 
C. Lauterbach, M. Taylor, Z. Ren, and D. Manocha, A. Chandak, C. Lauterbach, M. Taylor, Z. Ren, and 
D. Manocha, ADFrustum: Adaptive frustum tracing for interactive sound propagation, IEEE Trans. on Visualization 
and Computer Graphics,14, pp. 1707 -1722, 2008.  R. Kouyoumjian and P. Pathak, A uniform geometrical 
theory of diffraction for an edge in a perfectly conducting surface. In Proc. IEEE, vol. 62, pp. 1448 
-1461, 1974.  D. Schröder and A. Pohl, Real-time hybrid simulation method including edge diffraction, 
In Proc. EAA Symposium on Auralization, Otaniemi, 2009.  U. P. Svensson, R. I. Fred, and J. Vanderkooy, 
An analytic secondary-source model of edge diffraction impulse responses, J. Acoust. Soc. Am., 106(5), 
pp. 2331 -2344, 1999.  U. P. Svensson and P. Calamia, Edge-diffraction impulse responses near specular-zone 
and shadow-zone boundaries, responses near specular zone and shadow zone boundaries,Acta Acustica united 
with Acustica, 92(4), pp. 501 -512, 2006.  M. Taylor, A. Chandak, Z. Ren, C. Lauterbach, and D. Manocha, 
Fast edge-diffraction for sound propagation in complex virtual environments, In Proc. EAA Symposium on 
Auralization, Otaniemi,2009.    N Tsingos T Funkhouser A Ngan N. Tsingos, T. Funkhouser, A. Ngan, 
and I Carlbom and I. Carlbom, ModelingModelingacoustics in virtual environments using the Uniform Theory 
of Diffraction, In Proc. ACM Computer Graphics (SIGGRAPH 01),pp. 545 -552, 2001. N. Tsingos, I. Carlbom, 
G. Elko, T. Funkhouser, and R. Kubli, Validation of acoustical simulations in the Bell Labs box, IEEE 
Computer Graphics and Applications, 22(4), pp. 28 -37, 2002.  N. Tsingos and J.-D. Gascuel, Soundtracks 
for computer animation: Sound rendering in dynamic environments with occlusions, In Proc. Sound rendering 
in dynamic environments with occlusions, In Proc. Graphics Interface97, Kelowna, BC, 1997.  N. Tsingos 
and J.-D. Gascuel, Fast rendering of sound occlusion and diffraction effects for virtual acoustic environments, 
In Proc. 104th Aud. Engr. Soc. Conv., 1998. Preprint no. 4699.    of sound The study of propagation 
of sound Diverse applications Diverse applications  Earth Science (Seismic waves)  Engineering (Vibration 
and noise control)  Acoustics Arts (Musical Acoustics))  Architecture (Architectural Acoustics) 
  Games and Virtual Environments      A  The resultant pressure at P due to  two waves is simply 
their sum  Phase is crucial P out of phase: cancel in phase: add B signal A signal B A + B  A wave 
bends around obstacles of size approx.  . . it it s wavel l eng th , i i .e. w h en t h h . ~ s  s 
 P will have appreciab l e reception only if there is a g ood amount of diffraction  g  This is the 
reason sound    gets everywhere    Multiple reflections are audible: Full time domain solution 
required, unlike light simulationsolution required, unlike light simulation Interference is important. 
For example, DeadInterference is important. For example, Dead spots in auditoria  Diffraction is observable 
for sound and must be captured properly  Solve the Wave Equation:    is the laplacian operator in 
3D is the laplacian= 340m/s is the speed of sound in air is the p pressure field to solve  The RHS 
is the forcing term, corresponding to sound sources in the scene     Available on both CPUs and GPUs 
 Can be used for signal processing, numerical calculations and geometry processing  Multiple frusta 
and rays can be easily traced in parallel  Frusta tracing: specular reflections and edge diffractions 
 Ray tracing: diffuse reflections  Can scale linearly with the number of cores (tested up to 16 cores) 
     Theater 54 .s Factory 174 .s Game 14K .s Sibenik 71K .s City 72K .s SodaHa ll 1.5M .s diffraction 
diffraction NONO NONO NONO NONO YESYES YESYES #frusta 56K 40K 206K 198K 80K 108K time (msec) 33 27 273 
598 206 373    Rectangular Decomposition Numerical Acoustics can be solved very efficiently on a 
rectangular domain on a rectangular domain  Decompose complex domains into rectangles  Solution of 
Wave Equation within each rectangle can be done using a Discrete Cosine Transform can be done using 
a Discrete Cosine Transform (DCT)  DCTs can be done using FFT  Use an efficient FFT implementation 
on the GPU  GovindarajuGovindaraju, N. KK., LloydLloyd , BB., DotsenkoDotsenko , YY., Smith, BB., and 
   N Smith and Manferdelli, J. 2008. High performance discrete Fourier transforms on graphics processors. 
In Proceedings of the 2008 ACM/IEEE Conference on Supercomputing  Performance  SScene Name VlVolume 
(m3) Ti FDTDTime: FDTD (CPU) Ti OTime: Our Technique (GPU) S dSpeedup Corridor 375 365 min 4 min ~ 90x 
HouseHouse 1,2751,275 2718 min 2718 min 13 min 13 min ~ 200x200x Cathedral 13,650 ~1 week (projected) 
30 min ~ 300 x  Rectanggular decompposition leveragges GPU FFT combined with algorithmic improvements 
leading to ~100x improvement in performance for numerical acoustics  Use techniques similar to (reflective) 
shadow mapping  Compute qualitative occlusion or more physically grounded surface integral  GPUs can 
be used for audio processing and filtering       Case continued More information in: S. Siltanen, 
T. Lokki, and L. Savioja, `Frequency domain acoustic radiance transfer for real-time auralization,' Acta 
Acustica united with Acustica, vol. 95, no. 1, pp. 106-117, 2009.  Conclusions  Multi-core CPUs and 
many-core GPUs can be used to accelerate sound rendering  It is possible to develop interactive sound 
rendering systems by exploiting the commodity parallel hardware  Codecs . Synthesis / Audio « shaders 
»  Synthesis / Audio « shaders »  Codecs . Synthesis / Audio « shaders »  Synthesis / Audio « shaders 
»  Environmental sound propagation  DSP effects  Codecs . Synthesis / Audio « shaders »  Synthesis 
/ Audio « shaders »  Environmental sound propagation  DSP effects  3D audio rendering  Codecs . 
Synthesis / Audio « shaders »  Synthesis / Audio « shaders »  Environmental sound propagation  DSP 
effects  3D audio rendering  Complex bus routing and mixing Complex bus routing and mixing  Mastering, 
leveling and dynamic range control Bus grouping in Wwise         &#38;#169; Rob Bridgett  
 Powerful middleware available  WWise, FMOD, Niles, XAudio, SCREAM replace DirectSound/openAL/EAX 
 real-time processing engines and authoring tools  provide visual interfaces to sound designers  interactive 
scripting  managge largge databases of assets  debugging and profiling tools  State-of-the-art techs 
still tend to be implemented in-house  differentiating factor  Perceptual rendering  clustering and 
masking  Frequency-domain pipeline  Time vs. frequency domain  Geometrical propagation   Geometrical 
propagation  First use of perceptual audio rendering in commercial game engine N lf  Next-gen platforms 
  Test Drive Unlimited and Alone in the Dark: NDI  Test-Drive unlimited data:  200 to 400 sources 
 cars and « physics » clusters  error-based refinement  12 clusters sufficient   &#38;#169;Virtools 
&#38;#169;&#38;#169;Eden Games  x3 speed-up for 5.1 rendering  In-ggame sppatialized chat  Optimize 
bandwidth  Use a forwarding bridge  Bridge performs on the fly masking  70% audio frames discarded 
  Clients perform additional masking+clustering with other in­game sounds  Most game audio pipelines 
perform time-domain processing  IIR filtering, mixing, pitch-shifting  Fourier-domain FIR convolution 
  Advantages of time-domain processing  sample-accurate  pitch-shifting  no transforms  Advantages 
of frequency domain  arbitrary filtering, subband processing is possible (and fast)  progressive/level-of-detail 
rendering  integrates with the codec !  Choose your transform  FFT vs. DCT  can be built-in your 
codec !  Processing and mixing  EQ/convolution/granulation  pitch shifting is harder !  Zero padding 
might not  Zero-padding might not be necessary  Overlap-add reconstruction  apply windows twice 
before and after the transform (e.g., sine) one inverse transform per output channel is OK  Reflections 
and late reverberation  Early reflections as image-sources  Late reverberation as parametric model 
 pre-computed  hybrid directional/diffuse rendering model [SIRR, Pullki et al.]  FFT-based implementation 
  1024-sample blocks with 50% overlap-add reconstruction  2048 taps FFTs (zero padding)  Audio frame 
rate is ~86Hz @ 44.1KHz  Budget set to 40000 FFT coefficients max. per frame  Binaural rendering M 
dHRTF f LISTENd t b  Measured HRTFs from LISTEN database  Peak pipeline throughput is ~15000 blocks 
with ~4000 actually rendered, 3GHz Core2 Extreme        What next ?  Meta-data handling  
computational decision making and performance  improve mix quality and creativity  Integrated decoding, 
processing and mixing  perceptual processing  multi-core architectures  PParamettriic spati tiall 
audidio coding [DirAC, SAOC, MPEG surround]  di  Authoring tools ? Recording techniques ?  Massively 
multiplayer games  audio rendering for thousands of clients (e.g., Dolby Axon )  http://www.gamesound.org/articles.html 
 http://www.gamasutra.com  http://mygdc.gdconf.com/vault/   Real-Time Sound Synthesis andPropagation 
for Games  T he believability of a computer game depends on three main components: graphics, behavior 
(including physics and AI), and sound. Thanks to years of  research in computer graphics and the advent 
of mod­  ern graphics hardware, many of today s games render near-photorealistic images at interactive 
rates. To fur­ther increase immersive gameplay, several recent games (such as Valve s Half Life 2 and 
Crytek s Far Cry) have  added an integrated physics and behavior engine to enhance that realism, as 
objects interact with one another in a more physically plausible way. In contrast, sound gen­eration 
and propagation have not received as much attention due to the extremely high computational cost for 
simulating realistic sounds. The state of the art for sound production in games, even those with integrated 
physics engines, is to use recorded sound clips that are triggered by events in the game, not unlike 
how recorded animation sequences were once used to generate all char­acter motion in games. Artist-placed 
precom­puted effects are applied when playing back the sound to mimic spatial effects (such as echoes 
in a large room) caused by the reflections of sound waves in the scene. Although this tech­nique has 
the obvious advantage of being sim­ple and fast, it also has major drawbacks. First, the sound it generates 
is repetitive. Real sounds are dependent on how objects collide and where the impact occurs; prerecorded 
sound clips fail to capture these factors [7, 9, 10]. Sec­ond, recording original sound clips and rever­beration 
filters for all sound events in a game is labor-intensive and tedious. Finally, a static effect that 
is supposed to represent the acoustic properties of a room can be only a coarse approximation of the 
real sound and cannot account for many other important contribu­tions that affect immersion in a game 
environ­ment.  Physically based sound synthesis has clear advantages over recorded clips, as it automati­cally 
captures the subtle shift of tone and tim­bre due to change in impact location, material property, object 
geometry, and other factors. Physically based sound synthesis has two key requirements: Physics engine. 
An underlying physics engine must be able to inform a sound system of the exact collision geometry, as 
well as the forces involved for every collision, in the scene; many recent commercial games meet this 
requirement (www.havok.com/); and Compute power. Physically based sounds take signifi­cantly more compute 
power than recorded sounds. Thus, brute-force sound simulation cannot achieve real-time sound synthesis. 
Physically based sound propagation algorithms simulate a sound source in a virtual scene by comput­ ing 
how the sound waves are transmitted in the scene. Broadly, sound propagation simulation involves two 
different approaches: Numerical. Numerical methods numerically solve the wave equations to obtain more 
exact solutions but are impractical for interactive applications due to their demanding computational 
requirements; and Geometric. Geometric methods explicitly model the propagation of sound from the source 
based on rec­tilinear propagation of waves, accurately modeling early specular reflections and transmissions 
while Figure 1. System overview. In the preprocessing step, the input mesh for each sounding object 
is converted to a spring-mass system by replacing the mesh vertices with point masses and the edges with 
springs. The force matrix for each spring-mass system is preprocessed to yield its characteristic mode 
frequencies and damping parameters. At runtime, the game engine s physics simulator reports the impulses 
and their locations for each object to the sound system. The impulses are then used to determine the 
proportion in which the object s modes are to be mixed. The mixed modes are then sent as output to the 
audio card. accounting for wave effects; solutions for doing this include path-tracing [3] and beam-tracing 
[2] meth­ods. While the latter are the best of the geometric methods, existing implementations need pre­processed, 
static acceleration structures for the scene and do not work for either dynamic or general envi­ronments 
(such as those in games). Here, we describe techniques we ve developed to make sound simulation for synthesis, 
as well as for propaga­tion, much more efficient, thereby enabling realistic sound for games. SOUND SYNTHESIS 
Sound in nature is produced by surface vibrations of an elastic object under an external impulse. The 
vibrations 68 July 2007/Vol. 50, No. 7 COMMUNICATIONS OF THE ACM  disturb the surrounding air, resulting 
in a pressure wave that travels outward from the object. If the frequency of this pressure wave is within 
the range 20Hz 22,000Hz, our ears sense it and give us a subjective perception of sound. The most accurate 
method for modeling these surface vibrations is to directly nalizing the force matrix, K, as described 
in detail in [7]. The intuitive interpretation of the operation is that it translates the original problem 
in the spatial domain to a much simpler problem in terms of the characteris­tic vibration modes of the 
object. The sound of each of these modes is a sinusoid with apply classical mechanics to a fixed frequency 
and damping the problem while treating the rate. The key insight is that all object as a continuous 
(as the sounds of an object can be opposed to discrete) entity. represented as a mixture of This method 
results in equa­ these modes in varying propor­ tions for which analytical solu­ tions. From a computational 
 tions are not known for point of view, the diagonaliza­ arbitrary shapes. To address tion operation 
can be done this problem, one option is to Table 1. Human frequencymake suitable discrete approxi-discrimination 
as a function mations of the object geome-of the center frequency [8]. Our ability to distinguish try, 
making the problem more nearby frequenciesamenable to mathematical deteriorates considerably for higher 
frequencies, a fact analysis [6]. exploited by our approach Our approach (see Figure 1) to improve performance. 
discretizes an object in the fol­lowing way: Given an input mesh consisting of vertices and connecting 
edges, we construct an equivalent spring-mass system by replacing the mesh vertices with mass particles 
and the edges with damped springs. As described in detail in [7], given this spring-mass system, classical 
mechanics can be applied to yield the spring­mass system s equation of motion: where M is the mass matrix, 
K is the elastic force matrix, and y and n are the fluid and visco-elastic damping constants for the 
material, respectively. The matrix M is diagonal, with entries on the diagonal cor­responding to the 
masses. The elastic force matrix, K, incorporates the spring connections between the parti­cles. The 
variable r is the displacement vector of the particles with respect to their rest position, and f is 
the force vector. The damping constants, spring constants, and masses are intuitively determined by the 
material of the object alone and serve to capture the material s char­acteristic sound (such as, say, 
the thud of a wooden object, as opposed to the ring of a metallic one). The mass and force matrices encode 
the geometry of the object and hence determine the sound s timbre, as well as its dependence on the impact 
forces and position of impact contained in the force vector, f. The equation can be solved analytically 
by diago­ offline as a preprocess, as the frequency and damping of the modes depends solely on the object 
s material properties and geometry. The exact proportion in which these modes are mixed is com­puted 
at runtime and depends on the collision impulses and position of impact. A naive approach to sound sim­ulation 
would thus consider all the modes of an object and mix them in the appropriate proportions at run­time. 
A natural question at this stage concerns the effi­ciency of the naive approach. Typically, the number 
of modes of an object with a few thousand vertices is in the range of a few thousand, and the procedure 
described earlier runs in real time. But as the number of objects increases beyond two or three, performance 
degrades severely, resulting in pops and clicks at run­time. How can the performance of sound synthesis 
be improved to achieve real-time performance for interac­tive gameplay? The key idea is to somehow decrease 
the number of modes being mixed while tricking the lis­tener s perception from noticing the difference 
among them. EXPLOITING AUDITORY PERCEPTION Two main techniques mode compression and quality scaling enhance 
the efficiency of the approach we ve discussed by exploiting human auditory perception: Mode compression. 
A perceptual study described in [8] found that humans have a limited capacity to dis­criminate between 
frequencies that are close to each other. That is, if two close enough frequencies are played in succession, 
the average human listener is unable to tell whether they were two different frequen­cies or the same 
frequency played out twice. The fre­quency discrimination at different frequencies are listed in Table 
1. Note, for instance, that at 2KHz, the fre­quency discrimination is more than 1Hz. That is, a human 
subject cannot tell 1,999Hz from 2,000Hz. Observe that the frequency discrimination deteriorates dramatically 
as the frequencies are increased to higher  COMMUNICATIONS OF THE ACM July 2007/Vol. 50, No. 7 69  
Figure 2a. Falling dice on values. While synthesizing any xylophone. Dice fall on a three-octave xylophone 
in sound consisting of many fre­ close succession to quencies, we can easily cheat play the song The 
the listener s perception by Entertainer ; see and listen at gamma.cs. replacing multiple frequencies 
unc.edu/symphony. The close to each other with a single system produces the one representing all of them. 
 corresponding musical tones at more than This approach, which saves 500FPS for this complex computation 
because mixing scene, with audio generation taking 10% one frequency is much cheaper of total CPU time. 
than mixing many, is the main idea behind mode compression and leads to large gains in performance in 
practice. Quality scaling. Mode compression aims to increase the efficiency of sound synthesis for a 
single object. However, when the number of sounding objects in a scene grows beyond a few dozen, increasing 
the effi­ciency of individual objects is not sufficient [1]. More­over, it is critical for a player s 
gameplay experience that the sound system employ a graceful way of vary­ing quality in response to variable 
time constraints. We achieve this flexibility by scaling the sound quality for the objects. This quality 
is changed by controlling the number of modes being mixed for synthesizing its sound. The main idea is 
that in most scenes with many sounding objects, the listener s attention is on the objects in the foreground, 
or those contributing the most to the total sound in terms of amplitude. Therefore, mixing the foreground 
sounds at high qual­ity while mixing the background sounds at a relatively lower quality should reduce 
the resulting degradation in perceived aural quality. Quality scaling achieves variable mixing by assigning 
time quotas to all objects, prioritized on the loudness of the sound they re pro­ducing, then scaling 
their quality to force them to complete within the assigned time quota a tech­nique we ve developed that 
performs quite well in practice. We ve integrated this sound system with two game engines: Pulsk, developed 
in-house, and the widely used open source Crystal Space (www. crystalspace3d.org/). Crystal Space uses 
many open­source physics engines; we used the Open Dynamics Engine (www.ode.org/) for our implementation. 
All results were obtained on a 3.4GHz Pentium 4 laptop with 1GB RAM and a GeForce Go 6800 graphics card. 
To illustrate the realistic sounds achievable with our approach, we ll describe an application that uses 
Pulsk as its game engine. We modeled a three-octave xylophone (see Figure 2a), with each of its wooden 
keys consisting of about 1,000 vertices. The figure shows many dice falling onto the keys to produce 
the corre­ 70 July 2007/Vol. 50, No. 7 COMMUNICATIONS OF THE ACM  sponding musical notes. The audio 
simulation for this scene runs in the range of 500FPS 700FPS, depending on the frequency of the collisions, 
where we define an audio frame as enough audio samples to last one video frame. The overall system runs 
at a steady frame rate of 100FPS. We created a scene with 100 rings falling onto a table in an interval 
of one second. This scenario can be regarded as the worst-case test case for our system, as it is rare 
in a game for so many collisions to happen in such a short amount of time. Table 2 shows the result­ing 
performance of the system as a function of time. In light of the optimization we ve discussed here, the 
sound system is able to stay around 200 audio FPS (top curve), while a naive implementation would yield 
only about 30FPS (bottom curve). We integrated our sound system with Crystal Space to demonstrate the 
practicability of our approach. See Figure 2b for a screenshot from a game application with the modified 
game engine. The scene is a typical game environment, with complex shading involving substan­tial graphics 
overhead. The objects making sounds in the scene are the red ammunition shells on the floor. They make 
realistic impact and rolling sounds from falling on the floor; the user can toss in more shells and interact 
with them in real time. This demo runs steadily at more than 100FPS, with the Figure 2b. Real-time sound 
synthesis in a sound system taking approxi­ game. Screenshot from amately 10% of CPU time. game application 
(using the Crystal Space game engine) demonstrating SOUND PROPAGATION real-time sound synthesisOur approach 
for sound propa-for numerous objects. The sound being generated is gation is intended as an alterna­ 
shown below. All redtive to beam tracing and is ammunition shells, which especially well-suited for interac-were 
dropped into the scene in real time, are tive applications (such as games) sounding; the userwhile maintaining 
the advan-interacts freely with them to produce realistic tages of beam tracing. We use impact and rolling 
 bounding volume hierarchy-sounds. The application based ray tracers, since they have consistently runs 
above 100FPS, with sound been shown to work well for the taking 10% of CPU time; dynamic and general 
environ-for more images and sounds see gamma.cs. ments we are interested in [5, unc.edu/symphony. 11]. 
The main difference between our approach and beam tracing is the handling of intersections with the scene. 
Assume that a beam representing a sound wave hits a triangle of a virtual object. We want a reflection 
of this beam off the surface, or a secondary beam to represent the reflection, but we are also interested 
in the remaining part of the beam not hit by the triangle. In beam tracing, these computations are performed 
using  COMMUNICATIONS OF THE ACM July 2007/Vol. 50, No. 7 71 exact clipping between the beams and the 
triangles. The intersected part is cut out of the beam s previous shape, with the effect that the beam 
can have arbitrar­ily complex, non-convex shapes. This computation can be very slow due to the clipping 
algorithm and to the difficulty of testing for intersections against these com­plex beams. Our frustum 
tracing approach [4] simplifies this intersection computation by replacing the arbitrarily shaped beams 
with just one pyramidal frustum that can be uniformly subdivided into sub­frusta. We do not per­form 
exact clipping; instead, we test each sub­frustum against a primi­tive to see which sub-frusta intersect. 
Our approach can be inter­preted as a discretized version of the clipping algorithm. We represent each 
sub-frustum by a sampled ray for this pur­pose and therefore have a group of sample rays rep­resenting 
the whole frus-Table 2. Performance. Audio simulation FPS for a scene with tum. This simplification 
100 rings falling onto a tableenables game developers within one second during which almost all the collisions 
take place. to take advantage of The bottom-most plot is the FPSrecent advances in inter-for an implementation 
using none active ray tracing [5, 11], of our acceleration techniques. The topmost plot is the FPS with 
mode treating a group of rays as compression, mode truncation, a ray frustum to speed and quality scaling. 
FPS stays near 200, even when the other two operations. Intersection curves dip due to numerousinvolves 
testing each ray collisions during 1.5 2.0 seconds. with the triangle; the test can be inefficient when 
there are too many sub-frusta. To avoid overhead as much as possible, our method takes advantage of uniform 
sampling, conservatively determining which parts in frustum space are occluded by the triangle. This 
approach gives us a quick way to limit the number of rays that need to be tested for intersections. Since 
we represent each sub-frustum with a sample ray for this purpose, we introduce an approximation for the 
intersection test, as well as for the shape of the frustum; the extent of error depends on the number 
of sub-frusta we generate. Our experiments have shown good convergence rates when increasing the subdivi­sion 
count for the frustum. Acceptable quality can be achieved with sub-frusta of 4x4 resolution, with only 
minor quality improvements past 8x8 resolution. In general, this also has the interesting side effect 
of pro­ducing a general way to trade-off speed and quality by modifying the subdivision factor important 
for games where speed is critical. Using this approach, our algorithm is able to perform the sound simulation 
many times a second, allowing sev­eral orders of reflection in complex game-like scenes that are dynamic 
and have moving sound sources (see Figure 3). While the overall performance of our approach is still 
limited by CPU compu­ tation power, it is trivially parallelizable and easily integrated into a multi­threaded 
engine so it runs asynchronously to the other components. Given the trend toward multi­core systems, 
some part of the overall computational power can then be used for the simulation processes while adjusting 
the sub-frustum resolu­tion to achieve real-time performance. CONCLUSION This methodology, combined 
with the acceleration techniques we ve described, make it possible to simu­late sound for large-scale 
game environments contain­ing thousands of triangles and hundreds of interacting objects in real time 
with little loss in perceived audio quality. We expect that similar approaches can be applied to simulate 
sliding sounds, explosion noises, breaking sounds, and other more complex audio effects otherwise difficult 
to generate physically at interactive rates. Our sound synthesis techniques, in combination with interactive 
sound propagation, make it possible to fully simulate a sound, from its creation to how it is perceived 
by the listener, making future games aurally rich and, as a result, much more immersive. References 1. 
Fouad, H., Ballas, J., and Hahn, J. Perceptually based scheduling algo­rithms for real-time synthesis 
of complex sonic environments. In Proceed­ings of the International Conference on Auditory Display (Palo 
Alto, CA, Nov. 2 5). ICAD, 1997, 1 5. 2. Funkhouser, T., Carlbom, I., Elko, G., Pingali, G., Sondhi, 
M., and West,  J. A beam-tracing approach to acoustic modeling for interactive virtual environments. 
In Proceedings of the 25th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH) 
(Orlando, FL, July 19 24). ACM Press, New York, 1998, 21 32; doi.acm.org/10.1145/ 280814.280818. 3. Krokstad, 
A., Strom, S., and Sorsdal, S. Calculating the acoustical room response by the use of a ray tracing technique. 
Journal of Sound and Vibra­tion 8, 1 (July 1968), 118 125. 4. Lauterbach, C., Chandak, A., and Manocha, 
D. Interactive sound render­ing in complex and dynamic scenes using frustum tracing;  72 July 2007/Vol. 
50, No. 7 COMMUNICATIONS OF THE ACM  Figure 3. Real-time sound propagation. Screenshots from our system 
simulating sound propagation. Even though the scene has ~9,000 trian­gles, the algorithm still computes 
the sound up to four reflections by shooting more than 95,000 beams more than two times per second on 
a 2GHz laptop computer. The image sequence, right, shows the computed beams as seen from a top-down view 
for the first three reflections in a real-time animation of the blue curtains, from fully closed (top) 
to fully open (bottom); for more video, images, and sounds see gamma.cs.unc.edu/SOUND. gamma.cs.unc.edu/SOUND/. 
5. Lauterbach, C., Yoon, S.-E., Tuft, D., and Manocha, D. RT-DEFORM: Interactive ray tracing of dynamic 
scenes using BVHs. In Proceedings of the IEEE Symposium on Interactive Ray Tracing (Salt Lake City). 
IEEE Press, 2006, 39 46. 6. O Brien, J., Shen, C., and Gatchalian, C. Synthesizing sounds from rigid­body 
simulations. In the ACM SIGGRAPH 2002 Symposium on Computer Animation (San Antonio, TX, July 21 22). 
ACM Press, New York, 2002, 175 181. 7. Raghuvanshi, N. and Lin, M. Interactive sound synthesis for large-scale 
environments. In Proceedings of the ACM Symposium on Interactive 3D Graphics and Games (Redwood City, 
CA, Mar. 14 16). ACM Press, New York, 2006, 101 108. 8. Sek, A. and Moore, B. Frequency discrimination 
as a function of frequency, measured in several ways. Journal of the Acoustical Society of America 97, 
4 (Apr. 1995), 2479 2486. 9. van den Doel, K., Kry, P., and Pai, D. Foleyautomatic: Physically based 
sound effects for interactive simulation and animation. In Proceedings of the 28th Annual Conference 
on Computer Graphics and Interactive Techniques (SIGGRAPH) (Los Angeles, Aug. 12 17). ACM Press, New 
York, 2001, 537 544. 10. van den Doel, K. and Pai, D. The sounds of physical shapes. Presence 7, 4 (Aug. 
1998), 382 395. 11. Wald, I., Boulos, S., and Shirley, P. Ray tracing deformable scenes using dynamic 
bounding volume hierarchies. ACM Transactions on Graphics 26, 1 (Jan. 2007).  Nikunj Raghuvanshi (nikunj@cs.unc.edu) 
is a Ph.D. candidate in the Department of Computer Science at the University of North Car­olina at Chapel 
Hill. Christian Lauterbach (cl@cs.unc.edu) is a Ph.D. candidate in the Department of Computer Science 
at the University of North Carolina at Chapel Hill. Anish Chandak (achandak@cs.unc.edu) is a Ph.D. candidate 
in the Department of Computer Science at the University of North Carolina at Chapel Hill. Dinesh Manocha 
(dm@cs.unc.edu) is the Phi Delta Theta/Matthew Mason Distinguished Professor of Computer Science at the 
University of North Carolina at Chapel Hill. Ming C. Lin (lin@cs.unc.edu) is the Beverly W. Long Distinguished 
Professor of Computer Science at the University of North Carolina at Chapel Hill. The research described 
here is supported in part by the National Science Foundation, Office of Naval Research, U.S. Army Research 
Office, and Intel Corporation. Any opin­ions, findings, and conclusions or recommendations expressed 
here are those of the authors and do not necessarily reflect the views of the sponsors. &#38;#169; 2007 
ACM 0001-0782/07/0700 $5.00  COMMUNICATIONS OF THE ACM July 2007/Vol. 50, No. 7 73         
     HindawiPublishingCorporation EURASIPJournalonAdvances inSignalProcessing Volume 2007,ArticleID63560,10 
pages doi:10.1155/2007/63560 ResearchArticle FastTime-DomainEdge-DiffractionCalculationsfor InteractiveAcousticSimulations 
PaulT.Calamia1,2 andU.PeterSvensson3 1PrograminArchitecturalAcoustics, SchoolofArchitecture,RensselaerPolytechnicInstitute,Troy,NY 
12180, USA 2DepartmentofComputerScience,PrincetonUniversity,Princeton,NJ 08544,USA 3AcousticsResearchCentre,DepartmentofElectronics 
andTelecommunications,NorwegianUniversityof Science andTechnology, NO-7491Trondheim,Norway Received1May2006;Revised20September 
2006;Accepted16 October 2006 RecommendedbyWerner DeBruijn The inclusion of edge di.raction has long beenrecognized 
as an improvementtogeometrical-acoustics (GA) modelingtechniques, particularly for acoustic simulations 
of complex environments that are represented as collections of .nite-sized planar surfaces. One particularbene.t 
of combining edge di.raction with GA components is that the resulting total sound .eld is continuous 
when an acoustic source orreceiver crossesa specular-zoneor shadow-zone boundary,despite the discontinuity 
experiencedby theassociatedGAcomponent.IninteractiveacousticsimulationswhichincludeonlyGAcomponents,suchdiscontinuitiesmaybe 
heard as clicks or other undesirable audible artifacts, and thus di.raction calculations are importantfor 
highperceptualquality as well asphysicalrealism.Whileexactdi.raction calculations are di.culttocomputeatinteractiverates,approximate 
calculations arepossible and su.cientfor situationsin whichthe ultimate goalisaperceptually plausible 
simulationrather thananumerically exactone.In this paper,we describe an edge-subdivision strategythat 
allows for fasttime-domain edge-di.raction calculations withrelativelylowerrorwhencomparedwithresultsfroma 
morenumericallyaccuratesolution.Thetradeo. between computa­tion time andaccuracycanbecontrolledwithanumberofparameters, 
allowing the usertochoose the speed thatis necessaryand theerror thatistolerableforaspeci.c modeling 
scenario. Copyright&#38;#169;2007P.T. CalamiaandU.P.Svensson. Thisis an open access article distributed 
under theCreativeCommons Attribution License,whichpermits unrestricteduse, distribution, andreproductionin 
anymedium,providedthe originalworkis properly cited. 1. INTRODUCTION Edge-di.raction impulse responses 
(IRs) are useful for acousticsimulationsinvolving objectsorenvironmentscom­prising faceted surfaces and 
have been applied to many problems in acoustics such as loudspeaker radiation [1], noise-barrier analysis 
[2], and room-acoustics modeling [3]. Di.raction calculations correct for the high-frequency approximation 
inherent in modeling techniques based on geometrical-acoustics(GA) assumptions,allowforthemod­eling of 
sound propagation around occluders and into shadow zones, and provide a smooth, continuous sound­.eld 
at specular-zone and shadow-zone boundaries when combined with GA components. All ofthese factors are 
im­portanttoachieveperceptualrealismwhen auralizing sound .eldsforvirtual-acoustic simulations.Whendynamic 
or in­teractive simulations are required, continuity of the sound .eld becomes particularly important. 
Moving sources or receivers (i.e., listeners) maycross a zone boundary where the associated GA component 
(the direct sound or a specu­lar re.ection) experiences a discontinuity as it abruptly en­ters ordropsoutof 
the impulseresponse dueto thepresence of an occluder or a re.ecting surface. Such discontinuities maybe 
heard as clicks or otherundesirable audible artifacts if di.raction is not included in the simulatedsound 
.eld. Di.raction calculations are generally quite time con­suming, a problem that is exacerbated by complex 
virtual environments with many edges and by interactive simula­tionswhich requirefast updaterates.Existing 
modeling sys­tems such as those described in [4, 5] address this con­straint by using an approach based 
on the Uniform The­ory of Di.raction (UTD) [6], a high-frequency approxi­mation that can be computed 
quickly. Two assumptions built into the UTD are that the di.racting edge is in.nitely long, and that 
it is far (relative to wavelength) from the source andreceiver.An alternative approachwhichmakes no 
 EURASIPJournal onAdvances inSignalProcessing assumptions aboutfrequencyorgeometryinvolves theexact 
Biot-Tolstoy-Medwin(BTM)expression fordi.ractionfrom an in.niterigidwedge[7,8],which has been derivedina 
line-integralformulation for.niteedges[9].However, the computational complexity of this method has restricted 
its use to static scenarios and o.ine calculations for dynamic simulations. In this paper, we describe 
a technique which allows for fast calculations of edge-di.raction impulse responses based on theBTMformulationpresentedbySvenssonet 
al.in[9]. This formulation is given as an integral along the di.ract­ing edge, suggesting an approach 
in which the edge can be subdivided into segments for processing. We use a hybrid approach in which each 
edge is subdivided into two types of segments: sample-aligned segments, each of which con­tributestoexactlyone 
sampleof thedi.ractionIR; andlarge evenly sizedsegments which contribute to multiple IR sam­ples. The 
former provide a high level of accuracy, but their boundaries are relatively slow to compute and must 
be up­dated when the source or receiver is moved. Therefore, we use themonlyforasmallpartoftheedgewhich 
contributes asigni.cantportionofthetotaldi.racted energyto the early partof the IR.The latter segments 
introduce some error, but their boundaries are independent of the source and receiver positions and can 
be computed quickly in a preprocessing step for use with the IR tails. The subdivision process, and thus 
the tradeo. between computation time and accuracy, canbecontrolledwithanumberof parameters,allowing the 
usertochoose the speed thatisnecessaryand the error that istolerableforaspeci.c modeling scenario. The 
remainder of this paper is organized as follows. Section2 discusses related work on di.raction calculations 
andacoustic modeling. Section3 containsabriefreview of the di.ractionformulationin[9]thatweuse asa basis 
for ourmethod. Section4 describesanextensionto the edge­subdivision strategypresented in[10]which we 
use in our system,and Section5 addresses the variousparametersavail­able for adjusting the speed and 
accuracy of the di.raction calculations. Section6 presents example calculations along with timing andaccuracydata, 
and Section7 containscon­clusions andsuggestions forfuturework. 2. RELATEDWORK Modeling for interactive 
acoustic simulations is typically done with one of three basic techniques: the image-source method[11, 
12], raytracing[13], or beamtracing[14].1 All three are based on geometrical-acoustics assumptions, and 
thusconsider soundpropagation only alongstraight ray-like paths. Such behavior is only correct at asymptotically 
high frequencies, but GA modeling techniques can provide high levels of accuracy and realism when the 
dimensions of the 1 Other acoustic-modeling techniques such as the boundary element method, the .nite 
element method, and various .nite-di.erence schemes attempt to solve the wave equation numerically, but 
are generally too computationally intensive to allow for interactive calculations and are thus not consideredfurther 
in this work. re.ecting surfaces are largerelativeto wavelength.Foraccu­rate modeling at low frequencies, 
with relatively small sur­faces, and/orin denselyoccludedenvironments,edgedi.rac­tion must be taken into 
account. While there are many techniques to calculate edge dif­fraction, two are applied mostcommonlytoacoustic 
simula­tionsof virtual environments. The.rst,afrequency-domain method,is theUniformTheory ofDi.raction[6], 
anexten­sionof the GeometricalTheory ofDi.raction[15]. Because the UTDdescribes di.raction along ray-like 
paths, it is well suited for integration with GA modeling techniques. UTD­baseddi.raction can be calculated 
su.cientlyquicklyforuse in interactive sound-.eld simulations.However, the UTDis ahigh-frequency approximation 
which assumes the di.ract­ingedgehas in.nitelength,andisvalidonlyforsourceand receiver locationswhich 
arefarfrom the edge. TheUTD hasbeen usedintwo interactive acoustic mod­eling systems that utilize beam 
tracing to .nd the GA com­ponents. The.rst,developedbyFunkhouser et al. [4]and Tsingos et al.[16], uses 
aprecomputed beam treewith its rootata.xedsource locationtoidentifyareasina3D model which can be reached 
by direct, re.ected, and/or di.racted sound. As a receiver is moved throughout the modeled en­vironment, 
the beamtreeis usedto identify validpropaga­tion paths from the source to the receiver location rapidly, 
allowing for interactive modeling.For each di.racting edge in a valid path, UTD-based di.raction is calculatedbased 
on the shortest path throughthe edge.Forreducedcomputation time (with a corresponding reduction in accuracy), 
di.rac­tion calculations canbe limitedtoreceiversin shadowzones. In the second system, developed by Antonacci 
et al. [5,17,18],visibility diagrams forall re.ecting surfacesin a 2.5D model (i.e., arbitrarily placed 
vertical walls with hor­izontal .oors and ceilings) are precomputed using a dual­spacerepresentationofthe 
model sgeometry. Thesevisibility diagrams allow for rapid construction of beam trees, which in turn allows 
for interactive modeling with a moving source as well as a moving receiver. UTD di.raction coe.cients 
can be computed for eachdi.racted path, or canbe interpolated froma smallsetofprecomputed values forfasterprocessing. 
The second common di.raction-calculation method us­es theBiot-Tolstoy-Medwin(BTM) solution,a time-domain 
formulation for di.ractionfrom a rigid orpressure-release wedge[7,8,19].In particular,theBTM-basedexpression 
de­rivedbySvenssonet al.in[9]hasbeenusedby anumberof authors (e.g., see [3,20,21]) becauseitisformulated 
asa line integral along the di.ractingedgeandthusiswellsuited forusewith .niteedges,and because theBTMsolutiongives 
an exact solutionfor di.ractionfromarigid(orpressure re­lease)wedge.Furtherdetailsof theBTMformulationin[9] 
areprovidedin Section3 asitis the basisfor ourapproxima­tiontechnique. The computational complexity of 
the BTM method has thus far made it of limited use for interactive systems, and this has led to two approximations 
which were developed speci.cally to reduce computation time while limiting au­dible errors in the di.raction.In[22],Lokki 
et al. calcu­late di.raction impulse responses using the BTM method  P.T.Calamia andU.P.Svensson (speci.cally 
(2) below), use the FFT to .nd the di.rac­tion frequency response, and .t a warped in.nite-impulse response 
.lter to the smoothed di.raction magnitude re­sponse.While this method has been used indynamic sound 
rendering with the DIVA system (see [20, 23, 24]), the di.raction calculations must be done o.ine in 
a prepro­cessing stagegiventhe positionsofthe(moving) sourceand receiver over time. Interactive simulations 
are not possible with this approach due to the computation time needed to compute the di.raction IRs 
and construct the approxima­tion .lters.In[25,26],deRycker andTorreset al. approxi­mate edge di.ractionina 
somewhat similarfashionwith.­nite impulse-response(FIR) low-pass .lters.However, their methodwas onlytestedwith 
static source andreceiver pairs, was not integrated into a GA modeling system, and did not provide a 
way to estimate the frequency-domain di.rac­tion response needed for the .lter construction without full 
impulse-response calculations. Commercially available acoustic modeling tools such as CATT[27]andOdeon[28]simulate 
the e.ects of di.rac­tion on the re.ection and scattering from .nite surfaces by adjusting the spectra 
ofspecular re.ections and the fraction of energy that is scattered in nonspecular directions. How­ever, 
theyignoredi.raction into shadow zones, do not cal­culateexplicit di.raction impulseresponses, anddo 
notpro­videinteractivesimulations.Tsingosand Gascuel[29,30]in­teractively simulate occlusion e.ects duetodi.ractionfrom 
objectsbetweenasourceand receiver,but alsodonot calcu­late di.raction explicitly. 3. BTMEDGE DIFFRACTION 
As mentioned above, our di.raction approximations are based on a line-integral formulation of the exact 
BTM so­lutionas describedin[9]. Consider a rigid wedge of .nite length, a point source S,andareceiver 
R whose positions aregivenwith edge-aligned cylindricalcoordinates(rS,.S,zS) and(rR,.R,zR),respectively,asshowninFigure1.The 
source signal is de.ned as q(t)=.0A(t)/4p,where .0 is the density of airand A(t)isthevolumeaccelerationofthepoint 
source. Such a source signal implies that the free-.eld impulse re­sponse of the source is h(t)=d(t -d/c)/d,where 
d is the distancefrom the sourceto thereceiver and c is the speed of sound. Soundpressure canbe calculated 
throughtheconvo­lution integral . 8 p(t)=h(t)q(t -t)dt. (1) 0 The continuous-time edge-di.raction IR 
at the receiver is givenin[9]as an integralover the edge position z, 4 ... . z2 m +l ßi h(t)=- dt - 
dz, (2) 4pz1 c ml i=1 where . = p/.W is the wedge index, .W is the open wedge angle, d is the Diracdelta 
function, m and l are the distances from S and R,respectively,toaposition on the edge, and c is the speed 
of sound. The z-coordinate values of the edge  Figure 1: Wedge geometry and coordinate system. Locations 
are speci.ed in cylindrical coordinates where r is the radial distance from the edge, . is measured from 
one ofthe two wedge faces, and the z-axisisalignedwiththeedge. PS and PR are virtual half-planes that 
contain S and R, respectively, and the edge. endpoints are used for the integration limits z1 and z2.The 
functions ßi are sin ..i ßi = .. , (3) cosh(..)-cos ..i where the angles .i are the four combinations 
of p ±.S ±.R and the auxiliaryfunction . is ...... ml +z -zSz -zR . =cosh-1 . (4) rSrR The shortest 
pathfrom the sourceto thereceiver through the line that contains the edge goes through the so-called 
apex point on that line,and thisapexpoint may or maynot becontainedwithinthephysical edge.Ifitis,the 
onsettime of the di.raction IR isdeterminedby the path through the apex point. If it is not, the onset 
time is determined by the shorter of the two paths throughthe endpoints ofthe physi­cal edge. Theconversionof(2)intoa 
discrete-time formulation, h(n), canbeaccomplishedby subdividing the edge into seg­ments, and for each 
segment calculating the IR contribu­tion and distributing it among the appropriate time sam­ples. Numerical 
integration over each segment is generally straightforward, but the edge-di.raction IR expression is 
subjectto anonset singularitywhencosh(..)=cos(..i)=1 as seenin(3). This singularityisaddressedin[31], 
and an­alytical approximations aregiven for the .rst sample of the discrete-time IR, h(n0), whichis the 
only sample a.ected. 4. EDGE-SUBDIVISION STRATEGIES Two basic edge-subdivision strategies have been considered 
previously for the calculation of discrete-time edge-di.rac­tion IRs: subdivision into sample-aligned 
segments, and EURASIPJournal onAdvances inSignalProcessing  Boundaries for a 3-sample alignment zone 
Originaleven-segmentboundaries (upper edge) Modi.ed even-segment boundaries (lower edge) Figure 2: Unfolded 
2D view of a source, receiver, and segmented edge.The upperedgeis markedwith the boundaries fora 3-sample 
alignment zone (samples n0, n1,and n2) in black and the origi­naleven-segmentboundariesinred. Thelower 
edge(S and R not shown) is marked with the modi.ed segment boundaries for the hybrid subdivisionschemeinblue:even 
segmentsoverlapping the alignmentzonehavebeentruncatedat theedgesof thezone, and those completely within 
the alignment zone have been discarded. Theapexpointis markedwith an x. subdivision into evenly sized 
segments [9, 10].2 A third method,whichisahybridof thesetwo, wasproposedina simplerformin[10], and theremainderof 
this paper de­scribes the implementation and the evaluation of a more ro­bust form of that method. 4.1. 
Subdivision intosample-aligned segments Sample-aligned segments correspond to portions of an edge whichlie 
between intersections with two confocal ellipsoids (see Figure2). Thefociof theellipsoids are the source 
and receiver locations, and the lengths of the axes are deter­mined by the distances c(n ±0.5)/FS,where 
FS is the sam­pling frequencyand n is the sample index.Withsuchbound­aries, eachsegment contributes to 
exactly one sample of the discrete-time di.raction IR, which can be written 4 . .zn,2 . ßi h(n)=- dz. 
(5) 4p ml zn,1 i=1 2Medwinet al.[19]and ClayandKinney[32]alsoaddresstheconversion of a continuous-time 
di.raction IR to a discrete-time di.ractionIR, al­though theydo so usingaformof(2)given as an integralovertimeso 
theydo notformulatetheconversionasanedge-subdivisionproblem. Calculation of the integration limits zn,1 
and zn,2 involves .nding the roots of a quadratic equation and is described in[31]. Sample-aligned segments 
areadvantageousfor manyrea­sons. First, despite the low-pass .ltering impliedbythe area sampling in(5), 
the spectrum of the discrete-time IR canbe madetomatchthatoftheexact continuous-timeIRuptoa chosen frequencyby 
using a su.ciently highsampling rate.3 Second, this method can be used easily with the analytical approximations 
for sample n0in[31]toavoid the onset sin­gularitybecause the boundariescorrespondingto that sam­ple are 
given explicitly. Finally, the per-segment processing is straightforward: eachsegment s contribution 
is calculated using either numerical integration or the analytical approxi­mation,andtheresultis addedtothecorresponding 
sample of the IR. Sample-aligned segments unfortunately are not practi­calfor interactive simulationsbecauseof 
the associatedcom­putational demands.The segment-boundarycalculations are time consuming, the boundaries 
must be recalculated when the source or receiver is moved, and highsampling frequen­cies often result 
in highsegment counts. 4.2. Subdivision intoevenly sized segments Evenly sized segmentsfor an edgeof 
length L aregenerated bychoosinga maximum length.z,and subdividing the edge into k segments of length 
l where k =.L/.z.and l =L/k. The segment-boundary values are easily calculated and are independent of 
the source and receiver locations; thus they canbecalculated onceina simplepreprocessingstep.How­ever, 
excessively large values of l or .z can introduce signi.­cant errors in the resulting IR, while small 
values may result inaprohibitively largenumberofsegmentstoprocess.Per­segmentprocessingis somewhat morecomplicatedthanwith 
sample-aligned subdivisionbecause eachsegment may con­tributetomultipleIR samples.For eachsegment,thegroup 
of corresponding samples must be determined, and the to­tal segment contribution must be calculated and 
then spread appropriately across these samples. Finally, the boundaries corresponding to sample n0 are 
notgivenexplicitly, making it more di.culttoavoid the onset singularity. When using evenly sized segments 
that contribute to multiple IR samples, the amplitude value A obtainedbyin­tegrating over the length 
of a segment must be distributed among the appropriate samples. The path lengths from the source to the 
receiver through the endpoints of a segment canbeusedto calculatethespanof samples, Ssp,towhichthe segmentcontributes,and 
A canbedistributedinanumberof ways.The simplest approachistoevenly distribute A among the samples, but 
this leads to a staircase e.ectin the IR.As describedin[10],we applyacorrectionto theeven distri­bution 
to achieve a linear approximation of the local slope 3ClayandKinney[32]recommenda samplingrateofat leastfourtimes 
thehighestfrequencyofinterestfor analysis,which suggestsusing FS = 80kHzfor applicationsin audio andacoustics. 
  P.T.Calamia andU.P.Svensson .10. 5 Sample-aligned segments  Even segments without slope correction 
Even segments with slope correction Figure 3:Multi-sample distributionforevenly sized segmentswith and 
without the slope correction, which assumes the impulse re­sponse has a locally linear decay. oftheIR.Anexampleofthemulti-sample 
distributionwith andwithout the slopecorrection canbe seenin Figure3. 4.3. Hybrid subdivision strategy 
A hybrid subdivision strategy can be used to exploit some of the bene.tsofboth sample-alignedandevenly 
sized seg­ments.With this method,a small numberof sample-aligned segments is used to calculate the .rst 
N samples of the di.raction impulse response, and evenly sized segments are used to process the remainder 
of the edge. Any portion of an evenly sized segment that overlaps the alignment zone (i.e., would contribute 
to any of the .rst N samples) is dis­carded. An example of hybrid subdivision with N = 3is shown in Figure2.If 
the source orreceiver is moved, the boundaries for the .rst N segmentsmustberecalculated and the alignment-zoneoverlaptestsmustberepeated,but 
this is far lesstimeconsuming thanrecalculating sample-aligned boundaries for the entire edge.This method 
was introduced in[10], bututilized onlya one-sample alignmentzone. 5. CALCULATIONPARAMETERS Given the 
hybrid subdivision method described above, our goalisto minimizethedi.ractionprocessing timewhile lim­iting 
the error in the calculations.Three parameters provide controlovertheaccuracyandtiming:thenumberof samples 
in the alignment zone; the size of the evenly sizedsegments; and the integrationtechnique usedto calculate 
thecontribu­tionof eachsegment,whichcanbechosenindependentlyfor the alignment zone and the even segments. 
5.1. Sizeofthe alignmentzone Because di.raction impulse responses tend to have an im­pulsive onset followed 
by a rapidly decaying tail, the high­frequency response is governed by the early samples. The low-frequency 
response is determined by the total integral over the entire edge, but this value also is strongly depen­dent 
on the early part oftheIRwhichhasahigh amplitude relative to that of the tail. Therefore, accurate computation 
of the early part of a di.raction IR is criticalfor an accurate reproductionof itsbroadbandspectralcontentandthus 
its perceptualcharacteristics. Our implementationofthehybrid edge-subdivision scheme allows for an alignment 
zone of ar­bitrarysize, although asdescribedin Section6 the useof as few as4sample-aligned segments canbesu.cientforresults 
with low spectral error. 5.2. Segment size The sizeof theeven segmentsisgivenintermsof the max­imum number 
of IR samples, nS,tobe spannedby any one segment, andconvertedtoa length using nS · c .z = , (6) FS 
where c is the speed of sound and FS is the sampling fre­quency.Inpractice,theactualsamplespanof mostsegments 
iswellbelowthe speci.ed upperboundof nS.Asingle value of .z is usedforalledgesinagiven modelingenvironment. 
As .z increases, computation time decreases due to fewer callsofthe integration function, and accuracy 
decreasesbe­cause each segment s di.raction contribution must be dis­tributedoveralargerspanofsamples,and 
the assumptionof a locally linear slopeover such a spanbecomes less valid. 5.3. Numerical integrationtechnique 
Our implementationprovidesachoiceofthreenumericalin­tegrationtechniques: 5-pointcompoundSimpson srule 
in­tegrationwithonestepofRombergextrapolation, standard 3-pointSimpson srule integration, and 1-point 
midpoint in­tegration[33]. Because the integrand, ßi/ml, includes one hyperbolic and two standard trigonometric 
functions (see (3)), a reduction of the number of points at which it must beevaluated canlowerthetotalprocessingtimesigni.cantly 
formulti-edgeenvironments, albeitwithacorrespondingre­ductioninaccuracy.Any ofthe threetechniques canbecho­senfor 
the alignmentzoneand for theevenly sized segments independently.However,therelative importanceofthe early 
partof the di.ractionIRsuggests thatcombinationsinwhich the integration technique for the alignment zone 
is equal to or more accurate than thatfor theevenly sizedsegmentswill yield the best results. 6. RESULTS 
Toevaluatethee.ectofthe parameters describedin Section5 on the computation of di.raction impulse responses, 
we simulated the di.ractionfromanarrayof rigidrectangular EURASIPJournal onAdvances inSignalProcessing 
 S2  R1 S1 R2 (a) Figure 4: Planviewofthepanelarrayusedtoevaluatethedi.rac­tion approximation method. 
The array was located 5m above sources S1 and S2 and receivers R1 and R2. panels describedin[34], similartoonewhichmightbede­ployedoverthestageinaconcerthall 
or otherperformance space. The array comprises35 in.nitely thin 1.2mby1.2m panelsina5-by-7gridwithaninter-panel 
spacingof0.5m. 140 di.racting edges(4for each panel)wereevaluatedfor each calculation with the arraypositioned 
5m above two source/receiverpairs.The arrayand the source andreceiver positions are shown in Figure4.All 
calculations included .rst-order di.raction only; neither of the source/receiver pairsengenderedaspecularre.ectionfromthe 
array,and the directsound and higher di.ractionorderswereomitted.Due to the absence of GA components, 
our testing scenarios are conservative in the sense that they overemphasize the need for accurate di.raction 
modeling. All processing was done on a desktop computer with a 3.2GHzPentium4processor and2gigabytesof 
RAM, and all impulse responses were generated with a sampling rate of 96 kHz. All computation times are 
averages from 100 tri­als, and represent the time to compute all 140 di.raction IRs for the total panel-array 
response. For each of the two source/receiver pairs, the impulse response generated with sample-aligned 
segments and 5-point integration for allsam­ples was used as the baseline for allspeed and accuracy eval­uations. 
Such calculations previously have been shown to agreequitewell with measured data[35,36],sonocompar­isonstomeasureddataare 
includedhere. For a conservative approximation of the audibility of the errors in the di.raction IRs, 
we calculated the di.rac­tion magnitude spectra, smoothed them with 1/10-octave .lters, and compared 
them with the smoothed spectrum of the corresponding baseline case. Di.erences of less than 1dB between 
20 Hz and 20 kHz were assumed to be in­audible and thus acceptable for perceptual accuracy. Even though 
all di.racting edgeswere1.2m long, the individual impulse responses within the total response from the 
panel array ranged in size from 16 to 393 samples for the .rst source/receiver pair, and from 14to 441 
samplesfor the sec­ond source/receiverpair. Using the hybrid method, we tested 180 combinations of the 
calculation parameters with each of the two source/ 1/10th-octave smoothed-spectrum error (dB re. 1) 
14 12 10 8 6 4 2 0  Source/receiver pair1  Source/receiver pair2  Figure 5:Maximumerrorin the1/10th-octave 
smoothedspectra below20kHz forvarious alignment-zone sizes(1 to10 samples), segment sizes (40, 100, or 
300 samples), and integrationtechniques (1-point, 3-point,or 5-point) using thehybrid method. receiver 
pairs. These combinations included: variations in the size of the alignment zone from 1 to 10 samples; 
three sizes of evenly sized segments, speci.ed as maximum sample spans of40, 100, and 300 samples; and 
the three integration techniques used independently on the alignment zone and theevenly sized segments. 
Onlycombinationsforwhichthe alignment-zone integrationtechnique wasequalto,or more accurate than, thatfor 
theevenly sized segmentswere used. Forexample,given an alignmentzoneof5 samples,evenly sized segments 
limited to a span of no more than 100 sam­ples, and 3-point alignment-zone integration, only 3-point 
and 1-point integration were tested for the evenly sizedseg­ments. Overall resultsfromthe 360hybrid-subdivisiontestsare 
shownin Figure5,wherethe maximumerrorinthe 1/10th­ octave smoothed spectra (below 20 kHz) for the panel 
array is plottedagainst thetotalprocessing time.Thetrendof re­duced error with increased processing time 
is clear, and the e.ects of the various parameters are generally as expected. Forexample:all resultswitha 
maximum errorgreater than 4dBweregenerated using the largesteven-segment sizeand asingle-samplealignmentzone;all 
resultswitha maximum errorless than .09 dB were generatedusing the smallest seg­ment size and an alignment 
zone of at least 6 samples; all butoneof theresultswithaprocessing time less than4ms used 1-point integration 
with 100-sample or 300-sample even segments. Table1 contains the parameters which re­sulted in the .ve 
fastest processing times with a maximum error of less than 1dB in the smoothed spectrum for each of the 
two source/receiver pairs. While there is not a sin­gle combination of parameters that yields the best 
result for both source/receiver pairs, the use of a small alignment  P.T.Calamia andU.P.Svensson Table 
1:Parametersresultingin the5 fastestprocessing timesfor .10. 3 each S/R pairwith maximumerrorin the smoothed 
spectrumless 16 than 1dB. Data for the baseline calculations are also included for comparison as the 
last entryfor each S/R pair. 14   S/R Pair Zone Zone Segment Segment Proc. Size Integ. Size Integ. 
Time (Samples) (Samples) (ms) Max. Error (dB) 1 4 1-point 100 1-point 3.67 .97 1 4 1-point 300 1-point 
3.69 .94 1 5 1-point 100 1-point 3.86 .98 1 5 1-point 300 1-point 3.88 .96 1 2 3-point 100 1-point 4.01 
.69 1 all 5-point N/A N/A 171.25 0 2 3 1-point 100 1-point 3.31 .68 2 4 1-point 100 1-point 3.50 .41 
2 5 1-point 100 1-point 3.69 .38 2 6 1-point 100 1-point 3.88 .43 2 7 1-point 100 1-point 4.08 .32 2 
all 5-point N/A N/A 137.51 0 Amplitude (arbitrary units) 12 10 8 6 4 2 0 .2 .4 Figure 7:Impulseresponsesfor 
source position S1 andreceiverpo­sition R1.TheblueIR(showninthemain.gureandthe inset)isthe baseline calculation 
using sample-aligned segments and 5-point in­18 tegration, and theredIR(inset only)is an approximation 
usinghy­brid subdivisionwiththeparameters speci.edinLine1 of Table1. 16 14 .10 Processing time (ms) 
 12 10 8 6 1/10th-octavesmoothedmagnitude(dB re.1) .15 .20 .25 .30 .35 .40 2 Number of integrand evaluations 
 Source/receiver pair1  Source/receiver pair2  .45 Figure 6: Relationship between the processing 
time and number ofevaluationsofthe di.raction integrand.Processing timesare the Sample-aligned subdivision 
 same as thosein Figure5. zone(N 4samples) to compute the onset of the di.rac­tion IRs allows for the 
use of simpli.ed numerical integra­tion and moderately large evenly sized segments and thus rapid calculations 
with low error. Using the .rst entry for each source/receiverpairin Table1, the processing time for S1 
and R1 was reduced by a factor of 46.6 (from 171 ms to 3.67 ms) and that for S2 and R2 by a factor of 
41.7 (from 138msto 3.31).As canbe seen in Figure6, theprocessing time for computing the total di.raction 
impulse response grows linearly with the number of evaluations ofthe di.rac­tion integrand. This furthersupports 
the useofa smallalign- Hybrid subdivision Figure 8: 1/10th-octave smoothed magnitude spectra for the 
im­pulseresponsesin Figure7.See Figure9 forthedi.erencebetween the two. mentzone, moderately largeevenly 
sizedsegments, and sim­ple integration for rapid calculations. Resultsfrom anexamplecalculation canbe 
seenin Fig­ures 7and8.TheIRsandcorrespondingmagnitudespectra shown were generated for S1 and R1 using 
the baseline pa­rameter con.guration(all sample-aligned subdivision with 5-point integration), andwithhybridsubdivision 
using the EURASIPJournal onAdvances inSignalProcessing  Error in smoothedmagnitude(dB re.1) 1 0.8 0.6 
0.4 0.2 0 .0.2 .0.4 .0.6 .0.8 .1 Source/receiver pair1 Source/receiver pair2 Figure 9: Errorin the 
smoothed spectrum fordi.raction approxi­mationsutilizing thegeometryin Figure4.The solid lineis theer­rorfortheexampleshowninFigures 
7 and8 using S1 and R1 and theparametersin Line1 of Table1.The dashed lineistheerrorfor an approximation 
using S2 and R2 andtheparametersin Line7 of Table1. followingparameters(see Table1 Line1):analignmentzone 
of4samples,a maximum sample span for theeven segments of 100 samples, and 1-point integration for the 
entire edge. Figure7 shows thetotal impulseresponse calculated for the panel array, and the inset contains 
a zoomed-in view of a portion of the IR where the hybrid method s piecewise lin­earapproximation of theIR 
canbe seen. Figure8 contains the smoothed magnitude spectra of the two IRs. The error (di.erence betweenthetwo 
spectra)isplottedin Figure9,as is the errorfor anexamplecalculation using S2 and R2 with theparametersgiveninLine7of 
Table1.The maximumer­ rorbelow20kHzoccursat approximately 325Hzfor S1 and R1 and at 13.05 kHzfor S2 and 
R2. 7. CONCLUSIONS In this paper, we havepresented an edge-subdivision strat­egy that allows for fast 
time-domain edge-di.raction cal­culations with low error. For a given scenario, each edge in a 3D model 
initially is subdivided into evenly sized seg­ments. As a source and/or receiver is moved in or around 
the model, the section of each edge which contributes to the .rst few samples of the edge-di.raction 
IR,whichisde­pendenton the source andreceiverpositions,issubdivided into segments suchthat each one contributes 
to exactly one sample of the di.ractionIR.Even segmentsoverlapping this alignment zone are truncated 
at the edges of the zone, and those completely within the alignment zone are discarded. The di.raction 
integral is then evaluated for all remaining segments along the edge.Because the sample-aligned subdi­vision 
providesnumericallyaccurateresultswithahighcom­putationalcost,we generally restrict its useto the earlypor­tionof 
theIRwhich containsasigni.cantpercentageof the totaldi.racted energyandthusmustbe calculatedaccurately 
foraperceptuallyconvincing simulation.However,auseris free to choose the size of the alignment zone, 
as well as the size of the evenly sized segments and the integration tech­nique(s) used to evaluate the 
two types of segments to opti­mize the speed-accuracytradeo. for eachmodeling scenario. Testsonanarrayof 
rigid panels indicate thatfast,low-error results can be obtained with an alignment zone of approx­imately 
4 samples, even segments limited to a 100-sample span(witha samplingfrequencyof96kHz),andnumerically 
simple midpoint integration for all segments. Our method canbecombined easilywitha geometrical-acoustics 
model­ing approachsuchas image-sourcemodeling or beamtracing to provide rapid calculations of a smooth 
continuous sound .eld for interactive acoustic simulations. This work suggests several avenues for future 
research. First, we would like to study the audibility of the errors in­troducedby the subdivision strategy. 
Ourerror criterionof 1dB in the 1/10th-octave smoothed spectra is quite conser­vative andcan likelyberelaxedsigni.cantlywhen 
thedi.rac­tion calculations are combined with GA components which typically dominate the sound .eld. 
However, listening tests could provide more de.nitive results, and possibly could provide guidelines 
for choosing subdivision parameter val­uesappropriatefor di.erentapplications. The listeningtests alsocould 
includecomparisonsto otherfastdi.raction ap­proximations,forexample, those basedon the UTD.While our 
method shouldprovide higher accuracy, the perceptual bene.tsofthataccuracyhaveyettobe established. Second,theextensionofthis 
methodtohigherordersof di.raction and nonrigid surfaces should also be considered. Inregardtotheformer,thereis 
someevidencetosuggestthat higher orders of di.raction can be neglected when auraliz­ing simulated impulseresponses[3].However, 
this has not been tested in interactive or dynamic simulations in which discontinuities in the geometrical-acoustics 
.eld may result in audible artifacts.Inregardto nonrigid surfaces, solutions fordi.ractionfroma density-contrastwedgehavebeende­veloped[37,38], 
but there currentlyisno known general so­lution fordi.ractionfromawedgeof arbitrary impedance. Manyinteriorandexterior 
building materials(e.g.,concrete, brick, and thick plaster) which are likely to be simulated in virtual 
environments are acoustically hard, thus the as­sumptionofrigiditymaynotbeasigni.cant limitation.Nev­ertheless,amoregeneral 
edge-di.ractionexpression applica­bletoallmaterialswouldallowfor moreaccurateresults,and edge-subdivisioncouldbeappliedtoitforrapid 
calculations. Third, to reduce computation time further, we could consider a number of ways to omit edges 
during process­ing. For example, we could use an approach similar to the onein[16]inwhichdi.raction calculations 
can be limited to edgesforwhichthereceiverisin the shadowzone.How­ever, it might be helpful to extend 
the calculation range to include edgesforwhichthereceiveriswithina small angular distanceof azone boundary.Thiswould 
ensure sound-.eld continuityacrossallzoneboundaries, andwould capture the  P.T.Calamia andU.P.Svensson 
di.ractionIRswhere their amplitudesare maximal.Another approach could involve omitting edges for which 
the apex point is not included within the edge since the amplitude of the di.ractionIRfor such casesissmall.Preliminary 
results for these forms of di.raction culling are presentedin[39]. Finally, this subdivision scheme may 
aid in two addi­tional aspects of virtual acoustic simulations that include di.raction: visibility calculations 
and directional auraliza­tion.Inregard tovisibility, simulationsofdensely occluded environments require 
additional calculations to determine the portion(s) of eachedge to whichthe source and receiver bothhave 
clear lines ofsight.Approximatevisibility could becomputedbysimplyconsidering thevisibilityof the mid­pointof 
eachsegmentfromthe sourceandreceiver.Inregard to auralization of di.raction,Torreset al.[3]havesuggested 
convolving eachdi.ractionIRwith the head-related impulse response speci.cto the direction of the least-time 
di.racted path. In some cases, however, an edge (from apex point to endpoint) may subtend an angle large 
enough such that a singledirectionofarrivalforthe entireIRisnotsu.cientfor aperceptually accurate simulation.In 
suchcases, thedi.rac­tionIRcouldbeauralizedusingpathsthroughthe midpoints ofthesegments(ora subsetofsegments)toallowthedirec­tionof 
arrivaltochangeovertime. ACKNOWLEDGMENTS Theauthorswould liketo thank theNorway-AmericaAsso­ciation, 
theAcousticsResearchCentreproject from theRe­searchCouncilofNorway, and theNational ScienceFounda­tion 
(Grant CCR-0093343) for partially funding this work, aswell as the anonymousreviewers and theAssociate 
Editor, Werner de Bruijn, for their helpful comments and sugges­tions. REFERENCES [1] J. Vanderkooy, 
A simple theory of cabinet edge di.raction, Journal of the Audio Engineering Society, vol. 39, no. 12, 
pp. 923 933, 1991. [2] P. Menounou and J. H. You, Experimental study of the di.racted sound .eld around 
jagged edge noise barriers, The JournaloftheAcoustical SocietyofAmerica,vol. 116, no.5,pp. 2843 2854, 
2004. [3] R. R. Torres, U. P. Svensson, and M. Kleiner, Computation of edge di.ractionfor more accurateroom 
acoustics auraliza­tion, TheJournaloftheAcoustical SocietyofAmerica, vol. 109, no.2,pp. 600 610, 2001. 
[4]T.Funkhouser,N.Tsingos,I.Carlbom,etal., Abeamtracing method for interactive architectural acoustics, 
TheJournal of theAcoustical Society ofAmerica,vol. 115, no.2,pp. 739 756, 2004. [5] F.Antonacci,M.Foco,A.Sarti,andS.Tubaro, 
Fastmodel­ing of acoustic re.ections and di.raction in complex environ­ments using visibilitydiagrams, 
in Proceedingsof 12thEuro­pean Signal Processing Conference (EUSIPCO 04),pp. 1773 1776,Vienna,Austria,September 
2004. [6] R. G. Kouyoumjian and P. H. Pathak, A uniform geometri­caltheory ofdi.ractionfor an edgeinaperfectlyconducting 
surface, Proceedings ofthe IEEE, vol. 62, pp. 1448 1461, 1974. [7]M.A.Biot andI.Tolstoy, Formulationof 
wavepropagation in in.nite media by normal coordinates with an application todi.raction, TheJournaloftheAcoustical 
SocietyofAmerica, vol.29,no.3,pp. 381 391, 1957. [8] H.Medwin, Shadowingby.nite noise barriers, TheJournal 
of the Acoustical Society of America, vol. 69, no. 4,pp. 1060 1064, 1981. [9]U.P.Svensson,R.I.Fred, andJ.Vanderkooy, 
An analytic sec­ondary source model of edge di.raction impulse responses, TheJournaloftheAcoustical Society 
ofAmerica, vol. 106, no. 5, pp. 2331 2344, 1999. [10] P. T. Calamia and U. P. Svensson, Edge subdivision 
for fast di.raction calculations, in Proceedings of IEEE Workshop on Applications of Signal Processing 
to Audio and Acoustics,pp. 187 190,NewPaltz, NY, USA, October 2005. [11] J. B. Allen and D. A. Berkley, 
Image method for e.ciently simulating small-room acoustics, TheJournaloftheAcoustical Society ofAmerica,vol.65,no.4,pp. 
943 950, 1979. [12]J. Borish, Extensionof the image modelto arbitrarypolyhe­dra, TheJournalof theAcoustical 
Society ofAmerica, vol. 75, no.6,pp. 1827 1836, 1984. [13]A.Krokstad,S.Strøm,andS. Sørsdal, Calculatingtheacous­ticalroomresponsebythe 
useofaraytracingtechnique, Jour­nalofSound andVibration,vol.8, no.1,pp. 118 125, 1968. [14] T. Funkhouser, 
I. Carlbom, G. Elko, G. Pingali, M. Sondhi, andJ.West, A beamtracing approach to acoustic modeling for 
interactivevirtualenvironments, in Proceedingsofthe 25th AnnualConference onComputerGraphics andInteractiveTech­niques 
(SIGGRAPH 98), pp. 21 32, Orlando, Fla, USA, July 1998. [15]J.B.Keller, Geometricaltheory ofdi.raction, 
Journal ofthe OpticalSociety ofAmerica,vol.52,no.2,pp. 116 130, 1962. [16] N.Tsingos,T.Funkhouser,A. 
Ngan, andI.Carlbom, Model­ing acoustics in virtual environments using the uniform the­ory of di.raction, 
in Proceedings of the 28th Annual Confer­ence on Computer Graphics and Interactive Techniques (SIG-GRAPH 
01), pp. 545 552, Los Angeles, Calif, USA, August 2001. [17] F. Antonacci, M. Foco, A. Sarti, and S. 
Tubaro, Accurate and fast audio-realistic rendering of sounds in virtual envi­ronments, in Proceedingsof 
6th IEEEWorkshop onMultime­dia Signal Processing, pp. 271 274, Siena, Italy, September-October 2004. 
[18] F. Antonacci, M. Foco, A. Sarti, and S. Tubaro, Real time modeling of acoustic propagation in complex 
environments, in Proceedingsof7thInternationalConference onDigitalAudio E.ects (DAFx 04),pp. 274 279,Naples,Italy, 
October 2004. [19] H. Medwin, E. Childs, and G. M. Jebsen, Impulse studies ofdouble di.raction:a discreteHuygens 
interpretation, The Journalof theAcoustical Society ofAmerica,vol. 72, no.3,pp. 1005 1013, 1982. [20]V.Pulkki,T.Lokki, 
andL.Savioja, Implementation andvi­sualization of edge di.raction with image-source method, in Proceedingsof 
112thAudioEngineering SocietyConvention, Munich, Germany,May2002. [21]P.T. Calamia,U.P.Svensson, andT.Funkhouser, 
Integration of edge-di.raction calculations and geometrical-acoustics modeling, in ProceedingsofForumAcusticum,pp. 
2499 2504, Budapest,Hungary,August 2005. [22] T. Lokki,U.P.Svensson,andL.Savioja, Ane.cient aural­ization 
of edge di.raction, in Proceedingsof theAudioEngi­neering Society 21st International Conference on Architectural 
 EURASIPJournal onAdvances inSignalProcessing Acoustics and Sound Reinforcement, pp. 166 172, St. Peters-burg,Russia,June 
2002. [23] L.Savioja,J.Huopaniemi,T.Lokki,andR.Va¨¨anen, Cre­an¨ating interactive virtual acoustic environments, 
Journal ofthe AudioEngineering Society,vol.47,no.9,pp. 675 705, 1999. [24] L. Savioja, J. Huopaniemi, 
and T. Lokki, Auralization ap­plying the parametric room acoustic modeling technique-the DIVA auralization 
system, in Proceedings of the 8th Inter­national Conference on Auditory Display,Kyoto,Japan,July 2002. 
[25] N. de Rycker, Theoretical and numerical study of sound di.raction-application to room acoustics 
auralization, Rap­port de Stage D Option Scienti.que, ` Ecole Polytechnique, Paris,France, 2002. [26] 
R.Torres,N.deRycker, andM.Kleiner, Edgedi.raction and surface scatteringinconcert halls:physicalandperceptual 
as­pects, JournalofTemporalDesigninArchitectureand theEn­vironment, vol. 4, pp. 52 58, 2004. [27] B.-I. 
Dalenb¨ack, CATT-Acoustic v8Manual,http://www.catt.se/. [28] C. L. Christensen, ODEONRoomAcoustics Programver.8 
Manual, http://www.odeon.dk. [29] N.Tsingos andJ.-D. Gascuel, Soundtracksforcomputer an­imation: soundrenderingindynamicenvironmentswithoc­clusions, 
in Proceedings of the Conference on Graphics Inter­face,pp. 9 16,Kelowna,BritishColumbia, Canada,May1997. 
[30]N.TsingosandJ.-D. Gascuel, Fastrenderingof soundocclu­sion anddi.ractione.ects for virtual acoustic 
environments, in Proceedingsof theAudioEngineering Society104thConven­tion, Amsterdam, The Netherlands, 
May 1998, preprint no. 4699. [31] U. P. Svensson and P. T. Calamia, Edge-di.raction impulse responses 
near specular-zone and shadow-zone boundaries, ActaAcustica unitedwithAcustica,vol.92,no.4,pp. 501 512, 
2006. [32] C. S. Clay and W. A. Kinney, Numerical computations of time-domain di.ractions from wedges 
and re.ections from facets, TheJournaloftheAcousticalSocietyofAmerica,vol. 83, no.6,pp. 2126 2133, 1988. 
[33] P. J. Davis and P. Rabinowitz, Methods of Numerical Integra-tion,AcademicPress,NewYork, NY, USA, 
2nd edition, 1984. [34] R.Torres, Studies ofedge di.raction and scattering: applications toroom acoustics 
and auralization,Ph.D. thesis,ChalmersUni­versity ofTechnology,G¨oteborg,Sweden, 2000. [35]T. LokkiandV.Pulkki, 
Measurementand theoreticalvalida­tion of di.raction from a single edge, in Proceedings of the 18thInternationalCongress 
onAcoustics(ICA 04), vol. 2, pp. 929 932,Kyoto,Japan,April 2004. [36] A. Løvstad andU.P.Svensson, Di.racted 
sound .eld from an orchestra pit, AcousticalScience andTechnology,vol.26,no.2, pp. 237 239, 2005. [37] 
A.M.J.DavisandR.W.Scharstein, Thecompleteextension oftheBiot-Tolstoysolutiontothe densitycontrastwedgewith 
sample calculations, The Journal of the Acoustical Society of America,vol. 101, no.4,pp. 1821 1835, 1997. 
[38] J.C.NovariniandR.S.Kei.er, Impulse response of adensity contrast wedge: practical implementation 
and some aspects of its di.racted component, AppliedAcoustics,vol.58, no.2,pp. 195 210, 1999. [39]U.P.Svensson 
andP.T. Calamia, The useof edgedi.raction in computational room acoustics, TheJournaloftheAcousti­cal 
Society ofAmerica,vol. 120,p. 2998, 2006, (A). Paul T. Calamia is an Assistant Professor in the Graduate 
Program in Architectural Acoustics in the School of Architecture at RensselaerPolytechnicInstituteinTroy,NY. 
He earned a B.S. degree in mathematics from Duke University (1992), and an M.S. degree in electrical 
engineering in the Engi­neeringAcousticsProgramat theUniversity ofTexas atAustin(1998).He is currently 
in theprocessofcompletinga Ph.D.degreein computer scienceatPrincetonUniversity,andhe spentoneyearas a 
guest Ph.D. studentwith theAcousticsGroupat theNorwegian Universityof Science andTechnologyinTrondheim.Between 
de­grees, he spent twoyearsworking as anAcoustical Engineerwith Wyle LaboratoriesinArlington,VA, and 
fouryearsworking as an Acoustics Consultant with Kirkegaard Associates in Chicago, IL. He is anActiveMember 
of theAcousticalSocietyofAmerica and its Technical Committee on Architectural Acoustics. His research 
interests include computational room acoustics, interactive sound­.eld simulations, edge-di.raction modeling,and 
spatialaudioren­dering.  U. Peter Svensson received his M.S. de­gree (Eng. Phys.) in 1987 and his Ph.D. 
degree in 1994, both from Chalmers Uni­versity of Technology, Gothenburg, Swe­den.He has heldpostdoctoralpositions 
at Chalmers University, University of Water­loo, Ontario, Canada, andKobeUniversity, Japan. Since 1999, 
he has been a Professor in electroacousticsat NTNUinTrondheim. His research interests are auralization 
and soundreproductiontechniques, time-domain modelsofsoundra­diation and scattering, measurement techniques, 
and audio over packet networks. He has published 18 journal papers and more than 60 conference papers. 
He has been a Board Member of the Acoustical Societies ofSweden andNorway, includingaperiod as chairmanforthelatter.HeisAssociateEditorinthe 
.eldofcompu­tational acousticsforActaAcustica unitedwithAcustica.In 2001, he received an award for the 
best contribution by an author 35 yearsoryoungerto theJournal ofAudio Engineering Societyfor thepaper 
ErrorsinMLS measurements causedbytime variancein acoustic systems togetherwithDr.JohanL.Nielsen.HeisaMem­beroftheAcoustical 
SocietiesofAmerica andJapan and theAudio Engineering Society.   Fast Modal Sounds with Scalable Frequency-Domain 
Synthesis Nicolas Bonneel* George Drettakis* Nicolas Tsingos Isabelle Viaud-Delmon Doug James REVES/INRIA 
Sophia-Antipolis CNRS-UPMC UMR 7593 Cornell University Abstract Audio rendering of impact sounds, such 
as those caused by falling objects or explosion debris, adds realism to interactive 3D audio­visual applications, 
and can be convincingly achieved using modal sound synthesis. Unfortunately, mode-based computations 
can be­come prohibitively expensive when many objects, each with many modes, are impacted simultaneously. 
We introduce a fast sound synthesis approach, based on short-time Fourier Tranforms, that exploits the 
inherent sparsity of modal sounds in the frequency do­main. For our test scenes, this fast mode summation 
can give speedups of 5-8 times compared to a time-domain solution, with slight degradation in quality. 
We discuss different reconstruction windows, affecting the quality of impact sound attacks . Our Fourier-domain 
processing method allows us to introduce a scal­able, real-time, audio processing pipeline for both recorded 
and modal sounds, with auditory masking and sound source clustering. To avoid abrupt computation peaks, 
such as during the simultane­ous impacts of an explosion, we use crossmodal perception results on audiovisual 
synchrony to effect temporal scheduling. We also conducted a pilot perceptual user evaluation of our 
method. Our implementation results show that we can treat complex audiovisual scenes in real time with 
high quality. CR Categories: I.6.8 [Simulation and Modeling]: Types of Simulation Animation, I.3.5 [Computer 
Graphics]: Computa­tional Geometry and Object Modeling Physically based model­ing I.3.7 [Computer Graphics]: 
Three-Dimensional Graphics and Realism Virtual Reality Keywords: Sound synthesis, modal synthesis, real-time 
audio ren­dering, physically based animation 1 Introduction The rich content of today s interactive simulators 
and video games includes physical simulation, typically provided by ef.cient physics engines, and 3D 
sound rendering, which greatly increases our sense of presence in the virtual world [Larsson et al. 2002]. 
Physical simulations are a major source of audio events: e.g., debris from explosions or impacts from 
collisions (Fig. 1). In recent work sev­eral methods have been proposed to physically simulate these 
audio events notably using modal synthesis [O Brien et al. 2002; van den Doel and Pai 2003; James et 
al. 2006]. Such simulations result in a much richer virtual experience compared to simple recorded sounds 
due to the added variety and improved audio-visual coher­ *e-mail: {Nicolas.Bonneel|George.Drettakis}@sophia.inria.fr 
 Figure 1: Frequency-domain fast mode summation: Top: Frames of some of the test scenes our method can 
render in real time. Bottom: (Left) Time-domain modal synthesis requires sum­ming all modes at every 
sample. (Right) Our frequency-domain modal synthesis exploits the inherent sparsity of the modes discrete 
Fourier transforms to obtain lower costs per frame. ence. The user s audiovisual experience in interactive 
3D applica­tions is greatly enhanced when large numbers of such audio events are simulated. Previous 
modal sound approaches perform time-domain synthe­sis [van den Doel and Pai 2003]. Recent interactive 
methods pro­gressively reduce computational load by reducing the number of modes in the simulation [Raghuvanshi 
and Lin 2006; van den Doel et al. 2004]. Their computational overload however is still too high to handle 
environments with large numbers of impacts, especially given the limited budget allocated to sound processing, 
as is typi­cally the case in game engines. Interactive audiovisual applications also contain many recorded 
sounds. Recent advances in interactive 3D sound rendering use frequency-domain approaches, effecting 
perceptually validated pro­gressive processing at the level of Fourier Transform coef.cients [Tsingos 
2005]. For faster interactive rendering, perceptually based auditory masking and sound-source clustering 
can be used [Tsin­gos et al. 2004; Moeck et al. 2007]. These algorithms enable the use of high-quality 
effects such as Head-Related Transfer Func­tion (HRTF) spatialization, but are limited to pre-recorded 
sounds. While the provision of a common perceptually based pipeline for both recorded and synthesized 
sounds would be bene.cial, it is not directly obvious how modal synthesis can be ef.ciently adapted to 
bene.t from such solutions. In the particular case of contact sounds, a frequency-domain representation 
of the signal must be computed on-the-.y, since events causing the sounds are triggered and con­trolled 
in real-time using the output of the physics engine.  Our solution to the above problems is based on 
a fundamental intu­ition: modal sounds have an inherently sparse representation in the frequency domain. 
We can thus perform frequency-domain modal synthesis by fast summation of a small number of Fourier coef.­cients 
(see Fig. 1). To do this, we introduce an ef.cient approxima­tion to the short-time Fourier Transform 
(STFT) for modes. Com­pared to time-domain modal synthesis [van den Doel and Pai 2003], we observe 5-8 
times speedup in our test scenes, with slight degra­dation in quality. Quality is further degraded for 
sounds with faster decays and high frequencies. In addition to the inherent speed-up, we can integrate 
modal and recorded sounds into a common pipeline, with .ne-grain scalable processing as well as auditory 
masking and sound clustering. To compute our STFT we use a constant exponential approximation; we also 
reconstruct with an Hann window to simplify integration in the full pipeline. However, these approximations 
reduce the quality of the onset of the impact sound or attack . We propose a method to preserve the attacks 
(see Sect. 4.3), which can be directly used for modal sounds; using it in the combined pipeline is slightly 
more involved. We use the Hann window since it allows better recon­struction with a small number of FFT 
coef.cients compared to a rectangular window. A rectangular window is better at preserving the attacks, 
but results in ringing. Our attack-preserving approach starts with a rectangular subwindow, followed 
by appropriate Hann subwindows for correct overlap with subsequent frames. In contrast to typical usage 
of pre-recorded ambient sounds, physics-driven impact sounds often create peaks of computation load, 
for example the numerous impacts of debris just after an ex­plosion. We exploit results from human perception 
to perform tem­poral scheduling, thus smoothing out these computational peaks. We also performed a perceptually 
based user evaluation both for quality and temporal scheduling. In summary, our paper has the following 
contributions: A fast frequency-domain modal synthesis algorithm, leverag­ing the sparsity of Fourier 
transforms of modal sounds.  A full, perceptually based interactive audio rendering pipeline with scalable 
processing, auditory masking and sound source clustering, for both recorded and modal sounds.  A temporal 
scheduling approach based on research in percep­tion, which smooths out computational peaks due to the 
sud­den occurrence of a large number of impact sounds.  A pilot perceptual user study to evaluate our 
algorithms and pipeline.  We have implemented our complete pipeline; we present interac­tive rendering 
results in Sect. 7 and in the accompanying video for scenes such as those shown in Fig. 1 and Fig. 6. 
2 Previous Work In what follows we will use the term impact sound to designate a sound generated as a 
consequence of an event reported by the physics engine (impact, contact etc.); we assume that this sound 
is synthesized on-the-.y. There is extensive literature on sound synthesis and on spatializa­tion for 
virtual environments. We discuss here only a small selec­tion of methods directly related to our work: 
contact sound synthe­sis in the context of virtual environments, audio rendering of com­plex soundscapes, 
and .nally crossmodal perception, which we use to smooth computational peaks. Contact sound synthesis 
Modal synthesis excites the pre­computed vibration modes of the objects by the contact force to synthesize 
the corresponding audio signal. In our context, this force is most often provided by a real-time physics 
engine. The acoustic response of an object to an impulse is given by: s(t)= .ake-akt sin(.kt), (1) k 
where s(t) is the time-domain representation of the signal, .k is the angular frequency and ak is the 
decay rate of mode k; ak is the amplitude of the mode, which is calculated on the .y (see below), and 
may contain a radiation factor (e.g., see [James et al. 2006]). Eq. 1 can be ef.ciently implemented using 
a recursive formula­tion [van den Doel and Pai 2003] which makes modal synthesis attractive to represent 
contact sounds, both in terms of speed and memory. We de.ne mk(t) as follows for notational convenience: 
mk(t)= e-akt sin(.kt) (2) The modal frequencies .k s and decay rates ak s can be pre­computed by simulating 
the mechanical behavior of each speci.c object independently. Such simulations can be done through .­nite 
elements methods [O Brien et al. 2002], spring-mass system [Raghuvanshi and Lin 2006], or even with analytic 
solutions for simple cases [van den Doel et al. 2004]. The only quantities which must be computed at 
run-time are the gains ak since they depend on the contact position on the objects, the applied force, 
and the listen­ing position. Other methods based on measurements of the modes of certain objects are 
also possible [Pai et al. 2001], resulting in the precomputation of the gains. Audio rendering for complex 
soundscapes There has been some work on modal sound synthesis for complex scenes. In [van den Doel et 
al. 2004] a method is presented handling hundreds of im­pact sounds. Although their frequency masking 
approach was vali­dated by a user study [van den Doel et al. 2002], the mode culling algorithm considers 
each mode independently, removing those be­low audible threshold. [Raghuvanshi and Lin 2006] proposed 
a method based on mode pruning and sound sorting by mode ampli­tude; no perceptual validation of the 
approximation was presented however. For both, the granularity of progressive modal synthesis is the 
mode; in the examples they show, a few thousand modes are synthesized in real time. For pre-recorded 
sampled sounds, Tsingos et al. [2004] have pro­posed an approach based on precomputed perceptual data 
which are used to cull, mask and prioritize sounds in realtime. This approach was later extended to a 
fully scalable processing pipeline that ex­ploits the sparseness of the input audio signal in the Fourier 
domain to provide scalable or progressive rendering of complex mixtures of sounds [Tsingos 2005; Moeck 
et al. 2007]. They handle audio spatialization of several thousands of sound sources via clustering. 
One drawback related to precomputed metadata is that sounds syn­thesized in real time, such as modal 
sounds, are not supported. Cross-modal perceptual phenomena Audio-visual tolerance in asynchrony can 
be exploited for improved scheduling in audio ren­dering. The question of whether visual and auditory 
events are perceived as simultaneous has been extensively studied in neuro­science. Different physical 
and neural delays in the transmission of signals can result in contamination of temporal congruency. 
Therefore, the brain needs to compensate for temporal lags to recal­ibrate audiovisual simultaneity [Fujisaki 
et al. 2004]. For this rea­son, it is dif.cult to establish a time window during which percep­tion of 
synchrony is guaranteed, since it depends both on the nature of the event (moving or not) and its position 
in space (distance and direction) [Alais and Carlile 2005]. Some studies report that delay­ing a sound 
may actually improve perception of synchrony with re­spect to visuals [Begault 1999]. One study [Guski 
and Troje 2003] (among others [Sekuler et al. 1997; Sugita and Suzuki 2003]), re­ports that a temporal 
window of 200 msec represents the tolerance of our perception for a sound event to be considered the 
conse­quence of the visual event. We will therefore adopt this value as a threshold for our temporal 
scheduling algorithm.  3 Our Approach Overview The basic intuition behind our work is the fact that 
modal sounds have a sparse frequency domain representation. We will show some numerical evidence of this 
sparsity with examples, and then present our fast frequency-domain modal synthesis algorithm. To achieve 
this we introduce our ef.cient STFT approximation for modes, based on singular distributions. We then 
discuss our full perceptual pipeline including scalable processing, auditory mask­ing and sound source 
clustering. We introduce a fast energy esti­mator for modes, used both for masking and appropriate budget 
al­location. We next present our temporal scheduling approach which smooths out computation peaks due 
to abrupt increases in the num­ber of impacts. After discussing our implementation and results, we describe 
our pilot perceptual user study, allowing us to evaluate the overall quality of our approximations and 
the perception of asyn­chrony. Analysis of our experimental results gives an indication of the perceptual 
validity of our approach. Fourier-domain mode mixing Traditional time-domain modal synthesis computes 
Eq. 1 for each sample in time. For frequency­domain synthesis we use the discrete Fourier transform (FFT) 
of the signal (we show how to obtain this in Sect. 4.1). If we use a 1024-sample FFT we will obtain 512 
complex coef.cients or bins representing the signal of a given audio frame (since our signals are real-valued 
we will only consider positive frequencies). For each such frame, we add the coef.cients of each mode 
in the fre­quency domain, and then perform an inverse FFT (see Fig. 1) once per frame, after all sounds 
have been added together. The inverse FFT represents a negligible overhead, with a cost of 0.023 msec 
using an unoptimized implementation [Press et al. 1992]. If the number of coef.cients contributed by 
each mode is much less than 512, frequency-domain mixing will be more ef.cient than an equiv­alent time-domain 
approach. However, this will also result in lossy reconstruction, requiring overlapping frames to be 
blended to avoid possible artifacts in the time-domain. Such artifacts will be caused by discontinuities 
at frame boundaries resulting in very noticeable clicks. To avoid these artifacts, a window function 
is used, typically a Hann window. Numerous other options are available in standard signal processing 
literature [Oppenheim et al. 1999]. Our method shares some similarities with the work of [Rodet and Depalle 
1992] which uses inverse FFTs for additive synthesis. In what follows we assume that our audio frames 
overlap by a 50% factor, bringing-in and reconstructing only 512 new time-domain samples at each processing 
frame using a 1024-sample FFT. We implemented the Hann window as a product of two square roots of a Hann 
window, one in frequency to synthesize modes using few Fourier coef.cients and the other in time to blend 
overlapping frames [Z¨olzer 2002]. At 44.1kHz, we thus process and reconstruct our signal using 512/44100 
= 11msec-long frames. 4 Ef.cient Fourier-Domain Modal Synthesis We provide some numerical evidence of 
our intuition, that most of the energy of a modal sound is restricted to a few FFT bins around the mode 
s frequency. We constructed a small test scene, containing 12 objects with different masses and material 
properties. The scene is shown in the accompanying video and in Fig. 6. We computed the energy with the 
signal reconstructed using all 512 bins, then progressively reconstruct with a small number of bins distributed 
symmetrically around the mode s frequency, and measured the er­ror. We compute percent error averaged 
over all modes in the scene, for 1 bin (containing the mode s frequency), then 3 bins (i.e., to­gether 
with the 2 neighboring bins on each side), then both these together with the 2 next bins, etc. Using 
a single bin, we have 52.7% error in the reconstructed energy; with 3 bins the error drops to 4.7% and 
with 5 bins the error is at 1.1%. We thus assume that bins are sorted by decreasing energy in this manner, 
which is useful for our scalable processing stage (see Sect. 5). This property means that we should be 
able to reconstruct modal sounds by mixing a very small number of frequency bins, without signi.cant 
numerical error; however, we need a way to compute the STFT of modes ef.ciently. One possible alternative 
would be to precompute and store the FFTs of each mode and then weight them by their amplitude at runtime. 
However, this approach would suffer from an unacceptably high memory overhead and would thus be impractical. 
The STFT of a mode sampled at 44.1kHz requires the storage of 86 frames of 512 complex values, representing 
352 Kbytes per mode per second. A typical scene of two thousand modes would thus require 688 Mb. In what 
follows we use a formulation based on singular distribu­tions or generalized functions [Hormander 1983], 
allowing us to develop an ef.cient approximation of the STFTs of modes. 4.1 A Fast Short-time FFT Approximation 
for Modes We want to estimate the short-time Fourier transform over a given time-frame of a mode m(t) 
(Eq. 2), weighted by a windowing func­tion that we will denote H(t) (e.g., a Hann window). We thus pro­ceed 
to calculate the short time transform s(.) where . is the fre­quency, and t0 is the offset of the window: 
s(.)= F.{ m(t +t0)H(t) }. (3) The Fourier transform F.{ f (t) } that we used corresponds to the de.nition: 
. 8 F.{ f (t) } = f (t) e-i . tdt (4) -8 Note that the product in the time domain corresponds to the 
convo­lution in the frequency domain (see Eq. 19 in the Appendix). We can use a polynomial expansion 
(Taylor series) of the exponential function: 8 a(t+t0) at0 e= e. cn(at)n , (5) n=0 where cn = 1/n!. Next, 
the expression for the Fourier transform of a power term is a distribution given by: F.{ tn } = 2pind(n)(.), 
(6) where d is the Dirac distribution, and d(n) its n th derivative. From Eqs. 5 and 6, we have the expression 
for the Fourier transform of the exponential:  8 a(t+t0 )at0 cnan2pind(n)(.). F.e= e. (7) n=0 The Fourier 
transform of a sine wave is a distribution given by:  F.{ sin(.(t +t0)) } = ipe-i.t0 d(.+.) - ei.t0 
d(.-.). (8) We also know that d is the neutral element of the convolution (see Eq. 17 in the Appendix). 
Moreover, we can convolve the distribu­tions of the exponential and the sine since they both have compact 
support. From Eqs. 7 and 8, we .nally have: 8 F.{ m(t +t0) } = peat0 . cnanin+1 · n=0  e-i.t0 d(n)(.+.) 
- ei.t0 d(n)(.-.). (9) Convolution of Eq. 9 with a windowing function H leads to the de­sired short 
time Fourier transform of a mode. Using the properties of distributions (Eq. 17, 18 in the Appendix), 
and Eq. 9, we have: 8 at0 s(.)= 1 e. cnanin+1 2 n=0 e-i.t0 F.(H)(n)(.+.) - ei.t0 F.(H)(n)(.-.). (10) 
F.(H)(n)(.+.) is the n-th derivative of the (complex) Fourier transform of the window H, taken at the 
value (.+.). Note that Eq. 9 is still a distribution, since we did not constrain the mode to be computed 
only for positive times, and the mode itself is not square-integrable for negative times. However, this 
distribu­tion has compact support which makes the convolution of Eq. 10 possible [Hormander 1983]. For 
computational ef.ciency, we truncate the in.nite sum of Eq. 10, and approximate it by retaining only 
the .rst term. The .nal expres­sion of our approximation to the mode STFT is thus: 1 at0 c0is(.) e 2 
 e-i.t0 F.(H)(.+.) - ei.t0 F.(H)(.-.). (11) Instead of c0 = 1 (Eq. 5), we take c0 to be the value of 
the exponen­ t0+.t tial minimizing(e-at - c0)2dt, where .t is the duration of a t0 frame, resulting in 
a better piecewise constant approximation: e-at0 - e-a(t0+.t) c0 = . (12) a.t This single term formula 
is computationally ef.cient since the Fourier transform of the window can be precomputed and tabulated, 
which is the only memory requirement. Moreover both complex ex­ponentials are conjugate of each other 
meaning that we only need to compute one sine and one cosine. 4.2 Speedup and Numerical Validation Consider 
a scene requiring the synthesis of M modes. Using a standard recursive time-domain solution [van den 
Doel and Pai 2003], and assuming 512-sample frames, the cost of the frame is M × 512 × Cmt . The cost 
Cmt of evaluating a mode in the time domain is 5 or 6 multiplies and adds, using the recursive formula­tion 
of Eq. 6 in [van den Doel and Pai 2003]. In our approach, assuming an average of B bins per mode, the 
cost of a frame is M × B × CST FT plus the cost (once per frame) of the inverse FFT. The cost CST FT 
of evaluating Eq. 11, is about 25 operations. With a value of B = 3 (it is often lower in practice), 
we have a potential theoretical speedup factor of 30-40 times. If we take into account the fact that 
we have a 50% overlap due to windowing, this theoret­ical speedup factor drops to 15-20 times. We have 
used our previous test scene to measure the speedup of our approach in practice, compared to [van den 
Doel and Pai 2003]. When using B = 3 bins per mode, we found an average speedup of about 8, and with 
B = 5 bins per mode about 5. This reduction compared to the theoretical speedup is probably due to compiler 
and optimization issues of the two different algorithms. Finally, we examine the error of our approximation 
for a single sound. We tested two different windows, a Hann window with 50% overlap and a Rectangular 
window with 10% inter-frame blending. In Fig. 2 we show 3 frames, with the reference [van den Doel and 
Pai 2003] in red and our approximation in blue, taking B = 5 bins per mode. Taken over a sequence including 
three impacts (a single pipe in the Magnet scene, see Sect. 7), for a total duration of about 1 sec., 
the average overall error for the Rectangular window is 15% with 5 bins, and 21% with 3 bins (it is 8% 
if we use all 512 bins). This error is mainly due to small ringing artifacts and possibly to our constant 
exponential approximation, which can be seen at frame ends (see Fig. 2). Using the Hann window, we have 
35-36% error for both 512 and 5 bins, and 36% with 3 bins. This would indicate that the error is mainly 
due to the choice of window. As can be seen in the graph (Fig. 2 (right)) the error with the Hann window 
is almost entirely in the .rst frame, at the onset, or attack , of the sound for the .rst 512 samples 
(i.e., 11 msec at 44.1kHz). The overall quality of the signal is thus preserved in most frames; in contrast, 
the ringing due to the rectangular window can result in audible artifacts. For this reason, and to be 
compatible with the pipeline of [Moeck et al. 2007], we chose to use the Hann window. Figure 2: Comparison 
of Reference with Hann window and with Rectangular window reconstruction using a 1024-tap FFT. 4.3 Limitations 
for the Attacks of Impact Sounds Contrary to time domain approaches such as [van den Doel and Pai 2003; 
Raghuvanshi and Lin 2006], the use of the Hann window as well as the constant exponential approximation 
(CEA) degrades the onset or attack for high frequency modes. This attack is typically contained in the 
.rst few frames, depending on decay rate. To study the individual effect of each of the CEA and the Hann 
win­dow in our reconstruction process, we computed a time-domain so­lution using the CEA for the example 
of a falling box (Fig. 3(left)) and a time domain solution using a Hann window to reconstruct the signal 
(Fig. 3(right)). We plot the time-domain reference [van den Doel and Pai 2003] in red and the approximation 
in blue. As we can see, most of the error in the .rst 7msec is due to the Hann window whereas the CEA 
error remains lower. The effect of these approx­imations is the suppression of the crispness of the attacks 
of the impact sounds.  Use of the Hann window and the CEA as described previously has the bene.t of 
allowing seamless integration between recorded and impact sounds, as described next in Sect. 5. In complex 
sound­scapes containing many recorded sounds, this approximation may be acceptable. However, in other 
cases the crispness of the attacks of the modal sounds can be important. Figure 3: Comparison of Constant 
Exponential Approximation in the time domain (TD) and Hann window reconstruction in the TD, with the 
reference for the sharp sound of a falling box. To better preserve the attacks of the impacts sounds, 
we treat them in a separate buffer and split the .rst 1024-sample frame into four subframes. Each subframe 
has a corresponding CEA and with a specialized window. In what follows, we assume that all contact sounds 
start at the beginning of a frame. We design a windowing scheme satisfying four constraints: 1) avoid 
ramping up to avoid smoothing the attack, 2) end with a 512 sample square root of Hann window to blend 
with the buffer for all frames other than the attack, 3) achieve perfect reconstruc­tion, i.e., all windows 
sum to one, 4) require a minimal number of bins overall, i.e., use Hann windows which minimize the number 
of bins required for reconstruction [Oppenheim et al. 1999]. The .rst subframe is synthesized using a 
rectangular window for the .rst 128 samples (constraint 1) followed by half of a Hann window ( semi-Hann 
from now on) for the next 128 samples and zeros in the remaining 768 samples; this is shown in blue in 
Fig. 4. The next two subframes use full 256-sample Hann windows, starting at samples 128 and 256 respectively 
(red and green in Fig. 4). The last subframe is composed of a semi-Hann window from samples 384 to 512 
and a square root of a semi-Hann window for the last 512 sam­ples for correct overlap with the non-attack 
frames, thus satisfying constraint 2 (black in Fig. 4). All windows sum to 1 (constraint 3), and Hann 
windows are used everywhere except for the .rst 128 samples (constraint 4). These four buffers are summed 
before per­forming the inverse FFT, replacing the original 1024 sample frame by the new combined frame. 
We use 15 bins in the .rst subframe. Figure 4: Four sub-windows to better preserve the sound attack. 
mode: for the example shown in the video the additional cost of the mixed window for attacks is 1.2%. 
However, the integration of this method with recorded sounds is somewhat more involved; we discuss this 
in Sect. 9. 5 A Full Perceptually Based Scalable Pipeline for Modal and Recorded Sounds An inherent advantage 
of frequency domain processing is that it allows .ne-grain scalable audio processing at the level of 
an FFT bin. In [Tsingos 2005], such an approach was proposed to perform equalization and mixing, reverberation 
processing and spatializa­tion on prerecorded sounds. Signals are prioritized at runtime and a number 
of frequency bins are allocated to each source, thus respect­ing a prede.ned budget of operations. This 
importance sampling strategy is driven by the energy of each source at each frame and used to determine 
the cut-off point in the list of STFT coef.cients. Given our fast frequency-domain processing described 
above, we can also use such an approach. In addition to the fast STFT synthe­sis, we also require an 
estimation of energy, both of the entire im­pact sound, and of each individual mode. In [Tsingos 2005] 
sounds were pre-recorded, and the FFT bins were precomputed and pre­sorted by decreasing energy. For 
our scalable processing approach, we .rst .x an overall mix­ing budget, e.g., 10,000 bins to be mixed 
per audio frame. At each frame we compute the energy Es of each impact sound over the frame and allocate 
a budget of frequency bins per sound propor­tional to its energy. We compute the energy Em of each mode 
for the entire duration of the sound once, at the time of each impact, and we use this energy weighted 
by the mode s squared amplitude to proportionally allocate bins to modes within a sound. After ex­perimenting 
with several values, we assign 5 bins to the 3 modes with highest energy, 3 bins for the next 6 and 1 
bin for all remaining modes. We summarize these steps in the following pseudo-code. 1. PerImpactProcessing(ImpactSound 
S) // at impact noti.cation 2. foreach mode of S 3. compute total energy Em 4. Sort modes of S by decreasing 
Em 5. Compute total energy of S for cutoff 6. Schedule S for processing  1. ScalableAudioProcessing() 
// called at each audio frame 2. foreach sound S 3. Compute Es 4. Allocate FFT bin budget based on 
Es 5. Modes m1, m2, m3 get 5 bins 6. Modes m4 - m9 get 3 bins 7. 1 bin to remaining modes until end 
of budget 8. endfor  5.1 Ef.cient Energy Estimation To allocate the computation budget for each impact 
sound, we need to compute the energy, Es, of a modal sound s in a given frame, i.e., from time t to time 
t + .t: . t + .t Es = s2(x)dx. (13) t From Eq. 1 and 2, we express Es as: MM The increase in computational 
cost is negligible, since the addi- Es . . = < s,s > = aia j < mi,mj >. (14) tional operations are 
only performed in the .rst frame of each i=0 j=0  For a given frame, the scalar product < mi,mj > has 
an analytic expression (see Eq. 22 in the additional material). Because this scalar product is symmetric, 
we only have to compute half of the required operations. In our experiments, we observed that most of 
the energy of an im­pact sound is usually concentrated in a small number N of modes (typically 3). To 
identify the N modes with highest energy, we com­pute the total energy, Em, of each mode as: . 8 ..21 
.2 Em = sin(.x)e-ax dx = (15)0 4 a(a2 + .2) After computing the Em s for each mode, we weight them by 
the square of the mode s amplitude. We then sort the modes by de­creasing weighted energy. To evaluate 
Eq. 14 we only consider the N modes with highest energy. We re-use the result of this sort for budget 
allocation. We also compute the total energy for a sound, which is used to determine its duration, typically 
when 99% of the energy has been played back. We use Eq. 14 and an expression for the total energy (rather 
than over a frame), given in the additional material (Eq. 21). Numerical Validation We use the test scene 
presented in Sect. 4.1 (Fig. 6) to perform numerical tests with appropriate values for N. We evaluated 
the average error of the estimated energy, compared to a full computation. Using 3 modes, for all objects 
in this scene, the error is less than 9%; for 5 modes it falls to 4.9%. 5.2 A Complete Combined Audio 
Pipeline In addition to frequency-domain scalable processing, we can also use the perceptual masking 
and sound-source clustering approaches developed in [Tsingos et al. 2004; Moeck et al. 2007]. We can 
thus mix pre-recorded sounds, for which the STFT and energy have been precomputed, with our frequency 
domain representation for modal sounds and perform global budget allocation for all sounds. As a result, 
masking between sounds is taken into account, and we can cluster the surviving unmasked sources, thus 
optimizing the time for per-sound source operations such as spatialization. In previ­ous work, the masking 
power of a sound also depends on a tonal­ity indicator describing whether the signal is closer to a tone 
or a noise, noisier signals being stronger maskers. We computed tonal­ity values using a spectral .atness 
measure [Tsingos et al. 2004] for several modal sounds and obtained an average of 0.7. We use this constant 
value for all modal sounds in our masking pipeline. 6 Temporal Scheduling One problem with systems simulating 
impact sounds is that a large number of events may happen in a very short time interval (debris from 
an explosion, a collapsing pile of objects, etc.), typically dur­ing a single frame of the physics simulation. 
As a result, all sounds will be triggered simultaneously resulting in a large peak in system load and 
possible artifacts in the audio ( cracks ) or lower audio quality overall. Our idea is to spread out 
the peaks over time, ex­ploiting results on audio-visual human perception. As mentioned in Sect. 2, there 
has been extensive study of audio­visual asynchrony in neuroscience which indicates that the brain is 
able to compensate for the different delays between an auditory and a visual event in causal inference. 
To exploit this property, we in­troduce a scheduling step at the beginning of the treatment of each audio 
frame. In particular, we maintain a list of sound events pro­posed by the physics engine (which we call 
TempSoundsList) and a Figure 5: Effect of temporal scheduling; computational peaks are delayed, the 
slope of increase in computation time is smoothed out and the duration of peaks is reduced. Data for 
the Magnet se­quence, starting just before the group of objects hits the .oor. list of sounds currently 
processed by the audio engine (CurrSound­sList). At the beginning of each audio frame, we traverse Temp-SoundsList 
and add up to 20 new sounds to CurrSoundsList if one of the following is true: CurrSoundsList contains 
less than 50 sounds  Sound s has been in TempSoundsList for more than T msec.  The values 20 and 50 
were empirically chosen after experimenta­tion. We use our perceptual tolerance to audio-visual asynchrony 
by manipulating the threshold value T for each sound. In particular we set T to a desired value, for 
example 200 msec corresponding to the results of [Guski and Troje 2003]. We then further modulate T for 
sounds which are outside the visible frustum; T is increased progressively as the direction of the sound 
is further from the center of the .eld of view. For sounds completely behind the viewer the delay T is 
set to a maximum of 0.5 seconds. Temporal scheduling only occurs for impact sounds, and not for recorded 
sounds such as a gun .ring which are time-critical and can have a remote effect. Our approach reduces 
the number and density of computational peaks over time. Although some peaks still occur, they tend to 
be smaller and/or sharper, i.e., occur over a single frame (see Fig. 5). Since our interactive system 
uses buffered audio output, it can sus­tain such sparse peaks over a single frame, while it could not 
sustain such a computational overload over several consecutive frames. 7 Implementation and Results Our 
system is built on the Ogre3D1 game engine, and uses the PhysX 2 real-time physics engine simulator. 
Throughout this pa­per, we use our own (re-)implementations of [van den Doel and Pai 2003], [O Brien 
et al. 2002] and [Raghuvanshi and Lin 2006]. For [van den Doel and Pai 2003], we use T = 512 samples 
at 44.1KHz; the size of the impact .lter with a force pro.le of cos(2pt/T ) is T = 0.37ms or 16 samples 
(Eq.17 of that paper). For objects generating impact sounds, we precompute modes using the method of 
O Brien et al. [2002]. Sound radiation amplitudes of each mode are estimated with a far-.eld radiation 
model (Eq. 15, [James et al. 2006]). Audio processing was performed using our in-house audio engine, 
with appropriate links between the graphics and audio. The audio 1http://www.ogre3d.org 2http://www.ageia.com 
  engine is described in detail in [Moeck et al. 2007]. In total we run three separate threads: one 
for each of the physics engine, graphics and audio. All timings are reported on a dual-processor, dual-core 
Xeon running at 2.3Ghz. 7.1 Interactive Sessions Using the Pipeline We have constructed four main scenes 
for demonstration purposes, which we refer to as Oriental , Magnet , Truck and Boxes ; we show snapshots 
of each in Fig. 1 (Truck and Oriental) and 6. The goal was to construct environments which are similar 
to typical simulators or games settings, and which include a large number of impact sounds, as well as 
several prerecorded sounds. All sounds were processed in the Fourier-domain at 44.1kHz using 1024-tap 
FFTs and 50% overlap-add reconstruction with Hann windowing. The Oriental and Box scenes contain modal 
sounds only and thus use the attack preserving approach (Sect. 4.3). Hence, our audio thread runs at 
83Hz; we then output reconstructed audio frames of 512 samples. The physics thread updates object motion 
at 140Hz, and the video thread runs at between 30-60Hz depending on the scene and the rendering quality 
(shadows, etc.). The Magnet scene contains prerecorded industrial machinery and closing door sounds, 
while the Truck scene contains traf.c and he­licopter sounds. Basic scene statistics are given in Table 
1, for the demo versions of the scenes shown in the video. Scene O T P Mi M/o Oriental Boxes Magnet Truck 
173 200 110 214 730K 200K 300K 600K 0 0 16 15 665 678 971 268 214 376 164 221 Table 1: Basic statistics 
for example scenes. O: number of objects simulated by the physics engine producing contact sounds, T 
: total number of triangles in the scene, P: number of pre-recorded sounds in the scene and Mi: maximum 
number of impact sounds played per frame. M/o: average number of modes/object. 7.2 Quality and Performance 
We performed two comparisons for our fast modal synthesis: the .rst was with the standard time-domain 
(TD) method of [van den Doel and Pai 2003] using recursive evaluation, and the second with the mode-culling 
time-domain approach of [Raghuvanshi and Lin 2006], which is the fastest TD method to date. We used the 
Orien­tal scene for this comparison, containing only modal sounds. Comparison to standard TD synthesis 
In terms of quality, we tested examples of our frequency-domain synthesis with 3 and 5 bins per mode, 
together with a time-domain reference, shown in the accompanying video. The quality for 5 bins is very 
close to the ref­erence. The observed speedup was 5-8 times, compared to [van den Doel and Pai 2003]. 
Comparison to mode-culling TD synthesis To compare to mode­culling, we apply the mode truncation and 
quality scaling stages of [Raghuvanshi and Lin 2006] at each audio frame. We then per­form fast frequency 
domain synthesis for the modes which have not been culled. For the same mode budget our frequency processing 
allows a speedup of 4-8 times; the difference in speedup with the standard TD synthesis is due to implementation 
issues. The qual­ity of the two approaches is slightly different for the same mode budget, but in both 
cases subjectively gives satisfactory results. Scene Total Mixing Energy Masking Clustering Magnet Truck 
3.2 2.7 1.3 1.5 0.6 0.9 1.0 0.3 0.3 0.1 Table 2: Cost in milliseconds of each stage of our full pipeline. 
Full Combined Pipeline The above comparisons are restricted to modal sounds only. We also present results 
for other scenes, aug­mented with recorded sounds. These are rendered using our full pipeline, at low 
overall budgets but with satisfactory quality. We present statistics for our approach in Table 2, using 
a budget of 8000 bins. First we indicate the cost (in milliseconds) of each component of our new combined 
pipeline: mixing, energy com­putation, masking and clustering, as well as the total cost. As we can see 
there is a non-negligible overhead of the pipeline stages; however the bene.t of being able to globally 
allocate budget across modal and recorded sounds, and of course all the perceptually based accelerations, 
justi.es this cost. The number of sounds at each frame over the course of the interac­tive sequences 
shown in the video varied between 195 and 970. If no masking or culling were applied there would be between 
30,000 to 100,000 modes to be played per audio frame on average in these sequences. We use 15,000 to 
20,000 frequency bins in all inter­active sessions. The percentage of prerecorded sounds masked was around 
50% on average and that of impact sounds was around 30%. 8 Pilot Perceptual Evaluation Despite previous 
experimental studies for perceptually based au­dio rendering for pre-recorded sounds [Moeck et al. 2007; 
Tsingos et al. 2004], and the original neuroscience experiments for asyn­chrony [Guski and Troje 2003], 
we consider it imperative to con­duct our own pilot study, since our context is very different. We have 
two conditions in our experiment: the goal of the .rst condi­tion is to evaluate the overall audio quality 
of our approximations while that of the second is to evaluate our temporal scheduling. 8.1 Experiment 
Setup and Procedure In our experiment we used the Magnet and Truck (Sect. 7) environ­ments, but with 
fewer objects, to avoid making the task too hard for the subjects. We used two 6 second pre-recorded 
paths for each of the two scenes. To ensure that all the stimuli represent the exact same sequence of 
events and to allow the presentation of a refer­ence in real-time, we synchronize all threads and store 
the output audio and video frames of our application to disk. Evidently any other delay in the system 
has to be taken into account. We choose the parameters of our simulation to be such that we do not perceive 
cracks in the audio when running interactively with the same bud­get settings. Video sequences are then 
played back during the study. For the reference sequences, all contact sounds are computed in the time-domain 
and no perceptual processing is applied when mix­ing with the recorded sounds in the frequency domain. 
We use non-individualized binaural rendering using Head Related Transfer Functions (HRTFs) chosen from 
the Listen database3. The interface is a MUSHRA-like [ITU 2001-2003] slider panel (see the accompanying 
video), in which the user can choose between a reference and .ve different approximations (A, B, C, D, 
E), each with a different budget of frequency bins. The subject uses a slider to rate the quality of 
each stimulus. One of the .ve stimuli is a hidden reference. The radio button above each slider allows 
the 3http://recherche.ircam.fr/equipes/salles/listen/  Figure 6: From left to right, snapshots of the 
large Magnet scene, the Boxes scene, the test scene used for numerical validation and our prototype rolling 
demo. Please see and listen to the accompanying video showing these scenes. Budget FFT bins Audio Delay 
(msec) Scene C1 C2 C3 C4 T1 T2 T3 T4 Magnet 700 1.5K 2.5K 4K 0 120 200 400 Truck 1K 2K 4K 8K 0 120 200 
400 Table 3: Budget and delay values used for the perceptual experi­ments. Ci and Ti are the budget 
and delay conditions used. Percent Perceived Quality % Perceived Asynchrony Scene C1 C2 C3 C4 Re f T1 
T2 T3 T4 Magnet1 51.1 64.7 78.0 83.1 84.9 0 24 48 71 Magnet2 48.8 70.1 76.5 85.2 88.9 10 5 0 10 Truck1 
26.3 41.8 54.2 66.0 87.3 24 24 14 38 Truck2 24.8 28.6 42.7 66.0 89.0 14 14 38 29 Table 4: Results of 
our experiments: average quality and percent perceived asynchrony for the 2 scenes and 2 paths. subject 
to (re)start the corresponding sequence. For the synchro­nization condition, we choose one budget which 
has a good rating in the quality condition (typically C3 in Table 3), and delay the au­dio relative to 
graphics by a variable threshold T . The budgets and thresholds used are shown in Table 3. A tick box 
is added under each sound for synchrony judgment. The experiment interface can be seen in the accompanying 
video. The subject listens to the audio with headphones and is instructed to attend to both visuals and 
audio. There are 8 panels, corresponding to all the conditions; stimuli are presented in random order. 
De­tailed instructions are given on screen to the user, who is asked to rate the quality and tick if 
asynchrony between audio and visuals is detected. Rating of each panel is limited to 3 minutes, at which 
point rating is disabled. 8.2 Analysis of the Experiments We ran the experiment with 21 subjects who 
were members of our research institutes, and were all naive about the goal of our exper­iments. We show 
the average quality ratings and the percent per­ceived asynchrony averages for the experiment in Table 
4. As we can see, for the Magnet scene, the budget of 4,000 bins was suf.cient to give quality ratings 
of 83-85% very close to the hidden reference rated at 84-89%. An analysis of variance (ANOVA) [Howell 
1992] with repeated measures on quality ratings shows a main effect of budget on perceived quality (F(4,20)=84.8, 
p<0.00001). For the Truck scene the quality rating for 8,000 bins was lower. This is possibly due to 
the fact that the recorded sounds require a signi.cant part of the frequency bin budget, and as a result 
lower the overall perceived quality. In terms of asynchrony, the results have high variance. However, 
it is clear that audiovisual asynchrony was perceived less than 25% of the time, for delays under 200msec. 
Overall, we consider these results to be a satisfactory indication that our approximations work well 
both in terms of progressive pro­cessing and for our temporal scheduling algorithm. In particular, there 
is a strong indication that increasing the budget does result in perceptually improved sounds, and that 
only a small percentage of users perceive asynchrony with temporal scheduling with delays less than 200ms. 
9 Discussion and Conclusions We have presented a new frequency-domain approach to modal sound rendering, 
which exploits sparseness of the Fourier Trans­form of modal sounds, leading to an 4-8 speedup compared 
to time-domain approaches [van den Doel and Pai 2003; Raghuvan­shi and Lin 2006], with slight quality 
degradation. Furthermore, our approach allows us to introduce a combined perceptual audio pipeline, treating 
both prerecorded and on-the-.y impact sounds, and exploiting scalable processing, auditory masking, and 
cluster­ing of sound sources. We used crossmodal results on perception of audiovisual synchronization 
to smooth out computational peaks which are frequently caused by impact sounds, and we performed a pilot 
perceptual study to evaluate our combined pipeline. Use of the Hann window allows direct integration 
of modal and recorded sounds (see Sect. 5), but leads to lower quality attacks of impact sounds. We have 
developed a solution to this problem, splitting the treatment of the .rst frame of each attack into four 
sub­frames with appropriate windows. This solution can be easily used in scenes exclusively containing 
modal sounds. For the pipeline combining recorded and modal sounds, and in particular for clus­tering, 
we would need a separate buffer for attacks in each cluster thus performing twice as much post-processing 
(HRTF processing etc.). The rest of the pipeline would remain unchanged. We developed an initial solution 
for rolling sounds in our pipeline, using a noise-like sound pro.le, similar in spirit to [van den Doel 
et al. 2001]. To simulate the continuous rolling excitation, we pre­compute a noise pro.le in the Fourier 
domain, and perform dy­namic low-pass .ltering based on velocity. Convolution with the excitation is 
a simple product in the Fourier domain, making our approach ef.cient. The accompanying video contains 
a .rst exam­ple. Nonetheless, a general solution to continuous excitation in our framework requires mixing 
delayed copies of past frames, incurring additional costs. We expect masking and progressive processing 
to limit this overhead, similar to the reverberation in [Tsingos 2005]. Another limitation is the overhead 
of our pipeline which is not neg­ligible. For practical usage of real-time audio rendering, such as game 
engines, we believe that the bene.ts outweigh this drawback. In addition to the perceptually based accelerations, 
we believe that the ability to treat recorded and synthesized sounds in a uni.ed manner is very important 
for practical applications such as games.  Currently, when reducing the budget very aggressively the 
energy computation can become a dominant cost. It is possible to pre­compute a restricted version of 
the energy, if we assume that forces are always applied in the normal direction. This is the case for 
recording-based systems (e.g., [van den Doel and Pai 1998]). How­ever, this reduces the .exibility of 
the system. Acknowledgments This research was partly funded by the EU FET project CROSSMOD (014891-2 
http://www.crossmod.org). We thank Autodesk for the donation of Maya and Alexandre Olivier-Mangon and 
Fernanda Andrade Cabral for modeling the examples. We thank the anonymous reviewers whose suggestions 
made this a much better paper. The last author acknowledges a fellowship from the Alfred P. Sloan Foundation. 
References ALAIS, D., AND CARLILE, S. 2005. Synchronizing to real events: subjective audiovisual alignment 
scales with perceived auditory depth and speed of sound. Proc Natl Acad Sci 102, 6, 2244 7. BEGAULT, 
D. 1999. Auditory and non-auditory factors that poten­tially in.uence virtual acoustic imagery. In Proc. 
AES 16th Int. Conf. on Spatial Sound Reproduction, 13 26. FUJISAKI, W., SHIMOJO, S., KASHINO, M., AND 
NISHIDA, S. 2004. Recalibration of audiovisual simultaneity. Nature Neuro­science 7, 7, 773 8. GUSKI, 
R., AND TROJE, N. 2003. Audiovisual phenomenal causality. Perception and Psychophysics 65, 5, 789 800. 
HORMANDER, L. 1983. The Analysis of Linear Partial Differential Operators I. Springer-Verlag. HOWELL, 
D. C. 1992. Statistical Methods for Psychology. PWS-Kent. ITU. 2001-2003. Method for the subjective assessment 
of inter­mediate quality level of coding systems, rec. ITU-R BS.1534-1, http://www.itu.int/. JAMES, D. 
L., BARBIC, J., AND PAI, D. K. 2006. Precomputed acoustic transfer: Output-sensitive, accurate sound 
generation for geometrically complex vibration sources. ACM Transactions on Graphics (ACM SIGGRAPH) 25, 
3 (July), 987 995. LARSSON, P., V ¨ ALL, D., AND KLEINER, M. 2002. Better ASTFJ ¨ presence and performance 
in virtual environments by improved binaural sound rendering. Proc. AES 22nd Intl. Conf. on virtual, 
synthetic and entertainment audio (June), 31 38. MOECK, T., BONNEEL, N., TSINGOS, N., DRETTAKIS, G., 
VIAUD-DELMON, I., AND ALOZA, D. 2007. Progressive per­ceptual audio rendering of complex scenes. In ACM 
SIGGRAPH Symp. on Interactive 3D Graphics and Games (I3D), 189 196. O BRIEN, J. F., SHEN, C., AND GATCHALIAN, 
C. M. 2002. Synthesizing sounds from rigid-body simulations. In ACM SIG-GRAPH Symp. on Computer Animation, 
175 181. OPPENHEIM, A. V., SCHAFER, R. W., AND BUCK, J. R. 1999. Discrete-Time Signal Processing (2nd 
edition). Prentice-Hall. PAI, D. K., VAN DEN DOEL, K., JAMES, D. L., LANG, J., LLOYD, J. E., RICHMOND, 
J. L., AND YAU, S. H. 2001. Scan­ning physical interaction behavior of 3d objects. In Proc. ACM SIGGRAPH 
2001, 87 96. PRESS, W. H., TEUKOLSKY, S. A., VETTERLING, W. T., AND FLANNERY, B. P. 1992. Numerical recipes 
in C: The art of scienti.c computing. Cambridge University Press. RAGHUVANSHI, N., AND LIN, M. C. 2006. 
Interactive sound syn­thesis for large scale environments. In ACM SIGGRAPH Symp. on Interactive 3D Graphics 
and Games (I3D), 101 108. RODET, X., AND DEPALLE, P. 1992. Spectral envelopes and in­verse FFT synthesis. 
In Proc. 93rd Conv. AES, San Francisco. SEKULER, R., SEKULER, A. B., AND LAU, R. 1997. Sound alters visual 
motion perception. Nature 385, 6614, 308. SUGITA, Y., AND SUZUKI, Y. 2003. Audiovisual perception: Im­plicit 
estimation of sound-arrival time. Nature 421, 6926, 911. TSINGOS, N., GALLO, E., AND DRETTAKIS, G. 2004. 
Perceptual audio rendering of complex virtual environments. ACM Trans­actions on Graphics (ACM SIGGRAPH) 
23, 3 (July), 249 258. TSINGOS, N. 2005. Scalable perceptual mixing and .ltering of au­dio signals using 
an augmented spectral representation. In Proc. Int. Conf. on Digital Audio Effects, 277 282. VAN DEN 
DOEL, K., AND PAI, D. K. 1998. The sounds of physical shapes. Presence 7, 4, 382 395. VAN DEN DOEL, K., 
AND PAI, D. K. 2003. Modal synthesis for vibrating objects. Audio Anecdotes. VAN DEN DOEL, K., KRY, P. 
G., AND PAI, D. K. 2001. FoleyAu­tomatic: physically-based sound effects for interactive simula­tion 
and animation. In Proc. ACM SIGGRAPH 2001, 537 544. VAN DEN DOEL, K., PAI, D. K., ADAM, T., KORTCHMAR, 
L., AND PICHORA-FULLER, K. 2002. Measurements of perceptual quality of contact sound models. Intl. Conf. 
on Auditory Display, (ICAD), 345 349. VAN DEN DOEL, K., KNOTT, D., AND PAI, D. K. 2004. In­teractive 
simulation of complex audiovisual scenes. Presence: Teleoperators and Virtual Environments 13, 1, 99 
111. Z¨ OLZER, U. 2002. Digital Audio Effects (DAFX) chapter 8. Wiley. Appendix A.1 Some Elements of 
Distribution Theory Applying a distribution T to a smooth test function with local sup­port f , implies 
the following operation: . 8 < T, f >= Tf (x)dx. (16) -8 A commonly used distribution is the Dirac distribution 
(note that this is not the Kronecker delta) which has value 0 everywhere, ex­cept at 0. < dk, f >= f 
(k) is commonly used in signal processing (Dirac combs). We use the following properties of distributions: 
d(n) f (n) d0 . f = f 0 . f = (17) where f (n) denotes the nth derivative of f . d(n) da(t) . f (t)= 
f (t - a) a (t) . f (t)= f (n)(t - a) (18) 1 F ( f (t)g(t)) = F ( f (t)) . F (g(t)) (19) 2p  Additional 
Material: Formulas for energy com­putation We present here several expressions which we use in the computa­tion 
of energy for modes. The instant energy of a mode is given by: t2 2 sin(.x)e-axdx = t1  -2at1 1 ea2 
+ .2 - a2 cos(2.t1)+ a.sin(2.t1) 4 a(a2 + .2) -2at2 1 ea2 + .2 - a2 cos(2.t2)+ a. sin(2.t2) - (20) 4 
a(a2 + .2) To compute the total energy of two modes as IsI2 =< .i ai fi,. jajfj >= .i . j aiaj < fi, 
fj >, the expression < fi, fj > is given below, using Eq. 20: 8 -a1x-a2x sin(.1x)e.sin(.2x)edx = 2.1.2(a1 
+ a2) (21) (a1 + a2)2 +(.1 - .2)2(a1 + a2)2 +(.1 + .2)2 Similarly, the scalar product of two modes in 
a given interval (t,t+dt) is given as follows (we substitute a2 by a2 + a1 and .2 by .2 + .1): t+dt -a1x-a2x 
 sin(.1x)e.sin(.2x)edx = t(e-a2 (t+dt) ((.23 - 2 .1 .22+ a22 .2 - 2a22 .1) sin((t + dt).2 +(-2t - 2 dt).1)+ 
(-a2 .22 - a32) cos((t + dt).2 +(-2t - 2dt).1)+ ea2 dt ((-.23 + 2.1 .22 - a22 .2 + 2 a22 .1) sin(t .2 
- 2t .1)+ (a2 .22 + a23) cos(t .2 - 2t .1)+(.32 - 4.1 .22+ (4.21 + a22).2) sin(t .2)+ (-a2 .22 + 4 a2 
.1 .2 - 4a2 .21 - a23) cos(t .2))+ (-.32 + 4.1 .22 +(-4.21 - a22).2) sin((t + dt) .2)+ (a2 .22 - 4a2 
.1 .2 + 4 a2 .21 + a23) cos((t + dt).2))) /(2.42 - 8.1 .3 2 +(8.21 + 4a22) .22 - 8 a22 .1 .2 + 8a22 
.21 + 2 a24) (22) This expression can be computed, after appropriate factorization using 17 additions, 
24 multiplications, 8 cosine/sine operations, 2 exponentials and 1 division.  Progressive Perceptual 
Audio Rendering of Complex Scenes Thomas Moeck1,2 Nicolas Bonneel1 Nicolas Tsingos1 George Drettakis1 
IsabelleViaud-Delmon David Alloza 1REVES/INRIA Sophia-Antipolis CNRS-UPMC UMR 7593 EdenGames 2Computer 
Graphics Group, University of Erlangen-Nuremberg  Figure1: Left:A scene with1815 mobile sound sources. 
Audiois renderedin realtime with our progressive lossy processing technique using 15% of the frequencycoef.cients 
and with an average of 12 clusters for 3D audio processing. Degradations compared to the reference solution 
are minimal. Right: a scene with 1004 mobile sound sources, running with 25% of the frequencycoef.cients 
and 12 clusters. Abstract Despite recent advances, including sound source clustering and per­ceptual 
auditory masking, high quality rendering of complexvirtual scenes with thousands of sound sources remains 
a challenge. Two major bottlenecks appear as the scene complexity increases: the cost of clustering itself, 
and the cost of pre-mixing source signals within each cluster. In this paper, we .rst propose an improved 
hierarchical clustering algorithm that remains ef.cient for large numbers of sources and clusters while 
providing progressive re.nement capabilities. We then presenta lossy pre-mixing method based ona progressive 
rep­resentation of the input audio signals and the perceptual importance of each sound source. Our qualityevaluation 
user tests indicate that the recently introduced audio saliencymap is inappropriate for this task. Consequently 
we propose a pinnacle , loudness-based met­ric, which gives the best results for a variety of target 
computing budgets. We also performed a perceptual pilot study which indi­cates that in audio-visual environments, 
it is better to allocate more clustersto visible sound sources.We proposeanewclustering met­ric using 
this result. As a result of these three solutions, our sys­tem can provide high quality renderingof thousands 
of 3D-sound sources on a gamer-style PC. Keywords: Audio rendering, auditory masking, ventriloquism, 
clustering 1 Introduction Spatialized audio renderingisavery importantfactorforthe real­ism of interactive 
virtual environments, such as those used in com­putergames, virtual reality, or driving/.ight simulators, 
etc. The complexity and realism of the scenes used in these applications has increased dramatically over 
the last few years. The number of ob­jects which produce noise or sounds can become very large, e.g., 
carsina street scene, crowdsinthe stadiumofasportsgame etc.In addition, recent sophisticated physics 
engines can be used to syn­thesize complex sound effects driven by the physics, for example the individual 
impact sounds of thousands of pieces of a fractured object. Realistic3Daudio for such complex sounds 
scenes is beyond the capabilitiesofeventhe most recent3D audio rendering algorithms. The computational 
bottlenecks are numerous,but can be grouped into two broad types: the cost of spatialization, which is 
related to the audio restitution format used; and the per sound source cost, which relates to the different 
kinds of effects desired. An example ofthe former bottleneckisHead RelatedTransfer Function(HRTF) processing 
for binaural rendering [Møller 1992], or the rendering of numerous output channels foraWave FieldSynthesis 
(WFS) sys­tem [Berkhout et al. 1993], which can use hundreds of speakers. The latter bottleneck includes 
typical spatial effects such as delays, the Doppler effect, reverberation calculations,but also anykind 
of studio effect the sound engineers/designers wish to use in the appli­cation at hand. Recent research 
has proposed solutions to these computational lim­itations. Perceptual masking with sound source clustering 
[Tsingos et al. 2004], or other clustering methods [Herder 1999;Wand and Straßer 2004]do resolve someof 
the issues. However, the cluster­ing algorithms proposed to date are either restricted to static scenes, 
or add an unacceptable computation overhead due to a quadratic step in cluster construction when the 
number of sources is large. In addition, the cost of per source computation, sometimes called pre­mixing, 
can quickly becomeabottleneck,againfor complexsound­scapes. In this paper we present two contributions: 
The .rst contribution is the speed improvement of the clus­tering and premixing steps of the [Tsingos 
et al. 2004] ap­proach. First, we introduceanewrecursiveclustering method, which can operatewitha .xed 
orvariable target clusterbud­get, and thus addresses the issue of spatialization cost men­tioned above. 
Second, we develop a novel pinnacle-based scalable premixing algorithm, based on [Tsingos 2005], pro­viding 
a .exible, perceptually-based framework for the treat­ment of the per-source computation costs.  The 
second contribution is the investigation of perceptual is­sues related to clustering and premixing, based 
on pilot user studies we conducted. In particular, we investigate the in.u­ence of visuals on audio clustering 
for audio-visual scenes, and propose a modi.ed metric taking into account the indica­tion that it is 
probably better to have more sources in the view frustum.For scalablepremixing, weevaluated the different 
metrics which can be used, including recently developed per­ceptual models, such as the audio saliencymap 
[Kayser et al. 2005], and performed a perceptual quality test for the new al­gorithm. In the following, 
we brie.yoverviewrelated previouswork, includ­ing that in the acoustics and perception literature. We 
then present the new recursive clustering method in Sect. 3, the new study and the resulting algorithm 
for scalable premixing in Sect. 4 and the study and novel metric for crossmodal audio-visual clustering 
in Sect. 5. We present some implementation issues and results, then conclude with a discussion of our 
approach. 2 Previous work Relatively little effort has been devoted to the design of scalable rendering 
strategies for 3D audio, which can provide level-of-detail selectionand gracefuldegradation.Wegivebelowashortoverview 
of the previous work most relevant to our problem. Encoding and rendering of spatial auditory cues. Progressivespa­tial 
sound encoding techniques can be roughly subdivided in two categories. A .rst category is based on a 
physical reconstruction of the wave.eld at the ears of the listener. For instance, several ap­proaches 
have been proposed to perform progressive binaural ren­dering [Blauert 1997] by decomposing HRTFs on 
a set of basis functions through principal component analysis [Chen et al. 1995; Larcheretal.2000;JotandWalsh 
2006];whileprovidinglevel-of­detail, they are limited to this unique restitution format. Alterna­tively, 
decomposition of the wave.eld can be used (e.g., spherical harmonics) [Malham and Myatt 1995] for level-of-detail, 
relatively independently of restitution setup (e.g., conversion to binaural for­mat[Jotetal. 1999]). 
High accuracy requiresalarge numberof channels however, limiting the methods applicability. Another category 
performs world-space compression of positional cues by clustering nearby sound sources and using a unique 
repre­sentative position per cluster for spatial audio processing [Herder 1999; Wand and Straßer 2004; 
Tsingos et al. 2004]. Wand et al. [Wand and Straßer 2004] group sources in a hierarchical spa­tial data 
structure as used for point-based rendering. However, their approachisveryef.cientonlyinthe caseof static 
sources. Tsingos et al. [Tsingos et al. 2004] recently introduced a clustering algo­rithm drivenbyaloudness-weighted 
geometrical cost-function (see Sec. 3). Theyalso use precomputed descriptors (e.g., loudness and tonality) 
to sort the sources by decreasing importance and perform a greedy culling operation by evaluating auditory 
masking at each time-frame of the simulation. Inaudible sources can then be safely discarded. Although 
this approach was found to perform well for environments containing a few hundred sources, it is unclear 
that it scales well to larger numbers of sources and clusters due to the cost of the proposed clustering 
algorithm which, in their case, implies a near quadratic number of evaluations of the cost-function. 
Scalable audio processing. Fouad et al. [Fouad et al. 1997] pro­pose a level-of-detail progressive audio 
rendering approach in the time-domain; by processing every n-th sample, artifacts are intro­ducedatlowbudgetlevels.Wandand 
Straßer[Wandand Straßer 2004] introduce an importance sampling strategy using random se­lection, but 
ignore the signal properties, thus potentially limiting the applicability of this method. A family of 
approaches has been proposed to directly process perceptually-coded audio signals [Lanciani and Schafer 
1997; Lan­ciani and Schafer 1999; Darlington et al. 2002; Touimi 2000; Touimi et al. 2004] yielding faster 
implementations than a full decode-process-recode cycle. Although they are well suited to dis­tributed 
applications involving streaming over low-bandwith chan­nels, they require speci.c coding of the .lters 
and processing. Moreover, they cannot guarantee ef.cient processing for a mix­ture of several signals, 
nor that they would produce an optimal re­sult. Other methods have explored how to extend these approaches 
by concurrently prioritizing subparts of the original signals to pro­cesstoguarantee a minimaldegradationinthe 
.nal result [Gallo et al. 2005; Tsingos 2005; Kelly and Tew 2002]. Most of them exploit masking and continuity 
illusion phenomena[Kelly andTew 2002]to removeentire framesofthe originalaudiodatainthetime­domain[Galloetal.2005]or,ona.nerscale, 
processonlyalimited numberof frequency-domainFourier coef.cients [Tsingos 2005]. Crossmodal studies. 
While the primary application of 3D audio rendering techniques is simulation and gaming, no spatial audio 
rendering work to date evaluates the in.uence of combined vi­sual and audio restitution on the required 
quality of the simula­tion. However, a vast amount of literature in neurosciences suggest that cross-modal 
effects, such as ventriloquism, might signi.cantly affect 3D audio perception [Hairston et al. 2003; 
Alais and Burr 2004]. This effect tells us that in presence of visual cues, the lo­cation of a sound 
source is perceived shifted toward the visual cue, up to a certain threshold of spatial congruency. Above 
this thresh­old, there is a con.ict between the perceived sound location and its visual representation 
andthe ventriloquism effect no longer occurs. The spatial window (or angular threshold)of this effect 
seems to depend on severalfactors (e.g., temporal synchronicity between the two channels and perceptual 
unity of the bimodal event) and can vary from a few degrees [Lewald et al. 2001] up to 15. [Hairston 
et al. 2003]. 3 Optimized Recursive Clustering In previous work [Tsingos et al. 2004], large numbers 
of sound sources are dynamically grouped together in clusters, and a sin­gle new representative point 
sourceis created.While this approach allows the treatment of several hundred sound sources on a stan­dard 
platform, it does incur some computational overhead, which becomesapotential bottleneckasafunctionofthe 
numberof input­sources/target-clusters and theavailable computationalbudget.To resolve this limitation, 
we next present a new recursive clustering algorithm. For improved ef.ciencyand accuracywe propose two 
recursive algorithms: One witha .xedbudgetof clusters and one withavariable numberof clusters. The .xedbudget 
approach al­lows us to set a .xed computational cost for 3D audio processing andisalso usefulto addressa.xed 
numberof3D-audio hardware channels when available for rendering. The variable number of clusters dynamically 
adjusts the computational cost to obtain the best trade-off between quality and speed in each frame, 
given a user-speci.ed error threshold. We also discuss a variant of this algorithm which has been in­cludedin 
the commerciallyavailablegame Test Drive Unlimited by EdenGames/ATARI. In the method of [Tsingos et al. 
2004], sources are grouped together Figure2:Overviewofouroverallsound rendering pipeline.In particular,we 
introduceanimproved hierarchicalsound source clusteringthat better handles visible sources and a premixing 
technique for progressive per-source processing.  by using a clustering algorithm based on the Hochbaum-Shmoys 
heuristic [Hochbaum and Schmoys 1985]. First, this strategy se­lects n cluster representatives amongst 
the k original sourcesby do­ingafarthest-.rst traversalof the point set. The cost-function used isa combinationof 
angularand distance errorstothe listenerofthe candidate representative source with respect to the sources 
being clustered. An additional weighting term, based on each source s in­stantaneous loudness value, 
is used to limit error for loud sources. For details see [Tsingos et al. 2004]. 3.1 Recursive .xed-budget 
approach In a .rst pass, we run the original clustering algorithm with a target number of clusters n0. 
In each subsequent pass, every generated cluster gets divided into nk clusters. The totalbudget of clustering 
is thus .k nk. The original clustering algorithm can be considered asaspecial case where only n0 is set. 
In our tests, we typically used a two-level recursion. 3.2 Variable cluster budget This approach dynamically 
allocates the number of clusters in re­altime. This is especially useful for scenes where sounds are 
fre­quently changing during time in shape, energy as well as in loca­tion. The algorithm then .exibly 
allocates the required number of clusters; thus clusters are not wasted where they are not needed. First,every 
soundsourcewhichhasnotbeenmasked [Tsingosetal. 2004], is put in one cluster which is then recursively 
split into two until an appropriate condition is met. In every recursion step the error in angle relative 
to the listener position is computed, for each sound source in a cluster, relative to its centroid. If 
the average angle error is below a threshold, cluster splitting is terminated. In our tests, we found 
that a 25. threshold value proved satisfactory. 3.3 Quantitative error/performance comparison Wehaveperformedaquantitativecomparison 
betweentheprevious clustering approach of [Tsingos et al. 2004], and the two proposed recursive clustering 
methods. We ran several thousand tests with random con.gurations of 800 sound sources, using the different 
al­gorithms and measuring the time and the error (in terms of distance and angle) for each method. Figure3shows 
the results of the tests for .xedbudgets of 12 and 6 clusters using different two-level subdivision strategies. 
For in­stance, line3/4 correspondstoa clustering using12 clusters(3 top­level clusters recursively re.ned 
into 4). The line 12/1 corresponds Figure 3: Benchmarks for hierarchical clustering of 800 sources using 
different6 and12 cluster con.gurations. We displayaver­age clustering error (also denoted by circle size). 
Note the signif­icant speed-up compared to the non-hierarchical algorithm for the 12 cluster con.gurations 
(in red) while the errors remain similar. to the previous clustering approach of [Tsingos et al. 2004] 
where all 12 clusters are top level. As we can see, the performance of the recursive approaches are clearly 
better than the direct algorithm.For the same .nalbudget, the 3/4 and 4/3 con.gurations appear to be 
better choices in terms of speed. As expected the error is larger for hierarchical clustering since it 
leads to less optimal cluster placement. However, as the number of clusters grows this effect tends to 
disappear. The vari­able cluster methodisfasteronaverage.However,withour current settings it also created 
fewer clusters (6.6 clusters created on aver­age) and, asa consequence, hashigheraverage error. Interestingly, 
the peak number of clusters created by the variable method is 22, which underlines the .exibility and 
adaptability of the approach. To limit the error compared to a non-hierarchical implementation, it is 
preferable to create more top level clusters. For instance a 4/3 splitisbetterthana2/6splitfora12 cluster 
con.guration,altoughit might be slightly slower. Hence, this choice depends on the desired time vs. error 
tradeofffor the application athand. 3.4 Implementation in a commercial game The audio clustering technique 
was used in the development of the commercially available computer game Test Drive Unlimited. In this 
car racinggame, the soundof each racingvehiculeis synthe­sized based on numerous mechanical quantities. 
The sound emitted by each wheelis controlledby20physicalvariables while8 vari­ables controlthe engine/transmission 
sounds.Four additionalvari­Figure 4: Sound source clustering in the Test Drive Unlimited en­gine. Red 
wireframe spheres are clusters. c  .Eden Games-ATARI 2006. ables control aerodynamic phenomena. These 
variables are used for real-time control and playback of a set of pre-recorded sound samples. All sound 
sources are then rendered in 5.1 or 7.1 sur­round sound. For implementationon the XBOX360, Eden Games 
adopted a variant of the recursive variable budget technique de­scribedabove.In particular,abudgetof8clusterswasusedateach 
recursionlevel. If the quality criterionis notmet for thisbudget,a local clustering step is applied in 
each cluster. However, the local nature of clustering resulted in audible artifacts from one frame to 
thenext, becauseof sourcesmovingfromone clusterto another.To resolvethis problem, sources are orderedbyperceptual 
priority,and the most important ones will prefer to be clustered with the cluster of the previous frame, 
effecting a form of temporal coherence. Despite this improvement, extreme cases still presented some 
dif­.culty, notably the case of a car crashing into an obstacle. In this case, the physics engine generates 
numerous short-lasting sound sources in the immediate neighbourhood of the vehicle. Temporal coherence 
is thus useless in this context. The solution to this issue is to apply a separate clustering step to 
the sources generated by physicsevents;this resultsin more clustersoverall,but resolvesthe problems of 
quality. Asnapshot ofTest Drive Unlimited withavisualisationofthe sound source clusters superimposed 
in red, is shown in Figure 4. 4 Scalable perceptual premixing In order to apply the .nal spatial audio 
processing to each cluster (see Figure 2), the signals corresponding to each source must .rst be premixed. 
The premixing stage can be as simple as summing­up the signals of the different sources in each cluster. 
In addition, a number of audio effects usually have to be applied on a per­source basis. Such effects 
include .ltering (e.g., distance, occlu­sions), pitch-shifting (e.g., Doppler effect) or other studio-like 
ef­fects [Z ¨ olzer 2002]. Hence, when large numbers of sound sources are still present in the scene 
after auditory culling (masking) or for systems with limited processing power, the cost of this stage 
can quickly become the bottleneck of the audio rendering pipeline. In this section, we propose a progressive 
signal processing tech­nique that can be used to implement scalable per-source process­ing. Our work 
is based on the approach of [Tsingos 2005] which we brie.y summarize in Figure 5. This approach uses 
a speci.c time-frequency representation of audio signals. At each time-frame (typically 1024 samples 
at 44.1KHz), the complex-valued coef.cientsofa short-timeFourier transform (STFT) are precomputed and 
stored in decreasing mod­ulus order. In real-time during the simulation, the algorithm pri­oritizes the 
signals and allocates to each source a number of co­ef.cients to process, so that a prede.ned budget 
of operations is Figure 6: Reconstruction error as a function of target FFT bins. Tonal signals require 
fewer coef.cient than noisier signals since theirFourier representationismuch sparser.Thevaluenexttoeach 
curve corresponds to our pinnacle value. In [Tsingos 2005], the integral of these curves is used to measure 
the coding ef.ciencyfor each frame of input signal.  respected. In [Tsingos 2005], this importance sampling 
stage is driven by the energy of each source at each time-frame and used to determine the cut-off point 
in the list of STFT coef.cients. However, using only energy for importance sampling leads to sub­optimal 
results since it does not account for the sparseness of the representation obtained for each signal. 
For instance, a loud tonal signal might require fewer coef.cients than a weaker noisier signal for transparent 
reconstruction (Figure 6). An additional weighting term measuring the ef.ciency of coding for each frame 
of input signalwas thus proposed forbudget allocation. In the following, we introduce an improvedbudget 
allocation strat­egy.Wealso presentthe resultsofa perceptualqualitystudyaimed at evaluating our novel 
technique and several possible importance metrics used for prioritizing source signals. 4.1 Improved 
budget allocation The pinnacle value. Contrary to [Tsingos 2005], our improved budget allocation pre-computes 
the explicit number of STFT coef­.cients necessary to transparently reconstruct the original signal. 
This value, that we call pinnacle, is pre-computed for each time­frame of input audio data and stored 
together with the progressive STFT representation. To compute the pinnacle value, we .rst sort the STFT 
coef.cients by decreasingmodulus order. The energy of each coef.cient is integrated until a threshold 
of at least 99.5% of the total energy of the frame is reached and the number of corre­sponding coef.cients 
is greater than (1- tonality)N/2, where N is the total number of complex Fourier coef.cients in the frame 
and tonality . [0, 1] is the tonality index of the frame [Painter and Spanias 2000;Kurniawati et al. 
2002]. This index is close to1for tonal signals and drops to0for noisier signals.  Iterative importance 
sampling. We assume a constant number of arithmetic operations will be required for each complex STFT 
co­ef.cient. Hence, .tting a budget number of operations for our pipeline at each processing frame directly 
amounts to selecting a budget number of coef.cients for each frame of input sound sig­nal. We can take 
advantage of pre-storing our FFT in decreasing energy orderbydirectly processing the ni .rst coef.cients 
for each input signal si, so that the sum of all nis does not exceed our to­talbudget N. To determine 
thenis, we .rst assign an importance valuetoeachsignal.This importancevaluecan typicallybetheen­ergy 
or loudness of the signal as proposed in [Tsingos et al. 2004; Tsingos 2005]. In this work, we also experimented 
with a saliency value derived from the model recently proposed in [Kayser et al. 2005]. This model is 
very similar to the visual saliencymaps [Itti etal.1998]butitis appliedona time-frequencydomain represen­tation 
of audio signals. In our case, after computing the auditory saliencymap,weintegrated saliencyvaluesovera 
small numberof frequencysubbands(we typically use4 ona non-linear frequency scale). Then, every input 
signal gets assigned a number of bins ni relative to its relative importance as follows: ni = Ii/.Ii 
· targetCoeffs (1) i where Ii is the importance of the source i. Ideally, ni should be smallerthanthesignal 
spinnaclevaluetoavoid suboptimalbudget allocation as can be the case with the approach of [Tsingos 2005]. 
To avoid such situations, all remaining coef.cients above pinnacle threshold are re-assigned to the remaining 
signals, that do not al­ready satisfy the pinnacle value criterion. This allocation is again relative 
to the importance values of the signals: ni+= Ii/ .Ii · extraCoeffs (2) i The relativeimportance of 
each remaining signal is updated accord­ingtothe reallocationofcoef.cients.Ifthebudgetishighenough, the 
process is iterated until all signals satisfy the pinnacle criteria or receive a maximal number of coef.cients. 
4.2 Quality evaluation study To evaluate possible importance metrics and evaluate our pinnacle­based 
algorithm we conducted a quality evaluation study. Experimental procedure. Seven subjects aged from 23 
to 40 and reporting normal hearingvolunteeredfor.vedifferent test sessions. For each session, we used 
a Multiple Stimuli with Hidden Refer­ence and Anchors procedure (MUSHRA, ITU-R BS.1534) [Stoll andKozamernik 
2000; EBU2003; InternationalTelecom. Union 2001-2003]. Subjects were asked to simultaneously rank a total 
of 15 stimuli relativetoa reference stimulusona continuous0to100 quality scale. The highest score corresponds 
to a signal indistin­guishable from the reference. The reference stimuli were different mixtures of ambient, 
music and speech signals. In all cases, 12 of the 15 test-stimuli consisted of degraded versions of the 
mixture computed using our progressive mixing algorithm atvariousbud­gets (5%, 10% and 25% for music 
and ambient and 2%, 5% and 10% for speech), using our pinnacle-based technique, not using the pinnacle 
and using either loudness or saliency-based prioritization. In all cases, our processing is done using 
32-bit .oating point arith­metic and reconstructs signals at 44.1KHz. Two anchor stimuli, providing reference 
degradations, were also included. In our case, we chose a downsampled 16KHz/16-bit version of the mixture 
and a mp3-encoded version at 64Kbps. Finally, a hidden reference was also included. Stimuli were presented 
over headphones. Subjects could switch between stimuli at any point while listening. They could also 
de.ne looping regions to concentrate on speci.c parts of the stimuli. A volume adjustment slider was 
provided so that subjects could select a comfortable listening level. Results. Our studycon.rmed thatthe 
scalable processingapproach is capable of generatinghigh quality results using 25% of the origi­nal audio 
data and produces acceptable results withbudgets aslow as 10%. In the case of speech signals, for which 
the STFT represen­tation is sparser,the algorithm could generate an acceptable mixture (avg. score 56/100) 
with only 2% of the original coef.cients. As canbe seenonFigure7(left),our approachyieldssigni.cantlybet­ter 
results thana16KHz reference signal (16KHz processingwould correspond to a 30% reduction of data compared 
to our 44.1KHz processing). At 25% budget (or 10% in the case of speech), we obtain results comparable 
or better than the 64Kbps mp3-encoded reference. We performed an analysis of variance (ANOVA) [How­ell 
1992] on the results. As expected, the analysis con.rmed a sig­ni.canteffectof the computingbudget(p<0.01) 
on the quality of the resulting signal (Figure 7 right). We can see that the varia­tionof perceived qualtyis 
not generallya linear functionofbud­get, especially for more tonal signals (music, speech) which can 
be ef.ciently encoded until a sharp breakdown point is reached. In­teraction betweenbudgetand importance 
metricwas foundtobe signi.cant (0.05<p<0.01). At low or highbudgets, the two met­rics lead tovery similar 
results. However, for intermediatebudgets, loudness-based prioritization improved perceived quality relativeto 
the saliency-based alternative. Similarly,using our newpinnacle al­gorithm also leads to a slight improvement 
in the results, especially for cases where both tonal and noisier signals are present. Nois­ier stationary 
sounds, which do not contain strong spectral features, usually receive a lower saliency value although 
theymight contain signi.cant energy and require more coef.cients to be properly re­constructed.Webelievethismightexplainwhysaliency-basedpri­oritization 
led to lower perceived quality in some cases. 5 Cross-modal e.ects for sound scene sim­pli.cation In 
the preceding sections, we have improved different aspects of audio rendering for complex scenes, without 
consideration for the corresponding visuals. Intuitively, it would seem that such inter­action of visual 
and audio rendering should be taken into account, and play a role in the choice of metrics used in the 
audio clustering algorithm. A.rst attempt was presented in [Tsingos et al. 2004], butwas inconclusive 
presumablyduetothedif.cultieswith speech stimuli, which are generally considered to be a special case. 
Research in ventriloquism (see Section 2), could imply that we should be more tolerant to localization 
errors for sound render­ing when we have accompanying visuals. If this were the case, we could change 
the weighting terms in the clustering algorithm to create fewer clusters for sound sources in the visible 
frustum. How­ever, a counter argument would be that in the presence of visuals,  Figure 7: Left: Average 
MUSHRA scores and 95% con.dence intervals for our progressive processing tests. Right: Average MUSHRA 
scoresand95% con.denceintervalsasa functionofbudget.Notehowperceivedqualitydoesnotvarylinearlywiththe 
processingbudget and also varies depending on the type (i.e., sparseness) of the sounds. we are more 
sensitive to localization, and we shouldfavour more clusters in the viewing frustum. Our goal was to 
see whether we could provide some insight into this question with a pilot perceptual study. The next 
step was to develop and test an improved audio clustering algorithm based on the indications obtained 
experimentally. 5.1 Experimental setup and methodology We chose the following experimental setup to provide 
some insight on whether we need more clusters in the visible frustum or not. The subjects are presented 
with a scene composed of 10 animated ­but not moving -objects emitting ecologically valid sounds, i.e., 
a moo-ing sound for the cow, a helicopter sound, etc. (Figure 8; also see and hear accompanying video). 
We have two main conditions: audio only (i.e., no visuals) (condi­tion A)and audio-visual(AV).Within 
each main condition wehave a control condition, in which sources follow a uniform angular dis­tribution, 
and the condition we test, where the proportion of clusters in the visible frustum and outside the visible 
frustum is varied. We ran our test with6subjects (male, aged 23-45, with normal or corrected to normal 
vision, reporting normal hearing). All were naive about the experiment. Five of them had no experience 
in audio. Prior to the test, subjects were familiarized with isolated sound effects and their corresponding 
visual representation. The subject stands1meteraway froma136x102 cm screen (Barco BaronWorkbench), withan 
optical headtrackingdevice(ART)and active stereo glasses (see the video). The .eld ofview in this large 
screen experiment is approximately 70 . . Headphones are used for audio output and our system uses binaural 
rendering [Blauert 1997; Møller 1992] using the LISTEN HRTF database (http://recherche.ircam.fr/equipes/salles/listen/). 
Our sub­ jects were not part of the database. Hence, theyperformed a point and click pre-testto selectthebestHRTFsoverasubsetof6HRTF 
selected to be most representative similar to [Sarlat et al. 2006]. The marks attributed for the test 
are given with a joystick. The A condition was presented .rst for three candidates, while AV condition 
was presented .rst for the other three. No signi.cant ef­fect of ordering was observed. To achieve the 
desired effect, objects are placed in a circle around the observer; 5 are placed in the viewing frustum 
and 5 outside. For both control and main conditions, four con.gurations are used Figure8:Anexampleviewoftheexperimental 
setupforthe audio­visual pilot user study. randomly, by varying the proportion of clusters. Condition 
1/4 has one clusterintheview frustumand4outside,2/3,has2intheview frustumand3outside, etc. Auniform distributionof 
clusters cor­respondsto condition1/4,withonly1clusterinthe frustum. Each condition is repeated 15 times 
with randomized object positions; these repetitions are randomized to avoid ordering effects. We used 
the ITU-recommendedtriple stimulus, double blind with hidden reference technique [and 1993; ITU-R 1994]:2 
versionsof the scene were presented( A and B ) andagiven reference scene which corresponds to unclustered 
sound rendering. One of the 2 scenes was always the same as the reference (a hidden reference) and the 
other one corresponds to one of our clustering con.gura­tions. For each condition, the subject was presented 
with a screen with three rectangles ( A , R and B ), shown in Fig. 8. The subjects weregivenagamepad, 
and were instructed to switch be­tween A , B and R using threebuttons on the pad, which were highlighted 
depending on the version being rendered. The subjects were asked to compare the quality of the approximations 
( A or B ) compared to the reference. They were asked to perform a quality judgment paying particular 
attention to the localization of sounds forthe2test scenes,and instructedto attribute oneof4lev­els of 
evaluation No difference , Slightly different , Different and Clearly different from the reference, which 
were indicated in rectanglesnexttothe letter indicatingthe sceneversion(seeFig.8 and accompanying video). 
5.2 Analysis and results We attributeda mark for eachevaluation (from0 to3). As sug­gestedbythis ITU-R 
standard protocol, we onlykept the difference between the test sample and the hidden reference.We also 
normal­Figure9: Meanvaluesand95% con.dence intervals(N=6)in A and AV conditionsasa functionofthe numberof 
clusters inside/outside the viewfrustrum.For AV,the2/3 con.gurationgivesthebestqual­ity scores, which 
is not the case in the A condition. The * under­lines that quality judgements in 1/4 and 2/3 cluster 
con.gurations for AV are signi.cantly different(p<0.05), while the same compar­ison is non signi.cant 
(n.s.) in the A condition.  ized the data by dividing each mark by the mean score of the user (the 
average of all marks of the candidate over all his tests). There was no signi.cant difference between 
the A and AV condi­tions regarding the respective scores of each cluster con.guration. However, the difference 
of quality ratings between con.gurations was not similar in the two conditions. In condition A, 1/4 and 
2/3 con.gurations lead to a similar quality evaluation (see Figure 9). In condition AV, the best quality 
is perceived in con.guration 2/3. While 2/3 and 1/4 con.gurations are not perceived differently in condition 
A (Wilcoxon test, N=90, T=640.5, Z=0.21, p=0,83), the quality scoresof2/3 con.guration arehigher than 
thoseof1/4 con­.guration in condition AV (Wilcoxon test, N=90, T=306.5, Z=2.56, p=0.01). Overall, we 
consider the above results as a signi.cant indication that, when we use the audio clustering algorithm 
with visual repre­sentation of the sound sources, it is better to havetwoclusters in the view frustrum, 
compared to a uniform angular distribution. This is indicated by the results for the 2/3 con.guration, 
which is statisti­callydifferentfromalltheother con.gurationsintheAVcondition. We expect this effect 
to be particularly true for scenes where there are visible sound sources in the periphery ofthe view 
frustum. 5.3 An audio-visual metric for clustering Given the above observation, we developed a new weight 
in the clustering metric which encourages more clusters in the view frus­tum. We modify the cost-function 
of the clustering algorithm by adding the following weigthing term: ..n cos .s - cos . f 1+ a (3) 1- 
cos . f where .s is the angle between the view direction and the direc­tion of the sound source relative 
to the observer, .f is the angular half-width of the view frustum and a controls the amplitude and n 
decay-rateof this visual improvementfactor. 6 Implementation and Results We ran tests on two scenes, 
one is a variant of the highway scene from [Tsingos et al. 2004], and another is a city scene. Both scenes 
Figure 10: Left: the clusters without the audio-visual metric. Right: the clusters with our newmetric.Weclearly 
see that the newmetric separates the sources appropriately. are shown in Figure 1. In both cases, we 
used a platform with dual-core3GHz Xeon processor and NVidia 7950GX2 graphics ac­celerator;oursystemisbuiltusingtheOgre3D 
graphicsengineand our custom audio library. Audio was processed at 44.1KHz using 1024-sample-long time-frames 
(i.e., 23 msec.). The following tests were performed with masking disabled to get a stable performance 
measure. The highway scene contains 1004 sound sources, which are car engine and car stereo music sounds, 
cow mooing sound, train sounds,andwater soundsinastream.We foundthatascalablepre­mixbudgetof25%is satisfactoryin 
termsof audio quality (please hear and see the accompanying video). Comparing to the refer­ence, we found 
that our entire perceptual processing pipeline re­sultedinanaverage signal-to-interference ratioof18dB 
(min=5dB, max=30dB) for the sequence presented in the video. In this scene, clustering took 4.7 msec. 
per frame. Premixing was very simple and only included distance attenuation and accumulating source signals 
for each cluster. Premixing using 100% of the original audio data took6 msec. Using our scalable processing 
with 25%budget we bring this cost down to 1.83 msec. The street scene contains 1800 sound sources, which 
are footstep sounds and voices for the people in the crowd, car engine sounds, car radio sounds, bird 
sounds and sirens. Again, a scalable pre­mixbudgetof 15%is satisfactory for this scene. Overall, we mea­sured 
an average signal-to-interference ratio of 17dB (min=4dB, max=34dB) for the sequence presented in the 
video. In this scene, clustering took 5.46 msec. per frame while premixing using 100% of the original 
audio data took 6.84 msec. Using our scalable pro­cessing with 15%budget we bring the cost of premixing 
down to 2.17 msec. In the commercialgame Test Drive Unlimited, the average number of simultaneous sound 
sources is 130. Atypical player car can generate up to 75 sound sources, while AI cars have a simpli.ed 
model of maximum 42 sources. A maximum of 32 clusters were usedinthegame, althoughinavast majorityof 
cases10clusters are suf.cient. Overall, the clustering approach discussed in Section 3.4 resultsinareductionof 
50-60%ofCPU usageforthe audio engine, whichisfreedforother applicationtasks,suchasAI,gameplayetc. To 
test the new audio-visual criterion, we constructed a variant of the street scene and an appropriate 
path, in which the positive effect of this criterion is clearly audible. For this test, we used a = 10 
and n = 1.5, which proved to be satisfactory. The scene used can be seen and heard in the video; the 
user follows a path in the scene (seeaccompanyingvideo)andstopsinagivenlocationinthe scene. We have 132sources 
in the scene and targetbudget of8clusters. By switching between the reference, and the approximations 
with and without the audio-visual metric, we can clearly hear the im­provement when more clusters are 
used in the view frustum. In particular, the car on the right has a siren whose sound is audibly displaced 
towards the centre with the audio-only metric.  7 Discussion and Conclusion In this paper, we proposedafast 
hierarchical clustering approach that can handle large numbers of sources and clusters. We also proposed 
a progressive processing pipeline for per-source effects (i.e., the premixing) that allows us to choose 
the best perfor­mance/quality ratio depending on the application and hardware constraints. Combined with 
auditory masking evaluation, these new algorithms allow for real-time rendering of thousands of mo­bile 
sound sources while .nely controlling processing load vs. quality. In fact, while designing the test 
examples for the paper, the major problem wefacedwas authoring environments complex enough and most of 
the performance limitations actually came from the graphics engine. However, with next-generationgames 
mak­ing increased used of procedurally-generated audio (e.g., based on physics engines), scenes with 
thousands of sound events to process are likely to become common in the near future. In our examples 
weonlyusedsimpleper-source processing.However,inmostgam­ing applications each source is likely to be 
processed with a chain of various effects (e.g., occlusion .lters, echoes, re-timing, pitch­shifting, 
etc.) that would make our scalable approach even more attractive. We also presented our perceptual studies 
for clustering and scal­able premixing. A cross-modal perceptual study aimed at deter­mining possible 
in.uence of the visuals on the required quality for audio clustering. Although one couldexpectventriloquismto 
allow for rendering simpli.cations for visible sources, our study suggest that more clusters might actually 
be required in this case. Apos­sible explanation for this is that, in a complex scene, clustering is 
likely to simplify auditory localization cues beyond common ven­triloquism thresholds. Asaconsequence, 
we introducedanewmet­ric to augment the importance of sources inside the view frustum. We demonstrated 
an example where, with a large number of sound sources outside the view frustum, it leads to improved 
results. We also performed a user-study of quality for the scalable premixing approachandshowedthatit 
leadstohigh quality resultswithbud­gets as low as 20 to 15% of the original input audio data. Although 
saliency-based importance appeared to show some limitations for our scalable processing algorithm compared 
to loudness, it might still be useful for prioritizing sources for clustering. In the future, it would 
be interesting to experiment with auditory saliencymetrics to drive clustering and evaluate our algorithms 
on various combinations of A/V displays (e.g., 5.1 surround or WFS setups). Also, the in.uence of ventriloquism 
on these algorithms merits further study. We also believe that authoring is now be­coming a fundamental 
problem. Adapting our algorithms to handle combinations of sample-based and procedurally synthesized 
sounds seems a promising area of future research. 8 Acknowledgements This research was funded by the 
EU IST FET Open project IST-014891-2CROSSMOD (http://www.crossmod.org).We thank Au­ todeskforthe donationofMaya,P. 
RichardandA.Olivier-Mangon for modelling/animation and G. Lemaitre for help with ANOVA. References ALAIS,D., 
AND BURR,D. 2004. Theventriloquismeffect results from near-optimal bimodal integration. Current Biology 
14, 257 262. AND, C. G. 1993. Methods for quality assessment of low bit-rate audio codecs, proceedings 
of the 12th aes conference. 97 107. BERKHOUT, A., DE VRIES, D., AND VOGEL, P. 1993. Acoustic control 
by wave .eld synthesis. J. of the Acoustical Society of America 93,5(may), 2764 2778. BLAUERT,J. 1997. 
Spatial Hearing : The Psychophysics of Human Sound Localiza­tion. M.I.T. Press, Cambridge, MA. CHEN, 
J., VEEN, B. V., AND HECOX, K. 1995. A spatial feature extraction and regularization model for the head-related 
transfer function. J. of the Acoustical Society of America 97 (Jan.), 439 452. DARLINGTON, D., DAUDET, 
L., AND SANDLER, M. 2002. Digital audio effects in the wavelet domain. In Proceedings of COST-G6 Conference 
on Digital Audio Effects, DAFX2002, Hamburg, Germany. 2003. EBU subjective listening tests on low-bitrate 
audio codecs. Technical report 3296, European Broadcast Union (EBU), Projet Group B/AIM (june). FOUAD,H.,HAHN,J., 
AND BALLAS,J. 1997. Perceptually based scheduling algo­rithms for real-time synthesis of complex sonic 
environments. proceedings of the 1997 International Conference on Auditory Display (ICAD 97),. GALLO,E.,LEMAITRE,G., 
AND TSINGOS,N. 2005. Prioritizing signals for selec­tive real-time audio processing. In Proc. of ICAD 
2005. HAIRSTON, W., WALLACE, M., AMD B.E. STEIN, J. V., NORRIS, J., AND SCHIR-ILLO,J. 2003.Visual localization 
ability in.uences cross-modal bias. J. Cogn. Neuroscience 15, 20 29. HERDER,J. 1999. Optimizationof sound 
spatialization resource management through clustering. The Journal of Three Dimensional Images, 3D-Forum 
Society 13,3 (Sept.), 59 65. HOCHBAUM, D. S., AND SCHMOYS, D. B. 1985. Abest possible heuristic for the 
ik-center problem. Mathematics of Operations Research 10,2(May), 180 184. HOWELL,D.C. 1992. Statistical 
methods for psychology. PWS-Kent. INTERNATIONAL TELECOM. UNION. 2001-2003. Method for the subjective 
assess­ment of intermediate quality level of coding systems. Recommendation ITU-R BS.1534-1. ITTI,L.,KOCH,C., 
AND NIEBUR,E. 1998.Amodelof saliency-based visual atten­tion for rapid scene analysis. IEEE Transactions 
on Pattern Analysis and Machine Intelligence 20, 11 (Nov.), 1254 1259. ITU-R. 1994. Methods for subjective 
assessment of small impairments in audio systems including multichannel sound systems. itu-r bs 1116. 
Tech. rep. JOT,J.-M., AND WALSH,M. 2006. Binaural simulationof complex acoustic scenes for interactive 
audio. In 121th AES Convention, San Francisco, USA. Preprint 6950. JOT,J.-M.,LARCHER,V., AND PERNAUX,J.-M. 
1999.Acomparative studyof3D audio encoding and rendering techniques. Proceedings of the AES 16th interna­tional 
conference, Spatial sound reproduction, Rovaniemi, Finland (April). KAYSER,C.,PETKOV,C.,LIPPERT,M., AND 
LOGOTHETIS,N. 2005. Mechanisms for allocating auditory attention: An auditory saliencymap. Current Biology 
15 (Nov.), 1943 1947. KELLY, M., AND TEW, A. 2002. The continuity illusion in virtual auditory space. 
proc. of the 112th AES Conv., Munich, Germany (May). KURNIAWATI, E., ABSAR, J., GEORGE, S., LAU, C. T., 
AND PREMKUMAR, B. 2002. The signi.cance of tonality index and nonlinear psychoacoustics models for masking 
threshold estimation. In Proceedings of the International Conference on Virtual, Synthetic and Entertainment 
Audio AES22. LANCIANI,C.A., AND SCHAFER,R.W. 1997. Psychoacoustically-based processing of MPEG-I layer 
1-2 encoded signals. In Proc. IEEE Signal Processing Society 1997 Workshop on Multimedia Signal Processing, 
53 58. LANCIANI,C.A., AND SCHAFER,R.W. 1999. Subband-domain .lteringof MPEG audio signals. In Proceedings 
of Intl. Conf. on Acoustics, Speech and Signal Pro­cessing, 917 920. LARCHER,V.,JOT,J.,GUYARD,G., AND 
WARUSFEL,O. 2000. Study and compar­isonofef.cient methods for3d audio spatialization based onlinear decomposition 
of HRTF data. Proc. 108th Audio Engineering Society Convention. LEWALD, J., EHRENSTEIN, W. H., AND GUSKI, 
R. 2001. Spatio-temporal con­straints for auditory-visual integration. Beh. Brain Research 121, 1-2, 
69 79. MALHAM, D., AND MYATT, A. 1995. 3D sound spatialization using ambisonic techniques. Computer Music 
Journal 19, 4, 58 70. MØLLER, H. 1992. Fundamentals of binaural technology. Applied Acoustics 36, 171 
218. PAINTER, E. M., AND SPANIAS, A. S. 2000. Perceptual coding of digital audio. Proceedings of the 
IEEE 88,4(Apr.). SARLAT,L.,WARUSFEL,O., AND VIAUD-DELMON,I. 2006.Ventriloquism after­effects occur in 
the rear hemisphere. Neuroscience Letters 404, 324 329. STOLL,G., AND KOZAMERNIK,F. 2000. EBUsubjective 
listening tests on internet audio codecs. EBU TECHNICAL REVIEW, (June). TOUIMI,A.B.,EMERIT,M., AND PERNAUX,J.-M. 
2004. Ef.cient method for multiple compressed audio streams spatialization. In In Proceeding of ACM 3rd 
Intl. Conf. on Mobile and Ubiquitous multimedia. TOUIMI, A. B. 2000. A generic framework for .ltering 
in subband domain. In In Proc. of IEEE 9th Wkshp. on Digital Signal Processing, Hunt, Texas, USA. TSINGOS, 
N., GALLO, E., AND DRETTAKIS, G. 2004. Perceptual audio rendering of complex virtual environments. Proc. 
SIGGRAPH 04 (August). TSINGOS,N. 2005. Scalable perceptualmixingand.lteringofaudiosignalsusingan augmented 
spectral representation. Proc. of 8th Intl. Conf. on Digital Audio Effects (DAFX 05), Madrid, Spain (Sept.). 
WAND, M., AND STRASSER, W. 2004. Multi-resolution sound rendering. In Symp. Point-Based Graphics. Z¨ 
OLZER,U., Ed. 2002. DAFX -Digital Audio Effects. Wiley.  Proc. of the EAA Symposium on Auralization, 
Espoo, Finland, 15-17 June 2009 USING PROGRAMMABLE GRAPHICS HARDWARE FOR AURALIZATION Nicolas Tsingos 
Sound Technology Research Dolby Laboratories San Francisco, USA nicolas.tsingos@dolby.com ABSTRACT 
Over the last 10 years, the architecture of graphics accele rators (GPUs) has dramatically evolved, outpacing 
traditional gen­eral purpose processors (CPUs) with an average 2.25-fold increase in performance every 
year. With massive processing capabilities and high-level programmability, current GPUs can be leveraged 
for applications far beyond visual rendering. In this paper, we offer an overview of modern programmable 
GPUs and how they can be applied to audio rendering. For ap­plications ranging from sound synthesis and 
audio signal process­ing to numerical acoustic simulations, GPUs massive parallelism and dedicated instructions 
can offer a 5 to 100-fold performance improvement over traditional CPU implementations. We will il­lustrate 
such bene.ts with results from 3D audio processing and sound scattering simulations and discuss future 
opportunities for auralization on massively multicore processors. 1. INTRODUCTION Driven by an increasing 
consumer demand for high quality in­teractive visuals, 3D graphics processors (GPUs) have dramati­cally 
evolved in the past decade. GPUs have moved from a spe­cialized and pricy component only available on 
high-end profes­sional workstations to a commodity component available on ev­ery consumer PC, reaching 
a market of hundreds of million units per year. At the same time, thanks to advances in manufactur­ing 
technology, GPUs evolved from ASICs implementing limited .xed-function processing to fully programmable, 
massively par­allel processors capable of handling complex data structures. As can be seen in Figure 
1, the raw computational power of current GPUs largely exceeds that of the most powerful general purpose 
processors (CPU) [1]. Arguably, GPUs are the .rst truly success­ ful parallel processors. As a result 
of this widespread availability and custom pro­gramming capabilities, modern GPUs have generated a lot 
of ded­icated algorithmic research in the graphics community but also in many other areas in need of 
massive parallel performance. In to­day s games, GPUs are used not only to generate photorealistic 3D 
visuals but also accelerate visibility queries, character animation, path planning, arti.cial intelligence 
and physics calculations, in­cluding rigid and deformable body dynamics or .uid solvers. For a general 
overview of general purpose GPU (GP-GPU) applications, we refer the reader to the following references 
[2, 3, 4, 5, 6, 7]. Audio processing and rendering applications are among the most compute-intensive 
and often rely on additional DSP resources for real-time performance. However, programmable audio DSPs 
are often only available to product developers while consumer au­dio hardware (e.g., Creative Labs SoundBlaster) 
generally im-Figure 1: Evolution of GPU performance, memory bandwidth and commoditization. Courtesy of 
Pat Hanrahan and David Luebke. plements .xed-function pipelines, which traditionally evolved at a rather 
slow pace. Increasingly, commodity audio hardware tends to disappear from console or PC architectures 
and audio proces sing tasks are left to the multiple available CPU cores. GPU fea­tures, such as multiply-accumulate 
instructions or multiple SIMD execution units, are similar to those of most DSPs [8, 9]. More­ over, 
3D audio rendering and auralization [10, 11] not only require processing audio signals but also a signi.cant 
number of geomet­ric calculations, e.g. to determine sound occlusion or re.ections, which are well suited 
to graphics architectures. As a result, GPUs are a compelling alternative to traditional DSPs for auralization. 
In this paper, we review a number of recent contributions lever­aging GPUs to accelerate audio rendering 
tasks ranging from au­dio signal processing and sound synthesis to numerical simulations of room acoustics 
and surface scattering. In Section 2, we give a short overview of the architecture and programming models 
of current GPUs, emphasizing some key differences between GPU and CPUs. To better highlight the architecture 
and programming model of GPUs and how they interact with the processing tasks, we chose to organize the 
remainder of the paper according to the different problems handled by auralization systems, which we 
can roughly break into three categories: Audio signal processing: in this .rst category the GPU pro­cesses 
an audio stream at typical audio sampling rates, in time or frequency domain, to perform a variety of 
process­ing, such as .ltering, delay lines, synthesis, etc. We cover this application in Section 3. 
 Numerical calculations and linear algebra: in this case, the GPU is used for numerical acoustics simulations, 
for in­  1  Proc. of the EAA Symposium on Auralization, Espoo, Finland, 15-17 June 2009 stance to solve 
large .nite elements or .nite differences problems. We review applications to room acoustics in Sec­tion 
4. Geometry processing: in this .nal category, the GPU is used to process geometric primitives, such 
as polygons. This is the area where the highest gains can be expected com­pared to traditional CPU implementations 
since the calcu­lations are closer to the original intent of graphics archi­tectures. We review applications 
to acoustical ray-tracing, occlusion and scattering in Section 5. Despite a number of successful implementations 
and applications, audio processing is not generally becoming a key component of­.oaded to the GPU. In 
section 6, we offer possible insights as to why this may be and what limits the use of GPUs for audio 
pro­cessing. Finally, the last couple of years have seen a convergence of CPU and GPU architectures with 
CPUs becoming more paral­lel and data paths between CPU and GPU becoming increasingly faster. In section 
7, we discuss possible evolutions of integrated CPU and GPU programming and how this can bene.t a variety 
of audio applications for auralization and beyond. 2. A QUICK TOUR INSIDE THE GPU In this .rst section, 
we give a quick overview of the architecture of graphics accelerators and their recent evolution, focusing 
on the key differences with CPUs. A large body of work is available in the graphics community about GPU 
architecture and for additional detail we refer the reader to [12, 13]. 2.1. The graphics pipeline and 
GPU architecture Graphics hardware has a speci.c data.ow computational model. Its architecture is originally 
targeted at manipulating 3D primitives such as points, lines or polygons, performing some raster graphics 
operations and rendering the result to the screen. Primitives fol­low a sequence of operations before 
being processed to the screen. Figure 2 shows the essential steps of the pipeline. Figure 2: High-level 
overview of the graphics pipeline. In mod­ern GPUs both the geometry and rasterization step contain user­programmable 
components called shaders. The application transmits vertices of the primitives to the ge­ometry stage. 
The vertex is a structure containing 3D position and texture coordinates, color and normal vector. The 
vertex pro­cessor applies any mathematical operation to each vertex includ­ing transformations from world 
space to screen space (i.e., camera projection). In the second step, vertices are assembled to form the 
geometric primitives (generally triangles). In this step, culling is computed to discard invisible surfaces 
according to their normal and the view direction. Next, clipping to the view frustum is ap­plied before 
the rasterization. The view frustum is a set of planes which de.nes the .eld of view of the camera. The 
second step of the pipeline rasterizes the transformed prim­itives. Rasterization determines which fragment 
of the screen buffer is covered by a primitive. The term fragment is employed instead of pixel since 
additional operations are still required to determine its .nal color. A fragment also carries additional 
characteristics such as color, normals or textures coordinates. These characteris­tics are interpolated 
for all fragments across the transformed ver­tices of the primitive. At this stage additional operations 
can be done to determine the .nal color of the fragment, e.g. local illumi­nation models, cast shadows, 
etc. On recent hardware the fragment shading stage is fully programmable. Textures can also be applied 
to the fragments by sampling speci.c image buffers at non-integer coordinates. Several texture re-sampling 
schemes are provided by the hardware such as linear or bi-linear interpolation but any other scheme can 
be applied via the programming features of more re­cent GPUs. Finally, the aim of the compositing stage 
is to perform ad­ditional tests before writing the .nal fragment values (i.e.,color) to the corresponding 
pixels of the frame buffer. Tests includes depth testing, used to remove hidden pixels and alpha test 
to han­dle transparency blending. 2.2. From .xed shading to general programming Before the year 2000, 
most GPUs were .xed function and only available in high-end workstations (e.g., Silicon Graphics) for 
pro­duction or CAD-CAM applications. Starting in 2000, hardware ac­celerated graphics underwent a dramatic 
evolution in performance, features and commoditization. This evolution can be roughly split into three 
phases. First, the .xed function pipeline became con.gurable allowing several textures to be combined 
and supporting off-screen render­ing to a texture for multi-pass techniques. The ability to render into 
a texture and re-use the result for a subsequent rendering pass is one of the foundations of programmable 
graphics. In 2002, programmable shading was introduced and allowed the programmer to write custom programs 
to transform vertices and compute the .nal appearance of a fragment (see Figure 3 top). Each program 
is executed by a large number of concurrent pro­cessors. The G70 family of Nvidia GPUs contained 24 parallel 
fragment pipelines. The power of the GPU comes from this data parallel architecture. Circa 2003-2004, 
another key element was the apparition of .oating point support in the graphics pipeline in part motivated 
by the need for high-dynamic range rendering in computer graphics. This was key in evolving the GPU into 
a more general purpose processor. Today, the separated programmable shading stages have been uni.ed into 
a single model where a large number of small-footprint processing threads are executed by a global set 
of parallel process­ing cores (see Figure 3 bottom). The nVidia G80 GPUs, released 2  Proc. of the 
EAA Symposium on Auralization, Espoo, Finland, 15-17 June 2009 Figure 3: Evolution of GPU architecture. 
Top: architecture of a nVidia GeForce 6800 GPU, showing the separate vertex, fragment and compositing 
stages. Each stage is data-parallel and contains a large number of concurrent processing units. Bottom: 
Uni.ed architecture on recent GeForce 8800 GPUs and beyond. Thread processors are dynamically allocated 
to vertex or fragment opera­tions. Courtesy of Nick Triantos and Ian Buck, nVidia. in 2007, contains 
128 such stream processors which, in the case of graphics rendering, are automatically attributed to 
vertex or frag­ment processing. The latest GeForce GTX 295 features two on­board GPUs and a total of 
480 processing cores. The latest hard­ware also supports an additional stage after the vertex shader, 
the geometry shader which can be used to create new geometry. It can also operate on primitives, as opposed 
to the vertex shader where vertex adjacency information is unknown. Alongside with hardware evolution, 
programming tools have appeared to help developers leverage the capabilities of GPUs. Traditionally, 
graphics programming is achieved through two ma­jor APIs: OpenGL and Direct 3D [12]. It is now possible 
to pro­ gram the GPU with high level C-like languages (e.g., Cg, GLSL, HLSL) [14, 15, 16] integrated 
within these APIs. The primary ap­ plication of such tools is to create complex appearance models for 
surfaces, including shading and lighting operators. However, for GP-GPU applications, other languages 
have re­cently been developed such as CUDA and recently openCL and the upcoming DirectX 11 compute shaders 
[17], that let the devel­ oper access the GPU as a stream processor without going through a graphics-oriented 
rendering pipeline. The stream programming model exposes the parallelism and communication patterns inher­ent 
in the application by structuring data into streams and express­ing computation as arithmetic kernels 
that operate on streams [18, 19, 2, 20]. CUDA also includes fast Fourier transform (FFT) and linear algebra 
libraries optimized for nVidia GPUs (see also [21, 22]). CUDA has generated a lot of applications for 
GP-GPU pro­ gramming [17], e.g. GPUMat, a library accelerating MATLAB code on the GPU and available as 
a freeware [23]. GPUFFTW also provides an FFT routine 4× faster than the Intel Math Kernel Library on 
high-end Quad cores [24, 25, 26] (see Figure 6 bottom). 2.3. Differences between CPUs and modern GPUs 
A major difference between CPUs and GPUs is that CPUs are op­timized for high performance on sequential 
code so many of their transistors are dedicated to supporting non-computational tasks like branch prediction 
and caching. The highly parallel nature of graphics rendering enables GPUs to use additional transistors 
for computation achieving higher arithmetic intensity with the same transistor count. As a result, GPU 
threads are lighter but less ef­.cient than CPU threads and require lots of parallelism and com­pute 
intensive tasks for performance. However, the computational throughput of GPUs also comes with limitations. 
While GPU shaders/kernels can gather data (e.g., they can access textures), they cannot easily write/scatter 
data to different memory loca­tions1 Hence, GPUs are better suited for programs that ideally have no 
data dependency allowing independent cores to process the data in parallel. Another implication is that 
GPUs are good at traversing data structures but very bad at building them. Figure 4: Processing audio 
with graphics APIs. In this example each pixel encodes a time-domain sample. Color channels are used 
to encode different frequency subbands. Audio samples are pro­cessed by drawing textured lines. Color 
modulation and texture resampling are used for equalization and pitch-shifting. 3. USING THE GPU FOR 
AUDIO SIGNAL PROCESSING In this section, we review several applications of GPUs to audio processing and 
synthesis. 3.1. 3D audio processing and .ltering A .rst use of the GPU for auralization is to perform 
digital sig­nal processing and .ltering. Audio signal processing can be im­plemented through standard 
graphics APIs [27, 28] or in a more 1 although this is somewhat possible at the level of the vertex and 
ge­ ometry shaders since their output can affect where the fragments will even­ tually be rendered. 
 3  Proc. of the EAA Symposium on Auralization, Espoo, Finland, 15-17 June 2009 straightforward manner 
using the recent general purpose APIs, de­scribed in Section 2.2. Several commercial sound processing 
plu­ gins leveraging GPU processing are already available [29, 30, 31]. The latest release of Adobe Flash 
also provides integrated pro­gramming tools for GPU accelerated graphics and audio though the Pixel Bender 
API [32]. Graphics APIs being more standardized than GP-GPU APIs, using a graphics-oriented API ensures 
better compatibility among different hardware vendors. Processing audio through a graphics APIs involves 
converting the audio signal into textures and pro­cessing lines or polygons textured with the audio data. 
A variety of strategies has been used to store the audio data as textures. For instance, each pixel s 
color component can represent a single time­domain sample. RGBA components can also be used to store 
mul­tiple subbands for ef.cient multi-band processing at the expense of more memory. Effects such as 
equalization or pitch-shifting can then be ef.ciently implemented through the use of acceler­ated vector 
instructions, e.g. dot-products, and texture resampling (Figures 4). Mixing can be achieved through .oating-point 
blend­ ing in the compositing stage. Figure 5 illustrates an application to spatial audio rendering where 
the equalization coef.cients are derived from Head Related Transfer Functions (HRTFs) to pro­duce 3D 
sound over headphones [33, 10]. As can be seen in Fig­ ure 6 (top), this problem maps well on the GPU. 
With all audio samples pre-stored in graphics memory, the massive parallel ar­chitecture and fast memory 
bandwidth of modern GPUs delivers a 3-fold speedup over an SSE optimized implementation, in itself 20× 
faster than the original C code [28]. Figure 5: An illustration of a sphere of sounding particles aural­ized 
with the GPU. The input audio signals are equalized using an azimuth-elevation HRTF map. The intensity 
color of the RGBA components correspond to the attenuation for four different fre­quency subbands. The 
HRTF map is stored as a texture in graph­ics memory and can be ef.ciently queried using dedicated instruc­tions. 
Queried samples in the maps are highlighted as red dots. Alternatively, several contributions make use 
of GP-GPU APIs to perform fast .nite impulse response (FIR) convolution, e.g. for reverberation effects. 
In that case, the GPU can process time­domain or frequency domain samples, performing the required multiply-accumulate 
instructions in parallel. As mentioned pre­viously, the frequency domain transforms can also be directly 
im­plemented on the GPU [24, 21]. In [34], the authors report that the GPU outperforms an optimized SSE 
implementation for long FIR convolutions (more than 60000 taps) but fails for short .l­ters. However, 
GPU implementations appear to bring signi.cant speedups when compared to unoptimized CPU implementations 
[34, 35, 36]. With our own CUDA implementation of a Modi.ed Co­sine Transform (MDCT)-based reverberation 
engine, we observed a 6-fold speedup running on a Quadro FX3700 compared to a C++ version running on 
an Intel Core2 Extreme @3GHz. Other algorithms such as in.nite impulse response (recursive) .ltering 
cannot be ef.ciently implemented on the GPU since past values are usually unavailable when rendering 
a given pixel in fragment programs. As suggested in [19], including persistent registers to accumulate 
results across fragments would solve this problem. Recently, Trebien and Oliveira have proposed a solu­tion 
to implement general .ltering on the GPU, including recur­sive feedback coef.cients [37]. However, while 
the GPU has been shown to speed-up feedforward .lters, they do not provide explicit timing comparisons 
between GPU and CPU for recursive .lters. An issue affecting performance for all signal processing ap­plications 
on the GPU is the need for real-time streaming of data to the graphics memory. At audio sampling rates, 
streaming large numbers of audio streams will have a signi.cant impact on the per­formance of the application 
due to the slow interconnects between GPU and main memory( 8Gb/sec. on a PCI-E 16× bus vs. a typ­ical 
30Gb/sec. and up to 100Gb/sec. between GPU and graphics memory). As a result, speed-ups of 20× to 60× 
when processing data resident into graphics memory will tend to be reduced to 5× or 6× when audio has 
to be streamed. 3.2. Sound synthesis A number of papers have also tackled the problem of synthesizing 
sound on the GPU, either by generating simple combinations of basic waveforms [38] or for physically-based 
synthesis, e.g. using modal models [37, 39, 40]. It should be noted that modal synthe­ sis approaches 
are often implemented through recursive .ltering and are likely to be less suited to a GPU implementation. 
Zhang et al., however, still reported 5× speed-ups compared to a CPU implementation in their experiments 
[39]. In the case of synthesis, no streaming to the graphics memory is required and approaches using 
little or no recursive .ltering will tend to be more ef.cient, assuming that a large number of sounds 
are synthesized and mixed before being read-back to main memory. 4. USING THE GPU FOR NUMERICAL ACOUSTICS 
 GPUs can also be used to accelerate numerical acoustics, e.g. to solve .nite elements, boundary elements 
or .nite differences prob­lems [41, 42, 43]. Röber et al. [41] implemented a room acoustic modeling tool 
on the GPU using digital waveguide meshes and report speed-ups of 4.5 to 69-fold in the 2D case, depending 
on the resolution of the lattice. For 3D simulations, they report more limited speed-ups which they attribute 
to the lack of 3D texture re-sampling in their test hardware. Current generation GPUs do offer 3D texture 
re-sampling and should provide speedups more in line with the 2D case. Recently, Raghuvanshi et al. [43] 
have introduced an ef.cient frequency-domain approach for large 3D environments by exploiting an adaptive 
rectangular decomposition of the domain. The approach heavily relies on the discrete cosine transform 
(DCT) which can be very ef.ciently computed on the GPU. This approach brings enormous speed-up from 90 
to 300× 4  Proc. of the EAA Symposium on Auralization, Espoo, Finland, 15-17 June 2009 Figure 6: Performance 
comparison between GPU and CPU im­plementations. Top: A simple binaural renderer including 4-band equalization 
and a variable delay line [28]. Bottom: Massive 1D FFTs with GPUFFTW (from [24]). when compared to an 
alternative .nite difference time-domain ap­proach running on the CPU. Another popular numerical simulation 
technique, radiosity, com putes radiant exchanges between surface patches and has been widely used in 
both computer graphics and acoustics. As far as we are aware, no solution has been proposed that leverages 
GPU pro­cessing to simulate acoustic radiant exchanges. However, in [44], the GPU is used in conjunction 
with the CPU to auralize the re­sults of an acoustic radiance transfer simulation. In that case, the 
GPU is used to compute the accumulated response of all the sur­face patches in the environment and the 
.nal audio processing is performed on the CPU. Several approaches have been recently in­troduced in computer 
graphics to solve global illumination prob­lems on the GPU that could certainly be adapted to sound propaga­tion 
[45, 46, 47]. For instance, the antiradiance approach of [46], where visibility between surfaces is implicitly 
taken into account by propagating negative energy from backfacing surfaces shares many similarities with 
an acoustical boundary element approach. 5. GEOMETRY PROCESSING AND RASTERIZATION FOR AURALIZATION Interactive 
auralization generally requires signi.cant geometry pro­cessing, for instance to compute dynamic room 
acoustics or sound occlusion and scattering. Geometry processing typically includes sound source visibility 
queries and estimation of early sound re­.ections and diffraction. 5.1. Ray-casting for room acoustics 
Ray-casting has been one of the earliest applications for program mable GPUs [48], demonstrating their 
ability to traverse data struc­ tures, such as trees, and bring signi.cant speed-ups. GPU-accele rated 
ray-tracing has been used for auralization [49, 50] with sig­ ni.cant 10-fold speedups reported. However, 
it is currently un­clear how GPU approaches would compare against the latest ad­vances in ray or beam-tracing 
[51], that leverage multiple CPU cores and vector instructions for ray intersections. 5.2. Occlusion 
and scattering Several approaches have been proposed for determining sound oc­clusion and diffraction 
based on GPUs, even before graphics pipe lines became programmable. These approaches share similarities 
with the shadow mapping techniques used in graphics to com­pute cast shadows. In 1997, Tsingos and Gascuel 
proposed to compute a qualitative sound occlusion factor by evaluating the amount of geometry blocking 
the .rst Fresnel ellipsoids de.ned by a source and listening points [52] using the GPU. At the time, 
the GPU could render the necessary occlusion map for a single source/listener pair at around 1 Hz. A 
similar recent approach [53] reports update rates reaching 2000 Hz. Figure 7: Using graphics hardware 
for sound visibility" calcu­lations. (a) 3D view showing microphone, source, occluders and Fresnel zones 
for 400 and 4000 Hz. (b) Visibility map from micro­phone at 400Hz. (c) Visibility map from microphone 
at 4000Hz. Another approach using the more physically grounded Kir­choff approximation (KA) was proposed 
in 1998 [54] and extended to support .rst-order re.ections in 2007 [55]. This Kirchoff inte­ gral maps 
very well to direct rasterization by the graphics hard­ware, leading to a very ef.cient implementation. 
The approach renders all visible surfaces from the source(see Figure 8), simi­ lar to the re.ective shadow-map 
algorithm of [56]. A complex fragment program computes the integrand for each pixel and hier­archical 
image averaging (mip-mapping) is used, in a second pass, to compute the .nal integral. Repeated rendering 
passes are used for different frequencies. A DirectX implementation can compute occlusions and .rst order 
re.ections for two sources and 10 fre­ 5  Proc. of the EAA Symposium on Auralization, Espoo, Finland, 
15-17 June 2009 Figure 8: Left: Surfaces are sampled using hardware rendering from the point of view 
of the sound source. We evaluate the scat­tering terms at each pixel before global integration through 
mip­mapping. In this .gure, S and R denote the source resp. receiver. Right: Visualization of the scattering 
terms on all surfaces visible from a sound source (here, the engine of a car). quency subbands and also 
perform visual rendering at 100 Hz on a single GeForce FX8800. Using the same formalism, the GPU can 
be used for comput­ing higher-resolution scattering .lters from very complex geom­etry, such as highly 
tessellated CAD-CAM models or those ac­quired through scanning techniques. Figure 9 and 10 illustrate 
scattering .lters computed for large-scale real-world situations and compare the result to corresponding 
recordings taken in the .eld 2. Figure 11 illustrates scattering impulse responses obtained from different 
surfaces. Such .lters could be convolved along the prop­agation paths obtained with an image-source/beam-tracing 
tech­nique or to model form-factors when obstacles are present between surface patches in radiosity algorithms. 
 Figure 9: Left: Visualization of the scattering terms on the sur­face of a model of the Kukulkan temple, 
in Chichén Itzá, Mexico, for a 500Hz wave. The sound source is 15 meters in front of the stairs. Right: 
Comparison between spectrograms of a simulation and an on-site recording for the Kukulkan temple. The 
simulated response is convolved by the hand-clap of the original recording and convincingly captures 
the chirped echo from the stairs. 5.3. Acoustic re.ectance and geometrical simpli.cation Interactive 
geometrical acoustics (GA) simulations can be enhanced by introducing diffraction effects from wedges. 
However, as all GA models, the geometrical theory of diffraction (GTD) assumes edges to be large compared 
to the wavelength. Increasing geomet­rical complexity would imply using smaller primitives and even­tually 
would fall outside the validity domain of GA. Thus, it is 2Example audio .les can be found at: http://www­ 
sop.inria.fr/reves/projects/InstantScattering. Figure 10: Left: A 3D model of the scanned façade of the 
Duomo in Pisa, Italy and close-ups on surface detail. Time-of-.ight laser scanning was used to obtain 
this 13 million triangle model to a 2cm resolution. Right: Comparison between spectrograms of a simulation 
and an on-site recording. The simulated response is convolved by the hand-clap of the original recording. 
 Figure 11: Left: Responses from different 4×4m surface samples. Each surface is composed of 131072 triangles 
and generated from displacement maps. Note the secondary scattering component due to the .nite extent 
of the .at surface on the top row (green curve) and the increasingly diffusing" nature of the surfaces 
from top to bottom. Right: Scattering patterns for a detailed surface. The .gure compares sound pressure 
levels in a plane medial to the sur­face obtained by BEM and our approximation. Source is 5m di­rectly 
above the center of the face and the pressure is plotted at a distance of 10m. unclear how classical 
GA+GTD approaches could apply to more detailed scenes. Recent works have been devoted to level-of-detail 
(LOD) approaches for GA [57, 58, 59] but to our knowledge no general simpli.cation scheme that preserves 
the correct scattering properties has been proposed to date. Surface integrals and the use of GPUs offer 
the possibility to leverage level-of-detail schemes originally developed for appea rance-preserving simpli.cation 
in computer graphics [60, 61]. For instance, Tsingos et al. [62] proposed a strategy combining nor­ mal 
mapping with displacement correction to model complex sur­face detail for acoustic scattering calculations. 
Displacement sur­faces [63, 64, 61] use textures to encode .ne-grain surface detail which can be used 
at rendering time by a software ray-tracer or with the graphics hardware. Figure 12 shows scattering 
impulse responses calculated from a reference geometry and a .at poly­gon using normal and displacement 
textures. As can be seen, changes in the surface normal result in very little difference com­pared to 
a .at surface (compare to the top row in Figure 11). This demonstrates the importance of surface displacement 
and the re­sulting interference phenomena which are paramount in model­ing the proper scattering effect. 
A displacement-corrected normal 6  Proc. of the EAA Symposium on Auralization, Espoo, Finland, 15-17 
June 2009 Figure 12: Comparison of true displaced geometry with a proxy .at quadrilateral enhanced with 
normal-map only or combined normal/displacement maps. Source and receiver are respectively 10 and 20 
m directly above the center of the face. Note how the normal-map alone has little effect on the obtained 
response. The amplitude of displacement is 0.5 meters. mapping results in a much better approximation. 
An alternative solution would be to consider a more accurate edge-diffraction model, such as the Biot-Tolstoy-Medwin 
(BTM) model [65]. Contrary to the GTD, the BTM model can accu­rately model diffraction off .nite-length 
edges. For .nely tesse­lated meshes, a frequency-domain BTM approach [66] could cer­ tainly be accelerated 
on the GPU. 5.4. Individualized HRTF modeling Most advanced auralization pipelines support individualized 
bin­aural rendering, using measured or parametric HRTFs in order to best match individual listeners [33, 
67, 68]. Individualized HRTFs are traditionally obtained either through measurements [69] or nu­ merical 
simulation [70, 71]. GPU accelerated ray-tracing or sur­ face integrals can provide an ef.cient solution 
to individualized HRTF modeling, at a fraction of the computational cost of full boundary element simulations. 
Several approaches have been pro­posed toward this goal [49, 72] but no comparative study has been performed 
to date to evaluate the results. 6. DISCUSSION AND LIMITATIONS For audio signal processing applications, 
GPU implementations generally do bring signi.cant speedups over unoptimized C/C++ implementations. The 
gain is generally more limited when com­pared to SSE optimized implementations. although one could ar­gue 
that producing optimized SSE assembly code is less straight­forward than a GPU port using the available 
high-level APIs. For a number of key applications, games in particular, a very limited amount of spare 
GPU cycles is generally available and game devel­opers have been reluctant to use the graphics processing 
resources for other applications, although most consoles features fast inter­connections between GPU 
and CPU ( 20Gb/sec on Xbox360 and PS3). Other issues, such as inef.cient recursive .ltering might also 
affect the decision of porting audio processing pipelines to the GPU. The key factor that currently limits 
GPU ef.ciency for audio processing is certainly the limited communication bandwidth be­tween GPU and 
CPU, even with current PCI express buses. This impacts the ef.ciency of streaming audio data to graphics 
memory and reading-back of the processed streams for output. Approaches have been proposed to directly 
output audio data through the VGA port [73] but this is unlikely to become common practice, even through 
the HDMI output now available of most graphics cards. New interconnections such as PCI Express 3.0 or 
hypertransport with higher bandwidth might make GPUs better candidates for off­loading audio processing 
tasks in the future. For geometrical acoustics or occlusion/scattering applications, the GPU are very 
compelling alternatives, bringing much greater speedups and generally requiring slower and more limited 
asyn­chronous readbacks. 7. CONCLUSIONS AND FUTURE OPPORTUNITIES We are certainly at a critical point 
in time where GPU architectures have opened the way to widespread parallel programming. With new processors, 
such as Intel s Larrabee [74, 75] and increased interconnect speeds, GPU and CPU will increasingly be 
working together supported by general purpose programming APIs [76]. It is clear that massively parallel 
architectures will offer tremendous bene.ts to audio and auralization applications. As discussed in this 
paper, a variety of audio processing and computational acoustics algorithms have already been ported 
suc­cessfully to such architectures, sometimes twisting the original problem to recast it in terms of 
graphics programming. This issue increasingly disappears with new programming models and APIs and could 
lead to more widespread use for wave.eld synthesis [77] or microphone array techniques [78], which require 
massive paral­ lel processing. For acoustic design, massively parallel computing could enable fast goal-directed 
design and optimization of acoustic spaces or scattering surfaces, by allowing simulation algorithms 
to run in an interactive optimization loop [79]. GPUs could also al­ low for ef.cient audio coding/decoding 
and integrated transform­domain processing of a large number of audio streams for remote audio rendering 
and voice-chat applications. Beyond auralization, GPUs are also already used for speech recognition to 
compute acoustic likelihoods and hidden Markov models [80, 81, 82] and could contribute to making speech-driven 
interfaces more ef.cient, robust and widely accepted. Finally, following the evolution of the rendering 
hardware, new algorithms and approaches have been introduced in the graph­ics community. Even more than 
before, we believe it is important to follow the progress of computer graphics approaches, some of which 
address problems very close to their acoustical counterparts. They could lead to signi.cant advances 
in geometrical acoustic modeling, for instance for dynamic simpli.cation and geometry processing or improved 
scattering models. 8. REFERENCES [1] Magnus Ekman, Fredrik Warg, and Jim Nilsson, An in­depth look 
at computer performance growth, SIGARCH Comput. Archit. News, vol. 33, no. 1, pp. 144 147, 2005. [2] 
D. Geer, Taking the graphics processor beyond graphics, Computer, vol. 38, no. 9, pp. 14 16, Sept. 2005. 
[3] Avi Bleiweiss, GPU accelerated path.nding, in GH 08: Proceedings of the 23rd ACM SIG-GRAPH/EUROGRAPHICS 
symposium on Graphics hardware, Aire-la-Ville, Switzerland, Switzerland, 2008, pp. 65 74, Eurographics 
Association. [4] GPGPU -General-Purpose Computation on Graphics Hard­ware, Available at http://www.gpgpu.org, 
accessed May 15, 2009. 7  Proc. of the EAA Symposium on Auralization, Espoo, Finland, 15-17 June 2009 
[5] GPGPU -Tutorial and Courses., Available at http://gpgpu.org/tag/tutorials-courses, Accessed May 15, 
2009. [6] John D. Owens, David Luebke, Naga Govindaraju, Mark Harris, Jens Krüger, Aaron E. Lefohn, and 
Timothy J. Pur­cell, A survey of general-purpose computation on graphics hardware, Computer Graphics 
Forum, vol. 26, no. 1, pp. 80 113, 2007. [7] M. Pharr, Ed., GPU Gems 2: Programming Tech­niques, Tips 
and Tricks for Real-Time Graphics, Part IV: General-Purpose Computation on GPUS: A Primer, Addison-Wesley 
Professional, 2005, Available at [21] Kenneth Moreland and Edward Angel, The FFT on a GPU, in HWWS 03: 
Proceedings of the ACM SIG-GRAPH/EUROGRAPHICS conference on Graphics hard­ware, Aire-la-Ville, Switzerland, 
Switzerland, 2003, pp. 112 119, Eurographics Association. [22] Jens Krüger and Rüdiger Westermann, Linear 
algebra op­erators for gpu implementation of numerical algorithms, in SIGGRAPH 03: ACM SIGGRAPH 2003 
Papers, New York, NY, USA, 2003, pp. 908 916, ACM. [23] GPUmat: A GPU toolbox for MATLAB, Available at 
http://gp-you.org/, accessed May 15, 2009. [24] GPUFFTW: High Performance Power-of-two FFT http://http.developer.nvidia.com/GPUGems2/gpugems2_part04.html. 
library using graphics processors, Available at [8] P. Lapsley, J. Bier, A. Shoham, and E.A. Lee, DSP 
Processor Fundamentals, IEEE Press, 1997. [9] J. Eyre and J. Bier, The evolution of DSP proces­sors, 
IEEE Signal Processing Magazine, 2000, See also http://www.bdti.com/.  [10] Durand R. Begault, 3D Sound 
for Virtual Reality and Multi­media, Academic Press Professional, 1994. [11] L. Savioja, J. Huopaniemi, 
T. Lokki, and R. Väänänen, Cre­ating interactive virtual acoustic environments, J. Audio Eng. Soc., vol. 
47, no. 9, pp. 675 705, Sept. 1999. [12] D. Shreiner, M. Woo, J. Neider, and T. Davis, OpenGL Pro­gramming 
Guide: The Of.cial Guide to Learning OpenGL, Version 2.1 (6th Edition) , Addison-Wesley, 2007. [13] David 
Luebke and Greg Humphreys, How GPUs Work, Computer, vol. 40, no. 2, pp. 96 100, Feb. 2007. [14] R. Rost, 
OpenGL(R) Shading Language (2nd Edition) , Addison-Wesley, 2006. [15] William R. Mark, R. Steven Glanville, 
Kurt Akeley, and Mark J. Kilgard, Cg: a system for programming graph­ics hardware in a c-like language, 
in SIGGRAPH 03: ACM SIGGRAPH 2003 Papers, New York, NY, USA, 2003, pp. 896 907, ACM. [16] Randima Fernando 
and Mark J. Kilgard, The Cg Tutorial: The De.nitive Guide to Programmable Real-Time Graphics, Addison-Wesley 
Professional, 2003. [17] Nvidia CUDA, Available at http://www.nvidia.com/cuda, accessed May 15, 2009. 
[18] I. Buck and T. Purcell, GPU Gems: Programming Techniques, Tips and Tricks for Real-Time Graphics. 
Chapter 37: A Toolkit for Computation on GPUs, Addison-Wesley Professional, 2004, Available at http://http.developer.nvidia.com/GPUGems/gpugems_ch37.html. 
[19] Ian Buck, Tim Foley, Daniel Horn, Jeremy Sugerman, Kayvon Fatahalian, Mike Houston, and Pat Hanrahan, 
Brook for GPUs: stream computing on graphics hardware, ACM Trans. Graph., vol. 23, no. 3, pp. 777 786, 
2004. [20] John Owens, GPU Gems 2: Programming Tech­niques, Tips and Tricks for Real-Time Graphics. Chapter 
29: Streaming Architectures and Technology Trends, Addison-Wesley Professional, 2005, Available at http://developer.nvidia.com/object/gpu_gems_2_home.html. 
http://gamma.cs.unc.edu/GPUFFTW/, accessed May 15, 2009. [25] Naga K. Govindaraju and Dinesh Manocha, 
Cache-ef.cient numerical algorithms using graphics hardware, Parallel Comput., vol. 33, no. 10-11, pp. 
663 684, 2007. [26] Naga K. Govindaraju, Scott Larsen, Jim Gray, and Dinesh Manocha, A memory model for 
scienti.c algorithms on graphics processors, Nov. 2006, pp. 6 6. [27] S. Whalen, Audio and the Graphics 
Processing Unit, Avail­able at http://www.node99.org/papers/gpuaudio.pdf, ac­cessed May 15, 2009. [28] 
E. Gallo and N. Tsingos, Ef.cient 3D audio pro­cessing with the GPU, in Proc. ACM Workshop on General 
Purpose Computing on Graphics Proces­sors (poster), Los Angeles,, Aug. 2004, http://www­ sop.inria.fr/reves/projects/GPUAudio/. 
[29] LiquidSonics, Reverberate LE GPU Edition, Available at http://www.liquidsonics.com/software.htm, 
accessed May 15, 2009. [30] Nils Schneider, VST Plugin: Convolution Reverb on NVidia GPUs , Available 
at http://www.nilsschneider.de, accessed May 15, 2009. [31] Acustica Audio, Nebula 3 VST Plugin, Available 
at http://www.acusticaudio.net/, accessed May 15, 2009. [32] Adobe PixelBender API, Available at http://labs.adobe.com/technologies/pixelbender, 
accessed May 15, 2009. [33] J. Blauert, Spatial Hearing : The Psychophysics of Human Sound Localization, 
M.I.T. Press, Cambridge, MA, 1997. [34] A. Smirnov and T. Chiueh, An Implementa­tion of a FIR Filter 
on a GPU, Available at http://research.alexeysmirnov.name/index.php?area=gr&#38;proj=fog, accessed May 
15, 2009. [35] B. Cowan and B. Kapralos, Real-time GPU-base convo­lution: A follow-up, in Proc. of the 
FuturePlay @ GDC Canada Intl. Conf. on the Future of Game Design and Tech­nology, May 12-13 2009. [36] 
Brent Cowan and Bill Kapralos, Spatial sound for video games and virtual environments utilizing real-time 
GPU­based convolution, in Future Play 08: Proceedings of the 2008 Conference on Future Play, New York, 
NY, USA, 2008, pp. 166 172, ACM. 8  Proc. of the EAA Symposium on Auralization, Espoo, Finland, 15-17 
June 2009 [37] F. Trebien and M.M. Oliveira, Realistic real-time sound re­synthesis and processing for 
interactive virtual worlds, The Visual Computer, vol. 25, no. 5-7, pp. 469 477, May 2009. [38] F. Trebien 
and M.M. Oliveira, Real-time Audio Processing on the GPU, pp. 583 604, Charles River Media, 2008. [39] 
Q. Zhang, L. Ye, and Z. Pan, Physically-based sound syn­thesis on GPUs, in Proc. of the 4th Intl. Conf. 
on Entertain­ment Computing, Sept. 19-21 2007. [40] Christoph von Tycowicz and Jörn Loviscach, A malleable 
drum, in SIGGRAPH 08: ACM SIGGRAPH 2008 posters, New York, NY, USA, 2008, pp. 1 1, ACM. [41] N. Röber, 
M. Spindler, and M. Masuch, Waveguide-based Room Acoustics through Graphics Hardware, in Proc. Intl. 
Computer Music Conf. (ICMC), Nov. 6-11, 2006. [42] N. Raghuvanshi, B. Lloyd, and M.C. Lin, Ef.cient 
Nu­merical Acoustic Simulation on Graphics Processors Using Adaptive Rectangular Decomposition, in Proc. 
EAA Symp. on Auralization, June 15-17, 2009. [43] Nikunj Raghuvanshi, Rahul Narain, and Ming C. Lin, 
Ef.­cient and accurate sound propagation using adaptive rectan­gular decomposition, IEEE Transactions 
on Visualization and Computer Graphics, vol. 99, no. 2, 5555. [44] Samuel Siltanen, Tapio Lokki, and 
Lauri Savioja, Fre­quency domain acoustic radiance transfer for real-time au­ralization, Acta Acustica 
united with Acustica, vol. 95, pp. 106 117(12), January/February 2009. [45] Nathan A. Carr, Jesse D. 
Hall, and John C. Hart, GPU algorithms for radiosity and subsurface scatter­ing, in HWWS 03: Proceedings 
of the ACM SIG-GRAPH/EUROGRAPHICS conference on Graphics hard­ware, Aire-la-Ville, Switzerland, Switzerland, 
2003, pp. 51 59, Eurographics Association. [46] Carsten Dachsbacher, Marc Stamminger, George Drettakis, 
and Frédo Durand, Implicit visibility and antiradiance for interactive global illumination, in SIGGRAPH 
07: ACM SIGGRAPH 2007 papers, New York, NY, USA, 2007, p. 61, ACM. [47] Jaakko Lehtinen, Matthias Zwicker, 
Emmanuel Turquin, Janne Kontkanen, Frédo Durand, François Sillion, and Timo Aila, A meshless hierarchical 
representation for light trans­port, ACM Trans. Graph., vol. 27, no. 3, 2008. [48] Timothy J. Purcell, 
Ian Buck, William R. Mark, and Pat Han­rahan, Ray tracing on programmable graphics hardware, ACM Transactions 
on Graphics, vol. 21, no. 3, pp. 703 712, July 2002, ISSN 0730-0301 (Proceedings of ACM SIG-GRAPH 2002). 
[49] N. Röber, U. Kaminski, and M. Masuch, Ray-acoustics using Computer Graphics Technology, in Proc. 
10th Intl. Conf. on Digital Audio Effects (DAFx), Sep. 10-15, 2007. [50] M. Jedrzejewski and K. Marasek, 
 Computation of room acoustics using programmable video hardware, in Proc. Computer Vision and Graphics 
International Conference, ICCVG 2004, Warsaw, Poland, September 2004. [51] A. Chandak, C. Lauterbach, 
M. Taylor, Z. Ren, and D. Manocha, AD-Frustum: Adaptive Frustum Tracing for Interactive Sound Propagation, 
Visualization and Computer Graphics, IEEE Transactions on, vol. 14, no. 6, pp. 1707 1722, Nov.-Dec. 2008. 
 [52] Nicolas Tsingos and Jean-Dominique Gascuel, Soundtracks for computer animation: sound rendering 
in dynamic en­vironments with occlusions, in Proc. of Graphics Inter­face 97, May 1997, pp. 9 16. [53] 
B. Cowan and B. Kapralos, Real-time acoustical diffrac­tion modeling using the GPU, in Proc. of the 
10th Western Paci.c Acoustics Conf., Sept. 21-23 2009. [54] Nicolas Tsingos and Jean-Dominique Gascuel, 
 Fast ren­dering of sound occlusion and diffraction effects for virtual acoustic environments, in Proc. 
104th Audio Engineer­ing Society Convention, preprint 4699, Amsterdam, Nether­lands, May 1998. [55] Nicolas 
Tsingos, Carsten Dachsbacher, Sylvain Lefebvre, and Matteo Dellepiane, Instant sound scattering, in Ren­dering 
Techniques (Proc. of the Eurographics Symposium on Rendering), 2007. [56] C. Dachsbacher and M. Stamminger, 
 Re.ective shadow map, Proceedings of I3D 05, 2005. [57] S. Siltanen, Geometry reduction in room acoustics 
mod­eling, Master Thesis, Helsinki University Of Technology, Department of Computer Science Telecommunications 
Soft­ware and Multimedia Laboratory, September 2005. [58] L.M. Wang, J. Rathsam, and S.R. Ryherd, Interactions 
of model detail level and scattering coef.cients in room acous­tic computer simulation, Intl. Symp. on 
Room Acoustics, a satelite symposium of ICA, Kyoto, Japan, 2004. [59] C. Joslin and N. Magnenat-Thalmann, 
Signi.cant facet re­treival for real-time 3D sound rendering in complex virtual environments, Proc. of 
VRTST 2003, October 2003. [60] Robert L. Cook, Loren Carpenter, and Edwin Catmull, The reyes image rendering 
architecture, SIGGRAPH Comput. Graph., vol. 21, no. 4, pp. 95 102, 1987. [61] Jonathan Cohen, Marc Olano, 
and Dinesh Manocha, Appearance-preserving simpli.cation, in SIGGRAPH 98: Proceedings of the 25th annual 
conference on Computer graphics and interactive techniques, New York, NY, USA, 1998, pp. 115 122, ACM 
Press. [62] Nicolas Tsingos, Carsten Dachsbacher, Sylvain Lefebvre, and Matteo Dellepiane, Extending 
geometrical acoustics to highly detailed architectural environments, in 19th Intl. Congress on Acoustics, 
sep 2007. [63] J. Hirche, A. Ehlert, S. Guthe, and M. Doggett, Hard­ware accelerated per-pixel displacement 
mapping, Proc. of Graphics Interface 04. Canadian Human-Computer Com­munications Society, pp. 153 158, 
2004. [64] Lionel Baboud and Xavier Décoret, Rendering geometry with relief textures, in Graphics Interface 
06, 2006. [65] U. P. Svensson, R. I. Fred, and J. Vanderkooy, An an­alytic secondary source model of 
edge diffraction impulse responses, J. Acoust. Soc. Am., vol. 106, pp. 2331 2344, 1999. [66] U. P. Svensson, 
P. Calamia, and S. Nakanishi, Frequency­domain edge diffraction for .nite and in.nite edges, Acta Acustica 
united with Acustica, vol. 95, no. 3, 2009. [67] E. Wenzel, M. Arruda, D. Kistler, and F. Wightman, Local­ization 
using non-individualized head-related transfer func­tions, J. Acoustical Soc. Am., vol. 94, no. 1, pp. 
111 123, July 1993. 9  Proc. of the EAA Symposium on Auralization, Espoo, Finland, 15-17 June 2009 
[68] D. Begault, E. Wenzel, and M. Anderson, Direct compar­ison of the impact of head-tracking, reverberation, 
and indi­vidualized head-related transfer functions on the spatial per­ception of a virtual speech source, 
J. Audio Eng. Soc., vol. 49, no. 10, pp. 904 916, 2001. [69] J.C. Middlebrooks, E.A. Macpherson, and 
Z.A. Onsan, Psy­chophysical customization of directional transfer functions for virtual sound localization, 
Journal Acoustical Soc. Am., vol. 108, no. 6, pp. 3088 3091, 2000. [70] Y. Kahana and P.A. Nelson, Numerical 
modelling of the spatial acoustic response of the human pinna, Journal of Sound and Vibration, vol. 292, 
no. 1-2, pp. 148 178, Apr. 2006. [71] B. Katz, Boundary element method calculation of individ­ual head-related 
transfer function. part I: Rigid model cal­culation, Journal Acoustical Soc. Am., vol. 110, no. 5, pp. 
2440 2448, 2001. [72] Matteo Dellepiane, Nico Pietroni, Nicolas Tsingos, Manuel Asselot, and Roberto 
Scopigno, Reconstructing head mod­els from photographs for individualized 3D-audio process­ing, in Computer 
Graphics Forum (Special Issue -Proc. Paci.c Graphics) 27(7), 2008. [73] Jörn Loviscach, GPU-based audio 
via the VGA port, in SIGGRAPH 08: ACM SIGGRAPH 2008 posters, New York, NY, USA, 2008, pp. 1 1, ACM. [74] 
Brian Santo and Sally Adee, Multi-core made sim­pler, IEEE Spectrum, Jan. 2009, Available at http://www.spectrum.ieee.org/jan09/7129. 
[75] Larry Seiler, Doug Carmean, Eric Sprangle, Tom Forsyth, Michael Abrash, Pradeep Dubey, Stephen Junkins, 
Adam Lake, Jeremy Sugerman, Robert Cavin, Roger Espasa, Ed Grochowski, Toni Juan, and Pat Hanrahan, Larrabee: 
a many-core x86 architecture for visual computing, ACM Trans. Graph., vol. 27, no. 3, pp. 1 15, 2008. 
[76] M. Pharr, A. Lefohn, C. Kolb, P. Lalonde, T. Foley, and G. Berry, Programmable Graphics -The Future 
of Inter­active Rendering, Available at http://www.cerlsoundgroup .org/RealTimeMorph/, accessed March 
08, 2006.  [77] A.J. Berkhout, D. de Vries, and P. Vogel, Acoustic control by wave .eld synthesis, 
vol. 93, no. 5, pp. 2764 2778, may 1993. [78] Adam O Donovan, Ramani Duraiswami, and Nail A. Gumerov, 
Real time capture of audio images and their use with video, Oct. 2007, pp. 10 13. [79] M. Monks, B.M. 
Oh, and J. Dorsey, Audioptimization: Goal based acoustic design, IEEE Computer Graphics &#38; Appli­cations, 
pp. 76 91, May 2000. [80] Patrick Cardinal, Pierre Dumouchel, Gilles Boulianne, and Michel Comeau, GPU 
Accelerated Acoustic Likelihood Computations, in Proc. of INTERSPEECH, 2008. [81] Paul R. Dixon and Tasuku 
Oonishia and Sadaoki Furuia, Harnessing graphics processors for the fast computation of acoustic likelihoods 
in speech recognition, Computer Speech &#38; Language, vol. 23, no. 4, pp. 510 526, Oct. 2009. [82] Jike 
Chong, Youngmin Yi, Arlo Faria, Nadathur Rajagopalan Satish, and Kurt Keutzer, Data-parallel large vocabu­lary 
continuous speech recognition on graphics processors, Tech. Rep. UCB/EECS-2008-69, EECS Department, Univer­sity 
of California, Berkeley, May 2008. 10                                
                Perceptual Audio Rendering of Complex Virtual Environments Nicolas Tsingos, 
Emmanuel Gallo and George Drettakis REVES/INRIA Sophia-Antipolis*  Figure 1: Left, an overview of a 
test virtual environment, containing 174 sound sources. All vehicles are moving. Mid-left, the magenta 
dots indicate the locations of the sound sources while the red sphere represents the listener. Notice 
that the train and the river are extended sources modeled by collections of point sources. Mid-right, 
ray-paths from the sources to the listener. Paths in red correspond to the perceptually masked sound 
sources. Right, the blue boxes are clusters of sound sources with the representatives of each cluster 
in grey. Combination of auditory culling and spatial clustering allows us to render such complex audio-visual 
scenes in real-time. Abstract We propose a real-time 3D audio rendering pipeline for complex virtual 
scenes containing hundreds of moving sound sources. The approach, based on auditory culling and spatial 
level-of-detail, can handle more than ten times the number of sources commonly avail­able on consumer 
3D audio hardware, with minimal decrease in audio quality. The method performs well for both indoor and 
outdoor environments. It leverages the limited capabilities of au­dio hardware for many applications, 
including interactive architec­tural acoustics simulations and automatic 3D voice management for video 
games. Our approach dynamically eliminates inaudible sources and groups the remaining audible sources 
into a budget number of clus­ters. Each cluster is represented by one impostor sound source, po­sitioned 
using perceptual criteria. Spatial audio processing is then performed only on the impostor sound sources 
rather than on every original source thus greatly reducing the computational cost. A pilot validation 
study shows that degradation in audio quality, as well as localization impairment, are limited and do 
not seem to vary signi.cantly with the cluster budget. We conclude that our real-time perceptual audio 
rendering pipeline can generate spatial­ized audio for complex auditory environments without introducing 
disturbing changes in the resulting perceived sound.eld. Keywords: Virtual Environments, Spatialized 
Sound, Spatial Hearing Models, Perceptual Rendering, Audio Hardware. *contact Nicolas.Tsingos@sophia.inria.fror 
visit http://www-sop.inria.fr/reves/  1 Introduction Including spatialized audio is a key aspect in 
producing realistic vir­tual environments. Recent studies have shown that the combination of auditory 
and visual cues enhances the sense of immersion (e.g., [Larsson et al. 2002]). Unfortunately, high-quality 
spatialized au­dio rendering based on pre-recorded audio samples requires heavy signal processing, even 
for a small number of sound sources. Such processing typically includes rendering of source directivity 
pat­terns [Savioja et al. 1999], 3D positional audio [Begault 1994] and arti.cial reverberation [Gardner 
1997; Savioja et al. 1999]. Despite advances in commodity audio hardware (e.g., [Sound-Blaster 2004]), 
only a small number of processing channels (16 to 64) are usually available, corresponding to the number 
of sources that can be simultaneously rendered. Although point-sources can be used to simulate direct 
and low­order indirect contributions interactively using geometric tech­niques [Funkhouser et al. 1999], 
a large number of secondary images-sources are required if further indirect contributions are to be added 
[Borish 1984]. In addition, many real-world sources such as a train (see Figure 1) are extended sound 
sources; one solution allowing their improved, if not correct, representation is to simulate them with 
a collection of point sources, as proposed in [Sensaura 2001]. This further increases the number of sources 
to render. This also applies to more speci.c effects, such as rendering of aerody­namic sounds [Dobashi 
et al. 2003], that also require processing collections of point sources. For all the reasons presented 
above, current state-of-the-art solu­tions [Tsingos et al. 2001; Fouad et al. 2000; Wenzel et al. 2000; 
Savioja et al. 1999], still cannot provide high-quality audio render­ings for complex virtual environments 
which respect the manda­tory real-time constraints, since the number of sources required is not supported 
by hardware, and software processing would be over­whelming. To address this shortcoming, we propose 
novel algorithms per­mitting high-quality spatial audio rendering for complex virtual en­vironments, 
such as that shown in Figure 1. Our work is based on the observation that audio rendering operations 
(see Figure 2) are usually performed for every sound source while there is signi.cant psycho-acoustic 
evidence that this might not be necessary due to limits in our auditory perception and localization accuracy 
[Moore 1997; Blauert 1983].  Similar to the occlusion culling and level of detail algorithms widely 
used in computer graphics [Funkhouser and Sequin 1993], we introduce a dynamic sorting and culling algorithm 
and a spatial clustering technique for 3D sound sources that allows for 1) sig­ni.cantly reducing the 
number of sources to render, 2) amortizing costly spatial audio processing over groups of sources and 
3) lever­aging current commodity audio hardware for complex auditory sim­ulations. Contrary to prior 
work in audio rendering, we exploit a priori knowledge of the spectral characteristics of the input sound 
signals to optimize rendering. From this information, we interac­tively estimate the perceptual saliency 
of each sound source present in the environment. This saliency metric drives both our culling and clustering 
algorithms. We have implemented a system combining these approaches. The results of our tests show that 
our solution can render highly dynamic audio-visual virtual environments comprising hundreds of point-sound 
sources. It adapts well to a variety of applications in­cluding simulation of extended sound sources 
and indoor acoustics simulation using image-sources to model sound re.ections. We also present the results 
of a pilot user study providing a .rst validation of our choices. In particular, it shows that our algorithms 
have little impact on the perceived audio quality and spatial audio localization cues when compared to 
reference renderings. Traditional pipeline  Figure 2: A traditional hardware-accelerated audio rendering 
pipeline. Pre-mixing can usually be implemented with few oper­ations while positional audio and reverberation 
rendering require heavier processing. 2 Related Work Our approach builds upon prior work in the .elds 
of perceptual audio coding and audio rendering. The following sections give a short overview of the background 
most relevant to our problem. Perceptual audio coding and sound masking When a large number of sources 
are present in the environment, it is very unlikely that all will be audible due to masking occurring 
in the human auditory system [Moore 1997]. This masking mechanism has been successfully exploited in 
per­ceptual audio coding (PAC), such as the well known MPEG I Layer 3 (mp3) standard [Painter and Spanias 
1997; Brandenburg 1999]. Note that contrary to PAC, our primary goal is to detect masking occurring between 
several sounds in a dense sound mixture rather than intra-sound masking. Since our scenes are highly 
dynamic, masking thresholds have to be continuously updated. This requires an ef.cient evaluation of 
the necessary information. Interactive masking evaluation has also already been used for ef.cient modal 
synthesis [Lagrange and Marchand 2001; van den Doel et al. 2002; van den Doel et al. 2004] but, to our 
knowledge, no solution to date has been proposed to dynamically evaluate masking for mixtures of general 
digitally recorded sounds. Such techniques could nevertheless complement our approach for real-time synthe­sized 
sounds effects. In the context of spatialized audio, binaural masking (i.e., taking into account the 
signals reaching both ears) is of primary impor­tance. Although mp3 allows for joint-stereo coding, very 
few PAC approaches aim at encoding spatial audio and include the necessary binaural masking evaluation. 
This is quite a complex task since binaural masking thresholds are not entirely based on the spatial 
lo­cation of the sources but also depend on the relative phase of the signals at each ear [Moore 1997]. 
Finally, in the context of room acoustics simulation, several perceptual studies aimed at evaluating 
masking thresholds of individual re.ections were conducted using simple image-sources simulations [Begault 
et al. 2001]. Unfortu­nately, no general purpose thresholds were derived from this work. Spatial audio 
rendering Few solutions to date have been proposed which reduce the over­all cost of an audio rendering 
pipeline. Most of them speci.­cally target the .ltering operations involved in spatial audio ren­dering. 
Martens and Chen et al. [1987; 1995] proposed the use of principal component analysis of Head Related 
Transfer Func­tions (HRTFs) to speed up the signal processing operations. One approach, however, optimizes 
HRTF .ltering by avoiding the pro­cessing of psycho-acoustically insigni.cant spectral components of 
the input signal [Filipanits 1994]. Fouad et al. [1997] propose a level-of-detail rendering approach 
for spatialized audio where the sound samples are progressively generated based on a perceptual metric 
in order to respect a bud­get computing time. When the budget processing time is reached, missing samples 
are interpolated from the calculated ones. Since full processing still has to be performed on a per source 
basis, the approach might result in signi.cant degradation for large numbers of sources. Despite these 
advances, high-quality rendering of com­plex auditory scenes still requires dedicated multi-processor 
sys­tems or distributed audio servers [Chen et al. 2002; Fouad et al. 2000]. An alternative to software 
rendering is to use additional re­sources such as high-end DSP systems (Tucker Davis, Lake DSP, etc.) 
or commodity audio hardware (e.g., Sound Blaster [Sound-Blaster 2004]). The former are usually high audio 
.delity systems but are not widely available and usually support ad-hoc APIs. The latter provide hardware 
support for game-oriented APIs (e.g., Di­rect Sound 3D [Direct Sound 3D 2004], and its extensions such 
as EAX [EAX 2004]). Contrary to high-end systems, they are widely available, inexpensive and tend to 
become de facto standards. Both classes of systems provide specialized 3D audio processing for a variety 
of listening setups and additional effects such as reverber­ation processing. In both cases, however, 
only a small number of sources (typically 16 to 64) can be rendered using hardware chan­nels. Automatic 
adaptation to resources is available in Direct Sound but is based on distance-culling (far-away sources 
are simply not rendered) which can lead to discontinuities in the generated audio signal. Moreover, this 
solution would fail when many sources are close to the listener. A solution to the problem of rendering 
many sources using limited software or hardware resources has been presented by Herder [1999a; 1999b] 
and is based on a clustering strategy. Sim­ilar approaches have also been proposed in computer graphics 
for off-line rendering of scenes with many lights [Paquette et al. 1998]. In Herder [1999a; 1999b], a 
potentially large number of point­sound sources can be down-sampled to a limited number of rep­resentatives 
which are then used as actual sources for rendering. In theory, such a framework is general and can accommodate 
primary sound sources and image-sources. Herder s clustering scheme is based on .xed clusters, corresponding 
to a non-uniform spatial sub­division, which cannot be easily adapted to .t a pre-de.ned budget. Hence, 
the algorithm cannot be used as is for resource manage­ment purposes. Second, the choice of the cluster 
representative (the  Perceptual rendering pipeline Cartesian centroid of all sources in the cluster) 
is not optimal in the psycho-acoustical sense since it does not account for the character­istics of the 
input audio signals. 3 Overview of our contributions We propose a novel spatial audio rendering pipeline 
for sampled sound signals. Our approach can be decomposed into four steps (see Figure 3) repeated for 
each audio processing frame through time (typically every 20 to 30 milliseconds): First, we evaluate 
the perceptual saliency of all sources in the scene. After sorting all sources based on their binaural 
loud­ness, we cull perceptually inaudible sources by progressively inserting sources into the mix until 
their combination masks all remaining ones. This stage requires the pre-computation of some spectral 
information for each input sound signal.  We then group the remaining sound sources into a prede.ned 
budget number of clusters. We use a dynamic clustering algo­rithm based on the Hochbaum-Shmoys heuristic 
[Hochbaum and Schmoys 1985], taking into account the loudness of each source. A representative point 
source is constructed for each non-empty cluster.  Then, an equivalent source signal is generated for 
each cluster in order to feed the available audio hardware channels. This phase involves a number of 
operations on the original audio data (.ltering, re-sampling, mixing, etc.) which are different for each 
source.  Finally, the pre-mixed signals for each cluster together with their representative point location 
can be used to feed audio rendering hardware through standard APIs (e.g., Direct Sound 3D), or can be 
rendered in software.  Sections 4 to 7 detail each of these steps. We have also conducted a pilot perceptual 
study with 20 listeners showing that our approach has very limited impact on audio quality and localization 
abilities. The results of this study are discussed in Section 8. 4 Perceptual saliency of sound sources 
The .rst step of our algorithm aims at evaluating the perceptual saliency of every source. Saliency should 
re.ect the perceptual im­portance of each source relative to the global soundscape. Perception of multiple 
simultaneous sources is a complex prob­lem which is actively studied in the community of auditory scene 
analysis (ASA) [Bregman 1990; Ellis 1992] where perceptual orga­nization of the auditory world follows 
the principles of Gestalt psy­chology. However, computational ASA usually attempts to solve the inverse 
and more complex problem of segregating a complex sound mixture into discrete, perceptually relevant 
auditory events. This requires heavy processing in order to segment pitch, timbre and loudness patterns 
out of the original mixture and remains ap­plicable only to very limited cases. In our case, we chose 
the binaural loudness as a possible saliency metric. Due to sound masking occurring in our hearing process, 
some of the sounds in the environment might not be audible. Our saliency estimation accounts for this 
phenomenon by dynamically evaluating masked sound sources. 4.1 Pre-processing the audio data In this 
paper, we focus on applications where the input audio sam­ples are known in advance (i.e., do not come 
from a real-time input and are not synthesized in real-time). Based on this assumption, we can pre-compute 
spectral features of our input signals throughout their duration and dynamically access them at runtime. 
Speci.cally, for each input signal, we generate instantaneous short-time power spectrum distribution 
(PSD) and tonality index for a number of frequency sub-bands. Such features are widely used in perceptual 
audio coding [Painter and Spanias 1997]. The PSD measures the energy present in each frequency band, 
while the tonality index is an indication of the signal noisiness: low indices indicate a noisier component. 
This index will be used for interactive estimation of masking thresholds. Our input sound signals were 
sampled at 44100 Hz. In order to retain ef.ciency, we use four frequency bands f corresponding to 0-500 
Hz, 500-2000 Hz, 2000-8000 Hz and 8000-22050 Hz. Al­though this is far less than the 25 critical bands 
used in audio cod­ing, we found it worked well in practice for our application while limiting computational 
overhead. We derive our spectral cues from a short time fast Fourier trans­form (FFT) [Steiglitz 1996]. 
We used 1024 sample long Hanning­windowed frames with 50% overlap. We store for each band f its instantaneous 
power spectrum distribution (i.e., the integral of the square of the modulus of the Fourier transform), 
PSDt ( f ), for each frame t. From the PSD, we estimate a log-scale spectral .atness measure of the signal 
as: µg(PSDt ( f )) SFMt ( f )= 10 log10 , µa(PSDt ( f ))  where µg and µa are respectively the geometric 
and arithmetic mean of the PSD over all FFT bins contained in band f . We then estimate the tonality 
index, Tt (f ), as: SFMt (f ) Tt (f )=min(,1). -60 Note that, as a result, Tt (f ). [0,1]. This information 
is quite compact (8 .oating-point values per frame, i.e., 1.4 kbyte per second of input audio data at 
CD quality) and does not result in signi.cant memory overhead. This pre-processing can be done off-line 
or when the application is started but can also be performed in real-time for a small number of input 
signals since our unoptimized implementation runs about six times faster than real-time. 4.2 Binaural 
loudness estimation At any given time-frame t of our audio rendering simulation, each source Sk is characterized 
by : 1) its distance to the listener r,2)the corresponding propagation delay d =r/c, where c is the speed 
of sound, and 3) a frequency-dependent attenuation A which consists in a scalar factor for each frequency 
band. A is derived from the octave band attenuation values of the various .lters used to alter the source 
signal, such as occlusion, scattering and directivity .lters. For additional information on these .lters 
see [Pierce 1984; ANSI 1978; Tsingos and Gascuel 1997; Savioja et al. 1999]. For instance, in the case 
of a direct, unoccluded contribution from the source to the receiver, A will simply be the attenuation 
in each frequency band due to atmospheric scattering effects. If the sound is further re.ected or occluded, 
A will be obtained as the product of all scalar attenuation factors along the propagation path. Our saliency 
estimation .rst computes the perceptual loudness at time t, of each sound source k, using an estimate 
of the sound pressure level in each frequency band. This estimate pressure level is computed at each 
ear as: Ptk(f )=Spat(Sk)× PSDtk -d(f )× Atk(f )/r, (1) where Spat(Sk)returns a direction and frequency 
dependent atten­uation due to the spatial rendering (e.g., HRTF processing). In our case, we estimated 
this function using measurements of the output level of band-passed noise stimuli rendered with Direct 
Sound 3D on our audio board. As a result, Equation 1 must be evaluated twice since the Spat(Sk)values 
will be different for the left and right ear. The loudness values at both ears Lleftk and Lrighttk, are 
then t obtained from the sound pressure levels at each ear using the model of [Moore et al. 1997]. Loudness, 
expressed in phons, is a measure of the subjective intensity of a sound referenced to a 1kHz tone1. Based 
on Moore s model, we pre-compute a loudness table for each of our four frequency sub-bands assuming the 
original signal is a white noise. We use these tables to directly obtain a loudness value per frequency 
band given the value of Pk(f )at both ears. t Going back to linear scale, a scalar binaural loudness 
criterion Ltk is computed as: =||10Llefttk/20||2 +||10Lrightk Lk t /20||2 . (2) t Finally, we normalize 
this estimate and average it over a number of audio frames to obtain smoothly varying values (we typically 
average over 0.1-0.3 sec. i.e., 4-12 frames). 1by de.nition phons are equal to the sound pressure level, 
expressed in decibels, of a 1kHz sine wave. 4.3 Binaural masking and perceptual culling We evaluate masking 
in a conservative manner by .rst sorting the sources by decreasing order according to their normalized 
loudness Ltk and progressively inserting them into the current mix until they mask the remaining ones. 
We start by computing the total power level of our scene . PTOT =.Ptk(f ) k At each frame, we maintain 
the sum of the power of all sources to be added to the mix, PtoGo, which is initially equal to PTOT. 
We then progressively add sources to the mix, maintaining the current tonality Tmix, masking threshold 
Mmix, as well as the cur­rent power Pmix of the mix. We assume that sound power adds up which is a crude 
approximation but works reasonably well with real-world signals, which are typically noisy and uncorrelated. 
To perform the perceptual culling, we apply the following algo­rithm, where ATH is the absolute threshold 
of hearing (correspond­ing to 2 phons) [Moore 1997]: Mmix =-200 Pmix =0 T =0 PtoGo =PTOT while (dB(PtoGo) 
> dB(Pmix) - Mmix)and (PtoGo > ATH)do add source Sk to the mix PtoGo - =Pk Pmix +=Pk T +=Pk * Tk Tmix 
=T/Pmix Mmix =(14.5 +Bark(fmax))* Tmix +5.5 * (1 -Tmix) k++ end Similar to prior audio coding work [Painter 
and Spanias 1997], we estimate the masking threshold, Mmix(f )as: Mmix(f )=(14.5 +Bark(fmax))*Tmix(f 
) + 5.5 * (1 - Tmix(f )) (dB), where Bark(fmax)is the value of the maximum frequency in each frequency-band 
f expressed in Bark scale. The Bark scale is a map­ping of the frequencies in Hertz to Bark numbers, 
corresponding to the 25 critical bands of hearing [Zwicker and Fastl 1999]. In our case we have for our 
four bands: Bark(500)=5,Bark(2000)= 18,Bark(8000)=24,Bark(22050)=25. The masking threshold represents 
the limit below which a mas­kee is going to be masked by the considered signal. To better account for 
binaural masking phenomena, we evaluate masking for left and right ears and assume the culling process 
is over when the remaining power at both ears is below the masking threshold of the current mix. Since 
we always maintain an overall estimate for the power of the entire scene, our culling algorithm behaves 
well even in the case of a scene composed of many low-power sources. This is the case for instance with 
image-sources resulting from sound re.ections. A naive algorithm might have culled all sources while 
their combina­tion is actually audible. 5 Dynamic clustering of sound sources Sources that have not been 
culled by the previous stage are then grouped by our dynamic clustering algorithm. Each cluster will 
act as a new point source representing all the sources it contains (i.e., a point source with a complex 
impulse response). Our goal is to ensure minimal perceptible error between these auditory impostors and 
the original auditory scene.  5.1 Building clusters Sources are grouped based on a distance metric. 
In our case, we use the sum of two spatial deviation terms from a source Sk to the cluster representative 
Cn: a distance deviation term and an angular deviation term: 1 d(Cn,Sk)=Lk ßlog10(||Cn||/||Sk||)+. (1 
- Cn.Sk) , (3) t 2 where Lk is the loudness criterion calculated in the previous section t (Eq. 2), Sk 
and Cn are the positions of source Sk and representa­tive Cn expressed in a Cartesian coordinate system 
relative to the listener s position and orientation. The weighting term Lk ensures that error is minimal 
for percep­ t tually important sources. In our experiments we used ß = 2 and . = 1, to better balance 
distance and angle errors. Since human listeners perform poorly at estimating distances, our metric is 
non­uniform in distance space, resulting in bigger clusters for distant sources. We use a dynamic clustering 
strategy based on the Hochbaum-Shmoys heuristic [Hochbaum and Schmoys 1985]. In a .rst pass, this approach 
selects n potential cluster representatives amongst all k sources by performing a farthest-.rst traversal 
of the point set using the metric of Eq. 3. In a second pass, sources are affected to the closest representative, 
resulting in a disjoint partitioning and clusters are formed. We also experimented with a global k - 
means approach (e.g., [Likas et al. 2003]), with inferior results in terms of computing time. Both methods, 
however, gave similar results in terms of overall clustering error (the sum for every source of the distances 
as de.ned by Eq. 3). The representative for the cluster must ensure minimal acoustic distortion when 
used to spatialize the signal. In particular it must preserve the overall impression of distance and 
incoming direction on the listener. Thus, a good starting candidate is the centroid of the set of points 
in (distance, direction) space. Since we are not using a .xed spatial subdivision structure as in [Herder 
1999a], the Cartesian centroid would lead to incorrect results for spatially ex­tended clusters. Using 
the centroid in polar coordinates yields a better representative since it preserves the average distance 
to the listener. Moreover, source loudness will affect spatial perception of sound [Moore 1997]. Hence, 
we use our loudness criterion to shift the location of the representative once the clusters have been 
deter­mined. The location of the representative is thus de.ned, in spheri­cal coordinates relative to 
the listener s location, as: .j Ltjr j.Cn = , .Cn =.(.LtjS j), fCn =f(.LtjS j), (4) . jLj tj j where 
rj is the distance from source Sj to the listener (Sj s are the sources contained in the cluster). Figure 
4 illustrates the results of our clustering technique in a simple outdoor environment and an indoor environment 
with sound re.ections modeled as image-sources. 5.2 Spatial and temporal coherence As a result of culling 
and loudness variations through time, our clus­tering process might produce different clusters from one 
frame to another. Since the clusters are mapped one-to-one with audio ren­dering buffers and the position 
of a cluster might switch abruptly, audible artefacts might be introduced. To avoid this problem, we 
perform a consistency check by comparing our current cluster dis­tribution to the one obtained at the 
previous frame. We shuf.e the order of our clusters so that the location of the i - th cluster at frame 
t is as close as possible to the location of the i -th cluster at frame t - 1. We sort clusters using 
the summed loudness of all the sources they contain and perform the test greedily, by comparing distances 
between all pairs of clusters. Shuf.ing more perceptually relevant clusters .rst helps minimize possibly 
remaining artefacts. 6 Spatial rendering of clusters The third stage of our pipeline is to compute an 
aggregate signal (or pre-mix) for an entire cluster based on the signals emitted by each individual sound 
source it contains. This signal will then be spatialized in the .nal stage of the pipeline. 6.1 Cluster 
pre-mixing Computing this pre-mix of all sources involves a number of op­erations such as .ltering (to 
account for scattering, occlusion, etc.), resampling (to account for the variable propagation delay and 
Doppler shifting) and 1/r distance attenuation of the input signals. Filtering depends on the source 
directivity pattern or material properties in case of image-sources. Hence, it should be performed on 
each source individually. In our case, we use frequency depen­dent attenuation to account for all .ltering 
effects. We implemented such .ltering as a simple equalization over our four sub-bands. For ef.ciency 
reasons, we pre-compute four band-passed copies of the original input signals. The .ltered signal is 
then recon­structed as a sum of the band-passed copies weighted by the vector of attenuation values A 
(see Section 4.2). Propagation delay also has to be accounted for on a per source basis. Otherwise, clicking 
artefacts appear as noticed in [Herder 1999a]. A simple method to account for time-varying non-integer 
delays is to re-sample the input sound signal. Simple linear inter­polation gives good results in practice, 
especially if the signals are oversampled beforehand [Wenzel et al. 2000]. For maximum ef.ciency, we 
implemented these simple opera­tions using SSE (Intel s Streaming SIMD Extensions) optimized assembly 
code. Being able to pre-mix each source individually has several ad­vantages. First, we can preserve 
the delay and attenuation of each source, ensuring a correct distribution of the energy reaching the 
listener through time. Doing so will preserve most of the spatial cues associated with the perceived 
size of the cluster and, more importantly, the timbre of the exact mix which would suffer from comb-.lter 
effects if a single delay per cluster was used. This is particularly noticeable for reverberations rendered 
using image­sources. A second advantage of performing pre-mixing prior to audio hardware rendering is 
that we can provide additional effects currently not (or poorly) supported in existing audio hardware 
or APIs (e.g., arbitrary directivity patterns, time delays, etc.). 6.2 Spatializing clusters in hardware 
The pre-mixed signals for each cluster, along with their representa­tives, can be used to auralize the 
audio scene in a standard spatial­ized audio system. Each cluster is considered as a point-source located 
at the posi­tion of its representative. Any type of spatial sound reproduction strategy (amplitude panning, 
binaural processing, etc.) applicable to a point source model can thus be used. Spatialization can be 
done in software, limiting the cost of spa­tial audio processing to the number of clusters. More interest­ingly, 
it can be done using standard game-audio APIs such as Direct Sound (DS). In this case a hardware 3D audio 
buffer can be created for each cluster and fed with the pre-mixed signals. The sound buffer is then positioned 
at the location of the representative Figure 4: Top row: note how the four clusters (in blue) adapt to 
the listener s location (shown in red). Bottom row: a clustering example with image-sources in a simple 
building environment (seen in top view). The audible image-sources, shown in green in the right-hand 
side image, correspond to the set of re.ection paths (shown in white) in the left-hand side image.  
 (e.g., using DS SetPosition command). We synchronize all posi­tional commands at the beginning of each 
audio frame using DS noti.cation mechanism. We also use a simple cross-fading scheme by computing a few 
extra samples at each frame and blending them with the .rst samples of the next prior to the spatialization. 
This eliminates artefacts resulting from sources moving in or out of clus­ters. In our current implementation, 
we use a 100-sample overlap at 44.1kHz (i.e., 2ms or about a tenth of our audio frame). Since audio hardware 
usually performs sampling-rate conver­sion, it is also possible to assign a different rate to each cluster 
depending on its importance. We de.ne the importance of a cluster as the sum of the loudness values of 
all the sources it contains. We sort the clusters by decreasing importance prior to rendering, and map 
them to a set of hardware buffers whose sampling rate is de­creasing, hence requiring less data to be 
rendered for an equivalent time-step. This is similar in spirit to the approach of [Fouad et al. 1997] 
but does not require extra software processing and better in­tegrates with current hardware rendering 
pipelines. Finally, we can also bene.t from additional processing offered by current consumer audio hardware, 
such as arti.cial reverberation processing, as demonstrated in the video (see the trainstation and room 
acoustics sequences). 7 Applications and performance tests We evaluated our algorithms on two prototype 
applications: 1) ren­dering of one exterior (Highway) and one interior (Trainstation) scene with numerous 
point sound sources and 2) rendering of an interior scene (Busy of.ce) including modeling of sound re.ections. 
All tests were conducted on a Pentium 4 3GHz PC with a nVidia GeForce FX5800 ultra graphics accelerator 
and a CreativeLabs Au­digy2 platinum Ex audio board. Audio was rendered using 1200­sample long audio 
frames at 44.1kHz. Our .rst two examples feature extended sound sources resulting in many point sources 
to render. In our case, extended sources are collections of several point sources playing potentially 
different sig­nals (e.g., the helicopter has 4 sound sources for rotors, jet exhaust and engine, the 
river is modeled with 40 point sources, etc.). Each sound source can have its own location, orientation 
and directiv­ity function (e.g., the directivity of the jet exhaust of the helicopter and voice of the 
pedestrians in the train station are modeled using frequency dependent cosine lobes). The train station 
example contains 60 people, with a sound source for their footsteps and one for their voices, two trains, 
with a source at each wheel, and a number of other sources (pigeons, etc.). A total of 195 sound sources 
are included. The highway scene con­tains 100 vehicles and environmental sound effects resulting in 174 
       environment #sources % culled #clusters loudness (ms) culling (ms) clustering (ms) pre-mix 
(ms) FPS w/o culling (Hz) FPS w culling (Hz) Trainstation 195 62 20 1.15 0.42 0.61 2.7 19 27 Highway 
174 45 20 1.17 0.42 0.64 2.3 27 33 Busy of.ce 355 71 20 3.8 0.8 1.14 2.5 < 1 22 Table 1: Computing 
time breakdown for three test environments and corresponding display frame rate (FPS) with and without 
culling. Cluster range All runs Successful runs localization time (s) localization error (m) found (%) 
localization time (s) avg. min. max. avg. min. max. avg. min. max. 1to4 90.40 8.32 408.45 0.49 0.00 4.12 
85.2 66.08 8.32 272.86 5to7 52.46 7.74 151.31 0.52 0.00 3.16 88.5 34.16 7.74 113.08 8to 10 66.60 9.18 
270.64 0.21 0.00 1.00 100.0 51.98 9.18 127.20 11 to 13 52.43 6.76 204.51 0.32 0.00 2.24 82.1 49.55 6.76 
204.51 14 to 16 72.11 8.19 320.82 0.27 0.00 1.00 100.0 61.42 8.19 298.76 Table 2: Statistics for localization 
time and error for all tests and successful tests only. sources. The models used for visual rendering 
contain respectively about 70000 and 30000 polygons and no visibility optimization was used for display. 
In the second type of applications we evaluated our rendering pipeline in the context of an interactive 
room acoustics simulation including the modeling of sound re.ection of the walls of the en­vironment. 
We used a path tracing algorithm to build the image­sources corresponding to sound re.ections off the 
walls of a simple model (a small building .oor containing 387 polygons). We sim­ulated all direct paths 
and .rst-order re.ections to the listener for 60 sound sources resulting in up to 360 image-sources to 
spatial­ize. Late reverberation was added to the simulation using the audio hardware s arti.cial reverberation 
engine. Note in the video how late reverberation varies as it is driven by the direct and .rst re.ec­tions 
resulting from our geometrical simulation. Table 1 summarizes performance results for all sub-parts of 
our pipeline in the three environments. It also shows averaged video frame rate monitored with and without 
culling. Performing culling prior to clustering and pre-mixing is a key factor for the ef.ciency of the 
algorithm, since the number of sources to process is signif­icantly reduced. In the busy of.ce environment, 
rendering cannot run in real-time if culling is not applied. Loudness, however, still has to be computed 
on a per-source basis so its calculation cost becomes signi.cant as the number of sources increases. 
However, most of the calculation lies in the av­eraging process necessary to obtain smoothly varying 
values for the power and tonality estimates. It was our decision to leave this pa­rameter as a variable 
but we believe averaging could be directly included in the pre-processing step of our approach. 8 Pilot 
validation study In order to validate our perceptual rendering pipeline, we conducted a series of experiments 
aimed at evaluating the impact of culling and clustering sound sources on spatial audio rendering, both 
in terms of perceived audio quality and localization impairment. We also conducted cross-modal rendering 
tests to evaluate how visuals impact the perception of our spatial audio rendering. 8.1 Experimental 
conditions We conducted our tests using non-individualized binaural presen­tation over headphones in 
a quiet of.ce room. For spatial audio rendering, we used Direct Sound 3D accelerated by a CreativeLabs 
Audigy2 platinum Ex add-in board on a desktop computer. We used Sennheiser HD600 headphones, calibrated 
to a reference listening level at eardrum (using a 1kHz sine tone of maximum amplitude). The age of our 
20 test subjects (12 males/8 females) ranged from 13 to 52 (averaging to 31.8). Most of them were computer 
scientists but very few having any experience in virtual reality or acoustics. 8.2 Clustering and localization 
impairment Our .rst set of experiments aimed at evaluating the impact of clus­tering on the spatial perception 
of the virtual soundscape. In partic­ular, we wanted to evaluate whether localization impairment arises 
from the clustering process in a task-based experiment. During this test, sound source masking was disabled. 
Our experiment is similar in spirit to [Lokki et al. 2000] but uses a different experimental procedure. 
We asked the test sub­jects to perform a walkthrough in a 3D environment consisting of many small spheres 
located every meter on a 2D regular grid at listener s height. Sixteen sound sources corresponding to 
separate tracks (drums, vocals, guitars, etc.) of a short musical segment were randomly placed at some 
of these locations. The user was asked to locate an additional reference sound source, emitting a white 
noise signal, among all the spheres by pointing at it with the mouse and pressing the space bar. Only 
a single try was allowed. Naviga­tion was done with the mouse, holding the buttons to go forward or backwards. 
The user could also rotate in place using the arrow-keys of the keyboard. Each subject performed the 
test .ve successive times with a vari­able number of clusters ranging from small to large. The number 
of clusters was determined randomly for every test. All subjects un­derwent a short training period with 
a reference solution (without clustering) prior to performing the test to learn how to navigate and get 
accustomed to the 3D audio reproduction. Subjects performed well in the localization task. Over the 100 
runs of the experiment, the source was localized exactly 74% of the time and was found 90% of the time 
within a 1 meter range of its true location. These results are similar to the ones reported in [Lokki 
et al. 2000]. More than a half of our subjects localized the source with 100% accuracy. Table 2 reports 
localization time and error (distance of the selected sphere to the actual target sphere) for the .ve 
different cluster ranges. As can be seen, the number of clusters did not have a signi.cant impact on 
localization time or accuracy.  Music Voice  Trainstation   cluster rng with graphics w/o graphics 
both cluster rng avg. min. max. cluster rng avg. min. max. avg. min. max. avg. min. max. avg. min. max. 
1to 4 1.625 -2 4 1to8 1.01 -1 3 1to 8 0.37 0 2 0.35 -1 3 0.36 -1 3 5to 7 0.433 -1 3 9to 16 0.7 0 3 9to16 
0.011 -1 1 0.25 0 2 0.189 -1 2 8to 10 0.35 -1 3 17 to 24 0.875 0 2 17 to 24 0.364 -0.1 2 0.1 -1 1 0.281 
-1 2 11 to 13 0.53 -1 3 25 to 32 0.6 -1 3 25 to 32 0.34 -0.1 2 0.5 0 2 0.42 -0.1 2 14 to 16 0.59 -1 3 
33 to 40 0.65 0 3 33 to 40 1.1 -1 4 -0.05 -1 2 0.625 -1 4 Table 3: Statistics for Reference minus Stimulus 
marks for the Music, Voice and Trainstation environments (negative values correspond to cases where the 
hidden reference received a lower mark than the actual test stimulus). 8.3 Transparency of clustering 
and culling The second set of experiments aimed at evaluating the transparency of the combined clustering 
and culling algorithms on the perceived sound quality. We used the ITU-R2 recommended triple stimulus, 
double blind with hidden reference technique, previously used for quality assess­ment of low bit-rate 
audio codecs [Grewin 1993]. Subjects were presented with three stimuli, R, A and B, corresponding to 
the ref­erence, the test stimulus and a hidden reference stimulus3. The reference solution was a rendering 
with a single source per cluster and masking disabled. Subjects could switch between the three stimuli 
at any time dur­ing the test by pressing the corresponding keys on the keyboard. The reproduction level 
could also be slightly adjusted around the calibrated listening level until the subject felt comfortable. 
Subjects were asked to rate differences between each test stimuli (A and B) and the reference R from 
imperceptible to very annoying , using a scale from 5.0 to 1.0 (with one decimal) [ITU-R 1994]. We used 
two test environments, featuring different stimulus types. In the .rst environment (Music) the stimulus 
was a 16-track music signal where each track was rendered from a different loca­tion. The locations of 
the sources were randomized across tests. The second environment (Voice) featured a single source with 
a speech stimulus but also included the 39 .rst specular re.ections from the walls of the environment 
(a simple shoebox-shaped room). The location of the primary sound source was also randomized across tests. 
As before, each subject performed each test .ve successive times with a variable number of clusters ranging 
from small to large. The number of clusters was determined randomly for every test. On average, 63% of 
the signals were masked during the sim­ulation for the Music environment and 33% of the sources were 
masked in the Voice case. Table 3 reports detailed Reference minus Stimulus marks averaged over .ve cluster 
ranges. Our algorithm was rated 4.4 on average over all tests, very close to the reference according 
to our subjects (a mark of 4.0 corresponded to difference is perceptible but not annoying on our scale). 
This result con.rms our hypothesis that our approach primarily trades spatial accuracy and number of 
sources for computing time while maintaining high restitution quality. For very low cluster budgets (typically 
1 to 4) however, signi.cant differences were reported, especially in the Music experiment. In such cases 
cluster locations can vary a lot from frame to frame in an attempt to best-.t the instruments with higher 
loudness values, resulting in an annoying sensation. The room acoustics application, while being very 
well suited to our algorithms, is also very challenging since incorrectly culling image-sources might 
introduce noticeable changes in the level, tim­bre or perceived duration of the reverberation. Based 
on the results of our experiments, our algorithms were found to perform well at 2International Telecommunication 
Union 3i.e., the subjects did do not know which of A or B was the actual test or the reference. preserving 
the spatial and timbral characteristics of the reverbera­tion in the Voice experiment. Our other room-acoustic 
test (busy of.ce, shown in the video) con.rms that our algorithm can auto­matically cull inaudible image-sources 
while largely preserving the auditory qualities of the reverberation. In.uence of visual rendering We 
also attempted to evaluate the in.uence of possible interaction between image and sound rendering on 
the quality judgment of our perceptual audio rendering. We repeated the above quality evaluation test 
using audio only and audio-visual presentation. Half of our subjects performed the test with visuals 
and half without. The test environment was a more complex train station environment featuring 120 sources 
(see video) and we had to limit our maximum number of clusters to 40 to main­tain a good visual refresh 
rate. Interestingly, the train station example received signi.cantly bet­ter marks than the two other 
examples (see Table 3). This is prob­ably due to its auditory complexity since it contains many simulta­neous 
events, making it harder for the user to focus on a particular problem. For this test-case, the number 
of subjects was not high enough for a statistical validation of our results. Nonetheless, we note that 
the obtained marks are lower in the case where visuals were added. This is somewhat counter-intuitive 
since one could ex­pect that the ventriloquism effect4 would have helped the subjects compensate for 
any spatial audio rendering discrepancy [Vroomen and de Gelder 2004]. Actually, addition of graphics 
may have made it easier for the subjects to focus on discrepancies between the audio and visual rendering 
quality. In particular, some of our subjects speci.cally complained about having trouble associating 
voices with the pedestrians. We believe that our simple visual rep­resentation of the pedestrians (limited 
number of models, no facial animation) failed in this case to provide the necessary visual cues to achieve 
proper cross-modal integration of the voice and faces which is a situation we are highly sensitive to. 
This indicates that a cross-modal importance metric should prob­ably be used, possibly increasing the 
importance of visible sources (as suggested by [Fouad et al. 1997]) and that care should be taken in 
providing a suf.ciently high degree of visual .delity to avoid disturbing effects in cross-modal experiments. 
9 Limitations of our approach Based on this preliminary user study, our approach seems very promising 
although the tests were conducted using non­individualized binaural rendering. Using the test subjects 
own measured HRTFs might reveal signi.cant differences. 4 presenting synchronous auditory and visual 
information in slightly separate locations creates the illusion that the location of the sound is shifted 
in the direction of the visual stimulus [Vroomen and de Gelder 2004].  Although the method performs 
very well for a few hundred sound sources, it cannot easily scale to the thousands due to the cost of 
the clustering and pre-mixing algorithms. Loudness evalua­tion for every source would also become a signi.cant 
bottleneck in this case. More ef.cient alternatives, such as a simple A-weighting of the pressure level 
could be used and should be evaluated. For such cases, synthesis algorithms might also be used to generate 
an equivalent signal for the cluster without having to pre-mix all the sources. Our algorithm currently 
assumes input sound signals to be non­tonal (noise-like) and uncorrelated. However, the results of the 
pre­liminary study indicate that it does perform well on a variety of signals (music, voice, natural 
and mechanical sounds, etc.). Bet­ter results might be achieved by computing a .ner estimate of the loudness 
by combining two values computed assuming the signal is closer to a noise or a tone in each frequency 
band. This would require determining a representative frequency for each frequency band during the pre-computing 
step (e.g., the spectral centroid) and using an additional pure-tone loudness table. Loudness values 
ob­tained under both assumptions could then be combined using the tonality index to yield a better loudness 
value. A .ner estimate of the pressure level of the current mix and global scene would also improve the 
masking process. However, this is a more dif.cult problem for which we do not currently have a solution. 
Although we perform a sort of temporal averaging when estimating our crite­ria, we do not account for 
.ne-grain temporal masking phenomena. This is a very interesting area for future research. Our system 
currently uses pre-recorded input signals so that nec­essary spectral information can be pre-computed. 
However, we do not believe this is a strong limitation. Equivalent information could be extracted during 
the synthesis process if synthesized sounds are used. Our pre-processing step can also be performed interactively 
for a small number of real-time acquired signals (e.g., voice ac­quired from a microphone for telecommunication 
applications). Finally, accuracy of the culling and clustering process certainly depends on the number 
of frequency bands used. Further evaluation is required to .nd an optimal choice of frequency bands. 
10 Conclusion and future work We presented an interactive auditory culling and spatial level-of­detail 
approach for 3D audio rendering of pre-recorded audio sam­ples. Our approach allows for rendering of 
complex virtual audi­tory scenes comprising hundreds of moving sound sources on stan­dard platforms using 
off-the-shelf tools. It leverages audio hard­ware capabilities in the context of interactive architectural 
acous­tics/training simulations and can be used as an automatic 3D voice management scheme in video games. 
Our current pipeline can ren­der more than ten times the number of sources that could be ren­dered using 
consumer 3D audio hardware alone. Hence, future au­dio APIs and hardware could bene.t from including 
such a man­agement scheme at a lower level (e.g., as part of the DirectSound drivers) so that it becomes 
fully transparent to the user. We believe our techniques could also be used for dynamic cod­ing and transmission 
of spatial audio content with .exible render­ing, similar to [Faller and Baumgarte 2002]. This would 
be particu­larly useful for applications such as massively multi-player on-line games that wish to provide 
a spatialized chat room feature to their participants. A pilot validation study shows that degradation 
in audio quality as well as localization impairment is very limited and does not seem to signi.cantly 
vary with the number of used clusters. We are currently preparing a full-blown follow-up study to pro­vide 
additional statistical evaluation of the impact of our algorithm. Further experiments also need to be 
designed in order to evaluate several additional factors such as the pitch, beat or similarity of the 
signals in the culling and clustering process. Our results so far suggest that spatial rendering of complex 
au­ditory environments can be heavily simpli.ed without noticeable change in the resulting perceived 
sound.eld. This is consistent with the fact that human listeners usually attend to one perceptual stream 
at a time, which stands out from the background formed by other streams [Moore 1997]. Acknowledgments 
This research was supported in part by the CREATE 3-year RTD project funded by the 5th Framework Information 
Society Tech­nologies (IST) Program of the European Union (IST-2001-34231), http://www.cs.ucl.ac.uk/create/. 
The authors would like to thank Alexandre Olivier for the modeling and texturing work. The train station 
environment in the video was designed, modeled and ani­mated by Yannick Bachelart, Frank Quercioli, Paul 
Tumelaire, Flo­rent Sacr´e and Jean-Yves Regnault. We thank Alias|wavefront for the generous donation 
of their Maya software. Finally, the authors would like to thank Mel Slater for advice regarding the 
pilot valida­tion study, Agata Opalach for thoroughly proof-reading the paper and the anonymous reviewers 
for their helpful comments. References ANSI. 1978. American national standard method for the calcula­tion 
of the absorption of sound by the atmosphere. ANSI S1.26­1978, American Institute of Physics (for Acoustical 
Society of America), New York. BEGAULT, D. R., MCCLAIN, B. U., AND ANDERSON,M.R. 2001. Early re.ection 
thresholds for virtual sound sources. In Proc. 2001 Int. Workshop on Spatial Media. BEGAULT, D. R. 1994. 
3D Sound for Virtual Reality and Multime­dia. Academic Press Professional. BLAUERT, J. 1983. Spatial 
Hearing : The Psychophysics of Human Sound Localization. M.I.T. Press, Cambridge, MA. BORISH, J. 1984. 
Extension of the image model to arbitrary poly­hedra. J. of the Acoustical Society of America 75,6. BRANDENBURG, 
K. 1999. mp3 and AAC explained. AES 17th International Conference on High-Quality Audio Coding (Sept.). 
BREGMAN, A. 1990. Auditory Scene Analysis, The perceptual organization of sound. The MIT Press. CHEN, 
J., VEEN,B. V., AND HECOX, K. 1995. A spatial feature extraction and regularization model for the head-related 
transfer function. J. of the Acoustical Society of America 97 (Jan.), 439 452. CHEN, H., WALLACE, G., 
GUPTA, A., LI, K., FUNKHOUSER, T., AND COOK, P. 2002. Experiences with scalability of dis­play walls. 
Proceedings of the Immersive Projection Technology (IPT) Workshop (Mar.). DIRECT SOUND 3D, 2004. Direct 
X homepage, Microsoft c . http://www.microsoft.com/windows/directx/default.asp. DOBASHI,Y., YAMAMOTO,T., 
AND NISHITA, T. 2003. Real­time rendering of aerodynamic sound using sound textures based on computational 
.uid dynamics. ACM Transactions on Graph­ics 22, 3 (Aug.), 732 740. (Proceedings of ACM SIGGRAPH 2003). 
 EAX, 2004. Environmental audio extensions 4.0, Creative c. http://www.soundblaster.com/eaudio. ELLIS, 
D. 1992. A perceptual representation of audio. Master s thesis, Massachusets Institute of Technology. 
FALLER, C., AND BAUMGARTE, F. 2002. Binaural cue coding applied to audio compression with .exible rendering. 
In Proc. 113th AES Convention. FILIPANITS, F. 1994. Design and implementation of an aural­ization system 
with a spectrum-based temporal processing opti­mization. Master thesis, Univ. of Miami. FOUAD, H., HAHN, 
J., AND BALLAS, J. 1997. Perceptually based scheduling algorithms for real-time synthesis of complex 
sonic environments. proceedings of the 1997 International Conference on Auditory Display (ICAD 97), Xerox 
Palo Alto Research Cen­ter, Palo Alto, USA. FOUAD, H., BALLAS, J., AND BROCK, D. 2000. An extensible 
toolkit for creating virtual sonic environments. Proceedings of Intl. Conf. on Auditory Display (Atlanta, 
USA, May 2000). FUNKHOUSER,T., AND SEQUIN, C. 1993. Adaptive display al­gorithms for interactive frame 
rates during visualization of com­plex virtual environments. Computer Graphics (SIGGRAPH 93 proceedings), 
Los Angeles, CA (August), 247 254. FUNKHOUSER,T., MIN,P., AND CARLBOM, I. 1999. Real-time acoustic modeling 
for distributed virtual environments. ACM Computer Graphics, SIGGRAPH 99 Proceedings (Aug.), 365 374. 
GARDNER, W. 1997. Reverberation algorithms. In Applications of Digital Signal Processing to Audio and 
Acoustics, M. Kahrs and K. Brandenburg, Eds. Kluwer Academic Publishers, 85 131. GREWIN, C. 1993. Methods 
for quality assessment of low bit-rate audio codecs. proceedings of the 12th AES conference, 97 107. 
HERDER, J. 1999. Optimization of sound spatialization resource management through clustering. The Journal 
of Three Dimen­sional Images, 3D-Forum Society 13, 3 (Sept.), 59 65. HERDER, J. 1999. Visualization of 
a clustering algorithm of sound sources based on localization errors. The Journal of Three Di­mensional 
Images, 3D-Forum Society 13, 3 (Sept.), 66 70. HOCHBAUM, D. S., AND SCHMOYS, D. B. 1985. A best possible 
heuristic for the k-center problem. Mathematics of Operations Research 10, 2 (May), 180 184. ITU-R. 1994. 
Methods for subjective assessment of small impair­ments in audio systems including multichannel sound 
systems, ITU-R BS 1116. LAGRANGE, M., AND MARCHAND, S. 2001. Real-time addi­tive synthesis of sound by 
taking advantage of psychoacoustics. In Proceedings of the COST G-6 Conference on Digital Audio Effects 
(DAFX-01), Limerick, Ireland, December 6-8. LARSSON,P.,V ¨ALL, D., AND KLEINER, M. 2002. Better ASTFJ¨presence 
and performance in virtual environments by improved binaural sound rendering. proceedings of the AES 
22nd Intl. Conf. on virtual, synthetic and entertainment audio, Espoo, Fin­land (June), 31 38. LIKAS, 
A., VLASSIS, N., AND VERBEEK, J. 2003. The global k-means clustering algorithm. Pattern Recognition 36, 
2, 451 461. LOKKI,T., GR¨OHN, M., SAVIOJA, L., AND TAKALA, T. 2000. A case study of auditory navigation 
in virtual acoustic en­vironments. Proceedings of Intl. Conf. on Auditory Display (ICAD2000). MARTENS, 
W. 1987. Principal components analysis and resynthe­sis of spectral cues to perceived direction. In Proc. 
Int. Computer Music Conf. (ICMC 87), 274 281. MOORE, B. C. J., GLASBERG, B., AND BAER, T. 1997. A model 
for the prediction of thresholds, loudness and partial loudness. J. of the Audio Engineering Society 
45, 4, 224 240. Software avail­able at http://hearing.psychol.cam.ac.uk/Demos/demos.html. MOORE, B. C. 
1997. An introduction to the psychology of hearing. Academic Press, 4th edition. PAINTER, E. M., AND 
SPANIAS, A. S. 1997. A review of algo­rithms for perceptual coding of digital audio signals. DSP-97. 
PAQUETTE, E., POULIN,P., AND DRETTAKIS, G. 1998. A light hierarchy for fast rendering of scenes with 
many lights. Proceed­ings of EUROGRAPHICS 98. PIERCE, A. 1984. Acoustics. An introduction to its physical 
princi­ples and applications. 3rd edition, American Institute of Physics. SAVIOJA, L., HUOPANIEMI, J., 
LOKKI,T., AND VA¨¨ANEN,R. AN¨1999. Creating interactive virtual acoustic environments. J. of the Audio 
Engineering Society 47, 9 (Sept.), 675 705. SENSAURA, 2001. ZoomFX, MacroFX, Sensaura c. http://www.sensaura.co.uk. 
SOUNDBLASTER,2004. Creative Labs Soundblaster c. http://www.soundblaster.com. STEIGLITZ, K. 1996. A DSP 
Primer with applications to digital audio and computer music. Addison Wesley. TSINGOS, N., AND GASCUEL, 
J.-D. 1997. Soundtracks for com­puter animation: sound rendering in dynamic environments with occlusions. 
Proceedings of Graphics Interface 97 (May), 9 16. TSINGOS, N., FUNKHOUSER,T., NGAN, A., AND CARLBOM, 
I. 2001. Modeling acoustics in virtual environments using the uniform theory of diffraction. ACM Computer 
Graphics, SIG-GRAPH 01 Proceedings (Aug.), 545 552. VAN DEN DOEL, K., PAI,D.K.,ADAM,T., KORTCHMAR, L., 
AND PICHORA-FULLER, K. 2002. Measurements of perceptual quality of contact sound models. In Proceedings 
of the Inter­national Conference on Auditory Display (ICAD 2002), Kyoto, Japan, 345 349. VAN DEN DOEL, 
K., KNOTT, D., AND PAI, D. K. 2004. In­teractive simulation of complex audio-visual scenes. Presence: 
Teleoperators and Virtual Environments 13,1. VROOMEN, J., AND DE GELDER, B. 2004. Perceptual effects 
of cross-modal stimulation: Ventriloquism and the freezing phe­nomenon. In Handbook of multisensory processes, 
G. Calvert, C. Spence, and B. E. Stein, Eds. M.I.T. Press. WENZEL, E., MILLER, J., AND ABEL, J. 2000. 
A software­based system for interactive spatial sound synthesis. Proceeding of ICAD 2000, Atlanta, USA 
(April). ZWICKER, E., AND FASTL, H. 1999. Psychoacoustics: Facts and Models. Springer. Second Upadated 
Edition.  An analytic secondary source model of edge diffraction impulse responses U. Peter Svenssona) 
and Roger I. Fredb) Department of Applied Acoustics, Chalmers University of Technology, SE-412 96 Go¨teborg, 
Sweden John Vanderkooy Audio Research Group, Department of Physics, University of Waterloo, Waterloo, 
Ontario N2L 3G1, Canada (Received 14 July 1997; revised 18 June 1999; accepted 19 July 1999) A new impulse-response 
model for the edge diffraction from .nite rigid or soft wedges is presented which is based on the exact 
Biot Tolstoy solution. The new model is an extension of the work by Medwin et al. [H. Medwin et al., 
J. Acoust. Soc. Am. 72, 1005 1013 (1982)], in that the concept of secondary edge sources is used. It 
is shown that analytical directivity functions for such edge sources can be derived and that they give 
the correct solution for the in.nite wedge. These functions support the assumption for the .rst-order 
diffraction model suggested by Medwin et al. that the contributions to the impulse response from the 
two sides around the apex point are exactly identical. The analytical functions also indicate that Medwin 
s second-order diffraction model contains approximations which, however, might be of minor importance 
for most geometries. Access to analytical directivity functions makes it possible to derive explicit 
expressions for the .rst-and even second-order diffraction for certain geometries. An example of this 
is axisymmetric scattering from a thin circular rigid or soft disc, for which the new model gives .rst-order 
diffraction results within 0.20 dB of published reference frequency-domain results, and the second-order 
diffraction results also agree well with the reference results. Scattering from a rectangular plate is 
studied as well, and comparisons with published numerical results show that the new model gives accurate 
results. It is shown that the directivity functions can lead to ef.cient and accurate numerical implementations 
for .rst-and second-order diffraction. &#38;#169; 1999 Acoustical Society of America. [S0001-4966(99)02111-6] 
PACS numbers: 43.20.Bi, 43.20.Fn, 43.20.Px [DEC] INTRODUCTION The classic problem of edge diffraction 
from an in.nite wedge irradiated by a point source has explicit impulse­response (IR) solutions for the 
cases of a rigid wedge and a pressure-release wedge. These were presented by Biot and Tolstoy in 1957,1 
but few studies employed these solutions until Medwin applied them to underwater and noise-barrier cases.2 
Comparisons of his model with measurements show good accuracy for geometries with in.nite edges2 4 and 
Ouis has applied Biot Tolstoy s model in room acoustics to stud­ies of a room with balconies, a case 
which can be modeled with in.nite edges.5 An earlier alternative to the Biot Tolstoy solution has been 
the Kirchhoff diffraction approximation, which can be used for both frequency-and time-domain methods. 
In seismics,6,7 as well as in room acoustics,8 the Kirchhoff dif­fraction approximation has commonly 
been employed in time-domain methods for edge diffraction. However, as has been shown using the Biot 
Tolstoy expressions,9 and other accurate methods,10 the Kirchhoff diffraction approximation leads to 
large errors, not only for low frequencies but also for a)Present address: Department of Telecommunications, 
Norwegian Univer­ sity of Science and Technology, N-7491 Trondheim, Norway. b)Present address: Akustik 
Forum AB, Stampgatan 15, SE-416 64 Go¨teborg, Sweden. high frequencies in certain directions. Consequently, 
the Biot Tolstoy is the preferred method for impulse response models of edge diffraction. The Biot Tolstoy 
expressions, explicit as they are, do not immediately suggest how the in.nite wedge expressions can be 
interpreted to lead to expressions for .nite wedges or for multiple diffraction. Medwin, Childs, and 
Jebsen suggest an interpretation, a discrete Huygens interpretation, which can be used for .nite wedges 
and also be extended to handle multiple diffraction.3 This is the basis for numerical methods such as 
the wedge assemblage (WA) method.11 Measurements of noise barriers with a .nite thickness and comparisons 
with other calculation methods seem to support that model. Whereas Medwin s interpretation leads to nu­merical 
calculation methods, the current paper proposes an approach in which directivity functions for the secondary 
edge sources a´ la Medwin are derived analytically from the IR solution for the in.nite wedge. The expressions 
for ana­lytical directivities are directly applicable to nonstraight edges and multiple diffraction. 
It will be shown that these analytical expressions support Medwin s assumption for modeling .rst-order 
diffraction, namely that the contribu­tions from the two sides of the edge around the apex point are 
exactly identical. On the other hand, the analytical func­tions indicate approximations in Medwin s assumptions 
for second-and higher-order diffraction. These approximations, however, seem to be of minor importance 
for geometries  2331 J. Acoust. Soc. Am. 106 (5), November 1999 0001-4966/99/106(5)/2331/14/$15.00 &#38;#169; 
1999 Acoustical Society of America 2331 FIG. 1. The geometry of an in.nite wedge irradiated by a point 
source S. Cylindrical coordinates are used with the z-axis along the edge of the wedge. The source has 
coordinates rS and OS and is placed at zS=0. The receiver has the coordinates rR , OR , and zR and the 
wedge has an open angle of Ow . where the second-order sound paths pass edges with small angle deviations, 
as in most noise-barrier cases. Section I of this paper reviews the Biot Tolstoy solution and Medwin 
s extension to this. A new derivation using ana­lytical directivity functions is presented, and its extension 
to second-order diffraction is described. In Sec. II the numerical calculation of edge diffraction impulse 
responses is dis­cussed. Section III presents numerical calculations for .nite edges and analytical derivations 
for a thin circular disc. The wealth of exact and asymptotic frequency-domain solutions (see Pierce12 
for a review) is addressed here only for com­parisons with calculations where the uniform theory of dif­fraction 
(UTD) by Kouyoumjian and Pathak is used.13 I. THEORY The problem to be considered here is that of a point 
source irradiating a rigid or soft, (i.e., pressure-release) ob­ject, a special case of which is the 
in.nite wedge. Impulse responses will be used throughout as descriptors of the sound .elds, with the 
sound pressure p(t) as output signal and a source signal q(t)=p0A(t)/(47), where p0 is the density of 
the air and A(t) the volume acceleration of the point source. Free-.eld radiation is then described by 
the IR, hFF(7) =b(7-R/c)/R, where R is the source-to-receiver distance. The IR for plane-surfaced objects 
can be written as a sum of the geometrical acoustics IR, hGA, and diffraction compo­nents hdiffr. The 
direct sound and specular re.ections of .rst and higher orders will be contained in hGA, as long as their 
respective validity criteria are ful.lled. In the case of an object with an entirely convex geometry 
(i.e., no indents), hdiffr will consist of only .rst-and higher-order edge diffrac­tion, whereas other 
geometries might cause combinations of specular re.ections and edge diffraction. In this study, only 
convex geometries will be considered, in particular in.nite and .nite wedges, and circular and rectangular 
plates. As a starting point, consider an in.nite rigid wedge with a geometry as indicated in Fig. 1 where 
the cylindrical coor­dinates rS , OS , 0 are used for the source and rR , OR , zR for the receiver. The 
edge diffraction IR can be written in a form which is a combination of the forms given in Refs. 3 and 
14, and based on the solution presented in Ref. 1, cv f(7)hdiffr(7)=-H(7-70), (1) 27 rSrR sinh (7) 
 FIG. 2. A plane view of the edge constructed from the two half-planes containing the edge and the source 
S, and the edge and the receiver R, respectively. Two z-coordinates, zl and zu , are indicated for which 
the two sound paths S zl R and S zu R have identical path lengths. Also indi­cated is the shortest 
distance, L0 , via the apex point, denoted A, of the edge. Angles are de.ned with signs so that sin a=z/m 
and sin Y=(z-zR)/l. where f(7)=f++(7)+f+-(7)+f-+(7)+f--(7), (2) sin[v(7 OS OR )] f (7)= , (3) cosh[v 
(7)]-cos[v(7 OS OR)] 22 2 c272-(rS+rR+zR) (7)=cosh-1, (4) 2rSrR where c is the speed of sound, the wedge 
index v equals 7/Ow , H(7-70) in Eq. (1) is Heaviside s unit step function, and the time 70 equals L0/c. 
The distance L0=[(rS+rR)2 +zR 2 ]1/2 is the shortest path from the source to the receiver via the edge 
of the wedge, passing through the so-called apex point of the edge, indicated as A in Fig. 2. For an 
in.nite wedge with soft (pressure-release) surfaces, the IR is given by using a modi.ed version of the 
f-expression15 fsoft(7)=-f++(7)+f+-(7)+f-+(7)-f--(7). (5) Wedges with a combination of rigid and soft 
surfaces could be studied using variants of the f-expressions in Eqs. (2) or (5); however, in the following 
discussion, rigid surfaces will always be assumed except for the example of a circular soft disc, studied 
in Sec. III B. A. Models of .nite wedge diffraction Although the analytical solution in Eq. (1) describes 
ex­plicitly the IR of an in.nite wedge, it does not indicate what a solution for a .nite wedge might 
look like. Medwin et al. suggest a discrete Huygens interpretation, 3 placing a number of small secondary 
sources along the edge and ad­justing their strengths so that together they give the known exact solution. 
According to this model, these secondary sources emit pulses when they are hit by the incident, impul­sive, 
sound wave. Thus, the reaction at the edge is assumed to be instantaneous. This leads to the conclusion 
that the value of the IR hdiffr at time 7 is caused by the reradiation from the two parts of the edge 
indicated in Fig. 2, a lower  2332 J. Acoust. Soc. Am., Vol. 106, No. 5, November 1999 Svensson et 
al.: Secondary source model of edge diffraction 2332 FIG. 3. Illustration of a diffraction impulse response 
for a .nite wedge, h.nite (solid line) and the corresponding in.nite wedge, hin.nite (dotted line), indicating 
the three parts where [i] h.nite =hin.nite , [ii] h.nite is a scaled version of hin.nite ,and [iii] where 
h.nite =0. The initial part of the impulse responses has been truncated. branch indicated by the subscript 
l, and an upper branch indicated by a subscript u. The two branches must obey the condition ml+ll =mu+lu 
=c7. (6) The two edge portions at positions zl and zu will cause two radiated sound-.eld components, 
hl and hu , which summed together equal the known amplitude hdiffr , hl(7)+hu(7)=hdiffr(7). (7) As indicated 
in Fig. 3, an IR for a single .nite wedge will then have three parts. In the initial part, [i], the IR 
is identical to the in.nite wedge response. In the .nal part, [iii], the IR is zero, after the sound 
wave has reached the furthest end of the edge. The intermediate part, [ii], where only hl or hu in Eq. 
(7) is present, is then a scaled version of the in.nite wedge IR. The scaling for the intermediate part 
was sug­gested in Ref. 2, and is based on the .nite wedge IR having half the amplitude of the in.nite 
wedge IR, i.e., hl =hu .Ina later paper,3 it was instead suggested that this factor should be h.nite(7) 
hl(7) mulu == , (8)hin.nite(7) hl(7)+hu(7) mulu+mlll where it is assumed that the lower branch of the 
edge has the longest extension. It will be shown below that a theoretical derivation supports the relation 
hl =hu . For multiple scattering, Medwin s model and the WA method assume that the secondary edge sources 
radiate as point sources, with source strength modi.cations based on Eq. (8). Second-order diffraction 
is then calculated by having all these secondary edge sources along the .rst edge generate individual 
diffraction contributions via the second edge. Medwin et al. present comparisons between measure­ments 
and calculations for noise barriers of .nite widths where second-order diffraction must be taken into 
account, and the agreement seems to be quite good.3 The agreement is also good for the scattering from 
a circular disc.11 As will be shown in Sec. I C, however, the derived analytical directivity functions 
indicate that having the secondary sources along the .rst edge use the ordinary diffraction expression 
to gen­erate second-order diffraction via the second edge, as in Eq. (1), is only an approximation as 
far as we can determine. However, the errors in Medwin s model do not appear to show up for geometries 
with symmetrical situations, such as for noise barriers with parallel edges. The second-order dif­fraction 
paths which will be of highest amplitude are those passing the apex points, that is, with as little angular 
devia­tion as possible during the passage of the edge. The methods in Refs. 2, 3, and 11 correctly predict 
these high-amplitude parts, and thus predict the second-order diffraction quite well, as long as the 
geometry is such that sound paths with little angular deviation are possible. A critical benchmark case 
would then be one where all sound paths across edges experience large angular deviation. For such cases, 
an accu­rate secondary source model would be important. B. Derivation of analytical directivity functions 
for the secondary edge source A derivation of analytical directivity function for the secondary edge 
sources starts by assuming the existence of a directivity function for the secondary edge sources which 
depends only on the angles of the incident sound path, a and Os , and of the reradiated sound path, Y 
and Or in Figs. 1 and 2. This directivity function must be independent of the dis­tances m and l, and 
must be symmetric so that the source and receiver positions can be interchanged with identical results. 
Thus, the reciprocity principle is always ful.lled. A conse­quence of such directivity functions is that 
the incident wavefront is split up when it hits a point of the edge, and the reradiated wavefront spreads 
in all directions with different amplitudes. To derive such a directivity function, the method of retarded 
potentials is employed as in Ref. 7, and a proto­type solution can be formulated for the edge diffraction. 
The prototype solution is then matched to the known solution for the in.nite wedge. Thus, the diffracted 
pressure pdiffr(t) could be written as an integral over contributions from the entire wedge m(z)+l(z) 
D[a(z),Y(z),OS ,OR]Pdiffr(t)= qt-dz, - cm(z)l(z) (9) where D[a(z),Y(z),OS ,OR] is the unknown directivity 
func­tion. The position along the wedge is given by the z-coordinate, and this causes a retardation of 
the source sig­nal q(t), an amplitude attenuation caused by the ray paths m and l, and a further amplitude 
attenuation by the directivity function D. The variable z can be substituted for a variable 7=(m+l)/c, 
the time delay. There will, however, be two values of z giving the same value of 7, corresponding to 
the upper and lower branches of the edge. Thus, the integral in Eq. (9) is .rst rewritten as a sum of 
the upper and lower branch integrals, as zapex m(z)+l(z) D[a(z),Y(z),OS ,OR] pdiffr(t)= qt-dz - cm(z)l(z) 
m(z)+l(z) D[a(z),Y(z),OS ,OR] qt-dz, cm(z)l(z) zapex (10)  2333 J. Acoust. Soc. Am., Vol. 106, No. 
5, November 1999 Svensson et al.: Secondary source model of edge diffraction 2333 where zapex corresponds 
to the apex point on the edge, which is given by zapex =zRrS /(rS+rR). Now, a variable substitu­tion 
is carried out so that . 70 Dl(7) dzl pdiffr(t)= q(t-7) d7 ml(7)ll(7) d7 Du(7) dzu + q(t-7) d7 mu(7)lu(7) 
d7 70 Dl(7) dzl = q(t-7) ­ ml(7)ll(7) d7 70 Du(7) dzu + d7, (11) mu(7)lu(7) d7 where the subscripts 
l and u, respectively, indicate that for each value of 7, the solution of D(7), m(7), l(7), and z(7) 
is chosen to correspond to the lower and upper branches, respectively. Also, in Eq. (11) the directivity 
function is writ­ten as Dl(7) rather than D[al(7),Yl(7),OS ,OR]. The quan­tities dz/d7 can then be de.ned 
as being zero before 7 =70 , the .rst arrival time of the diffracted sound-.eld com­ponent, so that the 
lower integration limit can be expanded to - . From this it becomes clear that the integral in Eq. (11) 
is a convolution integral of the source signal q(t) with an IR which must be the diffraction IR, since 
pdiffr(t)= q(t-7)hdiffr(7)d7, (12) and hdiffr(7) can be identi.ed as Dl(7) dzl Du(7) dzuhdiffr(7)=-+ 
. (13) ml(7)ll(7) d7 mu(7)lu(7) d7 To .nd a directivity function D which satis.es Eq. (13), the observations 
1 dzl 1 dzu cH(7-70) -= = , (14) ml(7)ll(7) d7 mu(7)lu(7) d7 rSrR sinh (7) which are proven in Appendix 
A, will be used. The relations in Eq. (14) can be inserted into Eq. (13), which leads to vf(7) -=D[al(7),Yl(7),OS 
,OR] 27 +D[au(7),Yu(7),OS ,OR]. (15) Furthermore, it is possible to express f as function of the angles 
a and Y. To do this, another relation shown in Ap­pendix B, 1+sin a(7)sin Y(7) (7)=cosh-1, (16) cos 
a(7)cos Y(7) can be used. It should be noted that a and Y can be ex­changed without affecting the result, 
and thus reciprocity is assured. This relation is valid regardless of whether one chooses the lower or 
upper branch combination of the a and Y angles, an observation which could be formulated as [al(7),Yl(7),OS 
,OR]= [au(7),Yu(7),OS ,OR] and conse­quently, f[al(7),Yl(7),OS ,OR]=f[au(7),Yu(7),OS ,OR], so it can 
be stated that 2334 J. Acoust. Soc. Am., Vol. 106, No. 5, November 1999 f(7)= 21.f[al(7),Yl(7),OS ,OR] 
+f[au(7),Yu(7),OS ,OR].. (17) This expression can .nally be inserted into Eq. (15), and the unknown 
directivity function D can be identi.ed as vf[a(7),Y(7),OS ,OR] D[a(7),Y(7),OS ,OR]=-, (18) 47 where 
either al(7) and Yl(7), or au(7) and Yu(7), could be chosen as solutions for a(7) and Y(7). These derivations 
lead to the conclusion that the branches of the edge on each side of the apex point contribute with exactly 
equal amounts to the diffraction IR hdiffr(7). This has two consequences. First, one could choose either 
the upper or lower branch solutions for a(7), Y(7), z(7), m(7), and l(7) to formulate the total diffraction 
IR in this new form, hdiffr(7)=­v 47 f[a(7),Y(7),OS ,OR] m(7)l(7) dz d7 , (19) where f is as in Eqs. 
(2) and (3), and uses (7) from Eq. (16). Second, for a .nite wedge, hl(t)=hu(t) in Eq. (7), i.e., the 
part of the IR which is a scaled version of the in.nite wedge IR, illustrated as part [ii] in Fig. 3, 
has the simple scaling factor 1/2, h.nite(7) 1 = . (20)hin.nite(7) 2 This scaling factor supports the 
suggestion by Medwin in Ref. 2, but contradicts the suggested relationship in Ref. 3, as discussed in 
Sec. I A. Decomposition of the in.nite wedge diffraction into lo­cal edge contributions also makes it 
possible to derive dif­fraction from nonstraight edges. As is shown below among the examples in Sec. 
III, the .rst-and second-order diffrac­tion components of a circular thin disc can be derived ana­lytically. 
Such derivations can employ a rewritten version of the original expression in Eq. (9), using the f function, 
as  FIG. 4. The geometry of a truncated in.nite wedge, with two parallel in.­nite edges, a point source 
S and a receiver R. One sound path, via the two points z1 and z2 on the two edges, is marked. For the 
.rst edge with the open wedge angle Ow1 , the source S has the cylindrical coordinates rS1, OS1, zS1 
, and the point at z2 along edge 2, which acts as a receiver, has the coordinates w,Ow1,z2 . Relative 
to the second edge with the open wedge angle Ow2 , the point z1 along edge 1 acts as a source and has 
the coordinates w, OS2 =0, z1 , and the receiver R has the coordinates rR2, Ow2 OR2, zR2.It is assumed 
that the two z-axes are aligned with respect to each other. The angles a and Y are as de.ned in Fig. 
2. Svensson et al.: Secondary source model of edge diffraction 2334  v z2 m(z)+l(z) C. Second-order 
edge diffraction qt­ pdiffr(t)=­ 47 c z1 To derive expressions for second-order diffraction, the f[a(z),Y(z),OS 
,OR] case of a truncated in.nite wedge with two parallel, in.nitely X dz, (21) m(z)l(z) long edges as 
in Fig. 4 is studied. Expanding the formulation where z1 and z2 are the two endpoints of the .nite or 
in.nite in Eq. (21), and using the sound-path designations in Fig. 4, wedge. the second-order diffracted 
pressure can be written v1v2 m1(z1 )+m2(z1,z2)+l2(z2) f[a1(z1 ),Y1(z1,z2 ),OS1,0]f[a2(z1,z2 ),Y2(z2),0,OR2] 
= q t- dz1dz2, pdiffr(t) (47)2 - c 2m1(z1 )m2(z1,z2)l2(z2) (22) where it has been assumed that the path 
from edge 1 to edge 2 runs along a plane connecting the two edges. This is indicated by the values OR1 
=0 and OS2 =0 in the two f factors and the factor of 2 in the denominator of the integral. This factor 
of 2 compensates for the doubling in pressure generated by an acoustic source when it is mounted on a 
baf.e. It should have the value of 1 if the ray from edge 1 to edge 2 does not run along a plane. In 
the double integral in Eq. (22), the factor m1 and the delay corresponding to m1 can be moved out from 
the integral over dz2 , so that v1 b(tm1(z1)/c) pdiffr(t)= *I2dz1, (23) 47 - m1(z1 ) where* indicates 
a convolution and 1 v2 m2(z1,z2 )+l2(z2 ) f[a1(z1 ),Y1(z1,z2),OS1,0]f[a2(z1,z2),Y2(z2 ),0,OR2] I2= qt-dz2. 
(24)247 - cm2(z1,z2)l2(z2) For each value of z1 , there is a z2,apex around which the integral I2 can 
be split into the upper and lower branches, as was described before. Then, a variable substitution is 
pos­sible with (m2+l2)/c=7, and the factors dz2/d7 can be de.ned as being zero before 7=702 . Here, 702=L02 
/c where L02=[(w+rR2)2+(zR2-z1)2]1/2. Since the geometry is ba­sically the same as for the diffraction 
for a single wedge, the left-hand relation in Eq. (14) holds here, too. The integral I2 can then be written 
1 v2 (f1lf2l+f1uf2u) dz2 I2= q(t-7) d7 247 - m2l2 d7 =q(t)*hdiffr,z1 R(7), (25) where the values of 
m2, l2 , and dz2/d7 can be chosen as the upper or lower branch solutions for a certain value of 7.If 
one studies the impulse response h (7) further, diffr,z1 R 1 v21 dz2hdiffr,z1 R(7)= .f[a1,Y1l ,OS1,0] 
247 m2l2 d7 Xf[a2l ,Y2l,0,OR2] +f[a1,Y1u ,OS1,0]f[a2u ,Y2u,0,OR2].. (26)  Equation (17) can be applied, 
implying that f[a2l , Y2l,0,OR2]=f[a2u ,Y2u,0,OR2] and 1 v2 f(a2u ,Y2u,0,OR2 ) dz2hdiffr,z1 R(7)= 247 
m2l2 d7 X.f[a1,Y1l ,OS1,0]+f[a1,Y1u ,fS1,0]. = hdiffr,1st,edge2(7).f[a1,Y1l ,OS1,0] +f[a1,Y1u ,OS1,0]./2. 
(27) This means that the IR hdiffr,z1 R(7) is a .rst-order edge-diffraction IR from the position z1 
on edge 1 to the receiver position R via edge 2, scaled by 1/2, and multiplied with a weighting function 
which is the sum of the two f functions in Eq. (27). The approach for second-order diffraction in Ref. 
3 is to place discrete edge sources along edge 1, and to use an ordinary .rst-order diffraction IR to 
calculate their contributions at the receiver point. However, the factor with two f functions in Eq. 
(27) will modify the .rst-order diffraction IR hdiffr,1st,edge2 , which in principle makes it impossible 
to let the edge sources on edge 1 irradi­ate edge 2 via ordinary .rst-order diffraction IRs. The method 
used in Ref. 3 probably does calculate the onset of hdiffr,z1 R(7) correctly and, since the high-frequency 
proper­ties are determined to a large degree by the transient onset, the overall second-order edge diffraction 
is predicted rather well. Employing the IR hdiffr,z1 R , the second-order diffracted pressure becomes 
 2335 J. Acoust. Soc. Am., Vol. 106, No. 5, November 1999 Svensson et al.: Secondary source model of 
edge diffraction 2335 pdiffr(t) v1 b[t-m1(z1 )/c] = *[q(t)*hdiffr,z1 R(z1,t)]} 47- m1(z1 ) Xdz1 ­ v1 
b[tm1(z1)/c] =q(t)* *hdiffr,z1 R(z1,t) 47- m1(z1 ) Xdz1. (28) } This expression means that the second-order 
diffraction IR, hdiffr,2nd(7), can be found from a single integral, hdiffr,2nd(7) v1 b[7-m1(z1)/c] = 
47 *hdiffr,z1 R(z1,7) dz1 - m1(z1 ) - v1 hdiffr,z1 R[7m1(z1)/c] = dz1. (29) 47- m1(z1) However, since 
the IR hdiffr,z1 R(7) is zero for 7<702, the in.nite integration limits in Eq. (29) can be replaced by 
the values z1,1 and z1,2 , which both satisfy 7=702+m1/c. Ex­plicit solutions to Eq. (29) might be possible 
for certain ge­ometries, for instance the axisymmetric re.ection from a cir­cular disc, as shown in Sec. 
III B. II. NUMERICAL IMPLEMENTATION DISCRETE-TIME IMPULSE RESPONSES The default solution for numerical 
calculations is to use a discrete-time IR which, for instance, can be given by area sampling of the continuous-time 
expression in Eqs. (1) or (19). A value of the discrete-time diffraction IR hdiffr(n)ata certain discrete 
time 7=n/ fs , where fs is the sampling fre­quency and n is a sample number, is thus given by (n+1/2)/ 
fs hdiffr(n)=hdiffr(7)d7, (30) (n-1/2)/ fs where hdiffr(7) is the continuous-time diffraction IR, such 
as in Eq. (1). The singularity at 7=70 in Eq. (1) deserves spe­cial attention, and the original function 
can be approximated by an analytically integrable function for values of 7 close to 70.14 The integration 
in Eq. (30) corresponds to .ltering the continuous-time signal hdiffr(7) prior to sampling, employing 
a low-pass .lter with an IR in the form of a rectangular pulse of width 1/fs . This is quite a crude 
low-pass .lter, and Clay and Kinney suggest the use of a wider pulse of width 4/fs .14 The sampling frequency 
can, however, always be raised high enough to give a low enough aliasing error at the highest frequency 
of interest. In the numerical examples in this study, numerical integration as in Eq. (30) has been used 
throughout with rather high sampling frequencies (20 to 40 times the highest frequency of interest) to 
get accurate re­sults. Formulating the diffraction as a sum of contributions from along the edge, as 
in Eq. (19), it can be seen that the 2336 J. Acoust. Soc. Am., Vol. 106, No. 5, November 1999 integration 
of hdiffr(7) in Eq. (30) over a time segment d7 is equivalent to the integration over a segment of the 
edge which corresponds to this particular d7. To do this, another variable substitution can be done so 
that (n+0.5)/ fs zn2 d7 h(n)=hdiffr(7)d7=hdiffr(z) dz, (31) (n-0.5)/ fs dz zn1 where zn1 and zn2 are 
the z-coordinates that correspond to 7=(n 1)/(2 fs), respectively. As discussed in conjunction with Eq. 
(19), one could choose either the lower or upper branch z-coordinates that correspond to 7. Since Eq. 
(19) can be used to .nd that d7 v f[a(z),Y(z),OS ,OR]hdiffr(z) =-, (32) dz 47 m(z)l(z) the integration 
in Eq. (31) can be written v zn2 f[a(z),Y(z),OS ,OR]h(n)=-dz. (33) 47zn1 m(z)l(z) This integration 
can be approximated by the midpoint value so that v f[a(zn),Y(zn ),OS ,OR]h(n)=-azn , (34) 47 m(zn)l(zn) 
 where zn is the midpoint between zn1 and zn2. If,for a simpli.ed numerical implementation, one chooses 
to divide the edge into equally sized elements of size az, then an element i placed at zi will give a 
contribution ahi to the IR v f[a(zi),Y(zi),OS ,OR]ahi=-az. (35) 47 m(zi)l(zi ) This should be added 
to a single time sample n= fs(m +l)/c or divided among two or more consecutive time samples. This division 
is determined by the edge element s position and size relative to the time positions for sample n. The 
contribution ahi should be viewed as a rectangular pulse with a width at as given by the element size 
az and its position zi . If the edge element size is chosen so that az <c/ fs , the contribution from 
each element will never spread out over more than two time samples. For ef.cient numerical implementations, 
the f function can be written in a much more compact form than in Eqs. (2), (3), and (16) if one uses 
the relation - cosh(v )=(Av+Av)/2, (36) 21)1/2 where A=(y-+y and y=[ml+z(z-zr)]/(rsrr), which is taken 
from Eq. (A7), with the same variables as before. With this formulation, only basic mathematical com­putations 
need to be recalculated for each edge element and very ef.cient implementations are possible. Second-order 
diffraction is straightforward to implement using a formulation which is based on Eq. (22). If two .nite 
edges are divided into equally sized edge elements az1 and az2 , the second-order contribution ahij to 
the IR hdiffr,2nd , of edge element i at zi of edge 1 and edge element j at zj of edge 2, can be written 
 Svensson et al.: Secondary source model of edge diffraction 2336  v1v2 f[a1(z1 ),Y1(z1,z2 ),OS1,OR1]f[a2(z1,z2 
),Y2(z2),OS2,OR2] ahij = az1,az2. (37) 3272 m1(z1 )m2(z1,z2)l2(z2) This expression is valid for both 
parallel and nonparallel edges, as long as the appropriate expressions for the involved a, Y, and O angles, 
and m and l distances, are used. Note that a second-order contribution via two edge elements should be 
viewed as a triangular pulse, and a third-order contribution appears as a pulse with second-order polynomial 
shape, etc. This affects how each contribution should be divided among time samples. A simpli.ed implementation, 
giving the cor­rect low-frequency response, was used by Vanderkooy.16 The contribution ah can be distributed 
over the two time samples adjacent to the arrival time of the edge element cen­ter, with a linear weighting 
that depends on the center arrival time s position relative to the two sample times. Other models have 
used time-domain formulations for edge diffraction, the most common ones being based on the Kirchhoff 
diffraction approximation, as mentioned in the In­troduction. Another model, suggested by Vanderkooy, 
has an interesting equivalence to the new model suggested here.16 Vanderkooy s model started from a frequency-domain 
high­frequency asymptotic expression for the in.nite wedge.17 The model uses discrete edge sources, each 
of which gives a contribution v .(OS ,OR) ah=-az, (38) 47 ml which is delayed by the time 7=(m+l)/c. 
Vanderkooy used a parameter vVand which is the reciprocal of the wedge index v used here, but for clarity 
the parameter vVand was replaced by v in Eq. (38) so that the similarity with Eq. (35) is clear. The 
directivity function . is 1 .(OS ,OR )=sin v7 cos v7-cos[v(OR -OS )] + 1 cos v7+cos[v(27-OR -OS )]}, 
(39) which can be rewritten into the form .(OS ,OR )=f++ Vand+f+-Vand+f-+ Vand+f--Vand , (40) where fVand 
= sin[v(7 OS OR )] 1-cos[v(7 OS OR )] . (41)  If cosh(v )=1, which happens for the time 7=70 (i.e., 
at the onset of the diffraction), these f terms are identical with the analytical ones in Eqs. (2) and 
(3). Since rapid variations in the IR always occur at the onset, the high-frequency part of the response 
is determined entirely by this initial part of the diffracted signal. Thus, it is not surprising that 
Vanderkooy s model correctly reproduces the initial part of the IR and con­sequently the high-frequency 
response. The model does, however, give large errors at low frequencies, which is clearly due to using 
the constant value 1 in Eq. (41) instead of the monotonically increasing function cosh v . III. EXAMPLES 
In this section, the new model employing analytical di­rectivity function for the secondary edge sources 
is compared with other methods. Only IRs have been discussed so far, but results are often more interesting 
when presented as transfer functions. These are calculated from the IRs using a discrete Fourier transform 
(DFT), and it is then possible to check if a suf.ciently high sampling frequency has been used. The sampling 
frequency should be increased until the results at the highest frequencies of interest have stabilized. 
In all cal­culations, unless otherwise stated, a value of 344 m/s has been used for the speed of sound. 
A. In.nite wedges Two different in.nite and rigid wedges were studied as illustrated in Fig. 5. Figure 
5(a) shows a thin plate edge, with one .xed source position S (with cylindrical coordinates rS =25 cm, 
OS =15°, and zS =0) and with two different re­ceiver positions, R1 and R2 (at rR =25 cm, zR =0, and OR 
=300° for R1, and OR =196° for R2). The position R2 is 1° into the shadow zone, which is critical from 
a numerical point of view due to the singularity of the diffraction IR at the boundary of shadow zones. 
In Fig. 5(b), another case is shown with a plate of thickness w, an example which is duplicated from 
Ref. 3. In this case, the source is at cylindri­cal coordinates rS =25 cm, OS =15°, and the receiver 
is at rR =25 cm, OR =300°, zR =0. It should be noted that be­cause of the width w of the plate, the receiver 
coordinates are relative to the second edge, closest to the receiver. Calculations of IRs were made with 
the new model us­ing the discrete edge decomposition described by Eq. (35) for the single edge, and by 
Eq. (37) for the double edge. IRs  FIG. 5. The two in.nite wedge situations that have been studied. 
(a) shows a thin plate edge. (b) shows a plate of thickness w, where the values w =5, 22, and 35 mm have 
been studied.  2337 J. Acoust. Soc. Am., Vol. 106, No. 5, November 1999 Svensson et al.: Secondary source 
model of edge diffraction 2337 FIG. 6. Transfer function (relative to free-.eld propagation) at 50 cm 
dis­tance for the two receiver points at the in.nite wedge in Fig. 5(a). The frequency marked, f low,UTD 
, is the frequency above which the UTD method should give less than 0.5 dB error according to Ref. 16. 
were also calculated with the Biot Tolstoy model from Ref. 3, here denoted BTM, and transfer functions 
were calculated for the single edge with the uniform theory of diffraction (UTD) by Kouyoumjian and Pathak.13 
The lowest frequency for which the UTD method gives accurate results that is, within 0.5 dB of exact 
solutions was found to be f low,UTD =140 Hz based on Fig. 4 in Ref. 18. Sampling frequencies of 800 kHz 
were used for the IRs since, compared with a sampling frequency of 400 kHz, the difference was less than 
0.1 dB at 20 kHz. The edge element sizes were 0.43 mm for all cases, and the wedge had an extension of 
5 m relative to the source. Results for the wedge in Fig. 5(a) are presented in Fig. 6, where the results 
for the new model and the BTM model come within 0.001 dB. It can also be seen that the UTD method is 
indeed accurate down to f low,UTD =140 Hz. Figure 7 shows results for the double-edge wedge in Fig. 5(b), 
to­gether with single-edge diffraction for the corresponding in­.nitely thin wedge. Comparing these results 
with Fig. 9 in Ref. 3, it can be seen that the results with the new method and the BTM results are very 
close to each other. As is also observable for the double-edge plates in Ref. 3, the differ­ences between 
the thin plate limit and the low-frequency results decrease with decreasing thicknesses, yet a small 
but signi.cant difference always remains. This remaining differ­ence is probably caused by the lack of 
higher-order diffrac­tion components in these calculations. It can also be noted   FIG. 8. The geometry 
for the axisymmetric scattering from a thin, rigid circular disc of radius a, with colocated source and 
receiver at height d. that this new model and the BTM model give very similar results for double-edge 
diffraction for a geometry with par­allel long edges, a happenstance that does not display well the signi.cant 
differences in the two theories. B. On-axis scattering from a rigid circular disc The cases of on-axis 
scattering from rigid and soft cir­cular discs was studied in Ref. 11, employing both an accu­rate, frequency-domain 
T-matrix formalism, and time­domain expressions using the WA method. The accurate frequency-domain expressions 
were transformed into IRs us­ing the inverse discrete Fourier transform, and adjusting the sampling frequencies 
to the geometry so that delays corre­sponded to exact integer sample numbers, which should en­sure that 
accurate time-domain expressions were retrieved. Thus, these could be seen as accurate reference cases 
against which the present method can be compared. Nevertheless, although the possible effects of windowing 
on the time sig­nal were mentioned in Ref. 11, no estimate was given. The .rst-order diffraction is a 
single pulse, for which it should in principle be possible to accurately .nd the amplitude. Since second-and 
higher-order diffraction components are continuous-time signals with onsets exactly at the arrival of 
the .rst-order impulse, these higher-order components will inevitably in.uence the sample in which the 
impulse arrives. This effect is small since the onset amplitude of the higher­order diffraction is very 
small, but the accumulated effect is not clear. To derive analytical expressions for the diffraction 
IR, the formulation in Eq. (21) is used. The f function must be found for the case with the source and 
receiver colocated, as illustrated in Fig. 8. In this case, the angles OS and OR are equal and consequently, 
both will be referred to as O. The f function can be derived using the form of cosh(v ) which is valid 
for the wedge index v=0.5, as shown in Appendix C, cos[(a-Y)/2] cosh = (42) 2 (cos a cos Y)1/2 , which, 
as is also shown in Appendix C, leads to cos[(OS+OR )/2]f=2 cosh 2 cosh2( /2)-sin2[(OS+OR )/2] cos[(OS 
-OR )/2] + }. (43) cosh2( /2)-sin2[(OS -OR )/2]  When OS equals OR and both are denoted O, f can be 
written 2338 J. Acoust. Soc. Am., Vol. 106, No. 5, November 1999 Svensson et al.: Secondary source model 
of edge diffraction 2338 TABLE I. Amplitude of the .rst-order diffraction component for axisymmetric 
scattering from a circular rigid disc of radius 1 m, normalized to the specular re.ection. Results are 
either calculated using the new method, Eq. (47), or taken from Ref. 11. The errors are relative to the 
T-matrix solution which is considered as the reference result. Height Rel. error Rel. error [m] T matrix 
(Ref. 11) WA (Ref. 11)[%] Eq. (47)[%] 1.1 -1.221 -1.223 (-0.16) 1.238 (-1.4) 1.5 -1.273 1.247 (2.0) -1.294 
(-1.6) 3.0 -1.243 -1.252 (-0.7) -1.249 (-0.5) 5.0 -1.163 -1.118 (3.9) -1.173 (-0.9) 10.0 -1.093 -1.099 
(-0.5) -1.094 (-0.1) 21 f=+cos O cosh( /2) cosh( /2)+sin O 1 + . (44) cosh( /2)-sin O Furthermore, the 
cosh( /2) factor is simpli.ed for the axi­symmetric case that is studied here. For the entire circular 
edge, the incident ray angles a and the reradiated ray angles Y will have the value 0, and then the quantity 
cosh( /2) =1, as is clear from Eq. (42). The f function in Eq. (44) can consequently be further simpli.ed 
as 11 f=2+cos O. + . 1+sin O 1-sin O 1 a+l =2. 1+ .=2, (45) cos O a 2+d2)1/2 where a is the radius 
of the disc and l=(a, d being the height of the source and receiver above the disc. Inserting this constant 
value of the f term into Eq. (21), together with the constant values of m and l, it is found that if 
the z-coordinate runs along the disc perimeter, a+l 27a 2l pdiffr(t)=­ q. t-.dz 47al20 c a+l 2l =-q. 
t-., (46) 2l2 c where it can be noted that pdiffr(t) is nothing but a scaled and delayed version of the 
source signal. In other words, a+l 2l hdiffr(7)=-b7-. (47) 2l2 c The .rst-order diffraction for a soft 
disc can be found by employing the expression in Eq. (5) and repeating the deri­vations above, which 
yields a-l 2l hdiffr,soft(7)=-(48) b. 7-.. 2l2 c With Eqs. (47) and (48), the value of the .rst-order 
diffrac­tion component can be calculated directly as a Dirac impulse amplitude. In Tables I and II, the 
values given by Eqs. (47) and (48) are, respectively, compared with corresponding val­ues from Ref. 11, 
when normalized to the specular re.ection for the disc, 1/(2d). The results, while not identical, differ 
at most by 2.3%, or 0.20 dB, from the reference T-matrix re­sults. The sign of the error is always negative 
for the rigid disc and positive for the soft disc so that, if the second-order diffraction component 
contributes within the same sample as the .rst-order component, the error would decrease, if just marginally. 
The results with the WA method, as reproduced from Ref. 11, give errors of the same magnitude as those 
given by Eqs. (47) and (48) but with a larger spread, because of varying signs of the errors. An explicit 
expression for second-order diffraction can be derived as well, and it is done here for the rigid disc 
only. The expression in Eq. (22) can be used if the two z-coordinates run along the disc perimeter, from 
z=0to z =27a, so that 127am1+m2(z1,z2 )+l2 pdiffr(t)=2 qt­ (87)2 c 0 f[a1,Y1(z1,z2),OS1,0]f[a2(z1,z2),Y2,0,OR2] 
X 2m1m2(z1,z2 )l2 Xdz1dz2. (49) Here, the parameters m1 and l2 are constant and will be denoted l; OS1 
and OR2 are identical and will be denoted O; and a1 and Y2 are both equal to zero. The initial factor 
2 re.ects the fact that for a thin plate, there will always be identical diffraction components via the 
rear side of the plate which contribute to double the second-order diffraction am­plitude. Furthermore, 
because of rotational symmetry, one of the integrations can be replaced by a factor 27a, and any .xed 
value of z1 can be used in evaluating the z2-integration. Then, with z1=0, TABLE II. Amplitude of the 
.rst-order diffraction component for axisym­metric scattering from a circular soft disc of radius 1 m, 
normalized to the specular re.ection. Results are either calculated using the new method, Eq. (48), or 
taken from Ref. 11. The errors are relative to the T-matrix solution which is considered as the reference 
result. Height T matrix WA Rel. error Rel. error [m](Ref. 11)(Ref. 11)[%] Eq. (48)[%] 1.1 0.239 0.236 
(-1.3) 0.242 (1.3) 1.5 0.364 0.366 (0.5) 0.371 (1.8) 3.0 0.634 0.642 (1.3) 0.649 (2.3) 5.0 0.777 0.782 
(0.6) 0.789 (1.5) 10.0 0.890 0.880 (-1.1) 0.896 (0.7)  2339 J. Acoust. Soc. Am., Vol. 106, No. 5, 
November 1999 Svensson et al.: Secondary source model of edge diffraction 2339  FIG. 9. Sound paths 
for second-order diffraction for the circular disc, indi­cating that the incident sound path to the .rst 
edge has a constant length l and a constant incidence angle a1 =0. Also, the reradiated sound path from 
the second edge point has a constant length l and a constant angle Y2 =0. The intermediate sound path 
has the length m2 , and the two angles Y1 and a2 have the same value. a 27am2(0,z2)+2l pdiffr(t)= qt­ 
327l20 c f[0,Y1(0,z2 ),O,0]f[a2(0,z2 ),0,0,O]X dz2. m2(0,z2) (50) Because of symmetry around z2=7a, 
the upper integration limit can be halved, and the result doubled instead. Further­more, a variable substitution 
with (m2+2l)/c=7 leads to 7a a pdiffr(t)= q(t-7) 167l20 f[0,Y1(7),O,0]f[a2(7),0,0,O] dz2 X d7, (51) 
m2(7) d7 and, as before, the factor dz2/d7 can be de.ned as being zero before 7=2l/c and after 7=2(l+a)/c, 
so that the in­tegration limits can be expanded to , and a convolution integral can be identi.ed. The 
impulse response is then a f[0,Y1(7),O,0]f[a2(7),0,0,O] dz2hdiffr,2nd(7) = . 167l2 m2(7) d7 (52) The 
values of m2 , the angles Y1 and a2 , and dz2/d7 can be found by inspecting Fig. 9. Thus, m2=c7-2l, (53) 
m2 cos Y1=cos a2= , (54) 2a m2 dz2 c z2=a(7-2Y1 )=a. 7-2 cos-1 .. = . (55) 2ad7 sin Y1 By employing 
the expression in Eq. (43), the product of the f functions in Eq. (52) can be molded into the form f[0,Y1(7),O,0]f[a2(7),0,0,O] 
-2 OO =16 cos2 cosh2 . cosh2 -sin2 ., (56) 22 22 where the fact that cosh( /2) is identical for the 
two edges has been used. Using Eq. (42) and the fact that a1=Y2=0, the function cosh2( /2) can be simpli.ed 
to  FIG. 10. Transfer function for the case in Fig. 8, with a=1 m and d =3 m, calculated with the new 
method. The specular re.ection plus .rst­and second-order diffraction is included, and the amplitude 
is normalized to the specular re.ection. The .ne structure in this graph agrees with the de­tails in 
Fig. 6 of Ref. 7. 1+cos Y1 cosh2 = , (57) 2 2 cos Y1 which is valid for both the .rst and second edges, 
as Y1 =a2 . If the expression in Eq. (57), together with expres­sions for sin2(O/2) and cos2(O/2), are 
inserted into Eq. (56), which is then used in Eq. (52), an expression for hdiffr,2nd(7) results c[1+cos 
Y1(7)](1+cos O)hdiffr,2nd(7) = 27l2 sin Y1[1+cos O cos Y1(7)]2 w(7) 1/2 c(1+cos O) 1+cos Y1(7) = 27l2[1+cos 
O cos Y1(7)]21-cos Y1(7) Xw(7), (58) where cos Y1(7) can be found from Eqs. (53) and (54), cos O=a/l, 
and w(7)=H(7-2l/c)-H(7-2(a+l)/c). In­cluding the .rst-order diffraction, Eq. (47), and second-order diffraction, 
Eq. (58), IRs were calculated and transformed into transfer functions. Figure 10 presents the resulting 
trans­fer function for the case with a=1 m and d=3 m. Calcula­tion parameters were fs =78 569 Hz and 
a DFT size of 2048 was used. These results are very similar to those in Fig. 6 in Ref. 7. Figure 11(a) 
shows the second-order diffraction IR when normalized to the specular re.ection amplitude 1/(2d). In 
Ref. 11, the amplitude of the second-order diffraction was presented in terms of the peak value of the 
function illustrated in Fig. 11(a). This peak value is critically depen­dent on the sampling frequency 
and the low-pass .ltering technique used. For purposes of comparison, the process in Ref. 11 was reproduced 
as closely as possible. The sampling frequency was chosen so that the delay between the specular re.ection 
and the .rst-order diffraction pulse was an integer number of steps, using a speed of sound of 1500 m/s. 
Fur­thermore, an oversampling by a factor of 8 was used here and, after transforming the IR into a transfer 
function using a DFT size of 16 384, only the .rst 1024 frequencies were kept before transforming back 
to an IR using a DFT size of 2048.  2340 J. Acoust. Soc. Am., Vol. 106, No. 5, November 1999 Svensson 
et al.: Secondary source model of edge diffraction 2340 This frequency-domain low-pass .ltering should 
be equiva­lent to the technique used in Ref. 11. Figure 11(b) shows an example of one such second-order 
diffraction IR, where the small ripple indicates the .ltering effects. Table III gives the results for 
the peak amplitude of the second-order diffraction IR, both when calculated using edge sources as in 
Eq. (50), and after the oversampling/frequency­domain .ltering described above. Although the results 
from Ref. 11 are reproduced here, it should be noted that it is stated in Ref. 11 that the values in 
their Table IV are nor­malized to the .rst-order diffraction pulse rather than to the specular re.ection, 
which is used elsewhere in that paper. We believe that this is a mistake for three reasons. First, the 
values in Table IV of Ref. 11 are positive rather than nega­tive. Second, it is stated earlier in Ref. 
11 that the specular re.ection was used for normalization for all the results pre­sented, even for off-axis 
geometries. The third argument is that our results are very close to the reference results when the specular 
normalization is assumed, and especially after the frequency-domain low-pass .ltering was used. The results 
with our new method are close enough to the reference results for the circular disc to generally support 
the relations derived here, but more comparisons with reference calculations should be made. Comparisons 
of the frequency­domain values might then be easier to carry out, avoiding ambiguities involving .ltering 
effects as discussed above. C. Scattering from a rectangular plate The last example is scattering from 
a rectangular plate. Cox and Lam present an example of a rectangular plate of size 0.302X1.92X0.010 m 
and with calculated directivity plots at two single frequencies, 2012 and 3995 Hz.19 The source was positioned 
at a height of d=3.96 m, right above the center point of the plate. The receiver was moved along an arc 
at a constant radius of R=1.178 m, from the center points of the plate. The arc was in the direction 
of the short length of the plate. The total re.ection strength was calcu­lated using the boundary element 
method both for a three­dimensional model and for a thin plate limit model of the plate. It was normalized 
to the direct sound amplitude. Fig­ure 12 shows results using Eqs. (35) and (37) to calculate the .rst-and 
second-order diffraction IRs for an in.nitely thin model of the plate. A sampling frequency of 257 536 
Hz was used, corresponding to 128 times oversampling with respect to the target frequency of 2012 Hz. 
The four edges were divided into 0.67-mm elements for the .rst-order diffraction, and twice that size 
for the second-order diffraction. The re­ceiver was moved along the arc in steps of 1 deg. A DFT size 
of 8192 was used, to get the transfer function values exactly at 2012 Hz. The sound speed c=346 m/s was 
used in Ref. 19 and here, too. If one compares the level of the total .eld in Fig. 12 with the results 
for the thin plate limit in Fig. 4 of Ref. 19, one can see good agreement for angles between 0 and ca. 
80 deg. The larger deviations above 80 deg are probably due to the need for higher-order diffraction 
components. This is TABLE III. Peak amplitude of the second-order diffraction component for axisymmetric 
scattering from a circular rigid disc of radius 1 m, normalized to the specular re.ection. Results are 
either calculated using the new method [based on Eq. (50)] or taken from Ref. 11. The results denoted 
LPF have been low-pass .ltered as described in the text. The errors are relative to the T-matrix solution 
which is considered as the reference results. The T matrix and WA results were speci.ed in Ref. 11 as 
being relative to the .rst-order diffraction strength, but it has been assumed here that the specular 
re.ection was used for the normalization as discussed in the text. Height T matrix WA Rel. error Rel. 
error Eq. (50) Rel. error [m](Ref. 11)(Ref. 11)[%] Eq. (50)[%](LPF)[%] 1.1 0.043 0.039 (-9) 0.0415 (-3) 
0.0469 (+9) 1.5 0.043 0.031 (-28) 0.0374 (-13) 0.0427 (-1) 3.0 0.031 0.023 (-26) 0.0278 (-10) 0.0293 
(-5) 5.0 0.024 0.016 (-33) 0.0205 (-15) 0.0234 (-3) 10.0 0.014 0.009 (-36) 0.0114 (-19) 0.0126 (-10) 
  2341 J. Acoust. Soc. Am., Vol. 106, No. 5, November 1999 Svensson et al.: Secondary source model of 
edge diffraction 2341 also indicated in Fig. 12 by the large amplitude of the second-order diffraction 
component for large receiver angles. The deviation at the dip around 30 deg might be caused by a less 
dense sampling in Ref. 19, or small differences in the speed of sound. Furthermore, the smoothness of 
the total .eld around 10 12 deg, where the specular re.ection disap­pears, indicates that the numerical 
method used, Eq. (35),is accurate enough up to within half a deg from the transition zone where the specular 
re.ection disappears. This is com­pensated by the corresponding jump in value for the .rst­order diffraction. 
Very high oversampling was used, together with a very .ne division of the edge into elements. This is 
needed only for the positions close to the transition zone and for the largest receiver angles. For the 
larger receiver angles, higher-order diffraction is needed anyway, so the total .eld might be calculated 
more accurately by using fewer edge elements but higher orders of diffraction. A three-dimensional model 
of the plate was tested as well, but it was then clear that it was necessary to include higher-order 
diffraction since the results differed signi.­cantly from the thin plate limit model, when second-order 
edge diffraction was included. In conclusion, the results with the new model and the results in Ref. 
19 seem to agree well enough to support the new model as long as it is realized that it may be necessary 
to include higher-order diffraction com­ponents for some situations. IV. DISCUSSION AND CONCLUSIONS The 
interpretation of the exact Biot Tolstoy solution for the in.nite wedge diffraction that was presented 
here has not been proven to be true per se. It has, however, been shown that if the existence of analytical 
directivity functions for the secondary edge sources is assumed, such functions can in­deed be derived 
and yield the exact solution for an in.nite wedge. Since this also leads to a possible application to 
non­straight edges, and the comparison with the result for the circular rigid disc was accurate within 
0.20 dB, it is con­cluded that the suggested interpretation is generally valid. It should then, in principle, 
be possible to show that the total 2342 J. Acoust. Soc. Am., Vol. 106, No. 5, November 1999 .eld, specular 
re.ections plus diffraction components, satis­.es the wave equation. The complexity of the higher-order 
diffraction components might, however, prevent this possi­bility for cases other than the in.nite wedge. 
The proposed model gives results that, for .rst-order diffraction, should be identical to those by the 
WA model.11 For second-order diffraction, the results by the proposed model are very similar to results 
by methods which are based on Medwin s model.3,11 Conceptually, however, the differ­ence between Medwin 
s model and the proposed model is signi.cant since the proposed model gives a complete moti­vation and 
mathematical proof for the directivity functions of the secondary edge sources. As for the numerical 
implementation, very high sam­pling frequencies are often needed. This is because the crude low-pass 
 antialiasing .lter, which is equivalent to the single time sample integration, has a gentle roll-off 
charac­teristic. Re.ned integration techniques could be tested, and the directivity functions might lead 
to somewhat simpler functions to handle, compared to the original Biot Tolstoy solution in Eqs. (1) (4). 
Also, since each higher order of diffraction causes a response that basically falls 3 dB/octave more 
quickly than the previous order, lower and lower sam­pling frequencies could be used for each new order. 
This decrease occurs, however, above a cutoff frequency which is given by the size of the individual 
planes of the object, and the proximity to the various shadow zones. Below those fre­quencies, the response 
decreases in a way which is more dif.cult to predict. Higher-order diffraction IRs should tend towards 
Gaussian-shaped pulses, according to the central limit theo­rem in statistics, since it is equivalent 
to convolving any function with itself many times, resulting essentially in a Gaussian function. Thus, 
properly time-aligned Gaussian pulses with the right areas, widths, and polarities could serve as replacement 
functions for higher-order diffraction. It was shown that the case of axisymmetric backscatter­ing from 
a circular disc could yield explicit expressions for the .rst-and second-order diffractions. It is probably 
pos­sible to .nd such explicit expressions for several other ge­ometries too. Further developments could 
lead to solutions which satisfy more general boundary conditions. The proposed model is relevant for 
all cases where scattering/diffraction from idealized, rigid, or soft objects is studied, such as when 
the WA is applied to underwater acoustics cases, noise barriers, etc. In architectural acoustics, most 
IR prediction models are based on geometrical acous­tics, possibly handling surface scattering, but without 
the possibility to handle edge diffraction accurately.20 The Kirchhoff diffraction approximation has 
been tested before, but this new model should be much more accurate since it is valid at all frequencies 
and for all source and receiver positions.10 In electroacoustics, edge diffraction has impor­tant applications 
such as the effect of the loudspeaker enclo­sure on the radiated sound; many simpler edge diffraction 
models have been tested,16 but the proposed model should, here too, prove to be more accurate. In conclusion, 
deriving analytical directivity functions for the edge sources both supports, and takes a step beyond, 
Svensson et al.: Secondary source model of edge diffraction 2342   previous models such as those based 
on Medwin s work, and gives new possibilities to solving diffraction problems in the time domain. These 
directivity functions support some pre­vious .rst-order diffraction models, but also demonstrate that 
previous second-order diffraction models contain approxima­tions, re.ecting the greater accuracy of the 
proposed model. Derivation of explicit diffraction expressions for certain ge­ometries, and for ef.cient 
and accurate numerical calcula­tions, become possible using the proposed model. ACKNOWLEDGMENTS This 
project was .nanced in part by grants from the Swedish Institute and the Adlerbert Research Foundation, 
which supported the .rst author s research visit to the Uni­versity of Waterloo. Parts of the project 
were also .nanced by the Johnson Foundation. The third author has research support from the Natural Sciences 
and Engineering Research Council of Canada. The authors have appreciated the com­ments offered by an 
anonymous reviewer. APPENDIX A: PROOF OF EQ. 14 The factor c(rSrR sinh )-1 H(7-70)inEq. (1) can be rewritten, 
introducing an auxiliary variable y, 21)1/2], =cosh-1 y=log[y+(y-(A1) so that sinh can be written 21)1/2 
sinh =(y-. (A2) The auxiliary variable y introduced above is [see Eq. (4)] 272222 c-(rS+rR+zR) y= , (A3) 
2rSrR and with the relation c7=m+l, y can be written 22 2 (m+l)2-(rS+rR+zR ) y= . (A4) 2rSrR By further 
using the relations for m and l (see Figs. 1 and 2) 22 )1/2 m=(rS+z, (A5) and zR )2]1/2 l=[rR 2 +(z-, 
(A6) it is possible to write y as ml+z(z-zR) y= . (A7) rSrR Now, using Eqs. (A2) and (A7), it can be 
found that 22 .1/2 rSrR sinh =.[ml+z(z-zR)]2-rSrR 2l22(z22 ]1/2 =[m+z-zR )2+2mlz(z-zR)-rSrR . (A8) By 
further employing Eqs. (A5) and (A6) to get rid of rS and rR , it is found that 2l22]1/2 rRrR sinh =[z+2mlz(z-zR)+(z-zR)2m 
=.zl+(z-zR )m.. (A9) 2343 J. Acoust. Soc. Am., Vol. 106, No. 5, November 1999 All expressions so far 
have been valid for both the upper and lower branch solutions of m and l. Moving temporarily to the expressions 
for dz/d7, this quantity can be found by study­ing the total path length L=m+l. A small increase in path 
length aL corresponds to a small increase, az, along the edge, and the relation between these is found 
by the deriva­tive of L(z) dL z z-zR zl+(z-zR)m =+ = , (A10) dzml ml which, using aL=ca7, leads to 1 
dz c = , (A11) ml d7 zl+(z-zR)m for both the upper and lower branches of the edge, which is the left-hand 
relation in Eq. (14). Since dzu /d7 is always positive, Eq. (A11) implies that zulu+(zu -zR)mu also must 
always be positive. Then, Eqs. (A9) and (A11) show that the right-hand relation in Eq. (14) holds true. 
APPENDIX B: PROOF OF EQ. 16 The relation in Eq. (16) is shown using the de.nitions of the angles a and 
Y. sin a=z/m, cos a=rS /m, (B1) sin Y=(z-zR )/l, cos Y=rR /l. (B2) If these are inserted into Eq. (A7), 
z(z-zR ) 1+ ml+z(z-zR) ml 1+sin a sin Y y= == , rSrR rSrR cos a cos Y ml (B3) which is the relation 
in Eq. (16). APPENDIX C: PROOFS OF EQS. 42 AND 43 For the thin plate case, the wedge index v equals 0.5, 
and then the f term in Eq. (1) can be simpli.ed. The quantity cosh(. ) is, for the wedge index v=0.5, 
1 -1 exp()+1 cosh = exp +exp = (C1) ..)]1/2 . 222 22 [exp( Squaring this expression gives 1 exp(2 )+2 
exp()+1 cosh2 = . (C2) 2 4 exp() The expression for in Eq. (A1) can be employed, which readily leads 
to the simpli.cation y+1 cosh2 = . (C3) 22 The auxiliary quantity y can then be written as in Eq. (B3), 
which yields Svensson et al.: Secondary source model of edge diffraction 2343   1+sin a sin Y+cos a 
cos Y 1+cos(a-Y)cosh2 == 2 2 cos a cos Y 2 cos a cos Y cos2[(a-Y)/2] = . (C4) cos a cos Y Then, cosh( 
/2) can .nally be expressed as cos[(a-Y)/2] cosh = (C5) 2 (cos a cos Y)1/2 , which is the form in Eq. 
(42). Taking the square root of the right-hand term in Eq. (C4) is safe, as a and Y are always within 
the range -7/2 to 7/2. Furthermore, the sine and cosine terms in the expression for f in Eq. (1) can 
be sim­pli.ed, since sin[v(7 OS OR)]=sin.[7 (OS OR )]/2. =cos[(OS OR )/2], (C6) cos[v(7 OS OR)]=cos.[7 
(OS OR)]/2. = sin[(OS OR )/2], (C7) and allows the f term to be written as cos[(OS+OR )/2]f(a,Y,OS ,OR 
)= cosh( /2)+sin[(OS+OR )/2] cos[(OS -OR)/2] + cosh( /2)+sin[(OS -OR)/2] cos[(OS -OR)/2] + cosh( /2)-sin[(OS 
-OR)/2] cos[(OS+OR)/2] + cosh( /2)-sin[(OS+OR)/2]  =2 cosh( /2) cos[(OS+OR )/2]X cosh2( /2)-sin2[(OS+OR)/2] 
cos[(OS -OR)/2] + }, cosh2( /2)-sin2[(OS -OR )/2](C8) where cosh( /2) is given in Eq. (C5), and this 
is the form in Eq. (43). 1M. A. Biot and I. Tolstoy, Formulation of wave propagation in in.nite media 
by normal coordinates with an application to diffraction, J. Acoust. Soc. Am. 29, 381 391 (1957). 2H. 
Medwin, Shadowing by .nite noise barriers, J. Acoust. Soc. Am. 69, 1060 64 (1981). 3H. Medwin, E. Childs, 
and G. M. Jebsen, Impulse studies of double diffraction: A discrete Huygens interpretation, J. Acoust. 
Soc. Am. 72, 1005 1013 (1982). 4J. P. Chambers and Y. H. Berthelot, Time-domain experiments on the diffraction 
of sound by a step discontinuity, J. Acoust. Soc. Am. 96, 1887 1892 (1994). 5D. Ouis, Scattering by 
thin strip-like elements and applications in room acoustics, Dissertation, Report TVBA-1005, Lund University 
of Tech­nology, Lund, Sweden (1995). 6A. W. Trorey, Diffractions for arbitrary source receiver locations, 
 Geophysics 42, 1177 1182 (1977). 7J. R. Berryhill, Diffraction response for nonzero separation of source 
and receiver, Geophysics 42, 1158 1176 (1977). 8Y. Sakurai and K. Nagata, Sound re.ections of a rigid 
plane and of the live end composed by those panels, J. Acoust. Soc. Jpn. (E) 2, 5 14 (1981). 9G. M. 
Jebsen and H. Medwin, On the failure of the Kirchhoff assump­tion in backscatter, J. Acoust. Soc. Am. 
72, 1607 11 (1982). 10G. V. Norton, J. C. Novarini, and R. S. Keiffer, An evaluation of the Kirchhoff 
approximation in predicting the axial impulse response of hard and soft disks, J. Acoust. Soc. Am. 93, 
3049 3056 (1993). 11R. S. Keiffer, J. C. Novarini, and G. V. Norton, The impulse response of an aperture: 
Numerical calculations within the framework of the wedge assemblage method, J. Acoust. Soc. Am. 95, 
3 12 (1994). 12A. D. Pierce, Diffraction of sound around corners and over wide barri­ers, J. Acoust. 
Soc. Am. 55, 941 955 (1974). 13R. G. Kouyoumjian and P. H. Pathak, A uniform geometrical theory of diffraction 
for an edge in a perfectly conducting surface, Proc. IEEE 62, 1448 1461 (1974). 14C. S. Clay and W. 
A. Kinney, Numerical computations of time-domain diffractions from wedges and re.ections from facets, 
 J. Acoust. Soc. Am. 83, 2126 2133 (1988). 15W. A. Kinney, C. S. Clay, and G. A. Sandness, Scattering 
from a cor­rugated surface: Comparison between experiment, Helmholtz Kirchhoff theory, and the facet 
ensemble method, J. Acoust. Soc. Am. 73, 183 194 (1983). 16J. Vanderkooy, A simple theory of cabinet 
edge diffraction, J. Aud. Eng. Soc. 39, 923 933 (1991). 17J. J. Bowman and T. B. A. Senior, Electromagnetic 
and Acoustic Scatter­ing by Simple Shapes, edited by J. J. Bowman, T. B. A. Senior, and P. L. E. Uslenghi 
(North-Holland, Amsterdam, 1969), Chap. 6. 18T. Kawai, Sound diffraction by a many sided barrier or 
pillar, J. Sound Vib. 79, 229 242 (1981). 19T. J. Cox and Y. W. Lam, Evaluation of methods for predicting 
the scattering from simple rigid panels, Appl. Acoust. 40, 123 140 (1993). 20B.-I. Dalenba¨ck, Room 
acoustic prediction based on a uni.ed treatment of diffuse and specular re.ection, J. Acoust. Soc. Am. 
100, 899 909 (1996).  2344 J. Acoust. Soc. Am., Vol. 106, No. 5, November 1999 Svensson et al.: Secondary 
source model of edge diffraction 2344 Interactive Sound Synthesis for Large Scale Environments Nikunj 
Raghuvanshi* Ming C. Lin Department of Computer Science University of North Carolina atChapel Hill Abstract 
We present an interactive approach for generating realistic physically-based sounds from rigid-body dynamic 
simulations.We use spring-mass systems to model each object s local deformation and vibration, which 
we demonstrate to be an adequate approxi­mation for capturing physical effects such as magnitude of impact 
forces, location of impact, and rolling sounds. No assumption is made about the mesh connectivity or 
topology. Surface meshes used for rigid-body dynamic simulation are utilized for sound simu­lation withoutanymodi.cations.Weuse 
resultsin auditory percep­tion and a novel priority-based quality scaling scheme to enable the systemtomeetvariable,stringenttime 
constraintsinareal-timeap­plication, while ensuring minimal reduction in the perceived sound quality. 
With this approach, we have observed up to an order of magnitude speed-up compared to an implementation 
without the acceleration. As a result, we are able to simulate moderately com­plex simulations with upto 
hundreds of sounding objects at over 100 frames per second (FPS), making this technique well suited for 
interactive applications like games and virtual environments. Furthermore, we utilize OpenAL and EAXTM 
on Creative Sound Blaster Audigy 2TM cards for fast hardware-accelerated propaga­tion modeling of the 
synthesized sound. CR Categories: H.5.5 [Information Interfaces and Presenta­tion]: Sound and Music Computing 
Modeling, Methodologies and techniques; I.3.5 [Computer Graphics]: Computational Ge­ometry and Object 
Modeling Physically based modeling; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism 
Animation; Keywords: Sound Synthesis, Rigid-Body Simulation, OpenAL 1 Introduction Most interactiveapplications 
today employrecorded sound clips for providing sounds corresponding to object interactions in a scene. 
Although this approach has the advantage that the sounds are real­istic and the sound-generation processis 
quitefast, there are many physicaleffects which cannotbe capturedby sucha technique.For instance, in 
a typical collision between two objects, the loudness and timbre of the sound is determined by the magnitude 
and lo­cation of the impact forces a plate sounds very differently when struck on the edge compared 
to when it is struck in the middle. Con­sequently, if the collision scenario changes slightly, the sound 
ex­hibitsa corresponding change. Such subtleeffects canadd substan­tial realism to a typical scene by 
avoiding the repetitiveness com­mon to recorded sound clips. However, developing a system which produces 
sound usingphysically-based principlesin real time poses *nikunj@cs.unc.edu lin@cs.unc.edu Copyright 
&#38;#169; 2006 by the Association for Computing Machinery, Inc. Permission to make digital or hard copies 
of part or all of this work for personal or classroom use is granted without fee provided that copies 
are not made or distributed for commercial advantage and that copies bear this notice and the full citation 
on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting 
with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to 
lists, requires prior specific permission and/or a fee. Request permissions from Permissions Dept, ACM 
Inc., fax +1 (212) 869-0481 or e-mail permissions@acm.org. I3D 2006, Redwood City, California, 14 17 
March 2006. &#38;#169; 2006 ACM 1-59593-295-X/06/0003 $5.00 Figure1: Numerous dicefallona three-octave 
xylophonein close succession, playing out the song The Entertainer (see the video). Our algorithm is 
able to produce the corresponding musical tones at more than 500 FPS for this complexscene, with audio 
generation taking 10% of the total CPU time, on a 3.4GHz Pentium-4 Laptop with 1GB RAM. substantial dif.culties. 
The foremost requirement is the presence of an ef.cient dynamics engine which informs the sound system 
of object collisions and the forces involved. For this work, we have developedafast and robust rigid-body 
simulator,but manypresent daygames meetthis requirement.Givena dynamics simulator,the main challenge 
is to synthesize the sound ef.ciently enough to play in real time while taking only a small portion of 
the total running time, which is usually dominated by graphics and rigid-body sim­ulation. Typically 
the sound system can only afford a few hundred CPUcycles per object per sound sample for manyinteractive 
appli­cations. Main Results: In this paper, we present an approach whichmeets the interactive performance 
requirements outlined above, while en­suring high realism and .delity of the sound produced. Given an 
object s geometry and a few material parameters, we construct a spring-mass model approximating the object 
s surface. We show that although a spring-mass system is a coarser approximation than FEM models used 
in prior approaches [Chaigne and Doutaut 1997; O Brien et al. 2001; O Brien et al. 2002], it is an adequate 
model to capture the small-scale surface vibrations that lead to the gener­ation of sound in nature. 
We show how this formulation yields an analytical solution to the equation of motion for the surface 
of the object. However, a naive implementation of such an approach can han­dle only a few (less than 
ten) sounding objects in real time. We also present several acceleration techniques. The increased com­putationalef.ciencyis 
achievedby exploiting auditory perception, which ensures that the resulting degradation in perceived 
quality is minimal. In addition, the sound quality and the associated computa­tional cost for each object 
is scaled dynamically in a priority-based scheme which guarantees that the total sound production meets 
 101  stringent time constraints, while preserving the overall aural expe­rience. Our approach has 
the following characteristics: It is based on a discretized physically-based representation that offers 
simplicity of formulation and ease of implemen­tation;  It makes no assumptions about the input mesh 
topology sur­face meshes used for physics can be used directly for sound synthesis;  It is capable 
of yielding both impact and rolling sounds natu­rally, without anyspecial-case treatment;  It enables 
rich environments consisting of numerous sound­ing objects, with insigni.cant difference in the overall 
audio quality.  We also use OpenAL and EAXTM to provide hardware­accelerated propagation modeling of 
the synthesized sounds on Creative Sound Blaster Audigy2TM audio cards which easily pro­duce spatial 
and environmental sound effects such as distance atten­uation and room acoustics. To the best of our 
knowledge, with the possible exception of methods that rely on physical measurements, no prior work has 
been demonstrated to handle complex scenarios (e.g. see Figs.1and7)in real time. Organization: The rest 
of the paper is organized as follows. We review related work in Section 2. We present the mathematical 
formulation developed to model the surface vibrations for sound synthesisin Section3and describevarious 
acceleration techniques to enable real-time sound generation for a large-scale environment consisting 
of hundreds of sounding objects in Section 4. In Sec­tion 5, we discuss implementation issues and demonstrate 
the re­sults of our system on complex scenes. Finally, we conclude with possible future research directions. 
2 PreviousWork The concept of modeling the surface vibrations of objects using discretizedphysical modelsin 
real timewas .rst proposedby Flo­rens and Cadoz [1991], who used a system of masses and damped springs 
to model 3D shapes and developed theCORDIS-ANIMA system for physically-based sound synthesis. More recently, 
nu­merical integration with a .nite element approach was proposed as a more accurate technique for modeling 
vibrations [Chaigne and Doutaut 1997; O Brien et al. 2001]. These methods had the advan­tage that the 
simulation parameters corresponded directly to physi­cally measurable quantities and the resultswere 
more accurate. The main drawback was the complexity of formulation and implemen­tation and the low speed 
of the resulting simulation. To remedy the performance issue of the above methods, van den Doel andPai 
suggested [1996; 1998] using the analytically com­puted vibrational modes of an object, instead of numerical 
inte­gration, leading to considerable speedups and enabling real-time sound synthesis. But, since the 
PDEs governing the vibration of arbitrary shapes are very complicated, the proposed system could only 
handle simple systems, such as plates, for which the analytical solutions were known. To handle more 
complex systems which do not admit direct analytical solution, two approaches have been pro­posed in 
literature. The .rst approach, [van den Doel et al. 2001] uses physical measurements on a given shape 
to determine its vi­bration modes and their dependence on the point of impact. Later, these modes may 
be appropriately mixed in a real-time application to generate realistic synthetic sounds. But, arbitrary 
3D models have to be physically procured, in order to .nd their aural proper­ties. In [2002], O Brien 
et al. address this problem and propose a method for handling arbitrarily-shaped objectsbydiscretizing 
them into tetrahedral volume elements. Theyshow that the correspond­ing .nite element equations can be 
solved analytically after suitable approximations. Consequently, they are able to model arbitrarily shaped 
objects and simulate realistic sounds for a few objects at interactive rates. Our work shares some common 
themes with [O Brien et al. 2002]. However, we propose a simpler system of point-masses and damped springs 
for modeling the surface vibrations of the object and it also submits to an analytical solution in a 
similar fashion, while offering much greater simplicity of formulation and ease of implementation. Furthermore, 
the complexity of scenes demon­strated in [O Brien et al. 2002] is low, containing less than 10 sounding 
objects and the interactions captured are mainly due to impacts. As we will demonstrate in Section 5, 
our method extends to handling hundreds of objects in real time and is also capable of producing realistic 
rolling sounds in addition to impact sounds. Often, immersive environments are both visually and aurally 
complex. The problem of scheduling multiple objects for sound synthesiswas.rst addressedin[Fouadetal.1997].Theyexploited 
a model of imprecise computation proposed previously in [Chung et al. 1987], and proposed a system in 
which the objects are iter­atively assigned time quotas depending on the availability of re­sources and 
priorities of the objects. As described in Section 4, our approachto prioritizationand time-quota assignmentexploits 
prop­erties speci.c to our sound-generation technique, and thus achieves better results usinga much simpler 
scheme. Recently,van den Doel et al. [2004] proposed techniques to synthesize sounds in real time for 
sceneswithafewsoundingrigid bodiesand numerous particles, byexploiting frequencymasking.At runtime,they.nd 
emittedfre­quencies which are masked out by other neighboring frequencies with higher amplitude anddo 
not mix the masked frequencies.We useadifferentperceptual observation presentedin[Sekand Moore 1995], 
which report that humans are incapable of distinguishing frequencies that are very close to each other. 
As we will discuss in Section4,thiscanbeusedtopruneoutfrequenciesfromanob­ject s frequencyspectrum as 
a pre-processing step. Our technique leads to better performance and much lesser memory consumption at 
runtime while ensuring minimal loss in auditory quality. 3 Methodology Sound is produced by surface vibrations 
of an elastic object un­der an external impulse. These vibrations disturb the surrounding medium to result 
in a pressure wave which travels outwards from theobject.Ifthe frequencyofthis pressurewaveis withintherange 
20to 22000Hz,itis sensedbythe eartogiveusthe subjectiveper­ception of sound. The most accurate method 
for modeling these surface vibrations is to directly apply classical mechanics to the problem, while 
treating the object as a continuous (as opposed to discrete) entity. This results in PDEs for which analytical 
solutions are not known for arbitrary shapes. Thus, the only avenue left is to make suitable discrete 
approximations of the problem to reduce the PDEs to ODEs, which are more amenable to analysis. In this 
sec­tion, we show how a spring-mass system corresponding to a physi­cal object may be constructed to 
model its surface deformation and how it may be analyzed to extract the object s modes of vibration. 
For ease of illustration, we assume a homogeneous object; inho­mogeneous objects may be handled by a 
simple extension of the approach presented here. Further, we assume that the input object is in the form 
of a thin shell and is hollow inside. This assumption ismotivatedbypractical concernssincemostofthe geometrytoday 
is modeled for rendering and is invariably only a surface represen­tation with no guarantees on surface 
connectivity. If a volumetric model is available, the approach outlined in this paper applies with minor 
modi.cations. Figure2gives anoverviewof our approach. 3.1 Input Processing Given an input mesh consisting 
of vertices and edges, we construct an equivalent spring-mass system by replacing the mesh vertices with 
point masses and the edges with damped springs. We now  102   Figure 2: This diagram gives an overview 
of our approach. In the pre-processing step, each input surface mesh is converted to a spring-mass systemby 
replacing the meshvertices withpoint masses and the edges with springs,and the force matrices are diagonalized 
to yield its characteristic mode frequencies and damping parameters. At runtime, the rigid-body simulator 
reports the force impulses fi on a collision event. These are transformed into the modegains, gi with 
which the corresponding modes are excited. These yield damped sinusoids which are suitably combined to 
yield the output sound signal. discuss how to assign the spring constants and masses based on the material 
propertiesof the object so that thediscrete system closely approximates the physical object. The spring 
constant, k and the particle masses, mi aregiven by: k = Yt mi = .tai (1) where Y is theYoung s Modulus 
of elasticity for the material, t is the thickness of the object surface, . is the material density and 
ai is the area covered bya particle, whichis calculatedbydividingthe areaof each meshface equally amongstallits 
constituentvertices and summing all theface contributions for thevertexcorresponding to the mass in consideration. 
Note that we did not discuss .xing the spring damping parameters above, which we will return to shortly. 
3.2 Deformation Modeling Once the particle system has been constructed as above, we need to solve its 
equation of motion in order to generate the correspond­ing sound. Unfortunately, the resulting system 
of equations is still mathematically complexbecause the interaction forces between the masses are non-linear 
in their positions. However, by making the reasonable assumption that the deformation is small and linearizing 
about the rest positions, this problem can be cast in the form of a coupled linear system of ODEs: d2 
Mr +(.M + .K) dr + Kr = f (2)dt2 dt where M is the mass matrix, K is the elastic force matrix, . and 
. are the .uid and viscoelastic damping constants for the material respectively. The matrix M is diagonal 
with entries on the diagonal corresponding to the particle masses, mi. The elastic force matrix K is 
real symmetric, with entries relating two particles if and only if theyare connectedbyaspring. Thevariable 
r is the displacement vector of the particles with respect to their rest position and f is the forcevector. 
Intuitively,the termsintheaboveequation correspond to inertia, damping, elasticity and external force 
respectively. The speci.c form of damping used above, which expresses the overall damping matrix as a 
linear combination of K and M is known as Raleigh damping and works well in practice. For a system with 
N particles in three dimensional space, the dimensions of all the matrices above is 3N×3N. This formulation 
of the problem is well known and is similar to the one presented in [O Brien et al. 2002]. The main difference 
in our approach is that the force and inertia matrices are assembled fromaspring-mass system which makes 
the formulation much sim­pler. The solutionto Equation(2) canbe obtainedby diagonalizing K so that: K 
= GDG-1 (3) where G is a real matrix consisting of the eigenvectors of K and D isadiagonal matrix containing 
the eigenvalues.For reasons we will explain later, we will henceforth call G the gain matrix . Plugging 
the above expression for K into Equation (2) and multiplying by G-1 throughout, we obtain: d2 r `´ dr 
-1-1-1 -1 GM + .GM + .DG+ DGr = f (4) dt2 dt Observing that since M is diagonal, G-1M = MG-1 and de.n­ing 
z = G-1 r equation(4) may be expressed as: d2 z dz M +(.M + .D)+ Dz = G-1f (5) dt2 dt Since both M and 
D in the above equation are diagonal, Equa­tion(2)hasbeen decoupledintoasetof unrelateddifferentialequa­tions 
in the variables zi, which correspond to individual modes of vibration of the object. The equation for 
each mode is the standard equation of a damped oscillator and has the following solution for the i th 
mode: + - .t .t zi(t)= cie i + cie i q - (..i + .) ± (..i + .)2 - 4.i .i ± = (6) 2 where the constant 
ci, called the gain for the mode, is found by considering the impulses applied as we will discuss shortly.Weuse 
ci to denote the complexconjugate of ci. The constant .i is the i th eigenvalue in the diagonal matrix, 
D. The real part of .i ± gives the damping coef.cient for the mode, while the imaginary part, if any, 
gives the angular frequencyof the mode. 3.3 Handling ImpulsiveForces Once an input mesh has been processed 
as above and the corre­sponding modesextractedas outlinedin Equations (2)-(6),wehave all the information 
needed regarding the aural properties of the ob­ject. The sound produced by an object is governed by 
the mag­nitude and location of impulsive force on its surface. We model  103  Figure 3: This plot 
shows frequency discrimination in humans as a function of the center frequency. Note that the human capacity 
to discriminate between frequencies degrades considerably for fre­quenciesinthe range 2-22KHz, which 
formsabulkofthe human audible range.We use thisfact to guarantee that no more than 1000 modes need to 
be mixed for anyobject in the worst case, irrespec­tive of its geometric complexity. In most cases the 
actual number is much smaller, in the range of a few hundreds. The red curve shows the piecewise linear 
approximation of this curve that we use. may estimate the running time of the system as follows: A sim­short-duration 
impulsive contacts by dirac-delta functions. Given an impulse vector f containing the impulses applied 
to each vertex G-1 of an object, we compute the transformed impulse, g = f in order to evaluate the 
right-hand side of Equation (5). Once this is done, the equation for the i th modeisgivenby: d2 zi dzi 
mi +(.mi + ..i)+ .izi = gid(t - t0) (7) dt2 dt where t0 is the time of collision and d() is the dirac 
delta function. Integrating the above equation from a time just before t0 to a time Rt just after t0 
and noting that +0 - d(t - t0)dt =1, we obtain: t 0 « dzi mi. +(.mi + ..i).zi + zi.t = gi (8) dt Assuming 
that .t isvery small, and using thefact that the deforma­tion is small compared to the change in velocities, 
we can neglect the last two terms on the left-hand side to obtain: « dzi gi .= (9) dt mi The above gives 
a very simple rule which relates the change in the time derivative of the mode to the transformed impulse. 
Referring to Equation (6) and requiring that zi should stay the same just be­ dzi fore and after the 
collision while dt should increase as in Equa­tion (9), the update rule for the modegain ci can be shown 
to be: ci . ci + ` gi ´ (10) . + .+ - .- . e ii t0 i mi Initially, ci is set to 0 for all modes. 4 Real-time 
Sound Synthesis In this section, we describe how the mathematical formulation pre­sented in the previous 
section is utilized to ef.ciently generate sound in real time. First, we describe a naive implementation 
and then discuss techniques to increase its ef.ciency. Assume that there exists a rigid-body simulator 
which can han­dle all thedynamics. Duringa collision event, the sound system is informed of the object 
that undergoes the impact and the magni­tude and location of impact. This impact is processed as described 
in Section3.3to resultinthegainsforthedifferent modesof vibra­tion of the object, where thegain for the 
i th mode being ci. The equation for a mode from the time of collision onwards is given by (6). The amplitude 
contribution of a mode at any moment is proportional1 to its velocity (and not position). This is because 
the pressure contributionofaparticleis determinedby itsvelocity and the mode velocities are linearly 
related to the physical velocities of the particles. The modevelocityis foundby takinga differentialof 
Equation (6) with respect to time: ple spring-mass system with N particles has 3N modes, and the above 
operation needs to be repeated for each mode for each au­dio sample. Assuming a sampling rate of 44100 
Hz, the number of .oating-point operations (FLOPS) needed for this calculation for generating audio samples 
worth t seconds is: T =3N × 4 × 44100t FLOPS . (12) Consideringthatthe typicalvalueof N is about 5000 
or higher,pro­ducing soundworth1secondwould take 2646 MFLOPS. Since to­day sfastest processors operateatafew 
thousand MFLOPS [Don­garra 2005], the aboveprocessingwould takeaboutasecond. Given thatthis estimated 
amountoftimeisforjustoneobjectandatypical scene may contain manysuch objects, such an approach is clearly 
notfast enough for interactive applications. Furthermore, for many real-time environments such as games 
and virtual environments, only a very small fraction of the actual time can be allocated for sound production. 
Thus, in the rest of this section, we will discuss techniques to increase the ef.ciencyof the proposed 
base system to enhance its capability in handling scenarios with a large number of sounding objects at 
interactive rates. dzi - .i - t From Equation (12), it is clear that the running time is propor­ (11) 
 + = ci.i + e .t + ci. i vi = e i tional to the number of modes being mixed and the number of ob­ dt 
 For generating each audio sample, we need to evaluate the above equation for all vibration modes of 
the object, which is quite inef­.cient. As mentioned in [O Brien et al. 2002], the simple observa­ i.(t+.t) 
i.t tion that e = ee i..t offers somegain in performance since generating a new audio sample just requires 
a single complex multiply with the previous value. However, the ef.ciency is still not suf.cient to handle 
a large number of objects in real time. We 1The constant of proportionality is determined based on the 
geometry ofthe object and takesthefact into accountthat vibrations in the direction of the surface normal 
contribute more to the resulting pressure wave than vibrations perpendicular to the normal. We do not 
describe it in detail here as it is not critical to the approach being presented. jects. Next, we present 
acceleration techniques for sound simula­tion by reducing the number of modes per object: Mode Com­pression 
and ModeTruncation , andby scaling the audio quality of different objects dynamically with little degradation 
in perceived sound quality. 4.1 Mode Compression Humanshavealimited rangeof frequencyperception, ranging 
from 20 to 22000 Hz. It immediately follows that modes with frequen­cies lying outside this range can 
be clipped out and need not be mixed. However, there is another importantfact which can lead to large 
reductions in the number of modes to be mixed. A percep­tual study described in [Sek and Moore 1995] 
shows that humans have a limited capacity to discriminate between nearby frequen­  104  cies. Note 
that this is different from frequencymasking [Zwicker andFastl 1990]in which oneoftwo simultaneously 
played frequen­cies masks outthe other. Rather, this result reports that even if two close enough frequencies 
are played in succession, the listener is unable to tell whether they were two different frequencies 
or the same frequencyplayed out twice. The authors call the length of the interval of frequencies around 
a center frequencywhich sound the same, the Difference Limens to Change (DLC). Figure3shows aplotoftheDLCagainst 
center frequencies rangingfrom.25to8 KHz. Interestingly, the DLC shows a large variation over the audi­ble 
frequencyrange, gettingvery large as the center frequencygoes beyond2KHz. Even at2KHz, the DLC is more 
than1Hz. That is,a human subject cannot tell apart 1999Hz from 2000 Hz. We usetheabovefactto drastically 
reducethe numberofmodes that are mixed for an object. We linearly approximate the DLC curve with a piecewise 
linear curve shown as the red line in Figure 3.The approximationhastwosegments: onefrom20Hzto2KHz and 
another from2KHz to 22 KHz. As we show in the .gure we overestimate the DLC slightly. This increases 
the performance fur­ther and we have observed minimal loss in quality in all the cases we have tested. 
The main idea behind our compression scheme is to group together all the modes with perceptually indistinguishable 
frequencies. It can be easily shown that if the above mentioned lin­ear approximation to the DLC curve 
is used and indistinguishable modes clustered at the corresponding frequencycenters, the maxi­mum number 
of modes that need to be mixed is less than 1000. It is important to note that this is just the worst 
case scenario and it happens only when the frequency spectrum of the object consists of all frequencies 
from 20 to 22,000 Hz, which is very rare. For most objects, the frequencyspectrum is discrete and consequently, 
the number of modes after mode compression is much smaller than 1000, typically in the range of a few 
hundreds. We now describe the details of our technique. Recall thegain matrix from Equation (3), G. Thegain 
matrix hasavery simple physical interpretation: Rows of the matrix correspond to vertices of the object 
and columns correspond to the different modes of vi­bration (with their corresponding frequencies). Each 
row of G lists the gains for the various modes of vibration of the object, when a unit impulse is applied 
on the corresponding vertex. It is clear fromtheabovediscussionthatallthemodegains withinarowof G which 
correspond to modes with close frequencies need to be clus­tered together.Thisis achievedby replacingthegain 
entriesforall suchmodesbyasingleentrywithgainequaltothesumofthe con­stituentgains. Sincea mode corresponds 
toa whole column, this reduces to summing together columns element-wise based on their frequencies. The 
complete procedure is as follows: Sort the columns of G with the corresponding mode frequen­cies as 
thekey. 2  Traverse the modes in increasing order of frequency. Estimate the DLC, . at the current frequencyusing 
the piecewise lin­ear curve shown in Figure 3. If the currentfrequencyand next frequency are within . 
of each other the two mode frequen­cies are indistinguishable, replace the two columns by their element-wise 
sum.  Below, we enumerate the main advantages of this scheme: 1. The running time is constant instead 
of linear in the number of vertices in the object. For example, if the input mesh is complex with 5,000 
vertices, the number of modes mixed is bounded by 1000 instead of the earlier 3N = 15, 000 which isa 
substantial performancegain. 2This step is usually not needed as most linear algebra packages output 
the eigenvector matrix sorted on the eigenvalues Figure 4: This graph shows the number of modes mixed 
vs time, for a xylophone bar just after it is struck in the middle. t is the mode truncation threshold. 
Ahigher value of t leads to more aggressive truncation of modes with lowamplitude, leading to savings 
in terms of the number of modes mixed. In this case, t =2.0 results in about 30%gaininef.ciency over 
t =0.01 which only truncates modes with near-zero amplitude. The sound quality for both the cases is 
nearly identical. 2. Sincethis scheme requiresjustthe frequenciesofthedifferent modes, the whole processing 
can be done as a pre-process without requiring any extra runtime CPU cycles. 3. From the above mentioned 
procedure, it is clear that the num­ber of columns in the matrix G, which is the same as the num­ber 
of modes, is now bounded by 1000 instead of the earlier value of 3N. Since this matrix needs to be present 
in memory at runtime for transforming impulses to modegains, its mem­ory consumption is an important 
issue. Using this technique, for an object with 5000vertices, the memory requirement has been reducedfrom225MBtolessthan15MB,by 
morethan afactor of 15. 4. Most objects have a discrete frequency spectrum with pos­sibly many degenerate 
frequencies. Due to numerical inac­curacies while diagonalizing the elastic force matrix and the approximations 
introduced by the spring-mass discretization, these degenerate frequencies may appear as spurious distinct 
modes with near-equal frequencies. Obviously, it is wasteful to treat these as distinct modes. It is 
our observation that most of the times these modes frequencies are close enough so that they are naturally 
summed together in this scheme.  4.2 ModeTruncation The soundofa typical object on being struck consistsofa 
transient response composed of a blend of high frequencies, followed by a set of lower frequencies with 
low amplitude. The transient attack is essential to the quality of sound as it is perceived as the charac­teristic 
timbre of the object. The idea behind mode truncation is to stop mixing a mode as soon as its contribution 
to the total sound falls belowacertain preset threshold,t . Since mode truncation pre­serves the initial 
transient response of the object when t is suitably set, the resultingdegradationin qualityis minimal. 
Figure4shows a plot of the number of active modes with respect to time for a xy­lophone bar struck in 
the middle for two different values of t: .01 and 2. These values are normalized with respect to the 
maximum sample value which is 65536 for 16-bit audio. The .rst case with  105  t = .01 performs essentially 
no truncation, only deactivating those modes which have near-zero amplitude. Note that with t =2 the 
number of modes mixed is reduced by more than 30%. Also, the number of activemodes .oors offmuch earlier 
(.2 secs compared to .6 secs). It is important to note that this results in little perceptible loss in 
quality. The details of the technique are as follows: Assume that an ob­ject has just undergonea collision 
and the resulting modegains ci have been calculated asgivenbyEquation (10). From this time on­wards until 
the object undergoes another collision, Equation (11) gives a closed-form expression for the mode s contribution 
to the sound of the object. This can be used to predict exactly when 4.4 PuttingEverythingTogether To 
illustrate how all the techniques described above are integrated, we present a summary of our approach. 
Pre-processing Construct a spring-mass system corresponding to the input mesh. (Section 3.1)  Processthe 
spring-mass systemtoextractthegain matrix, G and the (complex) angular frequencies of the object s modes 
 of vibration: . + i . (Section 3.2, Eqns. (3) and (6)) and .- Mode Compression: i cj the mode s contribution 
drops below the threshold t. The required Aggregate columns of G based on frequencies of the corre­ cutofftime 
, t ci is such that for all times t>t ci : sponding modes, as described in Section 4.1. + - - ci. 
+ i e .. i Store the resultinggain matrix along with the (complex) con-Note that .- tt (13) + ci.<t ii 
e i -and .i + i for modes correspond to the columns of stants . Usingthefactthatforanytwocomplexnumbers 
x and y,|x + y|= |x| + |y| it can be shown that, G after compression. need not be stored in . + i = . 
has a non-zero imaginary part since in that . case case . .! .. + i . .- i Runtime Processing Load thegain 
matrix and mode data for each object. + i 2 |ci|t 1 ci (14) = ln -Re(. + i ) t Using the aboveinequality,the 
cutofftimes are calculated for all the modes just after a collision happens. While generating the sound 
samplesfromamode,onlyone.oatingpoint comparisonis needed to test if the current time exceeds the cutofftime 
for the mode. In Begin simulation loop: 1. Run rigid-body simulation case it does, the mode scontribution 
lies below t and consequently, it is not evaluated. 4.3 Quality Scaling The two techniques discussed 
above are aimed at increasing the ef­.ciencyof sound synthesis for a single object. However, when the 
numberof soundingobjectsinascenegrowsbeyondafewtens,this approach is not ef.cient enough to work in real 
time and it is not possible to output the sound for all the objects at thehighest quality. It is critical 
in most interactive applications that the sound system have a graceful way of varying quality in response 
to variable time constraints. We achieve this .exibility by scaling the sound qual­ ity for the objects. 
The sound quality of an object is changed by controlling the number of modes being mixed for synthesizing 
its sound. In most cases of scenes with many sounding objects, the user s attention is on the objects 
in the foreground , that is, the objects which contribute the most to the total sound in terms of am­plitude. 
Therefore, if it is ensured that the foreground sounds are mixed at high quality while the background 
sounds are mixed at a relatively lower quality, the resulting degradation in perceived aural quality 
should be reduced. 2. For each object, O: Collision Handling: If the rigid-body simulator reports that 
O under­goesa collisionevent, update itsgain coef.cients as per Equation (10) using the collision impulse 
and its location. (Section 3.3) ModeTruncation: Compute cutofftimes tfor each mode based on cj the mode 
truncation threshold, t . (Section 4.2, Equation (14)) 3. Quality Scaling: Sort objects based on amplitude 
contribution, assign time-quotas and compute the number of modes to be mixed for each object. (Section 
4.3) 4. Sound Synthesis: For each timestep at timet and for each object, O: Consider all modes permitted 
by the current qual­ ity settingwhich satisfy t>t. Sample and sum­ mate all these modes as described 
at the begin­ ning of this section. This is O s contribution to the sound. Output the sum of all objects 
sample contribution as the sound sample for time t. End simulation loop 5 Implementation and Results 
In this section we present results to demonstrate the ef.ciencyand realism achievable with our approach. 
 5.1 Rigid Body Simulation We use a simple scheme to ensure higher quality for the fore­ ground sounds. 
At the end of each video frame, we store the sum of the vibrational amplitudes of all modes for each 
object, which serve to determine the object s priority. At the next video frame, all objects are sortedin 
decreasing order based on their priority and the total time-quota for sound-generation divided among 
the objects as a linearly decreasing ramp witha preset slope, S. After this, all ob­jectsare processedintheirpriorityorder.Foreachobject,itsquality 
is .rst scaled so that it can .nish within its assigned time-quota and then the required modes are mixedfor 
the given time period. If an object .nishes before its time-quota has expired, the surplus is con­ sumed 
greedily by the next higher priority object. The slope, S of the ramp decides the degree to which the 
foreground sound qual­ityisfavoredoveradegradationin background sound quality. The case with S =0 corresponds 
to no priority scheduling at all, with the time-quota being divided equally among all objects. The con­verse 
case with S = 8 corresponds to greedy consumption of the time-quota. That is, the whole time-quota is 
assigned to the highest priority object. After the object is done, the remaining time, if any, is assigned 
to the next highest priority object and so on. 106 We have implemented the algorithm and acceleration 
techniques presented in this paper using C++ and OpenGL. Our rigid-body simulator extends the technique 
presented by Guendelman et al. [2003] to incorporate DEEP [Kim et al. 2002] for fast and more accurate 
penetration depth estimation, instead of sample-based es­timation using distance .elds. It also uses 
a more complex friction model presented by Mirtich and Canny[Mirtich and Canny1995], which results in 
more robust contact resolution.  Figure5:Ametalliccylinderfalls ontoawooden table,in the mid­dle (left) 
and on the edge (right) and rolls off. The bottom part shows the corresponding frequencyspectra for the 
two cases. Note that for the case on the left, most of the impulse is transferred to the low frequency 
fundamental mode while for the case on the right, the impulse is mostly transferred to higher frequencymodes. 
5.2 Position Dependent Sounds As discussed earlier, the main advantage of using physically-based sounds 
over recorded audio is the ability to capture effects such as the magnitude of impacts between objects 
and more impor­tantly, the subtle shift in sounds on striking an object at different points. Figure5 
showsa scene witha metalliccylinder tossed onto a wooden table. Both the table and cylinder are sounding. 
The .g­ure contrasts two cases: the .rst case, shown on the left, depicts the cylinder striking the table 
near the middle and rolling off, while in the second case it strikes the table near the edge. We discuss 
the rolling sound in the next subsection, and will discuss the impact sound here. Since the table-top 
is in the form of a plate, we would expect that striking it on the edge would transfer a larger fraction 
of the impulse to higher frequencies, while striking it in the middle should transfer most part of the 
impulse to the fundamental mode of vibration, leading to a deeper sound. To verify this, we plotted the 
frequencyspectraforthetwo casesjust afterthecylindermakes .rst impact with the table. The corresponding 
plots for the two cases are shown in the lower part of the .gure. The case on the left shows a marked 
peak near the fundamental mode while the peak is completely missing in the second case. Conversely, the 
second case shows manypeaks at higher frequencies which are missing in the .rst one. This difference 
clearly demonstrates that the sound for the two cases is markedly different, with the same qualitative 
characteristicsasexpected. Another importantpointto noteisthat this technique does not require the meshes 
to be highly tessellated to capture these effects. The table consists of just 600 vertices and the cylinder 
128 vertices. 5.3 Rolling Sounds In addition to handling impact sounds, we are able to simulate re­alistic 
rolling sounds without requiring anyspecial-case treatment for sound synthesis. This is in part made 
possible because of the rigid-body simulator we have developed, which is able to handle contacts in a 
more graceful manner than most impulse-based sim­ulators. Figure6shows the impulses on thecylinder and 
the cor­responding audio for the case shown in the right side of Figure 5. Thecylinder rollsonthe table 
after impact,fallstothe groundand rolls on the .oor for sometime. The initial rolling sound, when the 
cylinder is on the table, has a much richer quality. The sound of the Figure 6: Aplot of the impulses 
on a cylinder versus time for the scene shown on the right in Figure5and the corresponding audio samples. 
The peaks correspond to impacts while the numerous low­amplitude impulses correspond to rolling forces. 
 table as the cylinder rolls over it conveys a sense of the cylinder s heaviness, which is only partly 
conveyed by the sound of the im­pact. The cylinder, although uniformly tessellated, is very coarse, with 
only 32 circumferential subdivisions. Figure6shows the im­pulses applied on the cylinder against time. 
The peaks correspond to impacts: when thecylinderfalls on the table, and whenitfalls to the ground from 
the table. Note that the audio waveform shows the corresponding peaks correctly. The period of time stretching 
from6to8seconds consistsofthecylinder rollingonthe .oorand is characterizedby manyclosely-spaced small-magnitude 
impulses on the cylinder as it strikes the .oor again and again due to its tes­sellation. To test how 
important the periodicity of these impulses was for the realism of rolling sounds, we found the mean 
and stan­dard deviation of the interval between these impulses from the data presented in Figure 6. The 
mean time between the impulses was 17 ms witha standarddeviationof10 ms. Thefact that the stan­dard deviation 
is more than 50% of the mean demonstrates that the impulses showvery little periodicity.This suggests 
that the period­icity of collisions is not critical for the perceived realism of rolling sounds. 5.4 
Ef.ciency We are able to do audio simulation for complex scenes in real time using our approach. Figure7showsa 
scene with 100 metallic rings falling simultaneously onto a wooden table and undergoing elas­tic collision. 
All the rings and the table are sounding. Each ring is treated as a separate object with separate aural 
properties. The rings consistof200vertices each. Figure8showsthe audioFPS3 for this simulation against 
time for the one second interval during which almost all the collisions take place. The application frame 
rate is 100 FPS. Note that this is not the raw data but a moving average so that the short-range .uctuations 
are absorbed. The plot on the bottom is the base timing without using anyof the acceler­ation techniques 
described in Section 4. The audio in this case is verychoppysincetheaudio generationisnotabletokeepupwith 
the speedof rendering and rigid-body simulation.With mode trun­cation and mode compression, the performance 
shows signi.cant improvement. However, after initially starting at about 200 FPS, the frame rate drops 
in the latter part where the maximum number of collisions happen.With quality scalingin additionto mode 
com­pression and truncation (shown by the top-most curve), the frame rateexhibitsnosuchdrop, continuingtobe 
around200FPS.Thisis because quality scaling gives priority to sound generation for those rings which 
just underwent collision, while lowering the quality for other rings which may have collided earlier 
and are contributing less to the overall sound. This illustrates the importance of quality scaling for 
scenarios with multiple collisions. It is important to note that although this example suf.ciently demonstrates 
the capability 3An audio frame is de.ned as the amount of sound data suf.cient to play for a duration 
equal to the duration of one video frame.  107   Figure7: More than 100 metallic ringsfall ontoawooden 
table. All the rings and the table are sounding. The audio simulation runs at more than 200 FPS, the 
application frame rate being 100 FPS. Quality Scaling ensures that the perceived sound quality does not 
degrade, while ensuring steady frame rates(See Figure8) of the system to maintain steady frame rates, 
it is improbable in a real application, since there are about 100 collisions within a sec­ond. This is 
the reason whythe CPU utilization is high (50%). A more common scenariowouldbeasshownin Figure1, whichhas 
a much lower CPU utilization (10%). To illustrate the realistic sounds achievable with our approach, 
we designed a three-octave xylophone shown in Figure 1. The im­ageshowsmanydicefallingontothekeysofthe 
xylophonetopro­duce the corresponding musical notes. The audio simulation for this scene runsin the rangeof 
500-700 FPS,depending on the fre­quencyof collisions. The dice have been scripted tofall onto the xylophonekeys 
at precise moments in time to play out any set of musical notes. Because of the ef.ciency of the sound 
generation process,theoverallsystemiseasilyableto maintainasteadyframe rate of 100 FPS. Also, there are 
situations in which manydicefall on differentkeys withinafew millisecondsof each other,but the sound 
quality exhibits no perceptible degradation. Although we have not tuned the xylophone keys to match the 
exact frequency spectrum of a real xylophone, the resulting sound is realistic and captures the essential 
timbre of the instrument. The material pa­rameters for the xylophone were taken from [Chaigne and Doutaut 
1997]. 6 Conclusion We have presented a physically-based sound synthesis algorithm with several acceleration 
techniques for rendering a large-scale scene consisting of hundreds of interacting objects in real time, 
with little loss in perceived sound quality. This approach requires no special mesh structure, is simple 
to implement, and further takes advantageofexistinghardware acceleration.Weplantoextendthis framework 
to auditory display of sliding sounds, explosion noises, breaking sounds, and other more complexaudio 
effects that are dif­.cult to simulate at interactive rates. References CHAIGNE, A., AND DOUTAUT, V. 
1997. Numerical simulations of xylophones. i. time domain modeling of the vibrating bars. J. Acoust. 
Soc. Am. 101, 1, 539 557. CHUNG, J. Y., LIU, J., AND LIN, K. J. 1987. Scheduling real-time, periodic 
jobs using imprecise results. In Proc. IEEE RTS. DONGARRA, J. J. 2005. Performance of various computers 
using standard linear equations software (linpack benchmark report). Tech. rep., Knoxville, TN, USA. 
FLORENS, J. L., AND CADOZ, C. 1991. The physical model: modeling and simu­lating the instrumental universe. 
In Represenations of Musical Signals, G. D. Poli, A. Piccialli, and C. Roads, Eds. MIT Press, Cambridge, 
MA, USA, 227 268. Figure 8: This graph shows the audio simulation FPS for the scene shown in Figure7from 
time 1s to 2s, during which almost all the collisions take place. The bottom-most plot shows the FPS 
for an implementation using none of the acceleration techniques. The top­most curve shows the FPS with 
mode compression, mode trunca­tion and quality scaling. Note how the FPS stays near 200 even when the 
other two curves dip due to numerous collisions during 1.5-2.0s. FOUAD,H.,BALLAS,J., AND HAHN,J.1997. 
Perceptually based scheduling algo­rithms for real-time synthesis of complex sonic environments. In Proc. 
Int. Conf. Auditory Display. GUENDELMAN,E.,BRIDSON,R., AND FEDKIW,R. 2003. Nonconvex rigidbodies with 
stacking. ACM Trans. on Graphics (Proc. of ACM SIGGRAPH) 22, 871 878. KIM, Y. J., LIN, M. C., AND MANOCHA, 
D. 2002. DEEP: an incremental algo­rithm for penetration depth computation between convex polytopes. 
Proc. of IEEE Conference on Robotics and Automation, 921 926. MIRTICH, B., AND CANNY, J. 1995. Impulse-based 
simulation of rigid bodies. In 1995 Symposium on Interactive 3D Graphics, P. Hanrahan and J. Winget, 
Eds., ACM SIGGRAPH, 181 188. ISBN 0-89791-736-7. O BRIEN, J. F., COOK, P. R., AND ESSL, G. 2001. Synthesizing 
sounds from physically based motion. In SIGGRAPH 01: Proceedings of the 28th annual con­ference on Computer 
graphics and interactive techniques,ACM Press, NewYork, NY, USA, 529 536. O BRIEN, J. F., SHEN, C., AND 
GATCHALIAN, C. M. 2002. Synthesizing sounds from rigid-body simulations. In The ACM SIGGRAPH 2002 Symposium 
on Com­puter Animation,ACM Press, 175 181. SEK, A., AND MOORE, B. C. 1995. Frequency discrimination as 
a function of frequency,measuredinseveralways. J. Acoust. Soc. Am. 97,4(April), 2479 2486. VANDEN DOEL,K., 
AND PAI,D.K. 1996. Synthesisof shape dependent sounds with physical modeling. In Proceedings of the International 
Conference on Auditory Displays. VANDEN DOEL,K., AND PAI,D.K. 1998. The soundsofphysical shapes. Presence 
7, 4, 382 395. VANDEN DOEL,K.,KRY,P.G., AND PAI,D.K. 2001.Foleyautomatic:physically­based sound effects 
for interactive simulation and animation. In SIGGRAPH 01: Proceedings of the 28th annual conference on 
Computer graphics and interactive techniques,ACM Press,NewYork,NY, USA, 537 544. VAN DEN DOEL, K., KNOTT, 
D., AND PAI, D. K. 2004. Interactive simulation of complex audiovisual scenes. Presence: Teleoper. Virtual 
Environ. 13, 1, 99 111. ZWICKER,E., AND FASTL,H. 1990. In Psychoacoustics. Springer-Verlag, Berlin. 
 108  Interactive Sound Synthesis for Large Scale Environments Nikunj Raghuvanshi Ming C. Lin Dice 
play the song The Entertainer on a 3-octave xylophone Numerous rings rain onto a wooden table 227 
 m  Such abrupt changes in a simulation would introduce a mismatch with our real world experiences, 
which would result in negative training or at least confuse users. In this paper, we propose an ef.cient 
way for computing the acoustical effect of early re.ection and diffraction paths accord­ing to the Uniform 
Theory of Diffraction [21, 24, 26]. Speci.cally, we make three contributions. First, we describe a beam 
tracing method, for enumerating sequences of diffracting edges ef.ciently and without aliasing in densely 
occluded polyhedral environments. Second, we propose an approximation to simulated sound .elds suitable 
for immersive virtual environments in which diffraction is computed only in shadow regions. Finally, 
we describe a real­time auralization system that produces spatialized sound with early diffraction, transmission, 
and specular re.ection during interactive walkthroughs of complex environments. Our experimental results 
demonstrate that (1) beam tracing is an ef.cient and aliasing-free way to .nd diffraction sequences in 
densely occluded environments, (2) it is possible to construct early diffracting propagation paths and 
spatialize sounds in real-time, and (3) diffraction greatly improves the quality of spatialized sounds 
in immersive virtual environments. 2 Background and Related Work There are currently three major approximation 
theories for diffrac­tion problems in polyhedral environments: (1) the Huygens-Fresnel diffraction theory, 
(2) boundary integral representations using the Helmoltz-Kirchoff integral theorem, and (3) the Uniform 
Theory of Diffraction. Huygens' principle [17] predicts that every point on a wavefront can be regarded 
as the source of a secondary spherical wavelet. The wave.eld is de.ned at each point by the superposition 
of these wavelets, which extend in all directions, including shadow regions. Fresnel supplemented Huygens' 
theory by adding interference be­tween the wavelets to treat diffraction [6]. He also subdivided space 
between the source and the receiver into concentric ellip­soids with frequency-dependent radii: the Fresnel 
ellipsoids. By modeling diffraction effects as a loss in signal intensity, Bertoni [4] and Tsingos and 
Gascuel [43] use Fresnel ellipsoids to determine relevant obstacles at any given frequency. By replacing 
the binary geometrical visibility by an extended visibility term between 0 and 1, they achieve frequency-dependent 
sound muf.ing. This tech­nique removes abrupt cuts in the simulated sound, producing a more pleasing 
experience. However, it fails to capture the temporal as­pect of diffraction since new propagation paths 
are not introduced. While this approximation is not usually a concern for electromag­netic wave propagation, 
it is an important issue for acoustics. Analytic expressions give time-domain diffraction .lters for 
se­quences of .nite wedges based on a discrete Huygens interpreta­tion [27, 38]. In this case, the edges 
are discretized into sec­ondary point sources whose contributions must be summed to ob­tain the diffracted 
.eld. Such models prove very accurate for low order diffraction and have recently been used to assess 
audibility of diffraction in the case of a simple stage house [41]. However, it is unclear if the method 
can be applied in real-time since the edges must be discretized into a large number of point sources 
to compute the diffraction .lters. The Helmoltz-Kirchoff integral theorem provides a formaliza­tion of 
the Huygens-Fresnel principle [6, 10]. It expresses the scat­tered .eld at any point in space as a function 
of the .eld on the surface of the diffracting objects. Mathematically, it can be ex­pressed as a surface 
integral and solved by numerical methods such as Boundary Element Methods (BEM) [16, 18] that discretize 
sur­faces into patches. BEM allow for very accurate treatment of sound diffraction. But, they are far 
too compute intensive for interactive sound rendering over the whole audio spectrum and are mainly used 
for low frequencies (below 150Hz). In some cases the integral can be solved analytically [35], such as 
for height .elds or periodic sur­faces. However, neither of these cases usually applies to architec­tural 
models. The Uniform Theory of Diffraction (UTD) [21, 24, 26] incor­porates diffraction into the ray theory 
of light. The UTD treats an in.nite wedge as a secondary source of diffracted waves that in turn can 
be re.ected and diffracted before reaching the receiver. For a given point source and point receiver 
location, the diffrac­tion of a wave over an in.nite wedge is represented by a single ray whose contribution 
to the wave .eld is attenuated by a com­plex valued diffraction coef.cient [24] (see Appendix A). For 
any sequence of diffracting edges, the ray follows the path satisfying Fermat's principle: if the propagation 
medium is homogeneous, the ray follows the shortest path from the source to the receiver, stab­bing the 
diffracting edges. The UTD is a high frequency approxi­mation and applies to in.nite wedges, when the 
source and listener remain far from diffracting surfaces (compared to the wavelength). To date, the UTD 
has been applied successfully in several types of off-line simulations, including acoustical diffraction 
over solitary wedges [20], lighting effects in room-sized scenes [2], and radio frequency propagation 
in buildings and cities [33, 22]. For acoustic waves, the method has been validated down to 150Hz for 
a small combination of diffracting wedges [20, 38]. Validation of the ap­proach for more complex situations 
has not yet been achieved. . MR S Figure 2: According to the UTD, an incoming ray pgives rise to a cone 
of diffracted rays, where the aperture angle 0dof the cone is equal to the angle 0ibetween the incident 
ray and the edge (the axis of the cone is the edge). For a given receiver location, a single ray describes 
the diffracted .eld. Of these three approaches, the UTD is the most promising for spatializing sound 
in interactive virtual environments, as it inte­grates well into a geometrical framework, is physically-based, 
and provides satisfying results for most of the audio spectrum (for early diffraction orders). The main 
computational challenge in using the UTD into real­time auralization systems is the ef.cient enumeration 
of signi.cant early diffraction paths. Although many algorithms exist to .nd ap­proximate solutions [29], 
they are either too inef.cient or prone to aliasing. For instance, Aveneau [2] enumerated all permutations 
of polyhedral edges within the .rst few Fresnel ellipsoids, which is not practical for sound simulations 
in large environments. Ra­jkumar et al. [33] extended a ray tracing algorithm to broadcast rays in all 
directions for each edge intersection. Similarly, For­tune et al. [11, 22] and Stephenson [36] described 
a beam tracing approach in which propagation due to each edge diffraction was approximated by a set of 
beams emanating from point sources at sampled positions along the diffracting edge. These latter two 
ap­proaches approximate the set of potential diffraction paths by dis­crete sampling. Thus they are prone 
to aliasing, which would cause noticeable artifacts in an interactive sound system. Prior methods provide 
neither interactive response times nor guarantee .nding all signi.cant propagation paths.   "   
 Figure 4: Possible diffraction paths are conservatively bounded by the intersection of two cones and 
a polytope. Left: a beam inci­dent on the edge (red) of a window and the two diffraction cones (shown 
in blue and green only in the halfspace behind the window) computed at the endpoints of the beam-edge 
intersection. Right: a close-up view of the intersection (hatched) between the two cones and the polytope 
resulting from stepping through the next portal (yellow). 3.2 Path Construction and Validation Once beams 
are traced from a sound source, we construct a unique propagation path for each beam containing a receiver 
location. The geometry of the path determines the delay, amplitude, and directiv­ity of the sound wave 
traveling along the path from the source to the receiver. According to the UTD, the wave .eld resulting 
from a diffraction can be approximated by a piecewise-linear propagation path i.e., the shortest among 
all possible paths from the source to the receiver stabbing the faces and edges in the sequence. In order 
to construct this path for a given sequence of beams, we .nd the points of inter­section for every re.ecting 
face and diffracting edge. The intersec­tions with specularly re.ecting faces are uniquely determined 
by the locations of the source, receiver and diffraction points. Thus, the problem is reduced to .nding 
the locations of the diffraction points, Pi(i1:::n)(see Figure 5). At each of these points, the path 
satis.es a simple unfolding property (see Figure 2): the an­gle (0i) at which the path enters the edge 
must be the same as the angle (0d) at which it leaves [14]. Thus, for each potential path, we solve a 
non-linear system of nequations expressing equal angle constraints at the diffracting edges: 8 ; !;! 
;;!;! P1S.E1 P1P2.(;E1) ;;!;! ;;!;! ! P2P1.E2 P2P3.(;E2) (1) . . .  :; ;;;!;!;;!;! PnPn;1.En PnR.(;En) 
;! where Sis the source point, Ris the receiver point, Eiis the nor­ ;;;;! malized direction vector 
of the ithdiffracting edge, and Pi+1Piis a normalized direction vector between two adjacent points in 
the shortest path. To incorporate specular re.ections in this equation, ;!;;;;! Eiand Pi+1Piare transformed 
by successive mirroring operators accounting for the sequence of specularly re.ecting faces up to the 
ithdiffraction. ;! Parameterizing the edges, Pi Oi+tiEi(where Oiis a ref­erence point on edge i), the 
system of equations (1) is rewritten in terms of nunknowns tiand solved within a speci.ed tolerance using 
a non-linear system solving scheme. We use a locally con­vergent Newton scheme [32], with the middle 
of the edges as the initial estimate for the diffraction points. Since the equation satis­.ed by any 
diffraction point only depends on the previous and next E1 R E2 Figure 5: A single propagation path 
comprising a diffraction, two specular re.ections, and another diffraction. The two diffraction points 
(Pi) are determined by equal angle constraints at the corre­sponding edges (Ei). diffraction pointsin 
the sequence,the Jacobianmatrix is tridiagonal and is easily evaluated analytically. Thus, every Newton 
iteration is performed in time O(n)where nis the number of unknowns (i.e., edges). We found this method 
faster than the recursive construction proposed by Aveneau [3]. Once the diffraction points are found, 
we validate whether the resulting path intersects every cell boundary in the sequence (to compensate 
for the fact that the beams are conservatively approx­imate). If not, the path in the over-estimating 
part of the beam is discarded. Otherwise, it contributes to an impulse response used for spatializing 
sounds [23, 25] (see Appendix A). The proposed conservative beam tracing and path construction enumerate 
all sequences of diffracting edges (without aliasing) up to a speci.ed termination criterion, while most 
acoustically infea­sible sequences of edges and faces are culled already during beam tracing. 4 Shadow 
Region Approximation Our beam tracing technique provides a method for .nding diffrac­tion paths ef.ciently 
and without aliasing. However, contrary to specular re.ections or transmissions, diffraction introduces 
a scat­tering in all directions around the wedge, which results in a com­binatorial explosion of the 
number of beams to consider, even for moderately complex scenes. In this section we introduce an approx­imation 
to reduce the spatial extent of diffraction beams, while pre­serving a good modeling of the diffracted 
.eld. This approximation enables us to achieve interactive auralization in large environments. Recent 
psychoacoustic tests in the context of a simple stage house model [41] show that diffractions can be 
perceived in illu­minated regions where direct and re.ected contributions from a source also reach the 
listener. However, in this case, we conjec­ture that diffraction does not modify the main acoustic cues 
already carried by the direct and re.ected contributions since the ampli­tude of the diffracted .eld 
is usually much weaker than direct or even re.ected contributions [31] (p.500). On the other hand, we 
note that diffraction into shadow regions is crucial for typical vir­tual worlds as it provides the primary 
mode of propagation to most of the environment. Thus, we introduce an approximation in which the contribution 
of diffraction is considered only in shadow regions. Accordingly, our current on-line implementation 
allows for adding an extra halfspace to the polytope representing each diffrac­tion beam so that it tightly, 
yet conservatively, bounds the shadow region of each diffraction. However, discarding the diffracted 
.eld in the illuminated region of a wedge introduces a discontinuity at the shadow boundary, as the direct 
.eld is abruptly replaced by the diffracted .eld. This is due to the fact that the UTD diffracted .eld 
is de.ned to ensure that the sum of the direct and diffracted .elds is continuous for any   10 log(Etotal/Edirect) 
(dB SPL) 10 log(Eapprox/Edirect) (dB SPL) Error (dB SPL) ad (radians) ad (radians) a (radians) d (a) 
UTD total wave .eld (b) Approximation (c) Error Figure 6: Plots of (a) the UTD total wave .eld, (b) our 
approximation, and (c) the error as a function of diffraction angle (Cd), as the receiver rotates around 
the edge, for a single diffracting wedge (inset). Each plot shows several curves corresponding to the 
sound pressure level (SPL) for the center frequencies of octave bands ranging from 100 Hz (top) to 24kHz 
(bottom). Our approximation culls the diffracted .eld contribution in the illuminated region of the wedge 
but still closely matches the original UTD .eld. receiver location (see Figure 6(a)), while both .elds, 
independently, To test if our beam tracing approach is practical for modeling are discontinuous at shadow 
boundaries. diffraction in typical virtual environments, and to evaluate the ben- To preserve continuity 
at shadow boundaries, we normalize the e.ts of incorporating diffraction into real-time auralization, 
we ran diffracted .eld as predicted by the UTD, so that it is C0continu-a series of tests computing propagation 
paths both with and without ous with the direct .eld at the shadow boundary, SB.We de.ne the diffraction. 
During each test, we used a 3D model with 1,762 poly­normalized diffracted .eld at the receiver location 
Ras: gons representing one .oor of a building (see Figure 8). For sim­ plicity, we assumed that every 
polygon in the 3D model was 80% 0 SB SB Edifracted(R)Eincident(R)Edifracted(R)/Edifracted(R),(2) re.ective 
and acoustically opaque (no transmission). Before each test, we traced 50,000 beams in breadth-.rst order 
from a station­where ESB (R)and ESB (R)are the incident and ary sound source (located at the white dot 
in Figure 8) and stored incidentdifracteddiffracted .elds when the receiver Ris rotated to lie on the 
shadow them in a beam tree data structure. Then, as a receiver moved at boundary SB(at the same distance 
from the edge). three inch increments along a hallway (the long vertical one on the This modi.ed expression 
scales the diffracted .eld equally for right side of each image in Figure 8), we computed propagation 
all directions around the edge, unnecessarily modifying the origi­ paths from source to receiver, updated 
an impulse response, and nal UTD .eld away from the shadow boundary. Hence, our new auralized spatialized 
sound in real-time. All the tests were run on approximated diffracted .eld is derived by interpolating 
between a Silicon Graphics Onyx2 workstation using two 195MHz R10000 the expression given by equation 
(2) and the original UTD expres­ processors, one of which was dedicated to software convolution of sion 
of the diffracted .eld [24] (see Appendix A) as the receiver audio signals. further moves inside the 
shadow region (between SBand Ain Fig- The test sequence was executed three times, once for each of the 
ure 6(a)). Since the expressions are complex-valued, care must be following beam tracing constraints: 
taken in the interpolation: argument (i.e., phase) and modulus must 1. Specular re.ection only: We traced 
50,000 beams alongbe independently interpolated to give a new complex value. paths of specular re.ection, 
with no diffraction. The results Figure 6 shows a comparison of the total wave .eld as pre­ represent 
the state-of-the-art prior to this paper [12, 13].dicted by the UTD and our approximated wave .eld for 
the situation 2. Diffraction only: We traced 50,000 beams along paths ofshown in Figure 6(a). Although 
there are differences in the vicin­diffraction (around silhouette edges into shadow regions),ity of the 
shadow boundary, most properties of the original UTD with no specular re.ections. diffracted .eld are 
captured by the approximation: (1) the edge is 3. Both specular re.ection and diffraction: We traced 
50,000 still the source of the diffracted contribution and path delays are not beams along paths representing 
arbitrary permutations ofmodi.ed by our approximation, (2) the .eld is continuous, so no au­specular 
re.ection and diffraction (into shadow regions). dible artifact is heard when crossing the shadow boundary, 
(3) the .eld amplitude is independent of frequency at the shadow bound-Figure 7 shows plots with the 
number of propagation paths (the ary, and (4) it decays faster as frequency increases and tends toward 
top plot) and the power of the impulse responses (the bottom plot)the actual value of the UTD diffracted 
.eld as the receiver moves for each receiver location during the three tests.1 away from the shadow boundary. 
As a result, we conjecture that the From these plots, we con.rm that specular re.ection alone is not 
spatialized sound produced by our on-line system provides many of adequate to produce realistic spatialized 
sound in typical virtual en­the signi.cant cues useful for localization of objects, separation of vironments. 
The red curves in Figure 7 show that the number of signals, and comprehension of space in an immersive 
virtual envi­propagation paths and the power in the corresponding impulse re­ronment. sponsesvarieddramatically 
withsmall changesinreceiverlocation. This effect is easily understood by examining images of the beams 
and power distributions shown in Figure 8(a-d). Note the pattern 5 Simulation Results of thin beams 
zig-zagging across the hallways in the top-left image. As the receiver walks along the test trajectory, 
s/he moves in and out The 3D data structures and algorithms described in the preceding of these distinct 
specular re.ection beams, leading to sharp discon­sections are implemented in C++ and run both on SGI/Irix 
and tinuities in the computed early reverberations. Even worse, there PC/Windows computers. We integrated 
them into a prototype sys­are several locations where no specular re.ection paths reached the tem that 
allows a user to move through a virtual environment inter- P actively, while images and spatialized 
audio are rendered in real-1Power is computed as 10.log na2where nis the number of i.1itime according 
to the user's simulated position. propagation paths and aithe amplitude along the ith path.  ´   
 ´ ¨  ´  ´ ¨ ¨ ¨  ¨¨¨            A Auralizing the wedge diffracted .eld According 
to the UTD, the acoustic pressure .eld diffracted by a wedge can be expressed in terms of the incident 
.eld on the edge, Eincident(M), as: ;ikr Edifracted(R) Eincident(M)DA(r,p)e,(3) where Ris the receiver 
location, Mis the diffraction point (see p Figure 2) , A(r,p) pr/(p+r)is a scalar distance attenuation 
;ikr term along the propagation path, the complex exponential erepresents phase variation along the 
diffracted path, k27/.is the wave number (.is the wavelength). Equation (3) is applied successively for 
every diffracting wedge and multiplied by attenu­ations due to re.ections, transmissions, and path length 
to form a contribution to the impulse response for every propagation path. S np r R Figure 10: Notations 
for the UTD diffraction coef.cient. Dis the complex-valued UTD diffraction coef.cient [24, 26] ac­counting 
for amplitude and phase changes due to diffraction: ;i. D(n,k,p,r,0i,Ci,Cd);pe4 2n2 ;);k7sinei );17+(ad;ai)+ 
h tanFkLa(Cd;Ci) 2n ;1;7;(ad;ai));;) +tanFkLa(Cd;Ci)(4) {;2n );) ;17+(ad+ai)+ +tanFkLa(Cd+Ci) 2n i 
;1;7;(ad+ai));;) +tanFkLa(Cd+Ci), 2n where (see also Figure 10 and Figure 2): Z+1 p iX ;ir2 F(X)2iXeedT, 
(5) p X Lpr sin20i, (6) p+r (±)±227nN;f a(f)2cos , (7)2 N±is the integer that satis.es more closely 
the relations: 27nN+ ;f7and 27nN;;f;7(8) Several approximations exist in the related literature, useful 
for im­plementation of Eq. 4. In particular, relations (8) reduce to: { 0for f.7(n;1) N+ , 1for f7(n;1) 
((9) ;1for f7(1;n) N;0for7(1;n).f.7(1+n), 1for f7(1+n) and Kawai [20] gives an approximate rational expression 
for the integral in Eq. (5): (p)p pi. X+1:4 for X0:8:F(X) 7X1;pX e4 0:7X+1:2 i.X 0:8 4X+1:4 for X.0:8:F(X) 
1;e (X+1:25)2 (10) Cotangent terms in Equation (4) are still singular at a re.ection or shadow boundary 
and cannot be evaluated numerically at these boundaries. However, in the vicinity of such a boundary 
we can ± express the terms Ci±Cdas f27nN'(7;"). The coef.cient is continuous and its value can be computed 
using [24]: ;);) ;17± ± tanFkLa(f)' 2n ;p) ;i7.4;i7.4 ne27kLsgn(");2kL"e, where sgn(")1if "0and ;1otherwise. 
In order to render the virtual sound .eld, we compute a digital .l­ter [23, 25], with which audio signals 
emanating from the source can be convolved to produce a spatialized audio signal with re­verberation. 
For high quality auralization, this .lter is computed using complex values in Fourier frequency space 
at the desired sampling rate resolution. For interactive applications, fewer fre­quency bands can be 
considered, depending on how much process­ing power is available. The modulus of the complex .eld for 
the center frequency of each frequency band can be used to re-equalize the source signal. For more information 
on the signal processing involved in auralization, please refer to [23, 25, 34]   c1 .2009 UNC-Chapel 
Hill. Reprinted, with permission, from UNC-Chapel Hill Technical Report TR09-008 FastV: From-point Visibility 
Culling and Application to Sound Rendering Anish Chandak 1 Lakulish Antani 1 Micah Taylor§1 and Dinesh 
Manocha¶1 1Univeristy of North Carolina at Chapel Hill Abstract We present an ef.cient technique to compute 
the potentially visible set (PVS) of triangles in a complex 3D scene from a viewpoint. The algorithm 
computes a conservative PVS at object space accuracy. Our approach traces a high number of small, volumetric 
frusta and computes blockers for each frustum using simple intersection tests. In practice, the algorithm 
can compute the PVS of CAD and scanned models composed of millions of triangles at interactive rates 
on a multi-core PC. We also use the visibility algorithm to accurately compute the re.ection paths from 
a point sound source. The resulting sound propagation algorithm is 10-20X faster than prior accurate 
geometric acoustic methods. 1. Introduction Visibility computation is a widely-studied problem in com­puter 
graphics and related areas. Given a scene, the goal is to determine the set of primitives visible from 
a single point (i.e. from-point visibility), or from any point within a given region (i.e. from-region 
visibility). At a broad level, these al­gorithms can be classi.ed into object space and image space algorithms. 
The object space algorithms operate at object­precision and use the raw primitives for visibility computa­tions. 
The image space algorithms resolve visibility based on a discretized representation of the objects and 
the accu­racy typically corresponds to the resolution of the .nal im­age. These algorithms are able to 
exploit the capabilities of rasterization hardware and can render large, complex scenes composed of tens 
of millions of triangles at interactive rates using current GPUs. In this paper, we primarily focus on 
from-point, object space conservative visibility, whose goal is to compute a super­set of visible geometric 
primitives. Such algorithms are use­ful for walkthroughs, shadow generation, global illumina­tion and 
occlusion computations. Another application for achandak@cs.unc.edu lakulish@cs.unc.edu § taylormt@cs.unc.edu 
¶ dm@cs.unc.edu object space visibility algorithms is accurate computation of re.ection paths for acoustic 
simulation or sound render­ing. Given a point sound source, 3D models of the environ­ment with material 
data, and the receiver s position, geomet­ric acoustic (GA) methods perform multiple orders of re­.ections 
from the obstacles in the scene to compute the im­pulse response (IR). Sample-based propagation algorithms, 
such as stochastic ray-tracing for GA can result in statisti­cal errors or inaccurate IRs [Len93]. As 
a result, we need to use object space visibility techniques, such as beam trac­ing [FCE*98, LSLS09], 
to accurately compute the propa­ gation paths. However, current object space visibility algo­rithms only 
work well on simple scenes with tens of thou­sands of triangles or with large convex occluders. There 
is a general belief that it is hard to design fast and practical object space visibility algorithms for 
complex 3D models [Gha01]. Main Results: We present a novel algorithm (FastV) for conservative, from-point 
visibility computation. Our ap­proach is general and computes a potentially visible set (PVS) of scene 
triangles from a given view point. The main idea is to trace a high number of 4-sided volumetric frusta 
and ef.ciently compute simple connected blockers for each frustum. We use the blockers to compute a far 
plane and cull away the non-visible primitives, as described in Section 3. Our guiding principle is to 
opt for simplicity in the choice of different components, including frustum tracing, frustum­intersection 
tests, blocker and depth computations. The main  2c .2009 UNC-Chapel Hill. Reprinted, with permission, 
from UNC-Chapel Hill Technical Report TR09-008  Figure 1: Fast Acoustic Simulation: We use FastV for 
com­puting accurate re.ection paths in this Cathedral model with 80K triangles. Our propagation algorithm 
performs three orders of re.ections from the source and computes the IR at the receiver in less than 
5 seconds on a 16-core PC. To the best of our knowledge, ours is the .rst ef.cient and accurate propagation 
algorithm to handle models of this complexity. contribution of the paper is primarily in combining known 
algorithms (or their extensions) for these parts. Overall, FastV is the .rst practical method for visibility 
culling in complex 3D models due to the following reasons: 1. Generality: Our approach is applicable 
to all trian­gulated models and does not assume any large objects or occluders. The algorithm proceeds 
automatically and is not susceptible to degeneracies or robustness issues. 2. Ef.ciency: We present 
fast and conservative algo­rithms based on Plücker coordinates to perform intersection tests. We use 
hierarchies along with SIMD and multi-core capabilities to accelerate the computations. In practice, 
our algorithm can trace 101 -200K frusta per second on a sin­gle 2.93 Ghz Xeon Core on complex models 
with millions of triangles. 3. Conservative: Our algorithm computes a conserva­tive superset of the 
visible triangles at object-precision. As the frustum size decreases, the algorithm computes a tighter 
PVS. We have applied the algorithm to complex CAD and scanned models with millions of triangles and simple 
dy­namic scenes. In practice, we can compute conservative PVS, which is within a factor of 5-25% of the 
exact visible set, in a fraction of a second on a 16-core PC (as described in Section 4).  Accurate 
Sound Propagation: We use our PVS compu­tation algorithm to accurately compute the re.ection paths from 
a point sound source to a receiver, as described in Sec­tion 5. We use a two phase algorithm that .rst 
computes image-sources for scene primitives in the PVS computed for primary (or secondary) sources. This 
is followed by .nding valid re.ection paths to compute actual contributions at the receiver. We have 
applied our algorithm to complex mod­els with tens of thousands of triangles. In practice, we ob­serve 
performance improvement of up to 20X using a single­core implementation over prior accurate propagation 
meth­ods that use beam tracing. 2. Previous Work The problem of visibility has been extensively studied 
in computer graphics, computational geometry, acoustic simu­lation and related areas for more than four 
decades. We refer the readers to excellent recent surveys [Dur99, COCSD03]. Due to space limitations, 
we only give a brief overview of some object space and sampling-based methods. Object space visibility 
computations: There is extensive work on object-precision algorithms, including methods for hidden surface 
removal [Gha01] and exact visibility com­ putation from a point using beam tracing [HH84, FCE*98, ORM07] 
or Plücker coordinates [Nir03]. Many exact al­gorithms have also been proposed for region-based visi­bility 
[Dur99, DD02, Nir03, BW05]. There is considerable literature on conservative visibility computations 
from a point [BHS98, CT97, HMC*97, LG95] or from a region [KCCO00, LSCO03, Tel92]. Some of these algorithms 
have been designed for special types of models, e.g. architectural models represented as cells and portals, 
2.5D urban models, scenes with large convex occluders, etc. It is also possible to perform conservative 
rasterization [AMA05] on current GPUs to compute an object-precision PVS from a point. Image space or 
sample-based visibility computations: These methods either make use of rasterization hardware or ray-shooting 
techniques to compute a set of visible prim­itives [COCSD03]. Most of these methods tend to be ei­ther 
approximate or aggressive [NB04, WWZ*06]. Current GPUs provide support for performing occlusion queries 
for from-point visibility and are used for real-time display of complex 3D models on commodity GPUs [KS00,MBW08]. 
3. FastV: Visibility Computation In this section, we present our conservative visibility com­putation 
algorithm. The inputs to our algorithm are: a view point (v .R3), a set of scene primitives (.), and 
a viewing frustum (F), with an apex at v. Our goal is to compute a subset of primitives p .. such that 
every primitive p .., which is hit by some ray r .F is included in the computed subset p. The subset 
p is called the potentially visible set (PVS). The smallest such PVS is the set of exactly visible primitives 
(pexact ). The subset p computed by our algorithm is conservative, i.e., p . pexact . For the rest of 
the paper, we assume that the primitives are triangles, though our al­gorithm can be modi.ed to handle 
other primitives. We also assume that the connectivity information between the scene triangles is precomputed. 
We exploit this connectivity for ef­.cient computation; however our approach is also applicable to polygon 
soup models. In order to perform fast intersection  c 3 .2009 UNC-Chapel Hill. Reprinted, with permission, 
from UNC-Chapel Hill Technical Report TR09-008 Figure 2: Overview: We divide the view-frustum with an 
apex at v, into many small frusta. Each frustum is traced through the scene and its far plane is updated 
when it is blocked by a connected blocker. For example, frustum F5 is blocked by primitives of object 
V2 but frustum F1 has no blockers. tests, we store the scene primitives in a bounding volume hi­erarchy 
(BVH) of axis-aligned bounding boxes (AABBs). This hierarchy is updated for dynamic scenes. 3.1. Overview 
We trace pyramidal or volumetric beams from the viewpoint. Prior beam tracing algorithms perform expensive 
exact inter­section and clipping computations of the beam against the triangles and tend to compute pexact 
. Our goal is to avoid these expensive clipping computations, and rather perform simple intersection 
tests to compute the PVS. Moreover, it is hard to combine two or more non-overlapping occluders (i.e. 
occluder fusion) using object space techniques. This is shown in Figure 2, where object H1 is occluded 
by the com­bination of V1 and V2. As a result, prior conservative object space techniques are primarily 
limited to scenes with large occluders. We overcome these limitations by tracing a high number of relatively 
small frusta and computing the PVS for each frus­tum independently. In order to compute the PVS for each 
frustum, we try to compute a blocker that is composed of connected triangles (see Figure 3). The blockers 
are com­ puted on the .y and need not correspond to a convex set or a solid object; rather they are objects 
that are homeomorphic to a disk. We use simple and fast algorithms to perform in­tersection tests and 
blocker computation. Frustum Tracing: We use a simple 4-sided frustum, which is represented as a convex 
combination of four corner rays intersecting at the apex. Each frustum has a near plane, four side planes, 
and a far plane. The near plane and the four side planes of a frustum are .xed and the far plane is parallel 
to the near plane. The depth of the far plane from the view point is updated as the algorithm computes 
a new blocker for a frustum. Our algorithm sub-divides F into smaller frusta using uniform or adaptive 
subdivision and computes a PVS for each frustum. Eventually, we take the union of these dif­ferent PVSs 
to compute a PVS for F. Algorithm: The algorithm computes the PVS for each frus­tum independently. We 
initialize the far plane associated with a frustum to be at in.nity and update its value if any connected 
blocker is found. The algorithm traverses the BVH to ef.ciently compute the triangles that potentially 
in­tersect with the frustum. We perform fast Plücker intersec­tion tests between the frustum and a triangle 
to determine if the frustum is completely inside, completely outside, or partially intersecting the triangle. 
If the frustum is partially intersecting, we reuse the Plücker test from the frustum­triangle intersection 
step to quickly .nd the edges that in­tersect the frustum (see Section 3.3). We perform frustum­ triangle 
intersection with the neighboring triangles that are incident to these edges. We repeat this step of 
.nding edges that intersect with the frustum and performing intersection tests with the triangles incident 
to the edge till the frustum is completely blocked by some set of triangles. If a blocker is found (see 
Section 3.2), we update the far plane depth of the frustum. Triangles beyond the far plane of the frustum 
are discarded from the PVS. If there is no blocker associated with the frustum, then all the triangles 
overlapping with the frustum belong to the PVS. Additionally, we compute the PVS for each frustum in 
parallel as described in Section 3.5. 3.2. Frustum Blocker Computation We de.ne a blocker for a frustum 
as a set of connected trian­gles such that every ray inside the frustum hits some triangle in the frustum 
blocker (see Figure 3(a)). When we intersect a frustum with a triangle, the frustum could partially inter­sect 
the triangle. In such a case, we walk to the neighboring triangles based on that intersection and try 
to .nd a blocker for the frustum (see Figure 3). We compute all the edges of the triangle that intersect 
with the frustum. For every inter­secting edge, we walk to the neighboring triangle incident to the edge 
and perform frustum-triangle intersection test with the neighbor triangle. The intersection and walking 
steps are repeated until one of the following conditions is satis.ed: a All triangles incident to every 
intersecting edge found during the frustum blocker step have been processed. This implies that we have 
found a blocker. bA free-edge, i.e. an edge with only one incident triangle, or a silhouette edge, i.e. 
an edge with incident triangles facing in opposite directions as seen from the viewpoint, intersects 
with the frustum. In that case, we conclude that the current set of intersecting triangles does not constitute 
a blocker. Note that our termination condition (b) for blocker com­putation is conservative. It is possible 
that there may ex­ist a frustum blocker with a silhouette edge, but we need to perform additional computations 
to identify such block­ers [NRJS03, Lai06]. In this case, we opt for simplicity, and rather search for 
some other blocker de.ned by a possibly  4c .2009 UNC-Chapel Hill. Reprinted, with permission, from 
UNC-Chapel Hill Technical Report TR09-008  (a) (b) (c) (d) Figure 3: Frustum Blocker Computation: (a) 
Example of a blocker with connected triangles. (b)-(c) Intersection and Walking: Identify intersecting 
edges (e1, e2, e3, and e4) and walk to the adjacent triangles (denoted by arrows from edge to the triangle). 
(d) Abort frustum blocker computation if a free-edge or a silhouette-edge is found. different set of 
triangles. Or we subdivide the frustum and the current object will become a blocker for a smaller sub­frustum. 
If we terminate the traversal test due to condition (a), we have successfully found a frustum blocker. 
All triangles in the frustum blocker are marked visible and the far plane depth associated with the frustum 
is updated. Note that the depth of the far plane of the frustum is chosen such that all triangles in 
the frustum blocker lie in front of the far plane. If we terminate due to condition (b), then the algorithm 
cannot guarantee the existence of a frustum blocker. All triangles processed during this step are still 
marked visible but the far plane depth is not updated. 3.3. Frustum Intersection Tests A key component 
of the algorithm is performing the inter­section tests of the scene primitives with a frustum. The al­gorithm 
traverses the BVH and performs intersection tests between a frustum and the AABBs associated with the 
BVH. We use the technique proposed by Reshetov et al. [RSH05] to perform fast intersection tests between 
a frustum and an AABB. For every leaf node of the hierarchy we perform the intersection test with the 
frustum and triangle(s) asso­ciated with that leaf node. In order to perform the intersec­tion test ef.ciently, 
we represent the corner rays of a frustum and the oriented edges of the triangle using Plücker coordi­nates 
[Sho98]. The orientation of a ray as seen along the edges of a triangle governs the intersection status 
of the ray with the triangle (see Figure 4(a)). Similarly, the orientation of four corner rays of the 
frustum as seen along the edges of a triangle governs the intersection status of the frustum with the 
triangle. We can determine with object-precision accu­racy whether the frustum lies completely inside 
the triangle, completely outside the triangle, or partially intersects the tri­angle [CLT*08]. In practice, 
the Plücker test is conservative and it can wrongly classify a frustum to be partially intersecting a 
tri­angle even if the frustum is completely outside the triangle (as shown in Figure 4(b)). This can 
affect the correctness of our algorithm as we may wrongly classify an object as a blocker due to these 
conservative intersection tests. We make (a) (b) (c) Figure 4: Conservative Plücker Tests: (a) All 
four corner rays of the frustum F1 have the same orientation as seen along every directed edge of the 
triangle ABC. Thus, F1 is completely-inside ABC. (b) Intersection between a frustum and a triangle can 
be conservative. F2 will be classi.ed as partially intersecting. (c) Different cases of frustum-edge 
in­tersections: F1 does not intersect the edge AB, F2 intersects AB. F3 is falsely classi.ed as intersecting 
with AB. sure that atleast one of the corner rays is inside the blocker to avoid such cases. Frustum-Edge 
Intersection: When a frustum partially in­tersects with a triangle, we can quickly determine which edges 
of the triangle intersect the frustum. We reuse the Plücker test between the frustum and the triangle 
to .nd the edges of the triangle that intersect the frustum. As shown in Figure 4(c), a frustum intersects 
with an edge if all four cor­ ner rays of the frustum do not have the same orientation as seen along 
the edge. This test may falsely classify an edge as intersecting even if the frustum does not intersect 
the edge, as shown in Figure 4(c) and thereby make our algorithm con­servative. This test is also used 
in Section 3.2 to compute a set of triangles that may block the frustum completely. Far Plane Depth Update: 
The far plane associated with a frustum is updated whenever a blocker is found. The blocker may correspond 
to a single triangle or multiple triangles. If a frustum lies completely inside a triangle, the triangle 
blocks the frustum. We, therefore, mark the triangle as visible and update the depth of the far plane 
of the frustum as shown in Figure 5(a). The frustum intersects the triangle at points H1 and H2, and 
d1 and d2 are the projected distances of |V H1| and |V H2|along the near plane normal. We set the far 
plane depth of the frustum as the maximum of the projected dis­tances. In other cases, the blocker may 
be composed of mul­tiple triangles. We update the far plane depth of the frustum as shown in Figure 5(b). 
We compute the far plane depth for every triangle in the frustum blocker, assuming the frustum is completely 
inside the triangle. In Figure 5(b), d and d ' ' are the far plane depths for triangles T1 and T1, respectively, 
of the frustum blocker. The far plane depth of the frustum is set to the maximum of far plane depths 
computed for the triangles in the frustum blocker, which is d ' in this case. 3.4. Frustum Subdivision 
Our algorithm implicitly assumes that the size of connected blockers is larger that the cross-section 
of the frusta. The simplest algorithm subdivides a frustum in a uniform man­  c5 .2009 UNC-Chapel Hill. 
Reprinted, with permission, from UNC-Chapel Hill Technical Report TR09-008 (a) (b) Figure 5: Updating 
Far Plane Depth: (a) Frustum lies com­pletely inside triangle T1. The depth of the far plane is set to 
' the maximum of d1 and d2. (b) Triangles T1 and T 1 consti­tute the blocker. We compute the far plane 
depths of each triangle and use the maximum of the depth values. ner. This approach is simpler to implement 
and also simpler to parallelize on current multi-core and many-core architec­tures, in terms of load 
balancing. However, many complex models (e.g. CAD models) have a non-uniform distribution of primitives 
in 3D. In that case, it may be more useful to perform adaptive subdivision of the frusta. In that case, 
we use the AD-FRUSTUM representation [CLT*08], which uses a quadtree data structure. We use the following 
criteria to perform subdivision. If the PVS associated with a frus­tum is large, we recursively compute 
the PVS associated with each sub-frustum. Whenever the algorithm only com­putes a partial blocker of 
connected triangles using the inter­section tests, we estimate its cross-section area and use that area 
to compute the sub-frusta. There are other known tech­niques to estimate the distribution of primitives 
[WWZ*06] and they can be used to guide the subdivision. As compared to uniform subdivision, adaptive 
techniques reduce the to­tal number of frusta traced for PVS computation [CLT*08]. Moreover, we use spatial 
coherence to reduce the number of intersection tests between the parent and child frusta. 3.5. Many-core 
Implementation Our algorithm is highly parallelizable as the PVS for each frustum can be computed independently. 
However, the union of these different PVSs have to be performed in a thread safe manner. This can be 
done by maintaining an array of bits, one bit per triangle, and marking a bit visible only when the corresponding 
triangle is found visible. The bits are reset only once at the start of the algorithm. In this case the 
time to query if a triangle is visible is O(1) but enumerating the visible triangles is O(N), where N 
is the number of triangles in the scene. To improve upon this we maintain a per thread hash map to compute 
PVS per thread. The PVS per thread is combined in the end to compute the .nal PVS. The average time to 
query if a triangle is visible is O(1) and the time to enumerate the visible triangles is O(K), where 
K is the number of visible triangles. Model PVS PVS Time Name Tris Ratio Size (ms) Armadillo 345K 1.16 
98K 30 Blade 1.8M 1.05 190K 90 Thai 10M 1.06 210K 66 SodaHall 1.5M 1.22 2.1K 15 PowerPlant 12M 1.25 15K 
130 Flamenco 40K 1.11 7K 16 Table 1: Performance Results: From-point conservative vis­ibility computation 
of models with varying complexity (CAD models, scanned models and dynamic scenes). All the tim­ings were 
computed on a 16-core 64-bit Intel X7350@2.93 GHz. The PVS ratio provides a measures of how conserva­tive 
is the computed PVS with respect to exact visibility. 4. Results and Analysis In this section, we present 
our results on from-point con­servative visibility (Section 4.1). We also compare our ap­ proach with 
prior visibility computation algorithms. Our re­sults were generated on a workstation with 16-core, 64-bit 
Intel X7350@2.93 GHz processors. We generate timings by using different number of cores (1 -16) for visibility 
com­putations and sound propagation. We also use SSE instruc­tions to accelerate frustum intersection 
tests and OpenMP to parallelize on multiple cores. 4.1. Visibility Results We demonstrate our results 
on computing from-point object space conservative PVS on a variety of models ranging from simple models 
(like soda hall, armadillo, blade) to complex models (like power plant and thai statue) to a dynamic 
model (.amenco animation). These models are shown in Figure 6. Our results are summarized in Table 1. 
We are not aware of any prior method that can compute the exact visible set on these complex models. 
Therefore, we compute an approxi­mation to pexact by shooting frusta at 4K ×4K resolution. The PVS-ratio 
refers to: (size of PVS) / (size of pexact ), and is a measure of how conservative is the computed PVS. 
In all benchmarks, we are able to compute a conservative ap­proximation to the PVS at interactive rates 
on multi-core PC. The frame sequences used for generating average results are shown in accompanying video. 
Further, we show that our approach converges well to pexact as we shoot higher num­ber of frusta (see 
Figure 7). Detailed results on convergence for each model are provided in the Appendix. Also, our ap­proach 
maps well on multi-core architectures. We observe linear scaling in performance as the number of cores 
increase (see Figure 8). 4.2. Analysis We analyze our algorithm and compare it with prior tech­niques. 
The accuracy of our algorithm is governed by the  6c .2009 UNC-Chapel Hill. Reprinted, with permission, 
from UNC-Chapel Hill Technical Report TR09-008   Figure 6: Benchmarks: Left to right: (a) Armadillo 
(345K triangles). (b) Blade (1.8M triangles). (c) Thai Statue (10M triangles). (d) Soda Hall (1.5M triangles). 
(e) PowerPlant Figure 8: Performance scaling vs. #Cores: The performance of our system scales linearly 
with the #cores. We have bench­marked our system on upto 16 cores. mostly been applied to scenes with 
large occluders (e.g. ar­ chitectural models). Recently, Overbeck et al. [ORM07] pre­ 128x128 256x256 
Figure 7: PVS ratio vs. #Frusta: As the number of frusta increase, the PVS computed by our approach converges 
to pexact . This graph shows the rate of convergence for differ­ent benchmarks. The CAD models have a 
higher ratio as compared to scanned models. accuracy of the intersection tests, which exploit the IEEE 
.oating-point hardware. Our approach is robust and general, and not prone to any degeneracies. Conservative 
approach: We compute a conservative PVS for every frustum. This follows from our basic approach to compute 
the blockers and far planes for each frustum. In practice, our approach can be overly conservative in 
some cases. The underlying blocker computation algorithm is con­servative. Moreover, we don t consider 
the case when the union of two or more objects can serve as a blocker. This is shown in Figure 2) with 
two disjoint occluders, V1 and V2. Instead of using more sophisticated algorithms for blocker computation, 
we found it cheaper to subdivide the frustum into sub-frusta and compute blockers for them. As a result, 
we can make our approach less conservative by using more frusta and the PVS (p) converges to pexact (see 
Figure 7). Model connectivity and triangle soup models: Our algo­rithm exploits the connectivity information 
in the model to compute the blockers, which are formed using connected tri­angles. If the connectivity 
information is not available, then the algorithm would subdivide the frustum such that each blocker would 
consist of only one triangle. 4.3. Comparisons Our approach performs volumetric tracing, which is similar 
to beam tracing. However, we don t perform exact clipping operations to compute an exact representation 
of the visi­ble surface. Rather we only estimate the triangles belong­ing to the PVS by identifying the 
blockers for each frus­tum. Beam tracing algorithms can also be accelerated by us­ing spatial data structures 
[FCE*98,LSLS09], but they have sented a fast beam tracing algorithm that is based on spatial hierarchies. 
We performed a preliminary comparison with an implementation of this beam tracing algorithm with FastV. 
We chose multiple key frames from the armadillo model sequence (see Video) and compared the PVS computed 
by FastV algorithm (with 4K ×4K uniform frusta) with the PVS computed by the beam tracer. We observed 
that FastV s PVS converges to within 1 -10% of the exact from-point beam tracing PVS (see Appendix). 
Furthermore, FastV appears to be more robust than the beam tracing solution as the beams may leak between 
the triangles due to numerical issues (see Video). It is not clear whether current beam tracing algo­rithms 
can robustly handle complex models like the power­plant. In terms of performance, FastV is about 5 -8 
times faster on a single core on the armadillo model, as compared to [ORM07]. Moreover, FastV is relatively 
easier to paral­ lelize on multi-core architectures. Most of the prior object space conservative visibility 
culling algorithms are designed for scenes with large occluders [BHS98,CT97,HMC*97,LG95]. These algorithms 
can work well on special types of models, e.g. architectural models represented using cells and portals 
or urban scenes. In con­trast, our approach is mainly designed for general 3D models and doesn t make 
any assumption about large occluders. It is possible to perform conservative rasterization using current 
GPUs [AMA05]. However, it has the overhead of rendering additional triangles and CPU-GPU communication 
latency. It is hard to make a direct comparison with image space ap­proaches because of their accuracy. 
In practice, image space approaches can exploit the rasterization hardware or fast ray­tracing techniques 
[RSH05] and would be faster than FastV. Moreover, image space approaches also perform occluder fusion 
and in some cases may compute a smaller set of visi­ble primitives than FastV. However, the main issue 
with the image space approaches is deriving any tight bounds on the accuracy of the result. This is highlighted 
in the appendix, where we used ray tracing to approximate the visible prim­itives. In complex models 
like the powerplant, we need a sampling resolution of at least 32K × 32K to compute a good approximation 
of the visible primitives. At lower reso­  c7 .2009 UNC-Chapel Hill. Reprinted, with permission, from 
UNC-Chapel Hill Technical Report TR09-008 lutions, the visible set computed by the ray tracing algorithm 
doesn t seem to converge well. 5. Geometric Sound Propagation In this section, we describe our geometric 
sound propaga­tion algorithm based on FastV. Given a point sound source, the CAD model with material 
properties (i.e. the acoustic space), and the receiver position, the goal is to compute the impulse response 
(IR) of the acoustic space. Later the IRs are convolved with an audio signal to reproduce the sound. 
We use our PVS computation algorithm described above for fast image-source computation that only takes 
into account specular re.ections [AB79, FCE*98, LSLS09]. In practice, this approach is only accurate 
for high frequency sources. Each image source radiates in free space and considers sec­ondary sources 
generated by mirroring the location of the in­put source over each boundary element in the environment. 
For each secondary source, the specular re.ection path can be computed by performing repeated intersections 
of a line segment from the source position to the position of the re­ceiver. In order to accurately compute 
all propagation paths, the algorithm creates image-sources (secondary sources) for every polygon in the 
scene. This step is repeated for all the secondary sources upto some user speci.ed (say k) or­ders of 
re.ection. Clearly, the number of image sources are O(Nk+1), where N is the number of triangles in the 
scene. This can become expensive for complex models. We use our PVS computation algorithm to accelerate 
the computation for complex scenes. We use a two stage al­gorithm. In the .rst stage, we use our conservative 
visibil­ity culling algorithm and compute all the secondary image sources up to the speci.ed orders of 
re.ection. Since we overestimate the set of visibility triangles, we use the second stage to perform 
a validation step. For the .rst stage, we use a variant of Laine et al. s [LSLS09] algorithm and only 
com­ pute the secondary image-sources for the triangles that are visible from the source. Speci.cally, 
we shoot primary frusta from the sound source. For every primary frustum we com­pute its PVS. We then 
re.ect the primary frustum against all visible triangles to create secondary frusta, which is simi­lar 
to creating image-sources for visible triangles. This step is repeated for secondary frusta upto k orders 
of re.ection. In the second stage, we construct paths from the listener to the sound source for all the 
frusta which reach the listener. As our approach is conservative, we have to ensure that this path is 
a valid path. To validate the path, we intersect each segment of the path with the scene geometry and 
if an inter­section is found the path is discarded. 5.1. Results We present our results on accurate geometric 
sound propa­gation in this section. Table 2 summarizes our results. We  (a) (b) (c) (d) Figure 9: Geometric 
sound propagation approaches: Given a sound source, S, and triangles (a, b, c, d, and e) the image source 
method (see 9a) creates image-sources of S against all primitives in the scene. Beam tracing method (see 
9b) computes image-sources for only exactly visible triangles, b and c in this case. Accelerated beam 
tracing approach com­putes image-sources for all triangles inside the beam volume (see 9c), i.e., b, 
c, d, and e in this case. Our implementation (see 9d) computes image-sources for triangles b, c, and 
d in the PVS as computed according to the technique described in previous sections. perform geometric 
sound propagation on models of vary­ing complexity from 438 triangles to 212K triangles. We use three 
benchmarks presented in accelerated beam trac­ing (ABT) algorithm [LSLS09]. We also used two addi­tional 
complex benchmarks with 80K and 212K triangles. We are not aware of any implementation of accurate geomet­ric 
propagation that can handle models of such complexity. Model Tris Time Speed Up (sec) (ABT) Simple Room 
438 .16 10.1X Regular Room 1190 .93 22.2X Complex Room 5635 6.5 11.8X Sibenik 78.2K 72.0 Trade Show 212K 
217.6 Table 2: Accurate sound propagation: The performance of sound propagation algorithms for three 
orders of re.ection on a single core. We observe 10-20X speedup on the simple models over accelerated 
beam tracing (ABT) [LSLS09]. 5.2. Comparison with Prior Approaches Most accurate geometric acoustic methods 
can be described as variants of the image-source method. Figure 9 com­pares different accurate geometric 
sound propagation meth­ods. The main difference between these methods is in terms of which image-sources 
they choose to compute [FCE*98, LSLS09]. A naïve image source method computes image sources against all 
triangles in the scene [AB79]. Beam trac­ ing methods compute the image-sources for exactly visible triangles 
from a source. Methods based on beam tracing, like accelerated beam tracing [LSLS09], computes image­ 
sources for every triangle inside the beam volume. Our ap­proach, shown in Figure 9(d), .nds the conservative 
PVS from a source and computes image-sources for the trian­gles in the conservative PVS. Thus, for a 
given model our approach considers more image-sources compared to beam  8c .2009 UNC-Chapel Hill. Reprinted, 
with permission, from UNC-Chapel Hill Technical Report TR09-008 tracing. It is an ef.cient compromise 
between the expensive step to compute exactly visible triangles in beam tracing vs. computing extra image-sources 
in accelerated beam tracing. We observe 10 -20X speedups over prior accurate geomet­ric sound propagation 
algorithms. Chandak et al. [CLT*08] also used adaptive frustum tracing for geometric sound prop­agation. 
However, they perform discrete clipping and there­fore, it is hard to derive good bounds on its accuracy. 
6. Limitations and Conclusions Our approach has some limitations. Since we don t perform occluder fusion, 
the PVS computed by our algorithm can be overly conservative sometimes. If the scene has no big oc­cluders, 
we may need to trace a large number of frusta. Our intersection tests are fast, but the conservative 
nature of the blocker computation can result in a large PVS. The model and its hierarchy are stored in 
main memory, and therefore our approach is limited to in-core models. Our algorithm is easy to parallelize 
and works quite well, but is still slower than image space approaches that perform coherent ray trac­ing 
or use GPU rasterization capabilities. Conclusions: We present a fast and simple visibility culling algorithm 
and demonstrate its performance on complex models. The algorithm is general and works well on com­plex 
3D models. To the best of our knowledge, this is the .rst from-point object space visibility algorithm 
that can handle complex 3D models with millions of triangles at almost in­teractive rates. Future Work: 
There are many avenues for future work. We will like to implement the algorithm on a many-core GPU or 
upcoming Larrabee processor to further exploit the high parallel performance of commodity processors. 
This could provide capability to design more accurate rendering algo­rithms based on object-precision 
visibility computations on complex models (e.g. shadow generation). We will also like to evaluate the 
trade-offs of using more sophisticated blocker computation algorithms [NRJS03, Lai06]. In terms of sound 
propagation, our approach can be extended to compute edge diffraction based on uniform theory of diffraction 
(UTD). References [AB79] ALLEN J. B., BERKLEY D. A.: Image method for ef­ .ciently simulating small-room 
acoustics. The Journal of the Acoustical Society of America 65, 4 (April 1979), 943 950. [AMA05] AKENINE-MÖLLER 
T., AILA T.: Conservative and tiled rasterization using a modi.ed triangle set-up. journal of graphics 
tools 10, 3 (2005), 1 8. [BHS98] BITTNER J., HAVRAN V., SLAVIK P.: Hierarchical vis­ ibility culling 
with occlusion trees. Computer Graphics Interna­ tional, 1998. Proceedings (Jun 1998), 207 219. [BW05] 
BITTNER J., WONKA P.: Fast exact from-region visi­ bility in urban scenes. Eurographics Symposium on 
Rendering (2005), 223 230. [CLT* 08] CHANDAK A., LAUTERBACH C., TAYLOR M., REN Z., MANOCHA D.: Ad-frustum: 
Adaptive frustum tracing for in­teractive sound propagation. In Proc. IEEE Visualization (2008). [COCSD03] 
COHEN-OR D., CHRYSANTHOU Y., SILVA C., DU-RAND F.: A survey of visibility for walkthrough applications. 
Visualization and Computer Graphics, IEEE Transactions on 9, 3 (July-Sept. 2003), 412 431. [CT97] COORG 
S., TELLER S.: Real-time occlusion culling for models with large occluders. In SI3D 97: Proceedings of 
the 1997 symposium on Interactive 3D graphics (New York, NY, USA, April 1997), ACM, pp. 83 ff. [DD02] 
DUGUET F., DRETTAKIS G.: Robust epsilon visibility. Proc. of ACM SIGGRAPH (2002), 567 575. [Dur99] DURAND 
F.: 3D Visibility, Analysis and Applications. PhD thesis, U. Joseph Fourier, 1999. [FCE* 98] FUNKHOUSER 
T., CARLBOM I., ELKO G., PINGALI G., SONDHI M., WEST J.: A beam tracing approach to acoustic modeling 
for interactive virtual environments. In Proc. of ACM SIGGRAPH (1998), pp. 21 32. [Gha01] GHALI S.: A 
survey of practical object space visibility algorithms. In SIGGRAPH Tutorial Notes (2001). [HH84] HECKBERT 
P. S., HANRAHAN P.: Beam tracing polyg­onal objects. In Proc. of ACM SIGGRAPH (1984), pp. 119 127. [HMC* 
97] HUDSON T., MANOCHA D., COHEN J., LIN M., HOFF K., ZHANG H.: Accelerated occlusion culling using shadow 
frusta. In Proc. of ACM Symposium on Computational Geometry (1997), pp. 1 10. [KCCO00] KOLTUN V., CHRYSANTHOU 
Y., COHEN-OR D.: Virtual occluders: An ef.cient intermediate pvs representation. In Eurpographics Workshop 
on Rendering (2000), pp. 59 70. [KS00] KLOSOWSKI J., SILVA C.: The prioritized-layered pro­jection algorithm 
for visible set estimation. IEEE Trans. on Visu­alization and Computer Graphics 6, 2 (2000), 108 123. 
[Lai06] LAINE S.: An Incremental Shaft Subdivision Algorithm for Computing Shadows and Visibility. Master 
s thesis, Helsinki University of Technology, March 2006. [Len93] LENHERT H.: Systematic errors of the 
ray-tracing al­goirthm. Applied Acoustics 38 (1993), 207 221. [LG95] LUEBKE D., GEORGES C.: Portals and 
mirrors: Simple, fast evaluation of potentially visible sets. In ACM Interactive 3D Graphics Conference 
(Monterey, CA, 1995), pp. 105 108. [LSCO03] LEYVAND T., SORKINE O., , COHEN-OR D.: Ray space factorization 
for from-region visibility. Proc. of ACM SIG-GRAPH (2003), 595 604. [LSLS09] LAINE S., SILTANEN S., LOKKI 
T., SAVIOJA L.: Ac­celerated beam tracing algorithm. Applied Acoustic 70, 1 (2009), 172 181. [MBW08] 
MATTAUSCH O., BITTNER J., WIMMER M.: Chc++: Coherent hierarchical culling revisted. Proc. of Eurographics 
(2008), 221 230. [NB04] NIRENSTEIN S., BLAKE E.: Hardware accelerated vis­ibility preprocessing using 
adaptive sampling. In Eurographics Workshop on Rendering (2004). [Nir03] NIRENSTEIN S.: Fast and Accurate 
Visibility Preprocess­ing. PhD thesis, University of Cape Town, South Africa, 2003.  c9 .2009 UNC-Chapel 
Hill. Reprinted, with permission, from UNC-Chapel Hill Technical Report TR09-008 [NRJS03] NAVAZO I., 
ROSSIGNAC J., JOU J., SHARIF R.: Shieldtester: Cell-to-cell visibility test for surface occluderis. In 
Proc. of Eurographics (2003), pp. 291 302. [ORM07] OVERBECK R., RAMAMOORTHI R., MARK W. R.: A Real-time 
Beam Tracer with Application to Exact Soft Shadows. In Eurographics Symposium on Rendering (Jun 2007), 
pp. 85 98. [RSH05] RESHETOV A., SOUPIKOV A., HURLEY J.: Multi-level ray tracing algorithm. ACM Trans. 
Graph. 24, 3 (2005), 1176 1185. [Sho98] SHOEMAKE K.: Pluecker coordinate tutorial. Ray Trac­ing News 
11, 1 (1998). [Tel92] TELLER S. J.: Visibility Computations in Densely Oc­cluded Polyheral Environments. 
PhD thesis, CS Division, UC Berkeley, 1992. [WWZ* 06] WONKA P., WIMMER M., ZHOU K., MAIERHOFER S., HESINA 
G., RESHETOV A.: Guided visibility sampling. Proc. of ACM SIGGRAPH (2006), 494 502.  .2009 IEEE. Reprinted, 
with permission, from IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 13, NO. 6, NOVEMBER/DECEMBER 
2007 Interactive Sound Rendering in Complex and Dynamic Scenes using Frustum Tracing Christian Lauterbach, 
Anish Chandak, and Dinesh Manocha, Member, IEEE Abstract We present a new approach for real-time sound 
rendering in complex, virtual scenes with dynamic sources and objects. Our approach combines the ef.ciency 
of interactive ray tracing with the accuracy of tracing a volumetric representation. We use a four-sided 
convex frustum and perform clipping and intersection tests using ray packet tracing. A simple and ef.cient 
formulation is used to compute secondary frusta and perform hierarchical traversal. We demonstrate the 
performance of our algorithm in an interactive system for complex environments and architectural models 
with tens or hundreds of thousands of triangles. Our algorithm can perform real-time simulation and rendering 
on a high-end PC. Index Terms Acoustic propagation,Interactive systems 1 INTRODUCTION Traditionally, 
the focus in interactive visualization has been on high­quality, realistic visual rendering of complex 
datasets. These devel­opments are supported by high growth rates and programmability of current graphics 
hardware as well as advances in rendering accelera­tion algorithms. However, at the same time it is important 
to develop interactive algorithms for sound or auditory rendering. In a multi­sensory visualization system, 
spatial sound can be combined with vi­sual rendering to provide a more immersive experience for many 
ap­plications [28]. These can be used for the development of an audi­tory display [38, 16, 31] to convey 
intuitive spatial cues directly and therefore can result in better understanding and evaluation of complex 
datasets. In this paper we address the problem of interactive sound rendering and visualization in complex 
and dynamic environments.. Some of the driving applications include acoustic design of architectural 
mod­els or outdoor scenes, walk-throughs of a virtual prototype of a large CAD model with sounds of machine 
parts or moving people, virtual environments with multiple avatars, or even visualization of multi­dimensional 
datasets [28]etc. The sound rendering algorithms take into account the knowledge of sound sources, listener 
locations, 3D models of the environments, and material absorption data to generate realistic and spatialized 
sound effects. Over the last few decades, the problem of fast visual rendering of complex datasets has 
received considerable attention in computer graphics and visualization literature. Current algorithms 
and systems are able to handle complex datasets composed of millions of primitives at interactive rates 
on commodity hardware. In contrast, prior sound rendering are limited to relatively simple models and 
cannot handle complex or dynamic datasets at interactive rates. The main challenge in sound rendering 
is to compute the re.ection paths from the sound sources to the listeners at interactive rates. Prior 
approaches for com­plex environments have been based on geometric methods that use ei­ther ray or beam 
tracing methods to explicitly follow the paths. How­ever, ray tracing methods are prone to inaccuracies 
due to sampling or aliasing errors, and beam tracing methods involve considerable pre­processing and 
are limited to static, densely-occluded environments. As a result, current interactive applications are 
limited to using sound sources that are associated with a static, precomputed effect. The authors are 
with the Department of Computer Science, Campus Box 3175, Sitterson Hall, University of North Carolina-Chapel 
Hill, Chapel Hill, NC 27599. E-mail: {cl, achandak, dm}@cs.unc.edu. Manuscript received 31 March 2007; 
accepted 1 August 2007; posted online 27 October 2007. Published 14 September 2007. For information on 
obtaining reprints of this article, please send e-mail to: tvcg@computer.org.  Main Results: We present 
an interactive algorithm for sound ren­dering using frustum tracing. Our approach uses a simple volumetric 
representation based on a four-sided convex frustum, for which de­scribe ef.cient algorithms to perform 
hierarchy traversal, intersection and specular re.ection and transmission interactions at the geometric 
primitives. Unlike beam tracing and pyramid tracing algorithms, we perform approximate clipping by using 
a subdivision into sub-frusta. As a result our rendering algorithm reduces to tracing ray packets and 
maps well to the SIMD instructions available on current CPUs. We support dynamic scenes by using bounding 
volume hierarchies (BVHs) to accelerate the computations on complex models. Overall, our approach combines 
the ef.ciency of interactive ray tracing with the accuracy of tracing a volumetric representation. We 
have implemented our algorithm and have used it for interac­tive sound rendering in complex environments 
composed of tens or hundreds of thousands triangles and dynamically moving objects. The performance of 
our system varies with the complexity of the envi­ronments, especially as a function of the number of 
re.ections. In practice, our approach can trace enough frusta to simulate sound on a current high-end 
PC at interactive rates with up to 7 re.ections. As compared to prior geometric approaches for sound 
rendering, our approach offers the following advantages: Generality: No special or logical scene representation 
is neces­sary and our algorithm is able to handle all polygonal models.  Ef.ciency: Our algorithm scales 
with the complexity of the scenes as a logarithmic function of the model size (although a linear complexity 
update step is needed whenever geometry moves). Most of the bene.ts of ray packet tracing are directly 
ap­plicable, including SIMD implementation and trivial paralleliza­tion on multi-core processors.  Dynamic, 
complex scenes: We can handle all kind of dynamic scenes and make no assumptions on the motion of sound 
sources, listener or objects in the scene.  Integrated visual and sound rendering: We use a BVH to per­form 
fast intersection tests between ray packets and the primi­tives. The same hierarchy can be used for ray 
tracing for visual rendering and frustum tracing for sound rendering.  Organization: The rest of the 
paper is organized in the following manner: we give a brief overview of prior work on sound propagation 
in Section 2. Section 3 presents our frustum tracing algorithm and shows how to use the algorithm to 
compute the re.ection paths from the sound sources to the listeners. We describe our implementation in 
Section 4 and demonstrate its performance on different models in Section 5. We analyze the performance 
in Section 6 and highlight a few limitations of our approach.  2 PREVIOUS WORK There has been considerable 
work on sound generation and propaga­tion in computational acoustics, computer graphics, computational 
ge­ometry and related areas for more than four decades [5, 7, 14]. These include physically-based sound 
synthesis algorithms [19, 32], numer­ical and geometric methods for sound propagation and acceleration 
techniques. In this section we give a brief overview of sound propaga­tion algorithms. Numerical methods: 
Numerical solutions [24] attempt to accu­rately model the propagation of sound waves by numerically solving 
the wave equation. These methods are general and highly accurate [33]. However, they can be very compute 
and storage intensive [41]. Current approaches are too slow for interactive sound propagation in complex 
environments and are mainly limited to simple scenes. Geometric methods: These algorithms model the 
propagation of sound based on rectilinear propagation of waves and can accurately model the early re.ections. 
Most of these methods are closely related to parallel techniques in global illumination, and many advances 
in either .eld can also be applied to the other. The earliest of these ap­proaches were particle and 
ray based [23, 25] and simulated the prop­agation paths by stochastically sampling them using rays. Based 
on recent advances in interactive ray tracing, these methods are also appli­cable to dynamic scenes [45, 
26]. Approaches using discrete particle representations called phonons or sonels [3, 9, 22] have been 
devel­oped in the last few years. These methods look very promising but are currently limited to simple 
scenes. Moreover, particle and ray-based algorithms are susceptible to aliasing errors and may need a 
very high density of samples to overcome those problems. The image source algorithms create virtual sources 
for specular re­.ection from the scene geometry and can be combined with diffuse re.ections and diffractions 
[4, 8]. They accurately compute the prop­agation paths from the source to the listener, but the number 
of virtual sources can increase exponentially for complex scenes [4]. This makes these techniques suitable 
only for static scenes. The third type of geometric methods is based on beam tracing, which recursively 
traces pyramidal polyhedra from the source to the listener [18, 10, 11]. In their seminal work, Funkhouser 
et al. [12, 13] showed how beam tracing methods can be used for sound propagation at interactive rates 
in complex virtual environments. Some algorithms have been proposed to use beam tracing on moving sources 
[2, 15]. However, current algorithms take large pre-processing time and are not directly applicable to 
dynamic scenes with moving objects. Interactive Sound Propagation: Many other methods have been presented 
for rendering of room acoustics [29, 36, 42] or have been integrated with VR systems [30]. Joslin and 
Thalmann [21] present a technique to reduce the number of facets in order to accelerate the re­.ection 
computations in sound rendering. A point-based algorithm for multi-resolution sound rendering has been 
presented for scenes with a large number of emitters [46]. Doel et al. [43] present an al­gorithm for 
interactive simulation of complex auditory scenes using model-pruning techniques based on human auditory 
perception. Our approach is complementary to many of these algorithms and can be combined to further 
improve the performance. 3 FRUSTUM TRACING In this section we present our algorithm for interactive sound 
prop­agation in complex and dynamic scenes. Our approach is built on recent advances in interactive ray 
tracing, including packet traversal algorithms [44] and dynamic scenes [45, 26]. 3.1 Frustum Representation 
As discussed above, ray tracing algorithms for sound propagation suf­fer from noise and aliasing problems 
[27], both spatially and tempo­rally. In order to avoid these sampling issues, we trace a simple con­vex 
polyhedron instead of in.nitesimal rays. Speci.cally, we perform frustum tracing1, which is similar to 
beam tracing and pyramid trac­ 1We use the term frustum tracing in a different sense than earlier work 
on radio propagation presented in [40], which is very similar to beam tracing.  Fig. 1. Frustum-based 
packet: The frustum primitive used in our algorithm. a) The frustum is de.ned by the four side faces 
and the front face, or equiv­alently by the boundary rays on the sides where the faces intersect. b) 
the frustum is uniformly subdivided into sub-frusta de.ned by their center sample rays (dots), depending 
on a sampling factor. ing. We use a simple convex frustum so that we can perform fast intersection tests 
with the nodes of the hierarchy and the primitives. Unlike beam tracing algorithms, we perform approximate 
clipping us­ing ray packets. Overall, our representation combines some of the speed advantages of ray 
packet tracing with the bene.ts of volumetric formulations. We use a convex four-sided frustum, i.e. 
a pyramid with a quadri­lateral base (see Fig. 1(a)) that is de.ned by its four side faces and one front 
face. Equivalently, the frustum can be represented as the convex combination of four corner rays de.ning 
the frustum. At a broad level, the main difference between frustum and beam tracing is how we keep track 
of intersections with the primitive and the scene. Beam tracing performs exact clipping with each primitive 
in the scene and therefore needs to maintain a full list of clipped edges or faces of the beam. We avoid 
these relatively expensive operations by subdi­viding the frustum uniformly into smaller sub-frusta to 
perform dis­crete clipping, and only keep track of intersections at the level of those sub-frusta (see 
1(b)). Moreover, each sub-frustum is represented by a sample ray, and a sub-frusta is considered to intersect 
a primitive only if its sample ray hits the primitive. Essentially, this can be interpreted as a discrete 
version of a clipping algorithm and can introduce some errors in our propagation algorithm. The difference 
between the frustum and beam tracing process is also highlighted in Fig. 2. We show the intersection 
of the beam (left) and frusta (right) with three primitives and the resulting secondary beams and frusta 
computed for re.ection and transmission. Note that since the intersection is determined by the location 
of the sample ray, the frustum tracing algorithm in this example will underestimate the size of secondary 
beams at the primitive on the left. The amount of error introduced depends on the sampling rate, i.e. 
the rate of subdivision of the frustum. Bene.ts: Our formulation of the frustum and the clipping algo­rithm 
allows a faster and more general algorithm for propagation. We use the main frustum as a placeholder 
for all the enclosed sub-frusta during hierarchy traversal or intersection computations. As a result 
we are able to achieve very ef.cient and fast traversal using our represen­tation in both static and 
dynamic scenes. In addition, we organize our sample rays in ray packets similar to those used in interactive 
ray trac­ing, and exploit the uniform subdivision of frusta for faster primitive intersection computations. 
Finally, we defer constructing the actual sample ray computation until the sub-frusta are actually needed, 
i.e. if the whole frustum does not fully hit a primitive. This reduces the set-up cost, especially for 
very small beams. 3.2 Frustum Tracing The goal of frustum tracing is to identify the primitives (i.e. 
triangles) that intersect the frustum and then to construct new secondary beams that represent specular 
re.ection and transmission of sound. This in­volves traversing the scene hierarchy, computing the intersection 
with primitives and then constructing secondary frusta. We present algo­rithms for each of these computations. 
Construction of secondary frusta: Whenever a frustum hits a primitive, we construct secondary frusta 
for transmission and specular  Fig. 2. Beam vs. frustum tracing: Our approach compared to beam trac­ing 
for a simple example.(Left): beam tracing. (Right): frustum tracing. The discrete sampling in our frustum 
based approach underestimates the size of the exact re.ection and transmission frustum for primitive 
1 and overestimates the size for primitives 2 and 3. re.ection. If the entire frustum hits one primitive, 
the construction of the secondary frusta is simple and can be accomplished by just using the four corner 
rays. For the general case, when different sub-frusta hit different primitives, multiple secondary frusta 
have to be generated. A naïve solution would be to generate re.ection and transmission for each single 
sub-frustum de.ned by a sample ray. However, this could result in an extremely high number of additional 
frusta, and the com­plexity of the algorithm will grow as an exponential function of the number of re.ections. 
To avoid this, we combine those sub-frusta that hit the same primitive by hierarchically comparing four 
neighboring samples and treating them as one larger frustum (see Fig. 3). This can be seen as a quad-tree 
structure, although we do not compute the tree explicitly. If the samples hit neighboring primitives 
that have the same material and normal, we combine those primitives in the same way to avoid splitting 
too many sub-frusta. This is especially useful when rectangles are represented by two triangles, which 
is a common case in architectural models. In practice, we have found that our ap­proach yields a good 
compromise between the time taken to .nd opti­mal groups of sub-frusta and the number of secondary frusta 
needed. We also exploit the fact that the combined frustum exactly represents the sub-frusta, and there 
is no loss of accuracy due to this hierarchical grouping. If the primitives in the scene are over-tessellated, 
we could use simpli.cation algorithms to decrease their size [21]. This can in­troduce some additional 
error in our propagation algorithm, but big triangles in the scene would result in fewer secondary sub-frusta. 
Hierarchy traversal: We use a bounding volume hierarchy (BVH) as our choice of scene hierarchy, as it 
has been shown to work well for general dynamic scenes. However, our algorithm can also be adapted to 
be used with kd-trees or other hierarchies. The main operation for traversal of the BVH is checking for 
intersection with a BV, most commonly an axis-aligned bounding box (AABB). As described by Reshetov et 
al. [35], a frustum can be tested for overlap with an AABB quickly. If the frustum does not intersect 
the AABB node, the entire subtree rooted at that node can be culled. Otherwise the children of the node 
are tested in a recursive manner. However, this traversal method can result in traversing too many nodes, 
because traversal cannot stop until the .rst hit between the scene geometry and the frustum has been 
computed. Interactive ray tracing algorithms using BVHs also track which rays in the packet are still 
currently active (i.e. hit the current node) at any point during traversal [45, 26]. Since we want to 
avoid performing intersection tests with the frustum s sample rays as long as possible, we also keep 
track of the farthest intersection depth found so far to rule out intersecting nodes that cannot possibly 
contribute. Ef.cient primitive intersection: We assume that the models are triangulated. The main goal 
for intersection with triangles is to min­imize the number of ray-triangle intersections, as they can 
be more expensive than the traversal steps. Most importantly we want to avoid performing any ray intersections 
at all if we can determine that the Fig. 3. Constructing secondary frusta: We compute re.ected and transmitted 
frusta ef.ciently by grouping sub-frusta that hit the same primitive together in a single secondary frustum 
instead of having to trace each of them individually. Using a hierarchical process, we combine groups 
of four sub-frusta together as long as they hit the same primitive. entire frustum hits the primitive, 
which can happen many times. Con­sider Fig. 4, which shows the different con.gurations that can arise 
when intersecting a frustum with a primitive. Case 1 shows that the frustum fully misses the primitives 
(i.e. no overlap at all); therefore, we can skip that intersection right away. Case 2 shows that the 
frus­tum fully hits the primitives, which means we can construct secondary frusta right away without 
having to consider subdividing the frustum, unless a closer hit is found later on. In cases 3 and 4, 
the frustum par­tially overlaps the primitive or contains the primitive and we have to consider the individual 
sub-frusta. We test for these four cases by using a Plücker coordinate repre­sentation for the triangle 
edges and frustum rays [37], which gives us a way to test the orientation of any ray relative to an edge. 
Given a consistent orientation of edges (clockwise or counter-clockwise), we can test for intersection 
if all the edge orientations have the same sign. When testing the corner rays of the frustum, which can 
be performed in parallel using SIMD instructions, we check for Case 1 and Case 2 simply by testing whether 
all the corner rays are inside the triangle (Case 2) or fully outside one or more edges (Case 1). Note 
that the lat­ter test is conservative and may conclude that the frusta are intersecting the triangle, 
even if they are not. These intersections will eventually be culled in our handling of Cases 3 and 4. 
If no early culling is possible, we then perform a ray-triangle in­tersection using the actual sample 
rays. As the number of rays that actually intersect the triangle may be small compared to the number 
of sample rays representing all the sub-frusta, we .rst compute the subset of potential intersections 
ef.ciently. Since the sample rays are uniformly distributed in the frustum space, we compute bounds on 
the projected triangle in that space and only test those samples that fall within those bounds. In order 
to perform these computations, we clip the triangle to the bounds of the frustum by projecting the triangle 
to one of the coordinate planes and use a line clipping algorithm against the frustum s intersection 
with the plane. Finally, when looking at the clipped polygon s vertices, we can compute their bounding 
box in frustum parameter space (see Fig 5). The actual triangle intersection is only performed for the 
sample rays that fall within the boundary of the clipped triangle, and can easily be performed by using 
the indices. Note that this can also be reduced to a rasterization problem: given a triangle that is 
projected into the far plane of the frustum, we want to .nd the sub-frusta it covers. Therefore, we can 
use other ways to eval­uate this intersection. By using a higher set-up cost, the triangle could be projected 
and processed with a scan-line rendering algorithm, inter­secting with the respective sample ray for 
each covered sub-frustum. Another interesting approach would be to use a modi.ed A-buffer [6] for computing 
the sub-frusta covered by the triangle through lookup masks, at the cost of some precision. Handling 
non-specular interactions: As described above, specu­lar re.ections and transmissions can be handled 
directly. Although we have not implemented this, our frustum tracing approach can also use the diffraction 
formulation described by Funkhouser et al. [13] based on the uniform theory of diffraction. For diffuse 
scattering the frustum tracing approach could be adapted to also generate secondary frusta on a hemisphere 
around the hit point. However, this could increase the  Fig. 4. Primitive intersection: Five different 
cases can occur when intersecting a frustum with a triangle. From left to right: Frustum misses completely, 
frustum is contained, frustum intersects partially, frustum contains triangle. The last case shows a 
situation where the frustum is clearly outside the triangle, but is not detected by the edge based test 
since it is not fully on one side of any edge. This case is handled as intersecting, but is culled later 
on during the clipping test. Fig. 5. Packet-triangle intersection: Our novel intersection algorithm 
quickly computes the potential ray intersections in frustum space by clipping the triangle to the frustum 
s edges in 2-D, then .nding the rectangular bounds of the clipped point in frustum space. The bounds 
can then be used to effectively limit the number of actual sample rays that have to be tested. branching 
factor per interaction dramatically and therefore have a high impact on performance. 3.3 Sampling and 
Aliasing Our algorithm uses a discrete approximation of the exact secondary beams that would be computed 
by using an exact clipping algorithm. As a result the re.ections obtained by our method can suffer from 
aliasing artifacts, especially along object boundaries. As shown in Fig. 2, re.ected frusta often subtend 
areas that are outside of the prim­itive or do not cover all of the area. This is due to the fact that 
our tracing algorithm assumes that a sub-frustum hits the primitive in its full projected area if its 
sample ray hits the primitive. This can result in other possible effects such as missing paths, e.g. 
a small hole in the object might be missed due to our sampling density. Fortunately, these artifacts 
only result in some missed contribution paths from the re.ections. Moreover, in a dynamic environment 
these effects would be far less obvious to the listener as compared to the noise artifacts that can arise 
due to stochastic sampling in ray tracing methods. Note that our algorithm will also avoid creating holes 
or overlaps in the re.ec­tions .eld during the computation of re.ected or transmitted frusta. These holes 
or overlaps can have a far larger contribution of error since they tend to be more apparent in an interactive 
application be­cause of abrupt changes in the contribution. An interesting aspect of our approach is 
that having small geometric objects or primitives (i.e. a statue) in the scene will not result in a very 
high number of small secondary frusta. Instead, the number of re.ections is bounded by the sampling density 
in the packet. These very small frusta would be com­puted by an exact clipping algorithm, though they 
have very little or no contribution. One of the main challenges is to compute an appropriate sampling 
rate (i.e. the number of rays in the frustum). Ideally, the sampling rate could be chosen by taking the 
highest detail in the scene and setting the frequency so that detail could be reconstructed. Similar 
to rasteri­zation algorithms, performing this computation in a view-independent manner is almost infeasible 
due to its high complexity and can lead to very conservative bounds. As a result we use realistic sampling 
rates and allow some error. There are several approaches for choosing the sampling rate in this context: 
.rst, a good way of choosing the subdi­vision is to select the number of rays depending on the angular 
spread of the packet. For example, a very narrow frustum will likely need a lower sampling density than 
a wide frustum. Since the actual rays are not constructed until a suf.ciently small primitive is encountered, 
it is also possible to select the sampling rate relative to the local geometric complexity in order to 
avoid under-sampling. One way to measure lo­cal complexity, for instance, would be to use the current 
depth of the subtree in the BVH. Finally, the sampling rate can also be made depen­dent on the energy 
carried by a frustum or the number of re.ections before reaching the current position. This is a useful 
approximation as the actual contribution will likely decrease, and we can lower the sampling rate after 
a few re.ections. 4 IMPLEMENTATION We now describe the overall sound rendering system that uses our sound 
propagation algorithm. Our system is designed to be fully real­time and dynamic. We allow movement of 
the listener, the sound sources and the geometric primitives in the scene. The sound prop­agation algorithm 
is run as an asynchronous thread from the rest of the system. The sound propagation simulation starts 
out from each point sound source and constructs frusta from that origin that span the whole sphere of 
directions around it according to a prede.ned subdivision factor. Each of the frusta is traced through 
the scene, and secondary frusta are constructed based on the algorithm described in Section 3. There 
is a user-speci.ed maximum re.ection order that limits the number of total frusta that need to be computed. 
Attenuation and other wavelength­dependent effects are applied according to the material properties per 
frequency band. Since we regenerate the sound contributions at each frame, we do not save the full beam 
tree of the simulation, but just the those that actually contain the listener. Handling dynamic scenes: 
The choice of a BVH as an accel­eration structure allows us to update the hierarchy ef.ciently in linear 
time if the scene geometry is animated, or rebuild it if a heuristic deter­mines that culling ef.ciency 
of the hierarchy is low [26]. As the BVH is a general structure, our algorithm can handle any kind of 
scene in­cluding unstructured polygon soup and models with low occlusion. Furthermore, we can use lazy 
techniques to rebuild the nodes of a hi­erarchy in a top-down manner. Auralization: So far we have not 
described how the actual sound output is generated from the simulation algorithm described in the pre­vious 
section, i.e. the auralization process (we refer the reader to a more detailed overview such as [14] 
for an introduction). As men­tioned above, the simulation is performed asynchronously to the ren­dering 
and auralization, so we have a dedicated rendering thread and one or more simulation threads. During 
the simulation, we do not store the actual frusta, but test each frustum on whether the listener s position 
is contained in it. If so, we store the sound information such as source, delay and power for all bands 
in a temporary buffer. The rendering thread reloads this buffer at regular intervals and computes the 
contribution of each source as an impulse response function (IRF) for each band and channel. Conceptually, 
each contributing frustum represents a virtual source located at the apex of the frusta such as in image 
source methods. Note that this approach can therefore up­date the sound more often even if the simulation 
itself is only updated infrequently, which reduces the impact of listener movement.  Furthermore, to 
incorporate frequency dependent effects, each source s sound signal is decomposed into 10 frequency bands 
at 20, 40, 80, 160, 320, 640, 1280, 2560, 5120, 10240 and 20480 Hz and processed for two channels. For 
each channel the band-passed signal is convolved with the impulse response for that band and the chan­nel. 
The convolved signals are then added up and played at the corre­sponding channel. We also have provision 
for binaural hearing and we use Head Related Transfer Functions (HRTFs) from a public-domain HRTF database 
[1]. The sound pipeline is set up using the FMOD Ex sound API. We currently perform all convolutions 
in software in the rendering thread, but it would be possible to do this in dedicated sound hardware 
using DSPs as well. Implementation details: Our ray packet tracing implementation utilizes current CPUs 
SIMD instructions that allow small-scale vector operations on 4 operands in parallel. In the context 
of packet tracing, this allows us to perform intersections of multiple rays against a node of the hierarchy 
or against a geometric primitive in parallel. In our case this is especially ef.cient for all intersection 
tests involving the corner rays as we use exactly four rays to represent a frustum. Therefore most operations 
involving the frustum are implemented in that manner. The frustum-box culling test used during hierarchy 
traversal is also implemented very ef.ciently using SIMD instructions [35]. Finally, since all the frusta 
can be traced in parallel, performing the simulation using multiple threads on a multi-core processor 
is rather simple and can be easily scaled to multi-processor machines. 5 RESULTS We now present results 
of using frustum tracing in our system on sev­eral scenes. All benchmarks were run on an Intel Core 2 
Duo system at 3.0 GHz with a total of 4 cores. Our sound simulation runs asyn­chronously to the rendering 
thread and can be executed in parallel on the other three threads to exploit parallelism. As future CPUs 
will of­fer more cores, the performance of our sound propagation algorithm can therefore improve accordingly. 
Results are shown both for using just one thread and using all three threads. We tested our system on 
several different environments and con­ditions (see Fig. 6). Our main performance is summarized in table 
1 and shows that we can handle all of the benchmark models at in­teractive rates on our test system. 
The theater model is an architec­tural scene that is very open and therefore would be very challenging 
for beam tracing approaches. Even with 7 number of re.ections per frustum, we can perform our simulation 
in less than one second with dynamic geometric primitives and sound sources. The Quake model was chosen 
as a typical example of a game-like environment and fea­tures densely-occluded portions as well as open 
parts. Some dynamic geometric objects and moving sound sources are also included in our benchmark. We 
also tested a more complex, static scene with 190K triangles with just one moving sound source. The results 
in table 1 show that even though performance as mea­sured by frusta per second decreases with increasing 
number of prim­itives, the decrease is still sub-linear. This is due to the logarithmic scaling of ray 
packet tracing methods. We recompute the BVH when­ever the geometric objects in the scene move. Even 
though the time complexity of updating a BVH is linear in the number of primitives, the total time needed 
for updating a BVH is still negligible compared to the simulation time, as shown in table 2. Moreover, 
the BVH update can easily be parallelized using multiple threads between the simula­tion runs. A key 
measure in our algorithm is the number of sample rays that are used per frustum. It can have a signi.cant 
impact on the perfor­mance. Figure 7 shows the overall simulation performance as well as the total number 
of frusta used in our benchmark models when chang­ing the sampling rate. The graph shows that the scaling 
is logarith­mic, which is due to the ray-independent frustum traversal as well as Table 2. Construction 
and maintenance cost: Our results show that for all the models maintaining or updating the BVH hierarchy 
adds a negligible cost to the overall simulation. Note that construction only needs to be performed once 
and then the hierarchy is maintained through updates.  Samples per frustum Fig. 7. Sampling rates: The 
graphs show the impact of increasing the sam­pling rate per frustum on both the simulation times as well 
as number of frusta generated (all simulations are performed for 7 re.ections.) In addition to the benchmark 
scenes used in Table 1, the Boxes scene is a simple environment of two boxes connected by a small opening. 
Due to our frustum traversal al­gorithm, ef.cient triangle intersection and secondary frustum construction, 
in­creasing the sampling rate only causes logarithmic growth in the simulation time and number of frusta 
generated. This suggests that changing the frusta sampling rate can be an ef.cient method to control 
the accuracy of our simu­lation. our merging algorithm for constructing secondary frusta. This scaling 
makes the sampling rate a good parameter for trading off quality and runtime performance, depending on 
the requirements on the simula­tion. 6 ANALYSIS AND COMPARISON We now analyze the performance of our 
algorithm and discuss some of its limitations. As discussed in section 3 our approach introduces errors 
due to discrete clipping as compared to beam tracing. We have found that the artifacts created through 
aliasing are usually hardly no­ticeable except in contrived situations, and they are far less obtrusive 
than temporal aliasing that arises in ray tracing algorithms based on stochastic approaches. Note that 
the sample location in the sub-frusta does not need to be the center, so the aliasing due to sub-sampling 
could be ameliorated by stochastic sampling of the locations, e.g. by jittering. However, this may introduce 
temporal aliasing in animated scenes as stochastic sampling may change simulation results notice­ably 
over time. It is possible that Quasi-Monte Carlo sampling could eliminate these problems. Another source 
of potential errors stems from the construction of secondary frusta: since the re.ected or transmitted 
frustum is con­structed from the corner rays of the sub-frustum, the base surface of the new frustum 
can signi.cantly exceed the area of the primitive if the incoming frustum comes from a grazing angle 
and the sample rays hits close to he boundary of the object. Another limitation of the frustum-based 
approach are that we as­sume surfaces are locally .at, and our algorithm may not be able to  Fig. 6. 
Benchmark scenarios: We achieve interactive sound propagation performance on several benchmark models 
ranging from 9k to 235k triangles while simulating up to 7 re.ections. From left to right: Theater (9k), 
Quake (12k), Cathedral (196k). Cathedral 196344 D D -5 60k 1607 ms 550 ms 37k Table 1. Results: This 
table highlights the performance of our system on different benchmarks. The "D" indicates that listener, 
source or the scene objects are dynamic. Note that the frustum tracing performance does scale logarithmically 
with scene complexity and linearly with the number of threads. Please see the video for demonstration 
of the benchmark scenes. Fig. 8. Impulse Response (IR) vs Sampling Resolution: The above picture shows 
IRs generated from our frustum-tracing approach for a simple scene of two connected boxes (top) and the 
Theater scene (bottom), with re.ection order = 4 and varying frustum sampling resolution {4 ×4, 8 × 8, 
16 ×16, 32 ×32}. Notice that the sampling resolution of 4x4 misses some contributions compared to higher 
ones, but captures most of the detail correctly. As the sampling resolution increases, the accuracy of 
our method approaches that of the beam tracing method. These results indicate that the accuracy of our 
method for 4 ×4 or 8 ×8 sampling resolution can be close to that of beam tracing.  handle non-planar 
geometry correctly. This is common to most vol­umetric approaches, but we can still approximate the re.ections 
by increasing the number of sample rays and using the planar approxi­mation de.ned by the local surface 
normal. Our implementation is also currently limited to point sound sources. However, we can po­tentially 
simulate area and volumetric sources if the source can be ap­proximated by planar surfaces. The lack 
of non-specular re.ections is another limitation of our approach. For example, it could be hard to create 
a frusta for diffuse re.ection from a surface based on a scatter­ing coef.cient without signi.cantly 
affecting the performance of our algorithm. We also studied the behavior of our algorithm for different 
sampling rates. As Fig. 7 shows, the simulation time increases sub-linearly to the sampling rate due 
to our optimized intersection and sample com­bination algorithm. Fig 8 compares the resulting impulse 
response functions on two different models for varying sampling rates, which is signi.cant since it is 
obvious that as the sampling rate goes to in.n­ity our algorithm essentially becomes beam tracing. 
As the results show, even for low sampling resolutions, the response converges very quickly, which suggests 
that we can achieve almost the same qual­ity with very low sampling rates. Of course, our approach is 
still a geometric algorithm and like all others its accuracy for high-quality simulation is may therefore 
be limited compared to full numerical simulation[47]. Note that our frustum tracing technique is could 
be seen to be re­lated to adaptive super-sampling techniques in computer graphics such as [48, 17, 20]. 
However, recent work in interactive ray tracing (for vi­sual rendering) has shown, that adaptive sampling 
 despite its natural advantages does not perform near as fast as simpler approaches that are based on 
ray packets and frustum techniques. While high uniform sampling, as used in our algorithm, may seem uneconomical 
at .rst, our clipping algorithm reduces the actual work and simplicity makes this approach map much better 
to current hardware. Combining sam­ples only after the sampling has been performed reduces the detail 
of the uniform sampling to the same that adaptive sampling would gener­ate, but does not add any overhead 
to the traversal process. Similarly, there were parallel approaches in other areas such as radio [34] 
and sound propagation [39, 10] using adaptive beam methods, but for the same reasons they do not perform 
nearly as well and are limited in the scale and generality of scenes they can handle. 7 FUTURE WORK AND 
CONCLUSIONS There is a rich history on the synergies between the research direc­tions in sound and light, 
and we apply the lessons from one wave phe­nomenon to the other. Our goal was to utilize the recent developments 
in interactive ray tracing for sound propagation. As a result, we have presented an interactive frustum 
tracing algorithm, which combines the speed ef.ciencies of ray tracing with many of the accuracy bene.ts 
of volumetric representation. All the other bene.ts of ray packet trac­ing, including SIMD optimizations, 
multi-threaded implementations and handling dynamic scenes are directly applicable to sound render­ing. 
As a result we are able to render sound in complex and dynamic scenes at interactive rates. We hope that 
this will be a step towards in­cluding physical sound propagation into interactive applications such 
as games and virtual environments with dynamic environments. For future work we would be interested in 
further exploring the sampling issues in our discrete clipping algorithm to minimize the er­ror. A promising 
direction may be to investigate adaptive subdivision to adjust sampling rates to local geometric complexity. 
We are also interested in adding diffraction into the simulation, which has been shown to add important 
contributions to the realism. Finally, we would like to apply our algorithm to more complex scenarios 
and integrate them into interactive applications such as games. ACKNOWLEDGMENTS We would like to thank 
Paul Calamia for his feedback and Charles Ehrlich for the Candlestick theater model. This work was supported 
in part by ARO Contracts DAAD19-02-1-0390 and W911NF-04-1­0088, NSF awards 0400134, 0429583 and 0404088, 
DARPA/RDE-COM Contract N61339-04-C-0043, Disruptive Technology Of.ce and Intel. REFERENCES [1] V. Algazi, 
R. Duda, and D. Thompson. The CIPIC HRTF Database. In IEEE ASSP Workshop on Applications of Signal Processing 
to Audio and Acoustics, 2001. [2] F. Antonacci, M. Foco, A. Sarti, and S. Tubaro. Real time modeling 
of acoustic propagation in complex environments. In Proc. of 7th Interna­tional Conference on Digital 
Audio Effects, 2004. [3] M. Bertram, E. Deines, J. Mohring, J. Jegorovs, and H. Hagen. Phonon tracing 
for auralization and visualization of sound. In Proceedings of IEEE Visualization 2005, pages 151 158, 
2005. [4] J. Borish. Extension of the image model to arbitrary polyhedra. Journal of the Acoustical Society 
of America, 75(6):1827 1836, 1984. [5] C. Brebbia, editor. Computational Acoustics and its Environmental 
Ap­plications. Transactions of the Wessex Institute, 1995. [6] L. Carpenter. The a-buffer, an antialiased 
hidden surface method. In SIGGRAPH 84: Proceedings of the 11th annual conference on Com­puter graphics 
and interactive techniques, pages 103 108, New York, NY, USA, 1984. ACM Press. [7] P. R. Cook. Real Sound 
Synthesis for Interactive Applications. A. K. Peters, 2002. [8] B.-I. Dalenbäck, P. Svensson, and M. 
Kleiner. Room acoustic prediction and auralization based on an extended image source model. The Journal 
of the Acoustical Society of America, 92(4):2346, 1992. [9] E. Deines, M. Bertram, J. Mohring, J. Jegorovs, 
F. Michel, H. Hagen, and G. Nielson. Comparative visualization for wave-based and geometric acoustics. 
IEEE Transactions on Visualization and Computer Graphics, 12(5), 2006. [10] I. A. Drumm. The Development 
and Application of an Adaptive Beam Tracing Algorithm to Predict the Acoustics of Auditoria. PhD thesis, 
1997. [11] A. Farina. Ramsete -a new pyramid tracer for medium and large scale acoustic problems. In 
Proceedings of EURO-NOISE, 1995. [12] T. Funkhouser, I. Carlbom, G. Elko, G. Pingali, M. Sondhi, and 
J. West. A beam tracing approach to acoustic modeling for interactive virtual en­vironments. In Proc. 
of ACM SIGGRAPH, pages 21 32, 1998. [13] T. Funkhouser, N. Tsingos, I. Carlbom, G. Elko, M. Sondhi, J. 
West, G. Pingali, P. Min, and A. Ngan. A beam tracing method for interactive architectural acoustics. 
Journal of the Acoustical Society of America, 115(2):739 756, February 2004.  [14] T. Funkhouser, N. 
Tsingos, and J.-M. Jot. Survey of methods for model­ing sound propagation in interactive virtual environment 
systems. Pres­ence and Teleoperation, 2003. [15] T. A. Funkhouser, P. Min, and I. Carlbom. Real-time 
acoustic modeling for distributed virtual environments. In Proc. of ACM SIGGRAPH, pages 365 374, 1999. 
[16] M. A. Garcia-Ruiz and J. R. Gutierrez-Pulido. An overview of auditory display to assist comprehension 
of molecular information. Interact. Com­put., 18(4):853 868, 2006. [17] J. Genetti and D. Gordon. Ray 
tracing with adaptive supersampling in object space. In Graphics Interface 93, pages 70 77, 1993. [18] 
P. S. Heckbert and P. Hanrahan. Beam tracing polygonal objects. In Proc. of ACM SIGGRAPH, pages 119 127, 
1984. [19] D. L. James, J. Barbic, and D. K. Pai. Precomputed acoustic transfer: output-sensitive, accurate 
sound generation for geometrically complex vi­bration sources. In Proc. of ACM SIGGRAPH, pages 987 995, 
2006. [20] D. G. Jon Genetti and G. Williams. Adaptive supersampling in object space using pyramidal 
rays. Computer Graphics Forum, 17(1):29 54, 1998. [21] C. Joslin and N. Magnetat-Thalmann. Signi.cant 
facet retrieval for real­time 3d sound rendering. In Proceedings of the ACM VRST, 2003. [22] B. Kapralos, 
M. Jenkin, and E. Milios. Acoustic modeling utilizing an acoustic version of phonon mapping. In Proc. 
of IEEE Workshop on HAVE, 2004. [23] A. Krokstad, S. Strom, and S. Sorsdal. Calculating the acoustical 
room response by the use of a ray tracing technique. Journal of Sound and Vibration, 8(1):118 125, July 
1968. [24] K. Kunz and R. Luebbers. The Finite Difference Time Domain for Elec­tromagnetics. CRC Press, 
1993. [25] K. H. Kuttruff. Auralization of impulse responses modeled on the basis of ray-tracing results. 
Journal of Audio Engineering Society, 41(11):876  880, November 1993. [26] C. Lauterbach, S.-E. Yoon, 
D. Tuft, and D. Manocha. RT-DEFORM: In­teractive Ray Tracing of Dynamic Scenes using BVHs. IEEE Symposium 
on Interactive Ray Tracing, 2006. [27] H. Lehnert. Systematic errors of the ray-tracing algorithm. J. 
Applied Acoustics, 38(2-4):207 221, 1993. [28] R. B. Loftin. Multisensory perception: Beyond the visual 
in visualization. Computing in Science and Engineering, 05(4):56 58, 2003. [29] T. Lokki, L. Savioja, 
R. Vaananen, J. Huopaniemi, and T. Takala. Creat­ing interactive virtual auditory environments. IEEE 
Computer Graphics and Applications, 22(4):49 57, 2002. [30] M. Naef, O. Staadt, and M. Gross. Spatialized 
audio rendering for im­mersive virtual environments. In Proceedings of the ACM VRST, 2002. [31] K. V. 
Nesbitt. Modelling human perception to leverage the reuse of con­cepts across the multi-sensory design 
space. In APCCM 06: Proceed­ings of the 3rd Asia-Paci.c conference on Conceptual modelling, pages 65 
74, Darlinghurst, Australia, Australia, 2006. Australian Computer So­ciety, Inc. [32] J. F. O Brien, 
P. R. Cook, and G. Essl. Synthesizing sounds from physi­cally based motion. In Proc. of ACM SIGGRAPH, 
pages 529 536, 2001. [33] T. Otsuru, Y. Uchinoura, R. Tomiku, N. Okamoto, and Y. Takahashi. Ba­sic concept, 
accuracy and application of large-scale .nite element sound .eld analysis of rooms. In Proc. ICA 2004 
(Kyoto), pages I 479 I 482, April 2004. [34] A. Rajkumar, B. F. Naylor, F. Feisullin, and L. Rogers. 
Predicting rf coverage in large environments using ray-beam tracing and partitioning tree represented 
geometry. Wirel. Netw., 2(2):143 154, 1996. [35] A. Reshetov, A. Soupikov, and J. Hurley. Multi-level 
ray tracing algo­rithm. ACM Trans. Graph., 24(3):1176 1185, 2005. [36] L. Savioja. Modeling Techniques 
for Virtual Acoustics. PhD thesis, Helsinki University of Technology, 1999. [37] K. Shoemake. Pluecker 
coordinate tutorial. Ray Tracing News, 11(1), 1998. [38] S. Smith. Auditory representation of scienti.c 
data. In Focus on Scienti.c Visualization, pages 337 346, London, UK, 1993. Springer-Verlag. [39] U. 
Stephenson. Quantized pyramidal beam tracing -a new algorithm for room acoustics and noise immission 
prognosis. Acustica -Acta Acustica, 82(3):517 525, 1996. [40] H. Suzuki and A. S. Mohan. Frustum ray 
tracing technique for high spa­tial resolution channel characteristic map. In Radio and Wireless Confer­ence 
(RAWCON) 98, pages 253 256. IEEE Press, 1998. [41] R. Tomiku, T. Otsuru, Y. Takahashi, and D. Azuma. 
A computational investigation on measurements in reverberation rooms by .nite element sound .eld analysis. 
In Proc. ICA 2004 (Kyoto), pages II 941 II 942, April 2004. [42] N. Tsingos, E. Gallo, and G. Drettakis. 
Perceptual audio rendering of complex virtual environments. ACM Trans. Graph., 23(3):249 258, 2004. [43] 
K. van den Doel, D. Knott, and D. K. Pai. Interactive simulation of com­plex audio-visual scenes. Presence: 
Teleoperators and Virtual Environ­ments, 13(1):99 111, 2004. [44] I. Wald, C. Benthin, M. Wagner, and 
P. Slusallek. Interactive rendering with coherent ray tracing. In A. Chalmers and T.-M. Rhyne, editors, 
Com­puter Graphics Forum (Proceedings of EUROGRAPHICS 2001), vol­ume 20, pages 153 164. Blackwell Publishers, 
Oxford, 2001. [45] I. Wald, S. Boulos, and P. Shirley. Ray Tracing Deformable Scenes using Dynamic Bounding 
Volume Hierarchies. ACM Transactions on Graphics, 2006. [46] M. Wand and W. Straßer. Multi-resolution 
sound rendering. In SPBG 04 Symposium on Point -Based Graphics 2004, pages 3 11, 2004. [47] L. M. Wang, 
J. Rathsam, and S. R. Ryherd. Interactions of model detail level and scattering coef.cients in room acoustic 
computer simulation. In International Symposium on Room Acoustics: Design and Science, 2004. [48] T. 
Whitted. An improved illumination model for shaded display. Com­mun. ACM, 23(6):343 349, 1980.  .2009 
IEEE. Reprinted, with permission, from IEEE TRANSACTION ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 
14, NO. 6, NOVEMBER/DECEMBER 2008 AD-Frustum: Adaptive Frustum Tracing for Interactive Sound Propagation 
 Anish Chandak, Christian Lauterbach, Micah Taylor, Zhimin Ren, and Dinesh Manocha, Member, IEEE Abstract 
We present an interactive algorithm to compute sound propagation paths for transmission, specular re.ection 
and edge diffraction in complex scenes. Our formulation uses an adaptive frustum representation that 
is automatically sub-divided to accurately compute intersections with the scene primitives. We describe 
a simple and fast algorithm to approximate the visible surface for each frustum and generate new frusta 
based on specular re.ection and edge diffraction. Our approach is applicable to all triangulated models 
and we demonstrate its performance on architectural and outdoor models with tens or hundreds of thousands 
of triangles and moving objects. In practice, our algorithm can perform geometric sound propagation in 
complex scenes at 4-20 frames per second on a multi-core PC. Index Terms Sound propagation, interactive 
system, auralization 1 INTRODUCTION Sound simulation and spatialized audio rendering can signi.cantly 
en­hance the realism and sense of immersion in interactive virtual en­vironments. They are useful for 
computer-aided acoustic modeling, multi-sensory visualization, and training systems. Spatial sound can 
be used for development of auditory displays or provide auditory cues for evaluating complex datasets 
[22, 33]. A key component of 3D audio rendering is interactive sound propa­gation that simulates sound 
waves as they re.ect or diffract off objects in the virtual environment. One of the main challenges in 
sound ren­dering is handling complex datasets at interactive rates. Current visual rendering systems 
can render datasets composed of millions of primi­tives at real-time rates (i.e. 10-30 fps) on commodity 
desktop systems. On the other hand, interactive sound rendering algorithms are only limited to scenes 
with a few thousand triangles. Therefore, most in­teractive applications typically use precomputed, static 
sound effects based on .xed models of propagation. In this paper, we address the problem of interactive 
sound propa­gation in complex datasets from point sources. The exact solution to modeling propagation 
is based on solving the Helmholtz-Kirchhoff in­tegration equation. The numerical methods to solve this 
equation tend to be compute and storage intensive. As a result, fast algorithms for complex scenes mostly 
use geometric methods that propagate sound based on rectilinear propagation of waves and can accurately 
model transmission, early re.ection and edge diffraction [11]. The most ac­curate geometric approaches 
are based on exact beam or pyramid trac­ing, which keep track of the exact shape of the volumetric waves 
as they propagate through the scene. In practice, these approaches are mainly limited to static scenes 
and may not be able to handle complex scenes with curved surfaces at interactive rates. On the other 
hand, approximate geometric methods based on path-tracing or ray-frustum tracing can handle complex, 
dynamic environments, but may need a very large number of samples to overcome aliasing errors. Main results: 
We present a novel volumetric tracing approach that can generate propagation paths for early specular 
re.ections and edge diffraction by adapting to the scene primitives. Our approach is Anish Chandak, 
E-mail: achandak@cs.unc.edu.  Christian Lauterbach, E-mail: cl@cs.unc.edu.  Micah Taylor, E-mail: taylormt@cs.unc.edu. 
 Zhimin Ren, E-mail: zren@cs.unc.edu.  Dinesh Manocha, E-mail: dm@cs.unc.edu.  Project Webpage: http://gamma.cs.unc.edu/SOUND 
 Manuscript received 31 March 2008; accepted 1 August 2008; posted online 19 October 2008; mailed on 
13 October 2008. For information on obtaining reprints of this article, please send e-mailto:tvcg@computer.org. 
 general and can handle all triangulated models with moving objects. The underlying formulation uses 
a simple adaptive representation that augments a 4-sided frustum [19] with a quadtree and adaptively 
gener­ates sub-frusta. We exploit the representation to perform fast intersec­tion and visibility computations 
with scene primitives. As compared to prior adaptive algorithms for sound propagation, our approach pro­vides 
an automatic balance between accuracy and interactivity to gen­erate plausible sound rendering in complex 
scenes. Some novel as­pects of our work include: 1. AD-Frustum: We present a simple representation to 
adaptively generate 4-sided frusta to accurately compute propagation paths. Each sub-frustum represents 
a volume corresponding to a bundle of rays. We use ray-coherence techniques to accelerate intersection 
computa­tions with the corner rays. The algorithm uses an area subdivision method to compute an approximation 
of the visible surface for each frustum. 2. Edge-diffraction: We present an ef.cient algorithm to per­form 
edge diffraction on AD-Frustum based on the Uniform Theory of Diffraction [15, 37]. These include ef.cient 
techniques to compute the shape of diffraction frustum and the actual contribution that the diffraction 
makes at the listener. 3. Handling complex scenes: We use bounding volume hierarchies (BVHs) to accelerate 
the intersection computations with AD-Frusta in complex, dynamic scenes. We present techniques to bound 
the maxi­mum subdivision within each AD-Frustum based on scene complexity and thereby control the overall 
accuracy of propagation by computing all the important contributions.  We have applied our algorithm 
for interactive sound propagation in complex and dynamic scenes corresponding to architectural models, 
outdoor scenes, and game environments. In practice, our algorithm can accurately compute early sound 
propagation paths with up to 4-5 re.ections at 4-20 frames per second on scenes with hundreds of thou­sands 
of polygons on a multi-core PC. Our preliminary comparisons indicate that propagation based on AD-Frusta 
can offer considerable speedups over prior geometric propagation algorithms. We also evalu­ate the accuracy 
of our algorithm by comparing the impulse responses with an industrial strength implementation of an 
image source method. Organization: The rest of the paper is organized in the following manner. We brie.y 
survey related work on geometric sound prop­agation and interactive sound rendering in Section 2. Section 
3 de­scribes the AD-Frustum representation and intersection computations. We present algorithms to enumerate 
propagation paths in Section 4 and highlight techniques to handle complex environments in Section 5. 
We describe the overall performance and accuracy in Section 6.  Query direct contribution AD-Frustum 
(quadtree updated) Fig. 1. Overview of our algorithm: AD-Frusta are generated from sound sources (primary 
frusta) and by re.ection and diffraction (secondary frusta) from the scene primitives. Approximate visible 
surfaces are then computed for each frustum (quadtree update). Next, each updated frustum checks if the 
listener lies inside it and is visible. If visible, its contributions are registered with the audio rendering 
system. The audio rendering system queries the direct contribution of a sound source every time it renders 
its audio block. 2 PREVIOUS WORK The two main approaches to sound propagation are numerical meth­ods 
and geometric approaches [11, 32]. In practice, the numerical solutions are too slow for interactive 
applications or dynamic scenes. In this section, we focus on geometric propagation techniques, which 
are primarily used to model early specular re.ection and diffraction paths. Recently, techniques based 
on acoustic radiosity have also been developed to handle diffuse re.ections for simple indoor scenes. 
At a broad level, the geometric propagation methods can be classi.ed into ray-based or particle-based 
tracing, image source methods, and volu­metric tracing. Ray-based or Particle-based Techniques: Some 
of the earli­est methods for geometric sound propagation are based on tracing sampled-rays [16] or sound-particles 
(phonons) [14, 2, 6, 23] from a source to the listener. Recent improvements, including optimized hi­erarchies 
and exploiting ray-coherence, make it possible for ray-based and particle-based methods to handle complex, 
dynamic scenes on commodity hardware [39] or handle massive models [20]. However, due to discrete sampling 
of the space, these methods have to trace large number of paths or particles to avoid aliasing artifacts. 
Image Source Methods: These methods are the easiest and most popular for computing specular re.ections 
[1, 5]. They compute vir­tual sources from a sound source recursively for every re.ection and can have 
exponential complexity in the number of re.ections. They can guarantee all specular paths up to a given 
order. However, they can only handle simple static scenes or very low order of re.ections at interactive 
rates [17]. Many hybrid combinations [4] of ray-based and image source methods have also been proposed 
and used in commer­cial room acoustics prediction softwares (e.g. ODEON). Volumetric Methods: The volumetric 
methods trace pyramidal or volumetric beams to compute an accurate geometric solution. These include 
beam tracing that has been used for specular re.ection and edge diffraction for interactive sound propagation 
[10, 17, 37, 9]. The results of beam tracing can be used to guide sampling for path tracing [10]. However, 
the underlying complexity of beam tracing makes it hard to handle complex models. Other volumetric tracing 
methods are based on triangular pyramids [8], ray-beams [25], ray-frusta [19], as well as methods developed 
for visual rendering [12]. Interactive Sound Propagation: Various approaches have been proposed to improve 
the performance of acoustic simulation or han­dle complex scenarios. These include model simpli.cation 
algorithms [13], use of scattering .lters [36], and reducing the number of active sound sources by perceptual 
culling or sampling [38, 40]. These tech­niques are complementary to our approach and can be combined 
to handle scenarios with multiple sound sources or highly tessellated ob­jects in the scene. 3 ADAPTIVE 
VOLUME TRACING Our goal is to perform interactive geometric sound propagation in complex and dynamic 
scenes. We mainly focus on computing paths that combine transmission, specular re.ection, and edge diffraction 
up to a user-speci.ed criterion from each source to the receiver. Given the underlying complexity of 
exact approaches, we present an approx­imate volumetric tracing algorithm. Fig. 1 gives a top level view 
of our algorithm. We start by shooting frusta from the sound sources. A frustum traverses the scene hierarchy 
and .nds a list of potentially intersecting triangles. These triangles are intersected with the frus­tum 
and the frustum is adaptively sub-divided into sub-frusta. The sub-frusta approximate which triangles 
are visible to the frustum. The sub-frusta are subsequently re.ected and diffracted to generate more 
frusta, which in turn are traversed. Also, if the listener is inside a frustum and visible, the contribution 
is registered for use during audio rendering stage. Our approach builds on using a ray-frustum for volumetric 
tracing [19]. We trace the paths from the source to the receivers by tracing a sequence of ray-frusta. 
Each ray-frustum is a simple 4-sided frus­tum, represented as a convex combination of four corner rays. 
In­stead of computing an exact intersection of the frusta with a primitive, ray-frustum tracing performs 
discrete clipping by intersecting a .xed number of rays for each frustum. This approach can handle complex, 
dynamic scenes, but has the following limitations: The formulation uses uniformly spaced samples inside 
each frus­tum for fast intersection computations. Using a high sampling resolution can signi.cantly increase 
the number of traced frusta.  The approach cannot adapt to the scene complexity ef.ciently.  In order 
to overcome these limitations, we propose an adaptive frustum representation, AD-Frustum. Our goal is 
to retain the performance bene.ts of the original frustum formulation, but increase the accuracy of the 
simulation by adaptively varying the resolution. Various com­ponents of our algorithm are shown in Fig. 
1. 3.1 AD-Frustum AD-Frustum is a hierarchical representation of the subdivision of a frustum. We augment 
a 4-sided frustum with a quadtree structure to keep track of its subdivision and maintain the correct 
depth informa­tion, as shown in Fig. 2. Each leaf node of the quadtree represents the .nest level sub-frustum 
that is used for volumetric tracing. An inter­mediate node in the quadtree corresponds to the parent 
frustum or an internal frustum. We adaptively re.ne the quadtree in order to perform accurate intersection 
computations with the primitives in the scene and generate new frusta based on re.ections and diffraction. 
Representation: Each AD-Frustum is represented using an apex and a quadtree. Each 2D node of the quadtree 
includes: (a) corner rays of the sub-frustum that de.ne the extent of each sub-frustum; (b) intersection 
status corresponding to completely-inside, partially­intersecting with some primitive, or completely-outside 
of all scene primitives; (c) intersection depth, and primitive id to track the depth value of the closest 
primitive; (d) list of diffracting edges to support diffraction calculations. We also associate a maximum-subdivision 
depth parameter with each AD-Frustum that corresponds to the max­imum depth of the quadtree. 3.2 Intersection 
Tests The AD-Frusta are used for volumetric tracing and enumerating the propagation paths. The main operation 
is computing their intersec­tion with the scene primitives and computing re.ection and diffraction  
.2009 IEEE. Reprinted, with permission, from IEEE TRANSACTION ON VISUALIZATION AND COMPUTER GRAPHICS, 
VOL. 14, NO. 6, NOVEMBER/DECEMBER 2008 (a) (b) Fig. 2. AD-Frustum Representation: (a) A frustum, represented 
by con­vex combination of four corner rays and apex A. (b) A hierarchically divided adaptive frustum. 
It is augmented with a quadtree structure, where P is the 2D plane of quadtree. We use two sets of colors 
to show different nodes. Each node stores auxiliary information about cor­responding sub-frusta. frusta. 
The scene is represented using a bounding volume hierarchy (BVH) of axis-aligned bounding boxes (AABBs). 
The leaf nodes of the BVH are triangle primitives and the intermediate nodes represent an AABB. For dynamic 
scenes, the BVH is updated at each frame [39, 21, 43]. Given an AD-Frustum, we traverse the BVH from 
the root node to perform these tests. Intersection with AABBs: The intersection of an AD-Frustum with 
an AABB is performed by intersecting the four corner rays of the root node of the quadtree with the AABB, 
as described in [26]. If an AABB partially or fully overlaps with the frustum, we apply the al­gorithm 
recursively to the children of the AABB. The quadtree is not modi.ed during this traversal. Intersection 
with primitives: As a frustum traverses the BVH, we compute intersections with each triangle corresponding 
to the leaf nodes of the BVH. We only perform these tests to classify the node as completely-inside, 
completely-outside or partially-intersecting. This is illustrated in Fig. 3, where we show the completely-outside 
re­gions in green and completely-inside regions in orange. This intersec­tion test can be performed ef.ciently 
by using the Pl¨ ucker coordinate representation [31] for the triangle edges and four corner rays. The 
Pl¨ ucker coordinate representation ef.ciently determines, based on an assumed orientation of edges, 
whether the sub-frustum is completely inside, outside or partially intersecting the primitive. This test 
is con­servative and may conclude that a node is partially intersecting, even if it is completely-outside. 
If the frustum is classi.ed as partially­intersecting with the primitives, we sub-divide that quadtree 
node, generate four new sub-frusta, and perform the intersection test recur­sively. The maximum-subdivision 
depth parameter imposes a bound on the depth of the quadtree. Each leaf node of the quadtree is classi­.ed 
as completely-outside or completely-inside. 3.3 Visible Surface Approximation A key component of the 
traversal algorithm is the computation of the visible primitives associated with each leaf node sub-frustum. 
We use an area-subdivision technique, similar to classic Warnock s algorithm [41], to compute the visible 
primitives. Our algorithm associates in­tersection depth values of four corner rays with each leaf node 
of the quadtree as well as the id of the corresponding triangle. Moreover, we compute the minimum and 
maximum depth for each intermediate node of the triangle, that represents the extent of triangles that 
par­tially overlap with its frustum. The depth information of all the nodes is updated whenever we perform 
intersection computations with a new triangle primitive. In order to highlight the basic subdivision 
algorithm, we consider the case of two triangles in the scene shown as red and blue in Fig. 3. In this 
example, the projections of the triangles on the quadtree plane overlap. We illustrate different cases 
that can arise based on the rel­ative ordering and orientation of two triangles. The basic operation 
compares the depths of corner rays associated with the frustum and updates the node with the depth of 
the closer ray (as shown in Fig. 3(a)). If we cannot resolve the closest depth (see Fig. 3(d)), we ap­ply 
the algorithm recursively to its children (shown in orange). Fig. 3(b) shows the comparison between a 
partially-intersecting node with a completely-inside node. If the completely-inside node is closer, i.e., 
all the corner rays of the completely-inside node are closer than the minimum depth of the corner rays 
of the partially-intersecting node, then the quadtree is updated with the completely-inside node. Other­wise, 
we apply the algorithm recursively to their children as in Fig. 3(e). Lastly, in Fig. 3(c), both the 
nodes are partially-intersecting and we apply the algorithm recursively on their children. This approach 
can be easily generalized to handle all the triangles that overlap with an AD-Frustum. At any stage, 
the algorithm main­tains the depth values based on intersection with all the primitives tra­versed so 
far. As we perform intersection computations with a new primitive, we update the intersection depth values 
by comparing the previous values stored in the quadtree. The accuracy of our algorithm is governed by 
the resolution of the leaf nodes of the quadtree, which is based on the maximum-subdivision depth parameter 
associated with each AD-Frustum. 3.4 Nodes Reduction We perform an optimization step to reduce the number 
of frusta. This step is performed in a bottom up manner, after AD-Frustum .nishes the scene traversal. 
We look at the children of a node. Since, each child shares atleast one corner-ray with its siblings, 
we compare the depths of these corner-rays. Based on the difference in depth values, the normals of the 
surfaces the sub-frustum intersects, and the acoustic properties of those surfaces we collapse the children 
nodes into the parent node. Thus, we can easily combine the children nodes in the quadtree that hit the 
same plane with similar acoustic properties. Such an approach can also be used to combine children nodes 
that intersect slightly different surfaces. 4 ENUMERATING PROPAGATION PATHS In the previous section, 
we described the AD-Frustum representation and presented an ef.cient algorithm to compute its intersection 
and visibility with the scene primitives. In this section, we present algo­rithms to trace the frusta 
through the scene, and compute re.ection and diffraction frusta. We start the simulation by computing 
initial frusta around a sound source in quasi-uniform fashion [27] and per­form adaptive subdivision 
based on the intersection tests. Ultimately, the sub-frusta corresponding to the leaf nodes of the quadtree 
are used to compute re.ection and diffraction frusta. Specular Re.ections: Once a frustum has completely 
traced a scene, we consider all the leaf nodes within its quadtree and compute a re.ection frustum for 
all completely-intersecting leaf nodes. The corner rays of sub-frustum associated with the leaf nodes 
are re.ected (see Fig. 5) at the primitive hit by that sub-frustum. The convex com­bination of the re.ected 
corner rays creates the parent frustum for the re.ection AD-Frustum. 4.1 Edge Diffraction Diffraction 
happens when a sound wave hits an object whose size is the same order of magnitude as its wavelength, 
causing the wave to scatter. Our formulation of diffraction is based on the uniform theory of diffraction 
(UTD) [15], which predicts that a sound wave hitting an edge between the primitives is scattered in a 
cone. The half-angle of the cone is determined by the angle that the ray hits the edge. The basic ray-frustum 
tracing algorithm can compute edge diffraction based on UTD [34]. This involves identifying diffraction 
edges as part of a pre-process and computing diffraction paths using frusta tracing. In this section, 
we extend the approach described in [34] to handle AD-Frusta. Finding diffracting edges: In a given scene, 
only a subset of the edges are diffraction edges. For example, any planar rectangle is repre­sented by 
two triangles sharing an edge, but that edge cannot result in diffraction. Most edges in a model are 
shared by at least two triangles. As part of a pre-computation step, we represent the adjacency infor­mation 
using an edge-based data structure that associates all edges that can result in diffraction with its 
incident triangles. As part of the trac­ing algorithm, we compute all triangles that intersect a sub-frustum. 
Next, we check whether the sub-frustum intersects any of the edges of that triangle and based on that 
update the list of diffracting edges  (a) (b) (c) (d) (e) Fig. 3. Visibility computation based on area 
subdivision: We highlight different cases that arise as we resolve visibility between two triangle primitives 
(shown in red and blue) from the apex of the frustum. In cases (a)-(b) the quadtree is re.ned based on 
the intersection depth associated with the outermost frustum and compared with the triangles. However, 
in cases (c)-(e) the outermost frustum has to be sub-divided into sub-frusta and each of them is compared 
separately. Fig. 4. Generating a diffraction frustum from an edge. The result­ing wedge is shown as 
red and we show the generation of diffraction shadow frustum from the original sub-frustum. Note that 
the origin for the frustum is not just the edge, but the whole area of the sub-frustum that overlaps 
the edge. for that sub-frustum. We perform these tests ef.ciently using Pl¨ ucker coordinates. Computing 
Diffraction Frusta: Our adaptive diffraction approach is similar to the diffraction formulation described 
in [34, 37]. There are two main problems that arise in simulating edge diffraction using frusta: .rst, 
.nding the shape of the new frustum that corresponds to the real diffraction as predicted by the UTD; 
second, computing the actual contribution that the diffraction makes to the sound at the listener. Given 
a diffraction edge, we clip the sub-frustum against the edge to .nd the interval along the edge that 
it intersects. The diffraction cone originates from the respective part of the edge and its orientation 
and shape are predicted by the UTD based on the angle of the edge. The angle is computed from the triangles 
incident to the edge. Notice that we perform an approximation similar to [37] in that we ignore the non- 
shadow part of the diffraction. As part of frustum tracing, we ensure that the origin of the diffraction 
frustum is not limited to lie on the edge, but is chosen as the whole area of the frustum that overlaps 
the edge (see Fig. 4 for illustration). We compute this area by clipping the quadrilateral de.ned by 
the intersection of the frustum with the tri­angle s plane against the triangle edge. This computation 
extends the frustum shape beyond the actual diffraction .eld originating from the edge. This has the 
important practical advantage that it will include the direct contribution of the original frustum for 
parts of the frustum that do not overlap with the triangle. As a result, this formulation ac­tually adds 
back direct contributions that would have been lost due to the discrete nature of the frustum representation. 
One special case oc­curs if just one corner of the frustum is inside the triangle, since in that case 
the clipped frustum has .ve edges. We handle this case by using a conservative bounding frustum and performing 
an additional check for inclusion when we compute all the contributions to the lis­tener. Note that UTD 
is valid only for scenes with long edges, like the outdoor city scene or large walls and doors of architecture 
models. We therefore perform edge-diffraction on such models. 4.2 Contributions to the Listener For the 
specular re.ections the actual contribution of a frustum is de­termined by .nding a path inside all the 
frusta from the sound source to the listener. In case of diffraction, we have to ensure that the sub­frustum 
is actually in the real diffraction shadow part of the frustum. There are three distinct possibilities: 
.rst, the sub-frustum can be part of the direct contribution as described above. In that case, the contri­bution 
is simply computed like a direct one. The second case is that the sub-frustum is not a direct one, but 
also not inside the diffraction cone, i.e. inside the conservative bounding frustum. In that case, it 
can be safely ignored. Finally, if the listener s location is inside the diffraction frustum, we can 
compute the exact path due to all diffrac­tion events. The UTD formulation predicts the intensity of 
the sound .eld inside the diffraction cone, which depends on several variables, but most importantly 
the angle to the line of visibility (see [34, 37] for more details). For the frustum representation, 
the equation can either be evaluated while computing the exact path, or per sub-frustum, i.e. by discretizing 
the equation. In our formulation, we chose the latter such that the contribution can be evaluated very 
quickly per frustum, with a slight impact on the exact timing of the diffraction. 5 COMPLEX SCENES In 
this section, we give a brief overview of how to govern the ac­curacy of our algorithm for complex scenes. 
The complexity of the AD-Frustum tracing algorithm described in Sections 3 and 4 varies as a function 
of the maximum-subdivision depth parameter associated with each AD-Frustum. A higher value of this parameter 
results in a .ner subdivision of the quadtree and improves the accuracy of the in­tersection and volumetric 
tracing algorithms (see Fig. 5). At the same time, the number of leaf nodes can potentially increase 
as an exponen­tial function of this parameter and signi.cantly increase the number of traced frusta through 
the scene. Here, we present techniques for auto­matic computation of the depth parameter for each AD-Frustum. 
We consider two different scenarios: capturing the contributions from all important objects in the scene 
and constant frame-rate rendering. Our basic algorithm is applicable to all triangulated models. We do 
not make any assumptions about the connectivity of the triangles and can handle all polygonal soup datasets. 
Many times, a scene consists of objects that are represented using connected triangles. In the context 
of this paper, an object corresponds to a collection of triangles that are very close (or connected) 
to each other. 5.1 Maximum-subdivision depth computation One of the main challenges is to ensure that 
our algorithm doesn t miss the important contributions in terms of specular re.ections and diffraction. 
We take into account some of the characteristics of geo­metric sound propagation in characterizing the 
importance of different objects. For example, many experimental observations have suggested that low-resolution 
geometric models are more useful for specular re­.ections [13, 36]. Similarly, small objects in the scene 
only have signi.cant impact at high frequencies and can be excluded from the models in the presence of 
other signi.cant sources of re.ection and diffraction [10]. Based on these characterizations, we pre-compute 
ge­ometric simpli.cations of highly tessellated small objects and assign an importance function to each 
object based on its size and material properties. Given an object (O) with its importance function, our 
goal is to en­sure that the approximate frustum-intersection algorithm doesn t miss contributions from 
that object. Given a frustum with its apex (A) and  .2009 IEEE. Reprinted, with permission, from IEEE 
TRANSACTION ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 14, NO. 6, NOVEMBER/DECEMBER 2008 (a) (b) (c) 
(d) (e) (f) Fig. 5. Beam tracing vs. uniform frustum tracing vs. adaptive frustum tracing: We show the 
relative accuracy of three different algorithm in this simple 2D model with only .rst-order re.ections. 
(a) Re.ected beams generated using exact beam tracing. (b)-(c) Re.ected frusta generated with uniform 
frustum tracing for increased sampling: 22 samples per frustum and 24 samples per frustum. Uniform frustum 
tracing generates uniform number of samples in each frustum, independent of scene complexity. (d)-(e) 
Re.ected frusta generated using AD-Frustum. We highlight the accuracy of the algorithm by using the values 
of 2 and 4 for maximum-subdivision depth parameter. (f) An augmented binary tree for the 2D case (equivalent 
to a quadtree in 3D) showing the adaptive frusta. Note that the adaptive algorithm only generates more 
frusta in the regions where the input primitives have a high curvature and reduces the propagation error. 
the plane of its quadtree (P), we compute a perspective projection of 23 × 23 for uniform frustum tracing 
and maximum sub-division depth the bounding box of O on the P. Let s denote the projection on P as of 
3 for adaptive frustum tracing for the purposes of comparisons. o. Based on the size of o, we compute 
a bound on the size of leaf nodes of the quadtree such that the squares corresponding to the leaf nodes 
are inside o. If O has a high importance values attached to it, we ensure multiple leaf nodes of the 
quadtree are contained in o. We repeat this computation for all objects with high importance value in 
the scene. The maximum-subdivision depth parameter for that frus­tum is computed by considering the minimum 
size of the leaf nodes of the quadtree over all objects. Since we use a bounding box of O to compute 
the projection, our algorithm tends to be conservative. 5.2 Constant frame rate rendering In order to 
achieve target frame rate, we can bound the number of frusta that are traced through the scene. We .rst 
estimate the num­ ber of frustum that our algorithm can trace per second based on scene complexity, number 
of dynamic objects and sources. Given a bound on maximum number of frusta traced per second, we adjust 
the num­ber of primary frusta that are traced from each source. Moreover, we use a larger value of the 
maximum depth parameter for the .rst few re.ections and decrease this value for higher order re.ections. 
We compute the .rst few re.ections with higher accuracy by performing more adaptive subdivisions. We 
also perform adaptive subdivisions of a frustum based on its propagated distance. In this manner, our 
Model Complexity Results (7 threads) #.s diffraction #frusta time Theater 54 NO 56K 33.3 ms Factory 174 
NO 40K 27.3 ms Game 14K NO 206K 273.3 ms Sibenik 71K NO 198K 598.6 ms City 78K YES 80K 206.2 ms Soda 
Hall 1.5M YES 108K 373.3 ms Table 1. This table summarizes the performance of our system on six benchmarks. 
The complexity of a model is given by the number of trian­gles. We perform edge diffraction in two models 
and specular re.ection in all. We use a value of 3 for the maximum sub-division depth for these timings. 
The results are computed for up to 4 orders of re.ection. log(#Frustua) log(Time (msecs))  algorithm 
would compute more contributions from large objects in the scene, and tend to ignore contributions from 
relatively small ob­jects that are not close to the source or the receiver. We can further 16 Maximum 
Sub-division depth  improve the performance by using dynamic sorting and culling algo­ rithms along 
with perceptual metrics [38]. 6 IMPLEMENTATION AND RESULTS 14 12 10 In this section, we highlight the 
performance of our algorithm on dif­ferent benchmarks, describe our audio rendering pipeline, and analyze 
its accuracy. Our simulations were run on a 2.66 GHz Intel Core 2 Duo machine with 2GB of memory. 6.1 
Performance Results We perform geometric sound propagation using adaptive frustum trac­ing on many benchmarks. 
The complexity of our benchmarks ranges from a few hundred triangles to almost a million triangles. The 
perfor­mance results for adaptive frustum tracing and complexity of bench­marks are summarized in Table 
1. These results are generated for a maximum sub-division depth of 3. Further, the extent of dynamism 
in these benchmarks is shown in associated video. In Fig. 7, we show how the computation time for our 
approach and the number of frusta traced scale as we increase the maximum sub-division depth of AD-Frusta. 
Also, our algorithm scales well with the number of cores as shown in Fig. 8. The comparison of our approach 
with uniform frus­tum tracing [19] is given in Table 2. We chose a sampling resolution of 2345 Maximum 
Sub-division depth Fig. 7. This .gure shows the effect of increasing the maximum sub­division depth of 
adaptive frustum tracing on the overall computation time of the simulation and the number of total frusta 
traced. Different colors are used for different benchmarks. 6.2 Audio Rendering Audio rendering is the 
process of generating the .nal audio signal in an application. In our case, it corresponds to the convolution 
of the .lter generated by sound propagation simulation with the input audio to produce the .nal sound 
at the receiver. In this section, we outline an audio rendering pipeline that can be integrated with 
our geomet­ric propagation approach to perform interactive audio rendering for scenes with moving sources, 
moving listener, and dynamic objects. The two main methods for performing interactive audio rendering 
are direct room impulse response rendering and parametric room  (a) (b) (c) (d) Fig. 6. Different Benchmarks: 
(a) Game Model with low geometric complexity. It has dynamic objects and a moving listener. (b) Sibenik 
Cathedral, a complex architectural model with a lot of details and curved geometry. It consists of moving 
source and listener, and a door that opens and closes. (c) City Scene with many moving cars (dynamic 
objects) and tall buildings which cause edge diffraction. It has a moving sound source, a static sound 
source, and a listener. (d) Soda Hall, a complex architectural model with multiple .oors. The dimensions 
of a room are dynamically modi.ed and the sound propagation paths recomputed for this new room using 
AD-Frusta. This scenario demonstrates the potential of our approach for conceptual acoustic design. 7 
6 Number of Cores Fig. 8. This .gure shows that our algorithm scales well with the number of cores. 
It makes our approach favorable for parallel and many-core platforms. Re.ections for up to 4th order 
were computed and larger of maximum sub-division depths similar to those in Table 3 were used. Model 
Frusta traced Frusta Gain Time Gain UFT AFT Theater 404K 56K 7.2 6.1 Factory 288K 40K 7.2 5.7 Game 2330K 
206K 11.3 9.0 Sibenik 6566K 198K 33.2 21.9 City 377K 78K 4.9 5.2 Soda Hall 773K 108K 7.2 9.8 Table 2. 
This table presents a preliminary comparison of the number of frusta traced and time taken by Adaptive 
Frustum Tracing and Uniform Frustum Tracing [19] for results of similar accuracy. Our adaptive frus­tum 
formulation signi.cantly improves the performance as compared to prior approaches. impulse response rendering 
[29]. In direct room impulse response rendering , impulse responses are pre-computed at some .xed listener 
locations. Next, the impulse response at some arbitrary listener po­sition is computed by interpolating 
the impulse responses at nearby .xed listener locations. Such a rendering method is not very suitable 
when the source is moving or the scene geometry is changing. Parametric room impulse response rendering 
uses parameters like re.ection path to the listener, materials of the objects in re.ection path etc. 
to perform interactive audio rendering. However, in this case it is important that the underlying geometric 
propagation system is able to update these parameters at very high update rates. Sandvad [28] suggests 
that update rates above 10 Hz should be used. In the DIVA system [29], an update rate of 20 Hz is used 
to generate artifact free audio rendering for dynamic scenes. We also use similar audio ren­dering techniques. 
Table 3 shows that we can update parameters at a high rate with maximum sub-division depth of 2 on a 
single core, as required by Parametric room impulse response rendering . Simi­lar update rates for a 
sub-division depth of 3 on a single core can be achieved by decreasing the order of re.ection or by updating 
the low order re.ections more frequently than higher order re.ections (see Ta­ble 3). Furthermore, we 
need to interpolate the parameters between the updates, as suggested in [42, 35], to minimize the artifacts 
due to the changing impulse responses. In order to demonstrate the accuracy of our propagation algorithm, 
we perform audio rendering of.ine in some benchmarks. Also, the impulse responses used to generate the 
.nal rendered audio contain only early specular re.ections and no late reverberation. This is done so 
it is easy to evaluate the effect of early re.ections computed by our adaptive frustum tracing algorithm. 
Late reverberations can be computed as a pre-processing step [29] and can improve the audio quality. 
Model Tris Order of Re.ection (Running time in msec) Maximum sub-division depth 1st 2nd 3rd 4th Theater 
54 4.7 8.0 16.0 30.0 2 24 77 201 462 4 Factory 174 3.3 6.7 12.7 24.7 2 18 64 178 406 4 Game Scene 14K 
20 46 120 270 2 51 167 469 1134 3 Sibenik 71K 34 83 250 694 2 67 236 864 2736 3 City 72K 6.7 12 23 44 
2 18 38 85 174 3 Soda Hall 1.5M 21 38 98 229 2 35 102 297 723 3 Table 3. This table shows the performance 
of adaptive frustum tracing on a single core for different maximum sub-division depths. The timings are 
further broken down according to order of re.ection. Our propaga­tion algorithm achieves interactive 
performance for most benchmarks. 6.3 Accuracy and Validation Our approach is an approximate volume tracing 
approach and we can control its accuracy by varying the maximum-subdivision depth of each AD-Frustum. 
In order to evaluate the accuracy of propaga­tion paths, we compare the impulse responses computed by 
our algo­rithm with other geometric propagation methods. We are not aware of any public or commercial 
implementation of beam tracing which can handle complex scenes with dynamic objects highlighted in Ta­ble 
1. Rather, we use an industry strength implementation of im­age source method available as part of CATT-AcousticTM 
software. We compare our results for specular re.ection on various benchmarks available with CATT software 
(see Fig. 9 and Fig. 10). The results show that our method gives more accurate results with higher maxi­mum 
sub-division depths. 7 ANALYSIS AND COMPARISON In this section, we analyze the performance of our algorithm 
and com­pare it with other geometric propagation techniques. The running time of our algorithm is governed 
by three main factors: model complexity,  .2009 IEEE. Reprinted, with permission, from IEEE TRANSACTION 
ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 14, NO. 6, NOVEMBER/DECEMBER 2008 Image Source (Source=S1, 
Listener=L1) 0.9 0.8 0.7 0.6 amplitude   0.5 0.4 0.3 0.2 0.1 0 time Image Source (Source=S1, Listener=L1) 
1.4 (a) Theater benchmark (b) 65 contributions 1.2 Adaptve Volume Tracing (Source=S1, Listener=L1, 
Subdivision=2) Adaptve Volume Tracing (Source=S1, Listener=L1, Subdivision=4) Adaptve Volume Tracing 
(Source=S1, Listener=L1, Subdivision=5) 0.9 0.9 0.9 1 0.8 0.8 0.8  amplitude 0.8 0.7 0.7 0.7 0.6 
 0.6 0.6 0.6 amplitude amplitude amplitude 0.40.5 0.5 0.4 0.3 0.2 0.1 0 0.4 0.3 0.3 0.2 0.2 0.1 0.1 
0 0 0.4 0.2 0 2000 4000 6000 time 8000 10000 0 2000 4000 6000 time 8000 10000 0 2000 4000 6000 time 
8000 10000 (c) 23 contributions (d) 33 contributions (e) 60 contributions 1.2 1.4 1 Fig. 9. (a) Theater 
benchmark with 54 triangles. (b) Impulse response amplitude 0.8 0.6 generated by image source method 
for 3 re.ections. (c) (e) Impulse responses generated by our approach with maximum sub-division depth 
of 2, 4, and 5 respectively for 3 re.ections. level of dynamism in the scene, and the relative placement 
of objects with respect to the sources and receiver. As part of a pre-process, we compute a bounding 
volume hierarchy of AABBs. This hierarchy is updated as some objects in the scene move or some objects 
are added or deleted from the scene. Our current implementation uses a linear time re.tting algorithm 
that updates the BVH in a bottom-up manner. If there are topological changes in the scene (e.g. explosions), 
than the resulting hierarchy can have poor culling ef.ciency and can result in more intersection tests 
[39]. The complexity of each frustum intersec­tion test is almost logarithmic in the number of scene 
primitives and linear in the number of sub-frusta generated based on adaptive subdi­vision. The actual 
number of frusta traced also vary as a function of number of re.ections as well as the relative orientation 
of the objects. 7.1 Comparison The notion of using an adaptive technique for geometric sound prop­agation 
is not novel. There is extensive literature on performing adap­tive supersampling for path or ray tracing 
in both sound and visual rendering. However, the recent work in interactive ray tracing for vi­sual rendering 
has shown that adaptive sampling, despite its natural advantages, does not perform near as fast as simpler 
approaches that are based on ray-coherence [39]. On the other hand, we are able to perform fast intersection 
tests on the AD-Frusta using ray-coherence and the Pl¨ ucker coordinate representation. By limiting our 
formulation to 4-sided frusta, we are also able to exploit the SIMD capabilities of current commodity 
processors. Many adaptive volumetric techniques have also been proposed for geometric sound propagation. 
Shinya et al. [1987] traces a pencil of rays and require that the scene has smooth surfaces and no edges, 
which is infeasible as most models of interest would have sharp edges. Rajkumar et al. [1996] used static 
BSP tree structure and the beam starts out with just one sample ray and is subdivided only at primi­tive 
intersection events. This can result into sampling problems due to missed scene hierarchy nodes. Drumm 
and Lam [2000] describe an adaptive beam tracing algorithm, but it is not clear whether it can han­dle 
complex, dynamic scenes. The volume tracing formulation [12] shoots pyramidal volume and subdivides them 
in case they intersect with some object partially. This approach has been limited to visual rendering 
and there may be issues in combining this approach with a scene hierarchy. The bene.ts over the ray-frustum 
approach [19, 18] are shown in Fig. 5. The propagation based on AD-Frustum traces fewer frusta. In many 
ways, our adaptive volumetric tracing offers contrasting features as compared to ray tracing and beam 
tracing algorithms. Ray tracing algorithms can handle complex, dynamic scenes and can model diffuse re.ections 
and refraction on top of specular re.ection and diffraction. However, these algorithms need to perform 
very high su­ 0 0 2000 4000 6000 8000 10000 time (a) Factory benchmark (b) 40 contributions Adaptve Volume 
Tracing (Source=S1, Listener=L1, Subdivision=2) 1.4 1.2 1 amplitude 0.8 0.6 Adaptve Volume Tracing (Source=S1, 
Listener=L1, Subdivision=4) Adaptve Volume Tracing (Source=S1, Listener=L1, Subdivision=5) 1.4 1.2 1 
amplitude 0.8 0.6  0.4 0.4 0.4 0.2 0.2 0.2 0 0 0 time time time (c) 23 contributions (d) 32 contributions 
(e) 41 contributions Fig. 10. (a) Factory benchmark with 174 triangles. (b) Impulse response generated 
by image source method for 3 re.ections. (c) (e) Impulse responses generated by our approach with maximum 
sub-division depth of 2, 4, and 5 respectively for 3 re.ections. persampling to overcome noise and aliasing 
problems, both spatially and temporally. For the same accuracy, propagation based on AD-Frustum can be 
much faster than ray tracing for specular re.ections and edge diffraction. The beam tracing based propagation 
algorithms are more accurate as compared to our approach. Recent improvements in the perfor­mance of 
beam tracing algorithms [24, 17] are promising and can make them applicable to more complex static scenes 
with few tens of thousands of triangles. However, the underlying complexity of per­forming exact clipping 
operations makes beam tracing more expen­sive and complicated. In contract, AD-Frustum compromises on 
the accuracy by performing discrete clipping with the 4-sided frustum. Similarly, image-source methods 
are rather slow for interactive appli­cations. 7.2 Limitations Our approach has many limitations. We 
have already addressed the accuracy issue above. Our formulation cannot directly handle dif­fuse, lambertian 
or glossy re.ections. Moreover, it is limited to point sources though we can potentially simulate area 
and volumetric sources if they can be approximated with planar surfaces. Many of the underlying computations 
such as maximum-subdivision depth param­eter and intersection test based on Pl¨ucker coordinate representation 
are conservative. As a result, we may generate unnecessary sub-frusta for tracing. Moreover, the shape 
of the diffraction frustum may extend beyond the actual diffraction .eld originating from the edge. Limi­tations 
of UTD-based edge diffraction are mentioned in Section 4.1. Currently, our audio rendering system does 
not perform interpolation of parameters, as suggested in [42, 35], and hence could result in some clicking 
artifacts when performing interactive audio rendering. 8 CONCLUSION AND FUTURE WORK We present an adaptive 
volumetric tracing for interactive sound prop­agation based on transmissions, specular re.ections and 
edge diffrac­tion in complex scenes. Our approach uses a simple, adaptive frustum representation that 
provides a balance between accuracy and interac­tivity. We verify the accuracy of our approach by comparing 
the per­formance on simple benchmarks with commercial state-of-the-art soft­ware. Our approach can handle 
complex, dynamic benchmarks with tens or hundreds of thousands of triangles. The initial results seem 
to indicate that our approach may be able to generate plausible sound rendering for interactive applications 
such as conceptual acoustic de­sign, video games and urban simulations. Our approach maps well to the 
commodity hardware and empirical results indicate that it may scale linearly with the number of cores. 
 There are many avenues for future work. We would like to use perceptual techniques [38] to handle multiple 
sources as well as use scattering .lters for detailed geometry [36]. It may be possible to use visibility 
culling techniques to reduce the number of traced frusta. We can re.ne the criterion for computation 
of maximum-subdivision depth parameter based on perceptual metrics. It may be useful to use the Biot-Tolstoy-Medwin 
(BTM) model of edge diffraction instead of the UTD model [3]. We also want to develop a robust interactive 
au­dio rendering pipeline which can integrate well with UTD. Finally, we would like to extend the approach 
to handle more complex environ­ments and evaluate its use on applications that can combine interactive 
sound rendering with visual rendering. ACKNOWLEDGEMENTS We thank Paul Calamia, Nikunj Raghuvanshi, and 
Jason Sewall for their feedback and suggestions. This work was supported in part by ARO Contracts DAAD19-02-1-0390 
and W911NF-04-1-0088, NSF awards 0400134, 0429583 and 0404088, DARPA/RDECOM Contract N61339-04-C-0043, 
Intel, and Microsoft. REFERENCES [1] J. B. Allen and D. A. Berkley. Image method for ef.ciently simulating 
small-room acoustics. The Journal of the Acoustical Society of America, 65(4):943 950, April 1979. [2] 
M. Bertram, E. Deines, J. Mohring, J. Jegorovs, and H. Hagen. Phonon tracing for auralization and visualization 
of sound. In Proceedings of IEEE Visualization, pages 151 158, 2005. [3] P. Calamia and U. P. Svensson. 
Fast Time-Domain Edge-Diffraction Cal­culations for Interactive Acoustic Simulations. EURASIP Journal 
on Ad­vances in Signal Processing, 2007. [4] B.-I. Dalenb¨ack. Room acoustic prediction based on a uni.ed 
treatment of diffuse and specular re.ection. The Journal of the Acoustical Society of America, 100(2):899 
909, 1996. [5] B.-I. Dalenb¨omberg. Real time walkthrough auralization ­ ack and M. Str¨the .rst year. 
Proceedings of the Institute of Acoustics, 28(2), 2006. [6] E. Deines, M. Bertram, J. Mohring, J. Jegorovs, 
F. Michel, H. Hagen, and G. Nielson. Comparative visualization for wave-based and geometric acoustics. 
IEEE Transactions on Visualization and Computer Graphics, 12(5), 2006. [7] I. A. Drumm and Y. W. Lam. 
The adaptive beam-tracing algorithm. Jour­nal of the Acoustical Society of America, 107(3):1405 1412, 
March 2000. [8] A. Farina. RAMSETE -a new Pyramid Tracer for medium and large scale acoustic problems. 
In Proceedings of EURO-NOISE, 1995. [9] S. Fortune. Topological beam tracing. In SCG 99: Proceedings 
of the .fteenth annual symposium on Computational geometry, pages 59 68, New York, NY, USA, 1999. ACM. 
[10] T. Funkhouser, I. Carlbom, G. Elko, G. Pingali, M. Sondhi, and J. West. A beam tracing approach 
to acoustic modeling for interactive virtual en­vironments. In Proc. of ACM SIGGRAPH, pages 21 32, 1998. 
[11] T. Funkhouser, N. Tsingos, and J.-M. Jot. Survey of Methods for Model­ing Sound Propagation in Interactive 
Virtual Environment Systems. Pres­ence and Teleoperation, 2003. [12] J. Genetti, D. Gordon, and G. Williams. 
Adaptive supersampling in object space using pyramidal rays. Computer Graphics Forum, 17:29 54, 1998. 
[13] C. Joslin and N. Magnetat-Thalmann. Signi.cant facet retrieval for real­time 3D sound rendering. 
In Proceedings of the ACM VRST, 2003. [14] B. Kapralos, M. Jenkin, and E. Milios. Acoustic Modeling Utilizing 
an Acoustic Version of Phonon Mapping. In Proc. of IEEE Workshop on HAVE, 2004. [15] J. B. Keller. Geometrical 
theory of diffraction. Journal of the Optical Society of America, 52(2):116 130, 1962. [16] A. Krokstad, 
S. Strom, and S. Sorsdal. Calculating the acoustical room response by the use of a ray tracing technique. 
Journal of Sound and Vibration, 8(1):118 125, July 1968. [17] S. Laine, S. Siltanen, T. Lokki, and L. 
Savioja. Accelerated beam tracing algorithm. Applied Acoustic, 2008. to appear. [18] C. Lauterbach, A. 
Chandak, and D. Manocha. Adaptive sampling for frustum-based sound propagation in complex and dynamic 
environments. In Proceedings of the 19th International Congress on Acoustics, 2007. [19] C. Lauterbach, 
A. Chandak, and D. Manocha. Interactive sound propa­gation in dynamic scenes using frustum tracing. IEEE 
Trans. on Visual­ization and Computer Graphics, 13(6):1672 1679, 2007. [20] C. Lauterbach, S.-E. Yoon, 
M. Tang, and D. Manocha. ReduceM: Inter­active and Memory Ef.cient Ray Tracing of Large Models. In Proc. 
of the Eurographics Symposium on Rendering, 2008. [21] C. Lauterbach, S.-E. Yoon, D. Tuft, and D. Manocha. 
RT-DEFORM: In­teractive Ray Tracing of Dynamic Scenes using BVHs. IEEE Symposium on Interactive Ray Tracing, 
2006. [22] R. B. Loftin. Multisensory perception: Beyond the visual in visualization. Computing in Science 
and Engineering, 05(4):56 58, 2003. [23] F. Michel, E. Deines, M. Hering-Bertram, C. Garth, and H. Ha­gen. 
Listener-based Analysis of Surface Importance for Acoustic Met­rics. IEEE Transactions on Visualization 
and Computer Graphics, 13(6):1680 1687, 2007. [24] R. Overbeck, R. Ramamoorthi, and W. R. Mark. A Real-time 
Beam Tracer with Application to Exact Soft Shadows. In Eurographics Sympo­sium on Rendering, Jun 2007. 
[25] A. Rajkumar, B. F. Naylor, F. Feisullin, and L. Rogers. Predicting RF coverage in large environments 
using ray-beam tracing and partitioning tree represented geometry. Wirel. Netw., 2(2):143 154, 1996. 
[26] A. Reshetov, A. Soupikov, and J. Hurley. Multi-level ray tracing algo­rithm. ACM Trans. Graph., 
24(3):1176 1185, 2005. [27] C. Ronchi, R. Iacono, and P. Paolucci. The Cubed Sphere : A New Method for 
the Solution of Partial Differential Equations in Spherical Ge­ometry. Journal of Computational Physics, 
124:93 114(22), 1996. [28] J. Sandvad. Dynamic aspects of auditory virtual environment. In Audio Engineering 
Society 100th Convention preprints, page preprint no. 4246, April 1996. [29] L. Savioja, J. Huopaniemi, 
T. Lokki, and R. V¨a¨an¨anen. Creating inter­active virtual acoustic environments. Journal of the Audio 
Engineering Society (JAES), 47(9):675 705, September 1999. [30] M. Shinya, T. Takahashi, and S. Naito. 
Principles and applications of pencil tracing. Proc. of ACM SIGGRAPH, 21(4):45 54, 1987. [31] K. Shoemake. 
ucker coordinate tutorial. Pl¨Ray Tracing News, 11(1), 1998. [32] S. Siltanen, T. Lokki, S. Kiminki, 
and L. Savioja. The room acoustic rendering equation. The Journal of the Acoustical Society of America, 
122(3):1624 1635, September 2007. [33] S. Smith. Auditory representation of scienti.c data. In Focus 
on Scienti.c Visualization, pages 337 346, London, UK, 1993. Springer-Verlag. [34] M. Taylor, C. Lauterbach, 
A. Chandak, and D. Manocha. Edge Diffrac­tion in Frustum Tracing. Technical report, University of North 
Carolina at Chapel Hill, 2008. [35] N. Tsingos. A versatile software architecture for virtual audio simula­tions. 
In International Conference on Auditory Display (ICAD), Espoo, Finland, 2001. [36] N. Tsingos, C. Dachsbacher, 
S. Lefebvre, and M. Dellepiane. Instant sound scattering. In Proceedings of the Eurographics Symposium 
on Ren­dering, 2007. [37] N. Tsingos, T. Funkhouser, A. Ngan, and I. Carlbom. Modeling acoustics in virtual 
environments using the uniform theory of diffraction. In Proc. of ACM SIGGRAPH, pages 545 552, 2001. 
[38] N. Tsingos, E. Gallo, and G. Drettakis. Perceptual audio rendering of complex virtual environments. 
ACM Trans. Graph., 23(3):249 258, 2004. [39] I. Wald, W. Mark, J. Gunther, S. Boulos, T. Ize, W. Hunt, 
S. Parker, and P. Shirley. State of the Art in Ray Tracing Dynamic Scenes. Eurographics State of the 
Art Reports, 2007.  [40] M. Wand and W. Straßer. Multi-resolution sound rendering. In SPBG 04 Symposium 
on Point -Based Graphics 2004, pages 3 11, 2004. [41] J. Warnock. A hidden-surface algorithm for computer 
generated half-tone pictures. Technical Report TR 4-15, NTIS AD-753 671, Department of Computer Science, 
University of Utah, 1969. [42] E. Wenzel, J. Miller, and J. Abel. A software-based system for interac­tive 
sound synthesis. In International Conference on Auditory Display (ICAD), Atlanta, GA, April 2000. [43] 
S.-E. Yoon, S. Curtis, and D. Manocha. Ray Tracing Dynamic Scenes using Selective Restructuring. In Proc. 
of the Eurographics Symposium on Rendering, 2007.  Proc.of the EAA Symposium onAuralization, Espoo,Finland, 
15-17June 2009 FAST EDGE-DIFFRACTION FOR SOUND PROPAGATION IN COMPLEX VIRTUAL ENVIRONMENTS MicahTaylor, 
Anish Chandak, Zhimin Ren, Christian Lauterbach, and Dinesh Manocha University of North Carolina {taylormt,achandak,zren,cl,dm}@cs.unc.edu 
 http://gamma.cs.unc.edu/SOUND/Diffraction/ ABSTRACT We present an algorithm forfast computation of 
diffraction paths for geometric-acoustics in complexenvironments based on the UTD formulation. Our methodextends 
ray-frustum tracingtoef.ciently compute paths in the shadow region caused by long diffracting edges. 
Our approach can handle general scenes with moving sources, receivers, and dynamic objects.Weevaluate 
the accuracythrough comparisons with physically validated geometric simulations. In practice, our edge 
diffraction algorithm can perform sound propa­gation at nearly interactive rates in dynamic scenarios 
on a multi­core PC. 1. INTRODUCTION Modeling realistic propagation of sound in virtual environments 
can provide important cues for user immersion. There is exten­sive literature on simulating the propagation 
of sound, including re.ections and diffraction. However, prior methods do not pro­vide suf.cient .exibility 
and ef.ciency that is needed for use in interactive virtual environments. In this paper, we primarily 
focus on simulating early sound propagation paths, namely edge diffraction contributions. The in­clusion 
of paths in the diffraction region can conveyimportant au­dio cues from sources that are not directly 
visible to the receiver. Itis necessaryto simulatediffraction accuratelyin orderto obtain realistic and 
smooth transitions when the receiver or the source is moving. The accurate solution of diffraction involves 
numerically solv­ing the wave equation. However, the high computational require­ments of numerical solverslimit 
their use to of.ine simulations. Asaresult, current interactivesound propagation methods are based on 
Geometric-Acoustic (GA) techniques. The strength of GA tech­niques lies in thefact that they are able 
to quickly calculate au­dio contributions arising from direct contributions and re.ections. This performance 
bene.t comes at a price in that GA techniques model only high-frequencycomponents of direct propagation, 
spec­ular re.ection, and diffuse re.ection [1, 2, 3, 4]. Other effects, such as diffraction, are relatively 
dif.cult to capture and can have a high computational cost [5]. There has been much research on diffraction 
methods that integrate well with GA solutions [5, 6, 7, 8]. Main results: We present a near-interactive 
approach to perform edge diffraction around long edges in complex virtual environ­ments.Ourapproachextendsthe 
ray-frustumtracingmethod[3]to perform edge diffraction based on the Uniform Theory of Diffrac­tion (UTD) 
[9]. The resulting method takes advantage of thefact that frustum tracing sdiscrete, sub-division based 
formulation pro­videsaframeworkto creatediffraction frustawith adaptablespeed and accuracy. These newfrusta 
are used to compute the diffraction contribution paths in the region around long edges. The overall al­gorithm 
retains the underlying advantages of frustum tracing with respect to ef.ciency and can handle complex, 
dynamic environ­ments with moving sources or listeners. Our algorithm has been implemented and integrated 
with the frustum-tracing based sound propagation method.We compare its accuracywith previous methods 
on the well-known Bell Labs box [10]. We also highlight its performance on complex indoor and outdoor 
scenes with dynamic objects, moving sources and listen­ers. In practice, we can accurately compute the 
propagation paths with diffraction and specular re.ections at near-interactive rates on a multi-core 
PC workstation. Out method can be used to gen­erate plausible sound rendering in complex and dynamic 
virtual environments. Organization: The rest of the paper is organized in the follow­ing manner. We brie.y 
survey prior work in sound propagation in Section 2. We present the details of our algorithm in Section 
3. We then analyze its accuracy in Section 4 and highlight the performance on complex models in Section 
5. 2. PREVIOUSWORK ANDBACKGROUND In this section, we give a brief overview of prior work in sound propagation 
and diffraction computation. Numerical methods of audio propagation provide the most accurate results, 
correctly mod­eling diffraction as well as low frequency wave effects. There are several techniques that 
may be used, such as boundary element methods, .nite element methods, digital waveguides [11], and .­ 
nite difference time domain [12]. However, even with signi.cant performance improvements [13], these 
methods can take a few hours on simple scenes and are limited to static scenes. Geometric Acoustic (GA) 
techniques assume that high fre­quency sound waves can be modeled as rays. The primary GA techniques 
include image-source [14], ray tracing [15], beam trac­ ing[2],and frustum tracing[3]. Image-source methodshaveavery 
high cost when simulating higher orders of re.ection and are often combined with ray tracing to reduce 
the computation time needed for high order re.ections. Ray tracing techniques arefast,but can suffer 
from aliasing or sampling errors. Beam tracing is a vol­umetric technique and performs accurate geometric 
propagation, but relies on elaborate clipping algorithms and acceleration struc­tures that are limited 
to static scenes. Frustum tracing attempts to strikea balance between ray tracing and beam tracingby 
perform­ing discrete clipping along with volumetric tracing. This greatly 1  Proc.of the EAA Symposium 
onAuralization, Espoo,Finland, 15-17June 2009 the simulation, frusta are checked for diffracting edge 
containment. If so, a new diffraction frustum is created. After the propagation is complete, the diffraction 
paths are attenuatedby the UTD coef.cients. reduces sampling issues and uses hierarchical acceleration 
struc­tures to handle complex, dynamic scenes. There has been much work combining diffraction with GA 
methods. The two primary diffraction models used in geometrical simulations are the Uniform Theory of 
Diffraction (UTD) [9] and the Biot-Tolstoy-Medwin (BTM) [16, 17] method. These methods are widely used 
since they describe the diffraction of a ray path incident on an edge. The BTM method is considered more 
accu­rate than UTD and can be formulated for use with .nite edges [6]. However, the BTM method is compute 
intensive, which has led to different techniques to improve its performance [8, 18]. The UTD method has 
lower computational requirements and has been used to calculate diffraction coef.cients for several in­teractive 
simulations, based on beam tracing [5] and 2D visibility diagrams [19]. In practice, these approaches 
havebeen mainly lim­ itedto static scenes.Inthispaper,weusetheUTDasithasamuch lower computationaloverheadandis 
more amenableto interactive applications. We primarily focus on diffraction contributions in the shadow 
region (out of line-of-sight) for speed concerns. 3. ALGORITHM In this section, we present our algorithm 
in detail and address the issues that arise in terms of incorporating edge diffraction in ray­frustum 
tracing.For details on our approach, we refer to[3, 20]. The underlying algorithm generates 4-sided frusta 
based on spec­ular re.ections and edge diffractions and intersects the frusta with the scene primitives. 
After intersection, if a frustum is not fully contained within a scene triangle, part of the frustum 
must lie out­side the triangle edges. In this case, the frustum is sub-divided into sub-frusta using 
a quad-tree structure. These sub-frusta are then intersected with the triangle and the process repeats 
to a user de­.ned limit. This sub-division allows a frustum to more accurately represent the shape of 
the scene primitives encountered. In order to reduce the runtime overhead of checking if a tri­angle 
edge can diffract, our algorithm precomputes allthe diffrac­tion edges in the scene as part of a preprocessing 
step. When a sub-frustum is found to contain a potential diffracting edge, a new frustum is created to 
contain the possible diffraction contributions. After all propagation paths are found, they are attenuated 
based on the path characterization and scene primitives. Figure1shows thevarious stepsof our algorithm.Whilewe 
are immediately con­cerned with the direct, specular, and diffraction components, our algorithm can also 
be combined with diffuse and late reverberation calculations for more accurate simulations [21]. 3.1. 
Preprocess Rather than explicitly testing whether an edge is a candidate for diffraction at runtime, 
we .nd all possible diffraction edges as part ofapreprocessbyanalyzing the scene and marking the long 
edges. Speci.cally, we use a data structure that links each edge to its in­cident triangles. The edges 
of each triangle are classi.ed based on thesurface normalsofthe incidentfaces(seeFigure2). (a) (b) (c) 
(d) Figure 2: Preprocessed edge types: (a) Planar edges that never diffract; (b) exterior edges that 
always diffract; (c) interior edges and (d) disconnected edges that can be con.gured by user choice to 
diffract. If the triangles incident to an edge have similar normals, they are considered almost planar 
and the resulting edge is not consid­ered as a candidate for diffraction (see Figure 2(a)). If the normals 
are exterior and point away from one another, the edge is part of a diffracting wedge (Figure 2(b)). 
The two surface normals are used to compute the wedge angle that is later used for calculating the diffraction 
coef.cients. There are two other remaining cases that can be marked as diffracting or non-diffracting 
depending on the scene design. In scenes where triangles form both the interior and exterior sides of 
a wall, the user can elect to have triangles with normalsfacing inwards (Figure 2(c)) marked as diffracting 
edges from the backface. Similarly, disconnected edge (i.e. triangles without neighbors, see Figure 2(d)) 
can be marked as diffracting edge if the user desires. If marked, each disconnected edge would have a 
wedge angle of 2p radians. Edges may also be marked by other general criteria, such as direct user selection 
or minimum and maximum length. 3.2. Edge containment During scene traversal, it is necessary to identify 
the diffracting edges that are contained within a propagating frustum. Consider the case where a frustum 
intersects a triangle and is not fully con­tained within the triangle. In this case, at least one of 
the corners of the frustumface lies outsideof the triangle edges (Figure 3(a)). After manyiterations 
of the adaptive frustum subdivision, the sub­division limit is reached and the edge is approximated by 
many sub-frusta, as shown in Figure 3(b). Some of these sub-frusta must contain the edge that caused 
the initial subdivision. Aseries of tests determine whether a diffracting edge is con­tained within a 
frustum and thus whether we need to compute a 2  Proc.of the EAA Symposium onAuralization, Espoo,Finland, 
15-17June 2009 (a) (b) (c) (d) Figure 4: Diffraction frustum creation: (a) Given a frustum s origin 
o and its edge intersection points i1 and i2, (b) the edge axis e and the initial diffraction vectors 
d1 and d2 are created. (c) Rotating d1 and d2 about the edge axis towards the far side of the diffracting 
wedge sweeps a diffraction cone in the shadow region bounded by the .nal vectors d3 and d4. (d) We create 
the complete the frustum volume. (a) (b) (c) Figure 3: Edge containmentcheck: After the frustum encounters 
a triangle (a), its face is projected into the triangle plane (b). Eachdiffractingedgeisthencheckedfor 
intersectionwiththeface (c) to .nd the intersection points i1 and i2. diffraction frustum. Since the 
preprocessing step has assigned each of the triangle s edges a type, if none are marked as a diffracting 
edge, the test terminates. However, if one of the edges is diffract­ing,itis necessaryto.ndthe portionoftheedgethatisexposedto 
the sound .eld. This is tested by performing intersection between the edge and the four lines that form 
the frustumface boundaries. The diffracting edge is checked for intersection against the bound­aries 
of the frustumface. If the edge does not cross the bounds of the sub-frustum, the diffracting edge must 
not be within this sub-frustum and the test is repeated with the next sub-frustum. In the case that the 
frustum boundaries intersect the diffracting edge, the exact orientation of the edge within the frustum 
needs to be determined. This is performed by completing the intersection calculation and .nding the two 
intersection points i1,i2 (see Fig­ure3(c)),oftheedgeonthe frustum boundary. These intersection points 
are used in the construction of the diffraction frustum. 3.3. Diffraction frustum construction When a 
diffracting edge is found within a frustum, a diffraction frustum is created and propagated through the 
scene. This diffrac­tion frustum should encompass the shadow region that is hidden from the direct contribution 
or specular contribution.We will now detail the calculations used during frustum creation (see Figure 
4). Since most diffracting edges are located at wedges formed where two triangles meet, we will differentiate 
between the two triangles as the source side and the receiver side of the wedge. The source sideisthesidethatisexposedtothe 
original propagationpath;the receiver sideis the side where the new diffracted .eld will propa­gate in 
the shadow region. In order to create a diffraction frustum, given the diffracting edge, the region of 
the edge that is contained within the initial frustum must be known. The intersection points from the 
edge containment test describe this portion of the edge. Using these points i1 and i2 on the edge and 
the origin of the original frustum o, two new vectors d1 and d2 are de.ned as d1 = i1 - o and d2 = i2 
- o. Thesevectors describethesideofthenewdiffraction frustum that borders the transition from line-of-sight 
contribution to shadow contribution. Next,we constructthevectorsthatareusedto representthefar planeofthediffraction 
frustum. Thisfarplanewill bordertheface of the triangle on the receiver side of the diffracting edge, 
and com­bined with the .rst set of vectors, bounds a portion of the shadow region.Webegin the computationby 
de.ning an edge axisvector e = i1 - i2. There is a vector fr which is perpendicular to the diffracting 
edge and lies in the plane of the triangle that represents thereceiversideofthediffractingedge. Thisreceiverfacevectoris 
de.ned as fr = e×nr, wherenr is the normal of the receiver side triangle. We also compute the vector 
dperp by projecting d1 onto the plane perpendicular to e. Once thesevectors are computed, we .nd the 
angle between them, and rotate d1 and d2 about e towards fr by this angle. Beginning at d1 and d2, at 
intervals along the ro­tation, new frusta are created to approximate the diffraction cone, with the rotation 
ending at the vectors d3 and d4, which lie in the plane of the triangle corresponding to the receiver 
side. In order to create the full diffraction region about the edge (not just the shadow region), the 
vectors d3 and d4 can be created ef.ciently as: j (ds · e)e +(ds · fr)fr if ds · fr < 0 df = (ds · e)e 
- (ds · fr)fr otherwise Where df is the resulting vector in the plane of the receiver trian­gle and 
ds is a vector that borders the shadow region. Similar to theexamplein Figure4, d4 results from d1 and 
d3 results from d2. The vectors d4 and d3 are then rotated about e towards the trian­gleface on the source 
side of the wedge. At intervals along this swept region, new frusta are created to approximate the diffraction 
region. 3.4. Path generation Thefrustum tracing algorithm generates new re.ection and edge diffraction 
frusta based on the intersections with scene primitives. As each frustum traverses the scene, the data 
needed to attenuate itscontributionispushedonastack.This includesthedatathatde­scribes the direction 
and location of the frustum and the geometric primitives encountered. The re.ected frusta have the material 
at­tenuation values pushed, while diffraction frusta have the wedge angle and triangle data pushed onto 
the stack. This data is later used to create the contribution paths used in generating an IR (Im­pulse 
Response). As each frustum is propagated through the scene, it is checked for containment of the receiver. 
If the receiver is contained in the frustum, there is some re.ection or diffraction path from the source 
tothe receiver.Wewishto computethepathsegment found inside the receiver containing frustum as well as 
the segments inside each parent frustum that was propagated up to the containing frustum. 3  Proc.of 
the EAA Symposium onAuralization, Espoo,Finland, 15-17June 2009 These path segments are computed from 
a linear combination of the four the rays that form the frustum edges.Together, these path segments represent 
the entire contribution path through the scene. This is quick to compute and for specular re.ection, 
accurately represents the contribution path. However, for diffraction, there is slight error in the path 
vectors since the UTD cone is being ap­proximated by a .nite number of frusta. The details of this error 
are outlined in Section 4.2. 3.5. Attenuation After all the frusta are computed, the contribution paths 
are used to compute an impulse response for the scene. Algorithms to com­pute the attenuation values 
for re.ection have been described pre­viously [3]. In this section, we highlight the calculation of the 
diffraction attenuation coef.cients. TheUTDwas chosenasthe methodto generatethediffraction coef.cientsforthepathssinceitworkswellwiththe 
discreteinter­section tests performed for ray-frustum culling. The UTD assumes that an edge has in.nite 
length and the actual length or subset of the edge that is exposed to sound energy is not used. Speci.cally, 
a single frustum can be checked if it contains a valid diffraction path completely independently of all 
other frusta. Each path is attenuated using the UTD calculations for a user de.ned number of frequency 
bands (see Figure 6). Since only paths in the shadow region are attenuated, there will be a disconti­nuity 
in the .eld at the boundary of the shadow region. Near this boundary, approximate attenuationvalues are 
foundby normaliza­tion [5]. This allows a smooth transition at the shadow boundary. Figure 6: UTD attenuation: 
Aradio is playing behind the door. Thelightgreenregionshowsthe spectrumforthedirectpathwhen the door 
is open. The dark green region shows the spectrum of strongest diffraction path as the door closes. 4. 
ACCURACY Inthis sectionwevalidatethe results computedbyouredgediffrac­tion algorithm against results 
from the well known Bell Lab Box benchmark[10].We comparethe numberofpathsoursystem.nd forvarioussubdivisionlevelstothe 
numberofpathsthatthebeam tracing algorithm .nds in the Bell Lab Box benchmark. In addi­tion, we discuss 
the frustum tracing approximation of diffraction cones and detail the limitations of our approach. Throughout 
this section and the next, we willrefer to the sub­divisionlevel chosenforthe frustum tracing system.Givena 
max­imum subdivision level of x, a frustum may be recursively split up to 2x times. 4.1. Bell Lab Box 
comparison The "Bell Lab Box" is an acoustic benchmark. The Bell Lab Box is a large closed box which 
contains a sound source and receiver. An optional baf.e maybe inserted that obstructsthe visibility be­tween 
portionsofthebox.ThisBellLabBoxwasusedto conduct a controlled study [10] of audio diffraction. Knowing 
the scene dimensions and layout, physical measurements were compared to a beam tracing simulation of 
a similar virtual scene. The resulting output compares well with the physical measurements from the Bell 
Lab Box. The referenced paper provides the earliest 60 geometric paths traced from a source position 
to a receiver out of line-of-sight. We match the path sequences from their highly accurate geomet­ric 
simulation to sequences generated by our diffraction frustum propagation.For these comparisons, we compute 
frusta for the full diffraction region about the edge as predictedby the UTD, instead of using the shadow 
region approximation. IntheBellLabBoxpathdatatherearealarge numberofpaths that encounter the diffracting 
edge, and depart the edge traveling parallel along the diffracting wedge. Due to its basis on ray tracing 
methods, frustum tracing may not .nd these paths that are parallel to and travel along the plane of the 
diffracting wedge. Consider a frustum with corner rays that travel parallel to the wedgeface plane. Even 
if there are more diffracting edges in this plane, they will not be found, since such edges are not contained 
within the frustum shape. This is similar to intersecting a ray with a triangle oriented such that the 
triangle normal is perpendicular to the ray direction. We compare the number of paths found by each simulation 
in Figure 7. As the subdivision level of frustum tracing increases, more paths are found and the accuracyincreases. 
For clarity, we include results fromahypothetical frustum tracing simulator that correctly calculates 
all paths except those that are parallel to the diffracting surface. As shown in the .gure, the number 
of paths foundby high subdivision frustum tracing compares veryfavor­ably with this ideal frustum tracer. 
 Figure 7: Path length:As subdivision level increases, more paths are found and the error decreases. 
4.2. Accuracy of diffraction frustum Frustum tracing is an approximate method and can achieve high update 
rates by reducing simulation accuracy. Conversely, higher accuracy can be achieved by reducing the simulation 
update rate. The creation of diffraction frusta follows this same property. Diffrac­tion frusta are initially 
subdivided based on the subdivision level chosen by the user. Since each diffraction frustum is bounded 
 4  Proc.of the EAA Symposium onAuralization, Espoo,Finland, 15-17June 2009 above and below by an approximate 
diffraction cone, it is help­ful to evaluate the difference in volume between a subdivided ap­proximation 
anda perfect diffracting cone. Figure8andTable1 shows that as the subdivision level increases, the diffraction 
frus­tum quickly converges to the ideal volume. (a) (b) (c) (d) Figure 8: Frustum Subdivision accuracy: 
the resulting diffrac­tion cone witha subdivisionof0 (a), subdivision of1 (b), and subdivision of 2 (c). 
The diffraction frustum approximates the ideal cone (d). Table 1: Volume error: As the subdivision level 
increases, the error in the volume of the diffraction cone decreases 4.3. Limitations Since the UTD is 
used to calculate the diffraction coef.cients, the underlying restrictions of the UTD model naturally 
apply to our algorithm. UTD is a high frequency approximation, and is not very accurate for low frequency 
diffraction. Moreover, UTD as­sumes that the diffracting edge is of in.nite length and the source andreceiverarefarfromtheedge(relativetowavelength). 
These restrictions are discussed in further detail in [9]. Frustum tracing can be regarded as an approximation 
of beam tracing and introduces some additional limitations. While it is a volumetric technique, some 
paths are missed since the frusta can­not be subdivided in.nitely to represent the scene primitives. 
The level of subdivision is controlled either using a uniform global pa­rameter [3] or an adaptive subdivision 
scheme [20]. This error can beavoided entirelybyextending frustum tracingto compute accu­rate object-space 
visibility and perform accurate geometric acous­tics [22]. Since our method approximates the diffraction 
cone with many linearfrusta,thissubdivisioncan resultinover-estimationorunder­estimation in the .nal 
frustum volume. This results in the back projected path having a slightly over or under estimation of 
the path length. As discussed in 4.2, this error is reduced as the subdi­ vision limit is increased. 
The error may be eliminated entirely by computing conservative diffraction frusta [23]. Due to the approximation 
of computing only shadow region diffraction, a discontinuity exists at the shadow boundary. While this 
can certainly be resolved by computing the entire diffraction regionexactly,thisgreatly increasesthenumberoffrustathatmust 
be propagated. As such, we resolve the discontinuity by using approximate normalized attenuation values. 
As previously mentioned, frusta may have dif.culty .nding paths that lie parallel to the corner ray of 
the frustum. This may cause some important contribution paths to be unaccounted for. These paths could 
be found by including impostor structures at edges such that impostor will be encountered during propagation. 
Other potential approaches would be 2d ray intersection of the scene primitives in the plane of the wedge 
or propagating a spe­cial frustum along the plane that contains the region of the plane. 5. PERFORMANCE 
 In this section we evaluate the performance of our method with various scenes and settings. Unless otherwise 
noted, all simula­tions are rendered with8frequencybands and4threads ona mod­ern 2.66 Ghz multi-core 
machine. The scenes used are detailed in Table2.We usea maximum subdivisionlevelof3forall bench­marks. 
Also, unlike in the Bell Lab Box comparison, we only propagate diffraction frusta in the shadow region. 
Scene Type Triangles Diffracting edges Q3dm1 Closed 14k 4032 Atrocity Closed 12k 1531 Chartres Open 192k 
40489 Sibinek Open 76k 1358 Sponza Open 66k 1021 Highway Open 350k 1248 Sodahall Closed 1510k 9457  
Table 2: Scene overview: Data on the scenes used for the per­formanceresults. Some scenes are very open 
with muchgeometry visible from any given point. Others are closed, with short visibil­ity distances. 
5.1. Diffraction cost and bene.t Generating the diffraction frusta during simulation incurs a time cost 
in addition to direct contributions and specular re.ections. The time cost needed to propagate the diffraction 
frusta varies greatly from scene to scene due to the number of triangles and edges encountered. The bene.t 
of using diffraction also varies; depending on the scene layout and source/receiver position, very fewvaliddiffractionpathsmaybe 
found.Table3showsthe added costand bene.tofusingdiffractioninvarious sceneswith3orders of recursion. 
To highlight the effects of diffraction, in each scene we chose the source and receiver positions such 
that few specular paths are found. For example, in the highway scene, the receiver is placed behind an 
occluder that blocks all direct and specular contributions. 6. CONCLUSION AND FUTUREWORK The presented 
edge diffraction method enhances frustum tracing by allowing diffraction contribution paths that can 
be computed and auralized. Our resulting system computes direct contributions, specular re.ections and 
edge diffraction using ray-frustum trac­ing. To the best of our knowledge, this is the .rst edge diffrac­tion 
method that results in near-interactive performance in com­plex scenes with dynamic objects with reasonable 
GA accuracy. Theoverall performanceofthe system increasesasthe orderof re­.ections and diffraction increases. 
We have observed comparable results with the beam tracing method on the Bell Labs Box, and our algorithm 
can generate plausible acoustic simulation results on complex benchmarks. 5  Proc.of the EAA Symposium 
onAuralization, Espoo,Finland, 15-17June 2009 Scene Diffraction #Frusta Time Paths found Q3dm1 Off 80844 
219 ms 3 On 114372 338 ms 5 Atrocity Off 114183 282 ms 4 On 140454 370 ms 7 Chartres Off 219865 1306 
ms 2 On 292256 2078 ms 8 Sibinek Off 370594 1614 ms 12 On 377521 1636 ms 15 Sponza Off 198022 861 ms 
2 On 209737 921 ms 11 Highway Off 21178 62 ms 0 On 23553 84 ms 5 Sodahall Off 81269 436 ms 0 On 91879 
510 ms 3 Table 3: Diffraction bene.t: Diffraction incurs a slight perfor­mance decrease,but often .nds 
morepropagation paths. There are many avenues for future work. Stronger valida­tion of the found diffraction 
paths could be conducted to reduce or eliminate the slight path error. Computing more conservative diffraction 
frustawould reducethe possibilityof missing important paths. Conducting a conservative region visibility 
test from each encountered diffracting edge would make it possible to .nd paths suitable for auralization 
with the BTM (Biot-Tolstoy-Medwin) method. While this may have a large computational cost, it would allow 
more accurate simulation of lower frequencies and shorter diffract­ing edges. Another way to improve 
the performance is to reduce the numberof diffraction frusta generated. Thiswould allow more time to 
perform higher levels of subdivision or re.ections. There has been some work in the area of diffraction 
culling [8]. This would reduce the number of insigni.cant frusta created. Acknowledgements: Wewould like 
to thankPaul Calamia for his manyhelpful sugges­tions. Thisworkwas supportedinpartbyAROcontractsDAAD19­02-1-0390 
and W911NF-04-1-0088, NSFawards 0400134, 0429583 and 0404088,DARPA/RDECOM contract N61339-04-C-0043, 
and Intel. 7. REFERENCES [1] J. B. Allen and D. A. Berkley, Image method for ef.ciently simulating small-room 
acoustics, TheJournalofthe Acoustical Society of America, vol. 65, no. 4, pp. 943 950, April 1979. [2] 
T. Funkhouser, I. Carlbom, G. Elko, G. Pingali, M. Sondhi, and J. West, A beam tracing approach to acoustic 
modeling for interactive virtual environments, in SIGGRAPH 98: Proceed­ings of the 25th annual conference 
on Computer graphics and interactive techniques, NewYork, NY, USA, 1998, pp. 21 32, ACM.  [3] C. Lauterbach, 
A. Chandak, and D. Manocha, Interactive sound propagation in dynamic scenes using frustum tracing, IEEE 
Transactions on Visualization and Computer Graphics, vol. 13, no. 6, pp. 1672 1679, 2007. [4] M. Hodgson, 
Evidence of diffuse surface re.ections in rooms, TheJournalofthe Acoustical Societyof America,vol. 89, 
no. 2, pp. 765 771, February 1991. [5] N. Tsingos,T.Funkhouser,A.Ngan,andI. Carlbom, Model­ing acoustics 
in virtual environments using the uniform theory of diffraction, in SIGGRAPH 2001, Computer Graphics 
Pro­ceedings, 2001, pp. 545 552. [6] U. P. Svensson, R. I. Fred, and J. Vanderkooy, An analytic secondary 
source model of edge diffraction impulse responses , Acoustical Society of AmericaJournal, vol. 106, 
pp. 2331 2344, Nov. 1999. [7]V. Pulkki,T. Lokki,andL.Savioja, Implementationand visu­alization of edge 
diffraction with image-source method, in The 112thAudio Engineering Society (AES) Convention, 2002, pp. 
preprint no. 5603+. [8] P.T. Calamia and U.P. Svensson, Edge subdivision forfast diffraction calculations, 
in Proc. 2005 IEEEWorkshop on Ap­plicationsof SignalProcessingtoAudioand Acoustics(WAS-PAA 2005), October 
2005, pp. 187 190. [9] R.Kouyoumjian andP.H.Pathak, Auniform geometrical the­oryofdiffractionforanedgeina 
perfectly conductingsurface, IEEE, Proceedings, vol. 62, Nov. 1974, p. 1448-1461., 1974. [10] N. Tsingos, 
I. Carlbom, G. Elbo,R.Kubli, andT. Funkhouser, Validation of acoustical simulations in the "bell labs 
box", IEEE Computer Graphics and Applications, vol. 22, no. 4, pp. 28 37, June 2002. [11] S. Van Duyne 
and J. O. Smith, The 2-d digital waveguide mesh, in Applications of Signal Processing to Audio and Acoustics, 
1993.Final Program andPaper Summaries., 1993 IEEEWorkshop on, 1993, pp. 177 180. [12] D. Botteldooren, 
 Acoustical .nite-difference time-domain simulationina quasi-cartesian grid, TheJournalof the Acous­tical 
Society of America, vol. 95, no. 5, pp. 2313 2319, 1994. [13] N. Raghuvanshi, N. Galoppo, and M. C. Lin, 
 Accelerated wave-based acoustics simulation, in Symposium on Solid and Physical Modeling, 2008, pp. 
91 102. [14] J. Borish, Extension of the image model to arbitrary polyhe­dra, TheJournalof the Acoustical 
Societyof America, vol. 75, pp. 1827 1836, June 1984. [15] A. Krokstad, S. Strom, and S. Sørsdal, Calculating 
the acous­tical room response by the use of a ray tracing technique, in Journalof Sound andVibration, 
1968,vol.8, pp. 118 125. [16] M.A. Biot andI.Tolstoy, Formulationofwave propagation in in.nite media 
by normal coordinates with an application to diffraction, Journal of the Acoustical Society of America, 
vol. 29, no. 3, pp. 381 391, March 1957. [17] H. Medwin, Shadowingby.nite noise barriers, Journal of 
the Acoustical Society of America, vol. 69, no. 4, pp. 1060 1064, April 1981. [18] P. T. Calamia and 
U. P. Svensson, Fast time-domain edge­diffraction calculations for interactive acoustic simulations, 
EURASIPJ. Appl. Signal Process., vol. 2007, no. 1, pp. 186 186, 2007. [19] F. Antonacci, M.Foco, A. Sarti, 
and S.Tubaro, Fast model­ing of acoustic re.ections and diffraction in complex environ­ments using visibility 
diagrams, in Proceedings of 12th Euro­pean Signal Processing Conference (EUSIPCO 04),September 2004, 
pp. 1773 1776. [20] A. Chandak, C. Lauterbach, M. Taylor, Z. Ren, and D. Manocha, Ad-frustum: Adaptive 
frustum tracing for inter­active sound propagation, IEEETransactions onVisualization and Computer Graphics,vol. 
14, no. 6, pp. 1707 1722, Decem­ber 2008.  [21] M.Taylor,A. Chandak,L. Antani, andD. Manocha, Resound: 
Interactive sound rendering for dynamic virtual environments, Tech. Rep., University of Chapel Hill, 
2009. [22] A. Chandak, L. Antani, M.Taylor, and D. Manocha, Fastv: From-point visibility culling on 
complex models, Tech. Rep., University of Chapel Hill, 2009. [23] M. Taylor and D. Manocha, Fast accurate 
diffraction paths with frustum tracing, Tech. Rep., University of Chapel Hill, 2009. 6  Proc. of the 
EAA Symposium on Auralization, Espoo, Finland, 15-17 June 2009 EFFICIENT NUMERICAL ACOUSTIC SIMULATION 
ON GRAPHICS PROCESSORS USING ADAPTIVE RECTANGULAR DECOMPOSITION Nikunj Raghuvanshi,* Brandon Lloyd, 
Dept. of Computer Science, UNC Chapel Hill, USA Microsoft Corporation Naga K. Govindaraju, Ming C.Lin,§ 
Microsoft Corporation Dept. of Computer Science, UNC Chapel Hill, USA ABSTRACT Accurate acoustic simulation 
can enable realistic auralization that leads to enhanced immersion for visual applications, as well as 
facilitates accurate predictions for practical room acoustic scenar­ios. Numerical simulation provides 
realistic impulse responses that properly account for interference and diffraction effects by modeling 
the physics of wave propagation. However, it has posed a tough computational challenge owing to its large 
computation and memory requirements. We present a technique which relies on an adaptive rectangular decomposition 
of 3D scenes that yields two key advantages: Firstly, its key computational routine is DCT which can 
be ef.ciently parallelized on Graphics Processors. Sec­ondly, the analytical solution of the Wave Equation 
in rectangular domains is known, which can be exploited to gain in accuracy and perform simulations on 
coarse simulation meshes, reducing both the computation and memory requirements further. Our technique 
is able to achieve a gain of at least a hundred-fold in computation and ten-fold in memory compared to 
a standard Finite Difference Time Domain (FDTD) implementation with comparable accuracy. 1. INTRODUCTION 
 Ef.cient and accurate acoustic simulation can be used to calcu­late realistic impulse responses (IRs) 
without the need of capturing them on a real scene. In many cases, like virtual acoustics or au­ditorium 
design, the scene might not even exist in reality and such accurate predictions can add to the realism 
or predictive auraliza­tions, depending on the application. Numerical Acoustic simula­tion is a promising 
way to obtain IRs for arbitrary scenes while taking into account complex wave-based acoustic phenomena 
like interference and diffraction that are required for realistic auraliza­tion. The problem of acoustic 
simulation is challenging because au­dible sounds have wavelengths that falls in the range of the dimen­sions 
of common objects, and consequently sound diffracts appre­ciably around most objects, especially at frequencies 
up to around 1 kHz. Capturing diffraction has been a tough challenge for Geo­metric acoustic techniques. 
In addition, the speed of sound is small enough that a complete transient simulation needs to be performed 
in time-domain for practical auralization requirements. Computa­tional acoustics thus has its own unique 
challenges. * nikunjr@gmail.com dalloyd@microsoft.com nagag@microsoft.com § lin@cs.unc.edu  Since 
numerical approaches attempt to integrate the underly­ing Linear Wave Equation directly, they are capable 
of performing a full transient solution which correctly accounts for all wave phe­nomena, including diffraction, 
elegantly in one framework. This capability is their most attractive feature. But this accuracy comes 
at a very high computational cost and large memory requirements for practical scenes especially so at 
medium to high frequencies above a few hundred Hz. The reason is that typically numerical simulation 
running time scales as the fourth power and memory requirement as the third power of the maximum simulated 
fre­quency. Naively reducing the grid resolution leads to unacceptably large numerical errors. Main Results: 
Our technique is at least ten-fold ef.cient in mem­ory and hundred-fold ef.cient in computation compared 
to a stan­dard FDTD implementation and achieves competitive accuracy at the same time. It relies on an 
adaptive rectangular decomposition of the free space of the scene. This approach has two key advan­tages: 
1. The solution to the Wave equation in a Rectangular domain can be be performed ef.ciently through a 
Discrete Cosine Transform (DCT) that can be calculated ef.ciently through FFT. We have integrated our 
simulator with an implemen­tation of FFT on Graphics Processing Units (GPU) [1], to gain considerable 
speedups over CPU-based libraries. 2. The analytical solution to the wave equation within a rect­angular 
domain is known. This enables highly reduced nu­merical dispersion errors, even on grids approaching 
the Nyquist limit with only two to four spatial samples per wave­length (compared to about ten samples 
per wavelength re­quired by other numerical techniques like FDTD). This leads to signi.cant gains in 
both computation and memory re­quirements.  For practical auralizations, it is required to numerically 
model par­tially absorbing surfaces as well as fully absorbing surfaces cor­responding to open space, 
such as open doors and windows. We have implemented the Perfectly Matched Layer (PML) Absorbing Boundary 
Condition to handle such cases. We demonstrate our al­gorithm and resulting auralization on practical 
scenarios with high complexity and validate the results against FDTD. Owing to the computational ef.ciency 
of our approach, to the best of our knowl­edge, our work is the .rst to show accurate time-domain numerical 
acoustic simulation on such large, complex 3D scenes for all fre­quencies up to 1kHz on a high-end desktop 
computer. Organization: The rest of the paper is organized as follows: In section 2, we review related 
work in the .eld. Section 3 presents 1  Proc. of the EAA Symposium on Auralization, Espoo, Finland, 
15-17 June 2009 some mathematical background, which motivates our approach de­scribed in Section 4. We 
discuss results obtained with our tech­nique in Section 5. 2. PREVIOUS WORK The ever-increasing computational 
power of desktop comput­ers has enabled more and more accurate acoustic simulation and auralization on 
realistic scenes. The fundamental equation govern­ing wave propagation for most practical cases is the 
Linear Wave Equation. Depending on how wave propagation is approximated, techniques for simulating acoustics 
may be broadly classi.ed into Geometrical Acoustics (GA) and Numerical Acoustics (NA). We brie.y survey 
work in both these areas. Geometric Acoustics: GA approaches are derived using asymp­totic approximations 
of the Wave Equation in the in.nite frequency limit. Their basic assumption is that sound propagates 
in linear paths, like light. The .rst GA approaches to be investigated were the Image Source Method and 
Ray Tracing [2, 3]. Most room acoustics software use a combination of these techniques to this day [4]. 
Another ef.cient geometric approach proposed in liter­ ature is Beam Tracing [5, 6] or alternatively, 
Frustum Tracing [7] that assume continuous bundles of sound rays, instead of in.nitely­thin rays. There 
has been work on Phonon Tracing [8] that assumes linearly-propagating packets of energy, called Phonons. 
Also, re­searchers have proposed applying hierarchical radiosity to acous­tical energy transfer [9, 10]. 
Since all GA approaches inherently lack diffraction, it has to be accounted for explicitly. Most of such 
approaches rely on the Geometric Theory of Diffraction [11, 12, 13]. Diffraction remains a challenge 
for GA approaches and is an active area of research. Numerical Acoustics: Numerical approaches differ 
in how they discretize and approximate the Wave Equation. Based on this, they may be classi.ed into: 
Finite Element Method (FEM), Boundary Element Method (BEM), Digital Waveguide Mesh (DWM), Fi­nite Difference 
Time Domain (FDTD) and Functional Transform Method (FTM). In the following, we brie.y review each of 
these methods in turn. FEM and BEM have traditionally been employed mainly for the steady-state frequency 
domain response, as opposed to a full time-domain solution, with FEM applied mainly to interior and BEM 
to exterior scattering problems [14]. DWM approaches [15], on the other hand, use discrete waveguide 
elements forming a 3D cartesian mesh. Each waveguide is assumed to carry waves along its length in a 
single dimension. [16, 17, 18]. The FDTD method, owing to its simplicity and versatility, has been an 
active area of research in room acoustics for more than a decade [19]. FDTD works on a uniform grid and 
solves for the .eld values at each cell over time. Initial investigations into FDTD were hampered by 
the lack of computational power and memory, limiting its application to mostly small scenes in 2D for 
low fre­quencies. It is only recently that the possibility of applying FDTD to medium sized scenes in 
3D has been explored [20, 21]. Even then, the compute and memory requirements for FDTD are beyond the 
capability of most desktop systems today [21], requiring days of computation on a small cluster for medium-sized 
3D scenes for simulating frequencies up to 1 kHz. The Functional Transform Method (FTM) [22] is closely 
re­ lated to our technique. Although the mathematical frameworks of the two techniques are very different 
there are some similarities, like using a rectangular decomposition although the mathematical motivation 
is different. Our technique has the advantage of being very simple to formulate and works directly on 
the second order Wave Equation without casting it as a .rst order system which leads to gains in ef.ciency. 
Also, our technique requires just one mathematical transform, the DCT. 3. MATHEMATICAL BACKGROUND We 
brie.y discuss the mathematical background for our technique, as well as some details of the FDTD technique 
we compare against. 3.1. The Wave Equation The input to an acoustics simulator is a scene in 3D, along 
with the boundary conditions and the locations of the sound source. The re­sult is the entire sound .eld 
on a volumetric grid over time, which can be used to compute the impulse response at all possible listener 
locations in the scene. The propagation of sound in a domain is governed by the Lin­ear Acoustic Wave 
Equation, .2 p 2.2 - cp = F (x,t) , (1).t2 Sound is represented as a time-varying spatial pressure .eld 
p (x,t). The speed of sound is kept as c = 340ms -1 and F (x,t) is the forcing term that models sound 
sources present in the scene. The 222 operator .2 = .+ .+ .is the Laplacian in 3D. .x2 .y2 .z2 3.2. 
Reference FDTD Scheme FDTD works on a uniform axis-aligned grid with spatial spacing h. To capture the 
propagation of a prescribed maximum frequency .max c .max, the Nyquist theorem requires that h = 2 = 
. 2.max The Laplacian operator is then replaced with a discrete approxi­mation of sixth order accuracy 
with the following numerical dif­ferentiation formula: d2 pi 1 (2pi-3 - 27pi-2 + 270pi-1 - 490pi dx2 
180h2 +270pi+1 - 27pi+2 +2pi+3)+ O(h6), (2) where pi is the ith grid cell in the corresponding dimension. 
The above expression can be compactly expressed in terms of the Dis­crete Laplacian Matrix, K and vectors 
P and F containing the pressures and source terms respectively, for all grid cells. Dis­cretize Eqn. 
(1) in space as above and then in time at some time­ step .t, which is restricted by the CFL (Courant-Friedrich-Levy) 
h condition .t< v . Using the second-order Leapfrog integrator c 3 in time, the complete update rule 
is as follows: ..2 P n+1 - P n-1 c.t 26 =2P n + KP n + O .t + Oh. h Controlling numerical errors in 
FDTD typically requires not 2 (Nyquist) but 8-10 samples per wavelength [23]. This makes the method about 
54 = 625 times slower than if it had been running on a Nyquist mesh. The numerical errors lead to Numerical 
Dis­persion Waves with different frequenceis travel with different speeds. Reducing cell size increases 
accuracy, but is prohibitive as reducing the cell size by r times results in an increase in memory requirement 
by a factor of r 3 and total compute time by r 4 . 2  Proc. of the EAA Symposium on Auralization, Espoo, 
Finland, 15-17 June 2009 4. TECHNIQUE In this section, we brie.y describe our technique to perform acous­tic 
simulation on arbitrary domains. 4.1. Wave Equation and DCT Consider a rectangular space in 3D, with 
solid diagonal extending from (0, 0, 0) to (lx,ly,lz), with perfectly re.ective walls. Any pressure .eld 
p (x, y, z, t) in this space can be represented as p (x, y, z, t)= mi (t)Fi (x, y, z) , (3) i=(ix,iy,iz) 
 where mi are the time-varying mode coef.cients and Fi are the eigenfunctions of the Laplacian for a 
rectangular domain, given by pix piy piz Fi (x, y, z) = cos x cos y cos z. lx ly lz In the discrete 
interpretation, Eqn. (3) is simply an inverse Dis­ crete Cosine Transform (iDCT) in 3D, with Fi being 
the Cosine basis vectors for the given lengths. Therefore, we may ef.ciently transform from mode coef.cient 
vector (M) to pressure vector (P ) as P (t)= iDCT (M (t)) . (4) The DCT and iDCT operations can be performed 
in T(n log n) time and T(n) memory using the Fast Fourier Transform algo­rithm [24]. Re-interpreting 
Eqn. (1) in a discrete-space setting, substituting P from the expression above and re-arranging, we get, 
Figure 1: Numerical errors due to interface handling and PML absorber.  found in [1]. The GPU consists 
of a large number of scalar, in­ order processors that can execute the same program in parallel us­ing 
threads. Scalar processors are grouped together into multipro­cessors. The GPU memory hierarchy is designed 
for high band­width to the global memory that is visible to all multiprocessors. The shared memory has 
low latency and is organized into several banks to provide higher bandwidth. At a high-level, computation 
on the GPU proceeds as follows: The user allocates memory on the GPU, copies data to the GPU, speci.es 
a GPU program that exe­cutes on the multiprocessors and after execution, copies the data back to the 
host. The forward Discrete Fourier Transforms (DFT) of a real se­quence x = x0,...,xN-1 is an N-point 
complex sequence that is conjugate symmetric, X = X0,...,XN-1, where N-1-2pijk/N * Xk = xj e ,XN-1-i 
= Xi . (8) j=0 The inverse DFT is de.ned as 2 . mi + c 2ki 2 mi = iDCT (F (t)) , .t2 N-1 (5) 1 . 2 
2 i 2 . ii ki 2 = p2 2pijk/N . (9) y 2 2 xz xyz l + + xk = Xj e 2 ll N In the absence of any forcing 
term, the above equation describes a set of independent simple harmonic oscillators, with each one vi­brating 
with its own characteristic frequency, .i = cki. Assuming that F (t) is constant over a time-step .t, 
it may be transformed to mode-space as F.(t) = DCT (F (t)) (6) and one may derive the following update 
rule Mn+1 2F.n ii i =2Mn cos (.i.t) - Mn-1 + .2 (1 - cos (.i.t)) . i (7) This update rule is obtained 
by using the closed form solution of a simple harmonic oscillator over a time-step. Thus, the complete 
pressure .eld in a rectangular space can be updated in time by successive applications of Eqns. (6), 
(7) and (4), in that order, while incurring no numerical error except that inherent in DCT and errors 
in F . 4.2. Computing DCT on Graphics Processors (GPUs) We now provide a brief summary of GPU-based DCT 
and its in­tegration with our simulator. The details of this technique can be j=0 FFT algorithms compute 
the DFT in O(N log N ) operations. Us­ing symmetries in the FFT of real data, the operation count can 
be reduced by half. A detailed overview of FFT algorithms can found in Van Loan [24]. The DCT can be 
computed using the FFT. The elements of a sequence are .rst permuted and then each element is scaled 
by an appropriate factor. Suppose that the length of the sequence is N = 2p. The sequence is permuted 
by placing the p even elements .rst in the new sequence before the odd elements, which appear in re­ 
-2pik/(4N) verse order. Each element xk is multiplied by Re(e )= cos(-2pik/(4N)). The resulting sequence 
is transformed using the real FFT, yielding the DCT. The inverse DCT is computed us­ing a similar process. 
See Van Loan [24] for details. For 3D DCTs as required in the discussion above, we perform the permutations 
simultaneously for all three dimensions. We now describe the integration of our simulator with GPU-DCT. 
The simulator spawns two threads, one for the CPU and an­other for GPU processing. At each time-step, 
for each partition, the following operations are performed. In the GPU thread, the forcing coef.cients 
are sent to the GPU and DCT invoked, as in Eqn. (6). Mode update performed as in Eqn. (7), and an inverse 
DCT performed as in Eqn. (4) to recover the pressure .eld in the partition. Meanwhile, the pressure .eld 
in each absorber partition is updated on the CPU thread in parallel. The details of absorber 3  Proc. 
of the EAA Symposium on Auralization, Espoo, Finland, 15-17 June 2009 Figure 2: Numerical dispersion 
error in our method is comparable to that of the reference, while FDTD on the same mesh exhibits large 
error. partitions will be discussed shortly. This serves to hide the cost of partially re.ecting surfaces 
in the cost of propagation in the inte­rior, using the GPU as an effective co-processor. After this, 
both threads are synchronized and interface handling performed on the CPU, which takes negligible time 
compared to the GPU process­ing. 4.3. Rectangular Decomposition and Interface Handling Typical acoustic 
spaces are not rectangular, but can be always be partitioned into a set of rectangles touching each other. 
We per­form this rectangular decomposition by .rst voxelizing the scene with a cell size that is chosen 
based on the maximum frequency to be simulated. Rectangles are .t using a randomized greedy heuris­tic 
which tries to grow the largest rectangle from the current seed point by successively increasing the 
length in each dimension by one. Once this can t be done further, the rectangle is stored, an­other random 
seed chosen and the process repeated until the free space is exhausted. Acoustic simulation within each 
rectangular partition can be carried out as described above. However, interface operations need to be 
performed between partitions to propagate sound between them. The interface operators are based on a 
Finite Difference approximation. Assume two rectangular partitions share an inter­face with normal along 
the X-axis. Recall the discussion of FDTD in Section 3.2. Assuming, that cells i and i +1 are in different 
partitions and thus lie on a partition-partition interface. Using the the sixth-order Finite Difference 
stencil in Eqn. (2) the following interface operator may be derived -2pi-2 + 27pi-1 - 270pi + 270pi+1 
- 27pi+2 +2pi+3 S. = i 180h2 (10) This .nite difference operator is accounted for in the forcing term, 
thus yielding, 2. Fi = cSi. (11) Intuitively, the sound between two partitions is communicated us­ing 
point sources on their shared interface. The numerical errors introduced due to interface handling will 
be discussed in detail shortly. Absorbing Surfaces: To model partially re.ecting surfaces, we have used 
the Perfectly Matched Layer (PML) absorber [25]. PML Figure 3: Numerical acoustic simulation and auralization 
on the Sibenik Cathedral. The dimensions of this cathedral are 35m × 26m × 15m with a volume of 13,650 
m 3 . The images at the bottom show an impulse propagating in the scene over time.  applies a thin absorber 
on the surface patch of interest and models a highly absorptive Wave Equation in the interior. The interfacing 
operator between the PML medium and a partition in our method is identical to that discussed above. Variable 
re.ectivity is obtained by multiplying the forcing term calculated for interface handling by a number 
between 0 and 1, 0 corresponding to full re.ectivity and 1 corresponding to full absorption. Further 
details of this technique may be found in [26, 27]. 5. RESULTS AND ANALYSIS 5.1. Numerical Errors Figure 
1 shows the interface error for a simple scene, which appear as .ctitious re.ections at the interface. 
Although the interface er­rors increase with increasing frequency, they stay near -40dB for most of the 
spectrum. The .gure also shows the absorption errors for the PML absorber, which ranges from -20 to -30dB, 
causing very slight deviations from the actual re.ectivity of the material being modeled. Since we employ 
a rectangular decomposition to approximate the simulation domain, their are stair-casing errors near 
the bound­ary (see Figure 4). The size of the stair-casing is necessarily be­ low the smallest simulated 
wavelength causing only small errors. In the worst case, stair-casing makes the sound-.eld more diffuse 
than it should be. If very high boundary accuracy is critical, that can be achieved by coupling our approach 
with a .ne-mesh simu­lation near the boundary. 5.2. Auralization The input to all audio simulations we 
perform is a Gaussian deriva­tive impulse with desired bandwidth (typically 1kHz). Auralizing the sound 
at a moving listener location is performed as follows. A simulation is run from the source location, 
yielding the pressure 4  Proc. of the EAA Symposium on Auralization, Espoo, Finland, 15-17 June 2009 
  affects the reverberation time, as expected. Figure 5: Diffraction of sound around edges. Our method 
repro­ duces the frequency domain low-passing effect of an edge. signal at all cell centers. We then 
compute the IRs at all cells lying close to the listener path by performing a deconvolution by divi­ 
the same as the ideal response. FDTD running on the same mesh sion in frequency domain. Next, the sound 
at the current position undergoes large dispersion errors, while FDTD running on a 2.5x and time is estimated 
by linearly interpolating the sample values re.ned mesh (the reference) gives reasonably good results. 
Our obtained by convolving the source signal with the IRs at the two method achieves competitive accuracy 
with the reference while nearest cell centers. consuming 12 times less memory and 90 times less computation. 
Most of the simulations we have performed are band-limited to 1-2kHz due to computation and memory constraints. 
However, 5.4. House Scene this limitation can be partially removed for the purpose of aural­ization using 
a simple technique that we describe next. We .rst To illustrate that diffraction and the associated gradual 
variation up-sample the IR obtained from the simulation to 44kHz by 0-in intensity around an edge is 
actually observed, we modeled a padding in frequency domain and run a simple peak detector on the House 
scene, shown in Figure 5. Please see the supplementary resulting IR. The peak detector works by searching 
for local max-video for this auralization. Initially the listener is at the upper-right ima/minima and 
thus .nds out signi.cant re.ection/diffraction peaks corner and the sound source at the lower right corner 
of the scene. in the IR and their times. This IR is similar to that obtained with The source is placed 
such that initially, diffraction is the dominant GA approaches and is used for frequencies above the 
maximum energy path from the source to the listener. As the listener walks simulated frequency. The approximation 
introduced in this oper-and reaches the door of the living room, the sound intensity grows ation is that 
the diffraction at higher frequencies is approximated gradually. The dimensions of the House are 17m 
× 15m × 5m, since the peak detector doesn t differentiate between re.ection and with 8.8 million simulation 
cells. The wall re.ectivity was set to diffraction peaks. This means that high frequencies may also diffract 
50% and the simulation grid supported frequencies up to 4kHz. like low frequencies. The acoustic response 
was computed for .4 seconds. The total The reference solution for comparing our solution is the FDTD 
simulation time on this scene for the reference is about 3.5 days method described in Section 3.2 running 
on a .ne mesh that en-and about 24 minutes with our technique. The simulation takes sures 10 samples 
per wavelength (FDTD 2.5x). All the simula-about 700 MB of memory with our technique and nearly 8 GB 
for tions were performed on a 2.8GHz Intel Xeon CPU, with 8GB of the reference. RAM. The GPU used was 
an NVIDIA GeForce GTX 280. To validate the diffraction accuracy of our simulator, we placed the source 
and listener as shown in Figure 5, such that the domi­ 5.3. Numerical Dispersion: Anechoic Corridor 
nant path from the source to the listener is only around the diffract-We .rst demonstrate the reduced 
numerical dispersion in our scheme. ing edge of the door. The middle of the .gure shows a compari-Refer 
to Figure 2. The scene is a 20m × 5m × 5m corridor with son of the frequency response for the .rst arriving 
peak at the lis­ 6.5 million simulation cells in which the source and listener are tener location, between 
the reference and our solution. The two located 15m apart, as shown in the .gure. The simulation was 
responses agree in their trend but there s a slight discrepancy at band-limited to 4kHz, and the IR was 
calculated at the listener and higher frequencies. A possible explanation is that there are two only 
the direct sound part of the impulse response was retained. As partition interfaces right at the diffraction 
edge and the correspond-Figure 2 shows, our method s impulse response is almost exactly ing interface 
errors result in the observed difference. 5  Proc. of the EAA Symposium on Auralization, Espoo, Finland, 
15-17 June 2009 5.5. Cathedral Scene As our largest benchmark, we ran our sound simulator on a the Sibenik 
cathedral scene (shown in Figure 3) of size 35m × 26m × 15m, with 11.9 million simulation cells. The 
simulation was car­ried out till 1kHz on a mesh which supported up to 2kHz for a duration of 2 seconds. 
We could not run the reference solution for this benchmark because it would take approximately 25GB of 
memory, which is not available on a desktop systems today, with a projected 2 weeks of computation for 
this same scene. With our technique it the simulation took about 58 minutes, consuming less than 1 GB 
of memory. An important point to note is that we are able to compute even the Late Reverberation phase 
easily because numerical techniques are insensitive to the order of re.ection. Figure 4 shows the rectangular 
decomposition of this scene. The bottom of the .gure shows the impulse response of the two simulations 
with low and high absorptivity in dB against time. Note how in both cases, the Late Reverberation .eld 
decays ex­ponentially with time, as expected physically. 6. CONCLUSION AND FUTURE WORK We have presented 
a computation and memory-ef.cient technique for performing accurate numerical acoustic simulations on 
com­plex domains enabling auralization containing both diffraction as well as accurate Late Reverberation. 
We are actively looking into the integration of stereo sound in our framework, since numerical simulation 
does not provide directionality information explicitly. Also, we would like to add support for moving 
sound sources. An­other direction this work may be extended is to combine it with a geometric technique 
for performing the high-frequency part of the simulation, while our technique simulates frequencies up 
to 1-2 kHz. 7. REFERENCES [1] Naga K. Govindaraju, Brandon Lloyd, Yuri Dotsenko, Burton Smith, and John 
Manferdelli, High performance discrete fourier trans­forms on graphics processors, in SC 08: Proceedings 
of the 2008 ACM/IEEE conference on Supercomputing, Piscataway, NJ, USA, 2008, pp. 1 12, IEEE Press. [2] 
U.R. Krockstadt, Calculating the acoustical room response by the use of a ray tracing technique, Journal 
of Sound Vibration, 1968. [3] J. B. Allen and D. A. Berkley, Image method for ef.ciently simu­lating 
small-room acoustics, J. Acoust. Soc. Am, vol. 65, no. 4, pp. 943 950, 1979. [4] J. H. Rindel, The use 
of computer modeling in room acoustics, Journal of Vibroengineering, vol. 3, no. 4, pp. 219 224, 2000. 
[5] Thomas Funkhouser, Nicolas Tsingos, Ingrid Carlbom, Gary Elko, Mohan Sondhi, James E. West, Gopal 
Pingali, Patrick Min, and Addy Ngan, A beam tracing method for interactive architectural acous­tics, 
The Journal of the Acoustical Society of America, vol. 115, no. 2, pp. 739 756, 2004. [6] F. Antonacci, 
M. Foco, A. Sarti, and S. Tubaro, Real time modeling of acoustic propagation in complex environments, 
Proceedings of 7th International Conference on Digital Audio Effects, pp. 274 279, 2004. [7] Anish Chandak, 
Christian Lauterbach, Micah Taylor, Zhimin Ren, and Dinesh Manocha, Ad-frustum: Adaptive frustum tracing 
for interactive sound propagation, IEEE Transactions on Visualization and Computer Graphics, vol. 14, 
no. 6, pp. 1707 1722, 2008. [8] M. Bertram, E. Deines, J. Mohring, J. Jegorovs, and H. Hagen, Phonon 
tracing for auralization and visualization of sound, in IEEE Visualization 2005, 2005. [9] Nicolas Tsingos, 
Simulating High Quality Dynamic Virtual Sound Fields For Interactive Graphics Applications, Ph.D. thesis, 
Univer­site Joseph Fourier Grenoble I, December 1998. [10] Murray Hodgson and Eva M. Nosal, Experimental 
evaluation of radiosity for room sound-.eld prediction, The Journal of the Acous­tical Society of America, 
vol. 120, no. 2, pp. 808 819, 2006. [11] Nicolas Tsingos, Thomas Funkhouser, Addy Ngan, , and Ingrid 
Carl­bom, Modeling acoustics in virtual environments using the uniform theory of diffraction, in Computer 
Graphics (SIGGRAPH 2001), August 2001. [12] Paul T. Calamia and Peter U. Svensson, Fast time-domain edge-diffraction 
calculations for interactive acoustic simulations, EURASIP Journal on Advances in Signal Processing, 
vol. 2007, 2007. [13] Micah Taylor, Anish Chandak, Zhimin Ren, Christian Lauterbach, and Dinesh Manocha, 
Interactive edge diffraction for sound propa­gation in complex virtual environments, Tech. Rep., Department 
of Computer Science, UNC Chapel Hill, 2008. [14] Mendel Kleiner, Bengt-Inge Dalenbäck, and Peter Svensson, 
Aural­ization -an overview, JAES, vol. 41, pp. 861 875, 1993. [15] S. Van Duyne and J. O. Smith, The 
2-d digital waveguide mesh, in Applications of Signal Processing to Audio and Acoustics, 1993. Fi­nal 
Program and Paper Summaries., 1993 IEEE Workshop on, 1993, pp. 177 180. [16] Matti Karjalainen and Cumhur 
Erkut, Digital waveguides ver­sus .nite difference structures: equivalence and mixed modeling, EURASIP 
J. Appl. Signal Process., vol. 2004, no. 1, pp. 978 989, January 2004. [17] L. Savioja, Modeling Techniques 
for Virtual Acoustics, Doctoral thesis, Helsinki University of Technology, Telecommunications Soft­ware 
and Multimedia Laboratory, Report TML-A3, 1999. [18] D. Murphy, A. Kelloniemi, J. Mullen, and S. Shelley, 
Acoustic mod­eling using the digital waveguide mesh, Signal Processing Maga­zine, IEEE, vol. 24, no. 
2, pp. 55 66, 2007. [19] D. Botteldooren, Finite-difference time-domain simulation of low­frequency room 
acoustic problems, Acoustical Society of America Journal, vol. 98, pp. 3302 3308, December 1995. [20] 
Shinichi Sakamoto, Takuma Seimiya, and Hideki Tachibana, Vi­sualization of sound re.ection and diffraction 
using .nite difference time domain method, Acoustical Science and Technology, vol. 23, no. 1, pp. 34 
39, 2002. [21] S. Sakamoto, T. Yokota, and H. Tachibana, Numerical sound .eld analysis in halls using 
the .nite difference time domain method, in RADS 2004, Awaji, Japan, 2004. [22] R. Rabenstein, S. Petrausch, 
A. Sarti, G. De Sanctis, C. Erkut, and M. Karjalainen, Block-based physical modeling for digital sound 
synthesis, Signal Processing Magazine, IEEE, vol. 24, no. 2, pp. 42 54, 2007.  [23] Allen Ta.ove and 
Susan C. Hagness, Computational Electrody­namics: The Finite-Difference Time-Domain Method, Third Edition, 
Artech House Publishers, June 2005. [24] Charles Van Loan, Computational Frameworks for the Fast Fourier 
Transform, Society for Industrial Mathematics, 1992. [25] Y. S. Rickard, N. K. Georgieva, and Wei-Ping 
Huang, Application and optimization of pml abc for the 3-d wave equation in the time domain, Antennas 
and Propagation, IEEE Transactions on, vol. 51, no. 2, pp. 286 295, 2003. [26] Nikunj Raghuvanshi, Nico 
Galoppo, and Ming C. Lin, Accelerated wave-based acoustics simulation, in SPM 08: Proceedings of the 
2008 ACM symposium on Solid and physical modeling, New York, NY, USA, 2008, pp. 91 102, ACM. [27] Nikunj 
Raghuvanshi, Rahul Narain, and Ming C. Lin, Ef.cient and accurate sound propagation using adaptive rectangular 
decomposi­tion, IEEE Transactions on Visualization and Computer Graphics, vol. 99, no. 1, 2009. 6  
&#38;#169;2009 IEEE. Pre-print, with permission from IEEE Transactions on Visualization and Computer 
Graphics, to appear Ef.cient and Accurate Sound Propagation Using Adaptive Rectangular Decomposition 
 Nikunj Raghuvanshi, Rahul Narain and Ming C. Lin Department of Computer Science, UNC Chapel Hill, USA 
Abstract Accurate sound rendering can add signi.cant realism to complement visual display in interactive 
applications, as well as facilitate acoustic predictions for many engineering applications, like accurate 
acoustic analysis for architectural design [27]. Nu­merical simulation can provide this realism most 
naturally by modeling the underlying physics of wave propagation. However, wave simulation has traditionally 
posed a tough computational challenge. In this paper, we present a technique which relies on an adaptive 
rectangular decomposition of 3D scenes to enable ef.cient and accurate simulation of sound propagation 
in complex virtual environ­ments. It exploits the known analytical solution of the Wave Equation in rectangular 
domains, and utilizes ef.cient implementation of Discrete Cosine Transform on the GPU to achieve at least 
a hundred-fold performance gain compared to a standard Finite Difference Time Domain (FDTD) implementation 
with comparable accuracy, while also being an order of magnitude more memory-ef.cient. Consequently, 
we are able to perform accurate numerical acoustic simulation on large, complex scenes in the kilohertz 
range. To the best of our knowledge, it was not previously possible to perform such simulations on a 
desktop computer. Our work thus enables acoustic analysis on large scenes and auditory display for complex 
virtual environments on commodity hardware. 1 INTRODUCTION Sound rendering, or auditory display, was 
.rst introduced to computer graphics more than a decade ago by Takala and Hahn [44], who in­vestigated 
the integration of sound with interactive graphics applica­tions. Their work was motivated by the observation 
that accurate au­ditory display can augment graphical rendering and enhance human­computer interaction. 
There have been studies showing that such sys­tems provide the user with an enhanced spatial sense of 
presence [14]. Auditory display typically consists of two main components: sound synthesis that deals 
with how sound is produced [6, 13, 29, 33, 49] and sound propagation that is concerned with how sound 
travels in a scene. In this paper we address the problem of sound propagation, also referred to as computational 
acoustics. The input to an acoustic simulator is the geometry of the scene, along with the re.ective 
properties of different parts of the boundary and the locations of the sound sources and listener. The 
goal is to auralize predict the sound the listener would hear. Computational acoustics has a very diverse 
range of applications, from noise control and underwater acoustics [21] to architectural acoustics and 
acoustics for virtual environments (VEs) and games. Although each applica­tion has its own unique requirements 
from the simulation technique, all applications require physical accuracy. For noise control, accuracy 
translates directly into the loudness of the perceived noise, for archi­tectural acoustics, accuracy 
has implications on predicting how much an orchestra theater enhances (or degrades) the quality of music. 
For interactive applications like VEs and games, physical accuracy directly affects the perceived realism 
and immersion of the scene. This is be­cause we are used to observing many physical wave effects in reality 
and their presence in the scene helps to convince us that the computer­generated environment is indeed 
real. For example, we observe every day that when a sound source is occluded from the listener, the sound 
becomes muf.ed in reality. For light, the source would become in­visible, casting a shadow, which doesn 
t happen for sound because it Fig. 1. Sound simulation on a Cathedral. The dimensions of this scene bends, 
or diffracts, around the occluder. In fact, this is one of the ma-are 35m × 15m × 26m. We are able to 
perform numerical sound simula­jor reasons that sound compliments sight, both in reality and in virtual 
tion on this complex scene on a desktop computer and pre-compute environments it conveys information 
in places where light cannot. a 1 second long impulse response in about 29 minutes, taking less Our simulation 
technique naturally captures these subtle phenomena than 1 GB of memory. A commonly used approach that 
we compare occurring in nature. against, Finite Difference Time Domain (FDTD), would take 1 week of For 
most acoustic simulation techniques, the process of auralization computation and 25GB of memory for this 
scene to achieve competitive accuracy. The auralization, or sound rendering at runtime consists of can 
be further broken down into roughly two parts: (a) pre-processing; convolution of the calculated impulse 
responses with arbitrary source signals, that can be computed ef.ciently.  &#38;#169;2009 IEEE. Pre-print, 
with permission from IEEE Transactions on Visualization and Computer Graphics, to appear and (b) (acoustic) 
sound rendering. During pre-processing, an acous­tic simulator does computations on the environment to 
estimate its acoustical properties, which facilitate fast rendering of the sound at runtime. The exact 
pre-computation depends on the speci.c approach being used. For our approach, the pre-processing consists 
of running a simulation from the source location, which yields the impulse re­sponses at all points in 
the scene in one simulation. The rendering at runtime can then be performed by convolving the source 
signal with the calculated impulse response at the listener s location, which is a very fast operation 
as it can be performed through an FFT. The main focus of this paper is on the pre-processing phase of 
acoustic predic­tion. We present a novel and fast numerical approach that enables ef.cient and accurate 
acoustic simulations on large scenes on a desk­top system in minutes, which would have otherwise taken 
many days of computation on a small cluster. An example is shown in Figure 1. The problem of acoustic 
simulation is challenging mainly due to some speci.c properties of sound. The wavelength of audible sound 
falls exactly in the range of the dimensions of common objects, in the order of a few centimeters to 
a few meters. Consequently, unlike light, sound bends (diffracts) appreciably around most objects, especially 
at lower frequencies. This means that unlike light, sound doesn t ex­hibit any abrupt shadow edges. As 
discussed earlier, from a perceptual point of view, capturing correct sound diffraction is critical. 
In addi­tion, the speed of sound is small enough that the temporal sequence of multiple sound re.ections 
in a room is easily perceptible and dis­tinguishable by humans. As a result, a steady state solution, 
like in light simulation, is insuf.cient for sound a full transient solution is required. For example, 
speech intelligibility is greatly reduced in a room with very high wall re.ectivity, since all the echoes 
mix into the direct sound with varying delays. Therefore, the combination of low speed and large wavelength 
makes sound simulation a computational problem with its own unique challenges. Numerical approaches for 
sound propagation attempt to directly solve the Acoustic Wave Equa­tion, which governs all linear sound 
propagation phenomena, and are thus capable of performing a full transient solution which correctly ac­counts 
for all wave phenomena, including diffraction, elegantly in one framework. Since we use a numerical approach, 
our implementation inherits all these advantages. This is also the chief bene.t our method offers over 
geometric techniques, which we will discuss in detail in the next section. Applicability: Most interactive 
applications today, such as games, use reverb .lters (or equivalently, impulse responses) that are not 
physically-based and roughly correspond to acoustical spaces with dif­ferent sizes. In reality, the acoustics 
of a space exhibits perceptibly large variations depending on the wall material, room size and geom­etry, 
along with many other factors [24]. A handful of reverb .lters common to all scenes cannot possibly capture 
all the different acous­tical effects which we routinely observe in real life and thus, such a method 
at best provides a crude approximation of the actual acoustics of the scene. Moreover, an artist has 
to assign these reverb .lters to different parts of the environment manually, which requires a consid­erable 
amount of time and effort. One way to obtain realistic .lters would be to do actual measure­ments on 
a scene. Not only is it dif.cult and time-consuming for real scenes, but for virtual environments and 
games, one would need to physically construct scale physical prototypes which would be pro­hibitively 
expensive. This is even more impractical considering that most games today encourage users to author 
their own scenes. Nu­merical approaches offer a cheap and effective alternative to alleviate all of these 
problems by computing the .lters at different points in the scene directly from simulation and are thus 
capable of at once au­tomating the procedure, as well as providing much more realistic and immersive 
acoustics which account for all perceptually-important au­ditory effects, including diffraction. However, 
this realism comes at a very high computational cost and large memory requirements. In this paper, we 
offer a highly accelerated numerical technique that works on a desktop system and can be used to pre-compute 
high-quality reverb .lters for arbitrary scenes without any human intervention. These .l­ters can then 
be employed as-is in interactive applications for real-time auralization. For example, given that the 
artist speci.es a few salient locations where the acoustics must be captured, one just needs to store 
the reverb .lters obtained from simulation at those locations. Current game engines already use techniques 
to associate reverb .lters with physical locations [2]. Our technique would provide the actual values 
in the reverb .lters, the audio pipeline need not change at all. The artist would thus be relieved from 
the burden of .guring out and ex­perimenting exactly what kind of reverb captures the acoustics of the 
particular space he/she has modeled. Another advantage of our ap­proach is that since we are solving 
for the complete sound .eld in a scene, a sound designer can visualize how the sound propagates in the 
scene over time, to help him/her make guided decisions about what changes need to be made to the scene 
to counter any perceived acous­tic de.ciencies. Please refer to the accompanying video for examples of 
such visualizations. Main Results: Our technique takes at least an order of magnitude less memory and 
two orders of magnitude less computation compared to a standard numerical implementation, while achieving 
competitive accuracy at the same time. It relies on an adaptive rectangular de­composition of the free 
space of the scene. This approach has many advantages: 1. The analytical solution to the wave equation 
within a rectangular domain is known. This enables high numerical accuracy, even on grids approaching 
the Nyquist limit, that are much coarser than those required by most numerical techniques. Exploiting 
these analytical solutions is one of the key reasons for the signi.cant reduction in compute and memory 
requirements. 2. Owing to the rectangular shape of the domain partitions, the so­lution in their interior 
can be expressed in terms of the Discrete Cosine Transform (DCT). It is well-known that DCTs can be ef.ciently 
calculated through an FFT. We use a fast implemen­tation of FFT on the GPU [17], that effectively maps 
the FFT to the highly parallel architecture of the GPU to gain considerable speedups over CPU-based libraries. 
This implementation drasti­cally reduces the computation time for our overall approach. 3. The rectangular 
decomposition can be seamlessly coupled with other simulation techniques running in different parts of 
the sim­ulation domain.  We have also implemented the Perfectly Matched Layer (PML) Ab­sorber to model 
partially absorbing surfaces, as well as open scenes. We demonstrate our algorithm on several scenarios 
with high com­plexity and validate the results against FDTD, a standard Finite Dif­ference technique. 
We show that our approach is able to achieve the same level of accuracy with at least two orders of magnitude 
reduc­tion in computation time and an order of magnitude less memory re­quirements. Consequently, we 
are able to perform accurate numerical acoustic simulation on large scenes in the kilohertz range which, 
to the best of our knowledge, have not been previously possible on a desktop computer. Organization: 
The rest of the paper is organized as follows. In Section 2, we review related work in the .eld. Section 
3 presents the mathematical background, which motivates our approach described in Section 4. We show 
and discuss our results in Section 5. 2 PREVIOUS WORK Since its inception [36], computational acoustics 
has been a very active area of research due to its widespread practical applications. Depending on how 
wave propagation is approximated, techniques for simulating acoustics may be broadly classi.ed into Geometric 
Acous­tics (GA) and Numerical Acoustics (NA). For a general introduction to room acoustics, the reader 
may refer to [21, 24] or a more current survey [26]. Geometric Acoustics: All GA approaches are based 
on the basic assumption of rectilinear propagation of sound waves, just like light.  &#38;#169;2009 
IEEE. Pre-print, with permission from IEEE Transactions on Visualization and Computer Graphics, to appear 
Historically, the .rst GA approaches that were investigated were Ray Tracing and the Image Source Method 
[3, 23]. Most room acoustics software use a combination of these techniques to this day [35]. An­other 
ef.cient geometric approach that has been proposed in litera­ture, with emphasis on interactive graphics 
applications, is Beam Trac­ing [4, 16]. On the lines of Photon Mapping, there has been work on Phonon 
Tracing [5, 12] in acoustics. Also, researchers have proposed applying hierarchical radiosity to acoustical 
energy transfer [18, 46]. All GA approaches assume that sound propagates rectilinearly in rays, which 
results in unphysical sharp shadows and some techniques must be applied to ameliorate the resulting artifacts 
and include diffraction into the simulation, especially at lower frequencies. Most of such ap­proaches 
rely on the Geometrical Theory of Diffraction [48] and more recently, the Biot-Tolstoy-Medwin model of 
diffraction [10] which re­sult in improved simulations. However, accurately capturing diffrac­tion still 
remains a challenge for GA approaches and is an active area of research. In the context of interactive 
systems, most acoustic tech­niques explored to date are based on GA, simply because although numerical 
approaches typically achieve better quality results, the com­putational demands were out of the reach 
of most systems. Numerical Acoustics: Numerical approaches, in contrast to GA, solve the Wave Equation 
numerically to obtain the exact behavior of wave propagation in a domain. Based on how the spatial discretization 
is performed, numerical approaches for acoustics may be roughly clas­si.ed into: Finite Element Method 
(FEM), Boundary Element Method (BEM), Digital Waveguide Mesh (DWM), Finite Difference Time Do­main (FDTD) 
and Functional Transform Method (FTM). In the fol­lowing, we brie.y review each of these methods in turn. 
FEM and BEM have traditionally been employed mainly for the steady-state frequency domain response, as 
opposed to a full time­domain solution, with FEM applied mainly to interior and BEM to exterior scattering 
problems [22]. FEM and BEM approaches are gen­eral methods applicable to any Partial Differential Equation, 
the Wave Equation being one of them. DWM approaches [50], on the other hand, use discrete waveguide elements, 
each of which is assumed to carry waves along its length along a single dimension [20,28,40]. The FDTD 
method, owing to its simplicity and versatility, has been an active area of research in room acoustics 
for more than a decade [7, 8]. Originally proposed for electromagnetic simulations [41], FDTD works on 
a uniform grid and solves for the .eld values at each cell over time. Initial investigations into FDTD 
were ham­pered by the lack of computational power and memory, limiting its application to mostly small 
scenes in 2D. It is only recently that the possibility of applying FDTD to medium sized scenes in 3D 
has been explored [37 39]. Even then, the computational and memory require­ments for FDTD are beyond 
the capability of most desktop systems to­day [39], requiring days of computation on a small cluster 
for medium­sized 3D scenes for simulating frequencies up to 1 kHz. Another aspect of our work is that 
we divide the domain into many partitions. Such approaches, called Domain Decomposition Methods (DDM) 
have widespread applications in all areas where numerical so­lution to partial differential equations 
is required and it would be hard to list all areas of numerical simulation where they have been applied. 
For a brief history of DDM and its applications we refer the reader to the survey [11]. For an in-depth 
discussion of DDM, the reader is re­ferred to the books [31,45]. Also, the website [1] has many references 
to current work in the area. It is interesting to note that the main mo­tivation of DDM when it was conceptualized 
more than a century ago was to divide the domain into simpler partitions which could be ana­lyzed more 
easily [11], much like in our work. However, in nearly all Domain Decomposition approaches today, speci.cally 
for wave prop­agation, the principal goal is to divide and parallelize the workload across multiple processors. 
Therefore, the chief requirement in such cases is that the partitions be of equal size and have minimal 
interface area, since that corresponds to balancing the computation and mini­mizing the communication 
cost. The motivation of our approach for partitioning the domain is dif­ferent we want to ensure that 
the partitions have a particular rect­angular shape even if that implies partitions with highly varying 
sizes since it yields many algorithmic improvements in terms of computa­tion and numerical accuracy for 
simulation within the partitions. Our approach leads to improvements even in sequential performance by 
exploiting the analytical solution within a rectangular domain. In con­trast to prior work in high-performance 
computing, parallelization is not the driving priority in our work. Decomposing the domain into partitions 
and performing interface handling between them are very well-known techniques and by themselves are not 
the main contribu­tion of this work. Of course, it is still possible to parallelize our ap­proach by 
allocating the partitions to different cores or machines, and doing interface handling between them, 
and would be the way to scale our approach to very large scenes with billions of cells. Another method 
which is related to our work, although in a dif­ferent mathematical framework, is the Functional Transform 
Method (FTM) [30,32]. Our technique has the advantage of being very simple to formulate and works directly 
on the second order Wave Equation, instead of casting it as a .rst order system as in the FTM and just 
requires one mathematical transformation, the Discrete Cosine Trans­form. Also, we demonstrate our results 
on general scenes in 3D, along with detailed error analysis. Spectral techniques are a class of very 
high order numerical schemes in which the complete .eld is expressed in terms of global basis functions. 
Typically, the basis set is chosen to be the Fourier or Chebyshev polynomials [9] as fast, memory ef.cient 
transforms are available to transform to these bases from physical space and vice versa. Our method may 
also be regarded as a spectral method. How­ever, there are some important differences which we discuss 
later in the paper. It is interesting to note here that GA and NA approaches may be regarded as complimentary 
with regard to the range of frequencies they can simulate ef.ciently With geometric approaches it is 
hard to simulate low-frequency wave phenomena like diffraction because they assume that sound travels 
in straight lines like light, while with numerical approaches, simulating high frequencies above a few 
kilo­hertz becomes prohibitive due to the excessively .ne volume mesh that must be created. We wish to 
emphasize at this point that it is possible to integrate more elaborate techniques for modeling the surface 
properties and scattering [47] characteristics of the scene boundary into our tech­nique. Also, we assume 
all sound sources to be monopole, or point source, but complex emission patterns resulting from many 
monopole and dipole sources [19] can also be easily integrated in our framework. 3 MATHEMATICAL BACKGROUND 
In this section, we .rst brie.y present the FDTD method. We do this for two reasons: Firstly, this is 
the simulator we use as a reference to compare against and its details serve to illustrate the underlying 
math­ematical framework used throughout this paper. Secondly, this discus­sion illustrates numerical 
dispersion errors in FDTD and motivates our technique which uses the analytical solution to the Wave 
Equation on rectangular domains to remove numerical dispersion errors. 3.1 Basic Formulation The input 
to an acoustics simulator is a scene in 3D, along with the boundary conditions and the locations of the 
sound sources and lis­tener. The propagation of sound in a domain is governed by the Acous­tic Wave Equation, 
. 2 p 2.2 p = F (x,t), (1).t2 - c This equation captures the complete wave nature of sound, which is 
treated as a time-varying pressure .eld p(x,t) in space. The speed of sound is c = 340ms-1 and F (x,t) 
is the forcing term corresponding to .2 sound sources present in the scene. The operator .2 = .x2 + ..y22 
+ .2 . z2 is the Laplacian in 3D. The Wave Equation succinctly explains wave phenomena such as interference 
and diffraction that are observed in reality. We brie.y mention a few physical quantities and their rela­tions, 
which will be used throughout the paper. For a wave traveling in  &#38;#169;2009 IEEE. Pre-print, with 
permission from IEEE Transactions on Visualization and Computer Graphics, to appear free space, the frequency, 
. and wavelength, . are related by c = .. . It is also common to use the angular counterparts of these 
quantities: angular frequency, . = 2p. and wavenumber, k = 2.p . Because fre­quency and wavenumber are 
directly proportional to each other, we will be using the two terms interchangeably throughout the paper. 
In the next sub-section, we brie.y discuss the Finite Difference Time Domain (FDTD) method for numerically 
solving the Wave Equation. To avoid confusion, we note here that while the term FDTD is sometimes used 
to speci.cally refer to the original algorithm pro­posed by Yee [51] for Electromagnetic simulation, 
it is common to re­fer to any Finite Difference-based approach which computes the com­plete sound .eld 
in time domain as an FDTD method. In this paper, we use the latter de.nition. 3.2 A (2,6) FDTD Scheme 
FDTD works on a uniform grid with spacing h. To capture the propa­gation of a prescribed maximum frequency 
.max, the Nyquist theorem c requires that h = .max = . Once the spatial discretization is per­ 22.max 
formed, the continuum Laplacian operator is replaced with a discrete approximation of desired order of 
accuracy. Throughout this paper, we consider the sixth order accurate approximation to the Laplacian, 
which approximates the second order differential in each dimension with the following stencil: d2 pi 
1 (2pi-3 - 27pi-2 + 270pi-1 - 490pi dx2 180h2 +270pi+1 - 27pi+2 + 2pi+3)+ O(h6), (2) where pi is the 
ith grid cell in the corresponding dimension. Thus, the Laplacian operator at each cell can be represented 
as a Discrete Laplacian Matrix, K and equation (1) becomes, . 2Pc2 - KP = F (t) , (3) .t2 h2 where P 
is a long vector listing the pressure values at all the grid cells and F is the forcing term at each 
cell. Hard-walls may be modeled with the Neumann Boundary Condition . p = 0, where n is the normal to 
. n the boundary. The next step is to discretize equation (3) in time at some time-step h .t, which 
is restricted by the CFL condition .t < c v 3 . Using the Leapfrog integrator in time, the complete update 
rule is as follows. 2 .. .. c.t Pn+1 h6 = 2Pn - Pn-1 + KPn + O .t2 + O . h Since the temporal and spatial 
errors are second and sixth order re­spectively, this is a (2,6) FDTD scheme. In the next sub-section, 
we discuss the nature of the numerical errors in FDTD schemes and the resulting performance issues. 3.3 
Numerical Dispersion in FDTD and Ef.ciency Consid­erations As was previously discussed, the spatial cell 
size, h for FDTD is cho­sen depending on the maximum simulated frequency, .max and is lim­ited by the 
Nyquist sampling theorem. However, due to numerical errors arising from spatial and temporal discretization, 
accurate simu­lation with FDTD typically requires not 2 but 8-10 samples per wave­length [43]. These 
errors manifest themselves in the form of Numer­ical Dispersion Waves with different wavenumbers (or 
equivalently, different frequencies) do not travel with the same speed in the sim­ulation. This error 
may be quanti.ed by .nding the wavenumber­dependent numerical speed, c. (k), where k is the wavenumber. 
This speed is then normalized by dividing with the ideal wave speed, c, yielding the dispersion coef.cient, 
. (k). Ideally, the dispersion coef.­cient should be as close to 1 as possible, for all wavenumbers. 
Figure 2 shows a plot of the dispersion coef.cient for FDTD against frequency on a 3D grid and compares 
the error for different cell sizes. Observe Fig. 2. Numerical dispersion with a (2,6) FDTD scheme for 
different mesh resolutions. Increasing the sampling reduces the numerical dis­persion errors. Our method 
suffers from no dispersion errors in the inte­rior of rectangular partitions, while FDTD accumulates 
errors over each cell a signal propagates across. Reducing these errors with FDTD re­quires a very .ne 
grid. that at 1000 Hz, the dispersion coef.cient for FDTD is about .01c, while for FDTD running on a 
2.5× re.ned mesh the error is about .001c. This is because the spatial sampling increases from 4 samples 
per wavelength to 10 samples per wavelength. Consider a short-time signal containing many frequencies, 
for ex­ample, a spoken consonant. Due to Numerical Dispersion, each of the frequencies in the consonant 
will travel with a slightly different speed. As soon as the phase relations between different frequencies 
are lost, the signal is effectively destroyed and the result is a muf.ed sound. From the above values 
of the dispersion coef.cient, it can be shown that with FDTD a signal would have lost phase coherence 
after travel­ing just 17m, which is comparable to the diameter of most scenes. To increase accuracy, 
we need to increase the mesh resolution, but that greatly increases the compute and memory requirements 
of FDTD Re.ning the mesh r times implies an increase on memory by a factor 4 of r3 and the total compute 
time for a given interval of time by r. In practice, memory can be a much tighter constraint because 
if the method runs out of main memory, it will effectively fail to produce any results. 3.4 Wave Equation 
on a Rectangular Domain A lot of work has been done in the .eld of Spectral/Pseudo-spectral methods [25] 
to allow for accurate simulations with 2-4 samples per wavelength while still allowing for accurate simulations. 
Such meth­ods typically represent the whole .eld in terms of global basis func­tions, as opposed to local 
basis functions used in Finite Difference or Finite Element methods. With a suitable choice of the spectral 
ba­sis (typically Chebyshev polynomials), the differentiation represented by the Laplacian operator can 
be approximated to a very high degree of accuracy, leading to very accurate simulations. However, spectral 
methods still use discrete integration in time which introduces tempo­ral numerical errors. In this paper, 
we use a different approach and instead exploit the well-known analytical solution to the Wave Equa­tion 
on rectangular domains [24], which enables error-free propaga­tion within the domain. It is important 
to note here that we are able to do this because we assume that the speed of sound is constant in the 
medium, which is a reasonable assumption for architectural acoustics and virtual environments. Consider 
a rectangular domain in 3D, with its solid diagonal ex­ tending from the (0,0,0) to lx,ly,lz , with 
perfectly rigid, re.ective walls. It can be shown that any pressure .eld p(x,y, z,t) in this domain may 
be represented as p (x,y,z,t)= . mi (t)Fi (x,y,z) , (4) i=(ix,iy,iz)  &#38;#169;2009 IEEE. Pre-print, 
with permission from IEEE Transactions on Visualization and Computer Graphics, to appear where mi are 
the time-varying mode coef.cients and Fi are the eigen­functions of the Laplacian for a rectangular domain, 
given by pix piy piz Fi (x, y, z)= cosxcosycosz. lx ly lz Given that we want to simulate signals band-limited 
up to a prescribed smallest wavelength, the above continuum relation may be interpreted on a discrete 
uniform grid with the highest wavenumber eigenfunc­tions being spatially sampled at the Nyquist rate. 
Note that as long as the simulated signal is properly band-limited and all the modes are used in the 
calculation, this discretization introduces no numerical er­rors. This is the reason it becomes possible 
to have very coarse grids with only 2-4 samples per wavelength and still do accurate wave prop­agation 
simulations. In the discrete interpretation, equation (4) is sim­ply an inverse Discrete Cosine Transform 
(iDCT) in 3D, with Fi being the Cosine basis vectors for the given dimensions. Therefore, we may ef.ciently 
transform from mode coef.cients (M) to pressure values (P) as P(t)= iDCT (M (t)). (5) This is the main 
advantage of choosing a rectangular shape because the eigenfunctions of a rectangle are Cosines, the 
transformation ma­trix corresponds to applying the DCT, which can be performed in T (nlogn) time and 
T (n) memory using the Fast Fourier Transform algorithm [15], where n is the number of cells in the rectangle, 
which is proportional to its volume. For general shapes, we would get arbi­ a­ trary basis functions, 
and these requirements would increase to T n2in compute and memory, which quickly becomes prohibitive 
for large scenes, with n ranging in millions. Re-interpreting equation (1) in a discrete-space setting, 
substituting P from the expression above and re-arranging, we get, . 2Mi .t2 + c2ki 2 Mi = DCT (F (t)), 
i2 i2 i2 . (6) y xz k2 = p2++ il2 l2 l2 xyz In the absence of any forcing term, the above equation describes 
a set of independent simple harmonic oscillators, with each one vibrating with its own characteristic 
frequency, .i = cki. The above analysis may be equivalently regarded as Modal Analysis applied to a rectan­gular 
domain. However, our overall approach is different from Modal Analysis because the latter is typically 
applied to a domain as a whole, yielding arbitrary basis functions which do not yield to ef.cient trans­forms, 
and extracting all the modes is typically intractable for domains with millions of cells. We model arbitrary 
forcing functions, for example, due to a volume sound sources as follows. Assuming that the forcing function, 
F (t) is constant over a time-step .t, it may be transformed to mode-space as F (t) = DCT (F (t)) (7) 
 and one may derive the following update rule 2F Mn+1 i = 2Min cos(.i.t) - Min-1 + .F2 n (1 - cos(.i.t)). 
(8) i This update rule is obtained by using the closed form solution of a simple harmonic oscillator 
over a time-step. Since it is a second-order equation, we need to specify one more initial condition, 
which we choose to be that the function computed over the time-step evaluates correctly to the value 
at the previous time-step, Mn-1. This leads to a time-stepping scheme which has no numerical errors for 
propaga­tion in the interior of the rectangle, since we are directly using the closed-form solution for 
a simple harmonic oscillator. The only error introduced is in assuming that the forcing term is constant 
over a time­step. This is not a problem for input source sounds, as the time-step is necessarily below 
the sampling rate of the input signal. However, the communication of sound between two rectangular domains 
is en­sured through forcing terms on their interface and this approximation introduces numerical errors 
at the interface. We discuss these issues in detail in the next section. Fig. 3. Overview of our approach. 
The scene is .rst voxelized at a prescribed cell size depending on the highest simulated frequency. A 
rectangular decomposition is then performed and impulse response cal­culations then carried out on the 
resulting partitions. Each step is domi­nated by DCT and inverse DCT calculations withing partitions, 
followed by interface handling to communicate sound between partitions. 4 TECHNIQUE In the previous section, 
we discussed the errors and ef.ciency issues of the FDTD method and discussed a method to carry out numerical 
solution of the Wave Equation accurately and ef.ciently on rectan­gular domains. In this section, we 
discuss our technique which ex­ploits these observations to perform acoustic simulation on arbitrary 
domains by decomposing them into rectangular partitions. We end with a discussion of the numerical errors 
in our approach. 4.1 Rectangular Decomposition Most scenes of interest for the purpose of acoustic simulation 
nec­essarily have large empty spaces in their interior. Consider a large scene like, for example, a 30m 
high cathedral in which an impulse is triggered near the .oor. With FDTD, this impulse would travel up­wards 
and would accumulate numerical dispersion error at each cell it crosses. Given that the spatial step 
size is comparable to the wave­length of the impulse, which is typically a few centimeters, the impulse 
accumulates a lot of error, crossing hundreds to thousands of cells. In the previous section, we discussed 
that wave propagation on a rectan­gular domain can be performed very ef.ciently while introducing no 
numerical errors. If we .t a rectangle in the scene extending from the bottom to the top, the impulse 
would have no propagation error. This is the chief motivation for Rectangular Decomposition Since there 
are large empty spaces in typical scenes, a decomposition of the space into rectangular partitions would 
yield many partitions with large volume and exact propagation could be performed in the interior of each. 
We perform the rectangular decomposition by .rst voxelizing the scene. The cell size is chosen based 
on the maximum frequency to be simulated, as discussed previously. Next, the rectangular decom­position 
is performed using a greedy heuristic, which tries to .nd the largest rectangle it can grow from a random 
seed cell until all free cells are exhausted. We note here that the correctness of our technique does 
not depend on the optimality of the rectangular decomposition. A slightly sub-optimal partitioning with 
larger interface area affects the performance only slightly, as the interface area is roughly propor­tional 
to the surface area of the domain, while the runtime performance is dominated by the cost of DCT, which 
is performed on input propor­tional to the volume of the domain. 4.2 Interface Handling Once the domain 
of interest has been decomposed into many rectan­gles, propagation simulation can be carried out inside 
each rectangle as described in Section 3.4. However, since every rectangle is assumed  &#38;#169;2009 
IEEE. Pre-print, with permission from IEEE Transactions on Visualization and Computer Graphics, to appear 
to have perfectly re.ecting walls, sound will not propagate across rect­angles. We next discuss how this 
communication is performed using a Finite Difference approximation. Without loss of generality, lets 
as­sume that two rectangular partitions share an interface with normal along the X-axis. Recall the discussion 
of FDTD in Section 3.2. As­sume for the moment that (2,6) FDTD is running in each rectangular pi partition, 
using the stencil given in equation (2) to evaluate d2 . Fur­ dx2 ther, assume that cell i and i + 1 
are in different partitions and thus lie on their interface. As mentioned previously, Neumann boundary 
condition implies even symmetry of the pressure .eld about the in­terface and each partition is processed 
with this assumption. Thus, the Finite Difference stencil may also be thought of as a sum of two parts 
 The .rst part assumes that the pressure .eld has even symmetry about the interface, namely, pi = pi+1, 
pi-1 = pi+2 and pi-2 = pi+3, and this enforces Neumann boundary conditions. The residual part of the 
stencil accounts for deviations from this symmetry, cause by the pressure in the neighboring partition. 
Symbolically, representing the Finite Difference stencil in equation (2) as S Si = Si 0 + Si. , where 
S01 = i 180h2 (2pi-3 - 25pi-2 + 243pi-1 - 220pi) S. 1 i = 180h2 (-2pi-2 + 27pi-1 - 270pi + 270pi+1 
- 27pi+2 + 2pi+3) . Since Si . is a residual term not accounted for while evaluating the LHS of equation 
(3), it is transferred to the RHS and suitably accounted for in the forcing term, thus yielding, 2S. 
Fi = ci. (9) Similar relations for the forcing term may be derived for all cells near the partition 
boundary which index cells in neighboring parti­tions. If we were actually using (2,6) FDTD in each partition, 
this forcing term would be exact, with the same numerical errors due to spatial and temporal approximations 
appearing in the interior as well as the interface. However, because we are using an exact solution in 
the interior, the interface handling described above introduces numeri­cal errors equivalent to a (2,6) 
FDTD on the interface. We will discuss these errors in more detail shortly. We would like to note here 
that a sixth-order scheme was chosen as it gives suf.ciently low interface errors, while being reasonably 
ef.cient. Lower (second/fourth) order schemes would be more ef.cient and much easier to implement, but 
as we have experimented, they result in much higher errors, which results in undesirable, audible high 
frequency noise. One may use an even higher order scheme if more accuracy is required for a par­ticular 
application, at the expense of computation and implementaion effort. An interesting point to note at 
this point is that the interface handling doesn t need to know how the .eld inside each partition is 
being updated. Therefore, it is easy to mix different techniques for wave propagation in different parts 
of the domain, if so required. 4.3 Absorbing Boundary Condition Our discussion till this point has assumed 
that all scene boundaries are perfectly re.ecting. For modeling real scenes, this is an unreal­istic 
assumption. Moreover, since the computation is carried out on a volumetric grid, it is necessary to truncate 
the domain and model emission into free space. It is necessary to have absorbing boundaries for this 
purpose. For this work, we have implemented the Perfectly Matched Layer (PML) absorber [34], which is 
commonly employed in most numerical wave propagation simulations due to its high ab­sorption ef.ciency. 
PML works by applying an absorbing layer which uses coordinate stretching to model wave propagation in 
an unphysical medium with very high absorption, while ensuring that the impedance of the medium matches 
that of air at the interface to avoid re.ection errors. The interfacing between the PML medium and a 
partition in our method is simple to implement Since PML explicitly maintains a pressure .eld in the 
absorbing medium, the PML medium can also be treated as a partition and the same technique described 
above can be applied for the coupling between PML and other partitions. Vari­able re.ectivity can be 
easily obtained by multiplying the forcing term Fig. 4. Measurements of numerical error due to interface 
handling and PML absorbing boundary conditions. The interface handling er­rors stays near -40 dB for 
most of the frequency spectrum, which is not perceptible. The absorption error stays around -25 dB which 
introduces very small errors in the re.ectivity of different materials. calculated for interface handling 
by a number between 0 and 1, 0 cor­responding to full re.ectivity and 1 corresponding to full absorption. 
4.4 Putting everything together In this subsection, we give a step-by-step description of all the steps 
involved in our technique. Figure 3 shows a schematic diagram of the different steps in our approach, 
which are as follows 1. Pre-processing (a) Voxelize the scene. The cell-size is .xed by the minimum simulated 
wavelength and the required number of spatial samples per wavelength (typically 2-4) (b) Perform a rectangular 
decomposition on the resulting vox­elization, as described in Section 4.1. (c) Perform any necessary 
pre-computation for the DCTs to be performed at runtime. Compute all interfaces and the partitions that 
share them.  2. Simulation Loop (a) Update modes within each partition using equation (8) (b) Transform 
all modes to pressure values by applying an iDCT as given in equation (5) (c) Compute and accumulate 
forcing terms for each cell. For cells on the interface, use equation (9), and for cells with point sources, 
use the sample value. (d) Transform forcing terms back to modal space using a DCT as given in equation 
(7).  4.5 Numerical Errors Numerical errors in our method are introduced mainly through two sources 
 boundary approximation and interface errors. Since we em­ploy a rectangular decomposition to approximate 
the simulation do­main, there are stair-casing errors near the boundary (see Figure 7). These stair-casing 
errors are identical to those in FDTD because we do a rectangular decomposition of a uniform grid there 
is no additional geometry-approximation error due to using rectangular partitions. In most room acoustic 
software, it is common practice to approximate the geometry to varying degrees [42]. The reason for doing 
this is that we are not as sensitive to acoustic detail as much as we are to visual detail. Geometric 
features comparable or smaller than the wavelength of sound ( 34 cm at 1kHz) lead to very small variations 
in the overall  &#38;#169;2009 IEEE. Pre-print, with permission from IEEE Transactions on Visualization 
and Computer Graphics, to appear acoustics of the scene due to the presence of diffraction. In contrast, 
in light simulation, all geometric details are visible because of the ultra­small wavelength of light 
and thus stair-casing is a much more impor­tant problem. The net effect of stair-casing error for numerical 
simulators is that for frequencies with wavelengths comparable to the cell size ( 1kHz), the walls act 
as diffuse instead of specular re.ectors. For frequencies with large wavelengths (500 Hz and below), 
the roughness of the sur­face is effectively invisible to the wave, and the boundary errors are small 
with near-specular re.ections. Therefore, the perceptual impact of boundary approximation is lesser in 
acoustic simulation. However, if very high boundary accuracy is critical for a certain scene, this can 
be achieved by coupling our approach with a high­resolution grid near the boundary, running FDTD at a 
smaller time­step. As we had mentioned earlier, as long as the pressure values in neighboring cells are 
available, it is easy to couple the simulation in the rectangular partitions with another simulator running 
in some other part of the domain. Of course, this would create extra computational overhead, so its an 
ef.ciency-accuracy tradeoff. As we discussed theoretically in Section 3.4 and also demonstrate with experiments 
in the next section, our technique is able to nearly eliminate numerical dispersion errors. However, 
because the inter­partition interface handling is based on a less accurate (2,6) FDTD scheme, the coupling 
is not perfect, which leads to erroneous re.ec­tions at the interface. Figure 4 shows the interface error 
for a simple scene. The Nyquist frequency on the mesh is 2000Hz. The table at the bottom of the .gure 
shows the interface re.ection errors for different frequencies, in terms of sound intensity. Although 
the interface errors increase with increasing frequency, it stays ~-40dB for most of the spectrum. Roughly, 
that is the difference in sound intensity between normal conversation and a large orchestra. Since most 
scenes of practical interest have large empty spaces in their interior, the number of partition interfaces 
encountered by a wave traveling the diameter of the scene is quite low. For example, refer to Figure 
7 a wave traveling the 20 m distance from the source location to the dome at the top encounters only 
about 10 interfaces. Also, it is important to note that this is a worst-case scenario for our approach, 
since many rectangles are needed to .t the curved dome at the top. This is the chief advantage of our 
approach numerical dispersion is removed for traveling this distance and it is traded off for very small 
re.ection errors which are imperceptible. Please hear the accompa­nying video for examples of audio rendered 
on complex scenes with numerous interfaces. Figure 4 also shows the absorption errors for the PML Absorbing 
Boundary Condition. The absorption errors range from -20 to -30dB, which works well for most scenes, 
since this only causes a slight devi­ation from the actual re.ectivity of the material being modeled. 
How­ever, if higher accuracy absorption is required, one might increase the PML thickness. We have used 
a 5-cell thick PML in all our simula­tions. 5 RESULTS 5.1 Sound Rendering The input to all audio simulations 
we perform is a Gaussian-derivative impulse of unit amplitude. Given the maximum frequency to be sim­ulated, 
.max, we .x the width of the impulse so that its maxima in fre­quency domain is at .max , giving a broadband 
impulse in the frequency 2 range of interest. This impulse is triggered at the source location and simulation 
performed until the pressure .eld has dropped off to about -40 dB, which is roughly the numerical error 
of the simulation. The resulting signal is recorded at the listener position(s). Next, decon­volution 
is performed using a simple Fourier coef.cient division to obtain the Impulse Response (IR), which is 
used for sound rendering at a given location. Auralizing the sound at a moving listener location is performed 
as follows. First, note that running one simulation from a source location yields the pressure variation 
at all cell centers because we are solv­ing for the complete .eld on a volume grid. For auralizing sound, 
we .rst compute the IRs at all cells lying close to the listener path. Next, Fig. 5. Numerical results 
on the corridor scene, comparing numerical dispersion errors in FDTD and in our method. The reference 
FDTD so­lution has a mesh with s = 10 samples per wavelength. Note that only the magnitudes of the Fourier 
coef.cients are plotted. Our method suf­fers from very little numerical dispersion, reproducing the ideal 
impulse response very closely, while FDTD suffers from large amounts for nu­merical dispersion. We take 
an order of magnitude less memory and nearly two orders of magnitude less computation time to produce 
re­sults with accuracy comparable to the reference solution. the sound at the current position and time 
is estimated by linearly in­terpolating the .eld values at neighboring cell centers. To obtain the .eld 
value at a given cell center, a convolution of the IR at the corre­sponding location and the input sound 
is performed. We would like to emphasize here that there are more ef.cient ways of implementing the auralization 
but that is not the focus of this paper. Most of the simulations we have performed are band-limited to 
1­2kHz due to computation and memory constraints. However, this is not a big limitation. Although audible 
sounds go up to 22kHz, it is important to realize that only frequencies up to about 5kHz are percep­tually 
critical [24] for acoustics simulation. Moreover, the frequency perception of humans is logarithmic, 
which re.ects in the frequency doubling between musical octaves. This means that most of the per­ceptually 
important frequencies are contained till about 2kHz. For example, the frequency of a typical 88-key piano 
goes from about 30Hz to 4kHz, covering 7 octaves, out of which 6 octaves are be­low 2kHz. However, even 
though we don t have accurate perception of higher frequencies, their complete absence leads to perceptual 
arti­facts and therefore, there must be some way of accounting for higher frequencies, even if approximately. 
One way of doing that would be to combine our technique with a Geometrical Acoustic simulator for the 
higher frequency range. In this paper, we have used a much simpler technique that gives good results 
in practice. To auralize sounds in the full audible range up to 22kHz, we .rst up­sample the IR obtained 
from the simulation to 44kHz and run a sim­ple peak detector on the IR which works by searching for local 
max­ima/minima. The resulting IR contains peaks with varying amplitudes and delays, corresponding to 
incoming impulses. This is exactly the kind of IR that geometrical approaches compute by tracing paths 
for sound and computing the attenuation and delay along different paths. Each path yields a contribution 
to the IR. The difference here is that numerical simulation does not explicitly trace these paths. Instead, 
we extract this information from the computed impulse response through peak detection. We use an IR thus 
computed for higher frequencies.  &#38;#169;2009 IEEE. Pre-print, with permission from IEEE Transactions 
on Visualization and Computer Graphics, to appear Fig. 6. The House scene demonstrates diffraction of 
sound around obstacles. All the scene geometry shown was included in the simulation. Our method is able 
to reproduce the higher diffraction intensity of sound at lower frequencies, while reducing the memory 
requirements by about an order of magnitude and the computational requirements by more than two orders 
of magnitude. The reference solution is computed on a mesh with s = 10 samples per wavelength. The approximation 
introduced in this operation is that the diffraction at higher frequencies is approximated since the 
peak detector doesn t differentiate between re.ection and diffraction peaks. Intuitively, this means 
that high frequencies may also diffract like low frequencies, which is the approximation introduced by 
this technique. This IR .l­ter is then high-passed at the simulation cutoff frequency to yield a .lter 
to be used exclusively for higher frequencies. As a .nal step, the exact low-frequency IR and approximate 
high-frequency IR are com­bined in frequency domain to yield the required IR to be applied on input signals. 
We must emphasize here that this technique is applied to obtain an approximate response exclusively in 
the high-frequency range and it is ensured that numerical accuracy for lower frequencies till 1-2kHz 
is maintained. The reference solution for comparing our solution is the (2,6) FDTD method described in 
Section 3.2 running on a .ne mesh that ensures 10 samples per wavelength. Since the main bottleneck of 
our approach is DCT, which can be performed through an FFT, we used the GPU­based FFT implementation 
described in [17], to exploit the compute power available on today s high-end graphics cards. Combining 
op­timized transforms with algorithmic improvements described in the paper is the reason we gain considerable 
speedups over FDTD. All the simulations were performed on a 2.8GHz Intel Xeon CPU, with 8GB of RAM. The 
FFTs were performed on an NVIDIA GeForce GTX 280 graphics card. In the following sub-sections, to clearly 
demarcate the algorithmic gain of our appoach over FDTD and the speedups obtained due to using the GPU 
implementation of FFT, we provide three timings for each case: the running time for computing the reference 
solution with FDTD, the time if we use a serial version of FFTW [15] and the time with the GPU implementation 
of FFT. In general, we obtain a ten-fold performance gain due to algorithmic improvements and another 
ten­fold due to using GPU FFT. The ten-fold gain in memory usage is of course, purely due to algorithmic 
improvements. 5.2 Numerical Dispersion: Anechoic Corridor We .rst demonstrate the lack of numerical dispersion 
in our scheme. Refer to Figure 5. The scene is a 20m × 5m × 5m corridor in which the source and listener 
are located 15m apart, as shown in the .gure. To measure just the accumulation of numerical dispersion 
in the direct sound and isolate any errors due to interface or boundary handling, we modeled the scene 
as a single, fully re.ective rectangle. The simula­tion was band-limited to 4kHz, and the IR was calculated 
at the listener and only the direct sound part of the impulse response was retained. As Figure 5 shows, 
our method s impulse response is almost exactly the same as the ideal response. FDTD running on the same 
mesh under­goes large dispersion errors, while FDTD running on a re.ned mesh with s=10 samples per wavelength, 
(the reference) gives reasonably good results. Note that since there is only direct transmission from 
the source to the listener, the magnitude of the ideal frequency re­sponse is constant over all frequencies. 
This is faithfully observed for our method and the reference, but FDTD undergoes large errors, espe­cially 
for high frequencies. Referring to the video, this is the reason that with FDTD, the sound is muf.ed 
and dull, while with the refer­ence solution and our technique, the consonants are clear and bright . 
Therefore, as clearly demonstrated, our method achieves competitive accuracy with the reference while 
consuming 12 times less memory. The reference solution takes 365 minutes to compute, our technique with 
FFTW takes 31 minutes and our technique with GPU FFT takes about 4 minutes. 5.3 House Scene It is a physically-observed 
phenomenon that lower frequencies tend to diffract more around an obstacle than higher frequencies. To 
illus­trate that the associated gradual variation in intensity is actually ob­served with our method, 
we modeled a House scene, shown in Figure 6. Please listen to the accompanying video to listen to the 
correspond­ing sound clip. Initially, the listener is at the upper-right corner of the .gure shown, and 
the sound source at the lower-left corner of the scene. The source is placed such that initially, there 
is no re.ected path from the source to the listener. As the listener walks and reaches the door of the 
living room, the sound intensity grows gradually, instead of undergoing an unrealistic discontinuity 
as with geometric techniques which don t account explicitly for diffraction. This shows qualitatively 
that diffraction is captured properly by our simulator. The dimensions of the House are 17m × 15m × 5m 
and the simula­tion was carried out till 2kHz. The wall re.ectivity was set to 50%. The acoustic response 
was computed for .4 seconds. The total simu­lation time on this scene for the reference is about 3.5 
days, 4.5 hours with our technique using FFTW and about 24 minutes with our tech­nique using GPU FFT. 
The simulation takes about 700 MB of memory with our technique. This corresponds to speedups of about 
18x due to algorithmic improvements and an additional 11x due to using GPU FFT. To validate the diffraction 
accuracy of our simulator, we placed the source and listener as shown in Figure 6, such that the dominant 
path from the source to the listener is around the diffracting edge of the door. The middle of the .gure 
shows a comparison of the frequency response (FFT of the Impulse Response) at the listener location, 
be­tween the reference (FDTD on a .ne mesh with s=10 samples per wavelength) and our solution. Note that 
both responses have a similar downward trend. This corroborates with the physically observed fact that 
lower frequencies diffract more than higher frequencies. Also, the two responses agree quite well. However, 
the slight discrepancy at higher frequencies is explained by the fact that there are two partition interfaces 
right at the diffraction edge and the corresponding interface errors result in the observed difference. 
Referring to Figure 6, observe  &#38;#169;2009 IEEE. Pre-print, with permission from IEEE Transactions 
on Visualization and Computer Graphics, to appear  that our method takes 12x less memory and 200x less 
computation than the reference to produce reasonably accurate results. 5.4 Cathedral Scene As our largest 
benchmark, we ran our sound simulator on a Cathedral scene (shown in Figure 1) of size 35m × 26m × 15m. 
The simula­tion was carried out till 1kHz. The impulse response was computed for 2 seconds with absorptivity 
set to 10% and 40%, consuming less than 1GB of memory with our technique. We could not run the refer­ence 
solution for this benchmark because it would take approximately 25GB of memory, which is not available 
on a desktop systems today, with a projected 2 weeks of computation for this same scene. The run­ning 
times for this case are: 2 weeks for the reference (projected), 14 hours with our technique using FFTW 
and 58 minutes with our tech­nique using GPU FFT. This scenario highlights the memory and com­putational 
ef.ciency of our approach, as well as a challenging case that the current approaches cannot handle on 
desktop workstations. Fig­ure 7 shows the rectangular decomposition of this scene. Observe that our heuristic 
is able to .t very large rectangles in the interior of the do­main. The main advantage of our approach 
in terms of accuracy is that propagation over large distances within these rectangles is error-free, 
while an FDTD implementation would accumulate dispersion errors over all cells a signal has to cross. 
The bottom of the .gure shows the impulse response of the two simulations with low and high absorptiv­ity 
in dB. Note that in both cases, the sound .eld decays exponentially with time, which is as expected physically. 
Also, with 40% absorp­tion, the response decays much faster as compared to 10% absorption, decaying to 
-60 dB in 0.5 seconds. Therefore in the corresponding video, with low absorption, the sound is less coherent 
and individual notes are hard to discern, because strong reverberations from the walls interfere with 
the direct sound. This is similar to what is observed in cathedrals in reality. Also note that we are 
able to capture high order re.ections, corre­sponding to about 30 re.ections in this scene. This late 
reverberation phase captures the echoic trail-off of sound in an environment. Ge­ometric techniques typically 
have considerable degradation in perfor­mance with the order of re.ections and are therefore usually 
limited to a few re.ections. We are able to capture such high order re.ections because of two reasons: 
Firstly, we are using a numerical technique which works directly with the volumetric sound .eld and is 
thus in­sensitive to the number of re.ections. Secondly, as discussed in Sec­tion 5.2, our technique 
has very low numerical dispersion and thus preserves the signal well over long distances. For 30 re.ections 
in the Cathedral, the signal must travel about 600 meters without much dis­persion. As discussed earlier, 
with FDTD running on the same mesh, the signal would be destroyed in about 20 meters. 6 CONCLUSION AND 
FUTURE WORK Fig. 7. The voxelization and rectangular decomposition of the Cathedral We have presented 
a computation-and memory-ef.cient technique for scene. Varying the absorptivity of the Cathedral walls 
directly affects performing accurate numerical acoustic simulations on complex do-the reverberation time. 
Note that we are able to capture all re.ections in mains with millions of cells, for sounds in the kHz 
range. Our method the scene, including later reverberation. The impulse responses shown exploits the 
analytical solution to the Wave Equation in rectangular above correspond to high order re.ections, in 
the range of 30 re.ec­domains and is at least an order of magnitude more ef.cient, both tions, which 
would be prohibitively expensive to compute accurately for in terms of memory and computation, compared 
to a reference (2,6) geometric approaches. FDTD scheme. Consequently, we are able to perform physically 
ac­curate sound simulation, which yields perceptually convincing results containing physical effects 
such as diffraction. With our technique, we have been able to perform numerical sound simulations on 
large, com­plex scenes, which, to the best of our knowledge, was not previously possible on a desktop 
computer. One of the areas where our implementation may be improved is to add a .ne-grid simulation near 
the boundary to reduce boundary re­.ection errors. Further, we are actively looking into the integration 
of stereo sound in our framework, which requires the ability to model dy­namic objects in the scene. 
Also, we would like to model both moving sound sources and listener in the future. Another direction 
this work may be extended is to combine it with a geometric technique for per­forming the high-frequency 
part of the simulation, while our technique simulates frequencies up to 1-2 kHz.  &#38;#169;2009 IEEE. 
Pre-print, with permission from IEEE Transactions on Visualization and Computer Graphics, to appear REFERENCES 
[29] J. F. O Brien, C. Shen, and C. M. Gatchalian. Synthesizing sounds from rigid-body simulations. In 
SCA 02: Proceedings of the 2002 ACM SIG­ [1] Domain decomposition method. http://www.ddm.org. [2] Soundscapes 
in half-life 2, valve corporation. http://developer.valvesoftware.com/wiki/Soundscapes, 2008. [3] J. 
B. Allen and D. A. Berkley. Image method for ef.ciently simulating small-room acoustics. J. Acoust. Soc. 
Am, 65(4):943 950, 1979. [4] F. Antonacci, M. Foco, A. Sarti, and S. Tubaro. Real time modeling of acoustic 
propagation in complex environments. Proceedings of 7th International Conference on Digital Audio Effects, 
pages 274 279, 2004. [5] M. Bertram, E. Deines, J. Mohring, J. Jegorovs, and H. Hagen. Phonon tracing 
for auralization and visualization of sound. In IEEE Visualization 2005, 2005. [6] N. Bonneel, G. Drettakis, 
N. Tsingos, I. V. Delmon, and D. James. Fast modal sounds with scalable frequency-domain synthesis, August 
2008. [7] D. Botteldooren. Acoustical .nite-difference time-domain simulation in a quasi-cartesian grid. 
The Journal of the Acoustical Society of America, 95(5):2313 2319, 1994. [8] D. Botteldooren. Finite-difference 
time-domain simulation of low­frequency room acoustic problems. Acoustical Society of America Jour­nal, 
98:3302 3308, December 1995. [9] J. P. Boyd. Chebyshev and Fourier Spectral Methods : Second Revised 
Edition. Dover Publications, December 2001. [10] P. T. Calamia and P. U. Svensson. Fast time-domain edge-diffraction 
calculations for interactive acoustic simulations. EURASIP Journal on Advances in Signal Processing, 
2007, 2007. [11] C. A. de Moura. Parallel numerical methods for differential equations -a survey. [12] 
E. Deines, F. Michel, M. Bertram, H. Hagen, and G. Nielson. Visualizing the phonon map. In Eurovis, 2006. 
[13] Y. Dobashi, T. Yamamoto, and T. Nishita. Real-time rendering of aerody­namic sound using sound textures 
based on computational .uid dynam­ics. ACM Trans. Graph., 22(3):732 740, July 2003. [14] Durlach. Virtual 
reality scienti.c and technological challenges. Technical report, National Research Council, 1995. [15] 
M. Frigo and S. G. Johnson. The design and implementation of fftw3. Proc. IEEE, 93(2):216 231, 2005. 
[16] T. Funkhouser, N. Tsingos, I. Carlbom, G. Elko, M. Sondhi, J. E. West, G. Pingali, P. Min, and 
A. Ngan. A beam tracing method for interactive architectural acoustics. The Journal of the Acoustical 
Society of America, 115(2):739 756, 2004.  [17] N. K. Govindaraju, B. Lloyd, Y. Dotsenko, B. Smith, 
and J. Manferdelli. High performance discrete fourier transforms on graphics processors. In SC 08: Proceedings 
of the 2008 ACM/IEEE conference on Supercom­puting, pages 1 12, Piscataway, NJ, USA, 2008. IEEE Press. 
[18] M. Hodgson and E. M. Nosal. Experimental evaluation of radiosity for room sound-.eld prediction. 
The Journal of the Acoustical Society of America, 120(2):808 819, 2006. [19] D. L. James, J. Barbic, 
and D. K. Pai. Precomputed acoustic transfer: output-sensitive, accurate sound generation for geometrically 
complex vibration sources. ACM Transactions on Graphics, 25(3):987 995, July 2006. [20] M. Karjalainen 
and C. Erkut. Digital waveguides versus .nite difference structures: equivalence and mixed modeling. 
EURASIP J. Appl. Signal Process., 2004(1):978 989, January 2004. [21] L. E. Kinsler, A. R. Frey, A. B. 
Coppens, and J. V. Sanders. Fundamentals of Acoustics. Wiley, December 1999. [22] M. Kleiner, B.-I. Dalenb¨ack, 
and P. Svensson. Auralization -an overview. JAES, 41:861 875, 1993. [23] U. Krockstadt. Calculating the 
acoustical room response by the use of a ray tracing technique. Journal of Sound Vibration, 1968. [24] 
H. Kuttruff. Room Acoustics. Taylor &#38; Francis, October 2000. [25] Q. H. Liu. The pstd algorithm: 
A time-domain method combining the pseudospectral technique and perfectly matched layers. The Journal 
of the Acoustical Society of America, 101(5):3182, 1997. [26] T. Lokki. Physically-based Auralization. 
PhD thesis, Helsinki University of Technology, 2002. [27] M. Monks, B. M. Oh, and J. Dorsey. Audioptimization: 
Goal-based acoustic design. IEEE Computer Graphics and Applications, 20(3):76 91, 2000. [28] D. Murphy, 
A. Kelloniemi, J. Mullen, and S. Shelley. Acoustic modeling using the digital waveguide mesh. Signal 
Processing Magazine, IEEE, 24(2):55 66, 2007. GRAPH/Eurographics symposium on Computer animation, pages 
175 181, New York, NY, USA, 2002. ACM. [30] R. Petrausch and S. Rabenstein. Simulation of room acoustics 
via block­based physical modeling with the functional transformation method. Ap­plications of Signal 
Processing to Audio and Acoustics, 2005. IEEE Workshop on, pages 195 198, 16-19 Oct. 2005. [31] A. Quarteroni 
and A. Valli. Domain Decomposition Methods for Partial Differential Equations. Oxford University Press, 
1999. [32] R. Rabenstein, S. Petrausch, A. Sarti, G. De Sanctis, C. Erkut, and M. Karjalainen. Block-based 
physical modeling for digital sound syn­thesis. Signal Processing Magazine, IEEE, 24(2):42 54, 2007. 
  [33] N. Raghuvanshi and M. C. Lin. Interactive sound synthesis for large scale environments. In SI3D 
06: Proceedings of the 2006 symposium on Interactive 3D graphics and games, pages 101 108, New York, 
NY, USA, 2006. ACM Press. [34] Y. S. Rickard, N. K. Georgieva, and W.-P. Huang. Application and opti­mization 
of pml abc for the 3-d wave equation in the time domain. An­tennas and Propagation, IEEE Transactions 
on, 51(2):286 295, 2003. [35] J. H. Rindel. The use of computer modeling in room acoustics. [36] H. Sabine. 
Room acoustics. Audio, Transactions of the IRE Professional Group on, 1(4):4 12, 1953. [37] S. Sakamoto, 
T. Seimiya, and H. Tachibana. Visualization of sound re.ection and diffraction using .nite difference 
time domain method. Acoustical Science and Technology, 23(1):34 39, 2002. [38] S. Sakamoto, A. Ushiyama, 
and H. Nagatomo. Numerical analysis of sound propagation in rooms using the .nite difference time domain 
method. The Journal of the Acoustical Society of America, 120(5):3008, 2006. [39] S. Sakamoto, T. Yokota, 
and H. Tachibana. Numerical sound .eld anal­ysis in halls using the .nite difference time domain method. 
In RADS 2004, Awaji, Japan, 2004. [40] L. Savioja. Modeling Techniques for Virtual Acoustics. Doctoral 
thesis, Helsinki University of Technology, Telecommunications Software and Multimedia Laboratory, Report 
TML-A3, 1999. [41] K. L. Shlager and J. B. Schneider. A selective survey of the .nite­difference time-domain 
literature. Antennas and Propagation Magazine, IEEE, 37(4):39 57, 1995. [42] S. Siltanen. Geometry reduction 
in room acoustics modeling. Master s thesis, Helsinki University of Technology, 2005. [43] A. Ta.ove 
and S. C. Hagness. Computational Electrodynamics: The Finite-Difference Time-Domain Method, Third Edition. 
Artech House Publishers, June 2005. [44] T. Takala and J. Hahn. Sound rendering. SIGGRAPH Comput. Graph., 
26(2):211 220, July 1992. [45] A. Toselli and O. Widlund. Domain Decomposition Methods. Springer, 1 edition, 
November 2004. [46] N. Tsingos. Simulating High Quality Dynamic Virtual Sound Fields For Interactive 
Graphics Applications. PhD thesis, Universite Joseph Fourier Grenoble I, December 1998. [47] N. Tsingos, 
C. Dachsbacher, S. Lefebvre, and M. Dellepiane. Instant sound scattering. In Rendering Techniques (Proceedings 
of the Euro­graphics Symposium on Rendering), 2007. [48] N. Tsingos, T. Funkhouser, A. Ngan, , and I. 
Carlbom. Modeling acous­tics in virtual environments using the uniform theory of diffraction. In Computer 
Graphics (SIGGRAPH 2001), August 2001. [49] K. van den Doel, P. G. Kry, and D. K. Pai. Foleyautomatic: 
physically­based sound effects for interactive simulation and animation. In SIG-GRAPH 01: Proceedings 
of the 28th annual conference on Computer graphics and interactive techniques, pages 537 544, New York, 
NY, USA, 2001. ACM Press. [50] S. Van Duyne and J. O. Smith. The 2-d digital waveguide mesh. In Applications 
of Signal Processing to Audio and Acoustics, 1993. Final Program and Paper Summaries., 1993 IEEE Workshop 
on, pages 177 180, 1993. [51] K. Yee. Numerical solution of inital boundary value problems involv­ing 
maxwell s equations in isotropic media. Antennas and Propagation, IEEE Transactions on [legacy, pre -1988], 
14(3):302 307, 1966.   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1667255</article_id>
		<sort_key>160</sort_key>
		<display_label>Article No.</display_label>
		<display_no>16</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[Introduction to computer graphics]]></title>
		<page_from>1</page_from>
		<page_to>760</page_to>
		<doi_number>10.1145/1667239.1667255</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1667255</url>
		<abstract>
			<par><![CDATA[<p>Computer graphics is a broad and deep subject, and getting the most out of attending the annual SIGGRAPH conference requires a good understanding of the core ideas that lie at the heart of our existing techniques and future innovations. This course presents live demos of popular 2D and 3D software to demonstrate the key ideas that enable creation of scientific imagery, feature movies, interactive art, and more.</p> <p>In the world of 3D, the course shows how to use basic shapes to create complex objects and demonstrates how to move and manipulate those objects over time to create motion. It also shows how to generate images that communicate these models to the world and string the images together to create animation. In the world of 2D, the course follows roughly the same approach but looks more closely at how today's rich 2D development environments allow us to manipulate photos and create interactive installations in which users can explore their data, control simulations, or create new artwork.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.0</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>J.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405</concept_id>
				<concept_desc>CCS->Applied computing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1797155</person_id>
				<author_profile_id><![CDATA[81448592421]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Glassner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Introduction to 3D Modeling Andrew Glassner  3D Computer Graphics  Why Create 3D Models?  Image 
Synthesis  Design  Manufacturing  Simulation  Art  Models for Image Synthesis  Camera Viewpoint 
for image  Light Sources Radiate light Have size and shape  Objects  Physical structures Models for 
Simulation  Physics An airplane wing  Mechanics  Fit between parts Manufacturability  Model Attributes 
 Structure Geometry and Topology  Appearance  Looks and surfaces Levels of Detail  Visual detail 
for images  Structural detail for simulation   Seeing in 3D The world in basic shapes  Simple 
but not too simple      Detail for Image Synthesis Real shapes are complex!  More detail = more 
realism Takes longer to model, longer to render, and occupies more disk space  Procedural objects 
 More detail when you want it  Detail for Simulation  Can affect accuracy of simulation  Different 
simulations require detail in different places  Levels of Detail for Simulations  Does it fit in the 
box? Does it fit with the cover on?  Types of Modelers Interactive  Scripted  Data Collection  
Others      Primitives and Instances Platonic ideal  Shapes are instances of primitives  Each 
instance may be different   Choosing a Model Representation Cost  Effectiveness  Complexity  
Ease of Simulation  Ease of Animation  Model Cost  Designer s time  Computer Storage  Rendering 
Time  Simulation Time  Ease of Animation  Model Effectiveness  Geometry Looks Accuracy  Appearance 
  Looks Accuracy Model Complexity Number of primitives  Number of shapes  Complexity of each instance 
  Model Simulation  Is shape matched to simulator?  Cost of conversion  Time and storage Maintaining 
duplicate versions Model Animation  Articulation Getting at the part you want Getting it to move 
correctly   Physics of motion  Constraints  Modeling and Rendering Rendering adds light  The renderer 
tracks the light  Lights and cameras are part of the model.  Modeling and Animation  Animating is 
a model over time  Different keys given by the animator are interpolated to give  in-betweens Levels 
of Detail  Use only enough detail Complexity costs  Switch levels of detail Requires multiple models 
Switching is hard to hide  Automatic Methods  Procedural Models  Create model on demand  Models from 
coarse to fine  Requires skillful programming  Basic Linear Operations on Primitives Translate Rotate 
 Scale           Free-Form Deformation  Change the space, not the object  Great for animation 
 Allows flexible transformations  Bend, twist, taper, melt, etc.    Types of Primitives 0 Dimensions: 
Points  1 Dimension: Lines  2 Dimensions: Surfaces  3 Dimensions: Volumes  Point Primitives  Particle 
systems  Requires many particles  Often procedurally controlled   Surface Primitives  Polygons 
  Patches  Polygons Simple to define and use  Assemble to make polyhedra  Flat  Flat  Flat  
Really, really flat, always           Images by Neil Blevins  Images by Neil Blevins   
 Patches  Naturally curved  Defined by control points or curves  Interpolating  Approximating 
 Interpolation and Approximation Interpolation  Approximation  Continuity  Types of Patches Bezier 
 B-spline  Bicubic  NURBS  many more           Volumetric Primitives  Volumes that enclose 
some space  Open vs. closed  Can be complex, e.g. a donut  Voxels  Small blocks of space  Equally-sized 
(grid)  Varying sizes (octree)  Voxels for Approximation  Original Shape Voxel Approximation Constructive 
Solid Geometry  Combination rules for solids  Each combines two solids  Results can are new solid 
 CSG Tree  Three (or four) rules  CSG  Difference is Not Symmetrical  Difference is Useful for Cutting 
Holes  Block + Cylinder Block - Cylinder   Fillets and Blends Make a smooth join between surfaces 
 Hard to do automatically  Algebraic Functions  Blobs  Algebraic functions, usually spherical  Add 
together to make smooth blends   Blobby movie  Surface of Revolution Make an outline  Revolve it 
   Extrusion Make an outline  Sweep it along a line or curve      Shading         
  Texture             Knotwork                 Texturing Methods  Native 
Mapping  Planar Mapping  Native Mapping  Native Mapping  Cylindrical Mapping  Planar Mapping  Shrink 
Mapping  Spherical Mapping   Procedural Models  Fractals  Graphtals  Shape Grammars  General 
Procedural Models  Fractals  Self-similar  Infinite detail Computer only approximates  Difficult 
to control  Mountains and ferns   Graphtals Make the structure first  Add geometry later  Useful 
for plants and organic forms  Data Amplification  Shape Grammars  Shapes that turn into other shapes 
  Details that work with substrate  Data Amplification    2D Variations             
    3D Shape Grammars                        General Procedural Models  Most 
powerful technique of all  Smooth changes in detail  Supports simulation, animation  Allows models 
that interact with the scene    Modeling Environments  Interactive  Scripted  Captured  Clip 
Art  Interactive Modeling Interactive  Exploratory  Immediate Feedback   Scripted Modeling Precise 
 Repeatable  Physical Models Build up 3D intuition  Stretches visual imagination        Captured 
Modeling  Allows capturing real-world shapes  Generates rich models  Can be noisy  Can have geometric 
restrictions  Clip Art Fast acquisition  Can be cheaper  May not be articulated as you want  Difficult 
to customize  Modeling for Animation  Rigid structures are easiest to make  Articulated structures 
are easiest to animate  Plan for where you want motion  Built-in constraints  Conclusions  Many primitives 
 Many modelers  Use what you need  modeler, primitive, construction style, and level of detail  Think 
before you model!       Introduction to Rendering Andrew Glassner What is Rendering? Turning 
ideas into pictures  Communications tool  A means to an end  Rendering Roots The Visual Arts Painting, 
Sculpture  Physics Studying nature  Computers  Efficient algorithms Approaches to Rendering Simulate 
Nature  Anything else   Simulating Nature Advantages Precise goal Path is clear Practical value 
 Disadvantages  Ignores other styles  Photorealism The Big Winner so far coined in 1968 just one 
school of art alternatives emerging  Today s view Rendering as commodity  Buy, don t write  Still 
requires care  Efficiency Accuracy Effects Purpose of this talk The basic ideas Techniques Tradeoffs 
Vocabulary  High-level understanding  Not programming! The Big Picture Fake a photo (almost)  Data 
into picture  Geometry Surfaces Reflection  Transparency  Emission  Camera and film Rendering Transformation 
 Inputs Geometry, physics, programs, images, perception  Output  Images The Rendering Equation Unifies 
all algorithms  Based on nuclear physics  Trivial and self-evident   Rendering Interaction of Light 
and Matter Light Matter The Flow of Light Flux  Photons in space  Light and Matter Reflection  
Transmission  Absorption   The Interaction Light arrives  Energy transfers  Light departs  The 
Usual Question What s the color of the light from this point on this surface in this direction  Shading 
 1. Calculate incident light  2. Interact with material  3. Calculate outgoing light  1. Gather Incident 
Light  2. Interact With Material     3. Calculate Outgoing Light  Shading and Visibility Visibility 
 What we see  Shading  How it looks Visibility What do I see?  Paint by numbers  Shading Fill 
in the numbers  Don t stick to flat colors   Visibility: Paint-by-numbers 8 1 Shading: Object colors 
 Local &#38; Global Shading Use environment, or fake it?  Local shading Fake it, fast and dirty Don 
t worry, be happy  Global shading  Do it slow and correct Worry, be happy Why do it globally? Shadows 
 Reflections  Refraction (transparency)  Emissions (light sources)  Subtle touches  Why do it locally? 
 Often close enough  Fast!  Hardware support  Many effects can be faked  Unlimited scene size  
Ray Tracing  Ray Tracing 1 Follow lines of sight Ray Tracing 2 Shoot new rays to find illumination 
    Ray Tracing   Radiosity  Radiosity 1 Discretize the environment  Radiosity 2 Bounce energy 
back and forth  Radiosity 3 View results     Radiosity    Lighting  Images by Neil Blevins 
 Images by Neil Blevins  Image by Neil Blevins Image by Chris Pember Comparing  Hardware support 
Z-buffers Inifnite # of objects Local shading Fast Many tricks available Rendering at extremes Atoms 
or sub-atomic particles  Galaxies or the Milky Way  Relativity  Ocean depths  Inside people  Image-Based 
Rendering Image-Based Rendering Some photos  Some geometry  Sue Vision  Bob Graphics  Computer 
Graphics Output  Image Synthetic Camera  Computer Vision Output Model  Real Cameras CG Meets 
CV            Fluorescence   Solar Halos        Halo Movie  Interference   Relativity 
   Polarization  Phosphorescence     Caustics Focused light   Image by Johan Thorngren  
Image by Johan Thorngren Volumetric Effects   Non-Euclidean Space  Animation Film: 24 frames/second 
 30 minutes = 43,200 frames More for TV!  Motion blur  Exposure Cameras  Lenses  Films  Intended 
viewing conditions    Color 1 The RGB Myth  The HSV Myth  The 8-bit Myth  The Constancy Myth 
 The Fifth Myth with pith  Color 2 Devices vary Printers vs. monitors  Perception matters Metamers 
Diet, recent conditions  Interpolation is tricky  RGB is not perceptually uniform Non-Realistic Rendering 
  Real Materials Absorption spectra  Layers or coatings  Phosphors, fluorescents  Metals    
Reflectivity of Paper Images 1 Frame buffers Pixels 8 bits/color  Samples  1 sample/pixel Supersampling 
Images 2 Storage Components Integer, Floating point Symbolic  Compression  Lossy (MPEG, JPEG) Non-lossy 
(GIF) Speed vs. space   Compositing Layering of images  Backgrounds, foregrounds  Real Projection 
 Mattes Matte lines Matte operators   Alpha buffers    Image by Steven Blackmon  Image by Michael 
Spaw  Image by Johan Thorngren The future 1 Photorealism grows More accuracy More speed  Parallel 
algorithms  Subjective Rendering grows  New opportunities More personal Image by Tomek Baginski The 
future 2 Desktop animation Sophisticated support Standards and architectures Rendering one piece among 
many  You did what, Grandpa?  Where The Fun Is Non-Realistic rendering  Shaders  The fuzzy space 
  Image by Jorge Seva &#38; Sergio Miruri Image Credits  Discontinuity Meshing: Dani Lischinski, Filippo 
Tampieri, Donald P. Greenberg  Opera Lighting: Julie O'B. Dorsey, Francois X. Sillion, Donald P. Greenberg 
 Radiosity Factory and Museum: Michael F. Cohen, Shenchang Eric Chen, John R. Wallace, Donald P. Greenberg 
 Two Pass Rendering: John R. Wallace, Michael F. Cohen,Donald P. Greenberg  3 teapots, Caustic Pool: 
Eric Veach  Summer Lake: Matt Pharr, Craig Kolb, Reid Gershbein, Pat Hanrahan  Color Head: Philippe 
Lacroute, Marc Levoy  Material Vases: Rob Cook, Kenneth Torrance  1984: Tom Porter, Rob Cook, Loren 
Carpenter  The Compleat Angler: Turner Whitted  Still Life: Cassidy Curtis  Focused Caustics: Paul 
Heckbert  TV Room: Bob Zeleznik, Andy Forsberg, Loring Holden  Big Cloud: David Ebert  Interior: Michael 
Fowler  Thanks Michael F. Cohen  Peter-Pike Sloan  Jonathan Shade  Paul Debevec  Marc Levoy  
          Image by Steven Blackmon Introduction to Computer Animation Andrew Glassner  Why 
Animation Works  Many still images  Rapid succession  Persistence of vision  Must overcome flicker 
Animation is Expensive!  30 frames/second  30 minutes = 54,000 frames  5 minutes/frame, 12 hours/day 
~ 1 year  Limited animation  Computer-assisted animation  Thinking About Animation  Low level: Individual 
frames  Mid level: Sequences &#38; scenes  High level: Story and message  Computer helps all 3 levels 
  Traditional 2D Animation Hand-drawn cels  Stacks of cels over background  Only redraw cels that 
change Limited animation  Experimental forms  Traditional 3D Animation Individual frames  Stop motion 
 King Kong, Wizard of Space &#38; Time  Puppetry Claymation  Experimental forms  Computer-Assisted 
Animation 2D Create &#38; draw frames Computer helps ink &#38; paint Computer-Assisted Animation 3D 
Create models, sets, poses Computer interpolates Computer renders, composes   2D Computer Animation 
 What gets interpolated? Strokes Outlines Colors  High-quality compositing  2D Morphing Image interpolation 
 Feature matching  Multiple layers   3D Computer Animation What gets interpolated? Shape geometry 
 Shape appearance  Light source information Cameras Anything! 3D Computer Animation 1. Interpolate 
 2. Render  3. Compose  Object Animation Location  Geometry  Transformations &#38; deformations 
  Appearance and textures  Light sources  Camera Animation FOV  Focal length  Position &#38; orientation 
 Object Interpolation Parameterized transformation  Rotation Scaling Deformation  Interpolate parameters 
 Build new transformations   3D Animation Methods 1 Low level manual  High level manual  Automated 
 Parametric Interpolation Any number of parameters  User-specified smoothness  Transformation &#38; 
deformations  Parameter source  User Measured (rotoscope, mocap) Procedural Kinematics  Specify 
position and time  Give velocity, acceleration of parts  Nested interpolation  Each transformation 
accessible    Bounce Ball vertical position  Squash Ball squash percentage  Path Ball track 
position  Ball track percent  Path  Inverse Kinematics Goal-driven  Supports constraints  Control 
what you care about  Let computer fill in the rest   Constraints Parametric restrictions  Aids 
in inverse kinematics Put the foot on the ground  Restricts model to sensible poses and motion  Can 
be frustrating  Dynamics  Physical simulation  Based on physics  Requires physical data Mass, center 
of mass, moments of inertia, friction, etc.  Accurate motion  Difficult to control  Simulation Precompute 
model parameters  May be expensive  Many methods available  Complex motion  Flocking Cloth Liquids 
       Parameterized Motion Capture  Synthesize  MOCAP processing  The Process of Motion Capture: 
Dealing with the Data Bodenheimer, Rose, Rosenthal, Pella Computer Animation and Simulation 1997 Compositing 
 Multiple renders  Overlay results  Huge time savings  Extra control    Particle Systems Points 
 Born, Live, Die  Color  Motion  Geometry   Bubbles  Scripting Programming language  Results 
of simulation With constraints  Can include 2D transitions   Artificial Intelligence High level 
director s language  Conversations  Shots  Scripts?  Combined Methods Key-framing major components 
  Simulation for details  Secondary motion Slide Credits Charles F. Rose  Michael F. Cohen  Bobby 
Bodenheimer  Conclusions  The computer is not the animator!  Many types of motion control  Different 
skills involved  Emotion still requires acting   Notes on Processing Andrew Glassner  Processing 
 Free!  http://processing.org  Java-based  Great online learning tools Plus real books  Processing 
 Tons of online examples  Most with complete source code  Fool around You can t break it!  Processing 
 Super super casual  Playpen approach  Scribble and type I ll use that for these notes      Sandpainter 
 Invented by Jared Tarbell  http://www.complexification.net  Source code and examples I reverse-engineered 
from his code   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1667256</article_id>
		<sort_key>170</sort_key>
		<display_label>Article No.</display_label>
		<display_no>17</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[Next billion cameras]]></title>
		<page_from>1</page_from>
		<page_to>131</page_to>
		<doi_number>10.1145/1667239.1667256</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1667256</url>
		<abstract>
			<par><![CDATA[<p>What will a camera look like 10 years from now? How should we change the camera to improve mobile photography? How will a billion networked and portable cameras change the social culture? How can we use bio-inspired processing to decompose sensed values into perceptually critical elements? How will online photo collections transform visual social computing?</p> <p>Capture and analysis of visual information plays an important role in photography, art, medical imaging, tele-presence, worker safety, scene understanding, and robotics. But current computational approaches analyze images from single cameras that have only limited abilities. Cameras of the future will exploit unusual optics, novel illumination, and emerging sensors. A significant enhancement in the next billion cameras to support scene analysis and mechanisms for superior metadata tagging for effective sharing will bring about a revolution in visual communication. This course explores the trends and issues associated with that revolution.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010383</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Image processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1797156</person_id>
				<author_profile_id><![CDATA[81536557456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alexei]]></first_name>
				<middle_name><![CDATA[(Alyosha)]]></middle_name>
				<last_name><![CDATA[Efros]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797157</person_id>
				<author_profile_id><![CDATA[81548005482]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797158</person_id>
				<author_profile_id><![CDATA[81407593498]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Steve]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seitz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Siggraph 2009 Course Next Billion Cameras  Speakers Alexei (Alyosha) Efros http://www.cs.cmu.edu/~efros 
Ramesh Raskar http://www.media.mit.edu/~raskar Steve Seitz http://www.cs.washington.edu/homes/seitz With 
more than a billion people now using networked, mobile and location-aware cameras, we are seeing a rapid 
evolution in activities based on visual exchange. How will these tools impact research and applications 
in visual computing? Overview What will be the impact of billions of available photos (with text and 
location information) on vision and graphics research and applications? Online image collections (e.g., 
Flickr, Google), already capture the significant tourist sites from almost every conceivable viewing 
position and angle, different times of day and night, and changes in season, weather, and decade. Furthermore, 
entire cities are now being captured from satellite, air, and street level. These collections are (or 
will soon be) likely dwarfed by the massive numbers of photos of people, whether they be celebrities 
or personal collections of family and friends. Events (e.g., the presidential inauguration) are similarly 
captured in both images and video. In short, this is the ultimate dataset---a dense sampling of the world's 
people, places, and things. The capture and analysis of visual information plays an important role in 
photography, art, medical imaging, tele-presence, worker safety, scene understanding and robotics. But 
current computational approaches analyze images from single cameras that have only limited abilities. 
How will online photo collections transform visual social computing? What will a camera look like in 
ten years? How should we change the camera to improve mobile photography? How will a billion networked 
and portable cameras change the social culture? Cameras of the future will exploit unusual optics, novel 
illumination, and emerging sensors. A significant enhancement in the next billion cameras to support 
scene analysis, and mechanisms for superior metadata tagging for effective sharing will bring about a 
revolution in visual communication. We will explore the implications of these emerging devices and dataset 
for a range of research problems and applications in scene analysis and synthesis, including topics such 
as recognition, photo editing, 3D reconstruction, image-based rendering, photo editing, location estimation, 
and other areas. Course Schedule A. Introduction--5 minutes B. Cameras of the future (Raskar, 30 minutes) 
* Form factors, Modalities and Interaction * Enabling Visual Social Computing  C. Reconstruction the 
World (Seitz, 30 minutes) * Photo tourism and beyond * Image-based modeling and rendering on a massive 
scale * Scene summarization  D. Understanding a Billion Photos (Efros, 30 minutes) * What will the 
photos depict? * Photos as visual content for computer graphics * Solving computer vision  E. Discussion--10 
minutes Speakers Alexei (Alyosha) Efros is an assistant professor at the Robotics Institute and the 
Computer Science Department at Carnegie Mellon University. His research is in the area of computer vision 
and computer graphics, especially at the intersection of the two. He is particularly interested in using 
data-driven techniques to tackle problems which are very hard to model parametrically but where large 
quantities of data are readily available. Alyosha received his PhD in 2003 from UC Berkeley and spent 
the following year as a post-doctoral fellow in Oxford, England. Alyosha is a recipient of the NSF CAREER 
award (2006), the Sloan Fellowship (2008), the Guggenheim Fellowship (2008), and the Okawa Grant (2008). 
http://www.cs.cmu.edu/~efros/ Ramesh Raskar is an Associate Professor at the MIT Media Lab and heads 
the Camera Culture research group. The group focuses on creating a new class for imaging platforms to 
better capture and share the visual experience. This research involves developing novel cameras with 
unusual optical elements, programmable illumination, digital wavelength control, and femtosecond analysis 
of light transport, as well as tools to decompose pixels into perceptually meaningful components. Raskar's 
research also involves creating a universal platform for the sharing and consumption of visual media. 
Raskar received his PhD from the University of North Carolina at Chapel Hill. Raskar received the TR100 
Award from Technology Review in 2004 and Global Indus Technovator Award, instituted at MIT to recognize 
the top 20 Indian technology innovators worldwide in 2003. He holds 35 US patents and has received four 
Mitsubishi Electric Invention Awards. He is currently co-authoring, with Jack Tumblin, a book on computational 
photography. http://www.media.mit.edu/~raskar Steve Seitz is a Professor in the Department of Computer 
Science and Engineering at the University of Washington. He received his B.A. in computer science and 
mathematics at the University of California, Berkeley in 1991 and his Ph.D. in computer sciences at the 
University of Wisconsin, Madison in 1997. Following his doctoral work, he spent one year visiting the 
Vision Technology Group at Microsoft Research, and subsequently two years as an Assistant Professor in 
the Robotics Institute at Carnegie Mellon University. He joined the faculty at the University of Washington 
in July 2000. He was twice awarded the David Marr Prize for the best paper at the International Conference 
of Computer Vision, and has received an NSF Career Award, an ONR Young Investigator Award, and an Alfred 
P. Sloan Fellowship. His work on Photo Tourism (joint with Noah Snavely and Rick Szeliski) formed the 
basis of Microsoft's Photosynth technology. Professor Seitz is interested in problems in computer vision 
and computer graphics. His current research focuses on capturing the structure, appearance, and behavior 
of the real world from digital imagery. http://www.cs.washington.edu/homes/seitz/          
   Computational Photography [Raskar and Tumblin] 1. Epsilon Photography Low-level vision: Pixels 
 Multi-photos by perturbing camera parameters  HDR, panorama,  Ultimate camera  2. Coded Photography 
 Single/few snapshot  Reversible encoding of data  Additional sensors/optics/illum  Scene analysis 
: (Consumer software?)  3. Essence Photography Beyond single view/illum  Not miiimic hhuman eye  
New art form  Epsilon Photography  Epsilon Photography  Dynamic range Exposure bracketing [Mann-Picard, 
Debevec]  Wider FoV Stitching a panorama  Deppth of field Fusion of photos with limited DoF [Agrawala04] 
 Noise  Noise Flash/no-flash image pairs Frame rateFrame rate Triggering multiple cameras [Wilburn04] 
 Epsilon Photography  Epsilon Photography  Dynamic range Exposure braketing [Mann-Picard, Debevec] 
 Wider FoV Stitching a panorama  Depth of field FiFusion off phhotos wihith lilimiitedd DFDoF [Agrawala04] 
 Noise  Flh/ flhi i Flash/no-flash image pairs [Petschnigg04, Eisemann04] Frame rate Triggering multipleTriggering 
multiple camerascameras [[Wilbilburn05, Shechtman02]] hh Computational Photography        Computational 
Photography [Raskar and Tumblin]  Blind Camera  Capturing the Essence of Visual Experience Exploiting 
online collections Photo-tourism [Snavely2006] Photo tourism [Snavely2006] Scene Completion [Hays2007] 
 Multi-perspective Images  Mlti li P ti [Ji iY M Mill 2004] Multi-linear Perspective [Jingyi Yu, McMillan 
2004]  Unwrap Mosaics [Rav-Acha et al 2008]  Video texture panoramas [Agrawal et al 2005]   NNon-phthotoreali 
listi tic synththesiis Motion magnification [Liu05]  Imagge Priors  Learned features and natural statistics 
 Face Swapping: [Bitouk et al 2008]  Data-driven enhancement of facial attractiveness [Leyvand et al 
2008]  Deblurring [Fergus et al 2006, Several 2008 and 2009 papers]   Community Photo Collections 
U of Washington/Microsoft: Photosynth  Capturing the Essence of Visual Experience  Unwrap Mosaics 
+ Video Editing  Rav-Acha et al Siggraph 2008 Capturing the Essence of Visual Experience  Motion 
Magnification  Liu, Torralba, Freeman, Durand, Adelson Siggraph 2005 Motion Magnification  Liu, Torralba, 
Freeman, Durand, Adelson Siggraph 2005 Motion Magnification  Liu, Torralba, Freeman, Durand, Adelson 
 Siggraph 2005 Capturing the Essence of Visual Experience  Learned features and natural statistics 
 Face Swapping: [Bitouk et al 2008]  Data-driven enhancement of facial attractiveness [Leyvand et al 
2008]  Deblurring [Fergus et al 2006, Several 2007-2009 papers]  Face Swapping  [Bitouk et al 2008] 
  Computational Photography [Raskar and Tumblin]   SIGGRAPH Course: Next Billion Cameras Modelingg 
the World from Photos on the Internet Steve Seitz University of Washington Noah Snavely, Sameer Agarwala, 
Ian Simon, Brian Curless University of Washington Michael Goesele TU Darmstadt Huggues Hopppp e,, Rick 
Szeliski Microsoft Research, Redmond Billions of photos online  In situ Street side Oblique Satellite 
flickr.com g local.live.com google.com Google StreetView gg Entire cities captured from ground and air 
Entire cities captured from ground and air can we reconstruct and visualize the world using these images? 
these images? Flickr 3.5E+09 3E+09 3 billion 2.5E+09 2E+09 2 billion 1.5E+09 1 billion 1 1E 0909 billion 
500000000 0 6.3 billion photos on Photobucket, > 10 billion on Facebook Talk outline  Sources of imagery 
Ebli t l  Enabling techhnology  feature matching  structure-from-motion  multimulti-view stereoview 
stereo   Interesting directions  photosynth  cityy modelingg   Comparing images  Detect features 
using SIFT [Lowe, IJCV 2004] SIFT descriptor Full version Divide the 16x16 window into a 4x4 grid of 
cells (2x2 case shown below)  Comppute an orientation histoggram for each cell  16 cells * 8 orientations 
= 128 dimensional descriptor  Adapted from slide by David Lowe Properties of SIFT Extraordinarily 
robust matching technique  Can handle significant changes in viewpoint Can handle significant changes 
in viewpoint  Can handle significant changes in illumination Fast and efficient can run in real time 
 Fast and efficient can run in real time  Lots of code available  http://people csail mit edu/albert/ladypack/wiki/index 
php/Known http://people.csail.mit.edu/albert/ladypack/wiki/index.php/Known_implementations_of of_SIFT 
implementations SIFT  Structure from Motion  Images on the Internet Compputed 3D structure Structure 
from motion aka bundle adjustment (texts: Zisserman; Faugeras) p 4 p1 p3 minimize f((R,,T,,P)) p5 
p7 p 6  1 R2,t Camera 1 Camera 3 Camera 1 Camera 3 R1,t R3,t 3 Code online Noah Snavely s Bundler http://phototour 
cs washington edu/bundler/ http://phototour.cs.washington.edu/bundler/ builds on Sparse Bundle Adjustment 
package of Lourakis and Argyros Argyros Dense 3D Modeling  Michael Goesele, Noah Snavely, Brian Curless, 
Hugues Hoppe, Steve Seitz, ICCV 2007 Multi-view stereo: basic idea  Fua Seitz, Dyer Narayanan, Rander, 
Kanade Faugeras, Keriven 1995 1997 1998 1998 Hernandez, Schmitt Vogiatzis, Torr, Cipolla Pons, Keriven, 
Faugeras Furukawa, Ponce 2004 2005 2005 2006  photographp g p laser scan reconstruction byy one of 
317 Hernandez, Schmitt (accurate to < 0.4mm) Applications (show demos)  Photo tourism and Photosynth 
 MModdeliling citiities ((work i k in progress))  Be sure to check out... Related work on 3D modeling/visualization 
from communityypphoto collections includes: 4D Cities project [Schindler et al.] http://www cc gatech 
edu/4d-cities/dhtml/index html http://www.cc.gatech.edu/4d cities/dhtml/index.html Wiki-based city modeling 
[Irschara et al.] http://www icg tu-graz ac at/Members/irschara/arnold-irschara http://www.icg.tu graz.ac.at/Members/irschara/arnold 
irschara ARC web-based 3D reconstruction [Vergauwen et al.]  http://www icg tu-graz ac at/Members/irschara/arnold-irschara 
 Photorealistic virtual space [Sivic et al.]  http://www.icg.tu graz.ac.at/Members/irschara/arnold 
irschara http://www2 computer org/portal/web/csdl/doi/10 1109/CVPRW 2008 4562950 http://www2.computer.org/portal/web/csdl/doi/10.1109/CVPRW.2008.4562950 
 Subject-specific Data  Photos of Coliseum  Portraits of Bill Clinton What about Generic Data?  
generic scenes generic scenes  generic faces pedestrians A motivating example A motivating example 
 [Hays and Efros. Scene Completion Using Millions of Photographs. SIGGRAPH 2007 and CACM October 2008.] 
 Diffusion Result  Efros and Leung result  Criminisi et al. result  Road  Scene Matching for IC 
ti Image Complletion  Scene Completion Result  Algorithm Outline  Input image Scene Descriptor Image 
Collection  Context matching 20 completions 200 matches + blending     200 scene matches    
 Why does it work? Why does it work?  Nearest neighbors from a collection of 20 thousand images  
Nearest neighbors from a collection of 2 million images Unreasonable Effectiveness of Data [Hl N iP 
i2009] [Halevy, Norvig, Pereira 2009] Parts of our world can be explained by elegant mathematics elegant 
mathematics physics, chemistry, astronomy, etc. But much cannot psychology,gy, economics, genetics,, 
etc.  Enter The Data!  py ,g Great advances in several fields: e.g. speech recognition, machine translation, 
NLP  Case study: Google  Photos as Visual Data Not random samples of visual world M fbi  Many sources 
of bias:  Sampling bias  Photographer bias  Social bias   Social bias Flickr Paris  Real Paris 
 Real Notre Dame  Sampling Bias People like to take pictures on vacation  Photographer Bias People 
want their pictures to be recognizable and/or interesting and/or interesting vs.  Photographer Bias 
People follow photographic conventions  vs.  Social Bias  100 Special Moments by Jason Salavon Social 
Bias  P r o b a b i l i ty o f B i r t h Y e a r 0. 0 7  0. 0 6 0. 0 5 0. 0 4 0. 0 3 Mildred Lisa 
Nora Peyton Linda  0. 0 2 0. 0 1 0 1900 1 920 19 4 0 19 60 198 0 20 0 0 Bi r t h Y e a r  Bi r t 
h Ye a r Mildred and Lisa Source: U.S. Social Security Administration PP r o babi l i t y Gallagher 
et al CVPR 2008 Social Bias  Gallagher et al CVPR 2008  Gallagher et al, CVPR 2009 Reducing / Changing 
Bias  Street side Oblique Satellite Google StreetView local.live.com google.com Autonomous capture methods 
can reduce / change biaschange bias But it won t go away completely Need care in picking your data! Photos 
as Visual Content for Computer Graphics Some examples Appearance Transfer from unlablled data Semantic 
Photo S nthesis [Johnson et al EG 2006]  Semantic Photo Synthesis [Johnson et al, EG 2006]  Scene Completion 
(already discussed)  Face Swapping [Bitouk et al, SIGGRAPH 08]  Intrinsic Colorization [[Liu et al,, 
SIGGRAPH Asia 08]]   Appearance Transfer using lablled data  Photo Clip Art [Lalonde et al, SIGGRAPH 
07]  Deep Photo [Kopf et al, SIGGRAPH Asia 08]   Photos as Visual Knowledge for Computer Vision Some 
examples Label Transfer  TinTiny IImages [Torralba[Torralba et et al al, PAMI 2008] mages PAMI 2008] 
 im2gps [Hays et al, CVPR 08]  Dense Scene Alignment [Liu et al, CVPR 09]   Data-driven Priors 
 Data driven Priors Large Photo Collections and What they Reveal about Cameras [Kuthirummal et al ECCV 
08] about Cameras [Kuthirummal et al, ECCV 08]   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1667257</article_id>
		<sort_key>180</sort_key>
		<display_label>Article No.</display_label>
		<display_no>18</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[Point based graphics]]></title>
		<subtitle><![CDATA[state of the art and recent advances]]></subtitle>
		<page_from>1</page_from>
		<page_to>68</page_to>
		<doi_number>10.1145/1667239.1667257</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1667257</url>
		<abstract>
			<par><![CDATA[<p>This course presents the latest research results in point-based computer graphics. It begins with a discussion of novel concepts for mathematical representation of point-sampled shapes, focusing on moving least squares, spherical MLS, and robust statistics. Next, it addresses efficient algorithms for digital geometry processing and modeling of point models, including filtering, resampling, spectral processing, and deformation. IThis section covers high-quality rendering methods for point samples, such as EWA splatting and ray tracing, and explains hardware architectures for efficient point processing and rendering. The last part of the course summarizes how point representations can be utilized in the context of physically based animation.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1797159</person_id>
				<author_profile_id><![CDATA[81100260276]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Markus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gross]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Graphics Laboratory, ETH Z&#252;rich]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Point Based Graphics State of the Art and Recent Advances Markus Gross Computer Graphics Laboratory 
ETH Zürich grossm@inf.ethz.ch http://graphics.ethz.ch A SIGGRAPH 2009 Course  Acknowledgements Mario 
Botsch, Gael Guennebaud, Simon Heinzle, Richard Keiser, Oliver Knoll, Edouard Lamboray, Matthias Müller, 
Miguel Otaduy, Cengitz Oeztireli, Mark Pauly, Denis Steinemann, Matthias Teschner, Tim Weyrich, Martin 
Wicke, Stephan Wuermlin, Matthias Zwicker  Leo Guibas, Stanford University  Leif Kobbelt, RWTH Aachen 
 Andy Nealen, Marc Alexa, TU Darmstadt  Bart Adams, Phil Dutre, Uni Leuven  Hanspeter Pfister, Jeroen 
van Baar, MERL  2  Purpose of this Course  3 Overview 1. Motivation 2. Representation 3. Processing, 
Editing, and Modeling 4. Rendering and Display 5. Physics Based Animation 6. Point Based Video 7. Lessons 
learned 4 Course Material  1. The slides 2. Our book on Point Based Graphics Gross, M.; Pfister, H.: 
Point Based Graphics, Morgan Kaufmann, 2007 5 1. Motivation  6 Polynomials....  Rigorous mathematical 
concept  Robust evaluation of geometric entities  Shape control for smooth shapes  Require proper 
parameterization  Discontinuity modeling  Topological flexibility  Reduce p, refine h! 7  Triangles... 
  Simple geometric primitives  Hardware to support them  Digital processing  Explicit topology 
 The widely accepted queen of graphics primitives  Separation of geometry and attributes  Complex 
LOD management  Compression and streaming is highly non-trivial   8  Getting to the point Natural 
representation for many 3D acquisition systems  No separation of geometry and appearance/attributes 
 No separation of surfaces and volumes  No connectivity or topology  possibly more ?  9   History 
of Points in Graphics Particle systems [Reeves 1983] Points as a display primitive [Whitted, Levoy 1985] 
Oriented particles [Szeliski, Tonnesen 1992] Particles and implicit surfaces [Witkin, Heckbert 1994] 
Rendering Architectures [Grossmann, Dally 1998] Digital Michelangelo [Levoy et al. 2000] Surfels [Pfister 
et al. 2000] QSplat [Rusinkiewicz, Levoy 2000] Point Clouds [Linsen, Prautzsch 2001] Point set surfaces 
[Alexa et al. 2001] Radial basis functions [Carr et al. 2001] Surface splatting [Zwicker et al. 2001] 
Randomized z-buffer [Wand et al. 2001] Sampling [Stamminger, Drettakis 2001] Pointshop3D [Zwicker, Pauly, 
Knoll, Gross 2002] Raytracing [Alexa et al. 2003] Boolean Operations [Adams et al. 2003] Modeling [Pauly 
et al. 2003] blue-c [Gross et al. 2003] Meshless Physics [Muller et al. 2004] Spherical MLS [Guennebaud, 
Gross 2007].....many more 10  Points A Motivation 1997-2009 3D content creation pipeline Points 
generalize Pixels ! 11 2. Representation  12 Surface Model  Compute continuous surface from a setof 
discrete point samples point samples interpolating or P = { pi, ci, mi, ... } approximating P 13 Surface 
Model  Moving least squares (MLS) approximation Surface defined as stationary set of projection operator 
P implicit surface model   Weighted least squares optimization Gaussian kernel function local, smooth 
mesh-less, adaptive (Alexa, Levin, Amenta, et al.) 14  Point Set Surfaces (PSS) [Levin 2003], [Alexa 
et al. 2001,2003]  2D-example: smooth curve from a set of points using moving least squares (MLS) approximation 
15 Simple PSS Definition  Simple PSS definition  Simple PSS definition  18  Issues  Loss of detail, 
tight fits, stability    19 Issues  Loss of detail, tight fits, stability Spherical MLS (Guennebaud, 
Gross, Siggraph 2007)  Projection ontoalgebraic sphere  Improved Stability  Curvature for free  
Very fast on GPU    PointGraphics 2007 21 Algebraic Point Set Surfaces Key idea: plane fit sphere 
fit  Benefits: low sampling density tight approximation efficiency  22 Issue  How to fit a sphere 
onto a set of points ?  Algebraic fit  the sphere is described by Algebraic fit:  ! add constraint(s) 
24   Low Sampling Rates  planar fit spherical fit 25 Stability   APSS vs SPSS 26  Progressive 
Downsampling  from 150k to 5 pts ... 27 Differential Operators  => e.g., accessibility shading 28 
 Local Kernel Regression  (Oztireli, Gunnebaud, Gross, Eurographics 2009) 29 Surface Definition Surface 
definition becomes  Our Surface Definition  LKR Formulation of [Kolluri]  Iteratively Reweighted 
Least Squares (IRLS)    Our Surface Definition Surface definition becomes   32 Results  Edges 
and Corners  33  Results A tough one !   Results       35  Results  Stability    
 36 Results Noise &#38; Outliers     37  3. Processing, Editing, Modeling 38  Local Surface 
Analysis  local neighborhood (e.g. k-nearest)  eigenvectors span covariance ellipsoid  smallest eigenvector 
is least­squares normal  surface variation  measures deviation from   (Linsen, Prautzsch, Garland) 
tangent plane curvature 39 (Pauly, Gross, Kobbelt, IEEE Vis 2002) Local Surface Analysis  40  Resampling 
- Particles Resample surface by distributing particles Relaxation - Adjust repulsion radius Upsampling 
possible (Heckbert) original model uniform repulsion 296,850 points 2,000 points 41  Particle Simulation 
  Resampling- Simplification Iteratively contracts point pairs (Hoppe) Uses quadric error metric (Heckbert) 
 Similar to QSlim 43 3D Image Editing  Interactive 3D painting  Cleaning  Carving  Textureing and 
antialiasing  Modeling  Spectral processing  ...of point sampled geometry   From 2D Pixels to 
3D Points 44  remaining 296,850 points 2,000 points contraction pairs Pointshop 3D  Interactive system 
for point-based surface editing  Generalize 2D photo editing concepts andfunctionality to 3D point-sampled 
surfaces  Use 3D surface pixels (surfels) as versatile display andmodeling primitive  (Zwicker, Pauly, 
 Does not require intermediate triangulation Knoll, Gross, Siggraph 2002) 45 Concept   Parametrization 
 Constrained minimum distortion parameterization(Levy, Siggraph 2001)  Extension to irregular point 
clouds  Multigrid solver for resulting sparse linear least squares problem  47 Parametrization Landmarks 
set interactively by the user (Levy) landmarks parametrization 48 Examples Painting textures  49 
Examples  Examples  Filtering appearance and geometry  51 Examples Multiscale feature extraction 
 (Pauly, Keiser, Gross, Eurographics 2003) 52 3D Haptic Painting   Mass-spring skeleton Point-sampled 
surface Physically-based Geometric deformation deformation Paint transfer Force feedback (Adams, Wicke, 
et al. PBG 2004) 53 Paint Transfer  1. Sample Collection 2. Paint Buffer Construction 3. Object Sample 
Projection 4. Brush Sample Projection 5. Paint Model Evaluation 6. Reprojection  No separation of geometry 
and texture, no connectivity! 54  Brush Splitting  55 Paint Transfer  Results  Painted Bunnies 
  Nemo Day n Night Flower No separation of geometry and texture  No charting and parametrization 
 58  Surface Processing Toolbox Overview  59 Manual Hole Filling  1) Scan with holes (Weyrich et 
al.2004) 3) Cont. use of MLS spray can 4) Point relaxation 60  Manual Artifact Removal  Multi-Scale 
Modeling Scale spaces: levels of smoothness  same degrees of freedom on smoothest level 62  Multi-Scale 
Modeling  discretization in scale (Pauly, Kobbelt, Gross, TOG 2006) discretization in space 63  Filtering 
  Interactive Editing  editing metaphor continuous free-form deformation function smooth transition 
between deformed and un-deformed region deformation function composed of simple translation and rotation 
components Example Large Deformations, Dynamic Resampling (Pauly, Keiser, Kobbelt, Gross10,000 points 
Siggraph 2003) 66    Dynamic Sampling   Free-form Deformation  68 Boolean Operations Signed 
distance function by MLS operator  + -  -  69  Download: graphics.ethz.ch/Pointshop3D  4. Rendering 
   Surfels Framebuffer resolutions stayroughly the same Size of typical triangles incomplex 3D models 
project to < 1pixel Point or pixel based renderingmethods become more and more attractive Points store 
several surface attributes (surfels) To render, forward project eachpoint separately 72 Surfels  1. 
View independent sampling(preprocessing)  2. Rendering (runtime)   Block Forward Visibility Splatting 
Culling Warping (Pfister, Zwicker, Baar, Gross, Siggraph2000) 73 LDC Tree Texture Deferred Image Reconstruction 
and Antialiasing Filtering Shading    Store only pointers to surfels at level 0. Do not store empty 
blocks. 74 Problems    Texture and edge aliasing    75 Visibility Splatting  76  Image Reconstruction 
  2D filtering operations in screen space 77 Forward Warping   347 K points 204 K points 78 Sampling 
Artifacts screen space pixel sampling  A Closer Look onto Sampling Unified approach to reconstruction 
and aliasing  EWA, Heckbert 86  minification aliasing magnification  holes 128 x 192 80  Splatting 
 screen space splatting  81 Warping Reconstruction Kernels  screen space object space warped reconstruction 
kernel 82  Gaussian Kernels  Gaussian Gaussian reconstruction kernel low-pass filter   screen 
space screen space 84 Gaussian Kernels  Closed under affine mappings and convolution  Gaussian resampling 
filter screen space EWA Analytic expression of the resampling filter can be computed efficiently 85 
 Irregular Textures  pixel sampling  sampling pattern (Zwicker, Pfister, Baar, optimized screen 
space EWA Gross Siggraph 2001) 86  Examples  Combine reconstruction and bandlimitation PointGraphics 
2007 87 (Botsch at al.PBG 2005) 88 Ray-Tracing Compute ray intersection with MLS surface  (Adams, 
Keiser, Dutre, Pauly, Gross, Guibas, EG 05) 89 Examples  (Alexa et al. 2004, Adams et al. 2005) PointGraphics 
2007 90  Point Rendering Hardware ASIC and FPGA design  30 Mio. EWA splats/s  Lean architecture 
 FP units for geometry    Integration Into Poly Pipelines Re-use of existing GPUunits  Novel Units 
 Splat rasterizer Ternary depth test Accumulation Surface reconstruction  buffer Normalize (Weyrich 
et al., Siggraph 2007) 92  Video  93  Point Processing Units Spatial search: kNN and eNN Common 
in most point-operations Kd-tree [Bentley 75] and priority queue Heinzle, Gunnebaud, Botsch, Gross 
Graphics Hardware 2008 94 Coherent Neighbor Cache (eNN) Find neighbors in slightly bigger radius. 
 Reuse if  . 95  Coherent Neighbor Cache (kNN) Find (n+1) neighbors. The Architecture  Prototype 
 97  Coherent Neighbor Cache  Eight cached neighborhoods  Problem: parallel queries in kNN module 
 Interleave spatially similar queries 98 Kd-Tree Traversal  Processing Module  Results kNN and 
MLS   Results Approximation Error (MLS projection) Cache hits  102  5. Physically-based Animation 
 Concept Point-based approach forphysically-based animation Point based volume (physics) physical elements 
Point based surface (appearance) surface elements  (Müller, Keiser, Nealen, Pauly,surfels {si} nodes 
{pj} Gross, Alexa, SCA 2004)  104 Simulation Loop Start with undeformed object and apply external 
forces (per physical element)  Add external forces Time integration Gradient of displacement field Strain 
(Greens strain) Stress (Hookean material) Body forces (from elastic energy)  Body forces + new external 
forces = We start with pure elasticity, and add next integration step plasticity/flow thereafter 105 
 Topological Separation  106  Topological Separation  Viscous Fluids  Melting  109 Shell Physics- 
Lena Point shell structures (fibres) connect surface (Wicke, Steinmann, Gross, EG 2005) 110  Fracture 
and Cracks  Crack initiation Pauly et al. Siggraph 2005 where stress above threshold crack created 
by inserting 3 crack nodes each carrying 2 opposing surfels  connection is crack front  one fracture 
surface crack front  111  Example  112 Resampling   113 Fracture Dynamic sampling of facture 
surface  (Pauly, Keiser, Adams, Dutre, Gross, Guibas Siggraph 2005) 114 Mesh-Points  (Steinemann, 
Otaduy, Gross, SSCA 2006)  115 6. Point Based Video   (Gross et al.Siggraph 2003) 116 System Overview 
 117  3D Mirror 119  3D Video Pipeline  121  3D Video Representation  3D Video fragement  Generalizes 
pixels to 3D points   122  Differential 3D Video Stream  123 3D Video Stream Waschbüsch et al. 
PG 2005 124 3D Video Recorder  125 MoreExamples  Lamboray et al. ICIP 2004 Würmlin et al. VMV 2005 
 Active 3D Video Acquisition  Stereo cameras Texture camera Structured light projector 127 Final 3D 
Video  128 Results  So Points  Points are a useful modeling/graphics primitive  Sample based approach 
to graphics and modeling  Signal Processing view  Resample without restructuring  Local operations 
on large datasets  Do not store topology  Complement on other graphics representations  Graphics pipelines 
for points?  Consumer electronics?  130  Research Themes  Illustration and stylistic visualization 
 Implicits and Isosurfaces - MLS  Procedural point geometry  Add semantics to point samples( sameness 
)  Compression and out of corerepresentations  Dynamic point clouds  More advanced filters for pointsampled 
data  Explore combinations of primitives  Hardware for point graphics  131 Resources  http://graphics.ethz.ch/points/ 
 http://graphics.ethz.ch/pointshop3d/  http://www.graphics.ethz.ch/publications/ tutorials.php  http://graphics.ethz.ch/events/pbg/08/ 
 132  Researchers/Groups (Random&#38;Incomplete)  Marc Alexa TU Berlin  Hanspeter Pfister Harvard 
 Marc Pauly ETH Zurich  Matthias Zwicker University of Berne  Tim Weyrich UC London  Mario Botsch 
 University of Bielefeld  Leo Guibas Stanford University  Leif Kobbelt RWTH Aachen  Marc Stamminger 
 University of Erlangen  Gael Guennebaud INRIA Bordeaux  133 Researchers/Groups (Random&#38;Incomplete) 
  Nina Amenta UC Davis  Amitabh Varshney University of Maryland  Michael Wand MPI Saarbrücken 
 Bart Adams Stanford website  Loic Barthe, Mathias Paulin Toulouse  Wojciech Matusik Adobe Research 
 Renato Pajarola University of Zurich  Marc Levoy Stanford University  134 Thank You!   135 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1667258</article_id>
		<sort_key>190</sort_key>
		<display_label>Article No.</display_label>
		<display_no>19</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[Real-time global illumination for dynamic scenes]]></title>
		<page_from>1</page_from>
		<page_to>217</page_to>
		<doi_number>10.1145/1667239.1667258</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1667258</url>
		<abstract>
			<par><![CDATA[<p>Global illumination is an important factor in creating realistic scenes and provides visual cues for understanding scene geometry. However, global illumination is very costly and only recently has it become viable to render scenes with global-illumination effects at interactive frame rates by exploiting the parallelism and programmability of modern GPUs.</p> <p>This course provides a concise overview of recent GPU-based global-illumination techniques that support fully dynamic scenes, compares them, and discusses their various strengths and weaknesses. After introducing the necessary foundation (rendering equation, direct vs. indirect illumination, etc.), the course summarizes the three main streams of real-time global illumination techniques: virtual point lights, screen-space techniques, and hierarchical finite elements.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Intensity, color, photometry, and thresholding</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1797160</person_id>
				<author_profile_id><![CDATA[81100209816]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Carsten]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dachsbacher]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Stuttgart]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797161</person_id>
				<author_profile_id><![CDATA[81100016395]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kautz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University College London]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Louis Bavoil, Miguel Sainz: <i>Image-Space Horizon-Based Ambient Occlusion</i>, Shader X7]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1507161</ref_obj_id>
				<ref_obj_pid>1507149</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[T. Ritschel, T. Grosch, H.-P. Seidel: <i>Approximating Dynamic Global Illumination in Image Space</i>, I3D 2009]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1507162</ref_obj_id>
				<ref_obj_pid>1507149</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[G. Nichols, C. Wyman: <i>Multiresolution Splatting for Indirect Illumination</i>, I3D 2009]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1053460</ref_obj_id>
				<ref_obj_pid>1053427</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[C. Dachsbacher, M. Stamminger: Reflective Shadow Maps, I3D 2005]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1111428</ref_obj_id>
				<ref_obj_pid>1111411</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[C. Dachsbacher, M. Stamminger: Splatting Indirect Illumination, I3D 2006]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Bunnell, M. 2005. Dynamic Ambient Occlusion and Indirect Lighting. In GPU Gems 2, M. Pharr, Ed. Addison Wesley, Mar., ch. 2, 223--233.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1053460</ref_obj_id>
				<ref_obj_pid>1053427</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Dachsbacher, C., and Stamminger, M. 2005. Reflective Shadow Maps. In Proc. of the Symposium on Interactive 3D Graphics and Games, 203--213.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1111428</ref_obj_id>
				<ref_obj_pid>1111411</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Dachsbacher, C., and Stamminger, M. 2006. Splatting Indirect Illumination. In Proc. of the Symposium on Interactive 3D Graphics and Games, 93--100.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276453</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Dachsbacher, C., Stamminger, M., Drettakis, G., and Durand, F. 2007. Implicit Visibility and Antiradiance for Interactive Global Illumination. ACM Transactions on Graphics (Proc. of SIGGRAPH) 26, 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1338556</ref_obj_id>
				<ref_obj_pid>1338439</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Dong, Z., Kautz, J., Theobalt, C., and Seidel, H.-P. 2007. Interactive Global Illumination Using Implicit Visibility, in Pacific Graphics, 77--86.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258769</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Keller, A. 1997. Instant Radiosity. In SIGGRAPH '97, 49--56.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383883</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Laine, S., Saransaari, H., Kontkanen, J., Lehtinen, J., and Aila, T. 2007. Incremental Instant Radiosity for Real-Time Indirect Illumination. In Proc. of the Eurographics Symposium on Rendering, 277--286.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1281671</ref_obj_id>
				<ref_obj_pid>1281500</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Mittring, M. 2007. Finding next-gen: Cryengine 2. In ACM SIGGRAPH 2007 courses.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1507162</ref_obj_id>
				<ref_obj_pid>1507149</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Nichols, G., and Wyman, C. 2009. Multiresolution Splatting for Indirect Illumination. In Proc. of the Symposium on Interactive 3D Graphics and Games.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1409082</ref_obj_id>
				<ref_obj_pid>1409060</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Ritschel, T., Grosch, T., Kim, M. H., Seidel, H.-P., Dachsbacher, C., and Kautz, J. 2008. Imperfect shadow maps for efficient computation of indirect illumination. ACM Trans. Graph. 27, 5, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1507161</ref_obj_id>
				<ref_obj_pid>1507149</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Ritschel, T., Grosch, T., and Seidel, H.-P. 2009. Approximating Dynamic Global Illumination in Image Space. In Proceedings ACM SIGGRAPH Symposium on interactive 3D Graphics and Games (I3D) 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 .1 We see here an example of a real-world scene which has a lot of visual complexity and richness. 
Generating synthetic images that come close to this is an extremely challenging problem. Having them 
animate and respond to a users control is even more daunting. We will discuss some of the issues that 
need to be addressed to meet this challenge.  There are many types of scene complexity which operate 
individually and in synergy with each other to generate the visual complexity of the resulting image. 
We will concentrate on Transport Complexity.    The primary goal of this course is to accurately 
render objects in general lighting environments at interactive rates. There are several challenges that 
must be overcome to achieve this goal. Real objects have complex and spatially varying material properties 
 so we need tosupport general reflection models and handle spatial variation. General lighting environments 
are much more compelling compared to simple point lighting models. Run time integration over general 
area lighting models is expensive with traditionaltechniques. We would like to support complex transport 
effects soft shadows from area lights, inter-reflection, caustics and subsurface scattering, while maintaining 
interactive rendering rates.  Loosely put, rendering takes input (geometry, materials, lights) and 
produces an image. There are many different ways to compute an image given the input. Precomputed Radiance 
Transfer is a technique to accelerate that process. PRT deals with the shading computation (how much 
light is reflected from every visible point). The most general formulation for that is the so-called 
Rendering Equation. We will have a look at it in the following slides. The way we do this is as follows. 
Given an object, two emitters, and a point on the object, we integrate  Actually, it s a bit more complicated, 
because light can bounce. For instance it can bounce ones before it reaches the point, (click) or can 
it can bounce twice, before it is reflected towards the viewer. (click) Of course, (click) we need to 
make sure that bounced light can actually reach the point and isn t occluded as in this case here. A 
bit more formal, the amount of reflected light at a point x in direction omega_o is the emitted light 
at point x, plus the reflected light at x. The amount of reflected light is the integral of all (visible) 
incident light weighted by the BRDF.  Given an object illuminated in a lighting environment, the rendering 
equation models the equilibrium of the flow of light in the scene. It can be used to determine how light 
a visible point reflects towards the viewer. We will walk through a hemispherical formulation of this 
equation. The desired quantity is the radiance leaving a point on the object x in a given direction 
wr. Radiance is the intensity of light from a point to a certain direction. The first term is the radiance 
emitted directly from the point in the given direction. This is followed by an integral over the hemisphere 
around the point, where wi is used to denote a direction on this hemisphere The second term is the radiance 
arriving at point x from the direction wi, note that this is also the variable we are solving for so 
this is an integral equation. Also note, that visibility is implicitly defined in L_i(). It is the light 
arriving along direction wi, which refers to the closest point y along wi, that emits/reflects light 
towards x. The 1st factor inside the integral is the BRDF of the surface at point x, the BRDF is a 4D 
function that models what percent of light for some input direction wi leaves in some outgoing direction 
wr. The final term is the cosine term that comes from lamberts law due to projected area.  One convenient 
way to reason about the solution to this integral equation is by using a Neumann expansion of this expression, 
where outgoing radiance is expressed as an infinite series. The first term in this series is the direct 
lighting arriving at point x. The next term in the expansion models all paths from the source radiance 
function that reach the given point after a single bounce and contribute to outgoing radiance in the 
given direction. This is also just a conventional integral where the previous term (L_0) is inside of 
the integral. As before, visibility is implicit in L_0(). In general the ith bounce models how all of 
the energy from the previous bounce contributes to outgoing radiance in the given direction.   The 
primary goal of this course is to accurately render objects in general lighting environments at interactive 
rates. There are several challenges that must be overcome to achieve this goal. Real objects have complex 
and spatially varying material properties so we need tosupport general reflection models and handle 
spatial variation. General lighting environments are much more compelling compared to simple point lighting 
models. Run time integration over general area lighting models is expensive with traditionaltechniques. 
We would like to support complex transport effects soft shadows from area lights, inter-reflection, 
caustics and subsurface scattering, while maintaining interactive rendering rates. In real-time rendering, 
we usually make approximation to the full rendering equation, as it is too expensive to compute on the 
fly. Common approximations are: - only direct lighting (no indirect illumination at all, a simple ambient 
term can make up for it somewhat) - indirect lighting but without taking visibility (indirect shadowing) 
into account - indirect lighting taking visibility into account but only for static scenes - indirect 
lighting but only for one indirect bounce -   The challenging part is the dynamic indirect visibility. 
 Here is a frame from the previous animation and let s look at a particular light path. It starts at 
the direct light, bounces from a surface and is than blocked by geometry, casting an indirect shadow. 
 In the next frame, this path becomes unoccluded, and the indirect shadow turns into indirect light. 
So this has to be taken into account.  .1  One way to think of Instant Radiosity is the following. 
All the direct and indirect illumination is approximated with point lights, which are used for rendering. 
 Instant radiosity works as follows: Starting from a direct light source, so called virtual point lights 
 VPLs are created, which represent the indirect illumination. Note that a single VPL is essentially 
a hemispherical light with a cosine-falloff.  To compute the indirect illumination at some surface location, 
we gather light from all VPLs . However, we still need to take dynamic visibility into account. For instance, 
this path is blocked. The easiest idea is to use shadow maps, even though that is expensive.  Sample 
direct lighting (L_0), one-bounce indirect (L_1), two-bounce indirect (L_2), etc Sum up contributions. 
 Let s go back to the rendering equation and derive Instant Radiosity (IR). Using the operator notation, 
we know that radiance towards the eye = IR assumes diffuse BRDFs and explicitly samples all possible 
paths.  5/18/09  First integral part of sum: all light emitted from y through Pixel Second part of 
sum: -Integrate of pixel -Sum over all path lengths j -Integrate over all paths of length j, p_j( ) is 
assumed to be _valid_ paths! -Integrate over all light source (starting) positions -Note that: V() is 
the visibility of between y_j and y , i.e., y_j is a VPL and y are locations visible in screen-space. 
 The definition of p_j(): essentially the radiance after j reflections (assuming valid paths). Now sample 
those paths.  Instead of individually deciding which paths to continue, use fractional absorption based 
on the average reflectivity of the scene. Pseudo-code for IR.  VPLs become visibile for highly specular 
surfaces. 5/18/09   In practice, IR needs to render a large number of shadow maps, which is very costly. 
 In fact, the shadow map generation is the bottleneck: Assuming we use 1024 VPLs and a 100k triangle 
3d-model. This means drawing 100 million triangles to fill the 1000 shadow maps .  Incremental Instant 
Radiosity allows semi-dynamic scenes with moving lights. The ingredients are known but one: reuse of 
VPLs for moving lights. The main idea is to reuse VPLs from previous frames (assuming static geometry). 
The basic algorithm is simple:   Let s assume for now that our main light source is a hemispherical 
spot-light with a cosine-fall off.        Imperfect shadow maps are based on two key observations. 
1. indirect lighting varies smoothly in most scenes. 2. the individual contribution of each VPL is small. 
  Which leads to the conclusion that it is sufficient to use many low quality depth maps to determine 
visibility in indirect illumination, as errors tend to average out. Here you can see an example, where 
using low-quality depth maps does not impact the final rendering much. The main ingredient of ISM is 
to allow imperfection when creating a depth map, which enables a much more efficient generation. The 
algorithm consists of 4 steps: 1. VPL generation, 2. Point-based depth map generation 3. A pull-push 
operation to fill holes from point rendering 4. Shading  I will detail all four steps now. There should 
be no VPLs where there is no direct light and there should be VPLs where there is direct light . To achieve 
this, the scene is rendered as seen from the light source into an omnidirectional map. In particular, 
positions, normals and direct lighting are rendered. Then all textures are sampled in parallel at a 
number of random points, which are importance sampled according to the brightness of the direct illumination. 
For instance, this gives you this VPL here.  Recall, that our goal is to generate as many depth maps 
as possible. Using classic depth maps for this, takes around half a second for the Sponza scene. We want 
it much faster, but as high-quality as possible. We will do this by simplification. We will draw a 
small number of points instead of a large number of tris, which is much cheaper. Also LOD for points 
is very simpler, because they don t require connectivity. At startup, we approximate the surfaces with 
a set of points. Each VPL has it s own different set of points; typically, we use 8k points per VPL. 
 At run-time we deform this distribution according to surface deformations. The image to the right visualizes 
the point set for a single VPL, and as you can see it s quite sparse.  Here s a classic shadow map with 
triangles compared to an imperfect shadow map with points. There are quite a few holes. Using a process 
called pull-push, we fill holes, and then the maps are quite similar. Pull-push is essentially a hierarchical 
method to fill holes, essentially averaging nearby depth values. Here we show the imperfect shadow of 
an individual VPL. A depth map without pull-push will have light leaks, that are fixed by pull-push. 
At least mostly, of course, there are still some errors in the depth maps. However, since we accumulate 
the result of many VPLs, and the errors are uncorrelated, they tend to average out. We can do this pull-push 
step on all depth maps in parallel, as we work in texture space. The rendering of direct and indirect 
lighting is separated, like in most current methods. For the direct illumination, we use standard methods. 
For the indirect, we accumulate light from all VPLs, with a visibility lookup into the ISMs.  Interleaved 
sampling is used for the indirect illumination, i.e. not every pixel is lit with every VPL, but only, 
say, 16 random ones out of 1024 VPLs for every pixel (in order to save computation). Doing so, will result 
in noise, that is blurred away using a geometry-aware blur filter.  Time for some results. Our method 
is used to render this at 11 FPS, and it looks quite nice. You can see color bleeding as well as indirect 
shadows. But how does it compare to a reference solution?  This is a Monte Carlo reference rendering. 
There are some differences. But remember this is an extreme case, where indirect illumination dominates. 
In more normal scenes (without spot lights), the differences are then almost indistinguishable.  Here 
is a performance breakdown of the Christo s Sponza scene. It has 70k triangles and was rendered using 
1024 VPLs, with a shadow map of 256x256 each. 8k points are splatted into each individual depth map. 
The ISMs are generated in 44 ms. Generating classic shadow maps instead, takes around 10 to 15 times 
longer for this step, because 70k triangles would be drawn per depth map. This results is more than 11 
fps, on a Geforce 8800 GTX, around ten times faster than normal instant radiosity with classic shadow 
maps. There are two main parameters that can be tweaked: the number of points and the shadow map size. 
We have experimented with these, and the findings are not surprising: More points per VPL yields higher 
quality, and larger shadow maps are better, if there are sufficient points available. In general 1282 
or 2562 shadow maps with 8k points yields good results. Some ISM results, which range from diffuse bounces 
in a Cornell box to complex scenes, including multiple bounces, arbitrary local area lights, natural 
illumination to caustics. In this example, running at 20 fps, we placed animated meshes inside the Cornell 
box with a dynamic direct light. Most of the light in this scene is indirect. Note, how the animals 
feet cast high-frequency shadows, whereas the animal itself casts a correct soft shadow. Also note, 
the subtle variations in shadow color. Despite the fundamental changes, in indirect lighting there is 
no flickering.  This is a more complex scene, where a deforming cloth is placed inside the Sponza model. 
To achieve sufficient temporal coherence, we need 1024 VPLs in this example. Note, how the bounced light 
color changes drastically when the cloth is moving. Also note, the indirect shadow from the columns. 
No other method can do this - all previous real-time methods were essentially limited to static scenes. 
 Finally, we are not limited to diffuse materials. Here, we have a gold ring, casting a caustic at 15 
fps with full indirect visibility.  Imperfect reflective shadow map, generalize all this to additional 
bounces, details about this will be discussed later. Why couldn t we use the same idea for direct lighting 
and approximate it with a number of point lights. In fact, that is easily possible. Let s look at two 
results. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1667259</article_id>
		<sort_key>200</sort_key>
		<display_label>Article No.</display_label>
		<display_no>20</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>20</seq_no>
		<title><![CDATA[Realistic human body movement for emotional expressiveness]]></title>
		<page_from>1</page_from>
		<page_to>27</page_to>
		<doi_number>10.1145/1667239.1667259</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1667259</url>
		<abstract>
			<par><![CDATA[<p>Humans express their emotions in many ways, in particular through face, eye, and body motion. So creators of virtual humans strive to convincingly depict emotional movements using a variety of methods.This course focuses on the use of realistic human body motion to generate emotional expressiveness. Topics include: applications and research relating to procedural animation of humans with emotion and personality, biomechanical and physical principles of animation, physics-based human motion simulation, and data-driven animation. The course also provides some insights from the field of psychology and reviews issues relating to the perception and evaluation of realistic human body animation.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor>Modeling methodologies</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010342.10010343</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis->Modeling methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1797162</person_id>
				<author_profile_id><![CDATA[81100015154]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Aaron]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hertzmann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797163</person_id>
				<author_profile_id><![CDATA[81100465557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Carol]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[O'Sullivan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Trinity College Dublin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797164</person_id>
				<author_profile_id><![CDATA[81100250413]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Perlin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[New York University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1276384</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Active Learning for Real-time Motion Controllers, Cooper, S. Hertzmann, A. Popovi&#263;, Z. <i>ACM Transactions on Graphics</i> 26(3) (SIGGRAPH 2007)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073314</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Learning Physics-Based Motion Style with Nonlinear Inverse Optimization, C. K. Liu, A. Hertzmann, Z. Popovi&#263;. <i>ACM Trans. on Graphics</i> (Proc. SIGGRAPH 2005)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015755</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Style-Based Inverse Kinematics, K. Grochow, S. L. Martin, A. Hertzmann, Z. Popovi&#263;. <i>ACM Trans. on Graphics</i> (Proc. SIGGRAPH 2004)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614291</ref_obj_id>
				<ref_obj_pid>614257</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Real Time Responsive Animation with Personality, K Perlin - <i>IEEE Trans on Visualization and Comp. Graphics</i>, 1995]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237258</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Improv: a system for scripting interactive actors in virtual worlds, K. Perlin and A. Goldberg. SIGGRAPH 1996]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Building Virtual Actors Who Can Really Act, K. Perlin. <i>Virtual Storytelling</i>, LNCS, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Better acting in computer games: the use of procedural methods. K. Perlin, <i>Computers and Graphics</i> 26(1), 2002]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Autonomous Digital Actors, K. Perlin and G. Seidman. <i>Motion in Games</i>, 2008]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360625</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Clone Attack! Perception of Crowd Variety. Rachel McDonnell, Micheal Larkin, Simon Dobbyn, Steven Collins and Carol O'Sullivan, <i>ACM Trans. on Graphics</i> (SIGGRAPH 2008), 27, (3) (2008)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1462051</ref_obj_id>
				<ref_obj_pid>1462048</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Evaluating the Effect of Motion and Body Shape on the Perceived Sex of Virtual Characters. R. McDonnell, S. Joerg, J. K. Hodgins, F. Newell, and C. O'Sullivan, <i>ACM Trans. on Applied Perception</i>, 5, (4) (2008)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1394294</ref_obj_id>
				<ref_obj_pid>1394281</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Evaluating the emotional content of human motions on real and virtual characters. R. McDonnell, S. Joerg, J. McHugh, F. Newell and C. O'Sullivan, <i>ACM SIGGRAPH Symposium on Applied Perception in Graphics and Visualization (APGV'08)</i>, pp67--74 (2008)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1272726</ref_obj_id>
				<ref_obj_pid>1272690</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Smooth Movers: Perceptually Guided Human Motion Simulation. R. McDonnell, F. Newell and C. O'Sullivan. <i>Eurographics/ACM SIGGRAPH Symposium on Computer Animation (SCA'07).</i> pp259--269, (2007)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Chaminade et al. {2007}: The uncanny valley of eeriness, <i>ACM SIGGRAPH 2007</i> panels Mori {1970}: The valley of eeriness (japanese), <i>Energy</i>, 7 (4), 33--35]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Giese and Poggio, {2003}: Neural Mechanisms for the Recognition of Biological Movements, <i>Nature Reviews/Neuroscience</i>, (4), 179--192]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Johansson {1976}: Spatio-temporal differentiation and integration in visual motion perception, <i>Psychological Research</i>, 38, 379--393]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Johansson {1973}: Visual perception of biological motion and a model for its analysis, <i>Perception and Psychophysics</i>, 14, 2, 201--211]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Pollick et al. {2001}: Recognizing the style of spatially exaggerated tennis serves", <i>Perception</i>, 30, 323--338]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Cutting and Kozlowski {1977}: Recognizing friends by their walk: Gait perception without familiarity cues, <i>Bulletin of the Psychonomic Society</i>, 9, 5, 353--356]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Beardsworth and Buckner {1981}: The ability to recognize oneself from a video recording of ones movements without seeing ones body, <i>Bulletin of the Psychonomic Society</i>, 18, 1, 19--22]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073316</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Ren et al. {2005}: A Data-Driven Approach to Quantifying Natural Human Motion, <i>ACM Transactions on Graphics (SIGGRAPH 2005)</i>, 24(3), 1090--1097]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Perani et al. {2001}: Different brain correlates for watching real and virtual hand actions. <i>NeuroImage</i> 14, 749--758]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Han et al. {2005}: Distinct neural substrates for the perception of real and virtual visual worlds. <i>NeuroImage</i> 24, 928--935]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Mar et al. {2007}: Detecting agency from the biological motion of veridical vs animated agents. <i>Social Cognitive and Affective Neuroscience Advance Access</i> 2, 3, 199--205]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>976087</ref_obj_id>
				<ref_obj_pid>976083</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Nowak and Biocca {2003}: The effect of the agency and anthropomorphism on users' sense of telepresence, copresence, and social presence in virtual environments. <i>Presence</i> 12, 5, 481--494.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>236605</ref_obj_id>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Reeves and Naas {1996}: The media equation: How people treat computers, television, and new media like real people and places. In <i>Stanford</i>, CA, CSLI Publications]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>505808</ref_obj_id>
				<ref_obj_pid>505799</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Slater and Steed {2002}: Meeting people virtually: Experiments in shared virtual environments. In <i>The social life of avatars: Presence and interaction in shared virtual environments</i>, 146--171]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Coulson {2004}: Attributing emotion to static body postures: Recognition accuracy, confusions, and viewpoint dependence. <i>Journal of Nonverbal Behavior</i>, 28, 2, 117--139]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Crane and Gross {2007}: Motion capture and emotion: Affect detection in whole body movement. <i>Affective Computing and Intelligent Interaction</i>, 28, 95--101]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[De Gelder {2006}: Towards the neurobiology of emotional body language. <i>Nature Reviews Neuroscience</i>, 7, 242--249]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Johansson {1973}: Visual perception of biological motion and a model for its analysis. <i>Perception and Psychophysics</i>, 14, 2, 201--211]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Wallbott {1998}: Bodily expression of emotion. <i>European Journal of Social Psychology</i>, 28, 6, 879--896]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Atkinson et al. {2004}: Emotion perception from dynamic and static body expressions in point-light and full-light displays. <i>Perception</i>, 33, 6, 717--746]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Pasch and Poppe {2007}: Person or puppet? the role of stimulus realism in attributing emotion to static body postures. <i>Affective Computing and Intelligent Interaction (ACII)</i>, vol. 4738, 83--94]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Chaminade et al. {2007}: Anthropomorphism influences perception of computer-animated characters' actions, Social Cognitive and Affective Neuroscience, 2, 206--216]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614412</ref_obj_id>
				<ref_obj_pid>614272</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Hodgins et al. {1998}: Perception of Human Motion With Different Geometric Models, IEEE Transactions on Visualization and Computer Graphics, 4(4), 307--316]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[McDonnell et al. {2005}: LOD Human Representations: A Comparative Study, V-Crowds'05, First International Workshop on Crowd Simulation, 101--115]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Reitsma et al. {2008}: Effect of Character Animacy and Preparatory Motion on Perceptual Magnitude of Errors in Ballistic Motion, Computer Graphics Forum (Eurographics), 27(2), 201--210]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1462051</ref_obj_id>
				<ref_obj_pid>1462048</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[McDonnell et al. {2008}: Evaluating the effect of motion and body shape on the perceived sex of virtual characters. ACM Transactions on Applied Perception, 5, (4), 21:1--21:14]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1577757</ref_obj_id>
				<ref_obj_pid>1577755</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[McDonnell et al. {2009}, Investigating the role of body shape in the perception of emotion, <i>ACM Transactions on Applied Perception</i>, 6(3) (In Press)]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Realistic human body movement for emotional expressiveness SIGGRAPH 2009 Course Friday, 7 August 8:30 
AM -12:15 PM Auditorium B Instructors: Aaron Hertzmann, University of Toronto hertzman@dgp.toronto.edu 
Carol O'Sullivan, Trinity College Dublin Carol.OSullivan@cs.tcd.ie Ken Perlin, New York University 
perlin@courant.nyu.edu Course Description: Humans express their emotions in many ways, in particular 
through face, eye and body motion. Therefore, the creators of virtual humans strive to convincingly depict 
emotional movements using a variety of methods. In this course, we focus on the use of realistic human 
body motion in the service of emotional expressiveness. Applications and research relating to procedural 
animation of humans with emotion and personality, biomechanical and physical principles of animation, 
physics-based human motion simulation, and data-driven animation will be reviewed. We will also provide 
some insights from the field of psychology and discuss issues relating to the perception and evaluation 
of realistic human body animation. Prerequisites: Basic knowledge of human animation. Syllabus: Topics 
covered: Perlin: Hand-crafted procedural animation: Goals: "acting" versus "simulation"  Advantages/disadvantages 
of hand-crafted procedural animation  Path planning  Walking / Running  Expressivity of the torso 
 Faces and heads  Arm and hand gestures  Interaction between characters and with objects in the world 
 Connections with A/I, scripting, storytelling  Challenges for the future  Hertzmann: Biomechanical 
and physical principles of human animation: Kinematic animation  Biomechanical principles of locomotion 
 Efficiency of walking  Passive-based walking  Learning objective functions  Real-time control  
O Sullivan: Expressive Human Motion: Evaluation and Perception: Perception of biological motion an 
overview  Emotional body language  Evaluating the effect of body representation  Evaluating emotional 
body motion  Emotional crowds and multisensory cues  Speaker Bios Aaron Hertzmann is an Associate 
Professor of Computer Science at University of Toronto. He received a BA in Computer Science and Art 
&#38; Art History from Rice University in 1996, and an MS and PhD in Computer Science from New York University 
in 1998 and 2001, respectively. In the past, he has worked at University of Washington, Microsoft Research, 
Mitsubishi Electric Research Lab, Interval Research Corporation and NEC Research Institute. His awards 
include the MIT TR100 (2004), an Ontario Early Researcher Award (2005), a Sloan Foundation Fellowship 
(2006), a Microsoft New Faculty Fellowship (2006), and a University of Toronto teaching award (2008). 
His research interests include computer vision, computer graphics, and machine learning. Carol O'Sullivan 
is an Associate Professor at Trinity College Dublin. Her research interests include perception, animation, 
virtual humans and crowds. She has been a member of many IPCs, including the Eurographics and SIGGRAPH 
papers committee and has published over 100 papers in graphics, especially animation and perception. 
She is the programme co-chair of the SIGGRAPH Symposium on Applied Perception in Graphics and Visualization 
2009, co-Editor in Chief of ACM Transactions on Applied Perception and an editorial board member of IEEE 
Computer Graphics &#38; Applications. Amongst other conferences, she co-chaired Eurographics'05 in Dublin 
and SCA 06: Symposium on Computer Animation in Vienna. Ken Perlin is a professor in the Department of 
Computer Science at New York University, directs the NYU Games For Learning Institute. He was also founding 
director of the Media Research Laboratory and director of the NYU Center for Advanced Technology. His 
research interests include graphics, animation, user interfaces, science education and multimedia. Amongst 
many honours, he received an Academy Award for Technical Achievement from the Academy of Motion Picture 
Arts and Sciences for his noise and turbulence procedural texturing techniques, which are widely used 
in feature films and television, as well as the 2008 ACM/SIGGRAPH Computer Graphics Achievement Award. 
Schedule: 8:30 am Introduction / Overview [All] 8:45 am Hand-crafted procedural animation [Perlin] 9:45 
am Biomechanical and physical principles of human animation [Hertzmann] 10:15 am Break 10:30 am Biomechanical 
and physical principles -cont.. [Hertzmann] 11:00am Evaluating expressive human motion, and perceptual 
issues 12:00 am Conclusions/Discussion [All] 12:15 am Close  Representative Bibliography: Active Learning 
for Real-time Motion Controllers, Cooper, S. Hertzmann, A. Popovic, Z. ACM Transactions on Graphics 26(3) 
(SIGGRAPH 2007) Learning Physics-Based Motion Style with Nonlinear Inverse Optimization, C. K. Liu, A. 
Hertzmann, Z. Popovic. ACM Trans. on Graphics (Proc. SIGGRAPH 2005) Style-Based Inverse Kinematics, K. 
Grochow, S. L. Martin, A. Hertzmann, Z. Popovic. ACM Trans. on Graphics (Proc. SIGGRAPH 2004) Real Time 
Responsive Animation with Personality, K Perlin -IEEE Trans on Visualization and Comp. Graphics, 1995 
Improv: a system for scripting interactive actors in virtual worlds, K.Perlin and A. Goldberg. SIGGRAPH 
1996 Building Virtual Actors Who Can Really Act, K. Perlin. Virtual Storytelling, LNCS, 2003. Better 
acting in computer games: the use of procedural methods. K. Perlin, Computers and Graphics 26(1), 2002 
Autonomous Digital Actors, K. Perlin and G. Seidman. Motion in Games, 2008 Clone Attack! Perception of 
Crowd Variety. Rachel McDonnell, Micheal Larkin, Simon Dobbyn, Steven Collins and Carol O'Sullivan, ACM 
Trans. on Graphics (SIGGRAPH 2008), 27, (3) (2008) Evaluating the Effect of Motion and Body Shape on 
the Perceived Sex of Virtual Characters. R. McDonnell, S. Joerg, J. K. Hodgins, F. Newell, and C. O'Sullivan, 
ACM Trans. on Applied Perception, 5, (4) (2008) Evaluating the emotional content of human motions on 
real and virtual characters. R. McDonnell, S. Joerg, J. McHugh, F. Newell and C.O'Sullivan, ACM SIGGRAPH 
Symposium on Applied Perception in Graphics and Visualization (APGV'08), pp67-74 (2008) Smooth Movers: 
Perceptually Guided Human Motion Simulation. R. McDonnell, F. Newell and C. O'Sullivan. Eurographics/ACM 
SIGGRAPH Symposium on Computer Animation (SCA'07). pp259-269, (2007) Handd-Crafted PProcedura l Animatioon 
SIGGRAPH 2009 Coursee Notes: Realistic Humann Body Moveement for Emmotional Exppressivenesss KKen Perlin 
Quick Overv Data driven Dynamics / Hand-crafte view of Appr / learning b physics base ed roaches to P 
based ed rocedural A nimation Simple Exammple / Demoo  Goals: "Acti ng" versus " Simulation" From a 
high level user's view From an impplementatioon view Roles of Ressponsibility Different peeople have 
d ifferent skillls  Walking / Ruunning Feet  User-levvel paramet ric control  Mechannisms of impplementationn 
 Physical-world consstraints   Effect on leggs, pelvis, uppper body, a rms  Expressivity of the Torso 
 The "line" of the body  Location of the "acting center"  Sitting and standing  Mapping Same expression 
to different Morphologies Leaning   Faces and Heads High level expressivity  Salient parameters -coarse 
to fine  Implementation Arm and Hand Gestures  Expressivity  Grasping and holding  Hand Articulation 
 Path Planning Alternate approaches  Algorithmic costs  Obstacle avoidance  Dynamic scenes/objects 
Foot Steps  Low Level Personality Body Style Parameter settings High Level Personality  Acting Choices 
 Director's View  Interaction Between characters  Blocking  Interaction with objects in the world 
 Connections with A/I, scripting, storytelling Emotive Direction over Time  Advantages/Disadvantages 
of Handcrafted Procedural Animation Fidelity versus simplicity  Scott McCloud's realism scale  Perils 
of the uncanny valley Compromised dynamics  More flexible and controllable  CPU  Less than physical 
simulation  More than pre-created animation  Memory Orders of magnitude less than animation Users/Uses 
 Procedural Crowds  Animation/Film  Simulation  Games  Pre-Vis  Architectural Walkthroughs  Live 
Performance Challenges for the Future   Biomechanical Principles of Motion SIGGRAPH 2009 Course Notes: 
Realistic Human Body Movement for Emotional Expressiveness Aaron Hertzmann May 20, 2009 Despite some 
early enthusiasm, early attempts at physics-based character animation foundered due to the di.culty of 
creating realistic and expressive models, as well as signi.cant computational burdens. In comparison, 
motion capture and keyframe animation can give good results, provided enough time and e.ort are spent 
with them. However, neither method gives a fully realistic and .exible model of motion that can be used 
to generate highly-realistic motions in new circumstances. For example, motion capture provides little 
ability to create new motions that are very di.erent from the data, and modi.cations of mocap su.er noticeable 
violations of physics, such as footskate and implausibly large forces. There is now a small but growing 
resurgence of interest in physics-based character animation. Whereas mocap and animation systems require 
signi.cant labor to create motion, physics-based animation o.ers the promise of greater generality and 
.exibility. Many open research problems remain before physics-based animation becomes practical. The 
principles of human motion connect many .elds of scienti.c research, including biomechanics, optimal 
control, machine learning, robotics, motor neuroscience, psychology, and others, as well as theatre, 
animation, and dance. Each of these .elds can give a di.erent perspective on motion, each of which is 
useful for understanding how we move. This lecture aims to survey the most relevant principles from these 
areas, with an emphasis on human locomotion (especially walking). Even from a physical point of view, 
there are many ways we can look at motion. We can inspect all the individual forces involved in a motion, 
or we can look at the forces on the system as a whole and the center-of-mass, or we can look at higher-level 
properties such as energy and work. Research questions: why do we move the way we do, and what role does 
physics play? How do we model human rewards and objectives in movement? How can we deal with the di.cult 
optimization problems that arise? How do we use physical models in artistic contexts? Basic physical 
models and simulation The body is typically modeled as an articulated rigid-body system. Most of the 
details here are standard in physical simulation, and well-known at SIGGRAPH. However, di.erent choices 
in the body model a.ect the body s natural modes of movement.  For example, even topological choices 
make a di.erence: you cannot have toe-o. a crucial component of walking without toes. Yet most physics-based 
models to date do not have toes. As a further thought experiment, imagine (or observe) walking in ski 
boots (where the toe is rigid and the ankle is nearly rigid), as compared to walking barefoot, in formal 
shoes, or high heels.  When building a physical model, one has a number of choices in terms of degree 
of realism. Simpler methods are usually adequate for very simple problems (e.g., ragdoll simulation), 
but usually fail at capturing nuances of active locomotion, especially when optimizing motion and the 
motion can exploit modeling approximations and errors.  An important choice is constraint handling, 
at contacts and joint limits.  One can use exact constraint handling, or penalty models. While exact 
methods are much more complex, penalty methods can lead to serious issues, especially if motion is being 
opti­mized. For example, if a penalty-method is used for ground contact, the character can walk e.ciently 
by bouncing on the ground as if walking on a trampoline. It is usually easiest to begin with simple models, 
but be prepared to move to more complex models when problems arise.   Parameterization either with 
generalized coordinates q (i.e., joint angles and root position/orientation), or positions of each link. 
Latter has simple equations of motion, but requires rigid-link constraints, which is usually more trouble 
than it s worth.  Other important constraints: joint limits, torque limits, interpenetration constraints 
(these are usually ignored).  Joint angle parameterization: it is often observed that bad parameterization 
leads to bad motion, and this is true for simulation, and especially control. On every physics-based 
animation project that I have worked on or witnessed, the student begins with Euler angles because they 
are sim­plest, and eventually concludes that they have too many problems, and eventually ends up using 
exponential maps.  Segment (link) masses: these can be taken from standard tables. These tables usually 
show little variance in relative masses (e.g., proportion of arm mass to total body mass), and so can 
be used to scale according to body mass. However, this data may be somewhat limited in range; e.g., datasets 
based on cadavers of military personnel. Segmental parameters may be heuristically used to determine 
inertia tensors.  Given the physical model, simulation may be performed using the robotics equations-of-motion 
 1 (EOMs), usually written: M(q)q¨+ C(q, q.)+ G(q)= t (1) where M is the mass matrix, C is Coriolis/centrifugal 
forces, and G are gravitational forces, and t are the joint torques. These equations can be thought of 
as a generalized form of Newton s second law = ma, where the individual terms depend on the current con.guration 
q and generalized velocities q.. A controller is a mapping from state q, q.to torques t: t = f(q, q.) 
(2) The controller may have a persistent internal state as well, e.g., a state machine. The big question 
in simulation of characters is: what are the joint torques t? This depends on the character s control 
mechanism, and will be discussed in more detail below. Given the body model, a controller, and an initial 
state (q0, q.0) the basic steps of simulation are: 1. Compute control torques from the controller 2. 
Solve EOMs to estimate accelerations q¨ 3. Simulate forward one time-step, while resolving ground contacts, 
as necessary. Explicit or implicit integration may be used.   The EOMs and their solution can be derived 
either from Lagrange s Principle of Least of Action, or using Featherstone s Algorithm. Featherstone 
s algorithm is the most e.cient algorithm, though more involved, and there is no single tutorial paper/book 
that explains every aspect of it clearly (see Mirtich s thesis, the review by Featherstone and Orin, 
and Featherstone s book).  In practice, detailed solution of the EOMs are too complex to do by hand, 
especially since you will often need to experiment with di.erent parameterizations, ways of handling 
contact and so on. Hence, it is usually necessary to employ a symbolic math data structure (often home-grown) 
that can compute the EOMs, necessary Jacobians, and derivatives automatically.  2 2 Sources of data 
 When modeling motion it is crucial to look at real movement. Generate hypotheses by watching motion. 
Evaluate models by comparing them to new data.  Sources of data: video, motion capture, force plate 
data, electromyography (measures muscle activation)  There is no existing way to measure every force, 
mass, and velocity in the body. Some biome­chanicists have turned to physical simulation as one way to 
test hypotheses.  Personal laboratory: you can learn a lot simply by observing your own motion as you 
move around; sit in a co.eeshop and watch people walk. What trajectories do your feet take when they 
walk? When are they applying forces? What role does arm-swing play: if you hold your arms rigid by your 
sides and try to walk or jog, what happens? etc.  3 Optimality principles of movement Evolution has 
an amazing ability to craft biological systems to get ahead and multiply. This can be mathematically 
modeled as a process of optimization. The most natural objective function to describe biological organisms 
is reproductive .tness: e.g., number of grandchildren. But we can often use more short-term goals that 
organisms might have. Optimality principles have been used to explain many aspects of biological systems, 
such as bone densities, foraging behaviors, to movement.  What is the objective function? Basic concerns 
in movement include cost-of-transport, stability, balance, and achieving goals (e.g., get from one place 
to another).  Optimality, in principle, allows signi.cant generalization that other models cannot: if 
you know my objective function, then you should be able to predict how I move in a wide variety of new 
situations, just by adding appropriate new constraints.  The cost of transport is de.ned as the energy 
used divided by distance travelled. Note that measuring external work is a lower-bound (e.g., if you 
end up where you started, your total work is zero, but you have consumed energy nonetheless). Dimensionless 
cost of transport (cost of transport divided by weight) can be used to compare e.ciency di.erent robots/characters. 
 In experiments with humans, energy consumption is normally measured in terms of the amount of oxygen 
consumed.  Two main approaches:  Trajectory optimization ( Spacetime constraints ): obtain the single-best 
trajectory. Re­quires o.ine optimization/planning, and cannot respond to real-time commands and pertur­bations 
(e.g., new forces, user control)  Control synthesis/Optimal Control: de.ne a mapping from state to torques. 
Can theoretically run in real-time, but needs to be able to plan ahead. More on this below.   Limitations 
of optimality theory:  Optimality can be controversial in biology; not all evolution is adaptation. 
 We are not always optimal, e.g., compare a novice athlete or dancer to an expert. If one receives poor 
training, one may converge to a poor local minimum, e.g., you can get very good at snowplow skiing without 
ever making the leap to parallel. Dance instructors try to get you out of bad habits as soon as they 
can. For such reasons, optimality approaches can be controversial.  Simple objective terms say nothing 
about style (e.g., social signalling behaviors). We can sometimes embed style in physics: e.g., dragging 
your feet by modeling shoes as literally heavy; jaunty, bouncing style as the product of overdamped spring-mass 
systems.  Local optima and search problems are formidable: search space is very high-dimensional, dynamics 
are nonlinear (esp. due to ground contact), local optima are very prevalent. For low-energy motions (such 
as walking), the local minima problems get even worse.   3 4 Phases of walking It is useful to observe 
in detail the phases of walking. (You can .nd every aspect of it observed in minute detail in the biomechanics 
literature; much biomechanics research is very descriptive).  Walking: Swing phase, heel strike, full-foot 
contact, roll, heel-o., toe-o.. Try this at home.  The foot: extremely complex; the more you can model, 
the better. Ankle provides approx 53% of energy consumption during walking.  5 Simpli.ed Mechanical 
Models Study simpli.ed mechanical systems for insight into movement  Most powered robots (e.g., Asimo) 
appear very sti. and energy e.cient; they use very ine.cient control strategies.  Inverted pendulum: 
a simple one-parameter system matches human center-of-mass (COM) motion in walking  Bouncing ball: matches 
human COM during running  Passive-dynamic walking: McGeer showed a simple passive robot that walks downhill, 
powered only by gravity. Very similar to human movement. Much work on analysis of motion and stability 
has been done in the context of these simpli.ed 2D models; e.g., see work by Art Kuo.  Passive-based 
walking: Collins et al. describe level-ground walkers based on passive principles. These robots are extremely 
energy e.cient as compared to commercial robots, and only a little less e.cient than humans.  Templates 
and anchors: an approach to simplifying locomotor systems to their essence, which allows comparing related 
biological models (e.g., di.erent animals that move in similar ways)  Relevance for animation: da Silva 
et al. and Brubaker et al. demonstrate animation/tracking models that relate a low-degree-of-freedom 
mechanical model to a high-DOF kinematic model (without physics). Provides a natural dimension reduction 
or mechanical model, but may be di.cult to enforce consistency between the models, especially when you 
deviate from the model.  Kry et al. apply modal analysis to obtain a low-dimensional model of locomotion 
(though ignoring contacts).  4 6 Muscles, Bones, Ligaments The detailed workings of muscles, bones, 
ligaments, and tendons is itself quite involved, and the subject of many papers in both biomechanics 
and animation. One often abstracts away these details for full-body motion, with a corresponding loss 
of .ne detail. Many highly-detailed models of individual substructures (e.g., neck, knee, hands, face, 
etc.) have been devised, illustrating the importance of these body parts  Muscles supply active forces. 
Muscles attach to bones via tendons, and bones connect to each other via ligaments.  Muscles, tendons, 
and ligaments all have passive elastic (spring and damper properties). Compres­sion and expansion of 
passive elements conserves about 30% of energy during running.  Muscles can only contract (pull, not 
push). Muscles come in agonist/antagonist pairs: one muscle pulls one way, and the other pulls the other 
way. Both muscles can contract to create sti.­ness/tension. Sti.ness depends on task, e.g., compare walking 
on cement to sand/mud or ice, and has a noticeable a.ect on style and expressiveness of motion.  Bones 
connect and move relative to each other in fairly complex ways. For example, one might model the knee 
as a simple hinge joint for full-body motion, but the actual range of movement and degrees of freedom 
are quite a bit more complex.  Muscles, tendons, and ligaments may attach to bones in various ways which 
a.ect their mechanical e.ciency for various tasks, e.g., a muscle may have better leverage to move the 
bone in one direction than another.  Muscle activation models are based on somewhat simple experiments, 
e.g., extract a muscle from a frog and run a current through it in the laboratory to see how much it 
contracts.  The Hill muscle model is often used in biomechanics as an approximate muscle model of suitable 
accuracy. It includes spring and damper elements in parallel and in series with an active force element. 
 Tendons, muscles, and ligaments can tear when overly stressed. It is theorized that avoiding injury 
is a signi.cant factor in some animals preferred movements.  Fingers move in concert. Try to .ex your 
index .nger all the way without moving any other .nger.  7 Building controllers Mapping from state 
to actions: t = f(q, q.), possibly with persistent state.  Joint-space control: PD-servo at each DOF: 
 t = ks(q - q¯) + kdq. (3) where ks is the positional gain (analogous to a spring constant), and kd 
is damping coe.cient. SIMBICON: state machine model, with PD control at each joint, plus balance controller. 
Hand­tuned; produces robust, stable motions, but lacking several key features of human motion (e.g., 
long strides).  It is very di.cult to tune joint-space controllers for complex tasks with coordinated 
motions, where all joints are highly interdependent. How does the activation of my knee a.ect the style 
of my gait?  As a general non-linear mapping, an arti.cial neural network or basis function representation 
may be used.  Commercial robotic walking (ASIMO, zero-moment point control). Very conservative; much 
less e.cient than human motion.  Online optimization control: at each instance, optimize a local objective 
function based on task goals (such as COM trajectory, matching mocap, end-e.ector constraints)  Composable 
controllers  5 Optimal control Given a high-level reward/cost function: determine controller that 
minimizes an objective function.  Movement is subject to random variations of many types, e.g., nervous 
system noise, random variations in ground contact, unexpected external perturbations, sensory ambiguity, 
etc. (Without uncertainty, all we need is trajectory optimization).  Objective function: achieve goals 
(e.g., don t fall down), minimize e.ort, or expected value of reward. Analogy to rational choice economics. 
 Classical optimal control gives theoretically-optimal control, but makes unrealistically restrictive 
assumptions (e.g., linear dynamics).  Machine learning approaches to control try to learn from experience, 
but usually lack theoretical guarantees  Reinforcement learning is machine learning in which agents 
attempt to update controllers from experience, including methods such as Q-learning and value iteration. 
While guaranteed to work for any system, in practice they have a reputation for being impractical for 
systems with non­trivial dimensionality. (The term curse of dimensionality was coined by Bellman, an 
economist, to describe the di.culty of modeling value functions in high-dimensional action or state spaces). 
 Monte Carlo policy evaluation: given a stochastic system and a reward function, average many simulations 
to approximate expected reward.  LQR (linear-quadratic regulator): optimal for linear systems with quadratic 
reward functions. DDP (di.erential dynamic programming) uses LQR based on linearization around an optimal 
trajectory.  6 We are all becoming increasingly familiar with, and accepting of computer generated 
characters. Yet, even in this highly-developed application domain, some suggest that virtual humans have 
fallen into an uncanny valley , which was first described as a negative bias for robots when they become 
extremely close to emulating humans in motivation, characteristics or behaviour [Chaminade et al., 2007; 
Mori, 1970]. The realism of virtual humans is clearly dependent to a significant degree on their physical 
appearance, i.e., the quality of the modelling and rendering of their bodies. However, it is in the synthesis 
of their motions, such as walking, running, smiling, talking, that the biggest research challenges remain. 
If this is true even for the creation of movies where significant resources in terms of skilled animators, 
computational power and motion databases can be allocated to the simulation of realistic humans then 
the problems are significantly magnified for human animation in interactive applications running on commodity 
hardware (such as PCs, game consoles, or even mobile phones). Furthermore, simulating motions that have 
personality and individual characteristics, without capturing that motion directly from a real human, 
adds whole new dimensions of complexity. However, it is vital to achieve this goal if we wish to create 
truly compelling virtual humans with emotional expressiveness. Chaminade et al. [2007]: The uncanny valley 
of eeriness, ACM SIGGRAPH 2007 panels Mori [1970]: The valley of eeriness (japanese), Energy, 7 (4), 
33-35 As humans, we are very sensitive to subtle properties of human movements and actions. In fact, 
the survival of many species is dependent on their ability to recognise complex movements (e.g., of predators, 
prey and potential mates), as noted by Giese and Poggio. They present a comprehensive review that clearly 
demonstrates how the brains of humans and other primates have developed to a high level of performance 
for biological motion recognition. For example, neurons have been discovered that selectively respond 
to full­body, mouth and hand motion, and facial expressions, while so-called mirror neurons have been 
found that respond both when watching an action and performing it. Giese and Poggio, [2003]: Neural Mechanisms 
for the Recognition of Biological Movements, Nature Reviews/Neuroscience, (4), 179-192 Research in experimental 
psychology has also demonstrated how finely tuned the human brain is at recognising biological motions 
and distinguishing between very subtle characteristics of this motion. Johansson [1973,1976] illustrated 
that natural motion, in the absence of any spatial information, is a sufficient cue to recognise the 
sex of a walker. His point-light displays were designed to separate biological motion information from 
other sources of information that are normally intermingled with the motion of a human, such as form 
or outline. He showed that 12 moving light points suffice to create a rich perception of a moving human 
figure, within a very short space of time (200msec, or five frames of a movi)ie). MMany studies siince 
hhave usedd point-lightht displlays ttoshhow ththatt biollogicall motion tdi itlidi bii ti perception, 
an extreme example of sophisticated pattern analysis in the brain, extends even further, e.g., to distinguishing 
between different movement styles [Pollick et al. 2001] and recognising a particular walker [Cutting 
&#38; Kozlowski 1977] or even one s own walking pattern [Beardsworth &#38; Buckner 1981]. Johansson [[1976]]: 
Sppatio-tempporal differentiation and integgration in visual motion perception, Psychological Research, 
38, 379 393 Johansson [1973]: Visual perception of biological motion and a model for its analysis, Perception 
and Psychophysics, 14, 2, 201 211 Pollick et al. [2001]: Recognizing the style of spatially exaggerated 
tennis serves , Perception, 30, 323-338 Cutting and Kozlowski [1977]: Recognizing friends by their walk: 
Gait perception without familiarity cues, Bulletin of the Psychonomic Society, 9, 5, 353 356 Beardsworth 
and Buckner [1981]: The ability to recognize oneself from a video recording of ones movements without 
seeing ones body, Bulletin of the Psychonomic Society, 18, 1, 19 22 Ren et al. attempted to derive an 
automatic quantitative metric for the naturalness of human motion by performing a statistical analysis 
of a large motion capture database. Their metric allowed the particular areas where unnatural motion 
was present to be pin-pointed. Ren et al. [2005]: A Data-Driven Approach to Quantifying Natural Human 
Motion, ACM Transactions on Graphics (SIGGRAPH 2005), 24(3), 1090-1097 Evidence from the fields of psychology 
and neuroscience has shown that different neural networks are activated when presented with real and 
virtual stimuli, e.g., a real hand vs. a virtual reproduction [Perani et al 2001] or cartoons vs. movie 
clips [Han et al. 2005] . Findings suggest that the human brain functions in a different way when interacting 
with real people in everyday life than with artificial characters or static social stimuli. When identical 
biological motion from animated and live-action footage of a movie was displayed to participants, a higher 
level of activity was found in visual processing areas for real stimuli [Mar et al. 2007]. Other studies 
have shown that people can respond socially to human and nonhhuman entititities [Reeves &#38;&#38; NNaas 
19961996, SlSlater &#38;&#38; StSteedd 20022002]] and thatt they can engage [R t dthth with virtual 
humans whether or not they look human [Nowak &#38; Biocca 2003]. Perani et al. [2001]: Different brain 
correlates for watching real and virtual hand actions. NeuroImage 14, 749 758 Han et al. [2005]: Distinct 
neural substrates for the perception of real and virtual visual worlds. NeuroImage 24, 928 935 M tl[2007] 
Dt ti f th bi l i l ti f idi l i td Mar et al. [2007]: Detecting agency from the biological motion of 
veridical vs animated agents. Social Cognitive and Affective Neuroscience Advance Access 2, 3, 199 205 
Nowak and Biocca [2003]: The effect of the agency and anthropomorphism on users sense of telepresence, 
copresence, and social presence in virtual environments. Presence 12, 5, 481 494 Reeves and Naas [1996]: 
The media equation: How people treat computers, television, and new media like real people and places. 
In Stanford, CA, CSLI Publications Slater and Steed [2002]: Meeting people virtually: Experiments in 
shared virtual environments. In The social life of avatars: Presence and interaction in shared virtual 
environments, 146 171 When an emotional body motion is seen, it can unambiguously signal to the viewer 
not only what emotion the individual is feeling, but also what the reason and resulting actions are (e.g., 
a terrified body motion of somebody running to escape a predator). De Gelder [2006] presents a thorough 
survey on the perception of Emotional Body Language and the ways in which emotional whole-body movements 
are perceived and processed in the human brain. In the EBA literature, the responses to experiments which 
aim to recognize emotions from body motions are highly consistent [e.g., Wallbott98, Coulson04, Crane07]. 
It is widely accepted that people can recognize human motion from even a very small set of cues [J[Johhansson73]. 
73] Coulson [2004]: Attributing emotion to static body postures: Recognition accuracy, confusions, and 
viewpoint dependence. Journal of Nonverbal Behavior, 28, 2, 117 139 Crane and Gross [2007]: Motion capture 
and emotion: Affect detection in whole body movement Affective Computing and Intelligent Interaction 
28 95 101101 movement. Affective Computing and Intelligent Interaction, 28, 95 De Gelder [2006]: Towards 
the neurobiology of emotional body language. Nature Reviews Neuroscience, 7, 242-249 Johansson [1973]: 
Visual perception of biological motion and a model for its analysis. Perception and Psychophysics, 14, 
2, 201 211 Wallbott [1998]: Bodily expression of emotion. European Journal of Social Psyychology, 28,,,6, 
879 896 gy, Atkinson et al. [2004] showed that a small number of cues are enough to recognize every 
emotion with an above chance rate for point-light animations. They confirmed that emotions can be recognized 
from body motion even when static form is minimized by use of point­lights. Moreover, they found that 
the rated intensity of the motion depends more on the motion than on the shape, since point-light motions 
and real footage were equally rated. Pasch and Poppe [2007] compare the interpretation of static body 
postures for a realistic virtual character and a mannequin. They show that realism has an effect on the 
perception of body postures representing the basic emotions, with very different results for each emotition. 
Atkinson et al. [2004]: Emotion perception from dynamic and static body expressions in point-light and 
full-light displays. Perception, 33, 6, 717 746 Pasch and Poppe [2007]: Person or puppet? the role of 
stimulus realism in attributing emotion to static body postures. Affective Computing and Intelligent 
Interaction (ACII), vol. 4738 83 94 4738, 83 94 Chaminade et al. [2007] investigated how the appearance 
of computer animated characters influenced the perception of their actions. They found that the perceived 
biological nature of a motion decreased with characters anthropomorphism. Chaminade et al. [2007] : 
Anthropomorphism influences perception of computer-animated characters' actions, Social Cognitive and 
Affective Neuroscience, 2, 206-216 Hodgins et al. [1998] showed that the geometric model type used to 
represent the human affected people s ability to perceive the difference between two human motions. McDonnell 
et al. [2005] extended these studies and found that people were most sensitive to differences in human 
motions for high-resolution geometry and impostor (i.e., image based) representations, less sensitive 
for low resolution geometry and stick figures, and least sensitive for point-light representations. Finally, 
Reitsma et al. [2008] demonstrated that participants were more sensitive to errors in motion displayed 
on human characters than on less detailed human figures or simple geometric objects. Hodgins et al. [1998]: 
Perception of Human Motion With Different Geometric Models, IEEE Transactions on Visualization and Computer 
Graphics, 4(4), 307-316 McDonnell et al. [2005]: LOD Human Representations: A Comparative Study, V-Crowds 
05, First International Workshop on Crowd Simulation, 101-115 Reitsma et al. [2008]: Effect of Character 
Animacy and Preparatory Motion on Perceptual MMagniitudeoff Errors iin BBlli allistiic Motion, CComputer 
GGraphi hics FForum ((EEurographics)), 27(2), d E Mti hi 27(2) 201 -210 McDonnell et al. [2008] found 
that both form and motion influence sex perception of virtual characters: for neutral synthetic motions, 
form determines perceived sex, whereas natural motion affects the perceived sex of both androgynous and 
realistic forms. A second investigation into the influence of body shape and motion on realistic male 
and female models showed that adding stereotypical indicators of sex to body shapes influenced sex perception. 
Exaggerated female body shapes influences sex judgements more than exaggerated male shapes. McDonnell 
et al. [2008]: Evaluating the effect of motion and body shape on the perceived sex of virtual characters. 
ACM Transactions on Applied Perception, 5, (4), 21:1 -21:14 In order to analyze the emotional content 
of motions portrayed by different characters, we created real and virtual replicas of an actor exhibiting 
six basic emotions: sadness, happiness, surprise, fear, anger and disgust. In addition to the video of 
the real actor, his actions were applied to five virtual body shapes: a low and high resolution virtual 
counterpart, a cartoon-like character, a wooden mannequin, and a zombie-like character. In a point light 
condition, we also tested whether the absence of a body affected the perceived emotion of the movements. 
Participants were asked to rate the actions based on a list of 41 more complex emotions. We found that 
the perception of emotional actions is highly robust and to the most part independent of the character's 
body so long as form is present. When motion alone is present, emotions were generally perceived as less 
intense than in the cases where form was present. MMcDDonnell et all. [2009], IInvestiigatiing thhe 
rolle of boddy shhape iin the perceptiion off ll [2009] fb h emotion, ACM Transactions on Applied Perception, 
6(3) (In Press) Some examples of the emotional animations used in McDonnell et al. (2009) Participants 
task: categorise visually presented emotion 5x3 Within Subjects Design 5 proportions of emotion (0%-100%) 
3 sound (majority, minority, absent) Conclusions: Conclusions: Perception of emotion in virtual humans 
less efficient than in real  Emotion is recognisable in crowds of virtual humans  Perception of visual 
emotion in crowds affected by sound  Implications for simulating social crowds and groups  McHugh and 
Newell [2009]: Audition can modulate the visual perception of the emotion of a crowd, IMRF 09: International 
Multisensory Research Forum. Acknowledgements: Thanks to the other members of the Metropolis team in 
Trinity College Dublin, in particular Rachel McDonnell Micheal Larkin Peter Loksek Joanna McHugh and 
particular Rachel McDonnell, Micheal Larkin, Peter Loksek, Joanna McHugh and Fiona Newell. Metropolis 
is funded by Science Foundation Ireland.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1667260</article_id>
		<sort_key>210</sort_key>
		<display_label>Article No.</display_label>
		<display_no>21</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>21</seq_no>
		<title><![CDATA[Scattering]]></title>
		<page_from>1</page_from>
		<page_to>397</page_to>
		<doi_number>10.1145/1667239.1667260</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1667260</url>
		<abstract>
			<par><![CDATA[<p>Most of current computer-generated imagery represents scenes with clear atmospheres, neglecting light scattering effects, and most computer-vision systems have not enjoyed success when deployed in uncontrolled outdoor environments. Nevertheless, scattering is a fundamental aspect of light transport in a wide range of applications, whether simulating it or interpreting it, from medical imaging to driving simulators or underwater imagery. This course addresses the challenges that arise with light scattering in computer graphics and computer vision. Topics include: appearance modeling, underwater imagery, vision in bad weather, and measurement techniques.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1797165</person_id>
				<author_profile_id><![CDATA[81100022708]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Diego]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gutierrez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universidad de Zaragoza]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797166</person_id>
				<author_profile_id><![CDATA[81100389194]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wojciech]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jarosz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Research, Zurich]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797167</person_id>
				<author_profile_id><![CDATA[81100639133]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Craig]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Donner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Columbia University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797168</person_id>
				<author_profile_id><![CDATA[81100599730]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Srinivasa]]></first_name>
				<middle_name><![CDATA[G.]]></middle_name>
				<last_name><![CDATA[Narasimhan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1073308</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Donner, C., and Jensen, H. W. Light diffusion in multi-layered translucent materials. <i>24</i>, 3 (2005), 1032--1039.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383946</ref_obj_id>
				<ref_obj_pid>2383894</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Donner, C., and Jensen, H. W. A spectral BSSRDF for shading human skin. In <i>Rendering Techniques</i> (2006), pp. 409--417.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1409093</ref_obj_id>
				<ref_obj_pid>1409060</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Donner, C., Weyrich, T., d'Eon, E., Ramamoorthi, R., and Rusinkiewicz, S. A layered, heterogeneous reflectance model for acquiring and rendering human skin. <i>27</i>, 5 (2008), 1--12.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360671</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Fattal, R. Single image dehazing. <i>ACM Transactions on Graphics (SIGGRAPH 2008) 27</i>, 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1272693</ref_obj_id>
				<ref_obj_pid>1272690</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Gupta, M., and Narasimhan, S. G. Legendre fluids: A unified framework for analytic reduced space modeling and rendering of participating media. In <i>Eurographics/ACM SIGGRAPH Symposium on Computer Animation (2007)</i> (August 2007).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Gupta, M., Narasimhan, S. G., and Schechner, Y. Y. On controlling light transport in poor visibility environments. In <i>Proceedings IEEE CVPR</i> (June 2008).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Gutierrez, D., Munoz, A., Anson, O., and Seron, F. Non-linear volume photon mapping. <i>Rendering Techniques (Eurographics Symposium on Rendering</i>), 291--300.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Gutierrez, D., Seron, F., Anson, O., and Munoz, A. Visualizing underwater ocean optics. <i>Computer Graphics Forum (EUROGRAPHICS 2008) 27</i>, 2, 547--556.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073266</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Hawkins, T., Einarsson, P., and Debevec, P. Acquisition of time-varying participating media. <i>ACM Transactions on Graphics (SIGGRAPH 2005) 24</i>, 3 (2005).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383319</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Jensen, H. W., Marschner, S. R., Levoy, M., and Hanrahan, P. A practical model for subsurface light transport. In <i>Proceedings of SIGGRAPH 2001</i> (2001), pp. 511--518.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Joshi, N., Donner, C., and Jensen, H. W. Noninvasive measurement of scattering anisotropy in turbid materials by nonnormal incident illumination. <i>31</i> (2006), 936--938.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015806</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Levoy, M., Chen, B., Vaish, V., Horowitz, M., McDowall, I., and Bolas, M. Synthetic aperture confocal imaging. <i>ACM Transactions on Graphics (SIGGRAPH 2004) 23</i>, 3, 825--834.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Narasimhan, S., and Nayar, S. Structured light methods for underwater imaging: light stripe scanning and photometric stereo. <i>OCEANS, 2005. Proceedings of MTS/IEEE</i> (2005), 2610--2617 Vol. 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141986</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Narasimhan, S. G., Gupta, M., Donner, C., Ramamoorthi, R., Nayar, S. K., and Jensen, H. W. Acquiring scattering properties of participating media by dilution. <i>ACM Transactions on Graphics (SIGGRAPH 2006) 25</i>, 3 (2006).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Narasimhan, S. G., and Nayar, S. Interactive deweathering of an image using physical models. In <i>IEEE IEEE Workshop on Color and Photometric Methods in Computer Vision, In Conjunction with ICCV</i> (October 2003).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>598511</ref_obj_id>
				<ref_obj_pid>598432</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Narasimhan, S. G., and Nayar, S. K. Vision and the atmosphere. <i>IJCV 48</i>, 3 (2002), 233--254.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2286491</ref_obj_id>
				<ref_obj_pid>2286436</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Narasimhan, S. G., and Nayar, S. K. Contrast restoration of weather degraded images. <i>IEEE PAMI 25</i>, 6 (June 2003), 713--724.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1965928</ref_obj_id>
				<ref_obj_pid>1965841</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Narasimhan, S. G., and Nayar, S. K. Shedding light on the weather. In <i>Proceedings of the 2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</i> (June 2003), vol. 1, pp. 665--672.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Schechner, Y. Y., and Karpel, N. Recovery of underwater visibility and structure by polarization analysis. <i>Oceanic Engineering, IEEE Journal of 30</i>, 3 (July 2005), 570--587.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Schechner, Y. Y., Narasimhan, S. G., and Nayar, S. K. Instant dehazing of images using polarization. In <i>Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</i> (June 2001), vol. 1, pp. 325--332.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566612</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Sloan, P.-P., Kautz, J., and Snyder, J. Precomputed radiance transfer for real-time rendering in dynamic, low-frequency lighting environments. <i>ACM Transactions on Graphics (SIGGRAPH 2002) 21</i>, 3 (2002).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073309</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Sun, B., Ramamoorthi, R., Narasimhan, S. G., and Nayar, S. K. A practical analytic single scattering model for real-time rendering. <i>ACM Transactions on Graphics (SIGGRAPH 2005)</i> (August 2005).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141987</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Weyrich, T., Matusik, W., Pfister, H., Bickel, B., Donner, C., Tu, C., McAndless, J., Lee, J., Ngan, A., Jensen, H. W., and Gross, M. Analysis of human faces using a measurement-based skin reflectance model. <i>25</i> (2006), 1013--1024.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>358644</ref_obj_id>
				<ref_obj_pid>358636</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Ashikhmin, M., and Shirley, P. 2000. An anisotropic phong model. <i>Journal of Graphics Tools 5</i>, 2, 25--32.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>628884</ref_obj_id>
				<ref_obj_pid>628335</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Basri, R., and Jacobs, D. W. 2003. Lambertian reflectance and linear subspaces. <i>IEEE Trans. Pattern Anal. Mach. Intell. 25</i>, 2, 218--233.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Biri, V., Michelin, S., and Arques, D. 2004. Real-time single scattering with shadows. In <i>In review http://igm.univ-mlv.fr/~biri/indexCA_en.html.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>801255</ref_obj_id>
				<ref_obj_pid>800064</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Blinn, J. 1982. Light reflection functions for simulation of clouds and dusty surfaces. In <i>SIGGRAPH 82</i>, 21--29.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Chandrasekhar, S. 1960. <i>Radiative Transfer.</i> Oxford Univ. Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280864</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Debevec, P. 1998. Rendering synthetic objects into real scenes: bridging traditional and image-based graphics with global illumination and high dynamic range photography. In <i>SIGGRAPH 98</i>, 189--198.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>569060</ref_obj_id>
				<ref_obj_pid>569046</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Dobashi, Y., Yamamoto, T., and Nishita, T. 2002. Interactive rendering of atmospheric scattering effects using graphics hardware. In <i>Graphics Hardware Workshop 02</i>, 99--109.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166139</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Hanrahan, P., and Krueger, W. 1993. Reflection from layered surfaces due to subsurface scattering. In <i>SIGGRAPH 93</i>, 165--174.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Harris, M., and Lastra, A. 2001. Real-time cloud rendering. In <i>Eurographics 2001</i>, 76--84.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>957189</ref_obj_id>
				<ref_obj_pid>957155</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Hoffman, N., and Preetham, A. J. 2003. Real-time light-atmosphere interactions for outdoor scenes. <i>Graphics programming methods</i>, 337--352.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383319</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Jensen, H., Marschner, S., Levoy, M., and Hanrahan, P. 2001. A practical model for subsurface light transport. In <i>SIGGRAPH 01</i>, 511--518.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>500844</ref_obj_id>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Jensen, H. W. 2001. <i>Realistic Image Synthesis Using Photon Mapping.</i> AK Peters.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808594</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Kajiya, J., and Herzen, B. 1984. Ray tracing volume densities. In <i>SIGGRAPH 84</i>, 165--174.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Koschmeider, H. 1924. Theorie der horizontalen sichtweite. <i>Beitr. Phys. freien Atm.</i>, 12.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383579</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Liu, X., Sloan, P.-P. J., Shum, H.-Y., and Snyder, J. 2004. All-frequency precomputed radiance transfer for glossy objects. In <i>EuroGraphics Symposium on Rendering 04</i>, 337--344.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882343</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Matusik, W., Pfister, H., Brand, M., and McMillan, L. 2003. A data-driven reflectance model. <i>ACM Transactions on Graphics (SIGGRAPH 03) 22</i>, 3, 759--769.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15899</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Max., N. L. 1986. Atmospheric illumination and shadows. In <i>SIGGRAPH 86</i>, 117--124.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[Max, N. 1994. Efficient light propagation for multiple anisotropic volume scattering. In <i>Eurographics Rendering Workshop 94</i>, 87--104.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97922</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[Nakamae, E., Kaneda, K., Okamoto, T., and Nishita, T. 1990. A lighting model aiming at drive simulators. In <i>SIGGRAPH 90</i>, 395--404.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>598511</ref_obj_id>
				<ref_obj_pid>598432</ref_obj_pid>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[Narasimhan, S., and Nayar, S. 2002. Vision and the atmosphere. <i>IJCV 48</i>, 3 (August), 233--254.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1965928</ref_obj_id>
				<ref_obj_pid>1965841</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[Narasimhan, S., and Nayar, S. 2003. Shedding light on the weather. In <i>CVPR 03</i>, 665--672.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37437</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[Nishita, T., and Nakamae, E. 1987. A shading model for atmospheric scattering considering luminous intensity distribution of light sources. In <i>SIGGRAPH</i>, 303--310.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[Pattanaik, S., and Mudur, S. 1993. Computation of global illumination in a participating medium by monte carlo simulation. <i>Journal of Visualization and Computer Animation 4</i>, 3, 133--152.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311545</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[Preetham, A. J., Shirley, P., and Smits, B. 1999. A practical analytic model for daylight. In <i>SIGGRAPH</i>, 91--100.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383583</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[Premoze, S., Ashikhmin, M., Tesendorf, J., Ramamoorthi, R., and Nayar, S. 2004. Practical rendering of multiple scattering effects in participating media. In <i>EuroGraphics Symposium on Rendering 04</i>, 363--374.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383271</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[Ramamoorthi, R., and Hanrahan, P. 2001. A signal-processing framework for inverse rendering. In <i>SIGGRAPH 01</i>, 117--128.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566611</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[Ramamoorthi, R., and Hanrahan, P. 2002. Frequency space environment map rendering. <i>ACM Transactions on Graphics (SIGGRAPH 02) 21</i>, 3, 517--526.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383584</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[Riley, K., Ebert, D., Kraus, M., Tessendorf, J., and Hansen, C. 2004. Efficient rendering of atmospheric phenomena. In <i>EuroGraphics Symposium on Rendering 2004</i>, 375--386.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37436</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[Rushmeier, H., and Torrance, K. 1987. The zonal method for calculating light intensities in the presence of a participating medium. In <i>SIGGRAPH 87</i>, 293--302.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[Sakas, G. 1990. Fast rendering of arbitrary distributed volume densities. In <i>Eurographics 90</i>, 519--530.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566612</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[Sloan, P.-P., Kautz, J., and Snyder, J. 2002. Precomputed radiance transfer for real-time rendering in dynamic, low-frequency lighting environments. <i>ACM Transactions on Graphics (SIGGRAPH 02) 21</i>, 3, 527--536.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[Stam, J. 1995. Multiple scattering as a diffusion process. In <i>Eurographics Rendering Workshop 95</i>, 41--50.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383580</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[Wang, R., Tran, J., and Luebke, D. 2004. All-frequency relighting of non-diffuse objects using separable BRDF approximation. In <i>EuroGraphics Symposium on Rendering 2004</i>, 345--354.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[Antyufeev, S. 2000. <i>Monte Carlo Method for Solving Inverse Problems of Radiative Transfer.</i> Inverse and III-Posed Problems Series, VSP Publishers.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[Boss, E., and Pegau, W. S. 2001. Relationship of light scattering at an angle in the backward direction to the backscattering coefficient. <i>Applied Optics 40 (30)</i>, 5503--5507.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[Chandrasekhar, S. 1960. <i>Radiative Transfer.</i> Oxford University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>794511</ref_obj_id>
				<ref_obj_pid>794189</ref_obj_pid>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[Dana, K., Nayar, S., van Ginneken, B., and Koenderink, J. 1997. Reflectance and texture of real-world surfaces. In <i>Proc CVPR</i>, 151--157.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280864</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[Debevec, P. 1998. Rendering synthetic objects into real scenes: Bridging traditional and image-based graphics with global illumination and high dynamic range photography. <i>Proc. SIGGRAPH 98</i>, 189--198.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[Finsy, E. G., and Joosten, J. 1991. Maximum entropy inversion of static light scattering data for the particle size distribution by number and volume. In <i>Advances in measurements and control of colloidal processes. Butterworth-Heineman, Ch. 30.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[Fuchs, E., and Jaffe, J. S. 2002. Thin laser light sheet microscope for microbial oceanography. <i>OPTICS EXPRESS 10 (2)</i>, 145--154.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073266</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[Hawkins, T., Einarsson, P., and Debevec, P. 2005. Acquisition of time-varying participating media. <i>ACM Trans. on Graphics (SIGGRAPH) 24</i>, 3, 812--815.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[Henyey, L., and Greenstein, J. 1941. Diffuse radiation in the galaxy. vol. 93, 70--83.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[Hulst, V. D. 1957. <i>Light Scattering by small Particles.</i> John Wiley and Sons.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[Ishimaru, A. 1978. <i>Wave Propagation and Scattering in Random Media. Volume 1: Single Scattering and Transport Theory.</i> Academic Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[Jaeger, D., Demeyere, H., Finsy, R., Sneyers, R., Vanderdeelen, J., Van-Der-Meeren, P., and Van-Laethem, M. 1991. Particle sizing by photon correlation spectroscopy. part i: Monodisperse latices: influence of scattering angle and concentration of dispersed material. In <i>Part. Syst. Charact. 8, 179.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383319</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[Jensen, H., Marschner, S., Levoy, M., and Hanrahan, P. 2001. A practical model for subsurface light transport. In <i>Proc. SIGGRAPH 01</i>, 511--518.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[Key, J. R. 2005. Streamer: User's guide. <i>Tech Report, NOAA/NESDIS, Madison, Wisconsin.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>927098</ref_obj_id>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[Marschner, S. 1998. Inverse rendering for computer graphics. <i>PhD Thesis, Cornell University.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882343</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[Matusik, W., Pfister, H., Brand, M., and McMillan, L. 2003. A data-driven reflectance model. <i>ACM Trans. on Graphics (SIGGRAPH) 22</i>, 3, 759--769.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[McCormick, N. J. 1981. A critique of inverse solutions to slab geometry transport problems. <i>Prog. Nucl. Energy 8.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[McCormick, N. J. 1985. Sensitivity of multiple-scattering inverse transport methods to measurement errors. <i>JOSA A 2.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>75</ref_seq_no>
				<ref_text><![CDATA[McCormick, N. J. 1996. Analytical transport theory applications in optical oceanography. <i>Annals of Nuclear Energy 23</i>, 381--395.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1965928</ref_obj_id>
				<ref_obj_pid>1965841</ref_obj_pid>
				<ref_seq_no>76</ref_seq_no>
				<ref_text><![CDATA[Narasimhan, S. G., and Nayar, S. K. 2003. Shedding light on the weather. In <i>CVPR 03</i>, 665--672.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>77</ref_seq_no>
				<ref_text><![CDATA[Oishi, T. 1990. Significant relationship between the backward scattering coefficient of sea water and the scatterance at 120 degrees. <i>Applied Optics 29 (31)</i>, 4658--4665.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>78</ref_seq_no>
				<ref_text><![CDATA[Prahl, S. A. 1988. Light transport in tissue. <i>PhD Thesis, University of Texas at Austin.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383271</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>79</ref_seq_no>
				<ref_text><![CDATA[Ramamoorthi, R., and Hanrahan, P. 2001. A signal processing framework for inverse rendering. <i>Proc. SIGGRAPH 01</i>, 117--128.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>80</ref_seq_no>
				<ref_text><![CDATA[Sullivan, S. A. 1963. Experimental study of the absorption in distilled water, artificial sea water, and heavy water in the visible region of the spectrum. <i>JOSA 53.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073309</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>81</ref_seq_no>
				<ref_text><![CDATA[Sun, B., Ramamoorthi, R., Narasimhan, S. G., and Nayar, S. K. 2005. A practical analytic single scattering model for real time rendering. <i>ACM Trans. on Graphics (SIGGRAPH) 24</i>, 3, 1040--1049.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614380</ref_obj_id>
				<ref_obj_pid>614268</ref_obj_pid>
				<ref_seq_no>82</ref_seq_no>
				<ref_text><![CDATA[Ward-Larson, Rushmeier, H., and Piatko. 1997. A visibility matching tone reproduction operator for high dynamic range scenes. <i>IEEE Trans. on Visualization and Computer Graphics 3</i>, 4, 291--306.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617559</ref_obj_id>
				<ref_obj_pid>616011</ref_obj_pid>
				<ref_seq_no>83</ref_seq_no>
				<ref_text><![CDATA[{BTL90} Berger M., Trout T., Levit N.: Ray tracing mirages. <i>IEEE Computer Graphics and Applications 10</i>, 3 (May 1990), 36--41.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>84</ref_seq_no>
				<ref_text><![CDATA[{BW02} Born M., Wolf E.: <i>Principles of Optics: Electromagnetic Theory of Propagation, Interference and Diffraction of Light.</i> Cambridge University Press, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>85</ref_seq_no>
				<ref_text><![CDATA[{CIB88} CIBSE: <i>Standard File Format for Transfer of Luminaire Photometric Data.</i> The Chartered Institution of Building Services Engineers, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>581915</ref_obj_id>
				<ref_obj_pid>581896</ref_obj_pid>
				<ref_seq_no>86</ref_seq_no>
				<ref_text><![CDATA[{CJ02} Cammarano M., Jensen H. W.: Time dependent photon mapping. In <i>Proceedings of the 13th Eurographics workshop on Rendering</i> (2002), Eurographics Association, pp. 135--144.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>87</ref_seq_no>
				<ref_text><![CDATA[{CS03} Cerezo E., Seron F.: Inelastic scattering in participating media. application to the ocean. In <i>Proceedings of the Annual Conference of the European Association for Computer Graphics, Eurographics 2003</i> (2003), pp. CD-ROM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>88</ref_seq_no>
				<ref_text><![CDATA[{DP80} Dormand J., Prince P.: A family of embeded runge-kutta formulae. <i>Journal of Computational and Applied Mathematics 6</i>(1) (1980), 19--26.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>89</ref_seq_no>
				<ref_text><![CDATA[{FFLV82} Fabri E., Fiorzo G., Lazzeri F., Violino P.: Mirage in the laboratory. <i>Am. J. Physics 50(6)</i> (1982), 517--521.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>90</ref_seq_no>
				<ref_text><![CDATA[{GD58} Gladstone J. H., Dale J.: On the influence of temperature on the refraction of light. <i>Phil. Trans. 148</i> (1858), 887.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>527570</ref_obj_id>
				<ref_seq_no>91</ref_seq_no>
				<ref_text><![CDATA[{Gla95a} Glassner A.: <i>Principles of Digital Image Synthesis.</i> Morgan Kaufmann, San Francisco, California, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>92</ref_seq_no>
				<ref_text><![CDATA[{Gla95b} Glassner A. S.: A model for fluorescence and phosphorescence. In <i>Photorealistic Rendering Techniques</i> (1995), Sakas P. S. G., M&#252;ller S., (Eds.), Eurographics, Springer-Verlag Berlin Heidelberg New York, pp. 60--70.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>93</ref_seq_no>
				<ref_text><![CDATA[{Gr&#246;95} Gr&#246;ller E.: Nonlinear ray tracing: visualizing strange worlds. <i>The Visual Computer 11</i>, 5 (1995), 263--276.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1037225</ref_obj_id>
				<ref_obj_pid>1037210</ref_obj_pid>
				<ref_seq_no>94</ref_seq_no>
				<ref_text><![CDATA[{GSMA04} Gutierrez D., Seron F., Munoz A., Anson O.: Chasing the green flash: a global illumination solution for inhomogeneous media. In <i>Spring Conference on Computer Graphics</i> (2004), (in cooperation with ACM SIGGRAPH A. P., Eurographics), (Eds.), pp. 95--103.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>95</ref_seq_no>
				<ref_text><![CDATA[{Haw92} Hawes S.: <i>Quantum Fluorescence Efficiencies of Marine Fulvic and Humid Acids.</i> PhD thesis, Dept. of Marince Science, Univ. of South Florida, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>96</ref_seq_no>
				<ref_text><![CDATA[{HW01} Hanson A. J., Weiskopf D.: Visualizing relativity. siggraph 2001 course 15, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280925</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>97</ref_seq_no>
				<ref_text><![CDATA[{JC98} Jensen H. W., Christensen P. H.: Efficient simulation of light transport in scenes with participating media using photon maps. In <i>SIGGRAPH 98 Conference Proceedings</i> (jul 1998), Cohen M., (Ed.), Annual Conference Series, ACM SIGGRAPH, Addison Wesley, pp. 311--320. ISBN 0-89791-999-8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614380</ref_obj_id>
				<ref_obj_pid>614268</ref_obj_pid>
				<ref_seq_no>98</ref_seq_no>
				<ref_text><![CDATA[{LRP97} Larson G. W., Rushmeier H., Piatko C.: A visibility matching tone reproduction operator for high dynamic range scenes. <i>IEEE Transactions on Visualization and Computer Graphics 3</i>, 4 (Oct. 1997), 291--306.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>99</ref_seq_no>
				<ref_text><![CDATA[{Mob94} Mobley C.: <i>Light and Water. Radiative Transfer in Natural Waters.</i> Academic Press, Inc., 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383284</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>100</ref_seq_no>
				<ref_text><![CDATA[{MTAS01} Myszkowksi K., Tawara T., Akamine A., Seidel H. P.: Perception-guided global illumination solution for animation. In <i>Computer Graphics Proceedings, Annual Conference Series, 2001 (ACM SIGGRAPH 2001 Proceedings)</i> (Aug. 2001), pp. 221--230.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617599</ref_obj_id>
				<ref_obj_pid>616014</ref_obj_pid>
				<ref_seq_no>101</ref_seq_no>
				<ref_text><![CDATA[{Mus90} Musgrave F. K.: A note on ray tracing mirages. <i>IEEE Computer Graphics and Applications 10</i>, 6 (Nov. 1990), 10--12.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>102</ref_seq_no>
				<ref_text><![CDATA[{Nem93} Nemiroff R. J.: Visual distortions near a neutron star and black hole. <i>American Journal of Physics 61(7)</i> (1993), 619--632.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>844181</ref_obj_id>
				<ref_obj_pid>844174</ref_obj_pid>
				<ref_seq_no>103</ref_seq_no>
				<ref_text><![CDATA[{PDC*03} Purcell T. J., Donner C., Cammarano M., Jensen J., Hanrahan P.: Photon mapping on programmable graphics hardware. In <i>SIGGRAPH/Eurographics Workshop on Graphics Hardware</i> (2003), Eurographics Association, pp. 041--050.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732114</ref_obj_id>
				<ref_obj_pid>647651</ref_obj_pid>
				<ref_seq_no>104</ref_seq_no>
				<ref_text><![CDATA[{PPS97} Perez F., Pueyo X., Sillion F.: Global illumination techniques for the simulation of participating media. In <i>Proc. of the Eigth Eurographics Workshop on Rendering</i> (1997), pp. 16--18.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344810</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>105</ref_seq_no>
				<ref_text><![CDATA[{PTYG00} Pattanaik S., Tumblin J. E., Yee H., Greenberg. D. P.: Time-dependent visual adaptation for realistic image display. In <i>SIGGRAPH 2000, Computer Graphics Proceedings</i> (2000), Akeley K., (Ed.), Annual Conference Series, ACM Press/ACM SIGGRAPH/Addison Wesley Longman, pp. 47--54.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1652772</ref_obj_id>
				<ref_obj_pid>1652320</ref_obj_pid>
				<ref_seq_no>106</ref_seq_no>
				<ref_text><![CDATA[{SGGC05} Seron F., Gutierrez D., Gutierrez G., Cerezo E.: Implementation of a method of curved ray tracing for inhomogeneous atmospheres. <i>Computers and Graphics 29(1)</i> (2005).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>275482</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>107</ref_seq_no>
				<ref_text><![CDATA[{SL96} Stam J., Langu&#233;nou E.: Ray tracing in non-constant media. In <i>Eurographics Rendering Workshop 1996</i> (New York City, NY, June 1996), Pueyo X., Schr&#246;der P., (Eds.), Eurographics, Springer Wien, pp. 225--234. ISBN 3-211-82883-4.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>108</ref_seq_no>
				<ref_text><![CDATA[{USG76} USGPC: <i>U.S. Standard Atmosphere.</i> United State Government Printing Office, Washington, D.C., 1976.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>109</ref_seq_no>
				<ref_text><![CDATA[{VDWGL00} Van Der Werf S., Gunther G., Lehn W.: Novaya zemlya effects and sunsets. <i>Applied Optics 42</i>, 3 (2000).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732297</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>110</ref_seq_no>
				<ref_text><![CDATA[{WTP01} Wilkie A., Tobler R., Purgathofer W.: Combined rendering of polarization and fluorescence effects. In <i>Rendering Techniques '01 (Proc. Eurographics Workshop on Rendering 2001)</i> (2001), Gortler S. J. M. K. e., (Ed.), Eurographics, Springer-Verlag, pp. 197--204.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344801</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>111</ref_seq_no>
				<ref_text><![CDATA[{YOH00} Yngve G. D., O'Brien J. F., Hodgins H.: Animating explosions. In <i>Proceedings of the Computer Graphics Conference 2000 (SIGGRAPH-00)</i> (New York, July 23--28 2000), Hoffmeyer S., (Ed.), ACM Press, pp. 29--36.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>112</ref_seq_no>
				<ref_text><![CDATA[{BLSS93} Blasi P., Le Saec B., Schlick C.: A rendering algorithm for discrete volume density objects. <i>Computer Graphics Forum (Eurographics 93) 12</i>, 3 (1993), 201--210. 4]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>113</ref_seq_no>
				<ref_text><![CDATA[{BMP81} Bricaud A., Morel A., Prieur L.: Absorption by dissolved organic matter of the sea (yellow substance) in the uv and visible domains. <i>Limnol. Oceanogr. 26</i>, 1 (1981), 43--53. 3]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>114</ref_seq_no>
				<ref_text><![CDATA[{BSF*03} Babin M., Stramski D., Ferrari G. M., Claustre H., Bricaud A., Obolensky G., Hoepffner N.: Variations in the light absorption coefficients of phytoplankton, non-algal particles, and dissolved organic matter in coastal waters around europe. <i>J. Geophys. Res. 108(C7)</i>, 3211 (2003). 3, 9]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>115</ref_seq_no>
				<ref_text><![CDATA[{Cha60} Chandrasekhar S.: <i>Radiative Transfer.</i> Dover Publications, Inc., 1960. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1071153</ref_obj_id>
				<ref_obj_pid>1071152</ref_obj_pid>
				<ref_seq_no>116</ref_seq_no>
				<ref_text><![CDATA[{CS04} Cerezo E., Seron F. J.: Rendering natural waters taking fluorescence into account: Research articles. <i>Comput. Animat. Virtual Worlds 15</i>, 5 (2004), 471--484. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276452</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>117</ref_seq_no>
				<ref_text><![CDATA[{FCJ07} Frisvad J. R., Christensen N. J., Jensen H. W.: Computing the scattering properties of participating media using Lorenz-Mie theory. <i>ACM Trans. Graph. 26</i>, 3 (2007), 60. 2, 9]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>527570</ref_obj_id>
				<ref_seq_no>118</ref_seq_no>
				<ref_text><![CDATA[{Gla95} Glassner A. S.: <i>Principles of Digital Image Synthesis.</i> Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1995. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>119</ref_seq_no>
				<ref_text><![CDATA[{GM83} Gordon H. R., Morel A.: <i>Remote Assessment of Ocean Color for Interpretation of Satellite Visible Imagery: A Review</i>, vol. 4 of <i>Lecture Notes on Coastal and Estuarine Studies.</i> Springer-Verlag, New York, 1983. 4]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>120</ref_seq_no>
				<ref_text><![CDATA[{GMAS05} Gutierrez D., Munoz A., Anson O., Ser&#243;n F. J.: Non-linear volume photon mapping. In <i>Proc. of the Eurographics Symposium on Rendering Techniques, Konstanz, Germany, June 29 -- July 1, 2005</i> (2005), pp. 291--300. 2, 6, 9]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>121</ref_seq_no>
				<ref_text><![CDATA[{GSO03} Green R. E., Sosik H. M., Olson R. J.: Contributions of phytoplankton and other particles to inherent optical properties in new england continental shelf waters. <i>Limnol. Oceanogr. 48</i>, 6 (2003), 2377--2391. 4]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>122</ref_seq_no>
				<ref_text><![CDATA[{Haw92} Hawes S.: <i>Quantum fluorescence efficiencies of marine fulvic and humid acids.</i> PhD thesis, Dept. of Marince Science, Univ. of South Florida, 1992. 5]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>123</ref_seq_no>
				<ref_text><![CDATA[{HG41} Henyey L., Greenstein J.: Diffuse radiation in the galaxy. <i>Astrophysics Journal 93</i> (1941), 70--83. 4]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280925</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>124</ref_seq_no>
				<ref_text><![CDATA[{JC98} Jensen H. W., Christensen P. H.: Efficient simulation of light transport in scenes with participating media using photon maps. In <i>SIGGRAPH 98 Conference Proceedings</i> (jul 1998), Cohen M., (Ed.), Annual Conference Series, ACM SIGGRAPH, Addison Wesley, pp. 311--320. 7]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1330518</ref_obj_id>
				<ref_obj_pid>1330511</ref_obj_pid>
				<ref_seq_no>125</ref_seq_no>
				<ref_text><![CDATA[{JDZJ08} Jarosz W., Donner C., Zwicker M., Jensen H. W.: Radiance caching for participating media. <i>To appear in ACM Transactions on Graphics</i> (2008). 8]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>500844</ref_obj_id>
				<ref_seq_no>126</ref_seq_no>
				<ref_text><![CDATA[{Jen01} Jensen H. W.: <i>Realistic image synthesis using photon mapping.</i> A. K. Peters, Natick, Massachussets, 2001. 2, 6]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>127</ref_seq_no>
				<ref_text><![CDATA[{Kir94} Kirk J. T.: <i>Light and photosynthesis in aquatic ecosystems.</i> Cambridge University Press, New York, 1994. 3]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>128</ref_seq_no>
				<ref_text><![CDATA[{KYNN91} Kaneda K., Yuan G., Nakamae E., Nishita T.: Realistic visual simulation of water surfaces taking into account radiative transfer. In <i>Proc. of CAD/Graphics 91</i> (1991), pp. 25--30. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>129</ref_seq_no>
				<ref_text><![CDATA[{Maz02} Mazo R. M.: <i>Brownian Motion: Fluctuations, Dynamics and Applications</i>, vol. 112 of <i>International Series of Monographs on Physics.</i> Oxford University Press Inc., Great Clarendon Street, Oxford, 2002, ch. Einstein-Smoluchowski Theory, pp. 46--62. 4]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>130</ref_seq_no>
				<ref_text><![CDATA[{Mob94} Mobley C. D.: <i>Light and Water: Radiative Transfer in Natural Waters.</i> Academic Press, Inc., San Diego, 1994. 1, 2, 3, 5, 9]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>131</ref_seq_no>
				<ref_text><![CDATA[{Mor74} Morel A.: <i>Optical Aspects of Oceanography.</i> Academic Press, New York, 1974. 3, 4]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>132</ref_seq_no>
				<ref_text><![CDATA[{Mor88} Morel A.: Optical modeling of the upper ocean in relation to its biogenous matter content (case i waters). <i>Journal of Geophysical Research 93</i>, C9 (1988), 10749--10768. 3]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97922</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>133</ref_seq_no>
				<ref_text><![CDATA[{NKON90} Nakamae E., Kaneda K., Okamoto T., Nishita T.: A lighting model aiming at drive simulators. <i>Computer Graphics 24</i>, 4 (Aug. 1990), 395--404. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166140</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>134</ref_seq_no>
				<ref_text><![CDATA[{NSTN93} Nishita T., Shirai T., Tadamura K., Nakamae E.: Display of the earth taking into account atmosphere scattering. In <i>Computer Graphics (SIGGRAPH '93 Proceedings)</i> (1993), vol. 24, pp. 175--182. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>135</ref_seq_no>
				<ref_text><![CDATA[{PA01} Premoze S., Ashikhmin M.: Rendering natural waters. <i>Comput. Graph. Forum 20</i>, 4 (2001), 189--199. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>136</ref_seq_no>
				<ref_text><![CDATA[{PF97} Pope R. M., Fry E. S.: Absorption spectrum (380--700 nm) of pure water. ii. integrating cavity measurements. <i>Applied Optics 36</i>, 33 (1997), 8710--8723. 3]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>137</ref_seq_no>
				<ref_text><![CDATA[{Pre76} Preisendorfer R. W.: <i>Introduction</i>, vol. 1 of <i>Hydrologic Optics.</i> National Technical Information Service, Springfield, IL, 1976. 1]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>138</ref_seq_no>
				<ref_text><![CDATA[{RPC89} Roesler C. S., Perry M. J., Carder K. L.: Modeling in situ phytoplankton absorption from total absorption spectra in productive inland marine waters. <i>Limnol. Oceanogr. 34</i>, 8 (1989), 1510--1523. 3]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37436</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>139</ref_seq_no>
				<ref_text><![CDATA[{RT87} Rushmeier H. E., Torrance K. E.: The zonal method for calculating light intensities in the presence of a participating medium. <i>Computer Graphics 21</i>, 4 (July 1987), 293--302. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>140</ref_seq_no>
				<ref_text><![CDATA[{SB81} Smith R. C., Baker K. S.: Optical properties of the clearest natural waters (200--800 nm). <i>Appl. Opt. 20</i> (1981), 177--184. 3, 4]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>141</ref_seq_no>
				<ref_text><![CDATA[{SBM01} Stramski D., Bricaud A., Morel A.: Modeling the inherent optical properties of the ocean based on the detailed composition of planktonic community. <i>Applied Optics 40</i> (2001), 2929--2945. 4, 5]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>142</ref_seq_no>
				<ref_text><![CDATA[{SCP94} Spinrad R. W., Carder K. L., Perry M. J. (Eds.): <i>Ocean Optics.</i> No. 25 in Oxford Monographs on Geology and Geophysics. Oxford University Press, 1994. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1278389</ref_obj_id>
				<ref_obj_pid>1278387</ref_obj_pid>
				<ref_seq_no>143</ref_seq_no>
				<ref_text><![CDATA[{SGA*07} Sundstedt V., Gutierrez D., Anson O., Banterle F., Chalmers A.: Perceptual rendering of participating media. <i>ACM Transactions of Applied Perception 4</i>, 3 (2007). 8]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>144</ref_seq_no>
				<ref_text><![CDATA[{SLP87} Sathyendranath S., Lazzara L., Prieur L.: Variations in the spectral values of specific absorption of phytoplankton. <i>Limnol. Oceanogr. 32</i>, 2 (1987), 403--415. 3, 4]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>212199</ref_obj_id>
				<ref_obj_pid>212189</ref_obj_pid>
				<ref_seq_no>145</ref_seq_no>
				<ref_text><![CDATA[{TN95} Tadamura K., Nakamae E.: <i>Computer Graphics: Developments in Virtual Environments.</i> Academic Press, 1995, ch. Modeling the colour of Water in Lightning Design, pp. 97--114. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>146</ref_seq_no>
				<ref_text><![CDATA[{Wal69} Walrafen G. E.: Continuum model of water--an erroneous interpretation. <i>Journal of Chemical Physics 50</i>, 1 (January 1969), 567--569. 5]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>147</ref_seq_no>
				<ref_text><![CDATA[R. D. Ballard. <i>The Discovery of the Titanic.</i> Warner Books, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>148</ref_seq_no>
				<ref_text><![CDATA[R. Basri and D. W. Jacobs. Photometric stereo with general, unknown lighting. In <i>CVPR</i>, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>149</ref_seq_no>
				<ref_text><![CDATA[F. M. Caimi, D. M. Kocak, and V. L. Asper. Developments in laser-line scanned undersea surface mapping and image analysis systems for scientific applications. <i>In Proc. MTS/IEEE Oceans</i>, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>150</ref_seq_no>
				<ref_text><![CDATA[S. Chandrasekhar. <i>Radiative Transfer.</i> Dover Publications, Inc., 1960.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>151</ref_seq_no>
				<ref_text><![CDATA[P. C. Y. Chang, J. C. Flitton, K. I. Hopcraft, E. Jakeman, D. L. Jordan, and J. G. Walker. Improving visibility depth in passive underwater imaging by use of polarization. <i>App. Opt.</i>, 42 (15), 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>152</ref_seq_no>
				<ref_text><![CDATA[S. Y. Chen and Y. F. Li. Self-recalibration of a colour-encoded light system for automated 3-d measurements. <i>MeasureSciTech</i>, 14(1), 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>153</ref_seq_no>
				<ref_text><![CDATA[G. R. Fournier, D. Bonnier, J. L. Forand, and P. W Pace. Range-gated underwater laser imaging system. <i>Opt. Eng.</i>, 32 (9), 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>48906</ref_obj_id>
				<ref_obj_pid>48904</ref_obj_pid>
				<ref_seq_no>154</ref_seq_no>
				<ref_text><![CDATA[R. T. Frankot and R. Chellappa. A method for enforcing integrability in shape from shading algorithms. <i>PAMI</i>, 10(4), 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>155</ref_seq_no>
				<ref_text><![CDATA[E. Fuchs and J. S. Jaffe. Thin laser light sheet microscope for microbial oceanography. <i>OPTICS EXPRESS</i>, 10 (2), 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>156</ref_seq_no>
				<ref_text><![CDATA[G. D. Gilbert and J. C. Pernicka. Improvement of underwater visibility by reduction of backscatter with a circular polarization technique. <i>Applied Optics</i>, 6 (4):741--746, 1967.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1026579</ref_obj_id>
				<ref_obj_pid>1026574</ref_obj_pid>
				<ref_seq_no>157</ref_seq_no>
				<ref_text><![CDATA[M. D. Grossberg and S. K. Nayar. The raxel imaging model and ray-based calibration. <i>IJCV</i>, 61(2), 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1965911</ref_obj_id>
				<ref_obj_pid>1965841</ref_obj_pid>
				<ref_seq_no>158</ref_seq_no>
				<ref_text><![CDATA[A. Hertzmann and S. M. Seitz. Shape and materials by example: a photometric stereo approach. In <i>CVPR</i>, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>333275</ref_obj_id>
				<ref_obj_pid>333271</ref_obj_pid>
				<ref_seq_no>159</ref_seq_no>
				<ref_text><![CDATA[D. Q. Huynh, R. A. Owens, and P. E. Hartmann. Calibrating a structured light stripe system: A novel approach. <i>IJCV</i>, 33(1), 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>160</ref_seq_no>
				<ref_text><![CDATA[J. S. Jaffe, J. McLean, M. P. Strand, and K. D. Moore. Underwater optical imaging: Status and prospects. <i>Tech. Report, Scripps Institution of Oceanography, La Jolla</i>, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>161</ref_seq_no>
				<ref_text><![CDATA[C. Je, S. W. Lee, and R. H. Park. High-contrast color-stripe pattern for rapid structured-light range imaging. In <i>ECCV</i>, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>162</ref_seq_no>
				<ref_text><![CDATA[D. M. Kocak, F. M. Caimi, T. H. Jagielo, and J. Kloske. Laser projection photogrammetry and video system for quantification and mensuration. <i>MTS/IEEE Oceans</i>, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015806</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>163</ref_seq_no>
				<ref_text><![CDATA[M. Levoy, B. Chen, V. Vaish, M. Horowitz, I. McDowall, and M. Bolas. Synthetic aperture confocal imaging. In <i>SIGGRAPH</i>, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>598511</ref_obj_id>
				<ref_obj_pid>598432</ref_obj_pid>
				<ref_seq_no>164</ref_seq_no>
				<ref_text><![CDATA[S. G. Narasimhan and S. K. Nayar. Vision and the atmosphere. <i>IJCV</i>, 48(3).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>165</ref_seq_no>
				<ref_text><![CDATA[P. Naulleau and D. Dilworth. Motion-resolved imaging of moving objects embedded within scattering media by the use of time-gated speckle analysis. <i>App. Opt.</i>, 35 (26), 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>166</ref_seq_no>
				<ref_text><![CDATA[Y. Sato, H. Kitagawa, and H. Fujita. Shape measurement of curved objects using multiple slit-ray projections. <i>PAMI</i>, 4(6), 1982.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1965865</ref_obj_id>
				<ref_obj_pid>1965841</ref_obj_pid>
				<ref_seq_no>167</ref_seq_no>
				<ref_text><![CDATA[D. Scharstein and R. Szeliski. High-accuracy stereo depth maps using structured light. In <i>CVPR</i>, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>168</ref_seq_no>
				<ref_text><![CDATA[Y. Y. Schechner and N. Karpel. Attenuating natural flicker patterns. <i>In Proc. MTS/IEEE Oceans</i>, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>169</ref_seq_no>
				<ref_text><![CDATA[Y. Y. Schechner and N. Karpel. Clear underwater vision. <i>In Proc. CVPR</i>, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>170</ref_seq_no>
				<ref_text><![CDATA[J. S. Tyo, M. P. Rowe, Jr. E. N. Pugh, and N. Engheta. Target detection in optically scattering media by polarization-difference imaging. <i>App. Opt.</i>, 35 (11), 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>171</ref_seq_no>
				<ref_text><![CDATA[J. G. Walker, P. C. Y. Chang, and K. I. Hopcraft. Visibility depth improvement in active polarization imaging in scattering media. <i>App. Opt.</i>, 39 (27), 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>172</ref_seq_no>
				<ref_text><![CDATA[D. Walther, D. R. Edgington, and C. Koch. Detection and tracking of objects in underwater video. <i>In Proc. CVPR</i>, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>173</ref_seq_no>
				<ref_text><![CDATA[Authors Website. http://www.cs.cmu.edu/~srinivas/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1622884</ref_obj_id>
				<ref_obj_pid>1622876</ref_obj_pid>
				<ref_seq_no>174</ref_seq_no>
				<ref_text><![CDATA[P. M. Will and K. S. Pennington. Grid coding: A preprocessing technique for robot and machine vision. <i>AI</i>, 2, 1971.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>175</ref_seq_no>
				<ref_text><![CDATA[R. J. Woodham. Photometric method for determining surface orientation from multiple images. <i>OptEng</i>, 19(1), 1980.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>176</ref_seq_no>
				<ref_text><![CDATA[L. Zhang, B. Curless, and S. M. Seitz. Rapid shape acquisition using color structured light and multi-pass dynamic programming. In <i>The 1st IEEE International Symposium on 3D Data Processing, Visualization, and Transmission</i>, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>177</ref_seq_no>
				<ref_text><![CDATA[F. M. Caimi, F. R. Dalgleish, T. E. Giddings, J. J. Shirron, C. Mazel, and K. Chiang. Pulse versus CW laser line scan imaging detection methods: Simulation results. <i>In Proc. IEEE OCEANS</i>, pages 1--4, 2007. 5]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>178</ref_seq_no>
				<ref_text><![CDATA[S. Chandrasekhar. <i>Radiative Transfer.</i> Dover Publications, Inc., 1960. 1]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>179</ref_seq_no>
				<ref_text><![CDATA[T. Chen, H. P. A. Lensch, C. Fuchs, and H.-P. Seidel. Polarization and phase-shifting for 3D scanning of translucent objects. In <i>Proc. IEEE CVPR</i>, pages 1--8, 2007. 5]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>794397</ref_obj_id>
				<ref_obj_pid>794189</ref_obj_pid>
				<ref_seq_no>180</ref_seq_no>
				<ref_text><![CDATA[F. Cozman and E. Krotkov. Depth from scattering. <i>In Proc. IEEE CVPR</i>, pages 801--806, 1997. 1]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>181</ref_seq_no>
				<ref_text><![CDATA[T. E. Giddings, J. J. Shirron, and A. Tirat-Gefen. EODES-3: An electro-optic imaging and performance prediction model. <i>In Proc. IEEE OCEANS</i>, 2:1380--1387, 2005. 5]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>182</ref_seq_no>
				<ref_text><![CDATA[G. D. Gilbert and J. C. Pernicka. Improvement of underwater visibility by reduction of backscatter with a circular polarization technique. <i>Applied Optics</i>, 6(4):741--746, 1967. 2, 3]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>183</ref_seq_no>
				<ref_text><![CDATA[M. Gupta and S. G. Narasimhan. Light transport web-page. <i>http://graphics.cs.cmu.edu/projects/LightTransport/.</i> 1, 5]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>184</ref_seq_no>
				<ref_text><![CDATA[V. I. Haltrin. One-parameter two-term henyey-greenstein phase function for light scattering in seawater. <i>Applied Optics</i>, 41(6):1022--1028, 2002. 3, 6]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>185</ref_seq_no>
				<ref_text><![CDATA[J. Jaffe. Computer modeling and the design of optimal underwater imaging systems. <i>IEEE Journal of Oceanic Engineering</i>, 15(2):101--111, 1990. 2, 4, 5]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>186</ref_seq_no>
				<ref_text><![CDATA[D. M. Kocak and F. M. Caimi. The current art of underwater imaging with a glimpse of the past. <i>MTS Journal</i>, 39:5--26, 2005. 2, 4]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015806</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>187</ref_seq_no>
				<ref_text><![CDATA[M. Levoy, B. Chen, V. Vaish, M. Horowitz, I. McDowall, and M. Bolas. Synthetic aperture confocal imaging. <i>ACM Trans. Graph.</i>, 23(3):825--834, 2004. 7]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>997554</ref_obj_id>
				<ref_seq_no>188</ref_seq_no>
				<ref_text><![CDATA[S. G. Narasimhan. Models and algorithms for vision through the atmosphere. In <i>Columbia Univ. Dissertation</i>, 2004. 1]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141986</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>189</ref_seq_no>
				<ref_text><![CDATA[S. G. Narasimhan, M. Gupta, C. Donner, R. Ramamoorthi, S. K. Nayar, and H. W. Jensen. Acquiring scattering properties of participating media by dilution. <i>ACM Trans. Graph.</i>, 25(3):1003--1012, 2006. 6, 7]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2286491</ref_obj_id>
				<ref_obj_pid>2286436</ref_obj_pid>
				<ref_seq_no>190</ref_seq_no>
				<ref_text><![CDATA[S. G. Narasimhan and S. K. Nayar. Contrast restoration of weather degraded images. 25(6):713--724, 2003. 1]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1097677</ref_obj_id>
				<ref_obj_pid>1097114</ref_obj_pid>
				<ref_seq_no>191</ref_seq_no>
				<ref_text><![CDATA[S. G. Narasimhan, S. K. Nayar, B. Sun, and S. J. Koppal. Structured light in scattering media. In <i>In Proc. IEEE ICCV</i>, pages 420--427, 2005. 2, 4, 5]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141977</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>192</ref_seq_no>
				<ref_text><![CDATA[S. K. Nayar, G. Krishnan, M. D. Grossberg, and R. Raskar. Fast separation of direct and global components of a scene using high frequency illumination. <i>ACM Trans. Graph.</i>, 25(3):935--944, 2006. 2, 3]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2286573</ref_obj_id>
				<ref_obj_pid>2286439</ref_obj_pid>
				<ref_seq_no>193</ref_seq_no>
				<ref_text><![CDATA[Y. Y. Schechner and Y. Averbuch. Regularized image recovery in scattering media. <i>IEEE Trans. PAMI</i>, 29(9):1655--1660, 2007. 5]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>194</ref_seq_no>
				<ref_text><![CDATA[Y. Y. Schechner and N. Karpel. Recovery of underwater visibility and structure by polarization analysis. <i>IEEE Journal of Oceanic Engineering</i>, 30(3):570--587, 2005. 1]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>195</ref_seq_no>
				<ref_text><![CDATA[Y. Y. Schechner, S. G. Narasimhan, and S. K. Nayar. Polarization-based vision through haze. <i>Applied Optics</i>, 42(3):511--525, 2003. 2, 3]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>196</ref_seq_no>
				<ref_text><![CDATA[W. A. Shurcliff and S. S. Ballard. <i>Polarized Light</i>, pages 98--103. Van Nostrand, Princeton, N.J., 1964. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>197</ref_seq_no>
				<ref_text><![CDATA[B. Skerry and H. Hall. <i>Successful Underwater Photography.</i> New York: Amphoto books, 2002. 2, 5]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>198</ref_seq_no>
				<ref_text><![CDATA[K. Tan and J. P. Oakley. Physics-based approach to color image enhancement in poor visibility conditions. <i>JOSA A</i>, 18(10):2460--2467, 2001. 1]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1153670</ref_obj_id>
				<ref_obj_pid>1153171</ref_obj_pid>
				<ref_seq_no>199</ref_seq_no>
				<ref_text><![CDATA[T. Treibitz and Y. Y. Schechner. Instant 3Descatter. In <i>Proc. IEEE CVPR</i>, volume 2, pages 1861--1868, 2006. 3, 8]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1512368</ref_obj_id>
				<ref_obj_pid>1512152</ref_obj_pid>
				<ref_seq_no>200</ref_seq_no>
				<ref_text><![CDATA[T. Treibitz and Y. Y. Schechner. Active polarization descattering. <i>IEEE Trans. PAMI</i>, To appear, 2008. 2, 5]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>201</ref_seq_no>
				<ref_text><![CDATA[T. Treibitz, Y. Y. Schechner, and H. Singh. Flat refractive geometry. In <i>Proc. IEEE CVPR</i>, 2008. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>202</ref_seq_no>
				<ref_text><![CDATA[H. van de Hulst. <i>Light Scattering by Small Particles.</i> Chapter 5. Wiley, New York, 1957. 8]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>203</ref_seq_no>
				<ref_text><![CDATA[K. J. Voss and E. S. Fry. Measurement of the mueller matrix for ocean water. <i>Applied Optics</i>, 23:4427--4439, 1984. 7, 8]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>204</ref_seq_no>
				<ref_text><![CDATA[S. Zhang and S. Negahdaripour. 3D shape recovery of planar and curved surfaces from shading cues in underwater images. <i>IEEE Journal of Oceanic Engineering</i>, 27:100--116, 2002. 1]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>205</ref_seq_no>
				<ref_text><![CDATA[Acharya, P. K., Berk, A., Anderson, G. P., Larsen, N. F., Tsay, S. C., and Stamnes, K. H. 1999. Modtran4: Multiple scattering and BRDF upgrades to modtran. In <i>SPIE Proc. Optical Spectroscopic Techniques and Instrumentation for Atmospheric and Space Research III</i>, p. 3756.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>206</ref_seq_no>
				<ref_text><![CDATA[Allard, E. 1876. <i>Memoire sur l'intensite' et la portee des phares.</i> Dunod: Paris.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>207</ref_seq_no>
				<ref_text><![CDATA[Bouguer, P. 1729. <i>Traite' d'optique sur la gradation de la lumiere.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>208</ref_seq_no>
				<ref_text><![CDATA[Chandrasekhar, S. 1960. <i>Radiative Transfer.</i> Dover Publications: New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>209</ref_seq_no>
				<ref_text><![CDATA[Chu, T. S. and Hogg, D. C. 1968. Effects of precipitation on propagation at 0.63, 3.5 and 10.6 microns. <i>The Bell System Technical Journal.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>794397</ref_obj_id>
				<ref_obj_pid>794189</ref_obj_pid>
				<ref_seq_no>210</ref_seq_no>
				<ref_text><![CDATA[Cozman, F. and Krotkov, E. 1997. Depth from scattering. In <i>Proceedings of the 1997 Conference on Computer Vision and Pattern Recognition</i>, vol. 31, pp. 801--806.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>211</ref_seq_no>
				<ref_text><![CDATA[Gordon, J. and Church, P. 1966. Overcast sky luminances and directional luminous reflectances of objects and backgrounds under overcast skies. <i>Applied Optics</i>, 5:919.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>212</ref_seq_no>
				<ref_text><![CDATA[Hardy, A. C. 1967. How large is a point source? <i>Journal of Optical Society of America</i>, 57(1).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>213</ref_seq_no>
				<ref_text><![CDATA[Henderson, S. T. 1977. <i>Daylight and its Spectrum.</i> Wiley: New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>214</ref_seq_no>
				<ref_text><![CDATA[Hidy, G. M. 1972. <i>Aerosols and Atmospheric Chemistry.</i> Academic Press: New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>5603</ref_obj_id>
				<ref_seq_no>215</ref_seq_no>
				<ref_text><![CDATA[Horn, B. K. P. 1986. <i>Robot Vision.</i> The MIT Press: Cambridge, MA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>216</ref_seq_no>
				<ref_text><![CDATA[IRIA. 1978. <i>The Infrared Handbook.</i> Infrared Information and Analysis Center, Environmental Research Institute of Michigan.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>217</ref_seq_no>
				<ref_text><![CDATA[Koenderink, J. J. and Richards, W. A. 1992. Why is snow so bright? <i>Journal of Optical Society of America</i>, 9(5):643--648.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>218</ref_seq_no>
				<ref_text><![CDATA[Kopeika, N. S. 1998. <i>A System Engineering Approach to Imaging.</i> SPIE Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>219</ref_seq_no>
				<ref_text><![CDATA[Koschmieder, H. 1924. Theorie der horizontalen sichtweite. <i>Beitr. Phys. Freien Atm.</i>, 12:33--53, 171--181.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>220</ref_seq_no>
				<ref_text><![CDATA[Langer, M. S. and Zucker, S. W. 1994. Shape from shading on a cloudy day. <i>JOSA-A</i>, 11(2):467--478.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>221</ref_seq_no>
				<ref_text><![CDATA[Mason, B. J. 1975. <i>Clouds, Rain, and Rainmaking.</i> Cambridge University Press: Cambridge.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>222</ref_seq_no>
				<ref_text><![CDATA[McCartney, E. J. 1975. <i>Optics of the Atmosphere: Scattering by Molecules and Particles.</i> John Wiley and Sons: New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>223</ref_seq_no>
				<ref_text><![CDATA[Middleton, W. E. K. 1949. The effect of the angular aperture of a telephotometer on the telephotometry of collimated and non-collimated beams. <i>Journal of Optical Society of America</i>, 39:576--581.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>224</ref_seq_no>
				<ref_text><![CDATA[Middleton, W. E. K. 1952. <i>Vision Through the Atmosphere.</i> University of Toronto Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>225</ref_seq_no>
				<ref_text><![CDATA[Mie, G. 1908. A contribution to the optics of turbid media, especially colloidal metallic suspensions. <i>Ann. of Physics</i>, 25(4):377--445.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>226</ref_seq_no>
				<ref_text><![CDATA[Minnaert, M. 1954. <i>The Nature of Light and Color in the Open Air.</i> Dover: New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>227</ref_seq_no>
				<ref_text><![CDATA[Moon, P. and Spencer, D. E. 1942. Illumination from a non-uniform sky. <i>Illum Eng.</i>, 37:707--726.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>228</ref_seq_no>
				<ref_text><![CDATA[Myers, J. N. 1968. Fog. <i>Scientific American</i>, pp. 75--82.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>229</ref_seq_no>
				<ref_text><![CDATA[Narasimhan, S. G. and Nayar, S. K. 2000. Chromatic framework for vision in bad weather. In <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>230</ref_seq_no>
				<ref_text><![CDATA[Narasimhan, S. G. and Nayar, S. K. 2001. Vision and the weather. In <i>Proceedings of SPIE Conference on Human Vision and Electronic Imaging VI</i>, p. 4299.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>851572</ref_obj_id>
				<ref_obj_pid>850924</ref_obj_pid>
				<ref_seq_no>231</ref_seq_no>
				<ref_text><![CDATA[Nayar, S. K. and Narasimhan, S. G. 1999. Vision in bad weather. In <i>Proceedings of the 7th International Conference on Computer Vision.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>232</ref_seq_no>
				<ref_text><![CDATA[Nieto-Vesperinas, M. and Dainty, J. C. 1990. <i>Scattering in Volumes and Surfaces.</i> North-Holland: New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2319480</ref_obj_id>
				<ref_obj_pid>2318957</ref_obj_pid>
				<ref_seq_no>233</ref_seq_no>
				<ref_text><![CDATA[Oakley, J. P. and Satherley, B. L. 1998. Improving image quality in poor visibility conditions using a physical model for degradation. <i>IEEE Trans. on Image Processing</i>, 7.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>234</ref_seq_no>
				<ref_text><![CDATA[Ohtake, T. 1970. Factors affecting the size distribution of raindrops and snowflakes. <i>Journal of Atmospheric Science</i>, 27:804--813.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>235</ref_seq_no>
				<ref_text><![CDATA[Porch, W. M. 1975. Visibility of distant mountains as a measure of background aerosol pollution. <i>Applied Optics</i>, 14.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>236</ref_seq_no>
				<ref_text><![CDATA[Rensch, D. B. and Long, R. K. 1970. Comparative studies of extinction and backscattering by aerosols, fog, and rain at 10.6 and 0.63 microns. <i>Applied Optics</i>, 9(7).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>237</ref_seq_no>
				<ref_text><![CDATA[Shafer, S. 1985. Using color to separate reflection components. <i>Color Research and Applications</i>, pp. 210--218.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>238</ref_seq_no>
				<ref_text><![CDATA[Van De Hulst. 1957. <i>Light Scattering by Small Particles.</i> John Wiley and Sons: New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>239</ref_seq_no>
				<ref_text><![CDATA[Yitzhaky, Y., Dror, I., and Kopeika, N. S. 1998. Restoration of altmospherically blurred images according to weather-predicted atmospheric modulation transfer function. <i>Optical Engineering</i>, 36.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>240</ref_seq_no>
				<ref_text><![CDATA[T. E. Boult and L. B. Wolff, "Physically-Based Edge Labelling," <i>Proc. IEEE Conf. Computer Vision and Pattern Recognition</i>, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>241</ref_seq_no>
				<ref_text><![CDATA[B. Cairns, B. E. Carlson, A. A. Lacis, and E. E. Russell, "An Analysis Of Ground-Based Polarimetric Sky Radiance Measurements," <i>Proc. SPIE</i>, vol. 3121, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>242</ref_seq_no>
				<ref_text><![CDATA[S. Chandrasekhar, <i>Radiative Transfer.</i> Dover Publications, Inc., 1960.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>243</ref_seq_no>
				<ref_text><![CDATA[D. B. Chenault and J. L. Pezzaniti, "Polarization Imaging through Scattering Media," <i>Proc. SPIE</i>, vol. 4133, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>244</ref_seq_no>
				<ref_text><![CDATA[K. L. Coulson, "Polarization of Light in the Natural Environment," <i>Proc. SPIE</i>, vol. 1166, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>245</ref_seq_no>
				<ref_text><![CDATA[L. J. Denes, M. Gottlieb, B. Kaminsky, and P. Metes, "Aotf Polarization Difference Imaging," <i>Proc. SPIE</i>, vol. 3584, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>246</ref_seq_no>
				<ref_text><![CDATA[S. D. Gedzelman, "Atmospheric Optics in Art," <i>Applied Optics</i>, vol. 30, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>247</ref_seq_no>
				<ref_text><![CDATA[J. Gordon and P. Church, "Overcast Sky Luminances and Directional Luminous Reflectances of Objects and Backgrounds under Overcast Skies," <i>Applied Optics</i>, vol. 5, p. 919, 1966.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>248</ref_seq_no>
				<ref_text><![CDATA[L. L. Grewe and R. R. Brooks, "Atmospheric Attenuation Reduction through Multisensor Fusion," <i>Sensor Fusion: Architectures, Algorithms, and Applications II, Proc. SPIE</i>, vol. 3376, Apr. 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>249</ref_seq_no>
				<ref_text><![CDATA[Van De Hulst, <i>Light Scattering by Small Particles.</i> John Wiley and Sons, 1957.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>250</ref_seq_no>
				<ref_text><![CDATA[R. L. Lee Jr., "Horizon Brightness Revisited: Measurements and a Model of Clear-Sky Radiances," <i>Applied Optics</i>, vol. 20, pp. 4620--4628, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>251</ref_seq_no>
				<ref_text><![CDATA[N. S. Kopeika, "General Wavelength Dependence of Imaging through the Atmosphere," <i>Applied Optics</i>, vol. 20, no. 9, May 1981.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>252</ref_seq_no>
				<ref_text><![CDATA[N. S. Kopeika, <i>A System Engineering Approach to Imaging.</i> SPIE Press, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>253</ref_seq_no>
				<ref_text><![CDATA[D. K. Lynch, "Step Brightness Changes of Distant Mountain Ridges and Their Perception," <i>Applied Optics</i>, vol. 30, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>254</ref_seq_no>
				<ref_text><![CDATA[S. Mahadev and R. C. Henry, "Color Perception through Atmospheric Haze," <i>J. Optical Soc. Am. A</i>, vol. 17, no. 5, May 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>255</ref_seq_no>
				<ref_text><![CDATA[E. J. McCartney, <i>Optics of the Atmosphere: Scattering by Molecules and Particles.</i> John Wiley and Sons, 1975.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>256</ref_seq_no>
				<ref_text><![CDATA[W. E. K. Middleton, <i>Vision through the Atmosphere.</i> Univ. of Toronto Press, 1952.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>257</ref_seq_no>
				<ref_text><![CDATA[M. Minnaert, <i>The Nature of Light and Color in the Open Air.</i> Dover Publications, Inc., 1954.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>258</ref_seq_no>
				<ref_text><![CDATA[T. Mitsunaga and S. K. Nayar, "Radiometric Self Calibration," <i>Proc. IEEE Conf. Computer Vision and Pattern Recognition</i>, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>259</ref_seq_no>
				<ref_text><![CDATA[P. Moon and D. E. Spencer, "Illumination from a Non-Uniform Sky," <i>Illuminating Eng.</i>, vol. 37, pp. 707--726, 1942.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>260</ref_seq_no>
				<ref_text><![CDATA[S. G. Narasimhan and S. K. Nayar, "Chromatic Framework for Vision in Bad Weather," <i>Proc. IEEE Conf. Computer Vision and Pattern Recognition</i>, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>261</ref_seq_no>
				<ref_text><![CDATA[S. G. Narasimhan and S. K. Nayar, "Removing Weather Effects from Monochrome Images," <i>Proc. IEEE Conf. Computer Vision and Pattern Recognition</i>, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>598511</ref_obj_id>
				<ref_obj_pid>598432</ref_obj_pid>
				<ref_seq_no>262</ref_seq_no>
				<ref_text><![CDATA[S. G. Narasimhan and S. K. Nayar, "Vision and the Atmosphere," <i>Int'l J. Computer Vision</i>, vol. 48, no. 3, pp. 233--254, Aug. 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>851572</ref_obj_id>
				<ref_obj_pid>850924</ref_obj_pid>
				<ref_seq_no>263</ref_seq_no>
				<ref_text><![CDATA[S. K. Nayar and S. G. Narasimhan, "Vision in Bad Weather," <i>Proc. Seventh Int'l Conf. Computer Vision</i>, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2319480</ref_obj_id>
				<ref_obj_pid>2318957</ref_obj_pid>
				<ref_seq_no>264</ref_seq_no>
				<ref_text><![CDATA[J. P. Oakley and B. L. Satherley, "Improving Image Quality in Poor Visibility Conditions Using a Physical Model for Degradation," <i>IEEE Trans. Image Processing</i>, vol. 7, Feb. 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>265</ref_seq_no>
				<ref_text><![CDATA["Museum of Science," Leonardo's Perspective. http://www.mos.org/sln/Leonardo/InvestigatingAerialP.html, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>266</ref_seq_no>
				<ref_text><![CDATA[M. J. Rakovic, G. W. Kattawar, M. Mehrubeoglu, B. D. Cameron, L. V. Wang, S. Rastegar, and G. L. Cote, "Light Backscattering Polarization Patterns from Turbid Media: Theory and Experiment," <i>Applied Optics</i>, vol. 38, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>267</ref_seq_no>
				<ref_text><![CDATA[M. P. Rowe, E. N. Pugh Jr., J. S. Tyo, and N. Engheta, "Polarization-Difference Imaging: A Biologically Inspired Technique for Observation through Scattering Media," <i>Optical Letters</i>, vol. 20, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>268</ref_seq_no>
				<ref_text><![CDATA[Y. Y. Schechner, S. G. Narasimhan, and S. K. Nayar, "Instant Dehazing of Images Using Polarization," <i>Proc. IEEE Conf. Computer Vision and Pattern Recognition</i>, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>269</ref_seq_no>
				<ref_text><![CDATA[Y. Y. Schechner, S. G. Narasimhan, and S. K. Nayar, "Polarization Based Vision through Haze," <i>Applied Optics</i>, special issue: light and color in the open air, vol. 42, no. 3, Jan. 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>136817</ref_obj_id>
				<ref_obj_pid>136809</ref_obj_pid>
				<ref_seq_no>270</ref_seq_no>
				<ref_text><![CDATA[S. Shafer, "Using Color to Separate Reflection Components," <i>Color Research and Applications</i>, pp. 210--218, 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>271</ref_seq_no>
				<ref_text><![CDATA[C. Stauffer and W. E. L. Grimson, "Adaptive Background Mixture Models for Real-Time Tracking," <i>Proc. IEEE Conf. Computer Vision and Pattern Recognition</i>, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>272</ref_seq_no>
				<ref_text><![CDATA[K. Tan and J. P. Oakley, "Enhancement of Color Images in Poor Visibility Conditions," <i>Proc. Int'l Conf. Image Processing</i>, vol. 2, Sept. 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>273</ref_seq_no>
				<ref_text><![CDATA[K. Tan and J. P. Oakley, "Physics Based Approach to Color Image Enhancement in Poor Visibility Conditions," <i>J. Optical Soc. Am. A</i>, vol. 18, no. 10, pp. 2460--2467, Oct. 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>274</ref_seq_no>
				<ref_text><![CDATA[S. Ullman, "On the Visual Detection of Light Sources," <i>Biological Cybernetics</i>, pp. 205--212, 1976.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>275</ref_seq_no>
				<ref_text><![CDATA[J. G. Walker, P. C. Y. Chang, and K. I. Hopcraft, "Visibility Depth Improvement in Active Polarization Imaging in Scattering Media," <i>Applied Optics</i>, vol. 39, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>276</ref_seq_no>
				<ref_text><![CDATA[W. L. Wolfe and G. J. Zissis, <i>The Infrared Handbook.</i> Prepared for Office of Naval Research, Dept. of the Navy, 1978.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>277</ref_seq_no>
				<ref_text><![CDATA[Y. Yitzhaky, I. Dror, and N. S. Kopeika, "Restoration of Altmospherically Blurred Images According to Weather-Predicted Atmospheric Modulation Transfer Function," <i>Optical Eng.</i>, vol. 36, Nov. 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>278</ref_seq_no>
				<ref_text><![CDATA[L. Grewe and R. R. Brooks, "Atmospheric attenuation reduction through multi-sensor fusion," in <i>Sensor Fusion: Architectures, Algorithms, and Applications II</i>, B. V. Dasarathy, ed., Proc. SPIE &#60;b&#62;3376&#60;/b&#62;, 102--109 (1998).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>279</ref_seq_no>
				<ref_text><![CDATA[N. S. Kopeika, <i>A System Engineering Approach to Imaging</i> (SPIE, Bellingham, Wash., 1998), pp. 446--452.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2319480</ref_obj_id>
				<ref_obj_pid>2318957</ref_obj_pid>
				<ref_seq_no>280</ref_seq_no>
				<ref_text><![CDATA[J. P. Oakley and B. L. Satherley, "Improving image quality in poor visibility conditions using a physical model for contrast degradation," IEEE Trans. Imag. Proc. &#60;b&#62;7&#60;/b&#62;, 167--179 (1998).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>281</ref_seq_no>
				<ref_text><![CDATA[K. Tan and J. P. Oakley, "Physics-based approach to color image enhancement in poor visibility conditions," J. Opt. Soc. Am. A &#60;b&#62;18&#60;/b&#62;, 2460--2467 (2001).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>282</ref_seq_no>
				<ref_text><![CDATA[S. G. Narasimhan and S. K. Nayar, "Chromatic framework for vision in bad weather," in <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i> (Institute of Electrical and Electronics Engineers, New York, 2000), Vol. I, pp. 598--605.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>283</ref_seq_no>
				<ref_text><![CDATA[S. G. Narasimhan and S. K. Nayar, "Removing weather effects from monochrome images," in <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i> (Institute of Electrical and Electronics Engineers, New York, 2001), Vol. II, pp. 186--193.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>851572</ref_obj_id>
				<ref_obj_pid>850924</ref_obj_pid>
				<ref_seq_no>284</ref_seq_no>
				<ref_text><![CDATA[S. K. Nayar and S. G. Narasimhan, "Vision in bad weather," in <i>Proceedings of the IEEE International Conference on Computer Vision</i> (Institute of Electrical and Electronics Engineers, New York, 1999), pp. 820--827.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>285</ref_seq_no>
				<ref_text><![CDATA[P. S. Pencikowski, "Low-cost vehicle-mounted enhanced vision system comprised of a laser illuminator and range-gated camera," in <i>Enhanced and Synthetic Vision</i>, J. G. Verly, ed., Proc. SPIE &#60;b&#62;2736&#60;/b&#62;, 222--227 (1996).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>286</ref_seq_no>
				<ref_text><![CDATA[B. T. Sweet and C. L. Tiana, "Image processing and fusion for landing guidance," in <i>Enhanced and Synthetic Vision</i>, J. G. Verly, ed., Proc. SPIE &#60;b&#62;2736&#60;/b&#62;, 84--95 (1996).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>287</ref_seq_no>
				<ref_text><![CDATA[R. C. Henry, S. Mahadev, S. Urquijo, and D. Chitwood, "Color perception through atmospheric haze," J. Opt. Soc. Am. A &#60;b&#62;17&#60;/b&#62;, 831--835 (2000).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>288</ref_seq_no>
				<ref_text><![CDATA[D. K. Lynch, "Step brightness changes of distant mountain ridges and their perception," Appl. Opt. &#60;b&#62;30&#60;/b&#62;, 3508--3513 (1991).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>289</ref_seq_no>
				<ref_text><![CDATA[S. D. Gedzelman, "Atmospheric optics in art," Appl. Opt. &#60;b&#62;30&#60;/b&#62;, 3514--3522 (1991).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>794397</ref_obj_id>
				<ref_obj_pid>794189</ref_obj_pid>
				<ref_seq_no>290</ref_seq_no>
				<ref_text><![CDATA[F. Cozman and E. Krotkov, "Depth from scattering," in <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i> (Institute of Electrical and Electronics Engineers, New York, 1997), pp. 801--806.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>291</ref_seq_no>
				<ref_text><![CDATA[W. A. Shurcliff and S. S. Ballard, <i>Polarized Light</i> (Van Nostrand, Princeton, N.J., 1964), pp. 98--103.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>292</ref_seq_no>
				<ref_text><![CDATA[G. P. K&#246;nnen, <i>Polarized Light in Nature</i> (Cambridge University, Cambridge, UK, 1985), pp. 1--10, 29--54, 60--62, 131--137, 144--145.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>293</ref_seq_no>
				<ref_text><![CDATA[B. Cairns, B. E. Carlson, A. A. Lacis, and E. E. Russell, "An analysis of ground-based polarimetric sky radiance measurements," in <i>Polarization: Measurement, Analysis, and Remote Sensing</i>, D. H. Goldstein and R. A. Chipman, eds., Proc. SPIE &#60;b&#62;3121&#60;/b&#62;, 382--393 (1997).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>294</ref_seq_no>
				<ref_text><![CDATA[K. L. Coulson, "Polarization of light in the natural environment," in <i>Polarization Considerations for Optical Systems II</i>, R. A. Chipman, ed., Proc. SPIE &#60;b&#62;1166&#60;/b&#62;, 2--10 (1989).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>295</ref_seq_no>
				<ref_text><![CDATA[S. J. Hitzfelder, G. N. Plass, and G. W. Kattawar, "Radiation in the earth's atmosphere: its radiance, polarization, and ellipticity," Appl. Opt. &#60;b&#62;15&#60;/b&#62;, 2489--2500 (1976).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>296</ref_seq_no>
				<ref_text><![CDATA[D. K. Lynch and P. Schwartz, "Rainbows and fogbows," Appl. Opt. &#60;b&#62;30&#60;/b&#62;, 3415--3420 (1991).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>297</ref_seq_no>
				<ref_text><![CDATA[M. S. Quinby-Hunt, L. L. Erskine, and A. J. Hunt, "Polarized light scattering by aerosols in the marine atmospheric boundary layer," Appl. Opt. &#60;b&#62;36&#60;/b&#62;, 5168--5184 (1997).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>298</ref_seq_no>
				<ref_text><![CDATA[M. J. Rakovi&#263;, G. W. Kattawar, M. Mehr&#252;beo&#487;lu, B. D. Cameron, L. V. Wang, S. Rastegar, and G. L. Cot&#233;, "Light back-scattering polarization patterns from turbid media: theory and experiment," Appl. Opt. &#60;b&#62;38&#60;/b&#62;, 3399--3408 (1999).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>299</ref_seq_no>
				<ref_text><![CDATA[D. B. Chenault and J. L. Pezzaniti, "Polarization imaging through scattering media," in <i>Polarization Analysis, Measurement, and Remote Sensing III</i>, D. B. Chenault, M. J. Guggin, W. G. Egan, and D. H. Goldstein, eds., Proc. SPIE &#60;b&#62;4133&#60;/b&#62;, 124--133 (2000).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>300</ref_seq_no>
				<ref_text><![CDATA[L. J. Denes, M. Gottlieb, B. Kaminsky, and P. Metes, "AOTF polarization difference imaging," in <i>27th AIPR Workshop: Advances in Computer-Assisted Recognition</i>, R. J. Mericsko, ed., Proc. SPIE &#60;b&#62;3584&#60;/b&#62;, 106--115 (1998).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>301</ref_seq_no>
				<ref_text><![CDATA[O. Emile, F. Bretenaker, and A. Le Floch, "Rotating polarization imaging in turbid media," Opt. Lett. &#60;b&#62;21&#60;/b&#62;, 1706--1708 (1996).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>302</ref_seq_no>
				<ref_text><![CDATA[X. Gan, S. P. Schilders, and Min Gu, "Image enhancement through turbid media under a microscope by use of polarization gating method," J. Opt. Soc. Am. A &#60;b&#62;16&#60;/b&#62;, 2177--2184 (1999).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>303</ref_seq_no>
				<ref_text><![CDATA[H. Horinaka, K. Hashimoto, K. Wada, T. Umeda, and Y. Cho, "Optical CT imaging in highly scattering media by extraction of photons preserving initial polarization," in <i>International Symposium on Polarization Analysis and Applications to Device Technology</i>, T. Yoshizawa and H. Yokota, eds., Proc. SPIE &#60;b&#62;2873&#60;/b&#62;, 54--57 (1996).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>304</ref_seq_no>
				<ref_text><![CDATA[M. P. Rowe, E. N. Pugh Jr., J. S. Tyo, and N. Engheta, "Polarization-difference imaging: a biologically inspired technique for observation through scattering media," Opt. Lett. &#60;b&#62;20&#60;/b&#62;, 608--610 (1995).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>305</ref_seq_no>
				<ref_text><![CDATA[J. G. Walker, P. C. Y. Chang, and K. I. Hopcraft, "Visibility depth improvement in active polarization imaging in scattering media," Appl. Opt. &#60;b&#62;39&#60;/b&#62;, 4933--4941 (2000).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>306</ref_seq_no>
				<ref_text><![CDATA[Y. Y. Schechner, J. Shamir, and N. Kiryati, "Polarization and statistical analysis of scenes containing a semireflector," J. Opt. Soc. Am. A &#60;b&#62;17&#60;/b&#62;, 276--284 (2000).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>307</ref_seq_no>
				<ref_text><![CDATA[H. Farid and E. H. Adelson, "Separating reflections from images by use of independent component analysis," J. Opt. Soc. Am. A &#60;b&#62;16&#60;/b&#62;, 2136--2145 (1999).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>248634</ref_obj_id>
				<ref_obj_pid>248633</ref_obj_pid>
				<ref_seq_no>308</ref_seq_no>
				<ref_text><![CDATA[S. K. Nayar, X. S. Fang, and T. Boult, "Separation of reflection components using color and polarization," Int. J. Comput. Vision &#60;b&#62;21&#60;/b&#62;, 163--186 (1997).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>309</ref_seq_no>
				<ref_text><![CDATA[S. Rahmann and N. Canterakis, "Reconstruction of specular surfaces using polarization imaging," in <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i> (Institute of Electrical and Electronics Engineers, New York, 2001), Vol. 1, pp. 149--155.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>310</ref_seq_no>
				<ref_text><![CDATA[M. Saito, Y. Sato, K. Ikeuchi, and H. Kashiwagi, "Measurement of surface orientations of transparent objects by use of polarization in highlight," J. Opt. Soc. Am. A &#60;b&#62;16&#60;/b&#62;, 2286--2293 (1999).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1709131</ref_obj_id>
				<ref_obj_pid>1709130</ref_obj_pid>
				<ref_seq_no>311</ref_seq_no>
				<ref_text><![CDATA[L. B. Wolff, "Polarization vision: a new sensory approach to image understanding," Image Vision Comput. &#60;b&#62;15&#60;/b&#62;, 81--93 (1997).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>312</ref_seq_no>
				<ref_text><![CDATA[C. F. Bohren and A. B. Fraser, "At what altitude does the horizon cease to be visible?" Am. J. Phys. &#60;b&#62;54&#60;/b&#62;, 222--227 (1986).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>313</ref_seq_no>
				<ref_text><![CDATA[E. J. McCartney, <i>Optics of the Atmosphere: Scattering by Molecules and Particles</i> (Wiley, New York, 1976).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>314</ref_seq_no>
				<ref_text><![CDATA[J. S. Tyo, M. P. Rowe, E. N. Pugh Jr., and N. Engheta, "Target detection in optically scattering media by polarization-difference imaging," Appl. Opt. &#60;b&#62;35&#60;/b&#62;, 1855--1870 (1996).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>315</ref_seq_no>
				<ref_text><![CDATA[R. L. Lee Jr., "Digital imaging of clear-sky polarization," Appl. Opt. &#60;b&#62;37&#60;/b&#62;, 1465--1476 (1998).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>316</ref_seq_no>
				<ref_text><![CDATA[E. Hecht, <i>Optics</i>, 3rd ed. (Addison-Wesley, New York, 1998), pp. 340--342.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>317</ref_seq_no>
				<ref_text><![CDATA[S. Chandrasekhar, <i>Radiative Transfer</i> (Dover, New York, 1960), pp. 24--37, 280--284.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>318</ref_seq_no>
				<ref_text><![CDATA[M. Ben-Ezra, "Segmentation with invisible keying signal," in <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i> (Institute of Electrical and Electronics Engineers, New York, 2000), Vol. 1, pp. 32--37.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>319</ref_seq_no>
				<ref_text><![CDATA[T. Prosch, D. Hennings, and E. Raschke, "Video polarimetry: a new imaging technique in atmospheric science," Appl. Opt. &#60;b&#62;22&#60;/b&#62;, 1360--1363 (1983).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>320</ref_seq_no>
				<ref_text><![CDATA[A. M. Shutov, "Videopolarimeters," Sov. J. Opt. Technol. &#60;b&#62;60&#60;/b&#62;, 295--301 (1993).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>321</ref_seq_no>
				<ref_text><![CDATA[L. B. Wolff, "Polarization camera for computer vision with a beam splitter," J. Opt. Soc. Am. A &#60;b&#62;11&#60;/b&#62;, 2935--2945 (1994).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>527570</ref_obj_id>
				<ref_seq_no>322</ref_seq_no>
				<ref_text><![CDATA[A. S. Glassner, <i>Principles of Digital Image Synthesis</i> (Morgan Kaufmann, San Francisco, Calif., 1995), Appen. G.4.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>323</ref_seq_no>
				<ref_text><![CDATA[R. L. Lee Jr., "Horizon brightness revisited: measurements and a model of clear-sky radiances," Appl. Opt. &#60;b&#62;33&#60;/b&#62;, 4620--4628 (1994).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>324</ref_seq_no>
				<ref_text><![CDATA[Y. Y. Schechner, S. G. Narasimhan, and S. K. Nayar, "Instant dehazing of images using polarization," in <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i> (Institute of Electrical and Electronics Engineers, New York, 2001), Vol. 1, pp. 325--332.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>325</ref_seq_no>
				<ref_text><![CDATA[J. E. Solomon, "Polarization imaging," Appl. Opt. &#60;b&#62;20&#60;/b&#62;, 1537--1544 (1981).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>326</ref_seq_no>
				<ref_text><![CDATA[For the calculation of the path radiance integral, we assume k to be distance invariant. This is because typically most of the light in the scene comes from the Sun and sky and thus does not change much along the line of sight. Moreover, we assume that multiple scattering (which effects the angular scattering distribution) is dominated by single scattering.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SIGGRAPH 2009 Scattering Course notes Diego Gutierrez* Wojciech Jarosz Srinivasa G. Narasimhan Universidad 
de Zaragoza Disney Research, Zurich Carnegie Mellon University Craig Donner§ Columbia University  *diegog@unizar.es 
wjarosz@cs.ucsd.edu srinivas@cs.cmu.edu §cdonner@cs.columbia.edu 1 About the Lecturers Diego Gutierrez 
University of Zaragoza http://giga.cps.unizar.es/ diegog Diego Gutierrez is an Associate Professor at 
the University of Zaragoza, in Spain, where he received his PhD in Computer Science. His areas of expertise 
include physically based global illumination (specializing in participating media), perception and novel 
image processing techniques. He was Chair of the SIGGRAPH Asia Sketches &#38; Posters programme in 2008, 
and was Papers Chair for ACM Graphite in 2006. He s served on many other programme committees, including 
SIGGRAPH Asia 2009 and Eurographics 2007. Wojciech Jarosz Disney Research, Zurich http://graphics.ucsd.edu/ 
wjarosz Wojciech Jarosz is a computer graphics Ph.D. student at UC San Diego. His main research interest 
is production-quality global illumination techniques, speci.cally for participating media, and his current 
list of publications include three SIGGRAPH papers on those topics. He received his B.S. in Computer 
Science from University of Illinois, Urbana-Champaign and his M.S. in Computer Science from UC San Diego. 
Srinivasa G. Narasimham Carnegie Mellon University http://www.cs.cmu.edu/ srinivas/ Srinivasa Narasimhan 
is an Assistant Professor in the School of Computer Science at Carnegie Mellon University (since 2004). 
He obtained his masters and doctoral degrees in Computer Science from Columbia University in 2000 and 
2004 respectively. His research interests are at the intersection of computer vision, computer graphics 
and optics. He received a Best Paper award in IEEE CVPR 2000 and a CAREER award from NSF in 2007. Craig 
Donner Columbia University http://www.cs.columbia.edu/ cdonner/ Craig is currently a Postdoctoral Research 
Scientist at Columbia University. His core research interests are appearance modeling and global illumination 
in the context of photorealistic image synthesis, as well as the acquisition and modeling of light transport 
in complex materials. He has served on the international papers committee for the Eurographics Symposium 
on Rendering for the past two years, and has many publications concerning light scattering, including 
four SIGGRAPH papers and three journal articles. 2 2CourseAbstract Computer graphics and computer vision 
techniques deal with acquiring, interpreting and presenting the rich visual world around us. These are 
exciting multi-disciplinary .elds of research with a wide spectrum of applications that can impact our 
daily lives. However, most of the computer generated imagery today represents scenes with clear atmospheres, 
neglecting light scattering e.ects. Analogously, most computer vision systems have not enjoyed success 
when deployed in uncontrolled outdoor environments. Nevertheless, scattering is a fundamental aspect 
of light transport in a wide range of applications, whether simulating it or interpreting it, from medical 
imaging to driving simulators or underwater imagery. In this course we address the challenges that arise 
when faced with light scattering both in a computer graphics and a computer vision context. Both .elds 
have seen great advances over the past few years; however, most of the existing algorithms still assume 
that light emitted by a source or re.ected o. a surface reaches the sensor unaltered. From a computer 
graphics perspective, this is due mainly to the complex interactions that occur and the high computational 
costs of simulating them. In computer vision, scattering has traditionally been considered as noise that 
one should ideally get rid of. Scattering e.ects are one fundamental hurdle that must be overcome to 
signi.cantly extend and enhance current state-of-the-art graphics and vision techniques and achieve successful 
impact in a wide range of domains. Given the increasing overlapping of both .elds, including new and 
hot research .elds such as computational photography, we believe the course will be useful for both communities 
and everybody with research interests at the intersection of the two. We hope to increase awareness about 
this area and open up new research directions. Topics discussed include appearance modeling, underwater 
imagery, vision in bad weather and measurement techniques. This course is intended for people involved 
in computer graphics, computer vision or related .elds such as computational photography. It will provide 
a good understating of scattering phenomena, state-of-the-art techniques to simulate it and treat it 
in computer graphics and computer vision contexts, and a wide range of applications. More precisely, 
it is targeted to delegates interested in general .elds such as computer graphics, computer vision or 
computational photography, or in more particular applications such as medical imaging, oceanography, 
driving simulators, game developers 3 Introduction Our current understanding of the behavior of light 
relies on a progression of increasingly complete yet complicated models of light. These are (see Figure 
1): ray optics, wave optics, electromagnetic optics, and quantum optics saleh07fundamentals. Computer 
graphics typically relies on the simplest of these models, ray optics (also called geometric optics). 
This model makes several simplifying assumptions about the behavior of light that limit the types of 
phenomena that can be simulated. In essence, in this model light can only be emitted, re.ected, and transmitted. 
Additionally, light is assumed to travel in straight lines and at in.nite speed. This means that e.ects 
explained by the higher-level models cannot (easily) be incorporated into our simulations. In ray optics, 
e.ects such as di.raction and interference (wave optics), polarization and dispersion (electromagnetic 
optics), and .uorescence and phosphorescence (quantum optics) are completely ignored. In spite of these 
limitations, we are still able to correctly simulate a wide range of physical phenomena. Figure 1: The 
theory of light is described by a series of increasingly complete optical models, where each successive 
model is able to account for more optical phenomena. In computer graphics, we often restrict ourselves 
to the simplest model, ray optics. In most computer graphics applications, assumptions are made about 
the properties of the scattering media in order to more easily derive expressions about the behavior 
of light. In particular, we assume that the participating media can be modeled as a collection of microscopic 
particles. Since the particles are microscopic and randomly positioned, we do not represent each individual 
particle in the lighting simulation. Instead, we consider the aggregate probabilistic behavior as light 
travels through the medium. Moreover, these particles are assumed to be spaced far apart relative to 
the size of an individual particle. This assumption implies that as a photon travels through the medium 
and interacts at a particle, this interaction is statistically independent from the outcome of subsequent 
interaction events (Figure 2). 4 Figure 2: We treat participating media as a collection of microscopic 
scattering particles. When light travels through the medium, a change of radiance may occur as the photons 
interact with the particles. Real-time Rendering Techniques for Participating Media Most of computer 
generated imagery today in video games, movies, and scienti.c simulations are of scenes on clear days 
or nights. Volumetric e.ects such as the beautiful fog rolling down the hills, the bluish haze of mountains, 
the eerie night fog or mist reminiscent of Hitchcockian movies, the splendor and brilliance of underwater 
e.ects, the light streaming through clouds or the sun rising over the ocean provide pure artistic and 
entertainment value. They are used in movies and paintings to portray di.erent moods, used in photographs 
to provide realism, and used even for training in safety and hazardous situations. In the absence of 
such e.ects, current attempts at renderings appear unnatural and cartoonish. Thus, it becomes critical 
to render these e.ects accurately for achieving photo-realism. (a) (b) (c) (d) Figure 3: Real time rendering 
of participating media:. (a)-(b) Real time rendering of homogenous media using an analytic single-scattering 
model of light transport. (a) Clear scene. (b) Scene with fog added. (c)-(d) Fast rendering of smooth, 
non-homogenous and dynamic media. (c) Clear day scene. (d) Scene with fog added. Rendering of participating 
media requires solving complex light transport equations. Brute-force sim­ulations of light transport 
based on Monte Carlo and .nite element simulation can be prohibitively slow (taking CPU-days or even 
weeks). However, a variety of applications spanning entertainment (video games), medicine (surgery) and 
autonomous navigation require real-time or interactive performance. Thus, a recent research thrust has 
been to make smoothness assumptions, either on the media [5, 22] or the lighting [21]in order to achieve 
interactive rates. Sun et al [22] have implemented their model in modern programmable graphics hardware 
using a few small numerical lookup tables stored as texture maps, and achieved truly real time performance. 
Gupta et al [5] present a technique for fast rendering of non-homogenous, as well as dynamic media, by 
representing the density and intensity .elds in a low-dimensional basis. Sloan et al [21] 5 use the 
concept of pre-computed radiance transfer to allow for real-time changes in the environment lighting. 
Snapshots from a couple of representative papers are shown in Figure 3.  Measuring Scattering Properties 
of Participating Media The appearance of participating media is governed by their optical properties, 
which must be input to a rendering algorithm to generate realistic images. Even with the most accurate 
rendering algorithms, the image quality is often limited by the accuracy of these input parameters. Narasimhan 
et al presented a simple device and technique [14] for robustly estimating the scattering properties 
of a broad class of participating media such as juices, beverages, sugar/salt crystals, and suspended 
impurities (ocean water) from a single HDR photograph. They measured the scattering parameters of forty 
commonly found materials. The results are compiled into a freely available database which can be immediately 
used by the computer graphics community to render realistic images of arbitrary concentrations of the 
material with multiple scattering (Figure 4), as well as create realistic images of combinations of the 
original materials. This technique can be used to design portable devices as well, that can be used for 
in-situ measurements of impurity levels in natural water bodies (oceans, lakes, rivers) for environmental 
monitoring. (a) (b) (c) Figure 4: Measuring scattering properties of participating media: Renderings 
of a scene with four liquids in their diluted states (b) and their natural high density states (c). The 
corresponding input HDR images are shown in (a). Debevec et al presented a technique [9] for capturing 
time-varying volumetric data of participating media. In their technique, a laser sheet is swept repeatedly 
through the volume, and the scattered light is imaged using a high-speed camera. Each sweep of the laser 
provides a near-simultaneous volume of density values. They demonstrated rendered animations under changing 
viewpoint and illumination, making use of measured values for the scattering phase function and albedo. 
An example rendering is shown in Figure 5.  Computer Vision in Bad Weather In recent years, computer 
vision has seen signi.cant advances in the core areas of image sensing and interpretation. This success 
has resulted in great demand for vision techniques in application domains ranging from intelligent transportation 
and security to oceanography (underwater imaging), to astronomy (telescope 6 (a) (b) Figure 5: Measuring 
scattering properties of dynamic participating media:. Measured media can be rendered under a variety 
of lighting conditions. (a) A captured smoke volume rendered with two spotlights of di.erent colors. 
(c) A smoke volume rendered with environmental illumination. Images taken from [9]. and satellite imaging), 
to even biology and medical systems (microscopic and medical imaging). However, unfortunately, most computer 
vision systems have not enjoyed success when deployed in uncontrolled outdoor environments. Images of 
outdoor scenes captured in bad weather su.er from poor contrast. Under bad weather condi­tions, the light 
reaching a camera is severely scattered by the atmosphere. The resulting decay in contrast varies across 
the scene and is exponential in the depths of scene points. Therefore, traditional space invari­ant image 
processing techniques are not su.cient to remove weather e.ects from images. In such scenarios, physics-based 
models that describe the image-formation process in bad weather conditions can be used to restore contrast 
of images [17, 15, 16, 20]. Moreover, changes in intensities of scene points under di.erent weather conditions 
can be used as cues to compute scene structure (see Figures 6 and 7). These techniques can be applied 
to gray scale, RGB color, multispectral and even IR images. More recently, Fattal et al [4] presented 
a new method for dehazing the image and estimating the scene structure, given only a single input image, 
as compared to previous techniques which required multiple images. Sample results are shown in Figure 
9. This technique has been used for applications such as image refocusing and novel view synthesis. 
(a) (b) (c) Figure 6: Contrast restoration using scattering-based image formation model: (a) Image of 
a tra.c scene on a foggy day. (b) The defogged (contrast restored) image. (c) Depth map computed from 
two foggy images. . Experiments with videos of a tra.c scene on a foggy day. Images taken from [17]. 
 Multiple Scattering: Virtually all the previous methods in image processing and computer vision, 7 (a) 
(b) (c) Figure 7: Contrast restoration using polarization imaging: Images of the (a) perpendicular and 
(b) parallel polarization components. The parallel component has the best image contrast that optics 
alone can give, but in this case it is only slightly better than the contrast in the image of the worst 
polarization state. (c) The dehazed image has much better contrast and color than what optical .ltering 
alone yields, especially in the distant regions of the scene. Images taken from [20].  Figure 8: Dehazing 
based on a single input image and the corresponding depth estimate. Images taken from [4]. for removing 
weather e.ects from images, assume single scattering of light by particles in the atmosphere. In reality, 
multiple scattering e.ects are signi.cant. A common manifestation of multiple scattering is the appearance 
of glows around light sources in bad weather. Modeling multiple scattering is critical to understanding 
the complex e.ects of weather on images, and hence essential for improving the performance of outdoor 
vision systems. Narasimhan et al [18] develop a new physics-based model for the multiple scattering of 
light rays as they travel from a source to an observer. This model is valid for various weather conditions 
including fog, haze, mist and rain. Using this model, the shapes and depths of sources in the scene can 
be recovered. In addition, the weather condition and the visibility of the atmosphere can be estimated, 
thus making a camera observing a distant source as a visual weather meter. A bio-optical model of ocean 
water Simulating the in-water ocean light .eld is a daunting task. Ocean waters are one of the richest 
partici­pating media, where light interacts not only with water molecules, but with suspended particles 
and organic matter as well. The concentration of each constituent greatly a.ects these interactions, 
resulting in very di.erent hues. Inelastic scattering events such as .uorescence or Raman scattering 
imply energy transfers that are usually neglected in the simulations. A bio-optical model of ocean waters 
is presented in [8], along with a method to obtain the in-water light .eld based on radiative transfer 
theory. The bio-optical model 8 of the ocean uses published data from oceanography studies. The method 
builds on [7], and provides a link between the inherent optical properties that de.ne the medium and 
its apparent optical properties,which describe how it looks. For inelastic scattering, all frequency 
changes at higher and lower energy values are taken into account, based on the spectral quantum e.ciency 
function of the medium. Areas of applica­tion for this research span from underwater imagery to remote 
sensing; the resolution method is general enough to be usable in any type of participating medium simulation. 
Figure ?? shows some results varying the concentration of some of the components of the model: chlorophyll, 
minerals and detritus and yellow matter. Figure 9: Resulting pictures varying the chlorophyll concentration 
C, the minerals and detritus turbidity ad at 400nm and the CDOM turbidity ay at 440nm.  Underwater Imaging 
Poor visibility conditions due to murky water, bad weather, dust and smoke severely impede the perfor­mance 
of vision systems. A variety of passive methods, most notably based on polarization analysis have been 
used to restore scene contrast under moderate visibility by digital postprocessing [19]. However, such 
methods are ine.ective when the quality of acquired images is poor to begin with. Active vision systems 
control light transport even before the image is formed, and hence result in superior performance [13, 
6, 12]. Most of the underwater active vision systems are based on using structured lighting. Traditionally, 
9 (a) (b) (c) structured light methods assume that the scene and the sources are immersed in pure air 
and that light is neither scattered nor absorbed. Recently, however, structured lighting has found growing 
application in underwater and aerial imaging, where scattering e.ects cannot be ignored. A variety of 
structured light techniques, such as confocal imaging, structured light striping, polarized light striping 
have been used to recover scene albedos, depths and normals reliably, even in murky underwater conditions. 
These results have been used to restore the appearances of scenes as if captured in clear air. A compilation 
of results is shown in Figure 10.  E.cient Rendering of Highly Scattering Materials Highly scattering, 
or translucent, materials are all around us. For example, milk, leaves, skin, paper, paint, and marble 
are all highly scattering materials. These make up the foods and liquids we eat and drink, and the materials 
that compose everyday objects such as plastics and paper. They make up our environment, and even the 
various tissues of our own bodies. Translucent materials often have a soft, di.use appearance when viewed 
under direct lighting, and sometimes have a glow when backlit. To e.ciently render highly scattering 
materials, the di.usion dipole model, composed of a positive and negative point source, gives a good 
approximation [10], under the assumption that the material is homoge­ neous and semi-in.nite in depth. 
The main limitations of the dipole model are its restriction to semi-in.nite geometry and homogeneous 
materials. This is resolved by using arrays of multiple dipoles (a multipole) and convolutions of layer 
pro.les [1]. These extra sources take boundary conditions at the interfaces of thin slabs, and between 
the boundaries of scattering layers to produce re.ectance and transmittance pro.les for each layer. Convolving 
these pro.les together allows the rendering of layered materials like leaves, paint, and skin (see Figure 
11), while these pro.les by themselves can be used to render thin materials like paper. Determining 
parameters for the above models, however, is a di.cult task. Direct acquisition of the properties of 
translucent materials is time-consuming and expensive, and can even be inaccurate, due to the high orders 
of light scattering [11]. Instead, parameters can be estimated by .tting to a model, as done in 10 
the case of faces [23]. Here, the dipole model allowed the .tting of optical properties and albedo maps 
for a large database of human faces, and allowed the analysis of facial traits, along with novel facial 
renderings. More speci.c material models give more intuitive and .ner controls over appearance. For example, 
the multipole model provides a basis for a spectral model of human skin s appearance based on skin s 
chemical and structural composition [2]. Using only four parameters (two types of melanin, hemoglobin, 
and oiliness) based on real physical properties of skin, this model produces realistic images of many 
skin types (see Figure 12, top). It allows the intuitive description of appearance through these parameters, 
and compares well with actual measured re.ectance of real skin samples. Unfortunately, this skin model 
requires the use of albedo textures to modulate the layered-homogeneous translucent re.ectance predicted 
by the multipole model. Fully controllable spatially varying properties are controllable using a heterogeneous 
skin model [3]. In addition, using a simple acquisition setup (see Figure 12, middle), the properties 
of multi-layered skin are recovered through an inverse rendering process. This heterogeneous model allows 
the synthesis of dynamic changes in skin, and other e.ects such as inserting arti.cial pigment (e.g. 
a tattoo) between skin layers (see Figure 12, bottom). 11 12  References [1] Donner, C., and Jensen, 
H. W. Light di.usion in multi-layered translucent materials. 24, 3 (2005), 1032 1039. [2] Donner, C., 
and Jensen, H. W. A spectral BSSRDF for shading human skin. In Rendering Techniques (2006), pp. 409 417. 
[3] Donner, C., Weyrich, T., d Eon, E., Ramamoorthi, R., and Rusinkiewicz, S. A layered, heterogeneous 
re.ectance model for acquiring and rendering human skin. 27, 5 (2008), 1 12. [4] Fattal, R. Single image 
dehazing. ACM Transactions on Graphics (SIGGRAPH 2008) 27,3. [5] Gupta, M., and Narasimhan, S. G. Legendre 
.uids: A uni.ed framework for analytic reduced space modeling and rendering of participating media. In 
Eurographics/ ACM SIGGRAPH Symposium on Computer Animation (2007) (August 2007). [6] Gupta, M., Narasimhan, 
S. G., and Schechner, Y. Y. On controlling light transport in poor visibility environments. In Proceedings 
IEEE CVPR (June 2008). [7] Gutierrez, D., Munoz, A., Anson, O., and Seron, F. Non-linear volume photon 
mapping. Rendering Techniques (Eurographics Symposium on Rendering), 291 300. [8] Gutierrez, D., Seron, 
F., Anson, O., and Munoz, A. Visualizing underwater ocean optics. Computer Graphics Forum (EUROGRAPHICS 
2008) 27, 2, 547 556. [9] Hawkins, T., Einarsson, P., and Debevec, P. Acquisition of time-varying participating 
media. ACM Transactions on Graphics (SIGGRAPH 2005) 24, 3 (2005). [10] Jensen, H. W., Marschner, S. R., 
Levoy, M., and Hanrahan, P. A practical model for subsur­face light transport. In Proceedings of SIGGRAPH 
2001 (2001), pp. 511 518. [11] Joshi, N., Donner, C., and Jensen, H. W. Noninvasive measurement of scattering 
anisotropy in turbid materials by nonnormal incident illumination. 31 (2006), 936 938. [12] Levoy, M., 
Chen, B., Vaish, V., Horowitz, M., McDowall, I., and Bolas, M. Synthetic aperture confocal imaging. ACM 
Transactions on Graphics (SIGGRAPH 2004) 23, 3, 825 834. [13] Narasimhan, S., and Nayar, S. Structured 
light methods for underwater imaging: light stripe scanning and photometric stereo. OCEANS, 2005. Proceedings 
of MTS/IEEE (2005), 2610 2617 Vol. 3. [14] Narasimhan, S. G., Gupta, M., Donner, C., Ramamoorthi, R., 
Nayar, S. K., and Jensen, H. W. Acquiring scattering properties of participating media by dilution. ACM 
Transactions on Graphics (SIGGRAPH 2006) 25, 3 (2006). [15] Narasimhan, S. G., and Nayar, S. Interactive 
deweathering of an image using physical models. In IEEE IEEE Workshop on Color and Photometric Methods 
in Computer Vision, In Conjunction with ICCV (October 2003). [16] Narasimhan, S. G., and Nayar, S. K. 
Vision and the atmosphere. IJCV 48, 3 (2002), 233 254. [17] Narasimhan, S. G., and Nayar, S. K. Contrast 
restoration of weather degraded images. IEEE PAMI 25, 6 (June 2003), 713 724. 13 [18] Narasimhan, S. 
G., and Nayar, S. K. Shedding light on the weather. In Proceedings of the 2003 IEEE Computer Society 
Conference on Computer Vision and Pattern Recognition (June 2003), vol. 1, pp. 665 672. [19] Schechner, 
Y. Y., and Karpel, N. Recovery of underwater visibility and structure by polarization analysis. Oceanic 
Engineering, IEEE Journal of 30, 3 (July 2005), 570 587. [20] Schechner, Y. Y., Narasimhan, S. G., and 
Nayar, S. K. Instant dehazing of images using polarization. In Proceedings of the 2001 IEEE Computer 
Society Conference on Computer Vision and Pattern Recognition (June 2001), vol. 1, pp. 325 332. [21] 
Sloan, P.-P., Kautz, J., and Snyder, J. Precomputed radiance transfer for real-time rendering in dynamic, 
low-frequency lighting environments. ACM Transactions on Graphics (SIGGRAPH 2002) 21, 3 (2002). [22] 
Sun, B., Ramamoorthi, R., Narasimhan, S. G., and Nayar, S. K. A practical analytic single scattering 
model for real-time rendering. ACM Transactions on Graphics (SIGGRAPH 2005) (August 2005). [23] Weyrich, 
T., Matusik, W., Pfister, H., Bickel, B., Donner, C., Tu, C., McAndless, J., Lee, J., Ngan, A., Jensen, 
H. W., and Gross, M. Analysis of human faces using a measurement­based skin re.ectance model. 25 (2006), 
1013 1024. 14             Cache Storage Anisotropic Media Cached points store: 3D position 
.inscattered radiance is a  Value  spherical function Gradient } .projected onto SH Valid Radius 
37 Valid Radius 38                         Sample Parameters: Highly Scattering 
Media MediumMedium Volume Volume Extinction Coefficient (.) (x 10.2 mm.1) Scattering Coefficient (.) 
(x 10.2 mm.1) Average Cosine (g) Low.Fat MilkMilk 16 ml RR 0 9126 0.9126 0 9124 0.9124 0 932 0.932 G 
1.0748 1.0744 0.902 BB 1 25001.2500 1 24921.2492 0 8590.859 Regular Milk 15 ml R 1.1874 1.1873 0.750 
G 1.3296 1.3293 0.714 B 1.4602 1.4589 0.681 Regular Chocolate Milk 16 ml R 0.7359 0.7352 0.862 G 0.9172 
0.9142 0.838 B 1.0688 1.0588 0.806 Sample Parameters: Highly Absorbing Media MediumMedium Volume Volume 
Extinction Coefficient (.) (x 10.2 mm.1) Scattering Coefficient (.) (x 10.2 mm.1) Average Cosine (g) 
Yuengling BeerBeer 2900 ml RR 0 1535 0.1535 0 0495 0.0495 0 969 0.969 G 0.3322 0.0521 0.969 BB 0 74520.7452 
0 05970.0597 0 9750.975 Merlot Wine 1500 ml R 0.7639 0.0053 0.974 G 1.6429 0.0000 . B 1.9196 0.0000 . 
Era Detergent 2300 ml R 0.7987 0.0553 0.949 G 0.5746 0.0586 0.950 B 0.2849 0.0906 0.971       
         gy Virtual vibrational state 2 Higher vibrational V V state state lb l Initial vibrational 
state Lower vibrational state    Pure water  ..Raman scattering (S, aS) Phytonplankton  .Fluorescence 
(S) Detritus Yellow matter (CDOM) ..Fluorescence (S, aS)   Full Radiative Transfer EquationRadiative 
Transfer Equation   Optical properties determinedby water constituentts              
          Phitoplankton C = 0 C = 0.01  C = 0.1 C = 1     Clear Air Light Stripe Range 
Finding in Scattering Media Light.planeLight.plane Light plane Light.plane Source Source Camera Camera 
Surface Surface  Light Striping Model in Scattering Media Surface Irradiance due to Medium: Light.plane 
Extinction.coefficient Phase.Function x Ds .. x(x. y.) . y. E . L e eP( 1 . ELe eP( 1....)) gg coscos.)) 
di 0 0 me di um y y 4. Source L00 Dv D Irradiance due to Surface:IrradianceduetoSurface: Camera .. 
(D .D ) sv E . L Re surface 00 surface Radiance Final Image Irradiance: E . E . (x . D ) . E . (x . D 
) surface s medium s  VIDEO    VIDEO          The.Study.of.the.Atmosphere Atmospheric Oti 
Radiative.Transport. Telescopic.and.Satellite Imaging OpticsImaging. Chandrasekhar.1957,.1960 Modeling.Weatherand.Scattering 
Middleton.1952 Hulst 1954 Hulst..1954 Koshmeider.1924  Astronomy Remote.. Signal.Strength..     
  Weather Densities Sky Mean Angular Error Fog 5 Overcast 0.58 Mist 5 Overcast 1.25 Rain 5 Overcast 
1.13 Dense Haze 5 Overcast 1.47 Mild Haze 5 Sunny 3.61                     THANK 
YOU!  THANK YOU!    dLo(xo,.o) = fr(xi,.i,.o) dF(xi,.i) dLo(xo,.o)  = S(xi,.i;xo,.o) dF(xi,.i) 
    dL(r, ..) =-stL(r, ..)+ss p(.., ... )L(r, ... )d... dz 4p 13 Ld(r, .. )= f (r)+ E(r) .. 4p 
4p  -str r F e f(r)= 4pD r  -str r F e  f(r)= 4pD r     Di.usion pro.le  a.-str dr a.-str 
dv zr (1 + str dr )ezv (1 + str dv )e R(r)= + 4pd3 4pd3 rv       8  8  Ld(r, ..)(-.n · ..) 
d.. O+  Ld(r, ..)(-.n · ..) d..= Fdr Ld (r, .. )(.n · .. ) d.. O+ O-  Fdr  Ld(r, .. )(-.n .. ) d.. 
= F dr Ld (r, .. )(.n .. ) d.. O+ O-   Ld(r, .. )(-.n .. ) d.. = Fdr Ld (r, .. )(.n .. ) d.. O+ O- 
 Ld (r, .. )(.n · .. ) d.. = Fdr Ld (r, .. )(-.n · .. ) d..  O- O+  Ld (r, .. )(.n .. ) d.. = Fdr 
Ld (r, .. )(-.n .. ) d..  O- O+  Ld (r, .. )(.n .. ) d.. = Fdr Ld (r, .. )(-.n .. ) d..  O- O+ 
 . . .     . . .         T1 T2    T1 * T2  T1 * T2  T1 * T2   T1 * T2T1*R2 
*R1*T2  T12 = T1 * T2 + T1 * R2 * R1 * T2 + T1 * R2 * R1 * R2 * R1 * T2 + ...  R = H{R}T = H{T } T1T2 
= H{T1 * T2}  T12 = T1 * T2 + T1 * R2 * R1 * T2 + T1 * R2 * R1 * R2 * R1 * T2 + ...  R = H{R}T = H{T 
} T1T2 = H{T1 * T2}  T12 = T1 T2 + T1 R2 R1 T2 + T1 R2 R1 R2 R1 T2 + ... = T1 T2 (1 + R2 R1 +(R2 R1 
)2 + ···)  T1 T2 T12 = R1 R2 < 1 1 -R2 R1         +       +    400 450 500 550 600 
650 700    Caucasian Asian African   Cm = 0.1% Cm = 1% Cm = 2% Cm = 5% Cm = 10% Cm = 20% Cm = 30% 
Cm = 50%  ßm =0.7,Ch =0.5%   ßm =0 ßm = 0.25 ßm = 0.5 ßm = 0.7 ßm =1  Cm = 50%,Ch =0.1%  Ch = 
0.1% Ch = 1% Ch = 5% Ch = 10% Ch = 20%  Cm = 1%,ßm =0.5%  Ch 10%5%1%0.1% Cm 0.5% 3% 10% 30% 40% 
 ß m     *    F * R+ 12 1 1 2 2 1 12 F * R++ F * T +A * R+A * T - + F * T +A * R+A * R-A 
* R+A * T - 1 1 =  + ···      400 450 500 550 600 650 700           * +    A Practical 
Analytic Single Scattering Model for Real Time Rendering. Bo Sun Ravi Ramamoorthi Columbia University 
Columbia University Abstract We consider real-time rendering of scenes in participating media, capturing 
the effects of light scattering in fog, mist and haze. While a number of sophisticated approaches based 
on Monte Carlo and .­nite element simulation have been developed, those methods do not work at interactive 
rates. The most common real-time methods are essentially simple variants of the OpenGL fog model. While 
easy to use and specify, that model excludes many important qualitative ef­fects like glows around light 
sources, the impact of volumetric scat­tering on the appearance of surfaces such as the diffusing of 
glossy highlights, and the appearance under complex lighting such as en­vironment maps. In this paper, 
we present an alternative physically based approach that captures these effects while maintaining real­time 
performance and the ease-of-use of the OpenGL fog model. Our method is based on an explicit analytic 
integration of the sin­gle scattering light transport equations for an isotropic point light source in 
a homogeneous participating medium. We can implement the model in modern programmable graphics hardware 
using a few small numerical lookup tables stored as texture maps. Our model can also be easily adapted 
to generate the appearances of materials with arbitrary BRDFs, environment map lighting, and precomputed 
radiance transfer methods, in the presence of participating media. Hence, our techniques can be widely 
used in real-time rendering. 1 Introduction Many real-time rendering applications like games or interactive 
simulations seek to incorporate atmospheric effects such as mist, fog and haze. These participating media 
lead to a number of quali­tative effects not present in clear-day conditions (compare .gure 1a with our 
result in .gure 1c). For instance, there are often glows around light sources because of scattering. 
The shading on objects is also softer, with specular highlights diffused out, dark regions brightened 
and shadows softer. It is critical to capture these effects to create realistic renderings of scenes 
in participating media. In computer graphics, the approaches for capturing these effects represent two 
ends in the spectrum of speed and quality. For high­quality rendering, a number of Monte Carlo and .nite 
element tech­niques have been proposed. These methods can model very general volumetric phenomena and 
scattering effects. However, they are slow, usually taking hours to render a single image. Signi.cant 
gains in ef.ciency can generally be obtained only by substantial precomputation, and specializing to 
very speci.c types of scenes. At the other extreme, perhaps the most common approach for in­teractive 
rendering is to use the OpenGL fog model, which simply blends the fog color with the object color, based 
on the distance of the viewer (.gure 1b). The fog model captures the attenuation of surface radiance 
with distance in participating media. This model is also popular because of its simplicity implementation 
requires almost no modi.cation to the scene description, and the user need only specify one parameter, 
ß , corresponding to the scattering co­ef.cient of the medium (density of fog). However, many qualitative 
. e-mail: {bosun,ravir,nayar}@cs.columbia.edu; srinivas@cs.cmu.edu Srinivasa G. Narasimhan Shree K. Nayar 
Carnegie Mellon University Columbia University   Figure 1: Rendered images of a scene with 66,454 texture-mapped 
trian­gles and 4 point lights. The insets show an image for another view of the vase, with highlights 
from all 4 sources, to amplify shading differences. (a) Standard OpenGL rendering (without fog), (b) 
OpenGL fog which captures attenuation with distance and blending with fog color, and (c) Our real-time 
model, that includes the glows around light sources, and changes to surface shading such as dimming of 
diffuse radiance (.oor and wall), brightening of dark regions (back side of pillars and vases) and dimming 
and diffusing of specular highlights (inset). All the visual effects in this complex scene are rendered 
by our method at about 20 frames per second.   Figure 2: Diagrams showing three cases of how light 
travels to the viewer through the participating medium. In (a) light travels in a straight line and directly 
reaches the surface and the viewer. This is essentially what previous interactive models such as OpenGL 
fog compute. In (b), in addition to what happens in (a), airlight scatters to the viewer and produces 
effects like glows around the light source. In (c), in addition to what happens in (b), airlight also 
scatters to the surface and gets re.ected, leading to effects such as the diffusing out of specular highlights 
and brightening of darker regions. In image (d), re.ected rays from the surface also scatter to the viewer. 
effects are missing, such as the glows around light sources, the ef­fect of scattering on object shading, 
and the ability to incorporate complex lighting effects like environment maps. In this paper, we take 
a signi.cant step towards improving the realism of rendered images with participating media (.gure 1c), 
while maintaining the real-time performance and ease of use of the OpenGL fog model. Our model can be 
implemented as a sim­ple vertex or pixel shader (pseudocode in .gure 13), allowing it to be easily added 
to almost any interactive application. The method can also be applied with complex lighting, allowing 
environment mapping and precomputed radiance transfer to be used interactively with participating media 
for the .rst time (.gures 15 and 16). Figure 2 illustrates three important visual effects due to light 
transport in scattering media. In this discussion, and this paper, we assume single scattering (i.e. 
that light scatters at most once in the medium), which is a common approximation in volumetric scattering 
and can be shown to be accurate in many common situa­tions such as thin fog. Figure 2a corresponds to 
direct transmission of light from the source or surfaces to the viewer. We can sim­ply attenuate the 
clear-day radiance values based on the distance (optical thickness). This simple approach is essentially 
what in­teractive models like OpenGL fog implement. Figure 2b also in­cludes the glows around light sources, 
commonly referred to as airlight [Koschmeider 1924]. Glows occur because light reaches the viewer from 
different directions due to scattering in the atmo­sphere. Figure 2c further includes the effect of airlight 
on the out­going surface radiance, leading to effects such as the spreading out of specular highlights 
and softening of shadows. These are impor­tant effects, usually neglected in previous interactive methods. 
Our model renders all of the effects in .gure 2c in real-time. Figure 2d illustrates the case where the 
surface radiance is single scattered in addition to being attenuated, before reaching the view point. 
On one hand, the attenuation decreases the brightness of the radiance at the surface according to the 
distance of the surface from the viewer. On the other hand, the single scattering results in slight brightening 
and blurring of this surface radiance. Implementing the latter effect requires a depth-dependent convolution. 
In this paper, we will only consider attenuation of surface radiance, and we will set aside a more thorough 
investigation of the latter effect for future work1. The speci.c technical contributions of this paper 
are: Explicit Compact Formula for Single Scattering: The common approach to using single scattering is 
to numerically in­tegrate brightness contributions while marching along the viewing ray. However, this 
approach is too slow for interactive applications, which require an explicit formula such as the OpenGL 
fog model. One of the main contributions of this paper is the derivation of an explicit compact formula 
for the single scattering from an isotropic point source in a homogeneous participating medium, by analyti­cally 
integrating the single scattering equations. This airlight model (section 3) allows us to simulate effects 
like the glows around light sources (.gure 2b). We can also use the model to calculate the ef­fects of 
scattering on the surface shading (.gure 2c). These calcula­tions are very expensive even for numerical 
integration, because we 1Single scattering from different surface points in the scene can partially compensate 
for the loss of brightness due to attenuation. Neglecting this can produce consistently darker images, 
especially for indoor scenes. must consider incident airlight from the entire visible hemisphere. However, 
they can be directly implemented using our explicit sur­face radiance model (section 4). Implementation 
on Programmable Graphics Hardware: We speculate that an explicit formula for the single scattering in­tegrals 
has previously not been derived because of the complexity of the calculations involved. In this paper, 
we reduce these dif­.cult integrals to a combination of analytic functions that depend only on the physical 
parameters of the problem, and a few lookups of tabulated 2D functions, that have no simple analytic 
form, but are smooth and purely numerical independent of the physical pa­rameters. The numerical functions 
can be precomputed and stored as 2D texture maps, and the entire analytic computation and table lookups 
can be implemented in simple pixel or vertex shaders in modern programmable graphics hardware (section 
5). Extensions to Complex Lighting and BRDFs: It is also possible to extend our airlight and surface 
radiance models to in­corporate more complex illumination models and material proper­ties (section 6). 
Mathematically, we derive a point-spread function (PSF) to represent the glow around a light source. 
We can con­volve an environment map with this PSF to get the appearance of a foggy scene under natural 
lighting. We can also use a frequency domain spherical harmonic representation to enable rendering with 
arbitrary BRDFs, and add in shadows and interre.ections with pre­computed radiance transfer methods. 
This approach enables meth­ods such as environment mapping and precomputed radiance trans­fer to be used 
with volumetric scattering effects for the .rst time. Our goal is to achieve interactive rendering of 
participating media. To enable this, and derive an explicit compact expres­sion that can be implemented 
in real-time, we make a number of assumptions isotropic point light sources, homogeneous me­dia, the 
single scattering approximation, and no cast or volumetric shadows (shadows can, however, be added using 
precomputed light transport methods). More complex and general scattering effects are certainly desirable 
in many situations, but are not possible to obtain at real-time rates for general scenes. On the other 
hand, our method captures most of the important visual effects of scattering, while being very simple 
to add to any interactive application. 2 Related Work The literature on simulating volumetric effects 
is large, going back to [Blinn 1982], and we only discuss important representative pa­pers. Most techniques 
are based on numerical or analytic approx­imations to the radiative transfer equation [Chandrasekhar 
1960]. Monte Carlo ray tracing methods were adapted by computer graph­ics researchers to render impressive 
effects including multiple scat­tering and non-homogeneous media [Kajiya and Herzen 1984; Max 1994; Jensen 
2001]. However, such methods can take hours to ren­der a single image. To speed up rendering, numerical 
methods that only simulate single scattering have also been proposed [Pattanaik and Mudur 1993; Nakamae 
et al. 1990; Sakas 1990; Rushmeier and Torrance 1987]. However, they still require signi.cant running 
times, and are not suitable for interactive applications. Hardware-accelerated numerical methods: A 
number of recent hardware-accelerated techniques can signi.cantly decrease the running times of numerical 
simulations, although they are still usually not fast enough for many interactive applications such as 
games. Dobashi et al. [2002] describe a multi-pass rendering tech­nique that numerically integrates the 
single scattering equations, using graphics hardware to accumulate the results at a number of planes 
in the scene, similar to volume rendering. Harris and Las­tra [2001] render clouds by including a forward 
scattering term in addition to single scattering. Note that their method is geared to­ward the case when 
the viewer is far from the clouds, and they apply a different and slower approach when the viewer and 
scene are immersed inside the medium, as is the scenario in our work. These methods are intended to apply 
to speci.c phenomena like the sky or clouds [Dobashi et al. 2002; Riley et al. 2004; Harris and Lastra 
2001]. This allows them to make use of complex tabular volume speci.cations, precomputed lighting solutions 
or multipass rendering techniques to produce effects including inhomogeneous media and simple heuristics 
for multiple scattering. They allow for viewpoint, and in a few cases interactive lighting variation, 
but usually .x the medium properties and scene speci.cation. In contrast, our technique, while focusing 
on homogeneous me­dia and single scattering, can be encapsulated in a simple shader for general scenes, 
and allows for real time variation of the viewpoint, lighting, scattering properties of the medium, and 
even scene geom­etry and re.ectance. Another major bene.t of our method is that it addresses the effects 
of scattering on surface shading (.gure 2c) and complex lighting like environment maps. These effects 
are not included in previous methods because they are dif.cult to numeri­cally simulate ef.ciently, requiring 
an integration over all incident scattered lighting directions at each surface point. Analytically based 
methods: The diffusion approximation for optically thick media was applied to subsurface scattering [Stam 
1995; Jensen et al. 2001]. An analytic form for the single scattering term was also derived by Hanrahan 
and Krueger [1993]. However, the problem we are solving is very different from that of subsur­face scattering, 
where the light sources and viewer are outside the medium. In our case, both the sources and viewer are 
immersed inside the medium. Also, unlike in the case of diffusion, we are interested in strongly directional 
effects like glows around sources. Analytic expressions for airlight with directional light sources, 
based on the derivation by Koschmeider [1924], are used frequently for rendering skies [Preetham et al. 
1999; Hoffman and Preetham 2003; Narasimhan and Nayar 2002]. However, our focus is differ­ent. We wish 
to derive an analytic model with near-.eld point sources, which is a signi.cantly more complex lighting 
situation as compared to distant lighting (collimated beams). Analytic expressions for the glows around 
point light sources inside homogeneous media have also been derived [Max. 1986; Biri et al. 2004; Narasimhan 
and Nayar 2003]. Therefore, those methods could be used to render glows in real time. However, it is 
not clear how to extend them to a complete real-time rendering system that also considers the effects 
of airlight on surface shading, or handles complex environment map lighting. Furthermore, their derivations 
involve approximations that are not feasible in several common rendering scenarios. For instance, the 
model derived by Max [1986] does not take into account attenuation. Biri et al. [2004] use a polynomial 
approximation to single scattering which results in inaccurate glows along viewing directions near the 
source. The multiple scattering model in [Narasimhan and Nayar 2003] is not strictly valid when objects 
are present in the medium, especially near the sources (as is generally true in most common scenes), 
or for optically thin media. Further, the integration required for surface radiance cannot be computed 
analytically or simulated numerically at interactive rates. 3 The Airlight Model In this section, we 
will derive an explicit model for the single scat­tered radiance at a viewer, due to an isotropic point 
light source, assuming that both the viewer and the source are immersed in a ho­mogeneous scattering 
medium. Consider the scenario illustrated in s, v, p Subscripts for Source, Viewer, surface Point . Angle 
between light source and viewing ray Dsv Distance between source and viewer Dvp Distance between viewer 
and closest surface point Dsp Distance between source and surface point Tsv Optical thickness between 
source, viewer (ß Dsv) Tvp Optical thickness between viewer, surface point (ß Dvp) Tsp Optical thickness 
between source, surface point (ß Dsp) ß Scattering coef.cient of the participating medium a Angle of 
scattering x Distance along the ray from viewer (integration variable) d Distance of single scattering 
from light source I0 Radiant intensity of point light source fr BRDF of surface Figure 3: Notation used 
in our derivations. Point.Source, s Viewer, v Surface á Point, p Figure 4: Diagram showing how light 
is scattered once and travels from a point light source to the viewer. .gure 4 (the notations used are 
indicated in .gure 3). The point light source has a radiant intensity I0 and is at a distance Dsv from 
the view point, making an angle . with the viewing direction. The radiance, L, is composed of the direct 
transmission, Ld , and the single scattered radiance or airlight, La, L = Ld + La. (1) The direct term 
Ld simply attenuates the incident radiance from a point source (I0/D2 ) by an exponential corresponding 
to the dis­ sv tance between source and viewer, and the scattering coef.cient2 ß , -ß Dsv Ld (., Dsv, 
ß )= DI02 e· d (.), (2) sv where the delta function indicates that for direct transmission, we receive 
radiance only from the direction of the source (no glows). 3.1 The Airlight Integral We focus most of 
our attention on the airlight La. The standard ex­pression [Nishita and Nakamae 1987] is given by an 
integral along the viewing direction, e-ß dI0 · La(., Dsv, Dvp, ß )= Dvp ß k(a) ·· e-ß xdx , (3) d2 0 
where Dvp is the distance to the closest surface point along the viewing ray or in.nity if there are 
no objects in that direction, and k(a) is the particle phase function. The exponential attenuation corresponds 
to the total path length traveled, d + x. The two param­eters d and angle a in the integrand depend on 
x. In particular, d is 2When there is light absorption in addition to scattering, ß is called the extinction 
coef.cient and is given by the sum of the scattering and absorp­tion coef.cients. In this paper, we simply 
refer to ß as the scattering coef.­cient, and it is straightforward to include absorption in our models. 
1.6 1.2 0.8 0.4 0 1.5 0 v 4 u 5 0 Figure 5: 3D plot of special function F(u, v) in the range of 0 . 
u . 10 and 0 . v . p 2 . The plot shows that the function is well-behaved and smooth and can therefore 
be precomputed as a 2D table. As expected from the de.nition in equation 10, the function decreases as 
u increases, and increases as v increases. The maximum value in the plot above therefore occurs at (u 
= 0, v = p 2 ). Also note from equation 10, that for u = 0, there is no attenuation so the function is 
linear in v. given by the cosine rule as d = D2 + x2 - 2xDsv cos.. (4) sv Let us now substitute equation 
4 into equation 3. For now, we also assume the phase function k(a) is isotropic and normalized to 1/4p 
(our approach can also be generalized to arbitrary phase functions see appendix D on CDROM). In this 
case, ß I0 . Dvp e-ß Dsv2 +x2-2xDsv cos. La(., Dsv, Dvp, ß )= · e-ß xdx . 4p 0 D2 + x2 - 2xDsv cos . 
sv (5) We refer to this equation as the airlight single scattering integral and next focus on simplifying 
it further to derive an explicit form. 3.2 Solution to the Airlight Integral We take a hybrid approach 
to solve equation 5. The key result is that this integral can be factorized into two expressions (a) 
an an­alytic expression that depends on the physical parameters of the scene and (b) a two-dimensional 
numerically tabulated function that is independent of the physical parameters. Essentially, this factor­ization 
enables us to evaluate the integral in equation 5 analytically. A high-level sketch of the derivation 
is given below and detailed simpli.cations are included in appendix A. STEP 1. Reducing the dimensions 
of the integral: Since the integral in equation 5 depends on 4 parameters, our .rst step is to apply 
a series of substitutions that reduce the dependency of the integrand to only one parameter. For this, 
we .rst write the expressions in terms of optical thicknesses T. = ß D. and t = ß x. In most cases, this 
eliminates the separate dependence on both ß and the distance parameters, somewhat reducing the complexity, 
and giving us a simpler intuition regarding the expression s behav­ior. Then, we combine the dependence 
on Tsv and . by making the substitution z = t - Tsv cos., to obtain ß 2I0 Tvp-Tsv cos. e-z- z2+Tsv 2 
sin2 . -Tsv cos. La(., Tsv, Tvp, ß )= edz. 4p -Tsv cos. T 2 sin2 . + z2 sv (6) Now, the integrand really 
depends on only one physical parameter Tsv sin., beginning to make the computation tractable. It is possible 
to further simplify equation 6, as described in ap­pendix A. To encapsulate the dependence on the physical 
parame­ters of the problem, we de.ne the following two auxiliary expres­sions, corresponding respectively 
to the normalization term outside the integrand, and the single physical parameter in the integrand, 
0.3 0.25 0.2 0.15 0.1 0.05 0 -0.05 -0.1 -0.15 512256128 64 32 16 8 Figure 6: Accuracy of the airlight 
model. The plots show the error (versus numerically integrating equation 5) as a function of the resolution 
for the 2D tables for F(u, v). We report the fractional error, normalizing by the to­tal airlight over 
the hemisphere. The error for each resolution is averaged over 40000 parameter values of ß , Dsv, Dvp 
and .. Bilinear (red) and near­est neighbor (green) interpolation is used to interpolate F(u, v) at non-grid 
locations of the indices (u, v). The plots clearly indicate the high accuracy of our compact formula, 
and that a 64 × 64 table for F(u, v) suf.ces for a maximum error of less than 2%. 0.08 0.06 0.04 0.02 
 Figure 7: Comparison of the airlight model with a standard Monte Carlo simulation that includes multiple 
scattering. The plots show the relative RMS error between the two methods for the case of isotropic phase 
func­tion. [Left] The low RMS errors show that our model is physically accurate (less than 4% error) 
for optically thin media (Tsv . 2). [Right] From this plot, it is evident that multiple scattering becomes 
more important as op­tical thickness increases. However, the actual errors grow slowly and are still 
low for a wide range of optical thicknesses (Tsv < 10). It is also inter­esting to note that for very 
high optical thicknesses (Tsv > 20), attenuation dominates over scattering and once again the RMS errors 
decrease. Tsv sin.: ß 2I0e-Tsv cos. A0(Tsv, ., ß )= (7) 2pTsv sin. A1(Tsv, .)= Tsv sin.. (8) It is then 
possible to derive, as shown in appendix A, that Tvp-Tsv cos. . p 4 + 1 2 arctan Tsv sin . La = A0(Tsv, 
., ß ) exp[-A1(Tsv, .) tan. ] d. . ./2 (9) Although equation 9 might seem complicated, it is really in 
a simpli.ed form. We already have simple analytic expressions for A0 and A1. Further, the function A1 
is a numerical constant as far as the integration is concerned. STEP 2. Evaluating the integral using 
a special function: To encapsulate the key concepts in the integrand of equation 9, we de.ne the special 
function, v F(u, v)= exp[-u tan. ] d. . (10) 0  Figure 8: The images show glows around three identical 
point light sources (street lamps) at different distances from the viewer. From left to right, we show 
three different values of the scattering coef.cient ß (ß = 0, 0.01, 0.04). Larger values of ß correspond 
to larger optical thicknesses Tsv. We clearly see the effect of greater glows for larger ß . Also, the 
radiance from farther light sources is attenuated more in each individual image, resulting in smaller 
glows for the farther lights. In the fourth (rightmost) image, we show a different view with ß = 0.04, 
where all the light sources are approximately equidistant, with the result that they have similar glows. 
(The shading on the surfaces is computed using the surface radiance model in section 4.) Unfortunately, 
there exists no simple analytic expression for F(u, v). However, the function is a well behaved 2D function 
as shown in .gure 5. Therefore, we can simply store it numerically as a 2D table. This is really no different 
from de.ning functions like sines and cosines in terms of lookup tables. In practice, we will use texture 
mapping in graphics hardware to access this 2D table. Note that F(u, v) is purely numerical (independent 
of the physical parameters of the problem), and thus needs to be precomputed only once. Finally, we can 
obtain for La(., Tsv, Tvp, ß ), p 1 Tvp - Tsv cos.. La = A0F(A1, + arctan ) - F(A1, ), (11) 42 Tsv 
sin . 2 where we have omitted the parameters for La, A0 and A1 for brevity. In the important special 
case of Tvp = 8, corresponding to no objects along the viewing ray, we get La(., Tsv, 8, ß ) as  La 
= A0(Tsv, ., ß )F(A1(Tsv, .), p 2 ) - F(A1(Tsv, .), 2 . ). (12) In summary, we have reduced the computation 
of a seemingly complex single scattering integral in equation 5 into a com­bination of an analytic function 
computation that depends on the physical parameters of the problem and a lookup of a pre­computed 2D 
smooth function that is independent of the phys­ical parameters of the problem. In the rest of the paper, 
we will demonstrate several extensions and applications of our model. 3.3 Accuracy of the Airlight Model 
We .rst investigate the accuracy of our analytic model as compared to numerically integrating equation 
5. Figure 6 shows plots of the mean error in La as a function of the resolution of the 2D numerical table 
for the special function F(u, v). We use interpolation to eval­uate F(u, v) at non-grid locations for 
the indices (u, v) (bilinear and nearest neighbor interpolations are shown in .gure 6). For each resolution, 
the error computed is averaged over 40000 sets of pa­rameter values for ß , Dsv, Dvp, .. The error bars 
in the .gure show the standard deviation of the errors. The plots indicate that even a low resolution 
64 × 64 table suf.ces to compute F(u, v) accurately, with a maximum error of less than 2%. As expected, 
bilinear in­terpolation performs better, but, for faster rendering, one can use nearest neighbor interpolation 
with only a small loss in accuracy. We also validate the accuracy of the single scattering assumption 
in our airlight model. Figure 7 shows the relative RMS errors be­tween glows around light sources computed 
using our model and a standard volumetric Monte Carlo approach that takes into account multiple scattering 
as well. The Monte Carlo simulation took ap­proximately two hours to compute each glow, whereas our explicit 
model runs in real-time. The comparison was conducted for optical thicknesses over a wide range Tsv E 
(0.25, 25) and Tvp E (0.5, 50), which covers almost all real situations. As expected, for optically thin 
media (Tsv < 2), our model is very accurate (less than 4% rela­tive RMS error). Interestingly, even for 
greater optical thicknesses (Tsv > 2), the error only increases slowly. Thus, our single scatter­ing 
model may be used as a viable approximation for most common real-time rendering scenarios, such as games. 
 3.4 Visual E.ects of the Airlight Model The dependence of the model on the viewing direction . and the 
distance of the source from the observer Dsv, predicts visual effects like the glows around light sources 
and the fading of distant ob­jects. As discussed above, these effects are physically accurate for thin 
fog (low ß and T ), and qualitatively reasonable in other cases. In .gure 8, we also see how these glows 
change as a function of the medium properties (the scattering coef.cient ß ) and distance to the sources. 
As ß increases, we go from no glow (ß = T = 0) to a signi.cant glow due to scattering. The differences 
in the 3 light sources should also be observed. The farther lights are attenu­ated more, and we perceive 
this effect in the form of reduced glows around more distant sources. The .nal (rightmost) image in .gure 
8 shows a different viewpoint, where the sources are at approximately the same distance, and the glows 
therefore look the same. 4 The Surface Radiance Model In this section, we discuss the effects of airlight 
on the outgoing surface radiance. Consider the illustration in .gure 9, where an isotropic point light 
source s illuminates a surface point p. We will calculate the re.ected radiance at the surface. To get 
the actual ap­pearance at the viewer, we need to attenuate by exp[-Tvp] as usual, where Tvp is the optical 
thickness between viewer and surface point. The re.ected radiance Lp is the sum of contributions, Lp,d 
and Lp,a, due to direct transmission from the source, and single scat­tered airlight from the source 
respectively, Lp = Lp,d + Lp,a. (13) The direct transmission corresponds to the standard surface re­.ectance 
equation, only with an attenuation of exp[-Tsp] added be­cause of the medium, where Tsp is the optical 
thickness between the source and the surface point: I0e-Tsp L= fr(.s, fs, .v, fv) cos.s, (14) p,d D2 
sp where fr is the BRDF, (.s, fs) is the direction to the source, and therefore also the incident direction, 
and (.v, fv) is the viewing di­rection. All angles are measured with respect to the surface normal, in 
the local coordinate frame of the surface. 4.1 The Surface Radiance Integral On the other hand, the 
single-scattered radiance Lp,a is more com­plicated, involving an integral of the airlight (La from equation 
12) over all incident directions, J Lp,a = La(.'(.s, .i), Tsp, 8, ß ) fr(.i, fi, .v, fv) cos .id.i . 
(15) O2p Point.Source, s D sv (è.,.ö.)ss Dsp ã' (è.,.ö.)ii Viewer, v (è.,.ö.)vv n ^ Surface D vp Point, 
p Figure 10: 3D plots of functions G0 andGn forn = 20 in the range of 0 . Tsp . 10 and 0 . .s . p 2 . 
The plots show that both functions are well­de.ned and smooth and can therefore be precomputed as 2D 
tables. The functions reach their peak values for .s = Tsp = 0, decaying with increases in both parameters. 
The decay is faster for the peakier G20 on the right.  Figure 9: Diagram showing how light travels from 
a point light source to a surface point and gets re.ected towards the viewer by the surface point. Consider 
the parameters of La in the integrand of the above equa­ ' tion. The angle .in this case is the angle3 
between the incident di­rection .i and the source direction (.s, fs). Note that for isotropic BRDFs, 
we can always rotate the coordinate system so fs = 0, al­lowing us to write .' (.s, .i). Finally, La 
also depends on the optical thickness between the source and the surface point Tsp (instead of between 
source and viewer in equation 12). We refer to equation 15 as the surface radiance single scatter­ing 
integral, analogous to the airlight single scattering integral in equation 5, and next focus on deriving 
an explicit compact form. 4.2 Solution to the Surface Radiance Integral for Lambertian and Phong BRDFs 
First consider the Lambertian case, so the BRDF is a constant kd . ' The integral will then depend only 
on the parameters of La, i.e. ., Tsp and ß . Of these, the dependency on ß is primarily a normal­ ' 
ization factor and does not affect the integrand. The angle . is a function of the source direction .s, 
and the integration variable .i. Hence, the integrand depends on only two physical variables, Tsp and 
.s. Thus, as in the previous section, we can de.ne a special two-dimensional numerical function G0(Tsp, 
.s). For the Phong BRDF, we employ the reparameterization method in [Ramamoorthi and Hanrahan 2002], 
measuring angles from the re.ection of the viewing direction about the surface normal, rather ' than 
the surface normal itself. To indicate this, we denote by . s the angle the source makes with respect 
to this re.ected direction. Upon making this transformation, it can be shown that the Phong BRDF is mathematically 
analogous to the Lambertian case. To al­low for the Phong exponent n, we de.ne the 2D function Gn instead 
of G0. These functions are well-de.ned and smooth as shown by the plots in .gure 10. The details of these 
calculations are in ap­pendix B, and the formula for Gn is J e-Tsp cos .. ' p. ' Gn(Tsp, . )= F(A1, 
) - F(A1, ) cosn .id.i, s ' sin.22 O2p (16) ' '' where .and A1(Tsp, .' ) are functions of . and .i, 
i.e. .' (.s, .i). s The .nal shading formula, considering both direct transmission and single scattering 
is then given by (derivation in appendix B): e-Tsp Lp = I0kd cos.s + ß 2 G0(Tsp, .s)+ (17) D2 sp 2pTsp 
e-Tsp ' + ß 2 Gn(Tsp, .s) ' I0ks cosn .s . (18) D2 sp 2pTsp As in the airlight model derviation, we 
have reduced the computa­tion of surface radiance due to single scattering to a few analytic function 
evaluations and a few 2D table lookups. 3We use the prime on .. to make a technical distinction from 
the angle . between the source and viewer used in the previous section. An explicit trigonometric formula 
for .. is given in appendix B. 8 G0 2.7 G20 6 1.8 4 0.9 2 0 0 0 0 1.5 2 1.5 2 1 s è 0.5 0 5 4 sp T 1 
s è 0.5 0 5 4 sp T (a) (b) (c) Figure 11: In.uence of scattering on Lambertian surface radiance. In 
the foggy image (b), created using our surface radiance model, we see a dim­ming due to attenuation and 
diffusing of shading (note the brightening of darker areas compared to the clear day image in (a)). These 
effects are per­haps more apparent in (c), where we also include airlight from the source. (a) (b) (c) 
Figure 12: In.uence of scattering on specular surface radiance (top row has Phong exponent 10, bottom 
has Phong exponent 20). In the foggy im­ages (b), we see a dimming and diffusing of the specular highlight 
compared to the clear-day image in (a). Note also the overall loss in color saturation and contrast, 
especially in (c). These are important visual effects, usually missing in previous interactive techniques. 
 4.3 Visual E.ects of the Surface Radiance Model To illustrate the different qualitative effects we 
see on surfaces due to scattering, we rendered spheres with Lambertian (.gure 11) and Phong BRDFs (.gure 
12) using our model above. The columns are from left to right (a) no scattering, (b) the effects of scattering 
on surface shading, and (c) combining this with the airlight effects directly from the source. For the 
Lambertian sphere in .gure 11, we see a dimming due to attenuation of light through the scattering medium, 
and the diffusing of shading leading to a brightening of darker shadow regions. In the specular case, 
we see a dimming and diffusing out of the specular highlight due to scattering, combined with an overall 
reduction in color saturation and contrast. These are important qualitative shading effects that add 
to the realism of scene appearance in scattering media. frag2app fmain( .oat4 objPos : TEXCOORD3, // 
2D texture coords ... uniform samplerRECT F, // 2D special functions uniform samplerRECT G0, uniform 
samplerRECT Gn) { frag2app OUT; // output radiance // Set up and calculate Tsv, ., Dsv, Tvp, .s and . 
. s /********** Compute La from equation 11 ******/ A0 =(ß . I0 . exp[-Tsv . cos .])/(2p . Dsv . sin 
.);// equation 7 A1 = Tsv . sin.; // equation 8 v = p/4 +(1/2) arctan[(Tvp - Tsv . cos .)/(Tsv . sin 
.)]; // v is one of texture coords f1 = texRECT (F, f loat2(A1, v)); // 2D texture lookup f2 = texRECT 
(F, f loat2(A1, ./2)); airlight = A0 . ( f1 - f2); // equation 11 /********** Diffuse surface radiance 
from equation 17 ******/ d1 = kd . exp[-Tsp] . cos.s . I0/(Dsp . Dsp); d2 =(kd . I0 . ß . ß )/(2p . Tsp) 
. texRECT (G0, f loat2(Tsp, .s)); dif fuse = d1 + d2; /********** Specular surface radiance from equation 
18 ******/ s1 = ks . exp[-Tsp] . cosn .s . . I0 /(Dsp . Dsp ); s2 =(ks . I0 . ß . ß )/(2p . Tsp ) . texRECT 
(Gn, f loat2(Tsp , . . )); s specular = s1 + s2; /********** Final Color (equation 19) ******/ OUT.color 
= airlight +(dif fuse + specular) . exp[-Tvp]; return OUT ; } Figure 13: Pseudocode for the Cg fragment 
shader that implements our combined model for airlight and surface radiance. 5 The Complete Model and 
its Hardware Implementation While the mathematical derivations in the previous two sections are somewhat 
involved, the actual implementation is straightfor­ward. Our model provides an explicit form that can 
be easily imple­mented in modern programmable graphics hardware. This requires minimal changes to both 
the original rendering code and scene de­scription, and can therefore be easily integrated into other 
existing real-time rendering methods. Indeed, the user need only specify the coef.cient ß of the medium, 
as in standard OpenGL fog, and use the shader corresponding to our model (pseudocode is in .gure 13). 
To compute the .nal appearance, we sum up the attenuated re­.ected radiance from the surface and the 
airlight from the source, L = e-Tvp Lp + La. (19) La is the airlight and is given by equation 11. Lp 
is the exitant ra­diance at the surface and is given by equations 17 and 18. We only need to compute 
a few simple analytic terms and do 4 texture lookups for each vertex or pixel, two for special function 
F, and one each for G0 and Gn (these texture lookups correspond to the texRECT function call in the pseudocode 
of .gure 13). Clearly, these computations can be done by modern programable graphics cards interactively 
in a single rendering pass. In practice, we implement the model using Cg in the fragment shader of an 
NVidia Geforce 6800 graphics card. The special func­tions F, G0 and Gn are precomputed and tabulated 
as 64 × 64 .oat­ing point textures. Since these textures are precomputed only once, we minimize frequent 
data transfer between the graphics card and main memory. The rendering speed depends on a variety of 
variables, and ren­dering time is linear in the number of light sources. As shown in the video, we are 
able to achieve real-time rates even for fairly complex scenes with several light sources. As an example, 
we rendered the scene shown in .gure 1, with 39,999 vertices and 66,454 triangles. We simulated the scattering 
effects from 4 light sources and achieved about 20 fps using the graphics hardware mentioned above. The 
model for the scene was obtained from http://hdri.cgtechniques.com. 6 Complex BRDFs and Lighting So far, 
we have considered arbitrarily located point lights, and sim­ple Lambertian and Phong BRDFs, showing 
how an explicit expres­sion can be derived and implemented. Rendering time is linear in the number of 
lights. In this section, we show how these ideas can be extended to ef.ciently handle complex BRDFs and 
environment map lighting using convolution, if we are willing to make particular simplifying assumptions. 
We .rst introduce the notion of a point­spread function (PSF) for the radiance or glow from a point source 
due to single scattering. This is similar in spirit to the PSFs derived by Narasimhan and Nayar [2003] 
and Premoze et al. [2004] in the context of multiple scattering for of.ine rendering. We will then discuss 
a number of applications including Rendering arbitrary BRDFs with point light sources, by con­volving 
the BRDF with this PSF, as shown in .gure 14. This approach can be used if we are willing to precompute 
a tab­ular BRDF representation, instead of using a simple explicit formula, as for Lambertian and Phong 
BRDFs.  Convolving an environment map with the PSF to ef.ciently handle very complex lighting (with 
possibly thousands of lights, corresponding to the pixels of an environment map). This convolution is 
possible if we assume that all light sources are equally far away, as in a distant environment map. This 
enables us to obtain the characteristic glows and blurriness around light sources on foggy days, as 
shown in .gure 15.  Integrating volumetric scattering into precomputed radiance transfer methods that 
include complex lighting, realistic ma­terials, cast shadows and interre.ections (.gure 16). The idea 
of convolving with the point-spread function can be applied to almost any technique that uses environment 
maps, enabling environment mapping and precomputed radiance transfer to be used in participating media 
for the .rst time.  Throughout the section, we apply the signal-processing results of Ramamoorthi and 
Hanrahan [2001] and Basri and Jacobs [2003] to ef.ciently compute the convolutions in the frequency domain 
using spherical harmonics. 6.1 Airlight Point Spread Function (PSF) In section 3, we determined the radiance 
La(., Dsv, Dvp, ß ) from a point source reaching a viewer, due to single scattering. If we .x the distance 
to the source Dsv, the integrating distance Dvp, and the scattering coef.cient ß of the medium, the radiance 
becomes a function only of the angle .. We normalize this function by I0/D2 sv to account for the intensity 
of the source, and de.ne the PSF as D2 L(. sv ,Dsv, Dvp, ß ) PSF(.)Dsv,Dvp,ß = . (20) I0 Since the PSF 
is mostly applied for surface shading, we will gener­ally set Dvp = 8, as in section 4.  6.2 Empirical 
PSF factorization for Speedup The PSF de.ned above still depends on the parameters of the medium such 
as the coef.cient ß . So, changing these parameters changes the PSF and requires us to redo any convolutions. 
How­ever, we have observed empirically that the PSF above can be fac­tored into a purely angular component 
that is independent of the medium parameters and an amplitude component that de­pends on the medium parameters. 
This factorization enables us  to change the medium parameters interactively without having to re-compute 
the PSF or redo any convolutions. Speci.cally, . F(sin ., p 2 ) - F(sin., 2 ) NPSF(.)= (21) e(cos .-1) 
2p sin. · PSF(.)= Tsve-Tsv · NPSF(.), (22) Dsv,8,ß where NPSF has only angular dependence, independent 
of other physical parameters. In appendix C, we derive and validate this approximation, showing plots 
that indicate there is little noticeable numerical error. 6.3 Rendering with arbitrary BRDFs We can 
use convolution with the PSF to render with arbitrary tab­ulated BRDFs, such as measured re.ectance. 
For each outgoing direction, we tabulate the BRDF as a function over the sphere of incident directions. 
A new effective BRDF can be obtained for that outgoing direction by convolving this function with the 
PSF. Mathematically, we .rst write the (isotropic) BRDF in terms of spherical harmonic coef.cients for 
each outgoing angle as .orig(.i, .o, f)= ..orig (23) lm (.o)Ylm(.i, f), l,m where .orig are the coef.cients, 
and Ylm is the spherical harmonic. lm To perform the convolution [Ramamoorthi and Hanrahan 2001], we 
multiply the coef.cients .orig of the original BRDF by the corre­ lm sponding coef.cients of the point-spread 
function4 PSFl , 4p .eff .orig lm (.o)= PSFl (.o). (24) lm 2l + 1 Then, we can use the effective BRDF 
to compute the re.ected ra­diance due to airlight, and the original BRDF for the re.ected ra­diance due 
to direct transmission. Thus, standard rendering algo­rithms can be executed with only slight modi.cation 
and at virtually no additional cost. Note however, that while our previous formu­lae for Lambertian and 
Phong models required no precomputations, the convolution approach requires precomputation of the spherical 
harmonic coef.cients for a collection of outgoing angles. Figure 14 shows images rendered with the Blue 
metallic BRDF measured by Matusik et al. [2003]. In the left image, we simply render a tabular description 
of the BRDF without scattering. In the right image, we use the formula above to compute a new effective 
tabulated BRDF, including the effects of airlight. The brightening of darker regions owing to scattering 
is clearly visible on the right. 6.4 Rendering with Environment Maps Our point spread function can be 
applied directly to environment maps, with the effects of scattering obtained by convolving the en­vironment 
map with the PSF. To use a single PSF for all sources, we must assume that the lighting is made of small 
equidistant light 4Since the PSF is radially symmetric, depending only on ., only spheri­cal harmonic 
coef.cients with m = 0 are nonzero. Figure 15: [Top] Grace cathedral environment map with no scattering. 
[Middle] The environment map is convolved with the airlight single scatter­ing PSF to create a foggy/misty 
appearance. Notice the glows around the light sources, the blurring of the sources and the brightening 
of dark areas. [Bottom] A scene illuminated by the environment map without scattering (left) and with 
scattering (right). Notice the spreading of the highlights and brightening of the spheres due to scattering. 
sources (.xed Dsv). This is a good approximation when the size of the objects is small compared to the 
distance to the environment5. We .rst consider simply looking at the environment, where we would like 
to see the glows around the bright light sources, to create the effects of foggy or misty appearance. 
To achieve this effect, we simply need to convolve the environment map with the PSF, Lconvolved 4p PSFlLoriginal 
= lm . (25) lm 2l + 1 Furthermore, similar to equation 1, we can simply use a combi­nation of the original 
attenuated environment map Lattenuated (for direct transmission, and corresponds to Ld in equation 1) 
and the convolved version Lconvolved above (for airlight, and corresponds to La in equation 1) to compute 
the surface shading, Lfinal Lattenuated + Lconvolved = (26) Lattenuated Loriginal -Tsv = e. (27) Figure 
15 shows results obtained by convolving the Grace Cathe­dral environment map [Debevec 1998] with the 
single scattering PSF. The blurring of light sources and the overall increase in bright­ness in dark 
regions can be immediately seen. Below that, we com­pare the appearances of spheres rendered illuminated 
by this envi­ronment map with and without scattering. Notice the spreading of highlights and the brightening 
of the objects. 5Note that while this assumption is similar to standard environment map­ping, our PSF 
requires us to specify a .nite (but possibly large) Dsv.  6.5 Precomputed Radiance Transfer The traditional 
environment map rendering techniques do not take shadows or interre.ections into account. Precomputed 
radiance transport methods [Sloan et al. 2002] compute the visibility in an off-line manner, followed 
by interactive rendering. To add partici­pating media, we only need to convolve the lighting (environment 
map) with our PSF and use the result as input to the existing pre­computed radiance transfer techniques. 
To demonstrate this, we used the technique of [Wang et al. 2004; Liu et al. 2004], which handles non-diffuse 
objects under all-frequency environment illu­mination using a separable BRDF approximation. We show the 
result of the Happy Buddha model rendered with the Ashikhmin-Shirley BRDF [2000] in .gure 16. The left 
image is the standard result with no scattering. In the middle image, we show a view of the Buddha, where 
we include the effect of airlight from the environment on surface appearance, but there is no attenuation 
or scattering between the viewer and object itself (as if the observer were very close to the Buddha). 
We clearly see the foggy appear­ance of the background and the glows or airlight due to the light sources. 
On the face of the Buddha, we see a brightening of darker regions, along with a dimming and diffusing 
of specular highlights. A similar effect is seen on the base, where the dimming and diffus­ing of highlights 
reduces saturation and contrast. Finally, the shad­ows on the plane are blurred out, with a considerable 
softening and loss of detail. In the right image, there is also scattering or airlight between the object 
and the viewer (as if the observer were far away and seeing the Buddha through fog). This leads to a 
further loss of detail and contrast, so that the original glossy appearance of the object is essentially 
lost. 7 Conclusions and Future Work We have presented a simple method to add the effects of participat­ing 
media to interactive applications. Our approach can be easily implemented in programmable graphics hardware 
and leads to a number of new effects in the real-time domain, such as interactive rendering with glows 
around light sources, the effects of scatter­ing on surface shading, environment maps, and precomputed 
light transport. The key insight is a new analytic model for integrating the light transport equations 
assuming single scattering, which can also be extended to predict the impact of scattering or airlight 
on the inherent appearance of surfaces. More broadly, this paper indicates the power of using explicit 
formulae to simulate dif.cult effects like volumetric scattering, speeding up such a process by many 
orders of magnitude. We do sacri.ce some generality, considering only isotropic point light sources, 
single scattering, homogeneous media, and excluding most cast and volumetric shadowing, but believe this 
is a worthwhile tradeoff to enable a simple technique that achieves real-time rates. Future work can 
follow many avenues. For instance, we can attempt to extend our theoretical model to consider non-isotropic 
light sources (like spotlights) and inhomogeneous media. Our pre­liminary work in this area indicates 
that some of these generaliza­tions, while relatively simple in standard surface calculations, are rather 
non-trivial for volumetric media. However, we believe that many common cases can be addressed by extensions 
of the basic methods proposed in this paper. In general, we believe that analytic models of dif.cult 
to simulate volumetric phenomena are critical to achieving ef.cient renderings for real-time applications. 
Acknowledgements: We thank Rui Wang, John Tran and David Luebke for the precomputed radiance transfer 
code, and Simon Premoze for the Monte Carlo simulation code. This research was supported by a Columbia 
University Presidential Fellowhip, an ONR Grant #N00014-05-1­0188 (A Physical Approach to Underwater 
Optical Imaging), an NSF Grant #0305322 (Real-Time Rendering and Visualization of Complex Scenes), an 
NSF CAREER award #0446916 (Mathematical and Computational Funda­mentals of Visual Appearance), and equipment 
donations from Intel and NVIDIA. References ASHIKHMIN, M., AND SHIRLEY, P. 2000. An anisotropic phong 
model. Journal of Graphics Tools 5, 2, 25 32. BASRI, R., AND JACOBS, D. W. 2003. Lambertian re.ectance 
and linear subspaces. IEEE Trans. Pattern Anal. Mach. Intell. 25, 2, 218 233. BIRI, V., MICHELIN, S., 
AND ARQUES, D. 2004. Real­ time single scattering with shadows. In In review http://igm.univ­ mlv.fr/ 
biri/indexCA en.html. BLINN, J. 1982. Light re.ection functions for simulation of clouds and dusty surfaces. 
In SIGGRAPH 82, 21 29. CHANDRASEKHAR, S. 1960. Radiative Transfer. Oxford Univ. Press. DEBEVEC, P. 1998. 
Rendering synthetic objects into real scenes: bridging traditional and image-based graphics with global 
illumination and high dynamic range photography. In SIGGRAPH 98, 189 198. DOBASHI, Y., YAMAMOTO, T., 
AND NISHITA, T. 2002. Interactive ren­dering of atmospheric scattering effects using graphics hardware. 
In Graphics Hardware Workshop 02, 99 109. HANRAHAN, P., AND KRUEGER, W. 1993. Re.ection from layered 
sur­faces due to subsurface scattering. In SIGGRAPH 93, 165 174. HARRIS, M., AND LASTRA, A. 2001. Real-time 
cloud rendering. In Eurographics 2001, 76 84. HOFFMAN, N., AND PREETHAM, A. J. 2003. Real-time light-atmosphere 
interactions for outdoor scenes. Graphics programming methods, 337 352. JENSEN, H., MARSCHNER, S., LEVOY, 
M., AND HANRAHAN, P. 2001. A practical model for subsurface light transport. In SIGGRAPH 01, 511 518. 
JENSEN, H. W. 2001. Realistic Image Synthesis Using Photon Mapping. AK Peters. KAJIYA, J., AND HERZEN, 
B. 1984. Ray tracing volume densities. In SIGGRAPH 84, 165 174. KOSCHMEIDER, H. 1924. Theorie der horizontalen 
sichtweite. Beitr. Phys. freien Atm., 12. LIU, X., SLOAN, P.-P. J., SHUM, H.-Y., AND SNYDER, J. 2004. 
All­frequency precomputed radiance transfer for glossy objects. In Euro-Graphics Symposium on Rendering 
04, 337 344. MATUSIK, W., PFISTER, H., BRAND, M., AND MCMILLAN, L. 2003. A data-driven re.ectance model. 
ACM Transactions on Graphics (SIG-GRAPH 03) 22, 3, 759 769. MAX., N. L. 1986. Atmospheric illumination 
and shadows. In SIGGRAPH 86, 117 124. MAX, N. 1994. Ef.cient light propagation for multiple anisotropic 
volume scattering. In Eurographics Rendering Workshop 94, 87 104. NAKAMAE, E., KANEDA, K., OKAMOTO, T., 
AND NISHITA, T. 1990. A lighting model aiming at drive simulators. In SIGGRAPH 90, 395 404. NARASIMHAN, 
S., AND NAYAR, S. 2002. Vision and the atmosphere. IJCV 48, 3 (August), 233 254. NARASIMHAN, S., AND 
NAYAR, S. 2003. Shedding light on the weather. In CVPR 03, 665 672. NISHITA, T., AND NAKAMAE, E. 1987. 
A shading model for atmospheric scattering considering luminous intensity distribution of light sources. 
In SIGGRAPH, 303 310. PATTANAIK, S., AND MUDUR, S. 1993. Computation of global illumi­nation in a participating 
medium by monte carlo simulation. Journal of Visualization and Computer Animation 4, 3, 133 152. PREETHAM, 
A. J., SHIRLEY, P., AND SMITS, B. 1999. A practical analytic model for daylight. In SIGGRAPH, 91 100. 
PREMOZE, S., ASHIKHMIN, M., TESENDORF, J., RAMAMOORTHI, R., AND NAYAR, S. 2004. Practical rendering of 
multiple scattering effects in participating media. In EuroGraphics Symposium on Rendering 04, 363 374. 
RAMAMOORTHI, R., AND HANRAHAN, P. 2001. A signal-processing framework for inverse rendering. In SIGGRAPH 
01, 117 128. RAMAMOORTHI, R., AND HANRAHAN, P. 2002. Frequency space envi­ronment map rendering. ACM 
Transactions on Graphics (SIGGRAPH 02) 21, 3, 517 526. RILEY, K., EBERT, D., KRAUS, M., TESSENDORF, J., 
AND HANSEN, C. 2004. Ef.cient rendering of atmospheric phenomena. In EuroGraphics Symposium on Rendering 
2004, 375 386. RUSHMEIER, H., AND TORRANCE, K. 1987. The zonal method for cal­culating light intensities 
in the presence of a participating medium. In SIGGRAPH 87, 293 302. SAKAS, G. 1990. Fast rendering of 
arbitrary distributed volume densities. In Eurographics 90, 519 530. SLOAN, P.-P., KAUTZ, J., AND SNYDER, 
J. 2002. Precomputed radi­ance transfer for real-time rendering in dynamic, low-frequency lighting environments. 
ACM Transactions on Graphics (SIGGRAPH 02) 21, 3, 527 536. STAM, J. 1995. Multiple scattering as a diffusion 
process. In Eurographics Rendering Workshop 95, 41 50. WANG, R., TRAN, J., AND LUEBKE, D. 2004. All-frequency 
relighting of non-diffuse objects using separable BRDF approximation. In Euro-Graphics Symposium on Rendering 
2004, 345 354. Appendix A:Solution to Airlight Integral We start the derivation from equation 5.  ß 
I0 J Dvp e-ßDsv2 +x2-2xDsv cos. La = · e-ß xdx (28) 4p 0 Dsv 2 + x2 - 2xDsv cos . 10 0.1 0.08 8 0.06 
6 0.04 4 0.02 2 0 0 0 0.2 0.4 0.6 0.8 1 0 0.7 1.4 2.1 Figure 17: [Left] Plot of PSF(.)Dsv,8,ß normalized 
by Tsve-Tsv for different optical thicknesses Tsv ranging from 0.1 to 3.1. After normalization, the PSF 
depends on. and is largely independent of Tsv. This implies that we can factor it into a purely angular 
component and an amplitude component depending on Tsv. [Right] The average and standard deviation of 
the absolute error of the empirical PSF. While the error increases for smaller angles ., it remains well 
below 0.05. >substitute T, = ß D, and t = ß x e-Tsv2 +t2-2tTsv cos. = · e-tdt (29) ß 2 I0 JTvp 4p 0 
Tsv 2 + t2 - 2tTsv cos . >substitute z = t - Tsv cos . ß 2 I0e-Tsv cos . J Tvp-Tsv cos . e-z2 +T 2 sin2 
. sv = · e-zdz (30)4p -Tsv cos. z2 + T 2 sin2 . sv >substitute z = Tsv sin. tan . Tvp-Tsv cos . ß 2 I0e-Tsv 
cos . Jarctan Tsv sin . -Tsv sin. 1+sin . = ecos . d. (31) 4pTsv sin..- p 2 p >substitute . = 2. - 2 
Tvp-Tsv cos . J p 1 ß 2 I0e-Tsv cos . 2 arctan 4 + Tsv sin . = exp[-Tsv sin. tan . ]d. , (32) 2pTsv sin../2 
from which we obtain equation 9. Appendix B: Formula for Lambertian and Phong BRDFs Here, we derive the 
expression for Lambertian and Phong BRDFs. We .rst consider the Lambertian BRDF, beginning with equation 
15, Note that in the derivation below, .' is given from trigonometry by .'(.s, .i)= cos.i cos.s + sin.i 
sin.s cos fi. J Lp,a = La(.y(.s, .i), Tsp, 8, ß ) fr (.i , fi, .v, fv) cos .id.i (33) O2p >substitute 
equation 12 for La and a constant kd for fr J [y] p. = A0(Tsp, ., ß )F(A1(Tsp, .y ), ) - F(A1(Tsp, .y), 
)kd cos.id.i(34) 22 O2p >substitute equation 7 for A0 and take constants out of integration J -Tsp cos.y 
[] ß 2I0kd ep.y = F(A1(Tsp, .y), ) - F(A1(Tsp , .y), )cos .id.i 2pTsp sin.y 22 O2p ß 2I0kd = G0(Tsp , 
.s ). (35)2pTsp For the Phong BRDF after reparameterization, instead of kd cos.i, we will obtain ks cosn 
.i, where n is the Phong exponent. This can be handled ex­actly as above, simply replacing G0 with Gn. 
Appendix C:Empirical PSF factorization The empirical PSF factorization is inspired by the observation 
that after being normalized by Tsve-T sv, the PSF becomes essentially independent of the medium physical 
parameters (optical thickness) and largely depends on angle . as shown in .gure 17 (left). This implies 
we can factor the PSF into a purely angular component and an amplitude component that depends on the 
medium pa­rameters. We de.ne the angular component NPSF(.) as the PSF(.)Tsv =1 normalized by Tsve-T sv 
and de.ne the amplitude component as the normal­ization factor Tsve-Tsv . Then, the PSF can be expressed 
using these two terms as in equation 22. The absolute approximation error is plotted in .g­ure 17 (right) 
for 11 different optical thickness ranging from 0.1 to 3.1. Acquiring Scattering Properties of Participating 
Media by Dilution (a) Acquired photographs (b) Rendering at low concentrations (c) Rendering at natural 
concentrations Figure 1: (a) Photographs of our simple setup consisting of a glass tank and a bulb, .lled 
with diluted participating media (from top, MERLOT, CHARDON-NAY, YUENGLING beer and milk). The colors 
of the bulb and the glow around it illustrate the scattering and absorption properties in these media. 
At low concentrations, single scattering of light is dominant while multiple scattering of light is negligible. 
From a single HDR photograph, we robustly estimate all the scattering properties of the medium. Once 
these properties are estimated, a standard volumetric Monte Carlo technique can be used to create renderings 
at any concentration and with multiple scattering, as shown in (b) and (c). While the colors are only 
slightly visible in the diluted setting in (b), notice the bright colors of the liquids -deep red and 
golden-yellow wines, soft white milk, and orange-red beer -in their natural concentrations. Notice, also 
the differences in the caustics and the strong interre.ections of milk onto other liquids. Abstract 
The visual world around us displays a rich set of volumetric ef­fects due to participating media. The 
appearance of these media is governed by several physical properties such as particle densi­ties, shapes 
and sizes, which must be input (directly or indirectly) to a rendering algorithm to generate realistic 
images. While there has been signi.cant progress in developing rendering techniques (for instance, volumetric 
Monte Carlo methods and analytic ap­proximations), there are very few methods that measure or estimate 
these properties for media that are of relevance to computer graph­ics. In this paper, we present a simple 
device and technique for robustly estimating the properties of a broad class of participating media that 
can be either (a) diluted in water such as juices, bever­ages, paints and cleaning supplies, or (b) dissolved 
in water such as powders and sugar/salt crystals, or (c) suspended in water such as *e-mail:srinivas@cs.cmu.edu 
impurities. The key idea is to dilute the concentrations of the me­dia so that single scattering effects 
dominate and multiple scatter­ing becomes negligible, leading to a simple and robust estimation algorithm. 
Furthermore, unlike previous approaches that require complicated or separate measurement setups for different 
types or properties of media, our method and setup can be used to measure media with a complete range 
of absorption and scattering proper­ties from a single HDR photograph. Once the parameters of the diluted 
medium are estimated, a volumetric Monte Carlo technique may be used to create renderings of any medium 
concentration and with multiple scattering. We have measured the scattering param­eters of forty commonly 
found materials, that can be immediately used by the computer graphics community. We can also create 
re­alistic images of combinations or mixtures of the original measured materials, thus giving the user 
a wide .exibility in making realistic images of participating media. 1 Introduction Very often in our 
daily lives, we see participating media such as .uids (juices, beverages, milks) and underwater impurities 
(natu­ral ocean, river and lake waters). The propagation of light through these media results in a broad 
range of effects, including softer ap­pearance of milk, coloring of wines and juices, the transformation 
of appearances when liquids are mixed (coffee with milk, and cock­tails), the brilliant caustics from 
glasses containing these liquids, and low visibility in underwater situations. These effects inher­ently 
depend on several physical properties of the media such as scattering nature, sizes, shapes, and densities 
of particles [Hulst 1957; Chandrasekhar 1960]. Rendering these effects accurately is critical to achieving 
photo-realism in computer graphics. In the past few years, there has been a considerable effort to­wards 
developing ef.cient and accurate rendering algorithms for participating media, based on Monte Carlo simulation 
and analytic approximations. All these algorithms and models contain parame­ters (scattering coef.cient, 
absorption coef.cient, phase function) that directly or indirectly represent the physical properties 
of the medium. In order to faithfully render the effects of any participat­ing medium, the right parameters 
must be input. Given the progress in developing rendering algorithms, the quality of images is now often 
limited by the quality of these input parameters. Since there has so far been relatively little work 
in measuring or estimating scattering properties of media relevant to computer graphics, the parameters 
are currently often set in an ad-hoc manner. This situation is similar in some ways to that of standard 
surface rendering. In that case, global illumination algorithms have pro­gressed to the point of creating 
almost photo-realistic images, leav­ing the realism limited by the quality of the re.ectance models, 
and leading to much recent effort on measuring BRDFs. [Marschner 1998; Dana et al. 1997; Matusik et al. 
2003]. However, exist­ing methods for directly measuring physical properties for media usually require 
very expensive equipment, such as the particle siz­ing apparatus used in colloidal chemistry [Finsy and 
Joosten 1991; Jaeger et al. 1991], resulting in little usable data for graphics. Earlier efforts to estimate 
scattering properties from images of media have often yielded ill-conditioned and non-unique results, 
because of the dif.culties of solving the inverse light transport problem. The reasoning for the ill-conditioning 
of the inverse prob­lem is mainly due to multiple scattering, which blurs the incident light .eld and 
results in signi.cant loss of information [McCormick 1981; McCormick 1985; Antyufeev 2000]. This is analogous 
to the ill-conditioning of BRDF estimation under complex illumination [Ramamoorthi and Hanrahan 2001]. 
In this paper, we take a com­pletely different approach. The key idea is to estimate properties of media 
by acquiring the data in a state where multiple scatter­ing effects are negligible. Instead, the data 
is acquired when single scattering (which does not degrade the incident light signi.cantly) is the dominant 
effect. This is achieved by diluting the material to low concentrations. We present a simple and inexpensive 
experimental setup, along with a robust and accurate technique for measuring the scattering properties 
of a broad class of participating media that can be either (a) diluted in water such as juices, beverages, 
paints and clean­ing supplies, or (b) suspended in natural waters such as impurities and organisms, or 
even (c) dissolved in water such as powders and sugar or salt crystals. These media collectively have 
a wide range of scattering and absorption properties. We .rst derive a simple image formation model for 
single scattering of light in our setup. Through extensive simulations of both our model and ground truth 
(with multiple scattering), we then determine the space of concen­trations and scattering properties 
of media for which single scat­tering is dominant. Within this regime of valid concentrations, we conduct 
simulations to demonstrate that our estimation technique uniquely solves the inverse single scattering 
light transport prob­lem. Finally, we present a simple experimental procedure to deter­mine the best 
concentration (dilution) for any material despite no prior knowledge of its scattering properties. We 
have used our approach to create a dataset of scattering pa­rameters for forty commonly found materials, 
which can be di­rectly used for computer graphics rendering. Once the scattering parameters have been 
estimated, they can be used to render realis­tic images of arbitrary concentrations of the material with 
multiple scattering, using a standard physically based volumetric rendering algorithm. Figure 1 shows 
two renderings of a scene with four Medium Property Notation Concentration or Volume Fraction C Scattering 
Coef.cient (mm-1) ß Absorption Coef.cient (mm-1) . Extinction Coef.cient (mm-1) s =ß +. Single Scattering 
Albedo . =ß/s Scattering Angle . Henyey-Greenstein (H-G) Parameter g H-G Phase Function P(g,.)= 1 4p 
1-g2 (1+g2-2g cos. )3/2 Figure 2: The different scattering properties of a participating medium and 
their notations used in this paper. Light transport equations are usu­ally written in terms of three 
parameters s, ß and g. We estimate these parameters for participating media based on single scattering. 
liquids in their natural high density states and their diluted states. The scattering parameters of each 
material were computed using a single HDR photograph of our setup. Notice the bright saturated colors 
obtained despite the murky appearance of the diluted states. We can also create realistic images of mixtures 
of the original mea­sured materials, thus giving the user a wide .exibility in creating realistic images 
of participating media. 2 Related Work Figure 2 shows the most common properties of participating me­dia 
including the scattering and absorption coef.cients, and the phase function (angular scattering distribution 
represented by the Henyey-Greenstein (H-G) model [Henyey and Greenstein 1941]). The scattering and absorption 
coef.cients are proportional to the concentration or volume fraction of the particulate medium. We will 
brie.y review some of the representative works on the direct measurement and indirect estimation of these 
parameters. Estimation based on analytic approximations to light transport. Surprisingly, little work 
has been done in computer graphics on the measurement of scattering properties of media. A recent work 
is that of [Jensen et al. 2001], on the diffusion model for subsurface scattering. They present a measurement 
of a num­ber of translucent materials. However, the diffusion approxima­tion assumes multiple scattering 
for optically dense media, so that only a limited amount of information on the scattering parameters 
can be estimated. For instance, this approximation is independent of the phase function of the medium, 
and therefore this impor­tant property cannot be estimated. Furthermore, the diffusion is a poor approximation 
when scattering is comparable to absorption [Prahl 1988]. The analytic multiple scattering model presented 
in [Narasimhan and Nayar 2003] has also been used to estimate prop­erties of only purely scattering media 
(visibility and type of weather such as fog and mist). Our focus is somewhat different in consider­ing 
.uids like juices or beverages, instead of subsurface scattering in translucent solids like marble and 
skin, or weather conditions such as fog. Nevertheless, our approach is valid for media with the entire 
range of absorbing and scattering properties, signi.cantly ex­tending the class of measurable media for 
graphics. Most recently, Hawkins et. al., [2005] measure the extinction coef.cient of optically thin 
smoke from the exponential attenua­tion of a laser beam in a tank. They also use a separate mirror setup 
to directly measure the phase function (see below). In con­trast, our setup uses divergent beams from 
a simple bulb to include more light in the volume (than a single laser beam) for robust mea­surements, 
and requires only a single photograph to measure all scattering properties shown in Figure 2. Numerical 
solution to inverse light transport: In cases where there are no analytic solutions to light transport, 
several works have taken a numerical approach to estimate scattering prop­erties [McCormick 1996; Antyufeev 
2000]. However, it is widely known, that inverse problems in radiative transfer that take into ac­count 
multiple scattering are ill-conditioned and require regulariz­ing assumptions to obtain reliable estimates. 
See the reports and critiques by McCormick et al [1981; 1985]. Furthermore, the com­putational complexity 
of such inverse estimation techniques make it hard for measuring large sets of media for computer graphics 
or vision applications. Our focus here is on estimating scattering properties of media that can be measured 
in a state where multiple scattering is negligible. The observation that single scattering is dominant 
for optically thin media has been made by [Hawkins et al. 2005; Sun et al. 2005]. We exploit this observation 
and apply the single scattering model for the .rst time to a large class of materials which exhibit signi.­cant 
multiple scattering in their natural states of existence. We also determine the exact range of optical 
thicknesses for which single scattering is dominant for media with arbitrary scattering proper­ties, 
and propose an experimental procedure to ensure the domi­nance of single scattering in real data. Goniophotometry 
is often used to directly measure the phase function. Here, several detectors measure radiance in different 
directions after being scattered by a very small volume of the medium. [Fuchs and Jaffe 2002] use thin 
laser light sheet mi­croscopy for detecting and localizing microorganisms in ocean wa­ters. [Boss and 
Pegau 2001; Oishi 1990] investigate the relation­ship of light scattering at a single angle and the extinction 
coef­.cient using specialized receivers and transmitters. However, all these techniques assume that there 
is no attenuation of light through the sample and require expensive devices with precise alignment of 
detectors and transmitters. In contrast, our setup is extremely sim­ple (consisting of a glass tank and 
an off the shelf bulb), and our technique robustly estimates all properties from only a single pho­tograph, 
thus making it inexpensive and easy to measure a large number of participating media. 3 Single Scattering 
in Dilute Media Our approach is to measure media in a state where single scattering is dominant and multiple 
scattering is negligible. This is achieved by diluting the otherwise optically thick media, such as .uids, 
in water. The process of dilution does not usually corrupt the inher­ent scattering properties of media1 
since the scattering and absorp­tion of pure water itself is negligible for very small distances (less 
than 50 cm) [Sullivan 1963]. We begin by presenting our acquisi­tion setup and an image formation model 
for single scattered light transport within the measurement volume. We will then present extensive simulations 
of this model and compare with traditional Monte-Carlo approaches that include multiple scattering, to 
derive a valid space of scattering parameters over which single scattering is dominant. Based on this 
simulation, we design a simple experi­mental procedure to choose the best concentration for any particu­lar 
medium. Later, we will describe our algorithm to estimate the scattering parameters using our image formation 
model. 3.1 Acquisition Setup The measurement apparatus, shown in Figure 3, consists of a 25 × 30 × 30 
cm3 tank that is .lled with the diluted scattering medium. The depth of the tank is large enough to ensure 
the scat­tering angles are adequately covered (0 to 175 degrees). The vol­ume of the tank is designed 
to be large enough to dilute concen­trated media such as milk. Two sides of the tank are constructed 
using anti-re.ection glass and the other sides using diffuse black coated acrylic. A small frosted (diffuse) 
glass bulb .xed to a side 1When crystals are dissolved in water, they may exhibit different scat­tering 
properties due to ionization. Figure 3: Two views of the apparatus used to measure scattering proper­ties 
of water-soluble media. A glass tank with rectangular cross-section is .tted with a small light bulb. 
The glass is anti-re.ection coated. Different volumes of participating media are diluted with water in 
the tank, to simu­late different concentrations. A camera views the front face of the tank at normal 
incidence to avoid refractions at the medium-glass-air boundaries. Figure 4: A volume .lled with a homogeneous 
participating medium and illuminated by an isotropic point light source. A camera views the front face 
of the volume at normal incidence. The path of one single-scattered ray as it travels from the source 
to the camera is shown. This ray is .rst attenuated in intensity over a distance d, is then scattered 
at an angle p - ., and .nally, is attenuated again over a distance z, before reaching the camera. The 
irradiances due to all the rays that scatter into a viewing direction must be integrated to obtain the 
.nal camera irradiance. of the tank illuminates the medium. A Canon EOS-20D 12-bit 3504x2336 pixel digital 
camera with a zoom lens is placed .ve meters away from the tank and observes a face of the tank at nor­mal 
incidence. The .eld of view occupied by the tank in the im­age is three degrees and is therefore approximately 
orthographic. Orthographic projection avoids the need for modeling refractions of light rays at the medium-glass-air 
interfaces. In all our experi­ments, about 25 different exposures (1/500s to 10s) were used to acquire 
HDR images. 3.2 Image Formation Model Although the basic principles of single scattering are well known, 
the exact nature of the image formation model depends on the ge­ometry of the volume and the locations 
of the source and the cam­era. Figure 4 illustrates the illumination and measurement geome­try based 
on our acquisition setup. For simplicity, we will assume that the medium is illuminated by an isotropic 
point light source (later we extend the analysis to area sources) of intensity I0 that is located at 
the coordinates (0, B,H). Consider the path of one single-scattered light ray (thick ray in Figure 4) 
in the medium as it travels from the source to the camera. This ray is .rst exponentially attenuated 
in intensity for a distance d. At location U (x, y, z), depending on the phase function P,a fraction 
of the light intensity is scattered at an angle p -. . Finally, the ray is attenuated again for a distance 
z, before it reaches the camera. Mathematically, the irradiance at the camera produced by this ray is 
written as [Sun et al. 2005], I0 -sd -sz E(x,y,z)= . e. ß P(g,p - . ) . e. d2 d = x2 +(y - H)2 +(z 
- B)2 , cos .=(z - B)/d .(1) Here, P(g,p - . ) is the Henyey-Greenstein (H-G) phase function, and ß 
and s are the scattering and extinction coef.cients (Figure 2). Then, the total irradiance E at pixel 
(x,y) in the camera is ob­tained by integrating intensities due to all rays that are scattered at various 
angles along the pixel s line of sight (Z-direction), 2B E(x,y)= E(x,y,z)dz 0 2BI0 e-s(z+ v x2+(y-H)2+(z-B)2) 
 =ß P(g,p - . )dz. (2) x2 +(y - H)2 +(z - B)2 0  The above equation relates the camera irradiances as 
a function of the three medium parameters, s, ß and g. Although obtaining an analytic (closed-form) solution 
to the above integral is hard [Sun et al. 2005], it is straightforward to evaluate it numerically. 3.3 
Space of valid medium parameters Different materials have their own natural densities and scattering 
properties, which are all unknown before experimentation. So, how do we know if single scattering is 
dominant at a particular concen­tration for a given material? Note that the scattering ß , absorption 
. and extinction s, coef.cients are proportional to the concentra­tion (fraction of volume diluted in 
water) of the medium. Thus, we performed exhaustive simulations to derive the complete space of parameters 
for which the above image formation model is ac­curate2. For ground truth, we simulated the irradiances 
obtained using multiple scattering for the same set of parameter values, us­ing a standard volumetric 
Monte Carlo technique. Figure 5 shows a plot of the differences between energies captured by the single 
scattering and multiple scattering simulations for a set of parame­ter values. From the RMS errors in 
the plot, we can de.ne the up­per bounds on the parameters . and s =ß +. as those for which the energy 
differences between our model and the ground truth are less than .ve percent. For example, the valid 
domain where single scattering is dominant, is approximately s < 0.04 for . < 0.004. 3.4 How to choose 
the best concentration? Based on the simulations, we present an experimental method to determine the 
best concentration for our measurements. Figure 6 shows images acquired of different concentrations of 
milk and MERLOT. Which among these images should we use to measure the scattering properties? Several 
heuristics may be used to decide on a particular concentration. For instance, the extent of blurring 
of the light source provides us a good clue to determine whether multi­ple scattering is signi.cant (rightmost 
image in Figure 6). A better heuristic is to compute an approximation to the extinction coef.­cient s 
from the attenuated brightness of the light source. Under single scattering, the radiance in the direction 
of the source (dis­tance d) can be approximated using exponential attenuation as: I0 - E(0) d2 es d , 
(3) 2This extends the simulations in [Sun et al. 2005], where a small part of the possible parameter 
space (pure isotropic scattering) was considered. 18 16 14 12 10 8 6 4 2 0 10 0.1 -3 6 0.02 0.04 0.06 
0.08 x10 4 20 Absorption Coefficient, ê Extinction Coefficient, ó Figure 5: Plot showing the differences 
between irradiances obtained by simulating single scattering and multiple scattering (ground truth) models, 
for a large space of parameter values s and . =s -ß. An upper bound on the differences of, say, 5%, can 
be used to de.ne the range of parameters for which single scattering is a valid approximation. From the 
plot, the valid range is approximately s < 0.04 for . < 0.004. where s is an estimate of the extinction 
coef.cient s. In the ab­sence of multiple scattering, this estimate is closer to the true value of s 
(and varies linearly with concentration), whereas, in the pres­ence of multiple scattering, this estimate 
is called diffuse or reduced attenuation coef.cient [Ishimaru 1978] and is usually much lesser than s. 
Thus, we can determine whether the concentration can be used for measurement by observing the plot (Figure 
7 of s versus the volume fraction of the medium diluted with water). Figure 7 shows that after a certain 
amount of milk is added to water, the s no longer remains linear with concentration (dashed line), and 
must not be used for measurements. For a purely absorbing liquid like wine (MERLOT), the plot is completely 
linear and any image that has the best signal-to-noise ratio may be used. Similarly, the plot shows that 
coke scatters, albeit weakly, and ESPRESSO coffee scatters light strongly. We use this simple procedure 
to try several concentrations and observe where the linearity in the plot fails to determine the best 
concentration. As a further test, we check if the estimated parameters from this concentration lie within 
the valid space of parameters simulated above. 9ml 15ml 20ml 900ml 1500ml 16250ml Figure 6: Images illustrating 
different degrees of scattering and absorp­tion. [Top row] Images of milk at various concentrations. 
Since milk is a highly scattering liquid, we observe an increase in blurring with increasing concentration. 
[Bottom Row] Images of red wine at various concentrations. Red wine is a highly absorbing liquid, showing 
only a saturation of the bulb color with increasing concentration, and no blurring. The highlighted im­ages 
are chosen for estimating the parameters. Extinction Coefficient (per mm) 0.025 Regular Milk Expresso 
Coffee Coke 0.02 Merlot Wine Era Detergent 0.015 0.01 0.005 0 .. 0.0 0.5 1.0 1.5 2.0 2.5 21 38 55 72 
89 106 Concentration (ml per litre) Figure 7: Plot of extinction coef.cient estimate s as a function 
of the vol­ume of the media diluted in water in the measurement apparatus. The plots are linear when 
multiple scattering is negligible and single scattering is dominant. As the concentrations of media (and 
hence multiple scattering) increase, the estimated s is less than the true extinction coef.cient s.For 
a highly scattering medium such as milk, the linearity fails at very low con­centrations, while for an 
absorbing medium such as MERLOT, the linearity is always preserved. 4 Estimating Medium Properties based 
on Single Scattering In this section, we present a non-linear minimization algorithm to estimate the 
properties of the medium (s, ß and g), from the mea­sured image irradiances E(x,y)(see Equation (2)). 
We then demon­strate the accuracy of the algorithm through extensive simulations. 4.1 Formulating the 
Error Function The error at each pixel is written as the difference between the mea­sured irradiance 
E(x,y)and the irradiance predicted by the model in equation 2, F(x,y)=E(x,y)- RHS(x,y). (4) Here RHS(x,y)is 
the numerically evaluated right hand side integral in the model of equation 2. Then, the parameters s, 
ß and g can be estimated by computing the global minimum of the sum of squares of the errors of all the 
pixels, as, min .. F2(x,y). (5) ß,s,g yx The above function essentially requires a 3-parameter search. 
How­ever, note that the parameter ß is a global scale factor. Thus, we can eliminate ß by de.ning a normalized 
error function as, E(x,y) RHS(x,y) Fnorm(x,y)= - . (6) max E(x,y)max RHS(x,y) x,yx,y Now, instead of 
requiring a 3-parameter search, the above problem can be reduced to a 2-parameter search that minimizes 
the normal­ized objective function to estimate s and g: min.. F2 norm(x,y). (7) s,g yx Then, the scale 
factor ß can be recovered using the original func­tion F. To compute the global minimum, we use Nelder-Meade 
search implemented by the MatlabTM function fminsearch . Percentage RMS Error in Estimation -5 8 (x10 
) 6 4 2 0 10 1 8 0.8 6 0.6 4 0.4 Extinction 2 0.2 Phase 0 Coefficient, ó Function, q Figure 8: Plot 
showing the errors in reconstruction of the single scattering parameters s and q =|g|, where -1 < g < 
1, compared to ground truth values. The low errors indicate the accuracy of our estimation technique. 
The maximum of the errors for positive or negative g is shown. 4.2 Estimation Accuracy using Simulations 
Fortunately, since the space of the possible parameters is small (see Section 3.3), exhaustive simulation 
of the above algorithm is pos­sible. We only show the correctness of the estimated parameters s and g, 
using Equation (7). The estimation of the scale factor ß then follows trivially. Gaussian noise of unit 
standard deviation was added in all our simulations. The non-linear search was initialized randomly for 
both the parameters s and g. The plot in Figure 8 shows the error in the estimated parameters as compared 
to ground truth values. In all the cases, the estimation errors were less than 0.0001%, and the number 
of iterations required for convergence was less than 100. Since the numerical evaluation of the integral 
is very fast, the time for convergence is usually of the order of a few minutes. This demonstrates that 
the inverse estimation is fast and results in unique and correct parameters. 4.3 Implementation Issues 
We present two issues that need careful implementation for our al­gorithm to be successful on real images. 
Calibrating the area source: Our method does not rely on isotropic point sources but requires only a 
calibrated divergent source to take advantage of the different phase angles measured in the same view 
and hence, any off-the-shelf bulb suf.ces. For our real setup, we have implemented a spherical diffuse 
area source. To compute the irradiance at any point P within the tank, we sample (using roughly 10x10 
samples) the hemisphere of the bulb that is visible to that point P. The non-uniform directional intensities 
and intensity fall-off were calibrated carefully by using a light meter at discrete 3D locations within 
the tank. The camera also measures a pure water image (without any scattering or absorption) to give 
the image irradiance of each source element (sample). This irradi­ance along with the fall-off value 
and the pixel solid angle is used to determine the intensity without scattering. Instabilities in the 
H-G phase function for highly absorbing me­dia: The H-G phase function was designed for scattering media 
and is not de.ned for purely absorbing media. However, for highly absorbing media, the scattering coef.cient 
ß is very low and the average cosine g 1 since rays only pass straight through, much like highly forward 
scattering media. Even though this was not a problem in simulations, the instability for g > 0.95 can 
be high in real experiments. For this special case, we simply use a trun­cated legendre polynomial expansion 
of the H-G phase function as P(g,. )=.i (2i +1)gi Li(. ), and truncate to less than 100 terms. As an 
undesirable byproduct the .ts may show some ringing at the tail of the phase function. However, this 
truncated function still .ts higher brightness well and thus does not affect appearance strongly. Despite 
this instability, the H-G phase function is .exible enough to model the scattering behavior of all our 
materials. Grape Juice ERA Detergent Strawberry Shampoo Lemon Tea Powder Chocolate milk (regular) Pink 
Lemonade Powder Cappuccino Powder Coffee Espresso Low Fat Milk Figure 9: Captured photographs of a variety 
of water-soluble media illus­trating different degrees of scattering and absorption. For highly scattering 
media such as milk, chocolate milk and espresso, we observe a signi.cant blur around the bulb. For highly 
absorbing media such as grape juice, there is very little scattering. All the images have wide dynamic 
range of inten­sities and hence, we have tone-mapped them for illustration. Please see supplementary 
material for more images. 5 Actual Measurements and Validation Using our approach, we have measured the 
scattering properties of a broad class of forty commonly found participating media that can be either 
(a) diluted in water such as juices (for example, apple, strawberry, orange), beverages (for example, 
coffee, soft drinks, milks, wines, beers), cleaning supplies (detergents), or (b) sus­pended in natural 
waters such as impurities and organisms, or even (c) dissolved in water such as powders and sugar, salt 
crystals. In addition to liquids available at the usual supermarkets, we have also collected four samples 
from different locations and depths in the Paci.c ocean. We then present detailed validation by showing 
that our parameters extrapolate correctly to higher concentrations as well, where multiple scattering 
is prominent. A subset of nine photographs of the diluted set of liquids con­tained in the glass tank 
is shown in Figure 9, similar to the four in Figure 1. Together, these include representative types of 
media such as highly scattering, highly absorbing and moderate levels of absorption and scattering. The 
images show a high dynamic range of brightness and are enhanced to show the scattering effects. The set 
of scattering parameters for all the media is shown in Table 1. The extinction (s) and scattering (ß 
) coef.cients are given for each of the three color channels, red, green and blue. The phase function 
parameter g is also shown for the three color channels. Note that all the extinction and scattering coef.cients 
are less than 0.04 in accor­dance with our simulations in Section 3.3. Also, as expected, in all cases, 
the scattering coef.cient does not increase with wavelength. 5.1 Fits to Measured Brightness Pro.les 
We demonstrate the accuracy of our technique by reconstructing the photographs using the estimated parameters. 
Although we con­sidered the brightness at all pixels in the captured photographs, for illustration purposes 
we show only the pro.le of intensity values in the direction that is radially outward from the source. 
Figure 10 Intensity (0-255) 0.8 Original Profile Reconstructed Profile 4 0.6 3 0.4 2 0.2 1 0 0 50 100 
150 0 50 100 150 Distance from the Source (mm) Yuengling Beer Cappuccino Powder 2510 208 156 104 52 00 
0 50 100 150 0 50 100 150 Sports Gatorade Low Fat Milk 12 15 10 8 10 6 4 5 2 0 0 0 50 100 150 0 50 100 
150 Regular Soymilk Orange Powder Figure 10: Fits obtained using the estimated parameters as compared 
against the corresponding measured brightness pro.les in the captured pho­tographs. The brightness pro.le 
is measured radially outward from the source in the image. The red, green and blue plots correspond to 
the three color channels of the camera. The match between the estimated and mea­sured data demonstrates 
the accuracy of the estimation technique. The .ts for six (out of 40) representative materials with varying 
degrees of absorp­tion and scattering are shown. Please see the supplementary material for more plots. 
shows the good .ts obtained using the estimated parameters com­pared against the measured pro.les for 
a subset of six materials of varying degrees of scattering and absorption properties (please re­view 
supplementary document for plots of other materials). When there is no scattering (pure absorption), 
.tting a scattering model can induce some ringing effect in the dark tail end of the pro.le. We can detect 
this special case and use the attenuation model to compute the absorption coef.cient (. = s). 5.2 Extrapolation 
to higher concentrations The extinction and scattering coef.cients are proportional to the concentration 
of the medium. Thus, if ß1 and s1 are estimated at concentration c1, then the coef.cients ß2 and s2 at 
another concen­tration c2 can be extrapolated using: c2 c2 ß2 = ß1 , s2 = s1 . (8) c1 c1 Note, however, 
that g is independent of the medium concentration. While we estimate the parameters from lower concentrations, 
it is important to ensure that the parameters can be scaled to any con­centration (where multiple scattering 
cannot be ignored) to produce accurate scattering effects. We show an example validation using .ts obtained 
in comparison to the measured brightness pro.les of chocolate milk at various concentrations. Figure 
11 shows the .ts  Table 1: Scattering properties for 40 different water-soluble materials estimated 
using our technique. The second column lists the volumes V of the materials dissolved in 23 -V litres 
of water to achieve the desired levels of dilution where single scattering is dominant. These parameters 
can be proportionately scaled to any other volume Vn, using a scale factor of Vn/V. The percentage RMS 
errors (obtained over all pixels) quantify the accuracy of .ts achieved with the estimated parameters 
to the measured intensity pro.les. Errors for all the highly scattering media are less than 3%. For low-scattering 
materials, the total intensity of pro.les is relatively low, thus making the estimation more sensitive 
to noise. Even for such low-scattering media, the errors are less than 5 - 6%. The last four rows are 
the parameters for various ocean water samples at their original concentrations. The time elapsed between 
the collection of samples and the image acquisition is listed in the parentheses. Since the suspended 
particles in ocean water settle down with time, we observe a small decrease in scattering coef.cients 
in the sample for which 8 hours had been elapsed as compared to the one which was imaged just 30 minutes 
after collection. Note that all the extinction and scattering coef.cients are less than 0.04 in accordance 
with our simulations in Section 3.3. As expected, the scattering coef.cients do not decrease with wavelength. 
The scattering albedos (ratio of scattering coef.cients to the extinction coef.cients) is much higher 
for the scattering media (milk, coffee, orange powder) as compared to the absorbing ones (coke, wine). 
For materials that have ß = 0, the phase function parameter g is unde.ned. As seen from the values of 
g which are closer to 1, several media are predominantly forward scattering. The parameters for the milks 
match those in [Jensen et al. 2001] up to a scale factor (due to the different fat contents in the milks 
used), providing further support for our estimation. in this validation experiment. First, we estimate 
the parameters 8) and use them in a standard volumetric Monte Carlo renderer that from the photograph 
of only 8ml of chocolate milk diluted in wa-includes multiple scattering. The plots in (b) -(d) demonstrate 
the ter, where single scattering is dominant. In (a), we show the .t strong .ts obtained. This demonstrates 
that our parameters are ro­obtained compared against the measured intensity pro.le. How-bust enough to 
be extrapolated to higher concentrations. In fact, ever, for higher concentrations of 50ml, 100ml and 
150ml, multiple we will show renderings of most of the liquids at their natural con­scattering cannot 
be ignored. For these cases, we scaled the coef.-centrations (Section 6) despite measuring the parameters 
at signi.­cients (s and ß ) by factors of {50/8,100/8,150/8} (see Equation cantly dilute states. Brightness 
(log scale) Brightness (log scale) 7 6 10 10 Actual (dots) 6 10 5 5 10 10 4 10 4 10 3 10 2 3 
10 10 0 50 100 150 0 50 100 150 Pixels Pixels (a)8ml (b)50ml Brightness (log scale) Brightness (log 
scale) 6 6 10 10 Actual (dots) 5 5 10 10 4 104 10 3 103 10 0 50 100 150 Pixels Pixels (c)100 ml 
(d)150 ml Figure 11: Extrapolation of parameters to higher concentrations with mul­tiple scattering. 
(a) 8 ml of chocolate milk is diluted in water and the parameters are estimated using the measured brightness 
pro.le. (b) -(d) The parameters estimated in (a) are scaled to higher concentrations (50ml, 100ml and 
150ml) where multiple scattering cannot be ignored. Plots show a good .t between the brightness pro.le 
obtained by extrapolating our esti­mated parameters with a Monte Carlo renderer, and the ground truth 
mea­surements. The .ts are shown in logarithmic scale. MERLOT Wine CHARDONNAY Wine  ESPRESSO Coffee 
YUENGLING Beer Pink Lemonade Powder ERA Detergent  Strawberry Shampoo Orange Powder 6 Example Volumetric 
Renderings The scattering properties estimated in this work can be input to any volumetric rendering 
algorithm to create visual effects of partici­pating media. Here, we chose brute-force volumetric Monte-Carlo 
path tracing since it can be used to render arbitrary materials3.We use photon mapping for rendering 
caustics. For display purposes, we have applied a tone-mapping operator [Ward-Larson et al. 1997] to 
the renderings. Indices of refraction (IOR) of these media are also important for rendering. In initial 
experiments, we found the IOR to be between 1.33 (water) and 1.42 (milk) and varying lin­early with concentrations, 
by using location of total internal re.ec­tion from the top of the water surface in the tank. In current 
ren­derings, we have simply used an IOR proportionate to the medium concentrations between 1.33 and 1.42, 
since this does not alter the visual appearance of the liquid drastically. We wish to perform thorough 
experiments in the future. Figure 12 shows a mosaic of images of liquids rendered in their natural concentrations, 
partially .lled in a cognac glass and il­luminated by the Kitchen Environment Map [Debevec 1998]. These 
include two different types of wine (deep red MERLOT and golden-yellow CHARDONNAY), dark brown coffee 
ESPRESSO, and the golden-orange YUENGLING beer. Notice the color dif­ferences between MERLOT (no scattering) 
and ESPRESSO (mod­erate scattering) even though both of them are dark liquids. Ob­serve that while beer 
and CHARDONNAY are very clear liquids, coffee is noticeably more opaque. Similarly, Figure 13 shows a 
mosaic of predominantly bright colored liquids such as the deep 3Under-sampling of path-traces can cause 
speckle noise seen in the ren­derings, and is not an artifact of our estimation.  a = 0.03 a = 0.125 
a = 0.25 a = 0.99 a = 0.03 a = 0.125 a = 0.25 a = 0.99 Figure 14: Effect of changing concentrations 
of a highly absorbing (MERLOT) and a highly scattering (milk) liquid. In the case of wine, notice that 
while the color gradually becomes deep red, the liquid remains clear, due to the lack of scattering. 
In the case of milk, however, we see a quick transition froma murky appearance to a soft white appearance, 
due to the high scattering albedo of milk. blue ERA detergent, the reddish strawberry shampoo, and powders 
dissolved in water such as the pinkish strawberry lemonade and orange powders. These images are illuminated 
only by a strong directional source to illustrate the bright caustics whose colorings are primarily due 
to absorption. We also present different types of novel visual effects obtained by changing or blending 
the param­eters of different media to create realistic images of dilutions and mixtures of the original 
measured materials. Effect of changing concentrations: Figure 14 illustrates the ef­fects of changing 
concentrations of media in water. The top row shows a transition from pure water to MERLOT, obtained 
by scal­ing parameters of wine as in Equation 8. Notice the changes in caustics and the gradual deepening 
of the red color of the liquid. Note that as the transition occurs, the liquid remains clear even though 
the color changes; this is due to the pure absorbing nature of wine, as depicted by our parameters. The 
bottom row shows the effect of changing milk concentration in water. Since milk is a highly scattering 
medium, as expected, the appearance quickly changes from murky whitish water to soft and thick white 
milk. This is because the scattering albedo ß /s is high and the phase function parameter g is such that 
a signi.cant amount of light dif­fuses into different directions. Blending parameters for mixtures of 
media: For example, what are the properties of a mixture of ESPRESSO and milk, or otherwise known as 
light coffee? Consider a medium containing a mixture of two types of media, A and B. The properties of 
the indi­vidual media are denoted with the subscripts A and B. The scatter­ing coef.cient of the mixture 
is obtained by a weighted average, VAßA +VBßB ßmix = . (9) VA +VB The absorption and extinction coef.cients 
are similarly de.ned. Unlike above where we just changed the scattering and absorption coef.cients, here 
a new phase function parameter must be de.ned for the mixture as the weighted average [Key 2005], gAßA 
+ gBßB gmix = . (10)ßmix These equations can be used to render mixtures of participating me­dia or morph 
from one medium into another. Figure 15 shows mix­ing of different proportions of milk and wine. The 
second example shows a more common mixing of milk and coffee. Such mixing between materials, for the 
.rst time, gives a user the .exibility to create novel renderings of participating media. 7 Conclusion 
Rendering the rich visual effects of participating media, like .uids or underwater impurities, requires 
precise measurements of their scattering properties. In this paper, we have developed a simple de­vice 
and method for accurately estimating the scattering properties of a variety of media that can be diluted 
in water. Our approach only requires a single high dynamic range photograph. By dilut­ing the medium, 
we work in the single scattering regime, where the inverse light transport problem is well conditioned 
however, we can later render at arbitrary concentrations and even mix materials. We have presented a 
database of scattering parameters for 40 com­monly found materials. This database is the .rst of its 
kind, and enables computer graphics practitioners to accurately render a wide variety of participating 
media, rather than having to set parameters in an ad-hoc fashion. In the future, we would like to improve 
this work by investigating different phase functions and measuring in­dices of refraction more accurately. 
 50% Milk + 50% Coffee 75% Milk + 25% Coffee  50% Wine + 50% Milk 75% Wine + 25% Milk Acknowledgements: 
This work was supported by NSF Grants 0541307, 0305322, 0446916 and 0305399, and an ONR grant N00014-05-1-0188. 
Ramamoorthi, Jensen and Donner were sup­ported also by Sloan Fellowships. The authors thank Terrence 
Boult for ocean water samples, Chao-Kuo Lin for help with ex­periments, and Simon Premoze for early discussions. 
References ANTYUFEEV, S. 2000. Monte Carlo Method for Solving Inverse Problems of Radiative Transfer. 
Inverse and Ill-Posed Problems Series, VSP Publishers. BOSS, E., AND PEGAU, W. S. 2001. Relationship 
of light scatter­ing at an angle in the backward direction to the backscattering coef.cient. Applied 
Optics 40 (30), 5503 5507. CHANDRASEKHAR, S. 1960. Radiative Transfer. Oxford Univer­sity Press. DANA, 
K., NAYAR, S., VAN GINNEKEN, B., AND KOENDERINK, J. 1997. Re.ectance and texture of real-world surfaces. 
In Proc CVPR, 151 157. DEBEVEC, P. 1998. Rendering synthetic objects into real scenes: Bridging traditional 
and image-based graphics with global il­lumination and high dynamic range photography. Proc. SIG-GRAPH 
98, 189 198. FINSY, E. G., AND JOOSTEN, J. 1991. Maximum entropy inver­sion of static light scattering 
data for the particle size distribution by number and volume. In Advances in measurements and con­trol 
of colloidal processes. Butterworth-Heineman, Ch. 30. FUCHS, E., AND JAFFE, J. S. 2002. Thin laser light 
sheet micro­scope for microbial oceanography. OPTICS EXPRESS 10 (2), 145 154. HAWKINS,T., EINARSSON,P., 
AND DEBEVEC, P. 2005. Ac­quisition of time-varying participating media. ACM Trans. on Graphics (SIGGRAPH) 
24, 3, 812 815. HENYEY, L., AND GREENSTEIN, J. 1941. Diffuse radiation in the galaxy. vol. 93, 70 83. 
HULST, V. D. 1957. Light Scattering by small Particles. John Wiley and Sons. ISHIMARU, A. 1978. Wave 
Propagation and Scattering in Ran­dom Media. Volume 1: Single Scattering and Transport Theory. Academic 
Press. JAEGER, D., DEMEYERE, H., FINSY, R., SNEYERS, R., VANDERDEELEN, J., VAN-DER-MEEREN,P., AND VAN-LAETHEM, 
M. 1991. Particle sizing by photon correlation spectroscopy. part i: Monodisperse latices: in.uence of 
scatter­ing angle and concentration of dispersed material. In Part. Syst. Charact. 8, 179. JENSEN, H., 
MARSCHNER, S., LEVOY, M., AND HANRAHAN,P. 2001. A practical model for subsurface light transport. In 
Proc. SIGGRAPH 01, 511 518. KEY, J. R. 2005. Streamer: User s guide. Tech Report, NOAA/NESDIS, Madison, 
Wisconsin. MARSCHNER, S. 1998. Inverse rendering for computer graphics. PhD Thesis, Cornell University. 
MATUSIK,W., PFISTER, H., BRAND, M., AND MCMILLAN,L. 2003. A data-driven re.ectance model. ACM Trans. 
on Graphics (SIGGRAPH) 22, 3, 759 769. MCCORMICK, N. J. 1981. A critique of inverse solutions to slab 
geometry transport problems. Prog. Nucl. Energy 8. MCCORMICK, N. J. 1985. Sensitivity of multiple-scattering 
in­verse transport methods to measurement errors. JOSA A2. MCCORMICK, N. J. 1996. Analytical transport 
theory applications in optical oceanography. Annals of Nuclear Energy 23, 381 395. NARASIMHAN, S. G., 
AND NAYAR, S. K. 2003. Shedding light on the weather. In CVPR 03, 665 672. OISHI, T. 1990. Signi.cant 
relationship between the backward scattering coef.cient of sea water and the scatterance at 120 de­grees. 
Applied Optics 29 (31), 4658 4665. PRAHL, S. A. 1988. Light transport in tissue. PhD Thesis, Uni­versity 
of Texas at Austin. RAMAMOORTHI, R., AND HANRAHAN, P. 2001. A signal pro­cessing framework for inverse 
rendering. Proc. SIGGRAPH 01, 117 128. SULLIVAN, S. A. 1963. Experimental study of the absorption in 
distilled water, arti.cial sea water, and heavy water in the visible region of the spectrum. JOSA 53. 
SUN, B., RAMAMOORTHI, R., NARASIMHAN, S. G., AND NA-YAR, S. K. 2005. A practical analytic single scattering 
model for real time rendering. ACM Trans. on Graphics (SIGGRAPH) 24, 3, 1040 1049. WARD-LARSON,RUSHMEIER, 
H., AND PIATKO. 1997. A visibil­ity matching tone reproduction operator for high dynamic range scenes. 
IEEE Trans. on Visualization and Computer Graphics 3, 4, 291 306. Eurographics Symposium on Rendering 
(2005) Kavita Bala, Philip Dutré (Editors) Non-Linear Volume Photon Mapping Diego Gutierrez , Adolfo 
Munoz, Oscar Anson and Francisco J. Seron GIGA, Universidad de Zaragoza, Spain Abstract This paper describes 
a novel extension of the photon mapping algorithm, capable of handling both volume multiple inelastic 
scattering and curved light paths simultaneously. The extension is based on the Full Radiative Transfer 
Equation (FRTE) and Fermat s law, and yields physically accurate, high-dynamic data than can be used 
for image generation or for other simulation purposes, such as driving simulators, underwater vision 
or lighting studies in architecture. Photons are traced into the participating medium with a varying 
index of refraction, and their curved trajectories followed (curved paths are the cause of certain atmospheric 
effects such as mirages or rippling desert images). Every time a photon is absorbed, a Russian roulette 
algorithm based on the quantum ef.ciency of the medium determines whether the inelastic scattering event 
takes place (causing volume .uorescence). The simulation of both underwater and atmospheric effects is 
shown, providing a global illumination solution without the restrictions of previous approaches. Categories 
and Subject Descriptors (according to ACM CCS): I.3.7 [Computer Graphics]: Three-Dimensional Graphics 
and Realism 1. Introduction Simulation of nature has always been one of the loftiest goals of computer 
graphics, providing a rich range of visual phe­nomena. Most of the times, the effect to be reproduced 
can be faked using a top-down approach, where the .nal desired result guides the implementation. This 
usually turns out rel­atively fast, ad-hoc methods that yield more than acceptable results. However, 
a physically correct simulation is neces­sary in certain .elds where accuracy is a must. Underwater vision, 
driving simulators, the military, architectural light­ing design etc. are .elds where it is not enough 
to render an image which resembles reality. Predictive algorithms must be developed instead, where the 
image is the .nal visualiza­tion of the physically correct data generated. A bottom-up approach is then 
necessary: .rst, the basic laws of physics that govern the phenomenon need to be described and fed to 
the rendering system; the phenomenon itself will just be the logical, inevitable output. This approach 
sacri.ces rendering speed in exchange for reliable, physically accurate numerical data that can be used 
for purposes beyond image generation. e-mail: diegog@unizar.es . The Eurographics Association 2005. Two 
of the greatest sources of visually appealing phenom­ena in nature are participating media and a varying 
index of refraction. Participating media are the cause of such well­known effects such as fog, clouds 
or blurry underwater vi­sion, whereas a varying index of refraction yields mirages, rippling images, 
twinkling stars or some spectacular sunsets. Sources of inelastic scattering in ocean waters can greatly 
af­fect visibility and alter its color, whereas distortions caused by temperature differences can further 
alter the perception of things in such environment. Simulating underwater res­cue missions, laying submarine 
data cables or even the cor­rect interpretation of ancient World Heritage sites can bene.t from an accurate 
description of light that includes an ampler range of phenomena. We present in this paper a physically-based 
spectral simu­lation of light, solving the Full Radiative Transfer Equation (FRTE) and applying Fermat 
s law, which includes multi­ple inelastic scattering as well as an accurate description of the non-linear 
paths followed by the light rays in media with a varying index of refraction. It is based on an extension 
of the volume photon map algorithm presented by Wann Jensen and Christensen [JC98]. The main contributions 
are a full global illumination solution which supports non-linear light D. Gutierrez, A. Munoz, O. Anson 
&#38; F. J. Seron / Non-Linear Volume Photon Mapping paths and is free of the restrictions of previous 
works, and the physically-correct simulation of volume .uorescence in participating media, caused by 
inelastic scattering, including ef.cient computation of caustics. Atmospheric effects and underwater 
imagery are simulated as case studies to demon­strate the algorithm. To our knowledge, there is no previ­ous 
research in computer graphics literature that models to­gether physically-based inelastic scattering 
in participating, inhomogeneous media where the index of refraction varies continuously. Related previous 
works therefore span two dif­ferent categories: inelastic scattering in participating media and non-linear 
light propagation. Rendering participating media is not a new .eld in com­puter graphics, and an exhaustive 
review can be found in [PPS97]. There are two types of scattering events in a participating medium: elastic 
scattering, where no transfer of energy occurs between wavelengths, and inelastic scatter­ing, where 
such energy transfers do occur, from shorter to longer wavelengths. Spectral global illumination algorithms 
that handle participating media only take into account elas­tic scattering, with the strategy consisting 
on decoupling the solutions for each sampled wavelength, then adding them to obtain the .nal image. No 
interaction between wavelengths is computed. To the authors knowledge, the only previous work that simulates 
volume inelastic scattering in participat­ing media is owed to Cerezo and Seron [CS03], using a dis­ 
crete ordinate method. Unfortunately their method requires both rectangular meshing of the geometry, 
as well as an an­gular and spatial discretization which imposes high memory requirements, thus limiting 
the complexity of the scenes that can be reproduced (the problem is aggravated when simu­lating highly 
anisotropic scattering). They also cannot pro­vide a full solution, failing to render caustics. Surface 
inelas­tic scattering works include [Gla95b] or [WTP01], but their methods are not extensible to participating 
media. With respect to non-linear ray tracing, the .rst method to deal with non-straight light paths 
is owed to Berger et al. [BTL90], refracting the ray according to Snell s law in each of a series of 
.at homogeneous layers, thus achieving a piece-wise linear approximation of a curved path. This was challenged 
by Musgrave [Mus90], who develops a purely re.ective model where rays follow a parabolic path, fol­lowing 
the Kuhlar/Fabri physical model [FFLV82]. A more general approach to non-linear ray tracing is proposed 
by Gröller [Grö95], although the work does not study the in.u­ ence of the index of refraction in the 
curvature of the rays, vi­sualizing mathematical and physical systems instead. In the paper by Stam and 
Languenou [SL96], the authors use geo­ metrical optics to describe how light bends if the index of re­fraction 
of the medium varies continuously. They neverthe­less fail to provide a physically-based analytical expression 
for the index of refraction as a function of temperature and wavelength, and solve the equations only 
for two speci.c cases, thus losing generality. Seron et al. [SGGC05] imple­ ment a method of curved ray 
tracing capable of simulating the inferior mirage and some sunset effects, although they do not attempt 
to calculate any lighting, deforming pre-lit tex­tures instead. In [HW01] gravitational light bending 
is visu­ alized according to the theory of general relativity, whereas other relativity-and physics-related 
papers include the bend­ing caused by neutron stars or black holes [Nem93], so they cannot (nor pretend 
to) simulate the phenomena described in this paper. Yngve et al. [YOH00] describe a simple method to 
simulate the bending of light by interpolating a density .eld, but they need to exaggerate the variation 
of the index of refraction tenfold for the effect to be visible. The paper is organized as follows: section 
2 provides the physically-based background, with an overview of inelastic scattering, the FRTE and the 
Fermat s law. In section 3 we describe our extension of the volume photon map algorithm to include inelastic 
scattering and curved light paths, with sections 4 and 5 providing case studies of underwater im­agery 
and atmospheric effects respectively. The discussion of the results and some additional images are presented 
in section 6, to .nish the paper in section 7 with the conclu­sions and future work. 2. Physically-based 
Framework We now present the physical framework of our work, by .rst introducing what inelastic scattering 
is, then deriving the FRTE that needs to be solved to account for it. In or­der to be able to compute 
non-linear light paths, we will use Fermat s law to obtain the correct trajectories. 2.1. Inelastic scattering 
Inelastic scattering implies an energy transfer from wave­length d. to d, with d. < d within the visible 
spectrum, and gives rise to .uorescence and phosphorescence phenomena. Fluorescence occurs when a molecule 
absorbs a photon of wavelength d. (called excitation wavelength), and re-emits it at a longer wavelength 
d according to a .uorescence ef­.ciency function Pf (d). The time lapse between the two events is 10-11 
to 10-8 seconds, so for computer graphics it can be taken as an instantaneous process. For pure sub­stances, 
re-emission is isotropic and the wavelength of the re-emitted photons is independent of the different 
excitation wavelengths, although the intensity of the re-emission does depend on them. Phosphorescence 
is a similar process, gov­erned by the phosphorescence ef.ciency function, with the main difference being 
that the re-emitted energy declines with time according to a function d(t). 2.2. Full Radiative Transfer 
Equation Usually, participating media algorithms solve the integro­differential Radiative Transfer Equation 
(RTE), which takes into account emission, absorption and elastic scattering, but c . The Eurographics 
Association 2005. D. Gutierrez, A. Munoz, O. Anson &#38; F. J. Seron / Non-Linear Volume Photon Mapping 
does not yield a solution for inelastic scattering events. Fol­lowing the notation in [JC98], and reformulating 
to include wavelength dependencies, the RTE can be written as: sLA(x, -ww ) sx = aA(x)Le,A(x, -ww ) + 
pA(x)Li,A(x, -ww )- aA(x)LA(x, -ww ) - pA(x)LA(x, -ww ) (1) w where OL(x, -w ) represents the variation 
of radiance L at a Ox w point x in the direction -w , a and p are the absorption and scattering coef.cients, 
Le is the emitted radiance and Li is the in-scattered radiance. De.ning the extinction coef.cient as 
aA(x)= aA(x)+ pA(x) and integrating Li,A over the sphere Q we get: - w sLA(x, w ) - w = aA(x)Le,A(x, 
w )+ sx Z -' --' w' w www)d-- pA(x) 1 pA(x, w , w )LA(x, ww - aA(x)LA(x, w ) (2) which is the integro-differential, 
wavelength-dependent RTE governing the transport of light in participating media, with -' w w- pA(x, 
w , w ) being the phase function that de.nes the re­emission direction. However, this equation does not 
account for energy transfers between wavelengths, the phenomenon known as inelastic scattering. To be 
able to compute these inelastic scattering events, we need to develop the RTE equa­tion further, by adding 
a term that accounts for such energy transfers. This term can be expressed as a double integral over 
the domains of the solid angle and wavelength: ZZ w ' pA(x, -w i', w) w aAi (x) f (x, di w d)LAi (x, 
-ww ) d-w iddi 1A 4. (3) where aAi is the absorption coef.cient for wavelength di (re­member there is 
no inelastic scattering without previous ab­sorption), f (x, di w d) is the function that governs the 
ef.­ciency of the energy transfer between wavelengths, de.ned as the probability of a photon of di being 
re-emitted at d. For .uorescence and phosphorescence, this phase function is isotropic [Mob94]. Adding 
this term to the RTE (equation 2) we obtain the FRTE: - w sLA(x, w ) - w = aA(x)Le,A(x, w )+ sx Z -' 
--' w' w www- pA(x) 1 pA(x, w , w )LA(x, w )d-w - aA(x)LA(x, w )+ ZZ ww-' w -' pA(x, i , w) w aAi (x) 
f (x, di w d)LAi (x, w ) d-w iddi(4) 1A 4. which is the equation that must be solved to take into account 
multiple inelastic scattering in participating media, thus be­ing able to render volume .uorescence effects. 
c&#38;#169; The Eurographics Association 2005. 2.3. Varying index of refraction in inhomogeneous media 
A varying index of refraction nA de.nes an inhomogeneous medium where light travels in curved paths. 
These curved paths result in a distorted image, with the mirages being probably the best known manifestation 
of the effect. To be able to simulate this type of inhomogeneous medium, we therefore need to obtain 
the curved trajectory of light as it w traverses it. The direction -w in equation 4 therefore needs to 
be recomputed at each differential step, accounting for the changes in nA. We obtain this corrected direction 
at each step by solving Fermat s law, which de.nes how light tra­verses one given medium. The following 
derivation of Fermat s law uses the work of Gutierrez et al. [GSMA04] and is not meant to be exhaus­ 
tive. As stated in [Gla95a], a ray of light, when travelling from one point to another, follows a path 
that corresponds to a stationary value of the optical path length (OPL). The OPL is de.ned as the index 
of refraction times the travelled path (or the distance the light would have travelled in a vacuum during 
the .ight time through the material), and in its dif­ferential form it can be formulated as d(OPL)= ndl, 
where l is the path travelled by the light ray. The equation shows how light gets bent towards the areas 
with a greater index of refraction, as Snell s law also predicts for the boundary of two homogeneous 
media. A stationary value corresponds to a maximum or a minimum in the function, thus the derivative 
equals zero. We can therefore write: ZZ Z BB B G(OPL)= G ndl = Gndl + nG (dl)= AA A ZZ BB Gn Gxidl + 
nG(dl)= 0 (5)A GxiA where xi are the vector components of l. Considering dxi as dxi variables and taking 
increments we get G(dl)= dl G(dxi). Since light trajectories start and end at the stationary points A 
and B, we get Gxi(A)= 0 and Gxi(B)= 0. Equation 5 then results: Z [()] B snd dxi GL = - n Gxidl = 0 (6) 
A sxi dl dl Since this equation must hold for any value of Gxi, the integrand must equal zero, so we 
.nally come up with the equation that must be solved to obtain the path followed by light while traversing 
any medium, as a function of the index of refraction at each point: () () w dd-r ddx j sn n - n = 0 = 
n - = 0( j = 1, 2, 3) dl dl dl dl sx j (7) D. Gutierrez, A. Munoz, O. Anson &#38; F. J. Seron / Non-Linear 
Volume Photon Mapping Figure 1: Error and rendering time (secs.) as functions of the error tolerance 
in the Dormand-Prince RK5(4)7M method for a test scene. w where -r = x j are the coordinates (x, y, 
z) of each point. This equation cannot be solved analytically, and thus we must ap­ply a numerical method. 
We now need to rewrite equation 7 in order to solve it in a more ef.cient way than the Euler method presented 
in [GSMA04]: () d2x j 1 sn dn dx j = - (8) dl2 n sx j dl dl dx j Doing the change of variable y j = 
we obtain: dl () ' 1 sn dn yj = - y j(9) n sx j dl dn dxj where dn = dl . The change of variable can 
also be writ­ dl dxj ten as: ' x j = y j (10) Equations 9 and 10 de.ne a system where x j represents 
the position and y j the velocity at a given point in the trajec­tory, which can be written in matrix 
form as: ()' y j xj .. = 1 On dn (11) yj - n Ox j dl y j ' This equation 11 has the form Y = f (l,Y 
), which de­.nes an Initial Value Problem with Y (0)= a. We solve this problem by applying the embedded 
Runge-Kutta method RK5(4)7M from the Dormand-Prince family. A detailed de­scription of the method and 
the error tolerance can be found in [DP80]. We have tested the implementation in a simple scene where 
the index of refraction varies according to the equa­tion n = 1 + ky, with y representing height, and 
k varying from -0.1 to 0.1. This distribution of n can be solved ana­lytically, so we can measure the 
numerical error against the exact solution. Figure 1 shows the error of the Dormand-Prince RK5(4)7M method 
as the tolerance is reduced, along with the time it takes to reach the solution. As it can be seen, error 
tolerances in the range of 10-8 to 10-12 yield good results without much of a time penalty. Error tolerances 
be­yond 10-14 start increasing rendering times considerably. 3. Extension of the Volume Photon Mapping 
Algorithm Ray tracing techniques involve shooting rays into the scene from the camera and following them 
to detect hits with the geometry, then shooting shadow rays to the lights to .nd out direct illumination. 
With curved light paths this turns out to be highly impractical, though, since .nding the ray with the 
physically-correct curvature which goes from the intersection point to the light is computationally very 
expen­sive (or the solution might not even exist). Groeller [Grö95] proposes three solutions: considering 
shadow rays to fol­low straight paths, retrieving all lighting information straight from the textures, 
and .nally voxelizing the space and pre­storing the approximated incident directions of light sources 
for each voxel, by launching rays from the light sources into the scene prior to the render pass. The 
.rst two are clearly not physically-based, while the third only approximates the solution with a preprocessing 
step. In order to obtain a physically-based solution for multiple inelastic scattering in inhomogeneous 
media with a varying index of refraction n, we have extended the volume photon mapping algorithm [JC98] 
to account both for volume .uo­ rescence and the distortions caused by the changing n. For inelastic 
scattering, we need to model the possibility of an absorbed photon being re-emitted at a different wave­length. 
Equation 4 includes a term f (x, di w d) known as wavelength redistribution function, which represents 
the ef­ c &#38;#169; The Eurographics Association 2005. D. Gutierrez, A. Munoz, O. Anson &#38; F. J. 
Seron / Non-Linear Volume Photon Mapping .ciency of the energy transfer between wavelengths. It is de.ned 
as the quotient between the energy of the emitted wavelength and the energy of the absorbed excitation 
wave­length, per wavelength unit. Reformulating in terms of pho­tons instead of energy we have the spectral 
quantum ef.­ciency function O(x, di w d), de.ned as the number of pho­tons emitted at d per wavelength 
unit, divided by the number of absorbed photons at di. Both functions are dimensional (nm-1), and are 
related as follows: di f (x, di w d)= O(x, di w d) (12) d A related dimensionless function that describes 
inelastic scattering is the quantum ef.ciency ., de.ned as the total number of photons emitted at all 
wavelengths divided by the number of photons absorbed at excitation wavelength di. It is related to the 
spectral quantum ef.ciency function by the equation: Z .(di)= O(x, di w d)dd (13) A Our extension to 
the volume photon mapping algorithm includes a) solving Fermat s law to obtain the curved trajec­tory 
of each photon if the index of refraction varies (and also for the eye rays shot during the radiance 
estimate phase), thus being able to overcome the shadow ray problem pre­sented above and to obtain a 
full solution including effects such as color bleeding and caustics; and b) the inclusion of the quantum 
ef.ciency . to govern the probability of an inelastic scattering event. As shown in .gure 2, once the 
albedo-based Russian roulette determines that a certain pho­ton has been absorbed by the medium, a second 
Russian roulette based on the quantum ef.ciency determines whether an inelastic scattering event takes 
place, and therefore the photon has to be re-emitted at a different wavelength. This is done by generating 
a random number .in[0, 1] so that: .in . . Photon is re-emitted .in[0, 1] w (14) .in > . Photon remains 
absorbed If re-emitted, the new wavelength must be obtained, for which we must sample the spectral quantum 
ef.ciency func­tion O(x, di w d) for the excitation wavelength di. This can be simply done by rejection 
sampling the function, but to in­crease ef.ciency we perform importance sampling using the inverse of 
its cumulative distribution function (cdf). A ran­dom number .[0, 1] therefore yields the new wavelength 
for the re-emitted photon. Steeper areas of the cdf increase the probability of a photon being re-emitted 
at the corresponding wavelengths. Figure 2 shows the basic scheme of the algorithm. The  &#38;#169; 
The Eurographics Association 2005. Figure 2: Our extended volume photon mapping algorithm. sequence 
of events in the original volume photon mapping by [JC98] is represented inside the grey area. 4. Case 
Study: Underwater Imagery We chose deep ocean waters as our .rst case study, given its rich range of 
elastic and inelastic scattering phenomena and the fact that it is a medium well studied by oceanographers. 
Pure seawater absorbs most wavelengths except for blue: the absorption coef.cient peaks at 760 nanometers, 
and reaches a minimum at 430 nm. The phase function p is modelled as the phase function in pure sea water 
plus the phase func­tion of the scattering by suspended particles, as proposed in [Mob94] (p = pw + pp). 
For pure water we use a phase function similar to Rayleigh s: pw(.)= 0.06225(1 + 0.835cos 2.) (15) while 
the scattering caused by particles is modelled using a Henyey-Greenstein phase function with g = 0.924: 
1 - g2 pp(., g)= (16) (1 + g2 - 2gcos.)3/2 It is very common in ocean waters to see a color shift rang­ing 
from greenish to very bright green, or even yellowish. These hue shifts are due to the variation in the 
concentra­tion and type of the suspended microorganisms, mainly phy­toplankton, which presents a maximum 
absorption at 350 nm. rapidly decreasing to almost zero beyond 500 nm. The D. Gutierrez, A. Munoz, O. 
Anson &#38; F. J. Seron / Non-Linear Volume Photon Mapping Figure 3: Fluorescent ocean water in Cornell 
rooms. (a), (b) and (c) show varying concentrations of chlorophyll (0.05mg/m3 , 0.1mg/m3 and 5mg/m3 respectively). 
(d) High concentration of yellow matter (5mg/m3). most important element in the phytoplankton is chlorophyll, 
which presents spectral absorption peaks in the blue and red ends of the spectrum and is the most important 
source of volume .uorescence in the waters. For chlorophyll, .c(di) is wavelength-independent, with values 
ranging from 0.01 to 0.1 (we use the superscript c for chlorophyll). As with most inelastic scattering 
event, the re-emission phase function is isotropic. Another important source of .uorescence is the Color 
Dissolved Organic Matter (CDOM), also called yellow mat­ter, present in shallow ocean waters and harbors. 
.y(di) is also wavelength-independent, with values between 0.005 and 0.025, and re-emission is also isotropic 
[Haw92]. All the images in the paper have been rendered on a Be­owulf system composed of six nodes, 
each one being a Pen­tium 4 @ 2.8 GHz. with 1 Gb. of RAM. Figure 3 shows different colorations of ocean 
water, according to varying chlorophyll and yellow matter concentrations which trigger inelastic scattering 
events with different probabilities. The images were rendered with 250,000 photons stored in the volume 
photon map and 200 photons used for the radiance estimate. This high numbers are needed to obtain accurate 
results, since we use the volume photon map to compute both direct and indirect illumination. Direct 
illumination in participating media with a varying index of refraction can­not be ef.ciently computed 
using ray tracing techniques, as explained at the beginning of section 3. The spectrum was sampled at 
nine intervals. Below each picture, the result­ing absorption and extinction curves (functions of the 
dif­ferent concentrations of chlorophyll in the modelled waters) are shown for each case. Image (a) shows 
little .uorescence (low chlorophyll concentration of 0.05mg/m3), and the wa­ters are relatively clear. 
When chlorophyll concentration in­creases, .uorescence events become more prominent and the image .rst 
gets a milky aspect (b), losing visibility and reaching a characteristic green hue when chlorophyll reaches 
5mg/m3. Image (d) shows .uorescence owed to yellow mat­ter. The absorption function in this case has 
been modelled after [Mob94]: ay(d)= ay(440)-0.014(A-440) where ay(440) is the empirical absorption at 
440 nm. Rendering times for the images were six minutes. 5. Case Study: Atmospheric Phenomena The images 
in this section illustrate some of the most rele­vant effects in nature owed to curved light paths. To 
achieve physically correct results we have modelled the Earth as a sphere with a radius of 6371 units 
(one unit equals one kilo­meter); the atmosphere is another concentric sphere with a thickness of 40 
kilometers. Taking the 1976 USA Standard Atmosphere (USA76) [USG76], we .rst obtain a standard temperature 
and pressure pro.le of the whole 40 kilometers, with temperature decreasing at an approximate rate of 
0.6. C per 100 meters. In order to curve light correctly according to Fermat s law, we need to obtain 
the wavelength-dependent index of refraction as a function of both the temperature and pressure given 
by the USA76. To do so, we follow the method described in [GSMA04], by .rst obtaining density as a function 
of temperature T (h) and pressure P(h) using P(h)M the Perfect Gas law .(h)= , where M and R are con- 
RT (h) stants of values 28.93 · 10-3 kg/mol and 8.3145 J/mol · K respectively. The Gladstone-Dale law 
[GD58] relates n(d, h) as a function of both .(h) and n(d), given by the expression: c &#38;#169; The 
Eurographics Association 2005. D. Gutierrez, A. Munoz, O. Anson &#38; F. J. Seron / Non-Linear Volume 
Photon Mapping () b n(d)= a · 1 ++ 1 (18) d2 where a and b depend on the medium considered (for air, 
their values are a = 29.79 · 10-5 and b = 5.67 · 10-5). Sell­meier [BW02] provides a slightly more elaborated 
formula, but we have chosen Cauchy s for ef.ciency reasons. Combining equations 17 and 18 we .nally obtain 
our pro­ .le for n(d, h), which we can alter at will to obtain the de­sired effects. To interpolate the 
complete, altered pro.les for the whole 40 km. we use Fermi s distribution, as proposed in [VDWGL00]. 
 &#38;#169; The Eurographics Association 2005. The camera in the scenes is placed far from the mirages 
at a speci.c height for each effect to be seen (they can only appear if the observer s line of vision 
forms an angle less than one degree with the horizon). The error tolerance in the Dormand-Prince RK5(4)7M 
method has been set to 10-9 , and the spectrum has been sampled in three wavelengths. Figure 4 (top) 
shows our simulation of an inferior mirage, which occurs when the ground is very hot and heats up the 
air layers right above it, thus creating a steep temperature gradient (30. C in 20 meters). As a consequence, 
light rays get bent upwards, and an inverted image of the Happy Bud­dha and the background appears on 
the ground. The camera is placed 10 meters above the ground. The image took 14 minutes to render. Inversion 
layers are caused by an increase of air tem­perature with height, reversing the standard behavior where 
temperature decreases as a function of height. This happens most commonly above cold sea waters, and 
the light rays get bent downward, giving rise to the superior mirage. Figure 4 (middle) shows our simulation, 
modelling an inversion layer with a temperature gradient of 23. C. The apparent hole in the mountains 
is actually formed by the superior inverted image of the real mountains. The camera is placed also 10 
meters above the ground, and the image took four minutes and 32 seconds to render. The great decrease 
in rendering time compared to the inferior mirage is owed to the simpler geometry of the scene, since 
the far away mountains are tex­tured low-resolution objects. Maybe less known than the two previous examples, 
the Fata Morgana occurs as a concatenation of both superior and inferior mirages, and is a much rarer 
phenomenon. Figure 4 (bottom) shows our simulation with two inversion layers with steep temperature gradients. 
There is an inferior mirage image across the middle of the mountain plus a superior mi­rage with the 
inverted image on top. The shape of the moun­tain gets greatly distorted; the Fata Morgana has historically 
tricked arctic expeditions, making them believe they were seeing huge mountains that were just a complicated 
pattern of upright and inverted images of the real, much lower hill (Fata Morgana is in fact the name 
of a fairly enchantress skilled in the art of changing shape, which she learnt from Merlin the Magician). 
The camera is placed at 300 meters (for the Fata to be visible it needs to be between the inver­sion 
layers), and the rendering time was .ve minutes. 6. Discussion The method described has been implemented 
in Lucifer, our in-house global illumination renderer. It can handle multi­ple inelastic scattering in 
inhomogeneous participating me­dia with a varying index of refraction, thus rendering effects such as 
mirages or .uorescence in ocean waters with full lighting computation. It deals well with strong anisotropy 
in the phase functions and the effects of backscattering, since no discretizations of the scene must 
be performed, D. Gutierrez, A. Munoz, O. Anson &#38; F. J. Seron / Non-Linear Volume Photon Mapping 
and thus the shortcoming of the only previous work on vol­ume .uorescence [CS03] is overcome. It also 
supports real light sources, with photometric data input speci.ed in the standard CIBSE TM14 format [CIB88]. 
This is a must for predictive rendering and for generating physically accurate data. The real light sources 
are sampled so that photons are emitted proportionally to the distribution of the light, given by its 
photometry. Spectral images are calculated in high dynamic range, in order to obtain accurate data from 
the simulations. For tone reproduction purposes we map luminances to the display based on the work by 
Ward et al. [LRP97] and Pattanaik et al. [PTYG00]. To increase realism during the visualiza­ tion of 
the images, an additional operator has been added which simulates the effects of chromatic adaptation 
in the human eye. This operator is specially important in the real­istic depiction of underwater imagery, 
where the cones in the human eye might undergo a loss of spectral sensitivity after having been exposed 
to the same wavelength for a long pe­riod of time (underwater imagery being usually blue or green mostly). 
The complete description of such operator can be found in [GSMA04]. As stated in the introduction, the 
algorithm implemented is general and physically-based. This allows us to use the radiometric and photometric 
data obtained from the simula­tions for any purpose other than rendering, such as profes­sional architectural 
lighting or accurate simulations of deep underwater vision, given the exact description of the lumi­naire 
to be used and the water conditions. This accuracy ob­viously increases rendering times compared to faked, 
ad-hoc solutions. To improve ef.ciency, we impose an early light path termination and an adaptive integration 
step while solv­ing Fermat s law. Choosing the Dormand-Prince RK5(4)7M numerical method over the more 
standard Euler method has produced speedups of up to 106.4. We have also used a par­allel implementation 
on a six-PC Beowulf system of our non-linear photon mapping algorithm, achieving additional speedups 
between 4.2 and 4.8. The non-linear photon mapping implementation allows us to extend several sunset 
effects similar to the ones simulated in [GSMA04], by including a thin layer of fog between the observer 
and the sun. The solar disk gets distorted into dif­ferent shapes, while light is scattered through the 
layer of fog, thus achieving a "winter sunset" look (.gure 5, left and middle). Figure 5 right shows 
volume caustics generated by a crystal sphere in a .uorescent medium. Figure 6 shows several renders 
obtained with Lucifer. All  of them are lit by a Philips SW-type cluminaire, speci­.ed according to 
the CIBSE TM14 format. The only light source is immersed in the medium, so no caustics from the interaction 
of sunlight with the surface appear. The medium modelled does not emit light, although adding that to 
the model is straightforward and would allow us to simulate effects such as bioluminiscence in the water. 
Fluorescence Figure 5: Sunset effects through a layer of fog. Left: .attened sun. Middle: split sun. 
Right: Volume caustics in a .uores­cent medium. owed to inelastic scattering is computed according to 
the varying concentrations of chlorophyll in each image (be­tween 0.01and 0.1mg/m3). The volume photon 
map in all the images contains 500.000 photons, and the radiance esti­mate used 250. Again, these high 
numbers are needed since we compute direct lighting with the photon map. The top two images represent 
a sunken boat along a Happy Bud­dha in clear, shallow waters (left) or deep underwater with a chlorophyll 
concentration of 0.05mg/m3(right). For the bottom-left image, we have added a volume temperature .eld 
that simulates a heat source outside the image as ex­plained in [SGGC05], deriving the index of refraction 
us­ ing the formula n = 1 + TT o (no - 1) as proposed by Stam and Languenou [SL96]. The distortions caused 
by the vary­ ing index of refraction are visible, similar to the character­istic rippling in a real desert 
scene. The bottom-middle im­age uses a smoke-like medium, modelled as a 3D turbulence function, whereas 
the last to the right shows the effects of a highly anisotropic medium. The images are 400 pixels wide 
and took between 30 and 40 minutes to render, without any penalty imposed by the anisotropy in the last 
image. 7. Conclusion and Future Work We proposed a novel extension of the widely used photon mapping 
technique, which accounts for multiple inelastic scattering and can provide a full global illumination 
solution in inhomogeneous media with a varying index of refraction, where light paths are bent. No pre-lit 
textures are needed in this case, since both direct and indirect lighting is calculated from the photon 
map. The method is physically-based and yields accurate high-dynamic results that can either be out­put 
as an image to a display device (via tone mapping), or used in other .elds as raw data. Inelastic scattering 
is cal­culated during the photon tracing stage, so the extra cost re­quired is just a second Russian 
roulette per absorption. The accompanying video shows the feasibility of the approach for animations. 
Practically all inelastic scattering effects in the visible range of the spectrum mean a transfer of 
energy from shorter to longer wavelengths. Nevertheless, the algorithm presented in this work can handle 
rarer inelastic scattering events where energy gets transferred from longer to shorter wave­ c &#38;#169; 
The Eurographics Association 2005. D. Gutierrez, A. Munoz, O. Anson &#38; F. J. Seron / Non-Linear Volume 
Photon Mapping Figure 6: Different images with inelastic scattering in participating media. Top left: 
very low chlorophyll concentration. Top right: higher concentration yields more inelastic scattering 
events. Bottom left: distortions caused by a 3D temperature .eld. Bottom middle: 3D turbulence .eld simulating 
smoke. Bottom right: highly anisotropic medium. lengths (such as a fraction of the Raman scattering that 
oc­curs naturally in several solids, liquids and gases [Mob94]), since it does not follow a cascade, 
one-way scheme from the blue end to the red end of the spectrum. The application of these type of inelastic 
scattering to computer graphics is probably just marginal, but the data generated can be very useful 
to physicists or oceanographers. Adding phosphores­cence effects could make use of the work by Cammarano 
and Wann Jensen [CJ02], although a more straightforward approach would be to use the decay function d(t) 
in each frame. Any number of light sources can be used in the scene, even with different photometric 
descriptions. The bottleneck of the algorithm is solving the paths for each photon and eye-ray using 
Fermat s law. Although the use of a Dormand-Prince method has drastically reduced rendering times by 
two orders of magnitude, additional work needs to be done to achieve near real-time frame rates. Im­portance 
maps could be used for this purpose, although two other promising .elds of research lay ahead: the .rst 
one is the implementation of the algorithm on GPUs, as proposed by Purcell et al. [PDC. 03]. The second 
would try to take ad­ vantage of temporal coherence of light distribution, as pre­sented by Myszkowski 
et al. [MTAS01]. &#38;#169; The Eurographics Association 2005. 8. Acknowledgements This research was 
partly done under the sponsorship of the Spanish Ministry of Education and Research through the project 
TIN2004-07672-C03-03. The authors would also like to thank Eduardo Jiménez for his initial contribution 
to this work. References [BTL90] BERGER M., TROUT T., LEVIT N.: Ray tracing mirages. IEEE Computer Graphics 
and Applications 10, 3 (May 1990), 36 41. [BW02] BORN M., WOLF E.: Principles of Optics: Electromagnetic 
Theory of Propagation, Interference and Diffraction of Light. Cambridge University Press, 2002. [CIB88] 
CIBSE: Standard File Format for Transfer of Lu­minaire Photometric Data. The Chartered Institution of 
Building Services Engineers, 1988. [CJ02] CAMMARANO M., JENSEN H. W.: Time depen­dent photon mapping. 
In Proceedings of the 13th Eu­rographics workshop on Rendering (2002), Eurographics Association, pp. 
135 144. D. Gutierrez, A. Munoz, O. Anson &#38; F. J. Seron / Non-Linear Volume Photon Mapping [CS03] 
CEREZO E., SERON F.: Inelastic scattering in par­ticipating media. application to the ocean. In Proceedings 
of the Annual Conference of the European Association for Computer Graphics, Eurographics 2003 (2003), 
pp. CD ROM. [DP80] DORMAND J., PRINCE P.: A family of embeded runge-kutta formulae. Journal of Computational 
and Ap­plied Mathematics 6(1) (1980), 19 26. [FFLV82] FABRI E., FIORZO G., LAZZERI F., VIOLINO P.: Mirage 
in the laboratory. Am. J. Physics 50(6) (1982), 517 521. [GD58] GLADSTONE J. H., DALE J.: On the in.uence 
of temperature on the refraction of light. Phil. Trans. 148 (1858), 887. [Gla95a] GLASSNER A.: Principles 
of Digital Image Syn­thesis. Morgan Kaufmann, San Francisco, California, 1995. [Gla95b] GLASSNER A. S.: 
A model for .uorescence and phosphorescence. In Photorealistic Rendering Techniques (1995), Sakas P. 
S. G., Müller S., (Eds.), Eurographics, Springer-Verlag Berlin Heidelberg New York, pp. 60 70. [Grö95] 
GRÖLLER E.: Nonlinear ray tracing: visualizing strange worlds. The Visual Computer 11, 5 (1995), 263 
276. [GSMA04] GUTIERREZ D., SERON F., MUNOZ A., AN-SON O.: Chasing the green .ash: a global illumination 
solution for inhomogeneous media. In Spring Confer­ ence on Computer Graphics (2004), (in cooperation 
with ACM SIGGRAPH A. P., Eurographics), (Eds.), pp. 95 103. [Haw92] HAWES S.: Quantum Fluorescence Ef.ciencies 
of Marine Fulvic and Humid Acids. PhD thesis, Dept. of Marince Science, Univ. of South Florida, 1992. 
[HW01] HANSON A. J., WEISKOPF D.: Visualizing rel­ativity. siggraph 2001 course 15, 2001. [JC98] JENSEN 
H. W., CHRISTENSEN P. H.: Ef.cient simulation of light transport in scenes with participating media using 
photon maps. In SIGGRAPH 98 Confer­ence Proceedings (jul 1998), Cohen M., (Ed.), Annual Conference Series, 
ACM SIGGRAPH, Addison Wesley, pp. 311 320. ISBN 0-89791-999-8. [LRP97] LARSON G. W., RUSHMEIER H., PIATKO 
C.: A visibility matching tone reproduction operator for high dynamic range scenes. IEEE Transactions 
on Visualiza­tion and Computer Graphics 3, 4 (Oct. 1997), 291 306. [Mob94] MOBLEY C.: Light and Water. 
Radiative Trans­fer in Natural Waters. Academic Press, Inc., 1994. [MTAS01] MYSZKOWKSI K., TAWARA T., 
AKAMINE A., SEIDEL H. P.: Perception-guided global illumination solution for animation. In Computer Graphics 
Proceed­ ings, Annual Conference Series, 2001 (ACM SIGGRAPH 2001 Proceedings) (Aug. 2001), pp. 221 230. 
[Mus90] MUSGRAVE F. K.: A note on ray tracing mi­rages. IEEE Computer Graphics and Applications 10,6 
(Nov. 1990), 10 12. [Nem93] NEMIROFF R. J.: Visual distortions near a neu­tron star and black hole. American 
Journal of Physics 61(7) (1993), 619 632. [PDC. 03] PURCELL T. J., DONNER C., CAMMARANO M., JENSEN J., 
HANRAHAN P.: Photon map­ping on programmable graphics hardware. In SIG­GRAPH/Eurographics Workshop on 
Graphics Hardware (2003), Eurographics Association, pp. 041 050. [PPS97] PEREZ F., PUEYO X., SILLION 
F.: Global il­lumination techniques for the simulation of participating media. In Proc. of the Eigth 
Eurographics Workshop on Rendering (1997), pp. 16 18. [PTYG00] PATTANAIK S., TUMBLIN J. E., YEE H., GREENBERG. 
D. P.: Time-dependent visual adaptation for realistic image display. In SIGGRAPH 2000, Com­puter Graphics 
Proceedings (2000), Akeley K., (Ed.), An­nual Conference Series, ACM Press / ACM SIGGRAPH / Addison Wesley 
Longman, pp. 47 54. [SGGC05] SERON F., GUTIERREZ D., GUTIERREZ G., CEREZO E.: Implementation of a method 
of curved ray tracing for inhomogeneous atmospheres. Computers and Graphics 29(1) (2005). [SL96] STAM 
J., LANGUÉNOU E.: Ray tracing in non-constant media. In Eurographics Rendering Work­shop 1996 (New York 
City, NY, June 1996), Pueyo X., Schröder P., (Eds.), Eurographics, Springer Wien, pp. 225 234. ISBN 3-211-82883-4. 
[USG76] USGPC: U.S. Standard Atmosphere. United State Government Printing Of.ce, Washington, D.C., 1976. 
[VDWGL00] VAN DER WERF S., GUNTHER G., LEHN W.: Novaya zemlya effects and sunsets. Applied Optics 42, 
3 (2000). [WTP01] WILKIE A., TOBLER R., PURGATHOFER W.: Combined rendering of polarization and .uorescence 
ef­fects. In Rendering Techniques 01 (Proc. Eurograph­ics Workshop on Rendering 2001) (2001), Gortler 
S.J. M. K. e., (Ed.), Eurographics, Springer-Verlag, pp. 197 204. [YOH00] YNGVE G. D., O BRIEN J.F., 
HODGINS H.: Animating explosions. In Proceedings of the Com­puter Graphics Conference 2000 (SIGGRAPH-00) 
(New York, July 23 28 2000), Hoffmeyer S., (Ed.), ACMPress, pp. 29 36. c &#38;#169; The Eurographics 
Association 2005. D. Gutierrez, A. Munoz, O. Anson &#38; F. J. Seron / Non-Linear Volume Photon Mapping 
 &#38;#169; The Eurographics Association 2005. EUROGRAPHICS2008/G.Drettakis andR.Scopigno Volume27 (2008),Number2 
(GuestEditors) Visualizing Underwater Ocean Optics DiegoGutierrez,FranciscoJ.Seron,AdolfoMunoz andOscarAnson 
UniversityofZaragoza,Spain Abstract Simulatingthein-waterocean light.eldisadaunting task.Oceanwatersareoneoftherichestparticipating 
me­dia,where lightinteractsnot onlywithwatermolecules,but withsuspendedparticlesand organicmatteraswell. 
The concentration of each constituent greatly affects these interactions, resulting in very different 
hues. Inelastic scattering eventssuch as .uorescenceorRamanscatteringimplyenergytransfers that areusuallyneglectedin 
the simulations.Ourcontributions in thispaperareabio-optical model of oceanwaterssuitable forcomputergraph­ics 
simulations, along with an improved method to obtain an accurate solution of the in-water light .eld 
based on radiative transfer theory. The method provides a link between the inherent optical properties 
that de.ne the medium and its apparent opticalproperties, which describe how it looks. The bio-optical 
model of the ocean uses published data from oceanography studies. For inelastic scattering we compute 
all frequency changes at higher and lower energy values, based on the spectral quantum ef.ciency function 
of the medium. The results shown prove the usability of the system as a predictive rendering algorithm. 
Areas of application for this research span from underwater imagery to remote sensing; the resolution 
method is general enough to be usable in any type of participating medium simulation. CategoriesandSubjectDescriptors 
(accordingtoACMCCS): I.3.7[ComputerGraphics]:Three-Dimensional Graphics andRealism 1. Introduction Oceanwater 
isarguably therichestparticipatingmediumin terms of opticalthickness andthe number andtype ofinter­actionsthat 
occurin it.Thispaperdealswiththephysically­basedrendering of underwaterscenesby simulating the in­water 
light .eld,based onacompactbio-optical modelthat takes into account the dissolved and particulate matter, 
op­tically in.uential constituentsofthewater.Toensureaccu­racy, we usepublisheddata obtainedfrom a wide 
range of literature in the .eldof oceanography.Ourmodelisnot re­stricted to just the visible spectrum 
and can be adapted to anytypeofknownoceanwater inparticular,or toanykind ofparticipating medium ingeneral. 
Scattering in water is caused by interactions of light at molecular level and withparticles[Mob94].Itcanbeclas­si.ed 
in two broad categories: elastic or inelastic scatter­ing, depending on whether the scattered photon 
maintains orchanges itsenergyintheprocess.Theinelasticscattering eventscanbefurthersubclassi.edaccordingtothenatureof 
theenergytransfer: Stokesscattering,whenamoleculeofthe mediumabsorbs thephotonand re-emits itwitha loweren­ 
&#38;#169;2007TheAuthor(s) Journalcompilation c &#38;#169;2007TheEurographicsAssociationandBlackwellPublishingLtd. 
PublishedbyBlackwellPublishing,9600GarsingtonRoad,OxfordOX42DQ,UKand350 MainStreet,Malden,MA02148,USA. 
ergy, and anti-Stokes scattering, when the re-emittedphoton has ahigher energy.Both cases are coveredby 
our model. Theprocessimpliesanenergytransferfromwavelength . ' to ., with . ' being the excitation wavelength 
and . the re­emittedwavelength.Theformercaseimpliesashifttowards longerwavelengths,whereas in the latter 
thescatteredpho­tonhas a shorter wavelength.Majorforms of elastic events inwaterincludeEinstein-Smoluchowskiscattering(seeSec­tion 
3.2),whereasfor inelastic events,Raman scattering and .uorescence arethe twomostprominent(seeSection 
3.3). Thepresenceandconcentrationsoftheconstituents in the waterdetermine itsopticalproperties.Theseopticalproper­tiesaredividedin 
twoclasses: inherent and apparent.The inherent optical properties (IOP)only depend on the con­stituents 
of the water, whereas the apparent optical prop­erties (AOP) are not properties of the aquatic medium 
it­self, although they do depend on its characteristics. Typi­cal IOP are the absorption coef.cient, 
the scattering coef­.cient or the scattering phase function. Some of the AOP include irradiance re.ectance, 
attenuationcoef.cients or the averagecosines[Pre76].Toobtain the in-water light .eld, DiegoGutierrez 
et al./VisualizingUnderwaterOceanOptics we rely on the physically based theory of radiative trans­fer 
[Cha60],whichrelates theIOPandAOP.Moreprecisely, the linkisprovidedthroughtheRadiativeTransferEquation 
(RTE)[SCP94],whichtakes intoaccountemission,absorp­tion and elastic scattering. Unfortunately this equation 
can not account for the phenomenon known as inelastic scat­tering described previously, which is of signi.cant 
impor­tance in ocean waters. We consequently expand the RTE by adding an extra term, thus obtaining the 
Full Radiative TransferEquation(FRTE)[Gla95]and solving itby using anextended versionofthe methodpresentedbyGutierrez 
et al.[GMAS05]: .L(.,..o) .x = a(.)Le(.,..o) -.(.)L(.,..o) + s(.) p(.,..i,..o) L(.,..i) d..i O . . 
 + s(. ' , .) p(. ' , .,..i,..o) L(. ' ,..i) d. ' d..i (1) O W where L is the radiance and ..i and..o 
are, respectively, the incoming and outgoingdirectionsofthat radiance. a, s and . are the absorption, 
scattering and extinction coef.cients respectively.We assume Le(.,..o) tobezero, thusmaking themediumnon-emissive.Notethatthelasttermmodelsthe 
inelasticscattering eventsandisexpressed asadouble inte­gral over thedomainsof thesolidangle O and wavelength 
W. Here p(. ' , .,..i,..o) is the phase function for inelastic events and s(. ' , .) is the inelastic 
scattering function for the energy exchangebetween . ' and ..For simplicity,when consideringelastic interactions(. 
= . ' )parameters., . ' are simpli.edtoasingleparameter ..Forprocessessuch as .u­orescence,wherethephotonsareinelasticscatteredto 
longer wavelengths, thefunction s(. ' , .) is usually expressed as: s(. ' , .)= a(. ') f(. ' , .) (2) 
 where a(. ' ) is the inelastic absorption coef.cient and f(. ' , .) is the wavelength redistribution 
function, which governs theef.ciencyoftheenergytransferbetweenwave­lengths.Itisde.ned as theprobability 
of aphoton of . ' that inelasticallyscattersbeingre-emittedat ..Therefore,(2)ex­pressesthe inelasticscatteringasapercentageoftheinelas­tic 
absorption coef.cient.Section 3.3gives moredetails on how tomodelthisredistributionfunction f(. ' , .). 
Our researchon water simulation encompasses the .elds ofbothcomputergraphicsand oceanography,andit isfree 
from therestrictionsofpreviousworks.Themaincontribu­tions ofthispaper are: Acompact,parameterizedbio-opticalmodelofoceanwa­terswhich 
canbeusedin computergraphicsapplications.  A resolution method based on the theory of radiative transfer,which 
solves theFRTEbyhandling all kinds of inelastic scattering events and modelingboth absorption and elastic 
scattering accurately.This methodisbased on photon mapping[Jen01].  A link between the IOP of water 
and the resulting light .eld, which in turn de.nes its AOP, based on radiative transfer theory. Theremainderofthispaper 
isorganizedasfollows:Sec­tion 2 presents previous work on the simulation of light transportin waterbodies.InSection 
3a comprehensivebio­optical model is developed, whilst section 4 presents our simulationmethod.Thepaperends 
with theresults andcon­clusions. 2. Related work The simulation of light transport in participating media 
usually either relies on Monte-Carlo techniques for ray tracing(Rushmeier andTorrance[RT87];Nakamae et 
al. [NKON90];TadamuraandNakamae[TN95])orattempts to solve theRTE,such as themethodproposedbyKanedaet 
al.[KYNN91].Nishita etal.[NSTN93]displaywaterfrom outerspacemodifyingthismethod,butbothworksonly take 
intoaccount singlescattering.In thework ofPremozeand Ashikhmin[PA01],noradiancedue toscattering iscalcu­latedatall,usingempiricalequationsbasedonexperimental 
data instead.Mobley[Mob94]developedamethodtosolve theRTE analytically,butitcannotbeextended to takeinto 
account inelastic scattering. Recently, the Lorenz-Mie the­ory has been generalized and applied to rendering 
natural watersbyFrisvad,Christensen andJensen[FCJ07], also ne­glectingthe effects ofinelastic scattering.Cerezo 
andSeron [CS04]alsodevelop abio-optical model.Whilst thegoal of theirworkiscloselyrelated toours,weovercomeheresig­ni.cant 
shortcomings: They use a discrete ordinate method, which requires an angularand spatialdiscretizationofthevolume 
toberen­dered.This imposeshighmemoryrequirementswhichse­riously limitthecomplexityofthescenesthat canbere­produced. 
 In their work, inelastic scatteringsimulations are limited to .xed re-emissions in the680 nm. wavelength.. 
 Theycannotprovideafullsolution tothe light transport problem. Gutierrezetal.[GMAS05]presentamethod 
thatdeals withparticipating media inwhichthe indexof refraction is not homogeneous, while also taking 
into account the sim­ulationof some inelasticscattering events.They apply their methodto thesimulationofunderwaterimageryusingasim­pli.ed,four-parametermodelofoceanwaters.Inthisregard, 
ourpaper offers improvementin thefollowing ways:  Ourbio-opticalmodel of oceanwaters ismorecomplete, 
thus makingthe simulations more accurate.  They also fail to develop a complete description for the 
complex inelasticscatteringevents that occur underwater, andthe methodis limitedto re-emissions atlower 
energy levelsand at .xed wavelengths.In thispaper all inelastic scattering events can be modeled, including 
Anti Stokes scattering events like Raman scattering (seeSection3.3).  c &#38;#169;2007TheAuthor(s) Journalcompilation 
c &#38;#169;2007TheEurographicsAssociationandBlackwellPublishingLtd. DiegoGutierrez et al./VisualizingUnderwaterOceanOptics 
Weadditionallyoffersimulationsusingrealdatafromdif­ferentseas as a means of visual validation. 3. The 
Bio-Optical Model Thevariousconstituentsof oceanwaterhaveagreat in.u­encein itsopticalproperties.Inordertosolvetheforward 
problem in ocean optics, the IOP have to be modeled and usedin theFRTE.ThevaluesoftheseIOPcanbeobtainedas 
the sum ofthe contributions ofpure water andthedissolved particlesandparticulatematterpresent in thewater,aspro­posedin[Mob94].Opticallypurewater 
isdevoidofanydis­solved or suspended matter, and thus there is no scattering orabsorptionowedtoparticlesororganicmaterial[Mor74]. 
Forsaline purewaterthesaltconcentration(35 to39parts per thousand) does in.uence the scattering and absorption 
functions.Inparticular itabsorbsmost wavelengthsexcept forblue,with theabsorptioncoef.cientpeakingat760 
nm, and reaching a minimum at430 nm. Wedevelop ourbio-optical modelfrom threemainIOP, withothers liketheextinctioncoef.cient 
orthealbedode­rivedfrom those three.TheseIOP are theabsorptioncoef.­cient(3),thescatteringcoef.cient(4)andthephasefunction 
(5),whichfor theelasticcasecanbewrittenas(seeTable4 foramoredetaileddescriptionofthefunctionsused, includ­ingboth 
the elastic andinelastic cases): a(.)= aw(.)+ .ai(.) (3) i s(.)= sw(.)+ .si(.) (4) i sw(.) si(.) p(., 
.)= pw(., .)+ . pi(., .) (5) s(.) s(.) i where . is theanglebetween the incoming ..i and outgoing ..o 
directions, the subscriptw standsfor thecontributionof thepurewater(fresh orsalty) andthesubscript i 
standsfor theconstituents in thewaterbodysuchasbiologicalparticles ordissolvedsubstances.We include three 
typesofsuchcon­stituents inourmodel,namelyCDOM(ColoredDissolved OrganicMatter, alsoknow asyellow matter,presentmainly 
inshallowoceanwatersandharbors),phytoplankton(micro­scopicplantsrichinchlorophyll)and mineralsand organic 
detritus.Therest of thissectionwillcharacterize the three mainIOP(withelasticandinelasticscattering treated 
sep­arately)forpurewaterand the threeconstituents.Thenext sectionwillshowhowradiative transfertheoryisappliedto 
simulate the light .eld(whichde.ne theAOP) and render the .nalimages. 3.1. Modeling Absorption For the 
spectral absorption function of pure water aw(.) werelyon thework ofSmithandBaker[SB81],whose tabulated 
values are well known in oceanography studies (shown inTable1).FollowingfurtherstudiesbyPopeand &#38;#169;2007TheAuthor(s) 
Journalcompilation c&#38;#169;2007TheEurographicsAssociationandBlackwellPublishingLtd. Fry[PF97],weusethosevaluesasanupperbound, 
toac­count for the fact that the true absorption can be, in fact, lower.Thefunctionshows thatabsorption 
ismoreprominent both in theUV and red endsof thespectrum.[PF97]also shows that absorptionby salt inoceanicwater 
isnegligible. Based on thedatabyBricaud,Morel andPrieur[BMP81], we model absorption by CDOM by .tting 
an exponential curve oftheform: -Sy(.-.0) ay(.)= ay(.0)e (6) where thesubscript ydenotes theconstituentCDOM..0 
isa referencewavelength,oftenchosen tobe440 nm foryellow matter, and Sy is the slope of the semilogarithmic 
absorp­tion curve[Kir94].Sy isusually taken tobeconstant,with a value of0.014nm -1,buthasbeenfoundtovarybothge­ographically 
and temporally, and is also dependent on the wavelengthrangeoverwhichit iscalculated[BMP81].The values 
of absorption ay(.0) at reference wavelengths also varyinarangebetween0.01m -1 to20 m -1, as afunction 
ofturbidity[Kir94]. Phytoplankton absorbs a great amount of visible light, due to itschlorophyllpigment.Theabsorptionfunctionfor 
chlorophyll peaks strongly at 430 nm and 670 nm, being very weakin the mid range ofthe visible spectrum(thus 
the morephytoplankton thegreener thehueof thewater).The concentration ofthe chlorophyll in the water 
usually ranges from0.01mg/m 3foropenwaters to100mg/m 3.The spec­tralabsorptioncoef.cientofthephytoplanktonisusuallyex­pressedas 
afunction ofthis concentrationC as: ap(.)= Ca * p(.) (7) where C can be de.ned as the concentration of 
the main pigment chlorophyll-a (Chla)or as the sum of the concen­trations of Chla and itsdegradationproducts, 
thepheopig­ments. a* p is the speci.c spectral absorption coef.cient (the absorptionper unit of concentration)for 
aparticular species of phytoplankton, given in m 2/mg.Typical valuesfor spe­ci.c absorptions of different 
species of phytoplankton can befoundin theworkofSathyendranath,LazzaraandPrieur [SLP87](seeTable 1).A 
rough correspondencebetween chlorophyll concentrations and several oceanic water types isgivenbyMorel[Mor88].Theabsorptionowedtoorganic 
detritusandmineralscanbeapproximatedbyanexponential function,accordingtoRoesler,PerryandCarder[RPC89]: 
-Sd(.-.0) ad(.)= ad(.0)e (8) Here the referencewavelength400nm isselectedfor .0 and typical valuesfor 
theexponent coef.cient Sd willbe in the rangebetween0.006nm -1 to0.014nm -1, although0.011 nm -1 isthemost 
commonvalue[RPC89].Furtherstudies con.rm that theabsorptionspectraof mineralsanddetritus iswelldescribedbyanexponentialfunction 
withanaverage slope Sd of 0.0123 nm -1, with slightly lower values than predictedat wavelengthsbelow440 
nm [BSF*03]. DiegoGutierrez et al./VisualizingUnderwaterOceanOptics Table 1: Absorption coef.cient for 
a clear water body aw(afterSmith andBaker[SB81])and speci.c absorption coef.cient forphytoplankton a* 
p(afterSathyendranath,Lazzara andPrieur[SLP87]). . [nm] 380 440 500 550 610 670 720 780 aw [cm -1] 
0.00022 0.000145 0.000257 0.000638 0.00289 0.0043 0.01169 0.0236 2 a* [m · mg -1] 0.025 0.035 0.02 0.01 
0.007 0.015 0.001 0.0001 p 3.2. Modeling Elastic Scattering For thepurewater termweuse thevolumescatteringfunc-tionde.nedbytheEinstein-Smoluchowskitheory[Maz02], 
which models scattering at molecular level as small-scale .uctuations.Whilst usuallyRayleigh s scatteringis 
usedin­stead, Einstein-Smoluchowski provides more accurate re­sults,iswellde.ned andimposesnooverheadsin 
thesimu­lations.Itsscatteringcoef.cientandphasefunctionaregiven by: 4.32 .0 sw(.)=16.06ßw(.0, 90.)(9) 
. () pw(.)=0.062251+ 0.835cos2.(10) Typical values for ßw(.0, 90.) for both fresh and saline purewateraregiven 
in[Mor74].Thesevaluesrangefrom 14.1· 10-4 m -1 to134.5· 10-4 m -1.All thescatteringpro­duced by CDOM 
has inelastic nature and thus will be de­scribedin next section. Gordon and Morel [GM83] found that phytoplankton, 
eveninsmallconcentrations,alsocontribute to thetotalelas­tic scatteringin the water.Its contribution 
isgivenby: 550 0.62 sp(.)=0.30C(11) . where theconstant0.30isselected to.tthedatacollected frommanytypesofwaters.Theactual 
upperboundfor this constant has a value of 0.45[GM83]. The phase function due tophytoplankton isgivenbyan 
isotropicfunction(pp = 1/p). The elastic scatteringcausedby organicdetritus and min­eralscanbemodeledbased 
onMie theory[GSO03].The Henyey-Greenstein phasefunction modelsforward scatter­ingfairlywellbutfails toreproducebackscattering 
withthe same precision. We found that we can achieve a better .t by using a Two-Terms Henyey-Greenstein 
phase function (TTHG)[HG41]: pd(., ., gf, gb)= . pHG(., gf)+(1- .) pHG(., gb) (12) where . is a weightingfunctionbetween 
zero and one.This commonwayofutilizingthiscombinationde.nesaforward scatteringlobe(.rst term),plusabackscatteringlobe(sec­ondterm),with 
gf .[0..1] and gb .[-1..0]. pHG represents a simpleHenyey-Greensteinphasefunction(HG): 1-g 2 pHG(., g)= 
(13) (1+ g2-2gcos.)3/2 TheTTHGfunctionnotonly modelsbackscatteringmore precisely,but itcandescribemorecomplexparticlescatter­ing 
models, improving the .t at large and small angles as well.Theshapeof each ofthe twoHGfunctionscanbeap­proximatedbyanellipsoid,avoidingtherelativelyexpensive 
exponent in its evaluation. The observation was .rst intro­ducedbySchlick[BLSS93].Due tothegreatvarietyofpar­ticulatematter, 
thescatteringcoef.cient sd canadoptawide rangeofvalues.Table 2showstypicalvaluesofthisfunction (data 
afterStramskiet al.[SBM01]). 3.3. Modeling Inelastic Scattering Forinelasticscattering,weneedtomodelthepossibilityof 
an absorbed photon being re-emitted at a different wave­length.(2) includes a term f(. ' , .) known as 
wavelength redistribution function, which represents the ef.ciency of the energy transfer between wavelengths. 
It is de.ned as thequotientbetween theenergy of theemitted wavelength and the energy of the absorbed 
excitation wavelength, per wavelengthunit.Reformulatingin termsofphotons instead of energy wehave the 
spectral quantum ef.ciency function .(. ' , .),de.ned as the ratiobetween thenumber ofphotons emittedat. 
perwavelengthunit,andthenumberofabsorbed photons at . ' .Bothfunctions aredimensional(nm -1), and are 
related asfollows: . ' f(. ' , .)= .(. ' , .) (14) . The wavelength redistribution function f, and therefore 
its associated spectralquantum ef.ciencyfunction ., canbe seen as a re-radiation matrix.A relateddimensionlessfunc­tion 
thatdescribes inelasticscattering is the quantum yield G(. ' ),de.ned asthe total numberofphotonsemitted 
at all wavelengthsdividedby thenumberofphotonsabsorbed at excitationwavelength. ' .Itisrelatedto thespectralquantum 
ef.ciencyfunctionby: G(. ' )=.(. ' , .) d. (15) W The threefunctions G(. ' ), f(. ' , .) and .(. ' , 
.),depend onboththemediumandthetypeofinelasticevent.The two inelasticeventswithmorein.uencein thein-waterlight.eld 
are .uorescence and Raman scattering. Phytoplankton and CDOM are important .uorescence sources, whilst 
Raman scatteringisproducedbypurewater; mineralsanddetritus, on the otherhand,do notproduce anyinelastic 
event. c &#38;#169;2007TheAuthor(s) Journalcompilation c &#38;#169;2007TheEurographicsAssociationandBlackwellPublishingLtd. 
 DiegoGutierrez et al./VisualizingUnderwaterOceanOptics Table 2: Scattering coef.cient fordetritus sdt 
and minerals sm(AfterStramskiet al.[SBM01]).  . [nm] detritus sdt [m -1] minerals sm [m -1] total sd 
[m -1] 380 440 500 550 610 670 720 780 0.045 0.0375 0.0325 0.03 0.0285 0.0275 0.027 0.027 0.0675 0.0525 
0.05 0.045 0.04 0.036 0.034 0.032 0.1125 0.09 0.0825 0.075 0.0685 0.0635 0.061 0.059 3.3.1. Fluorescence 
Fluorescence occurs when a molecule absorbs a photon of wavelength . ' , and re-emits it at a longer 
wavelength . ac­cordingto the .uorescence ef.ciencyfunction .F(. ' , .).For the two main sources of .uorescence(phytoplankton 
and CDOM), re-emission follows an isotropic phase function. For phytoplankton, the wavelength of the 
re-emitted pho­tons is independent of the excitation wavelength, although the intensitydoes show wavelengthdependency[Mob94]. 
Itisverycommoninoceanwaterstoseeacolorshiftrang­ing from greenish to very bright green, or even yellowish. 
Thesehueshiftsaremainlydue tothevariationinthecon­centration and type of the suspended microorganisms, 
spe­cially phytoplankton and its related chlorophyll concentra­tion, whichpresents an absorptionfunctionpeaking 
at350 nm and rapidly decaying to almost zero beyond 500 nm. Only wavelengthsbetween370and690 nm can trigger 
.u­orescencedue tophytoplankton.Thiscanbemodeled asa dimensionlessfunctiongp(. ' ) so that: 1 if370 = 
. ' = 690nm gp(. ' ) = (16) 0 otherwise The wavelength-independent quantum yield for phyto­plankton 
Gp(. ' ) ranges from 0.01 to0.1.Using(14) and (16), therelationshipbetween thewavelengthredistribution 
function fp(. ' , .) andthespectralquantumef.ciencyfunc­tion .p(. ' , .) is: . ' . ' fp(. ' , .)= .p(. 
' , .) = Gpgp(. ' )hp(.) (17) .. where hp(.) is the .uorescence emission function per unit wavelength, 
and can be approximated by a gaussian [Mob94]: hp(.) = 1 v 2p .s exp . -(. -.0)2 2(.s)2 . (18) .0 = 
685nm is the wavelength of maximum emission and .s = 10.6nm represents thestandarddeviation.Using(7) 
and(17)we can now compute the inelastic scattering coef.­cient owedtophytoplankton sp(. ' , .) following(2). 
The other important source of .uorescence in water is CDOM. For relatively high concentrations of CDOM, 
its quantumyieldGy(. ' ) variesbetween0.005and0.025.Fol­lowing thework ofHawes[Haw92]weuse thefollowing 
formula to describe its spectral .uorescence quantum ef.­ciencyfunction: &#38;#169;2007TheAuthor(s) Journalcompilation 
c&#38;#169;2007TheEurographicsAssociationandBlackwellPublishingLtd. Table 3: Water constituents and interactions 
Constituent Absorption Elastic Scat. Inelastic Scat. Pure water (w) Yes Yes Raman Scattering Minerals, 
detritus (d) Yes Yes No Phytoplankton (p) Yes Yes Fluorescence CDOM(y) Yes No Fluorescence .. .. 2 . 
1 - A1 . ..' - B1 . ' (. ' .(). . A2 . . fy, .)= A0(. ')exp - (19) 0.6.' + B2 where A0, A1, A2, B1 and 
B2 areempiricalparameterswhose valuesdependonthespeci.ccompositionoftheCDOMand canbefound in[Mob94](seeTable5).A1 
and A2 are di­mensionless, whereas the rest are given in nm -1.Like .u­orescencedue tophytoplankton,wecanuse(6)and(19)to 
compute the inelastic scatteringcoef.cient sy(. ' , .) follow­ing(2). Our model can be easily extended 
to account for phos­phorescence phenomena, which are intrinsically similar to .uorescence and are governed 
by the phosphorescence ef­.ciency function.Theonlydifference is that there-emitted energydeclineswith 
timeaccordingtoafunction d(t). 3.3.2. Raman scattering Raman scattering in.uences the in-water light 
.eld, spe­ciallyatgreatdepthswheresun irradiancebecomeszeroand onlyRaman radiance remains.It occurs when 
vibration and rotation in water molecules exchange energy with incom­ingphotons,re-emittingthemwithapproximately 
thesame wavelength,but allowingforsmallshifts towardslongeror shorterwavelengths.Itcanalsobeconsideredaspontaneous 
process.To isolateRaman inelasticeventsfrom.uorescence and otherscattering events, it isusuallystudiedinpurewa­ter,.ltered 
severaltimes,sothatthesecond termin(4)be­comes zero. TheRaman wavelength redistribution function fw(. 
' , .) isusuallydescribedin termsofasumoffourGaussianfunc-tions[Mob94]: . .().2 . 1 107-1 -. i .41 .' 
. 107 j=1Ai . exp - 2 .i .. i fw(. ' , .)= . (20) .'2 p 4ln2 .4 j=1Aj where . isthewavenumber(. = 107/.)givenincm 
-1.Typ­ical parameter values Ai, . i and .. i for the Raman redis­tributionfunction aregivenbyWalrafen[Wal69] 
and are DiegoGutierrez et al./VisualizingUnderwaterOceanOptics shown in Table 5. The inelastic scattering 
coef.cient can nowbe obtained using aw and fw in(2). 4. The simulation method Havingsofardeveloped ourbio-optical 
model, we can now formalizeitintoaset ofparametersand equations tofully simulate the in-waterlight.eld.Tosummarize, 
thefourcon­stituents of the model and their interactions with light are given inTable3.Table 4showshow 
the mainfunctions that de.ne the modelarederivedfromIOP and relatedfunctions at constituentlevel. Table 
4: The main functions of the model Equations a(.)= ad(.)+ap(.)+ aw(.)+ ay(.) s(.)= sw(.)+ sd(.)+ sp(.) 
sw(.)pw(.,.)+sd(.)pd(.,.)+sp(.)pp(.,.) p(., .)= s(.) .(.)= a(.)+ s(.) aI(. ' )= ap(. ')+ aw(. ')+ ay(. 
') ap(. ' )pp(. ' ,.,.)+aw(. ' )pw(. ' ,.,.)+ay(. ' )py(. ' ,.,.) pI(. ' , ., .)= aI(.' ) ap(. ' ) fp(. 
' ,.)+aw(. ' ) fw(. ' ,.)+ay(. ' ) fy(. ' ,.) fI(. ' , .)= aI(.' ) Table 5: Parameters of the model Parameter 
Equations Simulatedvalues Units C ad(400) (7)(11) (8) [0..1.0] [0..0.1] mg m3 m-1 ay(440) (6) [0..0.1] 
m-1 Sy (6) 0.014 nm -1 Sd (8) 0.011 nm -1 A0 (19) 150 700 nm -1 A1 (19) 4 - A2 (19) 4 - B0 B1 (19) (19) 
1 450·10-7 1 650·10-7 nm-1 nm -1 Gp (17) 0.1 - Gy (19) 0.025 - Ai, i= 1..4 (20) 0.41, 0.39, 0.10, 0.10 
- ..i, i= 1..4 (20) 3250, 3425, 3530, 3625 - ...i, i = 1..4 (20) 210, 175, 140, 140 -  The model allows 
for easy adjusting of its parameters to simulate different types of water and thus obtain differ­ent 
in-water light .elds. As well as minerals and detritus, other particulate components of water can be 
added from oceanographic studies(although minerals anddetritushave thegreatest in.uence in the .nal appearanceof 
water).Mie theory can again be used to model the scattering by these newparticles,and thephasefunctioncanbeapproximated 
by using a Two Terms Henyey-Greenstein phase function (12).Anoverviewofthemostsigni.cantparametersofthe 
model, theequations inwhichtheycanbefoundandthecor­responding valuesusedfor thesimulations inthispapercan 
befoundinTable5.Note thatfor simplicity wehave notin­cluded the values that are already speci.ed throughout 
the textduring theexplanationof thebio-optical model(more speci.cally,thoseincludedin tables 1and 2).The.rstthree 
correspondtotheparametersanalyzedinFigure 2. Once we have formalized the model into a set of equa­tions, 
we rely on radiative transfer theory to obtain a solu­tion for the in-water light .eld. We solve the 
Full Radia­tiveTransferEquation(1)byextendingthe traditionalpho­tonmappingalgorithm[Jen01]bytakingintoaccountall 
ten differenteventsspeci.edinTable 3, while allowingforboth Stokes or anti-Stokes inelastic scattering.This 
enhancement isdoneinbothstages:photon tracing and radianceestima­tion. During the photon tracing stage 
in the original photon mapping method[Jen01],aRussianroulette algorithm is triggered at each interaction 
with the medium, deciding whether thephoton isscattered orabsorbed.In[GMAS05] the authors add a second 
Russian roulette which separates absorptionfrom inelastic scattering;in the latter case, a new photon 
is generated at a different wavelength, but the al­gorithm considers just a single type of inelastic 
event with Stokes behavior. No anti-Stokes events are simulated. In contrast, our method uses just a 
single Russian roulette to choosebetween tendifferentkindsof interactions(includ­ing three types of inelastic 
events where the photons may gainorloseenergy),and canbeeasilyextended tohandle anarbitrarynumberofdifferentinteractions.Finally,we 
im­prove the radiance estimation stage over previous methods byadding a term to takeintoaccountthecontributionsfrom 
the inelasticscattering events.Thenext subsectionspresent the algorithm in moredetail. 4.1. Stage 1: 
Photon tracing Weshootphotonsfromthelightsourcesandlettheminteract withthegeometry andthemediumaccording 
to itsoptical distance, whichis afunction ofthe extinction coef.cient(as in theoriginalphoton mapping 
method).Westatisticallyde­cideat eachinteractionwhichtypeof event occurs(refer to Table3)withjust asingleRussianroulette.At 
the interac­tions,photonsarestoredinakd-treeasin traditionalphoton mapping. ThewavelengthspectrumisboxsampledintoN. 
samples, so absorption(a(.))and scattering coef.cients(s(.))are implementedas N.-dimensionalarrayswhilewavelengthre­distributionfunctions(f(. 
' , .))are implementedasN. ×N. square matrices. Each of the photons carries information aboutaportionof.ux(.F)atacertainsampledwavelength 
(. ' ).Importancesamplingisusedforcomputingtheoptical distance, so .F does not change along the photon 
tracing stage, while . ' changesfor inelastic scattering events. Inorder toapplytheRussianroulettealgorithm,wewill 
de.ne an albedo. j(.) for eachinteraction jasfollows: Ifinteraction jrepresents an elastic scattering 
event, then . j(.)= s.j((..)) If j represents an absorption interaction that does not c &#38;#169;2007TheAuthor(s) 
Journalcompilation c &#38;#169;2007TheEurographicsAssociationandBlackwellPublishingLtd. DiegoGutierrez 
et al./VisualizingUnderwaterOceanOptics show inelastic scattering (detritus and minerals, basi­ cally), 
then . j(.)= a.j((..)) Foreachabsorption interaction thatcouldgenerate inelas­tic scattering(pure water,phytoplankton 
andCDOM) we de.ne itsinelasticprobability (. j), theprobability thatan absorption eventgenerates an inelastic 
scattering event: N. . j(. ' )= fI(. ' , .)d. . fI(. ' , .i) (21) .b .a i=1 where .a and .b are the 
lower and upper limits of the simulated wavelengths,and i.[1..N.] refer to samples in wavelengthdomain: 
If interaction j represents the effective inelastic scat­teringeventwithin theabsorption interaction: 
. j(.)= a j(.) .(.) . j(.) If interaction j represents the pure absorption event (no inelastic scattering 
happening at all): . j(.)= a j(.) .(.)(1-. j(.)) Thus, at each interaction a random number . between0 
and1isgenerated resultingin(betweenparenthesis,exam­ple values of. j at . = 500nm thatdetermine the size 
ofthe correspondinginterval are included): .e[0, .1) . absorptionbypure water(2.51· 10-1).  .e[.1, 
.2) . Raman scattering, inelastic scattering by pure water(1.21· 10-9).  .e[.2, .3) . absorptionby mineralsanddetritus(7.12· 
10-2).  .e[.3, .4) . absorptionbyphytoplankton(4.90· 10-3).  .e[.4, .5) . inelastic scatteringbyphytoplankton(2.18· 
10-3).  .e[.5, .6) . absorptionbyCDOM(7.83· 10-2).  .e[.6, .7) . inelastic scatteringbyCDOM(1.21· 10-2). 
 .e[.7, .8) . elasticscatteringbypurewater(7.44· 10-3)  .e[.8, .9) . elastic scattering by minerals 
and detritus (2.94· 10-1).  .e[.9, 1] . elastic scattering by phytoplankton (2.79· 10-1).  i where 
.i(.) isgivenby .i(.)= .j=1. j(.) To compute the new re-emittedwavelength after a inelas­tic scattering 
event i, the normalized wavelength redistribu­ fi(. ' ,.) tionfunction is treated asaprobabilitydistribution 
.i(.' ) function(PDF)given theexcitationwavelength. ' .To sam­pleitef.cientlywe.rstbuild itsnormalizedcumulativedis­tributionfunction(CDF) 
andthen inverse importancesam­ple thisCDF.Greatervaluesof thePDFforagivenwave­length will translate to 
steeper areas of the CDF, thus in­creasingtheprobabilityofare-emissionatsuchwavelength. Note that thede.nitionof 
fi(. ' , .) is not limitedto the vis­ible spectrum, which might result in re-emissions happen­ing at 
wavelengths beyond the visible spectrum. However, as .i(. ' ) is limitedtothesimulated(visible) spectrum,only 
inelastic interactionswithin thisspectrumareconsidered.It could happen that a photon inelastically scattered 
at such &#38;#169;2007TheAuthor(s) Journalcompilation c&#38;#169;2007TheEurographicsAssociationandBlackwellPublishingLtd. 
wavelengths suffers a second inelastic scattering event that bringsitbackto thevisible lightrange.Giventhe 
lowprob­ability ofthis chain of events and our computergraphics ap­proach,weassumethataphotonbeyondthevisiblespectrum 
isde.nitely absorbed.Figure 1shows aglobal overview of thealgorithmduringthephoton tracing stage. Wavelength 
sampling Figure 1: Photon tracing algorithm. Inelastic scattering events generate a photon with a different 
associated wave­length according to the wavelength redistribution function. 4.2. Stage 2: radiance estimate 
Toestimateradianceweadopta tradeoffbetweenspeed and memoryrequirementssimilar totheproposedbyJensenand 
Christensen[JC98]:weonlystorephotonsin thephotonmap iftheyhavebeenre.ectedor transmittedfromsurfaces,orif 
theyhave alreadybeen scattered atleast once.Thus, we can compute single scattering more ef.cientlyby 
ray marching throughthemediumand samplingthe light sourcesby cast­ing shadowrays.Takingintoaccount thewavelengthredis­tributionfunctionfor 
inelasticscattering,anewaddend will be addedat each step ofthe ray marchingprocess: NN. . . .. Ll .i' 
,.wl pI .i' , .,.wl,.wo aI .i ' fI .i' , ..x (22) l=1i=1 where i.[1..N.] and l .[1..N] refer tosamples 
in thewave­lengthandlight sourcedomainrespectively, .wl is thedirec­tion to the light with an incoming 
radiance Ll and .x repre­sentthe ray marching steps. Multiple scattering will be computed from the photon 
map, .nding in thekd-tree the n photons which are closest to the estimation point by using the typical 
nearest neigh­bours algorithm.To accountfor multipleinelastic scattering wemodifytheradiance estimateexpressionof[JC98]byin­cluding 
a new term: n . ... .Fk. pI k, .,.wk,.wofI k, . 4 2 (23) . ' . ' k=13 p r where r is theradius ofthe 
sphere that contains the n closest photons, andk represents each ofthe storedphotons. 5. Results Wehaveused 
thevaluesfromTable 5foroursimulations. In the images produced we only vary the chlorophyll con­centration 
C, minerals and detritus turbidity ad(400) and DiegoGutierrez et al./VisualizingUnderwaterOceanOptics 
 (a)C = 0, ad = 0,ay = 0 (b)C = 0.01, ad = 0,ay = 0 (c)C = 0.1, ad = 0,ay = 0 (d)C = 1, ad = 0,ay = 0 
 (e)C = 0, ad = 0.1,ay = 0 (f)C = 0.01, ad = 0.1,ay = 0 (g)C = 0.1, ad = 0.1,ay = 0 (h)C = 1, ad = 0.1,ay 
= 0 (i)C = 0, ad = 0,ay = 0.1 (j)C = 0.01, ad = 0,ay = 0.1 (k)C = 0.1, ad = 0,ay = 0.1 (l)C = 1, ad 
= 0,ay = 0.1 Figure 2: Resultingpicturesvarying thechlorophyllconcentrationC,themineralsanddetritus turbidity 
ad at 400nm and the CDOM turbidity ay at440nm. CDOMturbidityay(440).The choice ofthose threeparam­eters 
to reduce the dimensionality of the model was based on theirgreateroverall in.uenceon theresultinglight 
.eld. Thephoton mapcontains400000photons, with250 usedin the estimation of radiance.Ray-marchingdepth 
is set at200 steps.Each oftheimageshasbeenrenderedinaDualXeon Pentium4at2.8GHz with2GBRAMat512×384resolu­tion, 
casting one ray per pixel, and took approximately 20 minutes to render. This time is roughly independent 
of the numberofparametersofthebio-optical model.Inorder to reduce these computation times, several optimization 
tech­niques could be adopted, like using adaptive ray-marching or radiance caching strategies[JDZJ08].Additionally, 
per­ceptualissuescouldbe taken intoaccount,usingjust anap­proximate solution in areas ofthe image where 
the error is known tobeperceptuallynegligible[SGA*07]. Energybalancesshow thatonaveragealmost99% ofthe 
energy emittedbythe light sourcesisabsorbed afterjust a few interactionsofthephotons,withveryincrementalvari­ation 
after the fourth interaction and negligible contribu­tion after the .fth. This relatively fast convergence 
is due to the strong absorption in water. We have therefore lim­ited the number of interactions per photon 
to .ve, in order to speed up the simulations. Variations of the parameters C, ad(400) and ay(440) yielddifferentprobabilitiesfor 
ab­sorption,elasticandinelasticscatteringevents,whichin turn affectthein-water light.eld.TheresultscanbeseeninFig­ure 
2, with each of the varying parameters in.uencing the .nallight.eldasfollows: Chlorophyll concentration(C) 
affects mainlyboth elastic andinelasticscattering.Theeffectsofinelasticscattering aremostlymaskedbythemorepredominant 
elasticscat­tering and absorption,which increasesslowly.The third column inFigure 2showsbrighter images 
than thepre­vious twodue toin-scattering.Forhighervalues(fourth column), out-scattering prevails and 
the images become darker.  Minerals and detritus turbidity (ad(400)) increases ab­sorptionat lowerwavelengths, 
thusreducingthebright­nessofthesceneandtheoverallbluehue.Scattering is also increased, makingthe imagesappear 
murkier.Figure 2showsvariationsofthemineralsanddetritus turbidity between the .rstand second rowsfordirect 
comparison.  CDOM turbidity (ay(440)) slightly increases absorp­tion(darker images) and introduces inelastic 
scattering (change inhue).Thiscanbeseenbycomparing the .rst andthird rows inFigure 2.  We have undergone 
a visual validation of our model by renderingdifferentnaturalwaters.Figure 3shows theresult­ing underwater 
imagesforAtlantic,Mediterranean,Baltic, NorthSea andshallow coastal watersrichinCDOMrespec­tively.All 
the imageshavebeen simulated atthe samedepth c &#38;#169;2007TheAuthor(s) Journalcompilation c &#38;#169;2007TheEurographicsAssociationandBlackwellPublishingLtd. 
 DiegoGutierrez et al./VisualizingUnderwaterOceanOptics  Figure 3: Rendered images of different waters. 
From left to right: Atlantic, Mediterranean, Baltic, North Sea and shal­low coastal waters richinCDOM.Smallerpatchesbelow 
for comparisonpurposesbyFrisvad et al.[FCJ07](usedwith permission). and are illuminatedbythesameisotropicpointlight 
source. The changes in color are clearly noticeable, from a darker blueinthecaseofAtlanticwater, tothegreenerhueinthe 
imageoftheNorthSea.Thesmallerpatchesbelow the .rst fourimagescorrespondtothesimulationsbyFrisvad et al. 
[FCJ07]for thesame typesofwater,andareshownforcom­parisonpurposes.Oursimulationsbasedonradiative trans­fer 
approximately matchtheir simulationsbasedonLorenz­Mietheory.Thedifferencesaremainlyowedto twofactors: 
ontheonehand,theoveralldarker toneinourimagesisdue to in-water absorption, whereas[FCJ07]renders the 
surface ofthewaterbody; on theotherhand, theabsenceofinelas­ticscattering effects in[FCJ07]canhaveavisible 
in.uence the .nal appearanceof water,asshown inFigure 4for the Balticcase.Thepropertiesofthewaterhavebeenadjusted 
accordingtomeasurementsfoundin[BSF*03][Mob94]for ourbio-opticalmodeland[BSF*03]in themodelbyFrisvad et 
al.Inbothcases,it isonlythechangesin theconstituents ofthe waterswhichyield thedifferentcolors.Wehave 
addi­tionallyperformeda numerical analysis ofthe in-waterradi­ance.eld, toquantifythein.uenceof each 
constituent.The results canbe seen inFigure 5. 6. Conclusion We have presented a complete bio-optical 
model of ocean water based on parameterizing its intrinsic optical proper­ties.Relying onradiative transfertheory,weobtainthere­sultingin-water 
light.eldby extendingtherendering algo­rithmpresentedin[GMAS05].Theextensioncannowhan­dlemorecomplexinteractionsbetween 
lightand water,in­cludinginelastic scatteringwithanti-Stokesbehavior, where the scatteredphoton absorbs 
energyfrom the medium andis re-emitted at higher energies.Wehave additionally studied the in.uenceoftheparameters 
intheapparent opticalprop­ertiesof waterinthescene,which arede.nedbythe light .eld obtained.Wehaveperformed 
an energy-balance anal­ysis,andvisual validationofthemethodhasbeenprovided bydirectcomparison with imagesbyFrisvad 
et al.[FCJ07], renderingdifferent types of watersbased onpublished con­stituentdata. We have included 
Raman scattering by pure water and .uorescencebyphytoplankton andCDOM as inelastic scat­tering eventswithenergytransfers.Even 
thoughtheircom­bined quantitative contribution to the overall radiance .eld &#38;#169;2007TheAuthor(s) 
Journalcompilation c&#38;#169;2007TheEurographicsAssociationandBlackwellPublishingLtd. isusually less 
than2%(seeFigure 5), thisrelativelysmall percentagedoeshaveaclear in.uenceon theapparent op­tical properties, 
as Figure 4 shows. We thus argue that theseevents,usuallyoverlookedincomputergraphics litera­ture, arequalitatively 
importantfor underwater imagery and should be included in a complete simulation. Other types ofinelastic 
scattering such as Compton, Bragg or Brillouin couldalsobeadded,althoughtheir in.uence ismore incre­mental.Otherparticulate 
elements couldbe easily added as well just by including their corresponding absorption and scattering 
coef.cients in themodel;however, the threecon­stituents treatedhere(phytoplankton,mineralsanddetritus 
and CDOM) have the most in.uence in the .nal radiance .eld. The results show how the model developed 
can easily beusedforphysically-basedsimulationsof underwater im­agery. We believe this work can be of 
interest not only in the computer graphics community, but in remote sense or oceanographic studies as 
well. Figure 4: The in.uence of inelastic scattering in the appar­ent opticalpropertiesof water(Balticsea):Left,no 
inelastic scattering. Center, just chlorophyll inelastic scattering (as in[GMAS05]).Right,all inelasticscatteringevents 
included in the simulation. Figure 5: Radiance distribution of the resulting in-water light .eldper 
typeof event(BalticSea). Acknowledgments We thanktheanonymousreviewersfor theirvaluablecom­ments.Thisresearch 
waspartlydoneunder thesponsorship oftheSpanishMinistry ofEducationandResearchthrough theprojectsTIN2004-07672-C03-03 
andTIN2007-63025. References [BLSS93] BLASI P.,LESAEC B.,SCHLICK C.:Arendering al­gorithmfordiscrete 
volumedensityobjects. ComputerGraphics Forum(Eurographics93)12,3(1993),201 210. 4 DiegoGutierrez et 
al./VisualizingUnderwaterOceanOptics [BMP81] BRICAUD A.,MOREL A.,PRIEUR L.:Absorptionby dissolvedorganic 
matterofthesea(yellowsubstance)intheuv and visibledomains. Limnol.Oceanogr.26,1(1981),43 53. 3 [BSF*03] 
BABIN M.,STRAMSKI D.,FERRARI G.M.,CLAUS-TRE H., BRICAUD A., OBOLENSKY G., HOEPFFNER N.: Vari­ationsinthelight 
absorption coef.cients ofphytoplankton, non­algal particles, and dissolved organic matter in coastal 
waters around europe. J.Geophys.Res.108(C7),3211(2003). 3,9 [Cha60] CHANDRASEKHAR S.: Radiative Transfer. 
DoverPub-lications,Inc.,1960. 2 [CS04] CEREZO E.,SERON F.J.:Renderingnatural waterstak­ing .uorescenceintoaccount:Research 
articles. Comput. Ani­mat.VirtualWorlds15,5(2004),471 484. 2 [FCJ07] FRISVAD J. R., CHRISTENSEN N. J., 
JENSEN H. W.: Computingthescatteringpropertiesofparticipatingmediausing Lorenz-Mietheory. ACMTrans.Graph.26,3(2007),60. 
2,9 [Gla95] GLASSNER A. S.: Principles of Digital Image Synthe-sis.MorganKaufmannPublishersInc.,SanFrancisco,CA,USA, 
1995. 2 [GM83] GORDON H. R., MOREL A.: Remote Assessment of Ocean Color for Interpretation of Satellite 
Visible Imagery: A Review, vol.4 of Lecture Notes on Coastal and Estuarine Stud­ies.Springer-Verlag,NewYork,1983. 
4 [GMAS05] GUTIERREZ D., MUNOZ A., ANSON O., SERÓN F.J.:Non-linearvolumephoton mapping. In Proc. of the 
Eu­rographicsSymposium onRenderingTechniques,Konstanz,Ger­many,June29 -July1,2005 (2005),pp.291 300. 
2,6,9 [GSO03] GREEN R. E., SOSIK H. M., OLSON R. J.: Contri­butions of phytoplankton and other particles 
to inherent opti­calpropertiesin new england continental shelf waters. Limnol. Oceanogr.48,6(2003),2377 
2391. 4 [Haw92] HAWES S.:Quantum .uorescence ef.ciencies ofmarine fulvic and humid acids. PhD thesis,Dept.ofMarinceScience, 
Univ.ofSouthFlorida,1992. 5 [HG41] HENYEY L., GREENSTEIN J.: Diffuse radiation in the galaxy. AstrophysicsJournal93 
(1941),70 83. 4 [JC98] JENSENH.W.,CHRISTENSEN P.H.:Ef.cientsimulation oflighttransportinsceneswithparticipating 
mediausingpho­ton maps.InSIGGRAPH98ConferenceProceedings (jul1998), CohenM.,(Ed.),AnnualConferenceSeries,ACMSIGGRAPH, 
AddisonWesley,pp.311 320. 7 [JDZJ08] JAROSZ W., DONNER C., ZWICKER M., JENSEN H. W.: Radiance caching 
for participating media. To appear inACMTransactions onGraphics (2008). 8 [Jen01] JENSEN H. W.: Realistic 
image synthesis using photon mapping.A.K.Peters,Natick,Massachussets,2001. 2,6 [Kir94] KIRK J.T.: Light 
and photosynthesis in aquatic ecosys­tems.CambridgeUniversityPress,NewYork,1994. 3 [KYNN91] KANEDA K.,YUAN 
G.,NAKAMAE E.,NISHITA T.: Realistic visualsimulation of water surfaces takinginto account radiativetransfer. 
In Proc. of CAD/Graphics 91 (1991),pp.25 30. 2 [Maz02] MAZO R.M.: BrownianMotion:Fluctuations,Dynam­ics 
and Applications, vol.112 of International Series of Mono­graphs on Physics. OxfordUniversityPressInc.,GreatClaren­ 
don Street, Oxford, 2002, ch. Einstein-Smoluchowski Theory, pp.46 62. 4 [Mob94] MOBLEY C.D.:LightandWater:RadiativeTransferin 
NaturalWaters.AcademicPress,Inc.,SanDiego,1994. 1,2,3, 5,9 [Mor74] MOREL A.: Optical Aspects of Oceanography. 
Aca­demicPress,NewYork,1974. 3,4 [Mor88] MOREL A.:Opticalmodeling oftheupperoceaninre­lationtoitsbiogenous 
mattercontent(caseiwaters). Journal of GeophysicalResearch93,C9(1988),10749 10768. 3 [NKON90] NAKAMAE 
E., KANEDA K., OKAMOTO T., NISHITA T.: A lighting model aiming at drive simulators. ComputerGraphics24,4(Aug.1990),395 
404. 2 [NSTN93] NISHITA T., SHIRAI T., TADAMURA K., NAKAMAE E.: Display of the earth taking into account 
atmosphere scat­tering. In Computer Graphics (SIGGRAPH 93 Proceedings) (1993),vol.24,pp.175 182. 2 [PA01] 
PREMOZES.,ASHIKHMINM.:Renderingnaturalwaters. Comput.Graph.Forum20,4(2001),189 199. 2 [PF97] POPE R.M.,FRY 
E.S.:Absorptionspectrum(380 700 nm)ofpurewater.ii.integrating cavity measurements. Applied Optics36,33(1997),8710 
8723. 3 [Pre76] PREISENDORFER R. W.: Introduction, vol.1 of Hydro­logic Optics. National Technical Information 
Service, Spring­.eld,IL,1976. 1 [RPC89] ROESLER C.S.,PERRY M.J.,CARDER K.L.:Model­ingin situphytoplankton 
absorptionfrom totalabsorption spec­trainproductiveinland marinewaters. Limnol. Oceanogr. 34,8 (1989),1510 
1523. 3 [RT87] RUSHMEIER H.E.,TORRANCEK.E.:Thezonalmethod for calculatinglightintensitiesin thepresence 
ofaparticipating medium. ComputerGraphics21,4(July1987),293 302. 2 [SB81] SMITH R. C., BAKER K. S.: Optical 
properties of the clearestnaturalwaters(200 800nm). Appl.Opt.20(1981),177 184. 3,4 [SBM01] STRAMSKI D., 
BRICAUD A., MOREL A.: Modeling theinherent opticalproperties ofthe oceanbased onthedetailed compositionofplanktoniccommunity. 
AppliedOptics40(2001), 2929 2945. 4,5 [SCP94] SPINRAD R.W.,CARDER K.L.,PERRY M.J.(Eds.): Ocean Optics. 
No.25inOxfordMonographsonGeology and Geophysics.OxfordUniversityPress,1994. 2 [SGA*07] SUNDSTEDT V.,GUTIERREZ 
D.,ANSON O.,BAN-TERLE F., CHALMERS A.: Perceptualrendering ofparticipating media. ACMTransactions ofAppliedPerception4,3(2007). 
8 [SLP87] SATHYENDRANATH S., LAZZARA L., PRIEUR L.: Variations in the spectral values of speci.c absorption 
of phy­toplankton. Limnol.Oceanogr.32,2(1987),403 415. 3,4 [TN95] TADAMURA K., NAKAMAE E.: Computer Graphics: 
Developments in Virtual Environments. AcademicPress,1995, ch.Modeling the colour ofWaterinLightningDesign,pp.97 
114. 2 [Wal69] WALRAFEN G.E.:Continuum modelof water anerro­neousinterpretation. JournalofChemicalPhysics50,1(January 
1969),567 569. 5 c &#38;#169;2007TheAuthor(s) Journalcompilation c &#38;#169;2007TheEurographicsAssociationandBlackwellPublishingLtd. 
 Structured Light in Scattering Media* Srinivasa G. Narasimhan+, Shree K. Nayar++, Bo Sun++ and Sanjeev 
J. Koppal+ +Robotics Institute, Carnegie Mellon University, Pittsburgh, USA ++Computer Science Department, 
Columbia University, New York, USA Abstract Virtually all structured light methods assume that the 
scene and the sources are immersed in pure air and that light is neither scattered nor absorbed. Recently, 
how­ever, structured lighting has found growing application in underwater and aerial imaging, where scattering 
ef­fects cannot be ignored. In this paper, we present a com­prehensive analysis of two representative 
methods -light stripe range scanning and photometric stereo -in the pres­ence of scattering. For both 
methods, we derive physical models for the appearances of a surface immersed in a scattering medium. 
Based on these models, we present results on (a) the condition for object detectability in light striping 
and (b) the number of sources required for photometric stereo. In both cases, we demonstrate that while 
traditional methods fail when scattering is signi.­cant, our methods accurately recover the scene (depths, 
normals, albedos) as well as the properties of the medium. These results are in turn used to restore 
the appearances of scenes as if they were captured in clear air. Although we have focused on light striping 
and photometric stereo, our approach can also be extended to other methods such as grid coding, gated 
and active polarization imaging. 1 Introduction Structured lighting has received wide attention since 
the earlyworkof Will and Pennington [28]. Virtually all structured lighting techniques modulate the appearance 
of a surface (or volume) by projecting a particular pat­tern of light onto it [15]. Often this modulation 
makes subsequent processing of acquired images simple; the cor­respondence problem and issues related 
to calibration are often alleviated [6; 13; 30] and accurate 3D reconstruction is obtained irrespective 
of the complex surface texture [20; 21]. As a result, structured lighting has been a key en­abling technology 
for several industrial applications such as assembly, alignment and inspection. An implicit assumption 
made in most structured light methods is that light is neither scattered nor absorbed by the medium in 
which the scene and sources are immersed (as in pure air). This assumption, however, is violated in several 
media including atmospheric conditions (fog, haze, mist), .uids (murky water, milk) and smoke. An optical 
image taken in such a medium su.ers from signi.cant loss of contrast and attenuation of brightness. Further, 
this degradation increases exponentially with distance, mak­ing it hard to capture meaningful images 
of scenes that are not near the imaging system. Thus, it is critical to *This work is supported by an 
ONR contract #N00014-05-1­0188. The authors also thank Estuardo Rodas for building the tank used for 
the experiments in this paper. take into account the e.ects of scattering while applying structured light 
methods in such media. But why use structured light in scattering media at all? Consider, for example, 
underwater optical imaging. Be­cause of scattering by impurities in water, natural sources such as daylight 
attenuate completely before reaching sig­ni.cant depths. So, deep underwater (semi-)autonomous vehicles 
rely on active illumination to explore wreckages1 , .nd mines, and inspect vessels, docks and rigs, thus 
play­ing a key role in maintenance, construction and rescue scenarios. A variety of structured lighting 
techniques have been developed for these applications, ranging from using thin laser beams [14], to using 
a number of care­fully spaced confocal sources [17], to more sophisticated time-gated [19] and synchronization-gated 
techniques [7]. While these methods enhance visibility (resulting in bet­ter detection of targets), they 
do not explicitly analyze the appearances of scenes under structured light2 . In this paper, we are interested 
in both enhancing vis­ibility using structured light and also in analyzing the acquired images to recover 
properties of the scene and the medium. To achieve this, three relevant questions must be addressed. 
First, what are the scattering e.ects that result from the interaction of structured light with the medium 
and the scene? Second, how do we over­come these scattering e.ects to obtain the results that the structured 
light methods were traditionally designed for?3 Third, is there additional information that one can extract 
from these scattering e.ects that is not possible to obtain using the traditional methods? We address 
these questions speci.cally for two represen­tative techniques -light stripe range scanning and pho­tometric 
stereo. For each of these, we derive an analytic image formation model that is based on the physics of 
single scattering. These models describe the interactions of structured light with the medium and the 
scene. Using the image formation model for light striping, we develop a simple algorithm to reliably 
detect objects and obtain a 3D reconstruction of the scene in the presence of strong scattering. Based 
on the image formation model for pho­tometric stereo, we conclude that at least .ve light source directions 
(instead of the usual three) are required to re­construct surface normals and albedos of a lambertian 
1Famously, the 1985 Franco-American expedition discovered and explored the remains of the Titanic that 
sank in 1912 [1]. 2Work has also been done on a related but di.erent problem of analyzing the appearances 
of scenes in scattering media (underwater or the atmosphere) using passive methods [5; 24; 26; 23; 18; 
22] that rely on natural illumination external to the medium. 3Note that works that address this question 
(for instance, laser scanning to reconstruct sea .oors [3; 16]) by ignoring scattering e.ects (.rst question) 
are severly limited in their applicability. Proceedings of the Tenth IEEE International Conference on 
Computer Vision (ICCV 05) 1550-5499/05 $20.00 &#38;#169; 2005 IEEE  object. Interestingly, our method 
also yields a depth map of the scene, which is not possible using traditional photo­metric stereo. Further, 
in both techniques, the interaction of structured light with the medium allows us to estimate the properties 
of the medium. This result can in turn be used to remove the e.ects of scattering and compute the appearance 
of the scene as if seen in clear air. To verify our methods using real experiments, we have constructed 
a setup that consists of a glass tank .lled with a scattering medium (dilute milk), with a projector 
(source) and camera placed outside the tank. Note that calibration of this setup requires us to handle 
light re­fraction at the medium-glass-air interfaces. We present a calibration procedure that is similar 
in spirit to [11] and that does not require either explicit geometric calibration of the camera and the 
projector or the knowledge of re­fraction locations or refractive indices of media. Although we have 
focused on light striping and photometric stereo, our results can be used to extend several other techniques 
such as grid coding [28] and gated [7] and active polar­ization imaging [25; 10]. We believe that our 
results can signi.cantly bene.t a wide range of underwater [14],aerial and microscopic imaging [9] applications. 
2 Single Scattering in Media In order to keep our techniques tractable, we assume that the scattering 
medium is homogeneous and not highly dense (for example, murky water, light fog, mist, dilute milk). 
This allows us to develop simple models based on single scattering. We now de.ne the properties of scatter­ing 
media [4] and present the single scattering model. The scattering coe.cient ß is de.ned as the fraction 
of the incident .ux scattered by a unit volume of the medium in all directions4 . The phase function 
P(a) de.nes the angular scattering distribution of the incident .ux, where a is the angle between incident 
and scattered directions. In general, the phase function is smooth and may be rep­resented by a low-order 
polynomial of cos a [4].We use the .rst-order approximation as given in [4], P(g, a)= (1/4p)(1 + g cos 
a) , (1) where, g . (-1, 1) is a parameter that controls the shape of the phase function. Now consider 
a light ray with ra­diance L0 that travels a distance x, gets scattered by a particle at an angle a, 
before it further travels a distance y to reach the viewer. The intensity of this light ray is attenuated 
exponentially according to the total distance traveled. Then, the single scattering irradiance at the 
viewer is given by [4], -ß(x+y) Emed = L0 ß P(g, a) e. (2) For an isotropic point source with radiant 
intensity I0, we may further write L0 = I0/x2 , while for a collimated beam, L0 is constant with respect 
to x. We build upon equations 1 and 2 to derive image formation models for light striping and photometric 
stereo. 4When absorption is present, the scattering coe.cient is replaced by the extinction coe.cient 
de.ned as the fraction of incident .ux scattered and absorbed by a unit volume. Camera Intersection 
Curve Figure 1: Light striping in media. A sheet of light swept across a surface produces a brightness 
discontinuity (black curve). When there is no scattering, the pixel brightness is only due to this discontinuity 
(red ray). In the presence of scattering, the light plane itself becomes visible (dashed ray) making 
surface detection hard. 3 Light Striping in Scattering Media Light stripe range scanning is a technique 
where a plane (or sheet) of light is swept across an object (.gure 1) to obtain its 3D reconstruction. 
The key observation is that the plane of light intersects the object surface at a curve, producing a 
large brightness discontinuity. Then, the 3D coordinates of each point on this curve is com­puted by 
intersecting the camera ray and the light plane. A critical requirement here is that the intersection 
curve be detected reliably, which is usually done by threshold­ing the acquired image. Unfortunately, 
in the presence of scattering, the entire light plane itself becomes visible and detecting this intersection 
is not possible by simple thresholding. In this section, we derive the model for im­age formation when 
the light plane and the surface are immersed in a scattering medium and develop algorithms for reliable 
scene detection, and 3D reconstruction and for obtaining a clear-air appearance of the scene. 3.1 Image 
Formation Model Imagine a light plane sweeping across a surface in a scat­tering medium. The camera not 
only receives light re­.ected by the surface, but also from the medium after scattering (see .gure 2). 
The dashed lines indicate light rays that reach the camera after attenuation and scat­tering in the medium, 
but without reaching the surface. Then, the irradiance Emed at the camera is exactly given by equation 
2. The red line indicates the path traveled by a light ray from the source to the surface and then re.ected 
by the surface toward the camera. The intensity of this ray is exponentially attenuated according to 
the total distance traveled. Hence, the irradiance Esurf at the camera due to this ray is written as5 
, -ß(ds+dv Esurf = L0 e ) R, (3) where, R is the radiance (normalized by source intensity) in the absence 
of scattering. Thus, the image formation model may be compactly written using the Dirac delta function 
d as, E = Esurf d(x = ds)+ Emed d(x<ds) . (4) 5Single scattering of the exponentially attenuated surface 
radi­ance towards the camera is a minor e.ect compared to the scattering Emed from bright sources, and 
hence can be safely ignored. Proceedings of the Tenth IEEE International Conference on Computer Vision 
(ICCV 05) 1550-5499/05 $20.00 &#38;#169; 2005 IEEE Source x á d Camera s d v SurfaceScattering Medium 
 Figure 2: Image formation in light striping. The irradiance at the camera is produced by either the 
light rays that reach the camera after being scattered once by the medium (dashed) or by light rays that 
are re.ected by the surface (solid red). In both cases, the intensities of the rays are attenuated exponentially 
according to the distance traveled. 3.2 Intersection of Surface and Light Plane Figure 3 illustrates 
the pro.le of the camera irradiance E as a function of the distance x of the source from the surface, 
according to equation 4. The brightness pro­.le resembles an exponential fall-o. followed by a dis­continuity 
at the surface. When there is no scattering (ß =0), we have Emed =0 ,Esurf = L0R and hence Esurf >> Emed 
. In this case, the brightness pro.le is a delta function and it is easy to detect the intersection us­ing 
a threshold, as is done traditionally. For thresholding to work in the presence of scattering, we must 
have ß(ds-x+dv-y) R>> ß P(g, a) e. (5) However, when scattering is signi.cant (large ß), it is mostly 
the opposite case, Emed >= Esurf , as shown by the green and blue pro.les in .gure 3. Thus, the light 
plane itself becomes brightly visible (see second column in .gure 6). In order to detect the intersection 
of the light plane and the surface, we simply use the brightness pro.le as a template until a brightness 
discontinuity is reached at the end. Even for the hard case where the density of the medium is high, 
this simple scheme performs well. 3.3 Experimental Setup and Calibration The experimental setup consists 
of a 20. ×20. ×10. glass tank .lled with water (see .gure 4(a)). Di.erent quanti­ties of milk are mixed 
to emulate scattering media with di.erent densities (ß). The glass faces are anti-re.ection coated to 
avoid re.ections. We used an 8-bit Canon XL1S 3-CCD video camera and an Infocus LP120 1000 ANSI Lumens 
DLP projector in our experiments. To keep the size of the tank small, the camera and the projector are 
placed outside the tank. Hence, we need to handle light refractions at the air-glass-medium interfaces. 
Our cali­bration method is similar in spirit to techniques in [11]. Figure 4(b) illustrates a light plane 
from the projector shining into the glass tank after refraction. Calibration involves sweeping the light 
plane across two vertical pla­nar surfaces -the (u,v)-and the (s,t)-planes -placed in the medium. The 
3D world coordinates of a few points on these planes are measured apriori (the remaining points are interpolated). 
Then, the equation of each light plane E No E scattering Surf Emed Medium Scattering Significant Scattering 
 x x<d d ss Figure 3: Brightness pro.le for detecting the surface and light plane intersection. When 
there is no scattering (red), the pro.le is a delta function which can be thresholded to detect the intersection. 
As the density of the medium (ß) increases (green and blue), the brightness of the discontinuity (Esurf 
) decreases and the light plane becomes brighter (Emed). is obtained using its line intersections with 
the (u,v)-and (s,t)-planes. Let this be represented by, Ax + By + Cz + D =0 . (6) Next, we associate 
with each incoming camera ray (pixel (i, j)), its intersections P (u, v, r)and Q(s, t, 0) with the (u,v)-and 
the (s,t)-planes respectively (blue line in .gure 4(c)). This yields a parametric equation for each camera 
ray, which is represented by: [x, y,z]=[s, t, 0] + k [u - s, v - t, r - 0] , (7) where, k is a scalar 
parameter. We calibrated our setup with the two planes placed at z =0 inches and z =6.0 inches. To verify 
calibration accuracy, we reconstructed (as described in Section 3.4) a plane placed at z =4.18 inches 
with a low RMS error of 0.21 inch (.gure 5). In summary, our method does not require explicit geometric 
calibration of either the camera or the projector and does not require the position/orientation of the 
glass face or the refractive indices of media. 3.4 Scene and Medium Recovery Once calibrated the setup 
may be used to recover the 3D structure and clear-air appearance of any object in the medium as well 
as the properties of the medium itself. 3D surface reconstruction: Figure 4(c) shows a top­view (2D) 
illustration of the light striping setup and the pro.le of an object s surface. Since a point on the 
surface lies at the intersection of the re.ected ray (blue) and the light plane (red), we may substitute 
(x, y, z)fromequation 7intoequation 6, to solve forthe parameter k: As + Bt + D k = . (8) A(s - u)+ B(t 
- v) - Cr The value of k is then substituted back into equation 7 to obtain the 3D coordinates (x, y, 
z) of the surface point. Medium properties: The properties of the medium can be obtained by observing 
the brightness decay of the light plane without the surface (see pro.le of Emed in .gure 3). The distances 
x and y can be computed using the 3D co­ordinates of points on the light plane and the dimensions Proceedings 
of the Tenth IEEE International Conference on Computer Vision (ICCV 05) 1550-5499/05 $20.00 &#38;#169; 
2005 IEEE  (a) Experimental Setup (b) Calibration (c) 3D reconstruction Figure 4: Light striping experimental 
setup and calibration. (a) The setup consists of a glass tank .lled with a scattering medium (dilute 
milk). The scene of interest is immersed in the medium. A projector illuminates the medium and the scene 
with planes of light and a video camera views the scene with the e.ects of scattering. (b) The light 
plane sweeps (one at a time) two planar surfaces placed vertically in the tank at known distances (z 
=0 and z = r), called the (u,v)-and the (s,t)-planes. The discrete mappings between the light plane and 
the (u,v)-and (s,t)-planes, and between the camera ray and the (u,v)-and (s,t)-planes constitute calibration. 
Note that no knowledge of the refraction locations or indices is required. (c) The top view of the setup 
illustrating the intersection of the light plane and the camera ray to yield the 3D coordinates of a 
surface point. (a) Calibration Planes (b) Reconstruction Figure 5: Veri.cation of light striping calibration. 
(a) Two planes at z =0 and z =6.0 inches are used for calibration. (b) The computed equations of light 
planes and camera rays are thenusedto reconstruct a thirdplane at z =4.18 inches (with RMS error 0.21 
inch). The 3D view shows the three vertical planes and a light plane (red) for illustration. of the tank. 
Then, equation 2 is nonlinear in the two un­known medium parameters, ß and g. Thus, by observing the 
irradiances Emed along a pro.le on the light plane, we can estimate the two parameters ß and g usinganon­linear 
optimization method ( fminsearch in MatlabTM ). Scene appearance without scattering: Once the scattering 
coe.cient ß is estimated and the 3D surface is reconstructed, the scene appearance without scattering 
can be computed for each object intersection strip, from equation 3 as, + ß (ds+dv) L0R = Esurf e, (9) 
 where, Esurf is the observed brightness of the object in the presence of scattering. Then, all the intersection 
strips are mosaiced to create the appearance of the entire scene as if captured in clear air. The results 
of applying the scene and medium recovery algorithms are shown using real experiments in .gure 6. The 
detection of the object intersections and hence the 3D reconstruction obtained under di.erent densities 
of scat­tering compare well with the ground truth. Despite the strong e.ects of scattering, we are able 
to remove them completely to restore the original scene contrast. Also a comparison to the .oodlit images 
demonstrates that simply using bright sources does not enhance visibility in scattering media, and that 
structured lighting methods that are designed to focus light on the scene to alleviate blurring and backscattering 
must be used. 4 Photometric Stereo in Scattering Media In situations where light stripe scanning takes 
too long to be practical (for example, dynamic scenes), photometric stereo [29] provides an attractive 
alternative. Tradition­ally, photometric stereo is a technique for scene recon­struction (surface normal 
and albedo) from a small num­ber of images of the scene acquired under di.erent lighting directions. 
Many variants of this problem exist in vision literature [12; 2], but none of the proposed solutions 
are e.ective in scattering media. In this section, we show how photometric stereo can be extended to 
scattering media. We choose the simplest version of the problem that assumes the surfaces to be lambertian, 
the sources distant, interre.ections negligible and the camera to be orthographic. In the absence of 
scattering, it is known that three images of a scene illu­minated from di.erent but known directions 
are su.cient to uniquely determine the surface normals and albedos. We will .rst determine how many sources 
are needed in the presence of scattering and then show how scene prop­erties can be recovered from the 
corresponding images. Proceedings of the Tenth IEEE International Conference on Computer Vision (ICCV 
05) 1550-5499/05 $20.00 &#38;#169; 2005 IEEE  Floodlit Images Single Light Plane Surface Reconstruction 
Appearance without Scattering Pure Dilute HigherW ater Concentration(noMilk (mediumMilk scattering) scattering) 
(high scattering) Figure 6: Experimental results for light striping in scattering media. The scene consists 
of two objects immersed in pure water (no scattering, ground truth), water mixed with 6 ml milk (medium 
scattering) and 15 ml milk (high scattering). The .oodlit images (obtained by turning on all pixels in 
the projector) illustrate the adverse e.ects due to scattering by the medium. The brightness pro.le of 
a single light plane focused on the object con.rms the template of the pro.le model in .gure 3. For the 
two concentrations, our methods estimated ß =0.07 in-1 and 0.16 in-1 and g =0.9 . In the medium scattering 
case, our results (3D reconstruction and scene appearance without scattering) are nearly identical to 
the ground truth (percentage RMS error = 2.1%). In the 15 ml milk case, the green cup is barely visible 
(especially since its albedo is low) and yet the result is close to the ground truth. The handle of the 
cup is completely invisible and is hence missed (else, percentage RMS error = 5.5%). Note that our algorithm 
must be applied to separately to individual color channels. The color di.erence between the pure water 
and the other cases is due to white balancing di.erences between di.erent experiments. (Please see a 
video on our website [27] for better visualization and for other examples.) 4.1 Image Formation Model 
Consider the illumination and observation geometry in .gure 7. A distant source (direction s) illuminates 
a sur­face point P with unit normal n and albedo ..A camera observing the surface receives irradiance 
Esurf due to the light re.ected by the surface (solid red lines) and irradi­ance Emed due to light scattered 
by the medium (dashed lines) in the viewing direction. The irradiance Esurf is the same as for light 
striping (see equation 3), -ßds ( . n . s ) e -ßdv Esurf = L0e. (10) Here, we have replaced the normalized 
radiance R by (. n . s ) for a lambertian surface. The irradiance Emed at the camera due to single scattering 
by the medium is obtained by integrating the brightness along the viewing direction (see equation 2), 
. dv Emed = L0 e -ßx ß P(g, a) e -ßy dy . (11) 0 Parallel Rays from Scattering Distant Source Medium 
 xd s Orthographic s Camera n P y Surface d v Figure 7: Image formation for photometric stereo in scat­tering 
media. The sources, viewer and the surface of interest are immersed in the scattering medium. The sources 
are distant and thus illuminate the surface and the viewing ray in a collimated fashion. The brightness 
at a pixel is the sum of the contributions from the solid red and the dashed rays. the source uniformly 
illuminates the viewing distance dv. In other words, x = ds is constant with respect to y (this assumption 
will be relaxed when we discuss our speci.c setup). This allows us to simplify equation 11 as, -ßdv ) 
. Note that a, P(g, a), ß and L0 are all independent of Emed = L0 P(a) e -ßds (1 - e (12) the integration 
variable y. Further, we shall also assume Proceedings of the Tenth IEEE International Conference on Computer 
Vision (ICCV 05) 1550-5499/05 $20.00 &#38;#169; 2005 IEEE  Figure 8: Refraction of rays in the photometric 
stereo model. The sources and camera are outside the scattering medium. The viewing direction of the 
orthographic camera is normal to the air-medium interface to avoid refractions of incoming camera rays. 
However, refraction of light rays from the source must be modeled. Then, the total irradiance E at the 
camera can be written as the sum of the irradiances Emed and Esurf : -ß(ds+dv-ßds (1 - e E = L0 [e ) 
. n . s + P(g, a) e -ßdv )] . (13) For an isotropic point source, L0 = I0/d2 . Equation s 13 represents 
the image formation model for one distant source. Similar equations can be written for each distant source 
that illuminates the scene. 4.2 Experimental Setup and Calibration The glass tank described in Section 
3 is again used in these experiments and, as before, we place the camera and the sources outside the 
tank. A 12-bit per channel Canon EOS-20D camera with a 70 - 300 mm zoom lens is placed 20 feet away from 
the tank and observes the front face of the tank normally (perpendicularly). The .eld of view occupied 
by the tank in the image is 2.0 degrees and is hence approximately orthographic. During calibration, 
refraction of the light rays from sources at the air-medium boundary must be accounted for. Figure 8 
shows a schematic of the side view of the setup. The distances ds and dv are related using trigonometry, 
dv = ds cos a. (14) Notice that the light rays that illuminate the viewing ray and the surface travel 
di.erent distances in the medium (compare the lengths of the dashed parallel rays in .gures 7 and 8). 
Hence, the assumption in simplifying equation 11 that x is constant with respect to y, becomes invalid 
for our experimental setup. So, an appropriate correc­tion is derived for Emed usingequation 14toobtain 
the irradiance (see appendix A): -ßdv(1+1/ cos a) E = L0 e. n . s + L0 P(g, a)cos a (1 - e -ßdv(1+1/ 
cos a)) . (15) 1+cos a We will henceforth call equation 15 as the image forma­tion model. We calibrate 
our setup using images of a white lambertian sphere in pure water (scattering is minimal). The brightest 
point on the sphere yields the refracted di­rection s (and a) and intensity L0 of the source. 4.3 Scene 
and Medium Recovery Consider a set of images taken of an object under dif­ferent source directions. In 
order to .nd out how many source directions are required to recover the scene and the medium, let us 
count the number of knowns and un­knowns in equation 15. Recall that as part of calibration, the angle 
a, the source direction s and intensity L0 are all estimated apriori. Then, the unknowns for each scene 
point are the surface albedo ., unit normal n,and optical thickness Tv = ßdv . The medium parameter g 
in the ex­pression for P(g, a) (see equation 1) is constant and hence is a global unknown. Thus, there 
are four unknowns for each scene point and one global unknown. If there are P scene points and L light 
source directions, the number of unknowns 4P + 1 must be less than the number of equa­tions PL. So, simple 
variable counting suggests that a minimum of L = 5 is required6 . To empirically verify that indeed L 
= 5 su.ces (assum­ing the sources are not in degenerate positions), we per­formed numerical simulations 
on 4000 randomly gener­ated combinations of source directions si, surface normals n,albedos . . (0, 1), 
optical thicknesses Tv . (0, 2) and forward scattering parameters g . (-1, 1), for a single scene point. 
The MatlabTM function fminsearch was used to recover the unknowns by minimizing the sum of squared di.erences 
between the simulated values and the model in equation 15. In all trials, the search was initial­ized 
with random values for the unknowns. In all cases, the search algorithm converged to the global optimum 
so­lution within few seconds. This suggests the presence of a single global minimum of the error function 
7.Asa test of robustness, we added uniform random noise (up to 5% of the simulated values) and found 
that the errors in recovered unknowns were low, as evidenced by the error histograms in .gure 10. We 
also ran the above simula­tions using only 4 sources, but the global error minimum corresponded to several 
parameter sets, suggesting that 4 sources are insu.cient for unique estimation. Thus, we conclude that 
.ve non-degenerate light source directions are required and su.cient to uniquely estimate the prop­erties 
of the scene and the medium. In practice, however, more source directions may be used for robustness. 
The experiments performed with our setup are shown in .gure 9. Images of a teapot captured in the presence 
of scattering (by dilute milk) have poor contrast and col­ors. As expected, applying traditional photometric 
stereo results in poor results. On the other hand, the surface normals and the albedos obtained using 
our method8 are 6In appendix B, we present an interesting but practically limited case where a unique 
linear solution with four sources is possible. 7However, the error function does contain local minima 
and the search was conducted starting from several (typically 100) initial guesses and the minimum of 
all the solutions was used. 8The non-linear optimization can be executed independently for each pixel. 
But to speedup execution time, we masked the region where the object is not present (Esurf = 0)to.rstestimate 
the global parameter g, before estimating the 4 parameters for each pixel. As a side note, from our experience, 
it is critical to use high quality (12 bits per channel HDR) radiometrically linear input im­ages in 
order to obtain good reconstructions shown. Proceedings of the Tenth IEEE International Conference on 
Computer Vision (ICCV 05) 1550-5499/05 $20.00 &#38;#169; 2005 IEEE Pure Water (No scattering) -Ground 
Truth  (a) Images (2 out of 8) captured in pure water.  (b) Albedo and shape computed using traditional 
method Dilute Milk (medium scattering)  (c) Input Images (2 out of 8) captured in dilute milk (d) Albedo 
and shape computed using traditional method (e) Albedo and shape computed using our method   Figure 
9: Experimental results of Photometric Stereo in Scattering Media. (a) Two (out of eight) images of a 
teapot acquired under di.erent lighting directions (depicted in the insets). (b) Results on applying 
traditional photometric stereo to images in (a) serve as ground truth. (c) The images acquired in dilute 
milk. Notice the signi.cant loss of contrast. (d) If traditional photometric stereo applied to images 
in (c), the 3D shape obtained is very .at and the scattering e.ects are absorbed by the albedos. (e) 
The results obtained using our algorithm. The percentage RMS error in reconstructing the shape was 6.3%. 
In addition to surface normals and albedos, our method also yields a depth map, which is not possible 
using traditional photometric stereo (see .gure 10.) The  3D shapes were computed from the surface normals 
using [8]. very accurate (with only a percentage RMS error of 6.3% in computed shape). In addition, our 
method also yields a depth map of the scene Tv = ßdv, which is not possible using traditional photometric 
stereo. 5 Discussion: Real Underwater Scenarios Since all the experiments in this paper were done using 
milk as the scattering medium, it is important to identify issues that may arise in real underwater scenarios 
(lakes, oceans, seas). In initial experiments, we collected water samples from 4 di.erent locations near 
a Paci.c Ocean beach in San Diego, USA. These samples were collected close to the water surface where 
the impurity levels are generally high. By matching images of these water sam­ples with those of milk, 
we found that the low to moder­ate concentration range of milk used in this paper corre­sponds to the 
concentrations abserved in the ocean waters. Hence, our algorithms can be applied to many underwater 
scenarios as well. In the future, to make our techniques broadly applicable, we wish to develop underwater 
de­ployable systems and improve our algorithms to handle non-homogeneous and dynamic underwater media. 
References [1] R. D. Ballard. The Discovery of the Titanic.Warner Books, 1988. [2] R. Basri and D.W. 
Jacobs. Photometric stereo with general, unknown lighting. In CVPR, 2001. [3] F. M. Caimi, D. M. Kocak, 
and V. L. Asper. Developments in laser-line scanned undersea surface mapping and image anal­ysis systems 
for scienti.c applications. In Proc. MTS/IEEE Oceans, 1996. [4] S. Chandrasekhar. Radiative Transfer. 
Dover Publications, Inc., 1960. [5] P. C. Y. Chang, J. C. Flitton, K. I. Hopcraft, E. Jakeman, D. L. 
Jordan, and J. G. Walker. Improving visibility depth in passive underwater imaging by use of polarization. 
App. Opt., 42 (15), 2003.  [6] S. Y. Chen and Y. F. Li. Self-recalibration of a colour­encoded light 
system for automated 3-d measurements. Mea­sureSciTech, 14(1), 2003. [7] G.R. Fournier, D. Bonnier, J.L. 
Forand, and P.W Pace. Range­gated underwater laser imaging system. Opt. Eng.,32 (9), 1993. [8] R.T. Frankot 
and R. Chellappa. A method for enforcing in­tegrability in shape from shading algorithms. PAMI, 10(4), 
1988. [9] E. Fuchs and J. S. Ja.e. Thin laser light sheet microscope for microbial oceanography. OPTICS 
EXPRESS, 10 (2), 2002. [10] G. D. Gilbert and J. C. Pernicka. Improvement of underwater visibility by 
reduction of backscatter with a circular polariza­tion technique. Applied Optics, 6 (4):741 746, 1967. 
[11] M.D. Grossberg and S.K. Nayar. The raxel imaging model and ray-based calibration. IJCV, 61(2), 2005. 
[12] A. Hertzmann and S.M. Seitz. Shape and materials by example: a photometric stereo approach. In CVPR, 
2003. [13] D.Q. Huynh, R.A. Owens, and P.E. Hartmann. Calibrating a structured light stripe system: A 
novel approach. IJCV, 33(1), 1999. [14] J. S. Ja.e, J. McLean, M. P. Strand, and K. D. Moore. Un­derwater 
optical imaging: Status and prospects. Tech. Report, Scripps Institution of Oceanography, La Jolla, 2002. 
[15] C. Je, S.W. Lee, and R.H. Park. High-contrast color-stripe pattern for rapid structured-light range 
imaging. In ECCV, 2004. [16] D. M. Kocak, F. M. Caimi, T. H. Jagielo, and J. Kloske. Laser projection 
photogrammetry and video system for quanti.cation and mensuration. MTS/IEEE Oceans, 2002. Proceedings 
of the Tenth IEEE International Conference on Computer Vision (ICCV 05) 1550-5499/05 $20.00 &#38;#169; 
2005 IEEE   Depth map computed from images in Figure 9c. [17] M. Levoy, B. Chen, V. Vaish, M. Horowitz, 
I. McDowall, and M. Bolas. Synthetic aperture confocal imaging. In SIG-GRAPH, 2004. [18] S.G. Narasimhan 
and S.K. Nayar. Vision and the atmosphere. IJCV, 48(3). [19] P. Naulleau and D. Dilworth. Motion-resolved 
imaging of mov­ing objects embedded within scattering media by the use of time-gated speckle analysis. 
App. Opt., 35 (26), 1996. [20] Y. Sato, H. Kitagawa, and H. Fujita. Shape measurement of curved objects 
using multiple slit-ray projections. PAMI, 4(6), 1982. [21] D. Scharstein and R. Szeliski. High-accuracy 
stereo depth maps using structured light. In CVPR, 2003. [22] Y. Y. Schechner and N. Karpel. Attenuating 
natural .icker patterns. In Proc. MTS/IEEE Oceans, 2004. [23] Y. Y. Schechner and N. Karpel. Clear underwater 
vision. In Proc. CVPR, 2004. [24] J. S. Tyo, M. P. Rowe, Jr. E. N. Pugh, and N. Engheta. Target detection 
in optically scattering media by polarization­di.erence imaging. App. Opt., 35 (11), 1996. [25] J. G. 
Walker, P. C. Y. Chang, and K. I. Hopcraft. Visibility depth improvement in active polarization imaging 
in scattering media. App. Opt., 39 (27), 2000. [26] D. Walther, D. R. Edgington, and C. Koch. Detection 
and tracking of objects in underwater video. In Proc. CVPR, 2004. [27] Authors Website. http://www.cs.cmu.edu/~srinivas/. 
[28] P.M. Will and K.S. Pennington. Grid coding: A preprocessing technique for robot and machine vision. 
AI, 2, 1971. [29] R.J. Woodham. Photometric method for determining surface orientation from multiple 
images. OptEng, 19(1), 1980. [30] L. Zhang, B. Curless, and S. M. Seitz. Rapid shape acquisition using 
color structured light and multi-pass dynamic program­ming. In The 1st IEEE International Symposium on 
3D Data Processing, Visualization, and Transmission, 2002. A: Refractions in Photometric Stereo The expression 
for Esurf is the same as equation 10 with ds being substituted from equation 14. The irradiance Emed 
is derived by observing that x = y cos a and dv = ds cos a: dv dv -ßx -ßy -ß(1+1/ cos a)yEmed = L0 eß 
P(g, a) e dy = L0 ß P(g, a) e dy 00 L0 P(g, a)cos a -ß(1+1/ cos a)dv ) = (1 - e 1+ cos a which is rewritten 
in equation 15. Note that we have assumed here that the actual source outside the medium is still distant 
and does not show an inverse square fall-o. within the medium since the distance light traveled within 
the medium is much less compared to the distance of the source from the medium boundary. Thus, L0 is 
constant with respect to ds. B: Linear Solution with Four Sources We assume that (a) the sources and 
the camera are both 1 immersed in an isotropic medium (P(a)= ), and (b) 4p the sources are equidistant 
from any given surface point (ds same for all sources, however, ds mayvaryfor di.erent scene points). 
Although these assumptions limit practi­cality, this case serves as an illustration in comparison to 
the technique without any scattering. Let the intensities observed at a pixel under 4 di.erent sources 
be E1, E2, E3,and E4 . From equation 13: L0 Ei = L0 e -ß(dsi+dv) . n . si + e -ßdsi (1-e -ßdv ) , (16) 
4p where, the subscript i denotes the source. For this special case, ds1 = ds2 = ds3 = ds4 = ds . The 
second term in equation 16 can be eliminated by considering pair-wise di.erences between intensities 
to obtain a set of equations that are linear in the surface normal n: E1 - E2 s1 - s2 -ß(ds+dv) . E1 
- E3 = L0e s1 - s3 n . (17) E1 - E4 s1 - s4 In matrix and vector notation, we rewrite equation 17, -ß(ds+dv 
.E = L0e ) . .S n , (18) where, .E and n are 3 × 1 column vectors and .S is a 3 × 3 matrix. By applying 
.S-1, we obtain the surface -ß(ds+dv normal n and the attenuated appearance L0e) .. Thus, all the terms 
except for ßds and ßdv are estimated or known in equation 16. If a relationship between ds and dv is 
known (such as equation 14), then ßds and ßdv can be estimated using equation 16 and in turn the albedo 
. -ß(ds+dv can be estimated from the product L0e) .. Thus, in the case of isotropic scattering and equidistant 
sources, four light source directions are required to compute sur­face normals, albedos and optical thickness 
ßdv (or scaled distance). Proceedings of the Tenth IEEE International Conference on Computer Vision 
(ICCV 05) 1550-5499/05 $20.00 &#38;#169; 2005 IEEE  OnControllingLightTransportinPoorVisibilityEnvironments 
 Mohit Gupta, Srinivasa G. Narasimhan Yoav Y. Schechner Carnegie Mellon University Technion -Israel Institute 
of Technology School of Computer Sc., Pittsburgh15232, USA Dep. of Electrical Eng., Haifa 32000, Israel 
{mohitg, srinivas}@cs.cmu.edu yoav@ee.technion.ac.il Abstract  Poor visibility conditions due to murky 
water, bad weather, dust and smoke severely impede the performance of vision systems. Passive methods 
have been used to restore scene contrast under moderate visibility by digital post­processing. However, 
these methods are ineffective when the quality of acquired images is poor to begin with. In this work, 
we design active lighting and sensing systems for con­trolling light transport before image formation, 
and hence obtain higher quality data. First, we present a technique of polarized light striping based 
on combining polarization imaging and structured light striping. We show that this technique out-performs 
different existing illumination and sensing methodologies. Second, we present a numerical ap­proach for 
computing the optimal relative sensor-source po­sition, which results in the best quality image. Our 
analysis accounts for the limits imposed by sensor noise. 1.Introduction Computer vision systems are 
increasingly being de­ployed in domains such as surveillance and transportation (terrestrial, underwater 
or aerial). To be successful, these systems must perform satisfactorily in common poor visi­bility conditions 
including murky water, bad weather, dust and smoke. Unfortunately,images capturedin these condi­tions 
show severe contrastdegradationandblurring, making ithardtoperform meaningful scene analysis. Passive 
methodsfor restoringscene contrast[14,18,22] and estimating 3D scene structure[4,12,28]rely onpost­processing 
based on the models of light transport in nat­ural lighting. Such methods do not require special equip­ment 
and are effective under moderate visibility[12], but are of limited use in poor visibility environments. 
Very of­ten,thereis simply not enough useful sceneinformationin images. For example, in an 8-bit camera, 
the intensity due to dense fog might take up 7 bits, leaving only 1 bit for scene radiance. Active systems, 
on the other hand, give us .exibility in lighting and/or camera design, allowing us to control the light 
transport in the environmentfor better im­age quality. Figure 1 illustrates the signi.cant increase in 
image quality using our technique. In this experiment, the Figure 1. Polarized light striping versus 
.ood-lighting. In this ex­periment, the scene is comprised of objects immersed in murky water. Using 
the polarized light striping approach, we can control the light transport before image formation for 
capturing the same scene with better color and contrast. High-resolution images can bedownloaded from 
theproject web-page[7]. scene comprised of objectsimmersedin murky water. Whilepropagatingwithin a medium 
suchas murkywater orfog,lightgets absorbed and scattered. Broadly speaking, lighttransport[2]canbe classi.edbased 
onthree speci.c pathways:(a) fromthelight sourcetothe object,(b) from the objecttothe sensor and(c) fromthelight 
sourcetothe sensor without reaching the object(seeFigure 2). Ofthese, the third pathway causes loss of 
contrast and effective dy­namic range(for example, thebackscatter of car headlights infog), andis thus 
undesirable. We wishtobuildactiveillumination and sensing systems that maximize light transport along 
the .rst two pathways while simultaneously minimizing transport along the third. To this end, we exploit 
some real world observations. For example, while driving in foggy conditions, .ood-lighting the road 
ahead with a high-beam may reduce visibility due to backscatter. On the other hand, underwater divers 
real­ize that maintaining a good separation between the source and the camera reduces backscatter, and 
improves visibil­  (a) (b) (c) (d) Figure 2. Light transport in scattering media for different source 
and sensor con.gurations. (a) Illustration of the three light transport components.(b)Thebackscatter 
B reduces theimage contrast. The amount ofbackscatterincreases with the commonbackscatter volume. (c)Bychanging 
therelativeplacement of thesensor and source, wecanmodulatethelighttransportcomponentsforincreasing theimage 
contrast.(d)The common backscatter volume can be reduced by usinglight stripe scanning as well. ity[21,9].Polarization 
.ltershavealsobeenusedtoreduce contrastlossduetohaze and murky water[20,24,19,6]. Based on these observations, 
we attempt to address twokey questions. First, which illumination and sensing modality allows us to modulate 
the three light transport pathways mosteffectively?Second,whatisthe optimal placement of the source and 
the sensor? Thispaperhas two main con­tributions: (1)Wepresentan activeimagingtechnique called polar­ized 
light striping and show thatitperformsbetter thanpre­vious techniques such as .ood-lighting, unpolarized 
light striping[10,15,9], andhighfrequencyilluminationbased separation oflight transport components[16]. 
(2) We derive a numerical approach for computing the optimal relative sensor-source position in poor 
visibility conditions. We consider a variety of illumination and sens­ing techniques, while accountingforthelimitsimposedby 
sensor noise. Our model can be used forimproving visibil­ity in different outdoor applications. It is 
useful for tasks such asdesigningheadlightsfor vehicles(terrestrial and un­derwater). We validate our 
approachin real experiments. 2.How toIlluminate andCapture theScene? In this section, we present an active 
imaging technique: polarized light striping. We also analyze the relative mer­its ofdifferent existing 
techniques,and showthatpolarized light striping outperformsthem. While propagating through a medium, 
light gets ab­sorbed and scattered (Figure 2). The image irradiance at a particular pixel is given as 
a sum of the three compo­nents,thedirect signal(D),theindirect signal(A)and the backscatter(B): E(x,y)= 
D(x,y)+ A(x,y)+ B(x,y) . (1) " v- ' "v- ' Signal Backscatter The total signal S is S(x,y)= D(x,y)+ A(x,y) 
. (2) ExperimentalSetup KodakContrastChart Figure 3. Our experimental setup consisting of a glass tank, 
.lled with moderatetohigh concentrations of milk(fourtimes asthose in[15]). AnLCDprojectorilluminates 
the medium withpolarized light. The camera(with apolarizer attached) observes a contrast chart through 
the medium. ThebackscatterB degrades visibilityanddepends onthe opticalpropertiesof the mediumsuch asthe 
extinctionco­ef.cient and the phase function. The direct and the indi­rect components(D and A) depend 
on both the object re­.ectance and the medium. Our goal is to design an active illumination and sensing 
system that modulates the compo­nents oflight transport effectively. Speci.cally, we want to maximizethe 
signal S, while minimizingthebackscatter B. We demonstrate the effectiveness of different imaging techniques 
in laboratory experiments. Our experimental setup consists of a 60 × 60 × 38 cm3 glass tank .lled with 
dilute milk (see Figure 3). The glass facades are anti­re.ection coated to avoid stray re.ections.1 The 
scene con­sists of objects immersed in murky water or placed behind theglass tank. Aprojectorilluminates 
the scene and a cam­era .tted with apolarizerobservesthescene.WeuseaSony VPL-HS51A, Cineza 3-LCD video 
projector. The red and thegreenlight emittedfromtheprojector areinherentlypo­larized channels. If we 
want to illuminate the scene with bluelight, weplace apolarizerinfrontof theprojector. We use a 12-bit 
Canon EOS1D Mark-II camera, and a Kodak contrast chart as the object of interest to demonstrate the contrastloss 
or enhancementfordifferenttechniques. 1Imaging into a medium through a .at interface creates a non-single 
viewpoint system.Theassociateddistortions areanalyzedin[25].  (a)Maximumimage (b)Global component (c)Directcomponent 
(d)Directcomponent(lowfreq) Figure 4. Limitations of the high frequency illumination based method. A 
shifting checkerboard illumination pattern was used with the checker size of 10 × 10 pixels.(a)Maximumimage(b)Minimumimage(global 
component)(c)Directcomponent(d)Directcomponent obtained usinglowerfrequency illumination(checker size 
of 20 × 20 pixels). Thedirect component imageshavelowSNRin thepresence of moderatetoheavy volumetricscattering.Theglobalimageisapproximately 
thesameasa .ood-litimage,andhence,suffersfromlow contrast. This experiment was conducted in moderate 
scattering conditions, same as the second row ofFigure 6.  Figure5.Therelativedirectcomponent of thesignal 
reduceswith increasing optical thickness of the medium. This plot was calcu­lated using simulations, 
with a two-term Henyey-Greenstein scat­teringphase function[8]for aparameter value of0.8. High-frequency 
illumination: Ref. [16] presented a technique to separatedirect andglobal components oflight transport 
usinghighfrequencyillumination, withgood sep­aration results for inter-re.ections and sub-surface scatter­ing. 
Whathappensin the case oflight transportin volumet­ric media? Separation results in the presence of moderate 
volumetric scattering are illustrated in Figure 4. The direct componentis thedirect signal(D), whereastheglobalcom­ponentis 
the sum ofindirect signal(A)and the backscatter (B), as showninFigure2. Thus, this method seeks thefol­ 
lowingseparation: E(x,y) = D(x,y) " v-' Direct + A(x" ,y) + B(xv-Global ,y) ' . (3) However, to achieve 
the best contrast, we wish to sep­arate the signal D + A from the backscatter B. As the mediumbecomes 
more strongly scattering,the ratio D falls S rapidly due to heavy attenuation and scattering, as illus­tratedinFigure 
5.Thisplot was estimated using numerical simulations using the single scattering model oflight trans­port.Consequently, 
for moderate to high densities of the 2With multiple scattering, the ratio falls even more sharply. medium,thedirectimagesuffersfromlow 
signal-to-noise­ratio(SNR), as showninFigure 4. Further, theindirect sig­nal(A)remainsunseparatedfromthebackscatter 
B, in the global component. Thus, the global image is similar to a .ood-litimage, and suffersfromlow 
contrast. Polarized .ood-lighting: Polarization imaging has been used toimproveimage contrast[19,23,6]inpoor 
visibility environments. Itisbased on theprinciple that thebackscat­ter componentispartiallypolarized,whereasthe 
scene radi­anceis assumed tobe unpolarized. Using a sensor mounted with apolarizer,twoimagescanbetakenwith 
two orthog­onal orientations of thepolarizer: D+ AB(1 -p) Eb =+ (4) 22 D+ AB(1 + p) Ew =+ , (5) 22 where 
pis thedegree ofpolarization(DOP)ofthebackscat­ter. Here, Eb and Ew are the best-polarized image and 
the worst-polarizedimage ,respectively.Thus,using opti­cal .ltering alone,backscattercanberemovedpartially,de­pending 
on the value of p. Further, it is possible to recover an estimate of the signal S in apost-processingstep[19]: 
S = Eb 1 + 1 + Ew 1 - 1 . (6) p p However, in optically dense media, heavy backscatter dueto .ood-lightingcandominatethesignal,makingitim­possible 
for the signal to be recovered. This is illustrated inFigure 6,whereinthecaseof .ood-lighting underheavy 
scattering,polarizationimagingdoes notimprove visibility. Light stripe scanning: Here, a thin sheet of 
light is scanned across the scene. In comparison to the above ap­proaches, the common backscatter volume 
is considerably Flood-Lighting PolarizedFlood-Lighting LightStripeScanning PolarizedLightStriping (a) 
(b) (c) (d) (e) (f) (g) (h)   (i) (j) (k) (l) Figure 6. Comparison of various illumination and sensing 
techniques (zoom into the marked areas to better assess the image quality). Flood-litimages sufferfrom 
a severeloss of contrast, speciallyin thepresence ofheavy scattering(a,b). Polarized light striping achieves 
a signi.cantincreaseinimagecontrast,eveninthepresenceofheavy scattering(a-d).Inmoderatescattering, .nedetails(text) 
arerecovered more reliablyin(g) and(h), as compared to(e). See(i),(j),(k) and(l)for close-ups of the 
marked areasin(e),(f),(g) and(h) respectively. The moderate scattering experiment was conducted under 
the same conditions as the experiment inFigure 4. reduced(seeFigure 2d). Thesheet oflightintersectsthe 
object to create a stripe thatisdetected using agradient op­erator.3 All stripes are then mosaiced to 
create a composite image CI [10, 15, 9]. Alternatively, the composite image can be obtained by simply 
selecting the maximum value at eachpixel over all theindividuallight stripeimagesSIk: CI(x,y)= maxk{SIk(x,y)}. 
(7) Polarized light striping: We propose polarized light striping as atechniquethat combinestheadvantagesofpo­larization 
imaging and light striping, and thus, is applica­ble for an extended range of medium densities. Earlier, 
we demonstrated that light striping reduces the amount of backscatter. However, reliable localization 
of the object stripes(by usinggradient operator or by selecting the max­imum pixel value, as in Eq. 7)is 
severely impeded due to strongbackscatter. ThisisillustratedinFigure 7. To enable reliable detection 
of the object stripes even in 3In our particular implementation, the projector illuminates a single 
thepresence of strong scattering, we usepolarizationimag­ plane and has low power. We compensate for 
this by increasing the ex­posure time of the camera. ing in conjunction with light striping. A high DOP 
of the (a) (b) (c) (d) Figure7.Unpolarized versuspolarizedlightstripescanning.(a)Raydiagramillustratinglight 
stripescanning, adaptedfrom[15].(b)The camera observes alight stripe(1 out of 30)without a polarizer. 
The visible light plane is the backscatter and impedes reliable detection of the object stripe. (c)Through 
a polarizer, there is a considerable reduction in backscatter. The light plane-object intersection becomes 
moredistinct,thusenabling itsreliabledelineation.(d)Theremovedbackscatter(difference of(b) and(c)).Videoof 
acomplete scancan bedownloaded from theproject web-page[7]. backscatter is essential for removing the 
backscatter using polarization.ltering(Eq.4), orto recovera reliable estimate ofthesignal usingpost-processing(Eq. 
6). Inourexperi­ments,the camera observesthe scenethrough apolarization .lter and thelight sheetsirradiatingthe 
scene arepolarized. Sincetheincidentilluminationis completelypolarized,the DOP of thebackscatterishigh(seeappendix).Thisresults 
in a signi.cant reductioninthe amount ofbackscatter, and thus,enables reliabledetection ofthe stripes.4 
Thisis shown inFigure 7. We compare the results ofpolarizedlight strip­ing versus previous illumination 
and sensing techniques in Figure 6. Notice especially the differences in the contrast under strong scattering. 
Another resultis showninFigure 1. 3.OptimalCamera-SourcePlacement Conventional wisdomfromthe underwaterimaginglit­erature 
suggests maximizingthe sensor-source separationto reduce the backscatter, and hence, increase the image 
con­trast[9,21](seeFigure2). However,thisdoes not takeinto account the limitations posed by measurement 
noise. In­deed,placing the source and the sensor far from each other or the scene results in strong attenuation 
oflight, and a low SNR. In this section, we investigate this trade-off between image contrast and SNR 
to compute the optimal relative sensor-sourcepositions. 3.1.QualityMeasures In order to formalize the 
notion of optimal , we de.ne various image quality measures for different imaging and illumination techniques. 
These quality measures serve as objectivefunctionswhich canbemaximized to .nd theop­timalplacement of 
the source and the camera. ContrastQuality Measure: A major goal of an imaging systemis to maximize theimage 
contrast. Analogous to[1, 4Polarization imaging was previously used with phase-shifted struc­turedilluminationforimproved 
reconstruction of translucent objects[3]. 5], we de.ne the contrast quality measure, CQM(x,y) as the 
ratio of the signal S(x,y) to the totalintensity E(x,y): S CQM(x,y,p)= · (8) S + B(1 -p) This measure 
takes polarization imaging into account by de.ning the total intensity as that of the best polarized 
im­age,asinEq.(4). Intheabsenceofapolarizer, p =0. Delineation of light plane-scene intersection: Success 
of light striping in scattering media relies on reliable delin­eation of the object stripe. One schemeis 
todetect abright­ness discontinuity in the intensity pro.le across the stripe edge. Thus, for a light 
stripe scanning system, we de.ne a gradient quality measure (GQM) along the edge of the stripe in terms 
of the strength of gradient across the stripe edge. ConsiderFigure 8c; since the scenepoint O' does not 
have the direct component D or the backscatter component B(1 - p), the normalized difference in intensity 
of O and O' isgiven as: D+ B(1 -p) GQM(x,y,p)= · (9) D+ A+ B(1 -p) SNR dependent weighting: An image 
with high contrast butlow overallintensitymay resultin alowSNR, andhence be oflimiteduse. Thus,wede.ne 
anSNRdependentweight W as a monotonically increasing function of the total sig­nal value S. Thequality 
measures(CQM andGQM) are weighted by W so that signal values in the low SNR range are penalized. For 
example, W can be a linear function of S.Formore .exibilty,weuseasigmoidfunctionof S: 1 W(x,y)= , (10) 
-( S-µ z ) 1+ e where µ is the shift and z is the steepness of the sigmoid. For example, µ can be the 
dark current offset. Similarly, if the noise is derived from a Gaussian distribution, z can be the standard 
deviation. In addition, we should account for the effect ofpost-processing onimage noise[24,17].  (a) 
(b) (c) Figure 8. Simulating image formation for .nding the optimal sensor-source con.guration. (a)A 
schematic view of the volume. We use a pointlight source(L) and apinhole camera(C).The objectisLambertian, 
with re.ectance R.(b) We calculate D, A and B according to Eqs.(11-13). (c)Inthe case oflight striping,thepoint 
O ' is not getting directly irradiated by the source. Also, the viewing ray from O ' does not intersect 
the common backscatter volume. Thus, the direct component and the backscatter component at O ' are null. 
This results inabrightnessgradient acrossthestripeedge.Thestrength of thegradientisgivenbyEq. 9. 3.2.Simulations 
Consider an underwater scenario where a remote oper­ated vehicle(ROV) wantsto captureimages at agivendis­tance. 
Given an approximate estimate of the object albedo, medium scatteringparameters[13]and sensor noise, 
we can simulate theimageformationprocess. Toillustrate the con­cept, we simulate the image formation 
process for our ex­perimental setup. The Lambertian object re.ectance was assumed to be 0.6. For different 
source-camera con.gu­rations, we compute the appropriate quality measure de­scribed above. Then, the 
optimal con.guration is the one that maximizes thequality measure. Figure8illustratestheimageformationgeometry. 
In our experiments and simulations, the scene and camera remain .xed, while the source is moved to vary 
the sensor-source separation dLC. Point O onthe objectisbeing observedby the camera. Points X and Y are 
in the medium. The dis­tances dLO,dCO,dLX,dXO,dCO,dLY and dYC, and the angles f,a,.,. are as illustratedinFigure 
8. To keep our simula­tions simple, we assume a single scattering model of light transport and ahomogeneous 
medium. Theindividual com­ponents oflight transportare thengivenby: I0 -s(dLO+dCO)R(f) D = e (11) d2 
LO I0 A = e -s(dLX+dXO+dCO)F(a)R(.)dV (12) d2 V LX C I0B = e -s(dLY+dYC)F(.)dY , (13) d2 O LY where 
I0 is the source radiance, s is the extinction coef­.cient, R is the Lambertian object re.ectance, F 
is the scattering phase function (we use the two-term Henyey­Greensteinfunction[8])and V is theilluminatedvolume. 
Polarized images, Eb and Ew are simulated according toEqs.(4-5). ThisrequiresknowledgeoftheDOP ofthe 
backscatter p. Using our experimental setup, we estimated p to be approximately 0.8, from the regions 
of the image without any object. We can also compute p analytically, given the dependence of the DOP 
of scattered light on the scattering angle, such asgivenin theAppendix. Optimal con.guration for .ood-lighting: 
Let us .nd the con.guration that is optimal in terms of both image contrast and noise. We plot the product 
of the CQM and W versus the sensor-source separation dLC (Figure 9a). The tradeoff between contrast and 
SNR results in a local maximum. Notice thatpolarizationimprovesimagequality as compared to unpolarized 
imaging. However, since the DOP (and hence, the amount of contrast enhancement) is similar for all sensor-source 
positions, the location of the peak remains the same. The curve for the ideal case of zero noise increases 
monotonically. However, for real world scenarios, where measurement noise places limits onthe sensor 
s abilities, ourapproach canyield an optimal placement. This is illustrated in Figure 9 (b-c). The image 
taken using the optimal separation (40 cms) has high contrast and low noise. On the other hand, notice 
the signi.cant noisein theimage taken using alarge separation (60cms). Optimizing the light stripe scan: 
The case of light stripe scanning is more interesting. Instead of illuminating the whole scene at once, 
we illuminate it using one sheet of lightat atime.Wewantto .ndtheoptimallight stripescan. Should we scan 
the scene (a) by rotating the source, (b) by translatingit,or(c) acombinationthereof? Toanswer this, 
we plot the product of the GQM and the W for our setup(Figure 10). We observedifferentoptimal separations 
fordifferent(3 out of30) stripelocations. Figure 10 (e) shows thehigh-contrastimage acquired using the 
results of the simulations. The camera and theprojectorwereplaced (d) (e) (a) (b) (c)   Sensor-Source 
Separation d (cm) LC (a)  (b)LargeSeparation (c)OptimalSeparation Figure 9. Optimal sensor-source con.guration 
for .ood-lighting. (a) Plot of CQM × W versus dLC for our experimental setup. The tradeoff between contrast 
and SNR results in a maximum. (b)Large separation(60cms) resultsinheavyimage noise(c)Op­timal separation(40 
cms) resultsin ahigh contrast,lownoiseim­age(zoomintothe marked area). Both theframes were captured with 
the same exposure time. at a smalldistancefromthefacadeof theglasstankin real experiments. By carefully 
choosing the light rays, we can simulate alight sourceand a sensorplaced ontheglassfa­cade, as assumed 
in the simulations. The optimal scan for polarizedlightstripingisthe same as unpolarizedlightstrip­ing,but 
resultsinbetterimagequality. 4.Discussion We studynew ways to controllight transportfor thepur­pose of 
capturing better quality data in poor visibility en­vironments. With existing techniques for measurement 
of medium scattering[13]andpolarizationproperties[27], our simulation-based approach can be used to adapt 
the illu­mination and sensing system in-situ. Post-processing ap­proaches are expectedto recover the 
scene when applied to theimages acquired using our system. Our analysisfocused Figure10.We can scan the 
scene(a)by rotating the source,(b)by translatingit, or(c) a combinationthereof. (d) PlotofGQM × W versus 
dLC fordifferent stripelocations O1,O2 and O3,for our setup. We can notice different optimal separations 
for these stripe locations. (e) A high contrast image resulting from the optimal light stripe scan designed 
using simulations. on a singledivergent source and a single camera. Itis worth extendingour analysisto 
multiple cameras and sources[11]. More broadly, we believe that better control of the light transportcanbe 
achievedwithgreater .exibilityin choosing illumination and viewing rays. Acknowledgments The authors 
thank Shahriar Negahdaripour for helpful discussions. This researchwas supportedinpartsbyGrants # ONR 
N00014-08-1-0330, NSF CAREER IIS-0643628, NSF CCF-0541307 and the US-Israel Binational Science Foundation(BSF) 
Grant#2006384. YoavSchechnerisa LandauFellow -supportedby theTaubFoundation.Yoav s workwas conductedintheOllendorffMinervaCenter. 
Min­ervaisfunded throughtheBMBF. References [1] F. M. Caimi, F. R. Dalgleish, T. E. Giddings, J. J. Shirron, 
 C. Mazel, and K. Chiang. Pulse versus CW laser line scan imaging detection methods: Simulation results. 
In Proc. IEEE OCEANS,pages 1 4,2007. 5  [2] S. Chandrasekhar. Radiative Transfer. Dover Publications, 
Inc.,1960. 1 [3] T. Chen, H. P. A. Lensch, C. Fuchs, and H.-P. Seidel. Po­larization and phase-shifting 
for 3D scanning of translucent objects. In Proc. IEEE CVPR,pages 1 8,2007. 5 [4] F. Cozman and E. Krotkov. 
Depth from scattering. In Proc. IEEE CVPR,pages 801 806, 1997. 1 [5] T.E.Giddings,J.J.Shirron,andA.Tirat-Gefen. 
EODES-3: An electro-opticimaging andperformanceprediction model. In Proc. IEEE OCEANS,2:1380 1387, 2005. 
5 [6] G.D.GilbertandJ.C.Pernicka.Improvement of underwater visibilityby reduction ofbackscatter with 
a circularpolariza­tion technique. Applied Optics,6(4):741 746, 1967. 2,3 [7] M. Gupta and S. G. Narasimhan. 
Light transport web-page. http://graphics.cs.cmu.edu/projects/LightTransport/. 1,5 [8] V. I. Haltrin. 
One-parameter two-term henyey-greenstein phase function for light scattering in seawater. Applied Op­tics,41(6):1022 
1028, 2002. 3,6 [9] J. Jaffe. Computer modeling and the design of optimal un­derwater imaging systems. 
IEEE Journal of Oceanic Engi­neering, 15(2):101 111, 1990. 2,4,5 [10] D.M.KocakandF.M.Caimi.Thecurrent 
art of underwater imaging with a glimpse of the past. MTS Journal, 39:5 26, 2005. 2,4 [11] M.Levoy,B.Chen,V.Vaish,M.Horowitz,I.McDowall,and 
M.Bolas. Synthetic aperture confocalimaging. ACM Trans. Graph.,23(3):825 834, 2004. 7 [12] S.G.Narasimhan.Modelsandalgorithmsforvisionthrough 
the atmosphere. In Columbia Univ. Dissertation,2004. 1 [13] S. G. Narasimhan, M. Gupta, C. Donner, R. 
Ramamoorthi, S.K.Nayar,andH.W.Jensen. Acquiring scatteringproper­ties of participating media by dilution. 
ACM Trans. Graph., 25(3):1003 1012, 2006. 6,7 [14] S. G. Narasimhan and S. K. Nayar. Contrast restoration 
of weatherdegraded images. 25(6):713 724, 2003. 1 [15] S. G. Narasimhan, S. K. Nayar, B. Sun, and S. 
J. Koppal. Structuredlightin scattering media. In In Proc. IEEE ICCV, pages 420 427, 2005. 2,4,5 [16] 
S. K. Nayar, G. Krishnan, M. D. Grossberg, and R. Raskar. Fast separation of direct and global components 
of a scene using high frequency illumination. ACM Trans. Graph., 25(3):935 944, 2006. 2,3 [17] Y. Y. 
Schechner and Y. Averbuch. Regularized image re­covery in scattering media. IEEE Trans. PAMI,29(9):1655 
1660, 2007. 5 [18] Y.Y.SchechnerandN.Karpel.Recovery of underwatervis­ibility and structure by polarization 
analysis. IEEE Journal of Oceanic Engineering, 30(3):570 587, 2005. 1 [19] Y. Y. Schechner, S. G. Narasimhan, 
and S. K. Nayar. Polarization-based vision through haze. Applied Optics, 42(3):511 525, 2003. 2,3 [20] 
W. A. Shurcliff and S. S. Ballard. Polarized Light, pages 98 103. VanNostrand,Princeton,N.J.,1964. 2 
[21] B.SkerryandH.Hall. Successful Underwater Photography. NewYork: Amphoto books, 2002. 2,5 [22] K. 
Tan and J. P. Oakley. Physics-based approach to color image enhancement in poor visibility conditions. 
JOSA A, 18(10):2460 2467, 2001. 1 [23] T.TreibitzandY.Y.Schechner. Instant3Descatter. In Proc. IEEE CVPR, 
volume2,pages 1861 1868, 2006. 3,8 [24] T. Treibitz and Y. Y. Schechner. Active polarization descat­tering. 
IEEE Trans. PAMI,To appear,2008. 2,5 [25] T. Treibitz, Y. Y. Schechner, and H. Singh. Flat refractive 
geometry. In Proc. IEEE CVPR,2008. 2 [26] H. van de Hulst. Light Scattering by Small Particles. Chap­ter5.Wiley,NewYork,1957. 
8 [27] K.J.VossandE.S.Fry.Measurementofthemuellermatrix for ocean water. Applied Optics,23:4427 4439, 
1984. 7,8 [28] S.ZhangandS.Negahdaripour.3D shaperecovery ofplanar and curved surfacesfrom shading cuesin 
underwaterimages. IEEE Journal of Oceanic Engineering, 27:100 116, 2002. 1 A.Degree ofPolarization ofScattering 
In this appendix, we study the dependence of the DOP of the scatteredlight,DOPB, on the scattering angle 
and the DOPof theincidentlight,DOPL. We consider onlythe ver­tical andhorizontalpolarized components 
oflinearlypolar­izedlight.Hence,weconsiderthe .rst 2 × 2 sub-matrix of thefull4×4 Mueller matrix. Polarizationproperties 
ofscat­teredlight canbe characterizedby theMueller matrix[26]: IB m11 m12 IL = , (14) QB m21 m22 QL where 
IL isthe sum, andQListhedifference ofthehorizon­tal and verticallypolarized componentsoftheincidentlight. 
Similarly,IB and QB arethe sum anddifferencerespectively Q of the scattered light. Note that DOP = I 
. Consequently, based onEq.(14): m21 + m22 DOPL DOPB = · (15) m11 + m12 DOPL Using the above equation 
and the measuredMueller ma­trix data for ocean water [27], we plot DOPB versus the scattering angle in 
Figure 11. For comparison, we also plot thebehaviorforRayleigh scattering. Forlow values of DOPL(naturallight), 
the curvequalitativelyresembles that ofRayleigh scattering. On the otherhand,for a completely polarizedsource(for 
example, anLCDprojector),the curve is .atter, with an average value of 0.8 for backscattering angles. 
Interestingly, this agrees with the observation made in[23]as well. International Journal of Computer 
Vision 48(3), 233 254, 2002 c . 2002 Kluwer Academic Publishers. Manufactured in The Netherlands. Vision 
and the Atmosphere SRINIVASA G. NARASIMHAN AND SHREE K. NAYAR Computer Science Department, Columbia University, 
New York, NY, USA srinivas@cs.columbia.edu nayar @cs.columbia.edu Received January 31, 2001; Revised 
October 16, 2001; Accepted December 4, 2001 Abstract. Current vision systems are designed to perform 
in clear weather. Needless to say, in any outdoor application, there is no escape from bad weather. Ultimately, 
computer vision systems must include mechanisms that enable them to function (even if somewhat less reliably) 
in the presence of haze, fog, rain, hail and snow. We begin by studying the visual manifestations of 
different weather conditions. For this, we draw on what is already known about atmospheric optics, and 
identify effects caused by bad weather that can be turned to our advantage. Since the atmosphere modulates 
the information carried from a scene point to the observer, it can be viewed as a mechanism of visual 
information coding. We exploit two fundamental scattering models and develop methods for recovering pertinent 
scene properties, such as three-dimensional structure, from one or two images taken under poor weather 
conditions. Next, we model the chromatic effects of the atmospheric scattering and verify it for fog 
and haze. Based on this chromatic model we derive several geometric constraints on scene color changes 
caused by varying atmospheric conditions. Finally, using these constraints we develop algorithms for 
computing fog or haze color, depth segmen­tation, extracting three-dimensional structure, and recovering 
clear day scene colors, from two or more images taken under different but unknown weather conditions. 
Keywords: physics based vision, atmosphere, bad weather, fog, haze, visibility, scattering, attenuation, 
airlight, overcast sky, scene structure, defog, dehaze 1. Computer Vision and the Weather the most magni.cent 
visual experiences known to man, including, the colors of sunrise and sunset, the blueness Virtually 
all work in computer vision is based on the of the clear sky, and the rainbow (see Minnaert (1954) premise 
that the observer is immersed in a transparent and Henderson (1977)). The literature on this topic has 
medium (air). It is assumed that light rays re.ected by been written over the past two centuries. A summary 
of scene objects travel to the observer without attenuation where the subject as a whole stands would 
be too am­or alteration. Under this assumption, the brightness of bitious a pursuit. Instead, our objective 
will be to sieve an image point depends solely on the brightness of a out of this vast body of work, 
models of atmospheric single point in the scene. Quite simply, existing vision optics that are of direct 
relevance to computational vi­sensors and algorithms have been created only to func-sion. Our most prominent 
sources of background mate­tion on clear days. A dependable vision system how-rial are the works of McCartney 
(1975) and Middleton ever must reckon with the entire spectrum of weather (1952) whose books, though 
dated, serve as excellent conditions, including, haze, fog, rain, hail and snow. reviews of prior work. 
The study of the interaction of light with the atmo-The key characteristics of light, such as its intensity 
sphere (and hence weather) is widely known as atmo-and color, are altered by its interactions with the 
atmo­spheric optics. Atmospheric optics lies at the heart of sphere. These interactions can be broadly 
classi.ed 234 Narasimhan and Nayar into three categories, namely, scattering, absorption and emission. 
Of these, scattering due to suspended particles is the most pertinent to us. As can be expected, this 
phenomenon leads to complex visual effects. So, at .rst glance, atmospheric scattering may be viewed 
as no more than a hindrance to an observer. However, it turns out that bad weather can be put to good 
use. The farther light has to travel from its source (say, a surface) to its destination (say, a camera), 
the greater it will be effected by the weather. Hence, bad weather could serve as a powerful means for 
coding and conveying scene structure. This observation lies at the core of our investigation; we wish 
to understand not only what bad weather does to vision but also what it can do for vision. Surprisingly 
little work has been done in computer vision on weather related issues. An exception is the work of Cozman 
and Krotkov (1997) which uses the scattering models in McCartney (1975) to com­pute depth cues. Their 
algorithm assumes that all scene points used for depth estimation have the same inten­sity on a clear 
day. Since scene points can have their own re.ectances and illuminations, this assumption is hard to 
satisfy in practice. Research in image processing has been geared to­wards restoring contrast of images 
degraded by bad weather. Note that bad weather effects depend strongly on the depths of scene points. 
Hence, simple contrast enhancement techniques such as histogram equaliza­tion and contrast stretching 
do not suf.ce here. Oakley and Satherley (1998) use separately measured range data and describe an algorithm 
to restore contrast of at­mospherically degraded images based on the principles of scattering. However, 
they approximate the distribu­tion of radiances in the scene by a single gaussian with known variance. 
Kopeika (1998) and Yitzhaky et al. (1998) restore image contrast using weather predicted atmospheric 
modulation transfer function and an a pri­ori estimate of the distance from which the scene was imaged. 
The goal of our work is to lay the foundation for interpreting scenes from one or more images taken un­der 
bad weather conditions. We discuss various types of weather conditions and their formation processes. 
We summarize two models of atmospheric scattering attenuation and airlight that are most pertinent to 
us. Using these models, we develop algorithms that re­cover complete depth maps of scenes without requiring 
any prior information about the properties of the scene points or atmospheric conditions. Next, we study 
the color effects of atmospheric scat­tering. A new model that describes the appearance of scene colors 
under bad weather is presented and ver­i.ed for fog and haze. Based on this color model, we develop several 
geometric constraints on scene-color changes, caused by varying atmospheric conditions. Using these constraints, 
we present methods to recover structure as well as clear day scene colors from im­ages taken under poor 
weather conditions. All of these methods only require changes in weather conditions and accurate measurement 
of scene irradiance, and not any prior information about the scene points or weather conditions. 2. Bad 
Weather: Particles in Space Weather conditions differ mainly in the types and sizes of the particles 
involved and their concentrations in space. A great deal of effort has gone into measur­ing particle 
sizes and concentrations for a variety of conditions (see Table 1). Given the small size of air molecules, 
relative to the wavelength of visible light, scattering due to air is rather minimal. We will refer to 
the event of pure air scattering as a clear day (or night). Larger particles produce a variety of weather 
conditions which we will brie.y describe below. Haze. Haze is constituted of aerosol which is a dis­persed 
system of small particles suspended in gas. Haze has a diverse set of sources including volcanic ashes, 
foliage exudation, combustion products, and sea salt (see Hidy (1972)). The particles produced by these 
sources respond quickly to changes in relative humidity and act as nuclei (centers) of small water droplets 
when the humidity is high. Haze particles are larger than air molecules but smaller than fog droplets. 
Haze tends to produce a distinctive gray or bluish hue and is certain to effect visibility. Table 1. 
Weather conditions and associated particle types, sizes and concentrations (adapted from McCartney (1975)). 
Condition Particle type Radius (µm) Concentration (cm-3) Air Haze Molecule Aerosol 10-4 10-2 1 1019 103 
10 Fog Cloud Rain Water droplet Water droplet Water drop 1 10 1 10 102 104 100 10 300 10 10-2 10-5 Vision 
and the Atmosphere 235 Fog. Fog evolves when the relative humidity of an air parcel reaches saturation. 
Then, some of the nuclei grow by condensation into water droplets. Hence, fog and certain types of haze 
have similar origins and an increase in humidity is suf.cient to turn haze into fog. This transition 
is quite gradual and an inter­mediate state is referred to as mist. While perceptible haze extends to 
an altitude of several kilometers, fog is typically just a few hundred feet thick. A practical distinction 
between fog and haze lies in the greatly reduced visibility induced by the former. There are many types 
of fog (ex., radiation fog, advection fog, etc.) which differ from each other in their formation processes 
(Myers, 1968). Cloud. A cloud differs from fog only in existing at higher altitudes rather than sitting 
at ground level. While most clouds are made of water droplets like fog, some are composed of long ice 
crystals and ice­coated dust grains. Details on the physics of clouds and precipitation can be found 
in Mason (1975). For now, clouds are of less relevance to us as we restrict ourselves to vision at ground 
level rather than high altitudes. Rain and snow. The process by which cloud droplets turn to rain is 
a complex one (Mason, 1975). When viewed up close, rain causes random spatial and tem­poral variations 
in images and hence must be dealt with differently from the more static weather con­ditions mentioned 
above. Similar arguments apply to snow, where the .akes are rough and have more complex shapes and optical 
properties (Koenderink and Richards, 1992; Ohtake, 1970). Snow too, we will set aside for now. 3. Mechanisms 
of Atmospheric Scattering The manner in which a particle scatters incident light depends on its material 
properties, shape and size. The exact form and intensity of the scattering pattern varies dramatically 
with particle size (Minnaert, 1954). As Figure 1. A particle in the path of an incident light wave abstracts 
and reradiates incident energy. It therefore behaves like a point source of light. The exact scattering 
function is closely related to the ratio of particle size to wavelength of incident light. (Adapted from 
Minnaert (1954)). seen in Fig. 1, a small particle (about 1/10 ., where . is the wavelength of light) 
scatters almost equally in the forward (incidence) and backward directions, a medium size particle (about 
1/4 .) scatters more in the forward direction, and a large particle (larger than .) scatters almost entirely 
in the forward direc­tion. Substantial theory has been developed to derive scattering functions and their 
relations to particle size distributions (Mie, 1908; Hulst, 1957; Chandrasekhar, 1960; Chu and Hogg, 
1968; Rensch and Long, 1970; Nieto-Vesperinas and Dainty, 1990). Figure 1 illustrates scattering by a 
single particle. Clearly, particles are accompanied in close proximity by numerous other particles. However, 
the average sep­aration between atmospheric particles is several times the particle size. Hence, the 
particles can be viewed as independent scatterers whose scattered intensities do not interfere with each 
other. This does not imply that the incident light is scattered only by a single particle. Multiple scatterings 
take place and any given particle is exposed not only to the incident light but also light scattered 
by other particles. A simple analogy is the inter-re.ections between scene points. In effect, multi­ple 
scattering causes the single scattering functions in Fig. 1 to get smoother and less directional. Now, 
consider the simple illumination and detection geometry shown in Fig. 2. A unit volume of scatter­ing 
medium with suspended particles is illuminated with spectral irradiance E(.) per cross section area. 
The radiant intensity I (.,.) of the unit volume in the direction . of the observer is (see McCartney 
(1975)): I (.,.) = ß(.,.)E(.), (1) where, ß(.,.)isthe angular scattering coef.cient. The radiant intensity 
I (.,.)isthe .ux radiated per unit solid angle, per unit volume of the medium. The irradi­ance E(.) is, 
as always, the .ux incident on the volume Figure 2. A unit volume of randomly oriented suspended particles 
illuminated and observed. 236 Narasimhan and Nayar per unit cross-section area. The total .ux scattered 
(in all directions) by this volume is obtained by integrating over the entire sphere: f(.) =ß(.)E(.), 
(2) where, ß(.) is the total scattering coef.cient. It rep­resents the ability of the volume to scatter 
.ux of a given wavelength in all directions. It is generally as­sumed that the coef.cient ß(.) is constant 
(homoge­neous medium) over horizontal paths. To satisfy this constraint, we will restrict ourselves to 
the case where the observer is at (or close to) ground level and is in­terested not in the sky but other 
objects on (or close to) ground level. Also, we will assume that the atmo­sphere is more or less homogeneous 
in the scene of interest. 3.1. Attenuation The .rst mechanism that is relevant to us is the at­tenuation 
of a beam of light as it travels through the atmosphere. This causes the radiance of a scene point to 
fall as its depth from the observer increases. Here, we will summarize the derivation of the atten­uation 
model given in McCartney(1975). Consider a collimated beam of light incident on the atmospheric medium, 
as shown in Fig. 3. The beam is assumed to have unit cross-sectional area. Consider the beam pass­ing 
through an in.nitesimally small sheet (lamina) of thickness dx. The fractional change in irradiance at 
location x can be written as: dE(x,.) =-ß(.) dx. (3) E(x,.)  By integrating both sides between the limits 
x =0 and x =d we get: -ß(.)d E(d,.) =Eo(.)e, (4) where, Eo(.) is the irradiance at the source (x =0). 
This is Bouguer s exponential law of attenuation (Bouguer, 1729). At times, attenuation due to scattering 
is ex­pressed in terms of optical thickness, T =ß(.)d. The utility of Bouguer s law is somewhat limited 
as it as­sumes a collimated source of incident energy. This is easily remedied by incorporating the inverse-square 
law for diverging beams from point sources: -ß(.)d Io(.)e E(d,.) = , (5) d2 where, Io(.) is the radiant 
intensity of the point source. This is Allard s law (Allard, 1876). (See Hardy (1967) for an analysis 
of the applicability of the inverse square criterion for sources of various sizes.) In deriving Allard 
s law, we have assumed that all scattered .ux is removed from the incident energy. The fraction of energy 
that remains is called direct transmis­sion and is given by expression (5). We have ignored the .ux scattered 
in the forward direction (towards the ob­server) by each particle. Fortunately, this component is small 
in vision applications since the solid angles sub­tended by the source and the observer with respect 
to each other are small (see Middleton (1949)). In the re­mainder of the paper, we refer to the terms 
direct trans­mission model and attenuation model interchangeably. Finally, in some situations such as 
heavy fog, the exponential law may not hold due to signi.cant mul­tiple scatterings of light by atmospheric 
particles. We will assume here that once light .ux is scattered out of a column of atmosphere (seen by 
a pixel, say), it does not re-enter the same column (or only an insignif­icant amount does). Multiple 
scattering can also cause blurring in the image of a scene. In other words, the .ux scattered out of 
an atmospheric column (visible to a pixel) enters another column (seen by a neighboring pixel). In this 
work, we do not model the blurring effects of multiple scattering. 3.2. Airlight A second mechanism causes 
the atmosphere to behave like a source of light. This phenomenon is called airlight (Koschmieder, 1924) 
and it is caused by the scatter­ing of environmental illumination by particles in the atmosphere. The 
environmental illumination can have several sources, including, direct sunlight, diffuse sky­light and 
light re.ected by the ground. While attenua­tion causes scene radiance to decrease with pathlength, airlight 
increases with pathlength. It therefore causes the apparent brightness of a scene point to increase with 
depth. We now build upon McCartney s (1975) derivation of airlight as a function of pathlength. Consider 
the illumination and observation geometry shown in Fig. 4. The environmental illumination along the observer 
s line of sight is assumed to be constant but unknown in direction, intensity and spectrum. In effect, 
the cone of solid angle d.subtended by a sin­gle receptor at the observer s end, and truncated by a physical 
object at distance d, can be viewed as a source of airlight. The in.nitesimal volume dV at distance x 
from the observer may be written as the product of the cross section area, d.x2, and thickness dx: dV 
=d.x2 dx. (6) Irrespective of the exact type of environmental illumi­nation incident upon dV , its intensity 
due to scattering in the direction of the observer is: dI (x,.) =dV kß(.) =d.x2 dxkß(.), (7) where, 
ß(.) is the total scattering coef.cient and the proportionality constant k accounts for the exact na­ture 
of the illumination and the form of the scattering function. Vision and the Atmosphere 237 If we view 
element dV as a source with intensity dI (x,.), the irradiance it produces at the observer s end, after 
attenuation due to the medium, is given by (5): -ß(.)x dI (x,.)e dE(x,.) = . (8) x2 We can .nd the radiance 
of dV from its irradiance as: -ß(.)x dE(x,.) dI (x,.)e dL(x,.) == . (9) d. d.x2 By substituting (7) 
we get dL(x,.) =kß(.)e-ß(.)x dx. Now, the total radiance of the pathlength d from the observer to the 
object is found by integrating this expression between x =0 and x =d: -ß(.)d L(d,.) =k 1 -e. (10) If 
the object is at an in.nite distance (at the horizon), the radiance of airlight is maximum and is found 
by setting d =8 to get L8(.) =k. Therefore, the radiance of airlight for any given pathlength d is: -ß(.)d 
L(d,.) = L8(.)1 -e. (11) As expected, the radiance of airlight for an object right in front of the observer 
(d = 0) equals zero. Of great signi.cance to us is the fact that the above expression no longer includes 
the unknown factor k. Instead, we have the airlight radiance L8(.) at the horizon, which is an observable. 
3.3. Overcast Sky Illumination Allard s attenuation model in (5) is in terms of the radiant intensity 
of a point source. This formulation does not take into account the sky illumination and its re.ection 
by scene points. We make two simpli­fying assumptions regarding the illumination received by a scene 
point. Then, we reformulate the attenuation model in terms of sky illumination and the BRDF of scene 
points. Usually, the sky is overcast under foggy conditions. So, we use the overcast sky model for environmen­tal 
illumination (Gordon and Church, 1966; Moon and Spencer, 1942). We also assume that the irradiance at 
each scene point is dominated by the radiance of the sky, and that the irradiance due to other scene 
points is not signi.cant. In Appendix A, we show that the 238 Narasimhan and Nayar attenuated irradiance 
at the observer is given by, -ß(.)d L8(.).(.)e E(d,.) =g . (12) d2 where L8(.) is the horizon radiance. 
.(.) represents the sky aperture (the cone of sky visible to a scene point), and the re.ectance of the 
scene point in the di­rection of the viewer. g represents the optical settings of the camera (aperture, 
for instance). Note that we refer to (5) as the direct transmission model while dealing with images of 
light sources taken at night. However, while dealing with images of scenes taken during day­light, we 
refer to (12) as the direct transmission model. 4. Depths of Light Sources from Attenuation Consider 
the image of an urban setting taken at night (see Fig. 5). Environmental illumination of the scene due 
to sunlight, skylight and re.ected ground light are minimal and hence airlight can be safely ignored. 
The bright points in the image are mainly sources of light such as street lamps and windows of lit rooms. 
On a clear night, these sources are visible to a distant ob­server in their brightest and clearest forms. 
As haze or fog sets in, the radiant intensities of the sources dimin­ish due to attenuation. Our goal 
here is to recover the relative depths of the sources in the scene from two images taken under different 
(unknown) atmospheric conditions. Since environmental illumination is negligible at night, the image 
irradiance of a light source in the scene can be expressed using the attenuation model (5) as: -ß(.)d 
Io(.)e E(d,.) =g , (13) d2  where, Io(.) is the radiant intensity of the source, d is the distance between 
the source and the camera and the constant gain g accounts for the optical parameters (aperture, for 
instance) of the camera. It is important to note that ß(.) is the total scattering coef.cient and not 
the angular one. We are assuming here that the lines of sight are not too inclined and hence all lines 
of sight pass through the same atmospheric conditions. This removes all dependence on the exact form 
of the scattering function; the attenuation is determined by a single coef.cient ß(.) that is independent 
of viewing direction. If the detector of the camera has spectral response s(.), the .nal image brightness 
recorded is determined as: -ß(.)d Io(.)e E= s(.)E(d,.) d.= gs(.) d.. d2 (14) For the visible light spectrum, 
the relationship be­tween the scattering coef.cient ß, and the wavelength ., is given by the inverse 
power law (analogous to Rayleigh s law for small air particles): Constant ß(.) = , (15) .. where . .[0,4]. 
Fortunately, for fog and dense haze, . 0 (see Middleton (1952) and McCartney (1975)). In these cases, 
ß does not change appreciably with wavelength. Furthermore, since the spectral bandwidth of the camera 
is rather limited (visible light range for a gray-scale camera, and even narrower spectral bands when 
the camera is color), we will assume the scatter­ing coef.cient ß(.) to be constant over this bandwidth. 
Then, we have: -ßd -ßd ee E=gs(.)I (.) d.=gI . (16) d2 d2 Now consider two different weather conditions, 
say, mild and dense fog. Or, one of the conditions could be clear with ß =0. In either case we have two 
different attenuation coef.cients, ß1 and ß2. If we take the ratio of the two resulting image brightness 
values, we get: E 1 -(ß1-ß2)d R ==e. (17) E 2 Using the natural log, we obtain: R=ln R =-(ß1 - ß2)d. 
This quantity is independent of the sensor gain and the radiant intensity of the source. In fact, it 
is nothing but the difference in optical thicknesses (DOT) of the source for two weather conditions. 
In the at­mospheric optics literature, the term DOT is used as a quantitative measure of the change in 
weather con­ditions. Now, if we compute the DOTs of two different light sources in the scene (see Fig. 
5) and take their ra­tio, we determine the relative depths of the two source locations: R di i = (18) 
R j dj Hence, the relative depths of all sources (with unknown radiant intensities) in the scene can 
be computed from two images taken under unknown but different haze or fog conditions. Since we may not 
entirely trust the DOT computed for any single source, the above calculation may be made more robust 
by using: R i di = (19) j=N j=N R j=0 jj=0 dj By setting the denominator on the right hand side to an 
arbitrary constant we have computed the depths of all sources in the scene up to a scale factor. Figure 
6 shows experimental results on the recovery of light sources from night images. This experiment and 
all subsequent ones are based on images acquired using a Nikon N90s SLR camera and a Nikon LS-2000 slide 
scanner. All images are linearized using the ra­diometric response curve of the imaging system that is 
computed off-line using a color chart. Figure 6(a) shows a clear day image of a scene with .ve lamps. 
This image is provided only to give the reader an idea of where the lamps are located in the scene. Figures 
6(b) and (c) are clear night and foggy night images of the same scene. The above algorithm for depth 
estima­tion was used to recover the locations of all .ve light sources up to a scale factor. Figure 6(d) 
shows different perspectives of the recovered coordinates of the lamps in three-dimensional space. The 
poles and the ground plane are added only to aid visualization of the results. 5. Structure from Airlight 
Under dense fog and close by objects or mild fog and distant objects, attenuation of object brightness 
is se­vere and airlight is the main cause of image irradiance. Also, in the case of dense haze around 
noon, airlight dominates. In such cases, airlight causes object bright­ness to increase with distance 
from the observer. Here, Vision and the Atmosphere 239 we present a simple method for computing scene 
struc­ture from a single airlight image. A different but re­lated method for computing depth cues was 
proposed by Cozman and Krotkov (1997). Let a scene point at depth d produce airlight radiance L(d,.). 
If our camera has a spectral response s(.), the .nal brightness value recorded for the scene point is: 
E(d) = gs(.)L(d,.) d., (20) where, g accounts for the constant of proportionality between scene radiance 
and image irradiance (Horn, 1986). Substituting the model for airlight given by (11) we get: -ß(.)d E(d) 
= gs(.)L8(.)1 -ed. (21) where, L8(.) is again the radiance of airlight at the horizon. As before, we 
will assume that the scatter­ing coef.cient ß(.) is more or less constant over the spectral band of the 
camera. This allows us to write: - E(d) =E8(1 -eßd ). (22) Let us de.ne: E8-E(d) S = . (23) E8 By substituting 
(22) in the above expression and taking the natural logarithm, we get: S=ln S =-ßd. (24) Hence, the 
three-dimensional structure of the scene can be recovered up to a scale factor (the scattering coef.cient 
ß) from a single image. Clearly, at least a small part of the horizon must be visible to obtain E8. If 
so, this part is easily identi.ed as the bright­est region of the image. If there is a strong (direc­tional) 
sunlight component to the illumination, scat­tering would be greater is some directions and airlight 
could be dependent on viewing direction. This problem can be alleviated by using the horizon brightness 
E8 that lies closest to the scene point under consideration. Figure 7 shows the structure of an urban 
setting com­puted from a hazy image taken around noon, and the structure of a mountain range computed 
using a foggy image. Given that some of the objects are miles away 240 Narasimhan and Nayar Figure 6. 
Relative depths from brightness decay of point sources at night. (a) A scene with .ve light sources (street 
lamps). This image is shown only to convey the relative locations of the sources to the reader. (b) An 
image of the scene taken on a clear night. (c) An image of the scene taken on a foggy night. The three-dimensional 
coordinates of the .ve sources were computed from images (b) and (c). (d) Rotated graphical illustrations 
used to demonstrate the computed lamp coordinates (small bright spheres). The lamp poles and the ground 
plane are added only to aid visualization. from the camera, such scene structures are hard to com­pute 
using stereo or structure from motion. An interest­ing study of the visibility of distant mountains taking 
into account earth s curvature can be found in Porch (1975). 6. Dichromatic Atmospheric Scattering Thus 
far, we have not taken into account the chro­matic effects of atmospheric scattering. Furthermore, we 
have described attenuation and airlight separately. However, in most situations the effects of both attenua­tion 
and airlight coexist. In the remainder of the paper, we discuss the chromatic effects of atmospheric 
scat­tering that include both attenuation and airlight, and hence develop a general framework for analyzing 
color images taken in bad weather. For this, we .rst present a new model that describes the appearance 
of scene colors in poor visibility conditions. As we know, attenuation causes the radiance of the surface 
to decay as it travels to the observer. In addition, if the particle sizes are comparable to the wavelengths 
 Vision and the Atmosphere 241  Figure 7. Structure from one image taken under dense fog/haze. (Left 
column) (a) Image of an urban scene taken under noon haze. (b) Depth map of the scene computed using 
the image in (a). (c) A three-dimensional rendering of the scene. (Right column) (d) Image of a mountain 
range taken under foggy conditions. (e) Depth map computed from the image in (d). (f) A three-dimensional 
rendering of the scene. Some of the objects in these scenes are several kilometers away from the camera. 
of the re.ected light, the spectral composition of the re­.ected light can be expected to vary as it 
passes through the medium. For fog and dense haze, these shifts in the spectral composition are minimal 
(see Middleton (1952) for details), and hence we may assume the hue of direct transmission to be independent 
of the depth of the re.ecting surface. The hue of airlight depends on the particle size distribution 
and tends to be gray or light blue in the case of haze and fog. Therefore, the .nal spectral distribution 
E(d,.) received by the ob­server is a sum of the distributions D(d,.) of directly transmitted light and 
A(d,.) of airlight, which are de­termined by the attenuation model (12) and the airlight model (11) respectively: 
E(d,.) =D(d,.) +A(d,.), -ß(.)d e D(d,.) =gL8(.).(.), (25) d2 -ß(.)d A(d,.) =g 1 -eL8(.). Here, L8(.) 
is the radiance of the horizon (d =8), and g is a constant that accounts for the optical settings of 
the imaging system. .(.) represents the re.ectance 242 Narasimhan and Nayar properties and sky aperture 
of the scene point. We refer to the above expression as the dichromatic atmospheric scattering model. 
It is similar in its spirit to the dichro­matic re.ectance model (Shafer, 1985) that describes the spectral 
effects of diffuse and specular surface re­.ections. A fundamental difference here is that one of our 
chromatic components is due to surface and vol­ume scattering (transmission of re.ected light) while 
the other is due to pure volume scattering (airlight). If a chromatic .lter with a spectral response 
f (.) is in­corporated into the imaging system, image irradiance is obtained by multiplying (25) by f 
(.) and integrating over .: E( f )(d) = D( f )(d) + A( f )(d). (26) In the case of a color image detector 
several such .lters (say, red, green and blue) with different sensitivities are used to obtain a color 
measurement vector. The dichromatic model can then be written as: E(d) = D(d) + A(d) (27) where, E = 
[E( f1),E( f2),....E( fn )]T . As we men­tioned earlier (see (15)), for fog and haze, the depen­dence 
of the scattering coef.cient ß(.) on the wave­length (within the small bandwidth of the camera) of light 
tends to be rather small. Therefore, except in the case of certain types of metropolitan haze, we may 
as­sume the scattering coef.cient to be constant with re­spect to wavelength (ß(.) = ß). Then, expression 
(26) may be simpli.ed as: E( f )(d) = p(d)D( f ) + q(d)A( f ) , (28) where: D( f ) = gf (.)L8(.).(.) 
d., A( f ) = gf (.)L8(.) d., (29) -ßd e p(d) = , q(d) = (1 - e-ßd ). d2 Here, D( f ) is the image irradiance 
due to the scene point without atmospheric attenuation and A( f ) is the image irradiance at the horizon 
in the presence of bad weather. We are assuming here that the clear and bad weather have illuminations 
with similar spectral distributions. Hence, the color measurement given by (27) can be rewritten as: 
E(d) = p(d)D +q(d)A. Since the inten­sity of illumination (or magnitude of the illumination Figure 8. 
Dichromatic atmospheric scattering model. The color E of a scene point on a foggy or hazy day, is a linear 
combination of the direction D of direct transmission color, and the direction A of airlight color. spectrum) 
at a scene point is expected to vary between clear and bad weather, it is more convenient to write: E(d) 
= m|L8|p(d)D + n|L8|q(d)A (30) where D and A are unit vectors and m and n are scalars. |L8| is the magnitude 
of the illumination spectrum. For convenience, the dichromatic model is re-written as: E = pD + qA , 
(31) where p is the magnitude of direct transmission, and q is the magnitude of airlight (see Fig. 8). 
From (30) we have, E8re-ßd p = , q = E8(1 - e-ßd ). (32) d2 where E8= n|L8|, is termed as the sky intensity 
and r = m/n is a function that depends on the properties of the scene point (re.ectance and sky aperture). 
For our analysis, the exact nature of r is not important; it suf.ces to note that r does not depend on 
the weather condition ß.1 This simpli.ed dichromatic scattering model will prove useful in the coming 
sections when we attempt to recover scene structure and remove weather effects from images. It is easy 
to see that the simpli.ed dichromatic model (31) is linear in color space. In other words, D ,A and 
E lie on the same dichromatic plane in color space. As Vision and the Atmosphere 243  Figure 9. For 
fog and haze, the transmittance (e(-ß(.)d)) does not vary appreciably with wavelength within the visible 
spectrum. The plots were generated using the atmospheric transmission software MODTRAN 4.0, with a .xed 
viewing geometry (distance, d and viewing directions are .xed). stated earlier, we impose the restriction 
that the hue of illumination under various weather conditions remains the same although its intensity 
can vary. It follows that the unit vectors D and A do not change due to dif­ferent atmospheric conditions 
(say, mild fog and dense fog). Therefore, the colors of any scene point, observed under different atmospheric 
conditions, lie on a single dichromatic plane (see Fig. 10(a)). We performed simulations using the atmospheric 
transmission software MODTRAN 4.0 (Acharya et al., 1999) to verify that the scattering coef.cient does 
not vary with wavelength within the visible spectrum (0.4µ 0.7µ). Figure 9 shows plots of transmittance 
(e-ß(.)d ) for a particular viewing geometry in fog and haze respectively. The distance from the observer 
to the scene was .xed at d = 0.2 km and the viewing direc­tion was .xed at 5 degrees off the ground plane. 
The plots show that the variation in ß is very small within the visible spectrum. Experiments with real 
scenes (shown in Fig. 17) were performed to verify this model under three differ­ent fog and haze conditions. 
The sky was overcast in all these conditions. The images used contained around half a million pixels. 
The dichromatic plane for each pixel was computed by .tting a plane to the colors of that pixel, observed 
under the three atmospheric condi­tions. The error of the plane-.t was computed in terms of the angle 
between the observed color vectors and the estimated plane. The average absolute error (in de- Figure 
10. (a) Dichromatic plane geometry and its veri.cation. The observed color vectors Ei of a scene point 
under different (two in this case) foggy or hazy conditions lie on a plane called the dichromatic plane. 
(b) Experimental veri.cation of the dichromatic model with two scenes imaged under three different foggy 
and hazy conditions, respectively. The error was computed as the mean angular deviation (in degrees) 
of the observed scene color vectors from the estimated dichromatic planes, over half a million pixels 
in the images. grees) for all the pixels in each of the two scenes is shown in Fig. 10(b). The small 
error values indicate that the dichromatic model indeed works well for fog and haze. 7. Weather Removal 
and Structure using Chromatic Decomposition Consider color images of a scene taken under clear weather 
and foggy or hazy weather. Assume that the clear day image is taken under environmental illumi­nation 
with similar spectral characteristics as the bad weather image. If not, a white patch in the scene may 
be used to apply the needed color corrections. The sky in the bad weather image reveals the direction 
of the airlight color A . The direction of the color D of each scene point is revealed by the clear weather 
image. Therefore, Eq. (31) can be used to decompose the bad weather color E at each pixel into its two 
components and determine the airlight magnitude q(d). The result­ing airlight image is then used to compute 
a depth map as described in Section 5. Figure 11 shows ex­perimental results obtained using the above 
decompo­sition method. Figure 12 demonstrates a simple form of weather removal by defogging windows of 
buildings. In computing depth from the airlight component, we have assumed that the atmosphere itself 
is uniformly illuminated. Consider a pathlength that extends from a point on a building to an observer. 
Clearly, atmospheric points closer to the building see less of the sky due to occlusion by the building. 
This effect increases towards the foot of the building. Some of the errors in the com­puted structure 
can be attributed to this illumination 244 Narasimhan and Nayar  Figure 11. Structure from chromatic 
decomposition. (a) Clear day image of buildings. (b) Foggy day image of the same scene. (c) The direct 
transmission component (brightened) estimated by the chro­matic decomposition algorithm. Black and gray 
points (windows) are discarded due to lack of color. (d) Depth map of the scene com­puted from the airlight 
component (depths of window areas are inter­polated). (e) A three-dimensional rendering of the computed 
depth map.  Figure 12. Demonstration of fog removal. (a) A clear day image of a building taken under 
an overcast sky. The color directions (and not magnitudes) of scene points (non-window regions) are recorded 
as true colors or clear day colors. (b) A foggy day image of the same scene again captured under an overcast 
sky. Note: Even though both images in (a) and (b) were taken on overcast days (ie., spectral composition 
of the daylight on both days are more or less identical), the horizon brightnesses (and/or camera exposure 
parameters) can vary. (c) The true colors recorded were used to decompose the foggy image into direct 
transmission and airlight components. The airlight component was subtracted from the window regions to 
demonstrate a simple form of weather removal. occlusion effect (see Appendix B for a more detailed treatment). 
Finally, there are certain limitations to this type of decomposition. First, we cannot decompose (31) 
if both the airlight and scene points have the same color. Also, this algorithm for chromatic decomposition 
is re­strictive since it requires a clear day image of the scene. In the remainder of the paper, we develop 
more general constraints and algorithms to compute structure as well as recover clear day colors, without 
requiring a clear day image of the scene. 8. Computing the Direction of Airlight Color The direction 
of airlight (fog or haze) color can be simply computed by averaging a patch of the sky on a foggy or 
hazy day (as was done in Section 7), or from scene points whose direct transmission color is black.2 
However, these methods necessitate either (a) the inclusion of a part of the sky (which is more prone 
to color saturation or clipping) in the image or (b) a clear day image of the scene with suf.cient black 
points to yield a robust estimate of the direction of airlight color. Here, we present a method that 
does not require either the sky or a clear day image, to compute the direction of airlight color. Figure 
13 illustrates the dichromatic planes for two scene points Pi and Pj , with different direct transmis­sion 
colors D (i) and D ( j). The dichromatic planes Qi Vision and the Atmosphere 245 and Qj are given by 
their normals, = E(i) × E(i) Ni 12 , (33) = E( j) × E( j) Nj . 12 Since the direction A of the airlight 
color is the same for the entire scene, it must lie on the dichromatic planes of all scene points. Hence, 
A is given by the intersection of the two planes Qi and Qj , Ni × Nj A = . (34) Ni × Nj In practice, 
scenes have several points with different colors. Therefore, we can compute a robust intersec­tion of 
several dichromatic planes by minimizing the objective function = (Ni · A )2 . (35) i Thus, we are able 
to compute the color of fog or haze using only the observed colors of the scene points under two atmospheric 
conditions, and not relying on a patch of the sky being visible in the image. We veri.ed the above method 
for the two scenes shown in Fig. 17. First, the direction of airlight color was computed using (35). 
Then, we compared it with the direction of the airlight color obtained by averag­ing an unsaturated patch 
of the sky. For the two scenes, the angular deviations were found to be 1.2. and 1.6. respectively. These 
small errors in the computed di­rections of airlight color indicate the robustness of the method. 9. 
Dichromatic Constraints for Iso-depth Scene Points In this section, we derive a simple constraint for 
scene points that are at the same depth from the observer. This constraint can then be used to segment 
the scene based on depth, without knowing the actual re.ectances of the scene points and their sky apertures. 
For this, we .rst prove the following lemma. Lemma. Ratios of the direct transmission magni­tudes for 
points under two different weather conditions are equal, if and only if the scene points are at equal 
depths from the observer. Proof: Let ß1 and ß2 be two unknown weather con­ditions with horizon brightness 
values E81 and E82. 246 Narasimhan and Nayar Let Pi and Pj be two scene points at depths di and dj , 
from the observer. Also, let r(i) and r( j) represent sky apertures and re.ectances of these points. 
From (32), the direct transmission magnitudes of Pi under ß1 and ß2, can be written as (i)-ß1di (i)-ß2di 
rere (i) E81(i) E82 p= , p= . 1 d22 d2 ii Similarly, the direct transmission magnitudes of Pj under 
ß1 and ß2, are ( j)-ß1dj ( j)-ß2dj ( j) E81 re( j) E82 re p= , p= . 1 d22 d2 jj Then, we immediately 
see that the relation: (i) 2( j) E82 pp 2 -(ß2-ß1)d == e, (36) (i)( j) ppE81 11 holds if and only if 
di =dj =d. So, if we have the ratio of direct transmissions for each pixel in the image, we can group 
the scene points according to their depths from the observer. But how do we compute this ratio for any 
scene point without knowing the actual direct transmission magnitudes? Consider the dichromatic plane 
geometry for a scene point P, as shown in Fig. 14. Here, we denote a vector by the line segment between 
its end points. Let p1 and p2 be the unknown direct transmission magnitudes of P under ß1 and ß2, respectively. 
Similarly let q1 and q2 be the unknown airlight magnitudes for P under ß1 and ß2. We de.ne a magnitude 
|OAt |on the airlight vector such that E2 At E1 O. Also, since the direction of di­rect transmission 
color for a scene point does not vary due to different atmospheric conditions, E1 A1 E2 A2. Here A1 and 
A2 correspond to the end points of the airlight magnitudes of P under ß1 and ß2, as shown in Fig. 14. 
Thus, E1 OA1 ~E2 At A2. This implies, p2 q2 -|OAt ||E2 At | == . (37) p1 q1 |E1 O| Since the right hand 
side of (37) can be computed using the observed color vectors of the scene point P,we can compute the 
ratio ( p2/p1) of direct transmission magnitudes for P under two atmospheric conditions. Therefore, from 
(36), we have a simple method to .nd points at the same depth, without having to know their re.ectances 
and sky apertures. Let us now consider the numerical stability of the direct transmission ratio (37). 
Under heavy fog/haze (or when the dynamic range of the sensor is low), the direct transmission magnitudes 
are low and their ratio could be unstable. In such cases, the ratio constraint can be supported by another 
constraint for depth segmentation we describe brie.y. Consider the dichromatic planes of two different 
scene points as illustrated in Fig. 15. It can be shown (using the geometric analysis in Fig. 14) that 
the shaded triangles are similar if and only if the two scene points are at equal depths from the observer. 
Therefore, the constraint for two scene points to be iso-depth is given in terms of observables, E(1) 
E(2) E(1) E(2) . (38) 11 22 Using the constraints in (36) and (38) a sequential la­beling like algorithm 
can be used to ef.ciently segment scenes into regions of equal depth. 10. Scene Structure from Two Bad 
Weather Images We extend the direct transmission ratio constraint given in (36) one step further and 
present a method to con­struct the complete structure of an arbitrary scene, from two images taken under 
poor weather conditions. From (36), the ratio of direct transmissions of a scene point P under two atmospheric 
conditions, is given by p2 E82 -(ß2-ß1)d = e. (39) p1 E8 1 Note that we have already computed the left 
hand side of the above equation using (37). Taking natural loga­rithms on both sides, we get E8p2 (ß2 
-ß1)d =ln 2 -ln . (40)E8p1 1 So, if we know the horizon brightness values, E81 and E82, then we can 
compute the scaled depth (ß2 -ß1)d at P. As before, (ß2 -ß1)d is just the difference in optical thicknesses 
(DOT) for the pathlength d, under the two weather conditions. 10.1. Estimation of E81 and E82 The expression 
for scaled depth give in (40), includes the horizon brightness values, E81 and E82. These two terms are 
observables only if some part of the sky is visible in the image. However, the brightness values within 
the region of the image corresponding to the sky, cannot be trusted since they are prone to intensity 
saturation and color clipping. Here, we estimate E81 Vision and the Atmosphere 247 and E82 using only 
points in the non-sky region of the scene. Let q1 and q2 denote the magnitudes of airlight for a scene 
point P under atmospheric conditions ß1 and ß2. Using (32), we have - - q1 =E81(1 -eß1d ), q2 =E82(1 
-eß2d ). (41) Therefore, E8-q2 E8 22 -(ß2-ß1)d = e. (42) E81 -q1 E81 Substituting (39), we can rewrite 
the previous equation as p2 q2 -c p2 = , where, c =E82 - E81 . p1 q1 p1 (43) Comparing (43) and (37), 
we get c =|OAt |(see Fig. 14). hence, the expression for c in (43) repre­sents a straight line equation 
in the unknown param­eters, E81 and E82. Now consider several pairs of (i) (i)(i) {c, ( p/p)}corresponding 
to scene points Pi ,at 21 different depths. Then, the estimation of E81 and E82 is reduced to a line 
.tting problem. Quite simply, we have shown that the horizon brightnesses under dif­ferent weather conditions 
can be computed using only non-sky scene points. Since both the terms on the right hand side of (40) 
can be computed for every scene point, we have a simple algorithm for computing the scaled depth at each 
scene point, and hence the complete scene structure, from two images taken under different atmospheric 
conditions. 10.2. Experimental Results We now present results showing scene structure recov­ered from 
both synthetic and real images. The synthetic scene we used is shown on the left side of Fig. 16(a) as 
a 200 ×200 pixel image with 16 color patches. The colors in this image represent the direct transmission 
or clear day colors of the scene. We assigned a random depth value to each color patch. The rotated 3D 
struc­ture of the scene is shown on the right side of Fig. 16(a). Then, two different levels of fog (ß1/ß2 
=0.67) were added to the synthetic scene according to the dichro­matic model. To test robustness, we 
added noise to the foggy images. The noise was randomly selected from a uniformly distributed color cube 
of dimension 10. The 248 Narasimhan and Nayar Figure 16. Experiments with a synthetic scene. (a) On 
the left, a 200 ×200 pixel image representing a synthetic scene with 16 color patches, and on the right, 
its rotated 3D structure. (b) Two levels of fog (ß1/ß2 =0.67) are added to the synthetic image according 
to the dichromatic model. To test robustness, noise is added by random selection from a uniformly distributed 
color cube of dimension 10. (c) The recovered structure (3 ×3 median .ltered). resulting two foggy (and 
noisy) images are shown in Fig. 16(b). The structure shown in 16(c) is recovered from the two foggy images 
using the technique we de­scribed above. Simulations were repeated for the scene in Fig. 16(a) for two 
relative scattering coef.cient values (ß1/ß2), and three different noise levels. Once again, the noise 
was randomly selected from a uniformly distributed color cube of dimension .. Table 2 shows results of 
sim­ulations for two parameter sets {ß1/ß2, E81 , E8}= 2 {0.5, 100, 255}and {0.67, 200, 400}. The computed 
values for E81 , E82, and the percentage RMS error in the recovered scaled depths, computed over all 
200 ×200 pixels are given. These results show that our method for recovering structure is robust for 
rea­sonable amounts of noise. Experiments with two real scenes under foggy and hazy conditions are shown 
in Fig. 17. The .rst of the two scenes was imaged under two foggy conditions, and is shown in 17(a). 
The second scene was imaged Table 2. Simulations were repeated for the scene in Fig. 16(a), for two sets 
of parameter values, and three different noise levels. Noise was randomly selected from a uniformly distributed 
color cube of dimension .. Noise (.) 0 5 10 15 Actual values {ß1/ß2, E81 , E82 }={0.5, 100, 255} Estimated 
E81 100 108.7 109.2 119.0 Estimated E82 255 262.7 263.6 274.0 Depth error (%) 0.0 7.14 11.7 15.3 Actual 
values {ß1/ß2, E81 , E82 }={0.67, 200, 400} Estimated E81 200 204.3 223.7 249.5 Estimated E82 400 403.8 
417.5 444.2 Depth error (%) 0.0 12.3 15.3 17.8 under two hazy conditions as shown in 17(c). Figure 17(b) 
and (d) shows the corresponding re­covered depth maps. 11. Clear Day Scene Colors As we stated in the 
beginning of the paper, most out­door vision applications perform well only under clear weather. Any 
discernible amount of scattering due to fog or haze in the atmosphere, hinders a clear view of the scene. 
Earlier we presented a simple form of weather removal that requires a clear day image of the scene (see 
Figs. 11 and 12 in Section 7). In this section, we compute the scene colors as they would appear on a 
clear but overcast day from two bad weather images. More precisely, we compute the direct transmission 
colors of the entire scene using minimal a priori scene information. For this, we .rst show that, given 
addi­tional scene information (airlight or direct transmission vector) at a single point in the scene, 
we can compute the clear day colors of the entire scene from two bad weather images. Consider the dichromatic 
model given in (31). The color of a scene point Pi under weather condition ß is, E(i)(i)D (i)(i) =p+qA, 
(44) where p(i) is the direct transmission magnitude, and q(i) is the airlight magnitude of Pi . Suppose 
that the direction D (i) of direct transmission color for a single point Pi is given. Besides, the direction 
A of airlight color for the entire scene can be estimated using (35). Therefore, the coef.cients p(i) 
and q(i) can be computed using (44). Furthermore, the optical thickness ßdi of Pi can be computed from 
(32). Vision and the Atmosphere 249 Since we have already shown how to compute the scaled depth of every 
scene point (see (40)), the relative depth dj /di of any other scene point Pj with respect to Pi can 
be computed using the ratio of scaled depths. Hence, the optical thickness and airlight for the scene 
point Pj , under the same atmospheric condition are given by ßdj =ßdi (dj /di ), (45) ( j) - q=E8(1 -eßdj 
). Finally, the direct transmission color vector of Pj can be computed as ( j)D ( j)( j) p=E( j) -qA. 
(46) Thus, given a single measurement (in this case, the direction of direct transmission color of a 
single scene point), we have shown that the direct transmission and airlight color vectors of any other 
point, and hence the entire scene can be computed. But how do we specify the clear day color of any scene 
point without actually capturing the clear day image? For this, we assume that there exists at least 
one scene point whose direct transmission color D lies on 250 Narasimhan and Nayar  Figure 19. [(a) 
and (c)] Clear day scene colors recovered from the two foggy and hazy images shown in Fig. 17(a) and 
(c) respectively. The colors in some of the dark window interiors are dominated by airlight and thus 
their clear day colors are computed to be black. The images are median .ltered to reduce noise and brightened 
for display purposes. [(b) and (d)] Actual clear day images of the scenes are shown for qualitative comparison. 
Note: The clear day images on the right and the bad weather images (Fig. 17) were captured on different 
days. Some differences between actual and recovered clear day colors are due to the different spectral 
distributions of illumination in the scene, during image acquisition. the surface of the color cube (including 
origin or black) and we wish to identify such point(s) in the scene auto­matically. Consider the R-G-B 
color cube in Fig. 18. If the clear day color of a scene point lies on the surface Vision and the Atmosphere 
251  of the color cube, then the computed q is equal to the airlight magnitude q of that point. However, 
if it lies within the color cube, then clearly q > q. For each point Pi , we compute q (i) and optical 
thickness ß1di . Note that may or may not be the correct optical ß1di thickness. We normalize the optical 
thicknesses of the scene points by their scaled depths (DOTs) to get ß1di a i = . (47) (ß2 - ß1)di For 
scene points that do not lie on the color cube sur­face, a i is greater than what it should be. Since 
we have assumed that there exists at least one scene point whose clear day color is on the surface of 
the cube, it must be the point that has the minimum a i . So, q (i) of that point is its true airlight. 
Hence, from (45), the airlights and direct transmission colors of the entire scene can be computed without 
using a clear day image. For robust­ness, we use k least a i s. We call this the Color Cube Boundary 
Algorithm. Figure 19 illustrates experiments with real scenes. Usually in urban scenes, window interiors 
have very little color of their own. Their intensities are solely due to airlight and not due to direct 
transmission. In other words, their direct transmission color is black (the origin of the color cube). 
We detected such points in the scene using the above technique and recovered the clear day colors of 
foggy and hazy scenes. A second result is shown in Figs. 20 and 21. 12. Summary Research in atmospheric 
optics has been around for over two centuries. The physical processes that govern the effects of atmospheric 
scattering on scene appear­ance are well established. This article is just an initial attempt at understanding 
and exploiting the manifesta­tions of weather in order to interpret, recover and ren­der scenes under 
various atmospheric conditions. We summarized existing models in atmospheric optics and proposed new 
ones, keeping in mind the constraints faced by most vision applications. We presented sev­eral simple 
algorithms for recovering scene structure from one or two bad weather images and demonstrated that bad 
weather can be put to good use. Using scene structure, algorithms to remove weather effects were developed. 
We intend to use these results as building blocks for developing more advanced weather-tolerant vision 
techniques. Potential applications of this work are in outdoor surveillance, navigation, underwater ex­plorations 
and image based rendering. Appendix A: Direct Transmission Under Overcast Skies We present an analysis 
of the effect of sky illumina­tion and its re.ection by a scene point, on the direct transmission from 
the scene point. For this, we make two simplifying assumptions on the illumination re­ceived by scene 
points. Usually, the sky is overcast un­der foggy conditions. So we use the overcast sky model 252 Narasimhan 
and Nayar (Gordon and Church, 1966; IRIA, 1978) for environ­mental illumination. We also assume that 
the irradiance of each scene point is dominated by the radiance of the sky, and that the irradiance due 
to other scene points is not signi.cant. See Langer and Zucker s work (1994) for a related analysis. 
Consider the illumination geometry shown in Fig. 22. Let P be a point on a surface and n be its normal. 
We de.ne the sky aperture of point P, as the cone of sky visible from P. Consider an in­.nitesimal patch 
of the sky, of size d. in polar angle and df in azimuth as shown in Fig. 22. Let this patch subtend a 
solid angle d. at P. For overcast skies, Moon (Moon and Spencer, 1942) and Gordon (Gordon and Church, 
1966) have shown that the radiance of the in.nitesimal cone d, in the direction (.,f)is given by L(.,f) 
= L8(.)(1 + 2 cos .)d., where d. = sin .d.df. Hence, the irradiance at P due to the entire aperture ,isgiven 
by E(.) = L8(.)(1 + 2 cos .) cos . sin . d. df, (48) where cos . accounts for foreshortening (Horn, 
1986). If R is the BRDF of P, then the radiance from P toward the observer can be written as Lo(.) = 
L8(.) f (.)R(.,f,.) d. df, (49) where f (.) = (1 + 2 cos .) cos . sin .. Let s be the projection of 
a unit patch around P, on a plane per­pendicular to the viewing direction. Then, the radiant intensity 
of P is given by Io(.) = sLo(.). Since L8(.) is a constant with respect to . and f, we can factor it 
out of the integral and write concisely as Io(.) = L8(.).(.), (50) where .(.) = s f (.)R(.,f,.) d. df. 
(51) The term .(.) represents the sky aperture and the re­.ectance in the direction of the viewer. Substituting 
for Io(.) in the direct transmission model in (5), we obtain -ß(.)d L8(.).(.)e E(d,.) = g , (52) d2 
where g represents the optical setting of the camera (exposure, for instance). We have thus formulated 
the direct transmission model in terms of overcast sky il­lumination and the re.ectance of the scene 
points. Appendix B: Illumination Occlusion Problem In deriving the expression for the radiance due to 
airlight in Section 3.2, we assumed that the atmosphere is illuminated uniformly regardless of the type 
of illu­mination. This is not always true since not all points in the atmosphere see the same solid angle 
of the sky. In fact, the scene itself occludes part of the sky hemisphere visible to a point in the atmosphere. 
For explanation purposes, consider a scene with a single building. The solid angle subtended at any point 
in the atmosphere by the sky is called its sky aperture. As seen in Fig. 23, this solid angle decreases 
as the distance increases from the observer for any given pathlength. Similarly, the solid angle is smaller 
for points near the bottom of the building. We now present a simpli.ed analysis of this effect. We assume 
that the atmosphere is illuminated mainly by overcast skylight (ground light is ignored here). Then, 
the irradiance received by any point in the atmosphere is given by (see Eq. (48)), = E(hemisphere) - 
E(occluded) E , f . E(occluded) = L8(1 + 2 cos .) -f 0 × cos . sin .d.df, (53) p p/2 E(hemisphere) = 
L8(1 + 2 cos .) -p 0 × cos . sin .d.df, where E(hemisphere) is the irradiance the point would receive 
from the entire sky hemisphere (as if there were no occlusions). Eoccluded is the irradiance the point 
would have received from the occluded part. . and f denote the polar and azimuth of the occluded region. 
The above equation simpli.es to  7p- 7fcos2 .(3 + 4 cos .) E = L8 . (54) 3 To correct for the radiance 
of airlight in Section 3.2, we multiply by the fraction of irradiance received by each point and rewrite 
the airlight radiance (10) of a pathlength d as L(d,.) d fcos2 .(3 + 4 cos .) -ß(.)d = k 1 - e- k p 
× ß(.)e-ß(.)x dx. (55) 0 Note here that both . and fdepend on the depth from the observer x (see Fig. 
23). In other words, the integral in the previous equation depends on the exact extent of occlusion by 
the scene. In our experiments, we have as­sumed uniform illumination of the atmosphere and thus some 
of the errors in the depth maps can be attributed to this effect. Vision and the Atmosphere 253 Acknowledgments 
This work was supported in parts by a DARPA/ONR HumanID Contract (N00014-00-1-0916), an NSF Award (IIS-99-87979), 
and a DARPA/ONR MURI Grant (N00014-95-1-0601). The authors thank Jan Koenderink of Utrecht University 
for pointers to early work on atmospheric optics. The authors also thank Yoav Schechner for the discussions 
on this topic that helped improve the paper. Some of the results pre­sented in this paper have appeared 
in the proceedings of the IEEE International Conference on Computer Vision 1999 (Nayar and Narasimhan, 
1999), IEEE Conference on Computer Vision and Pattern Recog­nition 2000 (Narasimhan and Nayar, 2000) 
and SPIE Conference on Human Vision and Electronic Imag­ing 2001 (Narasimhan and Nayar, 2001). Notes 
1. We do not handle situations where wet materials may appear darker than dry materials. 2. Sky and 
black points take on the color of airlight on a bad weather day.  References Acharya, P.K., Berk, A., 
Anderson, G.P., Larsen, N.F., Tsay, S.C., and Stamnes, K.H. 1999. Modtran4: Multiple scattering and BRDF 
upgrades to modtran. In SPIE Proc. Optical Spectroscopic Tech­niques and Instrumentation for Atmospheric 
and Space Research III, p. 3756. Allard, E. 1876. Memoire sur l intensite et la portee des phares. Dunod: 
Paris. Bouguer, P. 1729. Traite d optique sur la gradation de la lumiere. Chandrasekhar, S. 1960. Radiative 
Transfer. Dover Publications: New York. Chu, T.S. and Hogg, D.C. 1968. Effects of precipitation on propa­gation 
at 0.63, 3.5 and 10.6 microns. The Bell System Technical Journal. Cozman, F. and Krotkov, E. 1997. Depth 
from scattering. In Pro­ceedings of the 1997 Conference on Computer Vision and Pattern Recognition, vol. 
31, pp. 801 806. Gordon, J. and Church, P. 1966. Overcast sky luminances and di­rectional luminous re.ectances 
of objects and backgrounds under overcast skies. Applied Optics, 5:919. Hardy, A.C. 1967. How large is 
a point source? Journal of Optical Society of America, 57(1). Henderson, S.T. 1977. Daylight and its 
Spectrum. Wiley: New York. Hidy, G.M. 1972. Aerosols and Atmospheric Chemistry. Academic Press: New York. 
Horn, B.K.P. 1986. Robot Vision. The MIT Press: Cambridge, MA. IRIA. 1978. The Infrared Handbook. Infrared 
Information and Anal­ ysis Center, Environmental Research Institute of Michigan. 254 Narasimhan and 
Nayar Koenderink, J.J. and Richards, W.A. 1992. Why is snow so bright? Journal of Optical Society of 
America, 9(5):643 648. Kopeika, N.S. 1998. A System Engineering Approach to Imaging. SPIE Press. Koschmieder, 
H. 1924. Theorie der horizontalen sichtweite. Beitr. Phys. Freien Atm., 12:33 53, 171 181. Langer, M.S. 
and Zucker, S.W. 1994. Shape from shading on a cloudy day. JOSA-A, 11(2):467 478. Mason, B.J. 1975. Clouds, 
Rain, and Rainmaking. Cambridge University Press: Cambridge. McCartney, E.J. 1975. Optics of the Atmosphere: 
Scattering by Molecules and Particles. John Wiley and Sons: New York. Middleton, W.E.K. 1949. The effect 
of the angular aperture of a telephotometer on the telephotometry of collimated and non-collimated beams. 
Journal of Optical Society of America, 39:576 581. Middleton, W.E.K. 1952. Vision Through the Atmosphere. 
University of Toronto Press. Mie, G. 1908. A contribution to the optics of turbid media, espe­cially 
colloidal metallic suspensions. Ann. of Physics, 25(4):377 445. Minnaert, M. 1954. The Nature of Light 
and Color in the Open Air. Dover: New York. Moon, P. and Spencer, D.E. 1942. Illumination from a non-uniform 
sky. Illum Eng., 37:707 726. Myers, J.N. 1968. Fog. Scienti.c American, pp. 75 82. Narasimhan, S.G. 
and Nayar, S.K. 2000. Chromatic framework for vision in bad weather. In Proceedings of the IEEE Conference 
on Computer Vision and Pattern Recognition. Narasimhan, S.G. and Nayar, S.K. 2001. Vision and the weather. 
In Proceedings of SPIE Conference on Human Vision and Electronic Imaging VI, p. 4299. Nayar, S.K. and 
Narasimhan, S.G. 1999. Vision in bad weather. In Proceedings of the 7th International Conference on Computer 
Vision. Nieto-Vesperinas, M. and Dainty, J.C. 1990. Scattering in Volumes and Surfaces. North-Holland: 
New York. Oakley, J.P. and Satherley, B.L. 1998. Improving image quality in poor visibility conditions 
using a physical model for degradation. IEEE Trans. on Image Processing,7. Ohtake, T. 1970. Factors affecting 
the size distribution of rain­drops and snow.akes. Journal of Atmospheric Science, 27:804 813. Porch, 
W.M. 1975. Visibility of distant mountains as a measure of background aerosol pollution. Applied Optics, 
14. Rensch, D.B. and Long, R.K. 1970. Comparative studies of extinction and backscattering by aerosols, 
fog, and rain at 10.6 and 0.63 microns. Applied Optics, 9(7). Shafer, S. 1985. Using color to separate 
re.ection components. Color Research and Applications, pp. 210 218. Van De Hulst. 1957. Light Scattering 
by Small Particles. John Wiley and Sons: New York. Yitzhaky, Y., Dror, I., and Kopeika, N.S. 1998. Restoration 
of altmospherically blurred images according to weather-predicted atmospheric modulation transfer function. 
Optical Engineering, 36. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 25, NO. 
6, JUNE 2003 713 Contrast Restoration of Weather Degraded Images Srinivasa G. Narasimhan and Shree K. 
Nayar Abstract Images of outdoor scenes captured in bad weather suffer from poor contrast. Under bad 
weather conditions, the light reaching a camera is severely scattered by the atmosphere. The resulting 
decay in contrast varies across the scene and is exponential in the depths of scene points. Therefore, 
traditional space invariant image processing techniques are not sufficient to remove weather effects 
from images. In this paper, we present a physics-based model that describes the appearances of scenes 
in uniform bad weather conditions. Changes in intensities of scene points under different weather conditions 
provide simple constraints to detect depth discontinuities in the scene and also to compute scene structure. 
Then, a fast algorithm to restore scene contrast is presented. In contrast to previous techniques, our 
weather removal algorithm does not require any a priori scene structure, distributions of scene reflectances, 
or detailed knowledge about the particular weather condition. All the methods described in this paper 
are effective under a wide range of weather conditions including haze, mist, fog, and conditions arising 
due to other aerosols. Further, our methods can be applied to gray scale, RGB color, multispectral and 
even IR images. We also extend our techniques to restore contrast of scenes with moving objects, captured 
using a video camera. Index Terms Physics-based vision, atmosphere, bad weather, fog, haze, visibility, 
scattering, attenuation, airlight, overcast sky, scene structure, defog, dehaze, contrast restoration, 
shape from X, shape from weather, scene reconstruction. 1TOWARD WEATHER-FREE VISION H UMAN perception 
of scene color and contrast through images using a weather-predicted atmospheric modulation the atmosphere 
has been extensively studied [14], [15], transfer function, and an a priori estimate of the distance 
[17], [18]. For centuries, artists have rendered their paint-from which the scene is imaged. Oakley and 
Satherley [25] ings with an atmospheric or aerial perspective [7]. They and Tan and Oakley [33], [34] 
describe a physics-based illustrate, in their paintings, optical phenomena such as the method to restore 
scene contrast without using predicted bluish haze of distant mountains and reduced visibility weather 
information. However, they assume that scene under adverse weather conditions such as mist, fog, rain, 
depths are known beforehand, and they approximate the and snow. Leonardo da Vinci s paintings often contain 
an distribution of radiances in the scene by a single Gaussian atmospheric perspective of the background 
scene [26], with known variance. Another work by Grewe and Brooks where farther scene points were painted 
brighter and bluer. [9] uses wavelet-based fusion of multiple bad weather While these optical phenomena 
can be argued to be images to get a less blurred image. aesthetically pleasing to humans, they are often 
hindrances Narasimhan and Nayar [21] analyze color variations in to the satisfactory working of a computer 
vision system. the scene under different weather conditions based on the Most outdoor vision applications 
such as surveillance, dichromatic atmospheric scattering model proposed in [24]. terrain classification, 
and autonomous navigation require Using constraints on scene color changes, they compute robust detection 
of image features. Under bad weather complete 3D structure and recover clear day scene colors conditions, 
however, the contrast and color of images are from two or more bad weather images [23]. However, they 
drastically altered or degraded. Hence, it is imperative to assume that the atmospheric scattering properties 
do not remove weather effects from images in order to make vision change with the wavelength of light. 
This property holds systems more reliable. Unfortunately, the effects of bad over the visible spectrum 
only for certain weather condi­weather increase exponentially with the distances of scene tions such 
as fog and dense haze. For several aerosols, points from the sensor. As a result, conventional space 
however, scattering strongly depends on the wavelength of invariant filtering techniques fail to adequately 
remove incident light. Furthermore, scene recovery using the weather effects from images. dichromatic 
model is ambiguous for scene points whose Recently, there has been an increased interest in the colors 
match the color of fog or haze. image processing and vision communities on issues Polarization has been 
used as a cue to reduce haze in related to imaging under bad weather. Kopeika [13] and images based on 
the effects of scattering on light polarization Yitzhaky et al. [38] deblur atmospherically degraded 
[2], [5], [27]. In many works [4], [28], the radiation from the object of interest is assumed to be polarized, 
whereas the natural illumination scattered toward the observer (airlight) . The authors are with the 
Computer Science Department, Columbia is assumed to be unpolarized. In other works [6], [29], [36], the 
University, 500 West 120th Street, Room 450, New York, NY 10027. radiation from the scene of interest 
is assumed to unpolar­ E-mail: {srinivas, nayar}@cs.columbia.edu. ized, whereas airlight is assumed to 
be partially polarized. Manuscript received 6 Feb. 2002; revised 25 Sept. 2002; accepted 4 Oct. 2002. 
Polarizing filters are, therefore, used widely by photogra- Recommended for acceptance by R. Beveridge. 
For information on obtaining reprints of this article, please send e-mail to: phers to reduce haziness 
in landscape images, where the tpami@computer.org, and reference IEEECS Log Number 115843. radiance from 
the landscapes is generally unpolarized. 0162-8828/03/$17.00 . 2003 IEEE Published by the IEEE Computer 
Society 714 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 25, NO. 6, JUNE 2003 
However, polarization filtering alone does not ensure complete removal of haze. Schechner et al. [29], 
[30] further analyzed two or more polarization filtered images to compute scene structure and dehaze 
images. The effectiveness of polarization as a cue to remove weather effects is limited under dense fog 
and mist with overcast sky illumination since scattered light is mostly depolarized. In this paper, we 
present a physics-based method to restore contrast of a scene from two or more images taken in uniform 
bad weather conditions. A monochrome atmospheric scatter­ing model that describes how scene intensities 
are affected by homogeneous weather conditions is presented. This model is valid in both the visible 
and near-IR spectra, and for a wide range of weather conditions such as mist, haze, fog, and other aerosols. 
The model does not require the scattering properties of the atmosphere to be constant with respect to 
wavelength of light over a large spectral range (for example, the range of visible spectrum, as in [23]). 
Since we are interested in a short range of distances (of the order of a few kilometers), we assume that 
the weather condition does not change spatially in the field of view.1 Using the monochrome weather model, 
we show how contrast of a scene degrades with distance. We conclude that standard contrast enhancement 
techniques can only handle a scene or a region within a scene at a fixed distance from the sensor. A 
simple contrast restoration technique similar to contrast stretching is derived for scenes where depth 
segmentation is known a priori. Changes in scene intensities, observed under different weather conditions, 
present strong physical constraints regarding scene struc­ture. These constraints are exploited to automatically 
detect depth discontinuities in the scene and also to recover complete scene structure from two images 
taken under different weather conditions during daytime. Using the computed structure, contrast is restored 
from a single weather-degraded image of the scene. Unlike previous methods for contrast restoration, 
we do not need accurately predicted weather information or prior distributions on scene radiances. We 
extend our algorithms to handle video and describe a simple heuristic to restore contrasts of moving 
objects in the scene whose depths are unknown. The entire analysis in this paper is done for monochrome 
(single narrow spectral band) images. However, the same methods can be applied independently to images 
with multiple spectral bands. We show that our methods can be applied to images taken using gray scale, 
wide-band RGB, multispectral, and also narrow-band IR cameras. 2ATMOSPHERIC SCATTERING MODELS Scattering 
of light by physical media has been one of the main topics of research in the atmospheric optics and 
astronomy communities. In general, the exact nature of scattering is highly complex and depends on the 
types, orientations, sizes, and distributions of particles constitut­ing the media, as well as wavelengths, 
polarization states, and directions of the incident light [3], [10]. Here, we focus on two models attenuation 
and airlight, which form the basis of our work. Since we are interested in a short range of 1. Multiple 
scattering effects are not taken into account in this model and hence, for highly dense weather conditions, 
the model will not be effective. Also, this model does not take into account blurring effects of bad 
weather due to turbulence. distances (of the order of a few kilometers), we assume that the properties 
of the weather condition (say, type of particles and their density) does not change spatially. In other 
words, we only consider homogeneous atmospheres in this paper. Also, given the limited dynamic range 
of the sensors (say, 8 bits per pixel), we do not explicitly model multiple scattering or blurring effects 
of bad weather [23]. 2.1 Attenuation and Airlight The attenuation model describes the way light gets 
attenu­ated as it traverses from a scene point to the observer. Due to atmospheric scattering, a fraction 
of light flux is removed from the incident beam. The unscattered flux, called direct transmission, is 
transmitted to the observer. The attenuated irradiance at the observer is given by (see [17], [21]), 
-;ðAÞd E1ðAÞrðAÞe Edtðd; AÞ¼ ; ð1Þ d2 where, d is the depth of the scene point from the observer and 
A is the wavelength. ;ðAÞ is called the scattering coefficient of the atmosphere; it represents the ability 
of a unit volume of atmosphere to scatter light in all directions. ;ðAÞd is called the optical depth 
of the scene point. E1 is the horizon brightness and r is a function that describes the reflectance properties 
and the sky aperture2 of the scene point. The attenuated irradiance is illustrated by the solid arrow 
from the scene to the sensor in Fig. 1. The sky is assumed to be mostly cloudy or overcast and that the 
radiance from the sky varies smoothly with respect to the polar and azimuth angles of the hemisphere 
[8], [11], [20]. For more details, we refer the reader to [21]. The second atmospheric scattering model 
we consider is called the airlight model. The airlight model quantifies how a column of atmosphere acts 
as a light source by reflecting environmental illumination towards an observer. Refer to the dotted arrows 
in Fig. 1. The light reflected into the line of sight is integrated along the entire path length, d, 
from the scene to the observer. Then, the irradiance due to airlight is given by (see [16]), -;ðAÞd 
Eaðd; AÞ¼E1ðAÞ 1 -e: ð2Þ The total irradiance E received by the sensor is the sum of irradiances due 
to attenuation and airlight, respectively, Eðd; AÞ¼Edtðd; AÞþEaðd; AÞ: ð3Þ 2.2 Wavelength Dependence 
of Scattering Generally, different wavelengths of light are scattered differently by atmospheric particles. 
Interesting atmospheric phenomena such as the blueness of the sky and the bluish haze of distant mountains 
are examples of the wavelength selective behavior of atmospheric scattering [12], [18]. In these cases, 
the blue wavelengths are scattered more compared to other visible wavelengths. On the other hand, fog 
and dense haze scatter all visible wavelengths more or less the same way. Over the visible spectrum, 
Rayleigh s law of atmospheric scattering provides the relationship between the scattering coefficient 
; and the wavelength A [16]: 2. Solid angle subtended by the area of sky visible to a scene point. NARASIMHAN 
AND NAYAR: CONTRAST RESTORATION OF WEATHER DEGRADED IMAGES Fig. 1. Scattering of light by atmospheric 
particles can be described by two models direct transmission (or attenuation) and airlight. Direct transmission 
is the attenuated irradiance received by the sensor from the scene point along the line of sight. Airlight 
is the total amount of environmental illumination (sunlight, skylight, ground light) reflected into the 
line of sight by atmospheric particles. 1 ;ðAÞ/ ; ð4Þ A. where 0 .. .4 depending on the exact particle 
size distribution in the atmosphere. For pure air, the constituent particle (molecules) sizes are very 
small compared to the wavelength of light and hence, there is a strong wavelength dependence of scattering. 
In this case, . ¼4; short (blue) wavelengths dominate and we see the clear blue sky. For fog, the constituent 
particle (water droplets) sizes are large compared to the wavelength of light and, hence, the scattering 
coefficient does not depend on wavelength. So, for fog, . .0; all wavelengths are scattered equally and 
we see grayish (or white) fog. A wide gamut of atmospheric conditions arise from aerosols whose particle 
sizes range between minute air molecules (10-4.m) and large fog droplets (1 -10.m). Such aerosols (e.g., 
mild haze) show a significant wavelength selectivity (0 <.< 4). 2.3 Weather Conditions and Camera Response 
Different cameras measure irradiance over different color bands. Some examples include gray-scale cameras 
(entire visible spectrum), conventional color cameras (three broad bands R, G, and B), and multispectral 
cameras (multiple narrow color bands). In the appendix, we derive an expression for the brightness recorded 
by a monochrome (narrow spectral band) camera, using (3). In this derivation, we assume that the scattering 
coefficient ; remains constant within the spectral bandwidth of the monochrome camera. Keeping the above 
assumption in mind, we now discuss under what weather conditions can our methods be applied to various 
sensors. Recall from Section 2.2 that the scattering coefficient for fog and dense haze remains more 
or less constant over the visible spectrum. Accordingly, a broadband RGB or gray-scale camera suffices 
to analyze images taken in fog and dense haze. For other aerosols such as mild haze, multispectral cameras 
or cameras fitted with narrow-band filters should be used in order to apply our methods satisfactorily. 
Finally, scattering coefficients of most weather conditions vary significantly in the near-IR spectrum 
[37] and hence, narrow-band IR cameras have to be used for the analysis beyond the visible wavelengths. 
In other words, the greater the variation in the scattering coefficient with respect to wavelength, the 
narrower the spectral bandwidth needed for effective results. We would like to clarify that multiple 
color channels are not required for our algorithms. We can, however, apply the methods we describe in 
this paper to each color channel of the sensor independently. This is in contrast to previous methods 
that required at least three color channels (say, R, G, and B) over which the scattering coefficient 
had to be equal [21]. 3CONTRAST DEGRADATION IN BAD WEATHER In this section, we show how contrast degrades 
in poor visibility conditions as a function of both the scattering coefficient of the atmosphere and 
the distance of the scene from the sensor. Consider an image taken in bad weather. The brightness at 
any pixel recorded by a monochrome camera is derived in the appendix: ;d þI1-;d E ¼I1.e -1 -e; ð5Þ where 
I1 is termed as sky intensity. We call . the normalized radiance of a scene point; it is a function of 
the scene point reflectance (BRDF), normalized sky illumination spectrum, and the spectral response of 
the camera, but not the weather condition defined by ð;; I1Þ(see the appendix). Using (5), we formulate 
the image contrast between two adjacent scene points as a function of the amount of scattering and their 
distance from the observer. Consider two adjacent scene points Pi and Pj at the same depth d from a sensor. 
Their pixel intensities are given by, EðiÞ¼I1.ðiÞ-;d þI1-;d e 1 -e; .. ð6Þ EðjÞ¼I1.ðjÞ-;d þI1-;d e 1 
-e: The observed contrast between Pi and Pj can be defined as, EðiÞ-EðjÞ .ðiÞ-.ðjÞ ¼ : ð7Þ EðiÞþEðjÞ 
.ðiÞþ.ðjÞþ2ðe;d -1Þ This shows that the contrast degrades exponentially with the scattering coefficient 
; and the depths of scene points in bad weather. As a result, conventional space-invariant image processing 
techniques cannot be used to completely remove 716 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE 
INTELLIGENCE, VOL. 25, NO. 6, JUNE 2003 weather effects. Note that other formulations for image contrast 
(e.g., MTF, log intensity) [13] can also be used to illustrate the exponential contrast decay. 4CONTRAST 
RESTORATION OF ISO-DEPTH REGIONS We now describe a simple method to restore scene contrast from one bad 
weather image using depth segmentation of the scene. We define depth segmentation as the extraction of 
iso-depth regions in the scene. Note, this does not mean that actual scene depths have to be known. In 
several situations, it may be easy to interactively provide the necessary segmentation. For instance, 
in urban scenes with frontal views of buildings, a user can easily mark out regions that roughly have 
the same depths. Later, we will present two automatic depth segmentation techniques using images taken 
under different weather conditions. Consider an image taken in bad weather. The brightness at any pixel 
recorded by a monochrome camera is given by, -;d E ¼ I1 .e -;d þ I1 1 -e: ð8Þ Now, consider two scene 
points Pi and Pj at the same depth d from a sensor. The observed contrast between Pi and Pj is given 
by (7). Eliminating the unknown e -;d from (6), we obtain, 1 -.ðiÞ-EðiÞ I1 ¼ : ð9Þ 1 -.ðjÞ-EðjÞ I1 For 
robustness, we consider all the pixels at the same depth, 1 -.ðiÞ-EðiÞ I1 P ¼ P : ð10Þ ð1 -.ðjÞÞ ðI1-EðjÞÞ 
jj Then, the normalized radiance of any scene point is obtained using, ! XX -EðiÞ I1 .ðjÞ .ðiÞ¼ 1 -1 
-P : ð11Þ ðI1 -EðjÞÞ jj j This procedure is repeated independently for each depth in the scene. So, 
if we have a priori depth segmentation of the scene and have measured the sky intensity I1, then .ðiÞ 
can be P .ðjÞ computed up to a linear factor : Since . is independent j of the weather condition, we 
have restored the contrast of the scene using just one bad weather image. What can we do if we do not 
have the sky intensity I1? Let us assume that Pj has the minimum brightness within the iso-depth scene 
points: EðjÞ¼ Emin . Also, since scene point brightnesses are much lower than sky brightness on an overcast 
day, i.e., ð8iÞ;I1 >EðiÞ, we can set I1¼ Emax . Then, the right-hand side of (9) is just contrast stretching 
the inverted bad weather image. In other words, by setting .min ¼ 0 .max ¼ 1, and contrast stretching 
(or histogram stretching) each iso-depth region in the image can restore contrast. Even though contrast 
is restored at each depth satisfactorily, the image can look unrealistic. In summary, simple image processing 
techniques such as contrast stretching can be effective for scenes that are at the same depth from the 
sensor (e.g., a planar scene at a fixed distance from the camera). Clearly, for scenes with significant 
depth variations, this simple method will not be effective. 5DEPTH EDGES FROM TWO WEATHER CONDITIONS 
In this section, we present a simple cue to automatically locate the depth edges (discontinuities) present 
in a scene from two monochrome images taken under different but unknown weather conditions. In other 
words, we present a method to label image edges as reflectance edges and depth edges. Several researchers 
have pursued the problem of classifying different types of edges (diffuse, specular, and occluding) based 
on image intensity/color cues [31], [35] and polarization cues [1]. As we shall show, changes in weather 
conditions can be exploited as a cue to differentiate depth edges from reflectance edges. Note that closed 
contours of depth edges can be used for depth segmentation. In outdoor surveillance applications, video 
cameras capture the same scene (albeit with moving objects) over long periods of time during which the 
weather may change. Also, depth edges in the static portion of any scene have to be computed just once 
and not for every video frame. Hence, we see this as an initialization step that needs to be done before 
applying the contrast restoration algo­rithm of Section 4 to all frames. Consider a small image neighborhood 
corresponding to scene points that are at the same depth from an observer (i.e., no depth edges present). 
We call such a neighborhood an iso-depth neighborhood. From (5), the average brightness of an iso-depth 
neighborhood is, -;d -;dÞ E ¼ I1 e. þ I1ð1 -e; ð12Þ and the standard deviation of the neighborhood is, 
s................................... n 1 X. .2 EðiÞ n .E ¼-E: ð13Þ i¼1 Using (5), we simplify to obtain, 
s................................ n X 1 .ðiÞ 2 -;d .E ¼ I1e ð-.Þ: ð14Þ n i¼1 Normalizing the pixel values 
in the neighborhood, we get, EðiÞ .ðiÞ -E -. ¼ q................................... : ð15Þ .E 1 Pn.ðiÞ 
2 ð-.Þ ni¼1 For iso-depth neighborhoods, clearly the above equation is invariant to the weather condition 
ð;; I1Þ: More impor­tantly, the invariance does not hold for a neighborhood that contains depth edges. 
This is easily explained as follows: The airlight does not remain constant across a neighbor­hood with 
depth discontinuities. Hence, subtracting the mean (as in (15)) will not remove the airlight completely. 
Now, let us consider two images captured under different weather conditions. We assume that the two images 
are taken under similar daylight distributions. However, the magnitudes of the distributions (I1) may 
vary. In other words, the shadow edges (if any) appear at the same pixel NARASIMHAN AND NAYAR: CONTRAST 
RESTORATION OF WEATHER DEGRADED IMAGES location in both the images. Figs. 2a and 2b illustrate the brightnesses 
within an iso-depth neighborhood under two weather conditions. Figs. 2c and 2d show that the normalized 
signals under the two weather conditions match perfectly. On the other hand, Fig. 3 illustrates that 
normalized signals of scene neighborhoods that contain depth edges, do not match. Normalized SSD can 
be used to determine the quality of the match. Note that (15) still holds if we apply a more robust estimate 
of mean and standard deviation (for e.g., median of absolute deviations from the neighborhood median). 
It is interesting to note what happens if we treat the entire image as a single neighborhood. Applying 
normalized SSD to two images of a scene, a poor match implies that the weather condition changed between 
the two images and a good match  Fig. 4. Classification of image edges into reflectance edges and depth 
edges. (a) and (b) Images of the same scene captured under different fog conditions (half an hour apart). 
(c) The image in (a) is histogram equalized to aid visualization of depth edges (shown using arrows). 
(d) White pixels denote depth edges and black pixels denote reflectance edges. Note that the edge detector 
was applied to the original image in (a) and not the histogram equalized image in (c).  implies otherwise. 
For this, the scene should have at least two different depths and the images should be linearized using 
the radiometric response function of the camera. This cue is helpful in deciding which frames can be 
used to compute depth edges in a video sequence. Fig. 4 shows the experimental results of classifying 
image edges into reflectance edges and depth edges for a real scene captured under two different foggy 
conditions. The time between the capture of the images was about half an hour. The edge map of one of 
the images was computed using the Canny edge detector. For each edge pixel, we considered 15 . 15 neighborhoods 
around the pixel in the two images. We applied normalized SSD to match these neighborhoods. For the depth 
edges, the normalized SSD value was high; for the reflectance edges, the value was low. The depth edges 
are shown in white and reflectance edges are shown in black (Fig. 4d). Note if both reflectance edges 
and depth edges are within the same neighborhood, this method may misclassify the reflectance edges as 
depth edges. Also, note that shadow edges (if any) will not be distinguished from 718 IEEE TRANSACTIONS 
ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 25, NO. 6, JUNE 2003 reflectance edges. Finally, this 
method to classify edges can be sensitive to noise, especially under poor weather conditions. Under poor 
weather conditions, due to the limited dynamic range of the sensor (typically 8 bits), the direct transmission 
(signal) to airlight (noise) ratio can be so low that the direct transmission magnitude can be compared 
to the sensor noise level. In this case, the results produced by the method may not be trusted. 6SCENE 
STRUCTURE FROM TWO WEATHER CONDITIONS In the previous section, we described a method to locate depth 
discontinuities from two bad weather images. Note, however, that normalized SSD is effective only in 
textured neighborhoods (reflectance edges and depth discontinu­ities). In other words, normalized SSD 
is not reliable for flat intensity regions and regions where depth changes are gradual. Moreover, due 
to the blurring seen in images taken under poor visibility conditions, the edge maps may not be reliable 
enough to create closed contours of depth discontinuities (needed for depth segmentation). In this section, 
we present a method to compute complete structure of an arbitrary scene, from two images taken under 
different weather conditions. In contrast to the methods proposed in [21], [24] that require color images 
(three color channels), our algorithm can be applied to both gray scale as well as color images. Consider 
the observed pixel values E1 and E2 of a scene point under two weather conditions ð;1;I11 Þ and ð;2;I12 
Þ. Let us examine how the brightness of this scene point changes from the first weather condition to 
the second. From (5), -;1d E1 ¼I11 .e -;1 d þI111 -e .. ð16Þ -;2d E2 ¼I12 .e -;2 d þI121 -e: Eliminating 
. from (16) we get, h i -ð;2 -;1 Þd -ð;2 -;1Þd E2 ¼ I12 eE1 þ I121 -e; ð17Þ I11 which is linear in E1 
and E2. Also, for the two weather conditions, the coefficients of the linear equation depend only on 
scene depth. In other words, for iso-depth scene points, the plot of E1 versus E2 is a straight line. 
Another significant constraint results from our physical model that suggests a means of estimating sky 
intensities. Interestingly, if we substitute E1 ¼I11 in (17), we get E2 ¼I12 , irrespective of the depth 
d. Therefore, the point ðI12 ;I11 Þ lies on all the straight lines corresponding to different depths 
in the scene (see Fig. 5). In other words, the intersection of straight lines corresponding to different 
depths yields the sky intensities I11 and I12 . The iso-depth lines in the plot of E1 versus E2 can be 
detected using the Hough transform. Then, the intersection (sky intensities) of the iso-depth lines can 
be computed using a least squares line-fitting algorithm. A problem arises if the iso-depth lines are 
not detected correctly in the plot of E1 versus E2. In other words, scene depths can change smoothly 
and the iso-depth lines could bunch up. In order to  Fig. 5. Plot of the pixel values E1 observed under 
one weather condition versus the corresponding pixel values E2 observed under another weather condition. 
Each line represents all the scene points at the same depth from the sensor. All iso-depth lines intersect 
at the horizon brightnesses (I11 ;I12 ) of the two weather conditions. compute sky intensities, we just 
divide the two images into blocks and within each block we fit lines to the (E2;E1) pairs of scene points. 
If the fit is good, we decide that the scene points in the block are at the same depth. Finally, we use 
at least two such iso-depth blocks to estimate sky the intensities. Substituting the values of I11 and 
I12 in (17), we obtain the scaled depth of each scene point: ð;2 -;1Þd ¼-ln I12 -E2 I11 -E1 -ln I11 I12 
: ð18Þ Thus, we have computed the depth map of a scene from two images taken under different weather 
conditions. 7CONTRAST RESTORATION USING SCENE STRUCTURE In Section 4, we described a method to restore 
scene contrast given a depth segmentation of the scene. This method is simple and effective for scenes 
where depth changes are abrupt (for example, an urban scene with frontal views of buildings). However, 
it is hard to define good depth segmentation when scene depths change gradually (for instance, a natural 
scene with mountains or an urban scene with a oblique view of a road). In this section, we present a 
method to restore contrast of an arbitrary scene using scaled depths (18) of scene points. We assume 
that there exists a patch Ezero in the scene whose direct transmission is zero. This can happen in two 
instances. First, Ezero can be a black patch with its scene radiance equal to zero. Note that the black 
scene patch will not appear black in the image due to the addition of airlight. Second, Ezero could be 
a distant scene patch that is completely invisible due to strong airlight. In other words, this distant 
scene patch has zero direct transmission and its contrast cannot be restored from a bad weather image. 
We can either mark such a patch manually or detect one automatically from the image. To detect zero direct 
transmission patches automatically in weather degraded images, we use the method described in [21]. Since 
the NARASIMHAN AND NAYAR: CONTRAST RESTORATION OF WEATHER DEGRADED IMAGES apparent brightness of the 
patch Ezero is solely due to airlight, its optical depth can be computed as, ;dzero ¼-ln ð1 -Ezero=I1Þ: 
ð19Þ Then, the optical depth of any other scene point Pi is obtained using, di ;di ¼ð;dzeroÞ ; ð20Þ 
dzero where the second term can be computed using the ratio of scaled depths (see (18)). Then, the normalized 
radiance .i of the scene point Pi is estimated using (5). Recall that . does not depend on the weather 
condition ð;; I1Þ: Thus, by computing . for each scene point, we restore contrast of the entire scene. 
Note that structure computation requires two images to be taken under different weather conditions, but 
under similar daylight spectra. However, once scene structure is computed, contrast can be restored from 
a single image of the scene taken under arbitrary weather and illumination conditions. 8EXPERIMENTAL 
RESULTS We performed experiments with both synthetic and real scenes. Fig. 6a shows a synthetic scene 
consisting of a stack of cylinders with random brightness values. To this image, two different amounts 
of fog are added according to the model described in (5). To this image, Gaussian random noise of . ¼3:0 
gray levels was added. Figs. 7a and 7b shows the results of applying the structure computation and contrast 
restoration algorithms to images of the synthetic scene. The error in the recovered depth map was less 
than 1 percent. Results of experiments performed on images of real scenes are shown in Figs. 8 and 9. 
The images were captured Fig. 7. Experiments on a synthetic scene a stack of discs textured with random 
gray dots. Two images of a synthetic scene with different amounts of fog are shown in Fig. 6b. (a) Iso-depth 
lines shown in the plot of pixel values under the first weather condition versus the correspond­ing pixel 
values under the second weather condition. X mark shows the intersection ðI12 ;I11 Þ of all the iso-depth 
lines. (b) The recovered structure and contrast restored image. Compare (b) with the original synthetic 
scene in Fig. 6a. using a Professional KODAK DCS 315 digital camera. Multiple exposures of the same scene 
were acquired and the radiometric response function of the camera was computed using the method proposed 
in [19]. Then, the multiple exposed images were linearized and combined using simple weighted averaging 
to obtain a high dynamic range image of the scene. Fig. 8a shows two high dynamic range images of the 
same scene captured under different conditions of mist (light and moderate). The depth map computed using 
the algorithm mentioned in Section 6 is shown in Fig. 8b. The mist was removed using the contrast restoration 
algorithm mentioned in Section 7. Notice the windows of the farther buildings that are clearly visible 
in Fig. 8d as compared to the images in Fig. 8a. We performed experiments under rainy conditions also. 
Since we are interested in a far away scene, the captured image does seem like a foggy or a misty one 
due to spatio­temporal averaging in the sensor. The results proved that we can apply our algorithm to 
rainy images of faraway scenes as well. In this case, we just captured one image under mild rain conditions 
shown in Fig. 9a. The depth map, precomputed from the misty images shown in Fig. 8a, was used to restore 
the contrast of the rainy day image. Thus, changes in weather conditions are required only to compute 
scene structure whereas contrast restoration can be applied to a single image of that scene taken under 
arbitrary weather conditions. Compare the results of our algorithm (Fig. 9d) with conventional histogram 
equalization (Fig. 9b). In general, removing the spatio-temporal effects of rain is a much harder problem 
compared to more stable weather 720 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 
25, NO. 6, JUNE 2003 Fig. 8. Structure computation and restoration of image contrast from two images 
taken under poor visibility conditions. The depth map is median filtered and averaged to reduce noise. 
Notice the significant increase in contrast in the farther buildings. Contrast stretching is applied 
to all the images for display purposes. (a) Images taken at 3 P.M. and 4 P.M. under poor visibility conditions 
(mist). (b) Computed depth map. (c) Contrast restore image. conditions such as fog, mist, and haze. The 
brightnesses due to raindrops in the scene cannot be modeled using the simple direct transmission and 
airlight models used in this paper. 8.1 Experiments with Video: Moving Objects Consider an outdoor surveillance 
video camera capturing a scene (with moving objects) over an extended period of time. We would like to 
process this video in real-time to obtain a weather-free video. Note that our algorithms cannot remove 
temporal effects of rain from a video of a rainy scene. For the purposes of discussion, we define the 
static part of the scene as the background and the moving objects in the scene as the foreground. The 
foreground objects can be separated from the background using any background subtraction method (for 
instance, [32]). Then, weather-free video is obtained using an algorithm that has the following two stages: 
. Initialization stage. We first detect any change in weather condition using normalized SSD (Section 
5). Then, the two frames that correspond to the different weather conditions are used to compute scaled 
depths of the background scene (Section 6). . Contrast Restoration. Note that the methods we described 
hitherto cannot be used to restore contrast of moving objects since their depths are unknown. Therefore, 
heuristics are needed to assign depths to foreground objects. One conservative heuristic is to examine 
the depths in a neighborhood around each moving object and assign the minimum depth to it. The algorithm 
presented in Section 7 can then applied to the entire frame to restore scene contrast. Experimental results 
with a video of a traffic scene taken under foggy conditions are shown in Fig. 10. We used an off­the-shelf 
8-bit digital video camera and captured two short video clips half an hour apart. As described in previous 
experiments, we linearized the frames with the radiometric response function of the video camera. We 
averaged 100 frames in each video clip to reduce noise and used the resulting images to compute structure 
of the background scene (buildings). The scaled depths in the road region were linearly interpolated 
using scaled depth values at pixels on the left and right corners of the road. Then, contrasts of buildings, 
the road, and moving vehicles were restored for each frame of the video. Notice the significant increase 
in contrast at various depths in the scene (Figs. 10d and 11). Compare our method to histogram equalization 
in Fig. 12. In our current implementation, contrast restoration was applied to the video offline. 9SUMMARY 
In this paper, we addressed the problem of restoring the contrast of atmospherically degraded images 
and video. We presented methods to locate depth discontinuities and to compute structure of a scene, 
from two images captured under different weather conditions. Using either depth segmentation (regions 
within closed contours of depth edges) NARASIMHAN AND NAYAR: CONTRAST RESTORATION OF WEATHER DEGRADED 
IMAGES Fig. 9. Contrast restoration from one bad weather (in this case, rain) image and precomputed 
scene structure. Scene structure can be computed using two images taken under possibly different weather 
conditions (say, mist). The depth map computed from two misty images (Fig. 8a) was used to restore contrast 
from just one image of the same scene under rain. The rainy image shown in (a) and the misty images shown 
in Fig. 8a were captured on different days. (b) Applying histogram equalization to the entire rainy image 
does not enhance contrast in all depths. (c) Contrast restoration using the algorithm proposed in Section 
7. or scene structure (scaled depths), we then showed how to restore contrast from any image of the scene 
taken in bad weather. Note, although structure computation requires changes in weather, the contrast 
restoration algorithms do not. The entire analysis is presented for monochrome images. However, our methods 
can be applied to images captured using multispectral cameras, IR cameras, and the usual broadband RGB 
and gray-scale cameras. APPENDIX MONOCHROME CAMERA SENSING IN BAD WEATHER In this section, we derive 
an expression for the intensity E,of a scene point under bad weather, recorded by a camera within a narrow 
wavelength band ðA; A þ.AÞ. From (3) we write, Z Aþ.A E ¼ sðAÞðEdtðd; AÞþEaðd; AÞÞdA; ð21Þ A where 
sðAÞ is the spectral response of the camera. We assume that the scattering coefficient ; does not change 
appreciably over the narrow spectral band and write, -;d Z Aþ.A e E ¼ E1ðAÞsðAÞrðAÞdA ... d2 A ð22Þ . 
.Z Aþ.A -;d þ 1 -eE1ðAÞsðAÞdA: A The sky illumination spectrum can be written as, E1ðAÞ¼I01E b1ðAÞ; 
ð23Þ where I01 is the magnitude of the sky illumination b spectrum and E 1ðAÞ is the normalized sky 
illumination spectrum. Letting Z Aþ.A b g ¼ E 1ðAÞsðAÞdA; A Z Aþ.A 1 ð24Þ b . ¼ E 1ðAÞsðAÞrðAÞdA; gd2 
A I1¼I0 1g; we rewrite the final brightness at any pixel as, ;d þI1ð1 -e -;dÞ; E ¼I1.e -ð25Þ where 
I1 is termed as sky intensity. Note that . is a function of normalized sky illumination spectrum, scene 
point reflectance, and the spectral response of the camera, but not the weather condition ;. The algorithm 
we present in the paper recovers . for each pixel to restore scene contrast. Let us now examine the wavelength 
range in which this model can be applied. By changing the limits of integration to ½A1;A2., and assuming 
the scattering coefficient to be constant over this wavelength band, we can use the same model for a 
722 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 25, NO. 6, JUNE 2003 Fig. 10. 
(a) Scene imaged at 5:00 P.M. (b) Scene imaged at 5:30 P.M. (c) Depth map computed from images (a) and 
(b). (d) Contrast restored using image (b). Experiments with videos of a traffic scene on a foggy day. 
(a) and (b) Two short video clips were captured half an hour apart using an 8-bit video camera. 100 frames 
were averaged to reduce noise. Note that the vehicles on the road in the two images are different. (c) 
The depth map was computed for the background image using the algorithm presented in Section 5. The scaled 
depths of the region corresponding to the road were linearly interpolated using scaled depth values at 
pixels on the left and right corners of the road. (d) The defogged (contrast restored) image obtained 
from the video frame in (b). Compare the contrast restored image with the histogram equalized image in 
Fig. 12. Fig. 11. Zoomed in regions of the frame (see the marked rectangles in Fig. 10b) demonstrate 
the significant increase in contrast at various depths of the scene. Note that different amounts of fog 
were removed at different depths. Also, notice the better contrast of moving objects (vehicles). black 
and white camera (entire visible range), or smaller color ACKNOWLEDGMENTS bands (R, G, B) for a color 
camera, or narrow band multi-This work was supported in part by a DARPA/ONR spectral cameras. Thus, for 
removal of fog and dense haze, we HumanID Contract (N00014-00-1-0916) and an US National can use RGB 
color or gray-scale cameras whereas we must use Science Foundation Award (IIS-99-87979). The authors 
thank narrow spectral band cameras for the removal of many Yoav Schechner for valuable discussions on 
this topic. Some aerosols. of the results presented in this paper have appeared in the NARASIMHAN AND 
NAYAR: CONTRAST RESTORATION OF WEATHER DEGRADED IMAGES Proceedings of the 2001 IEEE Conference on Computer 
Vision and Pattern Recognition [22]. REFERENCES [1] T.E. Boult and L.B. Wolff, Physically-Based Edge 
Labelling, Proc. IEEE Conf. Computer Vision and Pattern Recognition, 1991. [2] B. Cairns, B.E. Carlson, 
A.A. Lacis, and E.E. Russell, An Analysis Of Ground-Based Polarimetric Sky Radiance Measurements, Proc. 
SPIE, vol. 3121, 1997. [3] S. Chandrasekhar, Radiative Transfer. Dover Publications, Inc., 1960. [4] 
D.B. Chenault and J.L. Pezzaniti, Polarization Imaging through Scattering Media, Proc. SPIE, vol. 4133, 
2000. [5] K.L. Coulson, Polarization of Light in the Natural Environment, Proc. SPIE, vol. 1166, 1989. 
[6] L.J. Denes, M. Gottlieb, B. Kaminsky, and P. Metes, Aotf Polarization Difference Imaging, Proc. SPIE, 
vol. 3584, 1998. [7] S.D. Gedzelman, Atmospheric Optics in Art, Applied Optics, vol. 30, 1991. [8] J. 
Gordon and P. Church, Overcast Sky Luminances and Directional Luminous Reflectances of Objects and Backgrounds 
under Overcast Skies, Applied Optics, vol. 5, p. 919, 1966. [9] L.L. Grewe and R.R. Brooks, Atmospheric 
Attenuation Reduction through Multisensor Fusion, Sensor Fusion: Architectures, Algo­rithms, and Applications 
II, Proc. SPIE, vol. 3376, Apr. 1998. [10] Van De Hulst, Light Scattering by Small Particles. John Wiley 
and Sons, 1957. [11] R.L. Lee Jr., Horizon Brightness Revisited: Measurements and a Model of Clear-Sky 
Radiances, Applied Optics, vol. 20, pp. 4620­4628, 1994. [12] N.S. Kopeika, General Wavelength Dependence 
of Imaging through the Atmosphere, Applied Optics, vol. 20, no. 9, May 1981. [13] N.S. Kopeika, A System 
Engineering Approach to Imaging. SPIE Press, 1998. [14] D.K. Lynch, Step Brightness Changes of Distant 
Mountain Ridges and Their Perception, Applied Optics, vol. 30, 1991. [15] S. Mahadev and R.C. Henry, 
Color Perception through Atmo­spheric Haze, J. Optical Soc. Am. A, vol. 17, no. 5, May 2000. [16] E.J. 
McCartney, Optics of the Atmosphere: Scattering by Molecules and Particles. John Wiley and Sons, 1975. 
[17] W.E.K. Middleton, Vision through the Atmosphere. Univ. of Toronto Press, 1952. [18] M. Minnaert, 
The Nature of Light and Color in the Open Air. Dover Publications, Inc., 1954. [19] T. Mitsunaga and 
S.K. Nayar, Radiometric Self Calibration, Proc. IEEE Conf. Computer Vision and Pattern Recognition, 1999. 
[20] P. Moon and D.E. Spencer, Illumination from a Non-Uniform Sky, Illuminating Eng., vol. 37, pp. 707-726, 
1942. [21] S.G. Narasimhan and S.K. Nayar, Chromatic Framework for Vision in Bad Weather, Proc. IEEE 
Conf. Computer Vision and Pattern Recognition, 2000. [22] S.G. Narasimhan and S.K. Nayar, Removing Weather 
Effects from Monochrome Images, Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2001. [23] 
S.G. Narasimhan and S.K. Nayar, Vision and the Atmosphere, Int l J. Computer Vision, vol. 48, no. 3, 
pp. 233-254, Aug. 2002. [24] S.K. Nayar and S.G. Narasimhan, Vision in Bad Weather, Proc. Seventh Int 
l Conf. Computer Vision, 1999. [25] J.P. Oakley and B.L. Satherley, Improving Image Quality in Poor Visibility 
Conditions Using a Physical Model for Degradation, IEEE Trans. Image Processing, vol. 7, Feb. 1998. [26] 
 Museum of Science, Leonardo s Perspective. http://www. mos. org/sln/Leonardo/InvestigatingAerialP.html, 
1997. [27] M.J. Rakovic, G.W. Kattawar, M. Mehrubeoglu, B.D. Cameron, L.V. Wang, S. Rastegar, and G.L. 
Cote, Light Backscattering Polarization Patterns from Turbid Media: Theory and Experi­ment, Applied Optics, 
vol. 38, 1999. [28] M.P. Rowe, E.N. Pugh Jr., J.S. Tyo, and N. Engheta, Polarization-Difference Imaging: 
A Biologically Inspired Technique for Ob­servation through Scattering Media, Optical Letters, vol. 20, 
1995. [29] Y.Y. Schechner, S.G. Narasimhan, and S.K. Nayar, Instant Dehazing of Images Using Polarization, 
Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2001. [30] Y.Y. Schechner, S.G. Narasimhan, 
and S.K. Nayar, Polarization Based Vision through Haze, Applied Optics, special issue: light and color 
in the open air, vol. 42, no. 3, Jan. 2003. [31] S. Shafer, Using Color to Separate Reflection Components, 
Color Research and Applications, pp. 210-218, 1985. [32] C. Stauffer and W.E.L. Grimson, Adaptive Background 
Mixture Models for Real-Time Tracking, Proc. IEEE Conf. Computer Vision and Pattern Recognition, 1999. 
[33] K. Tan and J.P. Oakley, Enhancement of Color Images in Poor Visibility Conditions, Proc. Int l Conf. 
Image Processing, vol. 2, Sept. 2000. [34] K. Tan and J.P. Oakley, Physics Based Approach to Color Image 
Enhancement in Poor Visibility Conditions, J. Optical Soc. Am. A, vol. 18, no. 10, pp. 2460-2467, Oct. 
2001. [35] S. Ullman, On the Visual Detection of Light Sources, Biological Cybernetics, pp. 205-212, 
1976. [36] J.G. Walker, P.C.Y. Chang, and K.I. Hopcraft, Visibility Depth Improvement in Active Polarization 
Imaging in Scattering Media, Applied Optics, vol. 39, 1995. [37] W.L. Wolfe and G.J. Zissis, The Infrared 
Handbook. Prepared for Office of Naval Research, Dept. of the Navy, 1978. [38] Y. Yitzhaky, I. Dror, 
and N.S. Kopeika, Restoration of Altmo­spherically Blurred Images According to Weather-Predicted Atmospheric 
Modulation Transfer Function, Optical Eng., vol. 36, Nov. 1998. 724 IEEE TRANSACTIONS ON PATTERN ANALYSIS 
AND MACHINE INTELLIGENCE, VOL. 25, NO. 6, JUNE 2003 Srinivasa G. Narasimhan received the MS degree in 
computer science from Columbia University in 1999. He is currently pursuing the PhD degree in computer 
science at the Columbia University Vision Laboratory. His research is focused on the development of physics-based 
models and algorithms for scene understanding in bad weather. His paper received the Best Paper Honorable 
Mention Award at the 2000 IEEE Computer Vision and Pattern Recognition Conference. Shree K. Nayar received 
the PhD degree in electrical and computer engineering from the Robotics Institute at Carnegie Mellon 
University in 1990. He is the T.C. Chang Professor of Computer Science at Columbia University. He currently 
heads the Columbia Automated Vision Environment (CAVE), which is dedicated to the development of advanced 
computer vision sys­tems. His research is focused on three areas, namely, novel vision sensors, physics-based 
models for vision, and algorithms for scene understanding. His work is motivated by applications in the 
fields of computer vision, computer graphics, human-machine interfaces, and robotics. Dr. Nayar has authored 
and coauthored papers that have received the Best Paper Honorable Mention Award at the 2000 IEEE Computer 
Vision and Pattern Recognition Conference (CVPR), the David Marr Prize at the 1995 International Conference 
on Computer Vision (ICCV) held in Boston, Siemens Outstanding Paper Award at the 1994 IEEE CVPR Conference 
held in Seattle, the 1994 Annual Pattern Recognition Award from the Pattern Recognition Society, Best 
Industry Related Paper Award at the 1994 International Conference on Pattern Recognition held in Jerusalem, 
and the David Marr Prize at the 1990 ICCV held in Osaka. He holds several US and international patents 
for inventions related to computer vision and robotics. He was the recipient of the David and Lucile 
Packard Fellowship for Science and Engineering in 1992, the National Young Investigator Award from the 
US National Science Foundation in 1993, and the Excellence in Engineering Teaching Award from the Keck 
Foundation in 1995. . For more information on this or any other computing topic, please visit our Digital 
Library at http://computer.org/publications/dlib. Polarization-based vision through haze Yoav Y. Schechner, 
Srinivasa G. Narasimhan, and Shree K. Nayar We present an approach for easily removing the effects of 
haze from passively acquired images. Our approach is based on the fact that usually the natural illuminating 
light scattered by atmospheric particles (airlight) is partially polarized. Optical .ltering alone cannot 
remove the haze effects, except in restricted situations. Our method, however, stems from physics-based 
analysis that works under a wide range of atmospheric and viewing conditions, even if the polarization 
is low. The approach does not rely on speci.c scattering models such as Rayleigh scattering and does 
not rely on the knowledge of illumination directions. It can be used with as few as two images taken 
through a polarizer at different orientations. As a byproduct, the method yields a range map of the scene, 
which enables scene rendering as if imaged from different viewpoints. It also yields information about 
the atmospheric particles. We present experimental results of complete dehazing of outdoor scenes, in 
far-from-ideal conditions for polarization .ltering. We obtain a great improvement of scene contrast 
and correction of color. &#38;#169; 2003 Optical Society of America OCIS codes: 290.1310, 330.0330, 260.5430, 
100.2000, 100.3020, 150.5670. 1. Introduction Recently there has been a growing interest in the analysis 
of images acquired in poor-visibility condi­tions. The main objective has been to enhance1 4 images taken 
in poor visibility and even restore clear-day visibility of the scene.5 7 Some methods are based on specialized 
radiation sources and de­tection hardware.8,9 For natural light, visibility degradation effects due to 
haze vary as distances to the objects increase,10,11 and are referred to as aer­ial perspective.12 For 
this reason, some image­enhancement methods proposed in the past require prior information about the 
distances of objects3,4 or about the scene colors.4 It has been observed that aerial perspective can 
actually be exploited to obtain an estimated range map6,7,13 of the scene. Computer vision methods have 
restored clear-day visibility of scenes using nei­ther special radiation sources nor exact external knowledge 
about the scene structure or aerosols.5,7 These methods relied only on the acquired images but required 
weather conditions to change between im- The authors are with Columbia Automated Vision Environment, 
Department of Computer Science, Columbia University, New York, New York 10027. Y. Schechner s e-mail 
address is yoav@cs.columbia.edu. Received 18 January 2002; revised manuscript received 8 April 2002. 
0003-6935/03/030511-15$15.00/0 &#38;#169; 2003 Optical Society of America age acquisitions. This can 
take too long to make dehazing practical. In this paper we describe an ap­proach that does not need the 
weather conditions to change and can thus be applied instantly. Our approach is based on analyzing images 
taken through a polarizer. Polarization .ltering has long been used in photography through haze.14 Relying 
only on optical .ltering is, however, restrictive: It is suf.cient only on clear days, with weak light 
scatter­ing (mainly due to air molecules), when the Sun is =90° to the viewing direction.14,15 In these 
situa­tions photographers have set the polarization .lter at an orientation that best improves image 
contrast. In general, however, polarization .ltering alone can­not remove the haze from images. Our method 
fur­ther analyzes optically .ltered images to obtain signi.cantly better results. The effects of scattering 
on light polarization have been extensively studied.16 21 Polarization has mainly been considered in 
the context of arti.cial illumination,22 26 where the signal to be recovered is associated with polarized 
light while the light scat­tered by the medium is associated with depolarized light.22 27 It has also 
been observed that an object masked by scattered light is enhanced by a linear superposition of polarization-.ltered 
images.28 In contrast we show that in hazy conditions, polariza­tion can be associated with scattering 
of ambient illumination, rather than the signal (object radi­ance). Solving inverse problems associated 
with polarization-.ltered images has proved to be useful in other regimes of scene analysis. For example, 
it 20 January 2003 / Vol. 42, No. 3 / APPLIED OPTICS 511  Fig. 1. (Dashed rays) Light coming from the 
illuminant (e.g., Sun) and scattered toward the camera by atmospheric particles is the airlight (path 
radiance) A. The airlight increases with the distance z of the object. (Solid ray) The light emanating 
from the object is attenuated along the line of sight as z increases, leading to the direct transmission 
D. Without scattering, object radiance would have been Lobject . The scene is imaged through a polarizing 
.lter at angle .. The polarization component parallel to the plane of incidence is best transmitted through 
the .lter at .=.I. was used to separate transparent and semire.ected scenes,29,30 analyze specularities,31 
33 and classify materials.34 In this paper we describe an image-formation model that accounts for natural 
polarization effects in imaging through haze. We then invert the image­formation model to recover the 
dehazed scene and also to obtain information about scene structure and atmospheric properties. Our approach 
does not re­quire modeling of the scattering particles size or their precise scattering mechanisms. The 
principle is simple: The image is composed of two unknown components the scene radiance in the absence 
of haze, and airlight (the natural illumination scattered toward the viewer). To recover these two un­knowns, 
we need two independent images. We eas­ily obtain these images because airlight is usually partially 
polarized. The method requires only that the airlight induce detectable partial polarization. We demonstrate 
removal of haze effects from real scenes in situations in which pure optical .ltering (without applying 
our algorithm) does not suf.ce at all. 2. Theoretical Background As depicted in Fig. 1, when imaging 
through the at­mosphere we sense two sources. The .rst source is the scene object whose radiance is attenuated 
by scat­tering. The corresponding signal reaching the cam­era is called the direct transmission. The 
second source is the ambient illumination (Sun, sky, and so on). The part of the illumination scattered 
toward 7,10,11,35,36 the camera by aerosols is called the airlight.It is also referred to in the literature 
as path radi­ance10 and veiling light.37 In this section we de­scribe each of these signals and the polarization 
effects associated with them. A. Airlight Polarization One of the causes of image degradation associated 
with atmospheric scattering is airlight. The atmo­sphere scatters light coming from the illumination 
sources (e.g., the Sun) toward the viewer7 (see Fig. 1). The airlight increases with the distance z from 
the object. As discussed in Subappendix A.2, A . A.[1 . t(z)], (1) where A. is the airlight radiance 
corresponding to an object at an in.nite distance, e.g., the horizon. t(z) is the transmittance of incoherent 
light,35 given by z t(z) . exp[(z.)dz., (2) 0 where . is the coef.cient of extinction due to scatter­ing 
and absorption (see Subappendix A.2). When the extinction coef.cient is distance invariant, [(z.) =[, 
then t(z) . exp(-[z). (3) Assume for a moment that the illumination of any scattering particle comes 
from one direction (one il­lumination source). The light ray from the source to a scatterer and the line 
of sight from the camera to the scatterer de.ne a plane of incidence. We divide the airlight into two 
polarization components that are parallel and perpendicular to this plane, A. and A. , respectively. 
The airlight degree of polarization is then p . ( A. . AI)/A, (4) where A . A. . A. (5) is the total 
radiance due to airlight, given in Eq. (1). The degree of polarization greatly varies as a func­ 512 
APPLIED OPTICS / Vol. 42, No. 3 / 20 January 2003 tion of the viewing and illumination directions, the 
density of the scattering particles, and their sizes. Depending on the size distribution of the scattering 
particles,17,18,38 the airlight is partially linearly polar­ized either perpendicular to the plane of 
incidence15,39 or parallel to it. When scattering is dominated by independent air molecules and small 
dust particles (Rayleigh scattering), A. 2 AI. On the other hand, larger haze particles may cause A. 
< AI. We use the following convention throughout this paper: To avoid confusion without loss of generality, 
we associ­ate the parallel component notation (I) with the min­imum measured radiance at a pixel and 
perpendicular component notation (.) with the max­imum radiance. We now explain the effectiveness of 
polarization in various haze and illumination condi­tions. 1. Trivial Case Under special conditions, 
optical .ltering alone is suf­.cient to remove haziness in images. For Rayleigh scattering,14,15,34,36,40 
the degree of polarization, p,is (sin2 <)/(1 + cos2 <), where < is the scattering angle (the angle between 
the illumination ray and the line of sight). Only when the light source is normal to the viewing direction 
is the airlight totally polarized (p = 1) perpendicular to the plane of incidence. Thus it can be eliminated 
if the image is captured through a polarizing .lter oriented parallel to the plane of incidence. Dehazing 
in this case is trivial, since it is achieved with optical .ltering alone. This situation is quite restricted, 
since it occurs only when the aerosols are very small and when the Sun is in a favorable position. 2. 
General Case In general, the airlight will not be completely polar­ized.36 Thus a polarizing .lter cannot 
remove the airlight on its own. For example, in Rayleigh scat­tering the degree of polarization p decreases 
as the direction of illumination deviates from 90° (relative to the viewing direction). Reduction of 
polarization is caused by scattering from large haze particles, which never completely polarize light. 
Moreover, airlight due to large haze particles may be polarized orthog­onally to light scattered by air 
molecules17,18,38 caus­ing partial annihilation of polarization. The degree of polarization p is also 
decreased by depolarization. This is caused by multiple scatterings from multiple directions: An illuminant 
of a scattering particle may be another particle in the air (e.g., a haze par­ticle, a cloud drop, a 
molecule creating the skylight). Multiple scatterings14,15,39,40 are more probable when the density of 
scatterers is high (poorer visibility). To make matters more complicated, these mecha­nisms depend on 
the wavelength.14,15 Fortunately, our algorithm does not require explicit modeling of the precise mechanisms 
of scattering. The method is based on the fact that even a partial polarization of the airlight can be 
exploited in post­processing to remove scattering effects. This degree of polarization needs to be signi.cant 
enough to be detected by the sensor. For this reason we concen­trate in this paper on vision through 
haze, in which multiple scattering is much weaker than in fog. There are some weather conditions under 
which the algorithm may not be effective, as discussed in Sec­tion 8. B. Direct Transmission Polarization 
In addition to the presence of airlight, the scattering medium degrades images by attenuating the light 
emanating from scene objects. Let the object radi­ance be Lobject in the absence of haze (scattering) 
between the scene and the viewer. When haze is present, as a ray of light progresses toward the viewer 
(Fig. 1), part of its energy is scattered in other directions, and a small portion of it may be absorbed. 
Thus the radiance sensed by the viewer is an atten­uated fraction of Lobject, called the direct transmis­sion.7 
As a function of the distance z the direct transmission is D = Lobjectt(z), (6) where t(z) is given 
in Eq. (2). We make three approximations in this paper. First, we concentrate on the degradation due 
to the attenuation of the signal and due to the additive air­light. We do not deal with image blur. Second, 
we take single-scattering effects to be dominant over multiple-scattering effects, which cause image 
blur and reduce the degree of polarization along the line of sight. Finally, we assume that light emanating 
from scene objects has insigni.cant polarization. It follows from the last assumption that the polar­ization 
of the direct transmission is insigni.cant. If the scattering particles have random orientations, then 
the directly transmitted light will not be polar­ized in any macroscopically preferable orientation. 
Hence the polarization state of the unscattered light does not change,39,40 although the radiance is 
atten­uated. The last assumption is invalid for specular sur­faces. Nevertheless, when a specular object 
is far enough, its direct transmission makes a negligible contribution to the measured polarization. 
The rea­son is that the direct transmission decreases [Eq. (6)] whereas airlight10 increases [Eq. (1)] 
with distance. Thus airlight and its polarization dominate the mea­sured light for distant objects. Hence 
the model be­comes more accurate where it is needed most for distant objects that are most affected by 
haze. This property is useful if we know the relative distances to the scene objects. Recall that airlight 
is just the aggregation of light scattered by particles at various distances along the line of sight. 
Since the polarization of this light does not change along the line of sight39,40 p [Eq. (4)] does not 
depend on the distance. The observations regarding the polarization de­scribed in Subsections 2.A and 
2.B are unaffected by the exact dependence of t on z. The dominance of airlight polarization means that 
the polarizing .lter 20 January 2003 / Vol. 42, No. 3 / APPLIED OPTICS 513  Fig. 2. At each point the 
minimum measured image irradiance as a function of .is II . The maximum is I. . The difference between 
these measurements is due to the difference between the airlight components AI , A. . This difference 
is related to the unknown airlight A by the parameter p, which is the airlight degree of polarization. 
Without a polarizer the image irradiance is Itotal, which is proportional to the sum of airlight and 
the unknown direct transmission. modulates mainly the measured airlight but not the light originating 
from the objects. This is the key to subsequent calculations that remove the effects of haze. 3. Image 
Formation The scene radiance is measured by the detector plane of the camera. The detected image irradiance 
is pro­portional to scene radiance. Since the proportional­ity depends on the optical system parameters 
and not on the weather effects, we treat the image irradiance and the scene radiance as equivalent. The 
overall radiance we sense is the incoherent sum of the air­light and the direct transmission. Without 
mount­ing a polarizer on the camera, the image irradiance is Itotal = D . A, (7) up to the said proportionality 
factor. It has been shown10 that except for rather close objects (for which [z <0.2), Itotal is typically 
dominated by the airlight and not by the direct transmission. Thus typically most of the light we measure 
is not attributed to the signal D, whose origin is Lobject. This is reinforced by the fact that most 
terrestrial objects have a low albedo, further decreasing the signal. When a linear polarizer is mounted 
on the camera, the image irradiance changes as a function of the polarizer orientation angle .. Figure 
2 describes the irradiance at a single pixel. The irradiance is a co­sine function of .. On average, 
the image irradiance is Itotal/2. (We do not deal here with the global ab­sorptivity of unpolarized light 
that is common in sheet linear polarizers. Although this absorptivity effects the image irradiance, it 
does not modulate the effects of haze.) One of our goals is to decouple the airlight and direct transmission. 
Since we assume that direct transmission is not polarized, its energy is evenly distributed between the 
polarization components. The variations due to the polarizer rotation are as­sumed to be mainly due to 
airlight. As seen in Fig. 2, when the polarizing .lter is oriented such that the image irradiance is 
minimal (.=.I), we measure II = D/2 . AI , (8) where [from Eqs. (4) and (5)] AI = A(1 . p)/2. (9) This 
is the best state of the polarizer because here the image irradiance is the closest to the irradiance 
cor­responding to the direct transmission (except for a factor of 1/2). There is a difference between 
II and D/2, because the airlight is not completely polarized (AI . 0). In Section 4 we recover D by comparing 
two images taken with different orientations of the polarizer. For instance, one image can be II, whereas 
the other, I. = D/2 . A. , (10) is acquired when the .lter is oriented perpendicular to .I. From Eqs. 
(4) and (5), A. = A(1 . p)/2. (11) From Eqs. (5), (8), and (10), Itotal = II . I. . (12) Note that 
I. is the worst state of the polarizer, be­cause the airlight is enhanced relative to the direct transmission. 
To dehaze the image, we .rst have to remove the airlight A. The key step here is the es­timation of p, 
the degree of polarization of airlight. As shown in Fig. 2, p relates the unknown airlight A to the difference 
between the image irradiances I. and II . Acquisition of polarimetric images is easy and fea­sible at 
the video rate.34,41 44 However, for demon­stration purposes we photographed the scene on a Fuji Sensia 
100 slide .lm, using a common SLR (single­lens-re.ex)(Cannon EOS-5)camera. The slides were scanned by 
a Nikon LS2000 35-mm .lm scanner. Be­fore processing the images by the algorithm described in the following 
sections, we linearized the raw photo­graphs to compensate for the system s radiometric re­sponse. This 
response was estimated from images of the Macbeth ColorChecker.45 We modulated the po­larization by mounting 
a standard linear polarizer on the camera lens. We took images of a distant scene at two different orientations 
of a polarizer approximately correspond­ing to the perpendicular and the parallel airlight po­larization 
components. The images are shown in Fig. 3. The raw images were not acquired in the trivial situation 
described in Subsection 2.A.1: The Sun was almost behind the camera (the Sun was in the south, and the 
picture was taken toward the north), whereas the haze was rather dense. For this 514 APPLIED OPTICS / 
Vol. 42, No. 3 / 20 January 2003 Fig. 3. Images of the polarization components corresponding to the 
minimal and the maximal radiances. Note that II (the image of irradiance) has the best image contrast 
that optics alone can yield, and yet there is no signi.cant improvement over the image of the worst polarization 
state. reason, II has only a slight improvement of image con­trast relative to the contrast in I. . Because 
of the partial polarization of the airlight, II was lower than I. . For clarity of display, the brightness 
of each of the photos shown in Fig. 3 is contrast stretched. 4. Dehazing Images Both the attenuation 
[Eq. (6)] and the airlight [Eq. (1)] depend on the distance z of the scene point. The distance to the 
objects is spatially varying, since dif­ferent image pixels (x, y) correspond to objects at different 
distances. Therefore compensation for the effects of haze is spatially varying. Our dehazing method automatically 
accounts for this spatial vari­ation of scene depth. For each image pixel we have three unknowns: the 
object radiance (without haze) Lobject, the airlight A, and the transmittance t(z). These determine the 
irradiance at each image pixel. The airlight is re­lated to t(z) by Eq. (1). Thus the number of unknowns 
per pixel is reduced to two. These unknowns can be estimated from two images taken at almost any gen­eral 
unknown (but nondegenerate) orientations of the polarizing .lter. We actually do not need the four measurements 
commonly used for full estimation of polarization. The reason for this is that the goal is not the estimation 
of polarization but the dehazing of the scene, and this can be done with two raw images. This is proved 
in Subappendix A.1. Nevertheless, the most-stable results are obtained if the algorithm is based on II 
and I. . Therefore we focus here on this case. Let the raw images correspond to the estimated polarization 
components, I I and I . . We assume that we have an estimate of the global parameters A. and p. Estimation 
of these parameters is described in Section 7. From Eqs. (8) (11) it is seen that we can estimate the 
airlight of any point as A = (I . . I I)/p, (13) and the unpolarized image [Eq. (12)] as I total = I 
I . I . . (14) 20 January 2003 / Vol. 42, No. 3 / APPLIED OPTICS 515 With Eq. (7) the estimated direct 
transmission is therefore = I total . A D. (15) In this image the additive effect of the airlight is 
removed. Recall that besides the addition of airlight, the haze attenuates the light coming from the 
object. The transmittance is estimated from Eqs. (1) and (13) as t = 1 . A /A.. (16) Thus we can compensate 
for the attenuation of the transmitted light. From Eqs. (6), (15), and (16) we obtain an estimate for 
the radiance that would have been sensed in the absence of atmospheric attenua­tion, I total . A I total 
. A object L== . (17) t 1 . A /A. L object is hence the dehazed image. Dehazing thus amounts to simple 
pointwise operations such as sub­traction and division of corresponding pixel values. Since different 
image pixels are processed indepen­dently, the spatially varying inversion of the haze effects is implicit: 
A is spatially varying, since it is larger for the more-distant objects. We note that A., p, and the 
extinction coef.cient . are functions of the light wavelength .. For Ray­leigh scattering,39,40 [. 1/.4, 
so the airlight in mod­erate haze is typically bluish. To account for the wavelength dependence, it is 
best to analyze the im­ages with high spectral resolution. Each wave­length band can be analyzed independently. 
In our experiments, though, we used the traditional coarse wideband red, green, and blue (RGB) sampling 
of the spectrum. We applied the dehazing method to the images shown in Fig. 3, after estimating the parameters 
A. and p on the basis of sky measurements (for the parameter estimation, see Section 7 and in particular 
the stabilizing approach in Subsection 7.C). The re­sulting dehazed image is shown in Fig. 4(a). The 
brightness of the displayed image has the same scale as was used for displaying the best-polarized image 
II in the bottom of Fig. 3. The contrast of features in the dehazed image is greatly improved relative 
to II and I. . Note, for instance, the distant mountain ridge (especially on the left), which is not 
visible in the raw images. Moreover, the algorithm removed the blue color bias, which existed in the 
raw images. This enables distinguishing the different vegetation types by hue. Although most of the scene 
is de­hazed, the sky recovery is noisy, and there is a resid­ual haziness at the more distant objects. 
These artifacts are explained is Subsection 7.C. As another example consider the images shown in Figs. 
5(a) and 5(b). Here I I and I . were calculated from images taken at several orientations of the po­ 
larizer, as is commonly done in polarimetric imag­ing.29,31 This experiment was conducted in conditions 
far from the trivial case: The haze was dense (visibility of a few kilometers), and the contrast in the 
parallel component was only slightly better than in the perpendicular component (all displayed images 
are linearly contrast-stretched versions of the raw images). The dehazed image is shown in Fig. 5(c). 
We obtain a signi.cant improvement of con­trast and color: The green forest is visible in the distant 
scene, whereas in the raw images that area looks like grayish-blue noise. The colors of the red bricks 
and roofs of the distant buildings are also re­stored. 5. Range Map of the Scene A. Range Estimation 
As a byproduct of the dehazing process we get an estimate of the range (depth) map of the scene. The 
estimation exploits the dependence of the transmit­tance t on the distance z. Note that t is always a 
monotonically decreasing function of z, and thus Eq. (16) immediately indicates the distance ordering 
of objects in the scene. Assuming the extinction coef­.cient to be distance invariant, t(z)= exp(-[z). 
Then, from Eq. (16), [z= -ln[1 . A (x, y)/A.]. (18) Note that the distance z is estimated as a function 
of (x, y) up to a global scale factor, which is the unknown extinction coef.cient [. Recall that we get 
an independent estimated range  map for each color channel: [rz, [gz, and [bz, where the subscripts 
r, g, b denote the three color channels. These maps should differ only in their scale. This scale is 
set by the ratios of their scalar extinction coef.cients, [r, [g, and [b. We combine the range maps into 
a single, average one: [z(x, y) . [[ rz(x, y) . [ gz(x, y) . [bz(x, y)]/3. (19) Note that there may 
be more optimal methods of combinations, such as a weighted average, in which the weights depend on the 
noise in each channel. The estimation may be further improved if it is based on narrow spectral bands, 
rather than on RGB. We derived the range map of the scene correspond­ing to Fig. 3 as a byproduct of 
dehazing. To make the depth map more robust to noise, we median .l­tered A . The range map is shown in 
Fig. 6. The darker points correspond to more distant objects. The map is qualitatively consistent with 
the scene, for example, indicating the close tree in the fore­ground and the gradual increase of range 
in the back­ground. The range map can be used to render the scene from viewpoints other than the one 
used during ac­quisition. We texture map the dehazed image on the range map (surface) and then look at 
the texture­ 516 APPLIED OPTICS / Vol. 42, No. 3 / 20 January 2003 Fig. 4. (a) The dehazed image has 
much better contrast and color than the optically .ltered image, especially in the distant regions of 
the scene (compare with Fig. 3). (b) and (c), As described in Section 5, we estimate the range map of 
the scene. We use it to render the dehazed scene from different perspectives, as if the viewer descends. 
right. Note also the distant mountains occluded by the closer ridge. mapped surface from various viewpoints. 
Examples for this application are shown in Figs. 4(b) and 4(c). Here the appearance of the scene is shown 
from two different viewpoints. The images are rendered by rotation of the textured surface by 22° and 
31° relative to normal viewing direction shown at the top of Fig. 4. This creates the impression that 
the viewer gradually descends relative to the acquisition position. One may see that the valley with 
the agricultural segments is planar, since straight lines on it remain quite straight when viewed from 
different directions. It can clearly be seen in Fig. 4 how the large tree on the right-hand side of foreground 
occludes the build­ings in the valley behind it and the mountain range bounding the valley. In addition, 
one may see that there are consecutive mountain ranges (the close one, which bounds the valley, is greener, 
whereas the far- Note the occlusion of the background by the foreground tree on the ther ones are more 
pale as explained in Subsection 7.C). As the viewer descends, the farther ridges be­come occluded by 
the close ridge, and as the viewer ascends, the distant ridges gradually appear. B. Range Accuracy We 
now analyze the accuracy of range estimation on the basis of scattering. Let the uncertainty (scaled 
by the camera s dynamic range) in the measurement of I I or I . be .= 2-b . Here b is the number of bits 
per pixel of the camera, assuming that quantization is the dominant noise source and that the radiometric 
response is linear. The uncertainty of the estimated depth is then 11 .z . exp([z . b ln 2) . (20) . 
pA. 20 January 2003 / Vol. 42, No. 3 / APPLIED OPTICS 517  Fig. 5. Photograph with the best contrast 
that optics alone can give (a) is almost as poor as the worst polarization state (b). The dehazed image 
(c) has much better contrast and color, especially in the distant regions of the scene (note the green 
forest and the red roofs). The range uncertainty grows exponentially with the distance. Beyond some distance 
zmax, range es­timation is too uncertain to be reliably considered. Nevertheless, relation (20) shows 
that the degrading effect due to growth of distance z can be compensated by a proportional increase of 
the number of bits b. Thus, zmax . b. For example, a 12-bit camera will be able to probe 50% farther 
into the scene than an 8-bit camera. One may note that haze can actually be useful for estimating distances, 
since without scattering the perception of depth due to airlight is lost. Indeed, .z 3. when . 30, i.e., 
when attenuation is weak. On the other hand, .z 3. also when . 3., that is, when attenuation is too strong 
(e.g., in fog). The optimal extinction coef.cient minimizing .z is [= 1/z. For example, estimates of 
object ranges around the dis­tance of 3 km are most accurate on a day when the effective attenuation 
distance due to haze is 3 km [. = 1/3 (km-1)]. The estimation is prone to signi.cant error for close 
objects re.ecting signi.cantly polarized light such as specular objects. As discussed in Subsec­tion 
2.B, the degree of polarization of the distant, hazy objects is small relative to the airlight. This 
may not be true for close objects. Figure 7 shows 518 APPLIED OPTICS / Vol. 42, No. 3 / 20 January 2003 
 the range map obtained for the scene shown in Fig. 5. The map is qualitatively consistent with the scene, 
for instance, indicating the close buildings and the distant ridge. Yet a signi.cant partial po­larization 
was observed in some surfaces on the close buildings, especially those directly lit by the Sun. In Fig. 
7 this manifests in a dark shading of the points corresponding to these objects (rather than a bright 
shade). Note also that haze homo­geneity is the basis for Eq. (18); thus range esti­mation becomes less 
accurate when there are signi.cant spatial variations of the haze. To conclude this section, using haze 
and polariza­tion to estimate a range map of the scene is prone to several sources of inaccuracy. Nevertheless, 
it en­ables passive estimation of very long distances, with­out resorting to geometric methods (triangulation), 
which are prone to matching problems. Instead of geometry, the use of photometry enables the rough estimation 
of the large distances from a single view­point. The limit of the estimated range depends on the visibility. 
We demonstrated ranging in a scene with visible objects tens of kilometers away, but the maximum range 
can be much larger, depending on [ and b. 6. Information about the Aerosols In Section 4 we showed that 
on the basis of as few as two images we can dehaze the imaged scene. Now we will show that with the same 
raw images we can extract information related to the atmospheric parti­cles that degrade the scene visibility. 
Consider the range maps of each color channel, which were described in Section 5. Averaging over the 
image pixels, we de.ne scalars corresponding to each color channel: .[ rz( x, y) sr = x,y , (21).[z( 
x, y) x,y .[ gz( x, y) sg = x,y , (22).[z( x, y) x,y .[ bz( x, y) sb = x,y . (23).[z( x, y) x,y These 
scalars express the extinction coef.cients of the atmosphere, in each of the color channels, up to a 
single scale factor. This result is valuable because the relative scattering coef.cients are determined 
by the size of the scattering particles.2,15,39 Assuming that scattering is the dominant process of attenua­tion, 
these ratios provide rough indication about the distribution of the particles size. This information 
may be used in conjunction with other methods for estimating the particle size from spectral and polar­ization 
information.16 It may be incorporated into models that make explicit physical analysis of atmo­spheric 
scattering, as well as in applications of eco­logical monitoring. As with image dehazing and range estimation, 
this application would be more ac­curate if narrow spectral bands were used, rather than RGB. In the 
experiment based on the images shown in Fig. 5, we obtained sr 0.26 sg = 0.32 , (24) sb 0.42 which 
means that the scattering in the blue band is .60% stronger than the scattering in the red band. Had 
the dominant particles been small enough to obey Rayleigh s1/.4 rule, we might have expected that the 
scattering of the blue wavelengths were much stronger relative to the red wavelengths. It is dif.cult 
to pinpoint the exact relative strength when very broadband spectral channels are used. So to get a rough 
estimate, if we take 450 and 650 nm as typical mid-band wavelengths for blue and red, re­spectively, 
then in Rayleigh scattering we may expect the blue scattering to be (an order of ) 300% stronger 20 January 
2003 / Vol. 42, No. 3 / APPLIED OPTICS 519 than the red one. Therefore, as expected, the exper­iment 
was conducted under conditions in which scat­tering was dominated by particles that do not .t into the 
Rayleigh model. Therefore, even if the Sun had been perpendicular to the viewing direction (the triv­ial 
case for Rayleigh scattering), light would not have been suf.ciently polarized to enable dehazing by 
op­tical .ltering alone. 7. Estimating A. and p To dehaze the image with Eqs. (13), (15), and (17),we 
need an estimate of the global parameters A. and p. In Ref. 10 it was concluded that human vision can 
correct for effects of aerial perspective on the basis of context. We use this observation as the way 
to ob­tain A. and p. The image context we mainly rely on is the sky. This section discusses estimation 
of these parameters. The experimental results were based on the principles described in Subsections 7.A 
7.C. A. Sky Measurements at the Horizon Since t(z) 30as z 3., we get Itotal = Lobjectt( z) . A.[1 . t( 
z)] 3A.. (25) The degree of polarization of the measured scene (i.e., the direct transmission combined 
with airlight) is I( x, y) P ( x, y) = Itotal( x, y) , (26) where I( x, y) . [I .( x, y) . I I( x, y)]. 
(27) As z 3., A. . . A. I P ( x, y) 3 . I = p. (28)A. . A. We can measure these parameters directly 
from the images. We can use points that are seen through enough haze such that their direct transmission 
is practically zero. Such points are not always avail­able, so we use some heuristics based on context 
(as in human vision10) to estimate these parameters. The most direct way is to measure patches of the 
sky at the horizon: I(sky) . = Itotal(sky), A p = Itotal(sky) . (29) As an example, in the experiment 
corresponding to the images shown in Fig. 5, the average measured values of p with Eq. (29) were p r 
0.28 p g . 0.25 . (30) p b 0.22 a higher degree of polarization, compared with short (blue) wavelengths. 
Since p depends on the size and density of the scatterers,15 the estimation of p in the different spectral 
bands may provide additional in­formation about the aerosols in the scene.16 Note that if the horizon 
is cloudy and t(.) . 0, then the measured light is due not only to haze airlight but also to the object 
(cloud). Thus this method will be erroneous, and we may need to apply the method described in Subsection 
7.D. B. Spatial Variability Although we treat the parameters A. and p as global, they may vary across 
the .eld of view. The sky (horizon) radiance A. depends on the angular scat­tering coef.cient [(<), as 
explained in Subappendix A.2. Therefore it depends on the position of the Sun relative to the viewing 
direction.12 For instance, be­cause of strong forward scattering and backscatter­ing, A. will usually 
be larger when the Sun is in front or behind the camera. Also, p depends on the posi­tion of the Sun 
relative to the viewing direction. The spatial variations of A. and p across the .eld of view are much 
slower than the typical variations in radiance that are due to texture. Thus we can ac­count for the 
horizontal variation of A. and p by sparsely sampling the sky radiance at the horizon across the .eld 
of view and then interpolating the values, using a smoothly varying function. In the experiments we performed 
we estimated A . and p by sparsely measuring the sky values above the distant ridges across the images. 
We then .t a second-order polynomial to the measurements. Vertical variations are more complicated. Sky 
measurements can change as a function of altitude. Haze density can change signi.cantly as a function 
of altitude11 within the .rst few vertical kilometers of atmosphere,2 and even within a few hundred meters. 
Moreover, even on a clear day the sky radiance and polarization change as a function of the viewing di­rection 
relative to the zenith.38,46 The plane of po­larization may change by 90° above the solar and antisolar 
horizons when the Sun is low. Thus one has to be careful when applying this method in a large vertical 
.eld of view. A direct implication is that measuring A . and p from a sky patch at high elevation angle 
will usually be error prone. For this reason we measure the sky values close to the hori­zon. We also 
applied a stabilizing bias, described next. C. Stabilizing the Recovery Even if all the measurements 
were perfect, we believe that avoiding a complete inversion of the image for­mation is bene.cial from 
a perceptual-aesthetic point of view. There are two reasons for this. First, at­tempting to remove the 
atmospheric scattering from the sky itself means that the daylight sky should be removed. In a noiseless 
image the result would be Note that p r . p g . p b. This is consistent with the dark deep-space sky 
in a clear daylight image! This literature14,15: In haze, long wavelengths (red) un-is perceptually annoying. 
The sky appearance is dergo less multiple scattering and therefore maintain further degraded by unstable 
ampli.cation of noise, 520 APPLIED OPTICS / Vol. 42, No. 3 / 20 January 2003 since the denominator in 
Eq. (17) approaches zero when A 3A.. Second, even on the clearest day, we still encounter airlight because 
of scattering by the air molecules, which manifests in a bluish tint in the distance. Images lacking 
any aerial perspective,12 in which far objects are the same as close ones, look strange and arti.cial. 
If the dehazed images are meant for human inspec­tion, it is preferable to avoid these phenomena. This 
is easily achieved if we somewhat bias p by multiply­ing it by a factor ., such that 1 . . . 1/p : p 
3.p . (31) If .= 1, we get Eq. (17) as the solution to the inverse problem. In the other extreme we 
may set .p = 1 (recall that p . 1), and then the algorithm behaves as if the airlight were completely 
polarized: From Eqs. (13) and (15) D(x, y)= I I(x, y) when p = 1; hence the unprocessed best-polarized 
image compo­nent is used as the value for the direct transmission. For all intermediate values of . the 
inverse solution (17) is moderated by means of weighting it with the raw image. Using a value of . slightly 
larger than 1 leaves a residual haziness (originating from the raw image) that grows with the distance, 
thus making the image consistent with naked-eye experience. It can also be shown that when .. 1 the estimated 
dehazed Lobject(sky)= A. sky value is ; i.e., it retains its raw unpolarized sky value, automatically. 
The noise in the estimated sky value is object(sky) = .L.2.(1 . 1/.)-1, (32) where . is the noise standard 
deviation of the raw image components. It can be seen that noise is am­pli.ed in the sky by the dehazing 
process, but this ampli.cation decreases with the increase of .. This bias is also bene.cial to counter 
effects of error stemming from inaccurate estimation of p and A.. As described in Subsection 7.B, we 
may expect such inaccuracy if it is based on sky measurements. From Eqs. (13) (15), (26), and (27), I 
total(x, y)D (x, y) = [ p . P (x, y)]. (33) p If our estimate of the airlight degree of polarization 
is too low (p < p), then negative values can appear in the image of the direct transmission. This is 
espe­cially relevant to distant objects, because P(x, y) 3p when z 3.. Biasing p with Eq. (31) reduces 
the occurrence of such problems. In addition, Eq. (31) better conditions the compensation for attenuation 
in case A . is inaccurate [see Eqs. (13), (16), and (17)]. In the experiments we performed, we .rst esti­mated 
A . and p as described in Subsections 7.A and 7.B. Indeed, that heuristic method resulted in a slight 
error, and many of the resulting pixels of D and L object had negative values, especially in the distant 
areas. In addition, the sky was quite noisy. So, to get the dehazing results presented in this pa­per, 
we .ne tuned p by increasing its values globally by a few percent (. = 1.09). We thus gave up a few percent 
of the full inversion, and in return almost all the occurrences of negative values were eliminated and 
the sky regained a natural color (a tolerable noise is still present). As can be seen in Fig. 4, distant 
objects have a residual bluish haziness (aerial per­spective12), making the recovered scene look percep­tually 
acceptable. D. Unavailable Sky Measurements When direct sky measurements are not possible to obtain, 
we need to get the context for estimating p and A. from nonsky regions of the scene. We now show that 
such an estimate can be obtained from a priori information about the identity of several scene points 
with similar but unknown radiance Lobject, had the haze been absent. For instance, this is possible if 
parts of the scene had been observed on a clear day so that some objects are known to have corresponding 
colors. From Eqs. (13), (17), and (27), Lobject I total(x, y) = Lobject . 1 . I(x, y). (34) p pA. Assume 
that we know of a set of (at least two) pixels (xk, yk) for which the object radiance in the absence 
of haze is the same, Lobject = L1object, but their distances from the viewer are different. For instance, 
the set of pixels can correspond to similar bushes at different distances. The value L1object does not 
have to be known in advance, as we will see in the following. Because of the differing depths, these 
points (xk, yk) will have different values of I total(k) and I(k). For all these points, however, ..object1 
L1 C1 . . (35) p pA. Thus I total(k) is constant. as a function of I(k) forms a straight line, I total(k) 
= L1object . C1 I(k), (36) whose intercept on the I total axis is the radiance value object L1 . Therefore 
knowing points (xk, yk) that have corresponding radiances, we can estimate their corresponding dehazed 
radiance by .tting a line to the measured I total(k) and I(k). The slope of the .tted line is C1. Now 
that we know L1object and C1, we may rewrite Eq. (35) as 1 object/C1) 1 p =. (L1 . (37) C1 A. Thus the 
unknown p and A. are constrained to lie on a line in the (p,1/A.) plane. The line is de.ned by object 
and C1. the already estimated L1 Now we can look at a different set of (at least two) pixels (xn, yn), 
which in the absence of scattering effects have the object . L1 object same radiance L2object, where 
L2 . Once again, analogous to Eqs. (35) and (36), L2 object and the corresponding C2 are estimated if 
the pixels (xn, yn) 20 January 2003 / Vol. 42, No. 3 / APPLIED OPTICS 521 correspond to scene points 
at different distances. This supplies another line constraint, 1 object/C2) 1 p =. (L2 . (38) C2 A. 
The intersection of these lines (37, 38) yields the es­timated values for p and A.. For N . 2 sets of 
pixels corresponding to unknown dehazed object radiances object where l = 1, ... , N, the estimation 
of p and A. becomes more robust. The minimum, however, is two sets of two points. Note that for identifying 
the sets of pixels, this method requires some user inter­action, as in the estimation using sky measurement. 
Ll 8. Discussion We have shown that physics-based image analysis that follows acquisition of polarization-.ltered 
im­ages can remove visual effects of haze. Although it is based on some approximations, this approach 
proved to be effective in dehazing, when the problem could not be solved by optics alone. The method 
is quick and does not require temporal changes in weather conditions. In addition to the dehazed im­age, 
the method also yields information about scene structure and the atmospheric particles. These re­sults 
can form the basis for useful tools in photogra­phy and remote sensing. Our method is based on the partial 
polarization of airlight. Therefore its stability will decrease as the airlight degree of polarization 
decreases. For in­stance, the method may be less effective when the illumination is less directional 
(overcast skies).We expect it to have just a limited effect, or even fail, in cases of strong depolarization, 
as occurs in fog. Nev­ertheless, with more-exact scattering models, such as those that include multiple 
scattering, this research may be extended to complicated weather conditions and perhaps to other scattering 
media (e.g., under­water environments34 and tissues). Appendix A 1. Dehazing with Two Arbitrary Images 
In Sections 4 and 5 we used estimates of II and I. in the dehazing algorithm. We now show that in the­ory 
the method can work on the basis of two images taken through any nondegenerate polarization orien­tations. 
Let .I be the orientation of the polarizer for best transmission of the component parallel to the plane 
of incidence (Fig. 1). For a general orientation . the observed airlight is A(.) = A.1-p cos[2(. . .I)]./2, 
(A1) which coincides with Eqs. (9) and (11) if .=.I, .I + 90°. Assume that we take two images of the 
scene with arbitrary orientations of the polarizer, .1 ..2. Because the direct transmission is unaffected 
by the where we set A(.2) 2 A(.1), without loss of general­ity. We also de.ne an effective un.ltered 
image polarizer orientation, the images are I1 = D/2 . A(.1), (A2) I2 = D/2 . A(.2). (A3) Let us de.ne 
an effective airlight Aeffective . A(.1) . A(.2), (A4) with an effective degree of polarization peffective 
. A(.2) . A(.1) Aeffective , (A5) totalIeffective . I1 . I2 = D . Aeffective . (A6) It can easily be 
shown that Aeffective is proportional to the actual airlight, effective[1 . t(z)],Aeffective = fA = fA.[1 
. t(z)] = A. (A7) where A. effective is the effective airlight at in.nity (the horizon). The proportion 
factor f is f = 1 . p cos(.1 . .2 . 2.I)cos(.1 . .2). (A8) Since we do not know .I on the basis of two 
arbitrary polarizer angles, f is unknown. Assume now that we have estimates of the param­ effective 
eters peffective and A. . These parameters can be estimated by measurement of the image irradiances I1 
and I2 at the sky, similar to the way described in Section 7. Then we estimate the effective airlight 
at each point, I2 . I1 A effective = . (A9)peffective From Eq. (A6) the estimated direct transmission 
based on the raw images I1 and I2 is total D= Ieffective . Aeffective. (A10) From Eq. (A7) the estimated 
transmittance is Aeffective = 1 . t(A11) effective . A. Thus the dehazed image is total object Ieffective 
. A effective L= (A12) effective . 1 . Aeffective/A. We can check the stability of using an arbitrary 
pair of images. It is easy to show that Ap peffective = sin(.1 . .2 . 2.I)sin(.2 . .1).Aeffective (A13) 
 522 APPLIED OPTICS / Vol. 42, No. 3 / 20 January 2003 Equation (A9) becomes unstable when peffective 
30. rection is a fraction of the total amount of light re-Besides the obvious case in which p = 0, this 
happens moved from the incident beam by scattering in all when directions and by absorption. The scattered 
light radiance is also proportional to both the illumination (.1 . .2)/2 = .I, .I . 90°. (A14) irradiance 
and to dzr. Thus we may conclude that This is expected because the acquired images are equal if taken 
on symmetric angles relative to the extrema of the cosine in Eq. (A1). Therefore chang­ing the orientation 
from .1 to .2 is degenerate. Ex­cept for these singular cases, dehazing is possible with two images. 
The best stability of dehazing is achieved when peffective is maximum, that is, when .=.I, .I + 90°. 
Therefore we focus here on dehaz­ing based on II and I. . By rotating the polarizer to achieve an extremum 
of the image irradiance or contrast, it is often easy to detect visually the states corresponding to 
II and I. . However, it is easier and more accurate to estimate these components with 3 or more images 
taken through different general orientations of the polar­izer. This is a common practice in polarization 
im­aging, as detailed in Refs. 29, 31, 34, 43, 47, and 48. 2. Inhomogeneous Attenuation: Model and Recovery 
This subsection describes the image-formation model and the dehazing method when the attenuation var­ies 
along the line of sight. It is given here mainly to make the paper self-contained for readers unfamiliar 
with radiative transfer. When light propagating to­ward the camera passes through an in.nitesimal layer 
of scattering media, some percentage of it is lost as a result of scattering to other directions and 
as a result of absorption. For a layer of thickness dz., the direct transmission change dD is given7,36 
by the dif­ferential equation dD(z.) = -[(z.)dz.. (A15) D [(z.) is the extinction coef.cient at depth 
z..We obtain it by integrating the angular scattering coef­.cient [(<, z.) over all scattering angles 
< and adding the absorption coef.cient, if absorption exists. When we integrate Eq. (A15) over the distance 
z from the object, the measured transmitted light is D = Lobjectt(z), where Lobject = DIz=0, and the 
atmospheric transmittance is z t(z) = exp[(z.)dz.. (A16) 0 In the special case when [(z.)=[ independently 
of the distance, we obtain Eq. (3). Now let us derive the expression for airlight. Con­sider a layer 
of scattering media, of in.nitesimal depth dzr illuminated by a light source at an arbi­trary direction 
(say, the Sun). Part of this light is scattered toward the camera. The radiance of the scattered light 
is proportional to the angular scatter­ing coef.cient [(<, zr). Note7 that [(<, zr)r[(zr). This is because 
light scattered toward a certain di­the ambient light scattered by this layer toward the camera is given 
by K[(zr)dzr, where K encapsulates the illumination irradiance and the proportion of light scattered 
in the direction of the camera, relative to the total scattering (and absorption). This ex­pression also 
describes the case in which the layer is illuminated by a distribution of source directions.7 Once this 
light has been directed toward the cam­era, it undergoes attenuation on its way, as dictated by t(zr). 
Eventually, the airlight from the above­mentioned layer is z r dA(zr) = K[(zr)dzr exp[(z.)dz.. (A17) 
0 The total airlight radiance (path radiance) is ob­tained by means of integrating the airlight contribu­tion 
from all layers49: z r A(z) = .z dA(zr) = -K{exp.-.z[(z.)dz..-110 00 = K[1 . t(z)]. (A18) The airlight 
of a scene point at in.nity is A. = K[1 . t(.)]. (A19) Therefore the airlight is A. A = [1 . t(z)]. 
(A20) 1 . t(.) In the homogeneous haze model [Eq. (3)] we have t(.) = 0, that is, object points that 
are far enough are completely attenuated. If we apply the assumption of total attenuation for objects 
at in.nity also to in­homogeneous haze, then we set t(.) = 0 in Eq. (A20). We then obtain A = A.[1 . 
t(z)], (A21) as in Eq. (1). This is the situation assumed through­out the paper. It is interesting to 
examine how the solution is in.uenced when t(.) is unknown and is not zero. Assume that we perform the 
recovery in the same way. First, we estimate p and A . by sampling the image of the sky, as in Subsection 
7.A. This time, I(sky) = A.p, (A22) while Itotal(sky) = Lobject(sky)t(.) . A.. (A23) Fortunately, we 
can set Lobject(sky)= 0. The reason for this is that at night, when there is no airlight (A.= 0), the 
sky is dark [Itotal(sky)= 0]. Therefore, 20 January 2003 / Vol. 42, No. 3 / APPLIED OPTICS 523 when 
airlight exists, Itotal(sky)= A.. Hence we can safely use Eqs. (29) to estimate A . and p . Now that 
we have the parameters, we can look at the scene-dehazing equation. It can be shown that the estimation 
of the direct transmission D with Eq. (15) and p is correct. Therefore we can remove the additive airlight, 
using the same procedure as with t(.) = 0. To complete the dehazing process, we need to compensate for 
the attenuation. Similar to Eqs. (15) and (17),  object L= D /t . (A24) This time [see Eq. (A20)], 
 t = 1 . AA.[1 . t(.)], (A25) which is somewhat different from Eq. (16).If t(.) is unknown and cannot 
be neglected, then our estima­tion of the transmittance with Eq. (16) is biased to­ward a value lower 
than the true one. This will lead to some overampli.cation (brightening) of the image radiance corresponding 
to distant objects in the de­hazed image. The overampli.cation of distant ob­jects is reduced when we 
bias p by a factor of .. 1as in relation (31); this bias reduces A , thereby increas­ing t in Eq. (16) 
in the same way as the factor [1 ­t(.)] does in Eq. (A25). Partial and preliminary results were presented 
in Ref. 47. We thank the reviewers for their helpful remarks. This study was supported in part by a grant 
from the Defence Advance Research Projects Agency (DARPA) Human Identi.cation at a Distance program, 
contract N00014-00-1-0916; by National Science Foundation (NSF) award IIS-99-87979; and by the Morin 
Foundation. References and Note 1. L. Grewe and R. R. Brooks, Atmospheric attenuation reduc­tion through 
multi-sensor fusion, in Sensor Fusion: Archi­tectures, Algorithms, and Applications II, B. V. Dasarathy, 
ed., Proc. SPIE 3376, 102 109 (1998). 2. N. S. Kopeika, A System Engineering Approach to Imaging (SPIE, 
Bellingham, Wash., 1998), pp. 446 452. 3. J. P. Oakley and B. L. Satherley, Improving image quality 
in poor visibility conditions using a physical model for contrast degradation, IEEE Trans. Imag. Proc. 
7, 167 179 (1998). 4. K. Tan and J. P. Oakley, Physics-based approach to color image enhancement in 
poor visibility conditions, J. Opt. Soc. Am. A 18, 2460 2467 (2001). 5. S. G. Narasimhan and S. K. Nayar, 
Chromatic framework for vision in bad weather, in Proceedings of the IEEE Conference on Computer Vision 
and Pattern Recognition (Institute of Elec­trical and Electronics Engineers, New York, 2000), Vol. I, 
pp. 598 605. 6. S. G. Narasimhan and S. K. Nayar, Removing weather effects from monochrome images, in 
Proceedings of the IEEE Confer­ence on Computer Vision and Pattern Recognition (Institute of Electrical 
and Electronics Engineers, New York, 2001), Vol. II, pp. 186 193. 7. S. K. Nayar and S. G. Narasimhan, 
Vision in bad weather, in  Proceedings of the IEEE International Conference on Computer Vision (Institute 
of Electrical and Electronics Engineers, New York, 1999), pp. 820 827. 8. P. S. Pencikowski, Low-cost 
vehicle-mounted enhanced vision system comprised of a laser illuminator and range-gated cam­era, in Enhanced 
and Synthetic Vision, J. G. Verly, ed., Proc. SPIE 2736, 222 227 (1996). 9. B. T. Sweet and C. L. Tiana, 
Image processing and fusion for landing guidance, in Enhanced and Synthetic Vision,J.G. Verly, ed., Proc. 
SPIE 2736, 84 95 (1996).  10. R. C. Henry, S. Mahadev, S. Urquijo, and D. Chitwood, Color perception 
through atmospheric haze, J. Opt. Soc. Am. A 17, 831 835 (2000). 11. D. K. Lynch, Step brightness changes 
of distant mountain ridges and their perception, Appl. Opt. 30, 3508 3513 (1991). 12. S. D. Gedzelman, 
Atmospheric optics in art, Appl. Opt. 30, 3514 3522 (1991). 13. F. Cozman and E. Krotkov, Depth from 
scattering, in Pro­ceedings of the IEEE Conference on Computer Vision and Pat­tern Recognition (Institute 
of Electrical and Electronics Engineers, New York, 1997), pp. 801 806. 14. W. A. Shurcliff and S. S. 
Ballard, Polarized Light (Van Nos­trand, Princeton, N.J., 1964), pp. 98 103. 15. G. P. Ko¨nnen, Polarized 
Light in Nature (Cambridge Univer­sity, Cambridge, UK, 1985), pp. 1 10, 29 54, 60 62, 131 137, 144 145. 
 16. B. Cairns, B. E. Carlson, A. A. Lacis, and E. E. Russell, An analysis of ground-based polarimetric 
sky radiance measure­ments, in Polarization: Measurement, Analysis, and Remote Sensing, D. H. Goldstein 
and R. A. Chipman, eds., Proc. SPIE 3121, 382 393 (1997). 17. K. L. Coulson, Polarization of light in 
the natural environ­ment, in Polarization Considerations for Optical Systems II,  R. A. Chipman, ed., 
Proc. SPIE 1166, 2 10 (1989). 18. S. J. Hitzfelder, G. N. Plass, and G. W. Kattawar, Radiation in the 
earth s atmosphere: its radiance, polarization, and el­lipticity, Appl. Opt. 15, 2489 2500 (1976). 19. 
D. K. Lynch and P. Schwartz, Rainbows and fogbows, Appl. Opt. 30, 3415 3420 (1991). 20. M. S. Quinby-Hunt, 
L. L. Erskine, and A. J. Hunt, Polarized light scattering by aerosols in the marine atmospheric bound­ary 
layer, Appl. Opt. 36, 5168 5184 (1997). 21. M. J. Rakovic´, G. W. Kattawar, M. Mehru¨beog.lu, B. D. 
Cam­eron, L. V. Wang, S. Rastegar, and G. L. Cote´, Light back­scattering polarization patterns from 
turbid media: theory and experiment, Appl. Opt. 38, 3399 3408 (1999). 22. D. B. Chenault and J. L. Pezzaniti, 
 Polarization imaging through scattering media, in Polarization Analysis, Measure­ment, and Remote Sensing 
III, D. B. Chenault, M. J. Guggin,  W. G. Egan, and D. H. Goldstein, eds., Proc. SPIE 4133, 124 133 
(2000). 23. L. J. Denes, M. Gottlieb, B. Kaminsky, and P. Metes, AOTF polarization difference imaging, 
in 27th AIPR Workshop: Advances in Computer-Assisted Recognition, R. J. Mericsko, ed., Proc. SPIE 3584, 
106 115 (1998). 24. O. Emile, F. Bretenaker, and A. Le Floch, Rotating polariza­tion imaging in turbid 
media, Opt. Lett. 21, 1706 1708 (1996). 25. X. Gan, S. P. Schilders, and Min Gu, Image enhancement through 
turbid media under a microscope by use of polariza­tion gating method, J. Opt. Soc. Am. A 16, 2177 2184 
(1999). 26. H. Horinaka, K. Hashimoto, K. Wada, T. Umeda, and Y. Cho, Optical CT imaging in highly scattering 
media by extraction of photons preserving initial polarization, in International Symposium on Polarization 
Analysis and Applications to De­vice Technology, T. Yoshizawa and H. Yokota, eds., Proc. SPIE 2873, 54 
57 (1996). 27. M. P. Rowe, E. N. Pugh Jr., J. S. Tyo, and N. Engheta, Polarization-difference imaging: 
a biologically inspired  524 APPLIED OPTICS / Vol. 42, No. 3 / 20 January 2003 technique for observation 
through scattering media, Opt. Lett. 20, 608 610 (1995). 28. J. G. Walker, P. C. Y. Chang, and K. I. 
Hopcraft, Visibility depth improvement in active polarization imaging in scatter­ing media, Appl. Opt. 
39, 4933 4941 (2000). 29. Y. Y. Schechner, J. Shamir, and N. Kiryati, Polarization and statistical analysis 
of scenes containing a semire.ector, J. Opt. Soc. Am. A 17, 276 284 (2000). 30. H. Farid and E. H. Adelson, 
Separating re.ections from im­ages by use of independent component analysis, J. Opt. Soc. Am. A 16, 2136 
2145 (1999). 31. S. K. Nayar, X. S. Fang, and T. Boult, Separation of re.ection components using color 
and polarization, Int. J. Comput. Vi­sion 21, 163 186 (1997).  32. S. Rahmann and N. Canterakis, Reconstruction 
of specular surfaces using polarization imaging, in Proceedings of the IEEE Conference on Computer Vision 
and Pattern Recognition (Institute of Electrical and Electronics Engineers, New York, 2001), Vol. 1, 
pp. 149 155. 33. M. Saito, Y. Sato, K. Ikeuchi, and H. Kashiwagi, Measure­ment of surface orientations 
of transparent objects by use of polarization in highlight, J. Opt. Soc. Am. A 16, 2286 2293 (1999). 
 34. L. B. Wolff, Polarization vision: a new sensory approach to image understanding, Image Vision Comput. 
15, 81 93 (1997). 35. C. F. Bohren and A. B. Fraser, At what altitude does the horizon cease to be 
visible? Am. J. Phys. 54, 222 227 (1986). 36. E. J. McCartney, Optics of the Atmosphere: Scattering 
by Molecules and Particles (Wiley, New York, 1976). 37. J. S. Tyo, M. P. Rowe, E. N. Pugh Jr., and N. 
Engheta, Target detection in optically scattering media by polarization­difference imaging, Appl. Opt. 
35, 1855 1870 (1996). 38. R. L. Lee Jr., Digital imaging of clear-sky polarization, Appl. Opt. 37, 1465 
1476 (1998).  39. E. Hecht, Optics, 3rd ed. (Addison-Wesley, New York, 1998), pp. 340 342. 40. S. Chandrasekhar, 
Radiative Transfer (Dover, New York, 1960), pp. 24 37, 280 284.  41. M. Ben-Ezra, Segmentation with 
invisible keying signal, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 
(Institute of Electrical and Electronics En­gineers, New York, 2000), Vol. 1, pp. 32 37. 42. T. Prosch, 
D. Hennings, and E. Raschke, Video polarimetry: a new imaging technique in atmospheric science, Appl. 
Opt. 22, 1360 1363 (1983). 43. A. M. Shutov, Videopolarimeters, Sov. J. Opt. Technol. 60, 295 301 (1993). 
 44. L. B. Wolff, Polarization camera for computer vision with a beam splitter, J. Opt. Soc. Am. A 11, 
2935 2945 (1994). 45. A. S. Glassner, Principles of Digital Image Synthesis (Morgan Kaufmann, San Francisco, 
Calif., 1995), Appen. G.4. 46. R. L. Lee Jr., Horizon brightness revisited: measurements and a model 
of clear-sky radiances, Appl. Opt. 33, 4620 4628 (1994).  47. Y. Y. Schechner, S. G. Narasimhan, and 
S. K. Nayar, Instant dehazing of images using polarization, in Proceedings of the IEEE Conference on 
Computer Vision and Pattern Recognition (Institute of Electrical and Electronics Engineers, New York, 
2001), Vol. 1, pp. 325 332. 48. J. E. Solomon, Polarization imaging, Appl. Opt. 20, 1537 1544 (1981). 
 49. For the calculation of the path radiance integral, we assume K to be distance invariant. This is 
because typically most of the light in the scene comes from the Sun and sky and thus does not change 
much along the line of sight. Moreover, we as­sume that multiple scattering (which effects the angular 
scat­tering distribution) is dominated by single scattering.  20 January 2003 / Vol. 42, No. 3 / APPLIED 
OPTICS 525       
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1667261</article_id>
		<sort_key>220</sort_key>
		<display_label>Article No.</display_label>
		<display_no>22</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>22</seq_no>
		<title><![CDATA[Shape grammars]]></title>
		<page_from>1</page_from>
		<page_to>176</page_to>
		<doi_number>10.1145/1667239.1667261</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1667261</url>
		<abstract>
			<par><![CDATA[<p>The theory of shape grammars, first launched by Stiny and Gips in 1972, defines a formalism to support the ambiguity in creative processes that is generally ruled out by quantitative and symbolic computations. Since then, it has evolved into a ground-breaking pragmatist philosophy of shape and design. It is implemented in fields rnaging from architecture to art, graphic design, industrial design, and computer visualization. This course offers basic knowledge of the theory of shape grammars and some advanced issues useful for its implementation.</p> <p>The course is presented in two consecutive sessions. The introductory session presents the fundamentals of the theory, focusing on the basic knowledge of shapes, shape algebras, and shape rules in order to explain how shape grammars translate visual and spatial thinking into design computation. It includes several examples of shape grammar applications in design analysis and synthesis. Attendees with further and more technical interests in the topic are encouraged to continue with the advanced session, which dwells on the computational devices of shape grammars and discusses a number of selected studies on computational implementation of the shape grammar idea.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>K.3.2</cat_node>
				<descriptor>Computer science education</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003457.10003527.10003531.10003533</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing education->Computing education programs->Computer science education</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1797169</person_id>
				<author_profile_id><![CDATA[81365594108]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mine]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[&#214;zkar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Middle East Technical University., Ankara, Turkey]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797170</person_id>
				<author_profile_id><![CDATA[81100154070]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[George]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stiny]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Stiny, George, 2006, Shape, MIT Press]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Akgun, T, Koman, A, and Akleman, E., 2006, Developable Sculptures of Ilhan Koman, <i>Proceedings of Bridges</i> 2006, London.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Ando, N, Yamahata, N, Masumi, S, Chatani, M, 2001, Shape Grammar and Form Properties of Architectural Figures, <i>Journal for Geometry and Graphics</i>, Volume 5, No. 1, 23--33.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1138276</ref_obj_id>
				<ref_obj_pid>1138235</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Asokan, A and Cagan, J, 2005, Defining cultural identities using grammars: an exploration of "cultural languages" to create meaningful experiences, Proceedings of the Conference on Designing for User Experiences DUX05, pp. 2--11.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Bakirer, &#214;., 1981, Sel&#231;uklu &#214;ncesi ve Sel&#231;uklu D&#246;nemi Anadolu Mimarisinde Tugla Kullanimi {The use of brick in Anatolian architecture in pre-Seljuk and Seljuk era}. Ankara, Turkey, ODT&#220;.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Best-Maugard, A, 1926, <i>A Method for Creative Design</i>, Alfred A. Knopf, New York and London, p 1--2.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Botha, M, Sass, L, 2006, Instant House: Design and digital fabrication of housing for developing environments, <i>CAADRIA 2006 {Proceedings of the 11th International Conference on Computer Aided Architectural Design Research in Asia}</i> Kumamoto, Japan, pp. 209--216.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Duarte, J P, 2005, Towards the customization of mass-housing: the grammar of Siza's houses at Malagueira, <i>Environment and Planning B: Planning and Design</i>, 32, pp. 347--380]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Ghazarian, A and Ousterhout, R, 2001, A Muqarnas Drawing from Thirteenth-Century Armenia and the Use of Architectural Drawings during the Middle Ages, <i>Muqarnas</i>, Vol. 18, pp. 141--154.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1084428</ref_obj_id>
				<ref_obj_pid>1084015</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Huang, X W, Kolak Dudek, C, Sharman, L, Szabo, F E, 2005, From Form to Content: Using Shape Grammars for Image Visualization, <i>Ninth International Conference on Information Visualisation (IV'05)</i>, pp.439--444.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Kirsch, J L and Kirsch, R A, 1988, The Anatomy of Painting Style: Description with Computer Rules, <i>Leonardo</i>, Vol. 21, No. 4, pp. 437--444.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Kornhauser, D, 2002, <i>Designing a Craft Computing Environment for Non-Industrial Settings</i>, unpublished M.S. thesis, MIT, Cambridge, MA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360701</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Lipp, M, Wonka, P, and Wimmer, M, 2008, Interactive visual editing of grammars for procedural architecture. <i>ACM Trans. Graph. 27</i>, 3 (Aug. 2008), 1--10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Lu, P J, and Steinhardt, P J, 2007, Decagonal and Quasi-Crystalline Tilings in Medieval Islamic Architecture, <i>Science</i> 315 (5815), pp. 1106--1110. Supporting Online Material.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1237150</ref_obj_id>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Martino, J A, 2006, <i>The Immediacy of the Artist's Mark in Shape Computation: from Visualization to Representation.</i> Doctoral Thesis. UMI Order Number: AAI0809838., Massachusetts Institute of Technology.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[McCormack, J P, Cagan, J, and Vogel, C M, 2004, Speaking the Buick Language: Capturing, Understanding and Exploring Brand Identity with Shape Grammars, <i>Design Studies</i> 25, pp. 1--29.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141931</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[M&#252;ller, P, Wonka, P, Haegler, S, Ulmer, A, and Van Gool, L, 2006, Procedural modeling of buildings. <i>ACM Trans. Graph.</i> 25, 3 (Jul. 2006), 614--623.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Necipo&#487;lu, G, 1995, The Topkapi Scroll---Geometry and Ornament in Islamic Architecture, Getty Center for the History of Art and the Humanities, Santa Monica, CA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383292</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Parish, Y I and M&#252;ller, P, 2001, Procedural modeling of cities. In <i>Proceedings of the 28th Annual Conference on Computer Graphics and interactive Techniques</i> SIGGRAPH '01. ACM, New York, NY, 301--308.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Ross, D W, 1907, <i>A Theory of Pure Design: Harmony, Balance, Rhythm</i>, Houghton, Mifflin and Company, Boston and New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Sass, L, 2005, A Wood Frame Grammar, <i>Computer Aided Architectural Design Futures 2005 {Proceedings of the 11th International Conference on Computer Aided Architectural Design Futures</i> Vienna, Austria, pp. 383--392.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Sass, L, 2007, Synthesis of design production with integrated digital fabrication, <i>Automation in Construction</i>, Vol. 16, No.03, pp. 298--310.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Stiny, G and Gips, J, 1972, Shape Grammars and the Generative Specification, Petrocelli OR (ed) <i>Best computer papers of 1971</i>, pp. 125--135]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Chase, S, 2005, <i>Generative design tools for novice designers: Issues for selection, Automation in Construction</i> 14 (6), pp. 689--698.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Chomsky, N. 1957, <i>Syntactic Structures</i>, Mouton, The Hague]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Flemming, U, 1987a, More than the sum of parts: the grammar of Queen Anne houses, <i>Environment and Planning B:Planning and Design 14</i> pp. 323--350]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>109152</ref_obj_id>
				<ref_obj_pid>109149</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Flemming, U, 1990, Syntactic Structures in Architecture, The Electronic Design Studio, MIT Press, Cambridge pp. 31--47]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Knight, T, 1980, The generation of Hepplewhite-style chair back designs, <i>Environment and Planning B: Planning and Design 7</i> pp. 227--238]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Knight, T, 1986, 'Transformation of the Meander Motif on Greek Geometric Pottery' <i>Design Computing 1</i> pp. 29--67]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Knight, T, 1989, 'Transformations of De Stijl art: the paintings of Georges Vantongerloo and Fritz Glarner' <i>Environment and Planning B: Planning and Design 16</i> pp. 51--98]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Koning H, and Eizenberg, J, 1981, The language of the prairie: Frank Lloyd Wright's prairie houses, <i>Environment and Planning B: Planning and Design 8</i> pp. 295--323]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Kotsopoulos, S D, Constructing Design Concepts: A computational approach to the synthesis of architectural form, Doctorate Dissertation, Massachusetts Institute of Technology.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[McGill, M C, 2002, Shaper2D: visual software for learning shape grammars, in: K. Koszewski, S. Wrona (Eds.), Design e-ducation: Connecting the Real and the Virtual, Proceedings of the 20th Conference on Education in Computer Aided Architectural Design in Europe, eCAADe, Warsaw, pp. 148--151.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1165997</ref_obj_id>
				<ref_obj_pid>1165993</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Prats, M, Earl, C, Garner, S and Jowers, I, 2006, Shape exploration of designs in a style: Toward generation of product designs, <i>Artificial Intelligence for Engineering Design, Analysis and Manufacturing 20</i>, pp. 201--215.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Sass, L, 2007, A Palladian construction grammar-design reasoning with shape grammars and rapid prototyping <i>Environment and Planning B: Planning and Design 34</i> pp. 87--106]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Stiny, G and Mitchell, W J, 1978, The Palladian grammar, <i>Environment and Planning B 5</i> pp. 5--18]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Stiny, G, 1977, Ice-ray: a note on Chinese lattice designs, <i>Environment and Planning B4</i> pp. 89--98]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Stiny, G, 1980, Kindergarten grammars: designing with Froebel's building gifts, <i>Environment and Planning B 3</i>, pp. 409--462]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Tapia, M, 1999, A visual implementation of a shape grammar system, <i>Environment and Planning. B, Planning and Design</i> 26 (1), pp. 59--73.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Wang, Y, Duarte, J P, 2002, Automatic generation and fabrication of designs, <i>Automation in Construction</i> 11 (3), pp. 291--302.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SHAPE GRAMMARS SIGGRAPH 2009 Course Mine Özkar, Assistant Professor Middle East Technical University 
Faculty of Architecture Department of Architecture Ankara, 06531 TURKEY Contact email: ozkar@metu.edu.tr 
George Stiny, Professor of Computation Department of Architecture Massachusetts Institute of Technology 
Cambridge, MA 02138 USA SHAPE GRAMMARS SIGGRAPH 2009 Course Lecturers: Mine Özkar, METU, and George 
Stiny, MIT The theory of shape grammars, first launched by Stiny and Gips in 1972, defines a formalism 
to support the ambiguity in creative processes that is generally ruled out by quantitative and symbolic 
computations. Since then, it has evolved into a groundbreaking pragmatist philosophy of shape and design. 
It is implemented in fields varying from architecture, art, graphic design, industrial product design to 
computer visualization. This course offers basic knowledge on the theory and some advanced issues useful 
for its implementation. The course will be in two consecutive sessions which are introductory and advanced 
and last 1 ¾ hrs each. The two­partite introductory lecture presents the fundamentals of the theory, 
focusing on the basic knowledge of shapes, shape algebras, and shape rules in order to explain how shape 
grammars translate visual and spatial thinking into design computation. Examples of shape grammar applications 
in design analysis and synthesis will be presented. Attendees with further and more technical interest 
in the topic are encouraged to follow the advanced lecture which initially dwells on the computational 
devices of shape grammars then to discuss a number of selected studies on the computational implementation of 
the shape grammar idea. Prerequisites: No prerequisites for the first session other than enthusiasm for 
shapes and a keen interest in looking and seeing. For the second session, general knowledge of the theory 
of shape grammars, which can be acquired in the first session. SHAPE GRAMMARS SIGGRAPH 2009 Course SYLLABUS 
Session 1 Introduction 8:30 The theory 1. What are shape grammars? 2. Describing shape grammars in 
terms of seeing and counting 3. Describing shape grammars as a rule­based system 4. Decompositions 
 5. The mathematical set­up of shape grammars 6. Basic elements: shapes, labels, weights 7. Shape algebras 
 8. Shape boundaries 9. Part relations: embedding, overlapping, discrete elements 10. Euclidean transformations 
 11. Maximal shapes 12. Boolean operations on shapes  Break 9:30 What to do with it? 13. The arts 
 14. Cultural heritage 15. Procedural modeling of architecture 16. Mass­customized housing 17. Economy 
of architectural manufacturing 18. Classifying architectural form 19. Building brand identity 20. 
Movement grammar in interaction design 21. Arts and Crafts 22. Understanding design possibilities (in 
design education)  Close, Q&#38;A Session 2 Advanced Issues 10:30 Recursion, Identity, Embedding 1. 
Recursion is the key to calculating. 2. Units create designs with blind ease. 3. Recursion and identity 
go just so far. 4. Embedding alters everything. 5. Seeing never ends.  Break 11:30 Recursion, Identity, 
Embedding continued 7. Hierarchies are seductive. 8. The value of embedding is that units and hierarchies 
never get in the way. 9. Recursion plus embedding includes recursion plus identity.  Close, Q&#38;A 
 George Stiny, MIT Mine Özkar, METU | SIGGRAPH 2009 (1) It is such a delight to be talking about shape 
grammars at this convention. And it is a great challenge. Not only because the audience is diverse, but 
also because the shape grammar theory itself dwells on something that is quite obvious but we take for 
granted. With my students back at home, I sometimes manage to talk so convincingly about the theory that 
they say we knew that! But my real aim is to get them start thinking about how they use it. We could 
aim for something similar here. Some of the things I say will seem quite mundane, but I encourage the 
audience to think whether they use or not use these in what they do, or how they think about their work. 
 Mine Özkar Introduction to Shape Grammars | SIGGRAPH 2009 (2) Title page to Session I a) A computation 
theory that defines a formalism to represent visual (and spatial) thinking; that handles ambiguities 
which symbols do away with. b) A philosophy of looking at the world that is not through learnt or imposed 
definitions but through those that have a practical meaning at a given point in time; that values the 
continuity of matter and flexibility in how to cut it up into its parts. (3) Shape grammars may be described 
at two levels. Firstly, it is a computation theory that defines a formalism to represent visual, or even 
spatial, thinking. At the same time, it handles ambiguities which digital computing does away with. Off 
the center, marginal,  The phrase shape grammar more literally refers to visual design grammars. At 
the second level, the theory represents a philosophy of looking at the world that is not through learnt 
or imposed decompositions (definitions) but through those that have a practical meaning at that point 
in time.  (4) Shape grammars were first introduced in the beginning of the 70s by George Stiny and James 
Gips. Published as one of the best computer papers of 1971, their Shape Grammars and Generative Specification 
paper introduced a set of generative rules for a few paintings done by Stiny himself. The three paintings 
in the article, are from a series called Urform. These are going to be the basis for illustrating various 
concepts of shape grammars in this part of the lecture. (5) Stiny (2006) claims that design is calculating 
while expanding the meaning of calculation to visual thinking via his theory of shape grammars. The motto 
design is calculating, was a starting point in 1971 as well. The reasoning behind a visual product was 
described using a grammar-like formalism with a vocabulary, a set of rules, and a series of computations 
that produced designs as if they were sentences . (6) Stiny often equates the terms design, visual reasoning 
and calculation. This claim firstly enunciates an understanding that design has reasoning within. Secondly, 
in the theory of shape grammars, the terms calculation and computation, which are often interchangeably 
used, are seen under a new light.   It is important to reflect on seeing and counting simultaneously 
to understand the key idea in SG. I have put up a graphic of the abacus to represent counting, which 
is at the root of computing and calculating. Beads, discrete and of one kind, are counted. Counting is 
one aspect of reasoning. Visual calculation on the other hand, gives room for seeing as well as for 
counting.  How does one calculate with shapes? Urform II, George Stiny, 1970, acryclic on canvas 30 
ins x 57 ins, blue, red, orange, yellow. (7) Questions arise. How does one calculate with shapes? Do 
visual kinds of thinking exclude calculation? Or does calculation reduced to counting exclude visual 
and spatial kinds of thinking? Stiny argues that one has to really see in order to count and that seeing 
is where creativity lies. (8) As in the abacus, counting requires discrete parts.  (9) One can divide 
Urform II into smallest possible discrete bits, perhaps into dots on the screen, each assigned with a 
different color code. This image shows a small section of the imagined screen of dots. These smallest 
primitives are countable but irrelevant in the perception of the whole. (10) Alternatively, one can 
divide Urform II into some obvious parts, distinct therefore countable. There are two of However, the 
painting is possibly a much more dynamically formed formal arrangement and is not simply a sum of discrete 
parts that were known before hand. (11) There are always some other parts to see. Moreover, these may 
be the meaningful parts, or parts that are surprisingly merged with one another. In the visual world, 
there are wholes that coexist, and they share parts, or parts of parts. This image shows a part that 
is not readily there but can be seen. (12) Calculation then, is to see first, then count. Key idea. 
What we take for granted is seeing. This way, we can calculate with different parts each time we look 
at Urform II. The shape shown exists in ten instances in Urform II: one large, nine small ones. (13) 
 (14)   Seeing and Counting        The shape grammar way of seeing and counting is visual 
rules that tell: see the left side and then replace it with what is on the right. (15) Stiny and Gip 
s explanation for the process behind the Urform series is a visual rule that tells one to see the left 
side to replace it with the right side. The illustration shows one such possible rule. These kinds of 
rules form the basis of shape grammars. (16) This is how it basically works. Looking for the left side 
in an initial shape set, in this case Urform I, one can see two instances of it.  (17) The second one 
is rotated 180o. (18) The rule is applied to the second one shown.  (19) The rule is then applied to 
the first instance.     A . B Shape grammars is a rule­based formalism. Rules show the particular 
shapes to be replaced and the manner in which they are replaced. The marker shows how to align the two 
shapes. Rather than if A, then B, visual rules say see SHAPE1, do SHAPE2. (20) Shape grammars is a rule­based 
formalism. This aspect is picked up more easily. Applications A shape rule has two steps when applied: 
a recognition of a particular shape shown on the left side and its possible replacement shown on the 
right side. The defined rule is operational. The arrow indicates an action. The unique feature of a shape 
rule is that the left and right side are visually considered. As opposed to symbols, shapes can be looked 
at and seen differently. This is due to their inherent ambiguities.  (21) Because shapes are visual, 
they can be decomposed in infinitely many ways. There should be no preconceived decompositions and primitives 
acquired through such operations. Visual rules, which are subjective, will call for various decompositions. 
 For example, let us look at one of the most popular examples Stiny (2006) gives to explain why we need 
to be computing with visual rules. There is a shape, composed of three triangles that will be rotated 
around its center. The only catch is, it will be rotated by a rule that says rotate triangle. (22) 
 (23) (24) In the nine step computation, Stiny shows that the initial definition of the shape, that 
is three triangles , changes in step 4 and then back again in step 6. Decompositions should not be timeless. 
The initial shape could have been drawn as three triangles, six lines, or 9 lines. Whatever the history, 
a new definition can always come up while working with shapes. What you see is what you get. This is 
motivation to see more. Ambiguity should be maintained.     The visual rule: rotate an equilateral 
triangle 180o around its center. Any questions?  Shapes, Labels and Weights Shape algebras Boundaries 
of shapes Part relations of shapes Euclidean transformations Maximal shapes Boolean operations with shapes 
 (25) The mathematical set-up of the theory includes general definitions of shapes, shape, weight and 
label algebras, shape boundaries, the most important of all part relations, Euclidean transformations, 
maximal shapes, and Boolean operations with shapes. Basic elements of shapes are points, lines, planes, 
and solids, with labels, if necessary, to give abstract information about them, and weights, as indicators 
of magnitudes of some formal attributes. (26) Shapes can be points, lines, planes, solids or combinations 
of these. Shapes also can have labels that indicate additional information about them and weights that 
indicate the magnitude of some formal properties. Labels are useful for adding more constraints necessary 
for tasks such as establishing the order in which rules are applied in computations. U0 0 U0 1 U0 2 
U0 3 U1 1 U1 2 U1 3 U2 2 U2 3 U3 3 Shapes are categorized under different shape algebras. The left index 
shows the dimension of the basic elements, and the right index shows the dimension in which these basic 
elements are combined in shapes. (27) Basic elements in shapes are categorized under different shape 
algebras. The indices indicate the dimension of the basic element and the dimension of the space in which 
these elements are combined and transformed. (28) All shape algebras that have 0 for the first index 
are atomic. A basic element within these algebras can only be a point and has no parts other than itself. 
Beads on the abacus belong here. Symbols (even if visual), for example, are elements of these algebras 
and have a dimension of zero. Also, units that add up to a sum of units belong in these algebras but 
in those that have the second index higher than  U0 0 U0 1 U0 2 U0 3 U1 1 U1 2 U1 3 U2 2 U2 3 U3 3 
 Atomic algebras (of points in space) 1. U0 0 U0 1 U0 2 U0 3 U1 1 U1 2 U1 3 U2 2 U2 3 U3 3  Boolean 
algebra (of zeroes and ones) (29) The algebra where both indices are 0 is Boolean. There are only two 
values, null and one. Something either is or is not. (30) All algebras with the indices equal to or 
larger than one, show different properties than atomic algebras. They do not have atoms but shapes with 
parts such as lines, planes, solids, etc. The number of members within a set in one of those algebras 
does not have to be finite. For example, in algebra U11, on a line space, there can be infinitely many 
lines of different lengths.  U0 0 U0 1 U0 2 U0 3 U1 1 U1 2 U1 3 U2 2 U2 3 U3 3  Algebras with part 
relations Algebra Basic Number Boundary elements of parts shapes U0 j points finite none U1 j lines 
infinite U0 j U2 j planes infinite U1 j U3 j solids infinite U2 j (31) There is a clear relation between 
the categories of basic elements belonging to different algebras. The boundaries of solids are plane 
shapes, the boundaries of planes are line shapes, the boundaries of lines are points whereas points have 
no boundary. Number of parts is finite in point algebras, in others no hence the ambiguities. U12 the 
relation between shapes and shapes on their boundaries. (32) Shape boundaries constitute a practical 
relation between shapes, which, in turn, helps us in the way we visually think. The rule in the illustration 
is in U12+U22 algebras combined. Parts of plane boundaries appear as line shapes and are utilized in 
generating the final form with planes. Three types of part relations are those of overlapping, embedded, 
or discrete shapes (33) Part relations are what differentiates shapes from atoms. Three kinds of part 
relations are between overlapping, embedding and discrete shapes. (34) Planes with no shared boundaries 
are discrete.  (35) Shapes that share a common boundary but have no part in common are also discrete. 
 (36) The two planes highlighted in slides 36 and 37 share a common boundary, but share no plane parts. 
 (37) Thus they are discrete despite the common boundary.  (38) Those shapes that share a common part 
overlap.  (39) The two planes shown share a common part, and are overlapping. Both shapes have parts 
that are not common with the other. (40) Those shapes that share a common part overlap.  (41) If two 
shapes have common parts and at least one of these shapes has no part that is not a part of the other, 
then this shape is said to be embedded within the other. The darker shape is embedded within the larger 
and lighter colored shape.       Rotation Translation Mirror reflection Scaling and combinations 
of these (42) Euclidean transformations that are used in shape grammars are translation, scaling, rotating 
and reflecting along with their combinations. In the example of the painting, I can relocate the left 
side of the rule that I showed in so many places using these transformations. I can scale it down and 
up, I can see its rotations, I can see its reflections, and I can see it in multiple places, which are 
illustrated in slides 44 through 48. (43) Let us start with any perceived shape within Urform II.  
(44) I can identify it in a smaller size.  (45) I can identify it in a mirror reflection.  (46) I can 
identify it in a 90o counter clock wise rotation. (47) I can identify it in another location in the 
painting.     Sum A+B Difference A B Product A · B=A (A B) Symmetric difference A .B= (A B)+ 
(B A) A .B= (A +B) (A · B) (48) Within the defined shape algebras, we can add and subtract shapes of 
the same kind of basic elements. We can also take their unions and products. This is basically how we 
compute the visual rules. We can combine algebras to do Boolean operations on different kinds of basic 
elements in parallel. Symmetric difference of two planar shapes Sum of two line shapes (49) Here are 
illustrations to possible Boolean operations on shapes based on the Urform series. The first operation 
shows the symmetric difference of two plane shapes of the same weight in U22 whereas the second operation 
shows the sum of the boundaries of these two planes in U12. (50) Let us assume that there are three 
initial shapes for another set of examples of operations on shapes of equal weight value in U22. (51) 
Firstly, the difference of shapes one and two is calculated.  (52) Then, the sum of shapes two and three 
is calculated and  (53) subtracted from the result of the first step.  (55) is followed by the symmetric 
difference of the two.  (56) The three shapes, newly emerged from these operations, are assigned different 
weights and summed up.       Up until this point, we have shown how shape algebras, Boolean operations 
and part relations all work separately for computing with shapes. In the next part, more examples, from 
actual applications, will be utilized to illustrate these concepts further and together. Mine Özkar 
Introduction to Shape Grammars | SIGGRAPH 2009 (57) Break  Mine Özkar Introduction to Shape Grammars 
| SIGGRAPH 2009 (1) In this section, we will dwell on a selected set of existing and possible applications 
of the theory in various design related areas. In addition to architecture, where it is most popular, 
there are quite a few different venues that shape grammars are applied in, from crafting to brand identity, 
and from interaction design to urban design. Examples usually are categorized as analysis, synthesis 
or a combination of both approaches in design, all useful for different needs. Additionally, examples 
can be viewed according to whether they primarily make use of the rule-based approach (i.e. description 
of design decisions with rules and the design process as computations with these rules) which is straight 
forward to understand and apply from a systematic perspective, or of the unique formalism that allows 
for part relation (embedding) in the application of rules (i.e. computation without the need for predefined 
primitives and open to surprises). Looking at the upcoming examples, I would like to encourage you to 
engage in seeing parts as much as in understanding the recurring rules. FIGURE Ilhan Koman, Rolling 
Lady out of metal foil, 1983 (on the left) and PI series, 1980­1983 (on the right, photographed by Tayfun 
Tuncelli). Source: T. Akgun, A. Koman and E. Akleman, 2006, Developable Sculptures of Ilhan Koman, Proceedings 
of Bridges 2006, London. (2) Exploring formal constructions in art are directly relevant to the explorations 
that the theory of shape grammars dwells on and encourages. Ilhan Koman, a Turkish artist who has lived 
and produced mostly in Sweden, has dedicated his life to the systematic study of simple geometric forms 
and the variety attained from their derivatives. He stands for a conscious artist persona whose work 
is teaching to many art and design students on the pragmatic relation of art with mathematics and geometry. 
His works are mostly cases of recurring spatial relations that transform. The Rolling Lady is the display 
of two instances of a particular geometric shape spatially connected to one another. The PI series explore 
the different degrees in which a spatial relationship can be applied and that its recursions lead to 
various results. Although Koman is not quoted for having utilized shape grammars, his works showcase 
the theory from within the field. FIGURE Richard Diebenkorn, Ocean Park No. 111, oil and charcoal on 
canvas, 336.2 x 336.7 cm (1978), and steps in its generation from the grammar. Source: Joan L. Kirsch 
and Russell A. Kirsch, 1988,The Anatomy of Painting Style: Description with Computer Rules, Leonardo, 
Vol. 21, No. 4, pp. 437­444. (3) Russell Kirsch is not only known as the creator of the first digital 
image but also as one of the earlier people who embraced the idea of a rule-based picture grammar. In 
a study dating back to 1988, Kirsch and Kirsch analyze and define a grammar for Diebenkorn s Ocean Park 
painting series. (4) Focusing on the fluidity of the artist s mark on the canvas and attributing due 
value to seeing shapes as they are, Jacquelyn Martino identifies a curvilinear shape grammar in analysing 
her own art work and process.  The art work presented here to sample the style is an early digital 
painting in the Devotion series. On the right are three rules that show early phases of development of 
another work of similar process. FIGURE Source: Xiu Wu Huang, Cheryl Kolak Dudek, Lydia Sharman, Fred 
E. Szabo, 2005, From Form to Content: Using Shape Grammars for Image Visualization, Ninth International 
Conference on Information Visualisation (IV'05), pp.439­444. (5) Understanding, preserving, continuing, 
reinterpreting and progressing processes behind forms of cultural heritage is an important and developing 
application field for shape grammar studies. These studies may employ both the analysis of design corpus 
and synthesis of new designs if relevant to the context. Additionally, developing computer aids for such 
analysis and synthesis is of great practical value. In this example, we see a study that not only analyses 
a traditional craft of cloth patterns, namely Kuba cloth, but also introduces the notion of intelligent 
tracing of such forms and their spatial relations using computer visualization tools built in Mathematica. 
The illustrations are of a sample Kuba cloth (on the left), initial modular shapes (in the middle), and 
the Kuba grammar rules with left and right hand sides (on the right). (6) Islamic patterns have been 
of interest to the Shape Grammars community for some time. The studies so far dwell on the recursion 
of modules or tiles under Euclidean transformations. The continuity, perhaps the most important theological 
concept employed in these patterns, actually might be calling for a more thorough analysis of varying 
parts and wholes in perception. These patterns are not only works to be admired on facades of historic 
buildings or interior artifacts of various material, but also systems of lines that showcase geometric 
construction to many design students. Therefore the study of the process of how they are constructed, 
not as tesselation of tiles according to one scholar argument, may be quite relevant in synthesis of 
new ones. (7) Ömür Bakirer has observed in ancient documents that the patterns are constructed based 
on regular tesselations of circles. This is a different approach than identifying tiles that repeat. 
Lines are continuous and present different parts and wholes to the eye. (8) The basic visual rule to 
build the tesselation of circles is that each new circle is drawn with reference to an existing circle 
passing through its center, and at the same time centering on its perimeter. If this rule application 
is not narrowed, tesselations can be as varied as the group shown below the rule. (9) Nonetheless, circles 
are deterministically arranged, and overlaying grids of new lines are constructed. (10) New lines are 
added as groups of them start making up the shapes of the final pattern. (11) Alternatively new circle 
tesselations with circles of varying sizes can be introduced. (12) In the abundance of what one can 
see, various polygons can also come forth.  (13) The polygon in the previous slide is actually from 
an existing example carved in stone. One can identify different repeating tiles, stars or polygons each 
time one looks. Wholes keep changing to the eye. To approach these constructions as tilings could be 
an underestimation in most cases. (14) There are more complex examples where tiles are not easy to read 
at all!       FIGURE Source: Peter J. Lu and Paul J. Steinhardt, 2007, Decagonal and Quasi­Crystalline 
Tilings in Medieval Islamic Architecture, Science 315 (5815), pp. 1106­1110. Supporting Online Material. 
(15) A recent and most celebrated work on quasi-crystalline Islamic tilings highlight the understanding 
of these patterns as tesselations of predefined units. The end product however still allows for different 
readings of parts and wholes. (16) These patterns also exist in 3D and as structural architectural elements 
and not just decoration. In muqarnas, units are pre-cut and carefully placed to form a continuous inverted 
cascade.   FIGURE Source: Gülru Necipoglu, 1995, The Topkapi Scroll Geometry and Ornament in Islamic 
Architecture, Getty Center for the History of Art and the Humanities, Santa Monica, CA. (17) In the infamous 
Topkapi Scroll, a documentation of the repertory of geometric designs dating back to the 15th and 16th 
centuries, we see that the geometric relations between units are pre-studied and documented in drawings 
that reveal the geometric understanding behind them to guide the craftsmen in the manufacturing of these 
designs. FIGURE Source: Armen Ghazarian and Robert Ousterhout, 2001, A Muqarnas Drawing from Thirteenth­Century 
Armenia and the Use of Architectural Drawings during the Middle Ages, Muqarnas, Vol. 18, pp. 141­154. 
(18) Since most muqarnas are architectural elements that enclose spaces, full understanding of their 
construction and possible synthesis of new designs may be extremely relevant in restituting or restoring 
ruins. The photograph shows what is left from a muqarnas in an Armenian church as an example to suggest 
that the knowledge of its grammar might help in completing its missing parts. (19) Continuing on the 
thread of cultural heritage, another group of examples are from modeling of architecture or urban environments. 
The procedural modeling of Pompeii, as shown here, is one of the results of a study that may contribute 
to understanding and appreciating historic built environments. The study, in fact, has broader implications 
from building facades to building city structures such as roads, or constructing virtual gaming environments. 
The models are based on context sensitive shape grammar rules.  For more information and examples see 
http://www.vision.ee.ethz.ch/~pmueller/wiki/CityEngine/Documents 10. (20) Lipp, Wonka and Wimmer take 
the approach of procedural modeling to the next level and allow for interaction to edit rulebases visually 
rather than through the script. FIGURE Source: J P Duarte, 2005, Towards the mass customization of housing: 
the grammar of Siza s houses at Malagueira, Environment and Planning B: Planning and Design, 32(3), pp. 
347­380. (21) One of the well known applications of the theory of shape grammars in architecture is the 
Siza grammars developed by Jose Duarte for mass customizing social housing by a world renown Portugese 
architect. The project comprises of an analysis of Siza s Malagueira housing design corpus, developing 
its detailed shape grammar that Siza himself is content about, and the synthesis of new designs in which 
the users are actively involved through a computer interface. FIGURE Habitat for Humanity Housing Grammar, 
Kotsopoulos 2005 (22) In architecture, mass housing is a significant issue especially in developing countries. 
Shape grammars that can help designers enumerate customized alternatives without much cost are crucial 
in sustaining desired built environments that meet standards at the least. Sotirios Kotsopoulos has also 
dwelled on this notion in his entry for a Habitat for Humanity housing competition with a proposed shape 
grammar. FIGURE Habitat for Humanity Housing Grammar, Kotsopoulos 2005 (23) Parametric rules for general 
massing of units, and modularity were key elements of the proposal. FIGURE Source: M. Botha and L. Sass, 
2006, Instant House: Design and digital fabrication of housing for developing environments, CAADRIA 2006 
[Proceedings of the 11th International Conference on Computer Aided Architectural Design Research in 
Asia] Kumamoto, Japan, pp. 209­216. (24) Botha and Sass introduce mass customization in housing with 
elevated concern for economy and environmental conditions. The grammars they utilize allow for adaptation 
in designs based on changing conditions. FIGURE Source: M. Botha and L. Sass, 2006, Instant House: Design 
and digital fabrication of housing for developing environments, CAADRIA 2006 [Proceedings of the 11th 
International Conference on Computer Aided Architectural Design Research in Asia] Kumamoto, Japan, pp. 
209­216. (25) Their approach also addressed fast and transportable housing production needs in natural 
disaster emergency and poverty stricken locations. FIGURE Source: L. Sass, 2007, Synthesis of design 
production with integrated digital fabrication, Automation in Construction, Vol. 16, No.03, pp. 298­310. 
(26) Sass s approach overall aims to integrate design synthesis based on changing needs with digital 
fabrication for a low cost but customized production in the end. FIGURE Sources: L. Sass, 2005, A Wood 
Frame Grammar, Computer Aided Architectural Design Futures 2005 [Proceedings of the 11th International 
Conference on Computer Aided Architectural Design Futures Vienna, Austria, pp. 383­392. L. Sass, 2007, 
Synthesis of design production with integrated digital fabrication, Automation in Construction, Vol. 
16, No.03, pp. 298­310. (27) The wood frame grammar that is the construction system in these examples 
was developed by Sass. The tables show, in part, the rule set of joints or how larger parts come together 
(on the left) and the classification of building component types and their relations (on the right). 
 FIGURE Source: Naomi Ando, Nobuhiro Yamahata, Syuta Masumi, Masahiro Chatani, 2001, Shape Grammar and 
Form Properties of Architectural Figures, Journal for Geometry and Graphics, Volume 5, No. 1, 23­33. 
(28) Another application of the theory in relation with architecture is the attempt to classify general 
architectural forms through a grammar based on a set of common properties (of changing values). FIGURE 
Source: J.P. McCormack, J. Cagan and C.M. Vogel, 2004, Speaking the Buick Language: Capturing, Understanding 
and Exploring Brand Identity with Shape Grammars, Design Studies 25, pp. 1 29. (29) In an application 
in the industrial design field, shape grammars are utilized to showcase an established brand identity 
for Buick cars. In the figure above a sample of novel Buicks are shown. The variety is created to address 
specific needs or desires. FIGURE Source: A. Asokan and J. Cagan, 2005, Defining cultural identities 
using grammars: an exploration of "cultural languages" to create meaningful experiences, Proceedings 
of the Conference on Designing for User Experiences DUX05, pp. 2­11. (30) Asokan and Cagan introduce 
the movement grammar for actions of coffee drinking in a specific culture. They perform and analysis 
of movement rituals to form a grammar, and use this grammar in the design of objects that are directly 
utilized in the said actions. They address the unique notion of movement grammars, cultural languages 
and interaction design simultaneously. (31) To go back to the simple 2D shapes that we started with 
in the beginning of the section, let us go back in time as well. The interest in how shapes are constructed 
out of recurring parts or how they are decomposed into unprecedented parts has existed for a long time. 
In A Theory of Pure Design, Ross, Harvard professor, shows, in as early as 1907, how to construct various 
shapes out of parts. Source: Denman W. Ross, 1907, A Theory of Pure Design: Harmony, Balance, Rhythm, 
Houghton, Mifflin and Company, Boston and New York, p 25, 40, 41, 46, and 65. The first shows varying 
distance in the spatial relation of points, the second shows a symmetric group of mirror-reflected curvilinear 
parts, the third, fourth, and fifth show transformations of arcs compiled in groups to bring about continuous 
forms. Ross utilized this grammar and likes of it in creating wallpaper patterns as the one shown.  
 FIGURE Source: Daniel Kornhauser, 2002, Designing a Craft Computing Environment for Non­Industrial 
Settings, unpublished M.S. thesis, MIT, Cambridge, MA. (32) One of Ross s contemporaries, the Mexican 
artist Best Maugard, introduced a basic vocabulary of shapes that Daniel Kornhauser puts to use in his 
research on craft computing. Best Maugard writes, The suggestions and rules that we will follow are simple 
and easily understood by everyone. They are quickly grasped and retained in the mind of the student. 
In this method, there are seven simple motifs and signs, which we consider as fundamental, and a few 
rules to follow, and these, once in the student s memory, will enable him to make an infinite number 
of combinations and designs See Adolfo Best-Maugard, 1926, A Method for Creative Design, Alfred A. Knopf, 
New York and London, p 1-2. He aims to identify a finite global vocabulary of basic elements and sees 
design as combinatorial arrangement of these elements. His vocabulary is quite strict and limited compared 
to Ross s. Nonetheless, Kornhauser understands the value and utilizes this vocabulary to develop spatial 
relations and rules that result in designs ready to be crafted. FIGURE Source: Daniel Kornhauser, 2002, 
Designing a Craft Computing Environment for Non­Industrial Settings, unpublished M.S. thesis, MIT, Cambridge, 
MA. (33) Kornhauser shows the design and manufacturing process for a copper plate, from the digital tool 
to the hand crafting. FIGURE Source: Daniel Kornhauser, 2002, Designing a Craft Computing Environment 
for Non­Industrial Settings, unpublished M.S. thesis, MIT, Cambridge, MA. (34) Kornhauser illustrates, 
through numerous screenshots, the digital crafting of a spider web design. Understanding design possibilities 
HH H H H { { { H H {{ { {{ { { H {{ { {HH H { { { {{ H" { { { { H" H" H" H . H HH HH HHHH HH H H HH H{ 
H{ H{ H{ H{ H{ H{H{ H{ H{ H{ HH H{ HH H{ H{ H{ HH HH H{ H{ H{ H{ H{ H{ H{ H{ H{ H" HH HH H" H" H" HH 
HH HHHH HH (35) The theory can also be utilized to introduce beginning design students to an understanding 
of the design process in which decisions are traced, questioned, exploited to the full extent of possibilities. 
In a very simple formal organization exercise, the top left figure is an actual proposal to a given problem 
that asked for the arrangement of 9 identical units in a square format. In a scenario where possible 
spatial relations of two units are tried first (the visual rule given above), the complex internal arrangement 
of the unit is reduced down to one line. Eight transformed instances of this unit is given in the middle 
row. This reduced version already provides many possibilities to try out. On the right, the alternate 
black and white shadings are also introduced increasing the possibilities. The three layouts below left, 
showcase different arrangements of just these pairings in a group of 9 units. That the theory of shape 
grammars could be applied in design education to demystify design processes in the eyes of the novice 
designer is perhaps among its most valuable traits towards societies that are more design oriented. 
(36)  (1) Recursion is the key to calculating in the way Turing and others recommend, and a staple today 
in logic, linguistics, and computer science in fact, wherever calculating is tried. The way recursion 
works is clear when rules are used to change arrangements of independent units or symbols in a combinatory 
process. In this example, a rule made up of squares inscribes a small square in a big one. The square 
in the left side of the rule occurs twice in the right side to provide another place for the rule to 
apply again. (2) The Russian constructivist Jacob Tchérnikhov uses the rule for squares and another 
rule just like it for quadrilaterals in these designs in black and white. (3) Rules and recursion define 
Chinese window grilles. These lattice designs are called ice-rays. (4) The rules for ice-rays divide 
polygons into polygons. The division rule x . div(x) and the addition rule x . x + x are equivalent, 
when div(x) divides x into x and x . The polygons x, x , and x are always triangles, quadrilaterals, 
or pentagons. (5) The rules apply in this way to create an ice-ray lattice.   (6) This ice-ray was 
shown at SIGGRAPH 2008 in the first SIGGRAPH exhibit on design and computation. (7) Some ice-rays are 
produced in a definite way.  (8) Rules are applied from left to right. It s the same in action painting 
when up strokes and down strokes alternate across a surface. (9) Rules make smaller divisions to add 
finer detail.  (10) Multiple divisions are also possible with tri-axial motifs, and motifs with four, 
five, and six axes. (11) In these ice-rays, there s an initial division with a multi-axial motif.  
 (12) This ice-ray is also from the SIGGRAPH 2008 exhibit.  (13) It s easy to define rules for symmetrical 
ice-rays and ones with other special properties. (14) This ice-ray lattice is created almost entirely 
with tri-axial divisions.  (15) The ice-ray is one of my favorites, because its divisions are so novel. 
 (16) Ice-rays modulate light and cast changing shadows.  (17) The recursive division/addition rules 
for ice-rays can be used in many other ways, too. For example, the top figure is a painting by Georges 
Vantongerloo, and the bottom figure is a plan by the architect Alvaro Siza. Both have perpendicular divisions. 
 (18) This is a painting by Fritz Glarner. Angles vary, but not by much.  (19) These plans show the 
two floors in a medieval building in Venice.   (20) The ground floor has perpendicular divisions, and 
the upper floor has parallel divisions. The plans look different because of this.    George Stiny 
Recursion, Identity, Embedding | SIGGRAPH 2009 Title page to Session II          Current systems 
are not only remarkably inflexible, but tend to hang on to ontological commitments more than is necessary. 
Thus consider this sequence of computer drawings. Suppose that the figure in step 2 was created by first 
drawing a square, then duplicating it, as suggested in step 1, and then placing the second square so 
as to superimpose its left edge on the right edge of the first one. If you or I were to draw this, we 
could coherently say: now let us take out the middle vertical line, and leave a rectangle with a 2:1 
aspect ratio, as suggested in step 3. But only recently have we begun to know how to build systems that 
support these kinds of multiple perspectives on a single situation (even multiple perspectives of much 
the same kind, let alone perspectives in different, or even incommensurable, conceptual schemes). On 
the Origin of Objects Brian Cantwell Smith (1998) Step 1 Step 2 Step 3 (21) Recursion may not be all 
there is to calculating when it comes to visual experience. This is the problem shape grammars solved 
more than thirty years ago. The key is to use recursion with embedding instead of identity to calculate 
with shapes by seeing and not with units or symbols by counting. This may go beyond what Turing originally 
had in mind. It asks what calculating would be like if Turing had been an artist/designer and not a logician. 
Embedding alters everything. One shape is embedded in another shape if it can be cut out or traced. Units 
aren t defined in advance, because there s no telling what they are ahead of calculating. They change 
freely as rules are applied. This isn t so with identity. Units are defined once and for all at the start, 
in some sort of precalculating before rules are tried. Units stay the same. In combination, they determine 
(limit) what there is to see and how to go on. But this contrast may be misleading. Calculating isn t 
a dichotomy: at the very least, calculating by seeing includes calculating by counting, because identity 
is a special case of embedding. (22) These designs are created recursively by combining triangles. They 
re the units. The design f(n +1) is four copies of the design f(n), so triangles are times four. If there 
s a single triangle to start, f(n) = 4n. But visual experience may disagree. How many triangles are in 
f(2)? There are 42 = 16 and 12 more. And that s not the half of it what about the two squares in f(1), 
and the cross and 2 x 2 grid in f(2)? These new figures may be easier to see than triangles. They really 
stand out. Can I calculate with them if triangles disappear? Where do the triangles go? (23) It s true 
that f(3) = 64, but what does f(3) look like? Do you see any triangles? How many? Four copies of f(2) 
are in f(3). Do the figures in f(2) stand out? In general, 4m copies of f(n) are in f(n+m). Are these 
copies or the figures in them obvious? Aren t there other things to see? Is anything salient now a combination 
of triangles or anything you ve seen before? It s easy to combine units in designs, but the process is 
blind it misses the variability in visual experience. Recursion and identity go just so far. They re 
visually incomplete. Embedding fills in the rest, so that you can calculate with anything you see. (24) 
There are two squares.  (25) Two squares are lots of things four triangles, and also pentagons and 
hexagons in various ways. (26) A rule that translates polygons also rotates them! Polygons are embedded 
in surprising ways that may ignore what you ve done. There s no record of this to block your way, and 
nothing to remember. Anything you see can be used to go on. (27) How are polygons defined? Wikipedia 
gives a definition and helpful examples.  (28) Maybe these figures are polygons, too pentagons and 
heptagons.  (29) Then there are K s big ones and little ones like those in my dictionary. (30) Big 
K s come in any size.  (31) There are just as many little k s.  (32) Whether it s for a big K or a 
little k, the series is dense. There are myriads to see. (33) Seeing makes a difference. Both of these 
building plans (diagrams) are based on two squares, but they re articulated in alternative ways to express 
different things. The plans may even be opposites. One plan uses four K brackets to define exterior walls 
and interior corners, while the other plan uses four triangular pieces for exterior corners and interior 
walls. (34) This is a lesson that s taught in the third grade. It s a plan or map of a room. The schoolchildren 
draw it and describe what they find. There s seeing and saying. (35) This is the key for the objects 
in the room. It s a list of rules to see what s there. Are these rules shapes or symbols? Do they apply 
in terms of embedding or identity? The lesson is a nice way to introduce such questions, although educators 
may not know it. (This may be one of the many times in school when children are expected to give up their 
fickle ways and learn an adult answer, that is to say, to trade embedding for identity (creativity for 
greater certainty) as rules are tried. Then it seems that the purpose of education is to limit what there 
is to see and take what little remains seriously. The loss of ambiguity and breadth may make communication 
and shared understanding easier some value to the community is undeniable but the cost is too high 
if you aren t free to look again. Calculating with shapes and rules seeing doesn t work by rote. It 
s an open-ended process that s independent of what you ve done or may remember. What you see may change 
erratically at any time. It s always a surprise!) (36) How many tables are in the drawing? How many 
desks? How can you tell? What do you see? What did my daughter say when she was asked to count tables 
and desks? How did I reply? What answer did the teacher expect? Are there other ways to do this? When 
do embedding and identity agree? (37) This is another kind of Chinese window grille based on a checkerboard 
lattice. The squares in a grid are filled with H s that go this way or that on alternating diagonals. 
 (38) Here are more examples of the same kind of lattice design.  (39) These are the motifs that are 
used in the squares in the underlying grid. They alternate horizontally and vertically, and may be easier 
to find than H s.  (40) What do you see? Is this merely a checkerboard?   (41) A vector field goes 
through the lattice from left to right. It s for everyone to see.  (42) Maybe the forces are in equilibrium. 
 (43) No, it s an illusion! What a neat way to go from physics to a trick of perception. With embedding, 
you can change your mind about what you see every time you look. (44) Seeing never ends. Look again. 
 (45) It may help to erase the grid to create a new lattice design.   (46) The lattice is an array 
of squares.  (47) The lattice is a tessellation of Greek crosses in a figure-ground reversal.  (48) 
Is this a checkerboard lattice?  (49) The grid is rotated 45°, and a new motif is oriented left or right 
on diagonals. This is an effective way to create a checkerboard lattice, but there s usually more to 
calculate once the lattice is done. It s easy to go on with embedding. (50) What do you see? Maybe octagons, 
octagon-squares, and supplementary squares but surely, not a checkerboard. The grid and the motif have 
disappeared.  (51) This is a Palladian villa plan. It uses the same kinds of rules that are used for 
Chinese window lattices. The same rules can do all sorts of different things. (52) This partial catalog 
shows 20 possible plans.  (53) All of these plans were created by the rules. Palladio designed some 
of them. Can you guess which ones? Even the experts are fooled. The confusion is telling!  (54) A rule 
to translate polygons rotates them when rules are defined with embedding. There are other ways to calculate 
like this, too, in the same family of shapes. (55) A rule that rotates triangles about their centers 
keeps these points fixed. But the rule also rotates a pinwheel, so that these points change. This can 
t be right. Is it a new paradox? It isn t something to think about, it s something to see. (56) This 
is the way the rule is used to calculate. Three triangles are two, and two triangles are three when there 
s embedding. Isn t this cheating?   (57) Hierarchies are a seductive way to show how things work by 
dividing them into independent parts and mapping their relationships. Hierarchies take work they make 
your brain hum and they reward the effort with an aura of understanding. Many find hierarchies indispensible. 
They make things memorable and comprehensible. Rules also define hierarchies when they apply recursively. 
But this may be a clumsy and disappointing way to describe what s going on with embedding, because then 
things change. Parts aren t independent they fuse and divide freely, with scant regard for what they 
were before. Nonetheless, comparing alternative hierarchies may help to show how complicated it can be 
to turn seeing into counting when you calculate. (58) An erasing rule defines rival hierarchies for 
the same shape. There are three triangles or two in the way I ve been calculating. (59) This is how 
the triangles in the hierarchies are related. What these triangles have in common divides them retrospectively 
to define finer units that are consistent with both hierarchies and that augment them. (60) This is 
how to change one hierarchy into another. The graphs are isomorphic the one shows the switch from three 
triangles to two, while the other goes from two triangles to three. It may get complicated when units 
are moved around. There are apt to be knots and tangles. But this kind of thinking is tedious and unnecessary. 
It s much easier to see three triangles or two, to switch what you see whenever you like without worrying 
about what units there are and where they go. That s the value of embedding. Units and hierarchies never 
get in the way. They aren t needed to calculate, and they aren t needed to see. (61) Here s another 
example with triangles and squares.  (62) Four triangles are two squares, and vice versa. With embedding, 
it s easy to switch back and forth at any time. But with identity, this isn t as straightforward. It 
s hard to find the units that allow for this kind of change, especially before calculating begins. How 
do you know that triangles are going to be squares or anything else maybe pentagons, hexagons, or K 
s? You may need the prior ability to calculate with shapes and embedding in order to define the units 
you need to calculate with identity either that, or a special kind of prescience. Or maybe calculating 
with embedding is merely pseudo-calculating. You do it to learn how to calculate. But then, what s the 
point of calculating if you already know the answer? Perhaps it s to save time and effort doing the same 
kind of problem again. Ice-rays are like that, and so are checkerboard lattices. Of course, creating 
these designs is only a start. There s always something new to see. In art and design, it s hard just 
to do it again and not to see and do more. (63) These are the hierarchies for four triangles and two 
squares.  (64) This is the way the triangles and squares in the hierarchies match up to define finer 
units to add to the hierarchies. (65) This is the way the hierarchies are changed one into the other. 
It s a little more complicated than it was before with three triangles and two, but the pattern is clear. 
 (66) The two previous examples open an ongoing series of triangles, triangles and squares, triangles 
and pentagons, etc. (67) The shapes in the series can be elaborated with rules. The number of different 
ways of seeing the shapes in each row in terms of triangles, triangles and squares, triangles and pentagons, 
etc. grows exponentially the number of ways for the n-th shape is the n-th Fibonacci number, when Fibonacci 
numbers go 1, 2, 3, ··· . Of course, there are other polygons in the shapes, and many other surprises, 
too, that may pop in and out of view. There s no dismissing any of this without serious risk. It s impossible 
to tell in advance what will be of use or when it might be needed. (68) These are some of the simple 
rules I ve been using with embedding to calculate with shapes. (69) Twin lattices order various types 
of rules for art and design. Whatever creativity implies is possible when recursion and embedding are 
used together to calculate. For example, it s easy to do everything on the fly, to change what you see 
as you go on in a visual kind of improvisation. (70) Paul Klee did this drawing it s a palm-leaf umbrella. 
But it looks more like a fan. Try using the rules in the first lattice to make it. Is this easier to 
do with the rules in the second lattice?  (71) Recursion plus embedding (calculating by seeing in the 
artist s/designer s way) includes recursion plus identity (calculating by counting in Turing s way). 
Calculating by seeing extends calculating by counting. The inverse of this relationship is something 
to think about, too. Can recursion plus identity do (simulate) everything that recursion plus embedding 
does? This is an open question worth trying. But perhaps there s no complete answer, just many ad hoc 
ones. For example, the answer is yes for shapes made up of points, lines, planes, and solids in the algebras 
Uij, and their extensions with labels (numbers, symbols, etc.) and weights (colors, materials, properties, 
etc.) in the algebras Vij and Wij. This is a good start, and, in fact, it covers a lot. If you need more, 
there are also affirmative answers for conics, and cubic curves and surfaces. Or better yet, add to this 
list!         Tables Desks Rules Teacher 2 1 Symbols (identity) Catherine 3 1 Shapes (embedding 
+ isometry) Dad 5 4 Shapes (embedding + similarity)                   (72) Don t forget 
Klee s drawing! Try the rules they really work.  George Stiny Recursion, Identity, Embedding | SIGGRAPH 
2009 (73) Break  SHAPE GRAMMARS SIGGRAPH 2009 Course BIBLIOGRAPHY Main reference Stiny, George, 
2006, Shape, MIT Press References Akgun, T, Koman, A, and Akleman, E., 2006, Developable Sculptures of 
Ilhan Koman, Proceedings of Bridges 2006, London. Ando, N, Yamahata, N, Masumi, S, Chatani, M, 2001, 
Shape Grammar and Form Properties of Architectural Figures, Journal for Geometry and Graphics, Volume 
5, No. 1, 23­33. Asokan, A and Cagan, J, 2005, Defining cultural identities using grammars: an exploration 
of "cultural languages" to create meaningful experiences, Proceedings of the Conference on Designing 
for User Experiences DUX05, pp. 2­11. Bakirer, Ö., 1981, Selçuklu Öncesi ve Selçuklu Dönemi Anadolu Mimarisinde 
Tugla Kullanimi [The use of brick in Anatolian architecture in pre­Seljuk and Seljuk era]. Ankara, Turkey, 
ODTÜ. Best­Maugard, A, 1926, A Method for Creative Design, Alfred A. Knopf, New York and London, p 1­2. 
Botha, M, Sass, L, 2006, Instant House: Design and digital fabrication of housing for developing environments, 
CAADRIA 2006 [Proceedings of the 11th International Conference on Computer Aided Architectural Design 
Research in Asia] Kumamoto, Japan, pp. 209­216. Duarte, J P, 2005, Towards the customization of mass­housing: 
the grammar of Siza s houses at Malagueira, Environment and Planning B: Planning and Design, 32, pp. 
347­380 Ghazarian, A and Ousterhout, R, 2001, A Muqarnas Drawing from Thirteenth­Century Armenia and 
the Use of Architectural Drawings during the Middle Ages, Muqarnas, Vol. 18, pp. 141­154. Huang, X W, 
Kolak Dudek, C, Sharman, L, Szabo, F E, 2005, From Form to Content: Using Shape Grammars for Image Visualization, 
Ninth International Conference on Information Visualisation (IV'05), pp.439­444. Kirsch, J L and Kirsch, 
R A, 1988, The Anatomy of Painting Style: Description with Computer Rules, Leonardo, Vol. 21, No. 4, 
pp. 437­444. Kornhauser, D, 2002, Designing a Craft Computing Environment for Non­Industrial Settings, 
unpublished M.S. thesis, MIT, Cambridge, MA. Lipp, M, Wonka, P, and Wimmer, M, 2008, Interactive visual 
editing of grammars for procedural architecture. ACM Trans. Graph. 27, 3 (Aug. 2008), 1­10. Lu, P J, 
and Steinhardt, P J, 2007, Decagonal and Quasi­Crystalline Tilings in Medieval Islamic Architecture, 
Science 315 (5815), pp. 1106­1110. Supporting Online Material. Martino, J A, 2006, The Immediacy of the 
Artist's Mark in Shape Computation: from Visualization to Representation. Doctoral Thesis. UMI Order 
Number: AAI0809838., Massachusetts Institute of Technology. McCormack, J P, Cagan, J, and Vogel, C M, 
2004, Speaking the Buick Language: Capturing, Understanding and Exploring Brand Identity with Shape Grammars, 
Design Studies 25, pp. 1 29. Müller, P, Wonka, P, Haegler, S, Ulmer, A, and Van Gool, L, 2006, Procedural 
modeling of buildings. ACM Trans. Graph. 25, 3 (Jul. 2006), 614­623. Necipoglu, G, 1995, The Topkapi 
Scroll Geometry and Ornament in Islamic Architecture, Getty Center for the History of Art and the Humanities, 
Santa Monica, CA. Parish, Y I and Müller, P, 2001, Procedural modeling of cities. In Proceedings of the 
28th Annual Conference on Computer Graphics and interactive Techniques SIGGRAPH '01. ACM, New York, NY, 
301­308. Ross, D W, 1907, A Theory of Pure Design: Harmony, Balance, Rhythm, Houghton, Mifflin and Company, 
Boston and New York. Sass, L, 2005, A Wood Frame Grammar, Computer Aided Architectural Design Futures 
2005 [Proceedings of the 11th International Conference on Computer Aided Architectural Design Futures 
Vienna, Austria, pp. 383­392. Sass, L, 2007, Synthesis of design production with integrated digital fabrication, 
Automation in Construction, Vol. 16, No.03, pp. 298­310. Stiny, G and Gips, J, 1972, Shape Grammars and 
the Generative Specification, Petrocelli OR (ed) Best computer papers of 1971, pp. 125­135 Suggested 
readings Chase, S, 2005, Generative design tools for novice designers: Issues for selection, Automation 
in Construction 14 (6), pp. 689­698. Chomsky, N. 1957, Syntactic Structures, Mouton, The Hague Flemming, 
U, 1987a, More than the sum of parts: the grammar of Queen Anne houses, Environment and Planning B:Planning 
and Design 14 pp. 323­350 Flemming, U, 1990, Syntactic Structures in Architecture, The Electronic Design 
Studio, MIT Press, Cambridge pp. 31­47 Knight, T, 1980, The generation of Hepplewhite­style chair back 
designs, Environment and Planning B: Planning andDesign 7 pp. 227­238 Knight, T, 1986, Transformation 
of the Meander Motif on Greek Geometric Pottery Design Computing 1 pp. 29­67 Knight, T, 1989, Transformations 
of De Stijl art: the paintings of Georges Vantongerloo and Fritz Glarner Environment and Planning B: 
Planning and Design 16 pp. 51­98 Koning H, and Eizenberg, J, 1981, The language of the prairie: Frank 
Lloyd Wright's prairie houses, Environment andPlanning B: Planning and Design 8 pp. 295­323 Kotsopoulos, 
S D, Constructing Design Concepts: A computational approach to the synthesis of architectural form, Doctorate 
Dissertation, Massachusetts Institute of Technology. McGill, M C, 2002, Shaper2D: visual software for 
learning shape grammars, in: K. Koszewski, S. Wrona (Eds.), Design e­ducation: Connecting the Real and 
the Virtual, Proceedings of the 20th Conference on Education in Computer Aided Architectural Design in 
Europe, eCAADe, Warsaw, pp. 148 151. Prats, M, Earl, C, Garner, S and Jowers, I, 2006, Shape exploration 
of designs in a style: Toward generation of product designs, Artificial Intelligence for Engineering 
Design, Analysis and Manufacturing 20, pp. 201 215. Sass, L, 2007, A Palladian construction grammar­design 
reasoning with shape grammars and rapid prototyping Environment and Planning B: Planning and Design 34 
pp. 87­106 Stiny, G and Mitchell, W J, 1978, The Palladian grammar, Environment and Planning B 5 pp. 
5­18 Stiny, G, 1977, Ice­ray: a note on Chinese lattice designs, Environment and Planning B4 pp. 89­98 
Stiny, G, 1980, Kindergarten grammars: designing with Froebel s building gifts, Environment and Planning 
B 3, pp. 409­462 Tapia, M, 1999, A visual implementation of a shape grammar system, Environment and Planning. 
B, Planning and Design 26 (1), pp. 59 73. Wang, Y, Duarte, J P, 2002, Automatic generation and fabrication 
of designs, Automation in Construction 11 (3), pp. 291 302. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1667262</article_id>
		<sort_key>230</sort_key>
		<display_label>Article No.</display_label>
		<display_no>23</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>23</seq_no>
		<title><![CDATA[Visual algorithms for post production]]></title>
		<page_from>1</page_from>
		<page_to>52</page_to>
		<doi_number>10.1145/1667239.1667262</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1667262</url>
		<abstract>
			<par><![CDATA[<p>The work of the visual algorithms community (for example the work of SIGGRAPH Technical Papers authors) frequently affects real-world film post production. But often academics in the relevant fields have little idea of the tools and algorithms actually involved in day-to-day post production. This course surveys a range of typical tools and algorithmic techniques currently used in post production and shows how some emerging technologies may change these techniques in the future. The course attempts to demystify some of the processes and jargon involved, both to enlighten an academic audience and inspire new contributions to the industry.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor>Modeling methodologies</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010342.10010343</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis->Modeling methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010383</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Image processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1797171</person_id>
				<author_profile_id><![CDATA[81448593099]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Simon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Robinson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Foundry, Soho, London, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797172</person_id>
				<author_profile_id><![CDATA[81100510248]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Anil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kokaram]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sigmedia Research Group, Trinity College, Ireland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797173</person_id>
				<author_profile_id><![CDATA[81458653142]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seymour]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[fxphd and fxguide]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[A. Rares, M. J. T. Reinders, J. B. Complex event classification in degraded image sequences. In <i>Proceedings of ICIP 2001 (IEEE), ISBN 0-7803-6727-8</i> (Thessaloniki, Greece, October 2001).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[A. Rares, M. J. T. Reinders, J. B. Statistical analysis of pathological motion areas. In <i>The 2001 IEE Seminar on Digital Restoration of Film and Video Archives</i> (London, UK, January 2001).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[A. Rares, M. J. T. Reinders, J. B. Image sequence restoration in the presence of pathological motion and severe artifacts. In <i>Proceedings of ICASSP 2002 (IEEE)</i> (Orlando, Florida, USA, May 2002).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344972</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Bertalmio, M., Sapiro, G., Caselles, V., and Ballester, C. Image inpainting. In <i>SIGGRAPH '00: Proceedings of the 27th annual conference on Computer graphics and interactive techniques</i> (New York, NY, USA, 2000), pp. 417--424.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>229157</ref_obj_id>
				<ref_obj_pid>229144</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Black, M., and Anandan, P. The robust estimation of multiple motions: Parametric and piecewise-smooth flow fields. <i>Computer Vision and Image Understanding 63</i> (January 1996), 75--104.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Bornard, R. <i>Probabilistic approaches for the digital restoration of television archives.</i> PhD Thesis, Ecole Centrale, Paris, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Buisson, O. <i>Analyse de s&#233;quences d'images haute r&#233;solution, application &#224; la restauration num&#233;rique de films cin&#233;matographiques.</i> PhD thesis, Universit&#233; de La Rochelle, France, December 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>794358</ref_obj_id>
				<ref_obj_pid>794189</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Buisson, O., Besserer, B., Boukir, S., and Helt, F. Deterioration detection for digital film restoration. In <i>IEEE International Conference on Computer Vision and Pettern Recognition</i> (June 1997), vol. 1, IEEE, pp. 78--84.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566572</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Chuang, Y.-Y., Agarwala, A., Curless, B., Salesin, D. H., and Szeliski, R. Video matting of complex scenes. In <i>Proceedings of ACM SIGGRAPH</i> (2002).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Chuang, Y.-Y., Curless, B., Salesin, D. H., and Szeliski, R. A bayesian approach to digital matting. In <i>Proceedings of CVPR</i> (2001).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1462921</ref_obj_id>
				<ref_obj_pid>1376536</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Corrigan, D., Harte, N., and Kokaram, A. Pathological Motion Detection for Robust Missing Data Treatment. <i>EURASIP Journal on Advances in Signal Processing</i> (2008).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2319881</ref_obj_id>
				<ref_obj_pid>2318982</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Dufaux, F., and Konrad, J. Efficient, robust and fast global motion estimation for video coding. <i>IEEE Transactions on Image Processing 9</i> (2000), 497--501.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>851569</ref_obj_id>
				<ref_obj_pid>850924</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Efros, A. A., and Leung, T. K. Texture synthesis by non-parametric sampling. In <i>Proceedings of the IEEE International Conference on Computer Vision (ICCV)</i> (September 1999), vol. 2, pp. 1033--1038.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Ferrandi&#232;re, E. D. <i>Motion picture restoration using morphological tools.</i> Kluwer Academic Publishers, May 199, pp. 361--368.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Ferrandi&#232;re, E. D. <i>Restauration automatique de films anciens.</i> PhD thesis, Ecole des Mines de Paris, France, December 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Ferrandi&#232;re, E. D. <i>Mathematical morphology and motion picture restoration.</i> John Wiley and Sons, New York, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Ferrandi&#232;re, E. D., and Serra, J. Detection of local defects in old motion pictures. In <i>VII National Symposium on Pattern Recognition and Image Analysis</i> (April 1997), pp. 145--150.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Haan, G. D., and Bellers, E. Deinterlacing-an overview. In <i>Proceedings of the IEEE</i> (Sept 1998), vol. 86, no. 9, pp. 1839--1857.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Hill, L., and Vlachos, T. On the estimation of of global motion using phase correlation for broadcasting applications. In <i>Seventh International Conference on Image Processing and Its Applications</i> (July 1999), vol. 2, pp. 721--725.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Hill, L., and Vlachos, T. Global and local motion estimation using higher-order search. In <i>5th Meeting on Image Recognition and Understanding (MIRU 2000)</i> (July 2000), vol. 1, pp. 18--21.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Joyeux, L., Boukir, S., Besserer, B., and Buisson, O. Reconstruction of degraded image sequences. application to film restoration. <i>Image and Vision Computing</i>, 19 (2001), 503--516.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Kent, B., Kokaram, A., Collis, B., and Robinson, S. Two layer segmentation for handling pathological motion in degraded post production media. In <i>IEEE International Conference on Image Processing</i> (October 2004), pp. 299--302.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2276429</ref_obj_id>
				<ref_obj_pid>2275619</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Ko, S.-J., Lee, S.-H., Jeon, S.-W., and Kang, E.-S. Fast digital image stabilizer based on gray-coded bit-plane matching. <i>IEEE Transactions on Consumer Electronics 45</i>, 3 (Aug. 1999), 598--603.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2276173</ref_obj_id>
				<ref_obj_pid>2275614</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Ko, S.-J., Lee, S.-H., and Lee, K.-H. Digital image stabilizing algorithms based on bitplane matching. <i>IEEE Transactions on Consumer Electronics 44</i>, 3 (Aug. 1998), 617--622.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2320535</ref_obj_id>
				<ref_obj_pid>2319030</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Kokaram, A. On missing data treatment for degraded video and film archives: a survey and a new bayesian approach. <i>IEEE Transactions on Image Processing</i> (March 2004), 397--415.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2322108</ref_obj_id>
				<ref_obj_pid>2319116</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Kokaram, A., Morris, R., Fitzgerald, W., and Rayner, P. Detection of missing data in image sequences. <i>IEEE Image Processing</i> (November 1995), 1496--1508.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>687032</ref_obj_id>
				<ref_obj_pid>646276</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Kokaram, A. C. Reconstruction of severely degraded image sequence. In <i>Image Analysis and Processing</i> (September 1997), vol. 2, Springer--Verlag, pp. 773--780.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>522072</ref_obj_id>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Kokaram, A. C. <i>Motion Picture Restoration: Digital Algorithms for Artefact Suppression in Degraded Motion Picture Film and Video.</i> Springer Verlag, ISBN 3-540-76040-7, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2320535</ref_obj_id>
				<ref_obj_pid>2319030</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Kokaram, A. C. On missing data treatment for degraded video and film archives: a survey and a new bayesian approach. <i>IEEE Transactions on Image Processing 13</i> (March 2004), 397--415.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>138801</ref_obj_id>
				<ref_obj_pid>138791</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Konrad, J., and Dubois, E. Bayesian estimation of motion vector fields. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence 14</i>, 9 (September 1992).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1340234</ref_obj_id>
				<ref_obj_pid>1340087</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Levin, A., Lischinski, D., and Weiss, Y. A closed-form solution to natural image matting. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence 30</i>, 2 (2008), 228--242.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Manhall, S., and Harvey, N. Film and video archive restoration using mathematical morphology. In <i>IEE Seminar on Digital Restoration of Film and Video Archives (Ref. No. 2001/049)</i> (January 2001), pp. 9/1--9/5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2320001</ref_obj_id>
				<ref_obj_pid>2318989</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Mansouri, A., and Konrad, J. Bayesian winner-take-all reconstruction of intermediate views from stereoscopic images. <i>IEEE Image Processing 9</i>, 10 (October 2000), 1710--1722.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Nadenau, M. J., and Mitra, S. K. Blotch and scratch detection in image sequences based on rank ordered differences. In <i>5th International Workshop on Time-Varying Image Processing and Moving Object Recognition</i> (September 1996).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Odobez, J.-M., and Bouth&#233;my, P. Robust multiresolution estimation of parametric motion models. <i>Journal of visual communication and image representation 6</i> (1995), 348--365.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[O. J. Woodford, and and A. W. Fitzgibbon, I. R. Efficient new-view synthesis using pairwise dictionary priors. In <i>IEEE International Conference on Computer Vision and Pattern Recognition</i> (June 2007), pp. 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Paisan, F., and Crise, A. Restoration of signals degraded by impulsive noise by means of a low distortion, non--linear filter. <i>Signal Processing 6</i> (1984), 67--76.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1266121</ref_obj_id>
				<ref_obj_pid>1265986</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Piti, F., Kokaram, A., and Dahyot, R. Automated colour grading using colour distribution transfer. <i>Journal of Computer Vision and Image Understanding</i> (February 2007).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808606</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Porter, T., and Duff, T. Compositing digital images. In <i>Proceedings of ACM SIGGRAPH</i> (1984), vol. 18, pp. 253--259.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Prez, P., Blake, A., and Gangnet, M. Jetstream: Probabilistic contour extraction with particles. In <i>ICCV 2001, International Conference on Computer Vision</i> (July 2001), vol. II, pp. 524--531.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[Qi, W., and Zhong, Y. New robust global motion estimation approach used in mpeg-4. <i>Journal of Tsinghua University Science and Technology</i> (2001).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[Ratakonda, K. Real-time digital video stabilization for multimedia applications. In <i>Proceedings International Symposium on Circuits and Systems</i> (Monterey, CA, USA, May 1998), vol. 4, IEEE, pp. 69--72.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[Read, P., and Meyer, M.-P. <i>Restoration of Motion Picture Film.</i> Butterworth Heinemann, ISBN 0-7506-2793-X, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2323022</ref_obj_id>
				<ref_obj_pid>2322503</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[Roosmalen, P. M. B. V., Lagendijk, R. L., and Biemond, J. Correction of intensity flicker in old film sequences. <i>Submitted to: IEEE Transactions on Circuits and Systems for Video Technology</i> (December 1996).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[Roosmalen, P. M. B. V., Lagendijk, R. L., and Biemond, J. Flicker reduction in old film sequences. In <i>Time-varying Image Processing and Moving Object Recognition 4</i> (1997), Elsevier Science, pp. 9--17.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[S. Armstrong, P. J. W. R., and Kokaram, A. C. Restoring video images taken from scratched 2-inch tape. In <i>Workshop on Non-Linear Model Based Image Analysis, NMBIA'98; Eds: Stephen Marshall, Neal Harvey and Druti Shah</i> (September 1998), Springer Verlag, pp. 83--88.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[Sadhar, S., and Rajagopalan, A. N. Image estimation in film-grain noise. <i>IEEE Signal Processing Letters 12</i> (March 2005), 238--241.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>877231</ref_obj_id>
				<ref_obj_pid>876865</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[Saito, T., Komatsu, T., Ohuchi, T., and Seto, T. Image processing for restoration of heavily-corrupted old film sequences. In <i>International Conference on Pattern Recognition 2000</i> (2000), pp. Vol III: 17--20.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>598475</ref_obj_id>
				<ref_obj_pid>598429</ref_obj_pid>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[Scharstein, D., and Szeliski, R. A taxonomy and evaluation of dense two-frame stereo correspondence algorithms. <i>International Journal of Computer Vision 47</i> (April 2002), 7--42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[Sidorov, D., and Kokaram, A. Suppression of moir&#233; patterns via spectral analysis. In <i>SPIE Conference on Visual Communications and Image Processing</i> (January 2002), vol. 4671, pp. 895--906.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[Sidorov, D. N., and Kokaram, A. C. Removing moir from degraded video archives. In <i>XIth European Conference in Signal Processing (EUSIPCO 2002)</i> (September 2002).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[Smolic, A., and Ohm, J.-R. Robust global motion estimation using a simplified m-estimator approach. In <i>IEEE International Conference on Image Processing</i> (Vancouver, Canada, September 2000).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[Stiller, C. Motion--estimation for coding of moving video at 8kbit/sec with gibbs modelled vectorfield smoothing. In <i>SPIE VCIP.</i> (1990), vol. 1360, pp. 468--476.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[Storey, R. Electronic detection and concealment of film dirt. <i>UK Patent Specification No. 2139039</i> (1984).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[Storey, R. Electronic detection and concealment of film dirt. <i>SMPTE Journal</i> (June 1985), 642--647.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015721</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[Sun, J., Jia, J., Tang, C.-K., and Shum, H.-Y. Poisson matting. <i>ACM Transactions on Graphics 23</i>, 3 (2004).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[Tenze, L., Ramponi, G., and Carrato, S. Blotches correction and contrast enhancement for old film pictures. In <i>IEEE International Conference on Image Processing</i> (2000), p. TP06.05.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[Tenze, L., Ramponi, G., and Carrato, S. Robust detection and correction of blotches in old films using spatio-temporal information. In <i>Proceedings of SPIE International Symposium of Electronic Imaging 2002</i> (January 2002).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[Tucker, J., and de Sam Lazaro, A. Image stabilization for a camera on a moving platform. In <i>Proc. of the IEEE Pacific Rim Conf. on Communications, Computers and Signal Processing</i> (May 1993), vol. 2, pp. 734--737.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[Uomori, K., Morimura, A., Ishii, H., Sakaguchi, T., and Kitamura, Y. Automatic image stabilizing system by full-digital signal processing. <i>IEEE Transactions on Consumer Electronics 36</i>, 3 (Aug. 1990), 510--519.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[Vlachos, T. Simple method for estimation of global motion parameters using sparse translational motion vector fields. <i>Electronics Letters 34</i>, 1 (January 1998), 60--62.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[Vlachos, T., and Thomas, G. A. Motion estimation for the correction of twin-lens telecine flicker. In <i>IEEE International Conference on Image Processing</i> (September 1996), vol. 1, pp. 109--112.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[White, P., Collis, B., Robinson, S., and Kokaram, A. Inference matting. In <i>IEE European Conference on Visual Media Production</i> (November 2005), pp. 161--171.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 VISUAL ALGORITHMS FOR POST PRODUCTION Notes for the Course at SIGGRAPH 2009 Simon Robinson Anil Kokaram 
Mike Seymour Contents 1 Course Overview 5 1.1 Introduction...................................... 5 
1.2 Schedule........................................ 6 1.3 AShortHistory .................................... 
6 2 Ingest 9 2.1 DefectsinPictures .................................. 10 2.1.1 ModernFootage ............................... 
11 2.2 DustBustingAlgorithms ............................... 13 2.2.1 SDIx ..................................... 
14 2.2.2 ROD ..................................... 14 2.2.3 Morphological/Median Filter Approaches 
. . . . . . . . . . . . . . . . . . 16 2.2.4 MRFbasedapproaches............................ 16 2.2.5 
Reconstruction ................................ 18 2.2.6 Stillworktodo ................................ 
19 2.3 NoiseandGrain.................................... 19 2.4 MotionEstimation .................................. 
20 2.4.1 Practice.................................... 22 2.5 Summary ....................................... 
22 CONTENTS CONTENTS 3 Basics of Compositing 23 3.1 Node-basedCompositing ............................... 
24 3.2 Compositingin3D .................................. 27 3.3 CompositinginStereo-3D .............................. 
27 3.4 Summary ....................................... 28 4 Challenges in Visual Compositing 29 4.1 
Matting ........................................ 29 4.1.1 MakingaPracticalMattePuller ....................... 
31 4.2 Inbetweening ..................................... 32 4.2.1 FrameInterpolation.............................. 
34 4.2.2 Outstandingissues .............................. 35 5 Stereo-3D 37 5.1 ConvergenceandKeystoning 
............................. 37 5.2 Disparitiesshouldbehorizontal ........................... 39 5.3 
DenseDisparityEstimation.............................. 41 5.3.1 Movingthecameras ............................. 
43 5.3.2 Movingcompositingcomponents ...................... 45 5.4 Opticaldifferencesbetweentheeyes 
......................... 45  Chapter 1 Course Overview PRESENTERS Additional material thanks to Lucy 
Wilkes, The Foundry. 1.1 Introduction The modern post production house has evolved to contain a sophisticated 
work.ow in which a large variety of creative and algorithmic tasks are coordinated to achieve a .nished 
product. While the work of SIGGRAPH authors undoubtedly has in.uence on the evolution of the technology 
used in post production, not many are aware of the breadth of algorithms and people that are involved. 
The terminology used and the focus of those in this industry tends to be hard to access by those in academia. 
What is also surprising is the amount of algorithm work that is used for manipulating 2D pictures regardless 
of the requirement for 3D special effects. Tasks such as noise reduction, dust busting , matting, rotoscoping, 
colour correction, retiming, brightness balancing, are all 2D based and have evolved to include a range 
of statistical techniques. This course attempts to demystify part of the world of post production to 
the Siggraph audience. We expose the lesser known bread-and-butter parts of the work.ow and educate the 
audience about the tools that are commonly used. The course opens with an introduction to the world of 
post pro­duction given by Mike Seymour who is well known for his educational outreach. Simon Robinson 
and Anil Kokaram then expose the links between the practice of post production and the algorithms 1.2. 
SCHEDULE CHAPTER 1. COURSE OVERVIEW developed in academia. Prerequisites: We assume some familiarity 
with basic image and video processing ideas (linear .ltering, colour spaces, motion estimation) undergraduate 
mathematics and signal processing. 1.2 Schedule 8:30 am Introduction [Mike Seymour] 9:15 am Ingest [Anil 
Kokaram] 9:30 am break 9:40 am Basic Compositing [Simon Robinson] 10:15 am Challenges in 2D Compositing 
[Anil Kokaram] 10:50 am break 11:00 am 3D Stereoscopic post production [Simon Robinson] 11:25 am The 
Future Of Compositing [Simon Robinson] 12:00 Summary and wrap up 12:15 Close  1.3 A Short History In 
.lm production of the early 1980s, .lm effects were predominantly optical post-processes or clever live 
action. Computer graphics was a niche topic, seeing its .rst mainstream uses in .lms like Star Trek: 
The Wrath Of Khan. The early days of such effect work was dominated by 3D effects, possibly because the 
techniques enabled the creation of unique effects beyond what was possible by other means. The Industrial 
Light and Magic team behind the .lm effects later formed the core of Pixar, whose subsequent animation 
features are well known to children worldwide. 2D image processing lacked the wow factor of 3D, and without 
the compute power to apply such techniques to movies wholesale this remained an underdeveloped area. 
The early 1980s was also dominated by custom software. There was very little commercial exploitation 
in computer-generated graphics until the emergence of companies such as Softimage, Wavefront and Alias 
Research in the late 1980s, with their 3D modelling and animation packages. A company called Avid showed 
the .rst non-linear editing system in a private suite at the NAB conference in April 1988. The non-linear 
editing system allowed the image clips comprising a .lm to be digitally edited together using a computer 
interface. The resulting edit decisions (the Edit Decision List, EDL) was then used to produces the cuts 
and dissolves on the .lm using automated equipment. This was the start of a big industry change -in 1994 
three .lms used the new digital CHAPTER 1. COURSE OVERVIEW 1.3. A SHORT HISTORY editing system; today 
almost all do, and today the EDL output is almost entirely used to form a digital master. This latter 
development, where output to .lm is a .nal process only (and optional in the case of digital projection) 
has led today to the rise in Digital Intermediate systems, which combine the non-linear aspects of these 
early systems with real-time playback, as well as effects and grading capabilities. Initial systems from 
Avid involved little 2D effects work beyond the wipes, cuts and dissolves necessary to edit a sequence 
of clips. But also in the late 1980s, a UK company called Quantel began producing the .rst commercial 
compositing system/non-linear editor, following on from success they had enjoyed earlier in the decade 
with the Paintbox system -a tv graphics system. The new system was called Harry , and relied upon custom 
hardware to render special effects to a hardware disk array. Unlike Avid s long-form systems, the Harry 
system could only hold 80 seconds of uncompressed video; but the combination of effects, editing and 
playback made it a unique and successful tool for the commercials market. Quantel s market dominance 
was challenged by a newcomer -Discreet Logic from Montreal. Discreet Logic started business marketing 
an early node-based compositing system called Eddie , primarily developed by Bruno Nicoletti at a pioneering 
Australian effects house, Animal Logic. While Eddie was an important forerunner of most .lm-based compositing 
systems today, Discreet Logic sold their rights to the product and instead produced their .rst version 
of Flame . Flame was a clip-based compositing system which shared many of the same selling points of 
Quantel s Harry system, but for one important difference: it was software based and depended primarily 
on non-custom unix-based SGI hardware systems. As well as quickly becoming signi.cant in what had been 
thought of as Quantel s untouchable broadcast and commercials market, Discreet Logic were also more successful 
at capturing the growing digital .lm effects market. The .rst year of Flame , 1993, was the beginning 
of the end for Quantel s market leadership, and paved the way for the dominance of .exible software systems 
in the 2D .lm-effects market. Discreet Logic s systems, however, left a gap in the market for cheaper 
effects systems -lower processing power, but arguably more intricate in functionality. Early signi.cant 
systems built upon the node-based architecture pioneered by Eddie . Kodak s Cineon system and Avid s 
Media-Illusion (a follow-on from Parallax s Advance system) targeted some of the same market at Discreet 
Logic, but also showed appeal in the broadening base of .lm digital effects work. A company called Nothing 
Real launched the node-based Shake system in 1997 which eventually became (and still remains) the major 
single-shot .lm compositing tool worldwide. Over the growth of software-based systems from the early 
nineties to today, more and more algorithmic developments have been incorporated into 2D effects systems. 
It s signi.cant to note that what seems today like obvious developments have their history. Point tracking 
of a single location, for example, in order to allow the automated attachment of objects to scene elements, 
.rst appeared in Discreet Logic and Quantel systems around 1994 and it was seen as ground-breaking 1.3. 
A SHORT HISTORY CHAPTER 1. COURSE OVERVIEW at the time. Today, all compositing systems rely upon multi-point 
trackers and frequently on 3d-tracking systems. Digital Domain s TRACK technology for camera position 
calculation was .rst used in production in 1993. In 1998, Dr. Douglas R. Roble won the technical Achievement 
Award for his contribution to tracking technology. Today numerous commercial products exist, such as 
Boujou, PFTrack and 3D Equalizer, and their role extends both into 3D animation and into the 2D and 3D 
environments typical of modern compositing. Motion-estimation has also become ubiquitous, particularly 
for retiming image sequences. An early commercial pioneer was found in the Cinespeed component of the 
Cineon compositing system, released in 1993. The .lm What Dreams May Come , released in 1997, made the 
.rst important use of optical .ow for visual effects, outside conventional retiming. Here it was used 
to transfer motion captured from live action into synthetic imagery, using optical .ow techniques from 
Pierre Jasmin and Peter Litwinowic. Pierre and Peter later formed the RevisionFX company, now well known 
for its Twixtor retimer. The 1999 .lm, The Matrix , made famous the bullet-time technique -based upon 
inter­polating pixel motion between multiple cameras rather than multiple sequential frames of the same 
camera. This work was done in conjunction with Dr Bill Collis, then of Snell and Wilcox, now CEO of The 
Foundry, by some of the same team who had worked on the What Dreams May Come se­quences. Later at The 
Foundry, Bill Collis was one of the team awarded an Academy Award for the Furnace plug-in toolset. Furnace 
is a complex compositor s toolkit built around motion-estimation technology, and uses motion analysis 
for numerous tasks in image cleanup, object removal and retiming. Subsequently, in sequels to The Matrix 
, optical .ow photogrammetry was also used to allow markerless facial capture. Today there is signi.cant 
research effort into extending these techniques into more general multi-camera frameworks. The thrust 
of the research and commercial effort is to allow a new, richer extraction of scene information, with 
a view to improving the functionality and creativity of future compositing systems, as well as helping 
to bridge the gaps between 2D compositing and 3D scene information. In what follows, we consider the 
principal aspects of the work.ow and show how technology developed by the video and image processing 
research community is adapted for use in cinema production. Chapter 2  Ingest When pictures .rst enter 
the post production there are a number of processes applied to them as a matter of course. These actions 
are generally known collectively as Ingest. The data has to be converted into the right .le formats and 
also checked for quality. Typically every post house has some proprietary data network and database tools 
for handling data while it is being manipulated. Despite the prevalence of digital cameras, analog .lm 
is still used today. Some houses therefore have their own scanning facility although separate .lm scanning 
services do exist. We do not consider the data handling and management issues here, but needless to say, 
the association of descriptive metadata to every picture record has become a key enabler. The ingestion 
stage therefore involves a number of well established pre-processing tasks in the pipeline. 1. Film Scanning 
/ Data Collection 2. Metadata input 3. Image conditioning : Dust Busting, Noise reduction 4. A possible 
analysis step for the collection of automated metadata  After scanning, the images are viewed and decisions 
are made about how they should be treated. Shots are catalogued and marked up, and then treatments are 
scheduled. Shots and clips are re­viewed for quality and some marked for early stage treatment in Ingest. 
As picture processing technologies mature and become more automated, some tools that would previously 
be considered to be part of the creative/interactive element in the post production process, have moved 
into the Ingest stage. Increasingly Noise/Grain reduction and Dust-busting have therefore become standard 
tools. Line scratches often still occur in modern .lm and their automated removal remains dif.cult. One 
well used process in post production is dust busting. This is a classic bread and butter task required 
for removing the small corrupted pixels caused by dust stuck to the .lm, or in the case of 2.1. DEFECTS 
IN PICTURES CHAPTER 2. INGEST old .lm, data loss caused by tears or holes. In modern digital footage, 
data loss caused by inactive pixels, or pixels locked in a certain state, does occur. Noise in modern 
footage occurs quite often because of the low exposure that Directors of Photography sometimes use. Many 
Dust busting algorithms are actually derived from a well founded set of algorithms derived from the early 
work of [55, 28]. It is a suf.ciently interesting problem for it to be worth taking a look at some of 
the ideas behind a successful algorithm. This chapter .rst reviews some of the defects that can occur 
on .lm and video and then highlights some aspects of their treatment algorithms. 2.1 Defects in Pictures 
There are a huge range of defects that can be observed in video and .lm. As far as archived material 
is concerned, the BRAVA consortium (brava.ina.fr) during 2000-2002 were the .rst to make some attempt 
to catalogue these and educate the community about their nature. Figures 2.1-2.10 present part of this 
taxonomy of defects in an attempt to educate the SIGGRAPH readership as to their names as they are used 
in the archive industry. Missing data problems manifest as Fig­ures 2.1, 2.2, 2.4, 2.6. Massive loss 
of data can also occur as in Figures 2.3 and 2.7. These are better treated through temporal frame interpolation 
and the reader may see [28, 27] for some treatment of this issue. A possible solution for Kinescope Moire, 
as illustrated in Figure 2.5, can be found in [50, 51]. Two inch scratches (caused by scratching of old 
Two Inch video tape) are an example of a specialised missing data problem and a treatment can be found 
in [46, 32]. Two major defects are missing from the visual description: Shake and Flicker. Those are 
best viewed as video clips. Shake simply refers to unwanted global motion of the picture caused either 
by camera movement or problems during scanning. Algorithms for removing shake abound [23, 60, 59, 24, 
42, 61]. This is principally because it is related to the global motion estimation problem that is also 
important for video compression issues [35, 19, 20, 12, 52, 41]. Flicker manifests itself as a .uctuation 
in picture brightness from frame to frame. In stills the effect is very dif.cult to observe indeed, but 
as a sequence the effect is often very disturbing. Two different types of degradations result in a perceptible 
.icker artefact. The .rst realistic de­.icker algorithm was developed by P. V. M. Roosmalen [45, 44] 
and a real time hardware version was developed by Snell and Wilcox in the late 1990 s. Both changing 
.lm exposure (in old silent movies for instance) and varying lighting conditions result in luminance 
.uctuations. However, a misalignment in the two optical paths in a telecine machine also yields the same 
visible artefact, called Twin Lens Flicker. In that case, the two .elds of each interlaced TV frame are 
incorrectly aligned with respect to the original .lm frame, and the observed .uctuations are due more 
to the shake between the .elds than any real luminance changes. Vlachos et al considered this problem 
in [62] and a real time implementation was also developed by Snell &#38; Wilcox in the late 1990 s. CHAPTER 
2. INGEST 2.1. DEFECTS IN PICTURES Figure 2.1: Dirt and Sparkle oc- Figure 2.2: Film Grain Noise Figure 
2.3: Betacam Dropout curs when material adheres to is a common effect and is due manifests itself due 
to errors on the .lm due to electrostatic ef­ to the mechanism for the cre- Betacam tape. It is a missing 
fects (for instance) and when ation of images on .lm. It data effect and several .eld lines the .lm is 
abraded as it passes manifests itselg slightly differ­ are repeated for a portion of the through the 
transport mecha­ ently depending on the different frame. The repeating .eld lines nism. It is also referred 
to as .lm stocks. The image shows are the machine s mechanism for a Blotch in the literature. The clearly 
the textured visible effect interpolating the missing data. visual effect is that of bright of noise 
in the blue sky at the and dark .ashes at localised in­ top left. Blotches and noise typ­ stances in 
the frame. The image ically occur together and are the indicates where a piece of Dirt is main form of 
degradation found visible. on archived .lm and video. A piece of Dirt is indicated on the image. Video 
clips showing serious degradation by shake, .icker, lines, grain and blotches can be seen at www.sigmedia.tv/Research/DigitalFilmRestoration. 
The book by Read and Meyer [43] gives an excellent coverage of the physical nature of archive material, 
and the practices in the .lm preservation industry. 2.1.1 Modern Footage As a matter of course, .lm scanners 
result in images which are degraded by very slight amounts of dust or holes giving dark or white pixels. 
Figure 2.11 shows a typical example of a scanned 2k .lm frame at 2048 × 1556 showing mild degradation 
which must be dealt with. Increasingly we are seeing noise in digital footage because of low exposure 
in cameras such as the RED. Finally, as digital .lm material is compressed, and as producers mix high 
and medium quality media, the post production house has to deal with blocking artefacts and mosquito 
noise (decom­ 2.1. DEFECTS IN PICTURES CHAPTER 2. INGEST Figure 2.4: Digital Drop Out occurs because 
of errors on dig­ital video tape. This example is drop out from D1 tape. Figure 2.7: Vinegar Syndrome 
often results in a catastrophic breakdown of the .lm emulsion. This example shows long strands of missing 
data over the frame. Figure 2.5: Kinescope Moire is caused by aliasing during Telecine conversion and 
mani­fests itself as rings of degra­dation that move slightly from frame to frame. Figure 2.8: Echoes 
and Over­shoots manifest as image shad­ows slightly displaced from each object in the frame. When the 
effect is severe it is called Echo and when it is just limited to edges it is called Overshoot, as in 
this case. Figure 2.6: Film Tear is sim­ply the physical tearing of a .lm frame, sometimes due to a dirty 
splice nearby. Figure 2.9: Colour Fading im­plies that the picture colour is not saturated enough, giving 
the image a washed out look. persist in the same location from frame to frame. Figure 2.11: Showing 
three frames from a scanned .lm sequence at 2K resolution. Dust manifests itself in the central frame 
as bright spots. In modern .lm, the blotches are quite small. pression artefacts near sharp edges) in 
certain shots.  2.2 Dust Busting Algorithms The act of repairing the missing data in the .lm frames 
has come to be known as Dust Busting. In early academic literature all missing data defects were referrred 
to as blotches. Almost all automated dust busting tools rely on the idea that dust does not exist in 
the same location in consecutive frames and so inter-frame differencing along motion trajectories leads 
to a reasonable detector. The most successful approaches have been Detect-then-interpolate approaches. 
A low cost detector is used to detect blotches and then a hole-.lling algorithm is used for the reconstruction 
or inpainting step. The earliest work on designing an automatic system to electronically detect Dirt 
and Sparkle 2.2. DUST BUSTING ALGORITHMS CHAPTER 2. INGEST was undertaken by Richard Storey at the BBC 
[55, 54] as early as 1983. The design was incor­porated directly into hardware which was subsequently 
used in-house for video restoration before broadcast. The idea was to .ag a pixel as missing if the forward 
and backward pixel difference was high. This idea was of course beset with problems in parts of the image 
where motion occurred. The natural extension of this idea was presented by Kokaram around 1993 [28, 26] 
which allowed for motion compensated differences. That type of detector was called a Spike Detection 
Index (SDI) and the most useful are de.ned as follows. 2.2.1 SDIx The forward and backward motion compensated 
pixel differences Ef ,Eb of the observed, corrupted image sequence Gn(x), are de.ned as follows. Eb = 
Gn(x) - In-1(x + dn,n-1(x)) Ef = Gn(x) - In+1(x + dn,n+1(x)) (2.1) Note that the previous and next frames 
are assumed to be uncorrupted at the required motion com­pensated sites, hence Gn-1 = In-1 etc. Two detectors 
can then be proposed [28] as follows. .1 for (|Eb| >Et) AND (|Ef | >Et) bSDIa(x)= (2.2)0 otherwise .1 
for (|Eb| >Et) AND (|Ef | >Et) . . bSDIp(x)= AND sign(Ef ) = sign(Eb) (2.3) . 0 otherwise . Here, b(·) 
is a detection .eld variable set to 1 at sites that are corrupted by missing data. Et is a user de.ned 
threshold for detection of discontinuity. The SDIa is based on thresholding Ef ,Eb only. SDIp additionally 
applies the constraint that if corruption does not occur in identical locations in consecutive frames 
and the brightness constancy assumption holds, In-1 In+1, one should expect that the sign of the difference 
signals should be the same. It is now accepted that SDIp is the better detector in almost all situations 
because of this additional constraint.  2.2.2 ROD In 1996, Nadenau and Mitra [34] presented another 
scheme which used a spatio-temporal window for inference: the Rank Order Detector (ROD). It is generally 
more robust to motion estimation errors than any of the SDI detectors although it requires the setting 
of three thresholds. It uses some CHAPTER 2. INGEST 2.2. DUST BUSTING ALGORITHMS spatial information 
in making its decision. The essence of the detector is the premise that blotched pixels are outliers 
in the local distribution of intensity. De.ning a list of pixels as p1 = In-1(x + dn,n-1(x) + [0 0]) 
p2 = In-1(x + dn,n-1(x) + [0 1]) p3 = In-1(x + dn,n-1(x) + [0 - 1]) p4 = In+1(x + dn,n+1(x) + [0 0]) 
p5 = In+1(x + dn,n+1(x) + [0 1]) p6 = In+1(x + dn,n+1(x) + [0 - 1]) Ic = In(x) (2.4) where Ic is the 
pixel to be tested, the algorithm may be enumerated as follows. 1. Sort p1 to p6 into the list [r1,r2,r3,...,r6] 
where r1 is minimum. The median of these pixels is then calculated as M =(r3 + r4)/2. 2. Three motion 
compensated difference values are calculated as follows: If Ic >M  e1 = Ic - r6 e2 = Ic - r5 e3 = Ic 
- r4 If Ic = M e1 = r1 - Ic e2 = r2 - Ic e3 = r3 - Ic 3. Three thresholds are selected: t1,t2,t3. If 
any of the differences exceeds these thresholds, then a blotch is .agged as follows 1 if (e1 >t1) OR 
(e2 >t2) OR (e3 >t3) bROD(x)= 0 otherwise where t3 = t2 = t1. The choice of t1 is the most important. 
The detector works by measuring the outlierness of the current pixel when compared to a set of others 
chosen from other frames. The choice of the shape of the region from which the other pixels were chosen 
is arbitrary. 2.2. DUST BUSTING ALGORITHMS CHAPTER 2. INGEST 2.2.3 Morphological/Median Filter Approaches 
In the 1D case, Paisan and Crise [37] were the .rst to spot that one could use a median .ltered signal 
as a rough estimate of the original signal before corruption by impulsive noise. The difference between 
the observed, degraded signal and this rough estimate would be high at sites corrupted by impulsive defects. 
This is because the rank order .lter removes all outliers, but leaves lower scale trends untouched. This 
idea can be extended to treat small missing data artefacts in archived video and .lm, known as Dust. 
These are generally just a few pixels in area (3 × 3 pixels), and hence only a small median or morphological 
window need be used. Using a larger window to detect larger artefacts causes problems since more true 
image detail would then be removed causing an increased number of false alarms. Joyeux, Buisson, Decenciere, 
Harvey, Tenze, Saito, Boukir et al have been implementing these types of techniques for .lm restoration 
since the mid-1990 s [21, 14, 17, 16, 7, 15, 8, 57, 58, 48]. Joyeux [21] points out that these techniques 
are particularly attractive because of their low computational cost. They perform well when the artefact 
is small, and surrounded by a relatively low-activity homogenous region. The high resolution of .lm scans 
is therefore suitable for these tools. 2.2.4 MRF based approaches Generally speaking a good detector 
ought to incorporate both spatial and temporal information. The most natural way of doing this is to 
incorporate impose spatial smoothness on the blotch detector by modelling the blotch as a Markov Random 
Field. A set of MRF based detectors were developed by Kokaram et al between 1990 and 2004[28, 25]. The 
idea is ultimately to pose the problem as the optimal estimation of a detection .eld that is 1 at the 
sites of degradation and 0 otherwise. The model for degradation is a Replacement process with additive 
noise. A binary .eld b(x) is introduced that is 1 at a site of missing data and zero, otherwise. The 
degradation model can then be written as follows. Gn(x) = (1 - b(x))In(x)+ b(x)c(x)+ µ(x) (2.5) Where 
µ(·) ~N (0,sµ2) is the additive noise, and c(x) is a .eld of random variables that cause the corruption 
at sites where b(x)=1. Degradation information can be included in the pixel site information by de.ning 
a pixel as occupying one of six states. Each of these states s(x) . [S1 ...S6] is de.ned as a combination 
of 3 binary variables [b(x),Ob(x),Of (x)] as follows. 001 The pixel is not missing and there is occlusion 
in the forward direction only. 010 The pixel is not missing and there is occlusion in the backward direction 
only. CHAPTER 2. INGEST 2.2. DUST BUSTING ALGORITHMS 000 The pixel is not missing and there is no occlusion 
backward or forward. 100 The pixel is corrupted by a Blotch and there is no occlusion backward or forward. 
101 The pixel is corrupted by a Blotch and there is occlusion in the forward direction only. 110 The 
pixel is corrupted by a Blotch and there is occlusion in the backward direction only. Note that in this 
framework the [1 1 1] state is not allowed since it would imply that the data is missing and yet there 
is no temporal information for reconstruction. This is an interesting practical omission. A Bayesian 
Framework From the degradation model of (2.5) the principal unknown quantities in frame n are In(x), 
s(x), c(x), the motion dn,n-1 and the model error se 2(x). These variables are lumped together into a 
single vector .(x) at each pixel site x. The Bayesian approach presented here infers these unknowns conditional 
upon the corrupted data intensities from the current and surrounding frames Gn-1(x), Gn(x) and Gn+1(x). 
For the purposes of missing data treatment, it is assumed that corruption does not occur at the same 
location in consecutive frames, thus in effect Gn-1 = In-1,Gn+1 = In+1. Proceeding in a Bayesian fashion, 
the conditional may be written in terms of a product of a likelihood and a prior as follows: p(.|In-1,Gn,In+1) 
. p(Gn|.,In-1,In+1)p(.|In-1,In+1) This posterior may be expanded at the single pixel scale, exploiting 
conditional independence in the model, to yield p(.(x)|Gn(x),In-1,In+1,.(-x)) . p(Gn(x)|.(x),In-1,In+1)p(.(x)|In-1,In+1,.(-x)) 
= p(Gn(x)|In(x),c(x),b(x)) × p(In(x)|se(x)2 , d(x),Ob(x),Of (x),In-1,In+1) × p(b(x)|B)p(c(x)|C)p(d(x)|D)p(se(x)2) 
p(Ob(x)|Ob)p(Of (x)|Of ) (2.6) where .(-x) denotes the collection of . values in frame n with .(x) omitted 
and B, C, D, Ob , Of and I denote local dependence neighbourhoods around x (in frame n) for variables 
bc, d, Ob, Of and In, respectively. 2.2. DUST BUSTING ALGORITHMS CHAPTER 2. INGEST Figure 2.12: Two 
consecutive images from a sequence showing severe motion blur due to fast motion. Motion vectors are 
superimposed on the second frame (right). This pathological motion completely confuses motion estimators 
leading to erroneous detection of large blotches. Observe in particular how the motion blurred regions 
are behind sharp stationary objects. Needless to say, the .nal algorithm amounts to some energy minimisation, 
trading off interframe image differences with spatial smoothness of indicators and image material. For 
algorithmic details the reader is invited to consult [28]. The optimisation strategy used then has since 
been superceded by much more complete strategies like Belief Propagation and Graph Cuts. Practical constraints 
imposed by the massive sizes of images used in post production, however, imply that it is important to 
derive memory ef.cient techniques for implementing these methods. 2.2.5 Reconstruction Once the missing 
data is detected, the sites have to be .lled in. Early work in this area exploited motion interpolation, 
colour multistage median .lters and even Autoregressive texture interpolation [28]. The notion of image 
.lling though, has since matured into the area known as Inpainting [4], and there are a vast array of 
options that can be used here. However, in post production, the .delity of the interpolated data is extremely 
important. In addition, the large picture sizes imply that memory ef.ciency is paramount. Thus the techniques 
pioneered Efros [13] or Sapiro [4] are to be used with some care. Because of the strong temporal correlation 
in a sequence of pictures, and the success of motion interpolation ideas [25] for preserving image integrity, 
it is generally a variant of those techniques that are used for hole .lling in this problem. CHAPTER 
2. INGEST 2.3. NOISE AND GRAIN  2.2.6 Still work to do Automated techniques for dust busting are commercial 
products today. However they all suffer when the underlying sequence model breaks down. In that situation, 
the motion information be­comes error prone. Large defects remain dif.cult to remove automatically. Good 
reconstruction has been obtained by motion interpolation [29], but the detection of large defects is 
often confused with pathological motion in sequences. Thus fast moving hands, clothing or vehicles are 
always dif.cult for a Dirt Detector to ignore. In addition, the dirt does not completely obscure the 
underlying image, hence a binary detection .eld is ultimately not good enough. What is missing in the 
literature is improved detection in the presence of pathological motion combined with a treatment of 
non-binary occlusion. Figure 2.12 shows the severity of this problem.  2.3 Noise and Grain It is generally 
expected by researchers working on noise reduction algorithms that the requirement for noise reduction 
is to completely remove any noise in the image sequence. This is not so. Flat pictures, even with all 
the detail preserved, look bad. Film grain is essential for the .lm look. While most workers in this 
area concentrate on preserving detail but removing noise, what is required by those in the post production 
industry is in fact noise reduction that reduces the level of grain but leaves some behind. The practice 
today is to remove the grain as much as possible, then add grain back into the picture of the type that 
is required. Compounding the problem is that .lm grain is a multiplicative process, and the amount of 
degradation/grain varies with brightness. There is more grain at high brightness levels than at low brightness 
levels. There is very little published work targeted at .lm grain noise reduction in particular. One 
might expect that most of the existing noise reduction techniques could be adapted to meet these new 
requirements, but reducing grain levels while leaving the correlation structure of the grain untouched 
is a dif.cult issue [47]. After ten years of research and industrial development in .lm restoration, 
sharpening and en­hancement are about to become much more important for broadcast applications. This 
is because of the advent of consumer high de.nition displays and DVD players. However, it is well known 
that high levels of noise mitigate against good sharpening results e.g. superresolution. It remains to 
be seen how these problems can be resolved in degraded .lm material since the noise levels especially 
of .lm grain can be very high, and possess a non-white correlation structure. 2.4. MOTION ESTIMATION 
CHAPTER 2. INGEST  2.4 Motion Estimation It turns out that estimating the optic .ow of every image in 
a .lm sequence is one of the most common tasks in post production. It has become so important that some 
argue that it should be done at ingest and the motion information stored as metadata with each clip. 
Motion Estimation has been studied by the academic community for some time and the principal issues are 
well understood [30, 5]. Those working in Computer Vision tend to refer to the process as Optic Flow 
estimation, while those in the video processing and coding community refer to it as Motion Estimation 
or Local Motion Estimation. This is quite different from the process of object tracking which typically 
requires establishing a bounding box around an object moving through several frames. The principal requirement 
for motion estimation information in post production is that it should be usable for manipulating the 
smallest visible image detail. In addition, processes that exploit motion should not damage the picture 
in any way. These requirements are much more demanding than the use of motion estimation for video coding 
or some of the activities in the academic community. In video coding, erroneous motion vectors cause 
an increase in bit rate but the impact on picture quality is not necessarily visible. In practice, a 
usable motion estimator combines a motion detection process with the actual motion estimation process. 
In locations where motion is detected, some kind of matching process is used to generate motion vectors 
that explain the evolution of the image sequence and at the same time result in a piecewise smooth vector 
.eld. All the different processes for estimating motion rely on an assumption about the evolution of 
pixel data through image sequences. A typical model showing translation motion is as follows. In(x)= 
In-1(x + dn,n-1)+ e(x) (2.7) Here In(x) is the pixel intensity at site x in frame n, e(·) ~N (0,se 2) 
and dn,n-1 is the motion that maps site x in frame n into the corresponding site in the previous frame 
n - 1 along the direction of motion. The motion estimation problem is to estimate d at all pixel sites 
that are at moving object locations. Solution of this expression for motion is a massive optimisation 
problem, requiring the choice of values for d at all sites to minimise some function of the error e(x). 
Many motion estimators can be derived from the idea of choosing a value of d such that it results in 
the minimum DISPLACED FRAME DIFFERENCE (DFD) de.ned as DF D(x)= In(x) - In-1(x + dn,n-1) (2.8) The problem 
is complicated by the fact that the motion variable d is an argument to the image function In-1(·). The 
image function generally cannot be modelled explicitly as a function of posi­tion and this makes access 
to the motion information dif.cult. Note that the motion equation above cannot be solved at a single 
pixel site. There are 2 variables: x and y components of motion, so at least 2 equations are required 
i.e. at least 2 pixel sites. CHAPTER 2. INGEST  Figure 2.13: Left to right: Previous frame, Current 
frame, Motion estimation vectors superimposed on current frame, Next frame. Blue vectors point into the 
past while green vectors point into the future. Pathological motion of the left hand, due to fast motion 
leading to blurring and rapid change of object geometry, causes poor motion .eld estimation. Note how 
the left hand in the current frame has an appearance that is very divergent compared to the same hand 
in the other frames. The vectors here point in the same direction whether mapping to the future or past, 
and this should not be the case. Also note the divergence of the blue vector .eld in this region. A Bayesian 
framework allows the derivation of all the known motion estimation processes. The idea is to choose the 
motion vector d such that it maximises the probability p(d|In-1,In,In+1) which is the probability of 
motion d given the image data in the current and surrounding frames. Useful solutions only result when 
some prior constraints are placed on the motion .eld e.g. spatial and temporal smoothness as well as 
incorporating the notion that some image areas can be occluded and uncovered as the object moves. There 
is a large amount of literature on this [53, 30, 5, 28, 49] and the more modern optimisation strategies 
recently explored by the academic community, e.g. Graph Cuts and Belief Propagation, have yielded much 
better performance. The expression to be manipulated is typically as follows. p(d|In-1,In,In+1) . p(In-1,In,In+1|d)p(d|D) 
(2.9) where important motion .eld constraints are imposed by designing p(d|D) appropriately. In hardware, 
block matching and phase correlation motion estimation are popular. They are both a form of direct solution 
of 2.9 although most ignore the constraints on the motion .eld. In software, gradient-based motion estimation 
is popular. These latter processes linearise the image function about some initial estimate of motion 
and result in iterative update schemes. 2.5. SUMMARY CHAPTER 2. INGEST 2.4.1 Practice Useful motion 
estimation strategies in post production involve multi-resolution estimation with a pyramid of downsampled 
images. This handles fast motion bettter and is more memory and speed ef.cient than a single full resolution 
strategy. Figure 2.13 shows a typical output from a motion estimator described in [28]. There is no agreement 
on the required resolution of motion for very high de.nition scanned images e.g. 2K or 4K plates. While 
at standard de.nition 720 × 576 it is widely agreed that 1/8 pel accuracy is desirable, at 2k or 4k which 
are ×10 larger at least, that accuracy may be irrelevant. Memory and speed savings can be had by using 
integer or 0.5 pel (pixel) accuracy without loss of picture detail. Perhaps the most important issue 
though is how to deal with situations in which motion estimation simply does not apply i.e. pathological 
motion (PM). Self­occluding clothing, fast motion leading to blurry images, translucent objects, re.ections 
in polished surfaces, all cause most motion estimators to fail. Detection of failure is dif.cult since 
the DFD is not necessarily a good indicator. Figure 2.13 shows both good motion vector estimation and 
failure due to PM in the same scene. The motion of the arm in this case is too fast at 25fps for the 
motion estimator to perform well. It is reasonable to suggest that the proper action when detecting failure 
depends on the application of the motion information. Kent et al[22] propose a very simple scheme for 
dealing with PM when motion is used for deblotching. In that scheme, it is assumed that any area undergoing 
camera or global motion will have a low DFD. In other areas obeying other local motion, a MRF dirt detector 
is biased to be more conservative. The local/global mask is generated using motion information thus being 
robust to dirt degradation. This deals well with the mild levels of degradation in most post production 
cases. It causes dirt to be left in small parts of foreground objects that are moving rapidly, but removes 
dirt in the rest of the image that is well matched with previous frames. Bornard et al [6] and Corrigan 
et al[11] propose a discontinuity detection approach that relies on the observation that a failing motion 
estimator will cause fake motion mismatches in the same location in many consecutive frames. Corrigan 
also exploited the idea that local divergence of vector .elds also generally indicates failure. Rares 
et al [2, 3, 1] has proposed schemes based on image classi.cation. In the case of dust busting, it is 
best to leave the original picture alone in areas of PM. In frame interpolation or frame rate conversion 
though, it is much more dif.cult to propose an effective course of action.  2.5 Summary The requirements 
of image integrity and automation at ingest are demanding. Motion estimation has become an important 
part of the post production chain but the principal outstanding issue is detection of failure and proposing 
what action to take in that instance. This issue has not been well addressed in the academic community. 
  Chapter 3 Basics of Compositing commercial and bespoke alternatives. Figure 3.1: A simple compositing 
graph in NUKE. 3.1. NODE-BASED COMPOSITING CHAPTER 3. BASICS OF COMPOSITING Figure 3.2: The input and 
output from the simple compositing graph in Figure 3.1. 3.1 Node-based Compositing Node-based compositing 
can be thought of as a visual programming tool for describing an acyclic graph of image processing operations. 
For example, Figure 3.1 shows a simple graph or tree, with no branches. At the top of the tree, the leaf 
node is a Read node, here loading an image sequence which is a set of dpx .les on disk. Underneath that, 
we have connected the Read node to a Blur node. This is performing a Gaussian .ltering operation on all 
the channels in the image. Underneath that we have connected a viewer. The artist can step through the 
sequence and view the result at each frame. In this case the tree converts the image in the way shown 
in Figure 3.2. Obviously this isn t a terribly exciting graph, so let s jazz it up a little and create 
the tree shown in Figure 3.3. The graph now contains two new nodes. The .rst is a Merge, which in this 
case is multiplying each pixel in the Gaussian .ltered output by the corresponding pixel from the original. 
The ColorCorrect node underneath increases the gain of image by a factor of 4. In this case the output 
of the Viewer is different as shown. Each node comes with a set of controls which can be used to control 
the parameters of the algorithm inside the node. For example, the controls for the Blur node are shown 
in Figure 3.4. The top two controls show that we are blurring all the channels in the image, by a factor 
of 65. Many of the typical operations that can be inserted as nodes will be familiar to anyone with an 
image processing background. There. Now you can composite .lm clips like a professional! Bear in mind 
that a typical .lm shot will have trees an order of magnitude more complex than our simple example. A 
more typical tree for a .lm effects shot is shown in Figure 3.5, zoomed out a long way so you can admire 
its CHAPTER 3. BASICS OF COMPOSITING 3.1. NODE-BASED COMPOSITING Figure 3.3: The output (right) from 
a more complex compositing graph (left). Figure 3.4: Controls associated with a node. 3.1. NODE-BASED 
COMPOSITING CHAPTER 3. BASICS OF COMPOSITING Figure 3.5: A typical, complex tree. complexity. So -why 
work this way? The tree representation gives us two important attributes. Firstly, the operations applied 
to the footage are non-destructive -there is a visible audit-trail of the operations used to construct 
the effects for a shot. Secondly, the graph can be applied not just to this footage, but to any footage 
by swapping the inputs for new ones. This means that the tree is reusable in different shots. Note that 
systems such as this are also carefully designed to be ef.cient. Nuke, for example, will typically be 
used with images which are either 2K (say 2048x1556 pixels) or 4K (4096x3112 pixels) in size, containing 
at least three .oating-point image channels, frequently more. A typical tree such as the one on the right 
will have dozens of such inputs, and will need to handle the memory management and caching through the 
tree to allow the artist some measure of interactive feedback on the project they are working on. It 
is these data-sizes which make image processing in this environment quite demanding. For complex trees, 
these systems clearly do not perform in real-time . To cope with the compute burden, compositing pipelines 
in major facilities share warehouses of render computers with an even greater resource drain -CG rendering. 
Weta Digital in New Zealand, for example, runs four computing clusters, each of which features prominently 
in the list of the top 500 supercomputing facilities worldwide. CHAPTER 3. BASICS OF COMPOSITING 3.2. 
COMPOSITING IN 3D Figure 3.6: A 3D environment for compositing. In this case, the camera can be used 
to introduce natural parallax into a shot without requiring the complexity of a full 3D environment. 
 3.2 Compositing in 3D Most of us think of image processing as a 2D problem. But, most compositing systems 
in .lm work have 3D capabilities. This is actually a fairly natural move to make. The real-world shots 
these systems are to operate on are clearly 3D in the .rst case, even though they are now represented 
by a 2D projection of that environment. If an artist wants to simplify a particular image processing 
problem, it makes sense to allow the scene to be represented by at least a minimal 3D representation 
-perhaps layers in space, with a viewing camera. Figure 3.6 shows the 3D visualisation environment for 
a 2D compositing exercise. Compositing systems with this capability are used to load 3D assets from CG 
systems, to allow them to be composited in the context of live action. It also gives scope for compositing 
to exploit some of the 3D algorithms we touch on elsewhere -not least camera­tracking. 3.3 Compositing 
in Stereo-3D In the case where we are doing compositing work in multiple views (as opposed to true 3D 
as above), the node-compositing tree needs to be extended to be meaningful. The design shown during the 
course allows multiple views -two in the case of stereo -to be processed at the same time by the same 
node graph. This extension allows the use of more than one image stream without adding complexity. Nodes 
within the tree, or individual parameters within nodes, can be separated where different vies do need 
separate treatment. More on this topic is presented when we discuss stereo-3D. 3.4. SUMMARY CHAPTER 3. 
BASICS OF COMPOSITING  3.4 Summary Node-based environments are natural tinkering-ground for algorithm 
developers. They come with a rich set of common operations, allow easy con.guration of components and 
examination of results. Invariably, they are also programmable by third parties, who can use built-in 
SDKs to develop their own nodes. In addition, the most common compositing systems can be batch-run on 
command lines and scripted using common languages like Python. Chapter 4  Challenges in Visual Compositing 
Motion estimation, optic .ow, texture synthesis and deblurring are mature areas of study in the aca­demic 
community. Yet it is only recently that their use has been integrated into the post production work.ow 
with any degree of con.dence. This is principally because footage presented for post production is never 
well behaved. In this section of the course a sample of technologies that have made it into the work.ow 
will be presented. The focus is on highlighting the difference between the initial algorithmic proposal 
from the academic community and what has to be done to get these algorithms to a useful level of performance. 
4.1 Matting Pulling a matte from a .lm or video sequence is one of the oldest exercises in .lm and television 
post production. It is used for direct manipulation of the position and nature of objects/actors in scenes 
in order to create new sequences not originally recorded. In the simplest case, the object is .lmed against 
a green or blue screen. Then, in post production a combination of detailed manual contour delineation 
and colour based segmentation (i.e. all that is not blue or green is probably the object of interest) 
is used for creating a mask or matte. The mask is non-zero in the region of the object and zero otherwise. 
It describes the opacity of an object pixel at each location in the image. Thus a mask pixel setting 
of 1 indicates that that pixel is completely visible as the object of interest, while a mask pixel of 
0 indicates that the corresponding object pixel is obscured or not available in some way. This mask or 
matte can then be applied to mix the captured footage containing the object of interest with footage 
recorded elsewhere. The act of creating new scenes in this way by combining different object elements 
onto a background image is called compositing. An excellent introduction and background to the Matting 
problem can be found in the work 4.1. MATTING CHAPTER 4. CHALLENGES IN VISUAL COMPOSITING of Chuang 
et al [9]. As summarised there, traditional methods for pulling video mattes are blue screen matting, 
rotoscoping and difference matting. Blue screen matting or chroma keying relies on capturing the foreground 
objects against a solid colour background and subsequently pulling the foreground matte by segmentation 
on the basis of colour. Rotoscoping relies on user drawn editable curves (e.g. Splines) around the foreground 
object of interest. Snap-to-edge operations as found in commercial packages like Adobe Photoshop, and 
introduced in previous articles [40] are useful user complements here. Difference matting relies on the 
generation of a scene containing only background elements (e.g. recording without actors). The image 
difference between this scene and a scene subsequently recorded with actors is then exploited to generate 
the matte. The matte here is therefore 1 when the difference is large and 0 otherwise (for instance). 
While chroma keying demands a controlled environment for recording, rotoscoping can be achieved regardless 
of the complexity of the background environment. Employing tracking to­gether with user assisted rotoscoping 
can greatly improve the utility of contour based approaches. The main limitation of rotoscoping is the 
inability to correctly express image formation at the bound­ary between foreground and background. Useable 
mattes should express the notion that around the boundaries of objects the recorded light is a mixture 
of the background and foreground elements [39]. Difference matting and to some extent chroma keying suffer 
from the problem that in re­gions where foreground and background colour are similar, user interaction 
is required to resolve the matte. In [10, 9], the authors proposed a combination of limited user interaction 
followed by direct estimation of a non-binary alpha matte. They articulate this information to resolve 
many of the problems with previous methods. The underlying idea begins with the user specifying what 
they term a trimap. This map divides the scene into regions known to be background (matte pixels set 
to 0), known to be foreground (matte pixels set to 1), and unknown matte regions. They correctly exploit 
the knowledge that the dif.culty in pulling a very good and useable matte is the proper delineation of 
the mixing of light effect at the object edge. Hence the restriction of interest to the unknown matte 
region, by exploiting image information from the surrounding known matte regions. Their method is also 
able to exploit motion information to propagate mattes between user de.ned and delineated keyframes. 
They give convincing demonstrations of the matting of translucent material (smoke), traditionally a very 
dif.cult task. Since 2001, the notion of Matting as an inference problem has been explored by several 
authors [56, 63, 31] with increasing degrees of success. What is interesting, however, is that in practice, 
none of these systems operate with any degree of reliability for a wide variety of material. Artists 
typically really want the matte pulled for that 2 pixel wide strand of hair, and they really want temporally 
consistent mattes. Much of this can be achieved by allowing the artist to specify careful boundaries 
between the known and unknown layers, but this in itself is a painful task. In practice then, a workable 
matting solution requires both a method of generating garbage mattes aswellas a CHAPTER 4. CHALLENGES 
IN VISUAL COMPOSITING 4.1. MATTING tool for editing trimaps and rotoscoping. This implies further that 
tools for matte-propagation and roto-propagation are important in a real system. 4.1.1 Making a Practical 
Matte Puller We experimented with Chuang s techniques like many others did at the turn of this century. 
However we found that the mattes, while they look good when pulled from blue-screen backgrounds, were 
too active for practical purposes. Hence smoothness of the a matte was important. Furthermore, we found 
at the time that for real scenes, gaussian mixture models for layer colours were not rich enough. Two 
modi.cations were enough therefore to allow us to build a new matte-puller [63]. We used an MRF prior 
on a to impose smoothness on the opacity, and in addition used a sam­pling scheme to improve the textural 
modelling capability of the foreground and background priors. The prior for alpha is therefore con.gured 
as follows. p(a|aN ) . exp - .L .k|a - ak|(4.1) k.N where .k are the usual MRF hyperparameters and an 
eight connected neighbourhood system was used where ak are the current values of a at the eight sites 
surrounding x. The conditional distribu­tion for a is now changed from that used by Chuang et al to yield 
the following. .||(c - (af + (1 - ab)))||2 p(a|f, b, ·)=. exp - + L .k|a - ak|(4.2) 2s2 ek.N The speci.cation 
for the priors on the foreground and background images is also different. In order to model texture, 
the work of Efros [13] is used to allow more believable samples for the foreground and background data 
at the current site to be drawn from the surrounding neighbourhood. Those samples can be used to create 
the colour models for the clusters as in Chuang, or as part of a Gibbs sampling scheme for sample generation. 
This yields a more powerful textural model for the foreground and background estimate. The solution is 
decomposed into solving for a and f, b in two separate steps. A direct line search for a yields the required 
estimate. The new technique models the known foreground and background at a pixel site by drawing samples 
from suitably sized nearby regions in a process inspired by [13] using weights similar to Chuang. The 
size of this sample region scales adaptively so for a pixel far from the known region, the region is 
larger than if it is close by. Given a set of pairs, values for alpha are generated and the triplet maximizing 
the joint distribution is selected as the MAP estimate. 4.2. INBETWEENING CHAPTER 4. CHALLENGES IN VISUAL 
COMPOSITING Issues In principle this process is computationally cheaper than Chuang et al since it eliminates 
the need for clustering, Gaussian modelling, and the associated matrix manipulations. In addition, the 
solution yields much more believable and useful mattes for post production. Although other techniques 
attempt to enforce smoothness on a , it seems that the sampling scheme as is important. The point here 
is that further constraints on the triplet are necessary to create believable mattes. However, the notion 
of temporal smoothness remains dif.cult to impose, aside from using a 3D MRF on the a channel. We get 
workable results from 3D median .ltering and the user can adjust some smoothing parameters to help this. 
Here are some other open questions. How do we make sure the composited picture looks good? Good looking 
layers and mattes are irrelevant if this constraint is not met. What constraints does this idea imply? 
 How can rotoscope parameters for trimaps be propagated between frames?  How can Mattes themselves 
be propagated through a sequence?  Is it possible to pull a garbage mattte automatically?   4.2 Retiming/Inbetweening/Frame 
Rate Conversion The process of retiming changes the frame rate of a sequence of images. This implies 
creating im­ages where none existed before, and that requires interpolating frames at sites inbetween 
existing frames. In fact this task has been around for a very long time under the guise of standards 
con­version. Many television broadcasters and makers of television equipment have been converting between 
25fps and 30fps as a mater of course for PAL to NTSC conversion since the start of tele­vision itself. 
Notable early work was performed by the BBC R&#38;D Department, Snell and Wilcox and Philips. See [18] 
for an overview of now established ideas. The key idea is shown in Figure 4.1. The problem is ultimately 
to reconstruct a frame at an arbi­trary point in time between given frames. If the motion between that 
missing frame and the frames around was known, then building the frame is a matter of some variant on 
a motion compensated blending operation. So the problem of image reconstruction here is in fact the problem 
of interpo­lating the motion .eld at this unknown location. It turns out that this problem is intimately 
related to the problem of new-viewpoint interpolation in multi-view scene capture and synthesis. Viewpoint 
synthesis is a well explored area in the academic community [33, 36] and most solutions there are based 
on exploiting the camera geometry between different views taken at the same point in time. CHAPTER 4. 
CHALLENGES IN VISUAL COMPOSITING 4.2. INBETWEENING Figure 4.1: Left: The essence of motion-based inbetweening 
is to reconstruct the missing motion .eld (green and blue vectors) at the new temporal location such 
that some version of cut and paste from frames n and n +1 will build the new inbetween picture showing 
the right movement. Right: Initial guesses for the new motion .eld can be had by using a proportion of 
the motion estimated between n and n +1 (black vectors) to be assigned to the appropriate sites in the 
inbetween frame. The initial guesses are shown as dashed coloured vectors. 4.2. INBETWEENING CHAPTER 
4. CHALLENGES IN VISUAL COMPOSITING However, in practice new-view synthesis based on object modelling 
does not necessarily result in pictures of good enough quality for post production. In fact, if the available 
views are close enough together, an approach based on frame rate conversion tends to yield more reliable 
inbetweens. This equivalence between view synthesis and frame rate conversion was exploited in post pro­duction 
for the movie The Matrix. The effects for that movie popularised the notion of new-view synthesis and 
drew heavily from the artistic work of Tim MacMillan (www.timeslice.lms.com) in the early 1990 s. Dr. 
Bill Collis1 worked on the early algorithms using ideas from standards conver­sion and not considering 
the multi-view geometry as such. It turns out the early work of Kokaram [28, 27] for missing frame interpolation 
can also be adapted for this problem and together with Collis and Simon Robinson2 an automated inbetweening 
process was developed and deployed as plug-ins for various platforms. Simultaneous to this effort the 
team of Litwinowicz and Pierre Jas­min 3 had also developed similar tools, but emphasising instead the 
use of very good user interfaces to allow interaction with the interpolated frames. 4.2.1 Frame Interpolation 
Consider reconstructing a frame Im which is, say, halfway inbetween frames In,In+1 at time n and n +1. 
Assume also that the motion D = dn,n +1, dn+1,n, is known since the frames n, n +1 exist. Other motion 
.elds D. = dn,n - 1, dn+1,n+2 may also be known. A Bayesian approach to the problem proceeds by stating 
the following. p(dm,n, dm/n+1, s|In,In+1, D) . p(In,In+1|dm,n, dm/n+1)p(dm,n, dm/n+1, s|Dnay) (4.3) where 
the prior p(dm,n, dm/n+1|Dnay) imposes some smoothness constraints on the interpolated motion vector 
at a site given the local neighbourhood of vectors Dnay and p(In,In+1|·) (Likelihood) ensures that the 
interpolated vectors match similar data in the previous and next frames. An occlu­sion state s allows 
some incorporation of occlusion and uncovering into the problem. The simplest choice for the Likelihood 
function is a Gaussian distribution of frame differences given an interpo­lated vector. The choice of 
prior should encourage the interpolated .eld to agree with the existing temporal motion information and 
also result in smooth spatial behaviour. Smoothness is important here as it avoids picture break up as 
the objects move through the new frames. The resulting optimisation problem of choosing the best vector 
.eld can be approached using any number of schemes including ICM, BeliefPropagation, Graph Cuts and even 
one of the many Markov Chain techniques. Because of the large image sizes however, memory and speed is 
impor­tant in practice. Hence approaches based on selecting between possible candidates at each pixel 
site 1www.thefoundry.co.uk 2www.thefoundry.co.uk 3www.revisionfx.com CHAPTER 4. CHALLENGES IN VISUAL 
COMPOSITING 4.2. INBETWEENING tend to be more ef.cient. In a way ICM, BeliefPropagation and Graph Cuts 
all require generation of solution candidates as a pre-requisite. In the frame interpolation problem 
candidates can come from the existing motion .elds by simple linear interpolation 4.1. The reason that 
this technique tends to be more useful than a camera geometry based approach is that there are fewer 
assumptions here about the scene content between frames and issues such as lens distortions and so on 
tend to be im­plicitly compensated in the estimation of the optic .ow .eld between frames. This is not 
to say that a motion interpolation technique is better, just that it can be applied to more situations 
without too much effort, hence is useful as an automated tool. Some interesting results are shown in 
Figure 4.2. 4.2.2 Outstanding issues The creation of a reliable automated retimer is dif.cult indeed. 
Aside from the problems with motion estimation itself, the main unanswered questions worth exploring 
i) how to detect when the motion estimator has failed? ii) what to do when it has failed? iii) how to 
detect occluded and uncovered regions and what to do in those regions?. When the motion between frames 
is pathological it is dif.cult to .nd a reliable fallback technique. Typically errors tend to be visible 
as strange warping of object boundaries, or hard edges where they should be soft. Generating consistently 
smooth interpolated frames also requires multiple passes over the sequence, and that admits a longer 
processing time. Litwinowicz et al have developed very good interactive tools that allow the user to 
create just the desired effect. In general to create cinema quality pictures a good idea is to allow 
the user to retime the foreground and background data as two separate layers and this has been used to 
good effect in tools from The Foundry and Re:VisionEffects. Figure 4.2 also shows problems when the frame 
rate of the sequence is just not good enough to capture the original motion. Motion interpolation in 
that case is bound to fail. Even more paradoxical is that when used for slow-motion effects, any small 
error in the frames is more visible after new frame generation because of the lower effective object 
speeds. These problems mean that while these tools are popular for many applications, for some time to 
come they will remain heavily user interactive at the cinema level and very conservative at the broadcast 
level. 4.2. INBETWEENING  Figure 4.2: Inbetweening with frame blending (top row) and motion interpolation 
(bottom row). The 2nd and 4th frames are interpolated halfway between frames 1,3 and 3,5 reading from 
left to right. The motion interpolated images look very good in areas where the motion .eld can be well 
estimated e.g. head and body of the moving foreground. Inbetweening with motion is much better than frame 
blending without motion as can be seen with the very blurry images created in the top row. The hand shows 
very poor reproduction in the motion interpolated frames because i) the frame rate is just not good enough 
to capture the fast motion there and ii) the camera exposure is too long and so there is substantial 
motion blurring. Chapter 5   Stereo-3D Stereo-3D1 presents interesting algorithmic challenges. Many 
of the issues are common to other research efforts in multiple-camera work. Some are peculiar to the 
end-goal of the medium: to provide comfortable stereo viewing. Stereo-3D presents to the audience an 
approximation of normal human stereo vision. Why is it an approximation? The principal reason is that 
the viewer s focus is always on the screen-plane, regardless of object depth. This unnatural relationship 
with real-world views is uncomfortable for some. Also, some 5% of the adult population can t resolve 
depth from stereo at all, with maybe up to a quarter of the population having some milder depth-resolution 
de.cit. Any defects in our stereo approximation to the real world can tip the balance, and make it impossible 
to resolve. The full science of human vision perception is beyond the scope of this course. We instead 
will concentrate on some of the common post production issues which can be need attention. As a side-note, 
the current stereo-3D push is a revival of a old medium. Key factors encouraging the current growth are 
the reliability of digital capture and the reliability of digital projection. In particular the investment 
in digital projection in modern cinemas has provided an wider outlet for stereo-3D material outside specialised 
venues. 5.1 Convergence and Keystoning The point where cameras converge on an object in shot (ideally 
the point where their optical axes intersect) is known as the convergence point. 1Note our nomenclature 
here. Variously referred to as stereo, stereogrammetry, stereographic, 3D, the concatenation used here 
is at least familiar to both the image processing community and the marketing side of the business (who 
favour 3D, which this isn t). 5.1. CONVERGENCE AND KEYSTONING CHAPTER 5. STEREO-3D Figure 5.1: Parallax 
resulting from converging cameras. At the convergence point, objects in the two views will have zero 
horizontal disparity, also known as zero parallax, and will appear to be the same distance away as the 
screen on which the scene is being viewed. For a scene shot with cameras that are converging -i.e. pointing 
or Toeing-in towards each other -this will occur where a ray emerging from the front of one camera, perpendicular 
to its image plane, would meet a similar ray from the other camera (see Figure 5.1). Anything in front 
of this point will have negative parallax -the object in the left image will be to the right of the same 
object in the right image -and appear to be in front to the screen. Similarly, objects behind the point 
of convergence are said to have positive parallax and appear behind the screen. Convergence can be changed 
in post production with a simple horizontal shift of one image relative to the other. The eyes can take 
a while to adjust to sudden changes in convergence so it is desirable to try to minimise these when cutting 
between scenes, for example. Moving the point of convergence nearer to the cameras will have the effect 
of shifting everything else further away, while converging on a more distant object will bring everything 
closer. In this second instance, care must be taken to ensure that the scene stays inside the area which 
can comfortably be seen by the audience. During normal vision, our eyes converge on an object as we look 
at it, and focus on the object at the same time. However, when viewing a stereo presentation such as 
a 3D .lm, our eyes will always be focused on the screen, yet will be required to change convergence as 
the scene changes or as we look at different parts of it. To some extent, they are able to do this, but 
when the distance between the focal point and the convergence point becomes too great, viewers will experience 
discomfort. If an object appears too far in front of the screen, they will feel as if their eyes are 
crossing and may even be unable to fuse the left and right images into one. Similarly, if CHAPTER 5. 
STEREO-3D 5.2. DISPARITIES SHOULD BE HORIZONTAL objects are pushed too far away, the positive parallax 
between the views could increase to the extent that the eyes would be required to diverge in order to 
fuse the two images. Divergence never occurs in normal vision; it is generally accepted that a small 
amount is acceptable in 3D cinema2, though this can make for an uncomfortable viewing experience, causing 
eye strain, headaches or nausea for the audience. Beyond this, the 3D effect will be lost and the audience 
will see two separate images of the object. Stereoscopic footage is generally shot with one of two main 
physical camera con.gurations: parallel or converging. Each have their advantages and disadvantages3 
and some stereographers have strong views on the subject of which is the right con.guration to use4. 
The converging method is more akin to the operation of the human visual system, where our eyes converge 
to focus on an object of interest, and might therefore seem the more natural choice. The views from parallel 
cameras do not converge, so the desired convergence distance must be set in post by applying a horizontal 
shift to one or both images. When converging cameras are used, the convergence can still be adjusted 
in a similar manner, but is likely to have already been set to the desired value when the footage was 
acquired. However, although footage shot with converging cameras is less likely to need the convergence 
adjusted, this method of image acquisition does introduce an additional problem: keystoning. The term 
keystoning refers to the perspective distortions introduced by the fact that two converg­ing cameras 
view the scene from different directions, so that their image planes are not coplanar but are angled 
slightly with respect to each other (see Figure 5.2 and Figure 5.3). Avoiding keystoning differences 
between the eyes can be done by using perspective correction (or shift) lenses, which essentially allow 
a correction to be applied during a shoot5. Such a lens allows the centre of projection to be shifted 
away from the centre of the image sensor (.lm or CCD) while keeping the image plane parallel to the sensor 
in order to avoid perspective distortion. 5.2 Disparities should be horizontal When deriving depth cues 
from stereo-3D material, the human vision system expects objects to have a horizontal separation right-to-left. 
Filmed stereo-3D material can exhibit a vertical separation, which can cause eye-strain. Vertical separation 
comes from multiple sources, for example: Paired cameras on a rig need to be carefully aligned to avoid 
vertical disparities through mis­ 2See How a 3D Stereoscopic Movie is Made, 3-D Revolution Productions: 
http://www.the3drevolution.com/3dscreen.html 3See Digital Praxis -Stereoscopic 3D: http://www.digitalpraxis.net/stereoscopic3d.htm 
4See Stereo VFX -Convergence: http://www.stereovfx.com/convergence2.html 5See Perspective Correction 
Lens, Wikipedia article: http://en.wikipedia.org/wiki/Perspective correction lens for examples. 5.2. 
DISPARITIES SHOULD BE HORIZONTAL CHAPTER 5. STEREO-3D Figure 5.2: Image Planes of Converging Cameras. 
Figure 5.3: Keystone Distortion. CHAPTER 5. STEREO-3D 5.3. DENSE DISPARITY ESTIMATION aligned camera 
geometry. Mismatched zooming.  Keystoning effects from converging camera rigs.  For a nodal camera 
move, the keystoning disparity and other disparities introduced by nodal camera misalignments can be 
compensated for by calculating a suitable af.ne correction. There are no hard and fast rules here, but 
a reasonable approach is to perform feature-matching between left and right views, and then to .nd the 
parameters of a warp which minimises the vertical separation of matched features. This works well in 
practice, but fails to compensate for some rarer occurrences such as non-nodal camera displacement. In 
these cases, new view synthesis would be required to perform the correction. In practice, however, most 
defects can be .xed by assuming the camera misalignment is nodal. 5.3 Dense Disparity Estimation An 
image -such as a single frame of a motion picture -is a two-dimensional (2D) representation of a three-dimensional 
(3D) scene, which by de.nition will contain structures at different depths. In an image taken from a 
slightly different viewpoint -for example, that of the second camera used to produce a stereo pair -the 
same structures will appear slightly shifted. Stereo disparity is the term used to describe the shift 
that occurs for a particular point in 3D space between the left and right images. In a stereo pair the 
cameras are offset horizontally, so this usually corresponds to a purely horizontal shift. However, the 
amount of the shift is not constant but can vary from pixel to pixel; it will depend on the depth of 
the corresponding object within the scene. If parallel cameras are used, objects in the foreground -close 
to the viewer -will shift signi.cantly while those in the distant background might not move at all. In 
addition, as the viewpoint shifts from one eye to the other, areas that were visible before might become 
hidden, such as background areas that become occluded when a foreground object shifts across in front 
of them, or a surface of an object that becomes obscured when the object is viewed from a different angle. 
Similarly, previously hidden areas can be revealed. Figure 5.4 shows an example of a stereo pair of views 
of a scene in which different areas are occluded and revealed in each camera s view. We can build up 
a complete picture of the stereo disparity by estimating the change in position of every point in the 
scene between one view and the other. Disparity estimation is a well-explored topic and Stereo-3D provides 
a practical application for these techniques. Disparity estimation can be considered to be similar to 
local motion estimation, in that the goal is to estimate how each pixel moves from one image to another. 
However, local 5.3. DENSE DISPARITY ESTIMATION CHAPTER 5. STEREO-3D Figure 5.4: Left and right view 
of the same scene. motion estimation is typically unconstrained, making no assumptions about the nature 
of the scene (other than local smoothness) and pixels are allowed to move in any direction. This is because 
local motion estimation is usually performed on two frames of a sequence, separated in time. The time 
separation between the frames means objects within them might have have moved in the interim, and we 
have no prior knowledge of the motion of these objects. With a stereo pair of images there is no time 
separation, so the images should be unaffected by motion within the scene. The only motion between one 
view and another will be that resulting from the physical separation of the cameras. This has the advantage 
of being rigid body motion -scaling, rotation and translation only -and applies to the whole of the image, 
unlike the motion in a typical image sequence which could include local deformations (imagine a sequence 
showing a man walking down a street, for example). We therefore know that objects within the scene will 
be transformed only in this rigid way between one view and the other, and can use this additional knowledge 
to constrain the motion (i.e. disparity) estimation. In practice, we do this by detecting features in 
both views of the scene .rst of all, then calculating the stereo disparity of those features6. Given 
the stereo disparity of these pairs of features, we can then .nd a transform which maps points in one 
view to lines in the other view (the epipolar lines)7. The disparity estimation for the rest of the points 
in the image is then carried out under the condition that the corresponding pixels in the other image 
must lie along these lines. Because of this additional constraint, motion estimation done in this way 
tends to give more accurate results for the disparity than would local motion estimation performed on 
the same pair of 6The features in question are points of interest corresponding to recognisable structures 
within the scene, for example in a scene containing a wooden table they would include the corners of 
the table top and perhaps some conspicuous marks on the surface of it, such as a dark knot in the grain. 
7In order to be able to map points to points, we would need to know how far away the corresponding 3D 
points are from the camera. Without this, there is an additional, unsolved degree of freedom which means 
the points in the .rst view can lie anywhere along a line in the second. CHAPTER 5. STEREO-3D 5.3. DENSE 
DISPARITY ESTIMATION Figure 5.5: Disparity estimate (on the left) and one source image (on the right) 
inside a compositing system. The disparity estimate is represented by using red and green as the magnitude 
of the x and y components of the .eld in one direction, and blue and alpha (not shown) in the other direction. 
The two directions are almost mirror .elds of each other, with the exception of the occluded areas. images. 
See Figure 5.5 for an example image. The real problem with this and other techniques is occlusions. In 
the extreme example (a dense forest, for example) where few pixels in one eye are visible in the other, 
the disparity calculation is extremely unreliable. Possible solutions for static scenes would include 
using multiple frames to build up a dense 3D estimate of the scene data. Solutions for dynamic scenes 
could involve minimising optical .ow over multiple frames. At the higher level of compute cost, however, 
doing this may be more onerous than simply living with unreliable disparity maps. 5.3.1 Moving the cameras 
Disparity maps can be used to compute novel views somewhere in space between the original views, in a 
similar manner to how retiming can produce new samples in time. This provides a mechanism to modify the 
apparent camera geometry in post, in order to correct shots with excessive disparity. The occlusion .aws 
in the disparity maps for moving footage mean this is a good starting point for shots which in any case 
will involve some manual corrective work. It hasn t been shown to be a reliable automated process for 
the unassisted re-grading of depth in arbitrary clips. It is an interesting area for further development, 
however. Historically, one of the main prob­lems with shooting in stereo has been the need to make certain 
decisions about the desired 3D effect before the shoot takes place. To some extent, this problem has 
been reduced by the development 5.3. DENSE DISPARITY ESTIMATION CHAPTER 5. STEREO-3D of more advanced 
camera set-ups which allow for more .exibility during a shoot. However, it is still the case that some 
3D properties are .xed after .lming and have previously been dif.cult or impossible to alter as the .nished 
.lm takes shape. A fundamental property of a 3D shot is the inter­axial distance; this is the distance 
between the left and right cameras, and determines the perceived depth of the scene. Usually, this distance 
should be about the same as the distance between the viewer s left and right eyes8 but it can be adjusted 
in order to achieve speci.c effects. For instance, it might be necessary to increase it signi.cantly 
in order to get a better sense of depth over a distant landscape, where .lming with the standard interaxial 
separation would result in an essentially .at­looking scene (this is sometimes known as in.nity .atness9). 
Increasingly the interaxial distance substantially can also result in hyperstereo or miniaturisation, 
where the exaggerated stereo effect means that the viewer feels herself to be massive in comparison to 
the scene being viewed. This is because the human brain is used to experiencing the amount of stereo 
separation that results from the separation of one s eyes. Anything greater fools the brain into thinking 
that the eyes must be further apart in relation to the scene in front of them than normal. Since the 
distance between the eyes is .xed, the brain can only make sense of this by reasoning that the scene 
must be smaller than it really is. This can have the effect of making the viewer feel like a giant, or 
alternatively like a normal human looking down at a scale model10. Similarly, a reduced interaxial separation 
can have the opposite effect, making the viewer feel she has shrunk in relation to the scene. Although 
it is usually desirable to avoid such effects, sometimes they can be invaluable tools in helping to tell 
a story or ful.l a director s artistic vision (it is easy to imagine how they could be used to good effect 
in a 3D version of the children s classic Alice in Wonderland, for example, as the heroine shrinks and 
grows in response to eating certain things). In addition, maintaining the standard interaxial separation 
throughout -while being both realistic and comfortable for the viewer -can make for an unexciting stereo 
experience. After a while, the viewer s brain will adjust to the stereo effect and they might cease to 
appreciate the extra dimension once the initial novelty wears off11. Varying this separation -and thus 
the perceived depth -between scenes from time to time can help to provide the necessary stimulation to 
keep the viewer s brain alive to the differences between the 3D scene in front of them and their customary 
2D viewing experience. However, any variation in depth must be used with caution, as people s eyes take 
a while to adjust to signi.cant changes in the stereo separation, which means that fast cutting between 
scenes with signi.cantly different interocular separations will be uncomfortable for the viewer and should 
be avoided12. 8The interocular separation, about 65mm for an average adult. 9See Journey to the Center 
of the Earth Review, 3-D Revolution Productions: http://www.the3drevolution.com/3dreview.html 10See Digital 
Praxis -Stereoscopic 3D: http://www.digitalpraxis.net/stereoscopic3d.htm 11See Beowulf Review, 3-D Revolution 
Productions: http://www.the3drevolution.com/3dreview2.html 12See James Cameron Supercharges 3D, Variety, 
10 Apr 2008 (http://www.variety.com/article/VR1117983864.html?categoryid=2868&#38;cs=1) CHAPTER 5. STEREO-3D 
5.4. OPTICAL DIFFERENCES BETWEEN THE EYES 5.3.2 Moving compositing components Disparity information 
can also be used in compositing applications to push data from one view into another. Imagine a rotoscoping 
task, using spline-based object outlines, performed in one eye only. The point data representing the 
shapes can be warped from one eye to another by using the disparity estimate.  5.4 Optical differences 
between the eyes Colour discrepancies between the two views of a scene can also make it more dif.cult 
for the viewer to fuse objects in the scene or to view it comfortably. In order to reduce interaxial 
separation of the camera optical axes, .exible stereo rigs can be built with mirrors or prisms. This 
approach allows the optical axes to be more closely aligned than would otherwise be possible, given the 
large physical size of the cameras themselves. While this improves that apparent camera geometry, it 
introduces a new issue. The use of a mirror or a prism in one view introduces a polarising effect in 
that view. This means that the appearance of highlights can be signi.cantly different between the left 
and right views. Even with relatively diffuse lighting conditions, a colour density difference is visible 
between the eyes. It should be also be noted that even without the presence of a mirror or prism rig, 
it is hard to guarantee that matched cameras will produce the same colour pro.le. Correction of these 
differences in post usually requires skill and can be a painstaking process. However, some automated 
grading can be applied along the lines of the work of Pitie et al[38]. This work uses histogram matching 
techniques in order to transfer the colour distribution of one image to another. The transfer of missing 
highlights is a local phenomenon rather than a global histogram problem and there are no good techniques 
currently available to the stereo-3d post production com­munity. The use of mirrors and prisms can also 
introduce a .ltering effect in the re.ected view which again is hard to compensate for, although there 
is some scope to experiment with blind (or even non-blind) deconvolution techniques. 5.4. OPTICAL DIFFERENCES 
BETWEEN THE EYES CHAPTER 5. STEREO-3D  Bibliography [1] A. RARES, M. J.T. REINDERS, J. B. Complex event 
classi.cation in degraded image se­quences. In Proceedings of ICIP 2001 (IEEE), ISBN 0-7803-6727-8 (Thessaloniki, 
Greece, October 2001). [2] A. RARES, M. J.T. REINDERS, J. B. Statistical analysis of pathological motion 
areas. In The 2001 IEE Seminar on Digital Restoration of Film and Video Archives (London, UK, January 
2001). [3] A. RARES, M. J.T. REINDERS, J. B. Image sequence restoration in the presence of patho­logical 
motion and severe artifacts. In Proceedings of ICASSP 2002 (IEEE) (Orlando, Florida, USA, May 2002). 
[4] BERTALMIO, M., SAPIRO, G., CASELLES, V., AND BALLESTER, C. Image inpainting. In SIGGRAPH 00: Proceedings 
of the 27th annual conference on Computer graphics and interactive techniques (New York, NY, USA, 2000), 
pp. 417 424. [5] BLACK, M., AND ANANDAN, P. The robust estimation of multiple motions: Parametric and 
piecewise-smooth .ow .elds. Computer Vision and Image Understanding 63 (January 1996), 75 104. [6] BORNARD, 
R. Probabilistic approaches for the digital restoration of television archives. PhD Thesis, Ecole Centrale, 
Paris, 2002. [7] BUISSON, O. Analyse de s´esolution, application a la restaura­ equences d images haute 
r´` tion num´ematographiques. erique de .lms cin´PhD thesis, Universit´e de La Rochelle, France, December 
1997. [8] BUISSON, O., BESSERER, B., BOUKIR, S., AND HELT, F. Deterioration detection for digital .lm 
restoration. In IEEE International Conference on Computer Vision and Pettern Recogni­tion (June 1997), 
vol. 1, IEEE, pp. 78 84. [9] CHUANG, Y.-Y., AGARWALA, A., CURLESS, B., SALESIN, D. H., AND SZELISKI, 
R. Video matting of complex scenes. In Proceedings of ACM SIGGRAPH (2002). BIBLIOGRAPHY BIBLIOGRAPHY 
 [10] CHUANG, Y.-Y., CURLESS, B., SALESIN, D. H., AND SZELISKI, R. A bayesian approach to digital matting. 
In Proceedings of CVPR (2001). [11] CORRIGAN, D., HARTE, N., AND KOKARAM, A. Pathological Motion Detection 
for Robust Missing Data Treatment. EURASIP Journal on Advances in Signal Processing (2008). [12] DUFAUX, 
F., AND KONRAD, J. Ef.cient, robust and fast global motion estimation for video coding. IEEE Transactions 
on Image Processing 9 (2000), 497 501. [13] EFROS, A. A., AND LEUNG, T. K. Texture synthesis by non-parametric 
sampling. In Pro­ceedings of the IEEE International Conference on Computer Vision (ICCV) (September 1999), 
vol. 2, pp. 1033 1038. [14] FERRANDI `ERE, E. D. Motion picture restoration using morphological tools. 
Kluwer Aca­demic Publishers, May 199, pp. 361 368. [15] FERRANDI ERE` , E. D. Restauration automatique 
de .lms anciens. PhD thesis, Ecole des Mines de Paris, France, December 1997. [16] FERRANDI ` ERE, E. 
D. Mathematical morphology and motion picture restoration. John Wiley and Sons, New York, 2001. [17] 
FERRANDI `ERE, E. D., AND SERRA, J. Detection of local defects in old motion pictures. In VII National 
Symposium on Pattern Recognition and Image Analysis (April 1997), pp. 145 150. [18] HAAN, G. D., AND 
BELLERS, E. Deinterlacing-an overview. In Proceedings of the IEEE (Sept 1998), vol. 86, no. 9, pp. 1839-1857. 
[19] HILL, L., AND VLACHOS, T. On the estimation of of global motion using phase correlation for broadcasting 
applications. In Seventh International Conference on Image Processing and Its Applications (July 1999), 
vol. 2, pp. 721 725. [20] HILL, L., AND VLACHOS, T. Global and local motion estimation using higher-order 
search. In 5th Meeting on Image Recognition and Understanding (MIRU 2000) (July 2000), vol. 1, pp. 18 
21. [21] JOYEUX, L., BOUKIR, S., BESSERER, B., AND BUISSON, O. Reconstruction of degraded image sequences. 
application to .lm restoration. Image and Vision Computing, 19 (2001), 503 516. [22] KENT, B., KOKARAM, 
A., COLLIS, B., AND ROBINSON, S. Two layer segmentation for handling pathological motion in degraded 
post production media. In IEEE International Con­ference on Image Processing (October 2004), pp. 299 
302. BIBLIOGRAPHY BIBLIOGRAPHY [23] KO, S.-J., LEE, S.-H., JEON, S.-W., AND KANG, E.-S. Fast digital 
image stabilizer based on gray-coded bit-plane matching. IEEE Transactions on Consumer Electronics 45, 
3 (Aug. 1999), 598 603. [24] KO, S.-J., LEE, S.-H., AND LEE, K.-H. Digital image stabilizing algorithms 
based on bit­plane matching. IEEE Transactions on Consumer Electronics 44, 3 (Aug. 1998), 617 622. [25] 
KOKARAM, A. On missing data treatment for degraded video and .lm archives: a survey and a new bayesian 
approach. IEEE Transactions on Image Processing (March 2004), 397 415. [26] KOKARAM, A., MORRIS, R., 
FITZGERALD, W., AND RAYNER, P. Detection of missing data in image sequences. IEEE Image Processing (November 
1995), 1496 1508. [27] KOKARAM, A. C. Reconstruction of severely degraded image sequence. In Image Analysis 
and Processing (September 1997), vol. 2, Springer Verlag, pp. 773 780. [28] KOKARAM, A. C. Motion Picture 
Restoration: Digital Algorithms for Artefact Suppression in Degraded Motion Picture Film and Video. Springer 
Verlag, ISBN 3-540-76040-7, 1998. [29] KOKARAM, A. C. On missing data treatment for degraded video and 
.lm archives: a survey and a new bayesian approach. IEEE Transactions on Image Processing 13 (March 2004), 
397 415. [30] KONRAD, J., AND DUBOIS, E. Bayesian estimation of motion vector .elds. IEEE Transac­tions 
on Pattern Analysis and Machine Intelligence 14, 9 (September 1992). [31] LEVIN, A., LISCHINSKI, D., 
AND WEISS, Y. A closed-form solution to natural image mat­ting. IEEE Transactions on Pattern Analysis 
and Machine Intelligence 30, 2 (2008), 228 242. [32] MANHALL, S., AND HARVEY, N. Film and video archive 
restoration using mathematical morphology. In IEE Seminar on Digital Restoration of Film and Video Archives 
(Ref. No. 2001/049) (January 2001), pp. 9/1 9/5. [33] MANSOURI, A., AND KONRAD, J. Bayesian winner-take-all 
reconstruction of intermediate views from stereoscopic images. IEEE Image Processing 9, 10 (October 2000), 
1710 1722. [34] NADENAU, M. J., AND MITRA, S. K. Blotch and scratch detection in image sequences based 
on rank ordered differences. In 5th International Workshop on Time-Varying Image Processing and Moving 
Object Recognition (September 1996). [35] ODOBEZ, J.-M., AND BOUTH ´ EMY, P. Robust multiresolution estimation 
of parametric mo­tion models. Journal of visual communication and image representation 6 (1995), 348 
365. BIBLIOGRAPHY BIBLIOGRAPHY [36] O.J.WOODFORD, AND ANDA.W. FITZGIBBON, I. R. Ef.cient new-view synthesis 
using pairwise dictionary priors. In IEEE International Conference on Computer Vision and Pattern Recognition 
(June 2007), pp. 1 8. [37] PAISAN, F., AND CRISE, A. Restoration of signals degraded by impulsive noise 
by means of a low distortion, non linear .lter. Signal Processing 6 (1984), 67 76. [38] PITI, F., KOKARAM, 
A., AND DAHYOT, R. Automated colour grading using colour distribu­tion transfer. Journal of Computer 
Vision and Image Understanding (February 2007). [39] PORTER, T., AND DUFF, T. Compositing digital images. 
In Proceedings of ACM SIGGRAPH (1984), vol. 18, pp. 253 259. [40] PREZ, P., BLAKE, A., AND GANGNET, M. 
Jetstream: Probabilistic contour extraction with particles. In ICCV 2001, International Conference on 
Computer Vision (July 2001), vol. II, pp. 524 531. [41] QI, W., AND ZHONG, Y. New robust global motion 
estimation approach used in mpeg-4. Journal of Tsinghua University Science and Technology (2001). [42] 
RATAKONDA, K. Real-time digital video stabilization for multimedia applications. In Pro­ceedings International 
Symposium on Circuits and Systems (Monterey, CA, USA, May 1998), vol. 4, IEEE, pp. 69 72. [43] READ, 
P., AND MEYER, M.-P. Restoration of Motion Picture Film. Butterworth Heinemann, ISBN 0-7506-2793-X, 2000. 
[44] ROOSMALEN, P. M. B. V., LAGENDIJK, R. L., AND BIEMOND, J. Correction of intensity .icker in old 
.lm sequences. Submitted to: IEEE Transactions on Circuits and Systems for Video Technology (December 
1996). [45] ROOSMALEN, P. M. B. V., LAGENDIJK, R. L., AND BIEMOND, J. Flicker reduction in old .lm sequences. 
In Time-varying Image Processing and Moving Object Recognition 4 (1997), Elsevier Science, pp. 9 17. 
[46] S. ARMSTRONG, P. J. W. R., AND KOKARAM, A. C. Restoring video images taken from scratched 2-inch 
tape. In Workshop on Non-Linear Model Based Image Analysis, NMBIA 98; Eds: Stephen Marshall, Neal Harvey 
and Druti Shah (September 1998), Springer Verlag, pp. 83 88. [47] SADHAR, S., AND RAJAGOPALAN, A. N. 
Image estimation in .lm-grain noise. IEEE Signal Processing Letters 12 (March 2005), 238 241. BIBLIOGRAPHY 
BIBLIOGRAPHY [48] SAITO, T., KOMATSU, T., OHUCHI, T., AND SETO, T. Image processing for restoration 
of heavily-corrupted old .lm sequences. In International Conference on Pattern Recognition 2000 (2000), 
pp. Vol III: 17 20. [49] SCHARSTEIN, D., AND SZELISKI, R. A taxonomy and evaluation of dense two-frame 
stereo correspondence algorithms. International Journal of Computer Vision 47 (April 2002), 7 42. [50] 
SIDOROV, D., AND KOKARAM, A. Suppression of moir´e patterns via spectral analysis. In SPIE Conference 
on Visual Communications and Image Processing (January 2002), vol. 4671, pp. 895 906. [51] SIDOROV, D. 
N., AND KOKARAM, A. C. Removing moir from degraded video archives. In XIth European Conference in Signal 
Processing (EUSIPCO 2002) (September 2002). [52] SMOLIC, A., AND OHM, J.-R. Robust global motion estimation 
using a simpli.ed m­estimator approach. In IEEE International Conference on Image Processing (Vancouver, 
Canada, September 2000). [53] STILLER, C. Motion estimation for coding of moving video at 8kbit/sec with 
gibbs modelled vector.eld smoothing. In SPIE VCIP. (1990), vol. 1360, pp. 468 476. [54] STOREY, R. Electronic 
detection and concealment of .lm dirt. UK Patent Speci.cation No. 2139039 (1984). [55] STOREY, R. Electronic 
detection and concealment of .lm dirt. SMPTE Journal (June 1985), 642 647. [56] SUN, J., JIA, J., TANG, 
C.-K., AND SHUM, H.-Y. Poisson matting. ACM Transactions on Graphics 23, 3 (2004). [57] TENZE, L., RAMPONI, 
G., AND CARRATO, S. Blotches correction and contrast enhance­ment for old .lm pictures. In IEEE International 
Conference on Image Processing (2000), p. TP06.05. [58] TENZE, L., RAMPONI, G., AND CARRATO, S. Robust 
detection and correction of blotches in old .lms using spatio-temporal information. In Proceedings of 
SPIE International Symposium of Electronic Imaging 2002 (January 2002). [59] TUCKER, J., AND DE SAM LAZARO, 
A. Image stabilization for a camera on a moving platform. In Proc. of the IEEE Paci.c Rim Conf. on Communications, 
Computers and Signal Processing (May 1993), vol. 2, pp. 734 737. [60] UOMORI, K., MORIMURA, A., ISHII, 
H., SAKAGUCHI, T., AND KITAMURA, Y. Automatic image stabilizing system by full-digital signal processing. 
IEEE Transactions on Consumer Electronics 36, 3 (Aug. 1990), 510 519. BIBLIOGRAPHY BIBLIOGRAPHY [61] 
VLACHOS, T. Simple method for estimation of global motion parameters using sparse trans­lational motion 
vector .elds. Electronics Letters 34, 1 (January 1998), 60 62. [62] VLACHOS, T., AND THOMAS, G. A. Motion 
estimation for the correction of twin-lens telecine .icker. In IEEE International Conference on Image 
Processing (September 1996), vol. 1, pp. 109 112. [63] WHITE, P., COLLIS, B., ROBINSON, S., AND KOKARAM, 
A. Inference matting. In IEE European Conference on Visual Media Production (November 2005), pp. 161 
171. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1667263</article_id>
		<sort_key>240</sort_key>
		<display_label>Article No.</display_label>
		<display_no>24</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>24</seq_no>
		<title><![CDATA[Visual perception of 3D shape]]></title>
		<page_from>1</page_from>
		<page_to>94</page_to>
		<doi_number>10.1145/1667239.1667263</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1667263</url>
		<abstract>
			<par><![CDATA[<p>The human brain has the remarkable ability to turn 2D retinal images of an object into a vivid perception of the object's 3D shape. Mathematically, this should be impossible, and yet we do it effortlessly whenever we open our eyes. How does the brain achieve this? This course presents a number of key findings from the study of human visual perception of 3D shape. It shows how different sources of image information such as contours, texture gradients, shading, and optic flow each contribute to the reconstruction of 3D shape by the human visual system. The course also summarizes what happens when 3D shape perception fails, leading to some cool illusions, and describes current ideas about how 3D shapes are parsed and represented, and relates these ides to theories of 2D shape encoding. Throughout the course, connections will be made to common practices in the artistic depiction of 3D form. The course concludes with a discussion of how an understanding of human shape perception might be leveraged to enhance 3D shape visualization in photorealistic and non-photorealistic rendering.</p> <p>This course should be of interest to graphics researchers and practitioners who want to understand the portrayal of shape and, more broadly, anyone who is curious about human vision.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Shape</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010242</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Shape representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010249</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Shape inference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1797174</person_id>
				<author_profile_id><![CDATA[81100588271]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Roland]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Fleming]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Max Planck Institute for Biological Cybernetics]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797175</person_id>
				<author_profile_id><![CDATA[81317500081]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Manish]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Singh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rutgers University, New Brunswick]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1667264</article_id>
		<sort_key>250</sort_key>
		<display_label>Article No.</display_label>
		<display_no>25</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>25</seq_no>
		<title><![CDATA[The whys, how tos, and pitfalls of user studies]]></title>
		<page_from>1</page_from>
		<page_to>205</page_to>
		<doi_number>10.1145/1667239.1667264</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1667264</url>
		<abstract>
			<par><![CDATA[<p>Members of the SIGGRAPH community are both consumers and producers of algorithms that make images and techniques that let us interact with visual applications. This course explains the essential role of user studies in insuring that algorithms, products, and content are effective for their intended purposes. The course introduces user studies through real examples and case studies that highlight good practices and warn of mistakes that can compromise evaluation. The cases are chosen to demonstrate the range of the application of user studies in computer graphics (for examle, in developing better algorithms and in evaluating images and interaction techniques.</p> <p>The first part of the course summarizes the different types of studies that are appropriate at different times during development of a user-interface technique. The second part uses examples from the evaluation of graphics to illustrate the different techniques that can be used to assess perceived image quality and preference. It includes some well-known techniques such as ranking and introduces novel concepts such as maximum-likelihood difference scaling.</p> <p>The third part reviews how eye tracking can support user studies. Eye tracking is useful as an interaction device in virtual environments, but it can also be a helpful tool in usability testing and evaluation of algorithms and techniques. It can collect and analyze additional data that can not be measured explicitly using questionnaires.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.0</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1797176</person_id>
				<author_profile_id><![CDATA[81100357979]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Veronica]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sundstedt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Graphics Vision and Visualization Group, Trinity College Dublin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797177</person_id>
				<author_profile_id><![CDATA[81100122627]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mary]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Whitton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill, Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1797178</person_id>
				<author_profile_id><![CDATA[81100187056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Marina]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bloj]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bradford]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>897961</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Arthur, K. W. (2000). <i>Effects of field of view on performance with head-mounted displays (CS Tech Rpt. # TR00-019).</i> Ph.D Dissertation, University of North Carolina, Technical Report #00--019.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Berg, B. L. (1989). <i>Qualitative Research Methods for the Social Sciences.</i> Boston: Allyn and Bacon.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Burns, E., Razzaque, S., Whitton, M.,&amp;Brooks, F. (2007). MACBETH: Management of Avatar Conflict by Employment of a Technique Hybrid. <i>International Journal of Virtual Reality, 6</i>(2), 11--20.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1544227</ref_obj_id>
				<ref_obj_pid>1544196</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Feasel, J., Whitton, M. C.,&amp;Wendt, J. D. (2008). LLCM-WIP: Low Latency, Continuous - Motion Walking-In-Place. <i>Proceedings of IEEE Symposium on 3D User Interfaces, 97--104.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Field, A.,&amp;Hole, G. (2003). <i>How to design and report experiments:</i> Sage Publishing.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618665</ref_obj_id>
				<ref_obj_pid>616061</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Gabbard, J., Hix, D.,&amp;Swan, E. (1999). User-centered design and evaluation of virtual environments. <i>IEEE Computer Graphics and Applications</i>, 51--59.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1150574</ref_obj_id>
				<ref_obj_pid>1150566</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Helms, J. W., Arthur, J. D., Hix, D.,&amp;Hartson, H. R. (2006). A field study of the Wheel--a usability engineering process model. <i>The Journal of Systems and Software, 79</i>, 841--858.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>835707</ref_obj_id>
				<ref_obj_pid>554230</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Hix, D., Swan, J., Gabbard, J., McGee, M., Durbin, J.,&amp;King, T. (1999). User-Centered Design and Evaluation of a Real-Time Battlefield Visualization Virtual Environment. <i>IEEE Virtual Reality 1999</i>, 96--103.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>835935</ref_obj_id>
				<ref_obj_pid>832289</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Hudson, T., Helser, A., Sonnenwald, D. H.,&amp;Whitton, M. (2003). Managing Collaboration in the Distributed nanoManipulator. <i>Proceedings of IEEE on Virtual Reality 2003.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>933178</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Insko, B. (2001). <i>Passive Haptics Significantly Enhances Virtual Environments (CS Technical Report #01--017).</i> Unpublished Ph.D Dissertation, University of North Carolina at Chapel Hill]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Martin, D. W. (2007). <i>Doing Psychology Experiments, 7th Ed.</i> Belmont, CA: Wadsworth Publishing.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566630</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Meehan, M., B. Insko, M. C. Whitton, and F. P. Brooks. (2002). Physiological Measures of Presence in Stressful Virtual Environments. <i>ACM Transactions on Graphics (Proceedings of SIGGRAPH 2002), 21</i>(3), 645--652.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>835964</ref_obj_id>
				<ref_obj_pid>832289</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Meehan, M., S. Razzaque, M. Whitton, F. Brooks. (2002). Effects of Latency on Presence in Stressful Virtual Environments. <i>IEEE Virtual Reality 2003, 1</i>, 141--148.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Meehan, M. S., Razzaque, S., Insko, B., Whitton, M. C.,&amp;Brooks, F. P. (2005). Review of Four Studies on the Use of Physilogical Reaction as a Measure of Presence in Stressful Virtual Environments. <i>Applied Physiological and Biofeedback., 30</i>(3), 239--258.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>210084</ref_obj_id>
				<ref_obj_pid>210079</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Slater, M., Usoh, M.,&amp;Steed, A. (1995). Taking Steps: The Influence of a Walking Technique on Presence in Virtual Reality. <i>ACM Transactions on Computer-Human Interaction, 2</i>(3), 201--219.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>772051</ref_obj_id>
				<ref_obj_pid>772047</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Sonnenwald, D., Whitton, M.,&amp;Maglaughlin, K. (2003). Evaluating a Scientific Collaboratory: Results of a Controlled Experiment. <i>ACM Transactions on Computer Human Interaction, 10</i>(2), 151--176.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1036125</ref_obj_id>
				<ref_obj_pid>1036118</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Sonnenwald, D. H., K. Maglaughlin, M. Whitton. (2004). Designing to Support Situational Awareness Across Distances: An Example from a Scientific Collaboratory. <i>Information Processing&amp;Management, 40</i>(6), 989--1011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>715784</ref_obj_id>
				<ref_obj_pid>647069</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Sonnenwald, D. H., Maglaughlin, K.,&amp;Whitton, M. (2001). Using innovation diffusion theory to guide collaboration technology evaluation: Work in progress <i>IEEE 10th Workshop on Enabling Technologies: Infrastrucrure for Collabrative Enterterprises.</i>, Video Paper.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Sonnenwald, D. H., R. Berquist, K. Maglaughlin, E. Kupstas Soon, and M. Whitton. (2001). Designing to Support Scientific Research Across Distances: the nanoManipulator Environment. In D. S. E. Churchill, and A. Munro (Ed.), <i>Collaborative Virtual Environments</i> (pp. 202--224). London: Springer Verlag.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311589</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Usoh, M., K. Arthur, M. C. Whitton, A. Steed, M. Slater and F. P. Brooks. (1999). Walking&gt;Virtual Walking&gt;Flying, in Virtual Environments. <i>Proceedings of ACM SIGGRAPH 1999 (Computer Graphics Annual Conference Series 1999)</i>, 359--364.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Welleck, S. (2002). <i>Testing statistical hypotheses of equivalence.</i> Boca Raton, FL: Chapmen&amp;Hall/CRC Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Whitton, M.,&amp;Brooks, F. (2008). Evaluating VE Component Technologies. In D. Nicholson, J. Cohn&amp;D. Schmorrow (Eds.), <i>Virtual Environments for Training and Education: Developments for the Military and Beyond</i> (Vol. 2, pp. 240--261). Westport, CN: Praeger Security International.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1079764</ref_obj_id>
				<ref_obj_pid>1078037</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Whitton, M. C., Cohn, J., Feasel, J., Zimmons, P., Razzaque, S., Poulton, S., et al. (2005). Comparing VE Locomotion Interfaces. <i>Proceedings of IEEE Virtual Reality 2005</i>, 123--130.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Zimmons, P. (2004). <i>The Influence of Lighting Quality on Presence and Task Performance in Virtual Environments (CS Tech Rpt. # TR04-017).</i> The University of North Carolina at Chapel Hill.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Charrier, C., Maloney, L. T., et al. (2007). "Maximum likelihood difference scaling of image quality in compression-degraded images." &#60;u&#62;Journal of the Optical Society of America A&#60;/u&#62; &#60;b&#62;24:&#60;/b&#62; 3814--3826.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Connah, D., Finlayson, G. D., et al. (2007). "Seeing beyond luminance: A psychophysical comparison of techniques for converting colour images to greyscale." &#60;u&#62;Fifteenth Color Imaging Conference: Color Science and Engineering Systems, Technologies, and Applications, Final Program and Proceedings:&#60;/u&#62; 336--341.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Engeldrum, P. G. (2000). &#60;u&#62;Psychometric Scaling: A Toolkit for Imaging Systems Development&#60;/u&#62; Winchester, Massachusetts, USA, Imcotek Press]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1036774</ref_obj_id>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Field, A. (2005). &#60;u&#62;Discovering Statistics Using SPSS&#60;/u&#62; Sage Publications Ltd.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Knoblauch, K. and Maloney, L. T. (2008). "MLDS: Maximum likelihood difference scaling in R." &#60;u&#62;Journal of Statistical Software&#60;/u&#62; &#60;b&#62;25&#60;/b&#62;(2): 1--26.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Maloney, L. T. and Yang, J. N. (2003). "Maximum likelihood difference scaling." &#60;u&#62;Journal of Vision&#60;/u&#62; &#60;b&#62;3&#60;/b&#62;: 573--585.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>197783</ref_obj_id>
				<ref_obj_pid>197765</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Scott, D. (1993). The visible differences predictor: an algorithm for the assessment of image fidelity. &#60;u&#62;Digital images and human vision&#60;/u&#62;, MIT Press: 179--206.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Aaltonen A., Introduction to Eye Tracking, Tampere University Computer Human Interaction Group, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882443</ref_obj_id>
				<ref_obj_pid>882404</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Cater K., Chalmers A., and Ward G., Detail to attention: exploiting visual tasks for selective rendering. In EGRW '03: Proceedings of the 14th Eurographics workshop on Rendering, pages 270--280, Aire-la-Ville, Switzerland, Switzerland, Eurographics Association, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[CHI Workshop, The Hunt for Usability: Tracking Eye Movements, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Duchowski A. T., A Breadth-First Survey of Eye Tracking Applications, Behavior Research Methods, Instruments, and Computers, Nov;34(4):455--70, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>640601</ref_obj_id>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Duchowski A. T., Eye Tracking Methodology: Theory and Practice, Springer-Verlag, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Duchowski, A. T., Eye Tracking Techniques for Perceptually Adaptive Graphics, ACM SIGGRAPH, EUROGRAPHICS Campfire, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Goldberg, H. J. and Kotval, X. P., Computer interface evaluation using eye movements: Methods and constructs. International Journal of Industrial Ergonomics, 24, 631--645, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Goldberg, H. J. and Wichansky, A. M., Eye tracking in usability evaluation: A practitioner's guide. In J. Hy&#246;n&#228;, R. Radach,&amp;H. Deubel (Eds.), The mind's eye: Cognitive and applied aspects of eye movement research (pp. 493--516). Amsterdam: Elsevier, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383695</ref_obj_id>
				<ref_obj_pid>2383654</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Gutierrez D., Munoz A., Anson O., and Seron F. J., Non-linear Volume Photon Mapping. In Proc. of the Eurographics Symposium on Rendering Techniques, pages 291--300, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[Hillaire S., Lecuyer A., Cozot R., and Casiez, G., Using an Eye-Tracking System to Improve Depth-of-Field Blur Effects and Camera Motions in Virtual Environments, Proceedings of IEEE Virtual Reality (VR) Reno, Nevada, USA, pp. 47--51, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1077406</ref_obj_id>
				<ref_obj_pid>1077399</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[Howlett S., Hamill J., and O'Sullivan C., Predicting and Evaluating Saliency for Simplified Polygonal Models. ACMTrans. Appl. Percept., 2(3):286--308, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[Isokoski P. and Beno&#238;t M., Eye tracker input in first person shooter games. The 2&#60;sup&#62;nd&#60;/sup&#62; Conference on Communication by Gaze Interaction COGAIN 2006: Gazing into the Future, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>297870</ref_obj_id>
				<ref_obj_pid>297843</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[Itti L., Koch C., and Niebur E., A Model of Saliency-Based Visual Attention for Rapid Scene Analysis. IEEE Trans. Pattern Anal. Mach. Intell., 20(11):1254--1259, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[Jacob, R. J. K. and Karn, K. S., Eye tracking in Human-Computer Interaction and usability research: Ready to deliver the promises, In J. Hy&#246;n&#228;, R. Radach,&amp;H. Deubel (Eds.), The mind's eye: Cognitive and applied aspects of eye movement research (pp. 573--605). Amsterdam: Elsevier, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97246</ref_obj_id>
				<ref_obj_pid>97243</ref_obj_pid>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[Jacob R. J. K., What you look at is what you get: eye movement-based interaction techniques, CHI '90: Proceedings of the SIGCHI conference on Human factors in computing systems, 11--18, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[James W., The Principles of Psychology. Dover Publications Inc., 1957.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[Just, M. A., and Carpenter, P. A., Eye fixations and cognitive processes. Cognitive Psychology, 8, 441--480, 1976.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[J&#246;nsson E., If looks could kill an evaluation of eye tracking in computer games. Masters Thesis, Royal Institute of Technology, Stockholm, Sweden, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[Karn K. S., Eye Tracking for Usability Testing, You've Got to Know Your Strengths and Weaknesses, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1315187</ref_obj_id>
				<ref_obj_pid>1315184</ref_obj_pid>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[Lee S., Kim G. J., and Choi S., Real-time tracking of visually attended objects in interactive virtual environments. In ACM Symp. on Virtual Reality Software and Technology, 29.38, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[Luebke D., Hallen B., Newfield D., and Watson B., Perceptually Driven Simplification Using Gaze-Directed Rendering. Tech. Rep. CS-2000-04, University of Virginia, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[Lukander K., Mobile usability: Measuring gaze point on handheld devices. Master's thesis, Helsinki University of Technology, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531361</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[McDonnell R., Larkin M., Hernandez B., Rudomin I., and O'Sullivan C., Eye-catching Crowds: Saliency based selective variation, ACM Transactions on Graphics (SIGGRAPH 2009), 28, (3), 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[MacEvoy B. The Visual Field. http://www.handprint.com, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[Majaranta P., Implementation of New Interaction Techniques: Eye Tracking, TAUCHI, Visual Interaction Research Group, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[Marmitt G. and Duchowski A. T., Modeling Visual Attention in VR: Measuring the Accuracy of Predicted Scanpaths. In Eurographics 2002, Short Presentations, pages 217--226. The Eurographics Association, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[Nahman, Using eye tracking for usability testing: a research note, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[Nielsen J., Alertbox, Why You Only Need to Test with 5 Users, March 19, 2000. http://www.useit.com/alertbox/20000319.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[Nilsson T., A Tobii Technology Introduction and Presentation, And How Tobii Eye Tracking Could be used in advertising, at Beyond AdAsia2007, Jeju Island, Korea, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[Noton D. and Stark L. W., Scanpath in Saccadic Eye Movements While Viewing and Recognizing Patterns. <i>Vision Research</i>, 11:929--942, 1971.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1077407</ref_obj_id>
				<ref_obj_pid>1077399</ref_obj_pid>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[O'Sullivan C., Collisions and Attention. ACM Trans. Appl. Percept., 2(3):309--321, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[O'Sullivan C., Dingliana J., Bradshaw G., and McNamara A., Eye-tracking for Interactive Computer Graphics, Abstract of Talk, The 11th European Conference on Eye Movements (ECEM 11), Turku, Finland, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[Palmer S. E., Vision science: Photons to phenomenology. MIT Press, Boston, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[Poole, A., Ball, L. J., and Phillips, P., In search of salience: A response time and eye movement analysis of bookmark recognition. In S. Fincher, P. Markopolous, D. Moore,&amp;R. Ruddle (Eds.), People and Computers XVIII-Design for Life: Proceedings of HCI 2004. London: Springer-Verlag Ltd., 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[Poole, A., Tips for Using Eyetrackers in HCI Experiments, Lancaster University, Lecture, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[Poole, A. and Ball, L. J., Eye Tracking in Human-Computer Interaction and Usability Research: Current Status and Future Prospects. In Ghaoui, Claude (Ed.), 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[Rayner, K., and Pollatsek, A., The psychology of reading. Englewood Cliffs, NJ: Prentice Hall, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[Rensink R. A., Visual Attention. In L. Nadel, editor, Encyclopedia of Cognitive Science, pages 509--515. Nature Publishing Group, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>968388</ref_obj_id>
				<ref_obj_pid>968363</ref_obj_pid>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[Rothkopf C. A. and Pelz J. B., Head movement estimation for wearable eye tracker. In ETRA '04: Proceedings of the 2004 symposium on Eye tracking research&amp;applications, pages 123--130, New York, NY, USA, ACM Press, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[R&#228;ih&#228; K-J., New Interaction Techniques. Course Notes, TAUCHI --- Tampere Unit for Computer-Human Interaction, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[Saloj&#228;rvi J., Puolam&#228;ki K., Simola J., Kovanen L., Kojo I., and Kaski S., Inferring Relevance from Eye Movements: Feature Extraction. Technical Report Report A82, Helsinki University of Technology, Espoo, Finland, March, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>355028</ref_obj_id>
				<ref_obj_pid>355017</ref_obj_pid>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[Salvucci D. D. and Goldberg J. H., Identifying fixations and saccades in eye-tracking protocols. In ETRA '00: Proceedings of the 2000 symposium on Eye tracking research&amp;applications, pages 71--78, New York, NY, USA, ACM Press, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>633452</ref_obj_id>
				<ref_obj_pid>633292</ref_obj_pid>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[Schnipke S. K. and Todd M. W., Trials and tribulations of using an eye-tracking system, CHI '00: CHI '00 extended abstracts on Human factors in computing systems, pp 273--274, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1178847</ref_obj_id>
				<ref_obj_pid>1178823</ref_obj_pid>
				<ref_seq_no>75</ref_seq_no>
				<ref_text><![CDATA[Smith J. D. and Graham T. C. N., Use of eye movements for video game control. ACE'06: Proceedings of the 2006 ACM SIGCHI international conference on Advances in computer entertainment technology, 20. (ACM Press), 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>76</ref_seq_no>
				<ref_text><![CDATA[Snowden R., Thompson P., and Troscianko T., Basic Vision: an introduction to visual perception. Oxford University Press, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>77</ref_seq_no>
				<ref_text><![CDATA[Spillers F., Eye-Tracking studies- Usability holy grail?, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>78</ref_seq_no>
				<ref_text><![CDATA[Strandvall T., Eye Tracking on the Web, Tobii Technology, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>79</ref_seq_no>
				<ref_text><![CDATA[Strandvall T., Tobii Technology, http://eyetracking.me/, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1090150</ref_obj_id>
				<ref_obj_pid>1090122</ref_obj_pid>
				<ref_seq_no>80</ref_seq_no>
				<ref_text><![CDATA[Sundstedt V., Debattista K., Longhurst P., Chalmers A., Troscianko T., Visual Attention for Efficient High-Fidelity Graphics, SCCG '05 - Spring Conference on Computer Graphics, pp 169--175, May, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1278389</ref_obj_id>
				<ref_obj_pid>1278387</ref_obj_pid>
				<ref_seq_no>81</ref_seq_no>
				<ref_text><![CDATA[Sundstedt V., Gutierrez D., Anson O., Banterle F., Chalmers A., Perceptual Rendering of Participating Media, TAP - ACM Transactions of Applied Perception 4(3), article 15, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>82</ref_seq_no>
				<ref_text><![CDATA[Sundstedt V., Rendering and Validation of High-Fidelity Graphics using Region-of-Interest, PhD Thesis, University of Bristol, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1394288</ref_obj_id>
				<ref_obj_pid>1394281</ref_obj_pid>
				<ref_seq_no>83</ref_seq_no>
				<ref_text><![CDATA[Sundstedt V., Stavrakis E., Wimmer M., and Reinhard E., A Psychophysical Study of Fixation Behavior in a Computer Game, APGV '08 - The 5th Symposium on Applied Perception in Graphics and Visualization, Los Angeles, California, Aug, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>84</ref_seq_no>
				<ref_text><![CDATA[Tobii. UserManual: Tobii Eye Tracker, ClearView analysis software, February, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>85</ref_seq_no>
				<ref_text><![CDATA[Tobii. Usability Testing Brochure: See through the eyes of the user, January, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>86</ref_seq_no>
				<ref_text><![CDATA[Vilis T., The Physiology of the Senses Transformations for Perception and Action. Course Notes, University of Western Ontario, Canada, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1400972</ref_obj_id>
				<ref_obj_pid>1400885</ref_obj_pid>
				<ref_seq_no>87</ref_seq_no>
				<ref_text><![CDATA[Wilcox T., Evans M., Pearce C., Pollard N., and Sundstedt V., Gaze and Voice Based Game Interaction: The Revenge of the Killer Penguins. ACM SIGGRAPH posters - The 35th International Conference on Computer Graphics and Interactive Techniques, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>88</ref_seq_no>
				<ref_text><![CDATA[Yarbus A. L., Eye Movements and Vision. Plenum Press, 1967.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383748</ref_obj_id>
				<ref_obj_pid>383745</ref_obj_pid>
				<ref_seq_no>89</ref_seq_no>
				<ref_text><![CDATA[Yee H., Pattanaik S., and Greenberg D. P., Spatiotemporal sensitivity and visual attention for efficient rendering of dynamic environments. ACM Trans. Graph., 20(1):39--65, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Whys, How Tos, and Pitfalls of User Studies Course Info SIGGRAPH 2009 Course Monday, 3 August Auditorium 
C 1:45 PM 5:30 PM Summary Statement If people are going to look at your images or use your techniques, 
you need to include user studies in your development process; come and find out how to design, execute, 
and analyze studies --and how to avoid common mistakes with practical examples from graphics, interaction, 
and eye tracking. Level of Difficulty Introductory Prerequisites Familiarity with concepts in experimental 
design is useful, but not necessary. Intended Audience This course is relevant for everyone who is currently 
or will be incorporating user studies as part of the development of their algorithms and techniques. 
The content is suitable for beginners or experienced delegates who want to learn about designing, executing, 
and analyzing user studies. Instructors Veronica Sundstedt Lecturer in Computer Science Graphics Vision 
and Visualization Group Department of Computer Science Lloyd Institute 0.45 Trinity College Dublin Ph: 
+353 1 896 8436 Em: Veronica.Sundstedt AT cs.tcd.ie https://www.cs.tcd.ie/Veronica.Sundstedt/ Veronica 
Sundstedt is a Lecturer in the Graphics, Vision, and Visualisation Group in the School of Computer Science 
and Statistics at Trinity College Dublin, Ireland. Previously, she was a Postdoctoral Research Associate 
in the Department of Computer Science at the University of Bristol and the University of Bath, UK. She 
holds a PhD in Computer Graphics from the University of Bristol and an M.Sc. in Media Technology from 
the University of Linköping, Sweden. Her research interests are in Veronica Sundstedt, Mary Whitton, 
Marina Bloj _ SIGGRAPH09 1 graphics and perception, in particular perceptually-based rendering techniques 
(using models of human visual attention), cross-modal interaction, experimental validation, and eye tracking 
techniques. Mary Whitton Research Associate Professor Department of Computer Science Campus Box 3175 
University of North Carolina at Chapel Hill Chapel Hill, NC 27699-3175 Ph: +1 919 962 1950 Em: whitton_at_cs_dot_unc_dot_edu 
http://www.cs.unc.edu/~whitton Mary C. Whitton is a research associate professor of computer science 
at the University of North Carolina at Chapel Hill. She co-leads the Effective Virtual Environments research 
group that investigates what makes virtual environment systems effective and develops techniques to make 
them more effective for applications such as simulation, training, and rehabilitation. Before joining 
UNC in 1994, she was co-founder of two companies that produced high-end hardware for graphics, imaging, 
and visualization.. Ms. Whitton has held leadership roles in ACM SIGGRAPH including serving as President 
1993-1995. She is a member of ACM, ACM SIGGRAPH, IEEE, and the IEEE Computer Society. Marina Bloj Senior 
Lecturer Bradford Optometry Colour and Lighting (BOCAL) Lab University of Bradford, Room G36 School of 
Life Sciences Ph: +44 1274 236258 Em: m.bloj AT brad.ac.uk http://www.brad.ac.uk/acad/lifesci/optometry/index.php/Staff/DrMarinaBloj 
Marina Bloj is a Senior Lecturer at the Bradford School of Optometry and Vision Sciences. Her main research 
interest lies in using non-invasive techniques such as psychophysics, eye-tracking and computational 
modelling to explore how humans perceive objects and material appearance with particular emphasis on 
colour perception. She conducts research with both real and computer simulated stimuli and has become 
more involved in trying to establish what constitutes a perceptually realistic rendered image and what 
are the technical and perceptual aspects that limit the realism of simulations. Course Description Members 
of the SIGGRAPH community are both consumers and producers of algorithms that make images and techniques 
that let us interact with visual applications. This course is for people who care that what they produce 
meets the users needs. Insuring that algorithms, products, and content are effective for their intended 
uses is the domain of user studies. We will introduce attendees to user studies through real examples 
and case studies that highlight good practices and warn of mistakes that can compromise evaluation studies. 
The cases are chosen to demonstrate the range of Veronica Sundstedt, Mary Whitton, Marina Bloj _ SIGGRAPH09 
2 the application of user studies in computer graphics, e.g., in the development of better algorithms 
and in the evaluation of our images and interaction techniques. After an introduction to why user studies 
are valuable, the first part of the course will present the different types of studies that are appropriate 
at different times during the development of a user interface technique. The second part of the course 
will use examples from the evaluation of graphics to illustrate the different techniques that can be 
used to assess perceived image quality and preference. We will cover some well known techniques such 
as ranking and introduce novel ones such as maximum likelihood difference scaling. Knowledge of human 
perception can drive the creation images and virtual environments. The third part of the course will 
discuss how eye tracking can support user studies. Apart from using eye tracking as an interaction device 
in virtual environments, it can also be a helpful tool in usability testing and evaluation of algorithms 
and techniques. We will describe experimental methodologies based on case studies in computer graphics. 
We will discuss what additional data we can collect and analyze using eye tracking, which could not have 
been measured explicitly using questionnaires. Tentative Syllabus (double session, 3.75 hours) 1:45 Introduction 
(all) Who are we? (our relevant background) Purpose of course and structure -Practical examples -Background 
and introduction to user studies What will be our take home messages: -Plan and pilot -Redesign, plan 
and pilot -Think early about your data analysis -It is an iterative process Audience questions and participation 
will be encouraged throughout! 2:30 User evaluation during development (Mary Whitton) User involvement 
during the development process -Waterfalls, circles, circles with eddies: models of development What 
do you need to know, when do you need to know it, and who can tell you? -What is it supposed to do? -You 
think it works, but does it? -Does it work for its intended task? -Does it work better than alternatives? 
3:15 Panel/questions 3:30 Coffee break 3:45 User studies for graphics (Marina Bloj) Humans as measuring 
devices. Why do user studies? How (not) to do user studies? (illustrated via case studies) Veronica Sundstedt, 
Mary Whitton, Marina Bloj _ SIGGRAPH09 3  -Sample selection (type, content, range, number)  -Task choice 
and specification (instructions)  -Observer selection  -Data analysis considerations 4:30 Eye tracking 
in user studies (Veronica Sundstedt) Why use eye tracking in user studies? What can we measure? Visual 
attention Measuring eye movements  -Eye tracking technology  -Eye movement analysis Experimental design 
Analysis and statistics Case studies: Eye tracking in computer graphics and interactive techniques Other 
application areas 5:15 Panel/questions 5:30 FINISH Slides/Viewgraphs Please see course notes and slides. 
Web URL For slide updates and additional material please see our course website: https://www.cs.tcd.ie/Veronica.Sundstedt/siggraph2009course.html 
http://www.siggraph.org/s2009/sessions/courses/details/?type=course&#38;id=17 Veronica Sundstedt, Mary 
Whitton, Marina Bloj _ SIGGRAPH09 4 The Whys, How Tos, and Pitfalls of User Studies SIGGRAPH 2009 Course 
Monday, 3 August Auditorium C 1:45 PM 5:30 PM  Course aims  Welcome  We want to communicate the 
need to include user studies in your development process  We will talk about issues related to designing, 
executing, and analysing studies  We will highlight best practice with practical examples from graphics, 
virtual environments, interaction, and eye tracking  Audience questions and participation will be encouraged 
throughout!  Course structure  1:45 Introduction (all)  2:30 User evaluation during development (Mary 
Whitton)  3:15 Panel/questions  3:30 Coffee break  3:45 User studies for graphics (Marina Bloj) 
 4:30 Eye tracking in user studies (Veronica Sundstedt)  5:15 Panel/questions  5:30 FINISH   Veronica 
Sundstedt Mary C. Whitton Marina Bloj Trinity College Dublin University of North Carolina University 
of Bradford at Chapel Hill  1. User Evaluation During Development Development Models Mary C. Whitton 
University of North Carolina at Chapel Hill  Preview: Take Home  Evaluation is not a one time thing: 
Early and Often  At different times during development  Different evaluation styles at different times 
 Different people to evaluate  Different evaluation tasks  Different measures and analysis   Always 
consider the user s needs and goals! Whitton S09  Development Models  1. What is it supposed to do? 
 2. How is it supposed to work?  3. You think it works, but does it?  4. Does it work for a real 
task?  5. Is it better than alternatives?  6. Recap and Wrap  Whitton S09 What do you need to know? 
 Who can tell you? How do you get them to tell you?   Development Models  Usability Engineering 
 SW Eng: Iterative and agile   Whitton S09 Development Models  Requirements   Design Verification 
 Maintenance time Whitton S09 Development Models   Big Design Up Front isn t practical requirements 
change  Unplanned return to earlier stages is expensive  No evaluation criteria for moving between 
stages  Lessons The earlier changes are made, the cheaper.  Evaluate early and often.  Whitton 
S09 Development Models  You better make that steel thicker because you ll never be able to paint it 
again after it is installed. Painter in a design review Whitton S09 Development Models  Objectives Evaluate 
Alternatives Alternatives Identify &#38; Resolve risks Constraints Plan Develop Next Verify Phase Test 
Figure after: Boehm: ACM Software Engineering Notes, 11(4), Aug 1986, p. 16. Whitton S09 Development 
Models  An important feature of the spiral model is that each cycle is completed by a review involving 
the primary people or organizations concerned with the product. This review covers all of the products 
developed during the previous cycle, including the plans for the next cycle and the resources required 
to carry them out. The major objective of the review is to ensure that all concerned parties are mutually 
committed to the approach to be taken for the next phase. ACM Software Engineering Notes, 11(4), Aug 
1986, p. 16. Whitton S09 Development Models  Whitton S09 Development Models  Start End   Whitton 
S09 Development Models  Whitton S09 Development Models  Work on small pieces of project; each cycle 
is short  User involvement throughout  Process anticipates and accommodates requirements changes  
Whitton S09 Development Models  Identifiable stages  Same for large projects and small functional 
development  Requirements, plan, implement, evaluate   Iterate on each function Begin next phase 
ONLY when everyone is committed  Stakeholder involvement in all stages  SW: users  HW: users, manufacturing, 
maintenance   Whitton S09 Development Models Topics  Development Models  1. What is it supposed 
to do?  2. How is it supposed to work?  3. You think it works, but does it?  4. Does it work for 
a real task?  5. Is it better than alternatives?  6. Recap and Wrap  Whitton S09  2. User Studies 
for Graphics Marina Bloj Bradford Optometry Colour and Lighting (BOCAL) Lab University of Bradford 
SIGGRAPH09  Further information  For introductory slides, updates, and additional material please see 
our course website:  https://www.cs.tcd.ie/Veronica.Sundstedt/siggraph2009course.html  Bloj S09  
  Combination of two main processes  Bottom-up and top-down visual search  Models of visual attention 
 Task maps, saliency maps, importance maps etc. Sundstedt S09  Eye tracking allows us to determine 
where an observers gaze is focused at a given time Example eye movements from an observer free-viewing 
and performing a counting task. Sundstedt S09  Images used with permission from Springer Science and 
Business Media. Sundstedt S09 Corridor scene: (left) high quality and (right) saliency map using the 
algorithm by Itti et al. (1998). Sundstedt S09  Certain questions are difficult to answer with traditional 
techniques (Nahman 2001)  Time for task completion  Participant success rate  Subjective ratings 
  A questionnaire afterwards or think-aloud protocol during the experiment might not be appropriate 
or sufficient  Sundstedt S09  Support traditional data  Measuring human performance  Evaluating 
human interaction  Testing usability  Measuring importance (e.g. attention)  Measuring cognitive 
processes  Understanding task behaviour  Explaining individual differences  Developing new metrics 
and design principles  Making algorithms more efficient  Evaluating perceptually adaptive graphics 
 Optimising perceived quality  Enhancing the user experience  Constructing new research questions 
 Sundstedt S09 Sundstedt S09  Eye tracking in user studies  What is eye tracking?  Measuring eye 
movements  Eye tracking technology  Eye movement analysis   Experimental design  What can we measure? 
 Analysis and statistics   Case studies  Diagnostic uses of eye tracking  Interactive uses of eye 
tracking   Sundstedt S09  Preview: Take home  Eye tracking can add important objective information 
 Take the human observer into account in the design  Knowledge of eye movements can be used to improve 
our algorithms and techniques  Eye tracking can also be used as an alternative means of interaction 
 Plan your study, pilot your setup, minimise all risks that can affect your data recording, analyse 
your data  Sundstedt S09  Plan and pilot  Redesign, plan and pilot  Think early about your data 
analysis  It is an iterative process  Audience questions and participation is encouraged throughout! 
  User Evaluation During Development Mary C. Whitton University of North Carolina at Chapel Hill  
Whitton S09 2 Take Home  Evaluation is not a one time thing: Early and Often  At different times during 
development  Different evaluation styles at different times  Different people to evaluate  Different 
evaluation tasks  Different measures and analysis    Topics  Development Models: Waterfall, spiral; 
iterative and agile Objectives Evaluate 1. What is it supposed to do?  2. How is it supposed to work? 
 3. You think it works, but does it?  Develop Verify 4. Does it work for a real task? Test  5. Is 
it better than alternatives?  6. Recap and Wrap  Whitton S09 3 Whitton S09 4 Topics  Development 
Models: Waterfall, spiral; iterative and agile  1. What is it supposed to do?  What do you need 2. 
How is it supposed to work? to know? 3. You think it works, but does it? Who can tell you? 4. Does 
it work for a real task? How do you get 5. Is it better than alternatives? them to tell you? 6. Recap 
and Wrap  1. What is it supposed to do? Bright ideas &#38; user problems  Requirements  Whitton S09 
5 (1) Requirements: What is it supposed to do?  Whitton S09 6 Which comes first ? What s the problem 
my bright idea solves? Find someone who cares about your solution  Who has a really hard problem and 
needs you to help solve it?  Find someone with a problem you can solve Lesson Best if all parties benefit; 
Collaboration (1) Requirements: What is it supposed to do?  Me? Talk to people?  Observation  Interviews 
 Focus Groups  Iterate  Lessons  Learn to talk their talk (at least a little)  Technologists CAN 
learn interview skills  Whitton S09 7 (1) Requirements: What is it supposed to do?  Whitton S09 8 
Then what?  Synthesize requirements  Functionality  User interface   Collect data for later evaluation 
  Lessons Keep probing. Users explain problems based on their understanding of technology  Technologists 
are good at synthesis  (1) Requirements: What is it supposed to do?  Case: Collaborative nM What features? 
 Whitton S09 9 Whitton S09 10 How did we find out? Ethnographic Study  9 sessions observing scientists 
using nM  27 interviews with scientists  Experts at the single-user nM  Potential users of distributed 
nM   Methods  Observation  Semi-structured interviews; Critical incident interviews  Iterative 
  (1) Requirements: What is it supposed to do?  Whitton S09 11 S 1: Requirements Evaluation What you 
need to know? Have you properly understood the problem and the requirements?  Who reviews requirements 
document? Collaborators, other domain experts and potential users  Hint: Keep good records; expect 
requirements to change  (1) Requirements: What is it supposed to do?  Whitton S09 12 Aside: Product 
adoption Widely and quickly adopted products have these characteristics: Relative advantage Better than 
current solution Observability Value of use obvious to others Complexity Not hard to learn, use Compatibility 
Like current practices, morés Trialability Can try with low risk &#38; cost Diffusion of Innovation, 
5th Edition Everett M. Rogers (1) Requirements: What is it supposed to do?  2. How is it supposed to 
work?  Requirements &#38; Ideas A plan, but few details  Iterative Development  Partition the problem 
 Design to meet requirements  Hint: Record rationale of design decisions   Evaluate designs  Whitton 
S09 13 (2) Analysis and Design: How is it supposed to work?  Whitton S09 14 S 2: How is it supposed 
to work?  What you need to know  Does design use current best practices?  Will it meet functional 
and usability requirements?   Who can tell you  Literature; existing products; domain experts  Collaborators, 
potential users   How do you do evaluation?  Design reviews: mock ups, pseudo code, ..  Listen; keep 
records and write reports   (2) Analysis and Design: How is it supposed to work?  Whitton S09 15 3. 
Does it work? Test/Evaluate at frequent milestones  Exercise all functionality the new and the old 
 Sometimes informal; always rigorous  Keep records; write reports  What works; what doesn t work  
Suggestions, comments, requirements changes   (3) Implement, Test, and Iterate: Does it work?  Virtual 
Locomotion For HMD users, what is the best virtual­locomotion technique for training people who are 
moving on foot? Whitton S09 16 (3) Implement, Test, and Iterate: Does it work?  Virtual Locomotion 
 Task: Develop a good implementation of walking-in-place Whitton S09 17 (3) Implement, Test, and Iterate: 
Does it work?  Whitton S09 18 Case: Walking-in-Place Version 1: Neural Network (NN) Detect footsteps 
by patterns in head tracker reports  Custom NN:10-20 minutes of walking per person  Single NN: train 
with one person, use same for all   Person who trained the NN has distinctive, fluid gait  Impossible 
for some to mimic  Lesson If at first you don t succeed, try again. (3) Implement, Test, and Iterate: 
Does it work?  Whitton S09 19 Case: Walking-in-Place  Version 2: Accelerometer mounted on HMD Detect 
footsteps from peaks in acceleration  Great variation in peak accelerations  Slim, heavy need to adjust 
parameters  Hair under HMD damped shock at accelerometer  Î Some users could not make it work Î Almost 
no one liked it  Lesson If at first you don t succeed, try, try again. (3) Implement, Test, and Iterate: 
Does it work?  Case: Walking-in-Place Version 3: Trackers on knees Detect footsteps from velocity of 
feet  Smoothest motion to date  Most could do it; No one really hated it  Good enough   Lesson 
Train to competence in environment as difficult as the test environments. Whitton S09 20 (3) Implement, 
Test, and Iterate: Does it work?  Case: Walking-in-Place Version 4: Trackers on shins Detect footsteps 
from position and velocity of feet  V 0.1 expected knees to be raised high  Failed for first two (friendly) 
testers  Lessons  Learn characteristics of input data in requirements phase, not later.  Don t test 
on only one person during development.  Whitton S09 21 (3) Implement, Test, and Iterate: Does it work? 
 Whitton S09 22 S 3: Does it work? What you need to know?  Does it work properly every time?  Does 
it work for a variety of people?   Who can tell you? Collaborators, friendly users  How do you get 
them to tell you?  Test/Eval sessions with their data or tasks (3) Implement, Test, and Iterate: Does 
it work?  Whitton S09 23 4. Can users do their tasks? Test with a typical use case Integrated functionality 
 Formal user study (Ethics/IRB approval)  Participants: Target users  Performance metrics: speed, 
accuracy  Usability metrics: keystroke logs, questionnaires, interviews   (4) Formative Evaluation: 
Can users do their tasks?  Whitton S09 24 Case: Distributed nM  What is it? New UI for nM; enable remote 
collaboration  How is it supposed to work? Weekly team reviews  Does it work? Frequent testing shared 
lab space  Local collaboration: Beta SW deployed to laboratory  Remote collaboration:   Pilots for 
the summative evaluation verified usability (4) Formative Evaluation: Can users do their tasks?  Whitton 
S09 25 Case: Distributed nM NIH Question: good science when remote?  Concurrently ran Formative and 
Summative Studies  Q1: Can the task be done when not co-located? (Formative)  Q2: Compare to gold-standard 
of collaborating side­by-side (Summative)   (4) Formative Evaluation: Can users do their tasks?  Whitton 
S09 26 Case: Distributed nM Task/Use Case: Actual research tasks redesigned as lab sessions  Measures: 
lab reports, questionnaires, observations, interviews  Compromises: participants, tasks, metrics, not 
full 2x2 design  (4) Formative Evaluation: Can users do their tasks?  Level of Effort 20 pairs of 
participants 3 sessions of ~ 7 hours each  Two rooms, four computers, two Phantoms, 4 video recorders, 
2 audio recorders  4-5 grad students + 2 faculty  Study: 9 months; Project: 4 years   Whitton S09 
27 (4) Formative Evaluation: Can users do their tasks?  Whitton S09 28 Distributed nM Study Findings 
 No statistically significant difference in lab scores between co-located and remote pairs. Confounder: 
relative difficulty of the two lab tasks  Observations and interviews revealed  Good and bad features 
of face-to-face &#38; remote; work-arounds  Remote enabled divide and conquer and exclusive use  Video 
conferencing (head shots) seldom used   (4) Formative Evaluation: Can users do their tasks?  Case: 
Distributed nM  Lessons Compromises may be necessary; report them  Pilot, including statistics  Null 
statistical results can still be valuable; use qualitative data to help explain quantitative results 
 Whitton S09 29 (4) Formative Evaluation: Can users do their tasks?  Whitton S09 30 S 4: Can users 
do their tasks? What you need to know Does it work for the Use Case?  Who can tell you? Unbiased 
users  How do you get them to tell you?  Formal user study; Task performance and usability measures 
(4) Formative Evaluation: Can users do their tasks?  Whitton S09 31 5. Is it better than alternatives? 
 Formal study: Use case task  Independent variables: two or more ways to do the task  Dependent variables: 
performance, usability  Hypothesis driven; inferential statistics   Study design issues  Ecological 
validity  Logistical complexity  Access to target users   (5) Summative Evaluation: Is it better 
than alternatives?  Case: Comparing Locomotion Techniques Ambitious Study Goals:  Compare 5 conditions 
  Do people learn in VE?  Does training transfer from VE to real world??  (Some) ecological validity 
of task for military funders   Whitton S09 32 (5) Summative Evaluation: Is it better than alternatives? 
 UNC Locomotion Study #5  UNC-Loco5: Users maneuver through building rubble while minimizing exposure 
to gunfire (primary task) and counting the number of jets and explosions (secondary). Task Move between 
barriers Count explosions and planes  Measures Exposure to gunfire Accuracy of counts  Compares training 
outcome and locomotion interfaces  Pre-post test; between-subjects Training in 1 of 5 conditions Whitton 
S09 33 (5) Summative Evaluation: Is it better than alternatives?  UNC Locomotion Study #5 Task Move 
between barriers Count explosions and planes  Measures Exposure to gunfire Accuracy of counts  Compares 
training outcome and locomotion interfaces  Pre-post test; between-subjects Training in 1 of 5 conditions 
UNC-Loco5: Users maneuver through building rubble while minimizing exposure to gunfire (primary task) 
and counting the number of jets and explosions (secondary). (5) Summative Evaluation: Is it better than 
alternatives? Whitton S09 34 UNC Locomotion Study #5  High visuo-motor control demands Look around 
(and down) Maneuver  Improved walking-in-place and joystick techniques Fair comparisons  More distracters 
 Loud sounds  Audio-based secondary task   UNC-Loco5: Users maneuver through building rubble while 
minimizing exposure to gunfire (primary task) and counting the number of jets and explosions (secondary). 
Whitton S09 35 (5) Summative Evaluation: Is it better than alternatives?   Whitton S09 36 0,10 Whitton 
S09 37  Whitton S09 39 Success against goals?  8 participants in each of 5 conditions: Real, Cowl, 
VEWalk, JS, WIP  Comparing WIP and JS  Still unable to tease apart on locomotion metric (Exposure to 
gunfire)  WIP worse than other four on cognitive task (Counting sounds)   (5) Summative Evaluation: 
Is it better than alternatives?  Whitton S09 40 Success against goals? Do people learn?  Real-walking 
conditions indistinguishable over trials  WIP and JS improved: Learn task? Learn interface?  Does 
training transfer to Real World? Unproven.  Fatigue, design lowered post-training scores?  Declare 
victory since they were not proven worse?   (5) Summative Evaluation: Is it better than alternatives? 
 Whitton S09 41 Lessons from Loco5 Don t underestimate the effort; avoid burnout  Complexity from 
 Ecological validity  Stimulus Trials: different but ~ same difficulty  Passive haptics   Train 
to competence in conditions like those of test task  Number of subjects; length of session  (5) Summative 
Evaluation: Is it better than alternatives?  Whitton S09 42 S 5: Is it better than alternatives? What 
you need to know How your method compares to existing methods  Who can tell you? Unbiased target user 
population  How do you get them to tell you? Formal user study  Independent variables: different methods 
of doing task  Dependent variables: task performance, usability, ..   (5) Summative Evaluation: Is 
it better than alternatives?   Recap and Wrap  Whitton S09 44 Evaluation is not a one time thing 
Establish and review Requirements, v.1  Test and iterate until element works  Test and iterate as integrate 
elements into whole  Test against use case  Compare to alternatives  (6) Recap and Wrap  Whitton 
S09 45 Get user feedback early and often At different stages of development Different evaluation styles 
at different times  Different people to do evaluation  Different evaluation tasks  Different evaluation 
measures and analysis  (6) Recap and Wrap  Whitton S09 46 Get user feedback early and often  Different 
evaluation styles at different times  Observation, Interviews, Focus Groups  Rigorous reviews: of plans; 
trying it out  Formal user studies  Formative: performance on use case  Summative: compare performance 
between methods    Different people to do evaluation  Different evaluation tasks  Different evaluation 
measures and analysis  (6) Recap and Wrap  Whitton S09 47 Get user feedback early and often  Different 
evaluation styles at different times  Different people to do evaluation  Collaborators, friendly users 
 Unbiased target users   Different evaluation tasks  Different evaluation measures and analysis 
 (6) Recap and Wrap  Whitton S09 48 Get user feedback early and often Different evaluation styles 
at different times  Different people to do evaluation  Different evaluation tasks  Review and comment 
on requirements, design plan  Testing stand-alone or partially integrated functionality  Test completed 
product with use-case task   Different evaluation measures and analysis  (6) Recap and Wrap  Whitton 
S09 49 Get user feedback early and often Different evaluation styles at different times  Different 
people to do evaluation  Different evaluation tasks  Different evaluation measures and analysis  Expert 
opinions; logs, observations; speed and accuracy of performance  Descriptive statistics; Inferential 
statistics   (6) Recap and Wrap  User Evaluation During Development Early and Often Acknowledgements 
 The Effective Virtual Environments Team at UNC and EVE Co-PI, Fred Brooks  Funding  Office of Naval 
Research (VIRTE Project)  NIH National Institute of Biomedical Imaging and Bioengineering  SAIC, Link 
Foundation, NC Space Grant   User Evaluation during Development Lessons Learned from Ten Years of Studies 
of Virtual Environments1 Mary C. Whitton, Effective Virtual Environments Research Group University of 
North Carolina at Chapel Hill In 1997 Fred Brooks came back from a sabbatical spent in London (UK) with 
an idea for an extension of a user study performed at University College London in Mel Slater s laboratory 
(Slater, Usoh, &#38; Steed, 1995). In 1998 we performed that study to evaluate how locomotion technique 
influences sense of presence (Usoh, 1999). That work was the first in a thread of research studies examining 
how a variety of technologies influence the effectiveness of virtual environments. This paper reports 
lessons the Effective Virtual Environments (EVE) research team learned while doing a dozen or more studies 
and lessons learned by another, cross-disciplinary team in the Distributed nanoManipulator project. In 
that project we designed, implemented, and evaluated a tool for distributed scientific collaboration 
(D. H. Sonnenwald, R. Berquist, K. Maglaughlin, E. Kupstas Soo, and M. Whitton, 2001), (Hudson, Helser, 
Sonnenwald, &#38; Whitton, 2003), and (D. H. Sonnenwald, K. Maglaughlin, M. Whitton, 2004). The lessons 
are presented as free of the context of a specific study as possible, but examples from particular studies 
and references to published works, are included. The lessons are not intended to exhaustively cover all 
the issues that arise in designing and executing a user study. A useful primer in experimental design 
is Doing Psychology Experiments (Martin, 2007). 1. Know the question you re trying to answer Too often 
people perform an evaluation study without having first determined what question they are trying to answer 
and what hypothesis they will test to answer the question. In the extreme, this can result in nothing 
being learned and effort wasted. Different questions and hypotheses are appropriate at different stages 
of project development (Gabbard, Hix, &#38; Swan, 1999). Are you doing an early stage requirements study? 
A design review before implementation starts? A functionality and usability review of small to medium 
sized work products? A functionality and usability evaluation of an integrated system? Or are you comparing 
your method to other established methods of doing the same thing either to establish the validity of 
your method or to demonstrate the superiority of your method? Additional references: (Hix et al., 1999) 
and (Helms, Arthur, Hix, &#38; Hartson, 2006). One way to focus yourself on clearly defining your question 
and hypotheses is to, before you go any further, write the abstract of the your paper/report of this 
study as if you had already completed the study and everything went perfectly. 1 While my presentation 
User Evaluation During Development is organized by the types of evaluation done in the process of developing 
a product or technique, the lessons in these notes are organized around and focus on lessons learned 
in designing, planning, and executing user studies.. Mary Whitton_SIGGRAPH09 2. Designing your Experiment 
Reuse experimental procedures and measures. Re-use, or minimally modify, experimental methods, measures, 
and analysis protocols from published works. The methods have already been vetted by publication reviewers, 
and it makes it easier to compare work across studies. 2.1.Basic Design 2.1.1. How many independent 
variables and conditions? It is temping to study everything at once, but adding conditions has impact 
on the statistics you run, on how many participants you must have, on how long a participant session 
is, and how long it will take you to complete the evaluation project. Logistics--how long does it take. 
When possible, strive for within-subjects designs that expose all participants to all conditions. In 
our locomotion study reported in 2005 (M. C. Whitton et al., 2005) participants were able rate and rank 
all five virtual locomotion interface techniques because each had experienced all five. In a subsequent 
study, the length of sessions dictated that each participant experience only one of the five conditions. 
That limited our ability to use participant comments to make sharp distinctions among the conditions. 
The formative evaluation of the Distributed nanoManipulator (see course slides, Sections 1 and 4) involved 
participants in only two of the four possible conditions in a full 2 X 2 study design (D. Sonnenwald, 
Whitton, &#38; Maglaughlin, 2003). Had we had participants who did both labs face-to-face and both labs 
distributed, we could have eliminated any difference in difficulty of the two laboratory tasks as a confounding 
factor. Did it have an influence on the difference of scores between the first and second sessions? We 
re unable to tell from the data we have. Including the other two conditions in our work would have, however, 
required twice as many pairs of participants and another 6-8 months. Face-to-face first Distributed first 
Face-to-face second X Distributed second X The evaluation of the distributed nanoManipulator used only 
two of the four possible condition pairs. Between-subjects or Within-Subjects (repeated measures) or 
Mixed? Many considerations go into the decision of whether to run a between-subjects or a within-subjects 
design. Factors to look for include learning effects doing better on later trials because you ve learned 
something; order effects it is easier (or harder) to do a task after you ve completed some other condition. 
The statistics become somewhat more complex with mixed designs: some variables are between subjects, 
and some are within. An example of a mixed design is our Locomotion study #5 (Loco5): the locomotion 
condition was between-subjects (due to the length of time the study took, all participants couldn t do 
all conditions) and was within- Mary Whitton_SIGGRAPH09 subjects for the task performance scores of exposure 
to gunfire and counts of jets and explosions. 2.1.2. Participants, Multiple sessions, Compensation The 
traditional participant pool of undergraduate Psychology students may or may not be available to you. 
Our computer science students have had access to them when taking the Experimental Design course in the 
Psychology Department and, occasionally, when their doctoral committee has included a professor from 
Psychology. Don t assume you can use this pool. Also, be aware that tapping this pool early in the semester 
will bring you a different type of participant than late in the semester. Early in the semester you ll 
attract participants who are organized and motivated to complete their requirement: later the population 
may be dominated by less motivated students. Professionals as Study Participants. With funding from NIH 
we performed a multi-year, multi-faceted project to develop and evaluate of an instrument to enable scientific 
collaboration over the internet. The product is called the Distributed nanoManipulator (Dist nM). The 
question was whether good scientific collaboration could be done when the collaborators were not located 
together in the same laboratory. As conceived, study participants were to have been the system s target 
users graduate research assistants, post-docs, and working scientists. We quickly realized we were unlikely 
to find forty of that population willing to participate in a study requiring eight hours over three different 
days. Our decision to use undergraduate students broadened the participant pool, but constrained the 
sophistication of the science lab tasks (D. Sonnenwald et al., 2003). Active military as study participants. 
Using military personnel as study participants may require review by the military human-subjects protection 
organization. This includes ROTC cadets as they are considered active duty military personnel. It is 
hard to get people to come back. Require only one session with each participant if at all possible. It 
is often difficult to get volunteer or minimally-compensated ($7-$10/hour) participants to return to 
the lab for the multiple sessions that studying, for instance, training retention requires. Expect to 
offer larger incentives for multi-session studies. To incent participants to complete the study, you 
can withhold most of the payment until after the final session. 2.2.Conditions:Choose Wisely 2.2.1. 
Independent Variables Pragmatism 1. Select the levels of the independent variable carefully, balancing 
the number of conditions and the number of research questions with the reality of study design complexity 
and number of participants required. For reasons of expediency, in a study the value of passive haptics 
for training a maze task (Insko, 2001) did not include a condition exposing participants simultaneously 
to passive haptics and synthetic audio and visual cues indicating collisions of the user s hands with 
the maze. Had he included this additional condition, he could have examined the questions of whether 
using all cues together would result in even better real-maze performance than with passive haptics alone, 
and whether Mary Whitton_SIGGRAPH09 training with the audio tones, clearly absent in the real world, 
would, in fact, mistrain and lead to poorer performance. This is the perennial "training wheels" question 
for all simulation-based training. Pragmatism 2. In 2002 we ran a user study while demonstrating our 
virtual environment system to guests in the Emerging Technologies venue at SIGGRAPH. Our independent 
variable was the amount of end-to-end latency in the system (M. Meehan, S. Razzaque, M. Whitton, F. Brooks, 
2002). Pragmatism helped us choose both our high-and low-latency values. Although we could achieve 40 
ms latency with our best hardware, we chose 50 ms as the low-latency condition in case we had an equipment 
failure and had to continue with less capable hardware. A goal for the exhibition was that every participant 
have a very good VE experience, so we selected a high latency value, 90 ms, that is 10% less than 100 
ms, a generally accepted upper bound for interactive systems.  2.2.3. Dependent Variables Compare to 
a Gold Standard. It is helpful to include an experimental condition that compares the component or technique 
being evaluated to a similar, familiar component. The wide-FOV HMD used in (Arthur, 2000) is radically 
different from the commercial Virtual Research V8: It has six liquid crystal display panels per eye, 
it weighs twice as much as the V8, and brightness and contrast vary across the display panels. The data 
gathered for participants using the V8, and in a condition where the their natural eyesight was field-of­view 
restricted, gave us confidence that the data we collected for wide-FOV HMD users were reasonable. Baseline 
and delta-from-baseline. A series of studies led by Meehan and Insko (M. Meehan, B. Insko, M.C. Whitton, 
and F.P. Brooks, 2002) , (M. Meehan, S. Razzaque, M. Whitton, F. Brooks, 2002), and (Meehan, Razzaque, 
Insko, Whitton, &#38; Brooks, 2005), used the physiological measures of rise in heart rate, skin temperature, 
and skin conductivity (sweat) as dependent variables. In this series of studies we were looking for stress 
responses when users experienced the virtual-cliff environment that we call the PIT. In each study, baseline 
values for the physiological measures were gathered in the normally-floored, Mary Whitton_SIGGRAPH09 
low-stress ante-room of the environment and gathered again in the high-stress PIT room. Heart rate was 
the most successful for us. Because baseline heart-rate varies widely between people, the standard deviation 
of the sampled values was large, making statistical significance difficult to achieve. The alternative 
was to make the dependent variable the difference in heart rate between the two rooms. This delta-heart-rate 
variable varies much less across the participant population. Dependent variable s ability to discriminate 
among conditions. Always pilot test your study to ensure that the measured values of the dependent variables 
will discriminate among conditions. Insufficient pilot testing before a study turned a summer s work 
into an overly elaborate pilot test for a later study. The pilot test showed our task was too easy: Participants 
in every condition performed well; they were able to move through the virtual environment and successfully 
hide behind barriers. In the next iteration of the study (described in my course slides for Section 5 
Summative Evaluation) we added a distracter cognitive task, reasoning that if people were counting explosions 
and jets flying overhead, there would be fewer mental resources available for moving through the environment 
and hiding behind pillars to avoid being shot at. This strategy was generally successful, but if we were 
doing it again we would make the task even more difficult. If a relevant taxonomy of tasks is available, 
e.g., Bloom s taxonomy of cognitive tasks, consult it when choosing tasks. Eliminate Potential Confounders. 
Eliminate, or at least mitigate, confounders conditions of the experiment that are uncontrolled and may 
influence the outcomes by carefully considering all aspects of your stimuli and evaluation set-up. The 
virtual environment for the Zimmons visual search task was very simple a room with a table and a picture 
frame and was easy to develop. However, development of the stimulus models and images was complex and 
time-consuming because lighting, brightness, and colors had to be matched in order that inadvertent differences 
in them would not confound the results (Zimmons, 2004). Study designs usually demand compromises. In 
the Distributed nanoManipulator evaluation (Sonnenwald 2003), the metrics for the quality of science 
would, ideally, been long-term measures such as number and quality of papers and grants that result from 
the work. The study required short-term measures that were plausibly related to scientific quality: participants 
each wrote a report of their laboratory work and results. The lab reports were graded using a rubric 
developed iteratively by three graders scoring a subset of the lab reports. One grader, using the rubric, 
completed the grading. 2.3. Experimental Tasks 2.3.1. Ecological Validity Ecological validity is how 
well your study conditions mimic the environment in with the item under test will really be used. Study 
designers should consult with their target users and collaborators and engage them in defining experimental 
tasks. This will insure both that the task is a reasonable one and that it is as much like a real use 
situation as possible. Ecological validity is easy to achieve for an algorithm that will be used in a 
hospital image analysis center; it much more difficult to achieve in a virtual environment, particularly 
for tasks that would normally be done outdoors. Mary Whitton_SIGGRAPH09 Training Transfer Studies. Training-transfer 
studies are particularly difficult because they require a real condition. The laboratory environment 
imposes space and other limitations on the real condition, resulting in low ecological validity. There 
is not yet enough literature to enable us to define how generalizable laboratory training-transfer study 
results are to real-world training. Evaluate using real collaborator-supplied data and use cases. First, 
this makes the evaluation fair, and secondly, subject to the limits of ecological validity in a lab setting, 
shields you from the accusation of working on a toy problem. A third, and major, advantage is that if 
you are using your collaborator s data, they will be much more engaged in the process. 2.4. Analysis 
Designing the analysis procedures for studies is always one that is difficult because many of us doing 
studies are not expert in sophisticated statistical methods. Available statistics courses tend to be 
totally applied or totally mathematically oriented neither of which is satisfying for a technologist 
who needs to know how to use the tests, but is also capable of understanding the mathematics! Field and 
Hole (Field &#38; Hole, 2003) is a good introduction that blends material on the application of statistics 
with some of the mathematics. What is the proper statistical test? Do not expect all statistical analyses 
to be as simple as t­tests and ANOVAs. The experimental design may dictate more sophisticated techniques 
than those learned in a first statistics or experimental design course. In a recent locomotion study, 
the complexity increased unexpected when we found that the exposure-to-gunfire data did not meet the 
normality criteria required for use of parametric techniques. Beginning statistics courses don t teach 
you about setting up tests for mixed-model, non­parametric data. What can go wrong will. We unexpectedly 
added complexity to the data analysis of the locomotion study reported in (Whitton 2005) because the 
path segments the participants walked were not all the same length. The consequence was that the data 
from the different segments could not be naively combined in a repeated-measures analysis. Use on-campus 
statistics consulting services. We are fortunate to have an on-campus consulting service whose mission 
is to assist in developing the analysis component of studies. Make use of such a service if it is available. 
Expert advice while you are developing the study helps ensure that you will be able to answer our research 
questions with the hypotheses, study design, and analysis you have planned. Do you think you need a new 
measurement tool? Avoid developing a completely new measurement tool if you can. Developing new measurement 
tools and validating them is complex. Seek outside expertise. A center offering consulting on statistics 
will often also help with measurement tool development. We did develop a new questionnaire as part of 
the Distributed nanoManipulator project and used it in our summative study (D. H. Sonnenwald, Maglaughlin, 
&#38; Whitton, 2001) and (D. Sonnenwald et al., 2003). Mary Whitton_SIGGRAPH09 Colleagues in psychology 
departments frequently know of already existing standard tests for things such as baseline 3D spatial 
reasoning ability that you may be interested in as baseline characteristics of your participant population. 
3. Ethics Committees protecting human subjects The protection of the health and rights of human subjects 
participating in controlled studies is the job of what is called the Institutional Review Board in the 
United States and the Ethics Committee in the European Union. Persons proposing to run studies apply 
to the IRB for approval to do the work. In the United States, any research done with human subjects is 
not publishable unless the study has IRB approval. Design the study before you write the application. 
Preparing an application for ethics committee approval of a study has a reputation for being painful. 
My observation is that this is because people begin filling out the form before they have designed their 
study. People use the application form, with its systematic series of questions about study purpose, 
hypotheses, methods, materials, consent forms, etc. as their experiment design template. While that is 
one strategy for designing a study, it gives the ethics committee a bad reputation: Just because designing 
a study it hard work, there is no reason to blame the ethics board for it. Design your study; then write 
the application. Get to know your Ethics Committee. We have developed and maintain a good working relationship 
with the IRB at the University of North Carolina. The UNC IRB has, over the years, become familiar with 
our work and the precautions we take to ensure participant safety. The good relationship worked to our 
advantage when we sought permission to run the Latency study at SIGGRAPH 2002 (M. Meehan, S. Razzaque, 
M. Whitton, F. Brooks, 2002). Although the study locale was quite unusual, getting IRB approval for the 
exhibition­based study was straightforward. 4. Planning and Piloting 4.1. It will be harder than you 
think Don t underestimate the space, equipment, programming, modeling, logistical, time, and management 
resources required to design, implement, and execute studies, particularly if you are striving for ecological 
validity. Just the paper design of the four virtual scenes for the Loco5 study took well over 80 hours. 
The layouts were constrained by analysis requirements, available building blocks for passive haptics, 
cable management issues, the need to be able to switch from one physical (passive haptics) environment 
to another in three to four minutes, and the need that they be of comparable difficulty. Large, multifaceted 
studies are resource-intensive. For the Distributed nanoManipulator study, two rooms, each with two computers, 
a force-feedback Phantom device, four cameras, two video recorders, two audio recorders, and wireless 
telephones, were tied up for eight months. Seven people shared the study execution and observation duties; 
on the order of 400 person hours to simply gather the data. The forty participants were each paid $100. 
Including system development and the study, an average of four graduate students Mary Whitton_SIGGRAPH09 
worked on the project each semester for four years and three to five faculty members were involved over 
the life of the project (D. Sonnenwald et al., 2003). Supporting Equipment: Evaluating early stage prototype 
devices may require additional specialized equipment. Arthur s studies (Arthur, 2000) comparing performance 
across head­mounted displays with different fields-of-view required not only access to a DARPA-funded, 
Kaiser Electro-Optics-developed, experimental wide-field-of-view HMD, but also required a large graphics 
system at the time an SGI Reality Monster with 12 separate graphics pipelines and video outputs to drive 
the 12 display tiles in the HMD. The department s (then prototype) HiBall wide-area tracker (3rdTech, 
2006) enabled Arthur to design the maze-walking task so that participants really walked in the lab. Make 
session control easy on the experimenter. For computer-based studies, devise a control application that 
enables relatively naïve computer users (including the project PIs) to oversee and conduct study sessions. 
This makes it easier on the study lead and allows sharing the task of running subjects. 4.2. Try it 
all out first: Will you be able to answer your question? Debug the process; test everything: meet and 
greet thru payment and goodbye. Always run pilot studies. Besides bringing procedural problems to light, 
running a pilot study all the way from greeting participants through data analysis enables a statistical 
power analysis to determine if the experiment is likely to differentiate among the conditions without 
an untenable number of subjects. Can the users do the task? Pilot the task. Participants must be able 
to learn the interfaces and complete the task. Some participants were never able to successfully use 
the neural­network-based walking-in-place (WIP) interface (Usoh 1999) and the accelerometer based WIP 
(Whitton, 2005). Feasel s LL-CM WIP (Feasel, Whitton, &#38; Wendt, 2008) works sufficiently well (i.e., 
no complaints in the post-session interviews) that we feel that with it, for the first time, we can fairly 
compare WIP to other means of virtual locomotion. Train to competence in all conditions. Train in a setting 
with complexity comparable to the experimental scenario. Our training scene for the Joystick (JS) and 
Walking-in-Place (WIP) conditions in Locomotion study #5 was, unfortunately, less cluttered than the 
test scenes; it did not force the participants to maneuver through spaces as tight as those in the test 
scenes. Both JS and WIP users showed improved performance on the exposure measure (lower is better) over 
the first 6 of the 12 trials; for the final 6 trials, their performance approximated that of the other 
(real-walking) conditions. Time the task. You want to learn, during piloting, how long the experimental 
sessions will be and, from that, judge if participant fatigue is going to be an issue. 5. Execution Go 
slowly and carefully. Don t waste your efforts. Small errors can render months of work worthless. Simple 
errors include lost video tapes, bad batteries in an audio recorder, and paper notes mistakenly thrown 
out. Be careful and take your time. Mary Whitton_SIGGRAPH09 Record observations and/or record sessions. 
Log experimenter observations and reports from sensors in the system e.g., keystrokes, tracker data. 
The logs can help explain outlier data points and support the exclusion of those data from the statistical 
analysis. In a passive haptics study (Insko, 2001), observations caught a consistent, but unexpected, 
wrong-turn behavior when, after training in the virtual environment, blindfolded subjects walked the 
real maze. The observation that participants consistently tipped their heads to locate sound sources 
in 3D helped explain why our (unpublished) results comparing sound localization performance in 2D (Microsoft 
DirectSound) and 3D (AuSIM Gold Series) sound-generation conditions differed from those reported in the 
literature. We found no significant performance differences attributable to sound rendering method. Our 
participants could freely walk about and move their heads. In previous studies, participants, who were 
seated with their heads held stationary, performed better on the localization task with the stimuli presented 
in 3D sound (Wenzel, Wightman, &#38; Foster, 1988). 6. Reporting 6.1. Assumptions Devising an evaluation 
study often requires assumptions. Burns (Burns, Razzaque, Whitton, &#38; Brooks, 2007) used a single 
up-staircase method in a psychophysics study to determine the position-discrepancy detection threshold 
between the location a person s real hand (felt) and the location of the avatar hand. In a later study 
Burns used multiple, interleaved, adaptive staircases to determine the velocity-discrepancy detection 
threshold. Because the outputs of the two studies were not strictly comparable, Burns had to make some 
major, but plausible, assumptions in order to complete development of his technique. The lesson is the 
importance of reporting and justifying all assumptions. If the results seem implausible, revisit the 
assumptions. 6.2. Null Statistical results Not all is lost if your quantitative results are not statistically 
significant. Null results do not mean the work is valueless, but never claim that lack of statistical 
significance of differences implies that the conditions are the same. There are two ways to emphasize 
the practical significance of any differences in measured values. Field and Hole (2003) suggest that 
authors always report effect size as part of their statistical results. Reporting effect size allows 
readers to judge for themselves if differences matter practically. Statistical techniques for equivalence 
testing, testing the hypothesis that sample populations do not differ, are available. An important application 
is in studies verifying the efficacy of generic compared to brand name drugs. Wellek (Welleck, 2002) 
is a comprehensive study of equivalence testing written for statisticians. Triangulation. Multifaceted 
studies enable data triangulation (Berg, 1989). Triangulation, common in the social sciences, is the 
use of multiple research methodologies to study the same phenomena. The theory is that using multiple 
methodologies overcomes any biases Mary Whitton_SIGGRAPH09 inherent in the individual methods and, consequently, 
enables the researcher to draw conclusions from the aggregate data more confidently than from a single 
measure or method. In the Distributed nanoManipulator study, the null statistical results were plausibly 
explained by the interview data that showed participants found positive and negative elements for both 
face-to-face and distributed collaboration conditions; they developed workarounds when they encountered 
problems. We were trying to find out if there were problems with scientific collaboration systems that 
would suggest that development stop. Looking at the whole of our data, we are comfortable saying that 
we found no showstoppers and development should continue. 7. Post-Experiment Debrief We learn by doing, 
but we forget if we don t write it down. This is particularly true in an environment such as a graduate 
school research team with constantly changing members. The fact that a key individual from the early 
2000s still works in the area and regularly reads EVE group email has helped us a number of times. The 
electronic tools are there, so keeping notes is logistically easy; it is the will that is weak. While 
the experience of running subjects is fresh, before the data analysis is done, sit together as a team, 
and record what went right and wrong. Do it again when the analysis is finished and you know whether 
you are able to answer the question that you started with. Acknowledgements. The work reported here was 
largely funded by the Office of Naval Research (VIRTE Project), the NIH National Institute for Biomedical 
Imaging and Bioengineering, and SAIC. Additional support was provided by the Link Foundation and NC Space 
Grant. Many of these same lessons are included in a book chapter Evaluating VE Component Technologies 
(Whitton &#38; Brooks, 2008). Mary Whitton_SIGGRAPH09 References Arthur, K. W. (2000). Effects of field 
of view on performance with head-mounted displays (CS Tech Rpt. # TR00-019). Ph.D Dissertation, University 
of North Carolina, Technical Report #00-019. Berg, B. L. (1989). Qualitative Research Methods for the 
Social Sciences. Boston: Allyn and Bacon. Burns, E., Razzaque, S., Whitton, M., &#38; Brooks, F. (2007). 
MACBETH: Management of Avatar Conflict by Employment of a Technique Hybrid. International Journal of 
Virtual Reality, 6(2), 11-20. Feasel, J., Whitton, M. C., &#38; Wendt, J. D. (2008). LLCM-WIP: Low Latency, 
Continuous -Motion Walking-In-Place. Proceedings of IEEE Symposium on 3D User Interfaces, 97-104. Field, 
A., &#38; Hole, G. (2003). How to design and report experiments: Sage Publishing. Gabbard, J., Hix, D., 
&#38; Swan, E. (1999). User-centered design and evaluation of virtual environments. IEEE Computer Graphics 
and Applications, 51-59. Helms, J. W., Arthur, J. D., Hix, D., &#38; Hartson, H. R. (2006). A field study 
of the Wheel--a usability engineering process model. The Journal of Systems and Software, 79, 841-858. 
Hix, D., Swan, J., Gabbard, J., McGee, M., Durbin, J., &#38; King, T. (1999). User-Centered Design and 
Evaluation of a Real-Time Battlefield Visualization Virtual Environment. IEEE Virtual Reality 1999, 96-103. 
Hudson, T., Helser, A., Sonnenwald, D. H., &#38; Whitton, M. (2003). Managing Collaboration in the Distributed 
nanoManipulator. Proceedings of IEEE on Virtual Reality 2003. Insko, B. (2001). Passive Haptics Significantly 
Enhances Virtual Environments (CS Technical Report #01-017). Unpublished Ph.D Dissertation, University 
of North Carolina at Chapel Hill Martin, D. W. (2007). Doing Psychology Experiments, 7th Ed. Belmont, 
CA: Wadsworth Publishing. Meehan, M., B. Insko, M.C. Whitton, and F.P. Brooks. (2002). Physiological 
Measures of Presence in Stressful Virtual Environments. ACM Transactions on Graphics (Proceedings of 
SIGGRAPH 2002), 21(3), 645-652. Meehan, M., S. Razzaque, M. Whitton, F. Brooks. (2002). Effects of Latency 
on Presence in Stressful Virtual Environments. IEEE Virtual Reality 2003, 1, 141-148. Meehan, M. S., 
Razzaque, S., Insko, B., Whitton, M. C., &#38; Brooks, F. P. (2005). Review of Four Studies on the Use 
of Physilogical Reaction as a Measure of Presence in Stressful Virtual Environments. Applied Physiological 
and Biofeedback., 30(3), 239-258. Slater, M., Usoh, M., &#38; Steed, A. (1995). Taking Steps: The Influence 
of a Walking Technique on Presence in Virtual Reality. ACM Transactions on Computer-Human Interaction, 
2(3), 201-219. Sonnenwald, D., Whitton, M., &#38; Maglaughlin, K. (2003). Evaluating a Scientific Collaboratory: 
Results of a Controlled Experiment. ACM Transactions on Computer Human Interaction, 10(2), 151-176. Sonnenwald, 
D. H., K. Maglaughlin, M. Whitton. (2004). Designing to Support Situational Awareness Across Distances: 
An Example from a Scientific Collaboratory. Information Processing &#38; Management, 40(6), 989-1011. 
Sonnenwald, D. H., Maglaughlin, K., &#38; Whitton, M. (2001). Using innovation diffusion theory to guide 
collaboration technology evaluation: Work in progress IEEE 10th Workshop on Enabling Technologies: Infrastrucrure 
for Collabrative Enterterprises., Video Paper. Mary Whitton_SIGGRAPH09 Sonnenwald, D. H., R. Berquist, 
K. Maglaughlin, E. Kupstas Soon, and M. Whitton. (2001). Designing to Support Scientific Research Across 
Distances: the nanoManipulator Environment. In D. S. E. Churchill, and A. Munro (Ed.), Collaborative 
Virtual Environments (pp. 202-224). London: Springer Verlag. Usoh, M., K. Arthur, M.C. Whitton, A. Steed, 
M. Slater and F.P. Brooks. (1999). Walking>Virtual Walking>Flying, in Virtual Environments. Proceedings 
of ACM SIGGRAPH 1999 (Computer Graphics Annual Conference Series 1999), 359-364. Welleck, S. (2002). 
Testing statistical hypotheses of equivalence. Boca Raton, FL: Chapmen &#38; Hall/CRC Press. Whitton, 
M., &#38; Brooks, F. (2008). Evaluating VE Component Technologies. In D. Nicholson, J. Cohn &#38; D. 
Schmorrow (Eds.), Virtual Environments for Training and Education: Developments for the Military and 
Beyond (Vol. 2, pp. 240-261). Westport, CN: Praeger Security International. Whitton, M. C., Cohn, J., 
Feasel, J., Zimmons, P., Razzaque, S., Poulton, S., et al. (2005). Comparing VE Locomotion Interfaces. 
Proceedings of IEEE Virtual Reality 2005, 123-130. Zimmons, P. (2004). The Influence of Lighting Quality 
on Presence and Task Performance in Virtual Environments (CS Tech Rpt. # TR04-017). The University of 
North Carolina at Chapel Hill. Mary Whitton_SIGGRAPH09  User Studies for Graphics Marina Bloj Bradford 
Optometry Colour and Lighting (BOCAL) Lab University of Bradford  Outline  Issues to consider 1. What 
do we want to do? 2. How do we do it? 3. Nuts and bolts  Samples  Task instructions  Study participants 
  4. When do we do it?   Practical examples 1. Colour to greyscale algorithms 2. Difference scaling 
   Marina Bloj SIGGRAPH09 2 1. What do we want to do?  Involve people in our graphics development 
 Participants as reliable measuring devices  Not the only or always most appropriate way  Numerical 
methods  Algorithms, some perceptually based   Marina Bloj SIGGRAPH09 3 2. How do we do it?  Marina 
Bloj SIGGRAPH09 4 2. How do we do it?  Are the samples the output of different graphical methods we 
would like to compare? Or are they produced by the same method but with different parameter values? Is 
ground truth available? Which psychophysical method to use and which analysis technique we will apply 
needs to be decided at the start. Marina Bloj SIGGRAPH09 5 Marina Bloj SIGGRAPH09 6 3. Nuts and bolts 
-Samples How many samples do we need to use?  What type of samples?  Factors to consider:  time 
 sample generation methods  availability  generality  type of analysis used   sample selection establishes 
the generality of our study and the conclusions we draw  Marina Bloj SIGGRAPH09 7 3. Nuts and bolts 
-Samples More factors to consider:  emotional effects  spatial configuration  internal references 
(faces, typical colours, etc.)   Presentation artefacts:  randomisation during presentations  learning/training 
effects that might occur  adaptation  repeatability and consistency of conditions across participants 
  sample selection and presentation establishes the generality of our study and the conclusions we 
draw  3. Nuts and bolts Task Instructions  What and how we tell participants what to do in our studies 
is crucial for ensuring that the data we Sample task instructions  Contextual vs non-contextual (general) 
 Care when extrapolating  All participants must perform the same task  Marina Bloj SIGGRAPH09 8 
Marina Bloj SIGGRAPH09 9 3. Nuts and bolts Study participants  How many participants do I need for 
my study?  What type of participants do I need for my study?  What are the ethical implications of 
participating in my study?  Marina Bloj SIGGRAPH09 10 3. Nuts and bolts Study participants How many 
participants do I need for my study?  More better precision (square root)  Power analysis  but in 
reality.  More participants vs more repeats  Marina Bloj SIGGRAPH09 11 3. Nuts and bolts Study participants 
 What type of participants do I need for my study?  Scope of your question Context of your study 
 naïve vs experts  age  gender  social/cultural/ethnic/language backgrounds  health, in particular 
visual function. What screening tests will you use?   Test for effect of above?  Extrapolate with 
care.  Marina Bloj SIGGRAPH09 12 3. Nuts and bolts Study participants What are the ethical implications 
of participating in my study? Ethical approval can be a very time consuming process  it might influence 
which experimental techniques and/or participation criteria you will ultimately decide upon.  Participant 
consent form and recruitment. Payment.  Marina Bloj SIGGRAPH09 13 4. When do we do it?  There is 
a place for usability studies in all stages of the developmentprocess, with different questions, requiring 
different techniques andparticipants.  Pilot studies: provide targeted feedback so as to improve (debug, 
not datacollection)  reduced (or an extended) sample selection and on a few participants.Did participants 
find the instructions clear? How could they beimproved?  Was the task easy/difficult? Why? Can the participant 
describe thestrategy they used?  How did participants find the timing of the task? Did the get bored? 
 Listen to any other issues or suggestions that participants might have. Were they comfortable?  Data 
collected can aid sample selection and power analysis   5. Practical example 1: Collaborators  Dr 
David Connah  Prof Graham Finlayson   Computing Sciences, University of East Anglia, And support 
from the and Xerox. Connah, D., Finlayson, G. D., et al. (2007). "Seeing beyond luminance: A psychophysical 
comparison of techniques for converting colour images to greyscale." Fifteenth Color Imaging Conference: 
Color Science and Engineering Systems, Technologies, and Applications, Final Program and Proceedings: 
336-341. Marina Bloj SIGGRAPH09 14 5. Practical example 1: C2G What is the best way to convert colour 
images to greyscale? What do we mean by best ? (fastest, simplest, for users, etc.)  In our case for 
users  Marina Bloj SIGGRAPH09 15 5. Practical example 1: C2G  For users  Objective (reaction time, 
accuracy)  Subjective (preference)   This is all leading to selection of experimental design and development 
of task instructions  Marina Bloj SIGGRAPH09 16 Marina Bloj SIGGRAPH09 17 5. Practical example 1: 
C2G  Think of at least two possible tasks for participants to do? Will you include the colour original? 
 List pro and cons of each  Task (with/without Pro Cons original) Marina Bloj SIGGRAPH09 18 5. Practical 
example 1: C2G 2AFC forced choice Which of the two greyscale images contains the most detail? . Mean 
luminance 40cd/m2 average size of images 6.5 X 5.5 degrees Greyscale Reproductions Marina Bloj SIGGRAPH09 
19 5. Practical example 1: C2G 2AFC forced choice Given the colour original, which greyscale reproduction 
would you prefer? Mean luminance 40cd/m2 average size of images 6.5 X 5.5 degrees Greyscale Reproductions 
 Colour Original 5. Practical example 1: C2G  At the start of each session the subjects adapted for 
60 sec. to a neutral grey background (40 cd/m2).  Subjects had 8 seconds to make a decision  Average 
reaction time was 2-3 seconds Marina Bloj SIGGRAPH09 20 Marina Bloj SIGGRAPH09 21 5. Practical example 
1: C2G Who are the users ? Participant selection and screening Characteristics of How I will Limitations 
participants screen/record this Marina Bloj SIGGRAPH09 22 5. Practical example 1: C2G  Which and how 
many samples?  Reference  Randomization  Image content  Unwanted factors   Again influencing selection 
of experimental design and development of task instructions  5. Practical example 1: C2G Parrot Girl 
 Hats Heron Poppies Marina Bloj SIGGRAPH09 23 Marina Bloj SIGGRAPH09 24 5. Practical example 1: 
C2G  6 paid naive subjects (colour normal and had normal or corrected-to-normal acuity)  6 original 
images  6 algorithms  Each subject judged a given pair of algorithms 4 times for each test image (15 
× 4 = 60 comparisons for each original image)  For a given test image, each pair of algorithms were 
compared 24 times (4 ×6 observers)  Trials were split into 4 sessions, each lasting 20-25 minutes. 
 The monitor was calibrated and characterised beforehand. Images were scaled by the same scaling factor 
until they were guaranteed to be within the monitor gamut. (ViSaGe system).  Subjects were seated 1.14 
metres from the monitor in a dark room and their head position was stabilised using a chin-rest. 6. 
Practical example 2: Difference scaling Especially suitable for when samples are generated by different 
values of a single parameter  Charrier, C., Maloney, L. T., et al. (2007). "Maximum likelihood difference 
scaling of image quality in compression-degraded images." Journal of the Optical Society of America A 
24: 3814-3826.  Knoblauch, K. and Maloney, L. T. (2008). "MLDS: Maximum likelihood difference scaling 
in R." Journal of Statistical Software 25(2): 1-26.  Maloney, L. T. and Yang, J. N. (2003). "Maximum 
likelihood difference scaling." Journal of Vision 3: 573-585.   Marina Bloj SIGGRAPH09 25 6. Practical 
example 2: Difference scaling  A difference scaling quadruplet. The observer s task is to examine the 
upper pair of color patches and the lower pair, and to select whichever pair has a larger difference. 
 Number of Stimuli Number of Quadruples 10 210 12 495 15 1365 20 4845 Maloney, L. T. and Yang, J. N. 
(2003). "Maximum likelihood difference scaling." Journal of Vision 3: 573-585. Marina Bloj SIGGRAPH09 
26 6. Practical example 2: Difference scaling  An estimated difference scale. The stimuli are reproduced 
along thehorizontal axis. The values plotted along the vertical axis are scale values assigned to 11 
colored patches by a maximum likelihood difference scalingprocedure. Maloney, L. T. and Yang, J. N. (2003). 
"Maximum likelihood difference scaling." Journal of Vision 3: 573-585. Marina Bloj SIGGRAPH09 27 6. 
Practical example 2: Difference scaling  Maloney, L. T. and Yang, J. N. (2003). "Maximum likelihood 
difference scaling." Journal of Vision 3: 573-585. Marina Bloj SIGGRAPH09 28 Marina Bloj SIGGRAPH09 
29 7. Take home messages Careful question formulation  Know your analysis from the start  Trade offs 
in sample and participant selection  Extrapolate with care  Plan and Pilot  User studies for graphics 
Marina Bloj, Bradford Optometry Colour and Lighting (BOCAL) Lab, University of Bradford, UK As we have 
discussed in the introduction to this course we need to do user studies because the end­user of what 
we produce are people. In particular I am now going to look at some of the aspects of user studies in 
the field of graphics. Although in my examples I will be focusing on static images the issues are common 
to movies, rendered scenes, photographs and many other applications you can think of. These notes and 
course-slides follow a similar structure. First I will introduce the issues I think we need to consider 
and then I will describe a few case studies that will exemplify the issues. What I will not do is give 
a tutorial on psychophysical techniques; rather, I will share some of my experiences in doing these types 
of studies. 1. What do we want to do? We want to involve people users-in our graphics development work 
by using them as reliable measuring devices. These does not mean that they will (or should be) the only 
way we evaluate and choose among techniques or images. Among other factors that you might have to trade 
off against when making a choice are cost and time. Graphics can also be evaluated by non-user methods, 
by measuring and comparing their physical and numerical attributes (resolution, colours, etc.) or by 
the use of human inspired metrics and or algorithms such as image difference predictors (Scott 1993). 
2. How do we do it? Once we have decided to carry out a user study we need to consider what type of study 
we will do. This will partly be defined by the nature and use that the graphics will be put to (medical 
imaging, entertainment, training, etc. see image on the left for some examples-) and what samples we 
have available. If the purpose of the graphics is to provide some sort of diagnostic for medical or military 
purposes we will need to design an objective study based on, for example, measuring reaction time and 
accuracy of participants for a given task. If the purpose is to provide some sort of training then we 
might want to do a study were we compare which graphics produce the largest improvement of some pre-established 
performance indicator, which require us to record performance before and after intervention. Mor e commonly 
we want to have an indication of which graphics is preferred by th e user. These later types of studies 
are the ones the presentation focuses on. The nature of how the different graphic samples where generated 
will also play a role in deciding which type of task and analysis will be best suited to our user study. 
Are the samples the output of different graphical methods we would like to compare? Or are they produced 
by the same method but with different parameter values? Is grou nd truth available? In other words does 
the ideal or perfect version of the graphic output exist? Marina Bloj _ SIGGRAPH09 1 As we are considering 
all these factors to help us decide on our user study design we need to remember that we are making two 
co-dependant decisions: which psychophysical method to use and which analysis technique we will apply. 
For a successful user study it is necessary that both of these things are clear from the onset and that 
we avoid deciding on the analysis technique(s) after we have collected our participant data. 3. Nuts 
and bolts 3.1 Samples. I have already touched on some of the sample (i.e. set of graphics that will be 
evaluated) issues above but what people always want to know is how many samples they should use in their 
studies. There is no fast and easy answer to this question as the answer will depend on trade-off between 
many factors, some of which are:  time  sample generation methods  availability  generality  type 
of analysis used  Adapting Bartleson s proposed hierarchy of sample categories as cited in (Engeldrum 
2000) -can help us to consider the range of samples we might use. This goes from a very large random 
and independent set of samples which are probably unrealistic for most user studies to the use of a limited 
incidental and established gold standard or reference sample set. We might also need to consider the 
class of sample (text, images with people, etc.) which has obvious links to the application for which 
the graphics are being developed. It might be necessary to consider if the samples are to correspond 
to some sort of systematic variation of some physical or algorithm parameter. What we must remember is 
that our sample selection establishes the generality of our study and the conclusions we draw. When deciding 
on our study samples we will also need to consider the effect that extraneous factors might have on the 
choices our participants make. Some things to keep in mind when selecting samples for a user study are: 
 emotional effects  spatial configuration  internal references (faces, typical colours, etc.)  And 
once we have decided on our sample sets we then need to minimise possible artefacts that might arise 
from the way and order in which we present the samples for evaluation. This requires us to carefully 
consider issues such as sample randomisation during presentations, any learning/training effects that 
might occur, adaptation, how to best display the samples and how to guarantee repeatability and consistency 
of conditions across participants. I will come back to all this when I present the case studies. Marina 
Bloj _ SIGGRAPH09 2 3.2 Task instructions What and how we tell participants what to do in our studies 
is crucial for ensuring that the data we obtain is useful. The type of study that we carry out (section 
2 above) determines the type of task and this in turn establishes the instructions we need to give our 
observers. Some authors (Engeldrum 2000) argue that the use of non-contextual ( please express an image 
quality rating for these samples as opposed to please express an image quality rating for these samples 
to be used as family snapshots ) is preferable as it avoids constraining the resulting scale to a particular 
context. However, in my experience it is better to use specific context dependant instructions that are 
easier for participants to understand and produce more consistent results. We just need to remember to 
extrapolate with care to other contexts. If we are evaluating graphics for a surveillance application 
then the task we might ask our participants to perform is to locate targets among distractors (and we 
would measure reaction time and accuracy as a function of different graphics samples). We would then 
have to decide how to best word our instructions; do we want participants to emphasize accuracy over 
speed or to find the most targets in the shortest time at the cost of increased false alarms? Each of 
these scenarios would correspond to a different set of participant instructions. Or we might be doing 
an image preference study, such as the colour to greyscales examples I will talk about later (see Example 
1). Whatever the task it is important that we ensure that all participants perform the same task. I recommend 
preparing a study script that is provide to all participants (written, audio recording, short movie, 
etc) and to check for understanding with targeted questions. Be careful that in the subsequent dialogue 
with your participants you do not give different levels of information to different individuals. As part 
of our experimental design you might also want to include a task training phase to verify participants 
understanding of the task. As we can see, establishing the question we want to answer via our user study 
goes a very long way towards setting the samples we will use and the task we will ask observers to perform. 
3.3 Study participants The first question people always ask is How many participants do I need for my 
study? the one they should also be asking is What type of participants do I need for my study? and the 
one that is becoming increasingly important is What are the ethical implications of participating in 
my study? In general the more participants that take part in your study the better as this will lead 
to an increase in precision, but keep in mind that this increase is proportional to the square root of 
the number of participants. Once you know the analysis techniques you will use and the order of magnitude 
of the effect (or difference) you expect to establish via your study you can use power analysis to calculate 
the minimum number of participants required to accept the outcome of a statistical test with a particular 
level of confidence (Field 2005). Sometimes you will be constrained by time and availability and these 
will become the overruling factors in deciding on the number of participants, be aware that this is far 
from the best policy! The issue of what type of participants to use in your study is closely related 
to the scope of the question you are asking. If you are evaluating graphics for medical diagnostic purposes 
then your participants should come from appropriate user groups (e.g. radiologists), if you want to establish 
something about the general population s preference for graphics then your participants need to be Marina 
Bloj _ SIGGRAPH09 3 representative of that and not just computer science undergraduate students or your 
work colleagues. Some of the factors you might want to take into account are: naïve vs experts  age 
 gender  social/cultural/ethnic/language backgrounds  health, in particular visual function. What 
screening tests will you use?  If you would like to verify if any of the above aspects have a specific 
effect on your study you will need to design your study in such a way that you can then specifically 
test for those hypothesis. This will have an impact on the number of participants you will need as each 
group will need to have a large enough number to carry out the appropriate tests. If you ensure you have 
a more homogeneous sample by appropriate screening and selection then you will need fewer participants 
but your findings will be limited to that population sample and you will have to extrapolate with care. 
You will also need to address the ethical implications of your study, for most University based studies 
no matter how informal they might be-it is nowa pre-requisite to have ethical approval from the appropriate 
committee and in some cases also from external organisations (funding agencies, health boards, etc.). 
This is necessary for all and every study involving human subjects; even if the study involves looking 
at images and making decisions and if the persons involved are the experimenter and a colleague. Getting 
this approval can be a very time consuming process so it is something that you need to do very early 
in your study design as it might influence which experimental techniques and/or participation criteria 
you will ultimately decide upon. As part of the ethical consideration you might need to consider how 
you will pay participants (cash, in kind, etc) and recruit them. As part of your ethics process you will 
need to prepare participant information sheets and consent forms for them to read and sign. In Appendix 
1 I have included copies of the ethical approval document that covers most of the experiments carried 
out in my lab as well as participant consent sheets. 4. When do we do it? Here I would like to echo some 
of what Mary Whitton said in her previous presentation and that we have stressed in the introduction: 
there is a place for usability studies in all stages of the development process, with different questions, 
requiring different techniques and participants. Additionally I would like to emphasise the importance 
of pilot studies; all user studies should be piloted, it is a big resource saver in the long run. A pilot 
gives you the opportunity to seek targeted feedback so as to improve the study, you might run it with 
a reduced (or an extended) sample selection and on a few participants. The aim of a pilot is to debug, 
not data collection per se, so you need to ask the right questions from your participants at the end 
of the pilot. Some topics you might wish to include are: Did participants find the instructions clear? 
How could they be improved?  Was the task easy/difficult? Why? Can the participant describe the strategy 
they used?  How did participants find the timing of the task? Did the get bored?  Marina Bloj _ SIGGRAPH09 
4 Listen to any other issues or suggestions that participants might have. Were they comfortable? Although 
the main purpose of a pilot is not to collect data, some benefits can be gained from looking at the results. 
If an extended sample set was trialled then the results can help us decide which samples are worth using 
in the main study, any measured effect size can be used in the power analysis to help us estimate the 
number of participants needed for significant results to be obtained from the main study. What I would 
like to do now is take you through a couple of practical examples that will help to illustrate some of 
the issues discussed so far. 5. Practical example 1: Colour to greyscale algorithms I will now use an 
example to highlight some of the practicalities of the issues mentioned in the introduction. There are 
many algorithms for converting coloured images into greyscale versions (Connah, Finlayson et al. 2007). 
How can we choose among the available options? First we need to decide which the important aspect is 
for us, if speed or efficiency of the algorithm is the driving factor then user studies will not help 
us. But if we want to choose the method that produces the best one for users then we must do a user study. 
Even if we have decided that we that we want to choose the algorithm that provides the best image for 
users we need to keep asking ourselves what we really mean. In other words, to what use the images will 
be put to will help determine what type of task we will ask our participants to perform and the experimental 
design we will use. Will these images be used as part of a screening process or produced for aesthetic 
purposes? In the first case we might look to evaluate them via an objective task, where we will measure 
reaction time and accuracy. For the later, questions along the line of which image do you prefer? are 
more appropriate. In this particular example we have access to ground truth in the form of a colour original, 
should we show this image to participants as part of the task? I will now ask course participants to 
think which question they would pose and discuss the pro and cons of their suggestions. The course slides 
include the two options we have actually used in our study. Think of at least two possible tasks for 
participants to do? Will you include the colour original? Task 1 (with/without original) Pros Cons Marina 
Bloj _ SIGGRAPH09 5 Task 2 (with/without original) Pros Cons Who will be the participants in your study? 
Define them in terms of as many factors as you think are important for the question posed (naïve vs. 
expert, gender, age and cultural/background factors). How will you findings generalise? Will you screen 
your participants? Characteristics of participants How I will screen/record this Limitations Now that 
you have chosen a task and a group of participants take some time to list how this might limit the scope 
of your study. What trade-offs could you make? Marina Bloj _ SIGGRAPH09 6 Now we need to decide which 
images we will use in our studies. Below are the images we chose, how and why would you have chosen them? 
Parrot Girl Hats  Heron Poppies  In the following list I have summarise most of the implementation 
details of our task. Is there anything missing? Is there anything in the list you would have not considered 
before? 6 paid naive subjects (colour normal and had normal or corrected-to-normal acuity)  6 original 
images  6 algorithms  Each subject judged a given pair of algorithms 4 times for each test image (15 
× 4 = 60 comparisons for each original image)  For a given test image, each pair of algorithms were 
compared 24 times (4 × 6 observers)  Trials were split into 4 sessions, each lasting 20-25 minutes. 
 The monitor was calibrated and characterised beforehand. Images were scaled by the same scaling factor 
until they were guaranteed to be within the monitor gamut. (ViSaGe system).  Subjects were seated 1.14 
metres from the monitor in a dark room and their head position was stabilised using a chin-rest.  From 
the experimental results of our user study we calculated measures of consistency and agreement (within 
and across observers) as well as inferring a ranking for the different algorithms tested. Detail of all 
this can be found in (Connah, Finlayson et al. 2007). Marina Bloj _ SIGGRAPH09 7 6. Practical example 
2: Difference scaling The previous example used some fairly traditional techniques to arrive at a ranking 
among different algorithms. Now I would like to mention an alternative technique that is particularly 
useful when you have a given algorithm and would like to test the effect of changing a parameter value. 
The technique is called maximum likelihood difference scaling (MLDS), in the presentation I will only 
be able to give a brief example of its application but more information can be found in (Maloney and 
Yang 2003). An example of particular interest is the evaluation of an image compression algorithm using 
this technique (Charrier, Maloney et al. 2007) and an implementation of the technique in R http://www.r-project.org/ 
can be found at (Knoblauch and Maloney 2008). 7. Take home messages: Careful question formulation Know 
your analysis from the start Trade offs in sample and participant selection Extrapolate with care Plan 
and Pilot 8. References and bibliography Charrier, C., Maloney, L. T., et al. (2007). "Maximum likelihood 
difference scaling of image quality in compression-degraded images." Journal of the Optical Society of 
America A 24: 3814-3826. Connah, D., Finlayson, G. D., et al. (2007). "Seeing beyond luminance: A psychophysical 
comparison of techniques for converting colour images to greyscale." Fifteenth Color Imaging Conference: 
Color Science and Engineering Systems, Technologies, and Applications, Final Program and Proceedings: 
336-341. Engeldrum, P. G. (2000). Psychometric Scaling: A Toolkit for Imaging Systems Development Winchester, 
Massachusetts, USA, Imcotek Press Field, A. (2005). Discovering Statistics Using SPSS Sage Publications 
Ltd. Knoblauch, K. and Maloney, L. T. (2008). "MLDS: Maximum likelihood difference scaling in R." Journal 
of Statistical Software 25(2): 1-26. Maloney, L. T. and Yang, J. N. (2003). "Maximum likelihood difference 
scaling." Journal of Vision 3: 573-585. Scott, D. (1993). The visible differences predictor: an algorithm 
for the assessment of image fidelity. Digital images and human vision, MIT Press: 179-206. 9. Appendix 
1: Ethics and consent forms This is a collection of documents you might find useful and includes: -Participant 
consent form used in my lab. BOCAL_Participant_Consentform.pdf -The British Psychological Society (BPS) 
Code of Conduct. Each national organisation will have an equivalent document you might want to familiarise 
yourself with. http://www.bps.org.uk/the-society/code-of-conduct/ Marina Bloj _ SIGGRAPH09 8 -The general 
request for ethical approval document we submitted to the University of Bradford s Ethics Committee which 
once approved has become our umbrella ethical approval document. http://www.brad.ac.uk/acad/lifesci/optometry/index.php/Vsrfg/Ethics 
 10. Appendix 2: Articles and web-sources Most of the articles I have used as practical examples are 
available online. The studies on the colour to greyscale (C2G) preference are available from my website: 
http://www.brad.ac.uk/acad/lifesci/optometry/index.php/Staff/DrMarinaBloj The articles on difference 
scaling are available either from the one of author s webpage (Prof. Larry Maloney) http://www.psych.nyu.edu/maloney/ 
or from the journal s free online access Journal of Vision and Journal of Statistical Software. Marina 
Bloj _ SIGGRAPH09 9 Eye Tracking in User Studies Veronica Sundstedt Graphics, Vision, and Visualisation 
Group Trinity College Dublin  Outline  Topics  What is eye tracking?  Measuring eye movements  
Eye tracking technology  Eye movement analysis   Experimental design  What can we measure?  Analysis 
and statistics   Case studies  Diagnostic uses of eye tracking  Interactive uses of eye tracking 
  Image courtesy: Hillaire et al. 2008 Quake 3 Arena screenshot Veronica Sundstedt SIGGRAPH09 courtesy 
of Id Software.  What is eye tracking? Eye tracking allows us to determine where an observers gaze 
is focused at a given time Example eye movements from an observer free-viewing and performing a counting 
task. Veronica Sundstedt SIGGRAPH09  Schematic representation of the human visual field. Adapted from 
MacEvoy (2007). Veronica Sundstedt SIGGRAPH09  Five basic types  Saccades  Smooth pursuits  Vergence 
 Vestibular ocular reflex (VOR)  Optokinetic reflex (OKR)   Fixations  Scan paths  200-300 ms, 
90% of viewing time Trajectory between fixation points Veronica Sundstedt SIGGRAPH09  Eye tracking 
systems  Interactive Selective Gaze-contingent  Screen-based Model-based Adapted from Duchowski (2001). 
Veronica Sundstedt SIGGRAPH09  Video-based  Head-mounted  Portable  Wearable  Remote   Electronic 
 Skin electrodes  Mechanical  Lenses Veronica Sundstedt SIGGRAPH09  (Left): Purkinje images and 
(right) relative positions of pupil and first Purkinje images as seen by the eye-tracker s camera during 
calibration. Adapted from Duchowski (2003) and Räihä (2005). Veronica Sundstedt SIGGRAPH09   Piloting 
 Participants  Calibration  Recording issues  Visualising eye movements  Image courtesy: Analysing 
eye movements McDonnell et al. 2009 Veronica Sundstedt SIGGRAPH09   Setup  Subject interaction  
Accuracy tests  Setup documentation  Experiment duration   Feedback  Consent form  Questionnaire 
 Instructions   Veronica Sundstedt SIGGRAPH09   How many participants?  Depends on the type of 
study  Depends on stimuli and tasks   How do I obtain participants?  Working with participants 
 Poole (2005) and Strandvall (2009). Veronica Sundstedt SIGGRAPH09   Grid point calibration  Calibration 
issues  Using a chin rest  Re-calibrate between trials   Richmond Products, Inc. Veronica Sundstedt 
 SIGGRAPH09   Physiological reasons  Pupil  Eyelid   External reasons  Eyeglasses  Contacts 
  Internal reasons  Unnatural scenario  Learning and task   (Schnipke and Todd 2000) Veronica 
Sundstedt SIGGRAPH09  A lot of raw data!  Drift correction  Extracting eye movements  Fixations 
 Saccades  Blinks   Fixation filters  Type Timestamp Gaze PointX Left Gaze PointY Left CamXLeft 
CamYLeft DistanceLeft PupilLeft ValidityLeft Window-based Source: Tobii Technology Velocity-based Veronica 
Sundstedt SIGGRAPH09   Fixation count / duration / order  Areas of interest (per region)  Transitions 
between regions  Time to fixation (first)  Count of visits / re-visits  Task completion duration 
 Pupil size  Blinks  What is important?  Attention  Engagement  Interest or arousal Task  Task 
vs. free-viewing  Search efficiency  Optimisation  Confusion  Naïve vs. experienced users  Veronica 
Sundstedt SIGGRAPH09  Free-viewing Veronica Sundstedt SIGGRAPH09   Gaze replay (real-time)  Scan 
path or gaze plot(s)  Heat maps  Gaze opacity  Areas of interest  Percentage seen  Bee swarm effect 
 Veronica Sundstedt SIGGRAPH09  Video of an observer performing a 2AFC task  Veronica Sundstedt 
SIGGRAPH09  Gaze plot of the same observer  Veronica Sundstedt SIGGRAPH09  Heat map of the same 
observer  Veronica Sundstedt SIGGRAPH09   Item buffer is used to map fixations to objects  Objects 
playing a similar role are grouped into object classes relevant for solving the maze  Ex: correct path, 
incorrect path, etc. (Sundstedt et al. 2008) Veronica Sundstedt SIGGRAPH09   It is common to look 
at the object underneath the fixation point  Error-prone in cluttered environments (Lee et al. 2007) 
 Novel approach takes into account distance between pixels and a fixation point, as well as object size 
  Based on 0.7eye tracking accuracy and 2° foveal region, the area over which a fixation point bears 
relevance is a circle with a radius of around 43 pixels (a solid angle of 2.7)  (Sundstedt et al. 2008) 
 Veronica Sundstedt SIGGRAPH09  For frame nr f we assign a non-zero weight wf to all pixels (xp, yp) 
in a window of 400 x 400 pixels according to theirdistance to the fixation point (xf, yf), where  s 
= 43 pixels  Weights per f are added to objects in the item buffer  Repeat process for all frames with 
fixations  Normalize to account for trial duration  Produces weight associated with every object  
Group objects with the same role into object classes  Veronica Sundstedt SIGGRAPH09 (Sundstedt et al. 
2008)   Diagnostic uses of eye tracking 1. High-fidelity selective rendering 2. Crowd variety in 
virtual environments 3. Fixation behaviour in games   Interactive uses of eye tracking 4. Gaze based 
game interaction 5. Depth-of-field blur and camera motions   Veronica Sundstedt SIGGRAPH09  Calculating 
light transport through a participating media is very expensive Image courtesy (Kalabsha dust rendering): 
Diego Gutierrez and Oscar Anson. medieval house in an Egyptian temple Can we render less without noticing? 
 Veronica Sundstedt SIGGRAPH09   Extinction map that pre­computes light attenuation in inhomogenous 
participating media  Importance map guides number of rays traced  Brighter pixels = more rays  How 
well do these maps predict importance?  Image courtesy: Diego Gutierrez and Oscar Anson. Veronica Sundstedt 
 SIGGRAPH09  -Rendering cost can be reduced up to 10 times -Observers do not notice the reduction in 
quality Veronica Sundstedt SIGGRAPH09 (Sundstedt et al. 2007)  This study led to insights such as: 
The plot shows that the longest durations were concentrated on the facial region of the Lucy scene. Observers 
also looked at the scattering around light sources, this lead to the development of a light map. Veronica 
Sundstedt SIGGRAPH09  Creating a crowd of varied humans are difficult  Can we vary fewer body parts 
without noticing?  Mesh split into 14 regions  Ray cast from eye screen coordinates into scene  
What sub-mesh is it part of?  Increment fixations on that part  Counter for each body part  Cast 
ray around as well to account for inaccuracies   Veronica Sundstedt SIGGRAPH09   Image courtesy: 
McDonnell et al. 2009 Veronica Sundstedt SIGGRAPH09  Head and upper torso attracted the majority of 
first Image courtesy: McDonnell et al. 2009 Veronica Sundstedt SIGGRAPH09   Predicting gaze behavior 
in games can be a useful asset to game designers  Improve game play (AI, difficult level, etc.)  Selectively 
increase visual fidelity  Optimize distribution of computing resources  Verifying game mechanics  
 To which extent is gaze behavior altered by the presence of a task in a game?  (Sundstedt et al. 2008) 
Veronica Sundstedt SIGGRAPH09  1st Pass  Real-time run, eye tracking, logging of all game state necessary 
to reconstruct each frame later  Less intensive than encoding a video of the game  Can be played back 
game in real-time yielding a replica of an earlier trial for the passive condition   2nd Pass Reconstruct 
each frame, generate additional data for each frame (item buffer, frame buffer, object data)  3rd Pass 
 Use analysis tool to get useful info out of the stored game and gaze data, generate importance for 
each object class Veronica Sundstedt SIGGRAPH09 (Sundstedt et al. 2008)  Active Passive  (Sundstedt 
et al. 2008) Veronica Sundstedt SIGGRAPH09  A task is implied, even if no task is specified Computational 
metric that assumes absence of task cannot be applied to games, including saliency algorithms  Passive 
and active user behavior could not be statistically distinguished  Task-relevance, although more difficult 
to model, should play a larger role in models of gaze behavior then currently assumed  Eye tracking 
in the game design cycle can help predictfixation behavior  (Sundstedt et al. 2008) Veronica Sundstedt 
 SIGGRAPH09  Image courtesy: Wilcox et al. 2008 Veronica Sundstedt SIGGRAPH09  Image courtesy: Hillaire 
et al. 2008 Quake 3 Arena screenshot courtesy of Id Software Veronica Sundstedt SIGGRAPH09  Buy 
Eye trackers are generally expensive  Hire  Low cost solutions http://www.cogain.org/eyetrackers/low-cost-eye-trackers 
 Loan  Explore collaboration possibilities with other departments or institutes (e.g. computer science, 
engineering, psychology) Veronica Sundstedt SIGGRAPH09  Take home messages  Eye tracking can add important 
objective information  Take the human observer into account in the design  Knowledge of eye movements 
can be used to improve our algorithms and techniques  Eye tracking can also be used as an alternative 
means of interaction  Plan your study, pilot your setup, minimise all risks that can affect your data 
recording, analyse your data  Veronica Sundstedt SIGGRAPH09  Resources  ETRA 2010: Eye Tracking 
Research and Applications  http://etra.cs.uta.fi/  COGAIN: Communication by Gaze Interaction http://www.cogain.org/ 
 APGV 2009: Symposium on Applied Perception in Graphics and Visualisation http://www.apgv.org/  TAP: 
ACM Transactions on Applied Perception http://tap.acm.org/  Andrew Moss  Rachel McDonnell  GV2 Group, 
Trinity College Dublin  Mashhuda Glencross, Dave Shreiner, Stephen Spencer  Collaborators  Veronica 
Sundstedt SIGGRAPH09  Diego Gutierrez, Oscar Anson, Tom Wilcox, Erik Reinhard, Efstathios Stavrakis, 
Michael Wimmer Veronica Sundstedt SIGGRAPH09 Eye Tracking in User Studies Veronica Sundstedt, Graphics, 
Vision, and Visualisation Group, Trinity College Dublin, Ireland Veronica Sundstedt Lecturer in Computer 
Science Graphics Vision and Visualization Group Department of Computer Science Lloyd Institute 0.45 Trinity 
College Dublin Ph: +353 1 896 8436 Em: Veronica.Sundstedt AT cs.tcd.ie https://www.cs.tcd.ie/Veronica.Sundstedt/ 
This document contains the course notes from my introduction part to Eye Tracking in User Studies presented 
in the first session, as well as the notes from my main talk. Introduction Notes (From the first session 
with all three of us) Experiment stimuli Computer graphics and interactive techniques allow us to create 
a vast amount of visual stimuli. Knowledge of human perception can affect the creation of these images 
and virtual environments. Over the last few years the evaluation of computer graphics stimuli has become 
increasingly important. There are many different techniques for evaluating stimuli some of which involve 
human participants. This third part of the course will discuss how eye tracking can support user studies. 
Apart from using eye tracking as an interaction device in virtual environments, it can also be a helpful 
tool in usability testing and evaluation of algorithms and techniques. We will describe experimental 
methodologies based on case studies in computer graphics. We will discuss what additional data we can 
collect and analyze using eye tracking, which could not have been measured explicitly using questionnaires. 
We will also talk about how eye tracking has been and can be used in different application areas related 
to computer graphics and interactive techniques. What is visual attention? The information in the environment 
that reaches our eyes is much greater than our brain can process. Selective visual attention is a complex 
action consisting of conscious and subconscious processes in the brain, which are used to extract relevant 
information in a quick and efficient manner (Rensink 2003). There are two general visual attention processes, 
termed bottom-up and top-down, which determine where humans locate their attention (James 1957). In bottom-up 
processing the visual stimulus capture attention automatically without volitional control (Itti et al. 
1998). Several computational models have been developed, which aim to show what Veronica Sundstedt _ 
SIGGRAPH09 attracts attention in an environment. Low-level, bottom-up features which influence visual 
attention include contrast, size, shape, colour, brightness, orientation, edges, and motion. In contrast, 
top­down processes focus on the observer s goal; they depend on the task.  The top-down approach was 
studied by Yarbus (1967), who asked an observer to look at a picture while their eye movements were recorded. 
Yarbus study showed that the scanpath was influenced by the question being asked of the observer while 
studying the picture. Previous work that have exploited models of human visual attention in computer 
graphics include work by Yee et al. (2001), Cater et al. (2003), and Sundstedt at al. (2005). What is 
eye tracking? Eye tracking in computer graphics and interactive techniques user studies is a relatively 
new phenomenon. Recent advancements in eye tracking technology, specifically the availability of cheaper, 
faster, more accurate and easier to use trackers, have inspired increased eye movement and eye tracking 
research efforts (Duchowski 2003). Eye-tracking is a process that records eye movements allowing us to 
determine where an observer s gaze is fixed at a given time. The point being focused upon on a screen 
is called a gaze point. Eye tracking allow us to capture the gaze of an observer. The direction of gaze 
indicates where humans focus their attention. Eye-tracking techniques make it possible to capture the 
scan path of an observer. In this way we can gain insight into what the observer looked at, what they 
might have perceived, and what drew their attention (Duchowski 2003). We can also study how tasks affect 
gaze behaviour as seen in the figure above. The left image shows an observer free-viewing the Kalabsha 
temple scene and the image to the right shows the eye movements of the same observer performing the task 
of counting the stones in the courtyard. Top-down processing The figure shows seven scanpaths from one 
observer: (1) free­viewing, (2) estimate the material circumstances of the family, (3) estimate their 
age, (4) guess what the family did before the unexpected visitor arrived, (5) remember their clothes, 
(6) remember the position of the people and the objects, and (7) estimate how long the unexpected visitor 
had been away. Adapted from Yarbus (1967). Image used with permission from Springer Science and Business 
Media.  Veronica Sundstedt _ SIGGRAPH09 2 Bottom-up processing Examples of maps from the corridor scene 
Frame 1 (left) and saliency map (right). The saliency map is generated using the algorithm by Itti et 
al. (1998). Why use eye tracking in user studies? Incorporation of eye tracking techniques in user studies 
can provide us with insights that would not be available from more traditional user testing methods. 
Typically user studies rate human performance using measures such as time for task completion, participant 
success rate, and other subjective ratings. However, there are specific questions that can be difficult 
to answer using these traditional techniques. Imagine that an observer in your experiment is watching 
the stimuli for a long time before making a decision. Afterwards you as the experimenter have a very 
limited knowledge of what they might have looked at while making their decision. You can try and find 
out what people looked at by using questionnaires afterwards or think-aloud protocols during your experiment. 
However, think-aloud protocols might not always be suitable for your specific experiment. The participants 
might also not remember or describe what they saw in a sufficient manner. Knowing specifically what they 
looked at could have a significant impact on the design of your graphics algorithms and interactive techniques. 
Schnipke and Todd (2000) highlight that eye tracking should be used only if there is no alternative method 
for collecting the necessary data. What can eye tracking add? Support traditional data  Measuring human 
performance  Evaluating human interaction  Testing usability  Measuring importance (e.g. attention) 
 Measuring cognitive processes  Understanding task behaviour  Explaining individual differences  
How does it affect what we do? Developing new metrics and design principles  Making algorithms more 
efficient  Evaluating perceptually adaptive graphics  Optimising perceived quality  Enhancing the 
user experience  Constructing new research questions  Veronica Sundstedt _ SIGGRAPH09 3 Topics Eye 
tracking in user studies o What is eye tracking? o Measuring eye movements  Eye tracking technology 
 Eye movement analysis  o Experimental design o What can we measure? o Analysis and statistics 
  Case studies o Diagnostic uses of eye tracking o Interactive uses of eye tracking   Preview: Take 
home Eye tracking can add important objective information  Take the human observer into account in 
the design  Knowledge of eye movements can be used to improve our algorithms and techniques  Eye tracking 
can also be used as an alternative means of interaction  Plan your study, pilot your setup, minimise 
all risks that can affect your data recording, analyse your data  Main talk notes (My part) Title and 
Outline The third part of the course will discuss how eye tracking can support user studies. As we have 
mentioned in the introduction to this course computer graphics and interactive techniques allow us to 
create a vast amount of visual stimuli. Knowledge of human perception can affect the creation of these 
images and virtual environments. Apart from using eye tracking as an interaction device in virtual environments, 
it can also be a tool in usability testing and evaluation of algorithms and techniques (O'Sullivan 2001). 
Here is an outline of the third part of the course. First I will talk about what eye tracking is and 
what additional data we can obtain by using eye tracking systems. I will discuss the basis of eye­movement 
technology and describe how the collected information may be analysed using different eye movement metrics. 
I will go through the pipeline for working with eye trackers. I will highlight some important issues 
related to the experimental design process if you are interested in using eye tracking in your user studies. 
The aim is to share some experiences and lessons learned when using eye tracking systems in user studies. 
Some different methods for visualising eye tracking data will also be presented and I will talk about 
techniques that can be used to perform statistics. Finally I will describe some case studies in computer 
graphics and interactive techniques which have used eye tracking for diagnostic and interactive purposes. 
Eye tracking setup (top left in slides), Image courtesy: Howlett et al. 2005. Veronica Sundstedt _ SIGGRAPH09 
4 What is eye tracking? Using eye tracking in computer graphics and interactive techniques user studies 
is a relatively new phenomenon. Nowadays eye tracking technology has advanced and it is possible to obtain 
both cheaper, easier to use, faster, and more accurate eye tracking systems (Duchowski 2003). Eye-tracking 
is a process that records eye movements allowing us to determine where an observer s gaze is fixed at 
a given time. The point being focused upon on a screen is called a gaze point. Eye tracking allow us 
to capture the gaze of an observer. The direction of gaze indicates where humans focus their attention. 
Eye-tracking techniques make it possible to capture the scan path of an observer. In this way we can 
gain insight into what the observer looked at, what they might have perceived, and what drew their attention 
(Duchowski 2003). Eye tracking can be a crucial tool in studying usability problems (Karn 2006). We 
can also study how tasks affect gaze behaviour as seen in the figure above. The left image shows an observer 
free-viewing the Kalabsha temple scene and the image to the right shows the eye movements of the same 
observer performing the task of counting the stones in the courtyard. There are several application areas 
for using eye tracking (Duchowski 2002). Eye tracking has previously extensively been used in psychology, 
neuroscience, human factors, human computer interaction, in particular in the evaluation of web design 
and advertising to find out what people look at and how sites can be made more efficient for specific 
tasks etc. What do we see? As I mentioned in the introduction of this course the information in the environment 
that reaches our eyes is much greater than our brain can process. Humans use selective visual attention 
to extract relevant information. Visual acuity relates to the resolution limit of the eye and our ability 
to resolve fine details (Snowden et al. 2006). Due to the uneven distribution of photoreceptor cells 
in the retina humans have higher visual acuity in the fovea. The figure shows a schematic representation 
of the human visual field. The foveal vision has an area of approximately2°. Adapted from MacEvoy (2007). 
Veronica Sundstedt _ SIGGRAPH09 5 Eye movements As described in the previous slide, the highest visual 
acuity is obtained in the foveal region. To reposition the image onto this area, the human visual system 
uses five basis types of eye movements: saccades, smooth pursuits, vergence, vestibular ocular reflex, 
and optokinetic reflex (Duchowski 2003). The eye movements are controlled by six muscles, which allow 
the eye to move within six degrees of freedom. The five types of eye movements are summarised below based 
on descriptions by Vilis (2006) and Duchowski (2003): Saccades are fast and ballistic eye movements 
used to reposition the fovea. Ballistic means that when the saccade has begun the final destination cannot 
be changed. These movements are both voluntary and reflexive and last from 10-100 ms. There is virtually 
no visual information obtained during a saccade.  Smooth pursuits are movements used when tracking a 
visually moving target. Although it is dependent on the range and the speed of the target, the eyes are 
normally capable of matching its velocity.  Vergence movements are used for depth perception to focus 
the pair of eyes over a distant target.  Vestibular ocular reflex (VOR) movements are used to fixate 
the eyes on an object even if the head rotates. It works even if the eyes are closed.  Optokinetic reflex 
(OKR) movements are used to account for the motion of the visual field. They produce a sense of self-motion 
which can be experienced when sitting in a stationary train and the opposite train starts to move.  
Between eye movements fixations occur, which often last for about 200-300 ms and are rarely shorter than 
100 ms (Snowden et al. 2006). Approximately 90% of viewing time is spent on fixations (Duchowski 2003). 
During a fixation the image is held approximately still on the retina; the eyes are never completely 
still, but they always jitter using small movements called tremors or drifts (Snowden et al. 2006). The 
trajectory between fixation points is usually called a scanpath (Noton and Stark 1971). The image shows 
a scan path from an observer looking at the Lucy statue. The circles indicate fixation points where a 
larger radius represents longer fixation duration. The different types of eye movements are normally 
divided into two categories: stabilising movements and saccadic movements (Lukander 2003). Stabilising 
movements are responsible for holding an image still on the retina, and include smooth pursuits, VOR, 
OKR, and fixations. Saccadic movements on the other hand have the purpose of bringing objects of interest 
onto the fovea and include saccades and vergence movements (Lukander 2003). Rendering of the Lucy scene 
(Image courtesy of Diego Gutierrez and Oscar Anson). The model of Lucy was created by the Stanford University 
Computer Graphics Laboratory. Veronica Sundstedt _ SIGGRAPH09 6 Eye traccking appliccations There arre 
different ttypes of eye tracking appplications shoown in the h ierarchy beloow. Broadly eye trackingg 
systems cann be divided into two cattegories: inteeractive and diagnostic syystems. The image below iss 
adapted froom Duchowsski (2001). In interactive ssystems the user s gaze iis used to intteract with thee 
application . Interactive systems aree also dividedd into selectiive and gaze -contingent systems. In 
selecctive applicattions, the us er s gaze can be used as an a alternative innput device, for examplee 
as a selectioon tool (Duchowski 2001). WWork in percceptually  adaptivve graphics mmostly fall intto 
the gaze-coontingent cattegory, wherre parts of the vvirtual enviroonment are mmodified based oon the 
gaze oof the observver (Luebkee et al. 2000)). Gaze-contingent tecchniques are also dividedd into screen 
-based and mmodel-basedd techniques. Screen­based mmethods mannipulate the fframe bufferr prior to dis 
play, while mmodel-based methods ai ms at reducingg resolution by directly mmanipulatingg the geomettry 
before re ndering (Ducchowski 20001).  In contraast to intera ctive applicaations, diagn ostic eye tra 
cking systemms involves r ecording eyee movemeents over timme, as mentiooned earlier are known aas scan 
pathss. This data i s later analyysed. Diagnostic applicatioons do not geenerally requuire that thee 
display reaccts to the gazze of the obsserver. In this taalk we will mmainly describbe how eye ttracking 
is ussed in diagnoostic applicattions. Eye traccking technnology Many diifferent type s of eye traccking 
techniqques have beeen developeed since it waas first used in reading research aboout 100 yearrs ago 
(Rayneer and Pollattsek 1989, N ilsson 2009) . There are t wo general techniques ffor studying eye movemeents: 
(1) by mmeasuring thhe position oof the eye rellative to the headd, or (2) by mmeasuring thhe orientatio 
n of the eye in space (Duuchowski 20003). The mosst commonn system for the second technique iss the video-based 
corneaal reflection eeye-tracker (Duchowwski 2003). VVideo-based eye trackingg is the most common tecchnique 
usedd today. Videeo­ based eye trackking systems can be headd-mounted, pportable, or wwearable. Thhere 
are also electronic mmethods in wwhich skinn electrodes are used aroound the eyees to measurre the potential 
differeences in the eye or mechhanical methods whiich uses conttact lenses wwith a metal coil around 
tthe edge (Aaltonen 2005 ). Most eye ttrackers todaay use video images of t he eye to deetermine wh ere 
a personn is looking, i .e. their poinnt of regard (Poole and BBall 2004). In video -based eye-ttrackers, 
the light sourcee reflection oon the corneaa (caused byy infra-red ligght) is measureed relative too 
the locatio n of the puppil s centre (DDuchowski 20003). These ttwo points aare used as refereence to 
com pensate for head movemments (Duchoowski 2003).. The cornea l reflections are also known aas Purkinje 
reeflections orr Purkinje im ages (Duchoowski 2003). Due to the pproperties off the eye four Purrkinje 
reflect ions normal ly appear (Rääihä 2005). TThe first refleection is at thhe front of t he Veronicaa 
Sundstedt _ SIGGRAPHH09 cornea, the second aat the rear o f the cornea , the third att the front off 
the lens, annd the fourthh at the rear of tthe lens. Videeo-based eyee-trackers noormally locatte the 
first P urkinje imagge (Duchows ki 2003). There arre several immportant termms used in eyye tracking. 
TThe accuracyy of the eye tracker showws the expecte d difference in degrees oof visual ang le between 
ttrue eye pos ition and meean computeed eye position during a fixaation (Aaltonnen 2005). T he accuracy 
is usually 0.55-1O.. The sppatial resolut ion of the eye tracker showws the smalleest change inn eye 
positioon that can b e measured and the temmporal resolutioon or sampli ng rate is thee number off recorded 
eyye positions per second ((Aaltonen 20005). Many eyye trackers hhas a samplinng rate of ab out 60-120HHz. 
For usabillity studies 660Hz can be sufficiennt (Poole andd Ball 2004). Video-bbased eye ttracking For 
exammple, the To bii x50 eye-ttracker uses near infra-reed light-emittting diodes ((NIR-LEDs) a nd a 
high-ressolution cam era with a laarge field of vview (Tobii 22006). The NIIR-LEDs and the camera are 
used to generrate the Purkkinje images of the eyes and to captuure images oof the observver. Eye trackking 
softwaree consists of image proceessing algoritthms that exxtract importtant featuress, such as thee 
eyes and the Purkinje imaages generatted by the N IR-LEDs. It caan also calcu late a three--dimensional 
position in space of where the e yes were loccated to deteermine the ggaze point at a given timee. The 
image b elow shows Purkinje imaages (left) annd (right) relaative positio ns of pupil a nd first Purkkinje 
images aas seen by thhe eye-trackeer s camera during calibrration. Adaptted from Du chowski (20003) and 
Räihä (22005). Experimmental desi gn There arre many thinngs to consid er in the expperimental d esign 
processs. I will discuuss some iss ues related tto piloting, pparticipants, calibration, recording, a 
nalysing andd visualizing eeye movemeents. Experimmental desi gn: pilotingg Before rrunning any eexperiment 
iit is importannt to run piloot tests. Thhis is also truue for eye traacking experiiments. This can helpp 
you avoid mmistakes in t he real expe riment that could afffect your da ta collectionn. First it is immportant 
to test you r setup. If thhe participannt should inteeract with thhe computeer for exampple by using 
mmouse or ke yboard it is important to make ssure this doees not interfeere with yourr Veronicaa Sundstedt 
_ SIGGRAPHH09 recordinngs. It is also important too consider wwhat additionnal informatiion you mighht 
want to reecord, such as vvideo footagge, mouse annd keyboard input etc. You sho uld also makke sure 
that aall areas of t he screen caan be covere d by the eyee movementss. Some eye tracckers can havve 
problems with varyingg accuracy accross the screeen, for exammple the tra cking quality i s worse in 
thhe upper leftt and right coorners. A meethod to test for this is too use a grid t hat covers t he whole 
scrreen and alloow different subjects to llook at varioous points co vering the wwhole screen. Many eye 
traacking syste ms allow youu to display tthe gaze poinnt in real-timme. You can i nstruct the obseerver 
to lookk at differentt points on thhe screen, foor example loook at the toop right squa re as shown inn 
the figure. It can be ve ry time conssuming to finnd these typees of errors aafter runningg all participaants. 
If you ccannot guaraantee the acccuracy over tthe whole sccreen you ca n consider d isplaying your stimmuli 
in the ceentre where the accuraccy is sufficiennt. It is alsoo a good ideaa to measuree and record distances 
in your setup ffor later usee. Some eye tracking prooviders suggeest distancess to use fromm the observer 
to the eye tracker for mmaximum ac curacy (if yo u are workinng with a p ortable eye tracker). If y ou 
would likke to photogrraph your seetup it is also better to doo this in yourr pilot study to avoid distturbance 
to yyour participaants. As Marrina discusse d in more deetail many unniversities haave ethics coommittees 
thhat need to eevaluate youur experimennts before thhey can be cconducted. NNormally partticipants takking 
part in thhe experime nts need to sign a conseent form. A ppilot study shhould alwayss be done to obtain 
feeedback on yyour questioonnaires and instructionss. It is imporrtant to makee sure that thhe participannts 
understannd the informmation you ggive them. Byy using writt en instructiions you makke sure that all 
participa nts are givenn the same innformation. It is commonn that you nee d to refine t hese instructtions 
as an ooutcome of t he pilot studdy before runnning your reeal experimment. Image ccourtesy: Mi crosoft 
Poweerpoint. It is imp ortant to thi nk about thee questions iin your questtionnaire. Heere you havee 
the opport unity to discoverr informationn about yourr participant that could aaffect the reccording 
of eyye movemen ts in your stuudy in variou s ways, such as age, sex, backgroundd knowledge,, eye problemms 
related too colour blindnesss, stereo vission, lazy eyee, etc. If a pa rticipant is iddentified witth 
an eye prooblem that c ould affect thhe study this participant can then be excluded beefore the starrt 
of the exp eriment. Experimmental desi gn: particippants It is diffi cult to say e xactly how mmany 
particippants are needed in an eye traacking study.. Adding morre participannts will makke the data qquality 
more accurate. St randvall (20009) states thhat eye trackking can be uused both forr qualitative and 
quantitaative studies and hence tthe results shhould be interpreeted and usedd differentlyy depending 
oon this. In general the number of participa nts you needd depend on what stuudy you will conduct andd 
what stimu li and tasks yyou will havee. Image couurtesy: Microosoft Powerppoint. Veronicaa Sundstedt 
_ SIGGRAPHH09 If you are interested in quantitative research, such as usability studies including design 
or focus group studies in which the purpose is to identify specific problems only a smaller number of 
representative participants are needed (Strandvall 2009). The participants you use are then tested for 
a longer period. Adding more participants might not necessarily mean that you will find more usability 
or design problems (Strandvall 2009). Nielsen (2000) writes that by using five participants you can find 
around 85% of the usability problems, and if you include around 15 you can identify all. If you are planning 
to run a qualitative experiment the sample size depends on a number of factors identified by Strandvall 
(2009) as: 1. The subgroups (cells) that will be analysed independently (male/female) 2. The degree 
of risk involved in the decisions being made on the results 3. The amount of time and available resources 
 4. Which statistical tests you plan to use for analysing the data 5. The margin of standard error of 
the mean and significance levels accepted in the results  Poole (2005) recommends using around 30 participants 
and to use a within subject experimental design if possible, which needs less participants than a between 
subject design. You should also recruit more participants than you need since some of them are likely 
to give you calibration issues or will be excluded based on some questions in your questionnaire. As 
Marina mentioned there are various ways of obtaining participants. Be aware of their prior knowledge, 
biased experience and other things that could affect gaze behavior. I ran one experiment once with two 
groups (one performing a task and one free-viewing). The participants were volunteers from a student 
population and they were asked not to talk about the experiment to their friends until the experiment 
was over, one student was assigned a free-viewing condition but started counting objects in the image, 
it was obvious that they had spoken to each other and I had to remove the participant which took time 
and effort. You should also consider if your participants need to practice beforehand and remember that 
you limit the length of the experiment, which can affect tasks, tiredness, and learning effects (Poole 
2005). Strandvall (2009) also give some general rules for how many people you need. For complex stimulus 
you need more people than for simple stimulus. If the individual objects that are important for your 
analysis are highly visible you need less people, whereas if they have low visibility you need more people. 
The task complexity also affects how many people you need. In more complex tasks you need more people 
then in simpler tasks. If there is no task you also need more participants most times (Strandvall 2009). 
Experimental design: calibration Before starting a recording the video-based eye trackers need to be 
fine-tuned to the participant in a calibration process (Poole and Ball 2004). Calibration of the eye-tracker 
is achieved by measuring the gaze of the observer at specific grid points (usually 5, 9, or 16) (Duchowski 
2003, Goldberg and Wichansky 2003). The Purkinje images then appear as a small dot close to the pupil. 
 To avoid that the participant moves after calibration it is good to let them read instructions first 
and then perform the calibration procedure. Alternatively the instructions can be displayed on the screen. 
To increase the accuracy in your recording a chin rest could be used in the calibration Veronica Sundstedt 
_ SIGGRAPH09 10 process and during your experiment. Chin rest image courtesy of Richmond Products, Inc. 
It is also possible to improve the accuracy if you include breaks in your study for recalibration (Poole 
2005). Experimental design: recording issues In eye tracking there are several issues that can affect 
recording (Schnipke and Todd 2000). These can broadly be categorised into three areas: (1) physiological 
reasons, (2) external reasons, and (3) internal reasons. First, eye trackers are known to have some issues 
calibrating various lenses (bi-focal/tri-focal, super-dense, hard), lazy eye, large and small pupils, 
if there is a low contrast between pupil and eye white, and if the eyelid covers part of the pupil (Poole 
2005). Another reason is that the pupil does not reflect enough light. Participants wearing eyeglasses 
or hard contacts can also be two external reasons why recording can be difficult (Schnipke and Todd 2000). 
There are several internal reasons which can affect recording, including that an experimental setup is 
not a natural situation and the participants can be affected by learning. In some cases participants 
can also assume a task even if it was not given and try and guess what the experiment is about (Sundstedt 
et al. 2008). There are also other issues that can affect recording. For example if the participant is 
moving their head this could cause a delay until the eye tracker is capturing the eye again and it can 
also be a reason for calibration loss (Schnipke and Todd 2000). However, if a participant is restrained 
the actual situation can become unnatural and this could possible affect how they act and perform, and 
what they look at. If the participant is going to use the mouse and keyboard they might potentially move 
which then could result in loss of calibration. There are also other issues associated with eye­tracking 
technology which was identified at the CHI 1999 workshop The hunt for usability: tracking eye movements 
(CHI Workshop 1999). Filtering raw data Eye-trackers normally produce a large amount of raw data since 
humans perform several saccades per second. This raw data contains many parameters (for the left and 
right eye), including gaze point (x,y), pupil location in the camera image (x,y), distance from camera 
to eye, pupil size, time stamp in ms, and a frame number (Tobii 2006). The raw data needs to be filtered 
and reduced before it is analysed. In this process it is common to identify fixations and saccades (Rothkopf 
and Pelz 2004). Blinks can also be identified. It could be the case in certain situations that your recorded 
data needs to be corrected for drift. There are usually two different types of errors: an absolute drift 
in which all points have shifted together, or a relative warp which is harder to correct for (Poole 2005). 
Normally drift correction is included in modern eye trackers (Majaranta 2008). The identification of 
fixations is a complex problem and there is no unique method for filtering the raw data (Salvucci and 
Goldberg 2000). Many systems use a window-based segmentation algorithm (Salojärvi et al. 2005). A window 
is parameterised by a size of x pixels and a sequence of n consecutive frames. The algorithm draws a 
square of width and height x pixels around the gaze point location. If the following location is within 
the square it is a potential fixation. If n consecutive gaze Veronica Sundstedt _ SIGGRAPH09 11 locations 
are inside the square for the candidate location, the n points will be treated as a fixation. The duration 
of the fixation is n times the sampling interval. There are also velocity-based techniques in which the 
max velocity for a fixation is detected, if it is exceeding max it is a saccade (Salvucci and Goldberg 
2000). Type Description Timestamp Timestamp in milliseconds for when the gaze data was collected Gaze 
PointX Left Horizontal screen position of the gaze point for the left eye Gaze PointY Left Vertical screen 
position of the gaze point for the left eye CamXLeft Horizontal location of the left pupil in the camera 
image CamYLeft Vertical location of the left pupil in the camera image DistanceLeft Distance from the 
eye tracker to the left eye PupilLeft Size of the pupil (left eye) in mm ValidityLeft Validity of the 
gaze data (e.g. 0 = good tracking quality) Source: Tobii Technology What can we measure? There are many 
different types of eye tracking metrics (Nilsson 2009). In general, the more attention an area has received 
the more fixations are clustered there (Tobii 2006). Another commonly used metric is studying the average 
fixation duration for an area. One of the most common methods for analysis is to count the number of 
fixations, their duration, and look at the order in which they appear. You can also define areas of interest 
and study what percentage of fixations were in each region, what the total duration was for the region, 
and in which order these regions were attended. It is important to define the areas of interest large 
enough to capture all relevant eye movements (Poole and Ball 2004, Poole 2005). You can also measure 
time to first fixation and how long it takes to complete a specific task, or example finding a specified 
object. The video-based eye tracking systems can also provide measures of the pupil size and if the participant 
blinked. For further information a detailed description of eye­tracking metrics and their interpretation 
is provided by Poole and Ball (2004). For further information regarding what can be measured Jacob and 
Karn (2003) summarise 20 usability studies incorporating eye tracking. They discuss which users they 
used, what the tasks were, and what eye tracking related metrics were used. Veronica Sundstedt _ SIGGRAPH09 
12 What does it mean? The duration of an element being looked at reflects the importance. This could 
also mean that the user is engaged (Just and Carpenter 1976). It has also been reported that there is 
a negative correlation between the total number of fixations on a design and search efficiency (Tobii 
2009). A larger number of fixations can indicate poor search efficiency (Goldberg and Kotval 1999) and 
very random fixations that the participant is confused (Tobii 2009). Eye tracking metrics have also been 
knows not relate to interest and arousal by measuring the pupil dilation (Spillers 2004). Poole and Ball 
(2004) describe more eye movement metrics and how they can be interpreted in the context of interface 
design and usability evaluation. The images on the right show the scan paths of two participants, the 
upper image while performing a task and the lower image while free-viewing. The task was to count the 
fire safety items in the virtual environment. It can be seen that while free-viewing the participant 
was also attending the salient fire safety objects (Sundstedt et al. 2005). Visualising eye movements 
 There are many different ways of visualising eye movements apart from using the underlying numbers to 
compute various statistics. I will show a few examples from a study we undertook in which the participants 
were asked to choose which of two images were most similar to a reference (Sundstedt et al. 2007). The 
visualisation techniques are gaze replay in real­time, scan path or gaze plot(s), and heat maps. As you 
can see in the image on the right it is also possible to visualise the number of fixations per area of 
interest. The circles indicate fixation points where a larger radius represents longer fixation duration. 
The values are also shown in a graph. The bee swarm effect shows were everyone looked at the same time 
(Nilsson 2009). Rendering of the Road scene (Image courtesy of Diego Gutierrez and Oscar Anson).  Veronica 
Sundstedt _ SIGGRAPH09 13 Visualising eye movements: Gaze replay This video shows the gaze replay in 
real-time of an observer performing a 2AFC task. Rendering of the Lucy scene (Image courtesy of Diego 
Gutierrez and Oscar Anson). The model of Lucy was created by the Stanford University Computer Graphics 
Laboratory. Visualising eye movements: Scan paths This image shows the scan path or gaze plot of the 
same observer. Visualising eye movements: Heat maps One method for displaying graphically where an observer 
or a group of people focused upon is to create a hotspot visualisation (Tobii 2006). These hotspot visualisations 
consist of the experiment stimuli as a background image with a hotspot mask superimposed on top. The 
hotspot mask consists of a black background, with highlighted areas according to the duration that observers 
focused upon each area. The heat maps can also be based on the fixation count. Item buffer analysis Often 
in computer graphics and virtual environments we are interested in determining which objects in the scene 
are focused on, at any time during an experiment's session. The fixation points therefore need to be 
mapped to the objects making up the scene. Using an item buffer, each scene object is first assigned 
a unique number that represents a color value. Each frame of the session is then rendered out as an item 
buffer image, shown in the figure, which color codes all scene objects (Sundstedt et al. 2008). Each 
item buffer is also stamped with a start and end time over which it was displayed. Thus fixation points 
can be subsequently correlated, both temporally and spatially, with the scene objects using these item 
buffers. Splatting fixations to objects The item buffer is used to map fixation points to objects. During 
a trial, we count how many times each object was fixated upon. In eye tracking experiments, it is common 
to use relatively sparsely populated environments, because the analysis of fixation points becomes more 
complicated if several objects are located in the vicinity of a fixation point (Lee et al. 2007). A common 
technique is to simply select the object directly underneath the fixation point. However, games environments 
can be cluttered and complex, making such a simple approach error-prone. We have therefore developed 
a novel approach, taking the distance between pixels and a fixation point into account (Sundstedt et 
al. 2008). With the accuracy of the eye tracker at 0.7°, and the foveal region of human vision spanning 
approximately 2° of visual angle (Palmer 1999), the area over which a fixation point bears relevance 
is a circle with a radius of around 43 pixels (corresponding to a solid angle of 2.7°). Veronica Sundstedt 
_ SIGGRAPH09 14 Splatting algorithm Considering frame number f , we assign a non-zero weight wf to all 
pixels (xp; yp) in a window of 400×400 pixels (large enough to be accurate, and small enough to be computationally 
efficient) according to their distance to the fixation point (xf ; yf ): where s = 43 pixels. With the 
aid of the item buffer, the weights computed for frame f are then added to the weights of the different 
objects. This process is repeated for all frames for which a fixation point exists. After normalization, 
necessary to account for the fact that each trial may last for a different amount of time, this produces 
a weight wi associated with object i. After classification, each participant produces a normalized distribution 
of fixations per object class. This set of distributions is then subjected to further analysis using 
traditional statistical tests (Sundstedt et al. 2008). Analysis and statistics The four conditions are 
developed to test the distribution of fixation points over object classes in different tasks (active 
and passive game play for example).The distribution of fixations over the different object classes for 
each participant and each condition is shown in the figure. Not that these distributions are markedly 
different from the distribution one would obtain by counting the number of pixels that are covered by 
each object type as shown in the graph. Thus, this indicates that none of the results presented next 
can be explained by random fixation behavior (Sundstedt et al. 2008). Case studies using eye tracking 
I will now highlight a few case studies that have used eye tracking as a tool in the development of algorithm 
and techniques, but also as an interaction device. I will briefly describe the background and problem 
statement, what type of analysis was performed, and what some of the insights and major results were. 
I cannot talk about all work that has been done in the field, but some more related work can be found 
in (Howlett 2005, Marmitt and Duchowski 2005, O Sullivan 2005). I will talk about three applications 
which have used eye tracking in diagnostic purposes and two applications for interactive uses. The different 
applications are (1) high-fidelity selective rendering, crowd variety in virtual environments, and fixation 
behavior in games, and (2) gaze based game interaction and depth-of-field blur and camera motions. Veronica 
Sundstedt _ SIGGRAPH09 15 Case study 1: High-fidelity rendering High-fidelity image synthesis is the 
process of computing images that are perceptually indistinguishable from the real world they are attempting 
to portray. Such a level of fidelity requires that the physical processes of materials and the behavior 
of light are accurately simulated. Most computer graphics algorithms assume that light passes freely 
between surfaces within an environment. However, in many applications, we also need to take into account 
how the light interacts with media, such as dust, smoke, fog, etc., between the surfaces. The computational 
requirements for calculating the interaction of light with such participating media are substantial. 
This process can take many hours and rendering effort is often spent on computing parts of the scene 
that may not be perceived by the viewer (Sundstedt 2007). Over the last few years computer graphics techniques 
incorporating models of the human visual system have increased dramatically. Developed techniques include 
perceptually-based rendering algorithms exploiting limitations of the human visual system. The main goal 
behind this research is to reduce rendering time without people noticing. Models of human visual attention 
have been used to identify perceptually important areas and to indicate where computational resources 
are best spent to obtain a high perceptual quality result. This results in a rendered image with a varying 
degree of accuracy. User studies have been run to assess the impact of selective rendering on perceived 
quality, for example by comparing a selectively rendered image to a gold standard (rendered with a higher 
number of rays per pixel). In the evaluation process of these models eye tracking can be used to study 
if the models actually predict what an observer will look at (Sundstedt thesis 2007). Case study 1: Analysis 
 To selectively render inhomogeneous participating media, the rendering system Lucifer by Guiterrez 
et al. (2005) was extended with a perceptually-based director. The first step in the process of rendering 
PM in a reasonable time is to generate the X-map and the S-map. These two maps are then combined into 
the directing XS-map. An XS-map is a grey scale image, which is used in the selective rendering process 
by rendering areas of the map with higher values (brighter) with a more accurate solution. Eye tracking 
can be used in combination with a user study to investigate how well these maps predict what our models 
predict are important. Veronica Sundstedt _ SIGGRAPH09 16 Case study 1: Results The results of the psychophysical 
experiment showed that the rendering cost could be reduced up to 10 times without the observers noticing 
the reduction in quality (Sundstedt 2007). However, for one scene in one condition they could notice 
a difference. Eye tracking allowed us to come up with some new research questions which was later implemented 
and explored in another study. Case study 1: Heat maps Eye tracking allowed us to identify that the 
longest duration were concentrated on the facial region of the Lucy scene. Observers also looked at the 
scattering around light sources, which lead to the development of a light map. Using this light map in 
combination with the previously mentioned XS-map allowed us to obtain a perceptually high quality result 
also with the road scene that caused problems in the initial experiment (i.e. in which participants could 
detect the reduction in quality). Rendering of the scenes (Image courtesy of Diego Gutierrez and Oscar 
Anson). Case study 2: Crowd variety in virtual environments It is important to include crowds in virtual 
environments. When you populate crowds you would ideally like to have as much variety as possible between 
your virtual characters (McDonnell et al. 2009). It is expensive to store a lot of different textures 
in memory, for example you might need different textures for legs, tops, patterns, faces, beards, etc. 
It is also expensive to purchase template models and to buy textures. The man hours involved in creating 
a lot of varying textures does also cost and even for colour variety it takes time to make outfits. McDonnell 
et al. (2009) used eye tracking in their user studies to identify which body parts of the virtual characters 
are most looked at in scenes containing duplicate characters or clones. By identifying which are the 
most salient parts of the characters the available resources can be used were they are most needed. The 
image shows the various types tested in the selective variation experiment: (1) original character, (2) 
top texture variation, (3) face geometry variation, (4) face texture variation, and (5-9) head accessories 
(McDonnell et al. 2009).  Image courtesy: McDonnell et al. 2009. Veronica Sundstedt _ SIGGRAPH09 17 
Case study 2: Analysis McDonnell et al. 2009 used an EyeLink II eye-tracker to record eye movements of 
participants when they were asked to indicate whether clones were present or not. They cast a ray from 
the fixation point into the scene to determine which of the body parts were being looked at. They also 
took into account inaccuracies in the fixation point and performed a nearest-neighbour search on 100 
pixels surrounding the fixation point to determine the closest body part. When a participant fixated 
on a body region of the mesh, they recorded the fixation duration for that region. They also recorded 
the fixation count on each region. Image courtesy: McDonnell et al. 2009. Case study 2: Results One 
of the main results they found was that the head and upper torso attracted the majority of first fixations 
in the scene and were hence attended most. This was the case regardless of orientation, presence of absence 
of motion, sex, age, size, and clothing style of the character. This indicates that modelers should prioritize 
upper body and facial regions. The eye tracking study proved that less texture memory needed to be used 
and this will also reduce the cost of commissioning textures. Image courtesy: McDonnell et al. 2009. 
For more information regarding setting up and running the eye tracking experiment in this study the paper 
Eye-Catching Crowds: Saliency-Based Selective Variation will also be presented at SIGGRAPH 2009 in the 
session Creating Natural Variations , which is on Wednesday, 5 August, 3:45 PM 6:00 PM, inHall E1-2. 
Case study 3: Fixation behaviour in games Prediction of gaze behavior in gaming environments can be a 
tremendously useful asset to game designers, enabling them to improve gameplay, selectively increase 
visual fidelity, and optimize the distribution of computing resources. The use of saliency maps is currently 
being advocated as the method of choice for predicting visual attention, crucially under the assumption 
that no specific task is present. This is achieved by analyzing images for low-level features such as 
motion, contrast, luminance, etc. However, the majority of computer games are designed to be easily understood 
and pose a task readily apparent to most players. This psychophysical experiment showed that in a task-oriented 
context such as gaming, the predictive power of saliency maps at design time can be weak. Thus, a more 
involved protocol utilizing eye tracking can be used, as part of the computer game design cycle, to predict 
fixation behavior of players. In this user study, we performed a psychophysical eye tracking experiment, 
carried out on an easy-to-understand game, which was purpose-designed for psychophysics (Sundstedt et 
al. 2008). The experiment demonstrated the extent to which gaze behavior may be altered by the presence 
of a task, even if participants are only instructed to observe a pre-recorded video, i.e. in the absence 
of a specific task. Veronica Sundstedt _ SIGGRAPH09 18 Case study 3: Analysis In this user study the 
game itself was modified so that it can start and stop the eye tracker automatically at the start and 
end of each trial. The analysis process depends on three main steps as outlined below. In the first pass 
the participant plays the game while being recorded using an eye tracker. All game states are logged 
so that it is possible to reconstruct each frame later. This is much less resource intensive than encoding 
a video at the same time as eye tracking and playing the game. This novelty with this approach is that 
it also allows us to play back the game in real-time which can be used for another condition or another 
group of participants. After the first pass we can reconstruct each frame and generate the additional 
data, such as the item buffer discussed previously. Finally it is possible to use the analysis tool which 
makes use of fixation splatting as described earlier (Sundstedt et al. 2008). 1st Pass  Real-time run, 
eye tracking, logging of all game state necessary to reconstruct each frame later  Less intensive than 
encoding a video of the game  Can be played back game in real-time yielding a replica of an earlier 
trial for the passive condition   2nd Pass Reconstruct each frame, generate additional data for each 
frame (item buffer, frame buffer, object data)  3rd Pass  Use analysis tool to get useful info out 
of the stored game and gaze data, generate importance for each object class Case study 3: Results The 
main result of this study showed that in games, a task is usually implied, even if no task is specified. 
Knowing this computational metrics that assumes absence of task, including saliency algorithms, should 
be applied with care to games. Another interesting result was that passive user behavior is task dominated 
and cannot be statistically distinguished from active gameplay behavior. Even if task-relevance can be 
hard to model the user study shows that it should play a larger role in models of gaze behavior than 
currently assumed. Using eye tracking in the game design cycle can help predict fixation behavior. This 
experiment showed that a modification to the game engine (namely tagging each object with a colour-coded 
identifier) is sufficient to map fixations points back to objects. While eye tracking eye tracking sessions 
during actual game design by necessity will be less controlled than this experiment, simply accumulating 
fixations over different object classes may prove to be a fruitful approach in understanding where game 
players focus their attention. Such information cannot currently be extracted from an analysis of low-level 
features alone (Sundstedt et al. 2008). Case study 4: Gaze based game interaction Eye movements can also 
be used as control signals to enable users to interact with interfaces (drawing, typing, etc.) without 
the need for mouse or keyboard. This can be a major advantage for disabled users in particular (Poole 
and Ball 2004). However, Jacob (1990) identified an important problem in eye-based interaction systems. 
If the eyes are used to substitute a mouse there can be difficulties to determine what intended selections 
are. Eye-tracking based interaction tends to suffer from the Midas Touch problem. This is where everywhere 
you look, another command is Veronica Sundstedt _ SIGGRAPH09 19 activated; you cannot look anywhere 
without issuing a command (Jacob 1990). Some solutions to this problem include incorporating blinks or 
voice commands. Alternative means of interaction in games are especially important for disabled users 
for which traditional techniques using mouse and keyboard are not feasible. Apart from voice recognition 
gaze has lately been used as an input modality in games (Jönsson 2005, Isokoski and BenoÎt, 2006, Smith 
and Graham 2006). Wilcox et al. (2008) present a 3rd person adventure puzzle game using a combination 
of non intrusive eye tracking technology and voice recognition for game communication. The figure to 
the right shows the game, and its first person sub games that make use of eye tracker functionality in 
contrasting ways: a catapult challenge (bottom left) and a staring competition (bottom right). More user 
studies incorporating gaze in gaming can be found here: http://www.cogain.org/links/gaze-controlled-games 
Case study 5: Blur and camera motions Hillaire et al. (2008) used an eye tracking system to improve some 
visual effects in virtual environments. They first retrieved the user s focus point using an eye tracking 
system. Using this focus point they propose two rendering techniques which aim to improve the users sensations 
during first-person navigation in the virtual environment. First they use a camera motion which simulates 
eye movements when walking, and secondly they use depth-of-field blur effects to simulate that humans 
only perceive sharp objects within some range of distances around the focal distance. Hillaire et al. 
(2008) also performed a subjective user study evaluating these visual affects showing that users preferred 
these effects when they were dynamically adapted to the focus point. Image courtesy: Hillaire et al. 
2008 Quake 3 Arena screenshot courtesy of Id Software. More information, including a video, regarding 
this study can be found here: http://www.irisa.fr/bunraku/Sebastien.Hillaire/ Obtaining an eye tracker 
 Buy  o Eye trackers are generally expensive Hire  Low cost solutions o http://www.cogain.org/eyetrackers/low-cost-eye-trackers 
 Loan Veronica Sundstedt _ SIGGRAPH09 20 o Explore collaboration possibilities with other departments 
or institutes (e.g. computer science, engineering, psychology) Take home messages Eye tracking can add 
important objective information  Take the human observer into account in the design  Knowledge of eye 
movements can be used to improve our algorithms and techniques  Eye tracking can also be used as an 
alternative means of interaction  Plan your study, pilot your setup, minimise all risks that can affect 
your data recording, analyse your data  Resources ETRA 2010: Eye Tracking Research and Applications 
 http://etra.cs.uta.fi/  COGAIN: Communication by Gaze Interaction  http://www.cogain.org/  APGV 
2009: Symposium on Applied Perception in Graphics and Visualisation  http://www.apgv.org/  TAP ACM 
Transactions on Applied Perception  http://tap.acm.org/  Acknowledgements Andrew Moss  Rachel McDonnell 
 GV2 Group, Trinity College Dublin  Mashhuda Glencross, Dave Shreiner, Stephen Spencer  Collaborators 
 o Diego Gutierrez, Oscar Anson, Tom Wilcox, Erik Reinhard, Efstathios Stavrakis, Michael Wimmer Bibliography 
Aaltonen A., Introduction to Eye Tracking, Tampere University Computer Human Interaction Group, 2005. 
Cater K., Chalmers A., and Ward G., Detail to attention: exploiting visual tasks for selective rendering. 
In EGRW 03: Proceedings of the 14th Eurographics workshop on Rendering, pages 270 280, Aire-la-Ville, 
Switzerland, Switzerland, Eurographics Association, 2003. CHI Workshop, The Hunt for Usability: Tracking 
Eye Movements, 1999. Veronica Sundstedt _ SIGGRAPH09 21 Duchowski A.T. , A Breadth-First Survey of Eye 
Tracking Applications, , Behavior Research Methods, Instruments, and Computers, Nov;34(4):455-70, 2002. 
 Duchowski A. T., Eye Tracking Methodology: Theory and Practice, Springer-Verlag, 2003. Duchowski, A. 
T., Eye Tracking Techniques for Perceptually Adaptive Graphics, ACM SIGGRAPH, EUROGRAPHICS Campfire, 
2001. Goldberg, H. J. and Kotval, X. P., Computer interface evaluation using eye movements: Methods 
and constructs. International Journal of Industrial Ergonomics, 24, 631-645, 1999. Goldberg, H. J. and 
Wichansky, A. M., Eye tracking in usability evaluation: A practitioner s guide. In J. Hyönä, R. Radach, 
&#38; H. Deubel (Eds.), The mind's eye: Cognitive and applied aspects of eye movement research (pp. 493-516). 
Amsterdam: Elsevier, 2003. Gutierrez D., Munoz A., Anson O., and Seron F. J., Non-linear Volume Photon 
Mapping. In Proc. of the Eurographics Symposium on Rendering Techniques, pages 291 300, 2005. Hillaire 
S., Lecuyer A., Cozot R., and Casiez, G., Using an Eye-Tracking System to Improve Depth-of- Field Blur 
Effects and Camera Motions in Virtual Environments, Proceedings of IEEE Virtual Reality (VR) Reno, Nevada, 
USA, pp. 47-51, 2008. Howlett S., Hamill J., and O Sullivan C., Predicting and Evaluating Saliency for 
Simplified Polygonal Models. ACMTrans. Appl. Percept., 2(3):286 308, 2005. Isokoski P. and BenoÎt M., 
Eye tracker input in first person shooter games. The 2nd Conference on Communication by Gaze Interaction 
COGAIN 2006: Gazing into the Future, 2006. Itti L., Koch C., and Niebur E., A Model of Saliency-Based 
Visual Attention for Rapid Scene Analysis. IEEE Trans. Pattern Anal. Mach. Intell., 20(11):1254 1259, 
1998. Jacob, R. J. K. and Karn, K. S., Eye tracking in Human-Computer Interaction and usability research: 
Ready to deliver the promises, In J. Hyönä, R. Radach, &#38; H. Deubel (Eds.), The mind's eye: Cognitive 
and applied aspects of eye movement research (pp. 573-605). Amsterdam: Elsevier, 2003. Jacob R. J. K., 
What you look at is what you get: eye movement-based interaction techniques, CHI '90: Proceedings of 
the SIGCHI conference on Human factors in computing systems, 11-18, 1990. James W., The Principles of 
Psychology. Dover Publications Inc., 1957. Just, M. A., and Carpenter, P. A., Eye fixations and cognitive 
processes. Cognitive Psychology, 8, 441­480, 1976. Jönsson E., If looks could kill an evaluation of 
eye tracking in computer games. Masters Thesis, Royal Institute of Technology, Stockholm, Sweden, 2005. 
Karn K. S. , Eye Tracking for Usability Testing, You've Got to Know Your Strengths and Weaknesses, 2006. 
 Lee S., Kim G. J., and Choi S., Real-time tracking of visually attended objects in interactive virtual 
environments. In ACM Symp. on Virtual Reality Software and Technology, 29.38, 2007. Veronica Sundstedt 
_ SIGGRAPH09 22 Luebke D., Hallen B., Newfield D., and Watson B., Perceptually Driven Simplification 
Using Gaze-Directed Rendering. Tech. Rep. CS-2000-04, University of Virginia, 2000. Lukander K., Mobile 
usability: Measuring gaze point on handheld devices. Master s thesis, Helsinki University of Technology, 
2003. McDonnell R., Larkin M., Hernandez B., Rudomin I., and O'Sullivan C., Eye-catching Crowds: Saliency 
based selective variation, ACM Transactions on Graphics (SIGGRAPH 2009), 28, (3), 2009. MacEvoy B. The 
Visual Field. http://www.handprint.com, 2007. Majaranta P., Implementation of New Interaction Techniques: 
Eye Tracking, TAUCHI, Visual Interaction Research Group, 2008. Marmitt G. and Duchowski A. T., Modeling 
Visual Attention in VR: Measuring the Accuracy of Predicted Scanpaths. In Eurographics 2002, Short Presentations, 
pages 217 226. The Eurographics Association, 2002. Nahman, Using eye tracking for usability testing: 
a research note, 2001. Nielsen J., Alertbox, Why You Only Need to Test with 5 Users, March 19, 2000. 
http://www.useit.com/alertbox/20000319.html Nilsson T., A Tobii Technology Introduction and Presentation, 
And How Tobii Eye Tracking Could be used in advertising, at Beyond AdAsia2007, Jeju Island, Korea, 2009. 
Noton D. and Stark L. W., Scanpath in Saccadic Eye Movements While Viewing and Recognizing Patterns. 
Vision Research, 11:929 942, 1971. O Sullivan C., Collisions and Attention. ACM Trans. Appl. Percept., 
2(3):309 321, 2005. O'Sullivan C., Dingliana J., Bradshaw G., and McNamara A., Eye-tracking for Interactive 
Computer Graphics, Abstract of Talk, The 11th European Conference on Eye Movements (ECEM 11), Turku, 
Finland, 2001. Palmer S. E., Vision science: Photons to phenomenology. MIT Press, Boston, 1999. Poole, 
A., Ball, L. J., and Phillips, P., In search of salience: A response time and eye movement analysis of 
bookmark recognition. In S. Fincher, P. Markopolous, D. Moore, &#38; R. Ruddle (Eds.), People and Computers 
XVIII-Design for Life: Proceedings of HCI 2004. London: Springer-Verlag Ltd., 2004. Poole, A. , Tips 
for Using Eyetrackers in HCI Experiments, Lancaster University, Lecture, 2005. Poole, A. and Ball, L. 
J., Eye Tracking in Human-Computer Interaction and Usability Research: Current Status and Future Prospects. 
In Ghaoui, Claude (Ed.), 2004. Rayner, K., and Pollatsek, A., The psychology of reading. Englewood Cliffs, 
NJ: Prentice Hall, 1989. Rensink R. A., Visual Attention. In L. Nadel, editor, Encyclopedia of Cognitive 
Science, pages 509 515. Nature Publishing Group, 2003. Veronica Sundstedt _ SIGGRAPH09 23 Rothkopf C. 
A. and Pelz J. B., Head movement estimation for wearable eye tracker. In ETRA 04: Proceedings of the 
2004 symposium on Eye tracking research &#38; applications, pages 123 130, New York, NY, USA, ACM Press, 
2004. Räihä K-J., New Interaction Techniques. Course Notes, TAUCHI Tampere Unit for Computer-Human Interaction, 
2005. Salojärvi J., Puolamäki K., Simola J., Kovanen L., Kojo I., and Kaski S., Inferring Relevance from 
Eye Movements: Feature Extraction. Technical Report Report A82, Helsinki University of Technology, Espoo, 
Finland, March, 2005. Salvucci D. D. and Goldberg J. H., Identifying fixations and saccades in eye-tracking 
protocols. In ETRA 00: Proceedings of the 2000 symposium on Eye tracking research &#38; applications, 
pages 71 78, New York, NY, USA, ACM Press, 2000. Schnipke S. K. and Todd M. W., Trials and tribulations 
of using an eye-tracking system, CHI '00: CHI '00 extended abstracts on Human factors in computing systems, 
pp 273-274, 2000. Smith J. D. and Graham T. C. N., Use of eye movements for video game control. ACE 06: 
Proceedings of the 2006 ACM SIGCHI international conference on Advances in computer entertainment technology, 
20. (ACM Press), 2006. Snowden R., Thompson P., and Troscianko T., Basic Vision: an introduction to visual 
perception. Oxford University Press, 2006. Spillers F., Eye-Tracking studies-Usability holy grail?, 2004. 
Strandvall T., Eye Tracking on the Web, Tobii Technology, 2009. Strandvall T., Tobii Technology, http://eyetracking.me/, 
2009. Sundstedt V., Debattista K., Longhurst P., Chalmers A., Troscianko T., Visual Attention for Efficient 
High-Fidelity Graphics, SCCG 05 -Spring Conference on Computer Graphics, pp 169-175, May, 2005. Sundstedt 
V., Gutierrez D., Anson O., Banterle F., Chalmers A., Perceptual Rendering of Participating Media, TAP -ACM 
Transactions of Applied Perception 4(3), article 15, 2007. Sundstedt V., Rendering and Validation of 
High-Fidelity Graphics using Region-of-Interest, PhD Thesis, University of Bristol, 2007. Sundstedt V., 
Stavrakis E., Wimmer M., and Reinhard E., A Psychophysical Study of Fixation Behavior in a Computer Game, 
APGV 08 -The 5th Symposium on Applied Perception in Graphics and Visualization, Los Angeles, California, 
Aug, 2008. Tobii. UserManual: Tobii Eye Tracker, ClearView analysis software, February, 2006. Tobii. 
Usability Testing Brochure: See through the eyes of the user, January, 2009. Vilis T., The Physiology 
of the Senses Transformations for Perception and Action. Course Notes, University of Western Ontario, 
Canada, 2006. Veronica Sundstedt _ SIGGRAPH09 24 Wilcox T., Evans M., Pearce C., Pollard N., and Sundstedt 
V., Gaze and Voice Based Game Interaction: The Revenge of the Killer Penguins. ACM SIGGRAPH posters -The 
35th International Conference on Computer Graphics and Interactive Techniques, 2008. Yarbus A. L., Eye 
Movements and Vision. Plenum Press, 1967. Yee H., Pattanaik S., and Greenberg D. P., Spatiotemporal sensitivity 
and visual attention for efficient rendering of dynamic environments. ACM Trans.Graph., 20(1):39 65, 
2001. Veronica Sundstedt _ SIGGRAPH09 25  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
</content>
</proceeding>
