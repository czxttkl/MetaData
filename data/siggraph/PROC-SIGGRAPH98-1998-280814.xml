<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date></start_date>
		<end_date></end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[]]></city>
		<state></state>
		<country></country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>280814</proc_id>
	<acronym>SIGGRAPH '98</acronym>
	<proc_desc>Proceedings of the 25th annual conference</proc_desc>
	<conference_number></conference_number>
	<proc_class>conference</proc_class>
	<proc_title>Computer graphics and interactive techniques</proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn>0-89791-999-8</isbn>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>1998</copyright_year>
	<publication_date>07-24-1998</publication_date>
	<pages>472</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source>ACM member price $50</other_source>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node>I.3.0</cat_node>
			<descriptor/>
			<type/>
		</primary_category>
		<other_category>
			<cat_node>I.3.3</cat_node>
			<descriptor></descriptor>
			<type></type>
		</other_category>
		<other_category>
			<cat_node>I.3.5</cat_node>
			<descriptor></descriptor>
			<type></type>
		</other_category>
		<other_category>
			<cat_node>I.3.7</cat_node>
			<descriptor></descriptor>
			<type></type>
		</other_category>
	</categories>
	<ccs2012>
		<concept>
			<concept_id>0.10010147.10010371.10010396</concept_id>
			<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10010147.10010371.10010352</concept_id>
			<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
			<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10010147.10010371.10010372</concept_id>
			<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10003752.10010061.10010063</concept_id>
			<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10003752.10010061</concept_id>
			<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10010147.10010371.10010382</concept_id>
			<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10010147.10010371</concept_id>
			<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
			<concept_significance>500</concept_significance>
		</concept>
	</ccs2012>
	<general_terms>
		<gt>Algorithms</gt>
	</general_terms>
	<chair_editor>
		<ch_ed>
			<person_id>PP39048690</person_id>
			<author_profile_id><![CDATA[81100562918]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>1</seq_no>
			<first_name><![CDATA[Steve]]></first_name>
			<middle_name><![CDATA[]]></middle_name>
			<last_name><![CDATA[Cunningham]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[California State Univ. Stanislaus, Trulock, CA]]></affiliation>
			<role><![CDATA[Chairman]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
		<ch_ed>
			<person_id>P296110</person_id>
			<author_profile_id><![CDATA[81100652997]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>2</seq_no>
			<first_name><![CDATA[Walt]]></first_name>
			<middle_name><![CDATA[]]></middle_name>
			<last_name><![CDATA[Bransford]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[Thrillistic]]></affiliation>
			<role><![CDATA[Chairman]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
		<ch_ed>
			<person_id>PP77028331</person_id>
			<author_profile_id><![CDATA[81414610924]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>3</seq_no>
			<first_name><![CDATA[Michael]]></first_name>
			<middle_name><![CDATA[F.]]></middle_name>
			<last_name><![CDATA[Cohen]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[]]></affiliation>
			<role><![CDATA[Chairman]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
	</chair_editor>
	<ccc>
		<copyright_holder>
			<copyright_holder_name>ACM</copyright_holder_name>
			<copyright_holder_year>1998</copyright_holder_year>
		</copyright_holder>
	</ccc>
</proceeding_rec>
<content>
	<article_rec>
		<article_id>280816</article_id>
		<sort_key>9</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[NeuroAnimator]]></title>
		<subtitle><![CDATA[fast neural network emulation and control of physics-based models]]></subtitle>
		<page_from>9</page_from>
		<page_to>20</page_to>
		<doi_number>10.1145/280814.280816</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280816</url>
		<keywords>
			<kw><![CDATA[backpropagation]]></kw>
			<kw><![CDATA[dynamical systems]]></kw>
			<kw><![CDATA[learning]]></kw>
			<kw><![CDATA[motion control]]></kw>
			<kw><![CDATA[neural networks]]></kw>
			<kw><![CDATA[physics-based animation]]></kw>
			<kw><![CDATA[simulation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.6</cat_node>
				<descriptor>Connectionism and neural nets</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010257.10010293.10010294</concept_id>
				<concept_desc>CCS->Computing methodologies->Machine learning->Machine learning approaches->Neural networks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Languages</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P237369</person_id>
				<author_profile_id><![CDATA[81100560226]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Radek]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grzeszczuk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel Corp.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14109166</person_id>
				<author_profile_id><![CDATA[81100294834]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Demetri]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Terzopoulos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel Corp.; and Univ. of Toronto, Toronto, Ont., Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40028014</person_id>
				<author_profile_id><![CDATA[81100505762]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Geoffrey]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hinton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Toronto, Toronto, Ont., Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>74356</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[David Baraff. Analytical methods for dynamic simulation of non-penetrating rigid bodies. In Jeffrey Lane, editor, Computer Graphics (SIGGRAPH '89 Proceedings), volume 23, pages 223-232, July 1989.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>235248</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[C.M. Bishop. Neural Networks for Pattern Recognition. Clarendon Press, 1995.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[G. Cybenko. Approximation by superposition of sigmoidal function. Mathematics of Control Signals and Systems, 2(4):303-314, 1989.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>929770</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[R. Grzeszczuk. NeuroAnimator: Fast Neural Network Emulation and Control of Physics-BasedModels. PhD thesis, Department of Computer Science, University of Toronto, May 1998.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218411</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Radek Grzeszczuk and Demetri Terzopoulos. Automated learning of Muscle- Actuated locomotion through control abstraction. In Robert Cook, editor, SIG- GRAPH 95 Conference P1vceedings, Annual Conference Series, pages 63-70. ACM SIGGRAPH, Addison Wesley, August 1995. held in Los Angeles, California, 06-11 August 1995.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378530</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[James K. Hahn. Realistic animation of rigid bodies. In John Dill, editor, Computer Graphics (SIGGRAPH '88 P~vceedings), volume 22, pages 299-308, August 1988.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218414</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Jessica K. Hodgins, Wayne L. Wooten, David C. Brogan, and James F. O'Brien. Animating human athletics. In Robert Cook, editor, SIGGRAPH 95 Conference P~vceedings, Annual Conference Series, pages 71-78. ACM SIGGRAPH, Addison Wesley, August 1995. held in Los Angeles, California, 06-11 August 1995.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>70408</ref_obj_id>
				<ref_obj_pid>70405</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[K. Hornik, M. Stinchcomb, and H. White. Multilayer feedforward networks are universal approximators. Neural Networks, 2:359-366, 1989.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[M. I. Jordan and D. E. Rumelhart. Supervised learning with a distal teacher. Cognitive Science, 16:307-354, 1992.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378508</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Gavin S. P. Miller. The motion dynamics of snakes and worms. In John Dill, editor, Computer Graphics (SIGGRAPH '88 P~vceedings), volume 22, pages 169-178, August 1988.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166160</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[J. Thomas Ngo and Joe Marks. Spacetime constraints revisited. In James T. Kajiya, editor, Computer Graphics (SIGGRAPH '93 P1vceedings), volume 27, pages 343-350, August 1993.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[D. Nguyen and B. Widrow. The truck backer-upper: An example of self-learning in neural networks. In P~vceedings of the International Joint Conference on Neural Networks, volume 2, pages 357-363. IEEE Press, 1989.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>148286</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[W.H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery. Numerical Recipes: The Art of Scientific Computing, SecondEd#ion. Cambridge University Press, 1992.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[G. Ridsdale. Connectionist modeling of skill dynamics. Journal of Visualization and Computer Animation, 1 (2):66-72, 1990.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>104293</ref_obj_id>
				<ref_obj_pid>104279</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[D.E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning internal representations by error backpropagation. In D. E. Rumelhart, J. L. McCleland, and the PDP Research Group, editors, Parallel Distributed Processing: Explorations in the Mic~vstructure of Cognition, volume 1, pages 318-362. MIT Press, 1986.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192167</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Karl Sims. Evolving virtual creatures. In Andrew Glassner, editor, P~vceedings of SIGGRAPH '94 (Orlando, Florida, July 24-29, 1994), Computer Graphics Proceedings, Annual Conference Series, pages 15-22. ACM SIGGRAPH, ACM Press, July 1994. ISBN 0-89791-667-0.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37427</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Demetri Terzopoulos, John Platt, Alan Barr, and Kurt Fleischer. Elastically deformable models. In Maureen C. Stone, editor, Computer Graphics (SIGGRAPH '87 P~vceedings), volume 21, pages 205-214, July 1987.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192170</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Xiaoyuan Tu and Demetri Terzopoulos. Artificial fishes: Physics, locomotion, perception, behavior. In Andrew Glassner, editor, P~vceedings of SIGGRAPH '94 (Orlando, Florida, July 24-29, 1994), Computer Graphics Proceedings, Annual Conference Series, pages 43-50. ACM SIGGRAPH, ACM Press, July 1994. ISBN 0-89791-667-0.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166159</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Michiel van de Panne and Eugene Fiume. Sensor-actuator networks. In James T. Kajiya, editor, Computer Graphics (SIGGRAPH '93 P1vceedings), volume 27, pages 335-342, August 1993.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 NeuroAnimator: Copyright &#38;#169;1998 by the Association for Computing Machinery, Inc. Permission 
to make digital or hard copies of part or all of this work for personal or classroom use is granted without 
fee provided that copies are not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. Copyrights for components of this work owned 
by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, 
to post on servers or to redistribute to lists, requires specific permission and/or a fee. Fast Neural 
Network Emulation and Control of Physics-Based Models Radek Grzeszczuk 1 Demetri Terzopoulos 2;1 Geoffrey 
Hinton 2 12 Intel Corporation University of Toronto Abstract: Animation through the numerical simulation 
of physics­based graphics models offers unsurpassed realism, but it can be computationally demanding. 
Likewise, the search for controllers that enable physics-based models to produce desired animations usually 
entails formidable computational cost. This paper demon­strates the possibility of replacing the numerical 
simulation and control of dynamic models with a dramatically more ef.cient al­ternative. In particular, 
we propose the NeuroAnimator, a novel ap­proach to creating physically realistic animation that exploits 
neu­ral networks. NeuroAnimators are automatically trained off-line to emulate physical dynamics through 
the observation of physics­based models in action. Depending on the model, its neural net­work emulator 
can yield physically realistic animation one or two orders of magnitude faster than conventional numerical 
simulation. Furthermore, by exploiting the network structure of the NeuroAni­mator, we introduce a fast 
algorithm for learning controllers that en­ables either physics-based models or their neural network 
emulators to synthesize motions satisfying prescribed animation goals. We demonstrate NeuroAnimators 
for a variety of physics-based mod­els. CR Categories: I.3.7 [Computer Graphics]: Three-Dimensional Graphics 
and Realism Animation; I.6.8 [Simulation and Model­ing]: Types of Simulation Animation Keywords: physics-based 
animation, neural networks, learning, motion control, backpropagation, dynamical systems, simulation. 
1 Introduction Animation based on physical principles has been an in.uential trend in computer graphics. 
This is not only due to the unsur­passed realism that physics-based techniques offer. In conjunction 
with suitable control and constraint mechanisms, physical mod­els also facilitate the production of copious 
quantities of realistic animation in a highly automated fashion. Physics-based anima­tion techniques 
are beginning to .nd their way into high-end com­mercial systems. However, a well-known drawback has 
retarded 12200 Mission College Blvd., Santa Clara, CA 95052, RN6-35 E-mail: radek.grzeszczuk@intel.com 
210 King s College Road, Toronto, Ontario, Canada, M5S 3G4 E-mail: fdtjhintong@cs.toronto.edu their broader 
penetration compared to geometric models, physi­cal models typically entail formidable numerical simulation 
costs. This paper proposes a new approach to creating physically real­istic animation that differs radically 
from the conventional approach of numerically simulating the equations of motion of physics-based models. 
We replace physics-based models by fast emulators which automatically learn to produce similar motions 
by observing the models in action. Our emulators have a neural network structure, hence we dub them NeuroAnimators. 
The network structure of NeuroAnimators furthermore enables a new solution to the con­trol problem associated 
with physics-based models, leading to a re­markably fast algorithm for synthesizing motions that satisfy 
pre­scribed animation goals. 1.1 Overview of the NeuroAnimator Approach Our approach is motivated by 
the following considerations: Whether we are dealing with rigid [6, 1], articulated [7, 19], or non­rigid 
[17, 10] dynamic animation models, the numerical simulation of the associated equations of motion leads 
to the computation of a discrete-time dynamical system of the form st+t< st;ut;ft :(1) These (generally 
nonlinear) equations express the vector st+tof state variables of the system (values of the system s 
degrees of free­dom and their velocities) at time t+tin the future as a function < of the state vector 
st, the vector utof control inputs, and the vector ftof external forces acting on the system at time 
t. Physics-based animation through the numerical simulation of a dynamical system (1) requires the evaluation 
of the map <at every timestep, which usually involves a non-trivial computation. Eval­uating <using explicit 
time integration methods incurs a compu­tational cost of O(N)operations, where Nis proportional to the 
dimensionality of the state space. Unfortunately, for many dynamic models of interest, explicit methods 
are plagued by instability, ne­cessitating numerous tiny timestepstper unit simulation time. Al­ternatively, 
implicit time-integration methods usually permit larger timesteps, but they compute <by solving a system 
of Nalge­braic equations, generally incurring a cost of O(N3)operations per timestep. We pose an intriguing 
question: Is it possible to replace the con­ventional numerical simulator, which must repeatedly compute 
<, by a signi.cantly cheaper alternative? A crucial realization is that the substitute, or emulator, 
need not compute the map <exactly, but merely approximate it to a degree of precision that preserves 
the perceived faithfulness of the resulting animation to the simu­lated dynamics of the physical model. 
Neural networks [2] offer a general mechanism for approximat­ing complex maps in higher dimensional spaces.2Our 
premise is that, to a suf.cient degree of accuracy and at signi.cant compu­tational savings, trained 
neural networks can approximate maps < 2Note that .in (1) is in general a high-dimensional map from !s+u+f 
7!!s,where s, u,and fdenote the dimensionalities of the state, control, and external force vectors. not 
just for simple dynamical systems, but also for those associated with dynamic models that are among the 
most complex reported in the graphics literature to date. The NeuroAnimator, which uses neural networks 
to emulate physics-based animation, learns an approximation to the dynamic model by observing instances 
of state transitions, as well as control inputs and/or external forces that cause these transitions. 
Training a NeuroAnimator is quite unlike recording motion capture data, since the network observes isolated 
examples of state transitions rather then complete motion trajectories. By generalizing from the sparse 
examples presented to it, a trained NeuroAnimator can emulate an in.nite variety of continuous animations 
that it has never actually seen. Each emulation step costs only O(N2)operations, but it is possible to 
gain additional ef.ciency relative to a numerical simu­lator by training neural networks to approximate 
a lengthy chain of evaluations of (1). Thus, the emulator network can perform super timesteps !tnt, typically 
one or two orders of magnitude larger than tfor the competing implicit time-integration scheme, thereby 
achieving outstanding ef.ciency without serious loss of ac­curacy. The NeuroAnimator offers an additional 
bonus which has cru­cial consequences for animation control: Unlike the map <in the original dynamical 
system (1), its neural network approximation is analytically differentiable. In fact, the derivative 
of NeuroAnima­tor state outputs with respect to control and external force inputs is ef.ciently computable 
by applying the chain rule of differentiation. Easy differentiability enables us to arrive at a remarkably 
fast gra­dient descent optimization algorithm to compute optimal or near­optimal controllers. These controllers 
produce a series of control inputs utthat enable NeuroAnimators to synthesize motions satis­fying prescribed 
constraints on the desired animation. NeuroAni­mator controllers are equally applicable to controlling 
the original physics-based models. 1.2 Related Work To date, network architectures have found only a 
few applications in computer graphics. One application has been the control of an­imated characters. 
Ridsdale [14] reports a method for skill acqui­sition using a connectionist model of skill memory. The 
sensor­actuator networks of van de Panne and Fiume [19] are recurrent networks of units that take sensory 
information as input and pro­duce actuator controls as output. Sims [16] employed a network architecture 
to structure simple brains that control evolved crea­tures. Our work differs fundamentally from these 
efforts. The basis of our approach is related to work presented in the mainstream neural network literature 
on connectionist control of complex systems. Nguyen and Widrow demonstrated the neural network based 
approximation and control of a nonlinear kinematic system in their truck backer-upper [12]. More recently, 
Jordan and Rumelhart [9] proposed a two step approach to learning con­trollers for physical robots. In 
step one, a neural net learns a predic­tive internal model of the robot, which maps from actions to state 
transitions. Insteptwo this forwardmodel isusedtolearnanin­verse model that maps from intentions to actions, 
by training the inverse model so that it produces an identity transformation in cas­cade with the established 
forward model. Inspired by these results, we exploit neural networks to produce controlled, physically 
realistic animation satisfying user-speci.ed constraints at a fraction of the computational cost of conventional 
numerical simulation.  2 Arti.cial Neural Networks In this section we de.ne a common type of arti.cial 
neural net­work and describe the backpropagation training algorithm. Neu­ input hidden output layer layer 
layer 1 x =1 x =10 0 xw0j x 1 1y 1 zj yj y r x wpj xp p v pq (a) (b) Figure 1: (a) Mathematical model 
of a neuron j. (b) Three-layer feedforward neural network N. Bias units are not shaded. roAnimator makes 
use of a neural network simulator called Xerion which was developed at the University of Toronto and 
is available publicly.3The public availability of software such as Xerion con­tributes to making our 
NeuroAnimator approach easily accessible to the graphics community. 2.1 Neurons and Neural Networks In 
mathematical terms, a neuron is an operator that maps Rp7!R. Referring to Fig. 1(a), neuron jreceives 
a signal zjthat is the sum of pinputs xiscaled by associated connection weights wij: pp XX T zjw0j+xiwijxiwijxwj;(2) 
i=1 i=0 T where xx0;x1;:::;xpis the input vector (the superscript T T denotes transposition), wjw0j;w1j;:::;wpjis 
the weight vector of neuron j,and w0jis the bias parameter, which can be treated as an extra connection 
with constant unit input, x01, as shown in the .gure. The neuron outputs a signal yjg(zj), where gis 
a continuous, monotonic, and often nonlinear activation function, commonly the logistic sigmoid g(z)a(z)1/(1+ 
e ,z). A neural network is a set of interconnected neurons. In a simple feedforward neural network, the 
neurons are organized in layers so that a neuron in layer lreceives inputs only from the neurons in layer 
l,1. The .rst layer is commonly referred to as the input layer and the last layer as the output layer. 
The intermediate layers are called hidden layers. Fig. 1(b) shows a fully connected network with only 
a single hidden layer. We use this popular type of network in our algorithms. The hidden and output layers 
include bias units that group together the bias parameters of all the neurons in those layers. The input 
and output layers use linear activation functions, while the hidden layer uses the logistic sigmoid activation 
function. The output of the jth P p hidden unit is therefore given by hja(i=0xivij). 2.2 Approximation 
by Learning We denote a 3-layer feedforward network with pinput units, qhid­den units, routput units, 
and weight vector was N(x;w).It de­.nes a continuous map N:Rp7!Rr. With suf.ciently large q,a feedforward 
neural network with this architecture can approximate as accurately as necessary any continuous map <:Rp7!Rrover 
3Available from ftp://ftp.cs.toronto.edu/pub/xerion output where xTis the input vector and yTis the 
associated desired output vector. The goal of training is to utilize the examples to .nd a set of weights 
wfor the network N(x;w)such that, for all inputs of interest, the difference between the network output 
and the true output is suf.ciently small, as measured by the approximation error (3). Training a neural 
network to approximate a map is analogous to .tting a polynomial to data and it suffers from the same 
problems. Mainly, a network with two few free parameters (weights) will un­der.t the data, while a network 
with too many free parameters will over.t the data. Fig. 2 depicts these problems in a low-dimensional 
setting. To avoid under.tting, we use networks with a suf.cient number of weights. To avoid over.tting, 
we make sure that we use suf.cient training data. We use 8-10 times as many examples as there are weights 
in the network, which seems suf.cient to avoid serious over.tting or under.tting. 2.3 Backpropagation 
Learning Algorithm Rumelhart, Hinton and Williams [15] proposed an ef.cient al­gorithm for training multi-layer 
feedforward networks, called the backpropagation algorithm. The backpropagation algorithm seeks to minimize 
the objective function nn  X TX T E(w)e(x;w)E(w)(5) T=1 T=1 which sums the approximation errors efrom 
(3) over the ntraining examples. The off-line training version of the algorithm adjusts the weights of 
the network using the gradient descent formula l+1ll(6) ww+TrwE(w); where rwEdenotes the gradient of 
the objective function with respect to the weights and T1is referred to as the learning rate. input 
Figure 2: Depicted in a low-dimensional setting, a neural network with too few neurons under.ts the training 
data. One with too many neurons over.ts the data. The solid curve represents a properly chosen network 
which provides a good compromise between ap­proximation (.ts the training data) and generalization (generates 
reasonable output values away from the training examples). a compact domain x2X[3, 8]; i.e., for an arbitrarily 
small0 there exists a network Nsuch that 2 8x2 X;e(x;w)k<(x),N(x;w)k;(3) where eis the approximation 
error. A neural network can learn an approximation to a map <by observing training data consisting of 
input-output pairs that sample <. The training sequence is a set of examples, such that the Tth example 
comprises the pair TTTTT xx1;x2;:::;xp; TTTTTT(4) y<(x)y1;y2;:::;yr; t x w) Figure 3: The backpropagation 
algorithm learns a map <by ad­justing the weights wof the network Nin order to reduce the dif- T ference 
between in the network output N(x;w)and the desired T output <(x). Depicted here is the on-line version 
of the algorithm that adjusts the weights of the network after observing each training example. An on-line 
training version of the backpropagation algorithm that adjusts the weights of the network after each 
training example is presented in [15]. Fig. 3 illustrates this process. Backpropagation refers to the 
practical, recursive method to cal­culate the component error derivatives of the gradient term in (6). 
Applying the chain rule of differentiation, the backpropagation al­gorithm .rst computes the derivatives 
with respect to weights in the output layer and chains its way back to the input layer, computing the 
derivatives with respect to weights in each hidden layer as it proceeds. To improve the learning rate, 
the gradient descent rule of the basic backpropagation algorithm (6), which takes a .xed step in the 
direction of the gradient, can be replaced by more sophisticated nonlinear optimization techniques. Line 
search offers one way to accelerate the training by searching for the optimal size step in the gradient 
direction. Additional performance improvement can be achieved by taking at each optimization step a direction 
orthog­onal to previous directions. This is known as the conjugate gra­dient method [13]. A simple but 
effective method for increasing the learning rate augments the gradient descent update rule with a momentum 
term. The momentum method updates the weights as follows: l+1 ll w,TwrwE(w)+Cww;(7) l+1 ll+1 ww+w; (8) 
where the momentum parameter Cwmust be between 0 and 1. The neural network simulator Xerion includes 
the above optimization techniques and several others. Later in the paper we discuss the types of optimization 
methods that we used to train NeuroAnima­tors and to synthesize controllers. 3 From Physics-Based Models 
to NeuroAnimators In this section we explain the practical application of neural net­work concepts to 
the construction and training of different classes of NeuroAnimators. Among other subjects, this includes 
network input/output structure, the use of hierarchical networks to tackle physics-based models with 
large state spaces, and strategies for generating good training datasets. We also discuss the practical 
issue of applying the Xerion neural network simulator to train Neu­roAnimators. Finally, we show sample 
results demonstrating the accurate emulation of various dynamic models. Our task is to construct neural 
networks that approximate <in the dynamical system (1). We propose to employ backpropagation to train 
feedforward networks N to predict future states using su­per timesteps !tntwhile containing the approximation 
error so as not to appreciably degrade the physical realism of the result­ing animation. Analogous to 
(1), the basic emulation step is st+ tN st;ut;ft:(9) s M-1 s ....   .... ... .... .... ....  ... 
.... sM s2 .... .... ... .... .... .... ... 3 .... .... ... .... ........ ....... s ........ ....... 
sM+1 ........ ....... ........ ....... ... ........ ....... ... ........ ....... ........ ....... ........ 
....... ........ ....... ........ ....... ........ ....... ........ ....... .... .... .... .... ... 
  u1 u2 uM-1 uM controller Figure 4: Forward emulation using a neural network. At each itera­tion, 
the network output becomes the state input at the next iteration of the algorithm. Note that the same 
emulator network is used re­cursively. The trained emulator network Ntakes as input the state of the 
model, its control inputs, and the external forces acting on it at time t, and produces as output the 
state of the model at time t+!t by evaluating the network. The emulation process is a sequence of these 
evaluations. After each evaluation, the network control and force inputs receive new values, and the 
network state inputs receive the emulator outputs from the previous evaluation. Fig. 4 illustrates the 
emulation process. The .gure represents each emu­lation step by a separate network whose outputs become 
the inputs to the next network. In reality, the emulation process employs a recurrent network whose outputs 
become inputs for the subsequent evaluation step. Since the emulation step is large compared with the 
physical sim­ulation step, we often .nd the sampling rate of the motion trajectory produced by the emulator 
too coarse for animation. To avoid mo­tion artifacts, we resample the motion trajectory at the animation 
frame rate, computing intermediate states through linear interpola­tion of states obtained from the emulation. 
Linear interpolation pro­duces satisfactory motion, although a more sophisticated scheme could improve 
the result. 3.1 Network Input/Output Structure The emulator network has a single set of output variables 
specify­ing st+t. The number of input variable sets depends on whether the physical model is active or 
passive and the type of forces in­volved. A dynamical system of the form (1), such as the multi-link 
pendulum illustrated in Fig. 5(a), with control inputs ucomprising joint motor torques is known as active, 
otherwise, it is passive. Fig. 5(b) illustrates different emulator input/output structures. If we wish, 
in the fully general case, to emulate an active model under the in.uence of unpredictable applied forces, 
we employ a full net­work with three sets of input variables: st, ut,and ft,as shown in the .gure. For 
passive models, the control ut0and the network simpli.es to one with two sets of inputs, stand ft. In 
the special case when the forces ftare completely determined by the state of the system st, we can suppress 
the ftinputs, allow­ing the network to learn the effects of these forces from the state transition training 
data. For example, the active multi-link pendu­lum illustrated in Fig. 5(a) is under the in.uence of 
gravity gand joint friction forces '. However, since both gand 'are completely determined by st, they 
need not be provided as emulator inputs. A simple emulator with two input sets stand utcan learn the 
response of the multi-link pendulum to those external forces. The simplest type of emulator has only 
a single set of inputs st. This emulator can approximate passive models acted upon by deterministic external 
forces. st  st t+.t s ft  st+.t  ft  ut  st t+.t s    sst t+.t  ut   (a) (b) Figure 5: 
Three-link physical pendulum and network emulators. (a) An active pendulum with joint friction Ti, motor 
torques ui, applied forces fi, and gravity g. Without motor torques, the pendulum is passive. (b) Different 
types of emulators. NF (a) st ft ut . st+.t Ty (b) (c)  Figure 6: Transforming a simple feedforward 
neural network N into a practical emulator network N;that is easily trained to em­ulate physics-based 
models. The following operators perform the appropriate pre-and post-processing: T0 xtransforms inputs 
to lo­cal coordinates, T;xnormalizes inputs, T;yunnormalizes outputs, T0 ytransforms outputs to global 
coordinates, Tyconverts from a state change to the next state (see text). 3.2 Input and Output Transformations 
The accurate approximation of complex functional mappings using neural networks can be challenging. We 
have observed that a simple feedforward neural network with a single layer of sigmoid units has dif.culty 
producing an accurate approximation to the dynamics of physical models. In practice, we often must transform 
the emulator to ensure a good approximation of the map <, as we explain next. A fundamental problem is 
that the state variables of a dynamical system can have a large dynamic range (e.g., the position and 
veloc­ity of an unconstrained particle can take values from ,1to +1). A single sigmoid unit is nonlinear 
only over a small region of its input space and approximately constant elsewhere. To approximate a nonlinear 
map <accurately over a large domain, we would need to use a neural network with many sigmoid units, each 
shifted and scaled so that their nonlinear segments cover different parts of the domain. The direct approximation 
of <is therefore impractical. A successful strategy is to train networks to emulate changes in state 
variables rather than their actual values, since state changes over small timesteps will have a signi.cantly 
smaller dynamic range. Hence, in Fig. 6(a) we restructure our simple network N as a network Nwhich is 
trained to emulate the change in the state vector !stfor given state, external force, and control inputs, 
followed by an operator Tythat computes st+tst+!stto recover the next state. We can further improve the 
approximation power of the emula­tor network by exploiting natural invariances. In particular, note that 
the map <is invariant under rotation and translation; i.e., the state changes are independent of the 
absolute position and orien­tation of the physical model relative to the world coordinate sys­tem. Hence, 
in Fig. 6(b) we replace Nwith an operator T0 xthat converts the inputs from the world coordinate system 
to the local coordinate system of the model, a network N0that is trained to emulate state changes represented 
in the local coordinate system, andanoperator T0 ythat converts the output of N0back to world coordinates. 
A .nal improvement in the ability of the NeuroAnimator to ap­proximate the map <accrues from the normalization 
of groups of input and output variables. Since the values of state, force, and con­trol variables can 
deviate signi.cantly, their effect on the network outputs is uneven, causing problems when large inputs 
must have a small in.uence on outputs. To make inputs contribute more evenly to the network outputs, 
we normalize groups of variables so that they have zero means and unit variances. Appendix A provides 
the mathematical details. With normalization, we can furthermore ex­pect the weights of the trained network 
to be of order unity and they can be given a simple random initialization prior to training. Hence, in 
Fig. 6(c) we replace N0with an operator T;xthat normalizes its inputs, a network N;that assumes zero 
mean, unit variance inputs and outputs, and an operator T;ythat unnormalizes the outputs to recover their 
original distributions. Although the .nal emulator in Fig. 6(c) is structurally more com­plex than the 
standard feedforward neural network Nthat it re­places, the operators denoted by the letter Tare completely 
deter­mined by the state of the model and the distribution of the training data, and the emulator network 
N;is much easier to train. A more detailed presentation of the restructured emulator can be found in 
[4]. 3.3 Hierarchical Networks As a universal function approximator, a neural network should in principle 
be able to approximate the map <in (1) for any dynam­ical system given enough sigmoid hidden units and 
training data. In practice, however, signi.cant performance improvements accrue from tailoring the neural 
network to the physics-based model. In particular, neural networks are susceptible to the curse of dimensionality 
. The number of neurons needed in hidden layers and the training data requirements grow quickly with 
the size of the network, often making the training of large networks imprac­tical. We have found it prudent 
to structure NeuroAnimators for all but the simplest physics-based models as hierarchies of smaller networks 
rather than as large, monolithic networks. The strategy behind a hierarchical representation is to group 
state variables ac­cording to their dependencies and approximate each tightly cou­pled group with a subnet 
that takes part of its input from a parent network. A natural example of hierarchical networks arises 
when approx­imating complex articulated models, such as Hodgins mechanical human runner model [7] which 
has a tree structure with a torso and limbs. Rather than collect all of its 30 controlled degrees of 
free­dom into a single large network, it is natural to emulate the model using 5 smaller networks: a 
torso network plus left and right arm and leg networks. Hierarchical representations are also useful 
when dealing with deformable models with large state spaces, such as the biomechani­cal model of a dolphin 
described in [5] which we use in our experi­ments. The mass-spring-damper dolphin model (Fig. 7) consists 
of 6 1 Figure 7: Hierarchical state representation for the dolphin mechan­ical model. Red nodes mark 
point masses and green nodes mark numbered local centers of mass. Green lines associate groups of point 
masses to their local center. 23 point masses, yielding a state space with 23.369positions and 69 velocities, 
plus 6 controlled degrees of freedom one for each independent actuator. Rather than constructing a monolithic 
neural network with 69+69138state inputs stand outputs st+t, we subdivide hierarchically. A natural subdivision 
is to rep­resent each of the 6 body segments as a separate sub-network in the local center of mass coordinates 
of the segment, as shown in the .gure. 3.4 Training NeuroAnimators To arrive at a NeuroAnimator for 
a given physics-based model, we train the constituent neural network(s) by invoking the back­propagation 
algorithm on training examples generated by simulat­ing the model. Training requires the generation and 
processing of many examples, hence it is typically slow, often requiring several CPU hours. However, 
it is important to realize that training takes place off-line, in advance. Once a NeuroAnimator is trained, 
it can be reused readily to produce an in.nite variety of fast animations. Training a NeuroAnimator is 
quite unlike recording motion capture data. In fact, the network never observes complete motion trajecto­ries, 
only sparse examples of individual state transitions. The im­portant point is that by generalizing from 
the sparse examples that it has learned, a trained NeuroAnimator will produce an in.nite va­riety of 
extended, continuous animations that it has never seen. More speci.cally, each training example consists 
of an input vec­tor xand an output vector y. In the general case, the input vector TTT xs0;f0T;u0comprises 
the state of the model, the external forces, and the control inputs at time t0. The output vector ystis 
the state of the model at time t!t,where !tis the duration of the super timestep. To generate each training 
example, we would start the numerical simulator of the physics-based model with the initial conditions 
s0, f0,and u0, and run the dynamic sim­ulation for nnumerical time steps tsuch that !tnt.In princi­ple, 
we could generate an arbitrarily large set of training examples TT fx;yg, T1;2;:::, by repeating this 
process with different initial conditions. The initial conditions can be sampled at random among all 
valid state, external force, and control combinations. To learn a good neural network approximation Nof 
the map <in (1), we would like ideally to sample <as uniformly as possible over its domain. Unfortunately, 
for most physics-based models of interest, the do­main has high dimensionality, often making a uniform 
sampling impractical. However, we can make the best use of computational resources by concentrating them 
on sampling those state, force, and control inputs that typically occur as a physics-based model is used 
in practice. Fig. 8 illustrates an effective sampling strategy using the dy­namic dolphin model as an 
example. We simulate the model over an extended period of time with a .xed timestep t. During the simula­tion, 
we apply typical control inputs to the model. For the dolphin, the control inputs are coordinated muscle 
actions that produce loco­motion. At well-separated times ttkduring the simulation, we  t1 t2 t3 t4t5 
t +.t t +.t t +.t t +.t t +.t 1 2 345 Figure 8: An effective state transition sampling strategy illustrated 
using the dynamic dolphin model. The dynamic model is simulated numerically with typical control input 
functions u. For each train­ing example generated, the blue model represents the input state (and/or 
control and external forces) at time tk, while the red model represents the output state at time tk+!t. 
The long time lag en­forced between samples reduces the correlation of the training ex­amples that are 
produced. st ft ut NF Figure 9: Transforming the training data for consumption by the network N;in Fig. 
6(c). The inputs of the training set are trans­formed through the operators on the input side of the 
network in Fig. 6(c). The outputs of the training set are transformed through the inverses of the operators 
at the output side of the network in Fig. 6(c). TTT record a set of training examples fs;fT;u;stk+tg, 
k tktktk 1;2;:::The lag between successive samples is drawn randomly from a uniform distribution over 
the interval !t:(tk+1,tk). 5!t. The considerable separation of successive samples in time helps reduce 
the correlation of the training data, improving learn­ing. Furthermore, we randomize the order of the 
training samples before starting the backpropagation training algorithm. Clearly, the network observes 
many independent examples of typical state tran­sitions, rather than any continuous motion. 3.5 Network 
Training in Xerion As mentioned earlier, to train the emulator shown in Fig. 6(c) we need only train 
the network N;because the operators denoted by the letter Tare predetermined. As shown in Fig. 9, before 
present­ing the training data to the network N;, we transform the inputs of the training set through 
the operators T0 xand T;xand transform the associated outputs through the operators (Ty),1 , (T0 y),1,and 
(T;),1 which are the inverses of the corresponding operators used during the forward emulation step shown 
in Fig. 6(c). We begin the off-line training process by initializing the weights of N;to random values 
from a uniform distribution in the range 0;1(due to the normalization of inputs and outputs). Xerion 
auto­matically terminates the backpropagation learning algorithm when it can no longer reduce the network 
approximation error (3) signif­icantly. We use the conjugate gradient method to train networks of small 
and moderate size. This method converges faster than gradient Model State Force Control Hidden State 
Training Description Inputs Inputs Inputs Units Outputs Examples Pendulum passive 6  20 6 2,400 active 
6 3 20 6 3,000 ext. force 6 3 3 20 6 3,000 Lander 13 4 50 1313,500 Truck 6 2 40 65,200 Dolphin global 
net 78 6 50 78 64,000 local net 72 6 40 36 32,000 Table 1: Structure of the NeuroAnimators used in 
our experiments. Columns 2, 3, and 4 indicate the input groups of the emulator, col­umn 4 indicates the 
number of hidden units, and column 5 indicates the number of outputs. The .nal column shows the size 
of the data set used to train the model. The dolphin NeuroAnimator includes six local nets, one for each 
body segment. descent, but the ef.ciency becomes less signi.cant when training large networks. Since 
this technique works in batch mode, as the number of training examples grows, the weight updates become 
too time consuming. For this reason, we use gradient descent with the momentum term (7 8) when training 
large networks. We divide the training examples into small sets, called mini-batches, each con­sisting 
of approximately 30 uncorrelated examples, and update the network weights after processing each mini-batch. 
Appendix B contains an example Xerion script which speci.es and trains a NeuroAnimator. 3.6 Example 
NeuroAnimators We have successfully constructed and trained several NeuroAnima­tors to emulate a variety 
of physics-based models, including the 3-link pendulum from Fig. 5(a), a lunar lander spacecraft, a truck, 
and the dolphin model from Fig. 7. We used SD/FAST4to simu­late the dynamics of the rigid body and articulated 
models, and we employ the simulator developed in [18] to simulate the deformable­body dynamics of the 
dolphin. Fig. 10 shows rendered stills from animations created using NeuroAnimators trained with these 
mod­els. Table 1 summarizes the structures of the NeuroAnimators devel­oped to emulate these models (note 
that for the hierarchical dolphin NeuroAnimator, the table indicates the dimensions for only one of its 
six sub-networks; the other .ve are similar). In our experiments we have not attempted to minimize the 
number of network weights required for successful training. We have also not tried to minimize the number 
of hidden units, but rather used enough units to obtain networks that generalize well while not over.tting 
the training data. We can always expect to be able to satisfy these guidelines in view of our ability 
to generate suf.cient training data. Section 5 will present a detailed analysis of our results, including 
performance benchmarks indicating that the neural network emulators can yield physically realistic animation 
one or two orders of magnitude faster than conventional numerical simulation of the associated physics­based 
models.  4 NeuroAnimator Controller Synthesis We have demonstrated that it is possible to emulate a 
dynamical system using a trained neural network. We turn next to the prob­lem of control; i.e., producing 
physically realistic animation that satis.es goals speci.ed by the animator. 4SD/FAST is a commercial 
system for simulating rigid body dynamics, available from Symbolic Dynamics, Inc.  Figure 10: NeuroAnimators 
used in our experiments. The image at the upper left shows the emulator of a physics-based model of a 
planar multi-link pendulum suspended in gravity, subject to joint friction forces, external forces applied 
on the links, and controlled by independent motor torques at each of the three joints. The image at the 
upper right shows the emulator of a physics-based model of a truck implemented as a rigid body, subject 
to friction forces where the tires contact the ground, controlled by rear-wheel drive (forward and reverse) 
and steerable front wheels. The image at the lower left shows the emulator of a physics-based model of 
a lunar lander, implemented as a rigid body subject to gravitational forces and controlled by a main 
rocket thruster and three independent attitude jets. The image at the lower right shows the emulator 
of a physics-based deformable (mass-spring-damper) model of a dolphin capable of locomoting via the coordinated 
contraction of 6 independently controlled muscle actuators which deform its body, producing hydrodynamic 
propulsion forces. 4.1 Motivation A popular approach to the animation control problem is controller synthesis 
[11, 19, 5]. Controller synthesis is a generate-and-test strategy. Through repeated forward simulation 
of the physics-based model, controller synthesis optimizes a control objective function that measures 
the degree to which the animation generated by the controlled physical model achieves the desired goals. 
Each simula­tion is followed by an evaluation of the motion through the function, thus guiding the search. 
While the controller synthesis technique readily handles the complex optimal control problems characteristic 
of physics-based animation, it is computationally very costly. Evaluation of the objective function requires 
a forward simulation of the dynamic model, often subject to complex applied forces and constraints. Hence 
the function is almost never analytically differentiable, prompting the application of non-gradient optimization 
methods such as simulated annealing [19, 5] and genetic algorithms [11]. In general, since gradient-free 
optimization methods perform es­sentially a random walk through the huge search space of possible controllers, 
computing many dynamic simulations before .nding a good solution, they generally converge slowly compared 
to opti­mization methods guided by gradient directions. The NeuroAnimator enables a novel, highly ef.cient 
approach to controller synthesis. Outstanding ef.ciency results not only be­cause of fast controller 
evaluation through NeuroAnimator emula­tion of the dynamics of the physical model. To a large degree 
it also stems from the fact that we can exploit the neural network approxi­mation in the trained NeuroAnimator 
to compute partial derivatives of output states with respect to control inputs. This enables the computation 
of a gradient, hence the use of fast gradient-based op­timization for controller synthesis. In the remainder 
of this section, we .rst describe the objective function and its discrete approximation. We then propose 
an ef.­cient gradient based optimization procedure that computes deriva­tives of the objective function 
with respect to the control inputs through a backpropagation algorithm. 4.2 Objective Function and Optimization 
Using (9) we write a sequence of emulation steps si+1Nsi;ui;fi;1:i:M;(10) where iindexes the emulation 
step, and si, uiand fidenote, re­spectively, the state, control inputs and external forces in the ith 
step. Figure 4 illustrates forward emulation by the NeuroAnimator according to this index notation. Following 
the control learning formulation in [5], we de.ne a discrete objective function J(u)fuJu(u)+fsJs(s);(11) 
a weighted sum (with scalar weights fuand fs)of a term Juthat evaluates the controller uu1;u2;:::;uMand 
a term Jsthat evaluates the motion ss1;s2;:::;sM+1produced by the Neu­roAnimator using u, according to 
(10). Via the controller evalua­tion term Ju, we may wish to promote a preference for controllers with 
certain desirable qualities, such as smooth lower amplitude controllers. The distinction between good 
and bad control func­tions also depends on the goals that the animation must satisfy. In our applications, 
we used trajectory criteria Jssuch as the .nal distance to the goal, the deviation from a desired speed, 
etc. The objective function provides a quantitative measure of the progress of the controller learning 
process, with larger values of Jindicating better controllers. A typical objective function used in our 
experiments seeks an ef.cient controller that leaves the model in some desired state sdat the end of 
simulation. Mathematically, this is expressed as M X fu2fs 2 J(u) ui+(sM+1,sd);(12) 22 i=1 where the 
.rst term maximizes the ef.ciency of the controller and the second term constrains the .nal state of 
the model at the end of the animation. 4.3 Backpropagation Through Time Assuming a trained NeuroAnimator 
with a set of .xed weights, the essence of our control learning algorithm is to iteratively update the 
control parameters uso as to maximize the objective function Jin (11). As mentioned earlier, we exploit 
the NeuroAnimator structure to arrive at an ef.cient gradient descent optimizer: l+1ll uu+TxruJ(u);(13) 
where ldenotes the iteration of the minimization step, and the con­stant Txis the learning rate parameter. 
At each iteration l, the algorithm .rst emulates the for­ward dynamics according to (10) using the control 
inputs llll l uu1;u2;:::;uMto yield the motion sequence s lll s1;s2;:::;sM+1, as is illustrated in Fig. 
4. Next, it computes the components of ruJin (13) in an ef.cient manner. The cascade network structure 
enables us to apply the chain rule of differen­tiation within each network, chaining backwards across 
networks, yielding a variant of the backpropagation algorithm called back­propagation through time [15]. 
Instead of adjusting weights as in normal backpropagation, however, the algorithm adjusts neuronal inputs, 
speci.cally, the control inputs. It thus proceeds in reverse through the network cascade computing components 
of the gradi­ent. Fig. 11 illustrates the backpropagation through time process, showing the sequentially 
computed controller updates uMto u0. The forward emulation and control adjustment steps are repeated 
for each iteration of (13), quickly yielding a good controller. The ef.ciency stems from two factors. 
First, each NeuroAnimator em­ulation of the physics-based model consumes only a fraction of the time 
it would take to numerically simulate the model. Second, quick gradient descent towards an optimum is 
possible because the trained NeuroAnimator provides a gradient direction. The control algorithm based 
on the differentiation of the em­ulator of the forward model has important advantages. First, the backpropagation 
through time can solve fairly complex sequential decision problems where early decisions can have substantial 
ef­fects on the .nal results. Second, the algorithm can be applied to dynamic environments with changing 
control objectives since it re­learns very quickly. More ef.cient optimization techniques can be applied 
to improve a slow convergence rate of the gradient descent algorithm (13). Adding momentum (7 8) to the 
gradient descent rule improves the  d ds1 sM-1  .... d sM ...  ... .... .... .... .... ....... ........ 
ds2 ... ....... ........ d .... ... ... .... d s3 sM+1 ....... ........ ....... ....... ....... ........ 
....... ....... ... ... ....... ........ ....... ....... ....... ........ ....... ....... ....... ........ 
....... ....... ....... ........ ....... ... .... ....... .... .... ... ....... ....  .... ... .... 
.. .. dd dd uM u1 u2 uM-1 controller Figure 11: The backpropagation through time algorithm. At each 
iteration the algorithm computes the derivatives of the objective function with respect to the inputs 
of the emulator using the chain rule and it adjusts the control inputs to decrease the value of the objective 
function. effective learning: l+1 ll uTxruJ(u)+Cxu;(14) l+1 ll+1 uu+u; (15) where Cxis the momentum parameter 
used to update the inputs, and lis the iteration of the minimization step. Learning with the momentum 
term is very fast. Section 6 includes a performance comparison of the different optimization techniques. 
Up to now we have assumed that the objective is a known ana­lytic function of the states and the controls 
of the model, as in (11). Although this de.nition covers a wide range of practical problems, our approach 
to control learning can handle objective functions whose analytic form is unknown in advance. See [4] 
for further discussion. An additional advantage of our approach is that once an optimal controller has 
been computed, it can be applied to control either the NeuroAnimator emulator or to the original physical 
model, yielding animations that in most cases differ only minimally.  5 NeuroAnimator Synthesis Results 
As we discussed earlier, we have successfully constructed and trained several NeuroAnimators to emulate 
a variety of physics­based models pictured in Fig. 10. The ensuing discussion presents performance benchmarks 
and an error analysis. 5.1 Performance Benchmarks An important advantage of using neural networks to 
emulate dy­namical systems is the speed at which they can be iterated to pro­duce animation. Since the 
emulator for a dynamical system with the state vector of size Nnever uses more than O(N)hidden units, 
it can be evaluated using only O(N2)operations. Appendix C con­tains the computer code for the forward 
step. By comparison, a sin­gle simulation timestep using an implicit time integration scheme requires 
O(N3)operations. Moreover, a forward pass through the neural network is often equivalent to as many as 
50 physical simu­lation steps, so the ef.ciency is even more dramatic, yielding per­formance improvements 
up to two orders of magnitude faster than the physical simulator. In the remainder of this section we 
use Nnto denote a neural network model that was trained with super timestep !tnt. Table 2 compares the 
physical simulation times obtained using the 255010050 Model Physical N:N:N: N:with Description Simulation 
Regularization Passive Pendulum 4.70 0.10 0.05 0.02 Active Pendulum 4.52 0.12 0.06 0.03 Truck 4.88 
 0.07  Lunar Lander 6.44 0.12  Dolphin 63.00 0.95 2.48 Table 2: Comparison of simulation time between 
the physical sim­ulator and different neural network emulators. The duration of each test was 20,000 
physical simulation timesteps. SD/FAST physical simulator and 3 different neural network mod­els: N25 
, N50,and N100 . For the truck model and the lunar lan­der model, we have trained only N50emulators. 
The neural net­work model that predicts over 100 physical simulation steps offers a speedup of anywhere 
between 50 and 100 times depending on the type of physical model. 5.2 Approximation Error As Fig. 12 
shows, an interesting property of the neural network em­ulation is that the error does not increase appreciably 
for emulators with increasingly larger super timesteps; i.e., in the graphs, the error over time for 
N25 , N50,and N100is nearly constant. This is at­tributable to the fact that an emulator that can predict 
further into the future must be iterated fewer steps per given interval of animation than an emulator 
that cannot predict so far ahead. Thus, although the error per iteration may be higher for the longer-range 
emula­tor, the growth of the error over time can remain nearly the same for both the longer and shorter 
range predictors. This means that the only penalty for using emulators that predict far ahead might be 
a loss of detail (high frequency components in the motion) due to coarse sampling. However, we did not 
observe this effect for the physical models with which we experimented, suggesting that the physical 
systems are locally smooth. Of course, it is not possible to increase the neural network prediction time 
inde.nitely, because eventually the network will no longer be able to approximate the physical system 
at all adequately. Although it is hard to totally eliminate error, we noticed that the approximation 
error remained within reasonable bounds for the purposes of computer animation. The neural network emulation 
appears comparable to the physical simulation, and although the emulated trajectory differs slightly 
from the trajectory produced by the physical simulator, the emulator reproduces all of the visually salient 
properties of the physical motion.  5.3 Regularization of Deformable Models When emulating spring-mass 
systems in which the degrees of free­dom are subject to soft constraints, we discovered that the mod­est 
approximation error of even a well-trained emulator network can accumulate as the network is applied 
repeatedly to generate a lengthy animation. Unlike an articulated system whose state is represented by 
joint angles and hence is kinematically constrained to maintain its connectivity, the emulation of mass-spring 
systems can result in some unnatural deformations after many (hundreds or thousands) emulation steps. 
Accumulated error can be annihilated by periodically performing regularization steps through the applica­tion 
of the true dynamical system (1) using an inexpensive, explicit Euler time-integration step vt+tvt+td(st); 
xt+txt+tvt+t; TTT where the state is stvt;xtand d(st)are the deformation forces generated by the springs 
at time t. It is important to note that passive-kinematic chain active-kinematic chain 0.25 0.25 0.2 
0.2 0.15 0.15 0.1 0.1 0.05 0.05 0 0 time [s] time [s] (a) (b) Figure 12: The error e(x)in the state estimation 
incurred by differ­ent neural network emulators, measured as the absolute difference between the state 
variables of the emulator and the associated phys­ical model. Plot (a) compares the approximation error 
for the pas­sive pendulum for 3 different emulator networks: N25(solid), N50 (dashed), N100(dot-dashed). 
Plot (b) shows the same comparison for the active pendulum. All experiments show the averaged error over 
30 simulation trials and over all state variables. The duration of each trial was 6000 physical simulation 
timesteps. this inexpensive, explicit Euler step is adequate as a regularizer, but it is impractical 
for long-term physical simulation because of its inherent instability. To improve the stability when 
applying the e(x) explicit Euler step, we used a smaller spring stiffness and larger damping factor 
when compared to the semi-implicit Euler step used during the numerical simulation [18]. Otherwise the 
system would oscillate too much or would simply become unstable. We achieve the best results when performing 
a few regularization steps after each emulation step. This produces much smoother mo­tion than performing 
more regularization step but less frequently. Referring to Table 2 for the case of the deformable dolphin 
model, the second column indicates the simulation time using the physical simulator described in [18], 
the fourth column shows the simulation time using the N50emulator, and the last column reveals the impact 
of regularization on the emulation time. In this case, each emulation e(x) step includes 5 iterations 
of the above explicit Euler regularizer.  6 Control Learning Results We have successfully applied our 
backpropagation through time controller learning algorithm to the NeuroAnimators presented in Section 
5. We .nd the technique very effective it routinely com­putes solutions to non-trivial control problems 
in just a few itera­tions. The ef.ciency of the fast convergence rate is further ampli­.ed by the replacement 
of costly physical simulation with much faster NeuroAnimator emulation. These two factors yield outstand­ing 
speedups, as we report below. Fig. 13(a) shows the progress of the control learning algorithm for the 
3-link pendulum. The purple pendulum, animated by a Neu­roAnimator, is given the goal to end the animation 
with zero veloc­ity in the position indicated in green. We make the learning prob­lem very challenging 
by setting a low upper limit on the internal motor torques of the pendulum, so that it cannot reach its 
target in one shot, but must swing back and forth to gain the momentum necessary to reach the goal state. 
Our algorithm takes 20 backprop­agation through time iterations to learn a successful controller. Fig. 
13(b) shows the truck NeuroAnimator learning to park. The translucent truck in the background indicates 
the desired position and orientation of the model at the end of the simulation. The Neu­roAnimator produces 
a parking controller in 15 learning iterations. Fig. 13(c) shows the lunar lander NeuroAnimator learning 
a soft landing maneuver. The translucent lander resting on the surface in­dicates the desired position 
and orientation of the model at the end  1.2 of the animation. An additional constraint is that the 
descent veloc­ 1 1 ity prior to landing should be small in order to land softly. A suc­ cessful landing 
controller was computed in 15 learning iterations. Fig. 13(d) shows the dolphin NeuroAnimator learning 
to swim forward. The simple objective of moving as far forward as possible produces a natural, sinusoidal 
swimming pattern. All trained controllers have a duration of 20 seconds of anima­tion time; i.e., they 
take the equivalent of 2,000 physical simula­tion timesteps, or 40 emulator super-timesteps using N50emulator. 
The number of control variables (Min uu1;u2;:::;uM) op­timized varies: the pendulum optimizes 60 variables, 
20 for each actuator; the lunar lander optimizes 80 variables, 20 for the main thruster, and 20 for each 
of the 3 attitude thrusters; the truck op­timizes 40 variables 20 for acceleration/deceleration, and 
20 for the rate of turning; .nally, the dolphin optimizes 60 variables one variable for every 2 emulator 
steps for each of the 6 muscle actua­tors. Referring to the locomotion learning problem studied in [5], 
we next compare the ef.ciency of our new backpropagation through time control learning algorithm using 
NeuroAnimators and the undirected search techniques simulated annealing and simplex reported in [5]. 
The locomotion learning problem requires the dol­phin to learn how to actuate its 6 independent muscles 
over time in order to swim forward as ef.ciently as possible, as de.ned by an objective function that 
includes actuator work and distance traveled. Our earlier techniques take from 500 to 3500 learning iterations 
to converge because they need to perform extensive sampling of the control space in the absence of gradient 
information. By contrast, the gradient directed algorithm converges to a similar solution in as little 
as 20 learning iterations. Thus, the use of the neural network emulator offers a two orders of magnitude 
reduction in the number of iterations and a two orders of magnitude reduction in the exe­cution time 
of each learning iteration. In terms of actual running times, the synthesis of the swimming controller 
which took more than 1 hour using the technique in [5] now takes less than 10 sec­onds on the same computer. 
Hierarchically structured emulators, in which a global network represents the global aspects of motion 
and a set of sub-networks re.ne the motion produced by the global network, enable us to en­hance the 
performance of our controller learning algorithm. For example, when applying the dolphin NeuroAnimator 
to learn loco­motion controllers, we improve ef.ciency by employing only the global deformation network 
which accounts for the deformation of the entire body and suppressing the sub-networks that account for 
the local deformation of each body segment relative to its own center-of-mass coordinate system, since 
these small deformations do not signi.cantly impact the locomotion. Similarly for a hierar­chical human 
NeuroAnimator, when learning a controller that uses a subset of joints, we need only activate the sub-networks 
that rep­resent the active joints. We next compare the convergence of simple gradient descent and gradient 
descent with the momentum term on the control synthesis problem. Fig. 14 illustrates the progress of 
learning for the lunar lander problem. The results obtained using the momentum term are shown in the 
plot on the left and were generated using the param­eters Tx1:5, Cx0:5in (14). The results obtained using 
the simple gradient descent are shown in the plot on the right and were generated using Tx1:0in (13) 
the largest learning rate that would converge. Clearly, the momentum term decreases the error much more 
rapidly, yielding an improved learning rate.  7 Conclusion We have introduced the NeuroAnimator, an 
ef.cient alternative to the conventional approach of producing physically realistic anima­ 1.4 1.4 objective 
function  1.2 0.8 0.6 0.8 0.6 0.4 0.4 0.2 0.2 0 0 function evaluations function evaluations (a) (b) 
Figure 14: The plots show the value of the objective as a function of the iteration of the control learning 
algorithm. The plot on the left was produced using the momentum term. It converges faster than simple 
gradient descent plotted on the right. tion through numerical simulation. NeuroAnimator involves the 
learning of neural network emulators of physics-based models by observing the dynamic state transitions 
produced by such models in action. The training takes place off-line and in advance. Anima­tions subsequently 
produced by a trained NeuroAnimator approxi­mate physical dynamics with dramatic ef.ciency, yet without 
seri­ous loss of apparent .delity. We have demonstrated the practicality of our technique by constructing 
NeuroAnimators for a variety of nontrivial physics-based models. Our unusual approach to physics­based 
animation furthermore led us to a novel controller synthesis method which exploits fast emulation and 
the differentiability of the NeuroAnimator approximation. We presented a backpropaga­tion through time 
learning algorithm which computes controllers that satisfy nontrivial animation goals. Our new control 
learning algorithm is orders of magnitude faster than prior algorithms. Acknowledgments We thank Zoubin 
Ghahramani for valuable discussions that led to the idea of the rotation and translation invariant emulator, 
which was crucial to the success of this work. We are indebted to Steve Hunt for procuring the equipment 
that we needed to carry out our research at Intel. We thank Sonja Jeter for her assistance with the Viewpoint 
models and Mike Gendimenico for setting up the video editing suite and helping us to use it. We thank 
John Funge and Michiel van de Panne for their assistance in producing animations, Mike Revow and Drew 
van Camp for assistance with Xerion, and Alexander Reshetov for his valuable suggestions about building 
physical models.  A Normalizing Network Inputs &#38; Outputs In Section 3.2 we recommended the normalization 
of emulator in­puts and outputs. Variables in different groups (state, force, or con­trol) require independent 
normalization. We normalize each vari­able so that it has zero mean and unit variance as follows: n nki 
x~x,fx ;(16) kx a i where the mean of the ith group of inputs is Nki+Ki 1XX n fx i xk;(17) NK n=1k=ki 
and its variance is Nki+Ki  1XX 2 xn ai (xk,fi):(18) (N,1)(K,1) n=1k=ki  (a) (b) (c) (d) Figure 13: 
Results of applying the control learning algorithm to four different NeuroAnimators. (a) The 3-link pendulum 
NeuroAnimator in purple must reach the state indicated in green with zero .nal velocity. (b) The truck 
NeuroAnimator learning to park in the position and orientation of the translucent vehicle in the background. 
(c) The lunar lander NeuroAnimator learning to land with low descent velocity in the position and orientation 
of the translucent vehicle on the surface. (d) The dolphin NeuroAnimator learning to swim. The objective 
of locomoting as far forward as possible produces a natural, periodic swimming pattern. Here n1;:::;Nindexes 
the training example, kki;:::;ki+ Kiindexes the variables in group i, Kirepresents the size of group 
 n i,and xdenotes the kth input variable for the nth training ex­ kample. A similar set of equations 
computes the means fyjand the y variances ajfor the output layer of the network: n fy y,y~knkyj:(19) 
a j  B Example Xerion Script The following is a Xerion script that speci.es and trains the network N; 
used to build the NeuroAnimator for the lunar lander model. #! /u/xerion/uts/bin/bp_sh # The network 
has 13 inputs, 50 hidden units, and # 13 outputs. The hidden layer uses the logistic # sigmoid as the 
activation function (default). uts_simpleNet landerNet 13 50 13 bp_groupType landerNet.Hidden {HIDDEN 
DPROD LOGISTIC} # Initialize the example set. Read # the training data from a file. set trainSet "landerNet.data" 
uts_exampleSet $trainSet uts_loadExamples $trainSet landerNet.data # Randomize the weights in the network. 
random seed 3 uts_randomizeNet landerNet # Initialize the minimizer and tell it to use # the network 
and the training set defined above. bp_netMinimizer mz mz configure -net landerNet -exampleSet trainSet 
 # Start the training and save the weights # of the network after the training is finished. mz run uts_saveWeights 
landerNet landerNet.weights  C Forward Pass Through the Network The following is a C++ function for 
calculating the outputs of a neu­ral network from the inputs. It implements the core loop that takes 
a single super timestep in an animation sequence with a trained Neu­roAnimator. BasicNet::forwardStep(void) 
{ int i,j,k; double *input = inputLayer.units; double *hidden = hiddenLayer.units; double *output = 
outputLayer.units; double **ww = inputHiddenWeights; double **vv = hiddenOutputWeights; // compute the 
activity of the hidden layer for (j=0;j<hiddenSize;j++) { hidden[j] = biasHiddenWeights[j]; for (k=0;k<inputSize;k++) 
 hidden[j] += input[k]*ww[k][j]; hidden[j]=hiddenLayer.transFunc(hidden[j]); } // compute the activity 
of the output layer for (i=0;i<outputSize;i++) { output[i] = biasOutputWeights[i]; for (j=0;j<hiddenSize;j++) 
output[i] += hidden[j]*vv[j][i]; output[i]=outputLayer.transFunc(output[i]); } }  References [1] David 
Baraff. Analytical methods for dynamic simulation of non-penetrating rigid bodies. In Jeffrey Lane, editor, 
Computer Graphics (SIGGRAPH 89 Pro­ceedings), volume 23, pages 223 232, July 1989. [2] C. M. Bishop. 
Neural Networks for Pattern Recognition. Clarendon Press, 1995. [3] G. Cybenko. Approximation by superposition 
of sigmoidal function. Mathemat­ics of Control Signals and Systems, 2(4):303 314, 1989. [4] R. Grzeszczuk. 
NeuroAnimator: Fast Neural Network Emulation and Control of Physics-Based Models. PhD thesis, Department 
of Computer Science, University of Toronto, May 1998. [5] Radek Grzeszczuk and Demetri Terzopoulos. Automated 
learning of Muscle-Actuated locomotion through control abstraction. In Robert Cook, editor, SIG-GRAPH 
95 Conference Proceedings, Annual Conference Series, pages 63 70. ACM SIGGRAPH, Addison Wesley, August 
1995. held in Los Angeles, Cali­fornia, 06-11 August 1995. [6] James K. Hahn. Realistic animation of 
rigid bodies. In John Dill, editor, Com­puter Graphics (SIGGRAPH 88 Proceedings), volume 22, pages 299 
308, Au­gust 1988. [7] Jessica K. Hodgins, Wayne L. Wooten, David C. Brogan, and James F. O Brien. Animating 
human athletics. In Robert Cook, editor, SIGGRAPH 95 Conference Proceedings, Annual Conference Series, 
pages 71 78. ACM SIGGRAPH, Addi­son Wesley, August 1995. held in Los Angeles, California, 06-11 August 
1995. [8] K. Hornik, M. Stinchcomb, and H. White. Multilayer feedforward networks are universal approximators. 
Neural Networks, 2:359 366, 1989. [9] M. I. Jordan and D. E. Rumelhart. Supervised learning with a distal 
teacher. Cognitive Science, 16:307 354, 1992. [10] Gavin S. P. Miller. The motion dynamics of snakes 
and worms. In John Dill, editor, Computer Graphics (SIGGRAPH 88 Proceedings), volume 22, pages 169 178, 
August 1988. [11] J. Thomas Ngo and Joe Marks. Spacetime constraints revisited. In James T. Kajiya, editor, 
Computer Graphics (SIGGRAPH 93 Proceedings), volume 27, pages 343 350, August 1993. [12] D. Nguyen and 
B. Widrow. The truck backer-upper: An example of self-learning in neural networks. In Proceedings of 
the International Joint Conference on Neural Networks, volume 2, pages 357 363. IEEE Press, 1989. [13] 
W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery. Numerical Recipes: The Art of Scienti.c 
Computing, Second Edition. Cambridge University Press, 1992. [14] G. Ridsdale. Connectionist modeling 
of skill dynamics. Journal of Visualization and Computer Animation, 1(2):66 72, 1990. [15] D. E. Rumelhart, 
G. E. Hinton, and R. J. Williams. Learning internal represen­tations by error backpropagation. In D. 
E. Rumelhart, J. L. McCleland, and the PDP Research Group, editors, Parallel Distributed Processing: 
Explorations in the Microstructure of Cognition, volume 1, pages 318 362. MIT Press, 1986. [16] Karl 
Sims. Evolving virtual creatures. In Andrew Glassner, editor, Proceedings of SIGGRAPH 94 (Orlando, Florida, 
July 24 29, 1994), Computer Graphics Proceedings, Annual Conference Series, pages 15 22. ACM SIGGRAPH, 
ACM Press, July 1994. ISBN 0-89791-667-0. [17] Demetri Terzopoulos, John Platt, Alan Barr, and Kurt Fleischer. 
Elastically de­formable models. In Maureen C. Stone, editor, Computer Graphics (SIGGRAPH 87 Proceedings), 
volume 21, pages 205 214, July 1987. [18] Xiaoyuan Tu and Demetri Terzopoulos. Arti.cial .shes: Physics, 
locomotion, perception, behavior. In Andrew Glassner, editor, Proceedings of SIGGRAPH 94 (Orlando, Florida, 
July 24 29, 1994), Computer Graphics Proceedings, An­nual Conference Series, pages 43 50. ACM SIGGRAPH, 
ACM Press, July 1994. ISBN 0-89791-667-0. [19] Michiel van de Panne and Eugene Fiume. Sensor-actuator 
networks. In James T. Kajiya, editor, Computer Graphics (SIGGRAPH 93 Proceedings), volume 27, pages 335 
342, August 1993.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280818</article_id>
		<sort_key>21</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A beam tracing approach to acoustic modeling for interactive virtual environments]]></title>
		<page_from>21</page_from>
		<page_to>32</page_to>
		<doi_number>10.1145/280814.280818</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280818</url>
		<keywords>
			<kw><![CDATA[acoustic modeling]]></kw>
			<kw><![CDATA[auralization]]></kw>
			<kw><![CDATA[beam tracing]]></kw>
			<kw><![CDATA[spatialized sound]]></kw>
			<kw><![CDATA[virtual environment systems]]></kw>
			<kw><![CDATA[virtual reality]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.2.2</cat_node>
				<descriptor>Trees</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Auditory (non-speech) feedback</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003624.10003633.10003634</concept_id>
				<concept_desc>CCS->Mathematics of computing->Discrete mathematics->Graph theory->Trees</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003128.10010869</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction techniques->Auditory feedback</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14073484</person_id>
				<author_profile_id><![CDATA[81100182132]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Funkhouser]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bell Labs]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39045730</person_id>
				<author_profile_id><![CDATA[81100497708]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ingrid]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Carlbom]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bell Labs]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P93967</person_id>
				<author_profile_id><![CDATA[81407593959]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Gary]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Elko]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bell Labs]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP48025164</person_id>
				<author_profile_id><![CDATA[81100554010]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Gopal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pingali]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bell Labs]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P202505</person_id>
				<author_profile_id><![CDATA[81338491046]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Mohan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sondhi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bell Labs]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P140642</person_id>
				<author_profile_id><![CDATA[81332535131]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Jim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[West]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bell Labs]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ahnert, Wolfgang. EARS Auralization Software. J. Audio Eng. Soc., 41, 11, November, 1993,894-904.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Allen, J.B., Berkley, D.A. Image Method for Efficiently Simulating Small-Room Acoustics, J. Acoust. Soc. Am., 65, 4, April, 1979, 943-951.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808589</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Amanatides, J. Ray Tracing with Cones. Computer Graphics (SIGGRAPH 84). 18, 3,129-135.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>94794</ref_obj_id>
				<ref_obj_pid>94788</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Arvo, J. and D. Kirk. A Survey of Ray Tracing Acceleration Techniques. in An Introduction to Ray Tracing, Andrew Glassner editor, Academic Press, San Diego, CA, 1989.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Baumgart, Bruce G. Winged Edge Polyhedlvn Representation. Ph.D. Thesis, Computer Science Department, Stanford University, 1972.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Borish, Jeffrey. Extension of the Image Model to Arbitrary Polyhedra. J. Acoust. Soc.Am., 75, 6, June, 1984, 1827-1836.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Bose Modeler, Bose Corporation, Framingham, MA. http://www.bose.com.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[CATT-Acoustic, CATT, Gothenburg, Sweden, http://www.netg.se/catt.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Chuang, J.H. and S.A. Cheng. Computing caustic effects by backward beam tracing. The Visual Computer, 11, 3, 1995, 156-166.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808590</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Cook, Robert, L., Thomas Porter, and Loren Carpenter. Distributed Ray Tracing. Computer Graphics (SIGGRAPH 84). 18, 3,137-146.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[D'Antonio, Peter, and John Konnert. The Directional Scattering Coefficient: Experimental Determination. J, Audio Eng. Soc., 40, 12, December, 1992,997- 1017.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Dadoun, N., D.G. Kirkpatrick, and J.R Walsh. Hierarchical Approaches to Hidden Surface Intersection Testing. Graphics Intelface '82, Toronto, Canada, May, 1982, 49-56.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>323241</ref_obj_id>
				<ref_obj_pid>323233</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Dadoun, N., D.G. Kirkpatrick, and J.R Walsh. The Geometry of Beam Tracing. P~vceedings of the Symposium on Computational Geometry, Baltimore, June, 1985, 55-71.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Durlach, N.I., R.W. Pew, W.A. Aviles, RA. DiZio, and D.L. Zeltzer. Virtual Envilvnment Technology for Training (VETT). Report No. 7661, Bolt, Beranek, and Newmann, Cambridge, MA, 1992.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Durlach, N.I, and A.S. Mavor, editors, VirtuaIReality Scientific and Technological Challenges, National Research Council Report, National Academy Press, Washington, D.C., 1995.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>673421</ref_obj_id>
				<ref_obj_pid>645908</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Fortune, Steve. Algorithms for Prediction of Indoor Radio Propagation. Technical Memorandum, Document #11274-960117-03TM, Bell Laboratories, 1996. A partial version of this paper appears in Applied Computational Geometry, Towards Geometric Engineering, proceedings of the FCRC '96 Workshop in conjunction with WACG '96, Philadelphia, PA, May, 1996, 157-166.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Foster, S.H., E.M. Wenzel, and R.M. Taylor. Real-time Synthesis of Complex Acoustic Environments. P~vceedings of the IEEE Workshop on Applications of Signal P~vcessing to Audio andAcoustics, New Paltz, NY, 1991.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807481</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Fuchs, H. Kedem, Z., and Naylor, B. On Visible Surface Generation by a Priori Tree Structures. Computer Graphics (Proc. SIGGRAPH '80), 124-133.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Fujomoto, Akira. Turbo Beam Tracing - A Physically Accurate Lighting Simulation Environment. Knowledge Based Image Computing Systems, May, 1988, 1-5.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Ghazanfarpour, G. and J. Marc Hasenfratz. A Beam Tracing with Precise Antialiasing for Polyhedral Scenes. Computer &amp; Graphics, 22, 1, 1998.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808601</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Goral, Cindy M., Kenneth E. Torrance, Donald R Greenberg, and Bennett Battaile. Modeling the Interaction of Light Between Diffuse Surfaces. Computer Graphics (Proc. SIGGRAPH '84), 18, 3, July, 1984, 213-222.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Haines, Eric A. Beams O' Light: Confessions of a Hacker. Frontiers in Rendering Course Notes, SIGGRAPH' 91, 1991.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808588</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Heckbert, Paul, and Pat Hanrahan. Beam Tracing Polygonal Objects. Computer Graphics (SIGGRAPH 84), 18, 3, 119-127.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97895</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Heckbert, Paul. Adaptive Radiosity Textures for Bidirectional Ray Tracing. Computer Graphics (SIGGRAPH 90), 24, 4, 145-154.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Heinz, R. Binaural Room Simulation Based on an Image Source Model with Addition of Statistical Methods to Include the Diffuse Sound Scattering of Walls and to Predict the Reverberant Tail J. Applied Acoustics, 38, 2-4,1993,145-160.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Hodgson, M. Evidence of Diffuse Surface Reflections in Rooms. J. Acoust. Soc. Am., 89, 1991,765-771.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Jones, C.B. A New Approach to the 'Hidden Line' Problem. The Computer Journal, 14, 3 (August 1971), 232-237.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15902</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Kajiya, James T. The Rendering Equation. Computer Graphics (SIGGRAPH 86), 143-150.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Keller, Joseph B. Geometrical Theory of Diffraction. Journal of the Optical Society of America, 52, 2, February, 1962,116-130.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Kleiner, Mendel, Bengt-Inge Dalenback, and Peter Svensson. Auralization- An Overview. J. Audio Eng. Soc., 41, 11, November, 1993, 861-875.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Kreuzgruber, R, R Unterberger, and R. Gahleitner. A Ray Splitting Model for Indoor Radio Propagation Associated with Complex Geometries. P~vceedings of the 1993 43rd IEEE Vehicular Technology Conference, 1993,227-230.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Kristiansen, U.R., A. Krokstad, and T. Follestad. Extending the Image Method to Higher-Order Reflections. J. Applied Acoustics, 38, 2-4, 1993,195-206.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Krockstadt, U.R. Calculating the Acoustical Room Response by the Use of a Ray Tracing Technique, J. Sound and Vibrations, 8, 18, 1968.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Kuttruff, Heinrich Room Acoustics, 3rd Edition, Elsevier Science, London, England, 1991.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Lehnert, Hilmar. Systematic Errors of the Ray-Tracing Algorithm. J. Applied Acoustics, 38, 2-4, 1993, 207-221.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Lewers, T. A Combined Beam Tracing and Radiant Exchange Computer Model of Room Acoustics. J. Applied Acoustics, 38, 2-4, 1993,161-178.]]></ref_text>
				<ref_id>36</ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[McGrath, David, and Andrew Reilly. Convolution Processing for Realistic Reverberation. The 98th Convention of the Audio Engineering Society, February, 1995.]]></ref_text>
				<ref_id>37</ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Monks, Michael, Byong Mok Oh, and Julie Dorsey. Acoustic Simulation and Visualization using a New Unified Beam Tracing and Image Source Approach. Meeting of the Audio Engineering Society, November, 1996.]]></ref_text>
				<ref_id>38</ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Moore, G.R. An Applvach to the Analysis of Sound in Auditoria. Ph.D. Thesis, Cambridge, UK, 1984.]]></ref_text>
				<ref_id>39</ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Naylor, G.M. ODEON - Another Hybrid Room Acoustical Model. J. Applied Acoustics, 38, 2-4, 1993, 131-144.]]></ref_text>
				<ref_id>40</ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[Naylor, B.F. Constructing Good Partitioning Trees. Graphics Intelface '93, Toronto, CA, May, 1993.]]></ref_text>
				<ref_id>41</ref_id>
			</ref>
			<ref>
				<ref_obj_id>234826</ref_obj_id>
				<ref_obj_pid>234821</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[Rajkumar, A., B.F. Naylor, and L. Rogers. Predicting RF Coverage in Large Environments using Ray-Beam Tracing and Partitioning Tree Represented Geometry. Wireless Networks, 1995.]]></ref_text>
				<ref_id>42</ref_id>
			</ref>
			<ref>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[Rindel, J.H. Modelling the Angle-Dependent Pressure Reflection Factor. J. Applied Acoustics, 38, 2-4, 1993, 223-234.]]></ref_text>
				<ref_id>43</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74365</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[Sequin, Carlo, and Eliot Smyrl. Parameterized Ray Tracing. Computer Graphics (SIGGRAPH 89), 23, 3,307-314.]]></ref_text>
				<ref_id>44</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134080</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[Smits, Brian, James R. Arvo, and David H. Salesin. An Importance-Driven Radiosity Algorithm. Computer Graphics (SIGGRAPH 92), 26, 2,273-282.]]></ref_text>
				<ref_id>45</ref_id>
			</ref>
			<ref>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[Stephenson, U., and U. Kristiansen. Pyramidal Beam Tracing and Time Dependent Radiosity. Fifteenth International Congress on Acoustics, Tapir, June, 1995, 657-660.]]></ref_text>
				<ref_id>46</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74353</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[Stettner, Adam, and Donald R Greenberg. Computer Graphics Visualization for Acoustic Simulation. Computer Graphics (SIGGRAPH 89), 23, 3,195-206.]]></ref_text>
				<ref_id>47</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134063</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[Takala, Tapio, and James Hahn. Sound Rendering. Computer Graphics (SIG- GRAPH 92), 26, 2, 211-220.]]></ref_text>
				<ref_id>48</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122725</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[Teller, Seth J., and Carlo H. S6quin, Visibility Preprocessing for Interactive Walkthroughs. Computer Graphics (SIGGRAPH 91), 25, 4, 61-69.]]></ref_text>
				<ref_id>49</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134029</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[Teller, Seth J. Computing the Antiumbra Cast by an Area Light Source. Computer Graphics (P1vc. SIGGRAPH '92), 26, 2 (August 1992), 139-148.]]></ref_text>
				<ref_id>50</ref_id>
			</ref>
			<ref>
				<ref_obj_id>171029</ref_obj_id>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[Teller, Seth J. Visibility Computations in Densely Occluded Polyhedral Envi~vnments. Ph.D. thesis, Computer Science Division (EECS), University of California, Berkeley, 1992. Also available as UC Berkeley technical report UCB/CSD-92-708.]]></ref_text>
				<ref_id>51</ref_id>
			</ref>
			<ref>
				<ref_obj_id>259236</ref_obj_id>
				<ref_obj_pid>259081</ref_obj_pid>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[Tsingos, Nicolas, and Jean-Dominique Gascuel. A General Model for Simulation of Room Acoustics Based On Hierarchical Radiosity. Technical Sketches, SIGGRAPH 97 Visual P1vceedings, 1997.]]></ref_text>
				<ref_id>52</ref_id>
			</ref>
			<ref>
				<ref_obj_id>266779</ref_obj_id>
				<ref_obj_pid>266774</ref_obj_pid>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[Tsingos, Nicolas, and Jean-DominiqueGascuel. Soundtracks for Computer Animation: Sound Rendering in Dynamic Environments with Occlusions. Graphics Intelface '97, Kelowna, May 21-23, 1997, 9-16.]]></ref_text>
				<ref_id>53</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258775</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[Veach, Eric, and Leonidas J. Guibas. Metropolis Light Transport. Computer Graphics (SIGGRAPH 97), 65-76.]]></ref_text>
				<ref_id>54</ref_id>
			</ref>
			<ref>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[Vian, J.P. and D. van Maercke. Calculation of the Room Response Using a Ray Tracing Method. P~vceedings of the ICA Symposium on Acoustics and Theater Planning for the Pelforming Arts, Vancouver, CA, 1986, 74-78.]]></ref_text>
				<ref_id>55</ref_id>
			</ref>
			<ref>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[Vorlander, M. International Round Robin on Room Acoustical Computer Simulations. P~vceedings of the 15th International Congress of Acoustics, Trondheim, Norway, June, 1995.]]></ref_text>
				<ref_id>56</ref_id>
			</ref>
			<ref>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[Walsh, John R, and Norm Dadoun. What Are We Waiting for? The Development of Godot, II. presented at the 103 rd Meeting of the Acoustical Society of America, Chicago, April, 1982.]]></ref_text>
				<ref_id>57</ref_id>
			</ref>
			<ref>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[Ward, William C., Gary, W. Elko, Robert A. Kubli, and W. Craig McDougald. The New Varechoic chamber at AT&amp;T Bell Labs. P~vceeding of Wallace Clement Sabine Centennial Symposium, Acoustical Society of America, New York, June, 1994, 343-346.]]></ref_text>
				<ref_id>58</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97920</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[Watt, Mark. Light-Water Interaction Using Backward Beam Tracing. Computer Graphics (SIGGRAPH 90), 24, 377-385.]]></ref_text>
				<ref_id>59</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563896</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[Weiler, K. and R Atherton. Hidden Surface Removal Using Polygon Area Sorting. Computer Graphics (SIGGRAPH 77), 11, 2,214-222.]]></ref_text>
				<ref_id>60</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358882</ref_obj_id>
				<ref_obj_pid>358876</ref_obj_pid>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[Whitted, Turner. An Improved Illumination Model for Shaded Display. Communications oftheACM, 23, 6, June, 1980, 343-349.]]></ref_text>
				<ref_id>61</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Copyright &#38;#169;1998 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers or to redistribute to lists, requires specific permission and/or a fee. A Beam Tracing Approach 
to Acoustic Modeling for Interactive Virtual Environments Thomas Funkhouser. , Ingrid Carlbom, Gary 
Elko, Gopal Pingali, Mohan Sondhi, and Jim West Bell Laboratories Abstract Virtual environment research 
has focused on interactive image gen­eration and has largely ignored acoustic modeling for spatialization 
of sound. Yet, realistic auditory cues can complement and enhance visual cues to aid navigation, comprehension, 
and sense of presence in virtual environments. A primary challenge in acoustic model­ing is computation 
of reverberation paths from sound sources fast enough for real-time auralization. We have developed a 
system that uses precomputed spatial subdivision and beam tree data struc­tures to enable real-time acoustic 
modeling and auralization in inter­active virtual environments. The spatial subdivision is a partition 
of 3D space into convex polyhedral regions (cells) represented as a cell adjacency graph. A beam tracing 
algorithm recursively traces pyra­midal beams through the spatial subdivision to construct a beam tree 
data structure representing the regions of space reachable by each potential sequence of transmission 
and specular re.ection events at cell boundaries. From these precomputed data structures, we can generate 
high-order specular re.ection and transmission paths at interactive rates to spatialize .xed sound sources 
in real-time as the user moves through a virtual environment. Unlike previous acoustic modeling work, 
our beam tracing method: 1) supports evaluation of reverberation paths at interactive rates, 2) scales 
to compute high­order re.ections and large environments, and 3) extends naturally to compute paths of 
diffraction and diffuse re.ection ef.ciently. We are using this system to develop interactive applications 
in which a user experiences a virtual environment immersively via simultane­ous auralization and visualization. 
Key Words: Beam tracing, acoustic modeling, auralization, spatialized sound, virtual environment systems, 
virtual reality. 1 Introduction Interactive virtual environment systems combine graphics, acous­tics, 
and haptics to simulate the experience of immersive exploration of a three-dimensional virtual world 
by rendering the environment as perceived from the viewpoint of an observer moving under real­time control 
by the user. Most prior research in virtual environment systems has focused on visualization (i.e., methods 
for rendering more realistic images or for increasing image refresh rates), while relatively little attention 
has been paid to auralization (i.e., rendering spatialized sound based on acoustical modeling). Yet, 
it is clear that .Princeton University we must pay more attention to producing realistic sound in order 
to create a complete immersive experience in which aural cues com­bine with visual cues to support more 
natural interaction within a virtual environment. First, qualitative changes in sound reverbera­tion, 
such as more absorption in a room with more lush carpets, can enhance and reinforce visual comprehension 
of the environment. Second, spatialized sound can be useful for providing audio cues to aid navigation, 
communication, and sense of presence [14]. For example, the sounds of objects requiring user attention 
can be spa­tialized according to their positions in order to aid object location and binaural selectivity 
of desired signals (e.g., cocktail party ef­fect). The goal of this work is to augment a previous interactive 
image generation system to support real-time auralization of sound based on realistic acoustic modeling 
in large virtual environments. We hope to use this system to support virtual environment appli­cations 
such as distributed training, simulation, education, home shopping, virtual meetings, and multiplayer 
games. A primary challenge in acoustic modeling is computation of reverberation paths from a sound source 
to a listener (receiver) [30]. As sound may travel from source to receiver via a multitude of re.ection, 
transmission, and diffraction paths, accurate simulation is extremely compute intensive. For instance, 
consider the simple example shown in Figure 1. In order to present an accurate model of a sound source 
(labeled S ) at a receiver location (labeled R ), we must account for an in.nite number of possible reverberation 
paths (some of which are shown). If we are able to model the reverberation paths from a sound source 
to a receiver, we can render a spatialized representation of the sound according to their delays, attenuations, 
and source and receiver directivities. Figure 1: Example reverberation paths. Since sound and light 
are both wave phenomena, acoustic mod­eling is similar to global illumination in computer graphics. How­ever, 
there are several signi.cant differences. First, the wavelengths of audible sound fall between 0.02 and 
17 meters (20kHz to 20Hz), more than .ve orders of magnitude longer than visible light. As a result, 
though re.ection of sound waves off large walls tends to be primarily specular, signi.cant diffraction 
does occur around edges of objects like walls and tables. Small objects (like coffee mugs) have signi.cant 
effect on the sound .eld only at frequencies beyond 4 kHz, and can usually be excluded from models of 
acoustic en­vironments, especially in the presence of other signi.cant sources of re.ection and diffraction. 
Second, sound travels through air 106 times slower than light, causing signi.cantly different arrival 
times for sound propagating along different paths, and the resulting acous­tic signal is perceived as 
a combination of direct and re.ected sound (reverberation). The time distribution of the reverberation 
paths of the sound in a typical room is much longer than the integration period of the perception of 
sound by a human. Thus, it is important to accurately compute the exact time/frequency distribution of 
the reverberation. In contrast, the speed of light and the perception of light is such that the eye integrates 
out the transient response of a light source and only the energy steady-state response needs to be calculated. 
Third, since sound is a coherent wave phenomenon, the calculation of the re.ected and scattered sound 
waves must incor­porate the phase (complex amplitude) of the incident and re.ected wave(s), while for 
incoherent light, only the power must be summed. Although acoustic modeling has been well-studied in 
the con­text of non-interactive applications [34], such as concert hall design, there has been relatively 
little prior research in real-time acoustic modeling for virtual environment systems [15]. Currently 
available auralization systems generally model only early specular re.ections, while late reverberations 
and diffractions are modeled with statisti­cal approximations [1, 25, 40, 53]. Also, due to the computational 
complexity of current systems, they generally consider only simple geometric arrangements and low-order 
specular re.ections. For in­stance, the Acoustetron [17] computes only .rst-and second-order specular 
re.ections for box-shaped virtual environments. Video games provide spatialized sound with ad hoc localization 
methods (e.g., pan effects) rather than with realistic geometrical acoustic modeling methods. The 1995 
National Research Council Report on Virtual Reality Scienti.c and Technological Challenges [15] states 
that current technology is still unable to provide interactive systems with real-time rendering of acoustic 
environments with complex, re­alistic room re.ections. In this paper, we describe a beam tracing method 
that computes high-order specular re.ection and transmission paths from .xed sources in large polygonal 
models fast enough to be used for au­ralization in interactive virtual environment systems. The key idea 
behind our method is to precompute and store spatial data struc­tures that encode all possible transmission 
and specular re.ection paths from each audio source and then use these data structures to compute reverberation 
paths to an arbitrarily moving observer view­point for real-time auralization during an interactive user 
session. Our algorithms for construction and query of these data structures have the unique features 
that they scale well with increasing num­bers of re.ections and global geometric complexity, and they 
extend naturally to model paths of diffraction and diffuse re.ection. We have incorporated these algorithms 
and data structures into a system that supports real-time auralization and visualization of large virtual 
environments. 2 Previous Work There has been a large amount of work in acoustic modeling. Prior methods 
can be classi.ed into four types: 1) image source methods, 2) radiant exchange methods 3) path tracing, 
and 4) beam tracing. 2.1 Image Source Methods Image source methods [2, 6] compute specular re.ection 
paths by considering virtual sources generated by mirroring the location of the audio source, S, over 
each polygonal surface of the environment (see Figure 2). For each virtual source, Si, a specular re.ection 
path can be constructed by iterative intersection of a line segment from the source position to the receiver 
position, R, with the re.ecting surface planes (such a path is shown for virtual source Scin Fig­ure 
2). Specular re.ection paths can be computed up to any order by recursive generation of virtual sources. 
The primary advantage of image source methods is their robust­ness. They can guarantee that all specular 
paths up to a given order or reverberation time will be found. The disadvantages of image source methods 
are that they model only specular re.ections, and Figure 2: Image source method. Sd d c a Sc Sa Sb b 
 their expected computational complexity has exponential growth. In general, O(n r)virtual sources must 
be generated for rre.ections in environments with nsurface planes. Moreover, in all but the sim­plest 
environments (e.g., a box), complex validity/visibility checks must be performed for each of the O(n 
r)virtual sources since not all of the virtual sources represent physically realizable specular re.ection 
paths [6]. For instance, a virtual source generated by re.ection over the non-re.ective side of a surface 
is invalid. Like­wise, a virtual source whose re.ection is blocked by another surface in the environment 
or intersects a point on a surface s plane which is outside the surface s boundary (e.g., Sain Figure 
2) is invisi­ble. During recursive generation of virtual sources, descendents of invalid virtual sources 
can be ignored. However, descendents of invisible virtual sources must still be considered, as higher-order 
re.ections may generate visible virtual sources (consider mirroring Saover surface d). Due to the computational 
demands of O(n r)vis­ibility checks, image source methods are practical only for acoustic modeling of 
few re.ections in simple environments [32]. 2.2 Radiant Exchange Methods Radiant exchange methods have 
been used extensively in computer graphics to model diffuse re.ection of radiosity between patches [21]. 
Brie.y, radiosity methods consider every patch a potential emitter and re.ector of radiosity. Conceptually, 
for every pair of patches, Aand B, a form factor is computed which measures the fraction of the radiosity 
leaving patch Athat arrives at patch B.This approach yields a set of simultaneous equations which are 
solved to obtain the radiosity for each patch. Although this approach has been used with good results 
for modeling diffuse indirect illumination in computer graphics, it is not easily extensible to acoustics. 
In acoustics modeling, transport equations must account for phase, specular re.ection tends to dom­inate 
diffuse re.ection, and extended form factor computations mustconsiderpathsofdiffractionaswellasspecularre.ection. 
Fur­thermore, to meet error tolerances suitable for acoustic modeling, patches must be substructured 
to a very .ne element mesh (typi­cally much less than the acoustic wavelength), the solution must be 
computed for many frequencies, and the representation of the sound leaving an element must be very data 
intensive, a complex func­tion of phase, direction, and frequency usually requiring thousands of bytes. 
As a result, direct extensions to prior radiosity methods [36, 39, 52] do not seem practical for large 
environments. 2.3 Path Tracing Methods Ray tracing methods [33, 61] .nd reverberation paths between 
a source and receiver by generating rays emanating from the source position and following them through 
the environment until an ap­propriate set of rays has been found that reach a representation of the receiver 
position (see Figure 3). Monte Carlo path tracing methods consider randomly gener­ated paths from the 
source to the receiver [28]. For instance, the Metropolis Light Transport algorithm [54] generates a 
sequence of light transport paths by randomly mutating a single current path by  Figure 3: Ray tracing 
method. adding, deleting, or replacing vertices. Mutated paths are accepted according to probabilities 
based on the estimated contribution they make to the solution. As contributing paths are found, they 
are logged and then mutated further to generate new paths in a Markov chain. Mutation strategies and 
acceptance probabilities are chosen to insure that the method is unbiased, strati.ed, and ergodic. A 
primary advantage of these methods is their simplicity. They depend only on ray-surface intersection 
calculations, which are rel­atively easy to implement and have computational complexity that grows sublinearly 
with the number of surfaces in the model. Another advantage is generality. As each ray-surface intersection 
is found, paths of specular re.ection, diffuse re.ection, diffraction, and re­fraction can be sampled 
[10], thereby modeling arbitrary types of indirect reverberation, even for models with curved surfaces. 
The primary disadvantages of path tracing methods stem from the fact that the continuous 5D space of 
rays is sampled by a dis­crete set of paths, leading to aliasing and errors in predicted room responses 
[35]. For instance, in ray tracing, the receiver position and diffracting edges are often approximated 
by volumes of space (in order to admit intersections with in.nitely thin rays), which can lead to false 
hits and paths counted multiple times [35]. Moreover, important reverberation paths may be missed by 
all samples. In order to minimize the likelihood of large errors, path tracing sys­tems often generate 
a large number of samples, which requires a large amount of computation. Another disadvantage of path 
tracing is that the results are dependent on a particular receiver position, and thus these methods are 
not directly applicable in virtual envi­ronment applications where either the source or receiver is moving 
continuously. 2.4 Beam Tracing Methods Beam tracing methods [23] classify re.ection paths from a source 
by recursively tracing pyramidal beams (i.e., sets of rays) through the environment. Brie.y, a set of 
pyramidal beams are constructed that completely cover the 2D space of directions from the source. For 
each beam, polygons are considered for intersection in order from front to back. As intersecting polygons 
are detected, the original beam is clipped to remove the shadow region, a transmission beam is constructed 
matching the shadow region, and a re.ection beam is constructed by mirroring the transmission beam over 
the polygon s plane (see Figure 4). S S c ReflectionTransmission Beam Beam Figure 4: Beam tracing method. 
As compared to image source methods, the primary advantage of beam tracing is that fewer virtual sources 
must be considered for environments with arbitrary geometry. Since each beam represents the region of 
space for which a corresponding virtual source (at the apex of the beam) is visible, higher-order virtual 
sources must be considered only for re.ections of polygons intersecting the beam. For instance, in Figure 
5, consider the virtual source Sa,which results from re.ection of Sover polygon a. The corresponding 
re.ection beam, Ra, contains exactly the set of receiver points for which Sais valid and visible. Similarly, 
Raintersects exactly the set of polygons (cand d) for which second-order re.ections are possible after 
specular re.ection off polygon a. Other polygons (b, e, f,and g) need not be considered for second order 
specular re.ections after a. Beam tracing allows the recursion tree of virtual sources to be pruned signi.cantly. 
On the other hand, the image source method is more ef.cient for a box-shaped environment for which a 
regular lattice of virtual sources can be constructed that are guaranteed to be visible for all receiver 
locations [2]. d Sa e g f Figure 5: Beam tracing culls invisible virtual sources. As compared to path 
tracing methods, the primary advantage of beam tracing is that it takes advantage of spatial coherence, 
as each beam-surface intersection represents an in.nite number of ray­surface intersections. Also, pyramidal 
beam tracing does not suffer from sampling artifacts of ray tracing [35] or the overlap problems of cone 
tracing [3, 55], since the entire 2D space of directions leaving the source can be covered by beams exactly. 
The primary disadvantage of beam tracing is that the geometric operations required to trace a beam through 
a 3D model (i.e., inter­section and clipping) are relatively complex, as each beam may be re.ected and/or 
obstructed by several surfaces. Another limitation is that re.ections off curved surfaces and refractions 
are dif.cult to model. Geometric beam tracing has been used in a variety of appli­cations, including 
acoustic modeling [13, 38, 46, 57], illumination [9, 19, 20, 22, 23, 59], and radio propagation [16]. 
The chal­lenge is to perform geometric operations (i.e., intersection, clipping, and mirroring) on beams 
ef.ciently as they are traced recursively through a complex environment. Some systems avoid the geometric 
complexity of beam tracing by approximating each beam by its medial axis ray for intersection and mirror 
operations [36], possibly splitting rays as they diverge with distance [31, 42]. In this case, the beam 
representation is only useful for modeling the distribution of rays/energy with distance and for avoiding 
large tolerances in ray-receiver intersection calcu­lations. If beams are not clipped or split when they 
intersect more than one surface, signi.cant reverberation paths can be missed. Heckbert and Hanrahan 
[23] described an algorithm for illumi­nation in which pyramidal beams represented by their 2D polygonal 
cross-sections are traced recursively either forward from a viewpoint or backward from a point light 
source. For each beam, all polygons are processed in front to back order. For each polygon intersecting 
the beam, the shadow region is cut out of the original beam using a polygon clipping algorithm capable 
of handling concavities and holes. The authors describe construction of an intermediate light beam tree 
data structure that encodes the beam tracing recursion and is used for later evaluation of light paths. 
Their implementation does not scale well to large environments since its computational complexity grows 
with O(n 2)for npolygons. Dadoun et al. [12, 13] described a beam tracing algorithm for acoustic modeling 
in which a hierarchical scene representation (HSR) is used to accelerate polygon sorting and intersection 
testing. During a preprocessing phase, a binary space partition (BSP) tree structure is constructed and 
augmented with storage for the convex hull for each subtree. Then, during beam tracing, the HSR is used 
to accelerate queries to .nd an ordered set of polygons potentially in­tersecting each beam. As in [23], 
beams are represented by their 2D polygonal cross-sections and are updated using the Weiler-Atherton 
clipping algorithm [60] at polygon intersections. Fortune [16] described a beam tracing algorithm for 
indoor radio propagation prediction in which a spatial data structure comprising layers of 2D triangulations 
is used to accelerate polygon intersec­tion testing. A method is proposed in which beams are partitioned 
into convex regions by planes supporting the edges of occluding polygons. However, Fortune expects that 
method to be too expen­sive for use in indoor radio propagation (where attenuation due to transmission 
is small) due to the exponential growth in the number of beams. Instead, he has implemented a system 
in which beams are traced directly from the source and along paths of re.ection, but are not clipped 
by occluding polygons. Instead, attenuation due to occlusion is computed for each path, taking into accountthe 
attenua­tion of each occluding polygon along the path. This implementation trades-off more expensive 
computation during path generation for less expensive computation during beam tracing. Jones [27] and 
Teller [51] have described beam tracing algo­rithms to compute a potentially visible set of polygons 
to render from a particular viewpoint in a computer graphics scene. These algorithms preprocess the scene 
into a spatial subdivision of cells (convex polyhedra) and portals (transparent, boundaries between cells). 
Polyhedral beams are traced through portals to determine the region of space potentially visible from 
a view frustum in order to produce a conservative and approximate solution to the hidden surface problem. 
In this paper, we describe beam tracing data structures and algorithms for real-time acoustic modeling 
in interactive virtual environment applications. Our method is most closely related to work in [23] and 
[51]. As compared to previous acoustic modeling methods, the unique features of our method are the ability 
to: 1) generate specular re.ection and transmission paths at interactive rates, 2) scale to support large 
virtual environments, 3) scale to compute high-order re.ection and transmission paths, and 4) extend 
to support ef.cient computation of diffraction and diffuse re.ection paths. We have included these algorithms 
and data structures in an interactive virtual environment system that supports immersive auralization 
and visualization in complex polygonal environments.  3 System Organization Our virtual environment 
system takes as input: 1) a description of the geometry and visual/acoustic surface properties of the 
environment (i.e., sets of polygons), and 2) a set of anechoic audio source signals at .xed locations. 
As a user moves through the virtual environment interactively, the system generates images as seen from 
a simulated observer viewpoint, along with a stereo audio signal spatialized according to the computed 
reverberation paths from each audio source to the observer location. In order to support real-time auralization, 
we partition our sys­tem into four distinct phases (see Figure 6), two of which are pre­processing steps 
that execute off-line, while the last two execute in real-time as a user interactively controls an observer 
viewpoint moving through a virtual environment. First, during the spatial sub­division phase, we precompute 
spatial relationships inherent in the set of polygons describing the environment and represent them in 
a cell adjacency graph data structure that supports ef.cient traversals of space. Second, during the 
beam tracing phase, we recursively follow beams of transmission and specular re.ection through space 
for each audio source. The output of the beam tracing phase is a beam tree data structure that explicitly 
encodes the region of space reachable by each sequence of re.ection and transmission paths from each 
source point. Third, during the path generation phase, we compute reverberation paths from each source 
to the receiver via lookup into the precomputed beam tree data structure as the receiver (i.e., the observer 
viewpoint) is moved under interactive user control. Finally, during the auralization phase, we spatialize 
each source audio signal (in stereo) according to the lengths, atten­uations, and directions of the computed 
reverberation paths. The spatialized audio output is synchronized with real-time graphics output to provide 
an immersive virtual environment experience. Figure 6: System organization. 3.1 Spatial Subdivision 
Our system preprocesses the geometric properties of the environ­ment and builds a spatial subdivision 
to accelerate beam tracing. The goal of this phase is to precompute spatial relationships in­herent in 
the set of polygons describing the environment and to represent them in a data structure that supports 
ef.cient traversals of space. The spatial subdivision is constructed by partitioning 3D space into a 
set of convex polyhedral regions and building a graph that explicitly represents the adjacencies between 
the regions of the subdivision. We build the spatial subdivision using a Binary Space Partition (BSP) 
[18], a recursive binary split of 3D space into convex polyhe­dral regions (cells) separated by planes. 
To construct the BSP, we recursively split cells by candidate planes selected by the method described 
in [41]. The binary splitting process continues until no input polygon intersects the interior of any 
BSP cell. The result of the BSP is a set of convex polyhedral cells whose convex, planar boundaries contain 
all the input polygons. An adjacency graph is constructed that explicitly represents the neighbor relationships 
between cells of the spatial subdivision. Each cell of the BSP is represented by a node in the graph, 
and two nodeshave a link between them for each planar, polygonal boundary shared by the corresponding 
adjacent cells in the spatial subdivision. Construction of the cell adjacency graph is integrated with 
the binary space partitioning algorithm. If a leaf in the BSP is split into two by a plane, we create 
new nodes in the graph corresponding to the new cells in the BSP, and we update the links of the split 
leaf s neighbors to re.ect the new adjacencies. We create a separate link between two cells for each 
convex polygonal region that is entirely either transparent or opaque along the cells shared boundary. 
A simple 2D example model (on left) and its cell adjacency graph (on right) are shown in Figure 7. Input 
polygons appear as solid line segments labeled with lower-case letters (a,q); transpar­ent cell boundaries 
introduced by the BSP are shown as dashed line segments labeled with lower-case letters (r,u); constructed 
cell regions are labeled with upper-case letters (A,E); and, links are drawn between adjacent cells sharing 
a convex polygonal bound­ary. adk adk b b A f e C r j D u c g B i s t E m n ll q o q o h pC pE (a) 
Input model. (b) Cell adjacency graph. Figure 7: Example spatial subdivision. 3.2 Beam Tracing After 
the spatial subdivision has been constructed, we use it to accel­erate traversals of space in our beam 
tracing algorithm. Beams are traced through the cell adjacency graph via a recursive depth-.rst traversal 
starting in the cell containing the source point. Adjacent cells are visited recursively while a beam 
representing the region of space reachable from the source by a sequence of cell bound­ary re.ection 
and transmission events is incrementally updated. As the algorithm traverses a cell boundary into a new 
cell, the current convex pyramidal beam is clipped to include only the region of space passing through 
the convex polygonal boundary polygon. At re.ecting cell boundaries, the beam is mirrored across the 
plane supporting the cell boundary in order to model specular re.ections. As an example, Figure 8 shows 
a sequence of beams (green polyhe­dra) traced up to one re.ection from a source (white point) through 
the spatial subdivision (blue X s are cell boundaries) for a simple set of input polygons (red surfaces). 
 Figure 8: A beam clipped and re.ected at cell boundaries. Throughout the traversal, the algorithm maintains 
a current cell (a reference to a cell in the spatial subdivision) and a current beam (an in.nite convex 
pyramidal beam whose apex is the source point). Initially, the current cell is set to be the cell containing 
the source point and the current beam is set to cover all of space. During each step of the depth-.rst 
traversal, the algorithm continues recursively for each boundary polygon, P,of the current cell, C, that 
intersects the current beam, B.If Pdoes not coincide with an opaque input surface, the algorithm follows 
a transmission path, recursing to the cell adjacent to Cacross Pwith a transmission beam, Bt, constructed 
as the intersection of Bwith a pyramidal beam whose apex is the source point and whose sides pass through 
the edges of P. Likewise, if Pcoincides with a re.ecting input surface, the algorithm follows a specular 
re.ection path, recursing in cell Cwith a specular re.ection beam, Br, constructed by mirroring the transmission 
beam over the plane supporting P. The depth-.rst traversal along any path terminates when the length 
of a path exceeds a user-speci.ed threshold or when the cumulative absorption due to transmissionandre.ectionexceedsapresetthreshold. 
Thetraversal may also be terminated when the total number of re.ections or transmissions exceeds a third 
threshold. Figure 9 contains an illustration of the beam tracing algorithm execution for specular re.ections 
through the simple 2D example model shown in Figure 7. The depth-.rst traversal starts in the cell (labeled 
D ) containing the source point (labeled S ) with a beam containing the entire cell (shown as dark green). 
Beams are created and traced for each of the six boundary polygons of cell D (j, k, l, m, n,and u). For 
example, transmission through the cell boundary labeled u results in a beam (labeled Tu) that is trimmed 
as it enters cell E. Tuintersects only the polygon labeled o, which spawnsa re.ection beam (labeled TuRo). 
That beam intersects only the polygon labeled p, which spawns a re.ection beam (labeled TuRoRp). Execution 
continues recursively for each beam until the length of every path exceeds a user-speci.ed threshold 
or when the absorption along every path becomes too large. A C S D u B s t TuRoRp E Tu TuRo o p Figure 
9: Beam tracing through cell adjacency graph. While tracing beams through the spatial subdivision, our 
algo­rithm constructs a beam tree data structure [23] to be used for rapid determination of reverberation 
paths from the source point later dur­ingthepathgenerationphase. Thebeamtreecorrespondsdirectlyto the 
recursion tree generated during the depth-.rst traversal through the cell adjacency graph. It is similar 
to the stab tree data struc­ture used by Teller to encode visibility relationships for occlusion culling 
[51]. Each node of the beam tree stores: 1) a reference to the cell being traversed, 2) the cell boundary 
most recently traversed (if there is one), and 3) the convex beam representing the region of space reachable 
by the sequence of re.ection and transmission events along the current path of the depth-.rst traversal. 
To further accelerate reverberation path generation, each node of the beam tree also stores the cumulative 
attenuation due to re.ective and trans­missive absorption, and each cell of the spatial subdivision stores 
a list of back-pointers to its beam tree nodes. Figure 10 shows a partial beam tree corresponding to 
the traversal shown in Figure 9.  3.3 Path Generation Duringaninteractive sessionin whichtheusernavigatesasimulated 
observer (receiver) through the virtual environment, reverberation paths from a particular source point, 
S, to the moving receiver point, R, can be generated quickly via lookup in the beam tree data structure. 
First, the cell containing the receiver point is found by logarithmic-time search of the BSP. Then, each 
beam tree node, T, associated with that cell is checked to see whether the beam stored with Tcontains 
the receiver point. If it does, a viable ray path from the source point to the receiver point has been 
found, and the ancestors of Tin the beam tree explicitly encode the set of re.ections and transmissions 
through the boundaries of the spatial subdivision that a ray must traverse from the source to the receiver 
along this path (more generally, to any point inside the beam stored with T). The attenuation, length, 
and directional vectors for the corre­sponding reverberation path can be derived quickly from the data 
stored with the beam tree node, T. Speci.cally, the attenuation due to re.ection and transmission can 
be retrieved from Tdirectly. The length of the reverberation path and the directional vectors at the 
source and receiver points can be easily computed as the source s re.ected image for this path is stored 
explicitly in Tas the apex of its pyramidal beam. The actual ray path from the source point to the receiver 
point can be generated by iterative intersection with the re.ecting cell boundaries stored with the ancestors 
of T.For example, Figure 11 shows the specular re.ection path to a particular receiver point (labeled 
R ) for the example shown in Figure 9.   So Io o Ip p Sop 3.4 Auralization Once a set of reverberation 
paths from a source to the receiver has been computed, the source-receiver impulse response is generated 
by adding one pulse corresponding to each distinct path from the source to the receiver. The delay associated 
with each pulse is given by L/C,where Lis the length of the corresponding reverberation path, and Cis 
the speed of sound. Since the pulse is attenuated by every re.ection and dispersion, the amplitude of 
each pulse is given by A/L,where Ais the product of all the frequency­independent re.ectivity and transmission 
coef.cients for each of the re.ecting and transmitting surfaces along the corresponding reverberation 
path. At the receiver, the binaural impulse responses (response of the left and right ears) are different 
due to the directivity of each ear. These binaural impulse responses are generated by multiply­ing each 
pulse of the impulse response by the cardioid directivity function (1/2(1 +cos(0)),where 0is the angle 
of arrival of the pulse with respect to the normal vector pointing out of the ear) cor­responding to 
each ear. This rough approximation to actual head scattering and diffraction is similar to the standard 
two-point stereo microphone technique used in high .delity audio recording. Fi­nally, the (anechoic) 
input audio signal is auralized by convolving it with the binaural impulse responses to produce a stereo 
spatialized audio signal. In the future, we intend to incorporate source direc­tivity, frequency-dependent 
absorption [34], and angle-dependent absorption [11, 43] into our acoustic models. A separate, concurrently 
executing process is spawned to per­form convolution of the computed binaural impulse responses with 
the input audio signal. In order to support real-time auralization, transfer of the impulse responses 
from the path generation process to the convolution process utilizes double buffers synchronized by a 
semaphore. Each new pair of impulse responses is loaded by the path generation process into a back buffer 
as the convolution pro­cess continues to access the current impulse responses stored in the front buffer. 
A semaphore is used to synchronize the processes as the front and back buffer are switched.  4Results 
The 3D data structures and algorithms described in the preceding sections have been implemented in C++ 
and run on Silicon Graphics and PC/Windows computers. To test whether the algorithms scale well as the 
complexity of the 3Denvironmentand the numberof specularre.ectionsincrease,we executed a series of experiments 
with our system computing spatial subdivisions, beam trees, and specular re.ection paths for various 
architectural models of different complexities. Our test models ranged from a simple box to a complex 
building, Soda Hall, the computer science building at UC Berkeley (an image and description of each test 
model appears in Figure 12). The experiments were run on a Silicon Graphics Octane workstation with 640MB 
of memory and used one 195MHz R10000 processor. (a) Box: 1 cube. (b) Rooms: 2 rooms connected by door. 
(6 polygons) (20 polygons)  (c) Suite: 9 rooms in of.ce space. (d) Maze: 16 rooms connected by hallways. 
(184 polygons) (602 polygons) (e) Floor: .50 rooms of Soda Hall. (f) Building: .250 rooms of Soda Hall. 
(1,772 polygons) (10,057 polygons)  Figure 12: Test models (source locations are gray dots). 4.1 Spatial 
Subdivision Results We .rst constructed the spatial subdivision data structure (cell ad­jacency graph) 
for each test model. Statistics from this phase of the experiment are shown in Table 1. Column 2 lists 
the number of input polygons in each model, while Columns 3 and 4 contain the numbers of cells and links, 
respectively, generated by the spatial subdivision algorithm. Column 5 contains the wall-clock time (in 
seconds) for the algorithm to execute, while Column 6 shows the storage requirements (in MBs) for the 
resulting spatial subdivision. Model Name # Polys # Cells # Links Time (sec) Storage (MB) Box Rooms Suite 
Maze Floor Bldg 6 20 184 602 1,772 10,057 7 12 98 172 814 4,512 18 43 581 1,187 5,533 31,681 0.0 0.1 
3.0 4.9 22.7 186.3 0.004 0.029 0.352 0.803 3.310 18.694 Table 1: Spatial subdivision statistics. Empirically, 
we .nd that the numbers of cells and links created by our spatial subdivision algorithm grow linearly 
with the number of input polygons for typical architectural models (see Figure 13), rather than quadratically 
as is possible for worst case geometric ar­rangements. The reason for linear growth can be seen intuitively 
in the two images inlaid in Figure 13, which compare spatial sub­divisions for the Maze test model (on 
the left) and a 2x2 grid of Maze test models (on the right). The 2x2 grid of Mazes has ex­actly four 
times as many polygons and approximately four times as many cells. The storage requirements of the spatial 
subdivision data structure also grow linearly as they are dominated by the vertices of link polygons. 
5K 4K 3K 2K 1K # Cells in Spatial Subdivision # Polygons in Environment Figure 13: Plot of subdivision 
size vs. polygonal complexity. The time required to construct the spatial subdivisions grows super-linearly, 
dominated by the code that selects and orders split­ting planes during BSP construction (see [41]). It 
is important to note that the spatial subdivision phase must be executed only once off-line for each 
geometric model, as its results are stored in a .le, allowing rapid reconstruction in subsequent beam 
tracing execu­tions.  4.2 Beam Tracing Results We experimented with our beam tracing algorithm for sixteen 
source locations in each test model. The source locations were chosen to represent typical audio source 
positions (e.g., in of.ces, in common areas, etc.) they are shown as gray dots in Figure 12 (experiments 
with the Building test used the same source locations as are shown in the Floor model). For each source 
location, we traced beams (i.e., constructed a beam tree) .ve times, each time with a different limit 
on the maximum number of specular re.ections (e.g., up to 0, 1, 2, 4, or 8 re.ections). Other termination 
criteria based on attenuation or path length were disabled, and transmission was ignored, in order to 
isolate the impact of input model size and maximum number of specular re.ections on computational complexity. 
Table 2 contains statistics gathered during the beam tracing experiment each row represents an execution 
with a particular test model and maximum number of re.ections, averaged over all 16 source locations. 
Columns 2 and 3 show the number of polygons describing each test model and the maximum number of specular 
re.ections allowed in each test, respectively. Column 4 contains the average number of beams traced by 
our algorithm (i.e., the average number of nodes in the resulting beam trees), and Column 5 shows the 
average wall-clock time (in milliseconds) for the beam tracing algorithm to execute.  Beam Tracing 
# Time Path Generation Model # # # Time Name Polys R. Beams (ms) Paths (ms)    Box 6 0 1 0 1.0 0.0 
1 7 1 7.0 0.1 2 37 3 25.0 0.3 4 473 42 129.0 6.0 8 10,036 825 833.0 228.2    Rooms 20 0 3 0 1.0 0.0 
1 31 3 7.0 0.1 2 177 16 25.1 0.3 4 1,939 178 127.9 5.2 8 33,877 3,024 794.4 180.3 Suite 184 0 7 1 1.0 
0.0 1 90 9 6.8 0.1 2 576 59 25.3 0.4 4 7,217 722 120.2 6.5 8 132,920 13,070 672.5 188.9 Maze 602 0 11 
1 0.4 0.0 1 167 16 2.3 0.0 2 1,162 107 8.6 0.1 4 13,874 1,272 36.2 2.0 8 236,891 21,519 183.1 46.7 Floor 
1,772 0 23 4 1.0 0.0 1 289 39 6.1 0.1 2 1,713 213 21.5 0.4 4 18,239 2,097 93.7 5.3 8 294,635 32,061 467.0 
124.5 Bldg 10,057 0 28 5 1.0 0.0 1 347 49 6.3 0.1 2 2,135 293 22.7 0.4 4 23,264 2,830 101.8 6.8 8 411,640 
48,650 529.8 169.5 Table 2: Beam tracing and path generation statistics. Scale with Increasing Polygonal 
Complexity We readily see from the results in Column 4 that the number of beams traced by our algorithm 
(i.e., the number of nodes in the beam tree) does not grow at an exponential rate with the number of 
polygons in these environments (as it does using the image source method). Each beam traced by our algorithm 
pre-classi.es the regions of space according to whether the corresponding virtual source (i.e., the apex 
of the beam) is visible to a receiver. Rather than generating O(n)virtual sources (beams) at each step 
of the recursion as in the image source method, we directly .nd only the potentially visible virtual 
sources via beam-polygon intersection and cell adjacency graph traversal. We use the current beam and 
the current cell of the spatial subdivision to .nd the small set of polygon re.ections that admit visible 
higher-order virtual sources. The bene.t of this approach is particularly important for large environments 
in which the boundary of each convex cell is sim­ple, and yet the entire environment is very complex. 
As an ex­ample, consider computation of up to 8 specular re.ections in the Building test model (the last 
row of Table 2). The image source method must consider approximately 1,851,082,741 virtual sources P8 
( (10;057/2)r), assuming half of the 10,057 polygons are r.0 front-facing to each virtual source. Our 
beam tracing method con­siders only 411,640 virtual sources, a difference of four orders of magnitude. 
In most cases, it would be impractical to build and store the recursion tree without such effective pruning. 
In densely-occluded environments,in which all but a little part of the environment is occluded from any 
source point (e.g., most buildings, cities, etc.), the number of beams traced by our algorithm does not 
even grow linearly with the total number of polygons in the environment (see Figure 14). In these environments, 
the number of boundaries on each cell is nearly constant, and a nearly constant number of cells are reached 
by each beam, leading to near-constant expected-case complexity of our beam tracing algorithm with in­creasing 
global environment complexity. As an example, the two images inlaid in Figure 14 show that the number 
of beams (green) traced in the Maze test model (left) does not increase signi.cantly Tree Depth Total 
Nodes Interior Nodes Leaf Nodes Branching Factor 0 1 2 3 4 5 6-10 11-15 >15 1 16 104 447 1,302 3,100 
84,788 154,790 96,434 1 16 104 446 1,296 3,092 72,469 114,664 61,079 0 0 0 1 6 8 12,319 40,126 35,355 
16.0000 6.5000 4.2981 2.9193 2.3920 2.0715 1.2920 1.2685 1.1789 Table 3: Example beam tree branching 
statistics. if the model is increased to be a 2x2 grid of Maze models (right). The beam tracing algorithm 
is impacted only by local complexity, As an example, consider Table 3 which shows the average and not 
by global complexity. branching factor for nodes at each depth of the beam tree con­ # Beams Traced (up 
to 8 reflections) structed for up to 8 specular re.ections in the Building model from 400K one source 
location. The average branching factor (Column 5) gen­ erally decreases with tree depth and is generally 
bounded by a small constant in lower levels of the tree. 300K On the other hand, if a beam is trimmed 
by many cell bound­aries and becomes too narrow, the advantages of beam tracing over ray tracing are 
diminished. This observation suggests a possible future hybrid approach in which medial rays are used 
to approxi­mate intersections for beams whose cross-sectional area falls below 100K a threshold.  4.3 
Path Generation Results # Polygons in Environment In order to verify that specular re.ection paths can 
be computed from Figure 14: Plot of beam tree size vs. polygonal complexity. .xed sources at interactive 
rates as the receiver moves, we conducted Scale with Increasing Re.ections We see that the number of 
beams traced by our algorithm grows exponentially, but far slower than O(n r), as we increase the max­imum 
number of re.ections. Figure 15 shows a logscale plot of the average number of beams traced in the Building 
model with increasing numbers of specular re.ections. The beam tree growth is less than O(n r)because 
each beam narrows as it is clipped by the cell boundaries it has traversed, and thus it tends to intersect 
fewer cell boundaries (see the example beam inlaid in Figure 15). In the limit, each beam becomes so 
narrow that it intersects only one or two cell boundaries, on average, leading to a beam tree with a 
small branching factor (rather than a branching factor of O(n),asin the image source method). experiments 
to quantify the complexity of generating specular re­.ection paths to different receiver locations from 
precomputedbeam trees. Foreachbeamtreeconstructedinthepreviousexperiment,we logged statistics during 
generation of specular reverberation paths to 16 different receiver locations. Receivers were chosen 
randomly within a two foot sphere around the source to represent a typical au­dio scenario in which the 
source and receiver are in close proximity within the same room. We believe this represents a worst-case 
scenario as fewer paths would likely be found to more remote and more occluded receiver locations. Columns 
6 and 7 of Table 2 contain statistics gathered during path generation for each combination of model and 
termination cri­terion averaged over all 256 source-receiver pairs (i.e., 16 receivers for each of the 
16 sources). Column 6 contains the average num­ber of reverberation paths generated, while Column 7 shows 
the average wall-clock time (in milliseconds) for execution of the path generation algorithm. Figure 
16 shows a plot of the wall-clock time required to generate up to eighth-order specular re.ection paths 
for  each test model. Path Generation Time (in seconds)(up to 8 reflections) 0.20 0.15 0.10 0.05  
 Maximum Number of Reflections # Polygons in Environment We .nd that the number of specular re.ection 
paths between a source and receiver in close proximity of one another is nearly constant across all of 
our test models. Also, the time required by our path generation algorithm is generally not dependent 
on the number of polygons in the environment (see Figure 16), nor is it dependent on the total number 
of nodes in the precomputed beam tree. This result is due to the fact that our path generation algorithm 
considers only nodes of the beam tree with beams residing inside the cell containing the receiver location. 
Therefore, the computation time required by the algorithm is not dependent on the complexity of the whole 
environment, but instead on the number of beams that traverse the receiver s cell. Overall, we .nd that 
our algorithm supports generation of spec­ular re.ection paths between a .xed source and any (arbitrarily 
moving) receiver at interactive rates in complex environments. For instance, we are able to compute up 
to 8th order specular re.ection paths in the Building environment with more than 10,000 polygons at a 
rate of approximately 6 times per second (i.e., the rightmost point in the plot of Figure 16).  4.4 
Auralization Results We have integrated the acoustic modeling method described in this paper into an 
interactive system for audio/visual exploration of vir­tual environments (e.g., using VRML). The system 
allows a user to move through a virtual environment while images and spatialized audio are rendered in 
real-time according to the user s simulated viewpoint. Figure 17 shows one application we have developed, 
called VirtualWorks, in which a user may interact with objects (e.g., click on them with the mouse) in 
the virtual environment to invoke behaviors that present information in various media, including text, 
image, video, and spatialized audio. For instance, if the user clicks on the workstation sitting on the 
desk, the application invokes a video which is displayed on the screen of that workstation. We are using 
this system to experiment with 3D user interfaces for presentation of multimedia data and multi-user 
interaction. We ran experiments with this application using a Silicon Graph­ics Octane workstation with 
640MB of memory and two 195MHz R10000 processors. One processor was used for image generation and acoustic 
modeling (i.e., reverberation path generation), while the second processor was dedicated solely to auralization 
(i.e., con­volution of the computed stereo impulse responses with audio sig­nals). Due to the differences 
between graphics and acoustics described in Section 1, the geometry and surface characteristics of the 
virtual environment were input and represented in two separate forms, one for graphics and another for 
acoustics. The graphical model (shown in Figure 17) was represented as a scene graph containing 80,372 
polygons, most of which describe the furniture and other small, detailed, visually-important objects 
in the environment. The acoustical model contained only 184 polygons, which described the ceilings, walls, 
cubicles, .oors, and other large, acoustically­important features of the environment (it was identical 
to the Suite test model shown in Figure 12c). We gathered statistics during sample executions of this 
ap­plication. Figures 17b-c show an observer viewpoint path (red) along which the application was able 
to render between eight and twelve images per second, while simultaneously auralizing four au­diosources(labeled1-4)in 
stereoaccordingtofourth-orderspecular re.ection paths updated during each frame. While walking along 
this path, it was possible to notice subtle acoustic effects due to re.ections and occlusions. In particular, 
near the viewpoint labeled A in Figure 17b, audio source 2 became very reverberant due to re.ections 
(cyan lines) in the long room. Likewise, audio source 3 suddenlybecame much louder and then softer as 
the observer passed by an open doorway near the viewpoint labeled B in Figure 17c. Throughout our experiments, 
the auralization process was the bottleneck. Our C++ convolution code running on a R10000 pro­cessor 
could execute fast enough to output 8 KHz stereo audio for a set of impulse responses cumulatively containing 
around 500 non­zero elements. We are planning to integrate DSP-based hardware [37] with our system to 
implement real-time convolution in the near future.  5 Discussion 5.1 Geometric Limitations Our method 
is not practical for all virtual environments. First, the geometric input must comprise only planar polygons. 
Each acoustic re.ector is assumed to be locally reacting and to have dimensions far exceeding the wavelength 
of audible sound (since initially we are assuming that specular re.ections are the dominant components 
of reverberation). Second, the ef.ciency of our method is greatly impacted by the complexity and quality 
of the constructed spatial subdivision. For best results, the polygons should be connected (e.g., without 
small cracks between them) and arranged such that a large part of the model is occluded from any position 
in space (e.g., like most buildingsorcities). Speci.cally,ourmethodwouldnotperformwell for geometric 
models with high local geometric complexity (e.g., a forest of trees). In these cases, beams traced through 
boundaries of cells enclosing free space would quickly become fragmented into many smaller beams, leading 
to disadvantageous growth of the beam tree. For this reason, our method is not as well suited for global 
illumination as it is for acoustic modeling, in which small objects can be ignored and large surfaces 
can be modeled with little geometric surface detail due to the longer wavelengths of audible sound. Third, 
the major occluding and re.ecting surfaces of the virtual environment must be static during interactive 
path generation and auralization. If any acoustically signi.cant polygon moves, the spatial subdivision 
and every beam tree must be recomputed. The class of geometric models for which our method does work 
well includes most architectural and urban environments. In these models, acoustically signi.cant surfaces 
are generally planar, large, and stationary, and the acoustical effects of any sound source are limited 
to a local region of the environment.  5.2 Diffraction and Diffuse Re.ection Our current 3D implementation 
traces beams only along paths of specular re.ection and transmission, and it does not model other scattering 
effects. Of course, paths of diffraction and diffuse re­.ection are also important for accurate acoustic 
modeling [34, 26]. Fortunately, our beam tracing algorithm and beam tree represen­tation can be generalized 
to model these effects. For instance, new beams can be traced that enclose the region of space reached 
by diffracting and diffuse re.ection paths, and new nodes can be added to the beam tree representing 
diffractions and diffuse re.ec­tion events at cell boundaries. For these more complex scattering phenomena, 
the geometry of the beams is most useful for comput­ing candidate reverberation paths, while the amplitude 
of the signal along any of the these paths can be evaluated for a known receiver during path generation. 
We have already included these extensions in a 2D beam tracing implementation, and we are currently working 
on a similar 3D implementation. First, consider diffraction. According to the Geometrical The­ory of 
Diffraction [29], an acoustic .eld that is incident on a dis­continuity along an edge has a diffracted 
wave that propagates into the shadow region. The diffracted wave can be modeled in geo­metric terms by 
considering the edge to be a source of new waves emanating from the edge. Higher order re.ections and 
diffractions occur as diffracted waves impinge upon other surfaces and edge discontinuities. By using 
edge-based adjacency information in our spatial subdivision data structure, we can quickly perform the 
geo­metric operations required to construct and trace beams along paths of diffraction. For a given beam, 
we can .nd edges causing diffrac­tion, as they are the ones: 1) intersected by the beam, and 2) shared 
by cell boundaries with different acoustic properties (e.g., one is transparent and another is opaque). 
For each such edge, we can determine the region of space reached by diffraction at that edge by tracing 
a beam whose source coincides with the portion of the edge intersected by the impinging beam, and whose 
extent is bounded by the solid wedge of opaque surfaces sharing the edge. Fordensely-occludedenvironments,eachsuchdiffraction 
beamcan be computed and traced in expected-case constant time.  Second, consider diffuse re.ection. 
We may model complex re.ections and diffractions from some highly faceted surfaces as diffuse re.ections 
from planar surfaces emanating equally in all directions. To compute the region of space reached by such 
a re.ec­tion using our approach, we can construct a beam whose source is the convex polygonal region 
of the surface intersected by an im­pinging beam and whose initial extent encloses the entire halfspace 
in front of the surface. We can trace the beam through the cell adjacency graph to .nd the region of 
space reached from any point on the re.ecting part of the surface (i.e., the anti-penumbra [50]). We 
have implemented these methods so far in 2D using a pla­nar winged-edge representation [5] for the spatial 
subdivision and a bow-tie representation [49] for the beams. Unfortunately, tracing 3D beams of diffraction 
and diffuse re.ection is more compli­cated. First, the source of each diffraction beam is no longer a 
point, but a .nite edge, and the source of each diffuse re.ection beam is generally a convex polygon. 
Second, as we trace such beams through the spatial subdivision, splitting and trimming them as they passes 
through multiple convex polygonal cell boundaries, their bounding surfaces can become quadric surfaces 
(i.e., reguli) [50]. Finally, evaluation of the amplitude of the signal along a path of diffraction or 
diffuse re.ection requires integration over (possibly multiple) edges and polygons. We are currently 
extending our 3D data structures and algorithms to model these effects. Ini­tially, we are planning to 
trace polyhedral beams that conservatively over-estimate the region covered by an exact, more complex, 
repre­sentation of the scattering patterns. Then, as each reverberation path to a particular receiver 
is considered, we will check whether it lies within the exact scattering region, or whether it should 
be discarded because it lies in the over-estimating region of the polyhedral beam.  5.3 Visualization 
In order to aid understanding and debugging of our acoustic model­ing method, we have found it extremely 
valuable to use interactive visualization. So far, we have concentrated on visualization of our data 
structures and algorithms. Our system provides menu and key­board commands that may be used to toggle 
display of the: 1) input polygons (red), 2) source point (white), 3) receiver point (purple), 4) boundaries 
of the spatial subdivision (gray), 5) pyramidal beams (green), 6) image sources (cyan), and 7) reverberation 
paths (yel­low). The system also supports visualization of acoustic metrics (e.g., power, clarity, etc.) 
computed for a set of receiver locations on a regular planar grid displayed with a textured polygon. 
Example visualizations are shown in Figures 18-20.  Of course, many commercial [7, 8, 40] and research 
systems [38, 47] provide elaborate tools for visualizing computed acoustic metrics. The critical difference 
in our system is that it supports continuous interactive updates of reverberation paths and debugging 
information as a user moves the receiver point with the mouse. For instance, Figures 18 and 20 show eighth-order 
specular re.ection paths (yellow lines) from a single audio source (white points) to a receiver location 
(purple points) which can be updated more than six times per second as the receiver location is moved 
arbitrarily. The user may select any reverberation path for further inspection by clicking on it and 
then independently toggle display of re.ecting cell boundaries, transmitting cell boundaries, and the 
associated set of pyramidal beams for the selected path. data structure. Each beam tree node is dynamically 
colored in the diagram according to whether the receiver point is inside its associated beam (white) 
or cell (green). Another popup window shows a plot of the impulse response representing the reverberation 
paths from source to receiver (see Figure 20). A third popup window shows values of various acoustic 
metrics, including power, clarity, reverberation time, and frequency response. All of the information 
displayed is updated in real-time as the user moves the receiver interactively with the mouse.  6 Future 
Work 6.1 System Extensions Oursystemcouldbeextendedinmanyways. Forinstance,thebeam tracing algorithm 
is well-suited for parallelization, with much of the previous work in parallel ray tracing directly applicable 
[4]. Also, the geometric regions covered by each node of the beam tree could be stored in a single hierarchical 
spatial structure (e.g., a BSP), al­lowing logarithmic search during path generation, rather than linear 
search of the beams inside a single cell. HRFT (Head-Related Trans­fer Functions) directional .ltering, 
angle-dependent and frequency­dependent acoustic properties of absorption, and source directivity should 
be included in our acoustical models. Of course, we could also use beam trees to allow a user to manipulate 
the acoustic prop­erties of individual surfaces of the environment interactively with real-time feedback, 
like parameterized ray tracing [44]. 6.2 Moving Sources In order to support acoustic modeling in real-time, 
our current ap­proach is to .x the position of each sound source and to precompute and store potential 
reverberation paths from that position to all points in space (i.e., the beam tree) so that reverberation 
paths to a speci.c receiver can be generated quickly. This method achieves interactive performance by 
trading real-time processing for storage and precomputation time. Yet, it requires that each sound source 
be stationary, which is not adequate to support all virtual environ­ment applications (e.g., multi-user 
chat). In order to extend our method to support real-time acoustic modeling for virtual environ­ments 
with moving sound sources, one approach is to precompute and store beam trees for a .nite number of source 
locations (e.g., on a grid), and then derive the impulse response for any arbitrary source location via 
interpolation. A second approach is to rework our beam tracing algorithm (i.e., the second phase, which 
currently executes as a preprocessing step) to execute in real-time at interactive rates. Although real-time 
beam tracing requires improvement of one or two orders of magni­tude (beam tracing times for our test 
models ranged from 0.8 to 49 seconds for eigth-order specular re.ections), we are optimistic that this 
is possible. In contrast to our current beam tracing precomputa­tion, which must consider potential receivers 
at any point in space, a real-time beam tracing algorithm must compute reverberationpaths only to a speci.c 
set of known receiver positions (e.g., the locations of other avatars in a multi-user chat). Therefore, 
we can implement a far more ef.cient beam tracing algorithm by employing aggressive path pruning methods 
and importance heuristics [45] to trace only beams that represent (psychoacoustically) signi.cant reverberation 
paths between some source-receiver pair. Bi-directional beam trac­ing (computing kth-order re.ections 
by combining beams traced up to k/2 re.ections from both the source and the receiver positions [23, 24]) 
should also improve performance. We plan to experiment with these techniques and to incorporate moving 
sound sources into our system in the near future. 6.3 Simulation Veri.cation Veri.cation of our simulation 
results by comparison to measured data is an important topic for further discussion. Unlike sound rendering 
systems for animation in virtual environments [48, 53], we aim to simulate room impulse responses accurately 
enough to be used also for architectural and concert hall design applications. Although we do not present 
veri.cation results in this paper due to space limitations, it is useful to note that our current sys­tem 
computes (more ef.ciently) the same specular re.ection paths as the source image method, for which veri.cation 
results have been published [56]. We are currently making impulse response measurements for veri.cation 
of our simulations in the Varechoic Chamber, a specially constructed acoustics facility that allows one 
to vary the reverberation time by more than a factor of 10 by adjust­ing the acoustic re.ection coef.cient 
of 384 individually computer controllable acoustic panels [58]. 6.4 Psychoacoustics Experiments Perhaps 
the most interesting direction of future work is to investigate the possible applications of interactive 
acoustic modeling. What can we do with interactive manipulation of acoustic model parameters that would 
be dif.cult to do otherwise? As a .rst application, we hope to build a system that uses our interactive 
acoustic simulations to investigate the psychoacoustic effects of varying different acoustic modeling 
parameters. Our system will allow a user to interactively change various acoustics parameters with real-time 
auralization and visualization feedback. With this interactive simulation system, it may be possible 
to ad­dress psychoacoustic questions, such as how many re.ections are psychoacoustically important to 
model?, or which surface re.ec­tion model provides a psychoacoustically better approximation?" Moreover, 
we hope to investigate the interaction of visual and aural cues on spatial perception. We believe that 
the answers to such questions are of critical importance to future designers of 3D virtual environment 
systems.  7 Conclusion We have described a system that uses beam tracing data structures and algorithms 
to compute high-order specular re.ection and trans­mission paths from static sources to a moving receiver 
at interactive rates for real-time auralization in large virtual environments. As compared to previous 
acoustic modeling approaches, our beam tracing method takes unique advantage of precomputation and convexity. 
Precomputation is used twice, once to encode in the spatial subdivision data structure a depth-ordered 
sequence of (cell boundary) polygons to be considered during any traversal of space, and once to encode 
in the beam tree data structure the region of space reachable from a static source by sequences of specular 
re­.ections and transmissions at cell boundaries. We use the convexity of the beams, cell regions, and 
cell boundary polygons to enable ef­.cient and robust computation of beam-polygon and beam-receiver intersections. 
As a result, our method is uniquely able to: 1) sup­port evaluation of reverberation paths at interactive 
rates, 2) scale to compute high-order re.ections in large environments, and 3) extend to compute paths 
of diffraction and diffuse re.ection. Our virtual environment system integrates real-time auraliza­tion 
with visualization of large virtual environments. Based on our initial experiences with this system, 
we believe that accurately spatialized audio is a very important cue for experiencing and nav­igating 
virtual environments. We are continuing this research in order to further investigate the perceptual 
interaction of visual and acoustics effects and to better realize the opportunities possible with interactive 
acoustic modeling. Acknowledgements The authors thank Arun C. Surendran and Michael Gatlin for their 
valuable discussions and contributions to the project. We are also grateful to Bob Kubli who helped record 
audio for the accompanying video tape. References [1] Ahnert, Wolfgang. EARS Auralization Software. 
J. Audio Eng. Soc., 41, 11, November,1993, 894-904. [2] Allen, J.B., Berkley, D.A. Image Method for Ef.ciently 
Simulating Small-Room Acoustics, J. Acoust. Soc. Am., 65, 4, April, 1979, 943 951. [3] Amanatides, J. 
Ray Tracing with Cones. Computer Graphics (SIGGRAPH 84). 18, 3, 129-135. [4] Arvo, J. and D. Kirk. A 
Survey of Ray Tracing Acceleration Techniques. in An Introduction to Ray Tracing, Andrew Glassner editor, 
Academic Press, San Diego, CA, 1989. [5] Baumgart, Bruce G. Winged Edge Polyhedron Representation. Ph.D. 
Thesis, Computer Science Department, Stanford University, 1972. [6] Borish, Jeffrey. Extension of the 
Image Model to Arbitrary Polyhedra.J. Acoust. Soc. Am., 75, 6, June, 1984, 1827-1836. [7] Bose Modeler, 
Bose Corporation, Framingham, MA. http://www.bose.com. [8] CATT-Acoustic, CATT, Gothenburg, Sweden, http://www.netg.se/ 
catt. [9] Chuang, J.H. and S.A. Cheng. Computing caustic effects by backward beam tracing. The Visual 
Computer, 11, 3, 1995, 156 166. [10] Cook,Robert,L.,ThomasPorter,andLorenCarpenter.DistributedRayTracing. 
Computer Graphics (SIGGRAPH 84). 18, 3, 137-146. [11] D Antonio, Peter, and John Konnert. The Directional 
Scattering Coef.cient: Experimental Determination. J, Audio Eng. Soc., 40, 12, December, 1992, 997­1017. 
[12] Dadoun, N., D.G. Kirkpatrick, and J.P. Walsh. Hierarchical Approaches to Hidden Surface Intersection 
Testing. Graphics Interface 82, Toronto, Canada, May, 1982, 49-56. [13] Dadoun, N., D.G. Kirkpatrick, 
and J.P. Walsh. The Geometry of Beam Tracing. Proceedings of the Symposium on Computational Geometry, 
Baltimore, June, 1985, 55-71. [14] Durlach, N.I., R.W. Pew, W.A. Aviles, P.A. DiZio, and D.L. Zeltzer. 
Virtual Environment Technology for Training (VETT). Report No. 7661, Bolt, Beranek, and Newmann, Cambridge, 
MA, 1992. [15] Durlach, N.I, and A.S. Mavor, editors, Virtual Reality Scienti.c and Technolog­ical Challenges, 
National Research Council Report, National Academy Press, Washington, D.C., 1995. [16] Fortune, Steve. 
Algorithms for Prediction of Indoor Radio Propagation. Techni­cal Memorandum, Document #11274-960117-03TM, 
Bell Laboratories, 1996. A partial version of this paper appears in Applied Computational Geometry, Towards 
Geometric Engineering, proceedings of the FCRC 96 Workshop in conjunction with WACG 96, Philadelphia, 
PA, May, 1996, 157-166. [17] Foster, S.H., E.M. Wenzel, and R.M. Taylor. Real-time Synthesis of Complex 
Acoustic Environments. Proceedings of the IEEE Workshop on Applications of Signal Processing to Audio 
and Acoustics, New Paltz, NY, 1991. [18] Fuchs, H. Kedem, Z., and Naylor, B. On Visible Surface Generation 
by a Priori Tree Structures. Computer Graphics (Proc. SIGGRAPH 80), 124-133. [19] Fujomoto, Akira. Turbo 
Beam Tracing -A Physically Accurate Lighting Simu­lation Environment. Knowledge Based Image Computing 
Systems, May, 1988, 1-5. [20] Ghazanfarpour, G. and J. Marc Hasenfratz. A Beam Tracing with Precise An­tialiasing 
for Polyhedral Scenes. Computer &#38; Graphics, 22, 1, 1998. [21] Goral, Cindy M., Kenneth E. Torrance, 
Donald P. Greenberg, and Bennett Battaile. Modelingthe Interactionof Light Between Diffuse Surfaces. 
Computer Graphics (Proc. SIGGRAPH 84), 18, 3, July, 1984, 213-222. [22] Haines, Eric A. Beams O Light: 
Confessions of a Hacker. Frontiers in Render­ing Course Notes, SIGGRAPH 91, 1991. [23] Heckbert, Paul, 
and Pat Hanrahan. Beam Tracing Polygonal Objects. Computer Graphics (SIGGRAPH 84), 18, 3, 119-127. [24] 
Heckbert, Paul. Adaptive Radiosity Textures for Bidirectional Ray Tracing. Computer Graphics (SIGGRAPH 
90), 24, 4, 145-154. [25] Heinz, R. Binaural Room Simulation Based on an Image Source Model with Addition 
of Statistical Methods to Include the Diffuse Sound Scattering of Walls andtoPredictthe ReverberantTail 
J. Applied Acoustics,38, 2-4,1993,145-160. [26] Hodgson, M. Evidence of Diffuse Surface Re.ections in 
Rooms. J. Acoust. Soc. Am., 89, 1991, 765-771. [27] Jones, C.B. A New Approach to the Hidden Line Problem. 
The Computer Journal, 14, 3 (August 1971), 232-237. [28] Kajiya, James T. The Rendering Equation. Computer 
Graphics (SIGGRAPH 86), 143-150. [29] Keller, Joseph B. Geometrical Theory of Diffraction. Journal of 
the Optical Society of America, 52, 2, February, 1962, 116-130. [30] Kleiner,Mendel,Bengt-IngeDalenback,andPeterSvensson.Auralization 
An Overview. J. Audio Eng. Soc., 41, 11, November,1993,861-875. [31] Kreuzgruber, P., P. Unterberger, 
and R. Gahleitner. A Ray Splitting Model for Indoor Radio Propagation Associated with Complex Geometries. 
Proceedings of the 1993 43rd IEEE Vehicular Technology Conference, 1993, 227-230. [32] Kristiansen, U.R., 
A. Krokstad, and T. Follestad. Extending the Image Method to Higher-Order Re.ections. J. Applied Acoustics, 
38, 2-4, 1993, 195-206. [33] Krockstadt, U.R. Calculating the Acoustical Room Response by the Use of 
a Ray Tracing Technique, J. Sound and Vibrations, 8, 18, 1968. [34] Kuttruff, Heinrich Room Acoustics, 
3rd Edition, Elsevier Science, London, England, 1991. [35] Lehnert, Hilmar. Systematic Errors of the 
Ray-Tracing Algorithm. J. Applied Acoustics, 38, 2-4, 1993, 207-221. [36] Lewers,T.ACombinedBeamTracingandRadiantExchangeComputerModel 
of Room Acoustics. J. Applied Acoustics, 38, 2-4, 1993, 161-178. [37] McGrath, David, and Andrew Reilly. 
Convolution Processing for Realistic Re­verberation. The 98th Convention of the Audio Engineering Society, 
February, 1995. [38] Monks, Michael, Byong Mok Oh, and Julie Dorsey. Acoustic Simulation and Visualization 
using a New Uni.ed Beam Tracing and Image Source Approach. Meeting of the Audio Engineering Society, 
November,1996. [39] Moore, G.R. An Approach to the Analysis of Sound in Auditoria. Ph.D. Thesis, Cambridge, 
UK, 1984. [40] Naylor, G.M. ODEON -Another Hybrid Room Acoustical Model. J. Applied Acoustics, 38, 2-4, 
1993, 131-144. [41] Naylor, B.F. Constructing Good Partitioning Trees. Graphics Interface 93, Toronto, 
CA, May, 1993. [42] Rajkumar, A., B.F. Naylor, and L. Rogers. Predicting RF Coverage in Large Environments 
using Ray-Beam Tracing and Partitioning Tree Represented Ge­ometry. Wireless Networks, 1995. [43] Rindel, 
J.H. Modelling the Angle-Dependent Pressure Re.ection Factor. J. Applied Acoustics, 38, 2-4, 1993, 223-234. 
[44] Sequin,Carlo,andEliotSmyrl.ParameterizedRayTracing.Computer Graphics (SIGGRAPH 89), 23, 3, 307-314. 
[45] Smits, Brian, James R. Arvo, and David H. Salesin. An Importance-Driven Radiosity Algorithm. Computer 
Graphics (SIGGRAPH 92), 26, 2, 273-282. [46] Stephenson, U., and U. Kristiansen. Pyramidal Beam Tracing 
and Time De­pendent Radiosity. Fifteenth International Congress on Acoustics, Tapir, June, 1995, 657-660. 
[47] Stettner, Adam, and Donald P. Greenberg. Computer Graphics Visualization for Acoustic Simulation. 
Computer Graphics (SIGGRAPH 89), 23, 3, 195-206. [48] Takala, Tapio, and James Hahn. Sound Rendering. 
Computer Graphics (SIG-GRAPH 92), 26, 2, 211-220. [49] Teller, Seth J., and Carlo H. S´equin, Visibility 
Preprocessing for Interactive Walkthroughs. Computer Graphics (SIGGRAPH 91), 25, 4, 61-69. [50] Teller, 
Seth J. Computing the Antiumbra Cast by an Area Light Source. Com­puter Graphics (Proc. SIGGRAPH 92), 
26, 2 (August 1992), 139-148. [51] Teller, Seth J. Visibility Computations in Densely Occluded Polyhedral 
En­vironments. Ph.D. thesis, Computer Science Division (EECS), University of California, Berkeley, 1992. 
Also available as UC Berkeley technical report UCB/CSD-92-708. [52] Tsingos, Nicolas, and Jean-Dominique 
Gascuel. A General Model for Simula­tion of Room Acoustics Based On Hierarchical Radiosity. Technical 
Sketches, SIGGRAPH 97 Visual Proceedings, 1997. [53] Tsingos,Nicolas,andJean-DominiqueGascuel.SoundtracksforComputerAni­mation: 
SoundRenderinginDynamicEnvironmentswithOcclusions. Graphics Interface 97, Kelowna, May 21-23, 1997, 9-16. 
[54] Veach, Eric, and Leonidas J. Guibas. Metropolis Light Transport. Computer Graphics (SIGGRAPH 97), 
65-76. [55] Vian, J.P. and D. van Maercke. Calculation of the Room Response Using a Ray Tracing Method. 
Proceedings of the ICA Symposium on Acoustics and Theater Planning for the Performing Arts, Vancouver, 
CA, 1986, 74-78. [56] Vorlander, M. International Round Robin on Room Acoustical Computer Simu­lations. 
Proceedingsof the 15th International Congressof Acoustics,Trondheim, Norway, June, 1995. [57] Walsh, 
John P., and Norm Dadoun. What Are We Waiting for? The Development of Godot,II. presented at the 103rd 
Meeting of the Acoustical Society of America, Chicago, April, 1982. [58] Ward, William C., Gary, W. Elko, 
Robert A. Kubli, and W. Craig McDougald. The New Varechoic chamber at AT&#38;T Bell Labs. Proceeding 
of Wallace Clement Sabine Centennial Symposium, AcousticalSocietyofAmerica,New York,June, 1994, 343-346. 
[59] Watt, Mark. Light-Water Interaction Using Backward Beam Tracing. Computer Graphics (SIGGRAPH 90), 
24, 377-385. [60] Weiler, K. and P. Atherton. Hidden Surface Removal Using Polygon Area Sort­ing. Computer 
Graphics (SIGGRAPH 77), 11, 2, 214-222. [61] Whitted, Turner. An Improved Illumination Model for Shaded 
Display. Com­munications of the ACM, 23, 6, June, 1980, 343-349.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280820</article_id>
		<sort_key>33</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Retargetting motion to new characters]]></title>
		<page_from>33</page_from>
		<page_to>42</page_to>
		<doi_number>10.1145/280814.280820</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280820</url>
		<keywords>
			<kw><![CDATA[motion capture]]></kw>
			<kw><![CDATA[motion editing]]></kw>
			<kw><![CDATA[motion signal-processing]]></kw>
			<kw><![CDATA[spacetime constraints]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.6</cat_node>
				<descriptor>Nonlinear programming</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.10</cat_node>
				<descriptor>Morphological</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Motion</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010380</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003716</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Mathematical optimization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010242</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Shape representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10003716</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Mathematical optimization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39038559</person_id>
				<author_profile_id><![CDATA[81100342764]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gleicher]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Autodesk Vision Technology Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Richard Barrett, Michael Berry, Tony Chan, James Demmel, June Donato, Jack Dongarra, Victor Eikhout, Roldan Pozo, Charles Romine, and Henk van der Vorst. Templates for the solution of linear systems: Building Blocks for Iterative Methods. SIAM, 1994.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218421</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Armin Bruderlin and Lance Williams. Motion signal processing. In Robert Cook, editor, SIGGRAPH 95 Conference Proceedings, Annual Conference Series, pages 97-104, August 1995.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134083</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Michael F. Cohen. Interactive spacetime control for animation. In Edwin E. Catmull, editor, Computer Graphics (SIGGRAPH '92 Proceedings), volume 26, pages 293-302, July 1992.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>39857</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Roger Fletcher. Practical Methods of Optimization. John Wiley and Sons, 1987.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Phillip Gill, Walter Murray, and Margaret Wright. Practical Optimization. Academic Press, New York, NY, 1981.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>253321</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Michael Gleicher. Motion editing with spacetime constraints. In Michael Cohen and David Zeltzer, editors, Proceedings 1997 Symposium on Interactive 3D Graphics, pages 139-148, apr 1997.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Michael Gleicher and Peter Litwinowicz. Constraint-based motion adaptation. Journal of Visualization and Computer Animation, to appear.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Michael Gleicher and Andrew Witkin. Supporting numerical computations in interactive contexts. In Tom Calvert, editor, Proceedings of Graphics Interface '93, pages 138-145, May 1993.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258822</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Jessica Hodgins and Nancy Pollard. Adapting simulated behaviors for new characters. In Turner Whitted, editor, SIGGRAPH 97 Conference Proceedings, pages 153-162, August 1997.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Masao Iri. History of automatic differentiation and rounding error estimation. In Andreas Griewank and George Corliss, editors, Automatic Differentiation of Algorithms: Theory, Implementation and Application, pages 3-16. SIAM, January 1991.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Kinetix Division of Autodesk Inc. Character studio. Computer Program, 1997.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122731</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Peter C. Litwinowicz. Inkwell: A 2 g-D animation system. In Thomas W. Sederberg, editor, Computer Graphics (SIGGRAPH '91 Proceedings), volume 25, pages 113-122, July 1991.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192169</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Zicheng Liu, Steven J. Gortler, and Michael F. Cohen. Hierarchical spacetime control. In Andrew Glassner, editor, SIGGRAPH 94 Conference Proceedings, Annual Conference Series, pages 35-42, July 1994.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>6232</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Michael Mortenson. Geometric Modelling. John Wiley &amp; Sons, second edition, 1997.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166160</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[J. Thomas Ngo and Joe Marks. Spacetime constraints revisited. In James Kajiya, editor, Computer Graphics (SIGGRAPH '93 Proceedings), volume 27, pages 343-350, August 1993.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>355989</ref_obj_id>
				<ref_obj_pid>355984</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Christopher Paige and Michael Saunders. LSQR: an algorithm for sparse linear equations and sparse least squares. ACM Transactions on Mathematical Software, 8(1):43-71, March 1982.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>614291</ref_obj_id>
				<ref_obj_pid>614257</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Ken Perlin. Real time responsive animation with personality. IEEE Transactions on Visualization and Computer Graphics, 1(1):5-15, March 1995. ISSN 1077- 2626.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[William Press, Brian Flannery, Saul Teukolsky, and William Vetterling. Numerical Recipes in C. Cambridge University Press, Cambridge, England, 1986.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237229</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Charles F. Rose, Brian Guenter, Bobby Bodenheimer, and Michael F. Cohen. Efficient generation of motion transitions using spacetime constraints. In Holly Rushmeier, editor, SIGGRAPH 96 Conference Proceedings, Annual Conference Series, pages 147-154, August 1996.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Steven Seitz and Chuck Dyer. Analogically-guided animation. Masters Project Report, Department of Computer Science, University of Wisconsin, May 1993. unpublished.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218419</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Munetoshi Unuma, Ken Anjyo, and Ryozo Takeuchi. Fourier principles for emotion-based human figure animation. In Robert Cook, editor, SIGGRAPH 95 Conference Proceedings, Annual Conference Series, pages 91-96. ACM SIG- GRAPH, Addison Wesley, August 1995.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378507</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Andrew Witkin and Michael Kass. Spacetime constraints. In John Dill, editor, Computer Graphics (SIGGRAPH '88 Proceedings), volume 22, pages 159-168, August 1988.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218422</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Andrew Witkin and Zoran Popovid. Motion warping. In Robert Cook, editor, SIGGRAPH 95 Conference Proceedings, Annual Conference Series, pages 105- 108, August 1995.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Supplemental materials for this paper are available in the papers/gleicher directory. Retargetting Motion 
to New Characters * Michael Gleicher Autodesk Vision Technology Center Copyright &#38;#169;1998 by 
the Association for Computing Machinery, Inc. Permission to make digital or hard copies of part or all 
of this work for personal or classroom use is granted without fee provided that copies are not made or 
distributed for profit or commercial advantage and that copies bear this notice and the full citation 
on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting 
with credit is permitted. To copy otherwise, to republish, to post on servers or to redistribute to lists, 
requires specific permission and/or a fee. Abstract In this paper, we present a technique for retargetting 
motion: the problem of adapting an animated motion from one character to an­other. Our focus is on adapting 
the motion of one articulated .g­ure to another .gure with identical structure but different segment 
lengths, although we use this as a step when considering less simi­lar characters. Our method creates 
adaptations that preserve desir­able qualities of the original motion. We identify speci.c features of 
the motion as constraints that must be maintained. A spacetime constraints solver computes an adapted 
motion that re-establishes these constraints while preserving the frequency characteristics of the original 
signal. We demonstrate our approach on motion cap­ture data. CR Categories and Subject Descriptors: I.3.7 
[Computer Graphics]: Three Dimen­sional Graphics and Realism -Animation Additional Keywords: motion editing, 
motion signal-processing, spacetime con­straints, motion capture. 1 Introduction In this paper, we present 
techniques for retargetting motion: the problem of adapting an animated motion from one character to 
an­other. Our goal is to re-use motions created for one character on other characters, independently 
of how that motion was created. We aim to preserve as many of the desirable properties of the orig­inal 
motion as possible. That is, if we begin with the motion of a tall adult person, we expect to end up 
with a motion of a small child walking like an adult, or a crocodile swing dancing as if it were an adult 
human. Admittedly, this faithfulness to the original motion is not always artistically desirable. However, 
we prefer to relegate the dif.cult creative decisions (How do crocodiles dance?) to the user s selection 
of an initial motion. Our focus is on applying motion created for one articulated .g­ure to another .gure 
with identical structure (connectivity of limbs, types of joints, number of degrees of freedom) but different 
seg­ment lengths. Even when two articulated .gures share structure, the motion of one may not trivially 
apply to the other and therefore require adaptation. Good adaptations preserve important aspects of the 
motion by altering less important ones: in a walking motion, it is important that the feet touch the 
.oor, not that the pelvis is 32 inches above the .oor as in the original. The important properties of 
* Autodesk VTC, 2465 Latham St, Mountain View, CA 94040. gleicher@cs.cmu.edu http://www.gleicher.com/mike 
 , a given motion may not always be simple; realism, grace, like in Singing in the Rain ness, or other 
high-level properties may be desirable to preserve during adaptation. In practice, we are limited by 
our ability to de.ne high-level qualities of the motion mathemat­ically, by our ability to compute adaptations 
ef.ciently when the metrics become complex, and by the amount of effort we wish to expend in identifying 
(or having the user identify) these properties. These issues motivate a more pragmatic approach to retargetting. 
This paper presents a method for .nding the adaptations needed to retarget motions from one articulated 
.gure to another. We ac­complish this by requiring the basic features of the motion for example that 
the feet touch the .oor when walking to be identi­.ed as constraints. If the constraints are violated 
when the motion is applied to a different .gure, we .nd an adaptation to the mo­tion that re-establishes 
the constraints in a manner that .ts with the motion. Our premise is that by maintaining the basic features 
and avoiding uncharacteristic (in a basic signal-processing sense) changes, we .nd adaptations that generally 
preserve the desirable characteristics of a motion, without explicitly modeling them. The core of our 
retargetting method is a numerical solver that computes an adaptation to the original motion. The adaptation 
re­establishes the constraints while attempting to avoid adding any un­desirable artifacts. Our solver 
is a spacetime constraints method that considers the entire motion simultaneously, computing whole motions, 
not just individual frames. To preserve the qualities of the original motion, we minimize the magnitude 
of the changes and restrict their frequency content. After a review of previous work, we introduce our 
method in Section 3, and summarize the technique in Section 4. Section 5 de­scribes how the method can 
be applied to creating motions when the character is changing (morphing). In section 6, we discuss issues 
in solving the non-linear constraint problems. We provide a gallery of examples in Section 7 and consider 
the problem of retargetting a motion to a character with different structure in Section 8. 1.1 An Example 
We motivate our approach with an example: retargetting motion capture data of an actress walking up to, 
picking up, and carrying away a box. During pre-processing, we augment the motion data by specifying 
constraints that are essential to the action: the hands must grab the box in the middle frame, the hands 
must remain the correct distance apart while carrying the box, and the feet must be planted and not skid 
when they are on the ground. Without adaptation, our motion capture data does not apply to .gures of 
different sizes or proportions than our actress: the result­ing motions have the feet skating and the 
hands failing to reach the object. Our method enables us to re-use this data on .gures of vary­ing proportions, 
as shown in Figure 1. The method computes an adapted motion for each new character using the approach 
detailed in Section 3. Because the technique looks at the entire motion, it can make adjustments based 
on all the requirements. For example, it adjusts the footplant positions so that the characters reach 
the box using natural footstep sizes. Our approach makes many sacri.ces to achieve practicality. We tell 
our solver little about the original motion or general motion properties, and our choice of the mathematical 
problem is heavily  Figure 1: Differently sized characters pick up an object. Their positions are determined 
by the position of the object. The left shows the original actress. The center shows a .gure 60% as large. 
The right shows a .gure with extremely short legs and arms and an extremely long body. The yellow cones 
represent footplant positions. in.uenced by what can be solved ef.ciently. We sometimes pay for these 
sacri.ces in the quality of the resulting motions. For ex­ample, because our system did not consider 
gravity or posture we get an unrealistically unbalanced result in the right frame of Figure 1. The payoff 
is that our approach provides a practical solution to the retargetting problem and a framework in which 
to employ more sophisticated constraints, like balance, in the future.  2 Previous Work Few techniques 
speci.cally address the retargetting problem. Gen­erally, users are forced to adapt motions using the 
same tools that are used for motion creation: each frame or key must be manually tweaked. Some commercial 
systems, such as Kinetix s Character Studio [11], are beginning to support retargetting. For example, 
Character Studio can adjust keyframes to maintain footplants and balance when a motion is re-applied 
to a new character. Hodgins and Pollard [9] address a variant of the motion re-use problem, adjusting 
parameters of a physical simulation to adapt a controller for use with a new character or a character 
that is chang­ing. In general, procedural-and simulation-based approaches to animation offer representations 
independent of the character and therefore may be used generate new motions for new characters. Many 
of the procedural and simulation controllers are able to ad­just to different characters easily. Such 
methods do not address the retargetting problem: they can generate new motions for new characters but 
not reuse existing motions. Re-generation of motion risks losing qualities in the original. Our goal 
is to create methods that adapt existing motions obtained from a variety of sources, in­cluding motion 
capture and keyframing as well as simulation and procedural generation. Recently, there is an interest 
in tools that allow motion to be al­tered in ways that are independent of how it was created. At their 
core, these tools treat animated motions as time-varying signals and apply signal processing techniques 
to these signals. Litwinowicz s Inkwell system [12] .rst demonstrated the utility of applying sig­nal 
processing methods to animation data. Perlin [17] showed how existing motions could be blended together, 
and how the addition of noise to a motion could be used to transform it. Bruderlin and Williams [2] showed 
that many signal processing techniques could be applied to motion. Simultaneously, other authors showed 
some of these methods in greater detail. Unuma et al. [21] showed how band-pass .ltering methods could 
adjust emotional content, and Witkin and Popovi´ c [23] introduced motion-warping, a variant of Bruderlin 
and Williams motion displacement mapping. 2.1 Spacetime Constraints The spacetime constraint approach, 
introduced by Witkin and Kass [22], poses the motion synthesis problem as a constrained optimiza­tion: 
what is the best motion that meets a speci.ed set of con­straints? Cohen [3] extended this with a more 
complete system that allowed the user to focus the solution process. Recently, Rose et al. [19] applied 
the approach to the problem of generating tran­sitions between motion segments, and Gleicher and Litwinowicz 
[7] showed how the methods can be used for adjusting motions so that the characters have new goals. Gleicher 
[6] extends this work by simplifying the spacetime problem to achieve interactive perfor­mance for interactive 
editing. What differentiates spacetime from other constraint methods is that it poses a single large 
problem over a duration of motion, rather than on an individual frame. The original spacetime work, as 
well as most that followed, used spacetime to derive physically valid mo­tions: constraints enforced 
Newton s laws, and the objective func­tion minimized energy consumption. Previously, we [6] have sug­gested 
removing the physical constraints to achieve better perfor­mance and to apply the techniques to non-physical 
motions. Although Ngo and Marks[15] re-used the term spacetime con­straints to describe their work, their 
method belongs to a different family of approaches that generates control systems that create mo­tions, 
rather than generating the motions themselves. We prefer to reserve the term spacetime constraints for 
methods that compute speci.c motions.  3 An Approach to Retargetting In this section, we motivate and 
describe our approach to retarget­ting the motion between articulated .gures with identical structure 
but different segment lengths. We assume that the con.guration of an articulated .gure is speci.ed by 
a position for the root of the hierarchy and the angles of its joints. We will denote these con.g­urations 
as a vector that concatenates all of these parameters, often denoted by q,or by q t to refer to its value 
at time t.A motion is a vector-valued function that provides a con.guration given a time. While we often 
represent the initial motion as a dense array of sam­ples or as a set of key values that are interpolated, 
our methods are independent of how this motion is obtained. We refer to the retargetted motion as m t, 
() and often use the concept of a motion displacement which represents the difference between two motions, 
m()=m0 t d e.g. t t. ()+ () Because the target character has the same parameters as the orig­inal, reusing 
the original motion data will cause the new character to move its limbs as the original, but not necessarily 
lead to a de­sirable result as shown in the example of Figure 2. Because the length of the limbs are 
different, the parts of the new character do Figure 2: Left: Frames from a rotoscoped walking motion 
are shown. Right: Applying this motion to a character that is 60% of the size of the original yields 
a motion that skates along horizontally above the .oor. Figure 3: Adaptations are applied to the motion 
of Figure 2 to re-establish the con­straints. The .gure shows .ve frames before and after a heel strike, 
with the frame im­mediately before and after the heel strike darkened. A constraint on the heel s position 
applies on the frames after the strike. Left: inverse kinematics is applied to individ­ual frames, causing 
a noticeable discontinuity. Right: our approach re-establishes the constraints while maintaining the 
frequency characteristics of the original motion. not end up in the same place as in the original. Therefore, 
they may fail to interact correctly with other objects in the world or may move differently. In the example 
these problems appear as the feet not touching the .oor and skating horizontally when planted, as seen 
in Figure 2. The naive retargetting fails to preserve important properties of the initial motion. 3.1 
Inverse Kinematics The principal problem with the naively retargetted motion is that it violates some 
of the constraints that we expect in a satisfactory walking motion. For example, a walking motion requires 
charac­ter s feet to touch the .oor and to not skid during footplants. Retar­getting must re-establish 
these constraints. Inverse kinematics (IK) is a common technique for positioning end effectors of articulated 
.gures in individual frames of an anima­tion. An IK solver could be used to adjust the con.guration of 
the character to meet the constraints in each frame. Figure 3 shows the result of such a retargetting 
approach, re-establishing the planted foot positions. Because the IK solver considers each frame inde­pendently, 
it makes different alterations to each frame. This lack of consistency adds many undesirable artifacts 
to the motion. For example, because frame idoes not know that a foot will be planted in frame i+1,it 
cannot move towards this constraint, so that in frame i+1,the foot will snap to its new location. Even 
within a footplant, there is a lack of consistency: on each frame the solver will use a different combination 
of straightening the leg and lower­ing the pelvis. These artifacts appear as high frequency jerkiness, 
shown for the example in Figure 3. 3.2 Motion Frequency Response The problem with the IK solution is 
that we have added high fre­quencies to a primarily smooth motion. Extending the leg from bent /t5hof 
a second might be acceptable if this were a to straight in 11karate master s kick, but, this discontinuity 
is inappropriate in our walking motion. Generally, the high frequencies of a motion (or the lack thereof) 
are important, and therefore must not be disturbed. An adaptation that removed the snap from a karate 
kick might be just as inappropriate as adding the snap to our slow walking motion. The importance of 
preserving the high-frequency content of a motion (or the lack thereof) is an explanation for the success 
of motion-displacement mapping [2, 23] (also called motion-warping) techniques.1 The key spacing of the 
displacement curves restricts their frequency content such that the high frequencies of the motion are 
not disturbed. Changes should not necessarily be made at the lowest possible frequency. Consider retargetting 
a motion where a smaller character must grab an object in the middle frame, but there are no other constraints 
on the arm. To meet the constraint, the character must extend his arm in this one frame. This alteration 
can be made at any frequency: the single frame can be adjusted (e.g. the arm shoots out /t0hof a second), 
or the adjustment can be applied to the for the 13whole motion (e.g. the arm is extended while the character 
walks up to the object to pick it up). While the extreme high-frequencies of the former are undesirable, 
so are the extreme low frequencies of the latter (the added signal has only a DC component). A simple 
approach to avoiding the addition of high frequencies is to low-pass .lter the displacement signal generated 
by the inverse kinematics process. Unfortunately this change does not necessarily maintain the constraints 
that IK was used to achieve as shown in Figure 4. 3.3 Motivating Spacetime The failure of the per-frame 
approach to meet the needs of au­tomatic retargetting suggests that we require a constraint-based method 
that can take into consideration a span of the motion, e.g. spacetime constraints. The more global view 
of such a method al­lows it to consider relationships among multiple frames. Spacetime constraint s use 
of constrained optimization allows us to address both parts of the retargetting problem: establishing 
the constraints on the motion, while minimizing the changes our original motion. The spacetime constraints 
approach poses the retargetting prob­lem mathematically. We seek a motion mtthat, subject to () fmt =0and 
satisfying a set of constraints on the motion (()) fmt = (we divide the constraints as equality and inequal­ 
i( ()) 0 ity constraints for notational convenience), minimizes an objective function g()m.For retargetting, 
the objective compares the mo­tion with the original motion, m0 t (). By encoding the retargetting problem 
in this form, we can use numerical methods to solve the constrained optimization problem for our desired 
result. Because the spacetime approach looks at the entire motion, it can make choices based on other 
parts of the motion. For example, it can move footplants based on where the character needs to end up. 
Such look-ahead and -behind is not possible in approaches that consider each frame independently.  3.4 
Spacetime in Practice Ideally, the constrained optimization problem would fully encode our desires mathematically: 
there would be a single solution that was the desired motion. Realizing this ideal requires a rich set 
of constraints and objectives. For example, we could .nd con­straints that enforce the laws of physics, 
biomechanical limitations 1Albeit, one that is not emphasized in [2] but is a motivation for [23]. (A) 
(B) (C) (D) Figure 4: Ten frames of a ladder-climbing motion are shown. In the last 5 frames (shown 
darker), the hand is constrained to be attached to the handhold. (A) shows the original motion capture 
data. (B) shows the motion adapted to a smaller character by applying Inverse kinematics (IK) to each 
frame, causing a noticeable snap. (C) shows low-pass .ltering of the results of the IK process. This 
removes the snaps at the expense of violating the constraints. (D) shows our approach applied to the 
example. due to strength, and proper ballet form. We could de.ne objective functions that measure visual 
properties such as grace, Charlie Chaplin ness, and like Joe did it yesterday ness. We would aim to 
maintain the constraints that were satis.ed in the original motion while minimizing the amount of change 
in the important properties. There are central dif.culties in realizing the spacetime ideal for retargetting: 
.rst, some properties are dif.cult to encode mathemat­ically as constraints or objectives either because 
the forms of the equations are complex or because they elude a mathematical en­coding; second, we may 
not know all the properties required, such as the mass distribution of an imaginary character or the 
physical laws of an imaginary world; third, we must decide which properties are important in a given 
setting; fourth, many of the properties and constraints may be speci.c to a small set of examples, and 
therefore not worth the effort to de.ne. Even if we encoded the desired animation completely in a con­strained 
optimization, we still need to .nd the solution to these problems. Generally, richer sets of constraints 
and objective func­tions are likely to lead to more dif.cult problems to solve. The challenges of solution 
lead us to take a pragmatic view in de.ning spacetime problems. An extreme case of this pragmatism is 
our work on spacetime editing [6] where many sacri.ces were made in order to achieve interactive performance. 
Our approach to spacetime for retargetting is motivated by the pragmatic issues of de.ning, specifying, 
and solving constraints and objectives. We use constraints to de.ne speci.c features of the motion that 
must be maintained and use the objective function to limit certain generally unacceptable types of changes. 
Besides the constraints and objectives, we have two more pragmatic tools that we can use to help de.ne 
a spacetime problem with the desired so­lution: the representation used for the motion and the starting 
point for the constrained optimization. We will discuss these four in more detail in the following sections. 
 3.5 Sources of Constraints Constraints are the primary tool used to identify features of the orig­inal 
motion that must be present in the retargetted result. In general, our constraints will either come from 
restrictions on the character (e.g. the elbows do not bend backwards), the environment (noth­ing should 
be below the .oor), or the motion (the character must pick up the box in frame 50). Speci.cation of these 
constraints typically involves only a small amount of work in comparison with the tasks of creating the 
characters and motions, especially with semi-automatic detection (for example .nding footplants), graphi­cal 
speci.cation, and generic constraints (e.g. we use the same joint limits for most humanoid characters). 
Constraints are generally de­.ned once for each motion, and this one set of constraints is used for any 
retargettings (or editing, using the techniques of [6]) done with the motion. Even with these tools, 
augmenting our characters and motions with constraints does require some additional work. However, we 
feel this incremental effort is worthwhile because of the potential for reuse afforded by augmentation. 
Mathematically, constraints are differentiable functions of the parameters of the character. Although 
it is not required by the methods, our implementation always places constraints on con.g­urations at 
particular instants of time. Variational constraints, that is constraints that are to hold over a range 
of the motion curves, are approximated by sampling. Therefore, constraints are generally (q ti ) . c, 
. {== , =} c is a constant. written as f where is , and Some constraints consider two instants in time, 
and therefore have tt ij the form f(qq, ) . c. In our system, the user never needs to see an equation: 
the sys­tem includes a variety of pre-de.ned constraints that can be applied to a motion through a graphical 
user interface or via a scripting lan­guage. We have emphasized .nding (and using) constraints that we 
believe are applicable over a wide range of motions. Some of these include: 1. a parameter s value is 
in a range (useful for joint limits); 2. a point on the character (such as an end-effector) is in a 
spe­ci.c location (useful for footplants or grabbing an object); 3. a point on the character is in a 
certain region (for example, above the .oor); 4. a point on the character is in the same place at two 
different times (useful to prevent skidding), although this position is unspeci.ed so that it can be 
adjusted; 5. a point on the character is following the path of another point; 6. two points are a speci.ed 
distance apart (useful for when a character is carrying an object of a .xed size); 7. the vector between 
two points has a speci.ed orientation.  The architecture of our system is designed to minimize the effort 
required to add new types of constraints, although this does require programming and must be done at 
compile-time. In developing a new type of constraint, it is important to make restrictions in ways that 
are invariant of other aspects of the motion. For example, if one de.nes a footplant by the positions 
of the heel and toe strikes, the constraint cannot be satis.ed if the foot size is changed. Similarly, 
we often do not care where a footplant is, providing that is is on the .oor and that the foot does not 
skate while planted. For the examples in this paper, we will distinguish between footplant constraints 
that maintain the position on the .oor and those that only restrict height and skating. When the solver 
is permitted to move footplants, the resulting motion may cover a different distance, e.g. if the footsteps 
of a walk are made smaller, the character will travel a shorter distance since the system does not generate 
new footsteps. 3.6 Objective Functions Since there are typically many possible motions that satisfy 
the con­straints, we use an objective function to select the best choice. For retargetting, a simple 
objective is minimize the amount of notice­able change. This does not necessarily lead to a simple, generic 
manifestation: consider a ballet motion where a very slight bend of the knee might be a very noticeable 
deviation from the other­wise perfect form of the original with its straight leg. However, our strategy 
is to use constraints to prevent speci.c changes that are unwanted, and use the objective function to 
avoid undesirable frequency content and unnecessary large alterations, as discussed in Section 3.2. We 
avoid designing objective functions tuned to speci.c high-level goals. The most basic comparative objective 
function would be to com­pare the values of the parameters, matching pose in parameter space. For example, 
( )= m t - m0()) = d t g m ( () t 2 ()2 , (1) tt minimizes the magnitude of signal differences in the 
motions over time. This objective is similar to performing per-frame inverse kinematics as it provides 
no coupling between constraints at differ­ent times. The minimum magnitude solution effectively maximizes 
high frequency content. Intuitively, it prefers not to waste change preparing to meet goals at other 
times. Other frequency criteria can be implemented with an objective function that minimizes the output 
of a .lter that selects undesirable frequencies. In practice, we .nd that pragmatic concerns outweigh 
most other choices in the design of an objective function. For the experiments described in this paper, 
we use the objective function to minimize the magnitude of the changes, approximating Equation 1. Meth­ods 
described in the next section restrict high frequency content of the changes. This tactic affords the 
use of more ef.cient solving techniques (as we will describe in Section 6). 3.7 Representation Another 
issue in a spacetime approach is how to represent the mo­tions so that the optimization problems can 
be solved effectively. Liu et al. [13] .rst made use of a carefully selected representation by using 
wavelets to speed computations. Gleicher and Litwinow­icz [7] introduced the use of motion-displacement 
maps as a repre­sentation for spacetime problems where the objective function re­lated two motions. This 
approach de.nes m()=m0 t d tt ()+ () ()and uses the solver to .nd d t . The approach has a number of 
advantages. First, it decouples the solution from the form of the initial motion, providing generality. 
Secondly, it simpli.es placing constraints and objectives on the changes. Third, it allows a rep­resentation 
for d t ()to be chosen that includes constraints on the changes so they do not need to be expressed as 
explicit functions. To constrain the displacement signal not to include high frequen­cies, we use a representation 
for it that cannot represent the high frequencies: speci.cally, cubic B-splines [14] with control point 
spacing determined by the desired frequency limits. The control points of the displacement curve need 
not be uniformly spaced: we can place controls closer together for portions of the motion where higher 
frequencies are acceptable. Similarly, we do not need to use the same key spacing for all parameters, 
for example, if a chef is chopping, we might allow high frequencies in the motion of his arm (to accommodate 
the abrupt motions of the knife), and only permit smoother changes to the rest of his body. The spacing 
of B-spline control points allows us to determine the frequency response of our adaptations, although 
we do not have the .ne control afforded by carefully crafted .lters placed in an objec­tive function. 
We must determine how to place the control points to achieve the desired effect. For our experiments, 
we have limited our choices to using the uniformly spaced control points on all pa­rameters of a motion. 
2 For the examples in this paper, we further restrict ourselves to control points spaced every 2, 4 or 
8 frames. We have developed a simple heuristic method for determining which of these to apply: we compute 
a bandpass decomposition of the orig­inal motion (as described in [2]) and choose the key spacing that 
coincides with the lowest, that is highest-frequency, level of the pyramid whose energy contribution 
exceeds a threshold. While this simple heuristic has resulted in the correct recommendation for al­most 
all of our examples, the speed of our solver makes it practical to produce all three adaptations and 
to select the one that gives the most appealing result. With the constraints imposed by the restricted 
representation, there may not be a solution to the constraints. In such cases, there is a .tting problem: 
.nd the frequency-limited signal that comes closest to satisfying the constraints (where the constraints 
are the explicit equations from Section 3.5). In such a scheme, the nature of the mathematical problem 
is .ipped: our constraint is the fre­quency response, and our optimization objective attempts to mini­mize 
the residual of the constraints. We use a least-squares metric for the residual which enables simpler 
solution methods, as we will discuss in Section 6.  3.8 Starting Points Cohen [3] pointed out the importance 
of having good starting points for spacetime problems. Seitz and Dyer [20] observed the utility of a 
previously captured motion as a starting point for speeding their numerical solutions. With our retargetting 
approach, the ini­tial estimate of the solution is even more critical because our simple objective function 
explicitly de.nes the result in terms of the initial estimate. To improve the quality of our results, 
we must apply some simple transformations to the original motion so it better estimates the desired result. 
The process described in this section is summa­rized in Figure 5. Simply re-using the initial motion 
is possible because our .gures share the same parameters. For articulated .gures, most of the pa­rameters 
are angles and are independent of the scaling of the limbs: the angular value for a straight leg is the 
same, no matter how long the thigh and calf are. However, the positional offset of the root of the hierarchy 
is not scale-independent. The translation is a distance 2This was problematic only for the example of 
Figure 1 where the foot­steps have different frequency content than the grabbing motion. The arti­facts 
of this problem are subtle. A B C  Figure 5: A: An abstracted aerial view of a character walking 
up to, picking up, and carrying away an object. B: When the motion is scaled about the origin (the lower 
left corner of the frame), the character does not come close to the object. C: Because the position of 
the object is the only constraint that speci.es a position for the character, the entire motion can be 
translated. (from the origin), and therefore should be scaled as the limbs were. Such scalings are dif.cult 
to create with the additive displacement maps, so we perform the scaling as a separate step. If the character 
is scaled uniformly and does not interact with the world (or if the world is scaled similarly), the scaling 
is suf.cient for retargetting. In cases where the character is scaled non-uniformly, we make an estimate 
of the overall scaling to apply to the positions. Multiplying the positional parameters scales the motion 
around an arbitrary point, the center of the coordinate system. Typically, there is a better center for 
the scaling. For example, we might scale the heights around the .oor, which may not be zero. We recenter 
the scaling of the positional parameters by adding a translational component to them. To .nd the translation, 
we note that a constant positional shift of a motion is not noticeable, except in conjunction with constraints 
that relate the character to the world. Therefore, if we could re­establish the constraints by a simple 
shift of the motion, this would be ideal. We .nd the shift of the motion that comes closest to re­establishing 
the constraints by computing the average of the dis­placements. By displacement, we refer to the vector 
between the point on the character and a position that it is attached to. Con­straints only have displacements 
for axes that they restrict with a .xed position, for example, a footplant constraint may only specify 
the vertical direction if it only places non-skid restrictions on the other axes. Since the center of 
scaling might not be constant over the whole motion, we compute a translational signal to add to the 
positions. We perform the displacement averaging process on each frame in­dividually. Adding the per-frame 
constraint displacements to the motion may add undesirable high frequencies. Therefore, we inter­polate 
the offsets to frames that do not have any displacements and apply low-pass .ltering to remove high frequencies. 
The utility of interpolation can be seen in the example of Section 1.1 where a .gure walks up to, picks 
up, and carries away an object. In this example, the only constraint on the .gure s position on the .oor 
is provided by the constraint that the hands touch the object on the middle frame of the motion. When 
the motion is scaled, the entire motion is moved far away from the goal point. Interpolating the displacement 
of this one constraint shifts the entire motion back to the object, as shown in Figure 5. The desirability 
of constant shifts is unique to position; for angles it can have the undesirable behavior described in 
Section 3.2.   The Motion Retargetting Method To summarize, our approach to retargetting motion to 
another artic­ulated .gure with different limb lengths consists of the following steps: 1. Begin with 
an initial motion with identi.ed constraints. m1 t of the solution by scaling the 2. Find an initial 
estimate () translational parameters of the motion, and then adding a translation to de.ne the center 
of scaling. This translation is computed by .nding the constraint displacements of the  scaled motion 
for the target character, interpolating these val­ues, and smoothing. 3. Choose a representation for 
the motion-displacement curve based on the frequency decomposition of the original motion. 4. Solve 
the non-linear constraint problem for a displacement that when added to the result of step 2 provides 
a motion that satis.es the constraints. 5. (optional) If the result of step 4 does not satisfy the constraints 
suf.ciently, solve using the result of the step ( m1 t d() )  ()+ t as the initial motion, and a denser 
set of control points for the new displacement. 5 Motion for Morphing The same methods that are used 
to adapt a character to new seg­ment lengths can be used when the target lengths are not constant, i.e. 
when the target character is morphing. A simple example of a motion generated for morphing is shown in 
Figure 6. A more complex example is shown in Figure 10. The difference between motion for morphing and 
standard re­targetting is that the segment lengths of the target character is not constant over the motion. 
Therefore, it is better to use a differ­ent scaling amount on each frame in Step 2. As with the constant 
case, we estimate the scale in the event that the limb scalings are non-uniform. To apply this time-varying 
scale to the character s position, we scale the changes in translation between frames by the scale of 
the character in the frame, and add these changes together to .nd the characters positions.  6 Solving 
the Non-Linear Optimization The key computation of the retargetting approach is the solution of the spacetime 
constraint problem. In this section, we brie.y discuss our solver implementation. We emphasize that our 
approach casts retargetting as a standard mathematical problem, constrained opti­mization, for which 
there is a rich literature of solution methods. For a more detailed discussion of solution methods, we 
suggest a text on the subject such as Fletcher [4] or Gill et al. [5]. For simplicity of our discussion, 
we consider only equality con­straints as we implement inequality constraints using an active set method 
[4] that creates inequality constraints by switching sets of equality constraints on and off. The constrained 
optimization prob­lem we solve is generically: g x fx ( ) = . (2) minimize ( )subject to c The unknown 
in our spacetime problem is the motion­displacement curve, or more precisely, the values for the B-Spline 
control points of the displacement curve. The vector of parame­ters x is the concatenation of these points. 
We must express all of the constraints and objectives in terms of these variables, and so­lution methods 
require us to compute the values and derivatives of  Figure 6: The retargetting process is used to adapt 
the motion of Figure 2 as the character morphs to 60% of its original size. Left: the footplant positions 
are .xed to be the same as the original motion. Right: the solver repositions the footplants. these functions. 
We approximate the objective of Equation 1 as a weighted sum of squares of the controls 1 g()x= xMx (3) 
2 where Mis a diagonal matrix. We usually compute the entries in Mto account for differing sensitivities 
in the variables as described in [6] and [7]. The importance of the choice of Mis reduced by the large 
number of constraints, both explicit in equations and implicit in the representation, in the retargetting 
problems. Since our constraints are always de.ned on instants of time, the sampling of the continuous 
variational problem is implicit in their de.nition. While the expressions for individual constraints 
may grow complicated, we note that they are composed of smaller pieces that are more manageable. For 
example, a constraint specifying the height of a characters foot would combine the kinematic func­tion 
that takes the character s parameters and returns the foot height fk q composed with the function that 
computed the value of the () parameters at the instant of time in question q ti =m0()ti +() dti , dti 
bt, x). Through which in turn must sample the B-splines ()= ( the use of automatic differentiation [8, 
10], we can construct these pieces independently. Most previous spacetime work has used constrained 
optimiza­tion solvers that are variants of sequential quadratic programming (SQP). This standard method 
is described in texts such as [4], as well as spacetime papers such as [22] and [3]. In [6], we provided 
a variant of SQP that is more ef.cient for cases where the objective function has the special form of 
Equation 3. Our system includes solvers that operate both ways. An alternative solution approach focuses 
on minimizing the con­ r =/ fx - c · fx - c)(because of the straint residual 12(() ) (() implicit constraints 
of the representation, it is unreasonable to ex­pect that there will be an exact solution to the explicit, 
equational constraints). Because the constraints may not fully determine the solution, for example on 
a walking motion the legs may be over de­termined while there are no constraints on the arms, we add 
addi­tional constraints that specify that each variable should have a zero value. These constraints receive 
a smaller weighting. Such prob­lems are called damped least-squares problems [5, 16], and can be solved 
by performing an unconstrained minimization on the resid­ual 11 fx- c· fx - c)+E · r = (() )(() xx , 
(4) 22 where E is a small constant, or a diagonal matrix of weights. Our non-linear least-squares solver 
iteratively improves on an estimate of the solution. At each step, we construct a linear approx­imation 
of the constraint problem using Taylor expansion around the current estimate for x, .f fx . fx ()i + 
., (+ ) .x which gives us a linearized version of the constraint equations, fx - c. J.=() This linear 
least-squares problem can be solved in a variety of ways. We solve for .using a damped pseudo-inverse 
T +) T(() c). (5) (JJ EI. =J fx - Because Equation 5 is a positive de.nite linear system, we can solve 
it ef.ciently using either a Cholesky decomposition [18] or conju­gate gradient solver[1]. We use the 
latter exclusively as it allows us to exploit the sparsity in the matrix to achieve good performance. 
In both our constrained-optimization and least-squares solvers we use a line search [18] to determine 
how to use best the results of the linear subproblem. That is, once we compute ., we deter­mine a value 
of k such that x+k.best satis.es the non-linear constraints. In most cases, we .nd the least-squares 
solver to be faster than either of the SQP style solvers while providing equivalent results. For the 
rest of the paper, we will refer to the solvers as SQP (for the solver similar to that described in [3]), 
LMULT (for our imple­mentation of the method in [6]), and least squares (for the pseudo­inverse based 
solver). The running times of the iterative methods used in our solvers depend on many factors, including 
number of variables, number of constraints, sparsity, and desired stopping tol­erance. Small changes, 
especially in tolerance, can cause dramatic changes in solver times.  7 Examples We have used the retargetting 
approach of this paper on a number of examples. While there is nothing speci.c to motion capture data 
in our approach, our examples are exclusively done on performance data because of its availability. Other 
than the rotoscoped 2D walk­ing motion of Figure 2, the motions in this paper were captured with an optical 
motion capture system at a commercial studio. In all examples, the 120 Hz motion capture data was downsampled 
to 30Hz. Marker positions were converted to articulated .gure para­meters using our experimental automated 
software. Because of the differences in processing technologies, we have some diversity in the parameters 
for the .gures in different motions. In all cases, we use Euler angle representations for the joints. 
We do not have positional information for the hands. Therefore, we treat the end of the forearm as the 
hand. Similarly, some motion data is missing information for the feet, in which case the ankles are used 
as the end effectors. For many of the motions, we did not compute the head and neck parameters as they 
do not affect the computations. Joints generally have three degrees of freedom, except for the elbows, 
knees, and ankles which have one or two parameters.  Figure 7: A walk adapted to a .gure 60% of the 
size of the original actor. The smaller character is forced to use the original footplant positions. 
When the displacement keys are too distant, over.tting causes the wide swings shown in the alternate 
(yellow) foot traces. Proper key spacing (blue) results in a motion similar to the original (purple). 
When given, timing information refers to our prototype system running on an Apple Power Macintosh 8500/180 
computer with a 180Mhz PowerPC 604e processor and enough physical memory to complete the retargetting 
without paging. Timings are reported for the task of solving the non-linear optimization as the other 
parts of our retargetting approach take negligible amounts of computation. 7.1 Walking The initial 2D 
walking motion of Figure 2 was created by rotoscop­ing marker points and using a capture process like 
that described in Section 8 to compute the parameters of the articulated .gure. Our character has 14 
degrees of freedom (2 for position and 12 joint angles), and the motion is 15Hz. On the 82 frame motion, 
foot­plant constraints on the heels and toes give 146 scalar constraints, to which we add 328 inequality 
constraints to keep the feet above the .oor in each frame, and 1968 joint limit constraints. Our 3D walking 
example is similar. The character has 34 degrees of freedom, and does not have hands or feet. Because 
the feet in the motion are actually ankles, they were not planted in the original motion and skated. 
We therefore used our solver to establish these constraints initially. Including joint limits and feet-above-.oor 
con­straints there are a total of 4193 scalar equations on the 112 frame motion, although during solving 
there are generally only 354 active constraints. We have adapted the walking motion to a number of differently 
proportioned .gures. An example is shown in Figure 7. With .xed footplant positions to match the tall 
.gure, the shorter legged .g­ures must take unnaturally long strides, seen in Figure 8. As pre­dicted 
by the pyramid level heuristic of Section 3.7, a key spacing of 4 provides a better result to spacings 
of 8 or 2. With a key spac­ing of 8 there is considerable over-.tting that can be clearly seen in the 
yellow foot path traces of Figure 7. A key spacing of 2 provides a motion that is reasonable, however, 
the character seems to slow down with each step. While this is different from the original mo­tion, the 
character is taking very large steps, so it seems natural for it to regain its balance each time. Our 
system was able to generate all 3 motions in under 10 seconds of solution time, so it is practical to 
create all 3 motions and choose the one we .nd visually most desirable. Figure 8: Forcing a character 
with short legs to walk in the footsteps of a longer-legged character leads to an unnatural motion. 
 7.2 Climbing a Ladder The ladder example, shown in Figure 4, gives constraints on both hands and feet. 
The .gure has 35 degrees of freedom, no hands, and no neck or head. We use .xed position constraints 
for the foot­plants and handplants on the ladder. The least-squares solver takes approximately 9 seconds 
for keys spaced every other frame, and 7 seconds every fourth frame. The LMULT solver takes 6 and 4 seconds, 
although its answers do not satisfy the constraints as accu­rately. With the key spacing of 4, the LMULT 
solution has some constraints being violated by over half an inch, while the least­squares solution satis.es 
all constraints to within a quarter of an inch. The .xed position of the hand and footplants on the ladder 
lead to slightly unnatural motions: the small .gure must reach over its head to grasp the handholds and 
sometimes stands on its tip-toes to reach. We have implemented some less restrictive constraints: foot­plants 
that the solver can move along the ladder step (so the width of the steps is not an issue) and hand-holds 
that can be positioned along the rail. These constraints are relatively special purpose: they probably 
will be useful for ladder climbing motions. The motion obtained from using these constraints more closely 
resembles the original motion, although it is still unnatural as the ladder is very  Figure 9: female 
character. Left: original motion. Center: only female motion adapted. Right: both characters adapted. 
large in comparison to the resized character.  7.3 Swing Dancing When there are two characters in a 
scene, we may wish to adapt both together, even if only one changes size. For example, consider the swing 
dance motion in Figure 9. In this motion, the hands of the two characters must remain connected, in addition 
to the foot­plant constraints. If we change the size of the female .gure without changing the motion 
of the male .gure, the smaller .gure gets lifted by the hand-hold when spinning. If we adjust both motions 
simul­taneously, the male s part is adapted, and the female s spin is less noticeably forced. In Figure 
10, the female shrinks in size while spinning and the male part responds accordingly. On the 276 frame 
motion, we use 1200 equality constraints for the female character s footplants (which are free to be 
repositioned by the solver) and the connection between the characters hands. We only allow the upper 
body of the male character to be altered. If we adapt just the female motion, there are 33 parameters. 
Adapting both motions gives 44 parameters per key. The least-squares solver took approximately 14 seconds, 
while the LMULT solver ran for slightly over a minute, but with a solution that better satis.es the constraints 
(all to within an eighth of an inch).  8 Differing Characters When the characters share structure there 
is a direct mapping be­tween the parameters of one to the other. The more general retar­getting problem 
is harder. When we apply a human motion to a .gure with a different structure, there are creative choices 
in how the motion applies. What will the character use for knees? How do we choose a motion for the parts 
of the character that the human does not have? These creative choices correspond to mathematical problems: 
there may be different types of degrees of freedom, and there may be different numbers of degrees of 
freedom. Figure 11: A walking motion is adapted from a human to a soda can by .rst adapting it to a human 
with the proportions of a can, then using this motion to drive the motion of the can (shown transparently 
surrounding the humanoid). Our initial attempts at automatic anthropomorphism allow the user to make 
the creative choices, while having the system do the more tedious aspects. The user identi.es correspondences 
between externally visible features of the characters, not the degrees of free­dom that determine their 
positions. For example, we identify points on the new character that will serve as its feet when it walks, 
even if the foot is not at the end of a two-segment leg like the human. These correspondences pose a 
constraint problem, almost identical to the problem of motion capture processing: we must compute a motion 
that puts the character s features in the right location in each frame. We can use the same spacetime 
constraints techniques that we have used for retargetting for the anthropomorphic case. Our con­straints 
connect each feature on the new character to its correspond­ing feature on the original in each frame. 
If there are fewer degrees of freedom on the character, the motions will not be able to match exactly, 
and we .nd the best matching motion in a least squares sense. We have not yet developed a method for 
handling extra de­grees of freedom. For the spatial correspondences to apply, the characters must be 
approximately the same size. We use the retargetting methods of this paper to adapt the initial human 
motion to a new .gure that has proportions more similar to the target character. We then use this motion 
as the source of constraints to compute the target motion. Figure 11 shows an example in which we adapt 
a human motion to a rigid can (a cylinder with the same proportions as a soda can). We correspond three 
points on the can to the human: the ends of the legs are connected to points on the bottom of the can, 
and the center of the hips is attached to the center of the can. Even with the can s extremely limited 
degrees of freedom (it is a rigid body), it can convey a sense of the original human motion. In our tests, 
we have made the can walk, skip, and run.  9 Discussion In this paper we presented an approach to retargetting 
motions from one character to another by posing the problem of computing an adaptation as a constrained 
optimization. To realize the approach in a practical manner, we used geometric constraints and a simple 
objective function. This pragmatic strategy dodges dif.culties in using spacetime constraints. We compute 
retargettings of complex motions despite: not having developed mathematical encodings of concepts such 
as grace and Charlie Chaplin ness in motion; not having presented too many choices of constraints and 
objectives to users; and not having solved optimization problems for which we do not have ef.cient solution 
methods.  While our pragmatism pays off in the practicality of the method, we sometimes pay a cost in 
the quality of the resulting motions. Some of the problems we see are artifacts of the speci.c simple 
ob­jective we have chosen and our reliance on simple frequency lim­its on the adaptations. For instance, 
in the example of Figure 1 the balance between reaching, bending, and positioning is chosen by artifacts 
of the representation of the character s con.guration and different spatial frequencies in reaching and 
walking make se­lection of a single frequency limit for the adaptation problematic. Other problems occur 
because we have no guarantees on the many properties we do not explicitly model in our constraints and 
ob­jective. For instance, our lack of physics constraints can lead to unrealistic situations like Figure 
8 and the right image of Figure 1. Richer sets of constraints and objective functions, combined with 
improved solvers for the resulting numerical problems and tech­niques to avoid the burden of speci.cation, 
would cause our ap­proach to provide better results for a wider range of motions. Acknowledgments Jim 
Spohrer was instrumental in getting Apple Computer to give me access to the Timelines source code and 
sample data. Peter Litwinowicz suggested the retargetting problem to me, and suffered through my early 
attempts to solve it. My colleagues at the Au­todesk Vision Technology Center provided critical reads 
of this pa­per. Jane Wilhelms, Sebastian Grassia, Zoran Popovi´ c and Jessica Hodgins gave much needed 
writing advice. Lori Gleicher was my dance critic.   References [1] Richard Barrett, Michael Berry, 
Tony Chan, James Demmel, June Donato, Jack Dongarra, Victor Eikhout, Roldan Pozo, Charles Romine, and 
Henk van der Vorst. Templates for the solution of linear systems: Building Blocks for Itera­tive Methods 
. SIAM, 1994. [2] Armin Bruderlin and Lance Williams. Motion signal processing. In Robert Cook, editor, 
SIGGRAPH 95 Conference Proceedings , Annual Conference Se­ries, pages 97 104, August 1995. [3] Michael 
F. Cohen. Interactive spacetime control for animation. In Edwin E. Catmull, editor, Computer Graphics 
(SIGGRAPH 92 Proceedings) , volume 26, pages 293 302, July 1992. [4] Roger Fletcher. Practical Methods 
of Optimization . John Wiley and Sons, 1987. [5] Phillip Gill, Walter Murray, and Margaret Wright. Practical 
Optimization . Aca­demic Press, New York, NY, 1981. [6] Michael Gleicher. Motion editing with spacetime 
constraints. In Michael Co­hen and David Zeltzer, editors, Proceedings 1997 Symposium on Interactive 
3D Graphics , pages 139 148, apr 1997. [7] Michael Gleicher and Peter Litwinowicz. Constraint-based motion 
adaptation. Journal of Visualization and Computer Animation , to appear. [8] Michael Gleicher and Andrew 
Witkin. Supporting numerical computations in interactive contexts. In Tom Calvert, editor, Proceedings 
of Graphics Interface 93, pages 138 145, May 1993. [9] Jessica Hodgins and Nancy Pollard. Adapting simulated 
behaviors for new char­acters. In Turner Whitted, editor, SIGGRAPH 97 Conference Proceedings , pages 
153 162, August 1997. [10] Masao Iri. History of automatic differentiation and rounding error estimation. 
In Andreas Griewank and George Corliss, editors, Automatic Differentiation of Al­gorithms: Theory, Implementation 
and Application , pages 3 16. SIAM, January 1991. [11] Kinetix Division of Autodesk Inc. Character studio. 
Computer Program, 1997. [12] Peter C. Litwinowicz. Inkwell: A 2 12 -D animation system. In Thomas W. 
Seder­berg, editor, Computer Graphics (SIGGRAPH 91 Proceedings) , volume 25, pages 113 122, July 1991. 
[13] Zicheng Liu, Steven J. Gortler, and Michael F. Cohen. Hierarchical spacetime control. In Andrew 
Glassner, editor, SIGGRAPH 94 Conference Proceedings , Annual Conference Series, pages 35 42, July 1994. 
[14] Michael Mortenson. Geometric Modelling . John Wiley &#38; Sons, second edition, 1997. [15] J. Thomas 
Ngo and Joe Marks. Spacetime constraints revisited. In James Kajiya, editor, Computer Graphics (SIGGRAPH 
93 Proceedings) , volume 27, pages 343 350, August 1993. [16] Christopher Paige and Michael Saunders. 
LSQR: an algorithm for sparse linear equations and sparse least squares. ACM Transactions on Mathematical 
Soft­ware , 8(1):43 71, March 1982. [17] Ken Perlin. Real time responsive animation with personality. 
IEEE Transactions on Visualization and Computer Graphics , 1(1):5 15, March 1995. ISSN 1077­2626. [18] 
William Press, Brian Flannery, Saul Teukolsky, and William Vetterling. Numer­ical Recipes in C . Cambridge 
University Press, Cambridge, England, 1986. [19] Charles F. Rose, Brian Guenter, Bobby Bodenheimer, and 
Michael F. Cohen. Ef.cient generation of motion transitions using spacetime constraints. In Holly Rushmeier, 
editor, SIGGRAPH 96 Conference Proceedings , Annual Conference Series, pages 147 154, August 1996. [20] 
Steven Seitz and Chuck Dyer. Analogically-guided animation. Masters Project Report, Department of Computer 
Science, University of Wisconsin, May 1993. unpublished. [21] Munetoshi Unuma, Ken Anjyo, and Ryozo Takeuchi. 
Fourier principles for emotion-based human .gure animation. In Robert Cook, editor, SIGGRAPH 95 Conference 
Proceedings , Annual Conference Series, pages 91 96. ACM SIG-GRAPH, Addison Wesley, August 1995. [22] 
Andrew Witkin and Michael Kass. Spacetime constraints. In John Dill, editor, Computer Graphics (SIGGRAPH 
88 Proceedings) , volume 22, pages 159 168, August 1988. [23] Andrew Witkin and Zoran Popovic.´Motion 
warping. In Robert Cook, editor, SIGGRAPH 95 Conference Proceedings , Annual Conference Series, pages 
105 108, August 1995. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280821</article_id>
		<sort_key>43</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Large steps in cloth simulation]]></title>
		<page_from>43</page_from>
		<page_to>54</page_to>
		<doi_number>10.1145/280814.280821</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280821</url>
		<keywords>
			<kw><![CDATA[cloth]]></kw>
			<kw><![CDATA[constraints]]></kw>
			<kw><![CDATA[implicit integration]]></kw>
			<kw><![CDATA[physically-based modeling]]></kw>
			<kw><![CDATA[simulation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.3</cat_node>
				<descriptor>Linear systems (direct and iterative methods)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.3</cat_node>
				<descriptor>Sparse, structured, and very large systems (direct and iterative methods)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003719</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computations on matrices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010148.10010149.10010158</concept_id>
				<concept_desc>CCS->Computing methodologies->Symbolic and algebraic manipulation->Symbolic and algebraic algorithms->Linear algebra algorithms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003719</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computations on matrices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010148.10010149.10010158</concept_id>
				<concept_desc>CCS->Computing methodologies->Symbolic and algebraic manipulation->Symbolic and algebraic algorithms->Linear algebra algorithms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39038146</person_id>
				<author_profile_id><![CDATA[81100334025]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baraff]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon Univ., Pittsburgh, PA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P18516</person_id>
				<author_profile_id><![CDATA[81100295587]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Witkin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon Univ., Pittsburgh, PA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>165682</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[D. Baraff. Dynamic Simulation of Non-penetrating Rigid Bodies. PhD thesis, Cornell University, May 1992.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192168</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[D. Baraff. Fast contact force computation for nonpenetrating rigid bodies. Computer Graphics (Proc. SIGGRAPH), 28:23- 34, 1994.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192259</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[D.E. Breen, D.H. House, and M.J. Wozny. Predicting the drape of woven cloth using interacting particles. Computer Graphics (Proc. SIGGRAPH), pages 365-372, 1994.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134017</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[M. Carignan, Y. Yang, N. Magenenat-Thalmann, and D. Thaimann. Dressing animated synthetic actors with complex deformable clothes. Computer Graphics (Proc. SIGGRAPH), pages 99-104, 1992.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618378</ref_obj_id>
				<ref_obj_pid>616042</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[B. Eberhardt, A. Weber, and W. Strasser. A fast, flexible, particle-system model for cloth draping. IEEE Computer Graphics and Applications, 16:52-59, 1996.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[G. Golub and C. Van Loan. Matrix Computations. John Hopkins University Press, 1983.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[M. Kass. An Introduction To Physically Based Modeling, chapter Introduction to Continuum Dynamics for Computer Graphics. SIGGRAPH Course Notes, ACM SIGGRAPH, 1995.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97884</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[M. Kass and G. Miller. Rapid, stable fluid dynamics for computer graphics. Computer Graphics (Proc. SIGGRAPH), pages 49-58, 1990.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618376</ref_obj_id>
				<ref_obj_pid>616042</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[H.N. Ng and R.L. Grimsdale. Computer graphics techniques for modeling cloth. IEEE Computer Graphics and Applications, 16:28-41, 1996.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134019</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[H. Okabe, H. Imaoka, T. Tomiha, and H. Niwaya. Three dimensional apparel cad system. Computer Graphics (Proc. SIGGRAPH), pages 105-110, 1992.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378524</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[J.C. Platt and A.H. Barr. Constraint methods for flexible models. In Computer Graphics (Proc. SIGGRAPH), volume 22, pages 279-288. ACM, July 1988.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[W.H. Press, B.E Flannery, S.A. Teukolsky, and W.T. Vetterling. Numerical Recipes. Cambridge University Press, 1986.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[X. Provot. Deformation constraints in a mass-spring model to describe rigid cloth behavior. In Graphics Interface, pages 147-155, 1995.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>865018</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[J. Shewchuk. An introduction to the conjugate gradient method without the agonizing pain. Technical Report CMU- CS-TR-94-125, Carnegie Mellon University, 1994. (See also http ://www. cs. cmu. edu/~quake-papers/ painless-conjugate-gradient, ps.).]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[D. Terzopoulos and K. Fleischer. Deformable models. Visual Computer, 4:306-331, 1988.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378522</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[D. Terzopoulos and K. Fleischer. Modeling inelastic deformation: Viscoelasticity, plasticity, fracture. In Computer Graphics (Proc. SIGGRAPH), volume 22, pages 269-278. ACM, August 1988.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37427</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[D. Terzopoulos, J.C. Platt, and A.H. Barr. Elastically deformable models. Computer Graphics (Proc. SIGGRAPH), 21:205-214, 1987.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>176580</ref_obj_id>
				<ref_obj_pid>176579</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[D. Terzopoulos and H. Qin. Dynamics nurbs with geometric constraints for interactive sculpting. ACM Transactions on Graphics, 13:103-136, 1994.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>923809</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[X. Tu. Artificial Animals for Computer Animation: Biomechanics, Locomotion, Perception and Behavior. PhD thesis, University of Toronto, May 1996.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218432</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[E Volino, M. Courchesne, and N. Magnenat Thalmann. Versatile and efficient techniques for simulating cloth and other deformable objects. Computer Graphics (Proc. SIGGRAPH), pages 137-144, 1995.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618377</ref_obj_id>
				<ref_obj_pid>616042</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[E Volino, N. Magnenat Thalmann, S. Jianhua, and D. Thaimann. An evolving system for simulating clothes on virtual actors. IEEE Computer Graphics and Applications, 16:42-51, 1996.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280822</article_id>
		<sort_key>55</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Making faces]]></title>
		<page_from>55</page_from>
		<page_to>66</page_to>
		<doi_number>10.1145/280814.280822</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280822</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.2.2</cat_node>
				<descriptor>Graph algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003624.10003633.10010917</concept_id>
				<concept_desc>CCS->Mathematics of computing->Discrete mathematics->Graph theory->Graph algorithms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P32646</person_id>
				<author_profile_id><![CDATA[81100130209]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guenter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Corp.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15036040</person_id>
				<author_profile_id><![CDATA[81100553787]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Cindy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grimm]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Corp.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P59537</person_id>
				<author_profile_id><![CDATA[81100377878]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wood]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Washington, Seattle]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P109471</person_id>
				<author_profile_id><![CDATA[81100362010]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Henrique]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Malvar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Washington, Seattle]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P87000</person_id>
				<author_profile_id><![CDATA[81100026067]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Fredric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pighin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Washington, Seattle]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>134003</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BEIER, T., AND NEELY, S. Feature-based image metamorphosis. In Computer Graphics (SIGGRAPH '92 Proceedings) (July 1992), E. E. Catmull, Ed., vol. 26, pp. 35-42.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BREGLER, C., COVELL, M., AND SLANEY, M. Video rewrite: Driving visual speech with audio. Computer Graphics 31, 2 (Aug. 1997), 353-361.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[CASSELL, J., PELACHAUD, C., BADLER, N., STEEDMAN, M., ACHORN, B., BECKET, T., DOUVILLE, B., PREVOST, S., AND STONE, M. Animated conversation: Rule-based generation of facial expression, gesture and spoken intonation for multiple conversational agents. Computer Graphics 28, 2 (Aug. 1994), 413-420.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>794617</ref_obj_id>
				<ref_obj_pid>794190</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[DECARLO, D., AND METAXAS, D. The integration of optical flow and deformable models with applications to human face shape and motion estimation. Proceedings CVPR (1996), 231-238.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>261522</ref_obj_id>
				<ref_obj_pid>261506</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[ESSA, I., AND PENTLAND, A. Coding, analysis, interpretation and recognition of facial expressions. IEEE Transactions on Pattern Analysis and Machine Intelligence 19, 7 (1997), 757-763.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>171658</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[FAUGERAS, O. Three-dimensional computer vision. MIT Press, Cambridge, MA, 1993.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358692</ref_obj_id>
				<ref_obj_pid>358669</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[FISCHLER, M. A., AND BOOLES, R. C. Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM 24, 6 (Aug. 1981), 381-395.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237216</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[HOPPE, H. Progressive meshes. In SIGGRAPH 96 Conference Proceedings (Aug. 1996), H. Rushmeier, Ed., Annual Conference Series, ACM SIGGRAPH, Addison Wesley, pp. 99-108. held in New Orleans, Louisiana, 04-09 August 1996.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[HORN, B. K. P. Closed-form solution of absolute orientation using unit quaternions. Journal of the Optical Society of America 4, 4 (Apr. 1987).]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[LEE, Y., TERZOPOULOS, D., AND WATERS, K. Realistic modeling for facial animation. Computer Graphics 29, 2 (July 1995), 55-62.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[PIGHIN, F., AUSLANDER, J., LISHINSKI, D., SZELISKI, R., AND SALESIN, D. Realistic facial animation using image based 3d morphing. Tech. Report TR-97-01-03, Department of Computer Science and Engineering, University of Washington, Seattle, Wa, 1997.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>235125</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[SCHURMANN, J. Pattern Classification: A Unified View of Statistical and Neural Approaches. John Wiley and Sons, Inc., New York, 1996.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[STRANG. Linear Algebra and its Application. HBJ, 1988.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37405</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[WATERS, K. A muscle model for animating threedimensional facial expression. In Computer Graphics (SIG- GRAPH '87 Proceedings) (July 1987), M. C. Stone, Ed., vol. 21, pp. 17-24.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97906</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[WILLIAMS, L. Performance-driven facial animation. Computer Graphics 24, 2 (Aug. 1990), 235-242.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Making Faces Brian GuenteryCindy GrimmyDaniel Woodz Copyright &#38;#169;1998 by the Association for 
Computing Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal 
or classroom use is granted without fee provided that copies are not made or distributed for profit or 
commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. 
To copy otherwise, to republish, to post on servers or to redistribute to lists, requires specific permission 
and/or a fee. Henrique MalvaryFredrick Pighinz yMicrosoft Corporation zUniversity of Washington ABSTRACT 
We have created a system for capturing both the three-dimensional geometry and color and shading information 
for human facial ex­pressions. We use this data to reconstruct photorealistic, 3D ani­mations of the 
captured expressions. The system uses a large set of sampling points on the face to accurately track 
the three dimen­sional deformations of the face. Simultaneously with the tracking of the geometric data, 
we capture multiple high resolution, regis­tered video images of the face. These images are used to create 
a texture map sequence for a three dimensional polygonal face model which can then be rendered on standard 
3D graphics hardware. The resulting facial animation is surprisingly life-like and looks very much like 
the original live performance. Separating the capture of the geometry from the texture images eliminates 
much of the vari­ance in the image data due to motion, which increases compression ratios. Although the 
primary emphasis of our work is not compres­sion we have investigated the use of a novel method to compress 
the geometric data based on principal components analysis. The texture sequence is compressed using an 
MPEG4 video codec. An­imations reconstructed from 512x512 pixel textures look good at data rates as low 
as 240 Kbits per second. CR Categories: I.3.7 [Computer Graphics]: Three Dimen­sional Graphics and Realism: 
Animation; I.3.5 [Computer Graph­ics]: Computational Geometry and Object Modeling  Introduction One 
of the most elusive goals in computer animation has been the realistic animation of the human face. Possessed 
of many degrees of freedom and capable of deforming in many ways the face has been dif.cult to simulate 
accurately enough to convince the average person that a piece of computer animation is actually an image 
of a real person. We have created a system for capturing human facial expres­sion and replaying it as 
a highly realistic 3D talking head con­sisting of a deformable 3D polygonal face model with a changing 
texture map. The process begins with video of a live actor s face, recorded from multiple camera positions 
simultaneously. Fluores­cent colored 1/8 circular paper .ducials are glued on the actor s face and their 
3D position reconstructed over time as the actor talks and emotes. The 3D .ducial positions are used 
to distort a 3D polygonal face model in mimicry of the distortions of the real face. The .ducials are 
removed using image processing techniques and the video streams from the multiple cameras are merged 
into a sin­gle texture map. When the resulting .ducial-free texture map is ap­plied to the 3D reconstructed 
face mesh the result is a remarkably life-like 3D animation of facial expression. Both the time varying 
texture created from the video streams and the accurate reproduc­tion of the 3D face structure contribute 
to the believability of the resulting animation. Our system differs from much previous work in facial 
anima­tion, such as that of Lee [10], Waters [14], and Cassel [3], in that we are not synthesizing animations 
using a physical or procedu­ral model of the face. Instead, we capture facial movements in three dimensions 
and then replay them. The systems of [10], [14] are designed to make it relatively easy to animate facial 
expression manually. The system of [3] is designed to automatically create a dialog rather than faithfully 
reconstruct a particular person s fa­cial expression. The work of Williams [15] is most similar to ours 
except that he used a single static texture image of a real person s face and tracked points only in 
2D. The work of Bregler et al [2] is somewhat less related. They use speech recognition to locate visemes1in 
a video of a person talking and then synthesize new video, based on the original video sequence, for 
the mouth and jaw region of the face to correspond with synthetic utterances. They do not create a three 
dimensional face model nor do they vary the ex­pression on the remainder of the face. Since we are only 
concerned with capturing and reconstructing facial performances out work is unlike that of [5] which 
attempts to recognize expressions or that of [4] which can track only a limited set of facial expressions. 
An obvious application of this new method is the creation of believable virtual characters for movies 
and television. Another application is the construction of a .exible type of video compres­sion. Facial 
expression can be captured in a studio, delivered via CDROM or the internet to a user, and then reconstructed 
in real time on a user s computer in a virtual 3D environment. The user can select any arbitrary position 
for the face, any virtual camera viewpoint, and render the result at any size. One might think the second 
application would be dif.cult to achieve because of the huge amount of video data required for the time 
varying texture map. However, since our system generates ac­curate 3D deformation information, the texture 
image data is pre­cisely registered from frame to frame. This reduces most of the variation in image 
intensity due to geometric motion, leaving pri­marily shading and self shadowing effects. These effects 
tend to be of low spatial frequency and can be compressed very ef.ciently. The compressed animation looks 
good at data rates of 240 kbits per second for texture image sizes of 512x512 pixels, updating at 30 
frames per second. The main contributions of the paper are a method for robustly capturing both a 3D 
deformation model and a registered texture im­age sequence from video data. The resulting geometric and 
texture data can be compressed, with little loss of .delity, so that storage 1Visemes are the visual 
analog of phonemes. Figure 1: The six camera views of our actress face. requirements are reasonable 
for many applications. Section 2 of the paper explains the data capture stage of the process. Section 
3 describes the .ducial correspondence algorithm. In Section 4 we discuss capturing and moving the mesh. 
Sections 5 and 6 describe the process for making the texture maps. Section 7 of the paper describes the 
algorithm for compressing the geometric data. 2 Data Capture We used six studio quality video cameras 
arranged in the pattern shown in Plate 1 to capture the video data. The cameras were syn­chronized and 
the data saved digitally. Each of the six cameras was individually calibrated to determine its intrinsic 
and extrinsic parameters and to correct for lens distortion. The details of the calibration process are 
not germane to this paper but the interested reader can .nd a good overview of the topic in [6] as well 
as an extensive bibliography. We glued 182 dots of six different colors onto the actress face. The dots 
were arranged so that dots of the same color were as far apart as possible from each other and followed 
the contours of the face. This made the task of determining frame to frame dot corre­spondence (described 
in Section 3.3) much easier. The dot pattern was chosen to follow the contours of the face (i.e., outlining 
the eyes, lips, and nasio-labial furrows), although the manual applica­tion of the dots made it dif.cult 
to follow the pattern exactly. The actress head was kept relatively immobile using a padded foam box; 
this reduced rigid body motions and ensured that the actress face stayed centered in the video images. 
Note that rigid body motions can be captured later using a 3D motion tracker, if desired. The actress 
was illuminated with a combination of visible and near UV light. Because the dots were painted with .uorescent 
pig­ments the UV illumination increased the brightness of the dots sig­ni.cantly and moved them further 
away in color space from the colors of the face than they would ordinarily be. This made them easier 
to track reliably. Before the video shoot the actress face was digitized using a cyberware scanner. This 
scan was used to create the base 3D face mesh which was then distorted using the positions of the tracked 
dots.  3 Dot Labeling The .ducials are used to generate a set of 3D points which act as control points 
to warp the cyberware scan mesh of the actress head. They are also used to establish a stable mapping 
for the textures generated from each of the six camera views. This requires that each dot have a unique 
and consistent label over time so that it is associated with a consistent set of mesh vertices. Legend 
Done for all frames Figure 2: The sequence of operations needed to produce the labeled 3D dot movements 
over time. The dot labeling begins by .rst locating (for each camera view) connected components of pixels 
which correspond to the .ducials. The 2D location for each dot is computed by .nding the two dimen­sional 
centroid of each connected component. Correspondence be­tween 2D dots in different camera views is established 
and potential 3D locations of dots reconstructed by triangulation. We construct a reference set of dots 
and pair up this reference set with the 3D locations in each frame. This gives a unique labeling for 
the dots that is maintained throughout the video sequence. A .owchart of the dot labeling process is 
shown in Figure 2. The left side of the .owchart is described in Section 3.3.1, the middle in Sections 
3.1, 3.2, and 3.3.2, and the right side in Sec­tion 3.1.1. 3.1 Two-dimensional dot location For each 
camera view the 2D coordinates of the centroid of each colored .ducial must be computed. There are three 
steps to this process: color classi.cation, connected color component genera­tion, and centroid computation. 
First, each pixel is classi.ed as belonging to one of the six dot colors or to the background. Then depth 
.rst search is used to lo­cate connected blobs of similarly colored pixels. Each connected colored blob 
is grown by one pixel to create a mask used to mark those pixels to be included in the centroid computation. 
This pro­cess is illustrated in Figure 4. The classi.er requires the manual marking of the .ducials for 
one frame for each of the six cameras. From this data a robust color classi.er is created (exact details 
are discussed in Section 3.1.1). Although the training set was created using a single frame of a 3330 
frame sequence, the .ducial colors are reliably labeled throughout the sequence. False positives are 
quite rare, with one major ex­ception, and are almost always isolated pixels or two pixel clusters. The 
majority of exceptions arise because the highlights on the teeth and mouth match the color of the white 
.ducial training set. Fortu­nately, the incorrect white .ducial labelings occur at consistent 3D locations 
and are easily eliminated in the 3D dot processing stage. The classi.er generalizes well so that even 
fairly dramatic changes in .ducial color over time do not result in incorrect classi.cation. For example, 
Figure 5(b) shows the same green .ducial in two dif­ferent frames. This .ducial is correctly classi.ed 
as green in both frames. The next step, .nding connected color components, is com­plicated by the fact 
that the video is interlaced. There is signif­icant .eld to .eld movement, especially around the lips 
and jaw, sometimes great enough so that there is no spatial overlap at all between the pixels of a .ducial 
in one .eld and the pixels of the same .ducial in the next .eld. If the two .elds are treated as a sin­gle 
frame then a single .ducial can be fragmented, sometimes into many pieces. One could just .nd connected 
color components in each .eld and use these to compute the 2D dot locations. Unfortunately, this does 
not work well because the .ducials often deform and are sometimes partially occluded. Therefore, the 
threshold for the number of pixels needed to classify a group of pixels as a .ducial has to be set very 
low. In our implementation any connected com­ponent which has more than three pixels is classi.ed as 
a .ducial rather than noise. If just the connected pixels in a single .eld are counted then the threshold 
would have to be reduced to one pixel. This would cause many false .ducial classi.cations because there 
are typically a few 1 pixel false color classi.cations per frame and 2 or 3 pixel false clusters occur 
occasionally. Instead, we .nd con­nected components and generate lists of potential 2D dots in each .eld. 
Each potential 2D dot in .eld one is then paired with the closest 2D potential dot in .eld two. Because 
.ducials of the same color are spaced far apart, and because the .eld to .eld movement is not very large, 
the closest potential 2D dot is virtually guaran­teed to be the correct match. If the sum of the pixels 
in the two potential 2D dots is greater than three pixels then the connected components of the two 2D 
potential dots are merged, and the re­sulting connected component is marked as a 2D dot. The next step 
is to .nd the centroid of the connected compo­nents marked as 2D dots in the previous step. A two dimensional 
gradient magnitude image is computed by passing a one dimen­sional .rst derivative of Gaussian along 
the xand ydirections and then taking the magnitude of these two values at each pixel. The centroid of 
the colored blob is computed by taking a weighted sum of positions of the pixel (x;y)coordinates which 
lie inside the gra­dient mask, where the weights are equal to the gradient magnitude. 3.1.1 Training 
the color classi.er We create one color classi.er for each of the camera views, since the lighting can 
vary greatly between cameras. In the following discussion we build the classi.er for a single camera. 
The data for the color classi.er is created by manually marking the pixels of frame zero that belong 
to a particular .ducial color. This is repeated for each of the six colors. The marked data is stored 
as 6 color class images, each of which is created from the original camera image by setting all of the 
pixels not marked as the given color to black (we use black as an out-of-class label because pure black 
never occurred in any of our images). A typical color class image for the yellow dots is shown in Figure 
3. We generated the color class images using the magic wand tool available in many image editing programs. 
A seventh color class image is automatically created for the background color (e.g., skin and hair) by 
labeling as out-of-class any pixel in the image which was previously marked as a .ducial in any of the 
.ducial color class images. This produces an image of the face with black holes where the .ducials were. 
The color classi.er is a discrete approximation to a nearest neighbor classi.er [12]. In a nearest neighbor 
classi.er the item Figure 3: An image of the actress s face. A typical training set for the yellow dots, 
selected from the image on the left. to be classi.ed is given the label of the closest item in the training 
set, which in our case is the color data contained in the color class images. Because we have 3 dimensional 
data we can approximate the nearest neighbor classi.er by subdividing the RGB cube uni­formly into voxels, 
and assigning class labels to each RGB voxel. To classify a new color you quantize its RGB values and 
then index into the cube to extract the label. To create the color classi.er we use the color class images 
to assign color classes to each voxel. Assume that the color class image for color class Cihas ndistinct 
colors, c1:::cn. Each of the voxels corresponding to the color cjis labeled with the color class Ci. 
Once the voxels for all of the known colors are labeled, the remaining unlabeled voxels are assigned 
labels by searching through all of the colors in each color class Ciand .nding the color closest to pin 
RGB space. The color pis given the label of the color class containing the nearest color. Nearness in 
our case is the Euclidean distance between the two points in RGB space. If colors from different color 
classes map to the same sub-cube, we label that sub-cube with the background label since it is more important 
to avoid incorrect dot labeling than it is to try to label every dot pixel. For the results shown in 
this paper we quantized the RGB color cube into a 32x32x32 lattice.  3.2 Camera to camera dot correspondence 
and 3D reconstruction In order to capture good images of both the front and the sides of the face the 
cameras were spaced far apart. Because there are such extreme changes in perspective between the different 
camera views, the projected images of the colored .ducials are very different. Fig­ure 5 shows some examples 
of the changes in .ducial shape and color between camera views. Establishing .ducial correspondence between 
camera views by using image matching techniques such as optical .ow or template matching would be dif.cult 
and likely to generate incorrect matches. In addition, most of the camera views will only see a fraction 
of the .ducials so the correspondence has to be robust enough to cope with occlusion of .ducials in some 
of the camera views. With the large number of .ducials we have placed on the face false matches are also 
quite likely and these must be detected and removed. We used ray tracing in combination with a RANSAC 
[7] like algorithm to establish .ducial correspondence and to compute accurate 3D dot positions. This 
algorithm is robust to occlusion and to false matches as well. First, all potential point correspondences 
between cameras are generated. If there are kcameras, and n2D dots in each camera () view then k n 2point 
correspondences will be tested. Each 2correspondence gives rise to a 3D candidate point de.ned as the 
closest point of intersection of rays cast from the 2D dots in the Image Field 1 Merging with closet 
neighbor Classified Field 2 Connected components pixels in fields 1 &#38; 2   Figure 4: Finding the 
2D dots in the images. two camera views. The 3D candidate point is projected into each of the two camera 
views used to generate it. If the projection is further than a user-de.ned epsilon, in our case two pixels, 
from the centroid of either 2D point then the point is discarded as a potential 3D point candidate. All 
the 3D candidate points which remain are added to the 3D point list. Each of the points in the 3D point 
list is projected into a refer­ence camera view which is the camera with the best view of all the .ducials 
on the face. If the projected point lies within two pixels of the centroid of a 2D dot visible in the 
reference camera view then it is added to the list of potential 3D candidate positions for that 2D dot. 
This is the list of potential 3D matches for a given 2D dot. () For each 3D point in the potential 3D 
match list, npossi­ 3ble combinations of three points in the 3D point list are computed and the combination 
with the smallest variance is chosen as the true 3D position. Then all 3D points which lie within a user 
de.ned distance, in our case the sphere subtended by a cone two pixels in radius at the distance of the 
3D point, are averaged to generate the .nal 3D dot position. This 3D dot position is assigned to the 
corresponding 2D dot in the reference camera view. This algorithm could clearly be made more ef.cient 
because many more 3D candidate points are generated then necessary. One could search for potential camera 
to camera correspondences only along the epipolar lines and use a variety of space subdivision tech­niques 
to .nd 3D candidate points to test for a given 2D point. However, because the number of .ducials in each 
color set is small (never more than 40) both steps of this simple and robust algorithm are reasonably 
fast, taking less than a second to generate the 2D dot correspondences and 3D dot positions for six camera 
views. The 2D dot correspondence calculation is dominated by the time taken to read in the images of 
the six camera views and to locate the 2D dots in each view. Consequently, the extra complexity of more 
ef­.cient stereo matching algorithms does not appear to be justi.ed. 3.3 Frame to frame dot correspondence 
and la­beling We now have a set of unlabeled 3D dot locations for each frame. We need to assign, across 
the entire sequence, consistent labels to the 3D dot locations. We do this by de.ning a reference set 
of dots Dand matching this set to the 3D dot locations given for each frame. We can then describe how 
the reference dots move over time as follows: Let dj2Dbe the neutral location for the reference dot j. 
We de.ne the position of djat frame iby an offset, i.e., dij=dj+~vji (1) Because there are thousands 
of frames and 182 dots in our data Figure 5: Dot variation. Left: Two dots seen from three different 
cameras (the purple dot is occluded in one camera s view). Right: A single dot seen from a single camera 
but in two different frames. set we would like the correspondence computation to be automatic and quite 
ef.cient. To simplify the matching we used a .ducial pattern that separates .ducials of a given color 
as much as possi­ble so that only a small subset of the unlabeled 3D dots need be checked for a best 
match. Unfortunately, simple nearest neighbor matching fails for several reasons: some .ducials occasionally 
dis­appear, some 3D dots may move more than the average distance between 3D dots of the same color, and 
occasionally extraneous 3D dots appear, caused by highlights in the eyes or teeth. Fortunately, neighboring 
.ducials move similarly and we can exploit this fact, modifying the nearest neighbor matching algorithm 
so that it is still ef.cient but also robust. For each frame iwe .rst move the reference dots to the 
loca­tions found in the previous frame. Next, we .nd a (possibly incom­plete) match between the reference 
dots and the 3D dot locations for frame i. We then move each matched reference dot to the loca­tion of 
its corresponding 3D dot. If a reference dot does not have a match we guess a new location for it by 
moving it in the same direction as its neighbors. We then perform a .nal matching step. 3.3.1 Acquiring 
the reference set of dots The cyberware scan was taken with the dots glued onto the face. Since the dots 
are visible in both the geometric and color informa­tion of the scan, we can place the reference dots 
on the cyberware model by manually clicking on the model. We next need to align the reference dots and 
the model with the 3D dot locations found in frame zero. The coordinate system for the cyberware scan 
differs from the one used for the 3D dot locations, but only by a rigid body motion plus a uniform scale. 
We .nd this transform as follows: we .rst hand-align the 3D dots from frame zero with the reference dots 
acquired from the scan, then call the matching routine described in Section 3.3.2 below to .nd the correspondence 
between the 3D dot locations, fi, and the reference dots, di. We use the method de­scribed in [9] to 
.nd the exact transform, T, between the two sets of dots. Finally, we replace the temporary locations 
of the reference dots with di=fi. and use T,1to transform the cyberware model into the coordinate system 
of the video 3D dot locations. 3.3.2 The matching routine The matching routine is run twice per frame. 
We .rst perform a conservative match, move the reference dots (as described below in Section 3.3.3), 
then perform a second, less conservative, match. By moving the reference dots between matches we reduce 
the problem of large 3D dot position displacements. c c d 2 a 2 c  1a 2 1 a 0 b0b 3 d 1 b3d Reference 
dot Connected components Sort and pair 3D dot of edge graph Figure 6: Matching dots. Reference dot 3D 
dot Missing 3D dot Extra 3D dot   Big Small Big Small Big Small epsilon epsilon epsilon epsilon epsilon 
epsilon Figure 7: Examples of extra and missing dots and the effect of different values for E. The matching 
routine can be thought of as a graph problem where an edge between a reference dot and a frame dot indicates 
that the dots are potentially paired (see Figure 6). The matching routine proceeds in several steps; 
.rst, for each reference dot we add an edge for every 3D dot of the same color that is within a given 
distance E. We then search for connected components in the graph that have an equal number of 3D and 
reference dots (most con­nected components will have exactly two dots, one of each type). We sort the 
dots in the vertical dimension of the plane of the face and use the resulting ordering to pair up the 
reference dots with the 3D dot locations (see Figure 6). In the video sequences we captured, the difference 
in the 3D dot positions from frame to frame varied from zero to about 1:5times the average distance separating 
closest dots. To adjust for this, we run the matching routine with several values of Eand pick the run 
that generates the most matches. Different choices of Eproduce different results (see Figure 7): if Eis 
too small we may not .nd matches for 3D dots that have moved a lot. If Eis too large then the connected 
components in the graph will expand to include too many 3D dots. We try approximately .ve distances ranging 
from 0:5to 1:5of the average distance between closest reference dots. If we are doing the second match 
for the frame we add an ad­ditional step to locate matches where a dot may be missing (or ex­tra). We 
take those dots which have not been matched and run the matching routine on them with smaller and smaller 
Evalues. This resolves situations such as the one shown on the right of Figure 7. 3.3.3 Moving the dots 
We move all of the matched reference dots to their new locations then interpolate the locations for the 
remaining, unmatched refer­ence dots by using their nearest, matched neighbors. For each ref­erence dot 
we de.ne a valid set of neighbors using the routine in Section 4.2.1, ignoring the blending values returned 
by the routine. To move an unmatched dot dkwe use a combination of the offsets of all of its valid neighbors 
(refer to Equation 1). Let nkC Dbe the set of neighbor dots for dot dk.Let n^kbe the set of neighbors 
that have a match for the current frame i. Provided n^k=6 i ;, the offset vector for dot dkiis calculated 
as follows: let ~vj= di ,djbe the offset of dot j(recall that djis the initial position j for the reference 
dot j). X i1i ~v=~v kjjn^kjjj di2^ nk j If the dot has no matched neighbors we repeat as necessary, treating 
the moved, unmatched reference dots as matched dots. Eventually, the movements will propagate through 
all of the reference dots.   4 Mesh construction and deformation 4.1 Constructing the mesh To construct 
a mesh we begin with a cyberware scan of the head. Because we later need to align the scan with the 3D 
video dot data, we scanned the head with the .ducials glued on. The resulting scan suffers from four 
problems:  The .uorescent .ducials caused bumps on the mesh.  Several parts of the mesh were not adequately 
scanned, namely, the ears, one side of the nose, the eyes, and under the chin. These were manually corrected. 
 The mesh does not have an opening for the mouth.  The scan has too many polygons.  The bumps caused 
by the .uorescent .ducials were removed by selecting the vertices which were out of place (approximately 
10-30 surrounding each dot) and automatically .nding new locations for them by blending between four 
correct neighbors. Since the scan produces a rectangular grid of vertices we can pick the neighbors to 
blend between in (u;v)space, i.e., the nearest valid neighbors in the positive and negative uand vdirection. 
The polygons at the mouth were split and then .lled with six rows of polygons located slightly behind 
the lips. We map the teeth and tongue onto these polygons when the mouth is open. We reduced the number 
of polygons in the mesh from approxi­mately 460;000to 4800using Hoppe s simpli.cation method [8].  4.2 
Moving the mesh The vertices are moved by a linear combination of the offsets of the nearest dots (refer 
to Equation 1). The linear combination for each vertex vjis expressed as a set of blend coef.cients, 
j, one k P jj for each dot, such that =1(most of the s will be dk2Dkki zero). The new location pjof the 
vertex vjat frame iis then X i ji pj=pj+ kjjdk,dkjj k where pjis the initial location of the vertex vj. 
For most of the vertices the js are a weighted average of the k closest dots. The vertices in the eyes, 
mouth, behind the mouth, and outside of the facial area are treated slightly differently since, for example, 
we do not want the dots on the lower lip in.uencing vertices on the upper part of the lip. Also, although 
we tried to keep the head as still as possible, there is still some residual rigid body motion. We need 
to compensate for this for those vertices that are not directly in.uenced by a dot (e.g., the back of 
the head). We use a two-step process to assign the blend coef.cients to the vertices. We .rst .nd blend 
coef.cients for a grid of points evenly distributed across the face, then use this grid of points to 
Figure 8: Left: The original dots plus the extra dots (in white). The labeling curves are shown in light 
green. Right: The grid of dots. Outline dots are green or blue.  assign blend coef.cients to the vertices. 
This two-step process is helpful because both the .uorescent .ducials and the mesh vertices are unevenly 
distributed across the face, making it dif.cult to get smoothly changing blend coef.cients. The grid 
consists of roughly 1400points, evenly distributed and placed by hand to follow the contours of the face 
(see Figure 8). The points along the nasolabial furrows, nostrils, eyes, and lips are treated slightly 
differently than the other points to avoid blending across features such as the lips. Because we want 
the mesh movement to go to zero outside of the face, we add another set of unmoving dots to the reference 
set. These new dots form a ring around the face (see Figure 8) enclosing all of the reference dots. For 
each frame we determine the rigid body motion of the head (if any) using a subset of those reference 
dots which are relatively stable. This rigid body transformation is then applied to the new dots. We 
label the dots, grid points, and vertices as being above, be­low,or neither with respect to each of the 
eyes and the mouth. Dots which are above a given feature can not be combined with dots which are below 
that same feature (or vice-versa). Labeling is accomplished using three curves, one for each of the eyes 
and one for the mouth. Dots directly above (or below) a curve are labeled as above (or below) that curve. 
Otherwise, they are labeled neither. 4.2.1 Assigning blends to the grid points The algorithm for assigning 
blends to the grid points .rst .nds the closest dots, assigns blends, then .lters to more evenly distribute 
the blends. Finding the ideal set of reference dots to in.uence a grid point is complicated because the 
reference dots are not evenly distributed across the face. The algorithm attempts to .nd two or more 
dots distributed in a rough circle around the given grid point. To do this we both compensate for the 
dot density, by setting the search distance using the two closest dots, and by checking for dots which 
will both pull in the same direction. To .nd the closest dots to the grid point pwe .rst .nd 01and 02, 
the distance to the closest and second closest dot, respectively. Let CDbe the set of dots within 1:861+62distance 
of pwhose Dn2labels do not con.ict with p s label. Next, we check for pairs of dots that are more or 
less in the same direction from pand remove the furthest one. More precisely, let v^ibe the normalized 
vector from pto the dot di2Dnand let v^jbe the normalized vector from pto the dot dj2Dn.If v^1.v^2>0:8then 
remove the furthest of diand djfrom the set Dn. We assign blend values based on the distance of the dots 
from p. If the dot is not in Dnthen its corresponding value is 0.For Figure 9: Masks surrounding important 
facial features. The gradi­ent of a blurred version of this mask is used to orient the low-pass .lters 
used in the dot removal process. the dots in Dnlet li = 1:0. Then the corresponding s are jjdi,pjj li 
i = P (li) di2Dn We next .lter the blend coef.cients for the grid points. For each grid point we .nd 
the closest grid points since the grid points are distributed in a rough grid there will usually be 
4neighboring points using the above routine (replacing the dots with the grid points). We special case 
the outlining grid points; they are only blended with other outlining grid points. The new blend coef.­cients 
are found by taking 0:75of the grid point s blend coef.cients and 0:25of the average of the neighboring 
grid point s coef.cients. More formally, let gi =[0;:::;n]be the vector of blend co­ef.cients for the 
grid point i. Then the new vector g 0is found as ifollows, where Niis the set of neighboring grid points 
for the grid point i: 0:25X g 0 =0:75gi+gj i jjNijj j2Ni We apply this .lter twice to simulate a wide 
low pass .lter. To .nd the blend coef.cients for the vertices of the mesh we .nd the closest grid point 
with the same label as the vertex and copy the blend coef.cients. The only exception to this is the vertices 
for the polygons inside of the mouth. For these vertices we take jof the closest grid point on the top 
lip and 1:0,jof the closest grid point on the bottom lip. The jvalues are 0:8, 0:6, 0:4, 0:25,and 0:1from 
top to bottom of the mouth polygons.  5 Dot removal Before we create the textures, the dots and their 
associated illumi­nation effects have to be removed from the camera images. Inter­re.ection effects are 
surprisingly noticeable because some parts of the face fold dramatically, bringing the re.ective surface 
of some dots into close proximity with the skin. This is a big problem along the naso-labial furrow where 
diffuse interre.ection from the col­ored dots onto the face signi.cantly alters the skin color. First, 
the dot colors are removed from each of the six camera image sequences by substituting skin texture for 
pixels which are covered by colored dots. Next, diffuse interre.ection effects and any remaining color 
casts from stray pixels that have not been prop­erly substituted are removed. The skin texture substitution 
begins by .nding the pixels which correspond to colored dots. The nearest neighbor color classi.er described 
in Section 3.1.1 is used to mark all pixels which have any of the dot colors. A special training set 
is used since in this case false positives are much less detrimental than they are for the dot tracking 
case. Also, there is no need to distinguish between dot colors, only between dot colors and the background 
colors. The training set is created to capture as much of the dot color and the boundary region between 
dots and the background colors as possi­ble.  A dot mask is generated by applying the classi.er to each 
pixel in the image. The mask is grown by a few pixels to account for any remaining pixels which might 
be contaminated by the dot color. The dot mask marks all pixels which must have skin texture substi­tuted. 
The skin texture is broken into low spatial frequency and high frequency components. The low frequency 
components of the skin texture are interpolated by using a directional low pass .lter ori­ented parallel 
to features that might introduce intensity discontinu­ities. This prevents bleeding of colors across 
sharp intensity bound­aries such as the boundary between the lips and the lighter colored regions around 
the mouth. The directionality of the .lter is con­trolled by a two dimensional mask which is the projection 
into the image plane of a three dimensional polygon mask lying on the 3D face model. Because the polygon 
mask is .xed on the 3D mesh, the 2D projection of the polygon mask stays in registration with the texture 
map as the face deforms. All of the important intensity gradients have their own polygon mask: the eyes, 
the eyebrows, the lips, and the naso-labial furrows (see 9). The 2D polygon masks are .lled with white 
and the re­gion of the image outside the masks is .lled with black to create an image. This image is 
low-pass .ltered. The intensity of the result­ing image is used to control how directional the .lter 
is. The .lter is circularly symmetric where the image is black, i.e., far from in­tensity discontinuities, 
and it is very directional where the image is white. The directional .lter is oriented so that its long 
axis is orthogonal to the gradient of this image. The high frequency skin texture is created from a rectangular 
sample of skin texture taken from a part of the face that is free of dots. The skin sample is highpass 
.ltered to eliminate low fre­quency components. At each dot mask pixel location the highpass .ltered 
skin texture is .rst registered to the center of the 2D bound­ing box of the connected dot region and 
then added to the low fre­quency interpolated skin texture. The remaining diffuse interre.ection effects 
are removed by clamping the hue of the skin color to a narrow range determined from the actual skin colors. 
First the pixel values are converted from RGB to HSV space and then any hue outside the legal range is 
clamped to the extremes of the range. Pixels in the eyes and mouth, found using the eye and lip masks 
shown in Figure 9, are left unchanged. Some temporal variation remains in the substituted skin texture 
due to imperfect registration of the high frequency texture from frame to frame. A low pass temporal 
.lter is applied to the dot mask regions in the texture images, because in the texture map space the 
dots are relatively motionless. This temporal .lter effectively eliminates the temporal texture substitution 
artifacts.  6 Creating the texture maps Figure 11 is a .owchart of the texture creation process. We 
create texture maps for every frame of our animation in a four-step pro­cess. The .rst two steps are 
performed only once per mesh. First we de.ne a parameterization of the mesh. Second, using this pa­rameterization, 
we create a geometry map containing a location on the mesh for each texel. Third, for every frame, we 
create six pre­liminary texture maps, one from each camera image, along with weight maps. The weight 
maps indicate the relative quality of the data from the different cameras. Fourth, we take a weighted 
aver­age of these texture maps to make our .nal texture map. We create an initial set of texture coordinates 
for the head by tilting the mesh back 10 degrees to expose the nostrils and pro­jecting the mesh vertices 
onto a cylinder. A texture map generated using this parametrization is shown on the left of Figure 10. 
We specify a set of line pairs and warp the texture coordinates using the technique described by Beier 
and Neely[1]. This parametriza­tion results in the texture map shown on the right of Figure 10. Only 
the front of the head is textured with data from the six video streams. Next we create the geometry map 
containing a mesh location for each texel. A mesh location is a triple (k;j1;j2)specifying a triangle 
kand barycentric coordinates in the triangle (j1, j2, 1,j1,j2). To .nd the triangle identi.er kfor texel 
(u;v)we exhaustively search through the mesh s triangles to .nd the one that contains the texture coordinates 
(u;v).We then set the jistobe the barycentric coordinates of the point (u;v)in the texture coordi­nates 
of the triangle k. When .nding the mesh location for a pixel we already know in which triangles its neighbors 
above and to the left lie. Therefore, we speed our search by .rst searching through these triangles and 
their neighbors. However, the time required for this task is not critical as the geometry map need only 
be created once. Next we create preliminary texture maps for frame fone for each camera. This is a modi.ed 
version of the technique described in [11]. To create the texture map for camera c, we begin by de­forming 
the mesh into its frame fposition. Then, for each texel, we get its mesh location, (k;j1;j2), from the 
geometry map. With the 3D coordinates of triangle k s vertices and the barycentric coor­dinates ji, we 
compute the texel s 3D location t. We transform tby camera c s projection matrix to obtain a location, 
(x;y), on camera c s image plane. We then color the texel with the color from cam­era c s image at (x;y). 
We set the texel s weight to the dot product of the mesh normal at t, n^, with the direction back to 
the camera, ^ d(see Figure 12). Negative values are clamped to zero. Hence, weights are low where the 
camera s view is glancing. However, this weight map is not smooth at triangle boundaries, so we smooth 
it by convolving it with a Gaussian kernel. Last, we merge the six preliminary texture maps. As they 
do not align perfectly, averaging them blurs the texture and loses de­tail. Therefore, we use only the 
texture map of our bottom, center camera for the center 46 % of the .nal texture map. We smoothly transition 
(over 23 pixels) to using a weighted average of each pre­liminary texture map at the sides.  maps for 
each frame We texture the parts of the head not covered by the aforemen­tioned texture maps with the 
captured re.ectance data from our Cy­berware scan, modi.ed in two ways. First, because we replaced the 
mesh s ears with ears from a stock mesh (Section 4.1), we moved the ears in the texture to achieve better 
registration. Second, we set the alpha channel to zero (with a soft edge) in the region of the texture 
for the front of the head. Then we render in two passes to create an image of the head with both texture 
maps applied.  7 Compression 7.1 Principal Components Analysis The geometric and texture map data have 
different statistical char­acteristics and are best compressed in different ways. There is sig­ni.cant 
long-term temporal correlation in the geometric data since similar facial expressions occur throughout 
the sequence. The short term correlation of the texture data is signi.cantly increased over that of the 
raw video footage because in the texture image space the .ducials are essentially motionless. This eliminates 
most of the intensity changes associated with movement and leaves primarily shading changes. Shading 
changes tend to have low spatial fre­quencies and are highly compressible. Compression schemes such as 
MPEG, which can take advantage of short term temporal corre­lation, can exploit this increase in short 
term correlation. For the geometric data, one way to exploit the long term corre­lation is to use principal 
component analysis. If we represent our data set as a matrix A, where frame iof the data maps column 
iof A, then the .rst principal component of Ais TTT max(Au)(Au) (2) u The uwhich maximizes Equation 2 
is the eigenvector associated with the largest eigenvalue of AAT, which is also the value of the maximum. 
Succeeding principal components are de.ned similarly, except that they are required to be orthogonal 
to all preceding prin- T cipal components, i.e., uuj=0for j6i. =The principal com­ponents form an orthonormal 
basis set represented by the matrix U where the columns of Uare the principal components of Aordered 
by eigenvalue size with the most signi.cant principal component in the .rst column of U. i The data in 
the Amatrix can be projected onto the principal component basis as follows: W=UTA Row iof Wis the projection 
of column Aionto the basis vector ui. More precisely, the jth element in row iof Wcorresponds to the 
projection of frame jof the original data onto the ith basis vector. We will call the elements of the 
Wmatrix projection coef.cients. Similarly, Acan be reconstructed exactly from Wby multipli­cation by 
the basis set, i.e., A=UW. The most important property of the principal components for our purposes is 
that they are the best linear basis set for reconstruction in the l2norm sense. For any given matrix 
Uk,where kis the num­ber of columns of the matrix and k<rank(A), the reconstruction error e=jjA,UkUkTAjjF 
2 (3) where jjBjj2is the Frobenius norm de.ned to be F mn XX 2 jjBjj2 =b(4) ijF i=1j=1 will be minimized 
if Ukis the matrix containing the kmost signif­icant principal components of A. We can compress a data 
set Aby quantizing the elements of its corresponding Wand Umatrices and entropy coding them. Since the 
compressed data cannot be reconstructed without the principal component basis vectors both the Wand Umatrices 
have to be compressed. The basis vectors add overhead that is not present with basis sets that can be 
computed independent of the original data set, such as the DCT basis. For data sequences that have no 
particular structure the extra overhead of the basis vectors would probably out-weigh any gain in compression 
ef.ciency. However, for data sets with regular frame to frame structure the residual error for reconstruction 
with the principal component basis vectors can be much smaller than for other bases. This reduction in 
residual error can be great enough to compensate for the overhead bits of the basis vectors. The principal 
components can be computed using the singular value decomposition (SVD) [13]. Ef.cient implementations 
of this algorithm are widely available. The SVD of a matrix Ais A=U VT (5) where the columns of Uare 
the eigenvectors of AAT, the singular values, ai, along the diagonal matrix are the square roots of the 
eigenvalues of AAT, and the columns of Vare the eigenvectors of ATA.The ith column of Uis the ith principal 
component of A. Computing the .rst kleft singular vectors of Ais equivalent to computing the .rst kprincipal 
components. 7.2 Geometric Data The geometric data has the long term temporal coherence proper­ties mentioned 
above since the motion of the face is highly struc­tured. The overhead of the basis vectors for the geometric 
data is .xed because there are only 182.ducials on the face. The maxi­mum number of basis vectors is 
182*3since there are three num­bers, x, y,and z, associated with each .ducial. Consequently, the basis 
vector overhead steadily diminishes as the length of the ani­mation sequence increases. The geometric 
data is mapped to matrix form by taking the 3D offset data for the ith frame and mapping it the ith column 
of the data matrix Ag.The .rst kprincipal components, Ug,of Agare computed and Agis projected into the 
Ugbasis to give the projec­tion coef.cients Wg. There is signi.cant correlation between the columns of 
projec­tion coef.cients because the motion of the dots is relatively smooth over time. We can reduce 
the entropy of the quantized projection coef.cients by temporally predicting the projection coef.cients 
in column ifrom column i,1, i.e., ci =ci,1+fiwhere we encode fi. For our data set, only the projection 
coef.cients associated with the .rst 45 principal components, corresponding to the .rst 45 rows of Wg, 
have signi.cant temporal correlation so only the .rst 45 rows are temporally predicted. The remaining 
rows are entropy coded directly. After the temporal prediction the entropy is reduced by about 20 percent 
(Figure 13). The basis vectors are compressed by choosing a peak error rate and then varying the number 
of quantization levels allocated to each vector based on the standard deviation of the projection coef.cients 
for each vector. We visually examined animation sequences with Wgand Ug compressed at a variety of peak 
error rates and chose a level which resulted in undetectable geometric jitter in reconstructed animation. 
The entropy of Wgfor this error level is 26 Kbits per second and the entropy of Ugis 13 kbits per second 
for a total of 40 kbits per second for all the geometric data. These values were computed for our 3330 
frame animation sequence.  8 Results Figure 16 shows some typical frames from a reconstructed sequence 
of 3D facial expressions. These frames are taken from a 3330 frame 8 7 6 5 4 3 2 1 0 Coefficient index 
Figure 13: Reduction in entropy after temporal prediction. animation in which the actress makes random 
expressions while reading from a script2 . The facial expressions look remarkably life-like. The anima­tion 
sequence is similarly striking. Virtually all evidence of the colored .ducials and diffuse interre.ection 
artifacts is gone, which is surprising considering that in some regions of the face, especially around 
the lips, there is very little of the actress skin visible most of the area is covered by colored .ducials. 
Both the accurate 3D geometry and the accurate face texture contribute to the believability of the reconstructed 
expressions. Oc­clusion contours look correct and the subtle details of face geom­etry that are very 
dif.cult to capture as geometric data show up well in the texture images. Important examples of this 
occur at the nasolabial furrow which runs from just above the nares down to slightly below the lips, 
eyebrows, and eyes. Forehead furrows and wrinkles also are captured. To recreate these features using 
geometric data rather than texture data would require an extremely detailed 3D capture of the face geometry 
and a resulting high poly­gon count in the 3D model. In addition, shading these details prop­erly if 
they were represented as geometry would be dif.cult since it would require computing shadows and possibly 
even diffuse inter­re.ection effects in order to look correct. Subtle shading changes on the smooth parts 
of the skin, most prominent at the cheekbones, are also captured well in the texture images. There are 
still visible artifacts in the animation, some of which are polygonization or shading artifacts, others 
of which arise be­cause of limitations in our current implementation. Some polygonization of the face 
surface is visible, especially along the chin contour, because the front surface of the head con­tains 
only 4500polygons. This is not a limitation of the algorithm we chose this number of polygons because 
we wanted to verify that believable facial animation could be done at polygon resolutions low enough 
to potentially be displayed in real time on inexpensive ( $200) 3D graphics cards3. For .lm or television 
work, where real time rendering is not an issue, the polygon count can be made much higher and the polygonization 
artifacts will disappear. As graphics hardware becomes faster the differential in quality between of.ine 
and online rendered face images will diminish. Several artifacts are simply the result of our current 
implemen­tation. For example, occasionally the edge of the face, the tips of the nares, and the eyebrows 
appear to jitter. This usually oc­curs when dots are lost, either by falling below the minimum size threshold 
or by not being visible to three or more cameras. When a dot is lost the algorithm synthesizes dot position 
data which is 2The rubber cap on the actress head was used to keep her hair out of her face. 3In this 
paper we have not addressed the issue of real time texture decompression and rendering of the face model, 
but we plan to do so in future work Entropy, bits/sample usually incorrect enough that it is visible 
as jitter. More cameras, or better placement of the cameras, would eliminate this problem. However, overall 
the image is extremely stable. In retrospect, a mesh constructed by hand with the correct ge­ometry and 
then .t to the cyberware data [10] would be much sim­pler and possibly reduce some of the polygonization 
artifacts. Another implementation artifact that becomes most visible when the head is viewed near pro.le 
is that the teeth and tongue appear slightly distorted. This is because we do not use correct 3D models 
to represent them. Instead, the texture map of the teeth and tongue is projected onto a sheet of polygons 
stretching between the lips. It is possible that the teeth and tongue could be tracked using more sophisticated 
computer vision techniques and then more correct ge­ometric models could be used. Shading artifacts represent 
an intrinsic limitation of the algo­rithm. The highlights on the eyes and skin remain in .xed positions 
regardless of point of view, and shadowing is .xed at the time the video is captured. However, for many 
applications this should not be a limitation because these artifacts are surprisingly subtle. Most people 
do not notice that the shading is incorrect until it is pointed out to them, and even then frequently 
do not .nd it particularly ob­jectionable. The highlights on the eyes can probably be corrected by building 
a 3D eye model and creating synthetic highlights ap­propriate for the viewing situation. Correcting the 
skin shading and self shadowing artifacts is more dif.cult. The former will require very realistic and 
ef.cient skin re.ectance models while the lat­ter will require signi.cant improvements in rendering performance, 
especially if the shadowing effect of area light sources is to be ade­quately modeled. When both these 
problems are solved then it will no longer be necessary to capture the live video sequence only the 
3D geometric data and skin re.ectance properties will be needed. The compression numbers are quite good. 
Figure 14 shows a single frame from the original sequence, the same frame com­pressed by the MPEG4 codec 
at 460 Kbps and at 260 KBps. All of the images look quite good. The animated sequences also look good, 
with the 260 KBps sequence just beginning to show notice­able compression artifacts. The 260 KBps video 
is well within the bandwidth of single speed CDROM drives. This data rate is proba­bly low enough that 
decompression could be performed in real time in software on the fastest personal computers so there 
is the poten­tial for real time display of the resulting animations. We intend to investigate this possibility 
in future work. There is still room for signi.cant improvement in our compres­sion. A better mesh parameterization 
would signi.cantly reduce the number of bits needed to encode the eyes, which distort signif­icantly 
over time in the texture map space. Also the teeth, inner edges of the lips, and the tongue could potentially 
be tracked over time and at least partially stabilized, resulting in a signi.cant re­duction in bit rate 
for the mouth region. Since these two regions account for the majority of the bit budget, the potential 
for further reduction in bit rate is large.  Conclusion The system produces remarkably lifelike reconstructions 
of facial expressions recorded from live actors performances. The accurate 3D tracking of a large number 
of points on the face results in an accurate 3D model of facial expression. The texture map sequence 
captured simultaneously with the 3D deformation data captures de­tails of expression that would be dif.cult 
to capture any other way. By using the 3D deformation information to register the texture maps from frame 
to frame the variance of the texture map sequence is signi.cantly reduced which increases its compressibility. 
Image quality of 30 frame per second animations, reconstructed at approx­imately 300 by 400 pixels, is 
still good at data rates as low as 240 Kbits per second, and there is signi.cant potential for lowering 
this bit rate even further. Because the bit overhead for the geometric data is low in comparison to the 
texture data one can get a 3D talk­ing head, with all the attendant .exibility, for little more than 
the cost of a conventional video sequence. With the true 3D model of facial expression, the animation 
can be viewed from any angle and placed in a 3D virtual environment, making it much more .exible than 
conventional video.  References [1] BEIER,T., AND NEELY, S. Feature-based image metamor­phosis. In Computer 
Graphics (SIGGRAPH 92 Proceedings) (July 1992), E. E. Catmull, Ed., vol. 26, pp. 35 42. [2] BREGLER,C., 
COVELL,M., AND SLANEY,M. Video rewrite: Driving visual speech with audio. Computer Graph­ics 31, 2 (Aug. 
1997), 353 361. [3] CASSELL,J., PELACHAUD,C., BADLER,N., STEEDMAN, M., ACHORN,B., BECKET,T., DOUVILLE,B., 
PREVOST, S., AND STONE, M. Animated conversation: Rule-based generation of facial expression, gesture 
and spoken intona­tion for multiple conversational agents. Computer Graphics 28, 2 (Aug. 1994), 413 420. 
[4] DECARLO,D., AND METAXAS, D. The integration of op­tical .ow and deformable models with applications 
to human face shape and motion estimation. Proceedings CVPR (1996), 231 238. [5] ESSA,I., AND PENTLAND, 
A. Coding, analysis, interpreta­tion and recognition of facial expressions. IEEE Transactions on Pattern 
Analysis and Machine Intelligence 19, 7 (1997), 757 763. [6] FAUGERAS,O. Three-dimensional computer vision.MIT 
Press, Cambridge, MA, 1993. [7] FISCHLER,M. A., AND BOOLES, R. C. Random sample consensus: A paradigm 
for model .tting with applications to image analysis and automated cartography. Communications of the 
ACM 24, 6 (Aug. 1981), 381 395. [8] HOPPE, H. Progressive meshes. In SIGGRAPH 96 Con­ference Proceedings 
(Aug. 1996), H. Rushmeier, Ed., An­nual Conference Series, ACM SIGGRAPH, Addison Wesley, pp. 99 108. 
held in New Orleans, Louisiana, 04-09 August 1996. [9] HORN, B. K. P. Closed-form solution of absolute 
orienta­tion using unit quaternions. Journal of the Optical Society of America 4, 4 (Apr. 1987). [10] 
LEE,Y., TERZOPOULOS,D., AND WATERS, K. Realistic modeling for facial animation. Computer Graphics 29,2 
(July 1995), 55 62. [11] PIGHIN,F., AUSLANDER,J., LISHINSKI,D., SZELISKI,R., AND SALESIN, D. Realistic 
facial animation using image based 3d morphing. Tech. Report TR-97-01-03, Department of Computer Science 
and Engineering, University of Wash­ington, Seattle, Wa, 1997. [12] SCH¨Pattern Classi.cation: A Uni.ed 
View of URMANN,J. Statistical and Neural Approaches. John Wiley and Sons, Inc., New York, 1996.  [13] 
STRANG. Linear Algebra and its Application. HBJ, 1988. [14] WATERS, K. A muscle model for animating three­dimensional 
facial expression. In Computer Graphics (SIG-GRAPH 87 Proceedings) (July 1987), M. C. Stone, Ed., vol. 
21, pp. 17 24. [15] WILLIAMS, L. Performance-driven facial animation. Com­puter Graphics 24, 2 (Aug. 
1990), 235 242.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280823</article_id>
		<sort_key>67</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[An anthropometric face model using variational techniques]]></title>
		<page_from>67</page_from>
		<page_to>74</page_to>
		<doi_number>10.1145/280814.280823</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280823</url>
		<keywords>
			<kw><![CDATA[anthropometry]]></kw>
			<kw><![CDATA[crowd generation]]></kw>
			<kw><![CDATA[face modeling]]></kw>
			<kw><![CDATA[variational modeling]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP15034371</person_id>
				<author_profile_id><![CDATA[81100499844]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Douglas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[DeCarlo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Pennsylvania, Philadelphia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP77037568</person_id>
				<author_profile_id><![CDATA[81409597622]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dimitris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Metaxas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Pennsylvania, Philadelphia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31096091</person_id>
				<author_profile_id><![CDATA[81100388131]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Matthew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stone]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Pennsylvania, Philadelphia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>617851</ref_obj_id>
				<ref_obj_pid>616029</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[T. Akimoto, Y. Suenaga, and R.Wallace. Automatic creation of 3D facial models. IEEE Computer Graphics and Applications, 13(5):16-22, September 1993.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>924998</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[E Azuola. Erlvr in representation of standard anthlvpometric data by human figure models. PhD thesis, University of Pennsylvania, 1996.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134003</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[T. Beier and S. Neely. Feature-based image metamorphosis. In P~vceedings SIGGRAPH '92, volume 26, pages 35-42, July 1992.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>66134</ref_obj_id>
				<ref_obj_pid>66131</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[F. Bookstein. Principal warps: Thin-plate splines and the decomposition of deformations. IEEE Pattern Analysis and Machine Intelligence, 11 (6):567-585, 1989.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[F. Bookstein. Morphometric Tools for Landmark Data: Geometry and Biloogy. Cambridge University Press, 1991.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[K. Bush and O. Antonyshyn. 3-dimensional facial anthropometry using a lasersurface scanner-validation of the technique. Plastic and reconstructive surgery, 98(2):226-235, August 1996.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122746</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[G. Celniker and D. Gossard. Deformable curve and surface finite elements for free-form shape design. In P~vceedings SIGGRAPH'91, volume 25, pages 257- 266, 1991.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[S. DiPaola. Extending the range of facial types. Journal of Visualization and Computer Animation, 2(4): 129-131, 1991.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[M. Dooley. Anthropometric modeling programs - a survey. IEEE Computer Graphics and Applications, 2:17-25, November 1982.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>151048</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[G. Farin. Curves and Sulfaces for Computer Aided Geometric Design. Academic Press, 1993.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[L. Farkas. Anth~vpometric Facial P~vportions in Medicine. Thomas Books, 1987.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[L. Farkas. Anthlvpometry of the Head and Face. Raven Press, 1994.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[A. Gibbons. Algorithmic Graph Theory. Cambridge University Press, 1985.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[G. Golub and C. Van Loan. Matrix Computations. Johns Hopkins University Press, 1989.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[C. Gordon. 1988 anthlvpometric survey of U.S. Army personnel: methods and summary statistics. United States Army Natick Research, Development and Engineering Center, 1989.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>199410</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[S. Gortler and M. Cohen. Hierarchical and variational geometric modeling with wavelets. In 1995 Symposium on Interactive 3D Graphics, pages 35-42, April 1995.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[M. Grosso, R. Quach, and N. Badler. Anthropometry for computer animated human figures. In N. Magnenat-Thalmann and D. Thalmann, editors, State-ofthe-art in Computer Animation: Proceedings of Computer Animation ' 89, New York, 1989. Springer-Verlag.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166121</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[M. Halstead, M. Kass, and T. DeRose. Efficient, fair interpolation using Catmull-Clark surfaces. In P~vceedings SIGGRAPH '93, volume 27, pages 35- 44, August 1993.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[A. Hrdlicka. Practical anthlvpometry. AMS Press, 1972.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[J. Kolar and E. Salter. Craniofacial Anthlvpometry: Practical Measurement of the Head and Face for Clinical, Surgical and Research Use. Charles C. Thomas Publisher, LTD, 1996.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218407</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Y. Lee, D. Terzopoulos, and K.Waters. Realistic face modeling for animation. In P1vceedings SIGGRAPH '95, pages 55-62, 1995.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74360</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[J. R Lewis. Algorithms for solid noise synthesis. P1vceedings SIGGRAPH'89, 23(3):263-270, 1989.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[N. Magnenat-Thalmann, H. Minh, M. de Angelis, and D. Thalmann. Design, transformation and animation of human faces. The Visual Computer, 5(1/2):32- 39, March 1989.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134035</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[H. Moreton and C. S6quin. Functional optimization for fair surface design. In P1vceedings SIGGRAPH '92, volume 26, pages 167-176, 1992.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[F. Parke. Parameterized models for facial animation. IEEE Computer Graphics and Applications, 2(9):61-68, 1982.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>249651</ref_obj_id>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[F. Parke and K. Waters. Computer Facial Animation. A K Peters, 1996.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[M. Patel and R Willis. FACES: The facial animation construction and editing system. In Emvgraphics '91, 1991.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378524</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[J. Platt and A. Barr. Constraint methods for flexible models. In P~vceedings SIGGRAPH '88, volume 22, pages 279-288, 1988.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>148286</ref_obj_id>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[W. Press, S. Teukolsky, W. Vetterling, and B. Flannery. Numerical Recipes in C: The Art of Scientific Computing. Cambridge University Press, 1992.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[S. Rogers. Personal Identification fi'om Human Remains. Charles C. Thomas Publisher, LTD, 1984.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>176580</ref_obj_id>
				<ref_obj_pid>176579</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[D. Terzopoulos and H. Qin. Dynamic nurbs with geometric constrains for interactive sculpting. ACM Transactions on Graphics, 13(2):103-136, 1994.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>261519</ref_obj_id>
				<ref_obj_pid>261506</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[T. Vetter and T. Poggio. Linear object classes and image synthesis from a single example image. IEEE Pattern Analysis and Machine Intelligence, 19(7):733- 742, 1997.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134033</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[W. Welch and A. Witkin. Variational surface modeling. In P1vceedings SIG- GRAPH '92, volume 26, pages 157-166, 1992.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192216</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[W. Welch and A. Witkin. Free-Form shape design using triangulated surfaces. In P1vceedings SIGGRAPH '94, volume 28, pages 247-256, July 1994.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Copyright &#38;#169;1998 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers or to redistribute to lists, requires specific permission and/or a fee. An Anthropometric 
Face Model using Variational Techniques Douglas DeCarlo, Dimitris Metaxas and Matthew Stone Department 
of Computer and Information Science, University of Pennsylvania  fjjg dmd@gradient dnm@central matthew@linc.cis.upenn.edu 
Abstract anthropometric pro.le of a population characterize the distinctive features of a likely face 
in that population. We describe a system that automatically generates varied geomet-In the second step, 
our system constructs the best surface that ric models of human faces. A collection of random measurements 
satis.es the geometric constraints that a set of measurements im­of the face is generated according to 
anthropometric statistics for poses, using variational modeling [16, 31, 33]. Variational model­likely 
face measurements in a population. These measurements are ing is a framework for building surfaces by 
constrained optimiza­then treated as constraints on a parameterized surface. Variational tion; the output 
surface minimizes a measure of fairness, which in modeling is used to .nd a smooth surface that satis.es 
these con-our case formalizes how much the surface bends and stretches away straints while using a prototype 
shape as a reference. from the kind of shape that faces normally have. Having a fair­ ness measure is 
necessary, since the anthropometric measurements Keywords: face modeling, anthropometry, variational 
modeling, leave the resulting surface underdetermined. Bookstein [4] uses crowd generation this same 
fairness measure as a method of data interpolation for sparse biometric data, supporting its utility 
for determining the ge­ometry of an underdetermined biological shape. Variational mod­1 Introduction 
eling provides a powerful and elegant tool for capturing the com­ monalities in shape among faces along 
with the differences. Its use A hallmark of the diversity and individuality of the people we reduces 
the problem of generating face geometries into the problem encounter in daily life is the range of variation 
in the shape of of generating sets of anthropometric measurements. their faces. A simulation or animation 
that fails to reproduce this The remainder of the paper describes our techniques in more de­diversity 
whether by design or circumstance deprives its char-tail. We begin in Section 1.1 by introducing the 
problem of repre­acters of independent identities. To animate a bustling scene re-senting and specifying 
face geometry. In Section 2, we summarize alistically or to play out an extended virtual interaction 
believably the research from face anthropometry that we draw on; Section 3 requires hundreds of different 
facial geometries, maybe even a dis-describes how random measurements are generated from these re­tinct 
one for each person, as in real life. sults. In Section 4, we describe our use of variational techniques 
It is a monumental challenge to achieve such breadth with ex-to derive natural face geometries that satisfy 
anthropometric mea­isting modeling techniques. One possibility might be to use range surements. We .nish 
in Section 5 with illustrations of the output scanning technology. This involves all the complexities 
of casting of our system. extras for a .lm: with scanning, each new face must be found on a living subject. 
And although scanning permits detailed geometries 1.1 Background and related work to be extracted quickly, 
scanned data frequently includes artifacts that must be touched up by hand. Another alternative is manual 
Human face animation is a complex task requiring modeling and construction of face models, by deforming 
an existing model or rendering not only of face geometry, but also of distinctive facial having an artist 
design one from scratch; this tends to be slow and features (such as skin, hair, and tongue) and their 
motions. Most expensive. research in face modeling in computer graphics has addressed these This paper 
describes a new alternative: a system capable of auto­ latter problems [21, 23, 25, 26]. matically generating 
distinct, plausible face geometries. This sys-Research on human geometry itself falls into two camps, 
both tem constructs a face in two steps. The .rst step is the generation crucially dependent (in different 
ways) on human participation. of a random set of measurements that characterize the face. The The .rst 
approach is to extract geometry automatically from the form and values of these measurements are computed 
according to measurement of a live subject. Lee, et al. [21] use a range scan face anthropometry, the 
science dedicated to the measurement of of a subject, and produce a physics-based model capable of anima­the 
human face. Anthropometric studies like [11, 12] report statis­tion. Akimoto, et al. [1] use front and 
pro.le images of a subject tics on reliable differences in shape across faces within and across to produce 
a model. populations. Random measurements generated according to the The second approach is to facilitate 
manual speci.cation of new face geometry by a user. A certain facility is offered already by commercial 
modelers (though of course their use demands con­siderable artistic skill); several researchers have 
sought to provide higher levels of control. Parke [25] provides parameters which can control the face 
shape; and Magnenat-Thalmann, et al. [23] de­scribe a more comprehensive set of localized deformation 
param­eters. Patel [27] offers an alternative set of parameters similar in scope to [23] but more closely 
tied to the structure of the head. Di-Paola [8] uses a set of localized volumetric deformations, with 
a similar feel to [23] in their effects. Lewis [22] discusses the use of stochastic noise functions as 
a means of deforming natural objects (including faces). In this case, the control maintained by the user 
is limited to noise generation parameters. In contrast, we adopt a different approach: generating new 
face geometries automatically. More so than interactive methods, this approach depends on a precise mathematical 
description of pos­sible face geometries. Many conventional representations of face shape seem inadequate 
for this purpose. For example, the simple scaling parameters used by manual modeling techniques can perform 
useful effects like changing the width of the mouth or the height of the head; but they are unlikely 
to provide suf.cient generality to describe a wide sampling of face geometries. Meanwhile, for models 
based on principal components analysis (PCA) an alternative representation derived from work in face 
recognition [32] the opposite problem is likely. PCA describes a face shape as a weighted sum of an orthogonal 
basis of 3D shapes (called principal components). This basis is constructed from a large bank of examples 
that have been placed in mutual correspon­dence. (This correspondence is very much like that required 
for image morphing [3]; establishing it is a considerable task, but not one that has evaded automation 
[32].) PCA typically allows faces nearly identical to those in the bank to be accurately represented 
by weighting a truncated basis that only includes a few hundred of the most signi.cant components. However, 
because components are individually complex and com­bined simply by addition, alternative weightings 
could easily en­code implausible face shapes. Identifying which basis weights are reasonable is just 
the original problem (of characterizing possible faces) in a different guise. Bookstein [5] describes 
this problem in terms of latent variables, and notes that principal components of­ten bear little resemblance 
to the underlying interdependent struc­ture of biological forms. (In other words, it is quite dif.cult 
to extract non-linear dependencies between different shape aspects using a linear model like PCA.) At 
the same time, there is no guar­antee that faces considerably outside the example set will be ap­proximated 
well at all. We therefore adopt a representation of face shape based on con­strained optimization. The 
constraints generated as described in Section 3 are based on the anthropometric studies of the face of 
[11, 12, 20] described in the next section; we avoid the dif.culty of learning possible geometries since 
these studies identify the range of variation in real faces. The constraint optimization, as described 
in Section 4, is accomplished by variational surface modeling.  2 Face Anthropometry Anthropometry is 
the biological science of human body measure­ment. Anthropometric data informs a range of enterprises 
that de­pend on knowledge of the distribution of measurements across hu­man populations. For example, 
in human-factors analysis, a known range for human measurements can help guide the design of prod­ucts 
to .t most people [9]; in medicine, quantitative comparison of anthropometric data with patients measurements 
before and af­ter surgery furthers planning and assessment of plastic and recon­structive surgery [12]; 
in forensic anthropology, conjectures about likely measurements, derived from anthropometry, .gure in 
the de­termination of individuals appearance from their remains [12, 30]; and in the recovery of missing 
children, by aging their appearance taken from photographs [12]. This paper describes a similar use of 
anthropometry in the construction of face models for computer graphics applications.1 1An alternative 
source of such information might come from morpho­metrics [5], the study of the overall shape of biological 
forms, their devel­opment, and the interrelations of different aspects of their geometry. Mor­phometric 
analyses also provide detailed characterizations of the variability in the shape of faces. In order to 
develop useful statistics from anthropometric mea­surements, the measurements are made in a strictly 
de.ned way [19]. The rest of this section outlines one popular regime of such measurements and the information 
available from analyses of the resulting data. This provides an overview .rst of the anthropomet­ric 
structure that our model embodies and then of the statistical results our model exploits. Anthropometric 
evaluation begins with the identi.cation of par­ticular locations on a subject, called landmark points, 
de.ned in terms of visible or palpable features (skin or bone) on the sub­ject. A series of measurements 
between these landmarks is then taken using carefully speci.ed procedures and measuring instru­ments 
(such as calipers, levels and measuring tape). As a result, repeated measurements of the same individual 
(taken a few days apart) are very reliable, and measurements of different individuals can be successfully 
compared. Farkas [12] describes a widely used set of measurements for describing the human face. A large 
amount of anthropometric data using this system is available [11, 12]. The system uses a total of 47 
landmark points to describe the face; Figure 1 illustrates many of them. The landmarks are typically 
identi.ed by abbreviations of corresponding anatomical terms. For example, the inner corner of the eye 
is en for endocanthion, while the top of the .ap of cartilage (the tragus) in front of the ear is t for 
tragion. Two of the landmarks determine a canonical horizontal orienta­tion for the head. The horizontal 
plane is determined by the two lines (on either side of the head) connecting the landmark t to the landmark 
or (for orbitale), the lowest point of the eye socket on the skull. In measurement, anthropometrists 
actually align the head to this horizontal, in what is known as Frankfurt horizontal (FH) po­sition [12, 
20], so that measurements can be made easily and ac­curately with respect to this coordinate system. 
In addition to this, a vertical mid-line axis is de.ned by the landmarks n (for nasion), a skull feature 
roughly between the eyebrows; sn (for subnasale) the center point where the nose meets the upper lip; 
and gn (for gnathion), the lowest point on the chin. v . Figure 1: Anthropometric landmarks on the face 
[12] Farkas s inventory includes the .ve types of facial measure­ments described below and illustrated 
in Figure 2: . the shortest distance between two landmarks. An example is en-ex, the distance between 
the landmarks at the corners of the eye the axial distance between two landmarks the distance measured 
along one of the axes of the canonical coordinate system, with the head in FH position. An example is 
v-tr, the vertical distance (height difference) between the top of the head (v for vertex) and hairline 
(tr for trichion). the tangential distance between two landmarks the distance measured along a prescribed 
path on the surface of the face. An example is ch-t, the surface distance from the corner of the mouth 
(ch for cheilion) to the tragus. . the angle of inclination between two landmarks with respect to one 
of the canonical axes. An example is the inclination of the ear axis with respect to the vertical. the 
angle between locations, such as the mentocervical angle (the angle at the chin). We must represent measurements 
of each of these types to apply Farkas s anthropometry in creating models for graphics. Figure 2: Example 
anthropometric measurements [12] Farkas describes a total of 132 measurements on the face and head. Some 
of the measurements are paired, when there is a corre­sponding measurement on the left and right side 
of the face. Until recently, the measurement process could only be carried out by experienced anthropometrists 
by hand. However, recent work has investigated 3-D range scanners as an alternative to manual mea­surement 
[6, 12, 20]. Systematic collection of anthropometric measurements has made possible a variety of statistical 
investigations of groups of subjects. Subjects have been grouped on the basis of gender, race, age, attractiveness 
or the presence of a physical syndrome. Means and variances for the measurements within a group, tabu­lated 
in [12, 15], effectively provide a set of measurements which captures virtually all of the variation 
that can occur in the group. In addition to statistics on measurements, statistics on the pro­portions 
between measurements have also been derived. The de­scription of the human form by proportions goes back 
to D¨urer and da Vinci; anthropometrists have found that proportions give useful information about the 
correlations between features, and can serve as more reliable indicators of group membership than can 
simple measurements [11]. Many facial proportions have been found to show statistically signi.cant differences 
across population groups [19]. These proportions are averaged over a particular population group, and 
means and variances are provided in [11].  3 Generating measurements The rich descriptions of human 
geometry developed in anthropom­etry provide an invaluable resource for human modeling in com­puter graphics. 
This goes for artists as well as automatic systems: Parke and Waters [26] describe the importance of 
having a set of conformation guidelines for facial shape, which draw from artis­tic rules of face design. 
These guidelines provide qualitative in­formation about the shape and proportion of faces, respecting 
the quantitative information found in anthropometric measurements. In using such descriptions, automatic 
systems immediately con­front the problem of bringing a model into correspondence with a desired set 
of measurements. A widely-used approach is to de­sign a model whose degrees of freedom can be directly 
speci.ed by anthropometric measurements. For example, in the early visualiza­tion frameworks for human 
factors engineering surveyed in [9] where anthropometric data .rst .gured in graphics articulated humans 
were made to exhibit speci.ed body measurements by rigidly scaling each component of the articulation. 
Grosso, et al. [17] describe a similar model, but scale physical characteristics (such as mass) as well, 
to produce a model suitable for dynamic simulation and animation. Azuola [2] builds on Grosso s work, 
and generates random sets of (axis-aligned distance) measurements us­ing covariance information (but 
not proportions). The purpose of this generation is to produce a fairly small sampling of differently 
sized people for human factors analysis. Our work represents a departure in that we use anthropometric 
data to constrain the degrees of freedom of the model indirectly (as described in Section 4). This is 
a must for the diverse, abstract and interrelated measurements of face anthropometry. The .exibility 
of generating measurements as constraints offers additional bene.ts. In particular, it allows statistics 
about proportions to be taken into account as precisely as possible. This section describes how our system 
uses published facial measurement and proportion statistics [11, 12] to generate ran­dom sets of measurements. 
The generated measurements both re­spect a given population distribution, and thanks to the use of proportions 
produce a believable face. 3.1 The need for proportions Start with a given population, whose anthropometric 
measurements are tabulated for mean and standard deviation (we later use the measurements from [12]). 
We can assume that the measurements are given by a Gaussian normal distribution, as corroborated by sta­tistical 
tests on the raw data [12]. This gives a naive algorithm for deriving a set of measurements generate 
each measurement inde­pendently as if sampled from the normal distribution with its (es­timated) mean 
and variance. Such random values are easily com­puted [29]; then, given the constraint-based framework 
we use, a shape can be generated to .t the resulting suite of measurements as long as the measurements 
are geometrically consistent. Mere geometric consistency of measurements is no guarantee of the reasonable 
appearance of the resulting face shape, however. Anthropometric measurements are not independent. On 
the face, one striking illustration comes from the inclinations of the pro.le, which are highly intercorrelated. 
In the population described in [11], the inclinations to the front of the chin from under the nose (sn-pg) 
and from the lower lip (li-pg) take a wide range of values, but, despite the many curves in this part 
of the face, tend to agree very closely. Published proportions provide the best available resource to 
model correlations between measurements such as these. For ex­ample, [11] tabulates the mean and variance 
for statistically signi.­cant ratios between anthropometric measurements for a population of young North 
American Caucasian men and women. Given a cal­culated value for one measurement, the proportion allows 
the other measurement to be determined using a random value from the esti­mated distribution of the proportion. 
Since the proportion re.ects a correlation between these values, the resulting pair of measure­ments 
is more representative of the population than the two mea­surements would be if generated independently. 
With many measurements come many useful proportions, but each value will be calculated only once. We 
must .nd the propor­tions that provide the most evidence about the distribution. The next section describes 
the algorithm we use to do that. It assumes that proportions can be applied in either direction (by approximat­ing 
the distribution for the inverse proportion) and that we are gen­erating a set of measurements all of 
which are related by propor­tions. (We can split the measurements into groups before applying this algorithm.) 
The algorithm also assumes that we are given a .xed initial measurement (or measurements) in this set 
from which other measurements could be generated. If we are generating a ran­ dom face, the choice of 
which initial measurement to use is up in the most constrained features to determine the remaining features 
the air. We therefore .nd the best calculation scheme for each pos-via proportionality relationships. 
We can modify Prim s algorithm sible initial measurement, and then use the best of those. Random values 
for this initial measurement are generated by sampling its distribution. Thereafter, randomly generated 
proportions are used to generate the remaining dependent measurements. The same algorithm could also 
be used to .ll in measurements speci.ed by a user (as a rough guide of the kind of face needed) or selected 
to be representative of an extreme in the population (for use in human-factors analysis). In this case, 
the algorithm gives a way of generating a plausible, random variation on this given information.  3.2 
An algorithm for proportions Given base measurements, our goal is to .nd the best way to use an inventory 
of proportions to calculate dependent measurements. We can describe this problem more precisely by viewing 
measure­ments as vertices and proportions as edges in a graph. Figure 3(a) shows a portion of this graph, 
given the measurements and propor­tions from [11, 12] (some edge labels are omitted for the sake of readability). 
The presence of cycles in this graph exhibits the need to select proportions. A particular method for 
calculating measure­ments using proportions can be represented as a branching in this graph an acyclic 
directed graph in which each vertex has at most one incident edge. The edge e from s to d in this branching 
indi­cates that d is calculated by proportion e from s. By assumption, we will require this branching 
to span the graph (this means adding dummy edges connecting multiple base measurements). An exam­ple 
branching is illustrated in Figure 3(b), and contains a single base measurement (the vertex marked with 
a double circle). ex-ex / t-t n-sto / ex-ex (a) (b) Figure 3: Interpreting measurements and proportions 
as a graph (a); Example branching used to compute measurements (b) The algorithm associates each vertex 
v in the branching with a mean µv and variance s2 v . The variance is an indication of the precision 
of the statistical information applied in generating the measurement at v from given information. The 
smaller svµv, the hh more constrained the measurement. We take svµv as the weight of d. For base measurements, 
sv is simply the standard deviation of the measurement. Thereafter, if an edge connects s to d with a 
proportion with mean µe and standard deviation se, and s has mean µs and standard deviation ss, then 
the induced distribution at d is characterized by: µd µsµe s22s22s2 s2s2  e dd dµseµes es (This assumes 
proportions and measurements are independent and Gaussian.) Note that the weight of d is always larger 
than the weight of s this means the precision of the information concern­ing the distribution decreases 
as we go deeper into the branching. Our goal in selecting proportions is to derive a branching TM which 
assigns a minimum total weight to its vertices. This allows for minimum spanning tree to solve this problem. 
Our algorithm maintains a subtree T of some optimal branching. Initially, the subtree contains just the 
root for the initial measurement. At sub­sequent stages, each vertex is associated with the least weight 
in­duced by any edge running from the branching to it. The algorithm incorporates the vertex v whose 
weight is the least into the tree, by the appropriate new edge e. As with Prim s algorithm (c.f. [13]), 
the argument that this al­gorithm works ensures inductively that if T is a subtree of some optimal branching 
TM, then so is Te. If e is not an edge in TM, 0 d then TM contains some other directed path to v, ending 
with a dif­ferent edge e. This path starts at the root of T , so it must at some point leave T . Because 
e was chosen with minimum weight and weights increase along paths, in fact the path must leave T at e; 
since the algorithm chose e, e and einduce the same weight for v. 0 b i w 0 d 0 The inductive property 
is now established, sinceTMee is an optimal branching of which T is a subtree. 4 Variational Modeling 
Using the method outlined in Section 3, we generate complete sets of anthropometric measurements in Farkas 
s system. These con­straints describe the geometry of the face in great detail, but they by no means 
specify a unique geometry for the face surface. For example, Farkas s measurements are relatively silent 
about the dis­tribution of curvature over the face the particular measurement that speci.es the angle 
formed at the tip of the chin (the mentocer­vical angle; as in Figure 2), does not actually specify how 
sharply curved the chin is. What is needed then, intuitively, is a mecha­nism for generating a shape 
that shares the important properties of a typical face, as far as possible, but still respects a given 
set of an­thropometric measurements. This intuition allows the problem of building an anthropometric 
face model to be cast as a constrained optimization problem anthropometric measurements are treated as 
constraints, and the remainder of the face is determined by opti­mizing a surface objective function. 
This characterization allows us to apply variational modeling techniques [7, 16, 18, 24, 31, 33, 34]. 
This section brie.y introduces variational modeling, and de­scribes how we adapt existing variational 
modeling techniques to develop the anthropometric face model. Our approach to varia­tional modeling greatly 
resembles the framework in [33]; a key difference is that we perform most of the variational computation 
in advance and share results across different face generation runs. This amortization of computational 
cost makes it feasible to con­struct larger models subject to many constraints. However, it re­quires 
careful formulation of constraints and algorithms to exploit the constancy of the face model and its 
inventory of constraints. As described in Section 4.1, we begin by specifying a space of possible face 
geometries using a parametric surface suv, and b e w locating the landmark points on the surface. We 
use a B-spline surface [10] to represent s. This surface is speci.ed by a control mesh, where the mesh 
degrees of freedom are collected into a vec­tor p. A particular instantiation pof p provides a prototype 
shape, a reference geometry that epitomizes the kind of shape faces have. b e w 00 Both suvand pare designed 
by hand, but the same parame­terized surface and prototype shape are used to model any set of anthropometric 
measurements. Given this shape representation, the task of the face modeling system is to allow a given 
set of anthropometric measurements m to be used as degrees of freedom for s, in place of p. It does so 
in two logical steps: (1), expressing m as constraints on p in terms of the landmark points as described 
in Section 4.2; and (2), using vari­ational techniques as described in Section 4.3 through Section 4.5 
to .nd a surface that satis.es the constraints and which minimizes bending and stretching away from the 
prototype face shape. 4.1 Surface representation We choose a B-spline surface as a shape representation 
because of the demands both of anthropometric modeling and variational techniques. Our shape must be 
smooth, must permit evaluation of our constraints, and must have surface points and tangent vectors that 
are de.ned as linear combinations of its control mesh points. This scheme meets all of these requirements. 
The speci.cation of suvinvolved the manual construction of b e w a B-spline control mesh for the face, 
shown in Figure 4. The mesh is a tube with openings at the mouth and neck; the geometry fol­lows an available 
polygonal face model and (as required for ac­curate variational modeling) is parameterized to avoid excessive 
distortion of uvpatches.  b e w Figure 4: The prototype face model Anthropometric landmarks are assigned 
.xed locations on the surface in uvparameter space; some are also associated with b e w constraints that 
enforce their .xed geometric interpretations. For example, in the case of the v landmark, which represents 
the top of the head, we ensure that the tangent to the surface at the point representing the landmark 
is in fact horizontal. We likewise add constraints to keep the model in FH position, so that the horizon­tal 
axis of the model is consistent with the axis by which land­marks are identi.ed (and measurements taken). 
These constraints together constitute a set of base constraints which must be satis.ed to apply any anthropometric 
measurement. Further constraints are then added to the model one for each measurement. 4.2 Surface constraints 
Our framework derives a shape by applying both linear and non­linear constraints. The linear constraints 
are derived from axial distance anthropometric measurements and the base constraints on the model; both 
can be represented as a linear function of the de­grees of freedom of the model, p. A matrix A describes 
how the values of all linear constraints are calculated, while a vector b en­codes the intended values 
for those measurements. Thus solutions to these constraints satisfy: Ap b (1) e Because A depends only 
on the types of constraint measurements, A can be solved in advance; then values of p can be computed 
directly from b given particular measurements m. Many of the constraints are non-linear, however. Each 
non­linear constraint is associated with a positive function measuring how far the surface is from the 
correct measurement. These func­tions are summed to give an overall penalty functionP so that non­linear 
constraints impose the equation: P p0 (2) bw r bwe (P p0 for all p). The remainder of this section describes 
the penalty functions associated with each type of measurement con­straint. The shortest distance measurement 
constrains the points xi and x j at a distance r apart using the penalty:  b e we ih i h i e Pdistxix 
jxi x jr 2 (3) The tangential distance constraint, which speci.es the length of a surface curve to be 
r, is approximated using the chord-length approximation of a curve [10] using the points x1xn: n b e 
 e we x n ki p k i b e w Parclenx1xn. 1 xi xi1r(4) i1 The points xi all lie on a predetermined curve 
speci.ed in uv­space (using a B-spline), and are adaptively sampled as to achieve a good estimate of 
the arc length using the chord-length approxi­mation. The inclination measurement constraint .xes a vector 
v at an angle . to a .xed axis a:  bweb i nb e ww 2 Pinclvv Rota.2 (5) Using the rotation Rot, the axis 
a is aligned with the goal direc­tion. v can be the direction between two points on the surface, as well 
as a surface tangent vector. The angle measurement constraint positions the vectors v1 and v2 to be separated 
by the angle .. It is treated as two independent inclination constraints: v2 .2Pangle1 v1 v 1 Rot (6) 
 b w e b i b e i wwww Pangle2 v2v 2 Rotv 1. 4.3 Fairing A fair surface can be constructed by minimizing 
an objective func­tion Es. We will be using the thin-plate functional [7, 18, 33] bw which measures the 
bending of the surface s. It includes the thin­plate term Ep to measure bending, and a membrane termEm 
which ensures the approximation does not become inaccurate: Ep ssuusuu 2suv suv svvsvvdudv  b wwee Z 
b mm dd mm dd m w m 2 w e Emssu su 2susv svsvdu dv where the subscripts on s denote parametric differentiation. 
The overall fairness of the surface is determined by combining these terms together using weights a and 
ß (where typically a is just large enough to prevent approximation error): bwebwdbw (7) EsaEmsßEps(8) 
For linear surface representation schemes (including B-splines), the objective function in (8) can be 
evaluated exactly as a quadratic form 12 pHp [18, 33], where H is determined based on the surface representation 
scheme; the construction for B-splines is given in [33]. Due to the local re.nement property of B-splines,H 
is sparse. The objective function can also be measured with respect to the prototype shape p[33], so 
that the minimization is performed with respect to pp, resulting in 1 ppHpp. The use of b i 0 w 0 b 
i 0 wb i 0 w 2 a prototype shape instructs the fairing process to ignore expected regions of sharp curvature, 
such as the ears and nose on the face. Given H, the problem of fairing given purely linear constraints 
as in (1) is reduced to the following linearly constrained quadratic optimization problem [18, 33]: 1 
min ppHppsubject to Ap b (9) h b i 0 wb i 0 w h e p 2 4.4 Fairing with constraints There are a number 
of approaches for solving the constrained minimization problem in (9) including Lagrange multipliers 
and penalty methods [33] and null-space projection [18], each of which transform the problem to a unconstrained 
problem. The Lagrange multiplier y yields the unconstrained minimiza­tion: 1 min ppHppAp by(10) h b i 
0 wb i 0 wdb i w h py 2 At the minimum, the partial derivatives of the bracketed terms van­ish. Differentiation 
leads to the linear system: HAp Hp  e 0 A 0yb Solving such a system requires selecting a technique 
that is mathematically sound and computationally feasible. For example, interactive modeling, with varying 
constraints and response time demands, requires the use of iterative solution methods, such as the conjugate 
gradient technique [16, 34]. However, we can solve this system without iteration, using a sparse LU decomposition 
tech­nique [14]; producing the decomposition takes On2time given a  bw o bw (11) Onsparse nn system. 
This technique is applicable because the set of constraints is hand-constructed, so we can guarantee 
that the constraint matrix A contains no dependent rows, and hence that the LU decomposition is well 
de.ned. It is feasible because the con­trol mesh topology and the constraint matrix are unchanging, so 
that only one decomposition ever needs to be generated. Finding solutions is then quite ef.cient. In 
general, solving a system given an LU decomposition takes On2time. However, we have found bwbw that the 
LU decomposition is roughly Onsparse given our con­straints. (This is not too surprising given that the 
each constraint involves only a few points on the surface; note that an LU decom­position can be sparse 
even if the actual inverse is dense.) This means that, in practice, solution steps require roughly linear 
time. 4.5 Non-linear constraints As described in Section 4.2, the non-linear constraints are speci.ed 
using the penalty function P p. Since this function is positive, it is simply added into the minimization 
(10) [28, 33]. The extended 0 i bwbw h 0 linear system (11) has Hp.P p.p in place of Hp. Due to the non-linearity 
of P , this system must be solved iteratively. (By contrast, Section 4.2 described a non-iterative method 
for solving the linear constraints.) At iteration i, we determine Ci to be used in place of .P p.p  
i bw h as: .P pi1 Ci Ciµi (12) e n i b n w 1 .p with C0 0. The scalar value µ is a positive weight 
(analogous to a time-step in ODE integration), determined using an adaptive method such as step-doubling 
(for ODE solution) [29]. This results in the iterative linear system: HA pi HpCi (13) e     e 0 
d A 0yb where p0 is the solution corresponding to (11). Note that we still exploit the LU decomposition 
to allow steps to be solved quickly and exactly; this technique is stabler and faster to converge than 
the combination of a conjugate gradient technique with the penalty method. We experimented with linearizations 
of some of the non­linear constraints (and added them into A), but found little gain in ef.ciency, and 
decreased stability in solving. In practice, the simultaneous use of all anthropometric con­straints 
will lead to con.ict. For example, some measurements lead to linearly dependent constraints; they are 
easily identi.ed by in­spection, and culled to keep A invertible. Similarly, when multiple measurements 
place non-linear constraints on similar features of nearby points on the model (without providing additional 
variation in shape), including all can introduce a source of geometric incon­sistency and prevent the 
convergence of C . Our constraint set was selected by following a strategy of including only those constraints 
with the most locally con.ning de.nitions (i.e. constraints which affected fewer facial locations or 
more proximate facial locations were favored).  5 Results and discussion Sample face models derived 
using this technique are shown in Fig­ure 5. To produce the measurements for these models, we ran the 
generation algorithm described in Section 3 on the measurements from [12] and the proportions from [11] 
for North American Cau­casian young adult men and women. Faces for the random mea­surements were realized 
by applying the variational framework to a B-spline mesh (a grid 32 by 32) so as to satisfy the base 
constraints (a total of 15) and 65 measurements that give good coverage both of the shape of the face 
and of the kinds of measurements used in Farkas s system. There were a total of 120 proportions used 
as input to the algorithm in Section 3.2. Producing the LU decomposition used for all these examples 
in­volved a one-time cost of roughly 3 minutes on an SGI 175 MHz R10000. Faces typically found their 
rough shape within 50 itera­tions; our illustrations were allowed to run for up to 200 iterations to 
ensure convergence to millimeter accuracy, resulting in runs that took about 1 minute for each face. 
Models were rendered using RenderMan. Individual variation across the example males and females in Figure 
5 encompass a range of features; for example, clear differ­ences are found in the length and width of 
the nose and mouth, the inclinations of forehead and nose, as well as the overall shape of the face. 
At the same time, traits that distinguish men and women such as the angle at the chin, the slope of the 
eyes and the height of the lower face (particularly at the jaw) vary systematically and correctly (based 
on qualitative comparisons with the anthropomet­ric data). Examining the variation within a population 
group, the thirty generated males in Figure 6 exhibit the expected range of geometric variation. In order 
to quantify this comparison, the proportion-based mea­surement generation algorithm from Section 3.2 
was validated by generating a large number of measurement sets, and comparing the resulting measurement 
distributions to the published .gures from the corresponding population groups. On average, the means 
dif­fered by about 1% (with a maximum deviation of 4 5%) well below the differences in means between 
population groups. The standard deviations agreed comparably, where the generated mea­surements had standard 
deviations that range from being 5% lower to 20% higher than the published values. While this validation 
guarantees the plausibility of measurements on the generated face models, data is unfortunately not available 
for comparing the en­tire geometry (this would require having, for example, a set of measurements of 
an individual along with a corresponding range scan). One would not expect such a comparison to precisely 
agree anyway, as the prototype shape has a measurable effect on the re­sulting geometry. However, this 
effect decreases with the use of additional measurements, which suggests the need to search out additional 
data on face geometry (morphometrics [5] seems to be a good starting point). Despite the many changes, 
a single prototype shape was used for all examples. This gives the models commonalities in shape where 
anthropometric data is silent. Further, all the faces use the same texture so as not to exaggerate their 
differences (having a variety of textures would of course produce nicer results, but would be overlooking 
the main point of this work). The ears remain coarsely modeled (partly as a result of scarcity of measurements 
within the ear). 6 Conclusions This paper has described a two step procedure for generating novel face 
geometries. The .rst step produces a plausible set of con­straints on the geometry using anthropometric 
statistics; the sec­ond derives a surface that satis.es the constraints using varia­tional modeling. 
This fruitful combination of techniques offers broader lessons for modeling: in particular, ways to scale 
up vari­ational modeling a technique previously restricted to modeling frameworks that have seen limited 
use to surface .tting tasks for constrained classes of shapes, and ways to apply anthropometric proportions 
long valued by artists and scientists alike in graph­ics model generation. Of course, our models must 
ultimately be more richly repre­sented. Possible extensions might apply variational techniques to construct 
the face surface and the interior skull simultaneously; this would form the basis of a face animation 
model as in [21]. Simi­larly, landmarks on the face could be used to drive texture synthe­sis, deriving 
distinct but plausible patterns of skin and hair. In the meantime, our work already suggests new computational 
approaches for tasks that rely on anthropometric results, like foren­sic anthropology, plastic surgery 
planning, and child aging. It could also .gure in a user interface for editing face models, by allowing 
features to be edited while related features systematically changed preserving natural proportions or 
ensuring that faces re­spect anthropometric properties common to their population group. Both tasks underscore 
the importance of continuing to gather and analyze anthropometric data of diverse human populations. 
 Acknowledgements We would like to thank Will Welch, Nick Foster, Michael Collins, Max Mintz, Michael 
Gleicher, Scott King, Nathan Loofbourrow and Charles Loop for their helpful com­ments and discussion. 
This research is partially supported by ONR-YIP grant K-5­55043/3916-1552793; ONR DURIP N0001497-1-0396 
and N00014-97-1-0385; NSF IRI 95-04372; NSF Career Award grant 9624604; NASA-96-OLMSA-01-147; NIST grant 
60NANB7D0058; and ARO grant DAAH-04-96-1-007.  References [1] T. Akimoto, Y. Suenaga, and R.Wallace. 
Automatic creation of 3D facial mod­els. IEEE Computer Graphics and Applications, 13(5):16 22, September 
1993. [2] F. Azuola. Error in representation of standard anthropometric data by human .gure models. PhD 
thesis, University of Pennsylvania, 1996. [3] T. Beier and S. Neely. Feature-based image metamorphosis. 
In Proceedings SIGGRAPH 92, volume 26, pages 35 42, July 1992. [4] F. Bookstein. Principal warps: Thin-plate 
splines and the decomposition of deformations. IEEE Pattern Analysis and Machine Intelligence, 11(6):567 
585, 1989. [5] F. Bookstein. Morphometric Tools for Landmark Data: Geometry and Biloogy. Cambridge University 
Press, 1991. [6] K. Bush and O. Antonyshyn. 3-dimensional facial anthropometry using a laser­surface 
scanner validation of the technique. Plastic and reconstructive surgery, 98(2):226 235, August 1996. 
[7] G. Celniker and D. Gossard. Deformable curve and surface .nite elements for free-form shape design. 
In Proceedings SIGGRAPH 91, volume 25, pages 257 266, 1991. [8] S. DiPaola. Extending the range of facial 
types. Journal of Visualization and Computer Animation, 2(4):129 131, 1991. [9] M. Dooley. Anthropometric 
modeling programs a survey. IEEE Computer Graphics and Applications, 2:17 25, November 1982. [10] G. 
Farin. Curves and Surfaces for Computer Aided Geometric Design. Aca­demic Press, 1993. [11] L. Farkas. 
Anthropometric Facial Proportions in Medicine. Thomas Books, 1987. [12] L. Farkas. Anthropometry of the 
Head and Face. Raven Press, 1994. [13] A. Gibbons. Algorithmic Graph Theory. Cambridge University Press, 
1985. [14] G. Golub and C. Van Loan. Matrix Computations. Johns Hopkins University Press, 1989. [15] 
C. Gordon. 1988 anthropometric survey of U.S. Army personnel: methods and summary statistics. United 
States Army Natick Research, Development and Engineering Center, 1989. [16] S. Gortler and M. Cohen. 
Hierarchical and variational geometric modeling with wavelets. In 1995 Symposium on Interactive 3D Graphics, 
pages 35 42, April 1995. [17] M. Grosso, R. Quach, and N. Badler. Anthropometry for computer animated 
human .gures. In N. Magnenat-Thalmann and D. Thalmann, editors, State-of­the-art in Computer Animation: 
Proceedings of Computer Animation 89, New York, 1989. Springer-Verlag. [18] M. Halstead, M. Kass, and 
T. DeRose. Ef.cient, fair interpolation using Catmull-Clark surfaces. In Proceedings SIGGRAPH 93, volume 
27, pages 35 44, August 1993. [19] A. Hrdlicka. Practical anthropometry. AMS Press, 1972. [20] J. Kolar 
and E. Salter. Craniofacial Anthropometry: Practical Measurement of the Head and Face for Clinical, Surgical 
and Research Use. Charles C. Thomas Publisher, LTD, 1996. [21] Y. Lee, D. Terzopoulos, and K.Waters. 
Realistic face modeling for animation. In Proceedings SIGGRAPH 95, pages 55 62, 1995. [22] J. P. Lewis. 
Algorithms for solid noise synthesis. Proceedings SIGGRAPH 89, 23(3):263 270, 1989. [23] N. Magnenat-Thalmann, 
H. Minh, M. de Angelis, and D. Thalmann. Design, transformation and animation of human faces. The Visual 
Computer, 5(1/2):32 39, March 1989. [24] H. Moreton and C. S´equin. Functional optimization for fair 
surface design. In Proceedings SIGGRAPH 92, volume 26, pages 167 176, 1992. [25] F. Parke. Parameterized 
models for facial animation. IEEE Computer Graphics and Applications, 2(9):61 68, 1982. [26] F. Parke 
and K. Waters. Computer Facial Animation. A K Peters, 1996. [27] M. Patel and P. Willis. FACES: The facial 
animation construction and editing system. In Eurographics 91, 1991. [28] J. Platt and A. Barr. Constraint 
methods for .exible models. In Proceedings SIGGRAPH 88, volume 22, pages 279 288, 1988. [29] W. Press, 
S. Teukolsky, W. Vetterling, and B. Flannery. Numerical Recipes in C: The Art of Scienti.c Computing. 
Cambridge University Press, 1992. [30] S. Rogers. Personal Identi.cation from Human Remains. Charles 
C. Thomas Publisher, LTD, 1984. [31] D. Terzopoulos and H. Qin. Dynamic nurbs with geometric constrains 
for inter­active sculpting. ACM Transactions on Graphics, 13(2):103 136, 1994. [32] T. Vetter and T. 
Poggio. Linear object classes and image synthesis from a single example image. IEEE Pattern Analysis 
and Machine Intelligence, 19(7):733 742, 1997. [33] W. Welch and A. Witkin. Variational surface modeling. 
In Proceedings SIG-GRAPH 92, volume 26, pages 157 166, 1992. [34] W. Welch and A. Witkin. Free Form shape 
design using triangulated surfaces. In Proceedings SIGGRAPH 94, volume 28, pages 247 256, July 1994. 
 Males Females Figure 5: Automatically generated face models (3 views of each)  Figure 6: A male a 
minute 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280825</article_id>
		<sort_key>75</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Synthesizing realistic facial expressions from photographs]]></title>
		<page_from>75</page_from>
		<page_to>84</page_to>
		<doi_number>10.1145/280814.280825</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280825</url>
		<keywords>
			<kw><![CDATA[facial animation]]></kw>
			<kw><![CDATA[facial expression generation]]></kw>
			<kw><![CDATA[facial modeling]]></kw>
			<kw><![CDATA[morphing]]></kw>
			<kw><![CDATA[photogrammetry]]></kw>
			<kw><![CDATA[view-dependent texture-mapping]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Modeling and recovery of physical attributes</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Modeling and recovery of physical attributes</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP48022732</person_id>
				<author_profile_id><![CDATA[81100026067]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fr&#233;d&#233;ric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pighin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Washington, Seattle]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31079029</person_id>
				<author_profile_id><![CDATA[81332503356]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jamie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hecker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Washington, Seattle]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14153841</person_id>
				<author_profile_id><![CDATA[81311486606]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Dani]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lischinski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hebrew Univ.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15023234</person_id>
				<author_profile_id><![CDATA[81100122769]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Szeliski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P63622</person_id>
				<author_profile_id><![CDATA[81100188207]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Salesin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Washington, Seattle]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>617851</ref_obj_id>
				<ref_obj_pid>616029</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Takaaki Akimoto, Yasuhito Suenaga, and Richard S. Wallace. Automatic Creation of 3D Facial Models. IEEE Computer Graphics and Applications, 13(5): 16-22, September 1993.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Alias I Wavefront, Toronto, Ontario. Alias V7.0, 1995.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134003</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Thaddeus Beier and Shawn Neely. Feature-based Image Metamorphosis. In SIGGRAPH 92 Conference Proceedings, pages 35-42. ACM SIGGRAPH, July 1992.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Philippe Bergeron and Pierre Lachapelle. Controlling Facial Expressions and Body Movements in the Computer-Generated Animated Short "Tony De Peltrie". In SIGGRAPH 85 Advanced Computer Animation seminar notes. July 1985.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258880</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Christoph Bregler, Michele Covell, and Malcolm Slaney. Video Rewrite: Driving Visual Speech with Audio. In SIGGRAPH 97 Conference Proceedings, pages 353-360. ACM SIGGRAPH, August 1997.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>199411</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[David T. Chen, Andrei State, and David Banks. Interactive Shape Metamorphosis. In 1995 Symposium on Interactive 3D Graphics, pages 43-44. ACM SIGGRAPH, April 1995.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Chang S. Choi, Kiyoharu, Hiroshi Harashima, and Tsuyoshi Takebe. Analysis and Synthesis of Facial Image Sequences in Model-Based Image Coding. In IEEE Transactions on Circuits and Systems for Video Technology, volume 4, pages 257 - 275. June 1994.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Cyberware Laboratory, Inc, Monterey, California. 4020/RGB 3D Scanner with Color Digitizer, 1990.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237191</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Paul E. Debevec, Camillo J. Taylor, and Jitendra Malik. Modeling and Rendering Architecture from Photographs: A Hybrid Geometry- and Image-Based Approach. In SIGGRAPH 96 Conference Proceedings, pages 11-20. ACM SIGGRAPH, August 1996.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Eben Ostby, Pixar Animation Studios. Personal communication, January 1997.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Paul Ekman and Wallace V. Friesen. Unmasking the Face. A guide to recognizing emotions fron facial clues. Prentice-Hall, Inc., Englewood Cliffs, New Jersey, 1975.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Paul Ekman and Wallace V. Friesen. Manual for the Facial Action Coding System. Consulting Psychologists Press, Inc., Palo Alto, California, 1978.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>791488</ref_obj_id>
				<ref_obj_pid>791215</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Irfan Essa, Sumit Basu, Trevor Darrell, and Alex Pentland. Modeling, Tracking and Interactive Animation of Faces and Heads Using Input from Video. In Computer Animation Conference, pages 68-79. June 1996.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Gary Faigin. The Artist's Complete Guide to Facial Expression. Watson-Guptill Publications, New York, 1990.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>171658</ref_obj_id>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Olivier Faugeras. Three-Dimensional Computer Vision: A Geometric Viewpoint. MIT Press, Cambridge, Massachusetts, 1993.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>248979</ref_obj_id>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[G. Golub and C. F. Van Loan. Matrix Computation, third edition. The John Hopkins University Press, Baltimore and London, 1996.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280822</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Brian Guenter, Cindy Grimm, Daniel Wood, Henrique Malvar, and Fr6d6ric Pighin. Making Faces. In SIGGRAPH 98 Conference Proceedings. ACM SIGGRAPH, July 1998.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166139</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Pat Hanrahan and Wolfgang Krueger. Reflection from Layered Surfaces Due to Subsurface Scattering. In SIGGRAPH 93 Conference Proceedings, volume 27, pages 165-174. ACM SIGGRAPH, August 1993.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Bright Star Technologies Inc. Beginning Reading Software. Sierra On- Line, Inc., 1993.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Horace H. S. Ip and Lijun Yin. Constructing a 3D Individualized Head Model from Two Orthogonal Views. The Visual Computer, 12:254- 266, 1996.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378490</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Gregory Ward J., Francis M. Rubinstein, and Robert D. Clear. A Ray Tracing Solution for Diffuse Interreflection. In SIGGRAPH 88 Conference Proceedings, volume 22, pages 85-92. August 1988.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134007</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[James R. Kent, Wayne E. Carlson, and Richard E. Parent. Shape Transformation for Polyhedral Objects. In SIGGRAPH 92 Proceedings Conference, volume 26, pages 47-54. ACM SIGGRAPH, July 1992.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237281</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Rolf M. Koch, Markus H. Gross, Friedrich R. Carls, Daniel F. von Bfiren, George Fankhauser, and Yoav I. H. Parish. Simulating Facial Surgery Using Finite Element Methods. In SIGGRAPH 96 Conference Proceedings, pages 421-428. ACM SIGGRAPH, August 1996.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Tsuneya Kurihara and Kiyoshi Arai. A Transformation Method for Modeling and Animation of the Human Face from Photographs. In Nadia Magnenat Thalmann and Daniel Thalmann, editors, Computer Animation 91, pages 45-58. Springer-Verlag, Tokyo, 1991.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>840054</ref_obj_id>
				<ref_obj_pid>839277</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[A. Lanitis, C. J. Taylor, and T. F. Cootes. A Unified Approach for Coding and Interpreting Face Images. In Fifth International Conference on Computer Vision (ICCV 95), pages 368-373. Cambridge, Massachusetts, June 1995.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[C. L. Lawson and R. J. Hansen. Solving Least Squares Problems. Prentice-Hall, Englewood Cliffs, 1974.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218501</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Seung-Yong Lee, Kyung-Yong Chwa, Sung Yong Shin, and George Wolberg. Image Metamorphosis Using Snakes and Free-Form Deformations. In SIGGRAPH 95 Conference Proceedings, pages 439-448. ACM SIGGRAPH, August 1995.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>614355</ref_obj_id>
				<ref_obj_pid>614264</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Seung-Yong Lee, George Wolberg, Kyung-Yong Chwa, and Sung Yong Shin. Image Metamorphosis with Scattered Feature Constraints. IEEE Transactions on Visualization and Computer Graphics, 2(4), December 1996.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218407</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Yuencheng Lee, Demetri Terzopoulos, and Keith Waters. Realistic Modeling for Facial Animation. In SIGGRAPH 95 Conference Proceedings, pages 55-62. ACM SIGGRAPH, August 1995.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Yuencheng C. Lee, Demetri Terzopoulos, and Keith Waters. Constructing Physics-Based Facial Models of Individuals. In Proceedings of Graphics Interface 93, pages 1-8. May 1993.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Francis H. Moffitt and Edward M. Mikhail. Photogrammetry. Harper &amp; Row, New York, 3 edition, 1980.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>117636</ref_obj_id>
				<ref_obj_pid>117635</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Shree K. Nayar, Katsushi Ikeuchi, and Takeo Kanade. Shape from Interreflections. International Journal of Computer Vision, 6:173-195, 1991.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_obj_id>160683</ref_obj_id>
				<ref_obj_pid>160673</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Gregory M. Nielson. Scattered Data Modeling. IEEE Computer Graphics and Applications, 13(1):60-70, January 1993.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_obj_id>569955</ref_obj_id>
				<ref_obj_pid>800193</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Frederic I. Parke. Computer Generated Animation of Faces. Proceedings ACM annual conference., August 1972.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_obj_id>907312</ref_obj_id>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Frederic I. Parke. A Parametric Model for Human Faces. PhD thesis, University of Utah, Salt Lake City, Utah, December 1974. UTEC-CSc- 75 -047.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
			<ref>
				<ref_obj_id>249651</ref_obj_id>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Frederic I. Parke and Keith Waters. Computer Facial Animation. A K Peters, Wellesley, Massachusetts, 1996.]]></ref_text>
				<ref_id>36</ref_id>
			</ref>
			<ref>
				<ref_obj_id>148286</ref_obj_id>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[W.H. Press, B. R Flannery, S. A. Teukolsky, and W. T. Vetterling. Numerical Recipes in C: The Art of Scientific Computing. Cambridge University Press, Cambridge, England, second edition, 1992.]]></ref_text>
				<ref_id>37</ref_id>
			</ref>
			<ref>
				<ref_obj_id>732115</ref_obj_id>
				<ref_obj_pid>647651</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Karl Pulli, Michael Cohen, Tom Duchamp, Hugues Hoppe, Linda Shapiro, and Werner Stuetzle. View-based rendering: Visualizing real objects from scanned range and color data. In Proc. 8th Eurographics Workshop on Rendering. June 1997.]]></ref_text>
				<ref_id>38</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237196</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Steven M. Seitz and Charles R. Dyer. View Morphing. In SIGGRAPH 96 Conference Proceedings, Annual Conference Series, pages 21-30. ACM SIGGRAPH, August 1996.]]></ref_text>
				<ref_id>39</ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Chester C. Slama, editor. Manual of Photogrammetry. American Society of Photogrammetry, Falls Church, Virginia, fourth edition, 1980.]]></ref_text>
				<ref_id>40</ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[Richard Szeliski and Sing Bing Kang. Recovering 3D Shape and Motion from Image Streams using Nonlinear Least Squares. Journal of Visual Communication and Image Representation, 5(1): 10-28, March 1994.]]></ref_text>
				<ref_id>41</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258861</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[Richard Szeliski and Heung-Yeung Shum. Creating Full View Panoramic Image Mosaics and Texture-Mapped Models. In SIG- GRAPH 97 Conference Proceedings, pages 251-258. ACM SIG- GRAPH, August 1997.]]></ref_text>
				<ref_id>42</ref_id>
			</ref>
			<ref>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[Demetri Terzopoulos and Keith Waters. Physically-based Facial Modeling, Analysis, and Animation. Journal of Visualization and Computer Animation, 1 (4):73-80, March 1990.]]></ref_text>
				<ref_id>43</ref_id>
			</ref>
			<ref>
				<ref_obj_id>267823</ref_obj_id>
				<ref_obj_pid>267658</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[Kristinn R. Th6risson. Gandalf: An Embodied Humanoid Capable of Real-Time Multimodal Dialogue with People. In First ACM International Conference on Autonomous Agents. 1997.]]></ref_text>
				<ref_id>44</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801157</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[Michael W. Vannier, Jeffrey F. Marsh, and James O. Warren. Threedimentional Computer Graphics for Craniofacial Surgical Planning and Evaluation. In SIGGRAPH 83 Conference Proceedings, volume 17, pages 263-273. ACM SIGGRAPH, August 1983.]]></ref_text>
				<ref_id>45</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37405</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[Keith Waters. A Muscle Model for Animating Three-Dimensional Facial Expression. In SIGGRAPH 87 Conference Proceedings), volume 21, pages 17-24. ACM SIGGRAPH, July 1987.]]></ref_text>
				<ref_id>46</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97906</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[Lance Williams. Performance-Driven Facial Animation. In SIG- GRAPH 90 Conference Proceedings, volume 24, pages 235-242. August 1990.]]></ref_text>
				<ref_id>47</ref_id>
			</ref>
			<ref>
				<ref_obj_id>939158</ref_obj_id>
				<ref_obj_pid>938978</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[Z. Zhang, K. Isono, and S. Akamatsu. Euclidean Structure from Uncalibrated Images Using Fuzzy Domain Knowledge: Application to Facial Images Synthesis. In Proc. International Conference on Computer Vision (ICCV'98). January 1998.]]></ref_text>
				<ref_id>48</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Synthesizing Realistic Facial Expressions from Photographs Copyright &#38;#169;1998 by the Association 
for Computing Machinery, Inc. Permission to make digital or hard copies of part or all of this work for 
personal or classroom use is granted without fee provided that copies are not made or distributed for 
profit or commercial advantage and that copies bear this notice and the full citation on the first page. 
Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit 
is permitted. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires 
specific permission and/or a fee. Fr´eric Pighin Dani LischinskiyRichard SzeliskizDavid H. Salesin ed´Jamie 
Hecker University of Washington yThe Hebrew University zMicrosoft Research Abstract We present new techniques 
for creating photorealistic textured 3D facial models from photographs of a human subject, and for creat­ing 
smooth transitions between different facial expressions by mor­phing between these different models. 
Starting from several uncali­brated views of a human subject, we employ a user-assisted tech­nique to 
recover the camera poses corresponding to the views as well as the 3D coordinates of a sparse set of 
chosen locations on the subject s face. A scattered data interpolation technique is then used to deform 
a generic face mesh to .t the particular geometry of the subject s face. Having recovered the camera 
poses and the facial ge­ometry, we extract from the input images one or more texture maps for the model. 
This process is repeated for several facial expressions of a particular subject. To generate transitions 
between these facial expressions we use 3D shape morphing between the corresponding face models, while 
at the same time blending the corresponding tex­tures. Using our technique, we have been able to generate 
highly re­alistic face models and natural looking animations. CR Categories: I.2.10 [Arti.cial Intelligence]: 
Vision and Scene Under­standing Modeling and recovery of physical attributes; I.3.7 [Computer Graphics]: 
Three-Dimensional Graphics Animation; I.3.7 [Computer Graphics]: Three-Dimensional Graphics Color, 
shading, shadowing and texture. Additional Keywords: facial modeling, facial expression generation, facial 
animation, photogrammetry, morphing, view-dependent texture-mapping  Introduction There is no landscape 
that we know as well as the human face. The twenty-.ve-odd square inches containing the fea­tures is 
the most intimately scrutinized piece of territory in existence, examined constantly, and carefully, 
with far more than an intellectual interest. Every detail of the nose, eyes, and mouth, every regularity 
in proportion, every vari­ation from one individual to the next, are matters about which we are all authorities. 
Gary Faigin [14], from The Artist s Complete Guide to Facial Expression Realistic facial synthesis is 
one of the most fundamental problems in computer graphics and one of the most dif.cult. Indeed, attempts 
to model and animate realistic human faces date back to the early 70 s [34], with many dozens of research 
papers published since. The applications of facial animation include such diverse .elds as character 
animation for .lms and advertising, computer games [19], video teleconferencing [7], user-interface agents 
and avatars [44], and facial surgery planning [23, 45]. Yet no perfectly realistic facial animation has 
ever been generated by computer: no facial anima­tion Turing test has ever been passed. There are several 
factors that make realistic facial animation so elu­sive. First, the human face is an extremely complex 
geometric form. For example, the human face models used in Pixar s Toy Story had several thousand control 
points each [10]. Moreover, the face ex­hibits countless tiny creases and wrinkles, as well as subtle 
varia­tions in color and texture all of which are crucial for our compre­hension and appreciation of 
facial expressions. As dif.cult as the face is to model, it is even more problematic to animate, since 
fa­cial movement is a product of the underlying skeletal and muscu­lar forms, as well as the mechanical 
properties of the skin and sub­cutaneous layers (which vary in thickness and composition in dif­ferent 
parts of the face). All of these problems are enormously mag­ni.ed by the fact that we as humans have 
an uncanny ability to read expressions an ability that is not merely a learned skill, but part of our 
deep-rooted instincts. For facial expressions, the slightest de­viation from truth is something any person 
will immediately detect. A number of approaches have been developed to model and ani­mate realistic facial 
expressions in three dimensions. (The reader is referred to the recent book by Parke and Waters [36] 
for an excel­lent survey of this entire .eld.) Parke s pioneering work introduced simple geometric interpolation 
between face models that were dig­itized by hand [34]. A radically different approach is performance­based 
animation, in which measurements from real actors are used to drive synthetic characters [4, 13, 47]. 
Today, face models can also be obtained using laser-based cylindrical scanners, such as those produced 
by Cyberware [8]. The resulting range and color data can be .tted with a structured face mesh, augmented 
with a physically­based model of skin and muscles [29, 30, 43, 46]. The animations produced using these 
face models represent the state-of-the-art in automatic physically-based facial animation. For sheer 
photorealism, one of the most effective approaches to date has been the use of 2D morphing between photographic 
images [3]. Indeed, some remarkable results have been achieved in this way most notably, perhaps, the 
Michael Jackson video produced by PDI, in which very different-looking actors are seemingly transformed 
into one another as they dance. The production of this video, how­ever, required animators to painstakingly 
specify a few dozen care­fully chosen correspondences between physical features of the ac­tors in almost 
every frame. Another problem with 2D image morph­ing is that it does not correctly account for changes 
in viewpoint or object pose. Although this shortcoming has been recently addressed by a technique called 
view morphing [39], 2D morphing still lacks some of the advantages of a 3D model, such as the complete 
free­dom of viewpoint and the ability to composite the image with other 3D graphics. Morphing has also 
been applied in 3D: Chen et al. [6] applied Beier and Neely s 2D morphing technique [3] to morph be­tween 
cylindrical laser scans of human heads. Still, even in this case the animator must specify correspondences 
for every pair of expres­sions in order to produce a transition between them. More recently, Bregler 
et al. [5] used morphing of mouth regions to lip-synch ex­isting video to a novel sound-track. In this 
paper, we show how 2D morphing techniques can be com­bined with 3D transformations of a geometric model 
to automati­cally produce 3D facial expressions with a high degree of realism. Our process consists of 
several basic steps. First, we capture multi­ple views of a human subject (with a given facial expression) 
using cameras at arbitrary locations. Next, we digitize these photographs and manually mark a small set 
of initial corresponding points on the face in the different views (typically, corners of the eyes and 
mouth, tip of the nose, etc.). These points are then used to automat­ically recover the camera parameters 
(position, focal length, etc.) corresponding to each photograph, as well as the 3D positions of the marked 
points in space. The 3D positions are then used to deform a generic 3D face mesh to .t the face of the 
particular human sub­ject. At this stage, additional corresponding points may be marked to re.ne the 
.t. Finally, we extract one or more texture maps for the 3D model from the photos. Either a single view-independent 
tex­ture map can be extracted, or the original images can be used to perform view-dependent texture mapping. 
This whole process is re­peated for the same human subject, with several different facial ex­pressions. 
To produce facial animations, we interpolate between two or more different 3D models constructed in this 
way, while at the same time blending the textures. Since all the 3D models are con­structed from the 
same generic mesh, there is a natural correspon­dence between all geometric points for performing the 
morph. Thus, transitions between expressions can be produced entirely automati­cally once the different 
face models have been constructed, without having to specify pairwise correspondences between any of 
the ex­pressions. Our modeling approach is based on photogrammetric techniques in which images are used 
to create precise geometry [31, 40]. The earliest such techniques applied to facial modeling and animation 
employed grids that were drawn directly on the human subject s face [34, 35]. One consequence of these 
grids, however, is that the images used to construct geometry can no longer be used as valid texture 
maps for the subject. More recently, several methods have been proposed for modeling the face photogrammetrically 
without the use of grids [20, 24]. These modeling methods are similar in concept to the modeling technique 
described in this paper. How­ever, these previous techniques use a small predetermined set of fea­tures 
to deform the generic face mesh to the particular face being modeled, and offer no mechanism to further 
improve the .t. Such an approach may perform poorly on faces with unusual features or other signi.cant 
deviations from the normal. Our system, by con­trast, gives the user complete freedom in specifying the 
correspon­dences, and enables the user to re.ne the initial .t as needed. An­other advantage of our technique 
is its ability to handle fairly arbi­trary camera positions and lenses, rather than using a .xed pair 
that are precisely oriented. Our method is similar, in concept, to the work done in architectural modeling 
by Debevec et al. [9], where a set of annotated photographs are used to model buildings starting from 
a rough description of their shape. Compared to facial modeling meth­ods that utilize a laser scanner, 
our technique uses simpler acquisi­tion equipment (regular cameras), and it is capable of extracting 
tex­ture maps of higher resolution. (Cyberware scans typically produce a cylindrical grid of 512 by 256 
samples). The price we pay for these advantages is the need for user intervention in the modeling process. 
We employ our system not only for creating realistic face models, but also for performing realistic transitions 
between different ex­pressions. One advantage of our technique, compared to more tra­ditional animatable 
models with a single texture map, is that we can capture the subtle changes in illumination and appearance 
(e.g., fa­cial creases) that occur as the face is deformed. This degree of re­alism is dif.cult to achieve 
even with physically-based models, be­cause of the complexity of skin folding and the dif.culty of simu­lating 
interre.ections and self-shadowing [18, 21, 32]. This paper also presents several new expression synthesis 
tech­niques based on extensions to the idea of morphing. We develop a morphing technique that allows 
for different regions of the face to have different percentages or mixing proportions of facial ex­pressions. 
We also introduce a painting interface, which allows users to locally add in a little bit of an expression 
to an existing compos­ite expression. We believe that these novel methods for expression generation and 
animation may be more natural for the average user than more traditional animation systems, which rely 
on the manual adjustments of dozens or hundreds of control parameters. The rest of this paper is organized 
as follows. Section 2 describes our method for .tting a generic face mesh to a collection of si­multaneous 
photographs of an individual s head. Section 3 de­scribes our technique for extracting both view-dependent 
and view­independent texture maps for photorealistic rendering of the face. Section 4 presents the face 
morphing algorithm that is used to an­imate the face model. Section 5 describes the key aspects of our 
system s user interface. Section 6 presents the results of our experi­ments with the proposed techniques, 
and Section 7 offers directions for future research. 2 Model .tting The task of the model-.tting process 
is to adapt a generic face model to .t an individual s face and facial expression. As input to this pro­cess, 
we take several images of the face from different viewpoints (Figure 1a) and a generic face model (we 
use the generic face model created with AliasjWavefront [2] shown in Figure 1c). A few fea­tures points 
are chosen (13 in this case, shown in the frames of Fig­ure 1a) to recover the camera pose. These same 
points are also used to re.ne the generic face model (Figure 1d). The model can be fur­ther re.ned by 
drawing corresponding curves in the different views (Figure 1b). The output of the process is a face 
model that has been adapted to .t the face in the input images (Figure 1e), along with a precise estimate 
of the camera pose corresponding to each input image. The model-.tting process consists of three stages. 
In the pose re­covery stage, we apply computer vision techniques to estimate the viewing parameters (position, 
orientation, and focal length) for each of the input cameras. We simultaneously recover the 3D coordinates 
of a set of feature points on the face. These feature points are se­lected interactively from among the 
face mesh vertices, and their positions in each image (where visible) are speci.ed by hand. The scattered 
data interpolation stage uses the estimated 3D coordinates of the feature points to compute the positions 
of the remaining face mesh vertices. In the shape re.nement stage, we specify additional correspondences 
between facial vertices and image coordinates to improve the estimated shape of the face (while keeping 
the camera pose .xed). 2.1 Pose recovery Starting with a rough knowledge of the camera positions (e.g., 
frontal view, side view, etc.) and of the 3D shape (given by the generic head model), we iteratively 
improve the pose and the 3D shape estimates in order to minimize the difference between the pre­dicted 
and observed feature point positions. Our formulation is based on the non-linear least squares structure-from-motion 
algo­rithm introduced by Szeliski and Kang [41]. However, unlike the method they describe, which uses 
the Levenberg-Marquardt algo­rithm to perform a complete iterative minimization over all of the unknowns 
simultaneously, we break the problem down into a series of linear least squares problems that can be 
solved using very simple (a) (b) (c) (d) (e) Figure 1 Model-.tting process: (a) a set of input images 
with marked feature points, (b) facial features annotated using a set of curves, (c) generic face geometry 
(shaded surface rendering), (d) face adapted to initial 13 feature points (after pose estimation) (e) 
face after 99 additional correspondences have been given. and numerically stable techniques [16, 37]. 
To formulate the pose recovery problem, we associate a rotation ma­ trix Rk and a translation vector 
tk with each camera pose k.(The kk k three rows of Rk are rx, ry,and rz , and the three entries in tk 
are txk , tyk , tzk.) We write each 3D feature point as pi, and its 2D screen coor­dinates in the k-th 
image as (xik , yik ). Assuming that the origin of the (x, y) image coordinate system lies at the optical 
center of each image (i.e., where the optical axis inter­sects the image plane), the traditional 3D projection 
equation for a camera with a focal length fk (expressed in pixels) can be written as kk pi + tk pi + 
tk kxx kyy xi = fk ryi = fk r(1) rk pi + tk rk pi + tk zz zz (This is just an explicit rewriting of the 
traditional projection equa­ k kkk tion xi eRkpi + tk where xi =(xi , yi , fk).) Instead of using (1) 
directly, we reformulate the problem to estimate inverse distances to the object [41]. Let /k =1itzk 
be this inverse dis­tance and sk = fk/k be a world-to-image scale factor. The advantage of this formulation 
is that the scale factor sk can be reliably estimated even when the focal length is long, whereas the 
original formulation has a strong coupling between the fk and tzk parameters. Performing these substitution, 
we obtain k kk x rx pi + tk xi = s 1+ /krkz pi rk pi + tk kkyy y = s . i 1+ /krk pi z If we let wik 
=(1+ /k(rzk pi)),1 be the inverse denominator, and collect terms on the left-hand side, we get , kkkk 
kk wi xi + xi /k(rz pi) ,sk(rx pi + tx )=0 (2) , kkkk kk wi yi + yi /k(rz pi) ,sk(r pi + ty )=0 y Note 
that these equations are linear in each of the unknowns that we wish to recover, i.e., pi, txk , tyk 
, /k , sk,and Rk, if we ignore the vari­ation of wik with respect to these parameters. (The reason we 
keep the wik term, rather than just dropping it from these equations, is so that the linear equations 
being solved in the least squares step have the same magnitude as the original measurements (xik , yik). 
Hence, least-squares will produce a maximum likelihood estimate for the unknown parameters [26].) Given 
estimates for initial values, we can solve for different sub­sets of the unknowns. In our current algorithm, 
we solve for the un­knowns in .ve steps: .rst sk,then pi, Rk , txk and tyk, and .nally /k . This order 
is chosen to provide maximum numerical stability given the crude initial pose and shape estimates. For 
each parameter or set of parameters chosen, we solve for the unknowns using linear least squares (Appendix 
A). The simplicity of this approach is a result of solving for the unknowns in .ve separate stages, so 
that the parame­ters for a given camera or 3D point can be recovered independently of the other parameters. 
 2.2 Scattered data interpolation Once we have computed an initial set of coordinates for the fea­ture 
points pi, we use these values to deform the remaining vertices on the face mesh. We construct a smooth 
interpolation function that gives the 3D displacements between the original point positions and the new 
adapted positions for every vertex in the original generic face mesh. Constructing such an interpolation 
function is a standard problem in scattered data interpolation. Given a set of known dis­ (0) (0) placements 
ui = pi ,pi away from the original positions pi at every constrained vertex i, construct a function that 
gives the dis­placement uj for every unconstrained vertex j. There are several considerations in choosing 
the particular data in­terpolant [33]. The .rst consideration is the embedding space, that is, the domain 
of the function being computed. In our case, we use the original 3D coordinates of the points as the 
domain. (An alterna­tive would be to use some 2D parameterization of the surface mesh, for instance, 
the cylindrical coordinates described in Section 3.) We therefore attempt to .nd a smooth vector-valued 
function f (p) .tted to the known data ui = f(pi), from which we can compute uj = f(pj). There are also 
several choices for how to construct the interpolating function [33]. We use a method based on radial 
basis functions,that is, functions of the form X f (p)= ciq(kp ,pik), i where q(r) are radially symmetric 
basis functions. A more general form of this interpolant also adds some low-order polynomial terms to 
model global, e.g., af.ne, deformations [27, 28, 33]. In our sys­tem, we use an af.ne basis as part of 
our interpolation algorithm, so that our interpolant has the form: X f(p)= ciq(kp ,pik)+ Mp + t, (3) 
i To determine the coef.cients ci and the af.ne components M and t, we solve a set of linear equations 
that includes the interpolation P constraints ui = f(pi), as well as the constraints i ci =0 and P i 
cipi T = 0, which remove af.ne contributions from the radial ba­sis functions. Many different functions 
for q(r) have been proposed [33]. After experimenting with a number of functions, we have chosen to use 
q(r)= e,r/64, with units measured in inches. Figure 1d shows the shape of the face model after having 
inter­polated the set of computed 3D displacements at 13 feature points shown in Figure 1 and applied 
them to the entire face.  2.3 Correspondence-based shape re.nement After warping the generic face model 
into its new shape, we can fur­ther improve the shape by specifying additional correspondences. Since 
these correspondences may not be as easy to locate correctly, we do not use them to update the camera 
pose estimates. Instead, we simply solve for the values of the new feature points pi using a simple least-squares 
.t, which corresponds to .nding the point near­est the intersection of the viewing rays in 3D. We can 
then re-run the scattered data interpolation algorithm to update the vertices for which no correspondences 
are given. This process can be repeated until we are satis.ed with the shape. Figure 1e shows the shape 
of the face model after 99 additional cor­respondences have been speci.ed. To facilitate the annotation 
pro­cess, we grouped vertices into polylines. Each polyline corresponds to an easily identi.able facial 
feature such as the eyebrow, eyelid, lips, chin, or hairline. The features can be annotated by outlining 
them with hand-drawn curves on each photograph where they are visible. The curves are automatically converted 
into a set of feature points by stepping along them using an arc-length parametrization. Figure 1b shows 
annotated facial features using a set of curves on the front view.  3 Texture extraction In this section 
we describe the process of extracting the texture maps necessary for rendering photorealistic images 
of a reconstructed face model from various viewpoints. The texture extraction problem can be de.ned as 
follows. Given a collection of photographs, the recovered viewing parameters, and the .tted face model, 
compute for each point p on the face model its texture color T(p). Each point p may be visible in one 
or more photographs; therefore, we must identify the corresponding point in each photograph and decide 
how these potentially different values should be combined Figure 2 Geometry for texture extraction 
(blended) together. There are two principal ways to blend values from different photographs: view-independent 
blending, resulting in a texture map that can be used to render the face from any viewpoint; and view-dependent 
blending, which adjusts the blending weights at each point based on the direction of the current viewpoint 
[9, 38]. Rendering takes longer with view-dependent blending, but the re­sulting image is of slightly 
higher quality (see Figure 3). 3.1 Weight maps As outlined above, the texture value T(p) at each point 
on the face model can be expressed as a convex combination of the correspond­ing colors in the photographs: 
P mk(p) Ik(xk , yk) T(p)= k P. mk(p) k Here, Ik is the image function (color at each pixel of the k-th 
photo­graph,) and (xk , yk) are the image coordinates of the projection of p onto the k-th image plane. 
The weight map mk(p) is a function that speci.es the contribution of the k-th photograph to the texture 
at each facial surface point. The construction of these weight maps is probably the trickiest and the 
most interesting component of our texture extraction technique. There are several important considerations 
that must be taken into account when de.ning a weight map: 1. Self-occlusion: mk(p) should be zero unless 
p is front-facing with respect to the k-th image and visible in it. 2. Smoothness: the weight map should 
vary smoothly, in order to ensure a seamless blend between different input images. 3. Positional certainty: 
mk(p) should depend on the positional cer­tainty [24] ofp with respect to the k-th image. The positional 
certainty is de.ned as the dot product between the surface nor­mal at p and the k-th direction of projection. 
 4. View similarity: for view-dependent texture mapping, the weight mk(p) should also depend on the angle 
between the direction of projection of p onto the j-th image and its direction of projection in the new 
view.  Previous authors have taken only a subset of these considerations into account when designing 
their weighting functions. For ex­ample, Kurihara and Arai [24] use positional certainty as their weighting 
function, but they do not account for self-occlusion. Aki­moto et al. [1] and Ip and Yin [20] blend the 
images smoothly, but address neither self-occlusion nor positional certainty. De­bevec et al. [9], who 
describe a view-dependent texture mapping technique for modeling and rendering buildings from photographs, 
do address occlusion but do not account for positional certainty. (It should be noted, however, that 
positional certainty is less critical in photographs of buildings, since most buildings do not tend to 
curve away from the camera.) To facilitate fast visibility testing of points on the surface of the face 
from a particular camera pose, we .rst render the face model us­ing the recovered viewing parameters 
and save the resulting depth map from the Z-buffer. Then, with the aid of this depth map, we can quickly 
classify the visibility of each facial point by applying the viewing transformation and comparing the 
resulting depth to the corresponding value in the depth map.  3.2 View-independent texture mapping In 
order to support rapid display of the textured face model from any viewpoint, it is desirable to blend 
the individual photographs to­gether into a single texture map. This texture map is constructed on a 
virtual cylinder enclosing the face model. The mapping between the 3D coordinates on the face mesh and 
the 2D texture space is de­.ned using a cylindrical projection, as in several previous papers [6, 24, 
29]. For view-independent texture mapping, we will index the weight map mk by the (u, v) coordinates 
of the texture being created. Each weight mk(u, v) is determined by the following steps: 1. Construct 
a feathered visibility map Fk for each image k.These maps are de.ned in the same cylindrical coordinates 
as the tex­ture map. We initially set Fk(u, v) to 1 if the corresponding facial point p is visible in 
the k-th image, and to 0 otherwise. The result is a binary visibility map, which is then smoothly ramped 
(feath­ered) from 1 to 0 in the vicinity of the boundaries [42]. A cubic polynomial is used as the ramping 
function. 2. Compute the 3D point p on the surface of the face mesh whose cylindrical projection is 
(u, v) (see Figure 2). This computation is performed by casting a ray from (u, v) on the cylinder towards 
the cylinder s axis. The .rst intersection between this ray and the face mesh is the point p. (Note that 
there can be more than one intersection for certain regions of the face, most notably the ears. These 
special cases are discussed in Section 3.4.) Let Pk(p)be the positional certainty of p with respect to 
the k-th image. 3. Set weight mk(u, v) to the product Fk(u, v) Pk(p).  For view-independent texture 
mapping, we will compute each pixel of the resulting texture T(u, v) as a weighted sum of the original 
im­age functions, indexed by (u, v).  3.3 View-dependent texture mapping The main disadvantage of the 
view-independent cylindrical texture map described above is that its construction involves blending to­gether 
resampled versions of the original images of the face. Be­cause of this resampling, and also because 
of slight registration er­rors, the resulting texture is slightly blurry. This problem can be al­leviated 
to a large degree by using a view-dependent texture map [9] in which the blending weights are adjusted 
dynamically, according to the current view. For view-dependent texture mapping, we render the model several 
times, each time using a different input photograph as a texture map, and blend the results. More speci.cally, 
for each input photo­graph, we associate texture coordinates and a blending weight with each vertex in 
the face mesh. (The rendering hardware performs perspective-correct texture mapping along with linear 
interpolation of the blending weights.) Given a viewing direction d, we .rst select the subset of pho­tographs 
used for the rendering and then assign blending weights to each of these photographs. Pulli et al. [38] 
select three photographs based on a Delaunay triangulation of a sphere surrounding the ob­ject. Since 
our cameras were positioned roughly in the same plane,  Figure 3 Comparison between view-independent 
(left) and view­dependent (right) texture mapping. Higher frequency details are vis­ible in the view-dependent 
rendering. we select just the two photographs whose view directions d`and d`+1 are the closest to d 
and blend between the two. In choosing the view-dependent term Vk(d) of the blending weights, we wish 
to use just a single photo if that photo s view direction matches the current view direction precisely, 
and to blend smoothly between the nearest two photos otherwise. We used the simplest possible blending 
function having this effect: dk d`d`+1 d ,if `:k :`+1Vk(d)=0 otherwise For the .nal blending weights 
mk(p, d), we then use the product of all three terms Fk(xk , yk) Pk(p) Vk(d). View-dependent texture 
maps have several advantages over cylin­drical texture maps. First, they can make up for some lack of 
de­tail in the model. Second, whenever the model projects onto a cylin­der with overlap, a cylindrical 
texture map will not contain data for some parts of the model. This problem does not arise with view­dependent 
texture maps if the geometry of the mesh matches the photograph properly. One disadvantage of the view-dependent 
ap­proach is its higher memory requirements and slower speed due to the multi-pass rendering. Another 
drawback is that the resulting im­ages are much more sensitive to any variations in exposure or light­ing 
conditions in the original photographs. 3.4 Eyes, teeth, ears, and hair The parts of the mesh that correspond 
to the eyes, teeth, ears, and hair are textured in a separate process. The eyes and teeth are usually 
partially occluded by the face; hence it is dif.cult to extract a tex­ture map for these parts in every 
facial expression. The ears have an intricate geometry with many folds and usually fail to project with­out 
overlap on a cylinder. The hair has .ne-detailed texture that is dif.cult to register properly across 
facial expressions. For these rea­sons, each of these facial elements is assigned an individual texture 
map. The texture maps for the eyes, teeth, and ears are computed by projecting the corresponding mesh 
part onto a selected input image where that part is clearly visible (the front view for eyes and teeth, 
side views for ears). The eyes and the teeth are usually partially shadowed by the eye­lids and the mouth 
respectively. We approximate this shadowing by modulating the brightness of the eye and teeth texture 
maps accord­ing to the size of the eyelid and mouth openings. Figure 4 A global blend between surprised 
(left) and sad (cen-Figure 5 Combining the upper part of a neutral expression (left) ter) produces a 
worried expression (right). with the lower part of a happy expression (center) produces a fake smile 
(right).  4 Expression morphing A major goal of this work is the generation of continuous and re­alistic 
transitions between different facial expressions. We achieve these effects by morphing between corresponding 
face models. In general the problem of morphing between arbitrary polygonal meshes is a dif.cult one 
[22], since it requires a set of correspon­dences between meshes with potentially different topology 
that can produce a reasonable set of intermediate shapes. In our case, how­ever, the topology of all 
the face meshes is identical. Thus, there is already a natural correspondence between vertices. Furthermore, 
in creating the models we attempt to mark facial features consistently across different facial expressions, 
so that the major facial features correspond to the same vertices in all expressions. In this case, a 
sat­isfactory 3D morphing sequence can be obtained using simple linear interpolation between the geometric 
coordinates of corresponding vertices in each of the two face meshes. Together with the geometric interpolation, 
we need to blend the as­sociated textures. Again, in general, morphing between two images requires pairwise 
correspondences between images features [3]. In our case, however, correspondences between the two textures 
are implicit in the texture coordinates of the two associated face meshes. Rather than warping the two 
textures to form an intermediate one, the intermediate face model (obtained by geometric interpolation) 
is rendered once with the .rst texture, and again with the second. The two resulting images are then 
blended together. This approach is faster than warping the textures (which typically have high resolu­tion), 
and it avoids the resampling that is typically performed during warping. 4.1 Multiway blend and localized 
blend Given a set of facial expression meshes, we have explored ways to enlarge this set by combining 
expressions. The simplest approach is to use the morphing technique described above to create new fa­cial 
expressions, which can be added to the set. This idea can be generalized to an arbitrary number of starting 
expressions by tak­ing convex combinations of them all, using weights that apply both to the coordinates 
of the mesh vertices and to the values in the tex­ture map. (Extrapolation of expressions should also 
be possible by allowing weights to have values outside of the interval [0, 1]; note, however, that such 
weights might result in colors outside of the al­lowable gamut.) We can generate an even wider range 
of expressions using a local­ized blend of the facial expressions. Such a blend is speci.ed by a set 
of blend functions, one for each expression, de.ned over the ver­tices of the mesh. These blend functions 
describe the contribution of a given expression at a particular vertex. Although it would be possible 
to compute a texture map for each new expression, doing so would result in a loss of texture quality. 
Instead, the weights for each new blended expression are always factored into weights over the vertices 
of the original set of expres­sions. Thus, each blended expression is rendered using the texture map 
of an original expression, along with weights at each vertex, which control the opacity of that texture. 
The opacities are linearly interpolated over the face mesh using Gouraud shading.  4.2 Blend speci.cation 
In order to design new facial expressions easily, the user must be provided with useful tools for specifying 
the blending functions. These tools should satisfy several requirements. First, it should be possible 
to edit the blend at different resolutions. Moreover, we would like the speci.cation process to be continuous 
so that small changes in the blend parameters do not trigger radical changes in the resulting expression. 
Finally, the tools should be intuitive to the user; it should be easy to produce a particular target 
facial expres­sion from an existing set. We explored several different ways of specifying the blending 
weights: Global blend. The blending weights are constant over all vertices. A set of sliders controls 
the mixing proportions of the contribut­ing expressions. Figure 4 shows two facial expressions blended 
in equal proportions to produce a halfway blend.  Regional blend. According to studies in psychology, 
the face can be split into several regions that behave as coherent units [11]. Usually, three regions 
are considered: one for the forehead (in­cluding the eyebrows), another for the eyes, and another for 
the lower part of the face. Further splitting the face vertically down the center results in six regions 
and allows for asymmetric ex­pressions. We similarly partition the face mesh into several (softly feathered) 
regions and assign weights so that vertices belonging to the same region have the same weights. The mixing 
proportions describing a selected region can be adjusted by manipulating a set of sliders. Figure 5 illustrates 
the blend of two facial expressions with two regions: the upper part of the face (including eyes and 
forehead) and the lower part (including nose, mouth, and chin.)  Painterly interface. The blending weights 
can be assigned to the vertices using a 3D painting tool. This tool uses a palette in which the colors 
are facial expressions (both geometry and color), and the opacity of the brush controls how much the 
expression con­tributes to the result. Once an expression is selected, a 3D brush can be used to modify 
the blending weights in selected areas of the mesh. The fraction painted has a gradual drop-off and is 
con­trolled by the opacity of the brush. The strokes are applied directly on the rendering of the current 
facial blend, which is updated in real-time. To improve the rendering speed, only the portion of the 
mesh that is being painted is re-rendered. Figure 7 illustrates the design of a debauched smile: starting 
with a neutral expression, the face is locally modi.ed using three other expressions. Note that in the 
last step, the use of a partially transparent brush with the sleepy expression results in the actual 
geometry of the eye­lids becoming partially lowered.  Figure 6 Animation interface. On the left is 
the expression gallery ; on the right an expression is being designed. At the bottom expressions and 
poses are scheduled on the timeline. Combining different original expressions enlarges the repertoire 
of expressions obtained from a set of photographs. The expressions in this repertoire can themselves 
be blended to create even more ex­pressions, with the resulting expression still being representable 
as a (locally varying) linear combination of the original expressions.  5 User interface We designed 
an interactive tool to .t a 3D face mesh to a set of im­ages. This tool allows a user to select vertices 
on the mesh and mark where these curves or vertices should project on the images. After a .rst expression 
has been modeled, the set of annotations can be used as an initial guess for subsequent expressions. 
These guesses are au­tomatically re.ned using standard correlation-based search. Any re­sulting errors 
can be .xed up by hand. The extraction of the texture map does not require user intervention, but is 
included in the inter­face to provide feedback during the modeling phase. We also designed a keyframe 
animation system to generate facial animations. Our animation system permits a user to blend facial ex­pressions 
and to control the transitions between these different ex­pressions (Figure 6). The expression gallery 
is a key component of our system; it is used to select and display (as thumbnails) the set of facial 
expressions currently available. The thumbnails can be dragged and dropped onto the timeline (to set 
keyframes) or onto the facial design interface (to select or add facial expressions). The timeline is 
used to schedule the different expression blends and the changes in viewing parameters (pose) during 
the animation. The blends and poses have two distinct types of keyframes. Both types of keyframes are 
linearly interpolated with user-controlled cubic B´ezier curves. The timeline can also be used to display 
intermedi­ate frames at low resolution to provide a quick feedback to the ani­mator. A second timeline 
can be displayed next to the composition timeline. This feature is helpful for correctly synchronizing 
an ani­mation with live video or a soundtrack. The eyes are animated sepa­rately from the rest of the 
face, with the gaze direction parameterized by two Euler angles.  6Results In order to test our technique, 
we photographed both a man (J. R.) and a woman (Karla) in a variety of facial expressions. The photog-raphy 
was performed using .ve cameras simultaneously. The cam­eras were not calibrated in any particular way, 
and the lenses had different focal lengths. Since no special attempt was made to illu­minate the subject 
uniformly, the resulting photographs exhibited considerable variation in both hue and brightness. The 
photographs were digitized using the Kodak PhotoCD process. Five typical im­ages (cropped to the size 
of the subject s head) are shown in Fig­ure 1a.  We used the interactive modeling system described in 
Sections 2 and 3 to create the same set of eight face models for each subject: happy, amused, angry, 
 surprised, sad, sleepy, pained, and neutral. Following the modeling stage, we generated a facial 
animation for each of the individuals starting from the eight original expressions. We .rst created an 
animation for J. R. We then applied the very same morphs speci.ed by this animation to the models created 
for Karla. For most frames of the animation, the resulting expressions were quite realistic. Figure 8 
shows .ve frames from the animation se­quence for J. R. and the purely automatically generated frames 
in the corresponding animation for Karla. With just a small amount of additional retouching (using the 
blending tools described in Sec­tion 4.2), this derivative animation can be made to look as good as the 
original animation for J. R.  Future work The work described in this paper is just the .rst step towards 
build­ing a complete image-based facial modeling and animation system. There are many ways to further 
enhance and extend the techniques that we have described: Color correction. For better color consistency 
in facial textures ex­tracted from photographs, color correction should be applied to si­multaneous photographs 
of each expression. Improved registration. Some residual ghosting or blurring artifacts may occasionally 
be visible in the cylindrical texture map due to small misregistrations between the images, which can 
occur if ge­ometry is imperfectly modeled or not detailed enough. To improve the quality of the composite 
textures, we could locally warp each component texture (and weight) map before blending [42]. Texture 
relighting. Currently, extracted textures re.ect the light­ing conditions under which the photographs 
were taken. Relighting techniques should be developed for seamless integration of our face models with 
other elements. Automatic modeling. Our ultimate goal, as far as the facial model­ing part is concerned, 
is to construct a fully automated modeling sys­tem, which would automatically .nd features and correspondences 
with minimal user intervention. This is a challenging problem in­deed, but recent results on 2D face 
modeling in computer vision [25] give us cause for hope. Modeling from video. We would like to be able 
to create face mod­els from video or old movie footage. For this purpose, we would have to improve the 
robustness of our techniques in order to syn­thesize face meshes and texture maps from images that do 
not cor­respond to different views of the same expression. Adding anthro­pomorphic constraints to our 
face model might make up for the lack of coherence in the data [48]. Complex animations. In order to 
create complex animations, we must extend our vocabulary for describing facial movements be­yond blending 
between different expressions. There are several po­tential ways to attack this problem. One would be 
to adopt an action-unit-based system such as the Facial Action Coding System (a) (b) Figure 8 On the 
left are frames from an original animation, which we created for J. R. The morphs speci.ed in these frames 
were then directly used to create a derivative animation for Karla, shown on the right. (FACS) [12]. 
Another possibility would be to apply modal analysis (principal component analysis) techniques to describe 
facial expres­sion changes using a small number of motions [25]. Finding natural control parameters to 
facilitate animation and developing realistic­looking temporal pro.les for such movements are also challenging 
research problems. Lip-synching. Generating speech animation with our keyframe an­imation system would 
require a large number of keyframes. How­ever, we could use a technique similar to that of Bregler et 
al. [5] to automatically lip-synch an animation to a sound-track. This would require the synthesis of 
face models for a wide range of visemes. For example, such database of models could be constructed using 
video footage to reconstruct face models automatically [17]. Performance-driven animation. Ultimately, 
we would also like to support performance-driven animation, i.e., the ability to automati­cally track 
facial movements in a video sequence, and to automat­ically translate these into animation control parameters. 
Our cur­rent techniques for registering images and converting them into 3D movements should provide a 
good start, although they will probably need to be enhanced with feature-tracking techniques and some 
rudimentary expression-recognition capabilities. Such a sys­tem would enable not only very realistic 
facial animation, but also a new level of video coding and compression techniques (since only the expression 
parameters would need to be encoded), as well as real-time control of avatars in 3D chat systems.  Acknowledgments 
We would like to thank Katrin Petersen and Andrew Petty for mod­eling the generic face model, Cassidy 
Curtis for his invaluable ad­vice on animating faces, and Joel Auslander and Jason Grif.th for early 
contributions to this project. This work was supported by an NSF Presidential Faculty Fellow award (CCR-9553199), 
an ONR Young Investigator award (N00014-95-1-0728), and industrial gifts from Microsoft and Pixar.  
References [1] Takaaki Akimoto, Yasuhito Suenaga, and Richard S. Wallace. Auto­matic Creation of 3D Facial 
Models. IEEE Computer Graphics and Applications, 13(5):16 22, September 1993. [2] Alias jWavefront, Toronto, 
Ontario. Alias V7.0, 1995. [3] Thaddeus Beier and Shawn Neely. Feature-based Image Metamorpho­sis. In 
SIGGRAPH 92 Conference Proceedings, pages 35 42. ACM SIGGRAPH, July 1992. [4] Philippe Bergeron and Pierre 
Lachapelle. Controlling Facial Ex­pressions and Body Movements in the Computer-Generated Animated Short 
Tony De Peltrie . InSIGGRAPH 85 Advanced Computer Ani­mation seminar notes. July 1985. [5] Christoph 
Bregler, Michele Covell, and Malcolm Slaney. Video Rewrite: Driving Visual Speech with Audio. In SIGGRAPH 
97 Confer­ence Proceedings, pages 353 360. ACM SIGGRAPH, August 1997. [6] David T. Chen, Andrei State, 
and David Banks. Interactive Shape Metamorphosis. In 1995 Symposium on Interactive 3D Graphics, pages 
43 44. ACM SIGGRAPH, April 1995. [7] Chang S. Choi, Kiyoharu, Hiroshi Harashima, and Tsuyoshi Takebe. 
Analysis and Synthesis of Facial Image Sequences in Model-Based Image Coding. In IEEE Transactions on 
Circuits and Systems for Video Technology, volume 4, pages 257 275. June 1994. [8] Cyberware Laboratory, 
Inc, Monterey, California. 4020/RGB 3D Scanner with Color Digitizer, 1990. [9] Paul E. Debevec, Camillo 
J. Taylor, and Jitendra Malik. Modeling and Rendering Architecture from Photographs: A Hybrid Geometry-and 
Image-Based Approach. In SIGGRAPH 96 Conference Proceedings, pages 11 20. ACM SIGGRAPH, August 1996. 
 [10] Eben Ostby, Pixar Animation Studios. Personal communication, Jan­uary 1997. [11] Paul Ekman and 
Wallace V. Friesen. Unmasking the Face. A guide to recognizing emotions fron facial clues. Prentice-Hall, 
Inc., Englewood Cliffs, New Jersey, 1975. [12] Paul Ekman and Wallace V. Friesen. Manual for the Facial 
Action Coding System. Consulting Psychologists Press, Inc., Palo Alto, Cali­fornia, 1978. [13] Irfan 
Essa, Sumit Basu, Trevor Darrell, and Alex Pentland. Modeling, Tracking and Interactive Animation of 
Faces and Heads Using Input from Video. In Computer Animation Conference, pages 68 79. June 1996. [14] 
Gary Faigin. The Artist s Complete Guide to Facial Expression. Watson-Guptill Publications, New York, 
1990. [15] Olivier Faugeras. Three-Dimensional Computer Vision: A Geometric Viewpoint. MIT Press, Cambridge, 
Massachusetts, 1993. [16] G. Golub and C. F. Van Loan. Matrix Computation, third edition.The John Hopkins 
University Press, Baltimore and London, 1996. [17] Brian Guenter, Cindy Grimm, Daniel Wood, Henrique 
Malvar, and Fr´ed´eric Pighin. Making Faces. In SIGGRAPH 98 Conference Pro­ceedings. ACM SIGGRAPH, July 
1998. [18] Pat Hanrahan and Wolfgang Krueger. Re.ection from Layered Sur­faces Due to Subsurface Scattering. 
In SIGGRAPH 93 Conference Proceedings, volume 27, pages 165 174. ACM SIGGRAPH, August 1993. [19] Bright 
Star Technologies Inc. Beginning Reading Software. Sierra On-Line, Inc., 1993. [20] Horace H. S. Ip and 
Lijun Yin. Constructing a 3D Individualized Head Model from Two Orthogonal Views. The Visual Computer, 
12:254 266, 1996. [21] Gregory Ward J., Francis M. Rubinstein, and Robert D. Clear. A Ray Tracing Solution 
for Diffuse Interre.ection. In SIGGRAPH 88 Con­ference Proceedings, volume 22, pages 85 92. August 1988. 
[22] James R. Kent, Wayne E. Carlson, and Richard E. Parent. Shape Trans­formation for Polyhedral Objects. 
In SIGGRAPH 92 Proceedings Con­ference, volume 26, pages 47 54. ACM SIGGRAPH, July 1992. [23] Rolf M. 
Koch, Markus H. Gross, Friedrich R. Carls, Daniel F. von B¨uren, George Fankhauser, and Yoav I. H. Parish. 
Simulating Facial Surgery Using Finite Element Methods. In SIGGRAPH 96 Conference Proceedings, pages 
421 428. ACM SIGGRAPH, August 1996. [24] Tsuneya Kurihara and Kiyoshi Arai. A Transformation Method for 
Modeling and Animation of the Human Face from Photographs. In Nadia Magnenat Thalmann and Daniel Thalmann, 
editors, Computer Animation 91, pages 45 58. Springer-Verlag, Tokyo, 1991. [25] A. Lanitis, C. J. Taylor, 
and T. F. Cootes. A Uni.ed Approach for Coding and Interpreting Face Images. In Fifth International Confer­ence 
on Computer Vision (ICCV 95), pages 368 373. Cambridge, Mas­sachusetts, June 1995. [26] C. L. Lawson 
and R. J. Hansen. Solving Least Squares Problems. Prentice-Hall, Englewood Cliffs, 1974. [27] Seung-Yong 
Lee, Kyung-Yong Chwa, Sung Yong Shin, and George Wolberg. Image Metamorphosis Using Snakes and Free-Form 
Defor­mations. In SIGGRAPH 95 Conference Proceedings, pages 439 448. ACM SIGGRAPH, August 1995. [28] 
Seung-Yong Lee, George Wolberg, Kyung-Yong Chwa, and Sung Yong Shin. Image Metamorphosis with Scattered 
Feature Constraints. IEEE Transactions on Visualization and Computer Graphics, 2(4), December 1996. [29] 
Yuencheng Lee, Demetri Terzopoulos, and Keith Waters. Realistic Modeling for Facial Animation. In SIGGRAPH 
95 Conference Pro­ceedings, pages 55 62. ACM SIGGRAPH, August 1995. [30] Yuencheng C. Lee, Demetri Terzopoulos, 
and Keith Waters. Con­structing Physics-Based Facial Models of Individuals. In Proceedings of Graphics 
Interface 93, pages 1 8. May 1993. [31] Francis H. Mof.tt and Edward M. Mikhail. Photogrammetry.Harper 
&#38; Row, New York, 3 edition, 1980. [32] Shree K. Nayar, Katsushi Ikeuchi, and Takeo Kanade. Shape 
from In­terre.ections. International Journal of Computer Vision, 6:173 195, 1991. [33] Gregory M. Nielson. 
Scattered Data Modeling. IEEE Computer Graphics and Applications, 13(1):60 70, January 1993. [34] Frederic 
I. Parke. Computer Generated Animation of Faces. Proceed­ings ACM annual conference., August 1972. [35] 
Frederic I. Parke. A Parametric Model for Human Faces. PhD thesis, University of Utah, Salt Lake City, 
Utah, December 1974. UTEC-CSc­75-047. [36] Frederic I. Parke and Keith Waters. Computer Facial Animation.AK 
Peters, Wellesley, Massachusetts, 1996. [37] W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. 
Vetterling. Nu­merical Recipes in C: The Art of Scienti.c Computing. Cambridge University Press, Cambridge, 
England, second edition, 1992. [38] Kari Pulli, Michael Cohen, Tom Duchamp, Hugues Hoppe, Linda Shapiro, 
and Werner Stuetzle. View-based rendering: Visualizing real objects from scanned range and color data. 
In Proc. 8th Eurographics Workshop on Rendering. June 1997. [39] Steven M. Seitz and Charles R. Dyer. 
View Morphing. In SIGGRAPH 96 Conference Proceedings, Annual Conference Series, pages 21 30. ACM SIGGRAPH, 
August 1996. [40] Chester C. Slama, editor. Manual of Photogrammetry. American So­ciety of Photogrammetry, 
Falls Church, Virginia, fourth edition, 1980. [41] Richard Szeliski and Sing Bing Kang. Recovering 3D 
Shape and Mo­tion from Image Streams using Nonlinear Least Squares. Journal of Visual Communication and 
Image Representation, 5(1):10 28, March 1994. [42] Richard Szeliski and Heung-Yeung Shum. Creating Full 
View Panoramic Image Mosaics and Texture-Mapped Models. In SIG-GRAPH 97 Conference Proceedings, pages 
251 258. ACM SIG-GRAPH, August 1997. [43] Demetri Terzopoulos and Keith Waters. Physically-based Facial 
Mod­eling, Analysis, and Animation. Journal of Visualization and Com­puter Animation, 1(4):73 80, March 
1990. [44] Kristinn R. Th´orisson. Gandalf: An Embodied Humanoid Capable of Real-Time Multimodal Dialogue 
with People. In First ACM Interna­tional Conference on Autonomous Agents. 1997. [45] Michael W. Vannier, 
Jeffrey F. Marsh, and James O. Warren. Three­dimentional Computer Graphics for Craniofacial Surgical 
Planning and Evaluation. In SIGGRAPH 83 Conference Proceedings, vol­ume 17, pages 263 273. ACM SIGGRAPH, 
August 1983. [46] Keith Waters. A Muscle Model for Animating Three-Dimensional Facial Expression. In 
SIGGRAPH 87 Conference Proceedings), vol­ume 21, pages 17 24. ACM SIGGRAPH, July 1987. [47] Lance Williams. 
Performance-Driven Facial Animation. In SIG-GRAPH 90 Conference Proceedings, volume 24, pages 235 242. 
Au­gust 1990. [48] Z. Zhang, K. Isono, and S. Akamatsu. Euclidean Structure from Uncal­ibrated Images 
Using Fuzzy Domain Knowledge: Application to Fa­cial Images Synthesis. In Proc. International Conference 
on Computer Vision (ICCV 98). January 1998. A Least squares for pose recovery To solve for a subset 
of the parameters given in Equation (2), we use linear least squares. In general, given a set of linear 
equations of the form aj .x ,bj =0, (4) we solve for the vector x by minimizing X (aj .x ,bj)2. (5) 
j Setting the partial derivative of this sum with respect to x to zero, we obtain X T (ajaj )x ,bjaj 
=0, (6) j i.e., we solve the set of normal equations [16] ! XX ajaT x = bjaj. (7) j jj More numerically 
stable methods such as QR decomposition or Singular Value Decomposition [16] can also be used to solve 
the least squares prob­lem, but we have not found them to be necessary for our application. To update 
one of the parameters, we simply pull out the relevant linear co­ef.cient aj and scalar value bj from 
Equation (2). For example, to solve for pi,weset kk kk kk kktk k a2k+0 = wi (xi 1rz ,srx), b2k+0 = wi 
(sx ,xi ) kk kk kk kktk k a2k+1 = wi (yi 1rz ,sry), b2k+1 = wi (sy ,yi ). For a scalar variable like 
sk, we obtain scalar equations , kk kkkk a2k+0 = wi (rx .pi + txk), b2k+0 = wi xi + xi 1k (rz .pi) , 
kk kkkk a2k+1 = wi (ry .pi + tyk), b2k+1 = wi yi + yi 1k(rz .pi). Similar equations for aj and bj can 
be derived for the other parameters txk , tyk , and 1k . Note that the parameters for a given camera 
k or 3D point i can be recovered independently of the other parameters. Solving for rotation is a little 
trickier than for the other parameters, since R must be a valid rotation matrix. Instead of updating 
the elements in Rk di­rectly, we replace the rotation matrix Rk with R is given by RRk [42], where Rodriguez 
s formula [15]: R ( n, B)= I +sin BX( n)+ (1 ,cos B)X2( n), (8) where Bis an incremental rotation angle, 
n is a rotation axis, and X(v)isthe cross product operator " 0 ,vz vy X(v)= vz 0 ,vx . (9) ,vy vx 0 
A .rst order expansion of R in terms of the entries in v = Bn =(vx, vy, vz)is given by I + X(v). Substituting 
into Equation (2) and letting qi = Rkpi, we obtain , kkkk k i zxx wxi + xi 1k ( r.qi) ,sk( r.qi + tk) 
= 0 (10) , kkkk k i zyy wyi + yi 1k ( r.qi) ,sk( r.qi + tk) =0, kkk where rx =(1, ,vz, vy), ry =(vz 
,1, ,vx), rz =(,vy, vx, 1), are the rows of [I + X(v)]. This expression is linear in (vx, vy, vz ), and 
hence leads to a 3X3 set of normal equations in (vx , vy, vz). Once the elements of v have been estimated, 
we can compute Band n, and update the rotation matrix using k Rk .R ( n, Bk)Rk .   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280826</article_id>
		<sort_key>85</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Subdivision surfaces in character animation]]></title>
		<page_from>85</page_from>
		<page_to>94</page_to>
		<doi_number>10.1145/280814.280826</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280826</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39045514</person_id>
				<author_profile_id><![CDATA[81100493833]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tony]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[DeRose]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39074095</person_id>
				<author_profile_id><![CDATA[81100215003]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kass]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P282127</person_id>
				<author_profile_id><![CDATA[81536894456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tien]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Truong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>192259</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[David E. Breen, Donald H. House, and Michael J. Wozny. Predicting the drape of woven cloth using interacting particles. In Andrew Glassner, editor, Proceedings of SIGGRAPH '94 (Orlando, Florida, July 24-29, i994), Computer Graphics Proceedings, Annual Conference Series, pages 365-372. ACM SIGGRAPH, ACM Press, July 1994. ISBN 0-89791- 667-0.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[E. Catmull and J. Clark. Recursively generated B-spline surfaces on arbitrary topological meshes. Computer Aided Design, 10(6):350-355, 1978.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[G. Chaikin. An algorithm for high speed curve generation. Computer Graphics and Image Processing, 3:346-349, 1974.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37414</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Robert L. Cook, Loren Carpenter, and Edwin Catmull. The Reyes image rendering architecture. In Maureen C. Stone, editor, Computer Graphics (SIGGRAPH '87 Proceedings), pages 95-102, July 1987.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218432</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Martin Courshesnes, Pascal Volino, and Nadia Magnenat Thalmann. Versatile and efficient techniques for simulating cloth and other deformable objects. In Robert Cook, editor, SIGGRAPH 95 Conference Proceedings, Annual Conference Series, pages 137-144. ACM SIGGRAPH, Addison Wesley, August 1995. held in Los Angeles, California, 06-11 August 1995.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>78958</ref_obj_id>
				<ref_obj_pid>78956</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Nira Dyn, David Leven, and John Gregory. A butterfly subdivision scheme for surface interpolation with tension control. ACM Transactions on Graphics, 9(2):160-169, April 1990.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83821</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[James D. Foley, Andries van Dam, Steven K. Feiner, and John F. Hughes. Computer Graphics: Principles and Practice. Prentice-Hall, 1990.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Mark Halstead, Michael Kass, and Tony DeRose. Efficient, fair interpolation using Catmull-Clark surfaces. Computer Graphics, 27(3):35-44, August 1993.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97903</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Pat Hanrahan and Paul E. Haeberli. Direct WYSIWYG painting and texturing on 3D shapes. In Forest Baskett, editor, Computer Graphics (SIGGRAPH '90 Proceedings), volume 24, pages 215-223, August 1990.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[H. Hoppe, T. DeRose, T. Duchamp, M. Halstead, H. Jin, J. McDonald, J. Schweitzer, and W. Stuetzle. Piecewise smooth surface reconstruction. Computer Graphics, 28(3):295-302, July 1994.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Charles T. Loop. Smooth subdivision surfaces based on triangles. Master's thesis, Department of Mathematics, University of Utah, August 1987.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325246</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Darwyn R. Peachey. Solid texturing of complex surfaces. In B. A. Barsky, editor, Computer Graphics (SIGGRAPH '85 Proceedings), volume 19, pages 279-286, July 1985.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325247</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Ken Perlin. An image synthesizer. In B. A. Barsky, editor, Computer Graphics (SIGGRAPH '85 Proceedings), volume 19, pages 287-296, July 1985.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Ulrich Reif. A unified approach to subdivision algorithms. Mathematisches Institute A 92-16, Universitaet Stuttgart, 1992.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>923941</ref_obj_id>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Jean E. Schweitzer. Analysis and Application of Subdivision Surfaces. PhD thesis, Department of Computer Science and Engineering, University of Washington, 1996.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37427</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Demetri Terzopoulos, John Platt, Alan Barr, and Kurt Fleischer. Elastically deformable models. In Maureen C. Stone, editor, Computer Graphics (SIGGRAPH '87 Proceedings), volume 21, pages 205-214, July 1987.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Steve Upstill. The RenderMan Companion. Addison-Wesley, 1990.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Andrew Witkin, David Baraff, and Michael Kass. An introduction to physically based modeling. SIGGRAPH Course Notes, Course No. 32, 1994.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>927147</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Denis Zorin. Stationary Subdivision and Multiresolution Surface Representations. PhD thesis, Caltech, Pasadena, 1997.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280828</article_id>
		<sort_key>95</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[MAPS]]></title>
		<subtitle><![CDATA[multiresolution adaptive parameterization of surfaces]]></subtitle>
		<page_from>95</page_from>
		<page_to>104</page_to>
		<doi_number>10.1145/280814.280828</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280828</url>
		<keywords>
			<kw><![CDATA[loop scheme]]></kw>
			<kw><![CDATA[mesh simplification]]></kw>
			<kw><![CDATA[meshes]]></kw>
			<kw><![CDATA[multiresolution]]></kw>
			<kw><![CDATA[remeshing]]></kw>
			<kw><![CDATA[subdivision surfaces]]></kw>
			<kw><![CDATA[surface parameterization]]></kw>
			<kw><![CDATA[texture mapping]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Viewing algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Hierarchy and geometric transformations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Object hierarchies</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011066.10011067</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Development frameworks and environments->Object oriented frameworks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010244</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Hierarchical representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010244</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Hierarchical representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P10006</person_id>
				<author_profile_id><![CDATA[81100385346]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Aaron]]></first_name>
				<middle_name><![CDATA[W. F.]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton Univ., Princeton, NJ]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P300467</person_id>
				<author_profile_id><![CDATA[81100340025]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sweldens]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bell Labs]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40023875</person_id>
				<author_profile_id><![CDATA[81100117380]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schr&#246;der]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[California Institute]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P169605</person_id>
				<author_profile_id><![CDATA[81100603447]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Lawrence]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cowsar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bell Labs]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P61269</person_id>
				<author_profile_id><![CDATA[81100388507]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dobkin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton Univ., Princeton, NJ]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BAJAJ, C. L., BERNADINI, F., CHEN, J., AND SCHIKORE, D. R. Automatic Reconstruction of 3D CAD Models. Tech. Rep. 96-015, Purdue University, February 1996.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BROWN, P. J. C., AND FAIGLE, C. T. A Robust Efficient Algorithm for Point Location in Triangulations. Tech. rep., Cambridge University, February 1997.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237213</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[CERTAIN, A., POPOVld, J., DEROSE, T., DUCHAMP, T., SALESIN, D., AND STUETZLE, W. Interactive Multiresolution Surface Viewing. In Computer Graphics (SIGGRAPH 96 P1vceedings), 91-98, 1996.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>267108</ref_obj_id>
				<ref_obj_pid>266989</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[COHEN, J., MANOCHA, D., AND OLANO, M. Simplifying Polygonal Models Using Successive Mappings. In Proceedings IEEE Visualization 97, 395-402, October 1997.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[DOBKIN, D., AND KIRKPATRICK, D. A Linear Algorithm for Determining the Separation of Convex Polyhedra. Journal of Algorithms 6 (1985), 381-392.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[DUCHAMP, T., CERTAIN, A., DEROSE, T., AND STUETZLE, W. Hierarchical Computation of PL harmonic Embeddings. Tech. rep., University of Washington, July 1997.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218440</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[ECK, M., DEROSE, T., DUCHAMP, T., HOPPE, H., LOUNSBERY, M., AND STUETZLE, W. Multiresolution Analysis of Arbitrary Meshes. In Computer Graphics (SIGGRAPH 95 P1vceedings), 173-182, 1995.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237271</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[ECK, M., AND HOPPE, H. Automatic Reconstruction of B-Spline Surfaces of Arbitrary Topological Type. In Computer Graphics (SIGGRAPH 96 P1vceedings), 325-334, 1996.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[GARLAND, M., AND HECKBERT, P. S. Fast Polygonal Approximation of Terrains and Height Fields. Tech. Rep. CMU-CS-95-181, CS Dept., Carnegie Mellon U., September 1995.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>282923</ref_obj_id>
				<ref_obj_pid>282918</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[GUIBAS, L., AND STOLFI, J. Primitives for the Manipulation of General Subdivisions and the Computation of Voronoi Diagrams. ACM Transactions on Graphics 4, 2 (April 1985), 74-123.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[HECKBERT, P. S., AND GARLAND, M. Survey of Polygonal Surface Simplification Algorithms. Tech. rep., Camegie Mellon University, 1997.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237216</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[HOPPE, H. Progressive Meshes. In Computer Graphics (SIGGRAPH 96 P1vceedings), 99-108, 1996.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258843</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[HOPPE, H. View-Dependent Refinement of Progressive Meshes. In Computer Graphics (SIGGRAPH 97 P1vceedings), 189-198, 1997.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192233</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[HOPPE, H., DEROSE, T., DUCHAMP, T., HALSTEAD, M., JIN, H., MCDON- ALD, J., SCHWEITZER, J., AND STUETZLE, W. Piecewise Smooth Surface Reconstruction. In Computer Graphics (SIGGRAPH 94 Proceedings), 295-302, 1994.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[KIRKPATRICK, D. Optimal Search in Planar Subdivisions. SIAMJ. Comput. 12 (1983), 28-35.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[KLEIN, A., CERTAIN, A., DEROSE, T., DUCHAMP, T., AND STUETZLE, W. Vertex-based Delaunay Triangulation of Meshes of Arbitrary Topological Type. Tech. rep., University of Washington, July 1997.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237270</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[KRISHNAMURTHY, V., AND LEVOY, M. Fitting Smooth Surfaces to Dense Polygon Meshes. In Computer Graphics (SIGGRAPH 96 P1vceedings), 313- 324, 1996.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[LOOP, C. Smooth Subdivision Surfaces Based on Triangles. Master's thesis, University of Utah, Department of Mathematics, 1987.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>222932</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[LOUNSBERY, M. Multiresolution Analysis for Sulfaces of Arbitrary Topological Type. PhD thesis, Department of Computer Science, University of Washington, 1994.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237750</ref_obj_id>
				<ref_obj_pid>237748</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[LOUNSBERY, M., DEROSE, T., AND WARREN, J. Multiresolution Analysis for Surfaces of Arbitrary Topological Type. Transactions on Graphics 16, 1 (January 1997), 34-73.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>871037</ref_obj_id>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[M~CKE, E.P. Shapes and Implementations in Three-Dimensional Geometry. Technical Report UIUCDCS-R-93-1836, University of Illinois at Urbana- Champaign, 1993.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218439</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[SCHRODER, P., AND SWELDENS, W. Spherical Wavelets: Efficiently Representing Functions on the Sphere. In Computer Graphics (SIGGRAPH 95 P~vceedings), Annual Conference Series, 1995.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>923941</ref_obj_id>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[SCHWEITZER, J. E. Analysis and Application of Subdivision Sulfaces. PhD thesis, University of Washington, 1996.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[SPANIER, E. H. Algebraic Topology. McGraw-Hill, New York, 1966.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>245627</ref_obj_id>
				<ref_obj_pid>244979</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[XIA, J. C., AND VARSHNEY, A. Dynamic View-Dependent Simplification for Polygonal Models. In P~vceedings Visualization 96, 327-334, October 1996.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>927147</ref_obj_id>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[ZORIN, D. Subdivision and Multiresolution Smface Representations. PhD thesis, California Institute of Technology, 1997.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237254</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[ZORIN, D., SCHRODER, P., AND SWELDENS, W. Interpolating Subdivision for Meshes with Arbitrary Topology. In Computer Graphics (SIGGRAPH 96 P1vceedings), 189-192, 1996.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258863</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[ZORIN, D., SCHRODER, P., AND SWELDENS, W. Interactive Multiresolution Mesh Editing. In Computer Graphics (SIGGRAPH 97 P1vceedings), 259-268, 1997.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 MAPS: Multiresolution Adaptive Parameterization of Surfaces Copyright &#38;#169;1998 by the Association 
for Computing Machinery, Inc. Permission to make digital or hard copies of part or all of this work for 
personal or classroom use is granted without fee provided that copies are not made or distributed for 
profit or commercial advantage and that copies bear this notice and the full citation on the first page. 
Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit 
is permitted. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires 
specific permission and/or a fee. Aaron W.F.Lee. Wim Sweldens Peter Schr¨oder Lawrence Cowsar§ David 
Dobkin¶ Princeton University Bell Laboratories Caltech Bell Laboratories Princeton University Abstract 
We construct smooth parameterizations of irregular connectivity tri­angulations of arbitrary genus 2-manifolds. 
Our algorithm uses hi­erarchical simpli.cation to ef.ciently induce a parameterization of the original 
mesh over a base domain consisting of a small num­ber of triangles. This initial parameterization is 
further improved through a hierarchical smoothing procedure based on Loop sub­division applied in the 
parameter domain. Our method supports both fully automatic and user constrained operations. In the lat­ter, 
we accommodate point and edge constraints to force the align­ .wailee@cs.princeton.edu wim@bell-labs.com 
ps@cs.caltech.edu §cowsar@bell-labs.com ¶dpd@cs.princeton.edu Figure 1: Overview of our algorithm. Top 
left: a scanned input mesh (courtesy Cyber­ware). Next the parameter or base domain, obtained through 
mesh simpli.cation. Top right: regions of the original mesh colored according to their assigned base 
domain triangle. Bottom left: adaptive remesh­ing with subdivision connectivity (e = 1%). Bottom middle: 
multiresolution edit. ment of iso-parameter lines with desired features. We show how to use the parameterization 
for fast, hierarchical subdivision con­nectivity remeshing with guaranteed error bounds. The remeshing 
algorithm constructs an adaptively subdivided mesh directly with­out .rst resorting to uniform subdivision 
followed by subsequent sparsi.cation. It thus avoids the exponential cost of the latter. Our parameterizations 
are also useful for texture mapping and morphing applications, among others. CR Categories and Subject 
Descriptors: I.3.3 [Computer Graphics]: Picture/Image Generation Display Algorithms, Viewing Algorithms; 
I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling -Curve, Surface, Solid and Object 
Representations, Hierarchy and Geometric Transformations, Object Hierarchies. Additional Key Words and 
Phrases: Meshes, surface parameterization, mesh sim­pli.cation, remeshing, texture mapping, multiresolution, 
subdivision surfaces, Loop scheme. 1 Introduction Dense triangular meshes routinely result from a number 
of 3D ac­quisition techniques, e.g., laser range scanning and MRI volumetric imaging followed by iso-surface 
extraction (see Figure 1 top left). The triangulations form a surface of arbitrary topology genus, boundaries, 
connected components and have irregular connectiv­ity. Because of their complex structure and tremendous 
size, these meshes are awkward to handle in such common tasks as storage, display, editing, and transmission. 
Multiresolution representations are now established as a funda­mental component in addressing these issues. 
Two schools exist. One approach extends classical multiresolution analysis and subdi­vision techniques 
to arbitrary topology surfaces [19, 20, 7, 3]. The alternative is more general and is based on sequential 
mesh simpli.­cation, e.g., progressive meshes (PM) [12]; see [11] for a review. In either case, the objective 
is to represent triangulated 2-manifolds in an ef.cient and .exible way, and to use this description 
in fast algo­rithms addressing the challenges mentioned above. Our approach .ts in the .rst group, but 
draws on ideas from the second group. An important element in the design of algorithms which manip­ulate 
mesh approximations of 2-manifolds is the construction of nice parameterizations when none are given. 
Ideally, the man­ifold is parameterized over a base domain consisting of a small number of triangles. 
Once a surface is understood as a function from the base domain into R3 (or higher-D when surface attributes 
are considered), many tools from areas such as approximation the­ory, signal processing, and numerical 
analysis are at our disposal. In particular, classical multiresolution analysis can be used in the design 
and analysis of algorithms. For example, error controlled, adaptive remeshing can be performed easily 
and ef.ciently. Fig­ure 1 shows the outline of our procedure: beginning with an irregu­lar input mesh 
(top left), we .nd a base domain through mesh sim­pli.cation (top middle). Concurrent with simpli.cation, 
a mapping is constructed which assigns every vertex from the original mesh to a base triangle (top right). 
Using this mapping an adaptive remesh with subdivision connectivity can be built (bottom left) which 
is now suitable for such applications as multiresolution editing (bot­tom middle). Additionally, there 
are other practical payoffs to good parameterizations, for example in texture mapping and morphing. In 
this paper we present an algorithm for the fast computation of smooth parameterizations of dense 2-manifold 
meshes with ar­bitrary topology. Speci.cally, we make the following contribu­tions We describe an O(N 
logN) time and storage algorithm to con­struct a logarithmic level hierarchy of arbitrary topology, ir­regular 
connectivity meshes based on the Dobkin-Kirkpatrick (DK) algorithm. Our algorithm accommodates geometric 
crite­ria such as area and curvature as well as vertex and edge con­straints.  We construct a smooth 
parameterization of the original mesh over the base domain. This parameterization is derived through 
repeated conformal remapping during graph simpli.cation fol­lowed by a parameter space smoothing procedure 
based on the Loop scheme. The resulting parameterizations are of high visual and numerical quality. 
 Using the smooth parameterization, we describe an algorithm for adaptive, hierarchical remeshing of 
arbitrary meshes into subdivision connectivity meshes. The procedure is fully auto­matic, but also allows 
for user intervention in the form of .xing point or path features in the original mesh. The remeshed 
man­ifold meets conservative approximation bounds.  Even though the ingredients of our construction 
are reminiscent of mesh simpli.cation algorithms, we emphasize that our goal is not the construction 
of another mesh simpli.cation procedure, but rather the construction of smooth parameterizations. We 
are par­ticularly interested in using these parameterizations for remeshing, although they are useful 
for a variety of applications. 1.1 Related Work A number of researchers have considered either explicitly 
or implicitly the problem of building parameterizations for arbitrary topology, triangulated surfaces. 
This work falls into two main cat­egories: (1) algorithms which build a smoothly parameterized ap­proximation 
of a set of samples (e.g. [14, 1, 17]), and (2) algorithms which remesh an existing mesh with the goal 
of applying classical multiresolution approaches [7, 8]. A related, though quite different problem, is 
the maintenance of a given parameterization during mesh simpli.cation [4]. We em­phasize that our goal 
is the construction of mappings when none are given. In the following two sections, we discuss related 
work and con­trast it to our approach. 1.1.1 Approximation of a Given Set of Samples Hoppe and co-workers 
[14] describe a fully automatic algorithm to approximate a given polyhedral mesh with Loop subdivision 
patches [18] respecting features such as edges and corners. Their algorithm uses a non-linear optimization 
procedure taking into ac­count approximation error and the number of triangles of the base domain. The 
result is a smooth parameterization of the original polyhedral mesh over the base domain. Since the approach 
only uses subdivision, small features in the original mesh can only be re­solved accurately by increasing 
the number of triangles in the base domain accordingly. A similar approach, albeit using A-patches, was 
described by Bajaj and co-workers [1]. From the point of view of constructing parameterizations, the 
main drawback of algorithms in this class is that the number of triangles in the base domain de­pends 
heavily on the geometric complexity of the goal surface. This problem was addressed in work of Krishnamurthy 
and Levoy [17]. They approximate densely sampled geometry with bi­cubic spline patches and displacement 
maps. Arguing that a fully automatic system cannot put iso-parameter lines where a skilled animator would 
want them, they require the user to lay out the en­tire network of top level spline patch boundaries. 
A coarse to .ne matching procedure with relaxation is used to arrive at a high qual­ity patch mesh whose 
base domain need not mimic small scale ge­ometric features. The principal drawback of their procedure 
is that the user is re­quired to de.ne the entire base domain rather then only selected features. Additionally, 
given that the procedure works from coarse to .ne, it is possible for the procedure to latch onto the 
wrong surface in regions of high curvature [17, Figure 7]. 1.1.2 Remeshing Lounsbery and co-workers 
[19, 20] were the .rst to propose al­gorithms to extend classical multiresolution analysis to arbitrary 
topology surfaces. Because of its connection to the mathematical foundations of wavelets, this approach 
has proven very attractive (e.g. [22, 7, 27, 8, 3, 28]). The central requirement of these meth­ods is 
that the input mesh have subdivision connectivity. This is generally not true for meshes derived from 
3D scanning sources. To overcome this problem, Eck and co-workers [7] developed an algorithm to compute 
smooth parameterizations of high resolu­tion polyhedral meshes over a low face count base domain. Using 
such a mapping, the original surface can be remeshed using subdi­vision connectivity. After this conversion 
step, adaptive simpli.ca­tion, compression, progressive transmission, rendering, and editing become simple 
and ef.cient operations [3, 8, 28]. Eck et al. arrive at the base domain through a Voronoi tiling of 
the original mesh. Using a sequence of local harmonic maps, a param­eterization which is smooth over 
each triangle in the base domain and which meets with C0 continuity at base domain edges [7, Plate 1(f)] 
is constructed. Runtimes for the algorithm can be long be­cause of the many harmonic map computations. 
This problem was recently addressed by Duchamp and co-workers [6], who reduced the harmonic map computations 
from their initial O(N2) complex­ity to O(N logN) through hierarchical preconditioning. The hier­archy 
construction they employed for use in a multigrid solver is related to our hierarchy construction. The 
initial Voronoi tile construction relies on a number of heuris­tics which render the overall algorithm 
fragile (for an improved version see [16]). Moreover, there is no explicit control over the number of 
triangles in the base domain or the placement of patch boundaries. The algorithm generates only uniformly 
subdivided meshes which later can be decimated through classical wavelet methods. Many extra globally 
subdivided levels may be needed to resolve one small local feature; moreover, each additional level quadruples 
the amount of work and storage. This can lead to the intermedi­ate construction of many more triangles 
than were contained in the input mesh.  1.2 Features of MAPS Our algorithm was designed to overcome 
the drawbacks of previ­ous work as well as to introduce new features. We use a fast coar­si.cation strategy 
to de.ne the base domain, avoiding the potential dif.culties of .nding Voronoi tiles [7, 16]. Since our 
algorithm pro­ceeds from .ne to coarse, correspondence problems found in coarse to .ne strategies [17] 
are avoided, and all features are correctly re­solved. We use conformal maps for continued remapping 
during coarsi.cation to immediately produce a global parameterization of the original mesh. This map 
is further improved through the use of a hierarchical Loop smoothing procedure obviating the need for 
iterative numerical solvers [7]. Since the procedure is performed globally, derivative discontinuities 
at the edges of the base domain are avoided [7]. In contrast to fully automatic methods [7], the al­gorithm 
supports vertex and edge tags [14] to constrain the param­eterization to align with selected features; 
however, the user is not required to specify the entire patch network [17]. During remeshing we take 
advantage of the original .ne to coarse hierarchy to output a sparse, adaptive, subdivision connectivity 
mesh directly without resorting to a depth .rst oracle [22] or the need to produce a uni­form subdivision 
connectivity mesh at exponential cost followed by wavelet thresholding [3].  2 Hierarchical Surface 
Representation In this section we describe the main components of our algorithm, coarsi.cation and map 
construction. We begin by .xing our nota­tion. 2.1 Notation When describing surfaces mathematically, 
it is useful to separate the topological and geometric information. To this end we in­troduce some notation 
adapted from [24]. We denote a triangu­lar mesh as a pair (P ,K ),where P is a set of N point positions 
pi =(xi,yi,zi)E R3 with 1 v i v N,and K is an abstract simplicial complex which contains all the topological, 
i.e., adjacency informa­tion. The complex K is a set of subsets of {1,...,N}. These sub­sets are called 
simplices and come in 3 types: vertices v ={i}E K , edges e ={i, j}E K , and faces f ={i, j,k}E K , so 
that any non­empty subset of a simplex of K is again a simplex of K , e.g., if a face is present so are 
its edges and vertices. Let ei denote the standard i-th basis vector in RN . For each sim­plex s, its 
topological realization |s| is the strictly convex hull of {ei | i E s}. Thus |{i}| =ei, |{i, j}| is 
the open line segment be­tween ei and ej,and |{i, j,k}| is an open equilateral triangle. The topological 
realization |K | is de.ned as UsEK |s|.The geometric realization .(|K |)relies on a linear map . : RN 
. R3 de.ned by .(ei)=pi. The resulting polyhedron consists of points, segments, and triangles in R3. 
Two vertices {i} and { j} are neighbors if {i, j}E K .A set of vertices is independent if no two vertices 
are neighbors. A set of vertices is maximally independent if no larger independent set con­tains it (see 
Figure 3, left side). The 1-ring neighborhood of a vertex {i} is the set N (i)={j |{i, j}E K }. The outdegree 
Ki of a vertex is its number of neighbors. The star of a vertex {i} is the set of simplices star (i)=s. 
iEs,sEK We say that |K| is a two dimensional manifold (or 2-manifold) with boundaries if for each i, 
|star(i)| is homeomorphic to a disk (interior vertex) or half-disk (boundary vertex) in R2. An edge e 
={i, j} is called a boundary edge if there is only one face f with e c f . We de.ne a conservative curvature 
estimate, .(i)=|.1|+|.2| at pi, using the principal curvatures .1 and .2. These are estimated by the 
standard procedure of .rst establishing a tangent plane at pi and then using a second degree polynomial 
to approximate .(|star(i)|). 2.2 Mesh Hierarchies An important part of our algorithm is the construction 
of a mesh hierarchy. The original mesh (P ,K )=(PL ,K L) is successively simpli.ed into a series of homeomorphic 
meshes (P l ,K l )with 0 v l <L,where (P 0 ,K 0)is thecoarsest orbasemesh(seeFigure4). Several approaches 
for such mesh simpli.cation have been pro­posed, most notably progressive meshes (PM) [12]. In PM the 
basic operation is the edge collapse. A sequence of such atomic oper­ations is prioritized based on approximation 
error. The linear se­quence of edge collapses can be partially ordered based on topolog­ical dependence 
[25, 13], which de.nes levels in a hierarchy. The depth of these hierarchies appears reasonable in practice, 
though can vary considerably for the same dataset [13]. Our approach is similar in spirit, but inspired 
by the hierarchy proposed by Dobkin and Kirkpatrick (DK) [5], which guarantees that the number of levels 
L is O(logN). While the original DK hi­erarchy is built for convex polyhedra, we show how the idea behind 
DK can be used for general polyhedra. The DK atomic simpli.­cation step is a vertex remove, followed 
by a retriangulation of the hole. The two basic operations vertex remove and edge collapse are related 
since an edge collapse into one of its endpoints corre­sponds to a vertex remove with a particular retriangulation 
of the resulting hole (see Figure 2). The main reason we chose an algo­rithm based on the ideas of the 
DK hierarchy is that it guarantees a logarithmic bound on the number of levels. However, we empha­size 
that the ideas behind our map constructions apply equally well to PM type algorithms.  2.3 Vertex Removal 
One DK simpli.cation step K l . K l-1 consists of removing a maximally independent set of vertices with 
low outdegree (see Fig­ure 3). To .nd such a set, the original DK algorithm used a greedy approach based 
only on topological information. Instead, we use a priority queue based on both geometric and topological 
informa­tion. At the start of each level of the original DK algorithm, none of the vertices are marked 
and the set to be removed is empty. The algorithm randomly selects a non-marked vertex of outdegree less 
than 12, removes it and its star from K l , marks its neighbors as Vertex removal followed by retriangulation 
 Half edge collapse as vertex removal with special retriangulation  Mesh at level l Mesh at level l-1 
Figure 3: On the left a mesh with a maximally independent set of vertices marked by heavy dots. Each 
vertex in the independent set General Edge collapse operation Figure 2: Examples of different atomic 
mesh simpli.cation steps. At the top vertex removal, in the middle half-edge collapse, and edge collapse 
at the bottom. unremovable and iterates this until no further vertices can be re­moved. In a triangulated 
surface the average outdegree of a vertex is 6. Consequently, no more than half of the vertices can be 
of out­degree 12 or more. Thus it is guaranteed that at least 1/24 of the vertices will be removed at 
each level [5]. In practice, it turns out one can remove roughly 1/4 of the vertices re.ecting the fact 
that the graph is four-colorable. Given that a constant fraction can be removed on each level, the number 
of levels behaves as O(logN). The entire hierarchy can thus be constructed in linear time. In our approach, 
we stay in the DK framework, but replace the random selection of vertices by a priority queue based on 
geometric information. Roughly speaking, vertices with small and .at 1-ring neighborhoods will be chosen 
.rst. At level l,for a vertex pi E P l , we consider its 1-ring neighborhood .(|star(i)|)and compute 
its area a(i) and estimate its curvature .(i). These quantities are computed relative to K l , the current 
level. We assign a priority to {i}inversely proportional to a convex combination of relative area and 
curvature a(i) .(i) w(.,i)=. +(1 -.) . maxpi EP la(i)maxpiEP l .(i) (We found . =1/2 to work well in 
our experiments.) Omitting all vertices of outdegree greater than 12 from the queue, removal of a constant 
fraction of vertices is still guaranteed. Because of the sort implied by the priority queue, the complexity 
of building the entire hierarchy grows to O(N logN). Figure 4 shows three stages (original, intermediary, 
coarsest) of the DK hierarchy. Given that the coarsest mesh is homeomorphic to the original mesh, it 
can be used as the domain of a parameteri­zation. has its respective star highlighted. Note that the 
star s of the inde­pendent set do not tile the mesh (two triangles are left white). The right side gives 
the retriangulation after vertex removal.  2.4 Flattening and Retriangulation To .nd K l-1, we need 
to retriangulate the holes left by removing the independent set. One possibility is to .nd a plane into 
which to project the 1-ring neighborhood .(|star(i)|)of a removed vertex .(|i|)without overlapping triangles 
and then retriangulate the hole in that plane. However, .nding such a plane, which may not even exist, 
can be expensive and involves linear programming [4]. Instead, we use the conformal map za [6] which 
minimizes met­ric distortion to map the neighborhood of a removed vertex into the plane. Let {i}be a 
vertex to be removed. Enumerate cycli­cally the Ki vertices in the 1-ring N (i)={jk |1 vk vKi}such that 
{jk-1,i, jk}EK l with j0 = jKi . A piecewise linear approx­ a imation of z, which we denote by µi, is 
de.ned by its values for the center point and 1-ring neighbors; namely, µi(pi)=0and a µi(pjk )=rk exp(i.ka),where 
rk =.pi -p jk ., k .k = . (pjl-1 , pi, pjl ), * l=1 and a =2p/.Ki . The advantages of the conformal map 
are numer­ous: it always exists, it is easy to compute, it minimizes metric distortion, and it is a bijection 
and thus never maps two triangles on top of each other. Once the 1-ring is .attened, we can retriangulate 
the hole using, for example, a constrained Delaunay triangulation (CDT) (see Figure 5). This tells us 
how to build K l-1. When the vertex to be removed is a boundary vertex, we map to a half disk by setting 
a =p/.Ki (assuming j1 and jKi are boundary vertices and setting .1 =0). Retriangulation is again performed 
with a CDT.  3 Initial Parameterization To .nd a parameterization, we begin by constructing a bijection 
. from .(|K L|)to .(|K 0|). The parameterization of the original mesh over the base domain follows from 
.-1(.(|K 0|)).In other words, the mapping of a point p E.(|K L|) through . is a point 0 p=.(v)E.(|K 0|), 
which can be written as 0 p=a pi +ß pj +. pk, Original mesh (level 14) 3 space Intermediate mesh (level 
6) Coarsest mesh (level 0) Figure 4: Example of a modi.ed DK mesh hierarchy. At the top the .nest (original) 
mesh .(|K L|) followed by an intermediate mesh, and the coarsest (base) mesh .(|K 0|) at the bottom (orig­inal 
dataset courtesy University of Washington). where {i, j,k}EK 0 is a face of the base domain and a, ß 
and . are barycentric coordinates, i.e., a +ß +. =1. The mapping can be computed concurrently with the 
hierarchy construction. The basic idea is to successively compute piecewise linear bijections .l between 
.(|K L|)and .(|K l |)starting with .L , which is the identity, and ending with .0 =.. Notice that we 
only need to compute the value of .l at the ver­tices of K L. At any other point it follows from piecewise 
linearity.1 Assume we are given .l and want to compute .l-1. Each vertex {i}EK L falls into one of the 
following categories: 1. {i}EK l-1: The vertex is not removed on level l and sur­vives on level l -1. 
In this case nothing needs to be done. .l-1(pi)=.l (pi)=pi. 2. {i}EK l \K l-1: The vertex gets removed 
when going from l to l -1. Consider the .attening of the 1-ring around pi (see Figure 5). After retriangulation, 
the origin lies in a triangle which corresponds to some face t ={j,k,m}EK l-1 and has barycentric coordinates 
(a,ß,.)with respect to the vertices of  1In the vicinity of vertices in K l a triangle {i, j,k}EK L 
can straddle multiple triangles in K l . In this case the map depends on the .attening strategy used 
(see Section 2.4). Flattening into parameter plane retriangulation Figure 5: In order to remove a vertex 
pi, its star (i)is mapped from 3-space to a plane using the map za. In the plane the central vertex is 
removed and the resulting hole retriangulated (bottom right). k assign barycentric coordinates to old 
point in new triangle j m Figure 6: After retriangulation of a hole in the plane (see Figure 5), the 
just removed vertex gets assigned barycentric coordinates with respect to the containing triangle on 
the coarser level. Similarly, all the .nest level vertices that were mapped to a triangle of the hole 
now need to be reassigned to a triangle of the coarser level. that face, i.e., aµi(pj)+ßµi(pk)+.µi(pm)(see 
Figure 6). In that case, let .l-1(pi)=a pj +ß pk +. pm. 3. {i}E K L \K l : The vertex was removed earlier, 
thus .l (pi)= a.pj. +ß.pk. +..pm. for some triangle t. = {j. ,k. ,m.}EK l .If t.EK l-1, nothing needs 
to be done; oth­erwise, the independent set guarantees that exactly one ver­tex of t. is removed, say 
{j.}. Consider the conformal map µj. (Figure 6). After retriangulation, the µj.(pi)lies in a tri­ angle 
which corresponds to some face t ={j,k,m}EK l-1 with barycentric coordinates (a,ß,.)(black dots within 
high­lighted face in Figure 6). In that case, let .l-1(pi)=a pj + ß pk +. pm (i.e., all vertices in Figure 
6 are reparameterized in this way). Note that on every level, the algorithm requires a sweep through 
all the vertices of the .nest level resulting in an overall complexity of O(N logN). Figure 7 visualizes 
the mapping we just computed. For each point pi from the original mesh, its mapping .(pi)is shown with 
a dot on the base domain. Caution: Given that every association between a 1-ring and its retriangulated 
hole is a bijection, so is the mapping ..However, 3 space Figure 7: Base domain .(|K 0|). For each point 
pi from the original mesh, its mapping .(pi) is shown with a dot on the base domain. . does not necessarily 
map a .nest level triangle to a triangular region in the base domain. Instead the image of a triangle 
may be a non-convex region. In that case connecting the mapped vertices with straight lines can cause 
.ipping, i.e., triangles may end up on top of each other (see Figure 8 for an example). Two methods ex­ist 
for dealing with this problem. First one could further subdivide the original mesh in the problem regions. 
Given that the underlying continuous map is a bijection, this is guaranteed to .x the prob­lem. The alternative 
is to use some brute force triangle un.ipping mechanism. We have found the following scheme to work well: 
adjust the parameter values of every vertex whose 2-neighborhood contains a .ipped triangle, by replacing 
them with the averaged pa­rameter values of its 1-ring neighbors [7]. original mesh mapping onto base 
domain image of vertices image of triangle Figure 8: Although the mapping . from the original mesh to 
a base domain triangle is a bijection, triangles do not in general get mapped to triangles. Three vertices 
of the original mesh get mapped to a concave con.guration on the base domain, causing the piecewise linear 
approximation of the map to .ip the triangle. 3.1 Tagging and Feature Lines In the algorithm described 
so far, there is no apriori control over which vertices end up in the base domain or how they will be 
con­nected. However, often there are features which one wants to pre­serve in the base domain. These 
features can either be detected automatically or speci.ed by the user. We consider two types of features 
on the .nest mesh: vertices and paths of edges. Guaranteeing that a certain vertex of the orig­inal mesh 
ends up in the base domain is straightforward. Simply mark that vertex as unremovable throughout the 
DK hierarchy. We now describe an algorithm to guarantee that a certain path of edges on the .nest mesh 
gets mapped to an edge of the base do­main. Let {vi |1 vi vI}cK L be a set of vertices on the .nest level 
which form a path, i.e., {vi,vi+1}is an edge. Tag all the edges in the path as feature edges. First tag 
v1 and vI , so called dart points [14], as unremovable so they are guaranteed to end up in the base do­main. 
Let vi be the .rst vertex on the interior of the path which gets marked for removal in the DK hierarchy, 
say, when going from Flattening into parameter plane retriangulation Figure 9: When a vertex with two 
incident feature edges is removed, we want to ensure that the subsequent retriangulation adds a new feature 
edge to replace the two old ones. level l to l -1. Because of the independent set property, vi-1 and 
vi+1 cannot be removed and therefore must belong to K l-1.When .attening the hole around vi, tagged edges 
are treated like a bound­ary. We .rst straighten out the edges {vi-1,vi}and {vi,vi+1}along the x-axis, 
and use two boundary type conformal maps to the half disk above and below (cf. the last paragraph of 
Section 2.4). When retriangulating the hole around vi, we put the edge {vi-1,vi+1} in K l-1, tag it as 
a feature edge, and compute a CDT on the upper and lower parts (see Figure 9). If we apply similar procedures 
on coarser levels, we ensure that v1 and vI remain connected by a path (potentially a single edge) on 
the base domain. This guarantees that . maps the curved feature path onto the coarsest level edge(s) 
between v1 and vI . In general, there will be multiple feature paths which may be closed or cross each 
other. As usual, a vertex with more than 2 incident feature edges is considered a corner, and marked 
as unre­movable. The feature vertices and paths can be provided by the user or detected automatically. 
As an example of the latter case, we con­sider every edge whose dihedral angle is below a certain threshold 
to be a feature edge, and every vertex whose curvature is above a certain threshold to be a feature vertex. 
An example of this strategy is illustrated in Figure 13. 3.2 A Quick Review Before we consider the problem 
of remeshing, it may be helpful to review what we have at this point. We have established an ini­tial 
bijection . of the original surface .(|K L|) onto a base domain .(|K 0|) consisting of a small number 
of triangles (e.g. Figure 7). We use a simpli.cation hierarchy (Figure 4) in which the holes af­ter vertex 
removal are .attened and retriangulated (Figures 5 and 9). Original mesh points get successively reparametrized 
over coarser triangulations (Figure 6). The resulting mapping is always a bijec­tion; triangle .ipping 
(Figure 8) is possible but can be corrected.  4 Remeshing In this section, we consider remeshing using 
subdivision connectiv­ity triangulations since it is both a convenient way to illustrate the properties 
of a parameterization and is an important subject in its own right. In the process, we compute a smoothed 
version of our initial parameterization. We also show how to ef.ciently construct an adaptive remeshing 
with guaranteed error bounds. 4.1 Uniform Remeshing Since . is a bijection, we can use .-1 to map the 
base domain to the original mesh. We follow the strategy used in [7]: regu­larly (1:4) subdivide the 
base domain and use the inverse map to obtain a regular connectivity remeshing. This introduces a hierar­chy 
of regular meshes (Q m ,R m) (Q is the point set and R is the complex) obtained from m-fold midpoint 
subdivision of the base domain (P 0 ,K 0)=(Q 0 ,R 0). Midpoint subdivision implies that all new domain 
points lie in the base domain, Q m c .(|R 0|) and |R m| = |R 0|. All vertices of R m \R 0 have outdegree 
6. The uniform remeshing of the original mesh on level m is given by (.-1(Q m),R m). We thus need to 
compute .-1(q) where q is a point in the base domain with dyadic barycentric coordinates. In particular, 
we need to compute which triangle of .(|K L|) contains .-1(q), or, equiv­alently, which triangle of .(.(|K 
L|)) contains q.This is a stan­dard point location problem in an irregular triangulation. We use the 
point location algorithm of Brown and Faigle [2] which avoids looping that can occur with non-Delaunay 
meshes [10, 9]. Once we have found the triangle {i, j,k} which contains q, we can write q as q =a.(pi)+ß.(pj 
)+..(pk), and thus .-1(q)=api +ßpj +.pk E .(|K L|). Figure 10 shows the result of this procedure: a level 
3 uniform remeshing of a 3-holed torus using the .-1 map. A note on complexity: The point location algorithm 
is essen­ . tially a walk on the .nest level mesh with complexity O( N).Hi­erarchical point location 
algorithms, which have asymptotic com­plexity O(logN), exist [15] but have a much larger constant. Given 
that we schedule the queries in a systematic order, we almost always have an excellent starting guess 
and observe a constant number of steps. In practice, the .nest level walking algorithm beats the hi­erarchical 
point location algorithms for all meshes we encountered (upto100K faces). Figure 10: Remeshing of 3 holed 
torus using midpoint subdivision. The parameterization is smooth within each base domain triangle, but 
clearly not across base domain triangles. 4.2 Smoothing the Parameterization It is clear from Figure 
10 that the mapping we used is not smooth across global edges. One way to obtain global smoothness is 
to consider a map that minimizes a global smoothness functional and goes from .(|K L|) to |K 0| rather 
than to .(|K 0|). This would require an iterative PDE solver. We have found computation of mappings to 
topological realizations that live in a high dimensional space to be needlessly cumbersome. Instead, 
we use a much simpler and cheaper smoothing tech­nique based on Loop subdivision. The main idea is to 
compute .-1 at a smoothed version of the dyadic points, rather then at the dyadic points themselves (which 
can equivalently be viewed as changing the parameterization). To that end, we de.ne a map L from the 
base domain to itself by the following modi.cation of Loop: If all the points of the stencil needed 
for computing either a new point or smoothing an old point are inside the same triangle of the base domain, 
we can simply apply the Loop weights and the new points will be in that same face.  If the stencil stretches 
across two faces of the base domain, we .atten them out using a hinge map at their common edge. We then 
compute the point s position in this .attened domain and extract the triangle in which the point lies 
together with its barycentric coordinates.  If the stencil stretches across multiple faces, we use the 
confor­  mal .attening strategy discussed earlier. Note that the modi.cations to Loop force L to map 
the base do­main onto the base domain. We emphasize that we do not apply the classic Loop scheme (which 
would produce a blobby version of the base domain). Nor are the surface approximations that we later 
produce Loop surfaces. The composite map .-1 o L is our smoothed parameterization that maps the base 
domain onto the original surface. The m-th level of uniform remeshing with the smoothed parameterization 
is (.-1 o L(Qm),R m),where Qm, as before, are the dyadic points on the base domain. Figure 11 shows the 
result of this procedure: a level 3 uniform remeshing of a 3-holed torus using the smoothed parameterization. 
When the mesh is tagged, we cannot apply smoothing across the tagged edges since this would break the 
alignment with the features. Therefore, we use modi.ed versions of Loop which can deal with corners, 
dart points and feature edges [14, 23, 26] (see Figure 13). Figure 11: The same remeshing of the 3-holed 
torus as in Figure 10, but this time with respect to a Loop smoothed parameterization. Note: Because 
the Loop scheme only enters in smoothing the pa­rameterization the surface shown is still a sampling 
of the original mesh, not a Loop surface approximation of the original. 4.3 Adaptive Remeshing One of 
the advantages of meshes with subdivision connectivity is that classical multiresolution and wavelet 
algorithms can be em­ployed. The standard wavelet algorithms used, e.g., in image com­pression, start 
from the .nest level, compute the wavelet transform, and then obtain an ef.cient representation by discarding 
small wavelet coef.cients. Eck et al. [7, 8] as well as Certain et al. [3] fol­low a similar approach: 
remesh using a uniformly subdivided grid followed by decimation through wavelet thresholding. This has 
the drawback that in order to resolve a small local feature on the origi­nal mesh, one may need to subdivide 
to a very .ne level. Each extra level quadruples the number of triangles, most of which will later be 
decimated using the wavelet procedure. Imagine, e.g., a plane which is coarsely triangulated except for 
a narrow spike. Making the spike width suf.ciently small, the number of levels needed to resolve it can 
be made arbitrarily high. In this section we present an algorithm which avoids .rst build­ing a full 
tree and later pruning it. Instead, we immediately build the adaptive mesh with a guaranteed conservative 
error bound. This is possible because the DK hierarchy contains the information on how much subdivision 
is needed in any given area. Essentially, we let the irregular DK hierarchy drive the adaptive construction 
of the regular pyramid. We .rst compute for each triangle t E K 0 the following error quantity: E(t)= 
max dist(pi,.(|t|)). piEP Land .(pi)E.(|t|) This measures the distance between one triangle in the base 
domain and the vertices of the .nest level mapped to that triangle. The adaptive algorithm is now straightforward. 
Set a certain rel­ative error threshold e. Compute E(t) for all triangles of the base domain. If E(t)/B,where 
B is the largest side of the bounding box, is larger than e, subdivide the domain triangle using the 
Loop pro­cedure above. Next, we need to reassign vertices to the triangles of level m = 1. This is done 
as follows: For each point pi E P L con­sider the triangle t of K 0 to which it it is currently assigned. 
Next consider the 4 children of t on level 1, tj with j = 0,1,2,3and com­pute the distance between pi 
and each of the .(|tj|). Assign pi to the closest child. Once the .nest level vertices have been reassigned 
to level 1 triangles, the errors for those triangles can be computed. Now iterate this procedure until 
all triangles have an error below the threshold. Because all errors are computed from the .nest level, 
we are guaranteed to resolve all features within the error bound. Note that we are not computing the 
true distance between the original vertices and a given approximation, but rather an easy to compute 
upper bound for it. In order to be able to compute the Loop smoothing map L on an adaptively subdivided 
grid, the grid needs to satisfy a vertex re­striction criterion, i.e., if a vertex has a triangle incident 
to it with depth i, then it must have a complete 1-ring at level i - 1 [28]. This restriction may necessitate 
subdividing some triangles even if they are below the error threshold. Examples of adaptive remeshing 
can be seen in Figure 1 (lower left), Figure 12, and Figure 13. Figure 12: Example remesh of a surface 
with boundaries.  5 Results We have implemented MAPS as described above and applied it to a number 
of well known example datasets, as well as some new ones. The application was written in C++ using standard 
compu­tational geometry data structures, see e.g. [21], and all timings re­ported in this section were 
measured on a 200 MHz PentiumPro personal computer. Figure 13: Left (top to bottom): three levels in 
the DK pyramid, .nest (L = 15) with 12946, intermediate (l = 8) with 1530, and coarsest (l = 0) with 
168 triangles. Feature edges, dart and cor­ner vertices survive on the base domain. Right (bottom to 
top): adaptive mesh with e = 5% and 1120 triangles (bottom), e = 1% and 3430 triangles (middle), and 
uniform level 3 (top). (Original dataset courtesy University of Washington.) The .rst example used throughout 
the text is the 3-holed torus. The original mesh contained 11776 faces. These were reduced in the DK 
hierarchy to 120 faces over 14 levels implying an average removal of 30% of the faces on a given level. 
The remesh of Fig­ure 11 used 4 levels of uniform subdivision for a total of 30720 triangles. The original 
sampled geometry of the 3-holed torus is smooth and did not involve any feature constraints. A more challenging 
case is presented by the fandisk shown in Figure 13. The original mesh (top left) contains 12946 triangles 
which were reduced to 168 faces in the base domain over 15 levels (25% average face removal per level). 
The initial mesh had all edges with dihedral angles be­low 75o tagged (1487 edges), resulting in 141 
tagged edges at the coarsest level. Adaptive remeshing to within e = 5% and e = 1% (fraction of longest 
bounding box side) error results in the meshes shown in the right column. The top right image shows a 
uniform resampling to level 3, in effect showing iso-parameter lines of the parameterization used for 
remeshing. Note how the iso-parameter lines conform perfectly to the initially tagged features.  This 
dataset demonstrates one of the advantages of our method inclusion of feature constraints over the earlier 
work of Eck et al. [7]. In the original PM paper [12, Figure 12], Hoppe shows the simpli.cation of the 
fandisk based on Eck s algorithm which does not use tagging. He points out that the multiresolution approxima­tion 
is quite poor at low triangle counts and consequently requires many triangles to achieve high accuracy. 
The comparison between our Figure 13 and Figure 12 in [12] demonstrates that our multires­olution algorithm 
which incorporates feature tagging solves these problems. Another example of constrained parameterization 
and subse­quent adaptive remeshing is shown in Figure 14. The original dataset (100000 triangles) is 
shown on the left. The red lines in­dicate user supplied feature constraints which may facilitate subse­quent 
animation. The green lines show some representative iso­parameter lines of our parameterization subject 
to the red fea­ture constraints. Those can be used for computing texture coor­dinates. The middle image 
shows an adaptive subdivision connec­tivity remesh with 74698 triangles (e = 0.5%). On the right we have 
highlighted a group of patches, 2 over the right (constrained) eye and 1 over the left (unconstrained) 
eye. This indicates how user supplied constraints force domain patches to align with desired fea­tures. 
Other enforced patch boundaries are the eyebrows, center of the nose, and middle of lips (see red lines 
in left image). This example illustrates how one places constraints like Krishnamurthy and Levoy [17]. 
We remove the need in their algorithms to specify the entire base domain. A user may want to control 
patch outlines for editing in one region (e.g., on the face), but may not care about what happens in 
other regions (e.g., the back of the head). We present a .nal example in Figure 1. The original mesh 
(96966 triangles) is shown on the top left, with the adaptive, subdi­vision connectivity remesh on the 
bottom left. This remesh was subsequently edited in a interactive multiresolution editing sys­tem [28] 
and the result is shown on the bottom middle.  6 Conclusions and Future Research We have described an 
algorithm which establishes smooth parame­terizations for irregular connectivity, 2-manifold triangular 
meshes of arbitrary topology. Using a variant of the DK hierarchy con­struction, we simplify the original 
mesh and use piecewise linear approximations of conformal mappings to incrementally build a parameterization 
of the original mesh over a low face count base domain. This parameterization is further improved through 
a hier­archical smoothing procedure which is based on Loop smoothing in parameter space. The resulting 
parameterizations are of high qual­ity, and we demonstrated their utility in an adaptive, subdivision 
connectivity remeshing algorithm that has guaranteed error bounds. The new meshes satisfy the requirements 
of multiresolution repre­sentations which generalize classical wavelet representations and are thus of 
immediate use in applications such as multiresolution editing and compression. Using edge and vertex 
constraints, the parameterizations can be forced to respect feature lines of interest without requiring 
speci.cation of the entire patch network. In this paper we have chosen remeshing as the primary applica­tion 
to demonstrate the usefulness of the parameterizations we pro­Dataset Input size Hierarchy Levels P 0 
size Remeshing Remesh Output size (triangles) creation (triangles) tolerance creation (triangles) 3-hole 
11776 18 (s) 14 120 (NA) 8 (s) 30720 fandisk 12946 23 (s) 15 168 1% 10 (s) 3430 fandisk 12946 23 (s) 
15 168 5% 5 (s) 1130 head 100000 160 (s) 22 180 0.5% 440 (s) 74698 horse 96966 163 (s) 21 254 1% 60 (s) 
15684 horse 96966 163 (s) 21 254 0.5% 314 (s) 63060 Table 1: Selected statistics for the examples discussed 
in the text. All times are in seconds on a 200 MHz PentiumPro. duce. The resulting meshes may also .nd 
application in numerical analysis algorithms, such as fast multigrid solvers. Clearly there are many 
other applications which bene.t from smooth parame­terizations, e.g., texture mapping and morphing, which 
would be interesting to pursue in future work. Because of its independent set selection the standard 
DK hierarchy creates topologically uniform simpli.cations. We have begun to explore how the selection 
can be controlled using geometric properties. Alternatively, one could use a PM framework to control 
geometric criteria of simpli.cation. Perhaps the most interesting question for future research is how 
to incorporate topology changes into the MAPS construction.  Acknowledgments Aaron Lee and David Dobkin 
were partially supported by NSF Grant CCR-9643913 and the US Army Research Of.ce Grant DAAH04-96-1-0181. 
Aaron Lee was also partially supported by a Wu Graduate Fellowship and a Summer Internship at Bell Lab­oratories, 
Lucent Technologies. Peter Schr¨oder was partially supported by grants from the Intel Corporation, the 
Sloan Foundation, an NSF CAREER award (ASC-9624957), a MURI (AFOSR F49620-96-1-0471), and Bell Laboratories, 
Lucent Technologies. Special thanks to Timothy Baker, Ken Clarkson, Tom Duchamp, Tom Funkhouser, Amanda 
Galtman, and Ralph Howard for many interesting and stimulation discus­sions. Special thanks also to Andrei 
Khodakovsky, Louis Thomas, and Gary Wu for invaluable help in the production of the paper. Our implementation 
uses the triangle facet data structure and code of Ernst M¨ucke. References [1] BAJAJ,C. L., BERNADINI,F., 
CHEN,J., AND SCHIKORE, D. R. Automatic Reconstruction of 3D CAD Models. Tech. Rep. 96-015, Purdue University, 
February 1996. [2] BROWN,P. J. C., AND FAIGLE, C. T. A Robust Ef.cient Algorithm for Point Location in 
Triangulations. Tech. rep., Cambridge University, February 1997. [3] CERTAIN,A., POPOVI C´ ,J., DEROSE,T., 
DUCHAMP,T., SALESIN,D., AND STUETZLE, W. Interactive Multiresolution Surface Viewing. In Computer Graphics 
(SIGGRAPH 96 Proceedings), 91 98, 1996. [4] COHEN,J., MANOCHA,D., AND OLANO, M. Simplifying Polygonal 
Models Using Successive Mappings. In Proceedings IEEE Visualization 97, 395 402, October 1997. [5] DOBKIN,D., 
AND KIRKPATRICK, D. A Linear Algorithm for Determining the Separation of Convex Polyhedra. Journal of 
Algorithms 6 (1985), 381 392. [6] DUCHAMP,T., CERTAIN,A., DEROSE,T., AND STUETZLE, W. Hierarchical Computation 
of PL harmonic Embeddings. Tech. rep., University of Washington, July 1997. [7] ECK,M., DEROSE,T., DUCHAMP,T., 
HOPPE,H., LOUNSBERY,M., AND STUETZLE, W. Multiresolution Analysis of Arbitrary Meshes. In Computer Graphics 
(SIGGRAPH 95 Proceedings), 173 182, 1995. [8] ECK,M., AND HOPPE, H. Automatic Reconstruction of B-Spline 
Surfaces of Arbitrary Topological Type. In Computer Graphics (SIGGRAPH 96 Proceed­ings), 325 334, 1996. 
[9] GARLAND,M., AND HECKBERT, P. S. Fast Polygonal Approximation of Ter­rains and Height Fields. Tech. 
Rep. CMU-CS-95-181, CS Dept., Carnegie Mel­lon U., September 1995. [10] GUIBAS,L., AND STOLFI, J. Primitives 
for the Manipulation of General Subdi­visions and the Computation of Voronoi Diagrams. ACM Transactions 
on Graph­ics 4, 2 (April 1985), 74 123. [11] HECKBERT,P. S., AND GARLAND, M. Survey of Polygonal Surface 
Simpli.­cation Algorithms. Tech. rep., Carnegie Mellon University, 1997. [12] HOPPE, H. Progressive Meshes. 
In Computer Graphics (SIGGRAPH 96 Pro­ceedings), 99 108, 1996. [13] HOPPE, H. View-Dependent Re.nement 
of Progressive Meshes. In Computer Graphics (SIGGRAPH 97 Proceedings), 189 198, 1997. [14] HOPPE,H., 
DEROSE,T., DUCHAMP,T., HALSTEAD,M., JIN,H., MCDON-ALD,J., SCHWEITZER,J., AND STUETZLE, W. Piecewise Smooth 
Surface Reconstruction. In Computer Graphics (SIGGRAPH 94 Proceedings), 295 302, 1994. [15] KIRKPATRICK, 
D. Optimal Search in Planar Subdivisions. SIAM J. Comput. 12 (1983), 28 35. [16] KLEIN,A., CERTAIN,A., 
DEROSE,T., DUCHAMP,T., AND STUETZLE,W. Vertex-based Delaunay Triangulation of Meshes of Arbitrary Topological 
Type. Tech. rep., University of Washington, July 1997. [17] KRISHNAMURTHY,V., AND LEVOY, M. Fitting Smooth 
Surfaces to Dense Polygon Meshes. In Computer Graphics (SIGGRAPH 96 Proceedings), 313 324, 1996. [18] 
LOOP, C. Smooth Subdivision Surfaces Based on Triangles. Master s thesis, University of Utah, Department 
of Mathematics, 1987. [19] LOUNSBERY,M. Multiresolution Analysis for Surfaces of Arbitrary Topological 
Type. PhD thesis, Department of Computer Science, University of Washington, 1994. [20] LOUNSBERY,M., 
DEROSE,T., AND WARREN, J. Multiresolution Analysis for Surfaces of Arbitrary Topological Type. Transactions 
on Graphics 16, 1 (January 1997), 34 73. [21] M¨ E. P. UCKE, Shapes and Implementations in Three-Dimensional 
Geome­try. Technical Report UIUCDCS-R-93-1836, University of Illinois at Urbana-Champaign, 1993. [22] 
SCHR ¨ ODER,P., AND SWELDENS, W. Spherical Wavelets: Ef.ciently Repre­senting Functions on the Sphere. 
In Computer Graphics (SIGGRAPH 95 Pro­ceedings), Annual Conference Series, 1995. [23] SCHWEITZER,J. E. 
Analysis and Application of Subdivision Surfaces.PhD thesis, University of Washington, 1996. [24] SPANIER,E. 
H. Algebraic Topology. McGraw-Hill, New York, 1966. [25] XIA,J. C., AND VARSHNEY, A. Dynamic View-Dependent 
Simpli.cation for Polygonal Models. In Proceedings Visualization 96, 327 334, October 1996. [26] ZORIN,D. 
Subdivision and Multiresolution Surface Representations.PhD the­sis, California Institute of Technology, 
1997. [27] ZORIN,D., SCHR ¨ W. ODER,P., AND SWELDENS, Interpolating Subdivision for Meshes with Arbitrary 
Topology. In Computer Graphics (SIGGRAPH 96 Proceedings), 189 192, 1996. [28] ZORIN,D., SCHR ¨ ODER,P., 
AND SWELDENS, W. Interactive Multiresolution Mesh Editing. In Computer Graphics (SIGGRAPH 97 Proceedings), 
259 268, 1997.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280831</article_id>
		<sort_key>105</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Interactive multi-resolution modeling on arbitrary meshes]]></title>
		<page_from>105</page_from>
		<page_to>114</page_to>
		<doi_number>10.1145/280814.280831</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280831</url>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003719</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computations on matrices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010148.10010149.10010158</concept_id>
				<concept_desc>CCS->Computing methodologies->Symbolic and algebraic manipulation->Symbolic and algebraic algorithms->Linear algebra algorithms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP15023875</person_id>
				<author_profile_id><![CDATA[81100137073]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Leif]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kobbelt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Erlangen-N&#252;rnberg]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P272757</person_id>
				<author_profile_id><![CDATA[81100144582]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Swen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Campagna]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Erlangen-N&#252;rnberg]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P138614</person_id>
				<author_profile_id><![CDATA[81100297877]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jens]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vorsatz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Erlangen-N&#252;rnberg]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15028898</person_id>
				<author_profile_id><![CDATA[81100315426]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hans-Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seidel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Erlangen-N&#252;rnberg]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>244999</ref_obj_id>
				<ref_obj_pid>244979</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BONNEAU, G., HAHMANN, S., AND NIELSON, G. Blac-wavelets: a multiresolution analysis with non-nested spaces. In Visualization P1vceedings (1996), pp. 43-48.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[CATMULL, E., AND CLARK, J. Rcursively Generated B-Spline Surfaces on Arbitrary Topological Meshes. Computer AidedDesign 10, 6 (Nov. 1978), 239- 248.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122746</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[CELNIKER, G., AND GOSSARD, D. Deformable curve and surface finite elements for free-form shape design. In Computer Graphics (SIGGRAPH 91 P~vceedings) (July 1991), pp. 257-265.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>130655</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[DAUBECHIES, I. Ten Lectures on Wavelets. CBMS-NSF Regional Conf. Series in Appl. Math., Vol. 61. SIAM, Philadelphia, PA, 1992.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Doo, D., AND SABIN, M. Behaviour of recursive division surfaces near extraordinary points. CAD (1978).]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>78958</ref_obj_id>
				<ref_obj_pid>78956</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[DYN, N., LEVIN, D., AND GREGORY, J. A. A butterfly subdivision scheme for surface interpolation with tension control. ACM Transactions on Graphics 9, 2 (April 1990), 160-169.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218440</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[ECK, M., DEROSE, T., DUCHAMP, T., HOPPE, H., LOUNSBERY, M., AND STUETZLE, W. Multiresolution Analysis of Arbitrary Meshes. In Computer Graphics (SIGGRAPH 95 P1vceedings) (1995), pp. 173-182.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>501891</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[FARIN, G. Curves and Smfacesfor CAGD, 3rd ed. Academic Press, 1993.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192223</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[FINKELSTEIN, A., AND SALESIN, D. H. Multiresolution Curves. In Computer Graphics (SIGGRAPH 94 P1vceedings) (July 1994), pp. 261-268.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378512</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[FORSEY, D. R., AND BARTELS, R. H. Hierarchical B-spline refinement. In Computer Graphics (SIGGRAPH 88 P1vceedings) (1988), pp. 205-212.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258849</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[GARLAND, M., AND HECKBERT, P. S. Surface Simplification Using Quadric Error Metrics. In Computer Graphics (SIGGRAPH 97 P1vceedings) (1997), pp. 209-218.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>199410</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[GORTLER, S. J., AND COHEN, M. F. Hierarchical and Variational Geometric Modeling with Wavelets. In P~vceedings Symposium on Interactive 3D Graphics (May 1995).]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[GREINER, G. Variational design and fairing of spline surfaces. Computer Graphics Forum 13 (1994), 143-154.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[HACKBUSCH, W. Multi-Grid Methods and Applications. Springer Verlag, Berlin, 1985.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237216</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[HOPPE, H. Progressive Meshes. In Computer Graphics (SIGGRAPH 96 P~vceedings) (1996), pp. 99-108.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>174506</ref_obj_id>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[HOSCHEK, J., AND LASSER, D. Fundamentals of Computer Aided Geometric Design. AK Peters, 1993.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[KOBBELT, L. Iterative Erzeugung glatter Interpolanten. Shaker Verlag, ISBN 3-8265-0540-9, 1995.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[KOBBELT, L. Interpolatory Subdivision on Open Quadrilateral Nets with Arbitrary Topology. In Computer Graphics Forum, Proceedings of Eurographics '96 (1996), pp. C407-C420.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[KOBBELT, L. Discrete fairing. In Proceedings of the Seventh IMA Conference on the Mathematics of Smfaces (1997), pp. 101-131.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[KOBBELT, L., CAMPAGNA, S., AND SEIDEL, H.-P. A general framework for mesh decimation. In Proceedings of the Graphics Interface conference '98 (1998).]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237270</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[KRISHNAMURTHY, V., AND LEVOY, M. Fitting smooth surfaces to dense polygon meshes. In Computer Graphics (SIGGRAPH 96 Proceedings) (1996), pp. 313-324.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192238</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[LOOP, C. Smooth spline surfaces over irregular meshes. In Computer Graphics Proceedings (1994), Annual Conference Series, ACM Siggraph, pp. 303-310.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237750</ref_obj_id>
				<ref_obj_pid>237748</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[LOUNSBERY, M., DEROSE, T., AND WARREN, J. Multiresolution Analysis for Surfaces of Arbitrary Topological Type. ACM Transactions on Graphics 16, 1 (January 1997), 34-73.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258847</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[LUEBKE, D., AND ERIKSON, C. View-Dependent Simplification of Arbitrary Polygonal Environments. In Computer Graphics (SIGGRAPH 97 Proceedings) (1997), pp. 199-208.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134035</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[MORETON, H., AND SI~QUIN, C. Functional optimization for fair surface design. In Computer Graphics (SIGGRAPH 92 Proceedings) (1992), pp. 167-176.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>77000</ref_obj_id>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[OPPENHEIM, A., AND SCHAFER, R. Discrete-Time Signal Processing. Prentice Hall, 1989.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[ROSSIGNAC, J. Simplification and Compression of 3D Scenes, 1997. Tutorial Eurographics '97.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>527539</ref_obj_id>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[SAPIDIS, N. E. Designing Fair Curves and Smfaces. SIAM, 1994.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[SCHABAK, R., AND WERNER, H. Numerische Mathematik. Springer Verlag, 1993.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218439</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[SCHRCIDER, P., AND SWELDENS, W. Spherical wavelets: Efficiently representing functions on the sphere. In Computer Graphics (SIGGRAPH 95 P~vceedings) (1995), pp. 161-172.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134010</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[SCHROEDER, W. J., ZARGE, J. A., AND LORENSEN, W. E. Decimation of Triangle Meshes. In Computer Graphics (SIGGRAPH 92 P~vceedings) (1992), pp. 65-70.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[STOER, J. Einfiihrung in die Numerische Mathematik I. Springer Verlag, 1983.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_obj_id>286071</ref_obj_id>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[STOLLNITZ, E., DEROSE, T., AND SALESIN, D. Wavelets for Computer Graphics. Morgan Kaufmann Publishers, 1996.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218473</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[TAUBIN, G. A signal processing approach to fair surface design. In Computer Graphics (SIGGRAPH 95 Proceedings) (1995), pp. 351-358.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134008</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[TURK, G. Re-Tiling Polygonal Surfaces. In Computer Graphics (SIGGRAPH 92 P1vceedings) (1992), pp. 55-64.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[WARREN, J. Subdivision schemes for variational splines, 1997. Preprint.]]></ref_text>
				<ref_id>36</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134033</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[WELCH, W., AND WITKIN, A. Variational surface modeling. In Computer Graphics (SIGGRAPH 92 P1vceedings) (1992), pp. 157-166.]]></ref_text>
				<ref_id>37</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192216</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[WELCH, W., AND WITKIN, A. Free-Form shape design using triangulated surfaces. In Computer Graphics (SIGGRAPH 94 Proceedings) (1994), A. Glassner, Ed., pp. 247-256.]]></ref_text>
				<ref_id>38</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237254</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[ZORIN, D., SCHRODER, P., AND SWELDENS, W. Interpolating subdivision for meshes with arbitrary topology. In Computer Graphics (SIGGRAPH 96 P~vceedings) (1996), pp. 189-192.]]></ref_text>
				<ref_id>39</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258863</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[ZORIN, D., SCHRoDER, P., AND SWELDENS, W. Interactive multiresolution mesh editing. In Computer Graphics (SIGGRAPH 97 P1vceedings) (1997), pp. 259-268.]]></ref_text>
				<ref_id>40</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Copyright &#38;#169;1998 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers or to redistribute to lists, requires specific permission and/or a fee. Interactive Multi-Resolution 
Modeling on Arbitrary Meshes Leif Kobbelt. Swen Campagna Jens Vorsatz Hans-Peter Seidel University of 
Erlangen N¨urnberg Abstract During the last years the concept of multi-resolution modeling has gained 
special attention in many .elds of computer graphics and geometric modeling. In this paper we generalize 
powerful multi­resolution techniques to arbitrary triangle meshes without requiring subdivision connectivity. 
Our major observation is that the hierar­chy of nested spaces which is the structural core element of 
most multi-resolution algorithms can be replaced by the sequence of in­termediate meshes emerging from 
the application of incremental mesh decimation. Performing such schemes with local frame cod­ing of the 
detail coef.cients already provides effective and ef.cient algorithms to extract multi-resolution information 
from unstruc­tured meshes. In combination with discrete fairing techniques, i.e., the constrained minimization 
of discrete energy functionals, we ob­tain very fast mesh smoothing algorithms which are able to reduce 
noise from a geometrically speci.ed frequency band in a multi­resolution decomposition. Putting mesh 
hierarchies, local frame coding and multi-level smoothing together allows us to propose a .exible and 
intuitive paradigm for interactive detail-preserving mesh modi.cation. We show examples generated by 
our mesh modeling tool implementation to demonstrate its functionality. 1 Introduction Traditionally, 
geometric modeling is based on piecewise polyno­mial surface representations [8, 16]. However, while 
special poly­nomial basis functions are well suited for describing and modify­ing smooth triangular or 
quadrilateral patches, it turns out to be rather dif.cult to smoothly join several pieces of a composite 
sur­face along common (possibly trimmed) boundary curves. As .ex­ible patch layout is crucial for the 
construction of non-trivial geo­metric shapes, spline-based modeling tools do spend much effort to maintain 
the global smoothness of a surface. Subdivision schemes can be considered as an algorithmic gen­eralization 
of classical spline techniques enabling control meshes with arbitrary topology [2, 5, 6, 18, 22, 39]. 
They provide easy access to globally smooth surfaces of arbitrary shape by iteratively applying simple 
re.nement rules to the given control mesh. A se­quence of meshes generated by this process quickly converges 
to a smooth limit surface. For most practical applications, the re.ned .Computer Sciences Department 
(IMMD9), University of Erlangen-N¨urnberg, Am Weichselgarten 9, 91058 Erlangen, Germany, Leif.Kobbelt@informatik.uni-erlangen.de 
meshes are already suf.ciently close to the smooth limit after only a few re.nement steps. Within a multi-resolution 
framework, subdivision schemes pro­vide a set of basis functions fi;j =f(2i,j)which are suitable to build 
a cascade of nested spaces Vi =span([fi;j]j)[4, 33]. Since the functions fi;j are de.ned by uniform re.nement 
of a given control . mesh M0 =V0, the spaces Vi have to be isomorphic to meshes Mi with subdivision connectivity. 
While being much more .exible than classical (tensor-product) spline techniques, the multi-resolution 
representation based on the uniform re.nement of a polygonal base mesh is still rather rigid. When analyzing 
a given mesh Mk, i.e., when decomposing the mesh into disjoint frequency bands Wi =Vi+1 nVi, wehaveto 
invert the uniform re.nement operation Vi !Vi+1. Hence, the input mesh always has to be topologically 
isomorphic to an iteratively re.ned base grid. In general this requires a global remeshing/resampling 
of the input data prior to the multi-resolution analysis [7]. More­over, if we want to fuse several separately 
generated subdivision meshes (e.g. laser range scans) into one model, restrictive compat­ibility conditions 
have to be satis.ed. Hence, subdivision schemes are able to deal with arbitrary topology but not with 
arbitrary con­nectivity! The scales of subdivision based multi-resolution mesh represen­tations are de.ned 
in terms of topological distances. Since every vertex pi;j on each level of subdivision Mi represents 
the weight coef.cient of a particular basis function fi;j with .xed support, its region of in.uence is 
determined by topological neighborhood in the mesh instead of geometric proximity. Being derived from 
the regular functional setting, the re.nement rules of stationary subdi­vision schemes only depend on 
the valences of the vertices but not on the length of the adjacent edges. Hence, surface artifacts can 
occur when the given base mesh is locally strongly distorted. Assume we have a subdivision connectivity 
mesh and want to apply modi.cations on a speci.c scale Vi. The usual way to im­plement this operation 
is to run a decomposition scheme several steps until the desired resolution level is reached. On this 
level the mesh Mi is modi.ed and the reconstruction starting with Mi 0 yields the .nal result. The major 
draw-back of this procedure is the fact that coarse basis functions exist for the coarse-mesh vertices 
only and hence all low-frequency modi.cations have to be aligned to the grid imposed by the subdivision 
connectivity. Shifted low­frequency modi.cations can be faked by moving a group of vertices from a .ner 
scale simultaneously but this annihilates the mathemat­ical elegance of multi-resolution representations. 
A standard demo example for multi-resolution modeling is pulling the nose tip of a human head model. 
Depending on the chosen scale either the whole face is affected or just the nose is elongated. On uniformly 
re.ned meshes this operation only works if a coarse-scale control vertex happens to be located right 
at the nose tip. However, for an automatic remeshing algorithm it is very dif.cult, if not impossible, 
to place the coarse-scale vertices at the semantically relevant features of an object. In this paper 
we present an alternative approach to multi­resolution modeling which avoids these three major dif.culties, 
i.e. the restriction to subdivision connectivity meshes, the restriction to basis functions with .xed 
support and the alignment of potential coarser-scale modi.cations. The .rst problem is solved by using 
mesh hierarchies which emerge from the application of a mesh decimation scheme. In Sec­tion 2 we derive 
the necessary equipment to extract multi-resolution information from arbitrary meshes and geometrically 
encode detail information with respect to local frames which adapt to the local geometry of the coarser 
approximation of the object. To overcome the problems arising from the .xed support and aligned distribution 
of subdivision basis functions, we drop the structural concept of considering a surface in space to be 
a linear combination of scalar-valued basis functions. On each level of de­tail, the lower-frequency 
components of the geometric shape are simply characterized by energy minimization (fairing). In Section 
3 we overview the discrete fairing technique [19, 38] and show how a combination with the non-uniform 
mesh hierarchy leads to highly ef.cient mesh optimization algorithms. Due to the local smoothing properties 
of the fairing operators, we are able to de.ne a geomet­ric threshold for the wavelength up to which 
a low-pass .lter should remove noise. With an ef.cient hierarchical mesh smoothing scheme available, 
we propose a .exible mesh modi.cation paradigm in Section 4. The basic idea is to let the designer freely 
de.ne the region of in­.uence and the characteristics of the modi.cation which both can be adapted to 
the surface geometry instead of being determined by the connectivity. The selected region de.nes the 
frequency of the modi.cation since it provides the boundary conditions for a con­strained energy minimization. 
Nevertheless the detail information within the selected region is preserved and does change according 
to the global modi.cation. Exploiting the ef.cient schemes from Section 3 leads to interactive response 
times for moderately com­plex models. Throughout the paper, we consider a modeling scenario where atrianglemesh 
M with arbitrary connectivity is given (no from­scratch design). All modi.cations just alter the position 
of the ver­tices but not their adjacency. In particular, we do not consider ad in.nitum subdivision to 
establish in.nitesimal smoothness. The given mesh M =Mk represents per de.nition the .nest level of detail. 
 2 Multi-resolution representations Most schemes for the multi-resolution representation and modi.ca­tion 
of triangle meshes emerge from generalizing harmonic analysis techniques like the wavelet transform [1, 
23, 30, 33]. Since the fun­damentals have been derived in the scalar-valued functional setting IRd !IR, 
dif.culties emerge from the fact that manifolds in space are in general not topologically equivalent 
to simply connected re­gions in IRd . The philosophy behind multi-resolution modeling on surfaces is 
hence to mimic the algorithmic structure of the related func­tional transforms and preserve some of the 
important properties like locality, smoothness, stability or polynomial precision which have related 
meaning in both settings [9, 12, 40]. Accordingly, the nested sequence of spaces underlying the decomposition 
into dis­joint frequency bands is thought of being generated bottom-up from a coarse base mesh up to 
.ner and .ner resolutions. This implies that subdivision connectivity is mandatory on higher levels of 
de­tail. Not only the mesh has to consist of large regular regions with isolated extra-ordinary vertices 
in between. Additionally, we have to make sure that the topological distance between the singulari­ties 
is the same for every pair of neighboring singularities and this topological distance has to be a power 
of 2. Such special topological requirements prevent the schemes from being applicable to arbitrary input 
meshes. Global remeshing and resampling is necessary to obtain a proper hierarchy which gives rise to 
alias-errors and requires involved computations [7]. Luckily, the restricted topology is not necessary 
to de.ne dif­ferent levels of resolution or approximation for a triangle mesh. In the literature on mesh 
decimation we .nd many examples for hierarchies built on arbitrary meshes [11, 15, 20, 24, 27, 31, 35]. 
The key is always to build the hierarchy top-down by eliminating vertices from the current mesh (incremental 
reduction, progressive meshes). Running a mesh decimation algorithm, we can stop, e.g., every time a 
certain percentage of the vertices is removed. The in­termediate meshes can be used as a level-of-detail 
representation [15, 23]. In both cases, i.e., the bottom-up or the top-down generation of nested (vertex-) 
grids, the multi-resolution concept is rigidly at­tached to topological entities. This makes sense if 
hierarchies are merely used to reduce the complexity of the representation. In the context of multi-resolution 
modeling, however, we want the hierar­chy not necessarily to rate meshes according to their coarseness 
but rather according to their smoothness (cf. Fig 1). We will use multi-resolution hierarchies for two 
purposes. First we want to derive highly ef.cient algorithms for mesh optimiza­tion. In Section 3 we 
will see that topologically reduced meshes are the key to signi.cantly increase the performance (levels 
of coarse­ness). On the other hand, we want to avoid any restrictions that are imposed by topological 
peculiarities. In particular, when interac­tively modifying a triangle mesh, we do not want any alignment. 
The support of a modi.cation should have no in.uence on where this modi.cation can be applied (levels 
of smoothness). To describe the different set-ups for multi-resolution repre­sentation uniformly, we 
de.ne a generic decomposition scheme A =(AFjA.)T (analysis) as a general procedure that transforms a 
given mesh Mi into a coarser/smoother one Mi,1 =AFMi plus de­tail coef.cients Di,1 =A.Mi. In the standard 
wavelet setting the cardinalities satisfy #Di,1 +#Mi,1 =#Mi since decomposition is a proper basis transform. 
If a (bi-orthogonal) wavelet basis is not known, we have to store more detail information (#Di,1 +#Mi,1 
.#Mi) since the reconstruction operator A,1 might be computationally expensive or not even uniquely de.ned. 
Well-known examples for this kind of decomposition with extra detail coef.cients are the Laplacian­pyramid 
type of representation in [40] and the progressive mesh representation [15]. When AF is merely a smoothing 
operator which does not change the topological mesh structure of Mi we have A. =Id ,AF and #Di,1 =#Mi,1 
=#Mi. 2.1 Local Frames In a multi-resolution representation of a geometric object M =Mk, the detail 
coef.cients Di,1 describe the difference between two ap­proximations Mi,1 and Mi having different levels 
of detail. For parametric surfaces, the detail coef.cients, i.e., the spatial location of the vertices 
in Mi have to be encoded relative to the local ge­ometry of the coarser approximation Mi,1. This is necessary 
since modi.cations on the coarser level should have an intuitive effect on the geometric features from 
.ner scales. First proposed by [10] it has become standard to derive local coordinate frames from the 
partial derivative information of the coarse representation Mi,1. Since we do not assume the existence 
of any global structure or auxiliary information in the sequence of meshes Mi, we have to rely on intrinsic 
geometric properties of the triangles themselves. Depending on the intended application we assign the 
local frames to the triangles or the vertices of Mi,1. A detail vector is then de.ned by three coordinate 
values [u;v;n] plus an index i identifying the af.ne frame Fi =[pi;Ui;Vi;Ni]with respect to which the 
coordinates are given. 2.1.1 Vertex-based frames We can use any heuristic to estimate the normal vector 
Ni at a vertex pi in a polygonal mesh, e.g., taking the average of the adjacent tri­angle normals. The 
vector Ui =E ,(ET Ni)Ni is obtained by pro­Figure 1: The well-known Stanford-Bunny. Although the original 
mesh does not have subdivision connectivity, mesh decimation algorithms easily generate a hierarchy of 
topologically simpli.ed meshes. On the other hand, multi-resolution modeling also requires hierarchies 
of differently smooth approximations. Notice that the meshes in the lower row have identical connectivity. 
 jecting any adjacent edge E into the tangent plane and Vi :=Ni xUi. Let the vertices p3, p4,and p5 
be projected to (u3;v3), (u4;v4),and The data structure for storing the mesh Mi,1 has to make sure that 
(u5;v5)according to the frame [p0;p1;p2]. To additionally stabilize E is uniquely de.ned, e.g. as the 
.rst member in a list of neighbors. the interpolation scheme, we introduce a tension parameter t 2[0;1] 
which trades approximation error at p3, p4,and p5 for minimizing the bending energy f2 +2f2 +f2 . Using 
(1) we obtain uu uvvv  2.1.2 Face-based frames 01 11 2 u3 (u3 ,1)2 v3 (v3 ,1) It is tempting to simply 
use the local frame which is given by two u3 v3 triangle edges and their cross product. However, this 
will not lead to convincing detail reconstruction after modifying the coarser level. The reason for this 
is that the local frames would be rigidly attached to one coarse triangle. In fact, tracing the dependency 
over several levels of detail shows that the original mesh is implicitly partitioned into sub-meshes 
being assigned to the same coarse triangle T .Ap­ plying a transformation to T implies the same transformation 
for all BBBBBBB@ CCCCCCCA 11 2 u4 (u4 ,1)u4 v42 v4 (v4 ,1) ! fuu 1 2 u5 (u5 ,1)t u5 v5 0 1 2 v5 (v5 ,1) 
0 0 2t 0 0 0 t fuv = fvv 01 (p3 ,p0)+u3 (p0 ,p1)+v3 (p0 ,p2) (p4 ,p0u4 (p0 ,p1v4 (p0 ,p2) vertices being 
de.ned relative to T . This obviously leads to artifacts ))++ BBBB@ CCCCA between neighboring sub-meshes 
in the .ne mesh. A better choice is to use local low order polynomial interpolants or approximants that 
depend on more than one single triangle. Let p0, p1,and p2 be the vertices of a triangle T 2Mi,1 and 
p3, p4, (p5 ,p0)+u5 (p0 ,p1)+v5 (p0 ,p2) 0 0 0 and p5 be the opposite vertices of the triangles adjacent 
to T (cf. Fig. 2). To construct a quadratic polynomial which has to be solved in a least squares sense. 
To compute the detail coef.cients [u ;v ;h]for a point q with re­ 22 uv fvv spect to T , we start from 
the center (u;v )=( 1 13 )and simple New­ 3 ; F(u;v)=f +ufu +vfv + fuu +uvfuv + 2 2 ton iteration steps 
(u;v)+(u;v)+(4u;4v)with d =q ,F(u;v) and approximating the pi we have to de.ne a parameterization .rst. 
  FT FT Fu uu Fv FT u d 4u Note that the particular choice of this parameterization controls the = 
 FT Fv FT Fv4vFT d uv u quality of the approximant. Since we want to take the geometric constellation 
of the pi into account, we de.ne a parameterization quickly converge to the point F(u ;v )with the detail 
vector d per­by projecting the vertices into the supporting plane of T . pendicular to the surface F(u;v). 
The third coef.cient is then Exploiting the invariance of the polynomial interpolant with re­ h =sign(dT 
(Fu xFv))kdk. spect to af.ne re-parameterizations, we can require F(0;0):=p0, Although the parameter 
values (u ;v )can lie outside the unit tri­F(1;0):=p1,and F(0;1):=p2 which implies angle (which occasionally 
occurs for extremely distorted con.gu­rations) the detail coef.cient [u ;v ;h]is still well-de.ned and 
recon­  f =p0 1 struction works. Notice that the scheme might produce counter­ fu =p1 ,p0 ,2 fuu (1) 
intuitive results if the maximum dihedral angle between T and one fv =p2 ,p0 ,2 fvv: of its neighbors 
becomes larger than p 2 . In this case the parameter­ 1 ization for p3, p4,and p5 could be derived by 
rotation about T s edges instead of projection. P3 P4 Figure 2: Vertex labeling for the construction 
of a local frame. Obviously, the detail coef.cient [u ;v ;h]is not coded with respect to a local frame 
in the narrow sense. However, it has a similar se­mantics. Recovering the vertex position q0requires 
to construct the approximating polynomial F0(u;v)for the possibly modi.ed ver­tices p0 i,evaluate at 
(u ;v )and move in normal direction by h.The distance h is a measure for the size of the detail. In our 
current implementation on a SGI R10000/195 MHz work­station the analysis q ![u ;v ;h]takes about 20µS 
while the recon­struction [u ;v ;h]!q takes approximately 8µS. Since a progressive mesh representation 
introduces two triangles per vertex split, this means that for the reconstruction of a mesh with 105 
triangles, the computational overhead due to the local frame representation is less than half a second. 
  2.2 Decomposition and reconstruction To complete our equipment for the multi-resolution set-up we 
have to de.ne the decomposition and reconstruction operations which separate the high-frequency detail 
from the low-frequency shape and eventually recombine the two to recover the original mesh. We apply 
different strategies depending on whether decomposition generates a coarser approximation of the original 
geometry or a smoother approximation. In either case the decomposition operator A is applied to the orig­inal 
mesh Mi and the details Di,1 are coded in local frame coordi­nates with respect to Mi,1. Since the reconstruction 
is an extrapo­lation process, it is numerically unstable. To stabilize the operation we have to make 
the details as small as possible, i.e., when encod­ing the spatial position of a point q 2IR3 we pick 
that local frame on Mi,1 which is closest to q. Usually the computational complexity of generating the 
detail coef.cients is higher than the complexity of the evaluation during reconstruction. This is an 
important feature since for interactive modeling the (dynamic) reconstruction has to be done in real-time 
while the requirements for the (static) decomposition are not as de­manding. 2.2.1 Mesh decimation based 
decomposition When performing an incremental mesh decimation algorithm, each reduction step removes one 
vertex and retriangulates the remain­ing hole [15, 31]. We use a simpli.ed version of the algorithm described 
in [20] that controls the reduction process in order to op­timize the fairness of the coarse mesh while 
keeping the global ap­proximation error below a prescribed tolerance. The basic topological operation 
is the half edge collapse which shifts one vertex p into an adjacent vertex q and removes the two degenerate 
triangles. In [20] a fast algorithm is presented to deter­mine that triangle T in the neighborhood of 
the collapse which lies closest to the removed vertex p. The position of p is then coded with respect 
to the local frame associated with this triangle. The inverse operation of an edge collapse is the vertex 
split [15]. Since during reconstruction the vertices are inserted in the reverse order of their removal, 
it is guaranteed that, when p is inserted, the topological neighborhood looks the same as when it was 
deleted and hence the local frame to transform the detail coef.cient for p back into world coordinates 
is well-de.ned. During the iterative decimation, each intermediate mesh could be considered as one individual 
level of detail approximation. How­ever, if we want to de.ne disjoint frequency bands, it is reasonable 
to consider a whole sub-sequence of edge collapses as one atomic transformation from one level Mi to 
Mi,1. There are several criteria to determine which levels mark the boundaries between successive frequency 
bands. One possibility is to simply de.ne Mi to be the coarsest mesh that still keeps a maximum tolerance 
of less than some ei to the original data. Al­ternatively we can require the number of vertices in Mi,1 
tobe a .xed percentage of the number of vertices in Mi. This simulates the geometric decrease of complexity 
known from classical multi­resolution schemes. We can also let the human user decide when a signi.cant 
level of detail is reached by allowing her to browse through the sequence of incrementally reduced meshes. 
In order to achieve optimal performance with the multi-level smoothing algorithm described in the next 
section, we decided in our implementation to distribute the collapses evenly over the mesh: When a collapse 
p !q is performed, all vertices adjacent to q are locked for further collapsing (independent set of collapses). 
If no more collapses are possible, the current mesh de.nes the next level of detail and all vertices 
are un-locked. One pass of this reduction scheme removes about 25% of the vertices in average. 2.2.2 
Mesh smoothing based decomposition For multi-resolution modeling we have to separate high frequency features 
from the global shape in order to modify both individu­ally. Reducing the mesh complexity cannot help 
in this case since coarser meshes do no longer have enough degrees of freedom to be smooth, i.e., to 
have small angles between adjacent triangles. Hence, the decomposition operator AF becomes a mere smooth­ing 
operator that reduces the discrete bending energy in the mesh without changing the topology (cf. Section 
3). A natural way to de.ne the detail coef.cients would be to store the difference vectors between the 
original vertex position q and the shifted position q0with respect to the local frame de.ned at q0 . 
However, in view of numerical stability this choice is not op­timal. Depending on the special type of 
smoothing operator AF the vertices can move within the surface such that another vertex p02Mi,1 =AFMi 
could lie closer to q than q0(cf. Fig. 3). Figure 3: Although the bending energy minimizing smoothing 
op­erator AF is applied to a plane triangulation, the vertices are moved within the plane since linear 
operators always do the fairing with re­spect to a speci.c parameterization (cf. Section 3). To stabilize 
the reconstruction, i.e., to minimize the length of the detail vectors, we apply a simple local search 
procedure to .nd the nearest vertex p02Mi,1 to q and express the detail vector with respect to the local 
frame at p0or one of its adjacent triangles. This searching step does not noticeably increase the computation 
time (which is usually dominated by the smoothing operation AF)but leads to much shorter detail vectors 
(cf. Fig 4). Figure 4: The shortest detail vectors are obtained by representing the detail coef.cients 
with respect to the nearest local frame (left) instead of attaching the detail vectors to the topologically 
corre­sponding original vertices.  3 Discrete fairing From CAGD it is well-known that constrained energy 
minimization is a very powerful technique to generate high quality surfaces [3, 13, 25, 28, 37]. For 
ef.ciency, one usually de.nes a simple quadratic energy functional E(f )and searches among the set of 
functions satisfying prescribed interpolation constraints for that function f which minimizes E. Transferring 
the continuous concept of energy minimization to the discrete setting of triangle mesh optimization leads 
to the dis­crete fairing approach [19, 38]. Local polynomial interpolants are used to estimate derivative 
information at each vertex by divided difference operators. Hence, the differential equation characteriz­ing 
the functions with minimum energy is discretized into a linear system for the vertex positions. Since 
this system is global and sparse, we apply iterative solving algorithms like the Gauß-Seidel-scheme. 
For such algorithms one iteration step merely consists in the application of a simple local averaging 
operator. This makes discrete fairing an easy accessible technique for mesh optimization. 3.1 The umbrella-algorithm 
The most prominent energy functionals that are used in the theory and practice of surface design are 
the membrane energy Z EM(f ):=f 2 +fv 2 (2) u which prefers functions with smaller surface area and the 
thin plate energy Z ETP(f ):=f 2 +2 f 2 +f 2 (3) uu uvvv which punishes strong bending. The variational 
calculus leads to simple characterizations of the corresponding minimum energy sur­faces 4f =fuu +fvv 
=0 (4) or 42 f =fuuuu +2 fuuvv +fvvvv =0 (5) respectively. Obviously, low degree polynomials satisfy 
both differ­ential equations and hence appropriate (Dirichlet-) boundary condi­tions have to be imposed 
which make the semi-de.nite functionals EM and ETP positive-de.nite. The discrete fairing approach discretizes 
either the energy func­tionals (2 3) [19, 38] or the corresponding Euler-Lagrange equa­tions (4 5) [17, 
36] by replacing the differential operators with di­vided difference operators. To construct these operators, 
we have to choose an appropriate parameterization in the vicinity of each ver­tex. In [38] for example 
a discrete analogon to the exponential map is chosen. In [17] the umbrella-algorithm is derived by choosing 
a symmetric parameterization ii (ui;vi):=cos(2p );sin(2p );i =0;:::;n ,1 (6) nn with n being the valence 
of the center vertex p (cf. Fig 5). This pa­rameterization does not adapt to the local geometric constellation 
but it simpli.es the construction of the corresponding difference operators which are otherwise obtained 
by solving a Vandermonde system for local polynomial interpolation. With the special param­eterization 
(6) the discrete analogon of the Laplacian 4f turns out to be the umbrella-operator n,1 1 U(p)=. pi ,p 
n i=0 with pi being the direct neighbors of p (cf. Fig. 5). The umbrella­operator can be applied recursively 
leading to n,1 1 U2(p)=. U(pi),U(p) n i=0 as a discretization of 42 f . P P 2 ... n-1 Figure 5: To 
compute the discrete Laplacian, we need the 1­neighborhood of a vertex p (!umbrella-operator). The boundary 
conditions are imposed to the discrete problem by freezing certain vertices. When minimizing the discrete 
version of EM we hold a closed boundary polygon .xed and compute the membrane that is spanned in between. 
For the minimization of ETP we need two rings of boundary vertices, i.e., we keep a closed strip of triangles 
.xed. This imposes a (discrete) C1 boundary condition to the optimization problem (cf. Fig 6). All internal 
vertices can be moved freely to minimize the global energy. The properly cho­sen boundary conditions 
imply positive-de.niteness of the energy functional and guarantee the convergence of the iterative solving 
algorithm [29]. The characteristic (linear) system for the corresponding uncon­strained minimization 
problem has rows U(pi)=0or U2(pi)=0 respectively for the free vertices pi. An iterative solving scheme 
approaches the optimal solution by solving each row of the system separately and cycling through the 
list of free vertices until a stable solution is reached. In case of the membrane energy EM this leads 
to the local update rule pi +pi +U(pi) (7) and for the thin plate energy ETP , we obtain 1 pi +pi ,U2(pi) 
(8) . with the diagonal element . =1 + 1 . 1 ni j ni;j where ni and ni;j are the valences of the center 
vertex pi and its jth neighbor respectively. Figure 6: A closed polygon or a closed triangle strip provide 
C0 or C1 boundary conditions for the discrete fairing. On the left the triangle mesh minimizes EM on 
the right it minimizes ETP. Although the rule (8) can be implemented recursively, the perfor­mance is 
optimized if we use a two step process where all umbrella vectors U(pi)are computed in a .rst pass and 
U2(pi)in the sec­ond. This avoids double computation but it also forces us to use in fact a plain Jacobi-solver 
since no intermediate updates from neigh­boring vertices can be used. However the (n +2): 2 speed-up 
for a vertex with valence n amortizes the slower convergence of Jacobi compared to Gauß-Seidel. 3.2 
Connection to Taubin s signal processing ap­proach The local update operator (7) in the iterative solving 
scheme for constrained energy minimization is exactly the Laplace smoothing operator proposed by Taubin 
in [34] where he derived it (also in the context of mesh smoothing) from a .lter formalism based on gener­alized 
Fourier analysis for arbitrary polygonal meshes. In his paper, Taubin starts with a matrix version of 
the scaled update rule (7) 0 [pi]:=(I +.U)[pi] where . is a damping factor and formally rewrites it by 
using a transfer function notation f (k):=1 ,.k with respect to the eigenbasis of the (negative) Laplace 
operator. Since no proper boundary conditions are imposed, the continued .ltering by f (k)leads to severe 
shrinking and hence he proposes combined .lters f (k):=(1 ,.k)(1 ,µk) (9) where . and µ are set in order 
to minimize the shrinking. A feasible heuristic is to choose a pass-band frequency 11 kPB =+2[0:01:::0:1] 
. µ and set . and µ while observing the stability of the .lter. Obviously, the update rule for the difference 
equation U(pi)=0 which characterizes meshes with minimum membrane energy cor­responds to a special low-pass 
.lter with transfer function fU(k)= (1 ,k). For the minimization of the total curvature, characterized 
by U2(pi)=0, the iteration rule (8) can be re-written in transfer function notation as 1 11 fU2 (k)=(1 
,k2)=(1 +pk)(1 ,pk) . .. which corresponds to a combined Laplace .lter of the form (9) with pass-band 
frequency kPB =0. Although this is not optimal for reducing the shrinking effect, we observe that the 
transfer func­tion happens to have a vanishing derivative at k =0. From sig­nal processing theory it 
is known that this characterizes maximal smoothness [26], i.e., among the two step Laplace .lters, the 
U2­.lter achieves optimal smoothing properties. To stabilize the .lter we might want to introduce a damping 
factor 0 <s <1 . into the 2 update-rule s pi +pi ,U2(pi) .  3.3 Multi-level smoothing A well-known 
negative result from numerical analysis is that straight forward iterative solvers like the Gauß-Seidel 
scheme are not appropriate for large sparse problems [32]. More sophisticated solvers exploit knowledge 
about the structure of the problem. The important class of multi-grid solvers achieve linear running 
times in the number of degrees of freedom by solving the same problem on grids with different step sizes 
and combining the approximate solutions [14]. For difference (=discrete differential) equations of elliptic 
type the Gauß-Seidel iteration matrices have a special eigenstructure that causes high frequencies in 
the error to be attenuated very quickly while for lower frequencies no practically useful rate of conver­gence 
can be observed. Multi-level schemes hence solve a given problem on a very coarse scale .rst. This solution 
is used to predict initial values for a solution of the same problem on the next re.ne­ment level. If 
these predicted values have only small deviations from the true solution in low-frequency sub-spaces, 
then Gauß-Seidel performs well in reducing the high-frequency error. The alternating re.nement and smoothing 
leads to highly ef.cient varia­tional subdivision schemes [19] which generate fair high-resolution meshes 
with a rate of several thousand triangles per second (linear complexity!). As we saw in Section 2, the 
bottom-up way to build multi­resolution hierarchies is just one of two possibilities. To get rid of the 
restriction that the uniform multi-level approach to fairing cannot be applied to arbitrary meshes, we 
generate the hierarchy top-down by incremental mesh decimation. A complete V-cycle multi-grid solver 
recursively applies opera­tors Fi =.P Fi,1 R. where the .rst (right) . is a generic (pre­)smoothing operator 
 a Gauß-Seidel scheme in our case. R is a restriction operator to go one level coarser. This is where 
the mesh decimation comes in. On the coarser level, the same scheme is ap­plied recursively, Fi,1, until 
on the coarsest level the number of degrees of freedom is small enough to solve the system directly (or 
any other stopping criterion is met). On the way back-up, the pro­longation operator P inserts the previously 
removed vertices to go one level .ner again. P can be considered as a non-regular subdi­vision operator 
which has to predict the positions of the vertices in the next level s solution. The re-subdivided mesh 
is an approxima­tive solution with mostly high frequency error. (Post-)smoothing by some more iterations 
. removes the noise and yields the .nal solution. Fig 7 shows the effect of multi-level smoothing. On 
the left you see the original bunny with about 70K triangles. In the center left,  Figure 7: Four versions 
of the Stanford bunny. On the left the original data set. In the center left the same object after 200 
iterations of the non-shrinking Laplace-.lter. On the center right and far right the original data set 
after applying the multi-level umbrella .lter with three or six levels respectively. we applied the Laplace-.lter 
proposed in [34] with . =0:6307 and µ =,0:6732. The iterative application of the local smoothing op­erator 
pi +pi +[.jµ]U(pi) (10) removes the highest frequency noise after a few iterations but does not have 
enough impact to .atten out the fur even after several hun­dred iterations. On the right you see the 
meshes after applying a multi-level smoothing with the following schedule: Hierarchy lev­els are generated 
by incremental mesh decimation where each level has about 50% of the next .ner level s vertices. The 
pre-smoothing rule (8) is applied twice on every level before going downwards and .ve times after coming 
back up. On the center right model we computed a three level V-cycle and on the far right model a six 
level V-cycle. Notice that the computation time of the multi­level .lters (excluding restriction and 
prolongation) corresponds to about (2 +5)(1 +0:5 +0:52 +:::)<14 double-steps of the one­level Laplace-Filter 
(10).  3.4 Geometric .ltering The bunny example in Fig. 7 is well suited for demonstrating the effect 
of multi-level smoothing but we did not impose any bound­ary conditions and thus we applied the smoothing 
as a mere .lter and not as a solving scheme for a well-posed optimization prob­lem. This is the reason 
why we could use the number of levels to control the impact of the smoothing scheme on the .nal result. 
For constrained optimization, it does not make any sense to stop the recursion after a .xed number of 
decimation levels: we always re­duce the mesh down to a small .xed number of triangles. Properly chosen 
boundary condition will ensure the convergence to the true solution and prevent the mesh from shrinking. 
Nevertheless, we can exploit the effect observed in Fig. 7 to de­.ne more sophisticated geometric low-pass 
.lters. Since the sup­port of the Laplace-.lters is controlled by the neighborhood relation in the underlying 
mesh, the smoothing characteristics are de.ned relative to a topological wavelength . Noise which affects 
every other vertex is removed very quickly independent from the length of the edges while global distortions 
affecting a larger sub-mesh are hardly reduced. For geometric .lters however we would like to set the 
pass-band frequency in terms of Euclidian distances by postu­lating that all geometric features being 
smaller than some threshold e are considered as noise and should be removed. Such .lters can be implemented 
by using an appropriate mesh reduction scheme that tends to generate intermediate meshes with strong 
coherence in the length of the edges. In [20] we propose a mesh decimation scheme that rates the possible 
edge collapses ac­cording to some generic fairness functional. A suitable objective function for our 
application is to maximize the roundness of trian­gles, i.e., the ratio of its inner circle radius to 
its longest edge. If the mesh decimation scheme prefers those collapses that improve the global roundness, 
the resulting meshes tend to have only little variance in the lengths of the edges. For the bunny example, 
we can keep the standard deviation from the average edge length below one percent for incremental decimation 
down to about 5K triangles. By selecting the lowest level M0 down to which the V-cycle multi-level .ltering 
iterates, we set the threshold e =e(M0)for detail being removed by the multi-level smoothing scheme. 
The thresholding works very well due to the strong local and poor global convergence of the iterative 
update rule (8). Fig. 8 shows the base meshes for the multi-level smoothing during the computation of 
the two right bunnies of Fig. 7. Figure 8: Base meshes where the V-cycle recursion stopped when generating 
the right models in Fig. 7. The .nal meshes do not loose signi.cant detail (watch the silhouette). Notice 
how in the left ex­ample the fur is removed but the bunny s body preserved while in the right example 
the leg and the neck start to disappear.  4 Multi-resolution modeling on triangle meshes In this section 
we describe a .exible and intuitive multi-resolution mesh modeling metaphor which exploits the techniques 
presented in the last two sections. As we discussed in the introduction, our goal is to get rid of topological 
restrictions for the mesh but also to get rid of dif.culties emerging from the alignment of the basis 
functions in a hierarchical representation and the rigid shape of the basis function s support. From 
a designer s point of view, we have to distinguish three se­mantic levels of detail. These levels are 
de.ned relative to a speci.c modeling operation since, of course, in a multi-resolution environ­ment 
the features that are detail in a (global) modi.cation become the global shape for a minute adjustment. 
.The global shape is that part of the geometry that is the subject of the current modi.cation. Intuitively, 
the designer selects a piece of the global shape and applies a transformation to it. .The structural 
detail are features that are too small to be mod­i.ed by hand but still represent actual geometry. This 
detail should follow the modi.cation applied to the global shape in a   Figure 9: The wooden cat model 
Mk (178K triangles, left) is in progressive mesh representation. The high resolution is necessary to 
avoid alias errors in the displacement texture. The center left model Mi (23K triangles) is extracted 
by stopping the mesh reduction when a prescribed complexity is reached. On this level interactive mesh 
modi.cation is done which yields the model Mi 0(center right). The .nal result Mk 0(right) is obtained 
by running the reconstruction on the modi.ed mesh. geometrically intuitive manner. The preservation of 
structural detail during interactive modeling is crucial for plausible vi­sual feed-back (cf. the eyes 
and ears of the wooden cat model in Fig. 9). .The textural detail does not really describe geometric 
shape. It is necessary to let the surface appear more realistic and is often represented by displacement 
maps [21]. In high qual­ity mesh models it is the source for the explosive increase in complexity (cf. 
the wood texture in Fig. 9). Having identi.ed these three semantic levels of detail, we suggest a mesh 
modeling environment which provides .exible mesh modi.­cation functionality and allows the user to adapt 
the mesh complex­ity to the available hardware resources. In an off-line preprocessing step, an incremental 
mesh decima­tion algorithm is applied and the detail coef.cients are stored with respect to local frames 
as explained in Section 2.2.1. This trans­forms the highly complex input model into a progressive-mesh 
type multi-resolution representation without any remeshing or resam­pling. The representation allows 
the user to choose an appropriate number of triangles for generating a mesh model that is .ne enough 
to contain at least all the structural detail but which is also coarse enough to be modi.ed in realtime. 
This pre-process removes the textural detail prior to the actual interactive mesh modi.cation. Suppose 
the original mesh model Mk is transformed into the pro­gressive mesh sequence [Mk;:::;M0]with M0 being 
the coarsest base mesh. If the user picks the mesh Mi and applies modi.cations then this invalidates 
the subsequence [Mi,1;:::;M0]. If the work­ing resolution is to be reduced afterwards to M j ( j <i) 
then the in­termediate meshes have to be recomputed by online mesh decima­tion. The textural detail encoded 
in the subsequence [Mk;:::;Mi+1] however remains unchanged since it is stored with respect to local frames 
such that reconstruction starting from a modi.ed mesh Mi 0 leads to the intended result Mk 0 . Fig. 9 
shows an example of this procedure. 4.1 Interactive mesh modeling by discrete fairing The most important 
feature in the proposed multi-resolution mesh modeling environment is the modi.cation functionality itself 
(mod­eling metaphor) which hides the mesh connectivity to the designer. The designer starts by marking 
an arbitrary region on the mesh Mi. In fact, she picks a sequence of surface points (not necessarily 
vertices) on the triangle mesh and these points are connected either by geodesics or by projected lines. 
The strip of triangles S which are intersected by the geodesic (projected) boundary polygon sep­arates 
an interior region M.andanexterior region Mi n(M.uS). The interior region M.is to be affected by the 
following modi.ca­tion. A second polygon (not necessarily closed) is marked within the .rst one to de.ne 
the handle. The semantics of this arbitrarily shaped handle is quite similar to the handle metaphor in 
[37]: when the designer moves or scales the virtual tool, the same geometric transformation is applied 
to the rigid handle and the surrounding mesh M.follows according to a constrained energy minimization 
principle. The freedom to de.ne the boundary strip S and the handle geom­etry allows the designer to 
build custom tailored basis functions for the intended modi.cation. Particularly interesting is the de.ni­tion 
of a closed handle polygon which allows to control the char­acteristics of a bell-shaped dent: For the 
same region M., a tiny ring-shaped handle in the middle causes a rather sharp peak while a bigger ring 
causes a wider bubble (cf. Fig 10). Notice that the mesh vertices in the interior of the handle polygon 
move according to the energy minimization. Since we are working on triangle meshes, the energy minimiza­tion 
on M.is done by discrete fairing techniques (cf. Section 3). The boundary triangles S provide the correct 
C1 boundary condi­tions for minimizing the thin plate energy functional (3). The handle imposes additional 
interpolatory constraints on the location only derivatives should not be affect by the handle. Hence, 
we cannot have triangles being part of the handle geome­try. We implemented the handle constraint in 
the following way: like the boundary polygon, the handle polygon de.nes a strip of triangles being intersected 
by it. Whether the handle polygon is open or closed, we .nd two polygons of mesh edges on either side 
of that strip. We take any one of the two polygons and collect ev­ery other mesh vertex in a set of handle 
vertices. Keeping these handle vertices .xed during the mesh optimization is the additional interpolatory 
constraint. The reason for freezing only every other handle vertex is that three .xed vertices directly 
connected by two edges build a rigid constellation leaving no freedom to adjust the angle between them. 
During discrete optimization this would be the source of undesired artifacts in the smooth mesh. With 
the boundary conditions properly set we perform the thin plate energy minimization by using the umbrella 
algorithm de­scribed in Section 3.1. To obtain interactive response times, we exploit the multi-level 
technique: a mesh decimation algorithm is appliedtothe mesh M.uS to build up a hierarchy. Then starting 
from the coarsest level, we apply the U2 smoothing .lter alternat­ing with mesh re.nement. This process 
is fast enough to obtain sev­eral frames per second when modeling with meshes of #M..5K triangles (SGI 
R10000/195MHz). Typically, we set the ratio of the complexities between successive meshes in the hierarchy 
to 1 : 2 or 1 : 4 and iterate the smoothing .lter 3 to 5 times on each level. During the interactive 
mesh smoothing we do not compute the full V-cycle algorithm of Sect. 3.3. In fact, we omit the pre­smoothing 
and always start from the coarsest level. When a ver­tex is inserted during a mesh re.nement step we 
place it initially at its neighbor s center of gravity unless the vertex is a handle ver­tex. Handle 
vertices are placed at the location prescribed by the designer s interaction (handle interpolation constraint). 
Hence the mesh is computed from scratch in every iteration instead of just up­dating the last position. 
This is very important for the modeling dialog since only the current position, orientation and scale 
of the handle determines the smoothed mesh and not the whole history of movements. For the fast convergence 
of the optimization procedure it turns out to be important that the interpolation constraints imposed 
by the handle vertices show up already on rather coarse levels in the mesh hierarchy. Otherwise their 
impact cannot propagate far enough through the mesh such that cusps remain in the smoothed mesh which 
can only be removed by an excessive number of smoothing iterations. This additional requirement can easily 
be included into the mesh reduction scheme by lowering the priority ranking of col­lapses involving handle 
vertices. 4.2 Detail preservation If the modi.ed mesh M.0is merely de.ned by constrained energy minimization, 
we obviously loose all the detail of the originally selected submesh M.. Since only the boundary and 
the handle ver­tices put constraints on the mesh, all other geometric features are smoothed out. To preserve 
the detail, we use the multi-resolution representa­tion explained in Section 2.2.2. After the boundary 
S and the han­dle polygon are de.ned but before the handle is moved by the de­signer, we apply the multi-level 
smoothing scheme once. Although  5 Conclusions and future work We presented a new approach to multi-resolution 
mesh represen­tation and modeling which does not require the underlying trian­gle mesh to have subdivision 
connectivity. By adapting multi-level techniques known from numerical analysis to the non-regular set­ting 
of arbitrary mesh hierarchies, we are able to approximately solve constrained mesh optimization in realtime. 
Combining the two results allows us to present a .exible metaphor for interactive mesh modeling where 
the shape of the modi.cation is controlled by energy minimization while the geometric detail is preserved 
and updated according to the change of the global shape. Our current implementation of an experimental 
mesh model­ing tool already provides suf.cient functionality to apply sophis­ticated realtime modi.cations 
to arbitrary input meshes with up to 100K triangles. However, all changes do affect the geometry of the 
meshes only. So far we did not consider topological modi.cations of triangle meshes. In the future, when 
modifying a given mesh, we would like new vertices to be inserted where the mesh is locally stretched 
too much and, on the other hand, we would like vertices to be removed when strong global modi.cation 
causes local self­intersection of the reconstructed detail.  References [1] BONNEAU,G., HAHMANN,S., 
AND NIELSON, G. Blac-wavelets: a multires­olution analysis with non-nested spaces. In Visualization Proceedings 
(1996), pp. 43 48. [2] CATMULL,E., AND CLARK, J. Rcursively Generated B-Spline Surfaces on Arbitrary 
Topological Meshes. Computer Aided Design 10, 6 (Nov. 1978), 239 248. [3] CELNIKER,G., AND GOSSARD, D. 
Deformable curve and surface .nite ele­ments for free-form shape design. In Computer Graphics (SIGGRAPH 
91 Pro­ceedings) (July 1991), pp. 257 265. [4] DAUBECHIES,I. Ten Lectures on Wavelets. CBMS-NSF Regional 
Conf. Series in Appl. Math., Vol. 61. SIAM, Philadelphia, PA, 1992. [5] DOO,D., AND SABIN, M. Behaviour 
of recursive division surfaces near ex­traordinary points. CAD (1978). [6] DYN,N., LEVIN,D., AND GREGORY, 
J. A. A butter.y subdivision scheme for surface interpolation with tension control. ACM Transactions 
on Graphics 9,2 (April 1990), 160 169. [7] ECK,M., DEROSE,T., DUCHAMP,T., HOPPE,H., LOUNSBERY,M., AND 
STUETZLE, W. Multiresolution Analysis of Arbitrary Meshes. In Computer Graphics (SIGGRAPH 95 Proceedings) 
(1995), pp. 173 182. [8] FARIN,G. Curves and Surfaces for CAGD, 3rd ed. Academic Press, 1993. [9] FINKELSTEIN,A., 
AND SALESIN, D. H. Multiresolution Curves. In Computer Graphics (SIGGRAPH 94 Proceedings) (July 1994), 
pp. 261 268. [10] FORSEY,D. R., AND BARTELS, R. H. Hierarchical B-spline re.nement. In Computer Graphics 
(SIGGRAPH 88 Proceedings) (1988), pp. 205 212. f M. equivalent, they do have different levels of (geometric) 
resolution [11] GARLAND,M., AND HECKBERT, P. S. Surface Simpli.cation Using Quadric Error Metrics. In 
Computer Graphics (SIGGRAPH 97 Proceedings) (1997), the original mesh M.and the smoothed meshare topologically 
pp. 209 218. and hence constitute a two-scale decomposition based on varying levels of smoothness. We 
encode the difference D.between the [12] GORTLER,S. J., AND COHEN, M. F. Hierarchical and Variational 
Geometric Modeling with Wavelets. In Proceedings Symposium on Interactive 3D Graphics two meshes, i.e., 
the detail coef.cients for the vertices pi 2M.by (May 1995). storing the displacement vectors with respect 
to the local frame as­ f M.. When the designer moves the handle, the bottom-up mesh [14] smoothing 
is performed to re-adjust the mesh to the new interpo­ [13] GREINER, G. Variational design and fairing 
of spline surfaces. Computer sociated with the nearest triangle in Graphics Forum 13 (1994), 143 154. 
HACKBUSCH,W. Multi-Grid Methods and Applications. Springer Verlag, Berlin, 1985. lation conditions. On 
the resulting smooth mesh f. M 0 , the detail D. is added and the .nal mesh M.0is rendered on the screen. 
Due to [15] HOPPE, H. Progressive Meshes. In Computer Graphics (SIGGRAPH 96 Pro­ ceedings) (1996), pp. 
99 108. the geometric coding of the detail information, this leads to intuitive changes in the surface 
shape (cf. Figs. 11, 12). The frequency of [16] HOSCHEK,J., AND LASSER,D. Fundamentals of Computer Aided 
Geometric Design. AK Peters, 1993. the modi.cation is determined by the size of the area, i.e., by the 
boundary conditions and the fact that the supporting mesh is opti-[17] KOBBELT,L. Iterative Erzeugung 
glatter Interpolanten. Shaker Verlag, ISBN mal with respect to the thin-plate functional. 3-8265-0540-9, 
1995.   [18] KOBBELT, L. Interpolatory Subdivision on Open Quadrilateral Nets with Arbi­trary Topology. 
In Computer Graphics Forum, Proceedings of Eurographics 96 (1996), pp. C407 C420. [19] KOBBELT, L. Discrete 
fairing. In Proceedings of the Seventh IMA Conference on the Mathematics of Surfaces (1997), pp. 101 
131. [20] KOBBELT,L., CAMPAGNA,S., AND SEIDEL, H.-P. A general framework for mesh decimation. In Proceedings 
of the Graphics Interface conference 98 (1998). [21] KRISHNAMURTHY,V., AND LEVOY, M. Fitting smooth surfaces 
to dense polygon meshes. In Computer Graphics (SIGGRAPH 96 Proceedings) (1996), pp. 313 324. [22] LOOP, 
C. Smooth spline surfaces over irregular meshes. In Computer Graphics Proceedings (1994), Annual Conference 
Series, ACM Siggraph, pp. 303 310. [23] LOUNSBERY,M., DEROSE,T., AND WARREN, J. Multiresolution Analysis 
for Surfaces of Arbitrary Topological Type. ACM Transactions on Graphics 16,1 (January 1997), 34 73. 
[24] LUEBKE,D., AND ERIKSON, C. View-Dependent Simpli.cation of Arbitrary Polygonal Environments. In 
Computer Graphics (SIGGRAPH 97 Proceedings) (1997), pp. 199 208. ´ sign. In Computer Graphics (SIGGRAPH 
92 Proceedings) (1992), pp. 167 176. [25] MORETON,H., AND SEQUIN, C. Functional optimization for fair 
surface de­ [26] OPPENHEIM,A., AND SCHAFER,R. Discrete-Time Signal Processing. Prentice Hall, 1989. [27] 
ROSSIGNAC, J. Simpli.cation and Compression of 3D Scenes, 1997. Tutorial Eurographics 97. [28] SAPIDIS,N. 
E. Designing Fair Curves and Surfaces. SIAM, 1994. [29] SCHABAK,R., AND WERNER,H. Numerische Mathematik. 
Springer Verlag, 1993. [30] SCHR ¨ ODER,P., AND SWELDENS, W. Spherical wavelets: Ef.ciently represent­ing 
functions on the sphere. In Computer Graphics (SIGGRAPH 95 Proceedings) (1995), pp. 161 172. [31] SCHROEDER,W. 
J., ZARGE,J. A., AND LORENSEN, W. E. Decimation of Triangle Meshes. In Computer Graphics (SIGGRAPH 92 
Proceedings) (1992), pp. 65 70. [32] STOER,J. Einf¨uhrung in die Numerische Mathematik I. Springer Verlag, 
1983. [33] STOLLNITZ,E., DEROSE,T., AND SALESIN,D. Wavelets for Computer Graphics. Morgan Kaufmann Publishers, 
1996. [34] TAUBIN, G. A signal processing approach to fair surface design. In Computer Graphics (SIGGRAPH 
95 Proceedings) (1995), pp. 351 358. [35] TURK, G. Re-Tiling Polygonal Surfaces. In Computer Graphics 
(SIGGRAPH 92 Proceedings) (1992), pp. 55 64. [36] WARREN, J. Subdivision schemes for variational splines, 
1997. Preprint. [37] WELCH,W., AND WITKIN, A. Variational surface modeling. In Computer Graphics (SIGGRAPH 
92 Proceedings) (1992), pp. 157 166. [38] WELCH,W., AND WITKIN, A. Free Form shape design using triangulated 
sur­faces. In Computer Graphics (SIGGRAPH 94 Proceedings) (1994), A. Glassner, Ed., pp. 247 256. [39] 
ZORIN,D., SCHR ¨ ODER,P., AND SWELDENS, W. Interpolating subdivision for meshes with arbitrary topology. 
In Computer Graphics (SIGGRAPH 96 Pro­ceedings) (1996), pp. 189 192. [40] ZORIN,D., SCHR ¨ W. ODER,P., 
AND SWELDENS, Interactive multiresolu­tion mesh editing. In Computer Graphics (SIGGRAPH 97 Proceedings) 
(1997), pp. 259 268.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280832</article_id>
		<sort_key>115</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Appearance-preserving simplification]]></title>
		<page_from>115</page_from>
		<page_to>122</page_to>
		<doi_number>10.1145/280814.280832</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280832</url>
		<keywords>
			<kw><![CDATA[attributes]]></kw>
			<kw><![CDATA[color]]></kw>
			<kw><![CDATA[maps]]></kw>
			<kw><![CDATA[normal]]></kw>
			<kw><![CDATA[parameterization]]></kw>
			<kw><![CDATA[simplification]]></kw>
			<kw><![CDATA[texture]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Object hierarchies</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010244</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Hierarchical representations</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011066.10011067</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Development frameworks and environments->Object oriented frameworks</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Measurement</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP43124373</person_id>
				<author_profile_id><![CDATA[81452612087]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jonathan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cohen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of North Carolina at Chapel Hill, Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39081311</person_id>
				<author_profile_id><![CDATA[81334487411]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Olano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of North Carolina at Chapel Hill, Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40029106</person_id>
				<author_profile_id><![CDATA[81100618474]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Dinesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Manocha]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of North Carolina at Chapel Hill, Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>801126</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[L. Williams, "Pyramidal Parametrics," SIGGRAPH 83 Conference Proceedings, pp. 1--11, 1983.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[J. Rossignac and P. Borrel, "Multi-Resolution 3D Approximations for Rendering," in Modeling in Computer Graphics: Springer-Verlag, 1993, pp. 455--465.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237133</ref_obj_id>
				<ref_obj_pid>237121</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[G. Schaufler and W. Sturzlinger, "Generating Multiple Levels of Detail from Polygonal Geometry Models," Virtual Environments'95 (Eurographics Workshop), pp. 33-41, 1995.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134010</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[W.J. Schroeder, J. A. Zarge, and W. E. Lorensen, "Decimation of Triangle Meshes," in Proc. ofACMSiggraph, 1992, pp. 65--70.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134008</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[G. Turk, "Re-tiling polygonal surfaces," Comput. Graphics, vol. 26, pp. 55--64, 1992.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258849</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[M. Garland and P. Heckbert, "Surface Simplification using Quadric Error Bounds," SIGGRAPH'97 Conference Proceedings, pp. 209-216, 1997.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[A. Gueziec, "Surface Simplification with Variable Tolerance," in Second Annual IntL Symp. on Medical Robotics and Computer Assisted Surgery (MRCAS '95), November 1995, pp. 132--139.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>267108</ref_obj_id>
				<ref_obj_pid>266989</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[J. Cohen, D. Manocha, and M. Olano, "Simplifying Polygonal Models Using Successive Mappings," Proc. oflEEE Visualization'97, pp. 395-402, 1997.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[R. Ronfard and J. Rossignac, "Full-range approximation of triangulated polyhedra," Computer Graphics Forum, vol. 15, pp. 67--76 and 462, August 1996.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>245624</ref_obj_id>
				<ref_obj_pid>244979</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[R. Klein, G. Liebich, and W. Strager, "Mesh Reduction with Error Control," in IEEE Visualization '96: IEEE, October 1996.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237220</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[J. Cohen, A. Varshney, D. Manocha, G. Turk, H. Weber, P. Agarwal, F. Brooks, and W. Wright, "Simplification Envelopes," in SIGGRAPH'96 Conference Proceedings, 1996, pp. 119--128.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218440</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[M. Eck, T. DeRose, T. Duchamp, H. Hoppe, M. Lounsbery, and W. Stuetzle, "Multiresolution Analysis of Arbitrary Meshes," in SIGGRAPH'95 Conference Proceedings, 1995, pp. 173--182.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>267059</ref_obj_id>
				<ref_obj_pid>266989</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[W. Schroeder, "A Topology Modifying Progressive Decimation Algorithm," Proc. of lEEE Visualization'97, pp. 205-212, 1997.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>267109</ref_obj_id>
				<ref_obj_pid>266989</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[J. E1-Sana and A. Varshney, "Controlled Simplification of Genus for Polygonal Models," Proc. oflEEE Visualization'97, pp. 403-410, 1997.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237216</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[H. Hoppe, "Progressive Meshes," in SIGGRAPH 96 Conference Proceedings: ACM SIGGRAPH, 1996, pp. 99--108.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258843</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[H. Hoppe, "View-Dependent Refinement of Progressive Meshes," SIG- GRAPH'97 Conference Proceedings, pp. 189-198, 1997.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>614372</ref_obj_id>
				<ref_obj_pid>614266</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[J. Xia, J. E1-Sana, and A. Varshney, "Adaptive Real-Time Level-of-detailbased Rendering for Polygonal Models," IEEE Transactions on Visualization and Computer Graphics, vol. 3, pp. 171--183, 1997.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[C. Bajaj and D. Schikore, "Error-bounded reduction of triangle meshes with multivariate data," SPIE, vol. 2656, pp. 34--45, 1996.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[M. Hughes, A. Lastra, and E. Saxe, "Simplification of Global-Illumination Meshes," Proceedings of Eurographics '96, Computer Graphics Forum, vol. 15, pp. 339-345, 1996.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>897920</ref_obj_id>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[C. Erikson and D. Manocha, "Simplification Culling of Static and Dynamic Scene Graphs," UNC-Chapel Hill Computer Science TR98-009, 1998.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218439</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[P. Schroder and W. Sweldens, "Spherical Wavelets: Efficiently Representing Functions on the Sphere," SIGGRAPH 95 Conference Proceedings, pp. 161-- 172, August 1995.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237213</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[A. Certain, J. Popovic, T. Derose, T. Duchamp, D. Salesin, and W. Stuetzle, "Interactive Multiresolution Surface Viewing," in Proc. ofACMSiggraph, 1996, pp. 91--98.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808602</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[R.L. Cook, "Shade trees," in Computer Graphics (SIGGRAPH '84 Proceedings), vol. 18, H. Christiansen, Ed., July 1984, pp. 223--231.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>507101</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[J. Blinn, "Simulation of Wrinkled Surfaces," SIGGRAPH '78 Conference Proceedings, vol. 12, pp. 286--292, 1978.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[A. Fournier, "Normal distribution functions and multiple surfaces," Graphics Interface '92 Workshop on Local Illumination, pp. 45--52, 1992.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258873</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[M. Peercy, J. Airey, and B. Cabral, "Efficient Bump Mapping Hardware," SIGGRAPH'97 Conference Proceedings, pp. 303-306, 1997.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280857</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[M. Olano and A. Lastra, "A Shading Language on Graphics Hardware: The PixelFlow Shading System," SIGGRAPH 98 Conference Proceedings, 1998.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325167</ref_obj_id>
				<ref_obj_pid>325334</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[J. Kajiya, "Anisotropic Reflection Models," SIGGRAPH '85 Conference Proceedings, pp. 15--21, 1985.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37434</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[B. Cabral, N. Max, and R. Springmeyer, "Bidirectional Reflection Functions From Surface Bump Maps," SIGGRAPH '87 Conference Proceedings, pp. 273- -281, 1987.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134075</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[S. Westin, J. Arvo, and K. Torrance, "Predicting Reflectance Functions From Complex Surfaces," SIGGRAPH '92 Conference Proceedings, pp. 255--264, 1992.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166141</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[B.G. Becker and N. L. Max, "Smooth Transitions between Bump Rendering Algorithms," in Computer Graphics (SIGGRAPH '93 Proceedings), vol. 27, J. T. Kajiya, Ed., August 1993, pp. 183--190.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237270</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[V. Krishnamurthy and M. Levoy, "Fitting Smooth Surfaces to Dense Polygon Meshes," SIGGRAPH 96 Conference Proceedings, pp. 313--324, 1996.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_obj_id>245020</ref_obj_id>
				<ref_obj_pid>244979</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[D.G. Aliaga, "Visualization of Complex Models using Dynamic Texture-based Simplification," Proc. of IEEE Visualization'96, pp. 101--106, 1996.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_obj_id>253298</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[L. Darsa, B. Costa, and A. Varshney, "Navigating Static Environments using Image-space simplification and morphing," Proc. of 1997 Symposium on Interactive 3D Graphics, pp. 25-34, 1997.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_obj_id>199420</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[P.W.C. Maciel and P. Shirley, "Visual Navigation of Large Environments Using Textured Clusters," Proc. of 1995 Symposium on Interactive 3D Graphics, pp. 95--102, 1995.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237209</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[J. Shade, D. Lischinski, D. Salesin, T. DeRose, and J. Snyder, "Hierarchical Image Caching for Accelerated Walkthroughs of Complex Environments," SIGGRAPH 96 Conference Proceedings, pp. 75--82, August 1996.]]></ref_text>
				<ref_id>36</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134007</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[J. Kent, W. Carlson, and R. Parent, "Shape transformation for polyhedral objects," SIGGRAPH '92 Conference Proceedings, pp. 47--54, 1992.]]></ref_text>
				<ref_id>37</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166120</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[J. Maillot, H. Yahia, and A. Veroust, "Interactive Texture Mapping," SIGGRAPH'93 Conference Proceedings, pp. 27--34, 1993.]]></ref_text>
				<ref_id>38</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237268</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[H. Pedersen, "A Framework for Interactive Texturing Operations on Curved Surfaces," in SIGGRAPH 96 Conference Proceedings, Annual Conference Series, H. Rushmeier, Ed., August 1996, pp. 295--302.]]></ref_text>
				<ref_id>39</ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[H.d. Fraysseix, J. Pach, and R. Pollack, "How to Draw a Planar Graph on a Grid," Combinatorica, vol. 10, pp. 41--51, 1990.]]></ref_text>
				<ref_id>40</ref_id>
			</ref>
			<ref>
				<ref_obj_id>3491</ref_obj_id>
				<ref_obj_pid>3487</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[N. Chiba, T. Nishizeki, S. Abe, and T. Ozawa, "A Linear Algorithm for Embedding Planar Graphs Using PQ-Trees," J. Comput. Syst. Sci., vol. 30, pp. 54--76, 1985.]]></ref_text>
				<ref_id>41</ref_id>
			</ref>
			<ref>
				<ref_obj_id>195598</ref_obj_id>
				<ref_obj_pid>195595</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[G.D. Battista, P. Eades, R. Tamassia, and I. G. Tollis, "Algorithms for drawing graphs: an annotated bibliography," Comput. Geom. Theory Appl., vol. 4, pp. 235--282, 1994.]]></ref_text>
				<ref_id>42</ref_id>
			</ref>
			<ref>
				<ref_obj_id>261226</ref_obj_id>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[M.d. Berg, M. v. Kreveld, M. Overmars, and O. Schwarzkopf, Computational Geometry: Algorithms and Applications: Springer-Verlag, 1997.]]></ref_text>
				<ref_id>43</ref_id>
			</ref>
			<ref>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[R.F. Lyon, "Phong Shading Reformulation for Hardware Renderer Simplification," Apple Computer #43, 1993.]]></ref_text>
				<ref_id>44</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Appearance-Preserving Simplification Jonathan Cohen Marc Olano Dinesh Manocha University of North Carolina 
at Chapel Hill Copyright &#38;#169;1998 by the Association for Computing Machinery, Inc. Permission 
to make digital or hard copies of part or all of this work for personal or classroom use is granted without 
fee provided that copies are not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. Copyrights for components of this work owned 
by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, 
to post on servers or to redistribute to lists, requires specific permission and/or a fee. Abstract We 
present a new algorithm for appearance-preserving simplifi­cation. Not only does it generate a low-polygon-count 
approxi­mation of a model, but it also preserves the appearance. This is accomplished for a particular 
display resolution in the sense that we properly sample the surface position, curvature, and color attributes 
of the input surface. We convert the input surface to a representation that decouples the sampling of 
these three attrib­utes, storing the colors and normals in texture and normal maps, respectively. Our 
simplification algorithm employs a new texture deviation metric, which guarantees that these maps shift 
by no more than a user-specified number of pixels on the screen. The simplification process filters the 
surface position, while the run­time system filters the colors and normals on a per-pixel basis. We have 
applied our simplification technique to several large models achieving significant amounts of simplification 
with little or no loss in rendering quality. CR Categories: I.3.5: Object hierarchies, I.3.7: Color, 
shad­ing, shadowing, and texture Additional Keywords: simplification, attributes, parameteri­zation, 
color, normal, texture, maps 1 INTRODUCTION Simplification of polygonal surfaces has been an active 
area of research in computer graphics. The main goal of simplification is to generate a low-polygon-count 
approximation that maintains the high fidelity of the original model. This involves preserving the model 
s main features and overall appearance. Typically, there are three appearance attributes that contribute 
to the overall appear­ance of a polygonal surface: 1. Surface position, represented by the coordinates 
of the polygon vertices. 2. Surface curvature, represented by a field of normal vectors across the polygons. 
 3. Surface color, also represented as a field across the polygons.  The number of samples necessary 
to represent a surface accurately depends on the nature of the model and its area in screen pixels (which 
is related to its distance from the viewpoint). For a simplification algorithm to preserve the appearance 
of the input surface, it must guarantee adequate sampling of these three attributes. If it does, we say 
that it has preserved the appearance with respect to the display resolution. e-mail: {cohenj,dm}@cs.unc.edu, 
olano@engr.sgi.com WWW: http://www.cs.unc.edu/~geom/APS The majority of work in the field of simplification 
has focused on surface approximation algorithms. These algorithms bound the error in surface position 
only. Such bounds can be used to guarantee a maximum deviation of the object s silhouette in units of 
pixels on the screen. While this guarantees that the object will cover the correct pixels on the screen, 
it says nothing about the final colors of these pixels. Of the few simplification algorithms that deal 
with the remain­ing two attributes, most provide some threshold on a maximum or average deviation of 
these attribute values across the model. While such measures do guarantee adequate sampling of all three 
attributes, they do not generally allow increased simplification as the object becomes smaller on the 
screen. These threshold metrics do not incorporate information about the object s distance from the viewpoint 
or its area on the screen. As a result of these metrics and of the way we typically represent these appearance 
attributes, simplification algorithms have been quite restricted in their ability to simplify a surface 
while preserving its appearance. 1.1 Main Contribution We present a new algorithm for appearance-preserving 
simplifi­cation. We convert our input surface to a decoupled representa­tion. Surface position is represented 
in the typical way, by a set of triangles with 3D coordinates stored at the vertices. Surface colors 
and normals are stored in texture and normal maps, respectively. These colors and normals are mapped 
to the surface with the aid of a surface parameterization, represented as 2D texture coordi­nates at 
the triangle vertices. The surface position is filtered using a standard surface ap­proximation algorithm 
that makes local, complexity-reducing simplification operations (e.g. edge collapse, vertex removal, 
etc.). The color and normal attributes are filtered by the run-time system at the pixel level, using 
standard mip-mapping techniques [1]. Because the colors and normals are now decoupled from the surface 
position, we employ a new texture deviation metric, which effectively bounds the deviation of a mapped 
attribute value s position from its correct position on the original surface. We thus guarantee that 
each attribute is appropriately sampled and mapped to screen-space. The deviation metric necessarily 
constrains the simplification algorithm somewhat, but it is much less restrictive than retaining sufficient 
tessellation to accurately represent colors and normals in a standard, per-vertex representation. The 
preser­vation of colors using texture maps is possible on all current graphics systems that supports 
real-time texture maps. The preservation of normals using normal maps is possible on proto­type machines 
today, and there are indications that hardware support for real-time normal maps will become more widespread 
in the next several years. One of the nice properties of this approach is that the user­specified error 
tolerance, e, is both simple and intuitive; it is a screen-space deviation in pixel units. A particular 
point on the surface, with some color and some normal, may appear to shift by at most e pixels on the 
screen. We have applied our algorithm to several large models. Figure 1 clearly shows the improved quality 
of our appearance­preserving simplification technique over a standard surface approximation algorithm 
with per-vertex normals. By merely controlling the switching distances properly, we can discretely switch 
between a few statically-generated levels of detail (sampled from a progressive mesh representation) 
with no perceptible artifacts. Overall, we are able to achieve a significant speedup in rendering large 
models with little or no loss in rendering quality. 1.2 Paper Organization In Section 2, we review the 
related work from several areas. Section 3 presents an overview of our appearance-preserving simplification 
algorithm. Sections 4 through 6 describe the components of this algorithm, followed by a discussion of 
our particular implementation and results in Section 7. Finally, we mention our ongoing work and conclude 
in Section 8.  2 RELATED WORK Research areas related to this paper include geometric levels-of­detail, 
preservation of appearance attributes, and map-based representations. We now briefly survey these. 2.1 
Geometric Levels-Of-Detail Given a polygonal model, a number of algorithms have been proposed for generating 
levels-of-detail. These methods differ according to the local or global error metrics used for simplifica­tion 
and the underlying data structures or representations. Some approaches based on vertex clustering [2, 
3] are applicable to all polygonal models and do not preserve the topology of the original models. Other 
algorithms assume that the input model is a valid mesh. Algorithms based on vertex removal [4, 5] and 
local error metrics have been proposed by [6-10]. Cohen et al. [11] and Eck et al. [12] have presented 
algorithms that preserve topology and use a global error bound. Our appearance-preserving simplifica­tion 
algorithm can be combined with many of these. Other simplification algorithms include decimation techniques 
based on vertex removal [4], topology modification [13], and controlled simplification of genus [14]. 
All of these algorithms compute static levels-of-detail. Hoppe [15] has introduced an incremental representation, 
called the progressive mesh, and based on that representation view-dependent algorithms have been proposed 
by [16, 17]. These algorithms use different view­dependent criteria like local illumination, screen-space 
surface approximation error, and silhouette edges to adaptively refine the meshes. Our appearance preserving 
simplification algorithm generates a progressive mesh, which can be used by these view­dependent algorithms. 
 2.2 Preserving Appearance Attributes Bajaj and Schikore [18] have presented an algorithm to simplify 
meshes with associated scalar fields to within a given tolerance. Hughes et al. [19] have presented an 
algorithm to simplify radiositized meshes. Erikson and Manocha[20] grow error volumes for appearance 
attributes as well as geometry. Many algorithms based on multi-resolution analysis have been proposed 
as well. Schroeder and Sweldens [21] have presented algorithms for simplifying functions defined over 
a sphere. Eck et al. [12] apply multi-resolution analysis to simplify arbitrary meshes, and Certain et 
al. [22] extend this to colored meshes by separately analyzing surface geometry and color. They make 
use of texture mapping hardware to render the color at full resolution. It may be possible to extend 
this approach to handle other functions on the mesh. However, algorithms based on vertex removal and 
edge collapses [11, 15] have been able to obtain more drastic simplifi­cation (in terms of reducing the 
polygon count) and produce better looking simplifications [15]. Hoppe [15] has used an optimization framework 
to preserve discrete and scalar surface appearance attributes. Currently, this algorithm measures a maximum 
or average deviation of the scalar attributes across the model. Our approach can be incorporated into 
this comprehensive optimization framework to preserve the appearance of colors and normals, while allowing 
continued simplification as an object's screen size is reduced. 2.3 Map-based Representations Texture 
mapping is a common technique for defining color on a surface. It is just one instance of mapping, a 
general technique for defining attributes on a surface. Other forms of mapping use the same texture coordinate 
parameterization, but with maps that contain something other than surface color. Displacement maps [23] 
contain perturbations of the surface position. They are typically used to add surface detail to a simple 
model. Bump maps [24] are similar, but instead give perturbations of the surface normal. They can make 
a smooth surface appear bumpy, but will not change the surface s silhouette. Normal maps [25] can also 
make a smooth surface appear bumpy, but contain the actual normal instead of just a perturbation of the 
normal. Texture mapping is available in most current graphics systems, including workstations and PCs. 
We expect to see bump mapping and similar surface shading techniques on graphics systems in the near 
future [26]. In fact, many of these mapping techniques are already possible using the procedural shading 
capabilities of PixelFlow[27]. Several researchers have explored the possibility of replacing geometric 
information with texture. Kajiya first introduced the "hierarchy of scale" of geometric models, mapping, 
and light­ing[28]. Cabral et. al. [29] addressed the transition between bump mapping and lighting effects. 
Westin et. al. [30] generated BRDFs from a Monte-Carlo ray tracing of an idealized piece of surface. 
Becker and Max [31] handle transitions from geometric detail in the form of displacement maps to shading 
in the form of bump maps. Fournier [25] generates maps with normal and shading information directly from 
surface geometry. Krishnamurthy and Levoy [32] fit complex, scanned surfaces with a set of smooth B­spline 
patches, then store some of the lost geometric information in a displacement map or bump map. Many algorithms 
first capture the geometric complexity of a scene in an image-based representation by rendering several 
different views and then render the scene using texture maps [33-36]. Polygonal Surface  Figure 2: 
Components of an appearance-preserving simplification system.  3 OVERVIEW We now present an overview 
of our appearance-preserving simplification algorithm. Figure 2 presents a breakdown of the algorithm 
into its components. The input to the algorithm is the polygonal surface, M0, to be simplified. The surface 
may come from one of a wide variety of sources, and thus may have a variety of characteristics. The types 
of possible input models include: CAD models, with per-vertex normals and a single color  Radiositized 
models, with per-vertex colors and no normals  Scientific visualization models, with per-vertex normals 
and per-vertex colors  Textured models, with texture-mapped colors, with or without per-vertex normals 
 To store the colors and normals in maps, we need a parameteriza­tion of the surface, F0(X): M0.P, where 
P is a 2D texture domain (texture plane), as shown in Figure 3. If the input model is already textured, 
such a parameterization comes with the model. Other­wise, we create one and store it in the form of per-vertex 
texture coordinates. Using this parameterization, per-vertex colors and normals are then stored in texture 
and normal maps. The original surface and its texture coordinates are then fed to the surface simplification 
algorithm. This algorithm is responsible for choosing which simplification operations to perform and 
in what order. It calls our texture deviation component to measure the deviation of the texture coordinates 
caused by each proposed operation. It uses the resulting error bound to help make its choices of operations, 
and stores the bound with each operation in its progressive mesh output. We can use the resulting progressive 
mesh with error bounds to create a static set of levels of detail with error bounds, or we can use the 
progressive mesh directly with a view-dependent simplifi­cation system at run-time. Either way, the error 
bound allows the run-time system to choose or adjust the tessellation of the models to meet a user-specified 
tolerance. It is also possible for the user to choose a desired polygon count and have the run-time system 
increase or decrease the error bound to meet that target.  4 REPRESENTATION CONVERSION Before we apply 
the actual simplification component of our algorithm, we perform a representation conversion (as shown 
in Figure 2). The representation we choose for our surface has a significant impact on the amount of 
simplification we can perform for a given level of visual fidelity. To convert to a form which decouples 
the sampling rates of the colors and normals from the sampling rate of the surface, we first parameterize 
the surface, then store the color and normal information in separate maps. 4.1 Surface Parameterization 
To store a surface's color or normal attributes in a map, the surface must first have a 2D parameterization. 
This function, F0(X): M0.P, maps points, X, on the input surface, M0, to points, x, * on the texture 
plane, P (see Figure 3). The surface is typically decomposed into several polygonal patches, each with 
its own parameterization. The creation of such parameterizations has been an active area of research 
and is fundamental for shape transfor­mation, multi-resolution analysis, approximation of meshes by NURBS, 
and texture mapping. Though we do not present a new algorithm for such parameterization here, it is useful 
to consider * Capital letters (e.g. X) refer to points in 3D, while lowercase letters (e.g. x) refer 
to points in 2D. the desirable properties of such a parameterization for our algo­rithm. They are: 1. 
Number of patches: The parameterization should use as few patches as possible. The triangles of the simplified 
surface must each lie in a single patch, so the number of patches places a bound on the minimum mesh 
complexity. 2. Vertex distribution: The vertices should be as evenly distrib­uted in the texture plane 
as possible. If the parameterization causes too much area compression, we will require a greater map 
resolution to capture all of our original per-vertex data. 3. One-to-one mapping: The mapping from the 
surface to the texture plane should be one-to-one. If the surface has folds in the texture plane, parts 
of the texture will be incorrectly stored and mapped back to the surface  Our particular application 
of the parameterization makes us somewhat less concerned with preserving aspect ratios than some other 
applications are. For instance, many applications apply F-1(x) to map a pre-synthesized texture map to 
an arbitrary surface. In that case, distortions in the parameterization cause the texture to look distorted 
when applied to the surface. However, in our application, the color or normal data originates on the 
surface itself. Any distortion created by applying F(X) to map this data onto P is reversed when we apply 
F-1(x) to map it back to M. Algorithms for computing such parameterizations have been studied in the 
computer graphics and graph drawing literature. Computer Graphics: In the recent computer graphics litera­ture, 
[12, 37, 38] use a spring system with various energy terms to distribute the vertices of a polygonal 
patch in the plane. [12, 32, 38, 39] provide methods for subdividing surfaces into separate patches based 
on automatic criteria or user-guidance. This body of research addresses the above properties one and 
two, but unfortu­nately, parameterizations based on spring-system algorithms do not generally guarantee 
a one-to-one mapping. Graph Drawing: The field of graph drawing addresses the issue of one-to-one mappings 
more rigorously. Relevant topics include straight-line drawings on a grid [40] and convex straight­line 
drawings [41]. Battista et al. [42] present a survey of the field. These techniques produce guaranteed 
one-to-one mappings, but the necessary grids for a graph with V vertices are worst case (and typically) 
O(V) width and height, and the vertices are generally unevenly spaced. To break a surface into polygonal 
patches, we currently apply an automatic subdivision algorithm like that presented in [12]. Their application 
requires a patch network with more constraints than ours. We can generally subdivide the surface into 
fewer patches. During this process, which grows Voronoi-like patches, we simply require that each patch 
not expand far enough to touch itself. To produce the parameterization for each patch, we employ a spring 
system with uniform weights. A side-by-side comparison of various choices of weights in [12] shows that 
uniform weights produce more evenly-distributed vertices than some other choices. For parameterizations 
used only with one particular map, it is also possible to allow more area compression where data values 
are similar. While this technique will generally create reasonable parameterizations, it would be better 
if there were a way to also guarantee that F(X) is one-to-one, as in the graph drawing literature.  
Figure 4: A patch from the leg Figure 5: Lion of an armadillo model and its model. associated normal 
map.  4.2 Creating Texture and Normal Maps Given a polygonal surface patch, M0, and its 2D parameterization, 
F, it is straightforward to store per-vertex colors and normals into the appropriate maps using standard 
rendering software. To create a map, scan convert each triangle of M0, replacing each of its vertex coordinates, 
Vj, with F(Vj), the texture coordinates of the vertex. For a texture map, apply the Gouraud method for 
linearly interpolating the colors across the triangles. For a normal map, interpolate the per-vertex 
normals across the triangles instead (Figure 4). The most important question in creating these maps is 
what the maximum resolution of the map images should be. To capture all the information from the original 
mesh, each vertex's data should be stored in a unique texel. We can guarantee this conservatively by 
choosing 1/d x 1/d for our map resolution, where d is the minimum distance between vertex texture coordinates: 
(1) d = min FV () -FV i ( j ) Vi ,Vj .M 0, i j If the vertices of the polygonal surface patch happen 
to be a uniform sampling of the texture space (e.g. if the polygonal surface patch was generated from 
a parametric curved surface patch), then the issues of scan conversion and resolution are simplified 
considerably. Each vertex color (or normal) is simply stored in an element of a 2D array of the appropriate 
dimensions, and the array itself is the map image. It is possible to trade off accuracy of the map data 
for run-time texturing resources by scaling down the initial maps to a lower resolution.  5 SIMPLIFICATION 
ALGORITHM Once we have decomposed the surface into one or more parame­terized polygonal patches with 
associated maps, we begin the actual simplification process. Many simplification algorithms perform a 
series of edge collapses or other local simplification operations to gradually reduce the complexity 
of the input surface. Figure 6: Texture coordinate deviation and correction on the lion s tail. Left: 
1,740 triangles full resolution. Middle and Right: 0.25 mm maximum image deviation. Middle: 108 triangles, 
no texture deviation metric. Right: 434 triangles with texture metric. The order in which these operations 
are performed has a large impact on the quality of the resulting surface, so simplification algorithms 
typically choose the operations in order of increasing error according to some metric. This metric may 
be local or global in nature, and for surface approximation algorithms, it provides some bound or estimate 
on the error in surface position. The operations to be performed are typically maintained in a priority 
queue, which is continually updated as the simplification pro­gresses. This basic design is applied by 
many of the current simplification algorithms, including [6-8, 15]. To incorporate our appearance-preservation 
approach into such an algorithm, the original algorithm is modified to use our texture deviation metric 
in addition to its usual error metric. When an edge is collapsed, the error metric of the particular 
surface approximation algorithm is used to compute a value for Vgen, the surface position of the new 
vertex (see Figure 3). Our texture deviation metric is then applied to compute a value for vgen, the 
texture coordinates of the new vertex. For the purpose of computing an edge s priority, there are sev­eral 
ways to combine the error metrics of surface approximation along with the texture deviation metric, and 
the appropriate choice depends on the algorithm in question. Several possibilities for such a total error 
metric include a weighted combination of the two error metrics, the maximum or minimum of the error metrics, 
or one of the two error metrics taken alone. For instance, when integrating with Garland and Heckbert 
s algorithm [6], it would be desirable to take a weighted combination in order to retain the precedence 
their system accords the topology-preserving collapses over the topology-modifying collapses. Similarly, 
a weighted combination may be desirable for an integration with Hoppe s system [15], which already optimizes 
error terms corresponding to various mesh attributes. The interactive display system later uses the error 
metrics to determine appropriate distances from the viewpoint either for switching between static levels 
of detail or for collapsing/splitting the edges dynamically to produce adaptive, view-dependent tessellations. 
If the system intends to guarantee that certain tolerances are met, the maximum of the error metrics 
is often an appropriate choice. 6 TEXTURE DEVIATION METRIC A key element of our approach to appearance-preservation 
is the measurement of the texture coordinate deviation caused by the simplification process. We provide 
a bound on this deviation, to (a) Figure 7: (a) An invalid choice for vgen in P, causing the new triangles 
extend outside the polygon. (b) Valid choices must lie in the shaded kernel. be used by the simplification 
algorithm to prioritize the potential edge collapses and by the run-time visualization system to choose 
appropriate levels of detail based on the current viewpoint. The lion s tail in Figure 6 demonstrates 
the need to measure texture coordinate deviation. The center figure is simplified by a surface approximation 
algorithm without using a texture deviation metric. The distortions are visible in the areas marked by 
red circles. The right tail is simplified using our texture deviation metric and does not have visible 
distortions. The image-space deviation bound now applies to the texture as well as to the surface. For 
a given point, X, on simplified mesh Mi, this deviation is the distance in 3D from X to the point on 
the input surface with the same texture coordinates: (2) Ti (X) = X -F-1 (F ( X)) 0 i We define the texture 
coordinate deviation of a whole triangle to be the maximum deviation of all the points in the triangle, 
and similarly for the whole surface: T( ) = max T( X); T( M ) = max T( D X) (3) i iii i X.D X.M i To 
compute the texture coordinate deviation incurred by an edge collapse operation, our algorithm takes 
as input the set of triangles before the edge collapse and Vgen, the 3D coordinates of the new vertex 
generated by the collapse operation. The algorithm outputs vgen, the 2D texture coordinates for this 
generated vertex, and a bound on Ti(D) for each of the triangles after the collapse.  6.1 Computing 
New Texture Coordinates We visualize the neighborhood of an edge to be collapsed in the texture plane, 
P, as shown in Figure 3. The boundary of the edge neighborhood is a polygon in P. The edge collapse causes 
us to replace the two vertices of the edge with a single vertex. The 3D coordinates, Vgen of this generated 
vertex are provided to us by the surface approximation algorithm. The first task of the texture deviation 
algorithm is to compute vgen, the 2D texture coordinates of this generated vertex. For vgen to be valid, 
it must lie in the convex kernel of our polygon in the texture plane [43] (see Figure 7). Meeting this 
criterion ensures that the set of triangles after the edge collapse covers exactly the same portion of 
the texture plane as the set of triangles before the collapse. Given a candidate point in the texture 
plane, we efficiently test the kernel criterion with a series of dot products to see if it lies on the 
inward side of each polygon edge. We first test some heuristic choices for the texture coordinates the 
midpoint of the original edge in the texture plane or one of the edge vertices. If the heuristic choices 
fail we compute a point inside the kernel by averaging three corners, found using linear programming 
tech­niques [43]. 6.2 Patch Borders &#38; Continuity Unlike an interior edge collapse, an edge collapse 
on a patch border can change the coverage in the texture plane, either by cutting off some of texture 
space or by extending into a portion of texture space for which we have no map data. Since neither of 
Figure 8: (a) An overlay in P determines the mapping between Mi-1 and Mi. (b) A set of polygonal mapping 
cells, each containing a dot. these is acceptable, we add additional constraints on the choice of vgen 
at patch borders. We assume that the area of texture space for which we have map data is rectangular 
(though the method works for any map that covers a polygonal area in texture space), and that the edges 
of the patch are also the edges of the map. If the entire edge to be collapsed lies on a border of the 
map, we restrict vgen to lie on the edge. If one of the vertices of the edge lies on a corner of the 
map, we further restrict vgen to lie at that vertex. If only one vertex is on the border, we restrict 
vgen to lie at that vertex. If one vertex of the edge lies on one border of the map and the other vertex 
lies on a different border, we do not allow the edge collapse. The surface parameterization component 
typically breaks the input model into several connected patches. To preserve geomet­ric and texture continuity 
across the boundary between them, we add further restrictions on the simplifications that are performed 
along the border. The shared border edges must be simplified on both patches, with matching choices of 
Vgen and vgen. 6.3 Measuring Texture Deviation Texture deviation is a measure of the parametric distortion 
caused by the simplification process. We measure this deviation using a method similar to the one presented 
to measure surface deviation in [8]. The main difference is that we now measure the deviation using our 
mapping in the texture plane, rather than in the plane of some planar projection. While [8] presents 
an overview of this technique, we present it more formally. Given the overlay (see Figure 8(a)) in the 
texture plane, P, of two simplified versions of the surface, Mi and Mj, we define the incremental texture 
deviation between them: -1 -1 (4) E,( ) =x Fi () -Fj ( ) x x ij This is the deviation between corresponding 
3D points on the surfaces, both with texture coordinates, x. Between any two sequential surfaces, Mi 
and Mi-1, differing only by an edge col­lapse, the incremental deviation, Ei,i-1(x), is only non-zero 
in the neighborhood of the collapsed edge (i.e. only in the triangles that actually move). The edges 
on the overlay in P partition the region into a set of convex, polygonal mapping cells (each identified 
by a dot in Figure 8(b)). Within each mapping cell, the incremental deviation function is linear, so 
the maximum incremental deviation for each cell occurs at one of its boundary points. Thus, we bound 
the incremental deviation using only the deviation at the cell vertices, vk: E(P) = maxE x = maxE (v 
) () (5) ii , -1 ii , -1 ii , -1 k x.P v k In terms of the incremental deviation, the total texture deviation, 
defined in (2) (the distance from points on Mi to corresponding points on the original surface, M0) is 
T () X =E (F () X ) (6) i i,0 i We approximate Ei,0(x) using a set of axis-aligned boxes. This provides 
a convenient representation of a bound on Ti(X), which we can update from one simplified mesh to the 
next without having to refer to the original mesh. Each triangle, Dk in Mi, has its own axis-aligned 
box, bi,k such that at every point on the triangle, the Minkowski sum of the 3D point with the box gives 
a region that contains the point on the original surface with the same texture coordinates. -1 X b " 
X .Dk , F (F ( )).X ¯ , (7) 0 i ik Figure 9(a) shows an original surface (curve) in black and a simplification 
of it, consisting of the thick blue and green lines. The box associated with the blue line, bi,0, is 
shown in blue, while the box for the green line, bi,1, is shown in green. The blue box slides along the 
blue line; at every point of application, the point on the base mesh with the same texture coordinate 
is contained within the translated box. For example, one set of corresponding points is shown in red, 
with its box also in red. ~ From (2) and (7), we produce T i(X), a bound on the total tex­ture deviation, 
Ti(X). This our texture deviation output. ~ (8) Ti (X) £ Ti ( ) = max X X -X¢ X¢. X¯bi, j ~ Ti(X) is 
the distance from X to the farthest corner of the box at X. This will always bound the distance from 
X to F-10 (Fi(X)). The maximum deviation over an edge collapse neighborhood is the ~ maximum T i(X) for 
any cell vertex. The boxes, bi,k, are the only information we keep about the position of the original 
mesh as we simplify. We create a new set of boxes, bi+1,k , for mesh Mi+1 using an incremental computation 
(described in Figure 10). Figure 9(b) shows the propagation from Mi to Mi+1. The blue and green lines 
are simplified to the pink line. The new box, bi+1,0 is constant as it slides across the pink line. The 
size and offset is chosen so that, at every point of application, the pink box, bi+1,0, contains the 
corresponding blue or green boxes, bi,0 or bi,1. If X is a point on Mi in triangle k, and Y is the point 
with the same texture coordinate on Mi+1, the containment property of (7) holds: -1 () (9) F0 (Fi +1 
Y ).X ¯ bik , . Y ¯ bi+1,k ¢ For example, all three red dots Figure 9(b) have the same texture coordinates. 
The red point on Mo is contained in the smaller red box, X ¯ bi,0, which is contained in the larger red 
box, Y ¯ bi+1,0. Because each mapping cell in the overlay between Mi and Mi+1 is linear, we compute the 
sizes of the boxes, bi+1,k , by considering Figure 9: 2D illustration of the box approximation to total 
deviation error. a) A curve has been simplified to two segment, each with an associated box to bound 
the deviation. b) As we simplify one more step, the ap­proximation is propagated to the newly created 
seg­ment. PropagateError(): foreach cell vertex, v foreach triangle, Told, in Mi-1 touching v foreach 
triangle, Tnew, in Mi touching v PropagateBox(v, Told, Tnew)  PropagateBox(v, Told, Tnew): Pold = Fi-1-1(v), 
Pnew = Fi-1(v) Enlarge Told.box so that Told.box applied at Pold contains Tnew.box applied at Pnew 
Figure 10: Pseudo-code for the propagation of deviation error from mesh Mi-1 to mesh Mi. only the box 
correspondences at cell vertices. In Figure 9(b), there are three places we must consider. If the magenta 
box contains the blue and green boxes in all three places, it will contain them everywhere. Together, 
the propagation rules, which are simple to imple­ment, and the box-based approximation to the texture 
deviation, provide the tools we need to efficiently provide a texture devia­tion for the simplification 
process.    7 IMPLEMENTATION AND RESULTS In this section we present some details of our implementation 
of the various components of our appearance-preserving simplifica­tion algorithm. These include methods 
for representation conver­sion, simplification and, finally, interactive display. 7.1 Representation 
Conversion We have applied our technique to several large models, including those listed in Table 1. 
The bumpy torus model (Figure 1) was created from a parametric equation to demonstrate the need for greater 
sampling of the normals than of the surface position. The lion model (Figure 5) was designed from NURBS 
patches as part of a much larger garden environment, and we chose to decorate it with a marble texture 
(and a checkerboard texture to make texture deviation more apparent in static images). Neither of these 
models required the computation of a parameterization. The armadillo (Figure 12) was constructed by merging 
several laser-scanned meshes into a single, dense polygon mesh. It was decomposed into polygonal patches 
and parameterized using the algorithm presented in [32], which eventually converts the patches into a 
NURBS representation with associated displacement maps. Because all these models were not only parameterized, 
but available in piecewise-rational parametric representations, we generated polygonal patches by uniformly 
sampling these repre­sentations in the parameter space. We chose the original tessella­tion of the models 
to be high enough to capture all the detail available in their smooth representations. Due to the uniform 
sampling, we were able to use the simpler method of map creation (described in Section 4.2), avoiding 
the need for a scan­conversion process.  7.2 Simplification We integrated our texture deviation metric 
with the successive mapping algorithm for surface approximation [8]. The error metric for the successive 
mapping algorithm is simply a 3D surface deviation. We used this deviation only in the computation of 
Vgen. Our total error metric for prioritizing edges and choosing switching distances is just the texture 
deviation. This is sensible because the texture deviation metric is also a measure of surface deviation, 
whose particular mapping is the parameterization. Thus, if the successive mapping metric is less than 
the texture deviation metric, we must apply the texture deviation metric, because it is the minimum bound 
we know that guarantees the bound on our texture deviation. On the other hand, if the succes­sive mapping 
metric is greater than the texture deviation metric, the texture deviation bound is still sufficient 
to guarantee a bound on both the surface deviation and the texture. To achieve a simple and efficient 
run-time system, we apply a post-process to convert the progressive mesh output to a static set of levels 
of detail, reducing the mesh complexity by a factor of two at each level. Our implementation can either 
treat each patch as an independ­ent object or treat a connected set of patches as one object. If we simplify 
the patches independently, we have the freedom to switch their levels of detail independently, but we 
will see cracks be­tween the patches when they are rendered at a sufficiently large error tolerance. 
Simplifying the patches together allows us to prevent cracks by switching the levels of detail simultaneously. 
Table 1 gives the computation time to simplify several models, Table 1: Several models used to test appearance­ 
 Model Patches Input Tris Time Map Res. Torus Lion Armadillo 1 49 102 44,252 86,844 2,040,000 4.4 7.4 
190 512x128 N.A. 128x128 preserving simplification. Simplification time is in min­ utes on a MIPS R10000 
processor. as well as the resolution of each map image. Figure 11 and Figure 12 show results on the armadillo 
model. It should be noted that the latter figure is not intended to imply equal computational costs for 
rendering models with per-vertex normals and normal maps. Simplification using the normal map representation 
provides measurable quality and reduced triangle overhead, with an additional overhead dependent on the 
screen resolution.  7.3 Interactive Display System We have implemented two interactive display systems: 
one on top of SGI s IRIS Performer library, and one on top of a custom library running on a PixelFlow 
system. The SGI system supports color preservation using texture maps, and the PixelFlow system supports 
color and normal preservation using texture and normal maps, respectively. Both systems apply a bound 
on the distance from the viewpoint to the object to convert the texture deviation error in 3D to a number 
of pixels on the screen, and allow the user to specify a tolerance for the number of pixels of deviation. 
The tolerance is ultimately used to choose the primitives to render from among the statically generated 
set of levels of detail. Our custom shading function on the PixelFlow implementation performs a mip-mapped 
look-up of the normal and applies a  249,924 triangles 62,480 triangles 7,809 triangles 975 triangles 
0.05 mm max image deviation 0.25 mm max image deviation 1.3 mm max image deviation 6.6 mm max image deviation 
Phong lighting model to compute the output color of each pixel. The current implementation looks up normals 
with 8 bits per component, which seems sufficient in practice (using [44])   8 ONGOING WORK AND CONCLUSIONS 
There are several directions to pursue to improve our system for appearance-preserving simplification. 
We would like to experi­ment more with techniques to generate parameterizations that allow efficient 
representations of the mapped attributes as well as guarantee a one-to-one mapping to the texture plane. 
It would be nice for the simplification component to do a better job of optimizing the 3D and texture 
coordinates of the generated vertex for each edge collapse, both in 3D and the texture plane. Also, it 
may be interesting to allow the attribute data of a map to influence the error metric. We would also 
like to integrate our technique with a simplification algorithm like [6] that deals well with imperfect 
input meshes and allows some topological changes. Finally, we want to display our resulting progressive 
meshes in a system that performs dynamic, view-dependent management of LODs. Our current system demonstrates 
the feasibility and desirability of our approach to appearance-preserving simplification. It produces 
high-fidelity images using a small number of high­quality triangles. This approach should complement 
future graphics systems well as we strive for increasingly realistic real­time computer graphics. ACKNOWLEDGMENTS 
We would like to thank Venkat Krishnamurthy and Marc Levoy at the Stanford Computer Graphics Laboratory 
and Peter Schröder for the armadillo model, and Lifeng Wang and Xing Xing Com­puter for the lion model. 
Our visualization system implementation was made possible by the UNC PixelFlow Project and the Hewlett 
Packard Visualize PxFl team. We also appreciate the assistance of the UNC Walkthrough Project. This work 
was supported in part by an Alfred P. Sloan Foundation Fellowship, ARO Contract DAAH04-96-1-0257, NSF 
Grant CCR-9625217, ONR Young Investigator Award, Honda, Intel, NSF/ARPA Center for Com­puter Graphics 
and Scientific Visualization, and NIH/National Center for Research Resources Award 2P41RR02170-13 on 
Interactive Graphics for Molecular Studies and Microscopy. REFERENCES [1] L. Williams, Pyramidal Parametrics, 
SIGGRAPH 83 Conference Proceed­ings, pp. 1--11, 1983. [2] J. Rossignac and P. Borrel, Multi-Resolution 
3D Approximations for Rendering, in Modeling in Computer Graphics: Springer-Verlag, 1993, pp. 455--465. 
[3] G. Schaufler and W. Sturzlinger, Generating Multiple Levels of Detail from Polygonal Geometry Models, 
Virtual Environments'95 (Eurographics Work­shop), pp. 33-41, 1995. [4] W. J. Schroeder, J. A. Zarge, 
and W. E. Lorensen, Decimation of Triangle Meshes, in Proc. of ACM Siggraph, 1992, pp. 65--70. [5] G. 
Turk, Re-tiling polygonal surfaces, Comput. Graphics, vol. 26, pp. 55--64, 1992. [6] M. Garland and P. 
Heckbert, Surface Simplification using Quadric Error Bounds, SIGGRAPH'97 Conference Proceedings, pp. 
209-216, 1997. [7] A. Gueziec, Surface Simplification with Variable Tolerance, in Second Annual Intl. 
Symp. on Medical Robotics and Computer Assisted Surgery (MRCAS '95), November 1995, pp. 132--139. [8] 
J. Cohen, D. Manocha, and M. Olano, Simplifying Polygonal Models Using Successive Mappings, Proc. of 
IEEE Visualization'97, pp. 395-402, 1997. [9] R. Ronfard and J. Rossignac, Full-range approximation of 
triangulated polyhedra, Computer Graphics Forum, vol. 15, pp. 67--76 and 462, August 1996. [10] R. Klein, 
G. Liebich, and W. Straßer, Mesh Reduction with Error Control, in IEEE Visualization '96: IEEE, October 
1996. [11] J. Cohen, A. Varshney, D. Manocha, G. Turk, H. Weber, P. Agarwal, F. Brooks, and W. Wright, 
Simplification Envelopes, in SIGGRAPH'96 Confer­ence Proceedings, 1996, pp. 119--128. [12] M. Eck, T. 
DeRose, T. Duchamp, H. Hoppe, M. Lounsbery, and W. Stuetzle, Multiresolution Analysis of Arbitrary Meshes, 
in SIGGRAPH'95 Conference Proceedings, 1995, pp. 173--182. [13] W. Schroeder, A Topology Modifying Progressive 
Decimation Algorithm, Proc. of IEEE Visualization'97, pp. 205-212, 1997. [14] J. El-Sana and A. Varshney, 
Controlled Simplification of Genus for Polygonal Models, Proc. of IEEE Visualization'97, pp. 403-410, 
1997. [15] H. Hoppe, Progressive Meshes, in SIGGRAPH 96 Conference Proceedings: ACM SIGGRAPH, 1996, pp. 
99--108. [16] H. Hoppe, View-Dependent Refinement of Progressive Meshes, SIG-GRAPH'97 Conference Proceedings, 
pp. 189-198, 1997. [17] J. Xia, J. El-Sana, and A. Varshney, Adaptive Real-Time Level-of-detail­based 
Rendering for Polygonal Models, IEEE Transactions on Visualization and Computer Graphics, vol. 3, pp. 
171--183, 1997. [18] C. Bajaj and D. Schikore, Error-bounded reduction of triangle meshes with multivariate 
data, SPIE, vol. 2656, pp. 34--45, 1996. [19] M. Hughes, A. Lastra, and E. Saxe , Simplification of Global-Illumination 
Meshes, Proceedings of Eurographics '96, Computer Graphics Forum, vol. 15, pp. 339-345, 1996. [20] C. 
Erikson and D. Manocha, Simplification Culling of Static and Dynamic Scene Graphs, UNC-Chapel Hill Computer 
Science TR98-009, 1998. [21] P. Schroder and W. Sweldens, Spherical Wavelets: Efficiently Representing 
Functions on the Sphere, SIGGRAPH 95 Conference Proceedings, pp. 161-­172, August 1995. [22] A. Certain, 
J. Popovic, T. Derose, T. Duchamp, D. Salesin, and W. Stuetzle, Interactive Multiresolution Surface Viewing, 
in Proc. of ACM Siggraph, 1996, pp. 91--98. [23] R. L. Cook, Shade trees, in Computer Graphics (SIGGRAPH 
'84 Proceed­ings), vol. 18, H. Christiansen, Ed., July 1984, pp. 223--231. [24] J. Blinn, Simulation 
of Wrinkled Surfaces, SIGGRAPH '78 Conference Proceedings, vol. 12, pp. 286--292, 1978. [25] A. Fournier, 
Normal distribution functions and multiple surfaces, Graphics Interface '92 Workshop on Local Illumination, 
pp. 45--52, 1992. [26] M. Peercy, J. Airey, and B. Cabral, Efficient Bump Mapping Hardware, SIGGRAPH'97 
Conference Proceedings, pp. 303-306, 1997. [27] M. Olano and A. Lastra, A Shading Language on Graphics 
Hardware: The PixelFlow Shading System, SIGGRAPH 98 Conference Proceedings, 1998. [28] J. Kajiya, Anisotropic 
Reflection Models, SIGGRAPH '85 Conference Proceedings, pp. 15--21, 1985. [29] B. Cabral, N. Max, and 
R. Springmeyer, Bidirectional Reflection Functions From Surface Bump Maps, SIGGRAPH '87 Conference Proceedings, 
pp. 273­-281, 1987. [30] S. Westin, J. Arvo, and K. Torrance, Predicting Reflectance Functions From Complex 
Surfaces, SIGGRAPH '92 Conference Proceedings, pp. 255--264, 1992. [31] B. G. Becker and N. L. Max, Smooth 
Transitions between Bump Rendering Algorithms, in Computer Graphics (SIGGRAPH '93 Proceedings), vol. 
27, J. T. Kajiya, Ed., August 1993, pp. 183--190. [32] V. Krishnamurthy and M. Levoy, Fitting Smooth 
Surfaces to Dense Polygon Meshes, SIGGRAPH 96 Conference Proceedings, pp. 313--324, 1996. [33] D. G. 
Aliaga, Visualization of Complex Models using Dynamic Texture-based Simplification, Proc. of IEEE Visualization'96, 
pp. 101--106, 1996. [34] L. Darsa, B. Costa, and A. Varshney, Navigating Static Environments using Image-space 
simplification and morphing, Proc. of 1997 Symposium on Inter­active 3D Graphics, pp. 25-34, 1997. [35] 
P. W. C. Maciel and P. Shirley, Visual Navigation of Large Environments Using Textured Clusters, Proc. 
of 1995 Symposium on Interactive 3D Graph­ics, pp. 95--102, 1995. [36] J. Shade, D. Lischinski, D. Salesin, 
T. DeRose, and J. Snyder, Hierarchical Image Caching for Accelerated Walkthroughs of Complex Environments, 
SIGGRAPH 96 Conference Proceedings, pp. 75--82, August 1996. [37] J. Kent, W. Carlson, and R. Parent, 
Shape transformation for polyhedral objects, SIGGRAPH '92 Conference Proceedings, pp. 47--54, 1992. [38] 
J. Maillot, H. Yahia, and A. Veroust, Interactive Texture Mapping, SIGGRAPH'93 Conference Proceedings, 
pp. 27--34, 1993. [39] H. Pedersen, A Framework for Interactive Texturing Operations on Curved Surfaces, 
in SIGGRAPH 96 Conference Proceedings, Annual Conference Se­ries, H. Rushmeier, Ed., August 1996, pp. 
295--302. [40] H. d. Fraysseix, J. Pach, and R. Pollack, How to Draw a Planar Graph on a Grid, Combinatorica, 
vol. 10, pp. 41--51, 1990. [41] N. Chiba, T. Nishizeki, S. Abe, and T. Ozawa, A Linear Algorithm for 
Embedding Planar Graphs Using PQ-Trees, J. Comput. Syst. Sci., vol. 30, pp. 54--76, 1985. [42] G. D. 
Battista, P. Eades, R. Tamassia, and I. G. Tollis, Algorithms for drawing graphs: an annotated bibliography, 
Comput. Geom. Theory Appl., vol. 4, pp. 235--282, 1994. [43] M. d. Berg, M. v. Kreveld, M. Overmars, 
and O. Schwarzkopf, Computational Geometry: Algorithms and Applications: Springer-Verlag, 1997. [44] 
R. F. Lyon, Phong Shading Reformulation for Hardware Renderer Simplifica­tion, Apple Computer #43, 1993. 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280834</article_id>
		<sort_key>123</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Progressive forest split compression]]></title>
		<page_from>123</page_from>
		<page_to>132</page_to>
		<doi_number>10.1145/280814.280834</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280834</url>
		<keywords>
			<kw><![CDATA[algorithms]]></kw>
			<kw><![CDATA[geometric compression]]></kw>
			<kw><![CDATA[graphics]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>E.4</cat_node>
				<descriptor>Data compaction and compression</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10002952.10002971.10003451.10002975</concept_id>
				<concept_desc>CCS->Information systems->Data management systems->Data structures->Data layout->Data compression</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39036867</person_id>
				<author_profile_id><![CDATA[81100305012]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gabriel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Taubin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IBM T. J. Watson Research Center, Yorktown Heights, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P16872</person_id>
				<author_profile_id><![CDATA[81100280442]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Andr&#233;]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gu&#233;ziec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IBM T. J. Watson Research Center, Yorktown Heights, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P299235</person_id>
				<author_profile_id><![CDATA[81100199874]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Horn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IBM T. J. Watson Research Center, Yorktown Heights, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35023224</person_id>
				<author_profile_id><![CDATA[81100044604]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Francis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lazarus]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IRCOM-SIC, Futuroscope, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[MPEG4/SNHC verification model 5.0, July 1997. ISO/IEC JTC1/- SC29/WG 11 Document N 1820, Caspar Horne (ed.).]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[E. Catmull and J. Clark. Recursively generated B-spline surfaces on arbitrary topological meshes. Computer Aided Design, 10:350-355, 1978.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218391</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[M. Deering. Geometric compression. In Siggraph'95 Conference Proceedings, pages 13-20, August 1995.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[D. Doo and M. Sabin. Behaviour of recursive division surfaces near extraordinary points. Computer Aided Design, 10:356-360, 1978.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218440</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[M. Eck, T. DeRose, T. Duchamp, H. Hoppe, M. Lounsbery, and W. Stuetzle. Multiresolution analysis of arbitrary meshes. In Siggraph'95 Conference Proceedings, pages 173-182, August 1995.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[A. Gu6ziec. Surface simplification with variable tolerance. In Second Annual International Symposium on Medical Robotics and Computer Assisted Surgery, pages 132-139, Baltimore, MD, November 1995.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[A. Gu6ziec, G. Taubin, E Lazarus, and W. Horn. Cutting and Stitching: Efficient Conversion of a Non-Manifold Polygonal Surface to a Manifold. Technical Report RC-20935, IBM Research, July 1997.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>271907</ref_obj_id>
				<ref_obj_pid>271897</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[A. Gu6ziec, G. Taubin, E Lazarus, and W. Horn. Simplicial maps for progressive transmission of polygonal surfaces. In VRML 98. ACM, February 1998.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617601</ref_obj_id>
				<ref_obj_pid>616014</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[M. Hall and J. Warren. Adaptive polygonalization of implicitly defined surfaces. IEEE Computer Graphics and Applications, pages 33-42, November 1990.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>180223</ref_obj_id>
				<ref_obj_pid>180214</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[B. Hamann. A data reduction scheme for triangulated surfaces. Computer Aided Geometric Design, 11 (2): 197-214,1994.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237216</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[H. Hoppe. Progressive meshes. In Siggraph'96 Conference Proceedings, pages 99-108, August 1996.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166119</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[H. Hoppe, T. DeRose, T. Duchamp, J. McDonald, and W. Stuetzle. Mesh optimization. In Siggraph'93 Conference Proceedings, pages 19-25, July 1993.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[C. Loop. Smooth subdivision surfaces based on triangles. Master's thesis, Dept. of Mathematics, University of Utah, August 1987.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258852</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[J. Popovi6 and H. Hoppe. Progressive simplicial complexes. In Siggraph'97 Conference Proceedings, pages 217-224, August 1997.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[R. Ronfard and J. Rossignac. Simplifying a triangular mesh with multiple planar constraints. Technical report, IBM Research, 1994.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[R. Ronfard and J. Rossignac. Triangulating multiply-connected polygons: A simple, yet efficient algorithm. Computer Graphics Forum, 13(3):C281-C292,1994. Proc. Eurographics'94, Oslo, Norway.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[J. Rossignac and E Borrel. Geometric Modeling in Computer Graphics, chapter Multi-resolution 3D approximations for rendering complex scenes, pages 455-465. Springer Verlag, 1993.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218473</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[G. Taubin. A signal processing approach to fair surface design. In Siggraph'95 Conference Proceedings, pages 351-358, August 1995.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[G. Taubin, W.E Horn, E Lazarus, and J. Rossignac. Geometric Coding and VRML. Proceedings of the IEEE, July 1998. (to appear) Also IBM Research TR RC-20925, July 1997.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>274365</ref_obj_id>
				<ref_obj_pid>274363</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[G. Taubin and J. Rossignac. Geometry Compression through Topological Surgery. ACM Transactions on Graphics, April 1998. (to appear).]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258863</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[D. Zorin, E Schr6der, and W. Sweldens. Interactive multiresolution mesh editing. In Siggraph'97 Conference Proceedings, pages 259- 268, August 1997.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Copyright &#38;#169;1998 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers or to redistribute to lists, requires specific permission and/or a fee. Progressive Forest 
Split Compression Gabriel Taubin 1 Andr´eziec 1 William Horn 1 Francis Lazarus 2 eGu´ IBM T. J. Watson 
Research Center ABSTRACT In this paper we introduce the Progressive Forest Split (PFS) repre­sentation, 
a new adaptive re.nement scheme for storing and trans­mitting manifold triangular meshes in progressive 
and highly com­pressed form. As in the Progressive Mesh (PM) method of Hoppe, a triangular mesh is represented 
as a low resolution polygonal model followed by a sequence of re.nement operations, each one spec­ifying 
how to add triangles and vertices to the previous level of detail to obtain a new level. The PFS format 
shares with PM and other re.nement schemes the ability to smoothly interpolate between consecutive levels 
of detail. However, it achieves much higher compression ratios than PM by using a more complex re.ne­ment 
operation which can, at the expense of reduced granularity, be encoded more ef.ciently. A forest split 
operation doubling the number nof triangles of a mesh requires a maximum of approxi­mately 3:5nbits to 
represent the connectivity changes, as opposed to approximately (5+log2(n))nbits in PM. We describe algorithms 
to ef.ciently encode and decode the PFS format. We also show how any surface simpli.cation algo­rithm 
based on edge collapses can be modi.ed to convert single resolution triangular meshes to the PFS format. 
The modi.cations are simple and only require two additional topological tests on each candidate edge 
collapse. We show results obtained by applying these modi.cations to the Variable Tolerance method of 
Gu´eziec. CR Categories and Subject Descriptors: I.3.5 [Computer Graphics]: Computational Geometry and 
Object Modeling -surface, solid, and object representations. General Terms: Geometric Compression, Algorithms, 
Graphics.  1 INTRODUCTION Although modeling systems in Mechanical Computer Aided De­sign and in animation 
are expanding their geometric domain to free form surfaces, polygonal models remain the primary 3D represen­tation 
used in the manufacturing, architectural, and entertainment industries. Polygonal models are particularly 
effective for hardware assisted rendering, which is important for video-games, virtual re­ality, .y-through, 
and digital prototyping. A polygonal model is de.ned by the position of its vertices (geometry); by the 
association between each face and its sustaining vertices (connectivity); and optional colors, normals 
and texture coordinates (properties). In this paper we concentrate on manifold polygonal models described 
by triangular meshes without attached properties. However, we address issues of non-triangular polygons, 
properties, and non-manifolds in Section 7. A method to triangu­late arbitrary polygonal faces is described 
by Ronfard and Rossi­gnac [16]. A method to convert non-manifold polygonal models to manifold polygonal 
models is described by Gu´eziec et al. [7]. 1IBM T.J.Watson Research Center, P.O.Box 704, Yorktown Heights, 
NY 10598, ftaubin,gueziec,hornwpg@watson.ibm.com 2IRCOM-SIC (UMR CNRS 6615), SP2MI, Bvd. 3, Teleport 
2, B.P. 179, 86960 Futuroscope Cedex, France, lazarus@sic.univ-poitiers.fr AB CD Figure 1: The forest 
split operation. A: A triangular mesh with a forest of edges marked in red. B: Resulting mesh after cutting 
through the forest edges and splitting vertices in the resulting tree boundary loops. C: Simple polygonstobestitchedtotheboundaryloops. 
Thecorrespondencebetween polygon boundaries and tree boundary loops is implicit. D: The re.ned mesh. 
Normally, to produce a smooth transition, the vertices are displaced only after the boundary loops are 
triangulated. In the .gure they have been displaced immediately after the cutting to illustrate the connectivity 
re.nement process. Polygonal models are typically stored in .le servers and ex­changed over computer 
networks. It is frequently desirable to compress models to reduce storage and transmission time require­ments. 
Effectivesingle-resolution-compressionschemeshavebeen recently introduced by Deering [3] and Taubin and 
Rossignac [20]. While single resolution schemes can be used to reduce trans­mission bandwidth, it is 
frequently desirable to send the model in a progressive fashion. For example a progressive scheme may 
start by sending a compressed version of the lowest resolution level of a level-of-detail (LOD) hierarchy. 
After the lowest level has been sent, a sequence of additional re.nement operations may be sent in parallel 
to the rendering operation. In this manner, successively .ner levels of detail may be displayed while 
even more detailed levels are still arriving. To prevent visual artifacts, sometimes referred to as popping 
, it is also desirable to be able to transition smoothly, or geomorph, from one level of the LOD hierarchy 
to the next by interpolating the positions of corresponding vertices in consecutive levels of detail 
as a function of time. The Progressive Forest Split (PFS) scheme is introduced in this paperand features 
a new adaptive re.nementscheme for storing and transmitting triangle meshes in progressive and highly 
compressed form. In this scheme a manifold triangular mesh is represented as a low resolution polygonal 
model followed by a sequence of re.nement operations. The scheme permits the smooth transition between 
successive levels of re.nement. High compression ratios are achieved by using a new re.nement operation 
which can produce more changes per bit than existing schemes. The scheme requires only O(n)bits to double 
the size of a mesh with nvertices. The forest split operation, the re.nement operation of the PFS scheme, 
is illustrated in Figure 1. It is speci.ed by a forest in the graph of vertices and edges of the mesh, 
a sequence of simple polygons (triangulated with no internal vertices), and a sequence of vertex displacements. 
The mesh is re.ned by cutting the mesh through the forest, splitting the resulting boundaries apart, 
.lling each of the resulting tree boundary loops with one of the simple polygons, and .nally displacing 
the new vertices. In Section 3 we describe the algorithms for ef.ciently encoding anddecodingPFSmeshes. 
Weshowhowanysurfacesimpli.cation algorithm based on edge collapses can be modi.ed to convert single resolution 
triangular meshes to PFS format. The modi.cations require performing two simple additional topological 
tests on each candidate edge collapse. In Section 6 we show results obtained by applying these modi­.cations 
to the Variable Tolerance surface simpli.cation method of Gu´eziec [6]. Using this method we guarantee 
simpli.cation error bounds on all levels of details as a measure of the approximation quality. We .nish 
the paper with a short discussion on extensions and future work.  2 PREVIOUS WORK Single-resolution 
mesh compression schemes Deer­ing s method [3] is designed to compress the data for transmission from 
the CPU to the graphics adapter. The method uses a stack­buffer to store 16 of the previously used vertices 
instead of having random access to all the vertices of the model. The triangles of the mesh are partitioned 
into generalized triangle meshes,and the connectivity of the triangular mesh is lost. The vertex positions 
are quantized and the differences between consecutive values are entropy encoded. The Topological Surgery 
(TS) method of Taubin and Rossignac [20] was designed for fast network transmission and compact stor­age. 
In this method the connectivity of a manifold triangular mesh is encoded without loss of information, 
with storage rates approach­ing one bit per triangle for large models. In this scheme the vertices are 
organized as a spanning tree, and the triangles as a simple poly­gon. The vertex positions and properties 
are quantized, predicted as a linear combination of ancestors along the vertex tree, and the corrections 
are entropy encoded. A more detailed description is pre­sented in Section 3.1. The method has been extended 
to handle all the polygonal models which can be represented in the Virtual Real­ity Modeling Language 
(VRML) [19], including all properties and property bindings permitted by the language. Compression ratios 
of up to 50:1 or more can be achieved for large VRML models. Recursive subdivision and re.nement Recursive 
sub­division schemes [2, 4, 13] provide a truly progressive represen­tation for a limited family of meshes. 
Most have the ability to transition smoothly between consecutive levels of detail. In a re­cursive subdivision 
scheme a polygonal mesh is de.ned as a low resolution base mesh followed by a sequence of subdivision 
steps. Each subdivision step can be further decomposed into a connectiv­ity re.nement step, and a smoothing 
or geometry update step. In the connectivity re.nement step more vertices and faces are added to the 
mesh. These additions usually do not change the topological AB 1 2 3 12 6 11 5 4 97 10 8  CD 44 
 4  8 20 4  EF Figure 2: The topological surgery method of Taubin and Rossignac [20]. A: The vertex 
tree on the surface. Each run is painted with a different color. B: The result of cutting through the 
edges of the vertex tree (the vertex positions have been modi.ed here to arti.cially enlarge the gap 
created by the topological cut) is the simple polygon. C: The structure of the vertex tree with the vertex 
tree traversal indices. D: The vertex tree becomes the boundary loop after cutting through its edges. 
E: The simple polygon (arti.cially .attened) has no internal vertices. Simple polygon vertices are labeled 
here with their corresponding vertex tree traversal indices. F: The dual graph of the simple polygon 
is the triangle tree. One marching bit per regular node is also required to describe the triangulation 
of its runs. The order for the vertex indices is derived from the vertex tree traversal order. Simple 
polygon vertices are labeled here with their boundary loop indices. The correspondence between boundary 
loop indices and vertex tree indices is stored in a lookup table constructed by traversing the vertex 
tree. type and new vertices are positioned such that the overall geometry does not change. In the smoothing 
step, depending on whether the method is approximating or interpolating, some or all of the vertices 
of the mesh with re.ned connectivity are displaced. Usually the goal is to show that after an in.nite 
number of subdivision steps, the sequence of polygonal meshes converges to a smooth continuous surface, 
but in practice only a few subdivision steps are applied. Uniform subdivision schemes can be regarded 
as the optimal compression schemes. Both the connectivity re.nement and the smoothing steps are de.ned 
globally by a few parameters per sub­division step [18] and the schemes are optimal in the sense that 
the number of parameters is independent of the number of ver­tices and faces created at each subdivision 
step. However, these high compression ratios are obtained only for meshes with recur­sive subdivision 
connectivity. Eck et al. [5] describe a method to approximate a triangular mesh by a new mesh with recursive 
subdi­vision connectivity and approximately the same geometry, but very often the option of changing 
the connectivity of the mesh in this way is not possible. Adaptive subdivision schemes [9, 21] must specify 
how and where the mesh connectivity is to be re.ned. This may require as little as only one bit per vertex, 
edge, or face to specify each connectivity re.nement step. These schemes may also require vertex displacements 
for the newly created vertices, instead of, or in addition to, the global parameters de.ning the smoothing 
step. As noted above, the main problem with existing uniform and adaptive subdivision schemes is that 
a general triangular mesh usu­ally does not satisfy subdivision requirements and cannot be com­pressed 
with these methods. The Progressive Mesh (PM) scheme introduced by Hoppe [11] solves this problem. The 
scheme is not a subdivision scheme but an adaptive re.nement scheme where new faces are not created by 
subdividing existing faces, but by insert­ing them in between existing faces. Every triangular mesh can 
be represented as a base mesh followed by a sequence of re.nements referred to as vertex splits. Each 
vertex split is speci.ed for the current level of detail by identifying two edges and a shared vertex. 
The mesh is re.ned by cutting it through the pair of edges, splitting the common vertex into two vertices 
and creating a quadrilateral hole, which is .lled with two triangles sharing the edge connecting the 
two new vertices. The PM scheme is not an ef.cient compres­sion scheme. Since the re.nement operations 
perform very small and localized changes the scheme requires O(nlog2(n))bits to double the size of a 
mesh with nvertices. The re.nement operation of the PFS scheme introduced in this paper, the forest split, 
can be seen as a grouping of several con­secutive edge split operations into a set, instead of a sequence. 
In the PFS scheme there is a tradeoff between compression ratios and granularity. The highest compression 
ratios are achieved by mini­mizing the number of levels of detail. Most often the high number of levels 
of detail produced by the PM scheme are not required. Hoppe typically de.nes the levels of the LOD hierarchy 
of his PM representation using exponential steps.  3 THE PFS REPRESENTATION A multi-resolution mesh 
represented in the PFS format is composed of an initial low resolution level of detail followed by a 
sequence of forest split operations. Although any method could be used to represent the lowest resolution 
level of detail, we use the TS method because the PFS representation is a natural extension of the representation 
used in this scheme. For both the lowest resolution base mesh and the forest split operations we make 
a distinction between the representation and the encoding of the representation. 3.1 Topological Surgery 
In this section we give a brief description of the TS representation for a simple mesh, that is, a triangle 
mesh with sphere topology. With a few minor additions, manifolds of arbitrary genus, with or without 
boundaries, and orientable or non-orientable can also be represented. Since these additional concepts 
are not needed to describe the PFS format, we refer the interested reader to the original reference for 
the details. Representation Figure 2 illustrates the main concepts of the TS representation. In this 
method the vertices of a triangular mesh are organized as a rooted spanning tree in the graph of the 
mesh, called the vertex tree (Figure 2-A). As shown in Figure 2-B,E, when a simple mesh is cut through 
the vertex tree edges, the connectivity of the resulting mesh is a simple polygon. The edges of the simple 
polygon form a boundary loop. The order of traversal of the vertex tree (Figure 2-C) de.nes a one-to-two 
correspondence between the edges of the vertex tree and the edges of the boundary loop (Figure 2-D). 
This correspon­dence de.neswhich pairs of boundary loop edgesshould be stitched together to reconstruct 
the connectivity of the original mesh. Encoding The encoding of this representation in the com­pressed 
data stream is composed of, in order: the encoding of the vertex tree, the compressed coordinate information, 
and the encoding of the simple polygon. The vertex tree is run-length encoded. The tree is decomposed 
into runs (shown in different colors in Figure 2-A,C). A run connects a leaf or branching node to another 
leaf or branching node through a path of zero or more regular nodes. The order of traversal of the tree 
de.nes an order of traversals of the runs, and a .rst and last node for each run. Each run is encoded 
as a record composed of three .elds (is-last-run,length-of-run,ends-in-leaf).The is-last-run .eld is 
a bit that determines if the runs shares the .rst node with the next run or not. It determines the pushing 
of branching node indices onto a traversal stack.The length-of-run .eld is a variable length integer 
(same number of bits for all the runs in the tree) with a value equal to the number of edges in the run. 
The ends-in-leaf .eld is a bit which determines if the run ends in a leaf or branching node, and the 
popping of branching node indices from the traversal stack. The coordinate information is placed in the 
compressed stream in the order of traversal of the vertex tree. The coordinate data is compressed by 
storing errors instead of absolute coordinates. The errors are calculated with respect to a predictor. 
The predictor is computed as a linear combination of several ancestors in the tree and is quantized to 
a certain number of bits per coordinate with respect to a bounding box. The errors are then Huffman-encoded. 
Once the coordinate information is received, the geometry of the mesh can be reconstructed as an array 
of vertex coordinates. The dual graph of the simple polygon is also a tree (Figure 2-F). The structure 
of this triangle tree is run-length encoded in the same way as the vertex tree, except that the is-last-run 
.eld is not neces­sary, because the triangle tree is a binary tree. The structure of the triangle tree 
does not completely describe the triangulation of the polygon. To complete the description, an extra 
bit per triangle asso­ciated with each regular node of the triangle tree must be included. These marching 
bits determine how to triangulate the runs of the tree byadvancingeitherontheleft orontherightonthe boundaryof 
the polygon. This encoding scheme produces very good results for polygons with very few and long runs. 
Another encoding scheme for simple polygons is described in the next section. 3.2 The forest split operation 
Representation A forest split operation, illustrated in Figure 1, is represented by: a forest in the 
graph of vertices and edges of a mesh; a sequence of simple polygons; and a sequence of vertex displacements. 
The mesh is re.ned by cutting the mesh through the forest, splitting the resulting boundaries apart, 
.lling each of the resulting tree boundary loops with one of the simple polygons, and .nally, displacing 
the new vertices. Applying a forest split operation involves: 1) cutting the mesh through the forest 
edges; 2) triangulating each tree loop according to the corresponding simple polygon; and 3) displacing 
the new vertices to their new positions. As will be explained in the next sec­tion, some of the information 
required to perform these steps, such as the correspondence between trees of the forest and simple poly­gons, 
and between tree boundary loop edges and polygon boundary loop edges of each corresponding tree-polygon 
pair, is not given Ert Trt Ert V2 Trt Vrt A Vrt B 7 4 3 6 7 4 8 3 6 2 9 2 1 1 50 5 CD V2 Ert Trt 
 Vrt EF Figure 3: When a mesh is cut through a tree of edges (red and green edges in A), a tree boundary 
loop (red and green edges in B) is created with each edge of the tree corresponding to two edges of the 
boundary loop. Some vertex indices are assigned before cutting (C) to new tree boundary loop vertices, 
othersareassignedsubsequentindices(D). The hole created by the cutting operation is .lled by triangulating 
the boundary loop using a simple polygon (E) resulting in a re.ned mesh (F) with the same topological 
type as the initial mesh. explicitly, but is based on an implicit convention for enumerating mesh elements. 
Enumeration of mesh elements Given a triangular mesh with Vvertices and Ttriangles, we assume that the 
vertices have consecutive vertex indices in the range 0;:::;V,1,and the tri­angles have consecutive triangle 
indices in the range 0;:::;T,1. The edges of the mesh, which are represented by pairs of vertex in­dices 
(i;j)with ij, are ordered lexicographically and assigned consecutive edge indices in the range 0;:::;E,1. 
The trees in the forest are ordered according to the minimum vertex index of each tree. The root vertex 
vrt of each tree in the forest is the leaf of the tree with the minimum index. Starting at the root, 
the boundary loop created by cutting along the tree can be traversed in cyclic fashion in one of the 
two directions. The root edge ert of the tree is the only edge of the tree which has the root vertex 
as an endpoint. Of the two triangles incident to the root edge of the tree, the root triangle trt of 
the tree is the one with the minimum triangle index. The root triangle of the tree determines the direction 
of traversal of the tree boundary loop. Of the two edges of the tree boundary loop AB CD CD Figure 4: 
Construction and triangulation of tree boundary loops. A,B: No tree vertices in the mesh boundary. C,D: 
A tree vertex isolated in the mesh boundary requires an extra tree loop edge. E,F: A tree edge on the 
mesh boundary edges does not require an extra tree loop edge, but some of the new vertex indices may 
only be used by new triangles. Note that the tree may have several contacts with the mesh boundary. corresponding 
to the root edge of the tree, the root edge ert of the tree boundary loop is the one incident to the 
root triangle. Figures 3-A,B illustrate these concepts. Each simple polygon has a boundary edge identi.ed 
as the root edge ert, with one of the two endpoints labeled as the root vertex vrt, and the other endpoint 
labeled as the second vertex v2. Figure 3-E illustrates these concepts. The cyclical direction of traversal 
of the polygon boundary loop is determined by visiting the root vertex .rst, followed by the second vertex. 
The correspondence between vertices and edges in a tree boundary loop and the polygon boundary loop is 
de.ned by their directions of cyclical traversal and by the matching of their root vertices. Cutting 
through forest edges Cutting through a forest of edges can be performed sequentially, cutting through 
one tree at time. Each cut is typically a local operation, affecting only the triangles incident to vertices 
and edges of the tree. However, as in the TS method, a single cut could involve all the triangles of 
the mesh. Cutting requires duplicating some tree vertices, assigning additional indices to the new vertices 
and .xing the speci.cation of the affected triangles. As illustrated in Figure 4-A,B, if no tree vertex 
is a bound­ary vertex of the mesh, then the tree is completely surrounded by triangles. Starting at the 
root triangle, all the corners of affected triangles can be visited in the order of traversal of the 
tree bound­ary loop, by jumping from triangle to neighboring triangle, while always keeping contact with 
the tree. This process produces a list of triangle corners, called the corner loop, whose values need 
to be updated with the new vertex indices. While traversing this list, we encounter runs of corners corresponding 
to the same vertex index before the cut. A new vertex index must be assigned to each one of these runs. 
To prevent gaps in the list of vertex indices we .rst need to reuse the vertex indices of the tree vertices, 
which other­wise would not be corner values of any triangles. The .rst visited run corresponding to one 
of these vertices is assigned that vertex index. The next visited run corresponding to the same vertex 
index is assigned the .rst vertex index not yet assigned above the number of vertices of the mesh before 
the cut. This procedure performs the topological cut. For example, in Figure 3-C, the vertex index values 
of the corners in the corner loop list are: [1233333244444421111]: The list can be decomposed into 6 
runs [11111], [2], [33333], [2], [444444],and [2]. As shown in Figure 3-D, the vertex indices assigned 
to these runs are 1, 2, 3, 8, 4, 9. A tree with medges containing no mesh boundary vertices creates a 
tree boundary loop of 2medges. This may not be the case when one or more tree vertices are also part 
of the mesh boundary. As illustrated in Figures 4-C,D,E,F, several special cases, must be considered. 
These special cases treat collapsed edges incident to or on the mesh boundary produced by the PFS generation 
algorithms as described in Section 5. Triangulating tree boundary loops By replacing each run of corners 
in the corner loop with the assigned vertex index, we construct a new list representing the tree boundary 
loop, If the tree boundary loop has mvertices, so does the corresponding polygon boundary loop. Each 
triangle t=fi;j;kgof the simple polygon de.nes a new triangle of the re.ned mesh by replacing the polygon 
boundary loop indices i;j;kwith their corresponding tree boundary loop indices. This is done using the 
list representing the tree boundary loop as a lookup table. The triangles of the simple polygon are visited 
in the order of a depth .rst traversal of its dual tree. The traversal starts with the triangle opposite 
to the root triangle and always traverses the left branch of a branching triangle .rst. Displacing vertices 
To satisfy the smooth transition property, vertex coordinates corresponding to new vertices are .rst 
assigned the same coordinates as the corresponding tree vertices before the cut. Topreventtheappearanceofholes,theseverticesaredisplaced 
after the boundary loops are triangulated. Optionally, all affected vertices may be repositioned.  
4 COMPRESSION AND ENCODING In this section we describe how a model represented in PFS format is encoded/compressed 
for ef.cient transmission and storage. Com­pression and encoding of the .rst (lowest) resolution level 
of detail was discussed in Section 3.1. This block of data is followed by the compressed/encoded forest 
split operations in the order they are applied. Theencodingofeachforestsplitoperationiscomposedof, in 
order: 1) the encoding of the forest of edges, 2) the encoding of the sequence of simple polygons, and 
3) the compression/encoding of the vertex displacements. AB Figure 5: Constant-length encoding of a 
simple polygon. A: Triangles labels according to their order of traversal. B: Triangles labels according 
to their two bit code. The encoding of the polygon is the sequence between the brackets. Encoding the 
forest A simple encoding of the forest requires one bit per edge, for example a value 1forthe edgeswhichbelongto 
the forest and 0for the rest. These bits are placed in the compressed data stream in the edge index order 
de.ned above. However, since any subset of edges of a forest form a forest, the edges with bit value 
1read up to a certain point may determine that certain edges with higher edge index should have bit value 
0, otherwise they would create loops in the forest de.ned so far by the bits with bit value 1. These 
edges can be skipped in the compressed data stream. When very few edges belong to the forest, run-length 
encoding this bit-stream may result in fewer bits. In the experiments that we have performed so far, 
where the number of triangles increase by 50-70% with each forest split operation, the skipping of predictable 
bits described above trims the length of the forest bit-stream by only about 1-8% and the simple encoding 
of the resulting bit-stream is usually shorter that the run-length encoded stream. Encoding a simple 
polygon The variable length encoding scheme described in section 3.1 uses one record per triangle tree 
run and one marching bit per regular node of the triangle tree to encode a simple polygon. This encoding 
is not very ef.cient when the simple polygon is composed of a few triangles or short runs. A constant 
length encoding scheme, requiring exactly 2 bits per triangle, has been proposed by Frank Bossen as an 
alternative for the MPEG4 standard [1]. This scheme produces better results for polygons with few triangles 
or short runs. In the current implementation we compute both encodings for all simple polygons and put 
the shortest one in the compressed data stream, preceded by one bit to indicate which encoding is used. 
That is, the simple polygons are either all constant length encoded or all run-length encoded. The constant-length 
encoding scheme, illustrated in Figure 5, is performed by traversing the triangle tree. The traversal 
starts by entering the .rst triangle through the root edge, with the root vertex assigned to left vertex 
vL, and the second vertex assigned to right vertex vR. Thethirdvertexofthetriangleisassignedtothe opposite 
vertex vO, the edge eL=(vL;vO)is the left edge, and the edge eR=(vR;vO)is the right edge. One bit is 
used to indicate whether each edge (left and right) is a boundary edge or an internal edge of the polygon. 
If only the left edge is internal, we set vR=vOand we continue with the other triangle incident to eL. 
If only the right edge is internal, we set vL=vOand we continue with the other triangle incident to eR. 
If both edges are internal, we push vOand vRonto a traversal stack, we set vR=vOand we continue with 
the other triangle incident to eL. If both edges are boundary and the traversal stack is not empty, we 
pop vRand vLfrom the traversal stack, and we continue with the other triangle incident to the edge (vL;vR). 
If both edges are boundary, and the traversal stack is empty, we have .nished visiting all the triangles 
of the simple polygon. For example, in Figure 5-A, the triangles are labeled with their order of traversal, 
and in Figure 5-B, with their corresponding two-bit code, as a number in the range 0;:::;3. Here, for 
each digit the .rst bit equals 0if the left edge is boundary, 1if the left edge is interior. The second 
bit represents the right side and uses the same convention. The encoding of this polygon is [12122331001210]. 
Note that there is a very simple transformation from the constant­length encoding to the variable-length 
encoding. The 3sand 0sin the constant-length encoded sequence mark the end of the triangle runs. In this 
example, the triangle-runs are de.ned by the sub sequences [121223], [3], [10], [0],and [1210], which 
are in the same order de.ned by the variable-length scheme. The length-of­run .eld value is the number 
of two-bit codes in the corresponding subsequence(6,1,2,1,and4inthiscase). Theends-in-leafbitvalue is 
determined by the last code in the sub sequence (3!0, 0!1), and the marching bits by the other codes 
in the sequence, skipping the 3sand 0s((1!0, 2!1). In this example the marching pattern is the following 
sequence of bits [010110010]. The transformation from the variable-length encoding to the constant-length 
encoding is also straightforward. Decoding a simple polygon As described in Section 3.2, applying the 
forest split operation requires the triangles of each simple polygon to be represented by triplets t=fi;j;kgof 
polygon boundary loop indices. These indices are subsequently replaced with the vertex indices assigned 
to the corresponding tree boundary loop indices. Since the order in which the polygon vertices are visited 
during tree traversal is usually not the sequential order of the boundary loop, the following recursive 
procedure is used to reconstruct the triangles of each simple polygon. As described above the traversal 
of a simple polygon starts by entering the .rst triangle crossing the root edge, with the left boundary 
loop index iL=0corresponding to the root vertex, and the right boundary loop index iR=1 corresponding 
to the second vertex. In general, when we enter a triangle, we know the values of iLand iR, and only 
the opposite boundary loop index iOmust be determined. If the two-bit code is 1(leaf node with next triangle 
on the left), we set iO=iR+1(addition and subtraction is modulo the length of the polygon boundary loop) 
and reconstruct the triangle fiL;iO;iRg,we set iR=iO, and continue. If the two-bit code is 2(leaf node 
with next triangle on the right), we set iO=iL,1, reconstruct the triangle fiL;iO;iRg,we set iL=iO, and 
continue. To determine the value of iOfor a branching triangle, if we know the distance dalong the boundary 
loop from the left vertex to the right vertex for the run attached to the left edge, we set iO= iL+d, 
reconstruct the triangle fiL;iO;iRg, push iRand iOonto the traversal stack, set iR=iO, and continue. 
As explained by Taubin and Rossignac [20], these lengths can be recursively computed for all the runs 
from the encoding of the polygon based on the formula d=l,1+dL+dR,there dis the distance of one run, 
lis the length of the run. If the run ends in a branching node, dLis the distance of the run attached 
to the left edge of the last triangle of the run, and dRis the distance of the run attached to the right 
edge of the last triangle of the run. If the run ends in a leaf node, dL=dR=1. If the two-bit code of 
the triangle has the value 0(leaf node of the triangle tree), we set iO=iL,1or iO=iR+1, and reconstruct 
the triangle fiL;iO;iRg. If the stack is empty we have .nished reconstructing the polygons. Otherwise 
we pop iLand iRvalues from the stack, and continue. Encoding the sequence of simple polygons We encode 
a sequence of constant-length encoded simple polygons by spec­ifying the total number of triangles in 
the complete sequence of simple polygons followed by a concatentation of the two-bit en­coding sequences. 
It is not necessary to include special markers in the compressed data stream to indicate the beginning 
and end of each polygon. The following procedure, which uses a single integer variable called depth, 
determines the limits of the polygons. The depth variable is initialized to 1before starting traversing 
a poly­gon. Each time a two-bit code with the value 3is found (branching triangle), depth is incremented 
by one. Each time a two-bit code with the value 0is found (leaf triangle), depth is decremented by one. 
The variable depth is always positive while inside the polygon. The end of the polygon is reached after 
the depth becomes equal to zero. If the polygons are run-length encoded, instead of the total number 
of runs, the (length-of-run,ends-in-leaf) records are put into the data stream in the order of traversal 
of the trees, preceded by the total number of runs in the sequence, and the number of bits per length-of-run. 
The same procedure described above (using a depth variable) can be used to determine the limits of the 
simple polygon. Encoding the vertex displacements Rather than encod­ing the new absolutepositions of 
the marked vertices, their positions after and before the forest split operation are .rst quantized to 
a cer­tain number of bits per coordinate with respect to a global bounding box enclosing all the levels 
of detail. The differences between these values are then Huffman encoded. The Huffman encoding table, 
the speci.cation of the bounding box, and the number of bits per coordinate error, are included at the 
beginning of the compressed stream. The same bounding box and number of bits per coordinate is used by 
the TS scheme, described in Section 3.1, to encode the coordinates of the lowest resolution level of 
detail, but because the errors are computed in a different way, different Huffman tables are used. Also, 
since the errors keep growing smaller as more forest split operations are applied, we use a different 
Huffman table for each forest split operation. Pre and post smoothing The differences between vertex 
positions before and after each forest split operation can be made smaller by representing these errors 
as the sum of a global predictor plus a correction. We use the smoothing method of Taubin [18] as a global 
predictor. The method requires only three global parame­ters which are included in the compressed data 
stream. After the connectivity re.nement step of a forest split operation is applied, the new vertices 
are positioned where their corresponding vertices in the previous level of detail were positioned and 
the mesh has many edges of zero length (all the new triangles have zero surface area). The smoothing 
method of Taubin, which tends to equalize the size of neighboring triangles, brings the endpoints of 
most such edges apart, most often reducing the distance to the desired ver­tex positions. The corrections, 
the differences between the vertex positions after the split operation and the result of smoothing the 
positions before the split operation, are then quantized according to the global quantization grid and 
entropy encoded. To make sure that the resulting vertex positions have values on the quantization grid, 
the smoothed coordinates must be quantized before computing the corrections. In our experiments, this 
procedure reduces the total length of the entropy encoded corrections by up to 20-25%. If the polygonal 
model approximates a smooth shape, a post­smoothing step can be applied to reduced the visual artifacts 
pro­duced by the quantization process. In many cases, even fewer bits per coordinate can be used in the 
quantization process without a signi.cant perceptual difference, also reducing the total length of the 
encoded vertex displacements. Figure 6 illustrates the effect of post-smoothing. Compression ratios The 
simple encoding of the forest with one bit per edge and the constant-length encoding of the simple polygons 
provide an upper bound to the number of bits required to encode a forest split operation. Since the number 
of edges in the meshis independentofthe numberoftrianglesadded,the minimum AB Figure 6: Effect of post-smoothing. 
A: Coordinates quantized to 6 bits per coordinate. B: Result of applying the smoothing algorithm of Taubin 
[18] with parameters n=16>=0:60J=,0:64. Compare with the original in Figure 8-D. number of bits per triangle 
are obtained when the most triangles are added with one forest split operation. The most triangles we 
can add with one forest split operation is approximately equal to the current number of triangles. As 
the number of edges in the forest increases, so does the number of triangles added. The forest with the 
largest number of edges for a triangle mesh with Vvertices has V,1edges and corresponds to a forest composed 
of a single spanning tree. Ignoring the case of mesheswith boundaries,this singletreecreatesatree boundaryloop 
with 2V,2edges, and a simple polygon with that many boundary edges is composed of 2V,4triangles. Since 
typical meshes have about twice as many triangles as vertices, the number of triangles in this simple 
polygon is approximately equal to T, the number of triangles in the mesh. If we also assume that the 
mesh has low Euler number (V,E+T:0),then E:1:5T.If IT=eT is the number of triangles added by the forest 
split operation (0 e1), the total number of bits required to encode the connectivity information of this 
forest split operation is approximately equal to (1:5/e+2)IT.This is 3:5bits per triangle for e=1, 4bits 
per triangle for e=0:75,and 5bits per triangle for e=0:5. In the PM scheme each vertex split operation 
requires an average of 5+log2(n)bits to specify the re.nement in the connectivity of the mesh (log2(n)bits 
to specify the index of the common vertex, and an average of 5bits to specify the two incident edges), 
to a total storage of about n(5+log2(n))bits to double the number of vertices of the mesh. For example, 
let us consider a relatively small mesh with 1,000 triangles, and a forest split operation which increases 
the number of triangles by 75%. The PFS scheme needs about 4 bits per new triangles to represent the 
connectivity changes while the PM scheme needs about 5+10=15bits per triangle or almost four times the 
number of bits required by the PFS scheme. For larger meshes the differences become more pronounced. 
If no pre or post smoothing is used, the number of bits used to encode each vertex displacement is about 
the same as in PM. As discussed above, pre and post smoothing can be used to decrease the total size 
of the encoded vertex displacements by up to 20-25%. Because of the small granularity, it is not practical 
to apply pre smoothing before each vertex split operation in PM. Complexity of encoding and decoding 
Since the two triangles incident to an edge de.ned by a pair of vertex indices can beaccessedinconstanttime, 
theforestsplitoperationislinearin the number of triangles added. We represent the edges as quadruplets, 
with two vertex indices and two face indices, and keep them in a hash table or array of linked lists. 
For typical surfaces this provides constant or almost constant access time.  5 CONVERSION TO PFS FORMAT 
In this section we discuss simpli.cation algorithms to convert a single-resolution triangular mesh to 
PFS format. We show that most edge-collapse based simpli.cation algorithms can be modi.ed to represent 
a simpli.cation LOD hierarchy as a sequence of forest collapse operations. Clustered multi-resolution 
models Several existing meth­ods to generate multi-resolution polygonal models are based on ver­tex clustering 
algorithms [17]. In the multi-resolution polygonal model produced by these algorithms, the vertices of 
each level of detail are partitioned into disjoint subsets of vertices called clusters. All the vertices 
in each cluster are collapsed into a single vertex of the next (lower resolution) level of detail. Other 
examples of clus­tering algorithms for automatic surface simpli.cation are based on triangle collapsing 
[10], and edge collapsing [12, 15, 6, 11]. For a clustering algorithm producing an LOD hierarchy with 
L levels, the vertices of consecutive levels are related by clustering functions cl:f1;:::;Vlg!f1;:::;Vl+1gwhich 
map vertex in­dices of level lonto vertex indices of level l+1. Here, Vldenotes the number of vertices 
of the l-th. level. Triangles at each level of detail are completely speci.ed by the triangles of the 
.rst (highest reso­lution) level of detail and the clustering functions. If tl=fi;j;kg is a triangle 
of the l-th. level, then tl+1 =fcl(i);cl(j);cl(k)gis a triangle of the next level if it is composed of 
three different indices; otherwise we say that tlwas collapsed at the next level. Clustered multi-resolution 
polygonal models have the smooth transition property. Vertex iof the l-th. level is linearly interpolated 
with vertex cl(i)of the level as a function of time. A closely re­lated, but more compact data structure, 
optimized for fast switching between levels of detail was recently introduced by Gu´eziec et al. [8] 
to represent clustered multi-resolution polygonal models. The forest collapse operation The set of triangles 
of each level of detail that collapses in the next level can be partitioned into connected components, 
where two triangles are considered con­nected if they share an edge. A clustering function de.nes a forest 
collapse operation if the following two conditions are satis.ed: 1) each connected component is a simple 
polygon (triangulated with no internal vertices); and 2) no vertex is shared by two or more connected 
components. If these two conditions are satis.ed, the boundary edges of each connected component form 
a disconnected tree in the next level of detail. If the edges formed a graph with loops instead of a 
tree then the connected component would not be simply connected. Also, connected components with no common 
vertices always produce disconnected trees. To test whether or not a clustering function de.nes a forest 
collapseoperation,itissuf.cientto: 1)checkthattheEulernumbers of the connected components are all equal 
to one (V,E,T=1), where the edges which separate the triangles of each component from other triangles 
are consdiered boundary edges; and 2) count the number of times that each vertex of the mesh belongs 
to the boundary of a connected component and then check that all these numbers are equal to 0or 1. Permutations 
of vertex and triangle indices If all the clustering functions of a clustered multi-resolution model 
de.ne forest collapse operations, the transition from the lowest to the highest resolution level of detail 
can be represented as a sequence of forest split operations. The order of vertices and triangles in the 
highest resolution level of detail and those induced by the clustering functions in the lower resolution 
levels will, in general, not be the same as those produced by decompressing the low resolution mesh and 
applying the forest split operations. Special care must be taken to determine the permutations which 
put the vertices and faces of ACB Figure 7: Collapsibility tests. A: Acceptable edge of multiplicity 
1.B: Rejected edge of multiplicity 1. C: Rejected edge of multiplicity >1. different levels in the order 
that the decoder expects as described in Section 3.2. Edge-collapse simpli.cation algorithms In several 
sim­pli.cation algorithms based on edge collapsing, the edges of the mesh are ordered in a priority queue 
according to certain crite­ria, usually based on the geometry of a neighborhood, and on the changes that 
the edge would produce if collapsed. The .rst edge is removed from the queue and certain connectivity 
and geometry tests are applied to it. If it passes the tests it is classi.ed as col­lapsible, and the 
edge is collapsed. Otherwise, it is discarded. The collapsing of an edge may require changing the order 
of some of the neighboring edges in the queue. The process continues remov­ing edges from the queue until 
the queue is exhausted, or until a termination condition, such as maximum number of collapsed tri­angles, 
is reached. To prevent visual artifacts the collapsibility test should take into account the values of 
properties bound to vertices, triangles, or corners, and the discontinuity curves that such property 
.elds de.ne, as in the PM simpli.cation algorithm [11]. The initial mesh, before the simpli.cation algorithm 
starts col­lapsing edges, de.nes one level of detail, and the resulting mesh, after the collapsing stops, 
de.nes the next level of detail. An extended collapsibility test must be performed to ensure that the 
clustering function de.ned by a proposed edge collapse also de­.nes a valid forest collapse operation. 
This extended test requires two additional, simple tests to be performed on each candidate edge collapse. 
Figure 7 illustrates these tests. Both tests are straightfor­ward. The .rst test determines whether or 
not the connected compo­nents of the set of collapsed triangles are simple polygons after the edge is 
collapsed. The test can be implemented using an auxiliary data structure to maintain a forest in the 
dual graph of the mesh, with each tree of this forest corresponding to one connected com­ponent (simple 
polygon) constructed so far. When an edge collapse is accepted by the simpli.cation algorithm, it not 
only identi.es the two endpoints of the edge, but also the two remaining edges of each of the two (one 
for the case involving mesh boundary) col­lapsed triangles. This de.nes a multiplicity for each edge 
of the simpli.ed mesh. Initially all edges in the current level of detail are assigned a multiplicity 
of one. When two edges are identi.ed as the result of an edge collapse, they are removed from the priority 
queue, and a new edge with the sum of the two multiplicities is rein­serted in the queue. Of the four 
(two for the case involving mesh boundary) boundary edges of the quadrilateral de.ned by the two (one) 
collapsed triangles, between 0and 4(2) edges are boundary edges of neighboring connected components. 
The test is passed if all of these neighboring connected components are different, in which case, adding 
the collapsed edge and the edges shared with the connected components to the dual forest would not destroy 
the forest structure. Otherwise, one or more loops would be created. Two triangles incident to an edge 
with multiplicity higher than one correspond to two mesh triangles which each share an edge with a boundary 
edge of the same connected component but are not neigh- AB CD Figure 8: Highest resolution models used 
to converted to PFS format in Section 6. A: bunny. B: horse. C: crocodile. D: skull. All the models are 
.at-shaded. Properties are ignored. bors in the original mesh. Either one or two of the edges of each 
one of these triangles in the original mesh are boundary edges of the neighboring connected component, 
otherwise the previous test would have been violated. The test is passed if only one edge is shared with 
the neighboring connected component. The second test is applied only if the edge passes the .rst test. 
This test prevents vertices from being shared by two or more con­nected components and can be implemented 
using an auxiliary data structure to keep track of the vertices of the mesh which are bound­ary vertices 
of simple polygons constructed so far. An array of boolean variables, initialized to false, is suf.cient 
for this purpose. We call a vertex of the quadrilateral de.ned by the collapse of an edge with multiplicity 
1isolated, if neither one of the two bound­ary edges of the quadrilateral shared by the vertex are shared 
by neighboring connected components. The same name is given to the two vertices of the two triangles 
incident to an edge with multiplic­ity higher than 1, which are not endpoints of the collapsed edge. 
The second test is passed if neither one of the isolated vertices are boundary vertices of any connected 
component. If an edge passes both tests then the edge is collapsed, the priority queue and the data structures 
needed to perform the two new tests are updated, and the process repeats until the queue is exhausted. 
As explained in Section 4, the most triangles a forest split opera­tion can add to a mesh with Ttriangles 
is approximately T.There­fore, when the simpli.cation algorithm (with the two additional tests) stops 
because the queue has been exhausted, the resulting mesh cannot have less than half the triangles of 
the original mesh. In our experiments, a forest split operation adds between 45-90% Tto amesh of Ttriangles. 
 6 IMPLEMENTATION AND RESULTS Our prototype implementation is composed of three programs: 1) a simpli.cation 
program, 2) an encoding program, and 3) a decoding program. The current implementation handles manifold 
triangular meshes without properties. Level 0 Level 1 Level 2 Level 3 Figure 9: LOD hierarchy generated 
automatically from the horse of .gure 8-B (Level 4). Red triangles collapse in the next level For the 
simpli.cation progam we implemented the variable tol­erance simpli.cation method of Gu´eziec [6] with 
the modi.cations described in Section 5. The combined procedure takes a single­resolution triangular 
mesh as input, and produces as output a .le in clustered multi-resolution format. This is an ASCII .le 
containing the connectivity of the original mesh, the vertices coordinates of all the levels produced, 
and the clustering functions represented as arrays of vertex indices. The encoder program takes this 
.le format as input, checks whether the data is PFS compressible or not, and if so, produces a compressed 
data stream as described in Section 4. The decoder program takes this compressed data stream as input 
and produces a .le in clustered multi-resolution format. We report results for four models, each displayed 
in full reso­lution in .gures 8-A,B,C,D. The models all have roughly the same number of vertices and 
triangles (except the bunny) and represent different topological types. The bunny has the topology of 
a sphere with four holes (in the bottom). The horse is composed of three connected components each with 
the topology of a sphere with­out boundary (body and eyes). The crocodile is composed of 65 connected 
components each with the topology of a sphere (body, mandibles, eyes and teeth) and with large variations 
in the number of vertices and triangles in each component. The skull is connected, has no boundary and 
a very high genus. The simpli.cation program produced four levels of detail for the bunny, and .ve levels 
of detail for each one of the other models. Figure 9 shows the LOD hierarchy generated from the horse. 
Figure 10 shows the LOD hierarchy generated from the skull. The number of vertices and triangles for 
each level, as well as the number of bytes needed to encode the base meshes and forest split operations 
are shown in the Figure 11. Overall, the single resolution scheme used to encode and compress the lowest 
resolution level of detail has better compression rates (bits per triangle) than the forest split operations. 
However, the forest split operation compression rates approach the single resolution compression rates 
as the size of the forests grow and the vertex displacements decrease in magnitude. Level 0 Level 1 Level 
2 Level 3 Figure 10: LOD hierarchy generated automatically from the skull of .gure 8-D (Level 4). Red 
triangles collapse in the next level 7 EXTENSIONS To simplify the description of the representation, 
algorithms and data structures, and to re.ect the limitations of the current imple­mentation, we have 
limited the scope of this paper to manifold triangular meshes without properties. Some of these extensions 
are straightforward, others will require further work. Polygonal faces JustastheTSmethodwasextendedto 
handle the simply connected polygonal faces found in VRML models [19], the PFS method can be extended 
to handle manifold polygonal models with simply connected polygonal faces. The representation and encoding 
of the forest of edges do not require any modi.cation. Tree boundary loops would be polygonized instead 
of triangulated. Tree boundary loops can be polygonized by sending one extra bit per marching edge to 
indicate which of the internal edges of the triangulated simple polygon are actually face edges. Properties 
The current implementation supports topology and vector coordinates. The TS method has been extended 
to handle properties (colors, normals and texture coordinates) and property bindings for faces and corners 
[19]. The same techniques can be used to extend the PFS method to handle additional properties and property 
bindings. Normals can be quantized in a special way, taking into account the unit length nature of normals 
vectors [19]. When properties are bound to corners, they de.ne discontinuity curves [11, 19]. Encoding 
these discontinuity curves by simply requiring an extra bit per edge may be too expensive. A scheme to 
progressively update the discontinuity curves may be necessary. We intend to work on this issue in the 
near future. Non-manifolds and topology changing re.nement The topological type of the lowest resolution 
level of detail stays constant during the re.nement process for bothe the PFS and PM methods. The Progressive 
Simplicial Complexes (PSC) scheme was introduced by Popovic and Hoppe [14], to allow changes in topological 
type (genus) to occur during the re.nement process. CROCODILE (1.89) Level 0 1 2 3 4 T 3,506 5,128 7,680 
12,408 21,628 IT 3,506 1,622 2,552 4,728 9,220 IT(%) 1 46 50 62 74 C/IT 3.42 5.29 5.04 4.45 4.02 (C+G)/IT 
13.29 26.71 24.20 20.82 17.27 HORSE (2.76) T 2,894 4,306 6,774 11,754 22,258 IT 2,894 1,412 2,468 4,980 
10,504 IT(%) 1 49 57 74 90 C/IT 3.44 5.12 4.64 4.05 3.68 (C+G)/IT 13.74 25.04 25.04 19.16 17.91 SKULL 
(1.48) T 4,464 6,612 9,966 14,896 22,104 IT 4,464 2,148 3,354 4,930 7,208 IT(%) 1 48 51 50 48 C/IT 4.51 
5.15 4.97 5.04 5.10 (C+G)/IT 14.86 29.85 28.15 26.71 24.58 BUNNY (1.73) T 2,008 3,169 5,072 7,698 IT 
2,008 1,161 1,903 2,626 IT(%) 1 58 60 44 C/IT 3.27 4.69 4.56 4.95 (C+G)/IT 14.37 27.97 25.68 26.38 Figure 
11: Numerical results. T: number of triangles. IT: incre­ment with respect to previous level. C/IT: bits 
per triangle for connectivity. (C+G)/IT: total number of bits per triangle. The number in parentheses 
is the relative cost of progressive vs. single resolution transmission of connectivity (ratio of connectivity 
bits per triangle in Progressive Forest Split / Topological Surgery). The PSC representation retains 
most of the advantages of the PM representation, including smooth transitions between consecutive levels 
of detail and progressive transmission. However, the in­creased generality of the method requires a higher 
number of bits to specify each re.nement operation. We believe that a recursive subdivision approach 
related to the one taken in this paper, but with a more complex re.nement operation, can solve the problem 
of dealing with non-manifolds and changes of topological type. We intend to work on this issue in the 
near future as well.  8 CONCLUSIONS In thispaperweintroducedthePFSrepresentationasanewadaptive re.nement 
scheme for storing and transmitting triangular meshes in a progressive and highly compressed form. We 
started the paper by putting this new method and the PM method of Hoppe in the general context of recursive 
subdivision/re.nement schemes. We showed that PFS allows the user to tradeoff granularity in re.nement 
levels for complexity in the data stream. For a .ne granularity (PM) O(nlog2(n))bits are required to 
double the connectivity size of a mesh with O(n)levels of detail. For a coarse granularity (PFS) O(n)bits 
are required to perform the same doubling for O(1)levels of detail. We have described algorithms for 
ef.ciently encoding to and decoding from a data stream containing the PFS format. We also showed how 
to reduce the number of bits required to encode the geometry the mesh by using pre and post global smoothing 
steps. We showed how, with the addition of two simple tests, to modify any simpli.cation algorithm based 
on edge collapses to convert single resolution triangular meshes to PFS format. We presented results 
obtained by applying these modi.cations to the Variable TolerancemethodofGu´eziec. Finally,wediscussedhowtoextend 
the scheme to handle polygonal faces and various property bindings.  Acknowledgments Our thanks to Rhythm 
&#38; Hues Studios, Inc. and Greg Turk for the Horse model. Thanks to Marc Levoy for the Bunny model. 
 REFERENCES [1] MPEG4/SNHC veri.cation model 5.0, July 1997. ISO/IEC JTC1/-SC29/WG11 Document N1820, 
Caspar Horne (ed.). [2] E. Catmull and J. Clark. Recursively generated B-spline surfaces on arbitrary 
topological meshes. Computer Aided Design, 10:350 355, 1978. [3] M. Deering. Geometric compression. In 
Siggraph 95 Conference Proceedings, pages 13 20, August 1995. [4] D. Doo and M. Sabin. Behaviour of recursive 
division surfaces near extraordinary points. Computer Aided Design, 10:356 360, 1978. [5] M. Eck, T. 
DeRose, T. Duchamp, H. Hoppe, M. Lounsbery, and W. Stuetzle. Multiresolution analysis of arbitrary meshes. 
In Sig­graph 95 Conference Proceedings, pages 173 182, August 1995. [6] A. Gu´eziec. Surface simpli.cation 
with variable tolerance. In Second Annual International Symposium on Medical Robotics and Computer Assisted 
Surgery, pages 132 139, Baltimore, MD, November 1995. [7] A. Gu´eziec, G. Taubin, F. Lazarus, and W. 
Horn. Cutting and Stitch­ing: Ef.cient Conversion of a Non-Manifold Polygonal Surface to a Manifold. 
Technical Report RC-20935, IBM Research, July 1997. [8] A. Gu´eziec, G. Taubin, F. Lazarus, and W. Horn. 
Simplicial maps for progressive transmission of polygonal surfaces. In VRML 98.ACM, February 1998. [9] 
M.HallandJ.Warren.Adaptivepolygonalizationofimplicitlyde.ned surfaces. IEEE Computer Graphics and Applications, 
pages 33 42, November 1990. [10] B. Hamann. A data reduction scheme for triangulated surfaces. Com­puter 
Aided Geometric Design, 11(2):197 214, 1994. [11] H. Hoppe. Progressive meshes. In Siggraph 96 Conference 
Proceed­ings, pages 99 108, August 1996. [12] H. Hoppe, T. DeRose, T. Duchamp, J. McDonald, and W. Stuetzle. 
Mesh optimization. In Siggraph 93 Conference Proceedings, pages 19 25, July 1993. [13] C. Loop. Smooth 
subdivision surfaces based on triangles. Master s thesis, Dept. of Mathematics, University of Utah, August 
1987. [14] J. Popovi´c and H. Hoppe. Progressive simplicial complexes. In Sig­graph 97 Conference Proceedings, 
pages 217 224, August 1997. [15] R. Ronfard and J. Rossignac. Simplifying a triangular mesh with multiple 
planar constraints. Technical report, IBM Research, 1994. [16] R. Ronfard and J. Rossignac. Triangulating 
multiply-connected poly­gons: A simple, yet ef.cient algorithm. Computer Graphics Forum, 13(3):C281 C292, 
1994. Proc. Eurographics 94, Oslo, Norway. [17] J. Rossignac and P. Borrel. Geometric Modeling in Computer 
Graph­ics, chapter Multi-resolution 3D approximations for rendering com­plex scenes, pages 455 465. Springer 
Verlag, 1993. [18] G. Taubin. A signal processing approach to fair surface design. In Siggraph 95 Conference 
Proceedings, pages 351 358, August 1995. [19] G. Taubin, W.P. Horn, F. Lazarus, and J. Rossignac. Geometric 
Coding and VRML. Proceedings of the IEEE, July 1998. (to appear) Also IBM Research TR RC-20925, July 
1997. [20] G. Taubin and J. Rossignac. Geometry Compression through Topo­logical Surgery. ACM Transactions 
on Graphics, April 1998. (to appear). [21] D. Zorin, P. Schr¨oder, and W. Sweldens. Interactive multiresolution 
mesh editing. In Siggraph 97 Conference Proceedings, pages 259 268, August 1997. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280836</article_id>
		<sort_key>133</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Real time compression of triangle mesh connectivity]]></title>
		<page_from>133</page_from>
		<page_to>140</page_to>
		<doi_number>10.1145/280814.280836</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280836</url>
		<keywords>
			<kw><![CDATA[3D graphics hardware]]></kw>
			<kw><![CDATA[algorithms]]></kw>
			<kw><![CDATA[graphics]]></kw>
			<kw><![CDATA[mesh compression]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>E.4</cat_node>
				<descriptor>Data compaction and compression</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10002952.10002971.10003451.10002975</concept_id>
				<concept_desc>CCS->Information systems->Data management systems->Data structures->Data layout->Data compression</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Languages</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31078320</person_id>
				<author_profile_id><![CDATA[81100408275]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Stefan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gumhold]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[WSI/GRIS Univ. of T&#252;bingen]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39064022</person_id>
				<author_profile_id><![CDATA[81100575262]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wolfgang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stra&#223;er]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[WSI/GRIS Univ. of T&#252;bingen]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[E. M. Arkin, M. Held, J. S. B. Mitchell, and S. S. Skiena. Hamiltonian triangulations for fast rendering. Lecture Notes in Computer Science, 855:36-57, 1994.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>234976</ref_obj_id>
				<ref_obj_pid>234972</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Rueven Bar-Yehuda and Craig Gotsman. Time/space tradeoffs for polygon mesh rendering. ACM Transactions on Graphics, 15(2):141-152, April 1996.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218391</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[M. Deering. Geometry compression. In Computer Graphics (SIGGRAPH '95 Proceedings), pages 13-20, 1995.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>245626</ref_obj_id>
				<ref_obj_pid>244979</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Francine Evans, Steven S. Skiena, and Amitabh Varshney. Optimizing triangle strips for fast rendering. In IEEE Visualization '96. IEEE, October 1996. ISBN 0-89791-864-9.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Silicon Graphics Inc. GL programming guide. 1991.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>264000</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Scott Meyers. Effective C++ : 50 specific ways to improve your programs and designs. - 2. ed. Addison-Wesley, Reading, MA, USA, 1997.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>248518</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Jackie Neider, Tom Davis, and Mason Woo. OpenGL Programming Guide I The Official Guide to Learning OpenGL, Version 1.1. Addison-Wesley, Reading, MA, USA, 1997.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Gabriel Taubin and Jarek Rossignac. Geometric compression through topological surgery. Technical report, Yorktown Heights, NY 10598, January 1996. IBM Research Report RC 20340.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Copyright &#38;#169;1998 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers or to redistribute to lists, requires specific permission and/or a fee. Real Time Compression 
of Triangle Mesh Connectivity Stefan Gumhold, Wolfgang Straßer. WSI/GRIS University of T¨ubingen Abstract 
In this paper we introduce a new compressed representation for the connectivity of a triangle mesh. We 
present local compression and decompression algorithms which are fast enough for real time ap­plications. 
The achieved space compression rates keep pace with the best rates reported for any known global compression 
algorithm. These nice properties have great bene.ts for several important ap­plications. Naturally, the 
technique can be used to compress trian­gle meshes without signi.cant delay before they are stored on 
ex­ternal devices or transmitted over a network. The presented decom­pression algorithm is very simple 
allowing a possible hardware re­alization of the decompression algorithm which could signi.cantly increase 
the rendering speed of pipelined graphics hardware. CR Categories: I.3.1 [Computer Graphics]: Hardware 
Archi­tecture; I.3.3 [Computer Graphics]: Picture/Image Generation Display algorithms Keywords: Mesh 
Compression, Algorithms, 3D Graphics Hard­ware, Graphics 1 Introduction and Related Work The ability 
to handle very large geometric data sets becomes more and more important. Powerful compression techniques 
are manda­tory to solve this task. The compression approach presented in this paper has several advantages. 
The compression and decompression algorithms are fast and simple. The achieved space compression ra­tios 
are among the best known results and the algorithms act locally such that only a fraction of the vertices 
must be accessible. None of the available techniques combines these properties. All lack a very fast 
compression algorithm and therefore the compressed rep­resentation must be pre-computed before rendering. 
No speed up through compression could be achieved so far in the visualization of meshes with changing 
connectivity. The related work concentrates either on fast rendering or on maximum compression and therefore 
the following discussion is divided into two sections. 1.1 Compression for Fast Rendering In this section 
we discuss representations of triangle meshes that are used for transmission to graphics hardware. 3D-hardware 
sup­port is primarily based on the rendering of triangles. Each triangle .Email: fsgumhold/strasserg@gris.uni-tuebingen.de 
is speci.ed by its three vertices, where each vertex contains three coordinates, possibly the surface 
normal, material attributes and/or texture coordinates. The coordinates and normals are speci.ed with 
.oating point values, such that a vertex may contain data of up to 36 bytes1. Thus the transmission of 
a vertex is expensive and the simple approach of specifying each triangle by the data of its three vertices 
is wasteful as for an average triangle mesh each vertex must be transmitted six times. The introduction 
of triangle strips helped to save unnecessary transmission of vertices. Two successive triangles in a 
triangle strip join an edge. Therefore, from the second triangle on, the vertices of the previous triangle 
can be combined with only one new vertex to form the next triangle. As with each triangle at least one 
vertex is transmitted and as an average triangle mesh has twice as many triangles as vertices, the maximal 
gain is that each vertex has to be transmitted only about two times. Two kinds of triangle strips are 
commonly used the sequential and the generalized triangle strips. In generalized triangle strips an 
additional bit is sent with each vertex to specify to which of the free edges of the previous triangle 
the new vertex is attached. Sequential strips even drop this bit and impose that the triangles are attached 
alternating. OpenGL [7] which evolved to the commonly used standard for graphics li­braries allowed [5] 
generalized triangle strips in earlier versions, but the current version is restricted to sequential 
strips. Therefore, the demands on the stripping algorithms increased. None of the exist­ing algorithms 
reaches the optimum that each vertex is transmitted only twice. The algorithm of Evans et al. [4] produces 
strips such that each vertex is transmitted about 2:5times and this is currently the best algorithm. 
Arkin et al. [1] examined the problem of testing whether a tri­angulation can be covered with one triangle 
strip. For generalized triangle strips this problem is NP-complete, but for sequential strips there exists 
a simple linear time algorithm. But no results or algo­rithms were given to cover a mesh with several 
strips. To break the limit of sending each vertex at least twice Deer­ing [3] suggests the use of an 
on-board vertex buffer of sixteen vertices. With this approach, which he calls generalized mesh theoretically 
only six percent of the vertices have to be transmit­ted twice. But up to now no corresponding algorithms 
have been presented. Bar-Yehuda et al. [2] examined different sized vertex buffers. They prove that a 
triangle mesh with nvertices can be optimally rendered, i.e. each vertex is transmitted only once, if 
p a buffer for 12:72nvertices is provided. They also show that this upper bound is tight and no algorithm 
can work with less than p 1:649nbuffered vertices. In their estimation they neglect the time consumed 
by the referencing of buffered vertices, which makes it impossible to determine the suitability of the 
approach for connec­tivity compression. Again the algorithms to compute the rendering sequences are not 
fast enough for on-line generation. Our connectivity compression technique also utilizes a vertex buffer 
where each vertex has to be transmitted only once. As our technique is hard to analyze theoretically, 
we can only give exper­ p imental results of the size. These are all less than 12:72n.By de.ning a .xed 
traverse order our approach minimizes the number 1where we assumed four bytes per .oating point value 
and one byte per color component of indices needed to reference vertices in the buffer, which results 
in an additional speed up for rendering. If these indices are Huffman­encoded, in the average only two 
bits per triangle are needed for references. 1.2 Maximum Mesh Compression The work on fast rendering 
explained in section 1.1 can also be used for the compression of triangle mesh connectivity. Instead 
of retransmitting a vertex, a reference is inserted into a compressed representation. If a vertex from 
the buffer is referenced its index within the buffer enters the compressed representation. In the tri­angle 
strips of Evans et al. [4] each vertex appears about 2:5times. The vertices can be rearranged into the 
order they appear the .rst time and only the indices of 1:5nvertices need to be inserted in the compressed 
representation. One additional bit per triangle is needed to specify whether the next vertex is used 
the .rst time or the index of an already used vertex follows. This sums up to about 1+0:75dlog2 nebits 
per triangle. The disadvantage of this approach is that the storage needs grow with the size of the triangle 
mesh. The measurements of Deering in [3] show that the generalized mesh approach theoretically consumes 
between eight and eleven bits per triangle if an optimal stripper is available. The work of Bar-Yehuda 
et al. [2] cannot be compared to our work as no appropriate measurements are available. Taubin et al. 
[8] propose a very advanced global compression technique for the connectivity of triangle meshes. The 
method is based on a similar optimization problem as for sequential trian­gle strips and the authors 
guess that it is NP-complete. Their ap­proximation allows compression to only two bits per triangle and 
there exist triangle meshes which consume only one bit per trian­gle. The decompression splits into several 
processing steps over the complete mesh, which makes the approach unsuitable to speed up hardware driven 
rendering. Their compression and decompression algorithms are more complex than ours and although the 
asymp­totic running time should be the same we strongly believe that a comparable optimized implementation 
of our algorithms is several times faster than the algorithms proposed by Taubin. Our compres­sion technique 
yields nearly equivalent compression of the mesh connectivity. So far we described only lossless compression 
techniques. For applications which allow lossy compression also the vertex data can be compressed and 
the connectivity can be simpli.ed. Deering [3] uses the proximity of the vertices independently of the 
connectiv­ity. Taubin et al. [8] propose to predict the coordinates vnof the vertex, which is to be compressed 
next, from the preceding Kver­tices vn,1;:::;vn,Kand only encode the difference E(vn)to the predicted 
position E(vn)vn,P(A;vn,1;:::;vn,K); where Aspeci.es the parameters for the predictor function P, P K 
which was implemented as the linear .lter and the pa­ i.1Aivirameters where chosen to minimize the square 
sum of all E(vn). In both approaches of vertex data compression the positions are additionally entropy 
encoded after the delta compression. Similar techniques are used to compress the normals and the material 
data. Our approach of connectivity compression can be used with both of these geometry compression techniques. 
The result is a signi.cant speed up in both cases and the improvement of the compression ratios of Deering 
s approach2. 2The compression ratios for the models (triceratops, galleon, viper, 57chevy, insect) measured 
in Deering s paper would increase from (5.8X, 8.2X, 9.2X, 9.2X, 7.2X) to (7.4X, 11.2X, 11.6X, 12.0X, 
11.4X) Figure 1: Non manifold vertices must be duplicated in order to make their neighborhood 2-manifold 
with border.  2 Compression and Decompression Let us introduce the ideas of compression and decompression 
by comparison with generalized triangle strips. This approach utilizes a vertex buffer of only three 
vertices but in turn has to transmit each vertex twice. Thus the .rst idea is to simply increase the 
size of the vertex buffer, such that all the undecided vertices can be stored. The undecided vertices 
are those, which have not .nally been in­corporated into the so far decompressed triangle mesh, i.e. 
for these vertices exist adjacent triangles which will be transmitted later. The increased vertex buffer 
is not very useful, if still indices of the vertices must be transmitted to localize them within the 
buffer. The transmission of most indices can be avoided by .xing the or­der in which the edges formed 
by the vertices in the vertex buffer are traversed. Therefore, the traverse order need not be encoded. 
The rules to .x the traverse order must be chosen carefully as they constitute most degrees of freedom 
for optimization. In turn the compression algorithm becomes nearly as fast as the decompressed algorithm. 
In the case of generalized triangle strips the traverse or­der is not .xed. Each of the additional bits 
encodes which of the two free edges of the previous triangle the next triangle is attached to. To allow 
the encoding of an arbitrarily connected and oriented triangle mesh in one run, several basic building 
operations must be encodable. In the case of triangle strips there is only one operation the attachment 
of a new triangle to an existing edge. This has the advantage that the type of operation need not be 
encoded. In our approach all of the building operations also introduce one new triangle, but the new 
triangle can additionally be formed exclusively by buffered vertices. Section 2.1 introduces the rules 
which .x the order in which the triangle mesh is traversed. Then the different building operations are 
discussed in section 2.2. The building operations are Huffman encoded in a variable length bit stream 
to achieve the best compres­sion of the connectivity. Section 2.3 explains how to combine the bit stream 
with the vertex data and additional data. In what follows we assume that the triangle meshes consist 
of several connected components, all orientable and locally 2­manifold with borders. Thus the neighborhood 
of each vertex can be continuously mapped to a plane or to a half-plane if the vertex belongs to the 
border of a component. Our approach is extended to non orientable 2-manifold triangle meshes in section 
5.1. Non manifold models must be cut into manifold models by duplication of non manifold vertices as 
indicated in .gure 1. Figure 2: The shown sample triangle mesh is traversed in a breadth­.rst order. 
2.1 Traverse Order Figure 2 illustrates a breadth-.rst traverse order of a sample trian­gle mesh. The 
buffered vertices are connected with bold lines. The collection of these lines is called the cut-border. 
The cut-border divides the triangle mesh into two parts, the so far decompressed part the inner part 
(shaded) and the rest the outer part.The edges on the cut-border are enumerated in the order they are 
pro­cessed. Each time a cut-border edge is processed, a new triangle is added to this edge with a basic 
building operation imposed by the connectivity of the triangle mesh. The new triangle introduces new 
cut-border edges and the processed edge is removed from the cut-border. These are the basic rules which 
de.ne the traverse order. The degrees of freedom lay in the choice of the initial triangle, which constitutes 
the initial cut-border, and in the way the new edges are enumerated. It will turn out in section 4.2, 
that the choice of the initial triangle doesn t in.uence the compression signi.cantly. In the breadth-.rst 
strategy the new cut-border edges obtain increasing numbers, such that the cut-border is grown in a cyclic 
way. A depth­.rst order is achieved by enumerating the new cut-border edges in a decreasing order, such 
that the last introduced cut-border edges are processed .rst. The complete information in the vertex 
buffer can be used to determine the position of the new cut-border edges. As we concentrate on real time 
compression, only the strategies which consume no additional computation time are analyzed in section 
4.2. 2.2 Building Operations Let us take a closer look at .gure 2 as nearly all basic building operations 
arise in this small example. The triangle mesh is always built from an initial triangle consisting of 
the .rst three vertices. The initial building operation is not encoded but will be denoted with the symbol 
L . Between .gure 2a and 2b to each of the three initial cut-border edges a triangle is attached in the 
same way as to a triangle strip. Each operation introduces a new vertex and two new edges to the cut-border. 
Let us call this building operation new vertex and abbreviate it with the symbol * . The new cut-border 
edges are enumerated in the order they are added to the cut-border in order to bring about a breadth-.rst 
traverse order. Between .gure 2b and 2c the triangle of the outer part, which is split cut border " n" 
 cut border " p  cut border union i" part 2 y z x a) b) Figure 3: The split cut-border -/ cut-border 
union -operation needs one/two indices to specify the third vertex with which the current cut-border 
edge forms the next triangle. The vertices of the current edge are shaded dark and the newly attached 
triangle light. adjacent to edge 3, is added to the so far (de)compressed triangle mesh. This time no 
new vertex is inserted, but edge 3forms a triangle with the preceding cut-border edge. This operation 
will be called connect backward and is represented by the symbol . . Moving on to .gure 2d, two new vertex 
-operations arise at the cut-border edges 4and 5. At the cut-border edge 6the mirror image of the connect 
backward -operation is applied to connect this edge to the subsequent edge. Naturally, this operation 
is called connect forward and is abbreviated with ! . No triangle is added to cut-border edge 9as it 
is part of the mesh border. This fact has to be encoded, too, and is called border ( )-operation. A 
more complex operation appears at cut-border edge 10in .g­ure 2e. The adjacent triangle in the outer 
part is neither formed with the preceding nor with the subsequent cut-border vertex, but with a vertex 
further apart. The result is that the cut-border splits into two parts. In .gure 2f the .rst part is 
formed by the edges 11, 12and 16and the second part by 13, 14and 15. This operation will be called split 
cut-border ( 1i ), which takes the index i to specify the third vertex relative to the current cut-border 
edge. Figure 3a shows another split cut-border -operation. The rela­tive indices are written into the 
cut-border vertices. The split cut­border -operation has two consequences. Firstly, the cut-border cannot 
be represented anymore by a simple linked list, but by a list of linked lists. And secondly, the choice 
of the next cut-border part to be processed after a split cut-border -operation yields a new degree of 
freedom for the traverse order. To minimize the number of cut-border parts the cut-border part with fewer 
vertices is chosen. Another operation arises in .gure 2f at cut-border edge 11.The adjacent triangle 
closes the triangle mesh and the current cut-border part is removed. This operation is called close cut-border 
and is denoted by r . As the size of the current cut-border part is known during compression and decompression, 
the close cut-border ­operation can also be encoded with connect forward or connect backward and the 
different symbol is only introduced for didactic reasons. On the other hand if there really is a hole 
in the form of a triangle, three border -operations are encoded. Finally, there exists a somewhat inverse 
operation to the split cut-border -operation the cut-border union -operation. An ex­ample is visualized 
in .gure 3b. The .gure shows in perspective a cube with a quadratic hole. The so far compressed inner 
part con­sists of the two green shaded regions. There are two cut-border parts which are connected by 
the new yellow triangle, which is at­tached to the current edge (dark blue vertices). Therefore, this 
oper- S i ation is called cut-border union or for short . Two indices p are needed to specify the second 
cut-border part pand the index i of the vertex within the second cut-border part. The vertices in a cut-border 
part are enumerated according to the cut-border edges. Therefore, the vertex at the beginning of the 
cut-border edge with the smallest index in the cut-border part pis numbered zero, the vertex at the second 
smallest cut-border edge is numbered one and so forth. op.: vertex inner edge border edge triangle L 
3 0 0 1 * 1 1 0 1 !/ 0 2 0 1 0 0 1 0 1i 0 1 0 1 r 0 3 0 1 Si p 0 1 0 1 Table 1: The table shows for 
each basic building operation which mesh elements are .nally placed into the inner part. It can be shown 
that the number of cut-border union ­operations is exactly the genus of the compressed triangle mesh. 
Seen from a different angle, the operations r , !/ , 1i S i and provide the possibility to connect the 
current cut-border p edge to any possible vertex in the cut-border, whereas the opera­tions L and * utilize 
new vertices.  2.3 Compressed Representation The encoding of the sequence of atomic building operations 
uniquely de.nes the connectivity of a triangle mesh. The connectiv­ity of the sample mesh in .gure 2 
can be encoded by the following sequence of operations: 12! *** **! The symbols for the different operations 
can be encoded with Huff­man Codes to achieve good compression rates. Therefore, the mesh connectivity 
is sequentially stored in a bit stream. The geometry and material data must be supplied additionally. 
For each vertex this data can include the vertex position, the sur­face normal at the vertex and the 
texture coordinates or some mate­rial information. We will refer to all this data with the term vertex 
data. The material of the mesh can also be given for each triangle. Similarly, data can be supplied for 
the inner edges and the border edges of the mesh. We will collect the different kinds of data in the 
terms triangle data,the inner edge data and the border edge data. Thus for each type of mesh element, 
data can be supplied with the connectivity of the mesh. We refer to the collection of all additional 
data with the term mesh data. Depending on the application there exist two approaches to com­bine the 
connectivity and the mesh data of a compressed triangle mesh. If an application is supplied with enough 
storage for the com­plete mesh data, the bit stream for the connectivity can be stored separately. For 
each type of mesh element the speci.c data is stored in an array. While the triangle mesh is traversed 
a vertex, trian­gle, inner edge and border edge index is incremented after each operation, such that 
the current mesh elements can be found in the corresponding arrays with the suitable indices. Table 1 
shows the increments for each index after the different operations. For ex­ample after a connect forward 
-operation the inner edge index is incremented by two and the triangle index by one. The advantage of 
this representation is that the mesh data can be processed with­out traversing the mesh, for example 
to apply transformations to the coordinates and normals. If the compressed triangle mesh is passed to 
the graphics board or if the mesh data is encoded with variable length values, no ran­dom access to the 
vertex data is possible. Then the mesh data is inserted into the bit stream for the mesh connectivity. 
After each operation symbol in the stream, the corresponding mesh data is sent to the stream appropriately. 
For example after a split cut border ­symbol the mesh data for one inner edge and one triangle is trans­mitted 
(see table 1). If we only assume vertex and triangle data and th denote the vertex data for the ivertex 
with viand the triangle data for triangle jwith tj, the extended bit stream representation of the mesh 
in .gure 2 would be: v0v1v2t0 *v3t1 *v4t2 *v5t3t4 *v6t5 *v7t6!t7 12t8!t9 Remember that the initial 
triangle is implicitly stored without sym­bol and introduces the vertices v0;v1;v2and the triangle t0. 
If the triangle mesh consists of several unconnected components, the compressed bit stream representation 
consists of the concatena­tion of the bit streams of the different components.  3 Implementation All 
algorithms which process the compressed representation are based on the implementation of the data structure 
for the cut-border as introduced in section 3.1. This data structure implements the rules which de.ne 
the traverse order. All other algorithms such as the compression and decompression algorithms presented 
in the sections 3.2 and 3.3 use this implementation. Further algorithms such as homogeneous transformations 
of the mesh geometry would also use the cut-border data structure to iterate through the com­pressed 
representation The data structures and algorithms in this section are given in a C++-like syntax. For 
readability and brevity parentheses were replaced by indentation and additional key-words. 3.1 Cut-Border 
Data Structure Data Structure 1 cut border structPart int rootElement, nrEdges, nrVertices; structElement 
int prev, next; Data data; bool isEdgeBegin; structCutBorder Part *parts, *part; Element *elements, *element; 
Element *emptyElements; CutBorder(int maxParts, int maxElems); bool atEnd(); void traverseStep(Data &#38;v0, 
Data &#38;v1); void initial(Data v0, Data v1, Data v2); void newVertex(Data v); Data connectForward/Backward(); 
void border(); Data splitCutBorder(int i); Data cutBorderUnion(int i, int p); bool .ndAndUpdate(Data 
v, int i, int p); The data structure for the cut-border is a list of doubly linked lists storing the 
vertex data of the buffered vertices. All elements in the doubly linked lists of the different parts 
are stored within one homo­geneous buffer named elements. The maximum number of vertices in the buffer 
during the compression or decompression de.nes its size. The maximum buffer size is known once the triangle 
mesh is compressed and can be transmitted in front of the compressed representation. For the .rst compression 
of the mesh the maximum p number of vertices can be estimated by 10n(see section 4.2), where nis the 
number of vertices in the triangle mesh. With this approach a simple and ef.cient memory management as 
described in [6] is feasible. Only the pointer emptyElements is needed, which points to the .rst of the 
empty elements in the buffer. Any time a new element is needed, it is taken from the empty elements and 
the deleted elements are put back to the empty elements. On the one hand the memory management avoids 
dynamic storage allocation which is not available on graphics boards and on the other hand it speeds 
up the algorithms by a factor of two if no memory caches in.uence the performance. The different parts 
can be managed with an array parts with enough space for the maximum number of parts which are created 
while the mesh is traversed. Again this number must be estimated for the .rst compression and can be 
transmitted in front of the com­pressed representation. Thus the constructor for the cut-border data 
structure takes the maximum number of parts and the maximum number of cut-border elements. part and element 
point to the current part and the current ele­ment within the current part respectively. Each part stores 
the index of its root element, the number of edges and the number of vertices. These numbers may differ 
as each part is not simply a closed poly­gon. Any time a border -operation arises one cut-border edge 
is eliminated but the adjacent cut-border vertices can only be removed if they are adjacent to two removed 
edges. Therefore, each cut­border element stores in addition to the indices of the previous and next 
element and the vertex data, a .ag which denotes whether the edge beginning at this cut-border element 
belongs to the cut-border or not. The cut-border data structure provides methods to steer the traversal 
via a bit stream or with the help of a triangle mesh. The methods atEnd()and traverseStep(&#38;v0, &#38;v1)are 
used to form the main loop. The method traverseStep(&#38;v0, &#38;v1) steps to the next edge in the cut-border 
data structure and returns the vertex data of the two vertices forming this edge. If no more edges are 
available, atEnd()becomes true. During decompression the operations are read from the bit stream and 
the cut-border data structure is updated with the cor­responding methods initial, newVertex, connectForward/Backward, 
border, splitCutBorder and cutBorderUnion. For compression ad­ditionally the method .ndAndUpdate is needed 
to localize a vertex within the cut-border data structure. The part index and vertex in­dex are returned 
and can be used to deduce the current building op­eration. If the vertex has been found by the .ndAndUpdate-method, 
it is connected with the current cut-border edge. 3.2 Compression Algorithm Besides the cut-border we 
need two further data structures for the compression algorithm a triangle mesh, with random access to 
the third vertex of a triangle given a half edge, and a permutation. The random access representation 
of the triangle mesh provides two methods the chooseTriangle(v0;v1;v2)-method, which returns the vertex 
data v0;v1;v2of the three vertices in an initial triangle, and the method getVertexData(i0;i1), which 
takes the vertex in­dices i0and i1of a half edge v0v1and returns the vertex data of the third vertex 
of the triangle containing v0v1. The permutation is used to build a bijection between the vertex indices 
in the random access representation and the vertex indices in the compressed rep­resentation. It allows 
to map an index of the .rst kind to an index of the second kind and to determine whether a certain vertex 
index in the random access representation has been mapped. Given a random access triangle mesh, the compression 
algorithm computes the mentioned permutation and the compressed represen- Algorithm 1 compression Input: 
RAM ::: random access representation Output: bitStream ::: compressed representation perm ::: permutation 
of the vertices vertexIdx =3; RAM.chooseTriangle(v0, v1, v2); perm.map((v0.idx, 0), (v1.idx, 1), (v2.idx, 
2)); bitStream <<v0<<v1<<v2; cutBorder.initial(v0, v1, v2); whilenot cutBorder.atEnd()do cutBorder.traversalStep(v0, 
v1); v2 =RAM.getVertexData(v1.idx, v0.idx); ifv2.isUnde.ned()then bitStream << ; cutBorder.border(); 
else ifnot perm.isMapped(v2.idx)then bitStream << * <<v2; cutBorder.newVertex(v2); perm.map((v2.idx, 
vertexIdx++)); else cutBorder..ndAndUpdate(v2,i,p); S i ifp >0 then bitStream << ; p else ifi ==±1 then 
bitStream << !/ ; elsebitStream << 1i ; tation of the mesh, which is sent to a bit stream. The current 
vertex index of the compressed representation is counted in the index ver­texIdx. After the initial triangle 
is processed, the cut-border data structure is used to iterate through the triangle mesh. In each step 
the vertex data v0and v1of the current cut-border edge is deter­mined. From the vertex indices the vertex 
data of the third vertex in the triangle adjacent to the current edge is looked up in the random access 
triangle mesh. If no triangle is found, a border -operation is sent to the bit stream. Otherwise it is 
checked whether the new vertex has already been mapped, i.e. sent to the cut-border. If not, a new vertex 
-operation is sent to the bit stream and the vertex index is mapped to the current index in the compressed 
represen­tation. If the third vertex of the new triangle is contained in the cut-border, the .ndAndUpdate-method 
is used to determine the part index and the vertex index within that cut-border part. If the part index 
is greater than zero, a cut-border union -operation is writ­ten. Otherwise a connect forward/backward 
-operation or a split cut-border -operation is written dependent on the vertex index. 3.3 Decompression 
Algorithm The decompression algorithm reads the compressed representation from an input bit stream and 
enumerates all triangles. The trian­gles are processed with the subroutine handle(v0,v1,v2),which for 
example renders the triangles. As in the compression algorithm, .rstly, the initial triangle is processed 
and then the mesh is re-built with the help of the cut-border methods atEnd and traversalStep.In each 
step the next operation is read from the bit stream and the cor­responding method of the cut-border data 
structure is called such that the third vertex of the new triangle is determined in order to send it 
to the subroutine handle.  4 Measurements and Optimizations In this section we analyze our software 
implementation of the com­pression and decompression algorithm. Firstly, we introduce the Algorithm 2 
decompression Input: bitStream :::compressed representation Output: handle :::processes triangles bitStream 
>>v0>>v1>>v2; handle(v0, v1, v2); cutBorder.initial(v0, v1, v2); whilenot cutBorder.atEnd()do cutBorder.traversalStep(v0, 
v1); bitStream >>operation; switch (operation) case !/ : handle(v1, v0, cutBorder.connectForward/Backward()); 
case 1i : handle(v1, v0, cutBorder.splitCutBorder(i)); S i case : p handle(v1, v0, cutBorder.cutBorderUnion(i,p)); 
case * : bitStream >>v2; cutBorder.newVertex(v2); handle(v1, v0, v2); : cutBorder.border(); case triangle 
mesh compr decom storage name t n jbdj kL.s kL.s bits.t genus5 144 64 0 386 782 4.23±5.7% vase 180 97 
12 511 796 2.15±6.0% club 515 262 6 541 857 2.09±3.5% surface 2449 1340 213 490 790 1.87±0.8% spock 3525 
1779 30 496 820 1.97±0.7% face 24118 12530 940 430 791 1.81±0.3% jaw 77692 38918 148 332 809 1.62±0.5% 
head 391098 196386 1865 321 796 1.71±0.1% Table 2: The basic characteristics of the models, the compression 
and decompression speed and the storage needs per triangle. test set of models in section 4.1. Then we 
examine the in.uence of the traverse order on the compression ratio and the size of the cut­border (section 
4.2). And .nally we gather the important results on the performance of the presented algorithms in section 
4.3. 4.1 The Models The measurements were performed on the models shown in .gure 4. All models are simple 
connected 2-manifolds and differ in their size. From top left: genus5, vase, club, surface, spock, jaw, 
face, head. The detail of the head model is hidden in the small interior structures. Therefore, we present 
in .gure 4 a view into the inside of the head. The basic characteristics of the models are shown in the 
left half of table 2. Here the number of triangles t, the number of vertices n and the number of border 
edges jbdjare tabulated for each model. 4.2 Traverse Order and Cut-Border Size In section 2.1 we de.ned 
the traverse order up to the choice of the initial triangle and the enumeration of newly introduced cut-border 
edges. To study the in.uence of the initial triangle we measured the storage needs for the compressed 
connectivity of each model Figure 4: The models used to analyze the compression and decom­pression algorithms. 
 several times with randomly chosen initial triangles. Then we com­puted for each model the average value 
and the standard deviation as tabulated on the very right of table 2. The in.uence of the ini­tial triangle 
vanishes with increasing size of the model and is still less than ten percent for the smallest models. 
With the same mea­surements the .uctuation of the cut-border size was determined as shown in table 3. 
Here the .uctuation is higher and reaches up to twenty percent for the jaw and the club models. There 
are a large number of enumeration strategies for the newly introduced cut-border edges. For performance 
reasons and the sim­plicity of the implementation, we favored the enumeration strate­gies which can be 
implicitly handled with the cut-border data struc­ture introduced in section 3.1. Therefore a newly introduced 
cut­border edge may either be delayed until all present edges are pro­cessed or the new edge is processed 
next. These two strategies ap­plytothe connect forward/backward -operations and correspond to attaching 
the next highest and the next smallest edge index re­spectively to the new edge. In the case of a new 
vertex -operation two new edges are introduced to the cut-border. In this case three possible strategies 
are feasible. Either the .rst/second new edge is processed next or both edges are delayed. The split 
cut-border ­and the cut-border union -operations arise much more rarely and therefore were excluded from 
the analysis of the traversal strategy. Thus we were left with twelve strategies, three choices for the 
new vertex -operation and for each connect -operation two choices. Luckily, it turned out that the strategy, 
where the new edge is pro­cessed next after both connect -operations and where the second edge is processed 
next after a new vertex -operation, is superior over all others. This strategy achieved best compression 
and kept the cut-border smallest for all models. Table 3 shows for each model the maximum number of parts 
and the maximum number of buffered vertices needed for mesh traver­Table 3: The maximum number of parts 
and the maximum number of buffered vertices needed for mesh traversal. The last column name partmax vertmax 
prop genus5 3.21±12.7% 32.75±15.4% 5.35 vase 2.30±24.2% 22.56±10.2% 2.99 club 3.11±11.9% 44.24±21.0% 
4.45 surface 3.10±9.7% 83.16±12.1% 3.10 spock 3.24±13.2% 120.10±4.5% 3.23 face 3.40±15.6% 329.08±14.5% 
4.22 jaw 4.76±10.7% 564.42±19.7% 4.55 head 9.00±11.1% 1255:20±8.6% 3.56 p gives the quantity prop (vertmax+6svert)In. 
sal. The values are averaged over several random choices of the initial triangle. As the values .uctuate 
signi.cantly we add three standard deviations to the values such that 99.73% of the values are smaller 
than our estimation if we suppose a normal distribution. The maximum number of cut-border parts is comparably 
small and can safely be estimated by 100 for the .rst compression of a triangle mesh. To show that the 
maximum number of buffered vertices in­ p creases with nwe divide the measured values plus three standard 
p deviations by nand get values between three and six independent of the size of the model. Thus a save 
estimation for the size of the p vertex buffer in a .rst compression of a triangle mesh is 10n. 4.3 
Performance The last column of table 2 shows that our approach allows com­pression of the connectivity 
of a triangle mesh to two bits per tri­angle3 and less. The theoretical lower limit is 1:5bits per triangle 
which is achieved with uniform triangle meshes. To understand this fact let us neglect the split cut-border 
-and cut-border union ­operations. Each new vertex -operation introduces one vertex and one triangle, 
whereas each connect -operation only introduces a triangle to the mesh. To arrive at a mesh with twice 
as many tri­angles as vertices, equally many new vertex -and connect ­operations must appear. The Huffman 
code for the new vertex ­operation consumes one bit and the connect -operations are en­coded with two 
and three bits as still other operations must be en­coded. If both connect -operations are equally likely, 
we get a compression to 1:75bits per triangle. If on the other hand one connect -operation is completely 
negligible a compression to 1:5 bits is feasible. The optimal traversal strategy found in the previous 
section avoids connect backward -operation and therefore allows for better Huffman-encoding than the 
other strategies. Table 2 also shows the compression and decompression speed in thousands of triangles 
per second measured on a 175MHz SGI/O2 R10000. The decompression algorithm clearly performs in linear 
time in the number of triangles with about 800,000 triangles per second. But the performance of the compression 
algorithm seems to decrease with increasing n. Actually, this impression is caused by the 1 MB data cache 
of the O2 which cannot keep the com­plete random access representation of the larger models, whereas 
the small cut-border data structure nicely .ts into the cache during decompression. On machines without 
data cache the performance of the compression algorithm is also independent of n.The com­pression algorithm 
is approximately half as fast as the decompres­sion algorithm. About 40% of the compression time is consumed 
by the random access representation of the triangle mesh in order to .nd the third vertex of the current 
vertex. The other ten percent are used to determine the part and vertex index within the cut-border. 
3The genus5 model consumes more storage as its genus forces .ve cut­border union -operations and the 
model is relatively small. If our compression scheme is used to increase the bandwidth of transmission, 
storage or rendering, we can easily compute the break-even point of the bandwidth. The total time consumed 
by our compression scheme is the sum of the times spent for com­pression, transmission and decompression. 
The total time must be compared to the transmission time of the uncompressed mesh. Let us assume for 
the uncompressed representation an index size of 2 bytes, such that each triangle is encoded in 6 bytes. 
If we further use the estimation that the triangle mesh contains twice as many tri­angles as vertices, 
the break-even point computes4 to a bandwidth of 12MBit/sec. Thus the compression scheme can be used 
to im­prove transmission of triangle meshes over standard 10MBit Ether­net lines. As our approach allows 
us to compress and decompress the triangle mesh incrementally, the triangle mesh can also be com­pressed 
and decompressed in parallel to the transmission process. Then even the transmission over a 100MBit Ethernet 
line could be improved.  5 Extensions In this section we describe how to extend our method on non ori­entable 
triangle meshes. Additionally, we show how to encode at­tributes which are attached to vertex-triangle 
pairs. 5.1 Non Orientable Triangle Meshes As we restricted ourselves to 2-manifold triangle meshes, the 
neigh­borhood of each vertex must still be orientable even for non ori­entable meshes. From this follows 
that each cut-border part must be orientable at any time: a cut-border part is a closed loop of adja­cent 
edges. The orientation of one edge is passed on to an adjacent edge through the consistent orientation 
of the neighborhood of their common vertex. Therefore, only the split cut-border -and cut­border union 
-operations need to be extended as they introduce or eliminate cut-border parts. Both operations connect 
the current cut­border edge to a third vertex in the cut-border, which is either in the same or in another 
cut-border part. The only thing which can be different in a non orientable triangle mesh is that the 
orientation of the cut-border around this third vertex is in the opposite direction as in the orientable 
case. Therefore, only one additional bit is needed S i for each 1i -and -operation, which encodes whether 
the p orientation around the third vertex is different. During compression the value of the additional 
bit can be checked from the neighbor­hood of the third vertex. Previously a split cut-border -operation 
produced a new cut-border part. In the new case with different ori­entations around the third vertex, 
the orientation of one of the new parts must be reversed and the parts are concatenated again as illus­trated 
in .gure 5. In a cut-border union -operation the cut-border part containing the third vertex is concatenated 
to the current cut­border part and in the new case the orientation of the cut-border part with the third 
vertex is reversed before concatenation. 5.2 Attributes For Vertex-Triangle Pairs A lot of triangle 
meshes contain discontinuities that force attach­ment of certain vertex attributes to vertex-triangle 
pairs. See for ex­ample the genus5 model in .gure 4, which contains a lot of creases. For each vertex 
on a crease exist two or more different normals which must be attached to the same vertex which is contained 
in different triangles. Thus for models with creases it must be possible to store several different vertex 
normals for different vertex-triangle pairs. Similarly, discontinuities in the color attribute force 
storage 4with a compression rate of 400,000 triangles per second and a decom­pression rate of 800,000 
triangles per second  re-orientate cut-border  Figure 5: After some split cut-border -operations of 
a non ori­entable manifold half of the cut-border (drawn in red) must be re­oriented and no new part 
is generated. crease crease crease cut-border Figure 6: Creases divide the neighborhood of a vertex 
into regions. Each region contains the triangles with one vertex-triangle attribute. of several RGBA 
values within the vertex-triangle pairs. A simple solution to support vertex-triangle attributes is to 
encode these at­tributes with every appearance of a vertex-triangle pair. This im­plies that the same 
vertex-triangle attributes for one vertex may be replicated several times. On the other hand if we duplicated 
these vertices, which lay on creases, the vertex coordinates would be replicated. With a small amount 
of overhead we can do better and encode each vertex location and each vertex-triangle attribute exactly 
once. Let us denote the collection of all vertex speci.c data as for ex­ample its coordinates with vand 
the different collections of the t1 t vertex-triangle data with v;v2;:::. As an example let us de­ t 
scribe the encoding of vin the case of creases as illustrated in .gure 6. The neighborhood of each vertex 
is split by the creases into several regions. Within each region there is exactly one vertex­triangle 
attribute valid for the vertex and all triangles in this region. On the right side of .gure 6 a cut-border 
vertex is shown during compression or decompression. We see that at any time it is suf.­cient to store 
besides the vertex data vtwo vertex-triangle attributes tleft vand v tright for each vertex within the 
cut-border. When a new triangle is added to the cut-border, the vertex-triangle attributes of a vertex 
can only change, if the vertex is part of the current cut­border edge and if this edge is a crease. If 
one of the vertex-triangle tleft attributes for example vchanges after an operation which adds a triangle, 
there are two possible cases. Either a new vertex-triangle attribute is transmitted over the bit stream 
or the new attribute is tright copied from v. To encode when a new vertex-triangle attribute has to be 
trans­mitted we transmit one or two control bits after each operation, which adds a triangle to the current 
cut-border edge. Two control bits are needed only for the connect -operations as the new trian­gle contains 
two cut-border edges. The control bits encode whether the affected cut-border edges are creases. Afterwards, 
for each cut­border vertex on a cut-border edge, which is a crease, we trans­mit one further bit which 
encodes whether a new vertex-triangle attribute is transmitted or the attribute should be copied from 
the other vertex-triangle attribute stored in the cut-border. If we de­note the total number of inner 
edges with eand the total number of crease edges with ecthis approach results in an overhead of less 
than e+2ecbits.  6 Conclusion The presented compression technique provides not only a fast de­compression 
algorithm but also a compression algorithm which per­forms in only double the amount of time of the decompression 
al­gorithm. The slow down of the compression algorithm is primarily caused by the uncompressed random 
access mesh representation. Faster mesh data structures must be investigated as well as the us­age of 
hash tables to speed up the search for vertices within the cut-border. The simplicity of the algorithms 
allow for hardware implemen­tation and suitable hardware will be designed in future work. But also software 
implementations perform extremely well as shown in the previous section. Beside the good performance 
and the sim­plicity of the approach the connectivity of a triangle mesh is com­pressed similarly well 
as by the best known compression methods. Therefore even globally optimizing compression algorithms can 
be replaced by the faster and simpler approach.  Acknowledgments Many thanks to Reinhard Klein and Andreas 
Schilling for inspiring discussions and to Michael Doggett for reviewing the paper. References [1] E. 
M. Arkin, M. Held, J. S. B. Mitchell, and S. S. Skiena. Hamiltonian triangulations for fast rendering. 
Lecture Notes in Computer Science, 855:36 57, 1994. [2] Rueven Bar-Yehuda and Craig Gotsman. Time/space 
tradeoffs for polygon mesh rendering. ACM Transactions on Graphics, 15(2):141 152, April 1996. [3] M. 
Deering. Geometry compression. In Computer Graphics (SIGGRAPH 95 Proceedings), pages 13 20, 1995. [4] 
Francine Evans, Steven S. Skiena, and Amitabh Varshney. Op­timizing triangle strips for fast rendering. 
In IEEE Visualization 96. IEEE, October 1996. ISBN 0-89791-864-9. [5] Silicon Graphics Inc. GL programming 
guide. 1991. [6] Scott Meyers. Effective C++: 50 speci.c ways to improve your programs and designs. 
2. ed. Addison-Wesley, Reading, MA, USA, 1997. [7] Jackie Neider, Tom Davis, and Mason Woo. OpenGL Pro­gramming 
Guide The Of.cial Guide to Learning OpenGL, Version 1.1. Addison-Wesley, Reading, MA, USA, 1997. [8] 
Gabriel Taubin and Jarek Rossignac. Geometric compres­sion through topological surgery. Technical report, 
Yorktown Heights, NY 10598, January 1996. IBM Research Report RC 20340.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280837</article_id>
		<sort_key>141</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[The design of a parallel graphics interface]]></title>
		<page_from>141</page_from>
		<page_to>150</page_to>
		<doi_number>10.1145/280814.280837</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280837</url>
		<categories>
			<primary_category>
				<cat_node>C.0</cat_node>
				<descriptor>Hardware/software interfaces</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Parallel processing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>D.2.2</cat_node>
				<descriptor>Modules and interfaces</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>D.1.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010169</concept_id>
				<concept_desc>CCS->Computing methodologies->Parallel computing methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011008.10011009.10011014</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->General programming languages->Language types->Concurrent programming languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011008.10011024.10011031</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->General programming languages->Language features->Modules / packages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10011777.10011014</concept_id>
				<concept_desc>CCS->Computing methodologies->Concurrent computing methodologies->Concurrent programming languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P111544</person_id>
				<author_profile_id><![CDATA[81100571859]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Homan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Igehy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford Univ., Stanford, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39079633</person_id>
				<author_profile_id><![CDATA[81100189931]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gordon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stoll]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford Univ., Stanford, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15033698</person_id>
				<author_profile_id><![CDATA[81100482576]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Pat]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hanrahan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford Univ., Stanford, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>624014</ref_obj_id>
				<ref_obj_pid>623269</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[S. Amarasinghe, J. Anderson, C. Wilson, S. Liao, B. Murphy, R. French, M. Lam, and M. Hall. Multiprocessors from a Software Perspective. IEEE Micro, 16:3, pages 52-61, 1996.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237276</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[A. Beers, M. Agrawala, and N. Chaddha. Rendering from Compressed Textures. Computer Graphics (SIGGRAPH 96 Proceedings), volume 30, pages 373-378, 1996.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>44111</ref_obj_id>
				<ref_obj_pid>44102</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[J. Blinn. Me and My (Fake) Shadow. IEEE Computer Graphics and Applications, 8:1, pages 82-86, 1988.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>279372</ref_obj_id>
				<ref_obj_pid>279358</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[M. Cox, N. Bhandari, and M. Shantz. Multi-Level Texture Caching for 3D Graphics Hardware. Proceedings of the 25th International Symposium on Computer Architecture, 1998.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[T. Crockett. Design Considerations for Parallel Graphics Libraries. Proceedings of the Intel Supercomputer Users Group 1994, 1994.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218391</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[M. Deering. Geometry Compression. Computer Graphics (SIGGRAPH 95 Proceedings), volume 29, pages 13-20, 1995.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[E. Dijkstra. Cooperating Sequential Processes. Programming Languages, pages 43-112, 1968.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258714</ref_obj_id>
				<ref_obj_pid>258694</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[J. Eyles, S. Molnar, J. Poulton, T. Greer, A. Lastra, N. England, and L. Westover. PixelFlow: The Realization. Proceedings of the 1997 SIGGRAPH/Eurographics Workshop on Graphics Hardware, pages 57-68, 1997.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>105712</ref_obj_id>
				<ref_obj_pid>105710</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[J. Gettys and P. Karlton. The X Window System, Version 11. Software--Practice and Experience, 20:$2, pages 35-67, 1990.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>264152</ref_obj_id>
				<ref_obj_pid>264107</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Z. Hakura and A. Gupta. The Design and Analysis of a Cache Architecture for Texture Mapping. Proceedings of the 24th International Symposium on Computer Architecture, 1997.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258843</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[H. Hoppe. View-Dependent Refinement of Progressive Meshes. Computer Graphics (SIGGRAPH 97 Proceedings), volume 31, pages 189-198, 1997.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>232898</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[M. Kilgard. OpenGL Programming for the X Window System, Addison-Wesley, 1996.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[D. Kirkland. Personal Communication. Intergraph Corp., 1998.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[L. Lamport. How to Make a Multiprocessor Computer that Correctly Executes Multiprocess Programs. IEEE Transactions on Computers, 28:9, pages 241-248, 1979.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>264206</ref_obj_id>
				<ref_obj_pid>264107</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[J. Laudon and D. Lenoski. The SGI Origin: A ccNUMA Highly Scalable Server. Proceedings of the 24th Annual Symposium on Computer Architecture, 1997.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37422</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[W. Lorensen and H. Cline. Marching Cubes: A High-Resolution 3D Surface Reconstruction Algorithm. Computer Graphics (SIGGRAPH 87 Proceedings), volume 21, pages 163-169, 1987.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>182469</ref_obj_id>
				<ref_obj_pid>182466</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[S. Molnar, M. Cox, D. Ellsworth, and H. Fuchs. A Sorting Classification of Parallel Rendering. IEEE Computer Graphics and Applications, 14:4, pages 23-32, 1994.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134067</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[S. Molnar, J. Eyles, and J. Poulton. PixelFlow: High-Speed Rendering Using Image Composition. Computer Graphics (SIGGRAPH 92 Proceedings), volume 26, pages 231-240, 1992.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258871</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[J. Montrym, D. Baum, D. Dignam, and C. Migdal. InfiniteReality: A Real-Time Graphics System. Computer Graphics (SIGGRAPH 97 Proceedings), volume 31, pages 293-302, 1997.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[J. Neider, T. Davis, and M. Woo. OpenGL Programming Guide. Addison-Wesley, 1993.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808606</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[T. Porter and T. Duff. Compositing Digital Images. Computer Graphics (SIGGRAPH 84 Proceedings), volume 18, pages 253- 259, 1984.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192262</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[J. Rohlf and J. Helman. IRIS Performer: A High Performance Multiprocessing Toolkit for Real-Time 3D Graphics. Computer Graphics (SIGGRAPH 94 Proceedings), volume 28, pages 381- 395, 1994.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>24053</ref_obj_id>
				<ref_obj_pid>22949</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[R. Scheifler and J. Gettys. The X Window System. ACM Transactions on Graphics, 5:2, pages 79-109, 1986.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15903</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[T. Sederberg and S. Parry. Free-Form Deformation of Solid Geometric Models. Computer Graphics (SIGGRAPH 86 Proceedings), volume 20, pages 151-160, 1986.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237274</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[J. Torborg and J. Kajiya. Talisman: Commodity Real-Time 3D Graphics for the PC. Computer Graphics (SIGGRAPH 96 Proceedings), volume 30, pages 57-68, 1996.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378517</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[D. Voorhies, D. Kirk, and O. Lathrop. Virtual Graphics. Computer Graphics (SIGGRAPH 88 Proceedings), volume 22, pages 247-253, 1988.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Copyright &#38;#169;1998 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers or to redistribute to lists, requires specific permission and/or a fee. The Design of a Parallel 
Graphics Interface Homan Igehy Gordon Stoll Pat Hanrahan Computer Science Department Stanford University 
 Abstract It has become increasingly difficult to drive a modern high­performance graphics accelerator 
at full speed with a serial imme­diate-mode graphics interface. To resolve this problem, retained­mode 
constructs have been integrated into graphics interfaces. While retained-mode constructs provide a good 
solution in many cases, at times they provide an undesirable interface model for the application programmer, 
and in some cases they do not solve the performance problem. In order to resolve some of these cases, 
we present a parallel graphics interface that may be used in conjunc­tion with the existing API as a 
new paradigm for high­performance graphics applications. The parallel API extends existing ideas found 
in OpenGL and X11 that allow multiple graphics contexts to simultaneously draw into the same image. Through 
the introduction of synchronization primitives, the parallel API allows parallel traversal of an explic­itly 
ordered scene. We give code examples which demonstrate how the API can be used to expose parallelism 
while retaining many of the desirable features of serial immediate-mode pro­gramming. The viability of 
the API is demonstrated by the per­formance of our implementation which achieves scalable per­formance 
on a 24 processor system. CR Categories and Subject Descriptors: C.0 [Computer Systems Organization]: 
Hardware/Software Interfaces; D.1.3 [Programming Techniques]: Concurrent Programming; I.3.1 [Computer 
Graphics]: Hardware Architecture. 1 INTRODUCTION Computer graphics hardware has been rapidly increasing 
in per­formance. This has motivated immediate-mode graphics inter­faces like OpenGL [20] to adopt constructs 
such as display lists and packed vertex arrays in order to alleviate system bottlenecks. However, these 
constructs may impose an undesired paradigm shift for the application programmer, and they may not be 
useful in resolving the particular performance bottleneck. Furthermore, with the increasing use of multiprocessor 
systems for graphics applications, a serial interface to the graphics system can be inele­ {homan,gws,hanrahan}@graphics.stanford.edu 
gant. A parallel graphics interface seeks to resolve these issues. To provide a common framework, we 
first review three key issues in graphics interfaces. The first issue in designing a graphics interface 
is state. In a stateless interface, the behavior of every command is independent of every other command; 
thus, every command must include all the information required to execute that command. Conversely, in 
an interface with state, a command s behavior can be affected by previous commands. Some of the information 
required for the execution of commands may reside within the state maintained by the interface, and some 
commands modify that state. While stateless interfaces simplify many issues (especially with regard to 
parallelism), they are not well-suited to full-featured rendering systems. The problem is that a large 
amount of data is needed for each drawing command, and much of it is changed infrequently. Respecifying 
this data with each primitive is tedious and ineffi­cient, so most graphics interfaces contain state. 
The second key issue is whether the graphics interface is im­mediate-mode or retained-mode. In an immediate-mode 
API, the application sends commands to the graphics system one at a time, and the graphics system executes 
them more or less immediately. In a retained-mode API, the application first specifies an entire scene 
that is built on the graphics system and subsequently re­quests the scene to be rendered with certain 
viewing parameters. Though retained-mode interfaces can sometimes provide perform­ance benefits, programmers 
prefer immediate-mode interfaces due to their flexibility and ease of use. Many well-designed interfaces 
use the best of both worlds: they are based on immediate-mode semantics, and some retained-mode constructs 
are included to allow performance benefits. The third major issue that arises in graphics interface design 
is ordering. Ordering semantics are the set of rules that constrain the order in which API commands may 
be executed. In a strictly ordered interface, primitives must be drawn in the order in which they are 
specified. This behavior is essential for many algorithms such as the placement of ground-plane shadows 
[3] and transpar­ency through alpha-compositing [21]. Sometimes, however, a programmer may not care whether 
or not primitives are drawn in the order specified. For example, depth buffering alleviates the need 
for drawing a scene of opaque 3D primitives in any particu­lar order. In these cases, the programmer 
would gladly use less constrained ordering semantics if it meant increased performance. In the rest of 
the paper, we present the motivations and issues involved in designing a parallel extension to a serial 
immedi­ate-mode graphics interface with strict ordering and state. By adding synchronization commands 
(such as barriers and sema­phores) into multiple graphics command streams, application threads can issue 
explicitly ordered primitives in parallel without blocking. We also introduce the notion of a wait context 
com­mand for synchronizing contexts at the level of application threads. Given the resulting API, we 
explore how an application programmer would attain parallel issue of graphics commands. These ideas are 
demonstrated by an implementation which achieves scalable performance on a 24 processor system. We also 
discuss the issues surrounding the various ways of implementing the parallel API. Very little research 
has been done in the area of parallel graphics interfaces. This paper provides a common framework on 
which a new class of research and commercial sys­tems can be built as well as a common framework on which 
a new class of parallel algorithms can be designed. 2 MOTIVATION Graphics interfaces have been around 
for many years, so why investigate a parallel API now? While it is true that some appli­cations are most 
naturally expressed through a parallel graphics API, the main motivation is performance: it is becoming 
more and more difficult to drive a graphics system at full speed using a single CPU. First we look into 
the reasons behind this, and then we examine possible solutions. 2.1 Performance Limitations Although 
graphics systems are on the same technology curve as microprocessors, graphics systems have reached a 
level of per­formance at which they can process graphics commands faster than microprocessors can produce 
them: a single CPU running an immediate-mode interface cannot keep up with modern graphics hardware. 
This is primarily due to an increasing use of parallel­ism within graphics hardware. Within a computer, 
there are three sources of bottlenecks in a graphics application. First, perform­ance may be limited 
by the speed of the graphics system. In this case, the only solution is to use a faster graphics system. 
Second, performance may be limited by the rate of data generation. In this case, the programmer can use 
either a faster data generation algo­rithm or else, if the algorithm is parallelizable, multiple proces­sors. 
Third, performance may be limited by the interface between the host system and the graphics system. Possible 
sources of this limitation are: 1) Overhead for encoding API commands. 2) Data bandwidth from the API 
host. 3) Data bandwidth into the graphics system. 4) Overhead for decoding API commands. 2.2 Performance 
Solutions There are several possible ways to extend a serial immediate­mode API in order to address the 
interface bottlenecks. First, we describe two techniques that are currently in widespread use, packed 
primitive arrays and display lists. Then we describe a proposed technique, compression. Finally, we describe 
our pro­posal, a parallel graphics interface. It is important to note that many of these techniques can 
complement each other in resolving performance bottlenecks. 2.2.1 Packed Primitive Arrays A packed primitive 
array is an array of primitives that reside in system memory. By using a single API call to issue the 
entire array of primitives instead of one API call per primitive, the cost of encoding API commands is 
amortized. Furthermore, because the arrays may be transferred by direct memory access (DMA), bandwidth 
limitations from the API processor may be bypassed. Nothing is done, however, about the bandwidth limitations 
into the graphics system. Furthermore, although the decoding may be somewhat simplified, all the primitives 
in the array still have to be decoded on the graphics system. While packed primitive arrays are useful 
in a wide variety of applications, they may introduce an awkward programming model. 2.2.2 Display Lists 
A display list is a compiled set of graphics commands that resides on the graphics system. In a fashion 
similar to retained-mode interfaces, the user first specifies the list of commands to be stored in the 
display list and later invokes the commands within the display list. Because they are essentially command 
macros, display lists work well semantically with immediate-mode inter­faces. In cases where the scene 
is small enough to fit in the graphics system and the frame-to-frame scene changes are modest, display 
lists trivially resolve the first three bottlenecks. If the scene is too large and therefore must reside 
in system memory, display lists are similar to packed primitive arrays and only the first two bottlenecks 
are resolved. Display lists provide an excel­lent solution for performance bottlenecks if the same objects 
are drawn from frame to frame. But on applications that recompute the graphics data on every frame (e.g., 
[11, 24]), display lists are not useful. Furthermore, the use of display lists burdens the pro­grammer 
with the task of managing handles to the display lists. 2.2.3 Compression Whereas the idea of quantizing 
the data sent through the API has been used for quite some time, the idea of compressing the data has 
only recently been proposed. One system compresses the geometric data sent through the API [6]; other 
systems compress the texture data [2, 25]. All compression schemes increase the decoding costs, and systems 
which compress the data interactively increase the encoding costs. Systems which compress the data off-line, 
on the other hand, are useful only when the graphics data does not change. 2.2.4 Parallel Interface 
The motivation behind a parallel graphics interface is scalability: bottlenecks are overcome with increased 
parallelism. If the graphics system is too slow, it can be scaled by adding more graphics nodes. If the 
data generation is too slow, more proces­sors can be used to generate the data in parallel. Similarly, 
if the serial interface is too slow, then it should be parallelized. In a system with a single graphics 
port, a parallel API can be used to overcome the first two interface limitations. However, by build­ing 
a scalable system with multiple graphics ports, all interface limitations can be overcome. There are 
many challenges to designing a good parallel graphics interface; in formulating our design, we had several 
goals in mind. First and foremost were the ability to issue graph­ics primitives in parallel and the 
ability to explicitly constrain the ordering of these primitives. Ideally, the API should allow paral­lel 
issue of a set of primitives that need to be drawn in an exact order. The parallel API should be a minimal 
set of extensions to an immediate-mode interface such as OpenGL, and it should be compatible with existing 
features such as display lists. The design is constrained by the presence of state; this is required 
for a large feature set. A well designed parallel interface should be intuitive and useful in a wide 
variety of applications. And finally, the new API should extend the current framework of graphics architec­tures 
to provide a rich set of implementation choices.  3 RELATED WORK In the field of parallel graphics 
interfaces, Crockett introduced the Parallel Graphics Library (PGL) for use in visualizing 3D graph­ics 
data produced by message-passing supercomputers [5]. Due to the characteristics of its target architecture 
and target applica­tions, PGL was designed as a retained-mode interface. In parallel, each processor 
adds objects to a scene by passing pointers to graphics data residing in system memory. A separate command 
is used to render the objects into a framebuffer, and no ordering constraints are imposed by the interface. 
PixelFlow [8, 18] is another system designed to support multiple simultaneous inputs from a parallel 
host machine, and PixelFlow OpenGL includes extensions for this purpose. However, due to the underlying 
im­age composition architecture, PixelFlow OpenGL also imposes frame semantics and does not support ordering. 
Because of these constraints, PGL and PixelFlow OpenGL do not meet the re­quirements of many graphics 
applications. The X11 window system provides a parallel 2D graphics in­terface [9, 23]. A client with 
the proper permissions may open a connection to an X server and ask for X resources to be allocated. 
Among these resources are drawables (which are on- or off-screen framebuffers) and X contexts (which 
hold graphics state). Since resources are globally visible, any client may subsequently use the resource 
within X commands. Since X drawing calls always in­clude references to a drawable and an X context, client 
requests are simply inserted into a global queue and processed one at a time by the X server. Though 
it is not explicitly encouraged, multiple clients may draw into the same drawable or even use the same 
graphics context. While a 3D graphics interface was beyond the scope of the original design of X, OpenGL 
is a 3D interface that has been cou­pled with X. For explicitness, OpenGL within X will serve as our 
example API due to its popularity and elegant design [20]. OpenGL is an immediate-mode interface whose 
state is kept within an X resource called the GLX context. In the interest of efficiency, both display 
lists and packed primitive arrays are sup­ported. Furthermore, both texture data and display lists may 
be shared between contexts in order to allow the efficient sharing of hardware resources amongst related 
contexts [12]. Strict ordering semantics are enforced in X and OpenGL: from the point of view of the 
API, every command appears to be exe­cuted once the API call returns. However, in the interest of effi­ciency, 
both interfaces allow implementations to indefinitely buffer commands. This introduces the need for two 
types of API calls. Upon return from the flush call (XFlush, glFlush), the sys­tem guarantees that all 
previous commands will execute in a finite amount of time from the point of view of the drawable. Upon 
return from a finish call (XSync, glFinish), the system guarantees that all previous commands have been 
executed from the point of view of the drawable. Since OpenGL and X solve different problems, programs 
often use both. Because of buffering, however, a program must syn­chronize the operations of the two 
streams. Imagine a program that wants to draw a 3D scene with OpenGL and then place text on top of it 
with X. It is insufficient to simply make the drawing calls in the right order because commands do not 
execute imme­diately. Furthermore, a flush is insufficient because it only guar­antees eventual execution. 
A finish, on the other hand, guarantees the right order by forcing the application to wait for the OpenGL 
commands to execute before issuing X commands. In a sense, however, the finish is too much: the application 
need not wait for the actual execution of the OpenGL commands; it only needs a guarantee that all prior 
OpenGL commands execute before any subsequent X commands. The call glXWaitGL provides this guarantee, 
and glXWaitX provides the complement. Hardware implementations of OpenGL typically provide sup­port for 
a single context, and sharing of the hardware is done through a context switch. Though context switches 
are typically inexpensive enough to allow multiple windows, they are expen­sive enough to discourage 
fine-grained sharing of the graphics hardware between application threads. A few architectures actu­ally 
provide hardware support for multiple simultaneous contexts drawing into the same framebuffer [13, 26], 
but all commands must go through a single graphics port. Furthermore, these ar­chitectures do not have 
a mechanism for maintaining the parallel issue of graphics commands when an exact ordering of primitives 
is desired. 4 THE PARALLEL API EXTENSIONS While OpenGL is not intended for multithreaded use in most 
implementations, the interface provides mechanisms for having multiple application threads work simultaneously 
on the same image. In this section, we first demonstrate how such an interface may be used to attain 
parallel issue of graphics commands. Then, we show how additional extensions can be used to increase 
the performance of parallel issue. The specification of the API exten­sions is given in Figure 1, and 
the reader is encouraged to look back to it as necessary. The API extensions are most easily motivated 
through the use of an example. Suppose that we want to draw a 3D scene com­posed of opaque and transparent 
objects. Though depth buffering alleviates the need to draw the opaque primitives in any particular order, 
blending arithmetic requires that the transparent objects be drawn in back-to-front order after all the 
opaque objects have been drawn. By utilizing the strict ordering semantics of the se­rial graphics API, 
a serial program simply issues the primitives in the desired order. With a parallel API, order must be 
explicitly constrained. We assume the existence of two arrays: one holds opaque primitives, and the other 
holds transparent primitives in glpNewBarrier(GLuint barrier, GLuint numCtxs) barrier->numCtxs = numCtxs; 
barrier->count = numCtxs; glpBarrier(GLuint barrier) barrier->count--; if (barrier->count == 0) barrier->count 
= barrier->numCtxs; signal(all waiting contexts); else wait(); glpDeleteBarrier(GLuint barrier) glpNewSema(GLuint 
sema, GLuint count) sema->count = count; glpPSema(GLuint sema) if (sema->count == 0) wait(); sema->count--; 
glpVSema(GLuint sema) sema->count++; signal(one waiting context, if any); glpDeleteSema(GLuint sema) 
glpWaitContext(GLXContext ctx) Upon return, all subsequent commands from the issuing context are guaranteed 
to execute after all prior commands from ctx have finished execution. Figure 1: The Parallel Graphics 
Interface Extensions. back-to-front order. We also assume the existence of the follow­ing function: DrawPrimitives(prims[first..last]) 
 glBegin(GL_TRIANGLE_STRIP) for p = first..last glColor(prims[p].color) glVertex(prims[p].coord) 
glEnd() 4.1 Existing Constructs As a first attempt at parallel issue, imagine two application threads 
using the same context to draw into the same framebuffer. In such a situation, a set current color command 
intended for a primitive from one application thread could be used for a primi­tive from the other application 
thread. In general, the sharing of contexts between application threads provides unusable semantics because 
of the extensive use of state. By using separate contexts, dependencies between the state-modifying graphics 
commands of the two streams are trivially resolved. Given two application threads using separate contexts 
on the same framebuffer, the fol­lowing code could be used to attain parallel issue of the opaque primitives: 
Thread1 Thread2 DrawPrimitives(opaq[1..256]) DrawPrimitives(opaq[257..512]) glFinish() appBarrier(appBarrierVar) 
appBarrier(appBarrierVar) DrawPrimitives(tran[1..256]) glFinish() appBarrier(appBarrierVar) appBarrier(appBarrierVar) 
 DrawPrimitives(tran[257..512]) Both application threads first issue their share of opaque primitives 
without regard for order. After synchronizing in lock­step at the application barrier, Thread1 issues 
its half of the trans­parent primitives. These transparent primitives are guaranteed to be drawn in back-to-front 
order after Thread1 s share of opaque primitives through the strict ordering semantics of the serial 
API. They are also guaranteed to be drawn after Thread2 s share of opaque primitives through the combination 
of the barrier and the finish: the finish guarantees the drawing of all previously issued commands from 
Thread2. By using this same synchronization mechanism again, Thread2 s share of transparent primitives 
are then drawn in back-to-front order after Thread1 s share of trans­parent primitives. 4.2 The Wait 
Construct One inefficiency in the above code is the use of the finish com­mand; in a sense, it is too 
much. Synchronization between the threads does not require the actual execution of the graphics commands; 
it only requires a guarantee on the order of execution between the two graphics streams. In a fashion 
similar to what is used for synchronizing X and OpenGL, we introduce the wait context call in order to 
make guarantees about the execution of commands between contexts. We refer the reader to Figure 1 for 
an exact specification. In synchronization situations, the wait call is more efficient than the finish 
call because it does not require any application thread to wait for the completion of graphics commands. 
The following code demonstrates how the example scene may be drawn using the wait command: Thread1 Thread2 
 DrawPrimitives(opaq[1..256]) DrawPrimitives(opaq[257..512]) appBarrier(appBarrierVar) appBarrier(appBarrierVar) 
glpWaitContext(Thread2Ctx) DrawPrimitives(tran[1..256]) appBarrier(appBarrierVar) appBarrier(appBarrierVar) 
 glpWaitContext(Thread1Ctx) DrawPrimitives(tran[257..512]) Intergraph uses a different mechanism to 
provide the same ef­fect as the wait call [13]. Due to the underlying implementation of the single graphics 
port, returning from a flush call guarantees that all of a context s primitives will be drawn before 
any subse­quent primitives from any other context. While similar in spirit to the wait call, this mechanism 
does not scale very well to systems with multiple ports because of its underlying broadcast require­ment. 
The wait construct uses point-to-point communication. 4.3 Synchronization Constructs While the wait 
command provides an improvement over the finish command, a large problem remains: the synchronization 
of the graphics streams is done by the application threads. Conse­quently, application threads are forced 
to wait. But why should an application thread wait when it could be doing something more useful? For 
example, in the code of Section 4.2, the first thread must issue its entire half of the transparent primitives 
before the second thread can begin issuing its half; thus, the second thread is forced to wait. Every 
time an explicit ordering is needed between primitives from different threads, the interface essentially 
de­grades to a serial solution. The answer to this problem is the key idea of our parallel API: synchronization 
that is intended to synchronize graphics streams should be done between graphics streams not between 
applica­tion threads. To this end, we introduce a graphics barrier com­mand into the API. As with other 
API calls, the application thread merely issues the barrier command, and the command is later executed 
within the graphics system. Thus, the blocking associated with the barrier is done on graphics contexts, 
not on the application threads. The code below achieves our primary objective the parallel issue of explicitly 
ordered primitives: both application threads may execute this code without ever blocking if the graphics 
system provides sufficient buffering. Thread1 Thread2 DrawPrimitives(opaq[1..256]) DrawPrimitives(opaq[257..512]) 
glpBarrier(glpBarrierVar) glpBarrier(glpBarrierVar) DrawPrimitives(tran[1..256]) glpBarrier(glpBarrierVar) 
glpBarrier(glpBarrierVar) DrawPrimitives(tran[257..512]) We see the utility of the barrier primitive 
in the above code example, but what other synchronization primitives provide useful semantics within 
the realm of a parallel graphics interface? Whereas the barrier is an excellent mechanism for synchronizing 
a set of streams in lock-step, it is not the best mechanism for do­ing point-to-point synchronization. 
Borrowing from the field of concurrent programming, we find that semaphores provide an elegant solution 
for many problems [7]. Among them is a mecha­nism for signal-and-wait semantics between multiple streams. 
The specification of the barrier and semaphore commands can be found in Figure 1. As with texture data 
and display lists, the data associated with barriers and semaphores should be sharable be­tween contexts. 
Barriers and semaphores have been found to be good synchro­nization primitives in the applications we 
have considered. If found to be useful, other synchronization primitives can also be added to the API. 
It is important to note that the requirements for synchronization primitives within a graphics API are 
somewhat constrained. Because the expression of arbitrary computation through a graphics API is not feasible, 
a synchronization primi­tive s utility cannot rely on computation outside of its own set of predefined 
operations. For example, condition variables are not a suitable choice. Figure 2: A Simple Interactive 
Loop. Application compu­ tation and rendering are parallelized across slave threads, and a master thread 
coordinates per-frame operations.  5 USING THE PARALLEL GRAPHICS API In order to illustrate how the 
parallel graphics interface may be used to provide scalable command issue, we present two exam­ples. 
The first example is a generic interactive loop, and the sec­ond example is the marching cubes algorithm. 
5.1 Simple Interactive Loop Figure 2a shows a simple interactive loop expressed in a strictly ordered 
serial interface. The goal in this example is to parallelize the compute and draw stage. This can yield 
improved perform­ance in the host computation, the issue of the graphics commands, and the execution 
of graphics commands. For parallel issue, a master thread (Figure 2b) creates a num­ber of slave threads 
(Figure 2c) to help with the compute and draw stage. The master first issues a clear command and gets 
the user input. The application barrier ensures that the worker threads use the correct user input data 
for the rendering of each frame. This synchronizes the application threads, but not the graphics command 
streams. The slaves issue wait commands to ensure that the clear command issued by the master is executed 
first. The master is assured that the clear occurs first due to the strict ordering semantics of a single 
stream. After each thread issues its graphics commands, a graphics barrier is issued to re­strict the 
swap operation to occur only after all the graphics streams have finished drawing their share of the 
frame. Finally, a finish operation is needed to ensure that the image is completed and displayed before 
getting user input for the next frame. The finish itself is a context-local operation which only guarantees 
that all previous commands issued by the master are completed. However, in conjunction with the graphics 
barrier, the finish guarantees that the commands of the slaves are also completed. 5.2 Marching Cubes 
As a more demanding example, we consider the marching cubes algorithm [16]. Marching cubes is used to 
extract the polygonal approximation of an isosurface of a function which is sampled on a 3D grid. The 
grid is divided into a set of cells, and each cell is composed of one or more voxels. In Figure 3a, we 
present a sim­plification of marching cubes to 2D. The mechanics of surface extraction and rendering 
are abstracted as ExtractAndRender. ExtractAndRender operates on a single cell of the grid independ­ently. 
If any portion of the desired isosurface lies within the cell, polygons approximating it are calculated 
and issued to the graph-Figure 3: Parallel Marching Cubes. As the rendering of a cell completes, glpVSema 
operations are performed by the graphics context to release dependent neighboring cells closer to the 
eye. The rendering commands of the white cells are blocked on glpPSema operations which are waiting for 
the rendering of adjacent or more distant cells. MarchSerialOrdered (M, N, grid) for (i=0; i<M; i++) 
for (j=0; j<N; j++) ExtractAndRender(grid[i, j]) (a) MarchParallel (M, N, grid) for (i=0; i<M; i++) for 
(j=(myProc+i)%numProcs; j<N; j+=numProcs) ExtractAndRender(grid[i,j]) (b) MarchParallelOrdered (M, N, 
grid, sema) for (i=0; i<M; i++) for (j=(myProc+i)%numProcs; j<N; j+=numProcs) if (i>0) glpPSema(sema[i-1,j]) 
if (j>0) glpPSema(sema[i,j-1]) ExtractAndRender(grid[i,j]) if (i<M-1) glpVSema(sema[i,j]) if (j<N-1) 
glpVSema(sema[i,j]) (c) Completed Ready Blocked glpVSema (outstanding) eye glpPSema i,j i j (d) ics 
system immediately. Due to the grid structure, it is fairly sim­ple to perform the traversal in back-to-front 
order based on the current viewpoint, thus eliminating the need for depth buffering and allowing for 
alpha-based translucency. In our example, this corresponds to traversing the grid in raster order. Due 
to the independence of the processing of different cells, marching cubes is easily parallelized. In Figure 
3b, traversal is parallelized by interleaving the cells across processing elements. Unfortunately, this 
simple approach sacrifices back-to-front or­dering. Figure 3d illustrates the dependence relationships 
be­tween cells and their neighbors which must be obeyed in the or­dered drawing of primitives. These 
dependencies can be ex­pressed directly using semaphores injected into the graphics command streams. 
Such an implementation is shown in Figure 3c. Before processing a cell, a thread issues two P operations 
to constrain the rendering of a cell to occur after rendering of its two rear neighbor cells. After processing 
the cell, it issues two V op­erations to signal the rendering of its other neighbors. Note that the dependencies 
and traversal order given here are non-ideal; more efficient (and more complicated) approaches are possible. 
  6 IMPLEMENTATION In order to test the viability of the parallel API extensions, we have implemented 
a software graphics library which is capable of handling multiple simultaneous graphics contexts. The 
name of this implementation is Argus, and the performance achieved with the parallel API using this system 
demonstrates the utility and feasibility of the ideas presented in this paper.  6.1 Argus Argus is a 
shared memory multiprocessor graphics library which was designed to serve as a test-bed for various studies 
in graphics architecture. Argus implements a subset of OpenGL as well as the parallel API extensions. 
At the heart of Argus is a lightweight multiprocessor threads package. We implement a graphics archi­tecture 
by allocating a thread for each processing node in the ar­chitectural design. A custom scheduler is used 
to schedule these threads onto system processors appropriately. Furthermore, if a system processor running 
application code is blocked for some reason due to the graphics (e.g., a buffer fills up or a glFinish 
is pending), the threads package will run graphics threads on the otherwise idle application processor. 
There are three basic types of threads in the serial API version of Argus. An application thread runs 
application code and man­ages the graphics context. A geometry thread transforms and shades the primitives 
encoded in the graphics instruction stream. A rasterization thread is responsible for drawing these trans­formed 
primitives into the framebuffer. The version of Argus that implements the serial API is a sort-middle 
tiled parallel graphics system [17]. Graphics commands from a single application thread fill a global 
command queue which is drained by many geometry threads. The number of geometry threads is scalable since 
the data in this global command queue can be read in parallel. Of course, the geometry threads must synchronize 
at a single point of contention in order to distribute the work in the queue amongst themselves, but 
because the contention is amortized over a large number of primitives, this cost is insignificant in 
our implementa­tion. After the appropriate computation, the geometry threads distribute the transformed 
primitives among the appropriate tile rasterizers. Though the details are beyond the scope of this paper, 
reorder buffers in front of each rasterizer are used to maintain the ordering found in the global command 
queue across the rasteriz­ers. Since each tile rasterizer is responsible for a contiguous por­tion of 
the screen, no one rasterizer needs to see all of the primi­tives; thus, the rasterization architecture 
is scalable. Argus sup­ports a variety of schemes for load balancing tile rasterization. For the results 
presented here, we used distributed task queues with stealing. The version of Argus that implements the 
parallel API extends the serial API architecture to allow multiple simultaneous graph­ics streams. Each 
application thread is augmented by a local command queue and a synchronization thread. Instead of enter­ing 
graphics commands onto the global command queue, each application thread fills its local command queue. 
The synchroni­zation thread is then responsible for transferring commands from this local command queue 
onto the global command queue. Since the global command queue may be written in parallel, the archi­tecture 
is scalable. The box on the adjacent page describes the pipeline in greater detail and explains how state 
management and synchronization commands are implemented within Argus.  6.2 Performance Because poor 
performance often hides architectural bottlenecks, Argus was designed with performance as one of its 
main criteria. Although Argus can run on many architectures, particular care was taken to optimize the 
library for the Silicon Graphics Origin system [15]. The Origin is composed of 195 MHz R10000 proc­essors 
interconnected in a scalable NUMA architecture. De­pending on the rendering parameters, the single processor 
version of Argus is able to render up to 200K triangles per second; this rendering rate scales up to 
24 processors. In its original incarna­tion, Argus was designed for a serial interface and many serial 
applications were not able to keep up with the scalable perform­ance of the graphics system. Remedying 
this situation led us to the development of the parallel API. To study the performance of our parallel 
API implementation, we ran two applications: Nurbs and March. Nurbs is an immedi­ate-mode patch tessellator 
parallelized by distributing the individ­ual patches of a scene across processors in a round-robin manner. 
By tessellating patches on every frame, the application may vary the resolution of the patches interactively, 
and because depth buffering is enabled, no ordering constraints are imposed in the drawing of the patches 
synchronization commands are utilized only on frame boundaries. Our second application, March, is a parallel 
implementation of the marching cubes algorithm [16]. By extracting the isosurface on every frame, the 
application may choose the desired isosurfaces interactively. Rendering is per­formed in back-to-front 
order to allow transparency effects by issuing graphics semaphores which enforce the dependencies described 
in Section 5.2. One noteworthy difference between our implementation and the one outlined in Section 
5.2 is that cells are distributed from a centralized task queue rather than in round­robin order because 
the amount of work in each cell can be highly unbalanced. The input characteristics and parameter settings 
used with each of these applications are shown below: Nurbs March armadillo dataset skull dataset 102 
patches 256K voxels (64x64x64) 196 control points per patch cell size at 16x16x16 117504 stripped triangles 
53346 independent triangles 1200x1000 pixels 1200x1000 pixels Figure 4a and Figure 4b show the processor 
speedup curves for Nurbs and March, respectively. The various lines in the graph represent different 
numbers of application threads. The serial application bottleneck can be seen in each case by the flattening 
of the "1 Context" curve: as more processors are utilized, no more performance is gained. Whereas the 
uniprocessor version of Nurbs attains 1.65 Hz, and the serial API version is limited to 8.8 Hz, the parallel 
API version is able to achieve 32.2 Hz by using four contexts. Similarly, the uniprocessor version of 
March gets 0.90 Hz, and the serial API version of March is limited to 6.3 Hz, but the parallel API version 
is able to attain 17.8 Hz by utilizing three contexts. These speedups show high processor utilization 
and highlight the implementation s ability to handle extra con­texts gracefully. One extension to Argus 
that we have been considering is the use of commodity hardware for tile rasterization. Although this 
introduces many difficulties, it also increases rasterization rate significantly. In order to simulate 
the effects of faster rasteriza­tion on the viability of the parallel API, we stress the system by running 
Argus in a simulation mode which imitates infinite pixel fill rate. In this mode, the slope calculations 
for triangle setup do occur, as does the movement of the triangle data between the geometry processors 
and the tile rasterizers. Only the rasteriza­tion itself is skipped. The resulting system increases the 
through­put of Argus and stresses the parallel API: Figure 4c and Figure 4d show how a greater number 
of contexts are required to keep up with the faster rendering rate. The parallel API allows Argus to 
achieve peak frame rates of 50.5 Hz in Nurbs and 40.9 Hz in March. This corresponds to 5.9 million stripped 
triangles per second in Nurbs and 2.2 million independent triangles per second in March. These rates 
are approximately double the rate at which a single application thread can issue primitives into Argus 
even Figure 4: Performance Graphs. The speedup curves for two applications, Nurbs and March, are drawn 
in (a) and (b) for a varying numbers of contexts using the Argus graphics library. These same graphs 
are plotted for a version of Argus which assumes infinite fill rate in (c) and (d). The effects of buffering 
are illustrated in (e), and the effects of synchronization granularity are demon strated in (f).  when 
no application computation is involved, thus demonstrating the importance of multiple input ports. Again, 
the graphs illus­ trate Argus s capability of handling extra contexts without per­formance penalties. 
One important aspect of any implementation of the parallel API is the amount of buffering required to 
make the API work. Without enough buffering, the parallel API serializes: in Argus, if a local command 
queue fills up before its synchronization com­mands are resolved, the application thread is forced to 
wait. In­tuitively, we expect the amount of buffering required to be sensi­tive to the amount of synchronization 
between different threads. This is quantified in the speedup curves of Figure 4e for 24 proc­essors. 
The number of entries in the local command queue (which can each point to a 16 KB block of primitive 
commands or hold a single synchronization command) was varied from 1 to 256. The runs were performed 
on the March application with the sema­phores both enabled (the solid Ordered lines) and disabled (the 
dotted Unordered lines). As one would expect, the ordered version requires significantly larger buffers. 
Another key aspect of any parallel API implementation is its ability to minimize the cost of synchronization. 
If the granularity of the application is too fine, synchronization costs can dominate, and the application 
is forced to use a coarser subdivision of work. If the work is subdivided too coarsely, load imbalance 
can occur within the application. The effects of granularity on Argus were tested by varying the dimensions 
of the cells on both the ordered and unordered versions of March. The number of processors was held at 
24 and timings were taken with varying numbers of con­texts, as illustrated in Figure 4f. A granularity 
which is too fine deteriorates performance in both the application (as demonstrated by the unordered 
runs) as well as in the graphics system (as dem­onstrated by the extra performance hit taken by the ordered 
runs). For the March application, there is a wide range of granularities (well over an order of magnitude 
in the number of voxels) that work well since Argus was designed to keep the cost of synchro­nization 
low. When March is run without isosurface extraction and rendering (i.e., nothing but the synchronization 
primitives are issued), several hundred thousand semaphore operations are re­solved per second.  7 
DISCUSSION Argus is one implementation of the parallel API which performs well. Obviously, the architecture 
embodied by Argus is not the only possible choice, and it is instructive to examine the design considerations 
of alternative implementations due to the special architectural requirements imposed by the extensions. 
7.1 Consistency and Synchronization Until now, we have not said much about how the operations of the 
parallel API can be interleaved. Supporting multiple contexts that share a framebuffer means that the 
system must provide a consistency model. We borrow the notion of sequential consis­tency from the field 
of computer architecture [14]. Imagine a system consisting of multiple processes simultaneously perform­ing 
atomic operations. A sequentially consistent system computes a result that is realizable by some serial 
interleaving of these atomic operations. By making a single API command be the level of apparent atomicity, 
we define the notion of command­sequential consistency, the strongest form of consistency possible within 
the parallel API. At the other end of the spectrum is framebuffer-sequential consistency only framebuffer 
accesses are atomic. A whole spectrum of consistency models can be enumerated in such a fashion. The 
OpenGL specification does not require an implementation to support any consistency model. In order to 
support the parallel API, however, a graphics system should provide at least fragment-sequential consistency 
in order to support features which depend on an atomic read-modify-write operation on the framebuffer 
(such as depth buffering). The consistency model which an architecture supports is re­lated to the location 
in the pipeline where synchronization con­straints between graphics streams are resolved. The Argus pipe­line 
described in Section 6.1 synchronizes and merges multiple graphics streams early in the pipeline, thus 
supporting command­sequential consistency. One problem with such an architecture is that geometry processing 
cannot occur on primitives which are blocked due to synchronization constraints. Another problem is that 
ordering dependencies not required by the synchronization commands are introduced early in the pipeline 
at the global com­mand queue. An alternate architecture addresses these problems by merging graphics 
streams at the rasterizers, thus supporting fragment­sequential consistency. We implemented such an alternate 
ver­sion of Argus in which the entire pipeline up to and including the tile rasterization threads is 
replicated for each context. Every tile thread executes every synchronization command, and threads which 
share the same tile merge their streams by obtaining exclu­sive access to the tile. One disadvantage 
of this approach is the extra buffering requirements due to the fact that the size of the graphics data 
expands as it gets farther down the pipeline. An­other problem with this alternate approach is the high 
cost of synchronization since synchronization commands must be exe­cuted by every tile rasterizer this 
proved prohibitively expensive in the framework of Argus. Of course, many architectures other than the 
two we tried are possible, and an architect should evalu­ate the effects of a proposed architecture on 
these same issues. 7.2 Architectural Requirements While a graphics system which implements the parallel 
API is in many respects similar to one which implements a serial API, an architecture should take special 
care in addressing three particular areas. First, the architecture must have a mechanism that effi­ciently 
handles multiple simultaneous input streams. Second, the state management capabilities of the architecture 
must be able to handle multiple simultaneous graphics states. And third, the rasterization system must 
be able to handle texture data for multi­ple streams efficiently. In designing current systems, graphics 
architects have gone to great lengths to allow the seamless sharing of the graphics hard­ware between 
multiple windows by significantly reducing the context switch time. Although this same mechanism can 
be used for the parallel API, the context switch time must be reduced even further in order to handle 
multiple input streams at a much finer granularity. Argus does this by making use of a thread library 
which can switch threads in less than a microsecond as well as allowing multiple input ports. A hardware 
system could allow multiple input ports by replicating command processors. Ideally, each of the command 
processors could handle either a single graphics stream at a high rate or multiple graphics streams at 
lower rates. This would result in peak performance on serial ap­plications and good performance on highly 
parallel applications. The parallel API imposes special requirements on the handling of state. In past 
architectures, state changes have been expensive due to pipeline flushing. Recent graphics architectures, 
however, have taken measures to allow large numbers of state changes [19]. To a first order, the number 
of state changes for a given scene as issued by one application thread is the same as the number of state 
changes for the same scene as issued by multiple application threads since the number of inherent state 
changes in a scene is constant. However, the parallel API increases the amount of state that has to be 
accessible to the different portions of the graphics system: the various graphics processors must be 
able to switch between the states of different graphics streams without dramati­cally affecting performance. 
Hardware implementations which allow for multiple simultaneous contexts have already been dem­onstrated 
[13, 26]. In Argus, multiple simultaneous contexts are handled efficiently by taking advantage of state 
coherence in the state management algorithm through the use of shared memory and processor caching. One 
type of state which requires special attention is texture. Unlike the rest of the state associated with 
a context (with the exception of display lists), texture state can be shared amongst multiple contexts, 
thus exposing the need for efficient download of and access to shared texture data. The semantics of 
texture download are the same as all other graphics commands: it is sus­ceptible to buffering, and synchronization 
must occur to guarantee its effects from the point of view of other contexts. Efficient im­plementations 
of synchronized texture download can be realized by extending the idea of the texture download barrier 
found in the SGI InfiniteReality [19]. The access of texture memory may also require special care. Since 
hardware systems have a limited amount of local texture memory, applications issue primitives in an order 
which exploits texture locality. The parallel API can reduce this locality since the rasterizers can 
interleave the ren­dering of several command streams. In architectures which use implicit caching [4, 
10], the effectiveness of the cache can possi­bly be reduced. In architectures which utilize local texture 
mem­ory as an explicit cache, texture management is complicated. In Argus, shared texture download is 
facilitated by shared memory, and locality of texture access is provided by the caching hardware.  8 
FUTURE WORK The parallel API provides a new paradigm for writing parallel graphics applications. Many 
graphics algorithms exist that need an immediate-mode interface but are limited by application com­putation 
speed (e.g., [11, 24]), and parallelizing them can help greatly. There are two other uses of the parallel 
API which are of special interest. Scene graph libraries such as Performer [22] are parallel applications 
which traverse, cull, and issue scenes on multiple processors. Pipeline parallelism is used to distribute 
different tasks among different processors, but Performer is lim­ited on most applications by the single 
processor responsible for the issuing of the graphics commands. The parallel API can be used to write 
such libraries in a homogeneous, scalable fashion. A second novel use of the parallel API is to write 
a compiler that can automatically parallelize the graphics calls of a serial graphics application. Recent 
advances in compiler technology allow automatic parallelization of regular serial applications [1], and 
extending this work to encompass graphics applications would be an interesting research direction. Another 
significant step in validating the parallel API is im­plementing an architecture with hardware acceleration. 
While Argus is an excellent software system for studying the issues in the design of the parallel API, 
its performance is limited by poor rasterization speed, especially when texturing is enabled. One possible 
architecture consists of implementing the parallel API on a cluster of interconnected PCs with rasterization 
hardware. An­other possibility is to extend the basic sort-middle interleaved architecture [17] of a 
high-end system such as the SGI InfiniteRe­ality [19]. Though this task is by no means easy, we believe 
that such a system is feasible with the techniques described in Section 7.2. The parallel API can also 
be implemented by a variety of other, more exotic architectures. Image composition architectures such 
as PixelFlow [8, 18] are one class of rendering architectures that have not been addressed by the parallel 
API. Because these machines are not designed to do ordered drawing of primitives, the parallel API needs 
to be extended to allow a relaxed ordering model in which the require­ment of drawing in strict order 
can be enabled and disabled. 9 CONCLUSION We have designed a parallel immediate-mode graphics interface. 
By introducing synchronization commands into the API, ordering between multiple graphics streams can 
be explicitly constrained. Since synchronization is done between graphics streams, an ap­plication thread 
is able to continue issuing graphics commands even when its graphics stream is blocked. The API provides 
a natural paradigm for parallel graphics applications that can be used in conjunction with the existing 
retained-mode constructs of an immediate-mode interface. The feasibility of this API has been demonstrated 
by a sample implementation which provides scal­able performance on a 24 processor system.  Acknowledgements 
We would like to thank Matthew Eldridge, Kekoa Proudfoot, Milton Chen, John Owens, and the rest of the 
Stanford Graphics Lab for their insights about this work. We thank Kurt Akeley and the anonymous reviewers 
for their helpful comments in revising the paper. We thank Dale Kirkland for describing the parallel 
interface used by Intergraph. For support, we thank Silicon Graphics, Intel, and DARPA contract DABT63-95-C-0085-P00006. 
For machine time, we thank Chris Johnson and the Uni­versity of Utah. And finally, we thank our loved 
ones. References [1] S. Amarasinghe, J. Anderson, C. Wilson, S. Liao, B. Murphy, R. French, M. Lam, 
and M. Hall. Multiprocessors from a Software Perspective. IEEE Micro, 16:3, pages 52-61, 1996. [2] A. 
Beers, M. Agrawala, and N. Chaddha. Rendering from Com­pressed Textures. Computer Graphics (SIGGRAPH 
96 Proceed­ings), volume 30, pages 373-378, 1996. [3] J. Blinn. Me and My (Fake) Shadow. IEEE Computer 
Graphics and Applications, 8:1, pages 82-86, 1988. [4] M. Cox, N. Bhandari, and M. Shantz. Multi-Level 
Texture Cach­ing for 3D Graphics Hardware. Proceedings of the 25th Interna­tional Symposium on Computer 
Architecture, 1998. [5] T. Crockett. Design Considerations for Parallel Graphics Librar­ies. Proceedings 
of the Intel Supercomputer Users Group 1994, 1994. [6] M. Deering. Geometry Compression. Computer Graphics 
(SIGGRAPH 95 Proceedings), volume 29, pages 13-20, 1995. [7] E. Dijkstra. Cooperating Sequential Processes. 
Programming Languages, pages 43-112, 1968. [8] J. Eyles, S. Molnar, J. Poulton, T. Greer, A. Lastra, 
N. England, and L. Westover. PixelFlow: The Realization. Proceedings of the 1997 SIGGRAPH/Eurographics 
Workshop on Graphics Hard­ware, pages 57-68, 1997. [9] J. Gettys and P. Karlton. The X Window System, 
Version 11. Software Practice and Experience, 20:S2, pages 35-67, 1990. [10] Z. Hakura and A. Gupta. 
 The Design and Analysis of a Cache Ar­chitecture for Texture Mapping. Proceedings of the 24th Interna­tional 
Symposium on Computer Architecture, 1997. [11] H. Hoppe. View-Dependent Refinement of Progressive Meshes. 
Computer Graphics (SIGGRAPH 97 Proceedings), volume 31, pages 189-198, 1997. [12] M. Kilgard. OpenGL 
Programming for the X Window System, Addison-Wesley, 1996. [13] D. Kirkland. Personal Communication. 
Intergraph Corp., 1998. [14] L. Lamport. How to Make a Multiprocessor Computer that Cor­rectly Executes 
Multiprocess Programs. IEEE Transactions on Computers, 28:9, pages 241-248, 1979. [15] J. Laudon and 
D. Lenoski. The SGI Origin: A ccNUMA Highly Scalable Server. Proceedings of the 24th Annual Symposium 
on Computer Architecture, 1997. [16] W. Lorensen and H. Cline. Marching Cubes: A High-Resolution 3D 
Surface Reconstruction Algorithm. Computer Graphics (SIGGRAPH 87 Proceedings), volume 21, pages 163-169, 
1987. [17] S. Molnar, M. Cox, D. Ellsworth, and H. Fuchs. A Sorting Classi­fication of Parallel Rendering. 
IEEE Computer Graphics and Ap­plications, 14:4, pages 23-32, 1994. [18] S. Molnar, J. Eyles, and J. Poulton. 
 PixelFlow: High-Speed Ren­dering Using Image Composition. Computer Graphics (SIGGRAPH 92 Proceedings), 
volume 26, pages 231-240, 1992. [19] J. Montrym, D. Baum, D. Dignam, and C. Migdal. InfiniteReality: 
A Real-Time Graphics System. Computer Graphics (SIGGRAPH 97 Proceedings), volume 31, pages 293-302, 1997. 
[20] J. Neider, T. Davis, and M. Woo. OpenGL Programming Guide. Addison-Wesley, 1993. [21] T. Porter 
and T. Duff. Compositing Digital Images. Computer Graphics (SIGGRAPH 84 Proceedings), volume 18, pages 
253­259, 1984. [22] J. Rohlf and J. Helman. IRIS Performer: A High Performance Multiprocessing Toolkit 
for Real-Time 3D Graphics. Computer Graphics (SIGGRAPH 94 Proceedings), volume 28, pages 381­395, 1994. 
[23] R. Scheifler and J. Gettys. The X Window System. ACM Trans­actions on Graphics, 5:2, pages 79-109, 
1986. [24] T. Sederberg and S. Parry. Free-Form Deformation of Solid Geo­metric Models. Computer Graphics 
(SIGGRAPH 86 Proceed­ings), volume 20, pages 151-160, 1986. [25] J. Torborg and J. Kajiya. Talisman: 
Commodity Real-Time 3D Graphics for the PC. Computer Graphics (SIGGRAPH 96 Pro­ceedings), volume 30, 
pages 57-68, 1996. [26] D. Voorhies, D. Kirk, and O. Lathrop. Virtual Graphics. Com­puter Graphics (SIGGRAPH 
88 Proceedings), volume 22, pages 247-253, 1988.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280855</article_id>
		<sort_key>151</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[The clipmap]]></title>
		<subtitle><![CDATA[a virtual mipmap]]></subtitle>
		<page_from>151</page_from>
		<page_to>158</page_to>
		<doi_number>10.1145/280814.280855</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280855</url>
		<keywords>
			<kw><![CDATA[clipmap]]></kw>
			<kw><![CDATA[image exploitation]]></kw>
			<kw><![CDATA[load management]]></kw>
			<kw><![CDATA[mipmap]]></kw>
			<kw><![CDATA[terrain visualization]]></kw>
			<kw><![CDATA[texture]]></kw>
			<kw><![CDATA[visual simulation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Graphics processors</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Graphics packages</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010389</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics processors</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P47945</person_id>
				<author_profile_id><![CDATA[81100035850]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Christopher]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Tanner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Silicon Graphics Computer Systems]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P47813</person_id>
				<author_profile_id><![CDATA[81100332770]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Christopher]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Migdal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Silicon Graphics Computer Systems]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14193767</person_id>
				<author_profile_id><![CDATA[81332507321]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[T.]]></middle_name>
				<last_name><![CDATA[Jones]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Silicon Graphics Computer Systems]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cosman, Michael. Global Terrain Texture: Lowering the Cost. In Eric G. Monroe, editor, Proceedings of 1994 IMAGE VII Conference, pages 53-64. The IMAGE Society, 1994.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192262</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Rohlf, John and James Helman. IRIS Performer: A High Performance Multiprocessing Toolkit for Real-Time 3D Graphics. In Andrew Glassner, editor, SIGGRAPH 94 Conference Proceedings. Annual Conference Series, pages 381-394. ACM SIGGRAPH, Addison Wesley, July 1994. ISBN 0-89791-667-0.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258871</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Montrym, John S, Daniel R Baum, David L Dignam and Christopher J Migdal. InfiniteReality: A Real-Time Graphics System. In Turner Whitted, editor, SIGGRAPH 97 Conference Proceedings. Annual Conference Series, pages 293-301. ACM SIGGRAPH, Addison Wesley, August 1997. ISBN 0-89791-896-7.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Sanz-Pastor, Nacho and Luis Barcena. Computer Arts &amp; Development, Madrid, Spain. Private communication.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Walker, Chris, Nancy Cam, Jon Brandt and Phil Keslin. Image Vision Library 3.0 Programming Guide. Silicon Graphics Computer Systems, 1996.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801126</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Williams, Lance. Pyramidal Parametrics. In Peter Tanner, editor, Computer Graphics (SIGGRAPH 83 Conference Proceedings), volume 17, pages 1-11. ACM SIGGRAPH, July 1983. ISBN 0-89791-109-1.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Clipmap: A Virtual Mipmap Christopher C. Tanner, Christopher J. Migdal, and Michael T. Jones Silicon 
Graphics Computer Systems Copyright &#38;#169;1998 by the Association for Computing Machinery, Inc. 
Permission to make digital or hard copies of part or all of this work for personal or classroom use is 
granted without fee provided that copies are not made or distributed for profit or commercial advantage 
and that copies bear this notice and the full citation on the first page. Copyrights for components of 
this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, 
to republish, to post on servers or to redistribute to lists, requires specific permission and/or a fee. 
ABSTRACT We describe the clipmap, a dynamic texture representation that ef.ciently caches textures of 
arbitrarily large size in a .nite amount of physical memory for rendering at real-time rates. Fur­ther, 
we describe a software system for managing clipmaps that supports integration into demanding real-time 
applications. We show the scale and robustness of this integrated hardware/software architecture by reviewing 
an application virtualizing a 170 gigabyte texture at 60 Hertz. Finally, we suggest ways that other rendering 
systems may exploit the concepts underlying clipmaps to solve related problems. CR Categories and Subject 
Descriptors: I.3.1 [Computer Graphics]: Hardware Architecture Graphics Processors; I.3.3 [Computer Graphics]: 
Picture/Image Generation Display Algo­rithms; I.3.4 [Computer Graphics]: Graphics Utilities Graphics 
Packages; I.3.7 [Computer Graphics]: Three-Dimensional Graph­ics and Realism Color, shading, shadowing, 
and texture. Additional Keywords: clipmap, mipmap, texture, image exploi­tation, terrain visualization, 
load management, visual simulation. 1 INTRODUCTION Textures add important visual information to graphical 
applica­tions. Their scope, .delity, and presentation directly affects the level of realism and immersion 
achievable for a given object or environment. From being able to discern each brush stroke in every mural 
in a virtual Vatican to counting tire tracks in the sand half­way around a simulated globe, the amount 
of visual information applications require is growing without bound. It is this enormous visual dynamic 
range that illuminates the limitations of current texture representations and de.nes our problem space. 
Speci.cally, our representation addresses all of the issues rele­vant to the real-time rendering of the 
earth s surface as a single high resolution texture. Representing the earth with one meter tex­els requires 
a 40 million by 20 million texel texture and an overall square, power of two mipmap size of approximately 
11 petabytes. We identi.ed six goals for an effective solution to this problem. First, the new texture 
system must support full speed rendering using a small subset of an arbitrarily large texture. Second, 
it must be possible to rapidly update this subset simultaneously with real­time rendering. Third, the 
texture technique must not force subdi­vision or other constraints onto geometric scene components. Fourth, 
load control must be robust and automatic to avoid distract­ing visual discontinuities under overload. 
Fifth, it must be possible for the representation to be seamlessly integrated into existing applications. 
Finally, the incremental implementation cost should be small relative to existing hardware. Author Contacts: 
{cct|migdal|mtj}@sgi.com Our initial clipmap implementation addresses these goals through a combination 
of low-level hardware and higher-level sys­tem level software. The hardware provides real-time rendering 
capabilities based on the clipmap representation, while the soft­ware manages the representation and 
interfaces with applications. This paper describes our clipmap implementation and reviews how well it 
meets the goals and challenges already outlined: §2 presents past approaches to managing large texture 
images, §3 describes exactly what a clipmap is and what it achieves, §4 explains how the clipmap is updated 
and addresses memory bandwidth issues, §5 shows how the clipmap representation is a modi.cation of mipmap 
rendering, §6 describes how clipmaps are generalized to overcome hardware resource and precision limits, 
§7 discusses the higher­level software used to update clipmaps, manage system load, and deal with data 
on disk, §8 examines several applications of the sys­tem, and .nally, §9 considers the success of our 
.rst implementa­tion and suggests directions for further research. 2 PREVIOUS APPROACHES The common 
method for dealing with large textures requires subdividing a huge texture image into small tiles of 
sizes directly supportable by typical graphics hardware. This approach provides good paging granularity 
for the system both from disk to main memory and from main memory to texture memory. In practice, however, 
this approach has several drawbacks. First, geometric primitives must not straddle texture-tile boundaries, 
forcing unwanted geometric subdivision. Second, texture border support is required for each level of 
each tile if correct sampling is to be per­formed at tile boundaries. Lastly, the level of detail mechanisms 
take place at the granularity of the tiles themselves producing disconcerting visual pops of whole texture/geometry 
tiles. These limitations limit visual effectiveness and add an extra level of com­plexity to geometric 
modeling, morphing, and LOD de.nition all of which must take the texture tile boundaries into account. 
The higher-quality system of Sanz-Pastor and Barcena [4] blends multiple active textures of differing 
resolutions. Each of these lev­els roams based on its dynamic relationship to the eyepoint; poly­gons 
slide from one texture to another with textures closer to the eyepoint at higher-resolution. The drawback 
of such a system is that geometry is still tied directly to one of these textures so texture LOD decisions 
are made at the per-polygon rather than per-pixel. Developers must also obey a complex algorithm to subdivide 
geometry based on the boundaries between the different textures of different resolutions. Further, texture 
LOD choices made by the algorithm are based on coarse eyepoint range rather than the .ne display space 
projection of texel area. An advanced solution outlined by Cosman [1] offers per-pixel LOD selection 
in a static multiresolution environment. Although similar to our clipmap approach in some ways, Cosman 
does not address look-ahead caching, load control, or the virtualization of the algorithm beyond hardware 
limits. Although these approaches have solved large scale problems, they do not appear generalizable 
to fully address all six goals of §1. 3 THE CLIPMAP REPRESENTATION 3.1 Observations about Mipmaps The 
following review summarizes the key mipmap concepts on which clipmaps are based. A mipmap as de.ned by 
Williams [6] is a collection of correlated images of increasingly reduced resolu­tion arranged, in spirit, 
as a resolution pyramid. Starting with level 0, the largest and .nest level, each lower level represents 
the image using half as many texels in each dimension: 1D = 1/2, 2D = 1/4, and 3D = 1/8 the texels. The 
2D case is illustrated in Figure 1. Figure 1: Mipmap Image Pyramid and Side-View Diagram When rendering 
with a mipmap, pixels are projected into mip­map space using texture coordinates, underlying geometric 
posi­tions, geometric transformations, and texture transformations to de.ne the projection. Each rendered 
pixel is derived from one or more texel samples taken from one or more levels of the mipmap hierarchy. 
In particular, the samples chosen are taken from the immediate neighborhood of the mipmap level where 
the display s pixel-to-texel mapping is closest to a 1:1 mapping, a level dictated in part by display 
resolution. The texels are then .ltered to produce a single value for further processing. Given this 
simple overview of mipmap processing, we now ask which texels within a mipmap might be accessed during 
the ren­dering of an image. Clearly there can be many variations in the fac­tors that control pixel to 
display projection, such as differing triangle locations within a large database, so it may seem that 
all of the texels are potentially used if the universe of possible geometry is taken into consideration. 
Refer to Figure 2 for an illustration of the relationship between eyepoint position and texel access. 
 Eyepoint Near Eyepoint Far Eyepoint Oblique Figure 2: Texel Access within a Mipmap Mipmaps are potentially 
fully accessed during rendering when their size in texels is less than twice the size of the maximum 
dis­play extent in pixels. This observation derives from interactions between mipmap sample selection 
logic and .nite display resolu­tion. For example, when rendering a 327682 texel mipmap to a 10242 pixel 
display, the mipmap sample-selection logic will choose the texels that are closest to having a 1:1 mapping 
to pixel area. Thus it will use at most 10242 texels from a level before accessing an adjacent level. 
Implementations that blend samples from adjacent levels (as in trilinear .ltering) potentially access 
twice as many texels in each dimension. For an example of this refer to the center diagram in Figure 
2, where texels from level 1 are being fully used. If the eyepoint were just slightly closer, then samples 
would be blended from both level 0 and level 1, but 10242 texels from level 1 would still be accessible. 
The corresponding texels from level 0 needed to blend with these 10242 level 1 texels are distributed 
over a 20482 texel extent in level 0. When the indexing arithmetic for very large mimpaps is ana­lyzed, 
it becomes clear that the majority of the mipmap pyramid will not be used in the rendering of a single 
image no matter what geometry is rendered. The basis of our system is this realization that eyepoint 
and display resolution control access into the mip­map and that in the case of the very large textures 
we are con­cerned with, only a minute fraction of the texels in the mipmap are accessible. We can build 
hardware to exploit this fact by rendering from the minimal subset of the mipmap needed for each frame 
a structure we term a clipmap. 3.2 The Anatomy of a Clipmap A clipmap is an updatable representation 
of a partial mipmap, in which each level has been clipped to a speci.ed maximum size. This parameterization 
results in an obelisk shape for clipmaps as opposed to the pyramid of mipmaps. It also de.nes the size 
of the texture memory cache needed to fully represent the texture hierar­chy. 3.2.1 De.ning Clipmap Region 
with ClipSize ClipSize represents the limit, speci.ed in texels, of texture cache extent for any single 
level of a clipmap texture. Each level of a nor­mal mipmap is clipped to ClipSize if it would have been 
larger than ClipSize, as shown in Figure 3. All levels retain the logical size and render-time accessing 
arithmetic of the corresponding level of a full mipmap. Needed Clipped Clipped Figure 3: Clipmap Region 
within a Mipmap Based on this mipmap subset representation, we further de.ne the Clipmap Stack to be 
the set of levels that have been clipped from full mipmap-size by the limit imposed by ClipSize. These 
levels are not fully resident within the clipmap; only a ClipSize2 subset is cached. These levels are 
the topmost levels in Figure 4. Below the Clipmap Stack is the Clipmap Pyramid, de.ned as the set of 
levels of sizes not greater than the ClipSize limit. These lev­els are completely contained in texture 
memory and are identical to the corresponding portions of a full mipmap. ClipSize Clipmap Stack Clipmap 
Pyramid Figure 4: Clipmap Stack and Pyramid Levels  3.2.2 De.ning Clipmap Contents with ClipCenter Given 
the notion of clipping a mipmap to .t in a subset clipmap cache, we specify the data present in this 
cache by specifying a ClipCenter for each stack level. A ClipCenter is an arbitrary tex­ture space coordinate 
that de.nes the center of a cached layer. By de.ning ClipSize and ClipCenter for each level, we precisely 
select the texture region being cached by the ClipStack levels of our representation. One implementation 
is to specify the ClipCenter for stack level 0 and derive the ClipCenter of lower levels by shifting 
the center based on depth. This forces each level to be centered along a line from the level 0 center 
to the clipmap apex. This center is the dot indicated at the top of Figure 4. This type of centering 
yields con­centric rings of resolution surrounding the level 0 ClipCenter. The center location may be 
placed anywhere in full mipmap space. The image in Figure 5 shows an orthogonal view of a polygon (viewed 
from the Eyepoint Near position of Figure 2) that has been tex­tured with a clipmap having a very small 
ClipSize in order to dem­onstrate the concentric rings of texture resolution. When the cache is insuf.cient, 
due either to an overly small ClipSize or a poor choice of ClipCenter, the best available lower resolution 
data from lower in the clipmap stack or pyramid is used. This clip texture accesses to best available 
data nature is a sec­ond reason why we chose the name clipmap for this approach. In normal use, however, 
the ClipSize is set to equal or exceed the display size, in which case the rings of resolution shown 
in Figure 5 are large enough that, with a properly chosen ClipCenter, these rings form a superset of 
the needed precision. Thus, the subset nature of clipmaps does not limit the texture sample logic every 
texel addressed is present in the clipped mipmap and the resulting image will be pixel-exact with one 
drawn using the full mipmap, meeting the .rst of the goals outlined for this texture system. 3.2.3 Invalid 
Border for Parallel Update The previous subsection presents rendering from a Clipmap Stack level as a 
static subset of the complete mipmap level; it does not address the need to pre-page data needed for 
future frames con­currently with rendering. In order to provide storage for paged data and for load control, 
we introduce another parameter, the Invalid-Border, de.ned as a border of texels existing within each 
stack level that is InvalidBorder texels wide. Texels within the Invalid-Border are inactive and are 
not accessed by the texel-sample index­ing logic. They provide a destination region into which higher­level 
software prefetches data. The size of the active portion of a clip map is simply ClipSize minus twice 
the InvalidBorder, known as the EffectiveSize of each stack level. The relationship and posi­tion of 
the InvalidBorder and EffectiveSize are illustrated in Figure 6. InvalidBorder   Effective Size  
 Figure 6: Clipmap with InvalidBorder and EffectiveSize 3.2.4 TextureOffset for Ef.cient Update The 
.nal fundamental parameter of our low-level clipmap repre­sentation is the TextureOffset, the starting 
address used by the tex­ture address generator to locate the logical center of a clipmap s EffectiveSize 
within the ClipSize2 memory region allocated for each Clipmap Stack level. The addressing logic uses 
the Texture-Offeset and modular addressing to roam ef.ciently through the level s ClipSize2 memory region 
as discussed in detail in §4, where the process of clipmap updating and rendering is presented. This 
offset is speci.ed per level and affects addressing for the level exactly as a texture matrix translation 
with a wrap-style texture clamp mode.  3.3 Clipmap Storage Ef.ciency Consider a 16 level 327682 clipmap 
to be rendered on a 10242 display. We begin our analysis with pixel-exact rendering. Given the display 
size, we know that the upper bound on texture usage from a single level is 20482, so we set the ClipSize 
to 2048. There will be four clipped levels forming our Clipmap Stack and 12 lev­els in the Clipmap Pyramid. 
The storage required for this is 20482 texels*4levels+4/3* 20482 texels for the pyramid = 42.7 MB at 
2 bytes per texel. This perfect clipmap con.guration requires only 42.7 MB of the full 2.8 GB present 
in the complete mipmap. A more typical con.guration using a 1024 ClipSize will achieve attractive results 
using 10242*5 stack texels + 10242*4/3 pyramid texels = 12.7 MB. Finally, a 512 texel ClipSize yields 
reasonable results with 5122*6 stack texels + 5122*4/3 pyramid texels = 3.7 MB of storage. In general, 
a 2nx2n clipmap with a ClipSize of 2m requires only 4m(n-m + 4/3) -1/3 texels of texture storage. For 
full mipmaps, the storage needed is (4n+1 -1)/3 texels. Both of the equations must be scaled by the number 
of bytes per texel to compute physical memory use. Note that total mipmap storage is exponential in n 
where clipmap storage is linear, underscoring the practical ability of clipmaps to handle much larger 
textures than mipmaps. Comparative storage use is tabulated in Table 1, where KB = 1024 Bytes, and 16-bit 
texels are used. The .nal column shows memory use for a 226x226 image large enough to represent the earth 
at 1 meter per texel; the 34.7 MB clipmap requirement .ts nicely into the texture memory of a modern 
high-performance graphics workstation, con.rming the ability of clipmaps to address the problem identi.ed 
in §1. Type and Size 5122 10242 40962 327682 671088642 Full Mipmap 682KB 2.7MB 42.7MB 2.7GB 10923TB 5122 
Clipmap 682KB 1.1MB 2.2MB 3.7MB 9.1MB 10242 Clipmap 682KB 2.7MB 6.7MB 12.7MB 34.7MB 20482 Clipmap 682KB 
2.7MB 18.7MB 42.7MB 131.7MB Table 1: Clipmap Storage Requirements  4 UPDATING CLIPMAPS Clipmap caches 
are by intent just large enough to effectively cache the texels needed to render one view. It is therefore 
impor­tant to update the cache as the viewpoint changes, typically before each frame is rendered. This 
update is performed by considering the texture regions needed for each level as described in §3 and loading 
them into texture memory. Moreover, to take advantage of the signi.cant frame-to-frame coherence in cache 
contents, we can reuse cached texels in subsequent frames. We de.ne each level as an independently roaming 
2D image cache that moves through the entire region of that level within the complete mipmap using toroi­dal 
addressing, a commonly used approach in image processing applications [5]. We then update each level 
incrementally by downloading the new data into the cache and letting this address­ing scheme de.ne the 
destination. The same logic is used when reading memory to render the texture. The four steps of this 
load­ing/addressing scheme are presented in Figure 7. The process begins with a 2D image centered in 
the cache as shown on the left. We then specify a new ClipCenter, in this case d texels above and to 
the right of the old center. Comparing the old and new image regions, note that the central texels are 
unchanged (SAME) in each, so only the top (T), corner (C), and right (R) border areas are updated, as 
indicated in the third diagram.  Figure 7: 2D Image Roam using Toroidal Addressing Using toroidal addressing, 
new data at the top of the image is loaded at the bottom, data on the right is loaded at the left, and 
the upper right corner is loaded at the lower left. Implementation is easy: compute the virtual texel 
address by adding the ClipCenter to the texel address and then use the remainder modulo ClipSize as the 
physical address. The cache start address moves from (s/2, s/2) to (s/2+d, s/2+d), so the TextureOffset 
for this level is set to this new origin address, identifying the starting address of the cached image 
to the addressing unit. Given that each stack level roams independently based on its center and assuming 
that we are centering all the levels based on the same ClipCenter, we can visualize the complete multi-level 
update process as shown in Figure 8. Figure 8: 2D Image Roam of Complete Stack This side view of the 
toroidal indexing scheme shows how the entire set of stack levels appears to have moved when in fact 
all updates are performed in place as described previously. The over­lap areas are unchanged and require 
no update. In practice, this area is relatively large so only minor paging at the edges is required to 
update a clipmap stack. Since lower levels are coarser resolutions as in normal mipmapping, the same 
movement of cen­ter point will result in only half as much movement of a lower level than the level above 
it, and less movement means less paging. When movement of the TextureCenter is smaller than the InvalidBorder 
for a given level, then the update regions can be loaded while texture is being drawn with the previous 
center. This is true because the InvalidBorder guarantees those texels will not be used. This also allows 
large updates to be performed in an incre­mental manner across multiple frames before the ClipCenter 
is moved. 4.1 Update Bandwidth Considerations In addition to rendering, implementations must also offer 
enough texture download bandwidth to meet the demands implied by Clip­map Stack depth, ClipSize, and 
the speed of eyepoint travel through texture space as de.ned by ClipCenter. An upper limit to cache update 
time is that time needed to replace all clipmap stack levels. In the case of the 327682 clipmap with 
a 1024 StackSize that our implementation is designed to handle, reloading the cache levels requires 10MBytes 
of texture paging. Given memory-to­graphics rates of 270MBytes per second, a .ush requires 1/27th second. 
Fortunately, the cache need not be completely .ushed due to the texel reuse and incremental update allowed 
by toroidal addressing. With this ef.ciency, the ClipCenter can be moved 1000 texels on the highest level 
map in this con.guration in less than 1/ 60th of a second. This corresponds to an eyepoint speed of 134,160 
miles per hour when 1 meter texels are used, a rate that easily exceeds our update goals.  5 RENDERING 
WITH CLIPMAPS Rendering with a clipmap is much the same as rendering with a mipmap. The only modi.cations 
to the normal mipmap rendering are slight changes to the level-of-detail selection algorithm and to the 
texture memory addressing system. However, it is important to note that building clipmapping support 
into low level hardware is crucial. With this in mind, here are the steps to sample texture for a given 
pixel using a clipmap representation: 1. Determine the S, T coordinates for the current pixel. This is 
done exactly as addressing an equivalent mipmap. 2. Calculate the level of detail based on normal mipmapping 
crite­ria. This calculation yields a .oating-point LOD where the frac­tional part of the number represents 
blending between two LODs. If this LOD resides completely in the pyramidal portion of the clipmap then 
we simply proceed with addressing the appropriate LODs as a normal mipmap. If the .ner LOD being blended 
resides in the Clipmap Stack and the coarser LOD resides in the pyramid, then we address the coarser 
LOD as a mipmap but continue on the clipmap path for the .ner LOD addressing. 3. Calculate the .nest 
LOD available given the texture coordinates established in Step 1. This calculation is done by comparing 
the texture coordinate to all of the texture regions described in §3. This calculation has to be done 
in case the texels are not present at the LOD requested. There are several hardware optimizations that 
can be performed to make this Finest LOD for a texture coordinate calculation tenable and ef.cient in 
hardware. For cost sensitive solutions, a restriction can be made where all lev­els must be centered 
concentrically, such that calculating a sim­ple maximum distance from the TextureCenter to the texture 
coordinate generated can yield the .nest available LOD based on the following:  Sdist = roundUp(abs(Scenter 
 s)) Tdist = roundUp(abs(Tcenter t)) MaxDist = max(Sdist, Tdist) FinestLODAvailable = roundUp(log2(MaxDist) 
 log2(ClipSize)) The equation for FinestLODAvailable is adjusted to consider the InvalidBorder designed 
to prevent accessing texels that are in the process of being asynchronously updated. Having calcu­lated 
the LODmip and LODAvailable, we simply use the coarser of the two. We emphasize that it is possible to 
con.gure a system and application where the LODAvailable never limits the LOD calculation. This is done 
by selecting a suf.ciently large ClipSize and placing the ClipCenter appropriately. 4. Convert the s 
and t coordinates into level-speci.c and cache­speci.c coordinates. First, we split the calculation into 
two to account for the blending between two active LODs (.ne and coarse). Sf = (s >> LODclip) 0.5 Tf 
= (t >> LODclip) 0.5 Sc = (s >> (LODclip + 1)) 0.5 Tc = (t >> (LODclip + 1)) 0.5 Then, determine 
the offsets necessary to address the .ne and coarse LOD given the data currently cached inside each level. 
Sfoff = ClipCenter[LODclip].S TextureSize[LODclip]/2 Tfoff = ClipCenter[LODclip].T TextureSize[LODclip]/2 
Scoff = ClipCenter[LODclip+1].S TextureSize[LODclip]/2 Tcoff = ClipCenter[LODclip+1].T TextureSize[LODclip]/2 
Finally, determine the actual S, T address within the clipmap level cache for both the .ne and coarse 
LODs by using the actual texture coordinate, the recently computed center offset, and the user speci.ed 
TextureOffset. These addresses are inter­preted using modular addressing consistent with the way that 
§4 describes cache updates based on TextureOffset. Sclip.ne = (Sf Sfoff TextureOffset[LODclip].S)% 
ClipSize Tclip.ne = (Tf Tfoff TextureOffset[LODclip].T)% ClipSize Sclipcoarse = (Sc Scoff TextureOffset[LODclip+1].S)% 
ClipSize Tclipcoarse = (Tc Tcoff TextureOffset[LODclip+1].T)% ClipSize 5. Use the two sets of texture 
coordinates (.ne and coarse) to gen­erate a .ltered texture value for the pixel exactly as for a mip­map 
texture. 6 VIRTUAL CLIPMAPS Having de.ned the low-level architecture of the clipmap repre­sentation, 
we consider a key implementation issue the numerical range and precision required throughout the texture 
system. This issue is central because it controls cost and complexity in an imple­mentation, factors 
that we seek to minimize as goals of our devel­opment. Previous mipmap implementations have supported 
texture sizes as large as 28 to 212 in each dimension, far less than the 226 needed by a 1 meter per 
texel whole earth texture. Thus, having .rst solved the storage problem for huge texture images, we must 
now .nd an affordable way to address their compact representa­tion. The hardware environment of our .rst 
implementation [3] dic­tated that directly enlarging the numerical range and precision throughout the 
system to match the needs of 27-level and larger clipmaps was not practical. The impact would have included 
bus widths, gate counts, circuit complexity, logic delays, and packag­ing considerations; full-performance 
rendering would have been impossible. These issues, along with the practical matter of sched­ule, encouraged 
us to solve the 226 clipmap problem within the 215 precision supported in hardware. Our approach to virtualizing 
clipmaps is based on the observa­tion that while the polygons comprising a scene extend from the eyepoint 
to the horizon, each individual polygon only subtends a portion of this range. Thus, while a scene may 
need the full 27­level clipmap for proper texturing, individual polygons can be properly rendered with 
the 16-level hardware support so long as two conditions are met: the texture extent across each polygon 
.ts within the hardware limit, and an appropriate adjustment is made as polygons are rendered to select 
the proper 16 levels from the full 27 level virtual clipmap. We implement virtual clipmaps by adding 
two extensions to the clipmap system. First, we provide a .xed offset that is added to all texture addresses. 
This value is used to virtually locate a physical clipmap stack within a taller virtual clipmap stack, 
in essence mak­ing one of the virtual stack levels appear to be the top level to the hardware. Second, 
we add a scale and bias step to texture coordi­nate processing. This operation is equivalent to the memory 
address offset but modi.es texture coordinates rather than texel memory addresses. In concert, these 
extensions provide address precision accurate to the limit of either user speci.cation or coor­dinate 
transformation. With a 32-bit internal implementation, this is suf.cient to represent the entire earth 
at 1 centimeter per texel resolution, which exceeds our stated goal. This virtualization requires that 
the low-level clipmap algorithm be extended to address a sub-clipmap out of an arbitrary clipmap when 
the depth exceeds the directly supported precision. Given the stack and pyramid structure de.ned in a 
clipmap, we observe three possible types of addressing that must be supported as illustrated in Figure 
9.  Stack from Stack Virtual Pyramid from Stack Clipmap Pyramid from Pyramid Real Clipmap Figure 9: 
Virtual Addressing in Stack and Pyramid Levels 1. Address a stack level as a stack level. No address 
modi.cations necessary other than the upstream scale and bias. 2. Address a stack level as a pyramid 
level. Need to provide an off­set into the level to get to the correct data. This can be done by starting 
from the center texel, and offsetting backwards to .nd the origin texel of the sub-level based on desired 
level size. 3. Address a pyramid level as a pyramid level. This can be imple­mented by knowing the actual 
level size, .nding the center texel, and offsetting backwards to .nd the origin texel of the sub-level 
based on desired level size.  These three cases de.ne the additional addressing hardware used to switch 
between the normal stack address processing (as described in §5), sub-addressing as de.ned above for 
cases 2 and 3, and normal mipmap addressing. With these addressing exten­sions, software can select sub-clipmaps 
and implement texture coordinate scale and bias. Why add this additional complexity to an otherwise simple 
low­level implementation? Because there is seemingly no limit to desired texture size. While the size 
of directly supported clipmaps will likely grow in future implementations, so will the demand for larger 
textures. Since each extra level directly supported by hard­ware forces expensive precision-related system 
costs onto imple­mentations, we believe that virtual clipmap support will always be useful.  7 DYNAMIC 
CLIPMAP MANAGEMENT Having described the clipmap representation, the architecture, and the re.nements 
needed to virtualize a clipmap beyond imple­mentation limits, we now consider the remaining issue keeping 
the clipmap updated as the eyepoint moves through texture space. This is a signi.cant part of any implementation, 
since the tremen­dous ef.ciency of the clipmap representation reduces only the number of texels resident 
in texture memory at any one time, not the total number of texels in the mipmap. A series of images ren­dered 
interactively may visit the entire extent of the texture image over the course of time; an image which 
for a one meter per texel earth is an intimidating 10,923 TBytes of data. Our goals for dynamic clipmap 
management are these: effective use of system memory as a second-level cache to buffer disk reads, ef.cient 
management of multiple disk drives of differing speeds and latencies as the primary source for texture 
images, automated load management of the clipmap update process to avoid distract­ing visual artifacts 
in cases of system bandwidth overload, support for high resolution inset areas (which implies a sparse 
tile cache on disk), and .nally, complete integration of this higher-level clipmap support into software 
tools in order to provide developers the sense that clipmaps are as complete, automatic, and easy to 
use as stan­dard mipmaps. 7.1 Main Memory as Second-Level Cache To use main memory as a second level 
cache we must optimize throughput on two different paths: we need to optimize throughput from disk devices 
to memory and we need to optimize throughput from memory to the underlying hardware clipmap cache. In 
order to optimize utilization of system resources we use separate threads to manage all aspects of data 
.ow. We need at least one process scheduling data .ow, one process moving data from disk to mem­ory, 
and one process incrementally feeding clipmap updates to the graphics subsystem. In the context of our 
high-level graphics soft­ware toolkit, IRIS Performer [2], this means that scheduling is done by the 
software rendering pipeline in the Cull process, down­loading of data to the graphics pipeline is done 
in the Draw pro­cess, and a new lightweight asynchronous disk reading process is created to manage .le 
system activities. The second level cache represents each level of the mipmap using an array of tiles. 
These tiles must be large enough to enable maximum throughput from disk, but small enough to keep overall 
paging latency low. We use sizes between 1282 and 10242 depend­ing on disk con.gurations. For this second 
level cache to operate as a real-time look-ahead cache, we load into parts of the cache that are not 
currently needed by the underlying hardware. Thus each cache level contains at least enough tiles to 
completely hold the underlying clipmap level that is currently resident while prefetching border tiles. 
With a ClipSize of 1024 and a tile size of 2562, the cache must be con.gured to be at least 6 by 6 tiles 
or 1536 by 1536 texels, as shown in Figure 10. Due to memory alignment issues, low-level texture updates 
must be implemented as individual downloads based on tile boundaries. In Figure 10, the left drawing 
shows a minimal cache for one stack level. This cache has already copied the texels in the ClipSize2 
cen­tral area down to the clipmap in texture memory. The cache has also pre-fetched the minimal single-tile 
border of image tiles, the ready area. The center illustration indicates that the ClipCenter (the dot) 
has been moved up and to the right by half of a tile (128 texels in this case). What actions are implied 
by this recentering action? First, the clipmap must be updated. This is done by access­ing the border 
tiles above and to the right of the existing stack data. The white square indicates the new clipmap area, 
and the nine crosshatched rectangles are the texture downloads required to update the hardware clipmap 
stack for this level. Multiple down­loads are performed because it is infeasible to perform real-time 
memory to memory copies to realign memory when overall texture load rates are 270 MB per second. Once 
the clipmap layer is updated, the main memory tile-cache must be roamed up and to the right, by scheduling 
disk reads to retrieve the newly uncovered tiles, indicated as the page area in the diagram. While the 
new tiles are logically up and to the right, we store them at the bottom and the left, using the same 
toroidal addressing used in the clipmap itself as in that case, unchanged data is never updated. This 
main memory clipmap cache is very much a larger tiled ver­sion of the underlying hardware representation. 
It has a section of static pyramid levels and a section of cached roaming stack levels. The biggest difference 
being that an external tile border is used for paging in main memory and an internal InvalidBorder-sized 
region serves the same purpose within the clipmap. Each cache roams through its level of the full mipmap 
texture independently based on the level s ClipCenter. Each cache level performs look-ahead pag­ing, 
requesting disk reads for image tiles that are likely to be loaded into the clipmap in the near future. 
Each level is responsible for managing the corresponding level of the underlying hardware clipmap implementation: 
recentering the underlying hardware level and incrementally downloading the relevant texture data from 
its tiled cache. To con.gure the main memory cache system, developers specify information for each clipmap 
level. For stack levels, they provide image cache con.gurations that describe how to load tiles (such 
as an sprintf format-string to convert tile row and column indices into a full .lesystem path name for 
each level), how big the cache should be, how big the underlying hardware clipmap level is, and other 
basic information. For pyramid levels, they simply provide the static image. Global clipmap data is also 
provided, such as the ClipSize for the clipmap, the storage format of the texels, and parameterization 
for load control. 7.2 Cache Centering Algorithms In addition to the low and high level representations 
we have de.ned so far, there is still an important issue we have not yet dis­cussed: how to decide which 
part of the image should be cached. We encourage developers to set the center directly in both the lower 
and higher-level software since the optimal place to center the cache is inherently application dependent. 
Some applications use narrow .elds of view and thus look at data in the distance, which is where the 
ClipCenter should be located. Other applica­tions, such as head-tracked ground-based virtual reality 
applica­tions, want to ensure that quick panning will not cause the clipmap cache to be exceeded, arguing 
for placing the ClipCenter at the viewer s virtual foot position. Our higher-level software provides 
utilities to automatically set the clipmap ClipCenter based on a variety of simple viewpoint projections 
and viewing frustum related intersection calculations as a convenience in those cases where default processing 
is appropriate. 7.3 Managing Filesystem Paging The low-level clipmap representation provides fully determinis­tic 
performance because it is updated with whatever texels are required before rendering begins. Unfortunately, 
this is not true of the second-level cache in main memory due to the vagaries of .le­system access in 
terms of both bandwidth and latency. Compared to memory-to-graphics throughput of 200-450 MBytes/second, 
individual disk bandwidths of 3-15 MBytes per second are very slow. This could lead to a situation where 
speedy eyepoint motion exceeds the pace of second-level tile paging, causing the Effective-Size cache 
region to stall at the edge of a cache level awaiting data from disk before being recentered. We must 
anticipate this problem since we do not limit eyepoint speed, but we can avoid visual distraction should 
it occur. The important realization is that the requested data is always present in the clipmap, albeit 
at a coarser resolution than desired, since the top-most of the pyramid levels in the clipmap caches 
the entire image. We use the MaxTextureLOD feature already present in the hardware to disable render 
access to a hardware stack level whenever updating a level requires unavailable data. The clipmap software 
system attempts to catch up to outstanding paging requests, and will re-enable a level by changing the 
MaxTexture-LOD as soon as the main memory texture tile cache is up to date. Thus MaxTextureLOD converts 
the stack levels into a resolution bellows, causing the clipmap to always use the best available data 
without ever waiting for tardy data. This makes it possible for clip­maps to allow arbitrary rates of 
eyepoint motion no matter what .lesystem con.guration is used. In underpowered systems it is perfectly 
acceptable if the texture becomes slightly blurry when the ClipCenter is moved through the data faster 
than the tiles can be read. Recovery from this overload condition is automatic once the data is available; 
the system downloads the level and tells the hardware to incrementally fade that level back in by slowly 
chang­ing the MaxTextureLOD over an interval of several frames. To control latency in paging-limited 
cases, we provide a light­weight process to optimize the contents of the disk read queues. Without this 
the read queue could grow without bound when cache levels make requests that the .lesystem cannot service, 
causing the system to get ever further behind. The queue manager process orders tile read requests by 
priorities which are determined based on the stack level issuing the request (with higher priorities 
for coarser data) and on the estimated time until a tile will be needed. The priorities of outstanding 
requests are asynchronously updated by the cache software managing each level every frame and the read 
manager thread sorts the disk read queue based on these changing priorities, removing entries that are 
no longer relevant. This priority sort limits the read queue size and minimizes latency of read requests 
for more important tiles, thus ensuring that tiles needed to update coarser levels are loaded and used 
sooner. 7.4 Managing Stack Level Updates We now address load management issues in paging data from main 
memory to texture hardware. Since memory-to-graphics bandwidth is at a premium in immediate-mode graphics 
systems, it is important to understand and control its expenditure. A typical application might devote 
75% of the throughput for sending geo­metric information to the graphics subsystem, leaving 25% of the 
16.67ms (60 Hz) frame time for incremental texture paging, including both mipmaps and clipmaps. The time 
allocated for clip­map updates may be less than the time needed to fully update a clipmap because the 
center is moved a great distance requiring many texels to be updated or because the time allowed for 
updates is so little that even the smallest update may not be performed. Since unwavering 60 Hz update 
rates are a fundamental goal of our system, we must plan for and avoid these situations. To control clipmap 
update duration we precisely estimate the time required to perform the downloads for a given frame. A 
table of the measured time for texture downloads of each size that could potentially be performed based 
on clipmap and system con.gura­tion is generated at system start-up. Then, as we schedule down­loads, 
we start at the lowest (coarsest) cache level and work our way up the cache levels one by one. The total 
projected time is accumulated and updates are terminated when insuf.cient time remains to load a complete 
level. Coarse LODs are updated .rst, since they are both the most important and require the least band­width. 
Finer levels are disabled using MaxTextureLOD if there is insuf.cient time for the required updates. 
We also expose the MaxTextureLOD parameter for explicit coarse-grain adjustments to required download 
bandwidth. Since each higher stack level requires four times the paging of the next lower one, adjustment 
of maximum LOD has a powerful ability to reduce overall paging time. The visual effect of coarsening 
the MaxTextureLOD by one level is to remove the innermost of the concentric rings of texture precision. 
Since this has a visually dis­cernible effect, it must be gradually done over a few tens of frames by 
fractionally changing the MaxTextureLOD until an entire stack level is no longer in use, after which 
rendering and paging are dis­abled for that level. This method is illustrated in the right-most col­umn 
of Figure 11, where the inner resolution ring has been removed by decreasing the MaxTextureLOD. For .ne-grain 
control of paging bandwidth, we use the Invalid-Border control as described in §3.2.3 to reduce the area 
of texels that must be updated in each stack level. Our implementation allows the InvalidBorder to be 
adjusted on a per-frame basis with little overhead. Increasing the InvalidBorder reduces the Effec­tiveArea, 
reducing the radius of each concentric band of precision. This is illustrated at the bottom row of Figure 
11, where rendering with a larger InvalidBorder is seen to scale the Stack Levels closer to the ClipCenter. 
Only the stack levels are modi.ed by the InvalidBorder, as indicated by the outer region in the left 
diagram. Normal Decrease Max LOD Max LOD InvalidBorder Normal Invalid Border Max Large Texture Invalid 
LOD Border The load control mechanisms presented here have proven to be effective, allowing existing 
applications to integrate clipmaps vir­tualizing terabytes of .lesystem backing store, maintaining con­stant 
60 Hz real-time rendering rates irrespective of disk speeds and latencies.  7.5 High-Resolution Insets 
High-resolution insets are regions of extra-high texture resolu­tion embedded into a larger, lower-resolution 
texture image. They are important since applications often have areas within their data­base where higher-.delity 
is needed. An example occurs in com­mercial .ight simulators, where texture resolution at airports must 
be very high to support taxi, approach, and departure training, while the terrain areas traversed in 
.ight between airports require more moderate texture resolution. Our implementation supports insets as 
a side effect of the inte­grated .lesystem paging load control that constantly sorts tile read requests. 
To implement insets, developers specify sparse tile arrays in the .lesystem for each clip level. Sparse 
tile arrays are those where some or nearly all tiles are unde.ned, resulting in islands of resolution 
in various portions of an otherwise empty tile array (near airports, for the example above). In this 
situation the higher­level software uses the inset data when it can skipping the empty regions automatically 
using the TextureMaxLOD load-control mechanism as it discovers that nonexistent tiles are perpetually 
late in arriving from the .lesystem. The load control algorithm natu­rally falls back on coarser LODs 
when inset level data is not avail­able. To allow insets to blend in smoothly, tiles containing the inset 
texels must be surrounded by a border at least ClipSize texels wide using data magni.ed from the next 
lower level in a recursive man­ner, a requirement easily met by automatic database construction tools. 
This restriction exists because the decision to enable or dis­able a clipmap level is made for the level 
in its entirety using Tex­tureMaxLOD, and therefore partially valid cache levels partly in the inset 
region and partly outside of it would normally be dis­abled. Providing this border ensures that the load-management 
algorithm will enable the entire level when any pixel of the inset is addressed.  8 IMPLEMENTATION RESULTS 
We have implemented the clipmap rendering system described here. The implementation consists of low-level 
support for real and virtual clipmaps within the In.niteReality [3] graphics subsystem; special OpenGL 
clipmap control and virtualization extensions; and support within the IRIS Performer [2] real-time graphics 
software toolkit for clipmap virtualization, second-level tile caches, tile pag­ing from the .lesystem, 
and for automatic load-management con­trols as described above. In working with the system, developers 
.nd the results of using the clipmap approach to be excellent. Real-time graphics applica­tions can now 
use textures of any desired precision at full speed examples include planet-wide textures from satellite 
data, country and continent scale visual simulation applications with centimeter scale insets, architectural 
walkthrough applications showing murals with minute brush strokes visible at close inspection, and numerous 
advanced uses in government. The image in Figure 12 shows an overhead view of a 81922 tex­ture image 
of the Moffet Field Naval Air Station in Mountain View, California rendered onto a single polygon. The 
colored markers are diagnostic annotations indicating the extent of each clipmap level. The EffectiveArea 
has been signi.cantly reduced by enlarging the InvalidBorder to make these concentric bands of pre­cision 
easily visible. The image in Figure 13 shows a very small portion of a 25m per texel clipmap of the 
entire United States the area shown here is the southern half of the Yosemite National Park. That our 
approach makes this single 170 GByte texture displayable at 60 Hertz with an approximately 16 MByte clipmap 
cache is impressive; equally so is the fact that the application being used need not know about clipmaps 
all clipmap de.nition, updating, load-management and default ClipCenter selection happens automatically 
within IRIS Performer. 9 CONCLUSIONS We have developed a new texture representation the clipmap a look-ahead 
cache con.gured to exploit insights about mipmap rendering, spatial coherence, and object to texture 
space coordinate mappings. This representation is simply parameterized and can be implemented in hardware 
via small modi.cations to existing mip­map rendering designs. It has been implemented in a system fea­turing 
a fast system-to-graphics interface allowing real-time update of clipmap caches. This system renders 
high quality images in real-time from very large textures using relatively little texture memory. The 
hardware supports virtualization for textures larger than it can address directly. This approach is not 
only important for representing textures of arbitrary scale. Equally important is that it also liberates 
geometric modeling and level of detail decisions from texture management concerns. The guiding insights 
about mipmap utilization that made the clipmap solution possible can be applied to related problems in 
computer graphics. Display resolution provides an upper bound to the amount of data needed in any rendering. 
It seems possible to develop a system that stages geometric level-of-detail information for large databases 
similarly to the way clipmaps stage image data. If an adaptive rendering algorithm were de.ned to create 
continu­ous tessellations from partially speci.ed geometric levels of detail, then the same look ahead 
cache notions could be used to stage geometry. At a system level, this approach has predetermined throughputs 
and bandwidths that can be established to ensure both high .delity and robust real-time behavior. In 
this way, the overall data .ow of large systems can be easily sized and tuned for a wide range of applications. 
 Since the original development of clipmaps and their hardware and software implementations, we have 
explored several exten­sions including 3D clipmaps, texture download optimizations, and new inter-level 
texture blend modes. This work leads us to believe that there is considerably more to be discovered. 
We have seen realism in real-time visual simulation revolutionized in the wake of the introduction of 
clipmaps and we eagerly anticipate new ways in which clipmaps can have this effect in other application 
areas.  Acknowledgments We would like to recognize Jim Foran for his original idea, Don Hatch and Tom 
McReynolds of IRIS Performer, Mark Peercy of OpenGL and the technical marketing team for their contribution. 
 References [1] Cosman, Michael. Global Terrain Texture: Lowering the Cost. In Eric G. Monroe, editor, 
Proceedings of 1994 IMAGE VII Conference, pages 53-64. The IMAGE Society, 1994. [2] Rohlf, John and James 
Helman. IRIS Performer: A High Performance Multiprocessing Toolkit for Real-Time 3D Graphics. In Andrew 
Glassner, editor, SIGGRAPH 94 Conference Proceedings. Annual Conference Series, pages 381-394. ACM SIGGRAPH, 
Addison Wes­ley, July 1994. ISBN 0-89791-667-0. [3] Montrym, John S, Daniel R Baum, David L Dignam and 
Christopher J Migdal. In.niteReality: A Real-Time Graphics System. In Turner Whitted, editor, SIGGRAPH 
97 Conference Proceedings. Annual Con­ference Series, pages 293-301. ACM SIGGRAPH, Addison Wesley, August 
1997. ISBN 0-89791-896-7. [4] Sanz-Pastor, Nacho and Luis Barcena. Computer Arts &#38; Develop­ment, 
Madrid, Spain. Private communication. [5] Walker, Chris, Nancy Cam, Jon Brandt and Phil Keslin. Image 
Vision Library 3.0 Programming Guide. Silicon Graphics Computer Systems, 1996. [6] Williams, Lance. Pyramidal 
Parametrics. In Peter Tanner, editor, Com­puter Graphics (SIGGRAPH 83 Conference Proceedings), volume 
17, pages 1-11. ACM SIGGRAPH, July 1983. ISBN 0-89791-109-1.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280857</article_id>
		<sort_key>159</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[A shading language on graphics hardware]]></title>
		<subtitle><![CDATA[the pixelflow shading system]]></subtitle>
		<page_from>159</page_from>
		<page_to>168</page_to>
		<doi_number>10.1145/280814.280857</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280857</url>
		<keywords>
			<kw><![CDATA[procedural shading]]></kw>
			<kw><![CDATA[real-time image generation]]></kw>
			<kw><![CDATA[shading language]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>D.3.2</cat_node>
				<descriptor>Specialized application languages</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011050.10011023</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Context specific languages->Specialized application languages</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Languages</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39081311</person_id>
				<author_profile_id><![CDATA[81334487411]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Olano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of North Carolina at Chapel Hill, Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P20148</person_id>
				<author_profile_id><![CDATA[81100264426]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Anselmo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lastra]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of North Carolina at Chapel Hill, Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>165781</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Preston Briggs, Register Allocation via Graph Coloring, PhD Dissertation, Department of Computer Science, Rice University, Houston, Texas, 1992.]]></ref_text>
				<ref_id>Briggs92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808602</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Robert L. Cook, "Shade Trees", Proceedings of SIGGRAPH 84 (Minneapolis, Minnesota, July 23-27, 1984). In Computer Graphics, v18n3. ACM SIGGRAPH, July 1984. pp. 223-231.]]></ref_text>
				<ref_id>Cook84</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37414</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Robert L. Cook, "The Reyes Image Rendering Architecture", Proceedings of SIGGRAPH 87 (Anaheim, California, July 27-31, 1987). In Computer Graphics, v21n4. ACM SIGGRAPH, July 1987. pp. 95-102.]]></ref_text>
				<ref_id>Cook87</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378468</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Michael Deering, Stephanie Winner, Bic Schediwy, Chris Duffy and Neil Hunt, "The Triangle Processor and Normal Vector Shader: A VLSI System for High Performance Graphics", Proceedings of SIG- GRAPH 88 (Atlanta, Georgia, August 1-5, 1988). In Computer Graphics, v22n4, ACM SIGGRAPH, August 1988. pp. 21-30.]]></ref_text>
				<ref_id>Deering88</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Henry G. Dietz, "Common Subexpression Induction", Proceedings of the 1992 International Conference on Parallel Processing (Saint Charles, Illinois, August 1992). pp. 174-182.]]></ref_text>
				<ref_id>Dietz92</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[David Ellsworth, "Parallel Architectures and Algorithms for Real-time Synthesis of High-quality Images using Deferred Shading". Workshop on Algorithms and Parallel VLSI Architectures (Pont-g- Mousson, France, June 12, 1990).]]></ref_text>
				<ref_id>Ellsworth91</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258714</ref_obj_id>
				<ref_obj_pid>258694</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[John Eyles, Steven Molnar, John Poulton, Trey Greet, Anselmo Lastra, Nick England and Lee Westover, "PixelFlow: The Realization", Proceedings of the 1997 SIGGRAPH/Eurographics Workshop on Graphics Hardware (Los Angeles, California, August 3-4, 1992). ACM SIGGRAPH, August 1997. pp. 57-68.]]></ref_text>
				<ref_id>Eyles97</ref_id>
			</ref>
			<ref>
				<ref_obj_id>643327</ref_obj_id>
				<ref_obj_pid>643323</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Larry Gritz and James K. Hahn, "BMRT: A Global Illumination Implementation of the RenderMan Standard", Journal of Graphics Tools, vln3, 1996. pp. 29-47.]]></ref_text>
				<ref_id>Gritz96</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218470</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Brian Guenter, Todd B. Knoblock and Erik Ruf, "Specializing Shaders", Proceedings of SIGGRAPH 95 (Los Angeles, California, August 6-11, 1995). In Computer Graphics Proceedings, Annual Conference Series, ACM SIGGRAPH, 1995. pp. 343-348.]]></ref_text>
				<ref_id>Guenter95</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97911</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Pat Hanrahan and Jim Lawson, "A Language for Shading and Lighting Calculations", Proceedings of SIGGRAPH 90 (Dallas, Texas, August 6-10, 1990). In Computer Graphics, v24n4. ACM SIG- GRAPH, August 1990. pp. 289-298.]]></ref_text>
				<ref_id>Hanrahan90</ref_id>
			</ref>
			<ref>
				<ref_obj_id>248212</ref_obj_id>
				<ref_obj_pid>248210</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[B. Hill, Th. Roger and F. W. Vorhagen, "Comparative Analysis of the Quantization of Color Spaces on the Basis of the CIELAB Color- Difference Formula", ACM Transactions on Graphics, vl 6n2. ACM, April 1997. pp. 109-154.]]></ref_text>
				<ref_id>Hill97</ref_id>
			</ref>
			<ref>
				<ref_obj_id>199414</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Anselmo Lastra, Steven Molnar, Marc Olano and Yulan Wang, "Real-time Programmable Shading", Proceedings of the 1995 Symposium on Interactive 3D Graphics (Monterey, California, April 9-12, 1995). ACM SIGGRAPH, 1995. pp. 59-66.]]></ref_text>
				<ref_id>Lastra95</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Jon Leech, "OpenGL Extensions and Restrictions for PixelFlow", Technical Report TR98-019, Department of Computer Science, University of North Carolina at Chapel Hill.]]></ref_text>
				<ref_id>Leech98</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[MasPar Computer Corporation, MasPar Parallel Application Language (MPL) User Guide, 1990.]]></ref_text>
				<ref_id>MasPar90</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806820</ref_obj_id>
				<ref_obj_pid>800224</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Nelson L. Max, "Vectorized Procedural Models for Natural Terrain: Waves and Islands in the Sunset", Proceedings of SIGGRAPH 81 (Dallas, Texas, July 1981). In Computer Graphics, v15n3. ACM SIGGRAPH, August 1981. pp. 317-324.]]></ref_text>
				<ref_id>Max81</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134067</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Steven Molnar, John Eyles and John Poulton, "PixelFlow: Highspeed Rendering Using Image Composition", Proceedings of SIG- GRAPH 92 (Chicago, Illinois, July 26-31, 1992). In Computer Graphics, v26n2. ACM SIGGRAPH, July 1992. pp. 231-240.]]></ref_text>
				<ref_id>Molnar92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>286076</ref_obj_id>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Steven Muchnick, Compiler Design and Implementation. Morgan Kaufmann, San Francisco, CA, 1997.]]></ref_text>
				<ref_id>Muchnick97</ref_id>
			</ref>
			<ref>
				<ref_obj_id>562677</ref_obj_id>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Jackie Neider, Tom Davis and Mason Woo, OpenGL Programming Guide: the official guide to learning OpenGL release 1., Addison-Wesley, 1993.]]></ref_text>
				<ref_id>Neider93</ref_id>
			</ref>
			<ref>
				<ref_obj_id>927670</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Marc Olano, A Programmable Pipeline for Graphics Hardware, PhD Dissertation, Department of Computer Science, University of North Carolina at Chapel Hill, 1998.]]></ref_text>
				<ref_id>Olano98</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325247</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Ken Perlin, "An Image Synthesizer", Proceedings of SIGGRAPH 85 (San Francisco, California, July 22-26, 1985). In Computer Graphics, v19n3. ACM SIGGRAPH, July 1985. pp. 287-296.]]></ref_text>
				<ref_id>Perlin85</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Pixar Animation Studios, PhotoRealistic RenderMan 3.7 Shading Language Extensions. Pixar animation studios, March 1997.]]></ref_text>
				<ref_id>Pixar97</ref_id>
			</ref>
			<ref>
				<ref_obj_id>147171</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[John Rhoades, Greg Turk, Andrew Bell, Andrei State, Ulrich Neumann and Amitabh Varshney, "Real-time procedural textures", Proceedings of the 1992 Symposium on Interactive 3D Graphics (Cambridge, Massachusetts, March 29-April 1, 1992). In Computer Graphics special issue. ACM SIGGRAPH, March 1992. pp. 95-100.]]></ref_text>
				<ref_id>Rhoades92</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Philipp Slusallek, Thomas Pflaum and Hans-Peter Seidel, "Implementing RenderMan-Practice, Problems and Enhancements", Proceedings of Eurographics '94. In Computer Graphics Forum, v13n3, 1994. pp. 443-454.]]></ref_text>
				<ref_id>Slusallek94</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Thinking Machines Corporation, Connection Machine Model CM-2 Technical Summary. Thinking Machines Corporation, Version 5.1, May 1989.]]></ref_text>
				<ref_id>ThinkingMachines89</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Steve Upstill, The RenderMan Companion, Addison-Wesley, 1990.]]></ref_text>
				<ref_id>Upstill90</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806815</ref_obj_id>
				<ref_obj_pid>800224</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[T. Whitted and D. M. Weimer, "A software test-bed for the development of 3-D raster graphics systems", Proceedings of SIG- GRAPH 81 (Dallas, Texas, July 1981). In Computer Graphics, v15n3. ACM SIGGRAPH, August 1981. pp. 271-277.]]></ref_text>
				<ref_id>Whitted81</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Copyright &#38;#169;1998 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers or to redistribute to lists, requires specific permission and/or a fee. A Shading Language 
on Graphics Hardware: The PixelFlow Shading System Marc Olano Anselmo Lastra University of North Carolina 
at Chapel Hill Abstract Over the years, there have been two main branches of computer graphics image-synthesis 
research; one focused on interactivity, the other on image quality. Procedural shading is a powerful 
tool, commonly used for creating high-quality images and production animation. A key aspect of most procedural 
shading is the use of a shading language, which allows a high-level description of the color and shading 
of each surface. However, shading languages have been beyond the capabilities of the interactive graphics 
hardware community. We have created a parallel graphics multi­computer, PixelFlow, that can render images 
at 30 frames per second using a shading language. This is the first system to be able to support a shading 
language in real-time. In this paper, we describe some of the techniques that make this possible. CR 
Categories and Subject Descriptors: D.3.2 [Language Classifications] Specialized Application Languages; 
I.3.1 [Computer Graphics] Hardware Architecture; I.3.3 [Computer Graphics] Picture/Image Generation; 
I.3.6 [Computer Graphics] Methodologies and Techniques; I.3.7 [Computer Graphics] Three-dimensional Graphics 
and Realism. Additional Keywords: real-time image generation, procedural shading, shading language. 
1 INTRODUCTION We have created a SIMD graphics multicomputer, PixelFlow, which supports procedural shading 
using a shading language. Even a small (single chassis) PixelFlow system is capable of rendering scenes 
with procedural shading at 30 frames per sec­ond or more. Figure 1 shows several examples of shaders 
that were written in our shading language and rendered on PixelFlow. In procedural shading, a user (someone 
other than a system designer) creates a short procedure, called a shader, to determine the final color 
for each point on a surface. The shader is respon­ Now at Silicon Graphics, Inc., 2011 N. Shoreline Blvd., 
M/S #590, Mountain View, CA 94043 (email: olano@engr.sgi.com) UNC Department of Computer Science, Sitterson 
Hall, CB #3175, Chapel Hill, NC 27599 (email: lastra@cs.unc.edu) ab cd ef Figure 1: Some PixelFlow 
surface shaders. a) brick. b) mirror with animated ripple. c) wood planks. d) a vol­ ume-based wood. 
e) light shining through a paned win­ dow. f) view of a bowling scene. sible for color variations across 
the surface and the interaction of light with the surface. Shaders can use an assortment of input appearance 
parameters, usually including the surface normal, texture coordinates, texture maps, light direction 
and colors. Procedural shading is quite popular in the production industry where it is commonly used 
for rendering in feature films and commercials. The best known examples of this have been ren­dered using 
Pixar s PhotoRealistic RenderMan software [Upstill90]. A key aspect of RenderMan is its shading language. 
The shading language provides a high-level description of each procedural shader. Shaders written in 
the RenderMan shading language can be used by any compliant renderer, no matter what rendering method 
it uses. There are several reasons to provide procedural shading in­stead of just image texturing on 
a real-time graphics system: . It is easy to add noise and random variability to make a surface look 
more realistic. . It can be easier to create a procedural shader for a compli­cated surface than to try 
to eliminate the distortions caused by wrapping a flat, scanned texture over the surface. . It is easier 
to tweak a procedural shader than to rescan or repaint an image texture. . It is often easier to create 
detail on an object using a proce­dural shader instead of modifying the object geometry. . A procedurally 
shaded surface can change with time, dis­ tance, or viewing angle. Usually procedural shading is associated 
with images that take a while to generate from a few minutes to a day or so. Recently, graphics hardware 
reached the point where image texture map­ping was not just possible, but common; now hardware is reach­ing 
the point where shading languages for interactive graphics are possible. We have produced a shading language 
and shading language compiler for our high-end graphics machine, PixelFlow. This language is called pfman 
(pf for PixelFlow, man because it is similar to Pixar s RenderMan shading language). One of the great 
advantages of a shading language for procedural shading, particularly on a complex graphics engine, is 
that it effectively hides the implementation details from the shader-writer. The specifics of the graphics 
architecture are hidden in the shading language compiler, as are all of the tricks, optimizations, and 
special adaptations required by the machine. In this paper, we describe shading on PixelFlow, the pfman 
language, and the optimizations that were necessary to make it run in real-time. Section 2 is a review 
of the relevant prior work. Section 3 cov­ers features of the pfman shading language, paying particular 
attention to the ways that it differs from the RenderMan shading language. Section 4 describes our extensions 
to the OpenGL API [Neider93] to support procedural shading. Section 5 gives a brief overview of the PixelFlow 
hardware. Section 6 covers our im­plementation and the optimizations that are done by PixelFlow and the 
pfman compiler. Finally, Section 7 has some conclusions.  2 RELATED WORK Early forms of programmable 
shading were accomplished by rewriting the shading code for the renderer (see, for example, [Max81]). 
Whitted and Weimer specifically allowed this in their testbed system [Whitted81]. Their span buffers 
are an imple­mentation of a technique now called deferred shading, which we use on PixelFlow. In this 
technique, the parameters for shading are scan converted for a later shading pass. This allowed them 
to run multiple shaders on the same scene without having to re­render. Previous uses of deferred shading 
for interactive graphics systems include [Deering88] and [Ellsworth91]. More recently, easier access 
to procedural shading capabilities has been provided to the graphics programmer. Cook s shade trees [Cook84] 
were the base of most later shading works. He turned simple expressions, describing the shading at a 
point on the surface, into a parse tree form, which was interpreted. He introduced the name appearance 
parameters for the parameters that affect the shading calculations. He also proposed an or­thogonal subdivision 
of types of programmable functions into displacement, surface shading, light, and atmosphere trees. Perlin 
s image synthesizer extends the simple expressions in Cook s shade trees to a full language with control 
structures [Perlin85]. He also introduced the powerful Perlin noise func­tion, which produces random 
numbers with a band-limited fre­quency spectrum. This style of noise plays a major role in many procedural 
shaders. The RenderMan shading language [Hanrahan90][Upstill90] further extends the work of Cook and 
Perlin. It suggests new procedures for transformations, image operations, and volume effects. The shading 
language is presented as a standard, making shaders portable to any conforming implementation. In addition 
to the shading language, RenderMan also provides a geometry description library (the RenderMan API) and 
a geo­metric file format (called RIB). The reference implementation is Pixar s PhotoRealistic RenderMan 
based on the REYES render­ing algorithm [Cook87], but other implementations now exist [Slusallek94][Gritz96]. 
The same application will run on all of these without change. RenderMan effectively hides the details 
of the implementation. Not only does this allow multiple implementations using com­pletely different 
rendering algorithms, but it means the user writing the application and shaders doesn t need to know 
any­thing about the rendering algorithm being used. Knowledge of basic graphics concepts suffices. Previous 
efforts to support user-written procedural shading on a real-time graphics system are much more limited. 
The evolu­tion of graphics hardware is only just reaching the point where procedural shading is practical. 
The only implementation to date was Pixel-Planes 5, which supported a simple form of procedural shading 
[Rhoades92]. The language used by this system was quite low level. It used an assembly-like interpreted 
language with simple operations like copy, add, and multiply and a few more complex operations like a 
Perlin noise function. The hard­ware limitations of Pixel-Planes 5 limited the complexity of the shaders, 
and the low-level nature of the language limited its use. Lastra et. al. [Lastra95] presents previous 
work on the Pix­elFlow shading implementation. It analyzes results from a Pix­elFlow simulator for hand-coded 
shaders and draws a number of conclusions about the hardware requirements for procedural shading. At 
the time of that paper, the shading language compiler was in its infancy, and we had not addressed many 
of the issues that make a real-time shading language possible. [Lastra95] is the foundation on which 
we built our shading language.  3 SHADING LANGUAGE A surface shader produces a color for each point 
on a surface, taking into account the color variations of the surface itself and the lighting effects. 
As an example, we will show a shader for a brick wall. The wall is rendered as a single polygon with 
texture coordinates to parameterize the position on the surface. The shader requires several additional 
parameters to describe the size, shape, and color of the brick. These are the width and height of the 
brick, the width of the mortar, and the colors of the mortar and brick (Figure 2). These parameters are 
used to wrap mortar mortar height width Figure 2: Example bricks and the size and shape pa­rameters 
for the brick shader. // figure out which row of bricks this is (row is 8-bit integer) fixed<8,0> row 
= tt / height; // offset even rows by half a row if (row % 2 == 0) ss += width/2; // wrap texture coordinates 
to get brick coordinates ss = ss % width; tt = tt % height; // pick a color for the brick surface float 
surface_color[3] = brick_color; if (ss < mortar || tt < mortar) surface_color = mortar_color; Figure 
3: Code from a simple brick shader the texture coordinates into brick coordinates for each brick. These 
are (0,0) at the lower left corner of each brick, and are used to choose either the brick or mortar color. 
A portion of the brick shader is shown in Figure 3. The brick image in Figure 2 was generated with this 
shader. One advantage of procedural shading is the ease with which shaders can be modified to produce 
the desired results. Figure 1a shows a more realistic brick that resulted from small modifica­tions to 
the simple brick shader. It includes a simple proce­durally-defined bump map to indent the mortar, high-frequency 
band-limited noise to simulate grains in the mortar and brick, patches of color variation within each 
brick to simulate swirls of color in the clay, and variations in color from brick to brick. The remainder 
of this section covers some of the details of the pfman shading language and some of the differences 
between it and the RenderMan shading language. These differences are 1. the introduction of a fixed-point 
data type, 2. the use of arrays for points and vectors, 3. the introduction of transformation attributes, 
 4. the explicit listing of all shader parameters, and  5. the ability to link with external functions. 
Of these changes, 1 and 2 allow us to use the faster and more efficient fixed-point math on our SIMD 
processing elements. The third covers a hole in the RenderMan standard that has since been fixed. The 
fourth was not necessary, but simplified the im­plementation of our compiler. Finally, item 5 is a result 
of our language being compiled instead of interpreted (in contrast to most off-line renderer implementations 
of RenderMan). 3.1 Types As with the RenderMan shading language, variables may be declared to be either 
uniform or varying. A varying vari­able is one that might vary from pixel to pixel texture coordi­nates 
for example. A uniform variable is one that will never vary from pixel to pixel. For the brick shader 
presented above, the width, height and color of the bricks and the thickness and color of the mortar 
are all uniform parameters. These control the appearance of the brick, and allow us to use the same shader 
for a variety of different styles of brick. RenderMan has one representation for all numbers: floating­point. 
We also support floating-point (32-bit IEEE single preci­sion format) because it is such a forgiving 
representation. This format has about 10 7 relative error for the entire range of num­bers from 10-38 
to 1038. However, for some quantities used in shading this range is overkill (for colors, an 8 to 16 
bit fixed­point representation can be sufficient [Hill97]). Worse, there are cases where floating-point 
has too much range but not enough precision. For example, a Mandelbrot fractal shader has an insa­tiable 
appetite for precision, but only over the range [ 2,2] (Figure 4). In this case, it makes much more sense 
to use a fixed­ ab Figure 4: Fixed-point vs. floating-point comparison. a) Mandelbrot set computed using 
floating-point. b) Mandelbrot set computed using fixed-point point format instead of a 32 bit floating-point 
format: the float­ing-point format wastes one of the four bytes for an exponent that is hardly used. 
In general, it is easiest to prototype a shader using floating-point, then change to fixed-point as necessary 
for mem­ory usage, precision, and speed. Our fixed-point types may be signed or unsigned and have two 
parameters: the size in bits and an exponent, written fixed<size,exponent>. Fixed-point behaves like 
floating-point where the exponent is a compile-time constant. Small exponents can be interpreted as the 
number of fractional bits: a two byte integer is fixed<16,0>, while a two byte pure fraction is fixed<16,16>. 
Like recent versions of the RenderMan shading language [Pixar97], pfman supports arrays of its basic 
types. However, where RenderMan uses separate types for points, vectors, nor­mals, and colors, pfman 
uses arrays with transformation attrib­utes. By making each point be an array of floating-point or fixed­point 
numbers, we can choose the appropriate representation independently for every point. A transformation 
attribute indi­cates how the point or vector should be transformed. For exam­ple, points use the regular 
transformation matrix, vectors use the same transformation but without translation, and normals use the 
adjoint or inverse without translation. We also include a trans­formation attribute for texture coordinates, 
which are trans­formed by the OpenGL texture transformation matrix. 3.2 Explicit Shader Parameters RenderMan 
defines a set of standard parameters that are im­plicitly available for use by every surface shader. 
The surface shader does not need to declare these parameters and can use them as if they were global 
variables. In pfman, these parameters must be explicitly declared. This allows us to construct a transfer 
map (discussed later in Section 6) that contains only those pa­rameters that are actually needed by the 
shader. In retrospect, we should have done a static analysis of the shader function to decide which built-in 
parameters are used. This would have made pfman that much more like RenderMan, and consequently that 
much easier for new users already familiar with RenderMan.  3.3 External Linking Compiling a pfman shader 
is a two-stage process. The pfman compiler produces C++ source code. This C++ code is then com­piled 
by a C++ compiler to produce an object file for the shader. The function definitions and calls in pfman 
correspond directly to C++ function definitions and calls. Thus, unlike most Render-Man implementations, 
we support calling C++ functions from the shading language and vice versa. This facility is limited to 
functions using types that the shading language supports. Compiling to C++ also provides other advantages. 
We ignore certain optimizations in the pfman compiler since the C++ com-Figure 5: Instances of a brick 
surface shader.  piler does them. One could also use the generated C++ code as a starting point for 
a hand-optimized shader. Such a hand­optimized shader would no longer be portable, and performing the 
optimization would require considerable understanding of the PixelFlow internals normally hidden by the 
shading language. Not surprisingly, no one has done this yet.  4 API The RenderMan standard [Upstill90] 
defines not only the shad­ing language, but also a graphics application program interface (API). This 
is a library of graphics functions that the graphics application can call to describe the scene to the 
renderer. We elected to base our API on OpenGL [Neider93] instead of Ren­derMan. OpenGL is a popular 
API for interactive graphics appli­cations, supported on a number of graphics hardware platforms. It 
provides about the same capabilities as the RenderMan API, with a similar collection of functions, but 
with more focus on interactive graphics. By using OpenGL as our base we can easily port applications 
written for other hardware. We extended OpenGL to support procedural shading [Leech98]. We required that 
the procedural shading extensions have no impact on applications that do not use procedural shad­ing. 
We also endeavored to make them fit the framework and philosophy of OpenGL. Our efforts to extend OpenGL 
should be readily usable by future real-time shading language systems. Following the OpenGL standard, 
all of our extensions have the suffix EXT. We will follow that convention here to help clarify what is 
already part of OpenGL and what we added. OpenGL functions also include suffix letters (f, i, s, etc.) 
indicating the operand type. For brevity, we omit these in the text. 4.1 Loading Functions Procedural 
surface shaders and lights are written as pfman func­tions. The new API call, glLoadExtensionCodeEXT, 
loads a shader. Currently, we do not support dynamic linking of surface or light functions, so this call 
just declares which shaders will be used. In the future, we do plan to dynamically load shaders. 4.2 
Shading Parameters On PixelFlow, the default shader implements the OpenGL shading model. Applications 
that do not use procedural shad­ing use this default OpenGL shader without having to know any of the 
shading extensions to OpenGL. We set the values for shading parameters using the glMaterial call, already 
used by OpenGL to set parameters for the built-in shading model. Parameters set in this fashion go into 
the OpenGL global state, where they may be used by any shader. Any shader can use the same parameters 
as the OpenGL shader simply by sharing the same parameter names, or it can define its own new parameter 
names. OpenGL also has a handful of other, parameter-specific, calls. glColor can be set to change any 
of several possible color parameters, each of which can also be changed with glMaterial. We created similar 
parameter name equivalents for glNormaland glTexCoord. Other shaders may use these names to access the 
normals set with glNormal and texture coordinates from glTexcoord. 4.3 Shader Instances The RenderMan 
API allows some parameter values to be set when a shader function is chosen. Our equivalent is to allow 
certain bound parameter values. A shading function and its bound parameters together make a shader instance 
(or some­times just shader) that describes a particular type of surface. Because the character of a shader 
is as much a product of its parameter settings as its code, we may create and use several instances of 
each shading function. For example, given the brick shading function of Figure 3, we can define instances 
for fat red bricks and thin yellow bricks by using different bound values for the width, height, and 
color of the bricks (Figure 5). To set the bound parameter values for an instance, we use a glBoundMaterialEXT 
function. This is equivalent to gl-Material, but operates only on bound parameters. We create a new instance 
with a glNewShaderEXT, gl-EndShaderEXT pair. This is similar to the way OpenGL de­fines other objects, 
for example display list definitions are brack­eted by calls to glNewList and glEndList. glNewSha­derEXT 
takes the shading function to use and returns a shader ID that can be used to identify the instance later. 
Between the glNewShaderEXT and glEndShaderEXT we use glSha­derParameterBindingEXT, which takes a parameter 
ID and one of GL_MATERIAL_EXT or GL_BOUND_MATER-IAL_EXT. This indicates whether the parameter should 
be set by calls to glMaterial (for ordinary parameters) or gl-BoundMaterialEXT(for bound parameters). 
To choose a shader instance, we call glShaderEXT with a shader ID. Primitives drawn after the glShaderEXT 
call will use the specified shader instance. 4.4 Lights OpenGL normally supports up to eight lights, 
GL_LIGHT0 through GL_LIGHT7. New light IDs beyond these eight are created with glNewLightEXT. Lights 
are turned on and off through calls to glEnable and glDisable. Parameters for the lights are set with 
glLight, which takes the light ID, the parameter name, and the new value. As with surface shaders, we 
have a built-in OpenGL light that implements the OpenGL lighting model. The eight standard lights are 
pre-loaded to use this function. The OpenGL lighting model uses multiple colors for each light, with 
a different color for each of the ambient, diffuse and specular shading computations. In contrast, the 
RenderMan lighting model has only one color for each light. We allow a mix of these two styles. The only 
constraint is that surface shaders that use three different light colors can only be used with lights 
that provide three light colors. Surface shaders that follow the RenderMan model will use only the diffuse 
light color from lights that follow the OpenGL model.  5 PIXELFLOW We implemented the pfman shading 
language on PixelFlow, a high-performance graphics machine. The following sections give ab  Figure 
6: PixelFlow: a) machine organization. b) simplified view of the system. a brief overview of PixelFlow. 
For more details, refer to [Molnar92] or [Eyles97] 5.1 Low-level View A typical PixelFlow system consists 
of a host, a number of ren­dering nodes, a number of shading nodes, and a frame buffer node (Figure 6a). 
The rendering nodes and shading nodes are identical, so the balance between rendering performance and 
shading performance can be decided for each application. The frame buffer node is also the same, though 
it includes an addi­tional daughter card to produce video output. The host is con­nected through a daughter 
card on one of the rendering nodes. The pipelined design of PixelFlow allows the rendering per­formance 
to scale linearly with the number of rendering nodes and the shading performance to scale linearly with 
the number of shading nodes. Each rendering node is responsible for an effectively random subset of the 
primitives in the scene. The rendering nodes handle one 128x64 pixel region at a time. More precisely, 
the region is 128x64 image samples. When antialiasing, the image samples are blended into a smaller block 
of pixels after shading. For brevity, we will continue to use the word pixel , with the un­derstanding 
that sometimes they may be image samples instead of actual pixels. Since each rendering node has only 
a subset of the primitives, a region rendered by one node will have holes and missing poly­gons. The 
different versions of the region are merged using a technique called image composition. PixelFlow includes 
a spe­cial high-bandwidth composition network that allows image composition to proceed at the same time 
as pixel data communi­cation. As all of the rendering nodes simultaneously transmit their data for a 
region, the hardware on each node compares, pixel-by-pixel, the data it is transmitting with the data 
from the upstream nodes. It sends the closer of each pair of pixels down­stream. By the time all of the 
pixels reach their destination, one of the system s shading nodes, the composition is complete. Once 
the shading node has received the data, it does the sur­face shading for the entire region. In a PixelFlow 
system with n shading nodes, each shades every nth region. Once each region has been shaded, it is sent 
over the composition network (without compositing) to the frame buffer node, where the regions are collected 
and displayed. Each node has two RISC processors (HP PA-8000 s), a custom SIMD array of pixel processors, 
and a texture memory store. Each processing element of the SIMD array has 256 bytes of memory, an 8-bit 
ALU with support for integer multiplication, and an enable flag indicating the active processors. All 
enabled processors in the 128x64 array simultaneously compute, on their own data, the result of any operation. 
This provides a speedup of up to 8192 times the rate of a single processing element. Figure 7: Simple 
block diagram of a PixelFlow node. 5.2 High-level View The hardware and basic system software handle 
the details of scheduling primitives for the rendering nodes, compositing pixel samples from these nodes, 
assigning them to shading nodes, and moving the shaded pixel information to the frame buffer. Conse­quently, 
it is possible to take the simplified view of PixelFlow as a simple pipeline (Figure 6b). This view is 
based on the passage of a single displayed pixel through the system. Each displayed pixel arrives at 
the frame buffer, having been shaded by a single shading node. We can ignore the fact that displayed 
pixels in other regions were shaded by different physical shading nodes. Before arriving at the shading 
node, the pixel was part of a primitive on just one of the rendering nodes. We can ignore the fact that 
other pixels may display different primitives from dif­ferent rendering nodes. Only the rendering nodes 
make use of the second RISC proc­essor. The primitives assigned to the node are split between the processors. 
We can take the simplified view that there is only one processor on the node, and let the lower level 
software han­dle the scheduling between the physical processors. Figure 7 is simple block diagram of 
a PixelFlow node with these simplifica­tions. Each node is connected to two communication networks. The 
geometry network (800 MB/s in each direction), handles information about the scene geometry, bound parameter 
values, and other data bound for the RISC processors. It is 32 bits wide, operating at 200 MHz. The composition 
network (6.4 GB/s in each direction) handles transfers of pixel data from node to node. It is 256 bits 
wide, also operating at 200 MHz. Since our simpli­fied view of the PixelFlow system hides the image composition, 
it is reasonable to simply refer to the composition network as a pixel network.  6 IMPLEMENTATION Implementation 
of a shading language on PixelFlow requires optimizations. Some are necessary to achieve the targeted 
inter­active rates of 20-30 frames per second, whereas others are nec­essary just to enable shaders to 
run on PixelFlow. The three scarce resources impact our PixelFlow implementation: time, communication 
bandwidth, and memory. In this section, we pres­ent optimizations to address each. 6.1 Execution Optimizations 
Our target frame rate of 30 frames per second translates to 33 ms per frame. The system pipelining means 
that most of this time is actually available for shading. Each shading node can handle one 128x64 region 
at a time and a 1280x1024 screen (or 640x512 screen with 4-sample antialiasing) contains 160 such regions. 
On a system with four shading nodes, each is responsible for 40 regions and can take an average of 825 
Js shading each region. On a larger system with 16 shading nodes, each is responsible for Table 1: Memory 
and performance summary. shader bytes free execution time brick 46 613.15 Js ripple reflection 59 1058.07 
Js planks 105 532.30 Js bowling pin 86 401.96 Js nanoManipulator 1 75 567.95 Js nanoManipulator 2 1 2041.44 
Js nanoManipulator 3 51 1638.67 Js 10 regions and can spend an average of 3.3 ms shading a region. Table 
1 shows per-region execution times fore some sample shaders. The first four shaders appear in Figure 
1. The other shaders were written by the UNC nanoManipulator project for surface data visualization. 
6.1.1 Deferred Shading Deferred shading is the technique of performing shading compu­tations on pixels 
only after the visible pixels have been deter­mined [Whitted81][Deering88][Ellsworth91]. It provides 
several advantages for the execution of surface shading functions. First, no time is wasted on shading 
computations for pixels that will not be visible. Second, our SIMD array can simultaneously evaluate 
a single surface shader instance on every primitive that uses it in a 128x64 region. Finally, it decouples 
the rendering performance and shading performance of the system. To handle more complex shading, add 
more shading hardware. To handle more complex geometry, add more rendering hardware. 6.1.2 Uniform and 
Varying RenderMan has uniform and varying types (Section 3.1), in part for the efficiency of their software 
renderer. A uniform expres­sion uses only uniform operands and has a uniform result; a varying expression 
may have both uniform and varying operands but has a varying result. As Pixar s prman renderer evaluates 
the shading on a surface, it computes uniform expressions only once, sharing the results with all of 
the surface samples, but loops over the surface samples to compute the varying expressions. We can use 
a similar division of labor. The microprocessor on each PixelFlow node can compute the result of a single 
operation much faster than the SIMD array; but the microprocessor pro­duces one result, while the SIMD 
array can produce a different result on each of the 8K pixel processing elements. If the value is the 
same everywhere, it is faster for the microprocessor to com­pute and broadcast the result to the pixel 
processors. If the value is different at different pixel processors, it is much faster to al­low the 
SIMD array to compute all of the results in parallel. Since uniform expressions do not vary across the 
pixels, it is much more efficient to compute them using the microprocessor and store them in microprocessor 
memory. In contrast, varying expressions are the domain of the pixel processors. They can potentially 
have different values at every pixel, so must exist in pixel memory. They are fast and efficient because 
their storage and operations are replicated across the SIMD array. This same distinction between shared 
(uniform) and SIMD array (varying) memory was made by Thinking Machines for the Connection Operation 
16-bit fixed 32-bit fixed 32-bit float + 0.07 Js 0.13 Js 3.08 Js * 0.50 Js 2.00 Js 2.04 Js / 1.60 Js 
6.40 Js 7.07 Js sqrt 1.22 Js 3.33 Js 6.99 Js noise 5.71 Js  21.64 Js Table 2: Fixed-point and floating-point 
execution times for 128x64 SIMD array. Machine [ThinkingMachines89], though they called them mono and 
poly, and by MasPar for the MP-1 [MasPar90], though their terms were singular and plural. 6.1.3 Fixed-point 
We can achieve significant speed improvements by using fixed­point operations for varying computations 
instead of floating­point. Our pixel processors do not support floating-point in hardware: every floating-point 
operation is built from basic inte­ger math operations. These operations consist of the equivalent integer 
operation with bitwise shifts to align the operands and result. Fixed-point numbers may also require 
shifting to align the decimal points, but the shifts are known at compile-time. The timings of some fixed-point 
and floating-point operations are shown in Table 2. These operations may be done by as many as 8K pixel 
processors at once, yet we would still like them to be as fast as possible.  6.1.4 Math Functions We 
provide floating-point versions of the standard math library functions. An efficient SIMD implementation 
of these functions has slightly different constraints than a serial implementation. Piece-wise polynomial 
approximation is the typical method to evaluate transcendental math functions. This approach presents 
a problem on PixelFlow due to the handling of conditionals on a SIMD array. On a SIMD array, the condition 
determines which processing elements are enabled. The true part of an if/else is executed with some processing 
elements enabled, the set of enabled processors is flipped and the false part is executed. Thus the SIMD 
array spends the time to execute both branches of the if. This means that using a table of 32 polynomials 
takes as much time as a single polynomial with 32 times as many terms cover­ing the entire domain. Even 
so, a polynomial with, say, 160 terms is not practical. For each PixelFlow math function, we reduce the 
function domain using identities, but do not reduce it further. For example, the log of a floating-point 
number, m*2e, is e*log(2)+log(m). We fit log(m) with a single polyno­mial. Each polynomial is chosen 
to use as few terms as possible while remaining accurate to within the floating-point precision. Thus, 
we still do a piece-wise fit, but fit a single large piece with a polynomial of relatively high degree. 
While we provide accurate versions of the math functions, of­ten shaders do not really need the true 
function. With the rip­ple reflection shader in Figure 1b, it is not important that the ripples be sine 
waves. They just need to look like sine waves. For that reason, we also provide faster, visually accurate 
but numerically poor, versions of the math functions. The fast ver­sions use simpler polynomials, just 
matching value and first de­rivative at each endpoint of the range fit by the more exact ap­proximations. 
This provides a function that appears visually cor­ function exact fast sin 81.36 Js 45.64 Js cos 81.36 
Js 48.77 Js tan 93.25 Js 52.65 Js asin, acos 78.52 Js 47.50 Js atan 66.41 Js 35.34 Js atan2 66.17 Js 
35.15 Js exp 53.37 Js 37.86 Js exp2 51.09 Js 35.58 Js log 57.76 Js 21.57 Js log2 57.68 Js 21.49 Js Table 
3: Execution times for floating-point math functions on 128x64 SIMD array. // setup, compute base surface 
color illuminance() { // add in the contribution of one light } // wrap-up shaders can use. The results 
of the lighting computation, the color and direction of the light hitting each pixel, are stored in a 
special communications area to be shared by all surface shaders. The light functions themselves operate 
in the SIMD memory left over by the retained result of the greediest of the surface shader pre-illum 
stages. Above this high water mark, the light can Figure 8: Outline of a typical surface shader. rect 
but executes in about half the time.  6.1.5 Combined Execution Many shading functions have similar organizations. 
Combin­ing the execution of the common sections of code in multiple shaders can lead to large gains in 
performance. In the next few sections, we will discuss some of these methods. The easiest and most automatic 
of this class of optimizations is combined execu­tion of lights for all surface shaders. For some of 
the more tradi­tional surface shaders, involving image texture lookups and Phong shading, we can do further 
overlapped computation. 6.1.5.1 Lights One of the jobs of a surface shader is to incorporate the effects 
of each light in the scene. As in the RenderMan shading lan­guage, this is accomplished through the illuminance 
con­struct, which behaves like a loop over the active lights (Figure 8). This means that each surface 
shader effectively includes a loop over every light. For m shaders and n lights, this results in m*n 
light executions. This can be quite expensive since the lights themselves are procedural, and could be 
arbitrarily com­plex. Since the lights are the same for each of the m shaders, we compute each light 
just once and share its results among all of the shaders, resulting in only n light executions. We do 
this by interleaving the execution of all of the lights and shaders. We accomplish this interleaving 
by having each surface shader generate three instruction streams for the SIMD array. The first stream, 
which we call pre-illum, contains only the setup code (until the illuminance in Figure 8). The second 
stream con­tains the body of the illuminance construct. We call this the illum stream. Finally, the post-illum 
stream contains eve­rything after the illuminance. The lights themselves create their own stream of SIMD 
commands. The interleaving pattern of these streams is shown in Figure 9. The SIMD memory usage of the 
surfaces and lights must be chosen in such a way that each has room to operate, but none conflict. The 
surface shaders will not interfere with each other since any one pixel can only use one surface shader. 
Different surface shaders already use different pixel memory maps. Lights, however, must operate in an 
environment that does not disturb any surface shader, but provides results in a form that all surface 
freely allocate whatever memory it needs. The illum, and post-illum streams of all shaders can use all 
available mem­ory without interfering with either the other surfaces or the lights.  6.1.5.2 Surface 
Position For image composition, every pixel must contain the Z-buffer depth of the closest surface visible 
at that pixel. This Z value, along with the position of the pixel on the screen, is sufficient to compute 
where the surface sample is in 3D. Since the surface position can be reconstructed from these pieces 
of information, we do not store the surface position in pixel memory during ren­dering or waste composition 
bandwidth sending it from the ren­dering nodes to the shading nodes. Instead, we compute it on the shading 
nodes in a phase we call pre-shade, which occurs before any shading begins. Thus, we share the execution 
time necessary to reconstruct the surface position. We also save mem­ory and bandwidth early in the graphics 
pipeline, helping with the other two forms of optimization, to be mentioned later. 6.1.5.3 Support for 
Traditional Shaders Some optimizations have been added to assist in cases that are common for forms of 
the OpenGL shading model. Unlike the earlier execution optimizations, these special-purpose optimiza­tions 
are only enabled by setting flags in the shader. Surface shaders that use only the typical Phong shading 
model can use a shared illum stream. This allows shaders to set up different parameters to the Phong 
shader, but the code for the Phong shading model runs only once. Surface shaders that use a certain class 
of texture lookups can share the lookup computations. These shaders know what texture they want to look 
up in the pre-illum phase, but don t require the results until the post-illum phase. The PixelFlow hard­ware 
does not provide any significant improvement in actual lookup time for shared lookups, but this optimization 
allows the SIMD processors to perform other operations while the lookup is in progress. To share the 
lookup processing, they place their texture ID and texture coordinates in special shared magic parameters. 
The results of the lookup are placed in another shared magic parameter by the start of the post-illumstage. 
 6.1.6 Cached Instruction Streams On PixelFlow, the microprocessor code computes the uniform expressions 
and all of the uniform control flow (if s with uni­form conditions, while s, for s, etc.), generating 
a stream of SIMD processor instructions. This SIMD instruction stream is buffered for later execution. 
The set of SIMD instructions for a shader only changes when some uniform parameter of the shader changes, 
so we cache the instruction stream and re-use it. Any parameter change sets a flag that indicates that 
the stream must be regenerated. For most non-animated shaders, this means that the uniform code executes 
only once, when the application starts.  6.2 Bandwidth Optimizations Communication bandwidth is another 
scarce resource on Pix­elFlow. As mentioned in Section 5, there are two communication paths between nodes 
in the PixelFlow system, the geometry net and composition net. We are primarily concerned with the com­position 
net bandwidth. While its total bandwidth is 6.4 GB/s, four bytes of every transfer are reserved for the 
pixel depth, giv­ing an effective bandwidth of 5.6 GB/s. Since PixelFlow uses deferred shading, the 
complete set of varying shading parameters and the shader ID must be trans­ferred across the composition 
network. The final color must also be transferred from the shader node to the frame buffer. How­ever, 
the design of the composition network allows these two transfers to be overlapped, so we really only 
pay for the band­width to send data for each visible pixel from the rendering nodes to shading nodes. 
At 30 frames per second on a 1280x1024 screen, the maximum communication budget is 142 bytes per pixel. 
To deal with this limited communication budget, we must perform some optimizations to reduce the number 
of parameters that need to be sent from renderer node to shader node. 6.2.1 Shader-Specific Maps Even 
though each 128x64 pixel region is sent as a single trans­fer, every pixel could potentially be part 
of a different surface. Rather than use a transfer that is the union of all the parameters needed by 
all of those surface shaders, we allow each to have its own tailored transfer map. The first two bytes 
in every map con­tain the shader ID, which indicates what transfer map was used and which surface shader 
to run. 6.2.2 Bound Parameters The bound parameters of any shader instance cannot change from pixel 
to pixel (Section 4.3), so they are sent over the geometry network directly to the shading nodes. Since 
the shader nodes deal with visible pixels without any indication of when during the frame they were rendered, 
we must restrict bound parameters to only change between frames. Bound uniform parameters are used directly 
by the shading function running on the microproc­essor. Any bound varying parameters must be loaded into 
pixel memory. Based on the shader ID stored in each pixel, we identify which pixels use each shader instance 
and load their bound varying parameters into pixel memory before the shader exe­cutes. Any parameter 
that is bound in every instance of a shader should probably be uniform, since this gives other memory 
and execution time gains. However, it is occasionally helpful to have bound values for varying shading 
parameters. For example, our brick shader may include a dirtiness parameter. Some brick walls will be 
equally dirty everywhere. Others will be dirtiest near the ground and clean near the top. The instance 
used in one wall may have dirtiness as a bound parameter, while the instance used in a second wall allows 
dirtiness to be set using glMaterialwith a different value at each vertex. However, the set of parameters 
that should logically be bound in some instances and not in others is small. Allowing bound values for 
varying parameters would be only a minor bandwidth savings, were it not for another implication of deferred 
shading. Since bound parameters can only change once per frame, we find parameters that would otherwise 
be uniform are being declared as varying solely to allow them to be changed with glMaterial from primitive 
to primitive (instead of requiring hundreds of instances). This means that someone writing a Pix­elFlow 
shader may make a parameter varying for flexibility even though it will never actually vary across any 
primitives. Allowing instances to have bound values for all parameters helps counter the resulting explosion 
of pseudo-varying parameters.  6.3 Memory Optimizations The most limited resource when writing shaders 
on PixelFlow is pixel memory. The texture memory size (64 megabytes) affects the size of image textures 
a shader can use in its computations, but does not affect the shader complexity. The microprocessor memory 
(128 megabytes), is designed to be sufficient to hold large geometric databases. For shading purposes 
it is effectively unlimited. However, the pixel memory, at only 256 bytes, is quite limited. From those 
256 bytes, we further subtract the shader input parameters and an area used for communication between 
the light shaders and surface shaders. What is left is barely enough to support a full-fledged shading 
language. The memory limitations of Pixel-Planes 5 were one of the reasons that, while it supported a 
form of procedural shading, it could not handle a true shading language. In this section we highlight 
some of the pfman features and optimizations made by the pfman com­piler to make this limited memory 
work for real shaders. 6.3.1 Uniform vs. Varying We previously mentioned uniform and varying parameters 
in the context of execution optimizations. Bigger gains come from the storage savings: uniform values 
are stored in the large main memory instead of the much more limited pixel memory. 6.3.2 Fixed-point 
PixelFlow can only allocate and operate on multiples of single bytes, yet we specify the size of our 
fixed-point numbers in bits. This is because we can do a much better job of limiting the sizes of intermediate 
results in expressions with a more accurate idea of the true range of the values involved. For example, 
if we add two two-byte integers, we need three bytes for the result. How­ever, if we know the integers 
really only use 14 bits, the result is only 15 bits, which still fits into two bytes. A two-pass analysis 
determines the sizes of intermediate fixed-point results. A bottom-up pass determines the sizes neces­sary 
to keep all available precision. It starts with the sizes it knows (e.g. from a variable reference) and 
combines them ac­cording to simple rules. A top-down pass limits the fixed-point sizes of the intermediate 
results to only what is necessary.  6.3.3 Memory Allocation The primary feature that allows shaders 
to have any hope of working on PixelFlow is the memory allocation done by the compiler. Since every surface 
shader is running different code, we use a different memory map for each. We spend considerable compile-time 
effort creating these memory maps. Whereas even the simplest of shaders may define more than 256 bytes 
of varying variables, most shaders do not use that many variables at once. We effectively treat pixel 
memory as one giant register pool, and perform register allocation on it during compilation. This is 
one of the most compelling reasons to use a compiler when writing surface shaders to run on graphics 
hard­ware. It is possible to manually analyze which variables can co­exist in the same place in memory, 
but it is not easy. One of the authors did just such an analysis for the Pixel-Planes 5 shading code. 
It took about a month. With automatic allocation, it sud­denly becomes possible to prototype and change 
shaders in min­utes instead of months. The pfman memory allocator performs variable lifetime analy­sis 
by converting the code to a static single assignment (SSA) form [Muchnick97][Briggs92] (Figure 10). First, 
we go through the shader, creating a new temporary variable for the result of every assignment. This 
is where the method gets its name: we do i = 1; i1 = 1; i1 = 1; i = i + 1; i2 = i1 + 1; i2_3 = i1 + 
1; if (i > j) if (i2 > j1) if (i2_3 > j1) i = 5; i3 = 5; i2_3 = 5; j = i; j2 = .(i2,i3); j2 = i2_3; abc 
Figure 10: Example of lifetime analysis using SSA. a) original code fragment. b) code fragment in SSA 
form. Note the new variables used for every assignment and the use of the .-function for the ambiguous 
assignment. c) final code fragment with .-functions merged. a static analysis, resulting in one and only 
one assignment for every variable. In some places, a variable reference will be am­biguous, potentially 
referring to one of several of these new tem­poraries. During the analysis, we perform these references 
using a r-function. The .-function is a pseudo-function-call indicating that, depending on the control 
flow, one of several variables could be referenced. For example, the value of i in the last line of Figure 
10b, could have come from either i2 or i3. In these cases, we merge the separate temporaries back together 
into a single variable. What results is a program with many more vari­ables, but each having as short 
a lifetime as possible. Following the SSA lifetime analysis, we make a linear pass through the code, 
mapping these new variables to free memory as soon as they become live, and unmapping them when they 
are no longer live. Variables can only become live at assignments and can only die at their last reference. 
As a result of these two passes, variables with the same name in the user s code may shift from memory 
location to memory location. We only allow these shifts when the SSA name for the variable changes. One 
of the most noticeable effects of the this analysis is that a variable that is used independently in 
two sections of code does not take space between execution of the sections. Table 4 shows the performance 
of the memory allocation on an assortment of shaders. Table 1 shows the amount of memory left after the 
shading parameters, shader, light, and all overhead have been factored out.   7 CONCLUSIONS We have 
demonstrated an interactive graphics platform that sup­ports procedural shading through a shading language. 
With our system, we can write shaders in a high-level shading language, compile them, and generate images 
at 30 frames per second or more. To accomplish this, we modified a real-time API to sup­port procedural 
shading and an existing shading language to include features beneficial for a real-time implementation. 
Our API is based on OpenGL, with extensions to support the added flexibility of procedural shading. We 
believe the decision to extend OpenGL instead of using the existing RenderMan API was a good one. Many 
existing interactive graphics applications are already written in OpenGL, and can be ported to PixelFlow 
with relative ease. Whereas the RenderMan API has better sup­port of curved surface primitives important 
for its user commu­nity, OpenGL has better support for polygons, triangle strips and display lists, important 
for interactive graphics hardware. Our shading language is based on the RenderMan shading language. Of 
the differences we introduced, only the fixed-point data type was really necessary. We expect that future 
hardware­assisted shading language implementations may also want simi­lar fixed-point extensions. The 
other changes were either done for implementation convenience or to fill holes in the Render-Man shading 
language definition that have since been addressed by more recent versions of RenderMan. If we were starting 
the Table 4: Shader memory usage in bytes. shader total varying varying (uniform + only with varying) 
allocation simple brick 171 97 16 fancy brick 239 175 101 ripple reflection 341 193 137 wood planks 216 
152 97 project over again today, we would just add fixed-point to the current version of the RenderMan 
shading language. We have only addressed surface shading and procedural lights. RenderMan also allows 
other types of procedures, all of which could be implemented on PixelFlow, but have not been. We also 
do not have derivative functions, an important part of the Ren­derMan shading language. Details on how 
these features could be implemented on PixelFlow can be found in [Olano98] We created a shading language 
compiler, which hides the de­tails of our hardware architecture. The compiler also allows us to invisibly 
do the optimizations necessary to run on our hardware. We found the most useful optimizations to be those 
that happen automatically. This is consistent with the shading language phi­losophy of hiding system 
details from the shader writer. Using a compiler and shading language to mask details of the hardware 
architecture has been largely successful, but the hard­ware limitations do peek through as shaders become 
short on memory. Several of our users have been forced to manually con­vert portions of their large shaders 
to fixed-point to allow them to run. Even after such conversion, one of the shaders in Table 1 has only 
a single byte free. If a shader exceeds the memory re­sources after it is converted to fixed-point, it 
cannot run on Pix­elFlow. If this becomes a problem, we can add the capability to spill pixel memory 
into texture memory, at a cost in execution speed. Any graphics engine capable of real-time procedural 
shading will require significant pixel-level parallelism, though this par­allelism may be achieved through 
MIMD processors instead of SIMD as we used. For the near future, this level of parallelism will imply 
a limited per-pixel memory pool. Consequently, we expect our memory optimization techniques to be directly 
useful for at least the next several real-time procedural-shading ma­chines. Our bandwidth optimization 
techniques are somewhat specific to the PixelFlow architecture, though should apply to other deferred 
shading systems since they need to either transmit or store the per-pixel appearance parameters between 
rendering and shading. Deferred shading and our experience with function approximation will be of interest 
for future SIMD machines. The other execution optimizations, dealing with tasks that can be done once 
instead of multiple times, will be of lasting applica­bility to anyone attempting a procedural shading 
machine. There is future work to be done extending some of our optimi­zation techniques. In particular, 
we have barely scratched the surface of automatic combined execution of portions of different shaders. 
We do only the most basic of these optimizations auto­matically. Some others we do with hints from the 
shader-writer, whereas other possible optimizations are not done at all. For example, we currently run 
every shader instance independently. It would be relatively easy to identify and merge instances of the 
same shader function that did not differ in any uniform parame­ters. For a SIMD machine like ours, this 
would give linear speed improvement with the number of instances we can execute to­gether. Even more 
interesting would be to use the techniques of [Dietz92] and [Guenter95] to combine code within a shader 
and between shader instances with differing uniform parameter val­ues. Creating a system that renders 
in real-time using a shading language has been richly rewarding. We hope the experiences we have outlined 
here will benefit others who attempt real-time procedural shading.  8 ACKNOWLEDGMENTS PixelFlow was 
a joint project of the University of North Carolina and Hewlett-Packard and was supported in part by 
DARPA order numbers A410 and E278, and NSF grant numbers MIP-9306208 and MIP-9612643. The entire project 
team deserves recognition and thanks; this work exists by virtue of their labors. We would like to single 
out Voicu Popescu for his work on pfman memory allocation as well as the other project members who worked 
on the pfman com­piler, Peter McMurry and Rob Wheeler. Thanks to Steve Molnar and Yulan Wang for their 
early work on programmable shading on PixelFlow. Thanks to Jon Leech for his work on the OpenGL extensions. 
We would also like to express special thanks to the other people who worked on the PixelFlow shading 
system and the API extensions: Dan Aliaga, Greg Allen, Jon Cohen, Rich Holloway, Roman Kuchkuda, Paul 
Layne, Carl Mueller, Greg Pruett, Brad Ritter, and Lee Westover. Finally, we would like to gratefully 
acknowledge the help and patience of those who have used pfman, and provided several of the shaders used 
in this paper. They are Arthur Gregory, Chris Wynn, and members of the UNC nanoManipulator project, under 
the direction of Russ Taylor (Alexandra Bokinsky, Chun-Fa Chang, Aron Helser, Sang-Uok Kum, and Renee 
Maheshwari).  References [Briggs92] Preston Briggs, Register Allocation via Graph Coloring, PhD Dissertation, 
Department of Computer Science, Rice University, Houston, Texas, 1992. [Cook84] Robert L. Cook, Shade 
Trees , Proceedings of SIGGRAPH 84 (Minneapolis, Minnesota, July 23 27, 1984). In Computer Graphics, 
v18n3. ACM SIGGRAPH, July 1984. pp. 223 231. [Cook87] Robert L. Cook, The Reyes Image Rendering Architecture 
, Proceedings of SIGGRAPH 87 (Anaheim, California, July 27 31, 1987). In Computer Graphics, v21n4. ACM 
SIGGRAPH, July 1987. pp. 95 102. [Deering88] Michael Deering, Stephanie Winner, Bic Schediwy, Chris Duffy 
and Neil Hunt, The Triangle Processor and Normal Vector Shader: A VLSI System for High Performance Graphics 
, Proceedings of SIG-GRAPH 88 (Atlanta, Georgia, August 1 5, 1988). In Computer Graphics, v22n4, ACM 
SIGGRAPH, August 1988. pp. 21 30. [Dietz92] Henry G. Dietz, Common Subexpression Induction , Proceedings 
of the 1992 International Conference on Parallel Processing (Saint Charles, Illinois, August 1992). pp. 
174 182. [Ellsworth91] David Ellsworth, Parallel Architectures and Algorithms for Real-time Synthesis 
of High-quality Images using Deferred Shading . Workshop on Algorithms and Parallel VLSI Architectures 
(Pont-á-Mousson, France, June 12, 1990). [Eyles97] John Eyles, Steven Molnar, John Poulton, Trey Greer, 
Anselmo Lastra, Nick England and Lee Westover, PixelFlow: The Realiza­tion , Proceedings of the 1997 
SIGGRAPH/Eurographics Workshop on Graphics Hardware (Los Angeles, California, August 3 4, 1992). ACM 
SIGGRAPH, August 1997. pp. 57 68. [Gritz96] Larry Gritz and James K. Hahn, BMRT: A Global Illumination 
Implementation of the RenderMan Standard , Journal of Graphics Tools, v1n3, 1996. pp. 29 47. [Guenter95] 
Brian Guenter, Todd B. Knoblock and Erik Ruf, Specializing Shaders , Proceedings of SIGGRAPH 95 (Los 
Angeles, California, August 6 11, 1995). In Computer Graphics Proceedings, Annual Conference Series, 
ACM SIGGRAPH, 1995. pp. 343 348. [Hanrahan90] Pat Hanrahan and Jim Lawson, A Language for Shading and 
Lighting Calculations , Proceedings of SIGGRAPH 90 (Dallas, Texas, August 6 10, 1990). In Computer Graphics, 
v24n4. ACM SIG-GRAPH, August 1990. pp. 289 298. [Hill97] B. Hill, Th. Roger and F. W. Vorhagen, Comparative 
Analysis of the Quantization of Color Spaces on the Basis of the CIELAB Color-Difference Formula , ACM 
Transactions on Graphics, v16n2. ACM, April 1997. pp. 109 154. [Lastra95] Anselmo Lastra, Steven Molnar, 
Marc Olano and Yulan Wang, Real-time Programmable Shading , Proceedings of the 1995 Sympo­sium on Interactive 
3D Graphics (Monterey, California, April 9 12, 1995). ACM SIGGRAPH, 1995. pp. 59 66. [Leech98] Jon Leech, 
OpenGL Extensions and Restrictions for PixelFlow , Technical Report TR98-019, Department of Computer 
Science, Uni­versity of North Carolina at Chapel Hill. [MasPar90] MasPar Computer Corporation, MasPar 
Parallel Application Language (MPL) User Guide, 1990. [Max81] Nelson L. Max, Vectorized Procedural Models 
for Natural Ter­rain: Waves and Islands in the Sunset , Proceedings of SIGGRAPH 81 (Dallas, Texas, July 
1981). In Computer Graphics, v15n3. ACM SIGGRAPH, August 1981. pp. 317 324. [Molnar92] Steven Molnar, 
John Eyles and John Poulton, PixelFlow: High­speed Rendering Using Image Composition , Proceedings of 
SIG-GRAPH 92 (Chicago, Illinois, July 26 31, 1992). In Computer Graphics, v26n2. ACM SIGGRAPH, July 1992. 
pp. 231 240. [Muchnick97] Steven Muchnick, Compiler Design and Implementation. Morgan Kaufmann, San Francisco, 
CA, 1997. [Neider93] Jackie Neider, Tom Davis and Mason Woo, OpenGL Program­ming Guide: the official 
guide to learning OpenGL release 1., Addi­son-Wesley, 1993. [Olano98] Marc Olano, A Programmable Pipeline 
for Graphics Hardware, PhD Dissertation, Department of Computer Science, University of North Carolina 
at Chapel Hill, 1998. [Perlin85] Ken Perlin, An Image Synthesizer , Proceedings of SIGGRAPH 85 (San Francisco, 
California, July 22 26, 1985). In Computer Graphics, v19n3. ACM SIGGRAPH, July 1985. pp. 287 296. [Pixar97] 
Pixar Animation Studios, PhotoRealistic RenderMan 3.7 Shading Language Extensions. Pixar animation studios, 
March 1997. [Rhoades92] John Rhoades, Greg Turk, Andrew Bell, Andrei State, Ulrich Neumann and Amitabh 
Varshney, Real-time procedural textures , Proceedings of the 1992 Symposium on Interactive 3D Graphics 
(Cambridge, Massachusetts, March 29 April 1, 1992). In Computer Graphics special issue. ACM SIGGRAPH, 
March 1992. pp. 95 100. [Slusallek94] Philipp Slusallek, Thomas Pflaum and Hans-Peter Seidel, Implementing 
RenderMan Practice, Problems and Enhancements , Proceedings of Eurographics 94. In Computer Graphics 
Forum, v13n3, 1994. pp. 443 454. [ThinkingMachines89] Thinking Machines Corporation, Connection Ma­chine 
Model CM-2 Technical Summary. Thinking Machines Corpo­ration, Version 5.1, May 1989. [Upstill90] Steve 
Upstill, The RenderMan Companion, Addison-Wesley, 1990. [Whitted81] T. Whitted and D. M. Weimer, A software 
test-bed for the development of 3-D raster graphics systems , Proceedings of SIG-GRAPH 81 (Dallas, Texas, 
July 1981). In Computer Graphics, v15n3. ACM SIGGRAPH, August 1981. pp. 271 277.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280860</article_id>
		<sort_key>169</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[Efficiently using graphics hardware in volume rendering applications]]></title>
		<page_from>169</page_from>
		<page_to>177</page_to>
		<doi_number>10.1145/280814.280860</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280860</url>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Graphics processors</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010389</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics processors</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Experimentation</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31050462</person_id>
				<author_profile_id><![CDATA[81100626078]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[R&#252;diger]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Westermann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. Erlangen-N&#252;rnberg, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39071498</person_id>
				<author_profile_id><![CDATA[81332497603]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ertl]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. Erlangen-N&#252;rnberg, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>166131</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[K. Akeley. Reality Engine Graphics. ACM Computer Graphics, Proc. SIG- GRAPH '93, pages 109-116, July 1993.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>197972</ref_obj_id>
				<ref_obj_pid>197938</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[B. Cabral, N. Cam, and J. Foran. Accelerated Volume Rendering and Tomographic Reconstruction Using Texture Mapping Hardware. In ACM Symposium on Volume Visualization '94, pages 91-98, 1994.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>897857</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[T.J. Cullip and U. Neumann. Accelerating Volume Reconstruction with 3D Texture Hardware. Technical Report TR93-027, University of North Carolina, Chapel Hill N.C., 1993.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>147155</ref_obj_id>
				<ref_obj_pid>147130</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[J. Danskin and P. Hanrahan. Fast Algorithms for Volume Ray Tracing. In ACM Workshop on Volume Visualization '92, pages 91-98, 1992.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>99316</ref_obj_id>
				<ref_obj_pid>99307</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[M. Garrity. Ray Tracing Irregular Volume Data. In ACM Workshop on Volume Visualization '90, pages 35-40, 1990.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617735</ref_obj_id>
				<ref_obj_pid>616022</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[C. Giertsen. Volume Visualization of Sparse Irregular Meshes. Computer Graphics and Applications, 12(2):40-48, 1992.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97913</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[P. Haeberli and K. Akeley. The Accumulation Buffer: Hardware Support for High-Quality Rendering. ACM Computer Graphics, P1vc. SIGGRAPH '90, pages 309-318, July 1990.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>2222956</ref_obj_id>
				<ref_obj_pid>2222896</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[M. Haubner, Ch. Krapichler, A. L6sch, K.-H. Englmeier, and van Eimeren W. Virtual Reality in Medicine - Computer Graphics and Interaction Techiques. IEEE Transactions on Information Technology in Biomedicine, 1996.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808594</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[J.T. Kajiya and B. R Von Herzen. Ray Tracing Volume Densities. ACM Computer Graphics, P1vc. SIGGRAPH '84, 18(3): 165-174, July 1984.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>949574</ref_obj_id>
				<ref_obj_pid>949531</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[W. KriJger. The Application of Transport Theory to the Visualization of 3-D Scalar Data Fields. In IEEE Visualization '90, pages 273-280, 1990.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192283</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[R Lacroute and M Levoy. Fast Volume Rendering Using a Shear-Warp Factorization of the Viewing Transform. Computer Graphics, Proc. SIGGRAPH '94, 28(4):451-458, 1994.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122748</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[D. Laur and R Hanrahan. Hierarchical Splatting: A Progressive Refinement Algorithm for Volume Rendering. ACM Computer Graphics, Proc. SIGGRAPH '93, 25(4):285-288, July 1991.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>78965</ref_obj_id>
				<ref_obj_pid>78964</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[M. Levoy. Efficient Ray Tracing of Volume Data. ACM Transactions on Graphics, 9(3):245-261, July 1990.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37422</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[W.E. Lorensen and H.E. Cline. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. ACM Computer Graphics, Proc. SIGGRAPH '87, 21(4):163-169, 1987.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>99315</ref_obj_id>
				<ref_obj_pid>99307</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[N. Max, R Hanrahan, and R. Crawfis. Area and Volume Coherence for Efficient Visualization of 3D Scalar Functions. In ACM Workshop on Volume Visualization '91, pages 27-33, 1991.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>951139</ref_obj_id>
				<ref_obj_pid>951087</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[C. Montani, R. Scateni, and R. Scopigno. Discretized Marching Cubes. In IEEE Visualization' 94, pages 281-287, 1994.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258871</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[J. Montrym, D. Baum, D. Dignam, and C. Migdal. Infinite Reality: A Real-Time Graphics System. Computer Graphics, Proc. SIGGRAPH '97, pages 293-303, July 1997.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258873</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[M. Peercy, J. Airy, and B. Cabral. Efficient Bump Mapping Hardware. Computer Graphics, P1vc. SIGGRAPH '97, pages 303-307, July 1997.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>833868</ref_obj_id>
				<ref_obj_pid>832271</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[H. Shen and C. Johnson. Sweeping Simplices: A Fast Iso-Surface Axtraction Algorithm for Unstructured Grids. In IEEE Visualization '95, pages 143-150, 1995.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>99322</ref_obj_id>
				<ref_obj_pid>99308</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[R Shirley and A. Tuchman. A Polygonal Approximation to Direct Scalar Volume Rendering. ACM Computer Graphics, P1vc. SIGGRAPH '90, 24(5):63-70, 1990.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>614370</ref_obj_id>
				<ref_obj_pid>614266</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[C. Silva and J. Mitchell. The Lazy Sweep Ray Casting Algorithm for Rendering Irregular Grids. Transactions on Visualization and Computer Graphics, 4(2), June 1997.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>236228</ref_obj_id>
				<ref_obj_pid>236226</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[C. Silva, J. Mitchell, and A. Kaufman. Fast Rendering of Irregular Grids. In ACM Symposium on Volume Visualization '96, pages 15-23, 1996.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>197971</ref_obj_id>
				<ref_obj_pid>197938</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[C. Stein, B. Becker, and N. Max. Sorting and hardware assisted rendering for volume visualization. In ACM Symposium on Volume Visualization '94, pages 83-90, 1994.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>236229</ref_obj_id>
				<ref_obj_pid>236226</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[A. Van Gelder and K. Kwansik. Direct Volume Rendering with Shading via Three-Dimensional Textures. In R. Crawfis and Ch. Hansen, editors, ACM Symposium on Volume Visualization '96, pages 23-30, 1996.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>267012</ref_obj_id>
				<ref_obj_pid>266989</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[R. Westermann and T. Ertl. The VSBUFFER: Visibility Ordering unstructured Volume Primitives by Polygon Drawing. In IEEE Visualization '97, pages 35- 43, 1997.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>130882</ref_obj_id>
				<ref_obj_pid>130881</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[J. Wilhelms and A. Van Gelder. Octrees for taster Iso-Surface Generation. ACM Transactions on Graphics, 11(3):201-297, July 1992.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>245001</ref_obj_id>
				<ref_obj_pid>244979</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[J. Wilhelms, A. van Gelder, P. Tarantino, and J. Gibbs. Hierarchical and Parallelizable Direct Volume Rendering for Irregular and Multiple Grids. In IEEE Visualization 1996, pages 57-65, 1996.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>130899</ref_obj_id>
				<ref_obj_pid>130826</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[R Williams. Visibility Ordering Meshed Polyhedra. ACM Transactions on Graphics, 11(2):102-126, 1992.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>147151</ref_obj_id>
				<ref_obj_pid>147130</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[R Williams and N. Max. A Volume Density Optical Model. In ACM Workshop on Volume Visualization '92, pages 61-69, 1992.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>902673</ref_obj_id>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[O. Wilson, A. Van Gelder, and J. Wilhelms. Direct Volume Rendering via 3D Textures. Technical Report UCSC-CRL-94-19, University of California, Santa Cruz, 1994.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>236233</ref_obj_id>
				<ref_obj_pid>236226</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[R. Yagel, D. Reed, A. Law, R Shih, and N. Shareef. Hardware Assisted Volume Rendering of Unstructured Grids by Incremental Slicing. In ACM Symposium on Volume Visualization '96, pages 55-63, 1996.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280861</article_id>
		<sort_key>179</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[The office of the future]]></title>
		<subtitle><![CDATA[a unified approach to image-based modeling and spatially immersive displays]]></subtitle>
		<page_from>179</page_from>
		<page_to>188</page_to>
		<doi_number>10.1145/280814.280861</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280861</url>
		<keywords>
			<kw><![CDATA[autocalibration]]></kw>
			<kw><![CDATA[calibration]]></kw>
			<kw><![CDATA[depth]]></kw>
			<kw><![CDATA[display]]></kw>
			<kw><![CDATA[image-based modeling]]></kw>
			<kw><![CDATA[image-based rendering]]></kw>
			<kw><![CDATA[intensity blending]]></kw>
			<kw><![CDATA[projection]]></kw>
			<kw><![CDATA[range]]></kw>
			<kw><![CDATA[reflectance]]></kw>
			<kw><![CDATA[spatially immersive display]]></kw>
			<kw><![CDATA[virtual environments]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Viewing algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>B.4.2</cat_node>
				<descriptor>Image display</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Digitizing and scanning</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Sampling</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Scanning</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Range data</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Time-varying imagery</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Imaging geometry</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Reflectance</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Surface fitting</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Shading</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Shape</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Tracking</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Color</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.9</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010506</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Document scanning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010253</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Tracking</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010249</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Shape inference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010248</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Video segmentation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010235</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Epipolar geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010506</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Document scanning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP40022816</person_id>
				<author_profile_id><![CDATA[81100022847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of North Carolina at Chapel Hill, Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43126356</person_id>
				<author_profile_id><![CDATA[81407593950]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Welch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of North Carolina at Chapel Hill, Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP42050942</person_id>
				<author_profile_id><![CDATA[81341489583]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Matt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cutts]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of North Carolina at Chapel Hill, Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31051079</person_id>
				<author_profile_id><![CDATA[81100639760]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Adam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lake]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of North Carolina at Chapel Hill, Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P171189</person_id>
				<author_profile_id><![CDATA[81430605322]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Lev]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stesin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of North Carolina at Chapel Hill, Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39076497</person_id>
				<author_profile_id><![CDATA[81339500019]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Henry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fuchs]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of North Carolina at Chapel Hill, Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>218424</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bajaj, C.L., F. Bernardini, and G. Xu. "Automatic reconstruction of surfaces and scalar fields from 3D scans," SIGGRAPH 95 Conference Proceedings, Annual Conference Series, ACM SIGGRAPH, Addison-Wesley, pp. 109-118, August 1995.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bennett, David T. Chairman and Co-Founder of Alternate Realities Corporation. Internet: http://www.virtual-reality.com. 215 Southport Drive Suite 1300 Morrisville, NC 27560.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bowen, Loftin, R. "Hands Across the Atlantic," IEEE Computer Graphics and Applications, Vol. 17, No. 2, pp. 78-79, March-April 1997.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237289</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Bryson, Steve, David Zeltzer, Mark T. Bolas, Bertrand de La Chapelle, and David Bennett. "The Future of Virtual Reality: Head Mounted Displays Versus Spatially Immersive Displays," SIGGRAPH 97 Conference Proceedings, Annual Conference Series, ACM SIGGRAPH, Addison-Wesley, pp. 485-486, August 1997.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Buxton, W., Sellen, A. &amp; Sheasby, M. "Interfaces for multiparty videoconferencing," In K. Finn, A. Sellen &amp; S. Wilber (Eds.). Video Mediated Communication. Hillsdale, N.J.: Erlbaum, pp. 385-400, 1997.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618420</ref_obj_id>
				<ref_obj_pid>616045</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Capin, Tolga K., Hansrudi Noser, Daniel Thalmann, Igor Sunday Pandzic and Nadia Magnenat Thalman. "Virtual Human Representation and Communication in VLNet," IEEE Computer Graphics and Applications, Vol. 17, No. 2, pp. 42-53, March-April 1997.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Chi, Vern, Matt Cutts, Henry Fuchs, Kurtis Keller, Greg Welch, Mark Bloomenthal, Elaine Cohen, Sam Drake, Russ Fish, Rich Riesenfeld. 1998. "A Wide Field-of-View Camera Cluster", University of North Carolina at Chapel Hill, Dept of Computer Science, Technical Report TR98-018.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Chien, C.H., Y.B. Sire, and J.K. Aggarwal. "Generation of volume/surface octree from range data," In The Computer Graphics Society Conference on Computer Vision and Pattern Recognition, pp. 254-260, June 1988.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Clodfelter, Robert M. "Predicting Display System Performance," Presented at the 1996 IMAGE Conference, Scottsdale, AZ, pp. 1-5, June 23-28, 1996.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Conner, D.B, Cutts, M., Fish, R., Fuchs, H., Holden, L., Jacobs, M., Loss, B., Markosian, L., Riesenfeld, R., and Turk, G. "An Immersive Tool for Wide-Area Collaborative Design," TeamCAD, the First Graphics Visualization, and Usability (GVU) Workshop on Collaborative Design. Atlanta, Georgia, May 12-13, 1997.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Connolly, C.I. "Cumulative generation of octree models from range data," Proceedings, Int'l. Conference Robotics, pp. 25-32, March 1984.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166134</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Cruz-Neira, Carolina, Daniel J. Sandin, and Thomas A. DeFanti. "Surround-Screen Projection-Based Virtual Reality: The Design and Implementation of the CAVE," Computer Graphics, SIGGRAPH Annual Conference Proceedings, 1993.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237269</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Curless, Brian, and Marc Levoy. "A Volumetric Method for Building Complex Models from Range Images," SIGGRAPH 96 onference Proceedings, Annual Conference Series, ACM SIGGRAPH, Addison-Wesley. pp. 303-312, 1996.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[DePiero, F.W., and Trivedi, M.M., "3-D Computer Vision using Structured Light: Design, Calibration, and Implementation Issues," Advances in Computers(43), 1996, Academic Press, pp.243-278]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618421</ref_obj_id>
				<ref_obj_pid>616045</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Dias, Jos6 Miguel Salles, Ricardo Galli, Ant6nio Carlos Almeida, Carlos A. C. Belo, and Jos6 Manuel Rebord~. "mWorld: A Multiuser 3D Virtual Environment," IEEE Computer Graphics and Applications, Vol. 17, No. 2., pp. 55-64, March-April 1997.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122723</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Dorsey, Julie O'B., Fransco X. Sillion, Donald Greenberg. "Design and Simulation of Opera Lighting and Projection Effects," SIGGRAPH 91 Conference Proceedings, Annual Conference Series, ACM SIGGRAPH, Addison-Wesley, pp. 41-50, 1991.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>147153</ref_obj_id>
				<ref_obj_pid>147130</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Edelsbrunner, H. and E.E Mucke. "Three-dimensional alpha shapes," In Workshop on Volume Visualization, pp. 75-105, October 1992.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Fuchs, Henry, Gary Bishop, Kevin Arthur, Leonard McMillan, Ruzena Bajcsy, Sang Lee, Hany Farid, and Takeo Kanade. "Virtual Space Teleconferencing Using a Sea of Cameras," Proceedings of the First International Symposium on Medical Robotics and Computer Assisted Surgery, (Pittsburgh, PA.) Sept 22-24, 1994.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192716</ref_obj_id>
				<ref_obj_pid>192593</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Gajewska, Hania, Jay Kistler, Mark S. Manasse, and David D. Redell. "Argo: A System for Distributed Collaboration," (DEC, Multimedia '94)]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>314290</ref_obj_id>
				<ref_obj_pid>314286</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Gibbs, Simon, Constantin Arapis and Christian J. Breiteneder. "TELEPORT-Towards Immersive Copresence," accepted for publication in ACM Multimedia Systems Journal, 1998.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>840067</ref_obj_id>
				<ref_obj_pid>839277</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Goncalves, Luis, Enrico Di Bernardo, Enrico Ursella, Pietro Perona. "Monocular Tracking of the Human Arm in 3D," Proc. of the 5th Inter. Conf. on Computer Vision, ICCV 95.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>648979</ref_obj_id>
				<ref_obj_pid>645309</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Hilton, A., A.J. Toddart, J. Illingworth, and T. Windeatt. "Reliable surface reconstruction from multiple range images," In Fouth European Conference on Computer Vision, Volume 1, pp. 117-126. April 1996.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Holmes, Richard E. "Common Projector and Display Modules for Aircraft Simulator Visual Systems," Presented at the IMAGE V Conference, Phoenix, AZ, June 19-22, pp. 81-88, 1990.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134011</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Hoppe, Hugues, Tony DeRose, Tom Duchamp, John McDonald, Werner Stuetzle. "Surface Reconstruction from Unorganized Points," SIGGRAPH 92 Conference Proceedings, Annual Conference Series, ACM SIGGRAPH, Addison-Wesley, pp. 71-76, 1992.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Hornbeck, Larry J., "Deformable-Mirror Spatial Light Modulators,"Proceedings SPIE, Vol. 1150, Aug 1989.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Hornbeck, Larry J., "Digital Light Processing for High- Brightness High-Resolution Applications," {cited 21 April 1998}. Available from http://www.ti.com/dlp/docs/business/resources/ white/hornbeck.pdf, 1995.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1241976</ref_obj_id>
				<ref_obj_pid>1241958</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Ichikawa, Y., Okada, K., Jeong, G., Tanaka, S. and Matsushita, Y.: "MAJIC Videoconferencing System: Experiments, Evaluation and Improvement'," In Proceedings of ECSCW'95, pp. 279-292, Sept. 1995.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>179687</ref_obj_id>
				<ref_obj_pid>179606</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Ishii, Hiroshi, Minoru Kobayashi, Kazuho Arita. "Iterative Design of Seamless Collaboration Media," CACM, Volume 37, Number 8, pp. 83-97, August 1994.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Jarvis, Kevin. "Real-time 60Hz Distortion Correction on a Silicon Graphics IG," Real-time Graphics, Vol. 5, No. 7, pp. 6-7. February 1997.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Kanade, Takeo and Haruhiko Asada. "Noncontact Visual Three-Dimensional Ranging Devices," Proceedings of SPIE: 3D Machine Perception. Volume 283, Pages 48-54. April 23-24, 1981.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Kanade, Takeo, Hiroshi Kano, Shigeru Kimura, Atsushi Yoshida, Kazuo Oda. "Development of a Video-Rate Stereo Machine," Proceedings of International Robotics and Systems Conference (IROS '95). pp. 95-100, Pittsburgh, PA., August 5-9, 1995.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Lacroix, Michel. "A HDTV Projector For Wide Field of View Flight Simulators," pp. 493-500. Presented at the IMAGE VI Conference, Scottsdale, AZ, July 14-17, 1992.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618422</ref_obj_id>
				<ref_obj_pid>616045</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Lamotte, Wire, Eddy Flerackers, Frank Van Reeth, Rae Earnshaw, Joao Mena De Matos. Visinet: Collaborative 3D Visualization and VR over ATM Networks. IEEE Computer Graphics and Applications, Vol. 17, No. 2, pp. 66-75, March-April 1997.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618416</ref_obj_id>
				<ref_obj_pid>616045</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Lehner, Valerie D., and Thomas A. DeFanti. "Distributed Virtual Reality: Supporting Remote Collaboration in Vehicle Design," IEEE Computer Graphics and Applications, Vol. 17, No. 2, pp. 13-17, March-April 1997.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Lyon, Paul. "Edge-blending Multiple Projection Displays On A Dome Surface To Form Continuous Wide Angle Fields-of-View," pp. 203-209. Proceedings of 7th I/ITEC, 1985.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618423</ref_obj_id>
				<ref_obj_pid>616045</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Macedonia, Michale R. and Stefan Noll. "A Transatlantic Research and Development Environment," IEEE Computer Graphics and Applications, Vol. 17, No. 2, pp. 76-82, March-April 1997.]]></ref_text>
				<ref_id>36</ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Mandeville, J., T. Furness, M. Kawahata, D. Campbell, E Danset, A. Dahl, J. Dauner, J. Davidson, K. Kandie, and E Schwartz. "GreenSpace: Creating a Distributed Virtual Environment for Global Applications," Proceedings of IEEE Networked Virtual Reality Workshop, 1995.]]></ref_text>
				<ref_id>37</ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Nayar, Shree, Masahiro Watanabe, Minori Noguchi. "Realtime Focus Range Sensor," Columbia University, CUCS-028-94.]]></ref_text>
				<ref_id>38</ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Neumann, Ulrich and Henry Fuchs, "A Vision of Telepresence for Medical Consultation and Other Applications," Proceedings of the Sixth International Symposium on Robotics Research, Hidden Valley, PA, Oct. 1-5, 1993, pp. 565-571.]]></ref_text>
				<ref_id>39</ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Ohya, Jun, Kitamura, Yasuichi, Takemura, Haruo, et al. "Realtime Reproduction of 3D Human Images in Virtual Space Teleconferencing," IEEE Virtual Reality International Symposium. Sep 1993.]]></ref_text>
				<ref_id>40</ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[Panorama Project, WWW description. {cited 13 January 1998}. Available from http://www.tnt.uni-hannover.de/project/eu/ panorama/overview.html]]></ref_text>
				<ref_id>41</ref_id>
			</ref>
			<ref>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[Raskar, Ramesh, Matt Cutts, Greg Welch, Wolfgang Sttierzlinger. "Efficient Image Generation for Multiprojector and Multisurface Displays," University of North Carolina at Chapel Hill, Dept of Computer Science, Technical Report TR98-016, 1998.]]></ref_text>
				<ref_id>42</ref_id>
			</ref>
			<ref>
				<ref_obj_id>897926</ref_obj_id>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[Raskar, Ramesh, Henry Fuchs, Greg Welch, Adam Lake, Matt Cutts. "3D Talking Heads : Image Based Modeling at Interactive rate using Structured Light Projection,"University of North Carolina at Chapel Hill, Dept of Computer Science, Technical Report TR98-017, 1998]]></ref_text>
				<ref_id>43</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134071</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[Segal, Mark, Carl Korobkin, Rolf van Widenfelt, Jim Foran and Paul Haeberli, "Fast Shadows and Lighting Effects using Texture Mapping," Computer Graphics (SIGGRAPH 92 Proceedings), pp. 249-252, July, 1992.]]></ref_text>
				<ref_id>44</ref_id>
			</ref>
			<ref>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[Tsai, Roger Y. "An Efficient and Accurate Camera Calibration Technique for 3D Machine Vision," Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, Miami Beach, FL, pp. 364-374, 1986.]]></ref_text>
				<ref_id>45</ref_id>
			</ref>
			<ref>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[Underkoffler, J. "A View From the Luminous Room," Personal Technologies, Vol. 1, No. 2, pp. 49-59, June 1997.]]></ref_text>
				<ref_id>46</ref_id>
			</ref>
			<ref>
				<ref_obj_id>274717</ref_obj_id>
				<ref_obj_pid>274644</ref_obj_pid>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[Underkoffler, J., and Hiroshi Ishii. "Illuminating Light: An Optical Design Tool with a Luminous-Tangible Interface," Proceedings of CHI '98, ACM, April 1998.]]></ref_text>
				<ref_id>47</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Of.ce of the Future: A Uni.ed Approach to Image-Based Modeling and Spatially Immersive Displays 
 Ramesh Raskar, Greg Welch, Matt Cutts, Adam Lake, Lev Stesin, and Henry Fuchs University of North Carolina 
at Chapel Hill and the NSF Science and Technology Center for Computer Graphics and Scienti.c Visualization 
 Abstract We introduce ideas, proposed technologies, and initial results for an of.ce of the future that 
is based on a uni.ed application of computer vision and computer graphics in a system that combines and 
builds upon the notions of the CAVE , tiled display systems, and image-based modeling. The basic idea 
is to use real-time computer vision techniques to dynamically extract per-pixel depth and re.ectance 
information for the visible surfaces in the of.ce including walls, furniture, objects, and people, and 
then to either project images on the surfaces, render images of the surfaces, or interpret changes in 
the surfaces. In the .rst case, one could designate every-day (potentially irregular) real surfaces in 
the of.ce to be used as spatially immersive display surfaces, and then project high-resolution graphics 
and text onto those surfaces. In the second case, one could transmit the dynamic image-based models over 
a network for display at a remote site. Finally, one could interpret dynamic changes in the surfaces 
for the purposes of tracking, interaction, or augmented reality applications. To accomplish the simultaneous 
capture and display we envision an of.ce of the future where the ceiling lights are replaced by computer 
controlled cameras and smart projectors that are used to capture dynamic image-based models with imperceptible 
structured light techniques, and to display high-resolution images on designated display surfaces. By 
doing both simultaneously on the designated display surfaces, one can dynamically adjust or autocalibrate 
for geometric, intensity, and resolution variations resulting from irregular or changing display surfaces, 
or overlapped projector images. Our current approach to dynamic image-based modeling is to use an optimized 
structured light scheme that can capture per-pixel depth and re.ectance at interactive rates. Our system 
implementation is not yet imperceptible, but we can demonstrate the approach in the laboratory. Our approach 
to rendering on the designated (potentially irregular) display surfaces is to employ a two-pass projective 
texture scheme to generate images that when projected onto the surfaces appear correct to a moving head­tracked 
observer. We present here an initial implementation of the overall vision, in an of.ce-like setting, 
and preliminary demonstrations of our dynamic modeling and display techniques. CB 3175, Sitterson Hall, 
Chapel Hill, NC, 27599-3175 {raskar, welch, cutts, lake, stesin, fuchs}@cs.unc.edu http://www.cs.unc.edu/~{raskar, 
welch, cutts, lake, stesin, fuchs} http://www.cs.brown.edu/stc/ CR Categories and Subject Descriptors: 
I.3.3 [Computer Graphics]: Picture/Image Generation Digitizing and scanning; Display algorithms; Viewing 
algorithms; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism Virtual reality; I.4.1 
[Image Processing and Computer Vision]: Digitization and Image Capture Imaging geometry; Re.ectance; 
Sampling; Scanning; I.4.8 [Image Processing and Computer Vision]: Scene Analysis Color; Range data; Shading; 
Shape; Surface .tting; Time-varying imagery; Tracking; I.4.9 [Image Processing and Computer Vision]: 
Applications; B.4.2 [Input/ Output and Data Communications] Input/Output Devices Image display Additional 
Key Words and Phrases: display, spatially immersive display, intensity blending, image-based modeling, 
image-based rendering, range, depth, re.ectance, projection, virtual environments, calibration, autocalibration. 
Figure 1: A conceptual sketch of the of.ce of the future. By replacing the normal of.ce lights with projectors, 
one could obtain precise control over all of the light in the of.ce. With the help of synchronized cameras, 
the geometry and re.ectance information can be captured for all of the visible surfaces in the of.ce 
so that one can project images on the surfaces, render images of the surfaces, or interpret changes in 
the surfaces. The inset image is intended to help differentiate between the projected images and the 
real objects in the sketch.  1 INTRODUCTION The impetus for this work is Henry Fuchs s long-time desire 
to build more compelling and useful systems for shared telepresence and telecollaboration between distant 
individuals. It was Fuchs who .rst inspired us with ideas for using a sea of cameras [39] and imperceptible 
lights to extract a 3D scene and to reconstruct it at a remote location. These ideas have been re.ned 
over several years of collaboration with Ruzena Bajcsy of the University of Pennsylvania s GRASP Laboratory 
[18], and with our colleagues in the NSF Science and Technology Center for Computer Graphics and Scienti.c 
Visualization. While we are making progress toward our vision for the of.ce of the future, we do not 
yet have a complete working system the ideas are by de.nition futuristic. As such, throughout the paper 
we present a mix of demonstrated results from new methods, and plausible ideas for future systems. We 
do our best to distinguish between the two lest the reader be led to believe that we have implemented 
something that we have not. In the remainder of this section we present motivation for the idea in the 
form of a story that outlines the developments that led to the vision as presented in this paper. In 
section 2 we discuss the principal components of the envisioned system, without necessarily discussing 
a speci.c implementation. In section 3 we present our approach to actually implementing such an of.ce 
of the future. In section 4 we discuss our current implementation, and in section 5 we discuss work to 
be done and future research topics. Note that rather than including a speci.c previous work section we 
have chosen to discuss related work throughout the paper. Telecollaboration Interfaces While telecollaboration 
systems using 2D talking heads and shared white boards have improved signi.cantly over the years, we 
believe that the through-the-window paradigm itself often inhibits much of the interaction that would 
otherwise take place if the collaborators were actually together in the same room. In [5] Buxton identi.es 
several tasks for which commercial televideo systems provide only limited support. Aside from limited 
resolution, part of the problem is that users are forced to maintain two separate egocenters (notions 
of where they are and who they are with): one in their local environment, and another egocenter with 
the remote collaborators. The participants must then consider and adjust normal behavior to .t through 
the window. One alternative is to implement a shared immersive virtual environment where a user dons 
a head-mounted display (HMD), disappears from the real world, and enters a shared virtual environment 
inside the display where for example they might see virtual objects along with 2D video avatars of their 
collaborators. Indeed, we have experimented with such paradigms, as have others. However this interface 
has several disadvantages. Most obvious are the typical ergonomic problems, for example size, weight, 
mobility, limited resolution, and limited .eld of view. Furthermore, for immersive HMD s, the resulting 
disassociation from a person s comfortable surroundings can be disconcerting and can limit their ability 
to interact with other people and real objects in the environment. A more attractive alternative is to 
get the display off of the user s head, and to instead use a spatially immersive display (SID). A SID 
is a display that physically surrounds the viewer with a panorama of imagery [4]. SID s are typically 
room-sized, thus accommodating multiple viewers, and are usually implemented with multiple .xed front 
or rear-projection display units. Probably the most well-known examples of general-purpose SID s are 
the Cave Automated Virtual Environment (CAVE ) [12], the related tiled-display PowerWall and In.nity 
Wall systems, and Alternate Realities VisionDome [2]. There are several good examples of telecollaboration 
applications where the users see and interact with their remote collaborators using a CAVE or CAVE -like 
system, for example [34]. Such large systems typically require signi.cant physical .oor space, for example 
in a laboratory where there is room for both the screens and the projection units. But we would like 
to avoid going down the hall to use the system, instead we would like something as convenient as the 
telephone a SID built into an of.ce. While such an endeavor would probably not be cost-effective solely 
for the purpose of telecollaboration, if it were of high enough quality one could use it for every-day 
2D computer work, video, and 3D immersive visualization. However not only does the construction of such 
SID s require very careful engineering and assembly, but certain characteristics vary with time and environmental 
factors such as temperature or vibration. Such time­varying characteristics include the intrinsic and 
extrinsic projector parameters, intensity balance, color balance, edge alignment, and blending. These 
problems are most often addressed by periodic mechanical projector calibration, however this approach 
becomes increasingly dif.cult and less reliable as the number of projectors increases. Flight simulator 
developers have faced such problems for some time, and while they have developed digital calibration 
systems, the systems tend to be highly specialized, thus increasing development cost and overall complexity. 
A more general-purpose autocalibration scheme would be preferable so that one could modify the display 
surface or projector con.guration as needed. If one could modify the display surface, one could spontaneously 
add a drawing board onto their desktop and the system would account for it. If one had some .exibility 
over projector placement, one could for example add projectors in an overlapping manner to increase the 
display resolution (a high-resolution region), or image intensity, or side-by-side to increase the display 
surface area. Telecollaboration Infrastructure and Applications There exists a relatively large body 
of work in telecollaboration infrastructures and applications, not to mention a large body of work in 
the area of Computer-Supported Cooperative Work (CSCW). Some representative examples are [34, 37, 6, 
15, 28, 19, 27, 36, 3, 41, 10, 33, 37]. Our vision for the of.ce of the future is one in which all of 
this and similar work can be applied, we hope in new and exciting ways. We envision our of.ce as a particularly 
compelling interface to be used in support of these efforts, and every-day applications. We are aware 
of no other one system that attempts to achieve what we do in a similar uni.ed approach. Of the existing 
telecollaboration efforts that we know about, the only one that attempts to provide an of.ce-like interface 
is TelePort [20]. The TelePort system uses wall-sized displays that each show a synthetic scene that 
is blended with video images of remote participants. As participants move, their locations are tracked 
so that the images are rendered from the proper perspective. The TelePort display is built into a room 
that is carefully designed to match the rendered room. The goal is for the virtual room to seem as an 
extension of the real room. They use carefully constructed geometric models for the of.ce environment, 
and video-based human avatars obtained by separating the remote participants from the original background 
(via delta-keying). Rather than building a specialized telecollaboration system that resembles an of.ce, 
we want to build capability for a life-like shared-room experience into existing of.ces.  Every Millimeter 
at Every Millisecond One question in our minds was how should remote collaborators and their environment 
appear remotely? While acceptable for some tasks, we believe that 2D video-based avatars do not effectively 
engender the sense of being with another person that is necessary for effective interpersonal communication. 
We want to see and interact with collaborators in 3D, as naturally as we do when we are in the same physical 
room: gesturing, pointing, walking, waving, using all of the subtle nuances of both verbal and nonverbal 
communication. A visually attractive possibility would be to use a high-quality 3D image-based rendering 
or modeling system for each participant (see for example [30, 38, 40, 43]). However we dream of a room-sized 
working volume, not only because we want mobility, but also because we want to be able to see multiple 
participants, and to see everyone in their natural surroundings, i.e. their of.ces. In short, we envision 
a system similar to [20] where the local and remote of.ces appear to be physically joined together along 
some common junction such as a designated wall that is actually a SID. But unlike [20] which overlays 
2D video of the remote participants onto a virtual adjoining of.ce, we want to see an image-based 3D 
reconstruction of the remote of.ce and all of its real contents including people and every-day clutter. 
That is, we have the ability to capture and remotely display a dynamic image-based model of an entire 
of.ce. At some point when considering all of these factors, we came to the realization that if we had 
access to a dynamic image-based model of the entire of.ce, including the designated SID surfaces, we 
could automatically correct for changes in the time-varying geometric characteristics of the SID. Furthermore, 
if several cameras could see the display surfaces from a variety of angles, we should be able to observe 
view-dependent intensity and color variations in the designated display surfaces, thus inferring the 
surface re.ectance properties. In other words, while obtaining an image-based model for the of.ce we 
could autocalibrate all of the designated display surfaces. Thus the realization that the SID could in 
effect be almost anything or anywhere in the of.ce! It wouldn t matter if the surfaces were irregularly 
shaped, or if the geometry was changing over time, the image-based model would indicate the variations. 
And if one was willing to sacri.ce some dynamic range in the projected images, one might even be able 
to use the surface re.ectance information to account for slight variations in view­dependent intensity. 
Note that a crucial advantage of this uni.ed approach is that because the autocalibration and the projection 
are done by the same device, one eliminates the problems of calibration and drift of the calibration 
system itself. Finally, we also note that if one has access to a dynamic image-based model of the entire 
of.ce, including the occupants, one could potentially extract higher-level representations of the data, 
assign semantics to those higher-level objects, and then in real-time interpret and respond to object 
motion or collisions for the purpose of tracking, interaction, or augmented reality (AR). With such capability 
one could implement untethered interaction as in the Luminous Room, where cameras and projectors serve 
as I/O Bulbs [46, 47]. In this way for example one might be able to track a person s hands so that they 
could reach out and manipulate a .oating 3D model, or perhaps one could detect collisions between real 
and virtual objects so that virtual objects could be placed on the desk. Figure 1 depicts a conceptual 
sketch of our of.ce of the future, replicated and in use at three different sites. Note the ceiling­mounted 
projectors and cameras, the use of lightly-colored material on the designated SID wall and desk area, 
and the mixed use of that SID for simultaneous image-based and geometric model visualization. To achieve 
the above capabilities of acquisition, calibration, and display in a continuously changing of.ce scene 
with both local and remote participants, we dream of being able to control light in the of.ce over every 
millimeter at every millisecond.  2 FUNDAMENTAL COMPONENTS Our idea for the of.ce of the future brings 
together several fundamental areas of computer science, components that can be enumerated independently 
from descriptions of their actual implementations. While one goal of this paper is to present the speci.c 
implementation of such a system, this does not preclude the use of any of the techniques others have 
developed in each of these areas. Quite to the contrary, we believe there are trade-offs with all of 
these techniques which warrant further investigation. 2.1 Dynamic Image-Based Modeling One of the major 
components of the system is the module that will capture, continually and in real time, image-based models 
of the of.ce environment including all of the designated display surfaces. A large body of literature 
exists from computer vision regarding the determination of depth from a scene. Some of the more common 
approaches include depth from motion, stereo, focus, and defocus. For our system we are interested not 
only in dynamic image-based modeling, but over a large volume also. With real­time in mind, many of the 
techniques traditionally used are dif.cult because of computational and bandwidth requirements. At CMU, 
a specialized hardware real-time depth from stereo architecture system has been developed [31]. It can 
take input from six cameras and produce, at 30 frames/second, a 256 × 240 depth map aligned with an intensity 
image. They also have the ability to produce an uncertainty estimation for each pixel. One advantage 
of this technique is the instantaneous sample-and-hold nature of depth from stereo. In contrast, using 
a laser scanner that cannot complete a scan of the image in a single frame may result in distorted shapes 
as objects in the scene move. Any technique which depends on computations made with several frames sampled 
at different times, including the structured light method described in section 3.1, will have this problem. 
Another real-time depth system has been developed by the Columbia University Automated Visual Environment 
Group [38]. They have demonstrated the ability to produce 512 × 480 depth estimates at 30 Hz with an 
accuracy of 0.3%. Their technique relies on a precise physical model of all the optical sensing and computational 
elements in the system: the optical transfer function, defocus, image sensing and sampling, and focus 
measure operators. They project a high frequency texture onto the scene and, via the same optical path, 
image the scene. An advantage of this system over the depth from stereo is that they do not have to worry 
about the correspondence problem faced by depth from stereo. One concern is the distracting high frequency 
textures which much be projected onto the scene. These patterns could prove unacceptable if the user 
wants to be in the environment while the scene is being captured. 2.2 Rendering Our vision for the of.ce 
of the future requires the ability to generate images that when projected onto the display surfaces appear 
correct to a moving head-tracked observer. This is true also for systems such as the CAVE , but our situation 
is somewhat unusual in that we want to be able to project onto general surfaces whereas the CAVE system 
is tailored to planar surfaces. Future capabilities in image generation will allow the increased burden 
of display on arbitrary surfaces to be realized. An interesting technique is presented in [16] for the 
use of computer graphics systems in theater design, where she models the appearance of backdrops from 
the audience s perspective. If left uncorrected, the backdrops would appear distorted. Essentially, we 
are faced with the same problem in our system, except with multiple projectors. We need to determine 
how to predistort the images such that, when projected from the projector s viewpoint, it will look correct 
from the user s viewpoint. Dorsey et al. also extend this technique to model the projector optics and 
demonstrates an extended radiosity method to simulate directional lighting characteristics.  2.3 Spatially 
Immersive Displays The most well known spatially immersive display in the graphics community is probably 
the CAVE [12]. The CAVE exists in many forms, typically it is con.gured as a left, right, and rear wall 
rear projection system. In some implementations they use a mirror above the CAVE that projects an image 
onto the .oor. While the CAVE does provide head-tracked stereo views surrounding the user of the system, 
current implementations are limited to 1 projector displayed on each wall. The CAVE does not deal with 
intensity blending and has no method of capturing the geometry of the environment, which is reasonable 
since this was not an intended goal of their system. The military simulation/.ight simulator industry 
is full of numerous examples of spatially immersive displays [9, 23, 29, 32, 35]. These systems typical 
use CRT projectors which need frequent calibration. Also, they usually (but not always) restrict themselves 
to matching the seams of the display instead of considering the whole display area as something that 
needs to be blended seamlessly. Another technique of the .ight simulator industry is to place a high 
resolution display in the center of view of the user and project a low resolution image on the surrounding 
screen, or to only project an image in the view frustum of the user. While this is effective, it cannot 
easily be repositioned and may show a seam where the high resolution image meets the low resolution image. 
The seam is a problem because it is disconcerting and severely disrupts the goal of achieving a feeling 
of being somewhere else the user is always reminded they are looking at an imperfect display surface. 
The attempts at creating a seamless display are discussed in the previously cited .ight simulator papers. 
Domed displays are another example [2]. Such systems are often limited to only one high resolution projector 
and have rarely employed a mechanism to capture depth or projection surface information from the scene. 
A method is presented in [29] that corrects the warping of the dome by modeling a dome with a 5­degree 
polygon mesh and a GUI for manipulating the coordinates of this mesh, but this is not done in real-time 
or automatically: direct user intervention is required. This method is meant to only be used when the 
system is moved or for some infrequent reason falls out of alignment, it is not meant to be a method 
that can update the projection in real-time as the display surface changes shape or occlusion properties. 
A .nal important point about all of these systems is that they rely on special geometric con.gurations 
and they present no general solution, which is a completely reasonable design decision on their part: 
typically they had an unchanging environment with uniform, ideal display surfaces. Also, they had control 
of every issue of the display system, from the lighting to the precise calibration of the projectors. 
Instead, we propose a general solution to the problem of projecting onto arbitrary display surfaces with 
real-time, automatic calibration procedures. Understand that we do not necessarily believe people will 
want to project on every type of surface or object, but we feel that thinking about the problem in this 
way is useful.  3 METHODS In this section we describe the idea of using cameras and projectors that 
can be operated in either a capture or display mode. (See Figure 2 and Figure 8.) When in capture mode, 
the projectors and cameras can be used together to obtain per-pixel depth and re.ectance information 
from the designated display surfaces. When in display mode, the image-based models may be used to render 
and then project geometrically and photometrically correct images onto the (potentially irregular) display 
surfaces. 3.1 Dynamic Image-Based Modeling Image-based modeling is a dif.cult problem, one that has 
occupied the computer vision community for many years. Depth Extraction The depth extraction method 
for of.ce scenes should work in large working volumes populated with areas with high frequency texture 
as well as surfaces that lack texture. To model display surfaces, we typically need high accuracy so 
that the projections are correct. To model dynamically changing scenes we need higher update rates to 
represent motion with potentially decreased resolution or accuracy. The method should be non-intrusive 
so that people can work in the environment. This prevents the use of lasers or other invasive methods. 
 Our system currently uses one video camera and one projector in a pair, although multiple cameras could 
work with one projector [43]. The correspondence problem is solved by projecting binary coded vertical 
bars [14]. The camera looks at a set of n successive images, creates binary images using adaptive thresholding 
on a per-pixel basis, and generates an n-bit code for every pixel corresponding to the code for the vertical 
bar that was 2n imaged at this pixel. This allows us to distinguish between projected vertical bars. 
A pre-computed sparse triangulation lookup table based on calibration data allows trilinear interpolation 
to compute intersection of pseudo-camera rays and projected vertical planes. The 3D coordinates of the 
surface points imaged at every pixel are later used with color information to complete the image-based 
model. The vertical bars can be projected repeatedly to compute depth for dynamic scenes. The same method 
can be used for scanning walls, people or moving objects with different levels of sophistication for 
sub-pixel image thresholding. See Figure 3 for some still-frame sample results. A choice of a camera 
and projector to actively scan 3D environments is necessitated by the fact that display surfaces may 
lack texture to provide enough correspondence cues. Use of the same projector for scanning and display 
allows uni.cation of the two tasks and the only additional component is a camera. Speed versus accuracy 
trade-offs led us to two types of camera-projector pairs. Relatively static display surfaces such as 
walls and furniture in of.ce scenes are modeled more accurately and slowly by the outward looking pairs 
of camera-projector than people and moving objects, which are scanned by inward looking pairs of camera­projectors. 
The dif.cult part of using two separate devices for depth extraction is calibration. We use [45] .rst 
to .nd intrinsic and extrinsic parameters of the camera using a checkerboard pattern on p9 p1 p1 p2 
p2 p9 time Sequence of patterns and their complements. original complement  Figure 3: Some example 
results (still frames from live updates) of depth extraction using binary coded structured light. a .at 
surface. Then the same method is used to calibrate the projector with respect to the same .at surface. 
Combining the two gives the relationship between the camera and the projector. To .nd the relationship 
between two camera-projector pairs, the transformation between the two cameras is .rst determined by 
viewing a common checkerboard pattern on a .at surface. Then, using the method described above, the two 
pairs are calibrated with respect to the working volume. The procedure is easier if the frustums of the 
two cameras overlap considerably. Detection of changes in scene geometry by camera image differencing 
is not robust when display surfaces lack texture. However, changes in a projected random texture can 
be imaged The random texture itself will be imperceptible to the human eye as described in section 3.2. 
Detected changes in scene geometry over a period of time could be the result of either actual changes 
in the surfaces or drift in the calibrated system. The use of cameras allow the possibility to self-calibrate 
the system periodically to compensate for errors due to environmental factors such as temperature or 
vibrations in the setup.  Color and Reflectance The projector is used in two modes, scene extraction 
and display. To get color information about the surfaces the projector is used as a bright light source 
along with a synchronized camera. However, the modes can be interleaved by inserting completely white 
frames in between the display frames. In the binary pulse-coded modulation (PCM) coded light projectors, 
only a few bits are used to project white frames while other bits can be used to project the display 
frames at reduced color resolution. Currently we illuminate the scene with a black followed by white 
pattern and observe the resultant dark image and bright image from one view point to estimate the per-pixel 
re.ectance function. The re.ectance function is primarily used to threshold images of projected binary 
coded structured light patterns assuming the camera response is linear to intensity. Camera response 
curves can be estimated by illuminating the scene with different levels of intensity. To complete the 
image based model, surfaces in the scene can be sampled from multiple view points to estimate a bidirectional 
re.ectance distribution (BRDF) function. The camera is used for per-pixel depth extraction as well as 
color extraction. Since the two procedures share the same optical axis, there is no drift. Similarly, 
the same projector is used for projecting structured light patterns, for depth extraction, and for flat 
field Figure 4: Pattern and complement are visually integrated over time, the result is the appearance 
of a .at .eld, or white light. display on the depth extracted surface. This eliminating problems due 
to drift or misalignment.  3.2 Imperceptible Structured Light With respect to the structured light described 
in section 3.1, our goal is to make it appear to the casual observer as nothing more than incandescent 
white light, not a succession of .ashing binary patterns. Our method for doing this is to use imperceptible 
structured light. The approach is a combination of time-division multiplexing and light cancellation 
techniques to hide the patterns in a rapid series of white-light projections. Figure 4 depicts a sequence 
of patterns projected in time. A binary structured light approach such as ours uses n patterns to resolve 
2n projected vertical bars. The period of pattern repetition would be, for example, 1/60 second for a 
60 Hz depth extraction rate. Figure 4 depicts how a given pattern pi and complement pi are integrated 
by the visual system in such a way that the sequence appears to be the projection of a .at .eld or white 
light. The same approach can be applied to project imperceptible structured light along with video or 
graphics images, facilitating imperceptible autocalibration of designated display surfaces. We use a 
digital light projector [25, 26] which uses PCM to project the pattern and its complement. A synchronized 
camera can measure the structured light by integrating light during the pattern projection. While limited 
access to digital light projector speci.cations currently limits our ability to implement completely 
imperceptible image-based modeling, we are able to separately demonstrate real­time capture and imperceptible 
structured light. Figure 5 shows the effect in a laboratory experiment. Figure 6 shows the use of a similar 
approach to embed imperceptible structured light in a still image as opposed to white light. We are working 
together with the developers of the projector technology to obtain lower-level access to the technology, 
which will introduce a whole new realm of possibilities for dynamic structured light and display.  3.3 
Geometric Registration The depth data acquired from different cameras needs to be zipped together to 
complete the display surface geometry. This is required for generating correct images from a single projector 
for multisurfaces [42] and also for intensity blending if the projections from multiple projectors overlap. 
The depth images are taken from distinct cameras and are in different coordinate systems. Thus in order 
to tile the extracted surfaces together corresponding points Figure 5: Imperceptible structured light 
is demonstrated in the laboratory. The digital projector on the left is projecting the text shown in 
the monitor and its complement , however the text can only be seen with a synchronized camera such as 
that sitting on the projector above. The inset snapshot of an oscilloscope shows the pulses that correspond 
to the brief time when the pattern (text in this case) is being projected. If you can read this, then 
you are seeing Imperceptible Structured Light  Figure 6: Imperceptible structured light embedded in 
images. (a) An initial image (Tokaj, Hungary). (b) The binary image that is to be imperceptible. (c) 
The two images combined and mapped to the proper time-division bit sequence. (d) The .nal result, showing 
the initial image (with reduced dynamic range) being projected on the wall, while the embedded imperceptible 
image is captured and displayed on the monitor (lower left). from overlap regions are used. The corresponding 
points are generated using the binary coded structured light approach for rows and columns of projector 
pixels. The binary code of an imaged point uniquely identi.es the corresponding projector pixel. Pairs 
of pixels in two cameras that share the binary code are used to compute transformation between depth 
data sets. Note that this transformation between two cameras can also be pre-computed during calibration 
stage but is usually not suf.ciently accurate to register two depth data sets. Otherwise, for blending 
purposes, the geometric correspondence between pixels of two different projectors is established by observing 
the projection overlap region from a single camera. We assume that every pair of images has a substantial 
overlap (about one-third of its total area).  3.4 Rendering and Display Our goal is to generate images 
that appear correct to an observer when projected onto (potentially irregular) display surfaces. Since 
the observer can move around in the of.ce, we currently use magnetic head-tracking to determine the viewer 
s location. The inputs to the algorithm are a model of the surface, the projector s intrinsic and extrinsic 
parameters, the viewer s location, and a desired image, the image which we want the viewer to see. The 
desired image will typically be the result of conventional 3-D rendering. Our algorithm can work with 
any general type of surface representation (e.g. NURBs), as long as the model of the real­world display 
surface is accurate. Likewise, rendering could be done with many different methods (e.g. ray-tracing); 
our current implementation uses projective textures with OpenGL primitives to achieve hardware acceleration. 
The underlying projective textures [44] technique is an extension of perspectively-correct texture mapping 
that can be used to do arbitrary projection of two dimensional images onto geometry in real-time. We 
describe a two pass approach for rendering and displaying images of 3D scenes on potentially irregular 
surfaces. In the .rst pass, we compute the desired image for the viewer by rendering the 3D scene from 
the observer s viewpoint. This desired image is stored as a texture map. In the second pass the texture 
is effectively projected from the user s viewpoint onto the polygonal model of the display surface. The 
display surface (with the desired image texture mapped onto it) is then rendered from the projector s 
viewpoint. The resulting image, when displayed by the projector, will produce the desired image for the 
viewer. As the user moves, the desired image changes and it is also projected from the user s new location. 
Multiple projectors can be used to increase the display surface area. To ensure complete coverage of 
the display surface, every part of the display surface must be visible to at least one projector. To 
ensure complete coverage from a given viewpoint, at least one projector must be able to image on every 
surface visible from that viewpoint. The projectors viewpoints in the second pass typically remains .xed. 
Neither projector overlap nor self-occlusions of display surfaces from observer s viewpoint need hinder 
the effective image from the user s viewpoint. See Figure 7. To specify the viewing direction for projecting 
textures with monocular viewing, we only need the position of the user and not orientation. A .eld of 
view that contains all the polygons of the synthetic object is suf.cient for the frustum of texture projection. 
This frustum may be trimmed if it exceeds the frustum of the display surface model.The frustum is oriented 
from the viewer s location toward the polygonal model of display surfaces. The user s frustum parameters 
during the .rst pass and texture projection in the second pass are identical. We assume that the projectors 
have no radial distortion and hence the projectors can be modeled with a pinhole camera. For the optics 
of digital micromirror device (DMD) projectors, this assumption is valid. However, if the projector has 
radial distortion, we must pre-distort the rendered image before it is sent to the projector framebuffer. 
This pre-distortion can be done using non­linear 3D warp of display surface geometry or using screen 
space 2D warp with texture mapping. Challenges Speed. The two pass rendering method consists of normal 
3D rendering in the .rst pass, followed by a second pass that maps the desired image to a display surface 
model. The additional cost of the algorithm comes from transferring the framebuffer from the .rst pass 
into texture memory and rendering the display surface model with texture mapping applied in the second 
pass. Thus it is crucial to simplify the display surface geometries.  Figure 7: Multisurface rendering. 
(a) A teapot projected onto a single planar surface. (b) A distorted projection resulting from the introduction 
of a second planar display surface. (c) The .nal corrected projection obtained by extracting the display 
surface geometry and then employing our two-pass rendering scheme. The edges of the second planar surface 
shown leaning against the wall on the desk in (c) are highlighted with a dashed line.  Parallelization 
If more than one projector is used, each projector can have a separate display engine. Rendering can 
be done in parallel, and each display engine need only load those parts of the display surface model 
that are visible from the corresponding projector. We have some initial evidence [42] that our method 
is faster versus conventional rendering techniques with multiple projectors, complex displays, or complex 
graphics models. The reason is that the .rst pass of the algorithm (conventional rendering) only needs 
to be done once, and then the second pass can be performed for each projector. Conventional techniques 
require the graphics model to be re-rendered for each projector or even for each polygon in the display 
surface. If the .rst pass is also parallelized, all the corresponding graphics pipelines need to access 
the synthetic model being displayed simultaneously. This could be a problem if the model is dynamically 
changing and the graphics pipeline must read the model during every display iteration. Other parallelization 
issues include if the display surface model is dynamic, as well as network issues if the display engines 
are on different machines. Latency There is inherent latency in the system in addition to traditional 
tracking and rendering latency due to two pass method for drawing models. For large models, rendering 
times could be different for different projectors so that there is inter-projector delay during rendering. 
If all the projectors are driven from a single machine, then setting up viewports within a single rendering 
program for each projector and synchronously updating the framebuffers will eliminate this problem. 
 3.5 Generating Blending Functions We use the traditional computer graphics phrase alpha values to describe 
the blending functions. When building some sort of a tiled multi-projector SID, one is faced with two 
approaches for handling the transitions between the projected images: one can either design the system 
such that the images do not overlap but can be adjusted so that they are barely touching and thus seamless, 
or one can allow projected images to overlap and employ some means of blending. The second approach typically 
uses a roll-off function such as a linear ramp or a cosine curve to smooth the transition between projectors. 
Designers of the CAVE exercise the .rst option by limiting the system to a well-de.ned, relatively simple 
screen arrangement whereby no projectors overlap. However we want to be able to project images onto arbitrary 
potentially irregular display surfaces in the of.ce, which means that we cannot use the .rst approach 
as we assume no control over the surfaces. Furthermore, we envision a more .exible setup whereby multiple 
projectors can be used to project into the same space in order to achieve higher resolution (e.g., via 
a high-resolution insert ) or increased light. We implemented a weighting function by assigning alpha 
values between 0 and 1 to every pixel in every projector, and as described in section 3.3 ensure that 
every illuminated world point corresponding to a single camera pixel has an alpha sum equal to one. This 
assumes that the projectors have similar intensity response. There are two cases: a point resulting from 
a projection of only one projector and a point resulting from a number of projectors. In the .rst case 
the solution is trivial. However, in the second case, the case where overlap occurs, we make alpha values 
a function of the distance to the beginning/end of overlapping region, with the constraint that alpha 
values of points from different projectors, corresponding to the same point in space, must sum up to 
one. To assign different weights to projector pixels, we actually create an alpha image for each projector. 
This image contains (1 -desired_alpha) at each pixel. This alpha image is rendered last. In our OpenGL 
implementation this is achieved using transparent textures. A camera in the closed loop system allows 
one to photometrically correct images even when the projectors have different brightness or when the 
display surface has non-uniform re.ectance properties. Although the digital light projectors have linear 
intensity response, they use a de-gamma correction [25]. Use of alpha image allows us to compensate the 
de-gamma correction.  3.6 Simplification of Depth Data Dynamic image-based modeling of an entire of.ce 
will result in tremendously large data sets, given that the data would be per-pixel for multiple cameras, 
occurring at video rates. However it is reasonable to expect that the majority of the data is highly 
correlated both temporally and spatially. In fact most of the of.ce is unlikely to change dramatically 
over short periods of time, in particular this is likely to be true for the of.ce walls and most designated 
display surfaces. It makes sense to attempt to simplify the data so that the system does not have to 
deal with such a horrendous volume of data. For example, Radim Sara and Ruzena Bajcsy at the University 
of Pennsylvania have created a depth data set of an of.ce that has approximately half a million vertices. 
The simpli.cation method must be careful not to simplify in regions of rapid change or high curvature 
where information might be lost. The automatic reconstruction of surfaces from range data is explored 
in [13, 1, 8, 11, 17, 22]. Unfortunately, the dynamic nature and the presence of noise in our system, 
disallow the use of well-established simpli.cation algorithms. The method we currently employ is not 
a well-de.ned mathematical approach, but rather a heuristic-based method that produced qualitatively 
pleasing results based on the characteristics of our data sets. We .rst apply a curvature approach to 
the data set using a tangent method similar to [24], and we then use a Euclidean distance approach on 
the remaining points. We chose this particular sequence of steps because the curvature method is usually 
much more successful in eliminating points than the second one. This approach produces elimination rates 
of 80% to 90% without any visible loss of information. This is because most of the objects in the of.ce 
environment are locally planar.  3.7 Tracking While our of.ce of the future could certainly and possibly 
very effectively be used in a 2D-only mode, we believe that it is more compelling to consider its additional 
use as a 3D visualization environment. We need the ability to track viewers heads in order to render 
perspectively correct images. Interestingly enough, for monoscopic viewing one does not need the orientation 
of the eye because the display image is uniquely determined by the eye position. For stereoscopic viewing 
one needs to be able to either track one eye and the user s head orientation, or two eyes, each with 
position only. The system involves projecting synthetic images onto real surfaces for which the extracted 
surface model is assumed to be correct in world space. Any small error in tracker reading (after transformation) 
in world space will result in visibly incorrect registration between projected images and the display 
surfaces. This situation is similar to augmented reality systems where traditionally a vision based registration 
system is used to correct the tracker readings. A similar closed loop system may be necessary for accurate 
rendering for our system.  4 CURRENT IMPLEMENTATION While a complete realization of such an of.ce of 
the future is, by de.nition, years away, we are making steady progress and present here some promising 
results and demonstrations. We have implemented a working system using projective textures. We have also 
independently demonstrated 1) a depth extraction system running at 3 Hz, 2) imperceptible structured 
light, and 3) initial experiments in intensity blending. The of.ce size is 10 × 10 × 10 feet, and is 
populated with 640 × 480 resolution video cameras (Figure 2). The outward looking .ve projectors are 
driven simultaneously from an SGI In.nite Reality. Four of these project on relatively vertical surfaces 
and one projects down on the table and the .oor. The binary PCM coded light projectors project 24 bit 
color at a 60Hz update rate. The projectors allow off-axis projection with a small offset without signi.cant 
focus problems. The camera framegrabbers are SGI O2s and currently we use a video switcher to switch 
between the video cameras while capturing images of the environment. We expect that in the very near 
future the rendering will be done in different viewports of a single window and hence can be used to 
maintain synchronous updates of all projector framebuffers. The system also includes an Ascension magnetic 
tracker for tracking user position. The walls of the of.ce are made up of relatively inexpensive foam-core 
board, and does not need solid support because the system supports non-planar surfaces and off-axis projection. 
A separate projector-camera pair setup is used to create dynamic image based model in a small working 
volume of 332×× feet, and can be used for dynamic image-based modeling of human faces (see Figure 3) 
[43]. The system creates a 640 × 240 depth map at three updates per second, which is texture mapped with 
live video. Using a direct digital interface to the digital light projectors from a PC, we have been 
able to project patterns that are imperceptible to human eye but visible from a synchronized camera (see 
Figure 5). We have capability to change the binary PCM coding of light for the projectors allowing us 
to use different bits for different purposes; in particular we have the ability to burn an equal-bit-time 
PROM for the projectors which allows us to demonstrate compelling imperceptible structured light (see 
Figure 5). With the equal-bit-time PROM, a binary pattern can be displayed 24 times per 60 Hz frame, 
i.e. every 694 microseconds. Thus a pattern and its complement can be displayed sequentially in approximately 
1.4 milliseconds. The synchronized camera with exposure time less than 700 microsecond was used in Figure 
5. Using this PROM we could in theory project and capture 60 × 24 = 1440 binary patterns per second. 
Our current framegrabbers, however, can process only 60 images per second. A better digital interface 
to DLP s will also allow us to render stereo images at 60 Hz. Although we expect the participants to 
be seating in a chair most of the time (Figure 2), the current setup allows participants of average height 
(under 6 feet) to stand and move around without blocking the projection on the walls if they are at least 
4 feet away from the walls. The of.ce of the future setup allows scalability in terms of more pairs of 
camera and projector to either increase resolution of extracted surfaces, or resolution of display on 
surfaces. The system other than computer hardware costs approximately $35,000. We expect minimal maintenance 
of projector, cameras or display surfaces because the system employs self-calibration methods.  5 FUTURE 
WORK Much work remains to be done, some of which we have concrete plans to attack, some we are attacking 
with collaborations, and some we hope others will pursue. We plan to integrate scene acquisition and 
display in such a way that the acquisition is imperceptible, or at least unobtrusive. This will involve 
some combination of light control and cameras, possibly wide-.eld-of-view high-resolution clusters as 
described in [7]. Together with our collaborators in the GRASP Laboratory at the University of Pennsylvania, 
we are exploring the continuum of options between strict control of all of the lights in the environment 
(as outlined herein) and little or no control of the lights but using multiple cameras and passive correlation-based 
techniques. We expect to have within the coming year a new .ve 800 × 600 resolution digital light projectors 
and twomultibaseline correlation system on hand for experiments with our structured light acquisition 
and display environment. As part of scene acquisition, one can detect display surface changes and adapt 
the rendering accordingly. Currently it can be done at non-interactive rates. Eventually we also want 
to explore methods as in [21] to detect surface changes for purposes such as tracking and gestural input. 
We also want to improve image generation with better blending, by exploiting image-based rendering methods 
to construct a target image from multiple reference images. There is a good possibility that a distributed 
rendering scheme employing the multiprojector and multisurface display algorithms that we present and 
analyze in [42] will prove to be effective. In addition, we want to correct for surface re.ectance discontinuities 
dynamically, and to make use of the information during run-time to adjust the rendered images. We are 
planning to use our system in an on-going telecollaboration involving multi-disciplinary mechanical design 
and manufacturing with our collaborators in the NSF Science and Technology Center for Graphics and Visualization. 
In addition as part of The Tele-Immersion Initiative we are planning to make use of the CAVE library 
or similar framework to connect several laboratories over high speed networks with novel immersive display 
environments.  6 SUMMARY AND CONCLUSIONS We have shown initial results for a novel semi-immersive display 
in an of.ce-like environment, one that combines acquisition and display. We have developed techniques 
to acquire the geometry of an irregular surface and then modify rendering to allow projection onto that 
irregular surface so that it looks correct to an observer at a known location. We have described a method 
of injecting structured light into a scene that is imperceptible to the participants but measurable to 
synchronized cameras. These techniques can be applied to other display environments which use multiple 
projectors or that involve complex display geometries. In conclusion, we note that a major trend in computer 
science over the past few decades has been from one to many, from being restricted by resources' proximity 
to employing resources irrespective of their locations. One .eld unaffected by this global development 
has been the computer display or the area where the results of our work are being presented. Our system 
pushes this envelope, thus enabling any object, or a collection of such, located anywhere to be used 
as a display surface. From now on, one does not have to cramp the information into a relatively small 
monitor, but to have as much space as possible and to be limited only by the amount of space around. 
Anything can be a display surface - a wall or a table, and anywhere - be it an of.ce or a conference 
hall. Of course, the system faces many challenges, but they can be overcome by the increasing power of 
graphics hardware and general purpose computing. 7 ACKNOWLEDGMENTS This work was supported by (1) the 
National Science Foundation Cooperative Agreement no. ASC-8920219: Science and Technology Center for 
Computer Graphics and Scienti.c Visualization , Center Director Andy van Dam (Brown University). Principal 
Investigators Andy van Dam, Al Barr (California Institute of Technology), Don Greenberg (Cornell University), 
Henry Fuchs (University of North Carolina at Chapel Hill), Rich Riesenfeld (University of Utah); (2) 
the National Tele-Immersion Initiative which is sponsored by Advanced Networks and Services, President 
and Chief Executive Of.cer Al Weis, Chief Scientist Jaron Lanier; and (3) DARPA grant no. N66001-97-1­8919: 
Three Applications of Digital Light Projection for Tiled Display and 3-D Scene Capture. We thank Nick 
England for sharing his signi.cant knowledge about wide-area tiled display systems, both in terms of 
past work and fundamental issues; Mary Whitton, David Harrison, John Thomas, Kurtis Keller, and Jim Mahaney 
for their help in designing, arranging, and constructing the prototype of.ce of the future; Todd Gaul 
for help with our video taping; Jean-Yves Bouguet of Caltech for the camera calibration code and useful 
discussions; Nick Vallidis for his work on creating new DLP binary PCM coding; Andy Ade and Jai Glasgow 
for their administrative help; our department s Computer Services staff members for keeping our networks 
and machines humming in spite of our experimental modi.cations; and Hans Weber and Rui Bastos for help 
photographing the imperceptible structured light demonstrations. Finally, we gratefully acknowledge Andrei 
State for his illustrations of the of.ce of the future (Figure 1).  References [1] Bajaj, C.L., F. 
Bernardini, and G. Xu. Automatic reconstruction of surfaces and scalar .elds from 3D scans, SIGGRAPH 
95 Conference Proceedings, Annual Conference Series, ACM SIGGRAPH, Addison-Wesley, pp. 109-118, August 
1995. [2] Bennett, David T. Chairman and Co-Founder of Alternate Realities Corporation. Internet: http://www.virtual-reality.com. 
215 Southport Drive Suite 1300 Morrisville, NC 27560. [3] Bowen, Loftin, R. Hands Across the Atlantic, 
IEEE Computer Graphics and Applications, Vol. 17, No. 2, pp. 78-79, March-April 1997. [4] Bryson, Steve, 
David Zeltzer, Mark T. Bolas, Bertrand de La Chapelle, and David Bennett. The Future of Virtual Reality: 
Head Mounted Displays Versus Spatially Immersive Displays, SIGGRAPH 97 Conference Proceedings, Annual 
Conference Series, ACM SIGGRAPH, Addison-Wesley, pp. 485-486, August 1997. [5] Buxton, W., Sellen, A. 
&#38; Sheasby, M. Interfaces for multiparty videoconferencing, In K. Finn, A. Sellen &#38; S. Wilber 
(Eds.). Video Mediated Communication. Hillsdale, N.J.: Erlbaum, pp. 385-400, 1997. [6] Capin, Tolga K., 
Hansrudi Noser, Daniel Thalmann, Igor Sunday Pandzic and Nadia Magnenat Thalman. Virtual Human Representation 
and Communication in VLNet, IEEE Computer Graphics and Applications, Vol. 17, No. 2, pp. 42-53, March-April 
1997. [7] Chi, Vern, Matt Cutts, Henry Fuchs, Kurtis Keller, Greg Welch, Mark Bloomenthal, Elaine Cohen, 
Sam Drake, Russ Fish, Rich Riesenfeld. 1998. A Wide Field-of-View Camera Cluster , University of North 
Carolina at Chapel Hill, Dept of Computer Science, Technical Report TR98-018. [8] Chien, C.H., Y.B. Sim, 
and J.K. Aggarwal. Generation of volume/surface octree from range data, In The Computer Graphics Society 
Conference on Computer Vision and Pattern Recognition, pp. 254-260, June 1988. [9] Clodfelter, Robert 
M. Predicting Display System Performance, Presented at the 1996 IMAGE Conference, Scottsdale, AZ, pp. 
1-5, June 23-28, 1996. [10] Conner, D.B, Cutts, M., Fish, R., Fuchs, H., Holden, L., Jacobs, M., Loss, 
B., Markosian, L., Riesenfeld, R., and Turk, G. An Immersive Tool for Wide-Area Collaborative Design, 
TeamCAD, the First Graphics Visualization, and Usability (GVU) Workshop on Collaborative Design. Atlanta, 
Georgia, May 12-13, 1997. [11] Connolly, C.I. Cumulative generation of octree models from range data, 
Proceedings, Int l. Conference Robotics, pp. 25-32, March 1984. [12] Cruz-Neira, Carolina, Daniel J. 
Sandin, and Thomas A. DeFanti. Surround-Screen Projection-Based Virtual Reality: The Design and Implementation 
of the CAVE, Computer Graphics, SIGGRAPH Annual Conference Proceedings, 1993. [13] Curless, Brian, and 
Marc Levoy. A Volumetric Method for Building Complex Models from Range Images, SIGGRAPH 96 Conference 
Proceedings, Annual Conference Series, ACM SIGGRAPH, Addison-Wesley. pp. 303-312, 1996. [14] DePiero, 
F.W., and Trivedi, M.M., 3-D Computer Vision using Structured Light: Design, Calibration, and Implementation 
Issues, Advances in Computers(43), 1996, Academic Press, pp.243-278 [15] Dias, José Miguel Salles, Ricardo 
Galli, António Carlos Almeida, Carlos A. C. Belo, and José Manuel Rebordã. mWorld: A Multiuser 3D Virtual 
Environment, IEEE Computer Graphics and Applications, Vol. 17, No. 2., pp. 55-64, March-April 1997. [16] 
Dorsey, Julie O B., Fransco X. Sillion, Donald Greenberg. Design and Simulation of Opera Lighting and 
Projection Effects, SIGGRAPH 91 Conference Proceedings, Annual Conference Series, ACM SIGGRAPH, Addison-Wesley, 
pp. 41-50, 1991. [17] Edelsbrunner, H. and E.P. Mucke. Three-dimensional alpha shapes, In Workshop on 
Volume Visualization, pp. 75-105, October 1992. [18] Fuchs, Henry, Gary Bishop, Kevin Arthur, Leonard 
McMillan, Ruzena Bajcsy, Sang Lee, Hany Farid, and Takeo Kanade. Virtual Space Teleconferencing Using 
a Sea of Cameras, Proceedings of the First International Symposium on Medical Robotics and Computer Assisted 
Surgery, (Pittsburgh, PA.) Sept 22-24, 1994. [19] Gajewska , Hania , Jay Kistler, Mark S. Manasse, and 
David D. Redell. Argo: A System for Distributed Collaboration, (DEC, Multimedia '94)  [20] Gibbs, Simon, 
Constantin Arapis and Christian J. Breiteneder. TELEPORT-Towards Immersive Copresence, accepted for publication 
in ACM Multimedia Systems Journal, 1998. [21] Goncalves, Luis , Enrico Di Bernardo, Enrico Ursella, Pietro 
Perona. Monocular Tracking of the Human Arm in 3D, Proc. of the 5th Inter. Conf. on Computer Vision, 
ICCV 95. [22] Hilton, A., A.J. Toddart, J. Illingworth, and T. Windeatt. Reliable surface reconstruction 
from multiple range images, In Fouth European Conference on Computer Vision, Volume 1, pp. 117-126. April 
1996. [23] Holmes, Richard E. Common Projector and Display Modules for Aircraft Simulator Visual Systems, 
Presented at the IMAGE V Conference, Phoenix, AZ, June 19-22, pp. 81-88, 1990. [24] Hoppe, Hugues, Tony 
DeRose, Tom Duchamp, John McDonald, Werner Stuetzle. Surface Reconstruction from Unorganized Points, 
SIGGRAPH 92 Conference Proceedings, Annual Conference Series, ACM SIGGRAPH, Addison-Wesley, pp. 71-76, 
1992. [25] Hornbeck, Larry J., Deformable-Mirror Spatial Light Modulators, Proceedings SPIE, Vol. 1150, 
Aug 1989. [26] Hornbeck, Larry J., Digital Light Processing for High-Brightness High-Resolution Applications, 
[cited 21 April 1998]. Available from http://www.ti.com/dlp/docs/business/resources/ white/hornbeck.pdf, 
1995. [27] Ichikawa, Y., Okada, K., Jeong, G., Tanaka, S. and Matsushita, Y.: MAJIC Videoconferencing 
System: Experiments, Evaluation and Improvement', In Proceedings of ECSCW'95, pp. 279-292, Sept. 1995. 
[28] Ishii, Hiroshi, Minoru Kobayashi, Kazuho Arita. Iterative Design of Seamless Collaboration Media, 
CACM, Volume 37, Number 8, pp. 83-97, August 1994. [29] Jarvis, Kevin. Real-time 60Hz Distortion Correction 
on a Silicon Graphics IG, Real-time Graphics, Vol. 5, No. 7, pp. 6-7. February 1997. [30] Kanade, Takeo 
and Haruhiko Asada. Noncontact Visual Three-Dimensional Ranging Devices, Proceedings of SPIE: 3D Machine 
Perception. Volume 283, Pages 48-54. April 23-24, 1981. [31] Kanade, Takeo, Hiroshi Kano, Shigeru Kimura, 
Atsushi Yoshida, Kazuo Oda. Development of a Video-Rate Stereo Machine, Proceedings of International 
Robotics and Systems Conference (IROS 95). pp. 95-100, Pittsburgh, PA., August 5-9, 1995. [32] Lacroix, 
Michel. A HDTV Projector For Wide Field of View Flight Simulators, pp. 493-500. Presented at the IMAGE 
VI Conference, Scottsdale, AZ, July 14-17, 1992. [33] Lamotte, Wim, Eddy Flerackers, Frank Van Reeth, 
Rae Earnshaw, Joao Mena De Matos. Visinet: Collaborative 3D Visualization and VR over ATM Networks. IEEE 
Computer Graphics and Applications, Vol. 17, No. 2, pp. 66-75, March-April 1997. [34] Lehner, Valerie 
D., and Thomas A. DeFanti. Distributed Virtual Reality: Supporting Remote Collaboration in Vehicle Design, 
IEEE Computer Graphics and Applications, Vol. 17, No. 2, pp. 13-17, March-April 1997. [35] Lyon, Paul. 
Edge-blending Multiple Projection Displays On A Dome Surface To Form Continuous Wide Angle Fields-of-View, 
pp. 203-209. Proceedings of 7th I/ITEC, 1985. [36] Macedonia, Michale R. and Stefan Noll. A Transatlantic 
Research and Development Environment, IEEE Computer Graphics and Applications, Vol. 17, No. 2, pp. 76-82, 
March-April 1997. [37] Mandeville, J., T. Furness, M. Kawahata, D. Campbell, P. Danset, A. Dahl, J. Dauner, 
J. Davidson, K. Kandie, and P. Schwartz. GreenSpace: Creating a Distributed Virtual Environment for Global 
Applications, Proceedings of IEEE Networked Virtual Reality Workshop, 1995. [38] Nayar, Shree, Masahiro 
Watanabe, Minori Noguchi. Real­time Focus Range Sensor, Columbia University, CUCS-028-94. [39] Neumann, 
Ulrich and Henry Fuchs, "A Vision of Telepresence for Medical Consultation and Other Applications," Proceedings 
of the Sixth International Symposium on Robotics Research, Hidden Valley, PA, Oct. 1-5, 1993, pp. 565-571. 
[40] Ohya, Jun, Kitamura, Yasuichi, Takemura, Haruo, et al. Real­time Reproduction of 3D Human Images 
in Virtual Space Teleconferencing, IEEE Virtual Reality International Symposium. Sep 1993. [41] Panorama 
Project, WWW description. [cited 13 January 1998]. Available from http://www.tnt.uni-hannover.de/project/eu/ 
panorama/overview.html [42] Raskar, Ramesh, Matt Cutts, Greg Welch, Wolfgang Stüerzlinger. Ef.cient Image 
Generation for Multiprojector and Multisurface Displays, University of North Carolina at Chapel Hill, 
Dept of Computer Science, Technical Report TR98-016, 1998. [43] Raskar, Ramesh, Henry Fuchs, Greg Welch, 
Adam Lake, Matt Cutts. 3D Talking Heads : Image Based Modeling at Interactive rate using Structured Light 
Projection, University of North Carolina at Chapel Hill, Dept of Computer Science, Technical Report TR98-017, 
1998 [44] Segal, Mark, Carl Korobkin, Rolf van Widenfelt, Jim Foran and Paul Haeberli, Fast Shadows and 
Lighting Effects using Texture Mapping, Computer Graphics (SIGGRAPH 92 Proceedings), pp. 249-252, July, 
1992. [45] Tsai , Roger Y. An Ef.cient and Accurate Camera Calibration Technique for 3D Machine Vision, 
Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, Miami Beach, FL, pp. 364-374, 
1986. [46] Underkof.er, J. A View From the Luminous Room, Personal Technologies, Vol. 1, No. 2, pp. 49-59, 
June 1997. [47] Underkof.er, J., and Hiroshi Ishii. Illuminating Light: An Optical Design Tool with a 
Luminous-Tangible Interface, Proceedings of CHI '98, ACM, April 1998.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280864</article_id>
		<sort_key>189</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[Rendering synthetic objects into real scenes]]></title>
		<subtitle><![CDATA[bridging traditional and image-based graphics with global illumination and high dynamic range photography]]></subtitle>
		<page_from>189</page_from>
		<page_to>198</page_to>
		<doi_number>10.1145/280814.280864</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280864</url>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Intensity, color, photometry, and thresholding</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Scanning</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Photometry</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Sensor fusion</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010233</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Vision for robotics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010506</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Document scanning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP40023545</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of California at Berkeley, Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ADELSON,E.H.,ANDBERGEN,J.R.ComputationalModelsofVisualPro-cessing. MITPress,Cambridge,Mass.,1991,ch.1.ThePlenopticFunctionand theElementsofEarlyVision.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[AZARMI,M.OpticalEffectsCinematography:ItsDevelopment,Methods,and Techniques.UniversityMicrofilmsInternational,AnnArbor,Michigan,1973.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360353</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BLINN,J.F.Textureandreflectionincomputergeneratedimages.Communica-tionsoftheACM19, 10(October1976),542-547.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218395</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[CHEN,E.QuickTimeVR-animage-basedapproachtovirtualenvironmentnav-igation. InSIGGRAPH'95(1995).]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97894</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[CHEN,S.E.Incrementalradiosity:Anextensionofprogressiveradiositytoan interactivesynthesissystem.InSIGGRAPH'90(1990),pp.135-144.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378487</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[COHEN,M.F.,CHEN,S.E.,WALLACE,J.R.,ANDGREENBERG,D.P.Apro-gressiverefinementapproachtofastradiosityimagegeneration. InSIGGRAPH '88(1988),pp.75-84.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237269</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[CURLESS,B.,ANDLEVOY,M.Avolumetricmethodforbuildingcomplexmod-elsfromrangeimages. InSIGGRAPH'96(1996),pp.303-312.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>794511</ref_obj_id>
				<ref_obj_pid>794189</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[DANA,K.J.,GINNEKEN,B.,NAYAR,S.K.,ANDKOENDERINK,J.J.Re-flectanceandtextureofrealworldsurfaces.InProc.IEEEConf.onComp.Vision andPatt.Recog.(1997),pp.151-157.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258884</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[DEBEVEC,P.E.,ANDMALIK,J.Recoveringhighdynamicrangeradiancemaps fromphotographs.InSIGGRAPH'97(August1997),pp.369-378.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237191</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[DEBEVEC,P.E.,TAYLOR,C.J.,ANDMALIK,J.Modelingandrenderingar-chitecturefromphotographs: Ahybridgeometry-andimage-basedapproach.In SIGGRAPH'96(August1996),pp.11-20.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>893689</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[DEBEVEC,P.E.,YU,Y.,ANDBORSHUKOV,G.D.Efficientview-dependent image-basedrenderingwithprojectivetexture-mapping.Tech.Rep.UCB//CSD-98- 1003,UniversityofCaliforniaatBerkeley,1998.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>732112</ref_obj_id>
				<ref_obj_pid>647651</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[DRETTAKIS,G.,ROBERT,L.,ANDBOUGNOUX,S.Interactivecommonillu-minationforcomputeraugmentedreality. In8thEurographicsworkshoponRen-dering, St.Etienne,France(May1997),J.DorseyandP.Slusallek,Eds.,pp.45- 57]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[FIELDING,R.TheTechniqueofSpecialEffectsCinematography.Hastings House,NewYork,1968.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[FOURNIER,A.,GUNAWAN,A.,ANDROMANZIN,C.Commonillumination betweenrealandcomputergeneratedscenes.InGraphicsInterface(May1993), pp.254-262.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192171</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[GERSHBEIN,R.,SCHRODER,P.,ANDHANRAHAN,P.Texturesandradiosity: Controllingemissionandreflectionwithtexturemaps.InSIGGRAPH'94(1994).]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808601</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[GORAL,C.M.,TORRANCE,K.E.,GREENBERG,D.P.,ANDBATTAILE,B. Modelingtheinteractionoflightbetweendiffusesurfaces.InSIGGRAPH'84 (1984),pp.213-222.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237200</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[GORTLER,S.J.,GRZESZCZUK,R.,SZELISKI,R.,ANDCOHEN,M.F.The Lumigraph.InSIGGRAPH'96(1996),pp.43-54.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>13027</ref_obj_id>
				<ref_obj_pid>13021</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[HECKBERT,P.S.Surveyoftexturemapping.IEEEComputerGraphicsand Applications6,11(November1986),56-67.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15902</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[KAJIYA,J.Therenderingequation.InSIGGRAPH'86(1986),pp.143-150.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[KARNER,K.F.,MAYER,H.,ANDGERVAUTZ,M.Animagebasedmeasure-mentsystemforanisotropicreflection. InEUROGRAPHICSAnnualConference Proceedings(1996).]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[KOENDERINK,J.J.,ANDVANDOORN,A.J.Illuminancetextureduetosurface mesostructure.J.Opt.Soc.Am.13,3(1996).]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[LAVEAU,S.,ANDFAUGERAS,O.3-Dscenerepresentationasacollectionof images.InProceedingsof12thInternationalConferenceonPatternRecognition (1994),vol.1,pp.689-691.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237199</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[LEVOY,M.,ANDHANRAHAN,P.Lightfieldrendering.InSIGGRAPH'96 (1996),pp.31-42.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218398</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[MCMILLAN,L.,ANDBISHOP,G.PlenopticModeling:Animage-basedren-deringsystem. InSIGGRAPH'95(1995).]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15909</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[NAKAMAE,E.,HARADA,K.,ANDISHIZAKI,T.Amontagemethod:Theover-layingofthecomputergeneratedimagesontoabackgroundphotograph. InSIG-GRAPH'86( 1986),pp.207-214.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808606</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[PORTER,T.,ANDDUFF,T.Compositingdigitalimages.InSIGGRAPH84(July 1984),pp.253-259.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258885</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[SATO,Y.,WHEELER,M.D.,ANDIKEUCHI,K.Objectshapeandreflectance modelingfromobservation.InSIGGRAPH'97(1997),pp.379-387.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[SMITH,T.G.IndustrialLightandMagic:TheArtofSpecialEffects.Ballantine Books,NewYork,1986.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[SZELISKI,R.Imagemosaicingfortele-realityapplications.InIEEEComputer GraphicsandApplications(1996).]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192241</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[TURK,G.,ANDLEVOY,M.Zipperedpolygonmeshesfromrangeimages.In SIGGRAPH'94(1994),pp.311-318.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258775</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[VEACH,E.,ANDGUIBAS,L.J.Metropolislighttransport.InSIGGRAPH'97 (August1997),pp.65-76.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134078</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[WARD,G.J.Measuringandmodelinganisotropicreflection.InSIGGRAPH'92 (July1992),pp.265-272.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192286</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[WARD,G.J.Theradiancelightingsimulationandrenderingsystem.InSIG-GRAPH'94( July1994),pp.459-472.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[WATANABE,M.,ANDNAYAR,S.K.Telecentricopticsforcomputationalvi-sion. InProceedingsofImageUnderstandingWorkshop(IUW96)(February 1996).]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_obj_id>138633</ref_obj_id>
				<ref_obj_pid>138628</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Y.CHEN,ANDMEDIONI,G.Objectmodelingfrommultiplerangeimages.Im-ageandVisionComputing10, 3(April1992),145-155.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 To appear in the SIGGRAPH 98 conference proceedings Rendering Synthetic Objects into Real Scenes: Supplemental 
materials for this paper are available in the papers/debevec directory. Bridging Traditional and Image-based 
Graphics with Global Illumination and High Dynamic Range Photography Paul Debevec University of California 
at Berkeley1 ABSTRACT We present a method that uses measured scene radiance and global illumination 
in order to add new objects to light-based models with correct lighting. The method uses a high dynamic 
range image­based model of the scene, rather than synthetic light sources, to il­luminate the new objects. 
To compute the illumination, the scene is considered as three components: the distant scene, the local 
scene, and the synthetic objects. The distant scene is assumed to be pho­tometrically unaffected by the 
objects, obviating the need for re­.ectance model information. The local scene is endowed with es­timated 
re.ectance model information so that it can catch shadows and receive re.ected light from the new objects. 
Renderings are created with a standard global illumination method by simulating the interaction of light 
amongst the three components. A differen­tial rendering technique allows for good results to be obtained 
when only an estimate of the local scene re.ectance properties is known. We apply the general method 
to the problem of rendering syn­thetic objects into real scenes. The light-based model is constructed 
from an approximate geometric model of the scene and by using a light probe to measure the incident illumination 
at the location of the synthetic objects. The global illumination solution is then com­posited into a 
photograph of the scene using the differential render­ing technique. We conclude by discussing the relevance 
of the tech­nique to recovering surface re.ectance properties in uncontrolled lighting situations. Applications 
of the method include visual ef­fects, interior design, and architectural visualization. CR Descriptors: 
I.2.10 [Arti.cial Intelligence]: Vision and Scene Understanding -Intensity, color, photometry and threshold­ing; 
I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism -Color, shading, shadowing, and texture; 
I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism -Radiosity; I.4.1 [Image Processing]: 
Digitization -Scanning; I.4.8 [Image Processing]: Scene Analysis -Photometry, Sensor Fusion. 1Computer 
Science Division, University of California at Berke­ley, Berkeley, CA 94720-1776. Email: debevec@cs.berkeley.edu. 
More information and additional results may be found at: http://www.cs.berkeley.edu/ debevec/Research 
 1 Introduction Rendering synthetic objects into real-world scenes is an important application of computer 
graphics, particularly in architectural and visual effects domains. Oftentimes, a piece of furniture, 
a prop, or a digital creature or actor needs to be rendered seamlessly into a real scene. This dif.cult 
task requires that the objects be lit con­sistently with the surfaces in their vicinity, and that the 
interplay of light between the objects and their surroundings be properly simu­lated. Speci.cally, the 
objects should cast shadows, appear in re.ec­tions, and refract, focus, and emit light just as real objects 
would. Figure 1: The General Method In our method for adding synthetic objects into light-based scenes, 
the scene is partitioned into three components: the distant scene, the local scene, and the synthetic 
ob­jects. Global illumination is used to simulate the interplay of light amongst all three components, 
except that light re.ected back at the distant scene is ignored. As a result, BRDF information for the 
dis­tant scene is unnecessary. Estimates of the geometry and material properties of the local scene are 
used to simulate the interaction of light between it and the synthetic objects. Currently available techniques 
for realistically rendering syn­thetic objects into scenes are labor intensive and not always success­ful. 
A common technique is to manually survey the positions of the light sources, and to instantiate a virtual 
light of equal color and in­tensity for each real light to illuminate the synthetic objects. An­other 
technique is to photograph a reference object (such as a gray sphere) in the scene where the new object 
is to be rendered, and use its appearance as a qualitative guide in manually con.guring the lighting 
environment. Lastly, the technique of re.ection mapping is useful for mirror-like re.ections. These methods 
typically require considerable hand-re.nement and none of them easily simulates the effects of indirect 
illumination from the environment. Accurately simulating the effects of both direct and indirect light­ing 
has been the subject of research in global illumination. With a global illumination algorithm, if the 
entire scene were modeled with its full geometric and re.ectance (BRDF) characteristics, one could correctly 
render a synthetic object into the scene simply by adding it to the model and recomputing the global 
illumination solution. Un­fortunately, obtaining a full geometric and re.ectance model of a large environment 
is extremeley dif.cult. Furthermore, global il­lumination solutions for large complex environments are 
extremely computationally intensive. Moreover, it seems that having a full re.ectance model of the large-scale 
scene should be unnecessary: under most circumstances, a new object will have no signi.cant effect on 
the appearance of most of the of the distant scene. Thus, for such distant areas, know­ing just its radiance 
(under the desired lighting conditions) should suf.ce. Recently, [9] introduced a high dynamic range 
photographic technique that allows accurate measurements of scene radiance to be derived from a set of 
differently exposed photographs. This tech­nique allows both low levels of indirect radiance from surfaces 
and high levels of direct radiance from light sources to be accurately recorded. When combined with image-based 
modeling techniques (e.g. [22, 24, 4, 10, 23, 17, 29]), and possibly active techniques for measuring 
geometry (e.g. [35, 30, 7, 27]) these derived radiance maps can be used to construct spatial representations 
of scene ra­diance. We will use the term light-based model to refer to a repre­sentation of a scene that 
consists of radiance information, possi­bly with speci.c reference to light leaving surfaces, but not 
neces­sarily containing material property (BRDF) information. A light­based model can be used to evaluate 
the 5D plenoptic function [1] P((;q;Vx;V;Vz)for a given virtual or real subset of space1.A y material-based 
model is converted to a light-based model by com­puting an illumination solution for it. A light-based 
model is differ­entiated from an image-based model in that its light values are ac­tual measures of radiance2, 
whereas image-based models may con­tain pixel values already transformed and truncated by the response 
function of an image acquisition or synthesis process. In this paper, we present a general method for 
using accurate measurements of scene radiance in conjunction with global illumi­nation to realistically 
add new objects to light-based models. The synthetic objects may have arbitrary material properties and 
can be rendered with appropriate illumination in arbitrary lighting environ­ments. Furthermore, the objects 
can correctly interact with the en­vironment around them: they cast the appropriate shadows, they are 
properly re.ected, they can re.ect and focus light, and they exhibit appropriate diffuse interre.ection. 
The method can be carried out with commonly available equipment and software. In this method (see Fig. 
1), the scene is partitioned into three components. The .rst is the distant scene, which is the visible 
part of the environment too remote to be perceptibly affected by the syn­thetic object. The second is 
the local scene, which is the part of the environment which will be signi.cantly affected by the presence 
of the objects. The third component is the synthetic objects. Our ap­proach uses global illumination 
to correctly simulate the interaction of light amongst these three elements, with the exception that 
light radiated toward the distant environment will not be considered in the calculation. As a result, 
the BRDF of the distant environment need not be known the technique uses BRDF information only for the 
local scene and the synthetic objects. We discuss the challenges in estimating the BRDF of the local 
scene, and methods for obtain­ing usable approximations. We also present a differential rendering 1Time 
and wavelength dependence can be included to represent the gen­eral 7D plenoptic function as appropriate. 
2In practice, the measures of radiance are with respect to a discrete set of spectral distributions such 
as the standard tristimulus model. technique that produces perceptually accurate results even when the 
estimated BRDF is somewhat inaccurate. We demonstrate the general method for the speci.c case of ren­dering 
synthetic objects into particular views of a scene (such as background plates) rather than into a general 
image-based model. In this method, a light probe is used to acquire a high dynamic range panoramic radiance 
map near the location where the object will be rendered. A simple example of a light probe is a camera 
aimed at a mirrored sphere, a con.guration commonly used for acquiring envi­ronment maps. An approximate 
geometric model of the scene is cre­ated (via surveying, photogrammetry, or 3D scanning) and mapped with 
radiance values measured with the light probe. The distant scene, local scene, and synthetic objects 
are rendered with global illumination from the same point of view as the background plate, and the results 
are composited into the background plate with a dif­ferential rendering technique. 1.1 Overview The rest 
of this paper is organized as follows. In the next section we discuss work related to this paper. Section 
3 introduces the ba­sic technique of using acquired maps of scene radiance to illuminate synthetic objects. 
Section 4 presents the general method we will use to render synthetic objects into real scenes. Section 
5 describes a practical technique based on this method using a light probe to mea­sure incident illumination. 
Section 6 presents a differential render­ing technique for rendering the local environment with only 
an ap­proximate description of its re.ectance. Section 7 presents a sim­ple method to approximately recover 
the diffuse re.ectance char­acteristics of the local environment. Section 8 presents results ob­tained 
with the technique. Section 9 discusses future directions for this work, and we conclude in Section 10. 
  2 Background and Related Work The practice of adding new objects to photographs dates to the early 
days of photography in the simple form of pasting a cut-out from one picture onto another. While the 
technique conveys the idea of the new object being in the scene, it usually fails to produce an image 
that as a whole is a believable photograph. Attaining such realism requires a number of aspects of the 
two images to match. First, the camera projections should be consistent, otherwise the object may seem 
too foreshortened or skewed relative to the rest of the picture. Second, the patterns of .lm grain and 
.lm response should match. Third, the lighting on the object needs to be consistent with other objects 
in the environment. Lastly, the object needs to cast realistic shadows and re.ections on the scene. Skilled 
artists found that by giving these considerations due attention, synthetic objects could be painted into 
still photographs convincingly. In optical .lm compositing, the use of object mattes to prevent particular 
sections of .lm from being exposed made the same sort of cut-and-paste compositing possible for moving 
images. However, the increased demands of realism imposed by the dynamic nature of .lm made matching 
camera positions and lighting even more criti­cal. As a result, care was taken to light the objects appropriately 
for the scene into which they were to be composited. This would still not account for the objects casting 
shadows onto the scene, so often these were painted in by an artist frame by frame [13, 2, 28]. Digi­tal 
.lm scanning and compositing [26] helped make this process far more ef.cient. Work in global illumination 
[16, 19] has recently produced algo­rithms (e.g. [31]) and software (e.g. [33]) to realistically simulate 
lighting in synthetic scenes, including indirect lighting with both specular and diffuse re.ections. 
We leverage this work in order to create realistic renderings. Some work has been done on the speci.c 
problem of composit­ing objects into photography. [25] presented a procedure for ren­dering architecture 
into background photographs using knowledge of the sun position and measurements or approximations of 
the lo­cal ambient light. For diffuse buildings in diffuse scenes, the tech­nique is effective. The technique 
of re.ection mapping (also called environment mapping) [3, 18] produces realistic results for mirror­like 
objects. In re.ection mapping, a panoramic image is rendered or photographed from the location of the 
object. Then, the surface normals of the object are used to index into the panoramic image by re.ecting 
rays from the desired viewpoint. As a result, the shiny object appears to properly re.ect the desired 
environment3.How­ever, the technique is limited to mirror-like re.ection and does not account for objects 
casting light or shadows on the environment.  A common visual effects technique for having synthetic 
objects cast shadows on an existing environment is to create an approximate geometric model of the environment 
local to the object, and then compute the shadows from the various light sources. The shadows can then 
be subtracted from the background image. In the hands of professional artists this technique can produce 
excellent results, but it requires knowing the position, size, shape, color, and intensity of each of 
the scene s light sources. Furthermore, it does not account for diffuse re.ection from the scene, and 
light re.ected by the ob­jects onto the scene must be handled specially. To properly model the interaction 
of light between the objects and the local scene, we pose the compositing problem as a global illumi­nation 
computation as in [14] and [12]. As in this work, we apply the effect of the synthetic objects in the 
lighting solution as a dif­ferential update to the original appearance of the scene. In the pre­vious 
work an approximate model of the entire scene and its origi­nal light sources is constructed; the positions 
and sizes of the light sources are measured manually. Rough methods are used to esti­mate diffuse-only 
re.ectance characteristics of the scene, which are then used to estimate the intensities of the light 
sources. [12] addi­tionally presents a method for performing fast updates of the illu­mination solution 
in the case of moving objects. As in the previous work, we leverage the basic result from incremental 
radiosity [6, 5] that making a small change to a scene does not require recomputing the entire solution. 
 3 Illuminating synthetic objects with real light In this section we propose that computer-generated 
objects be lit by actual recordings of light from the scene, using global illumination. Performing the 
lighting in this manner provides a uni.ed and phys­ically accurate alternative to manually attempting 
to replicate inci­dent illumination conditions. Accurately recording light in a scene is dif.cult because 
of the high dynamic range that scenes typically exhibit; this wide range of brightness is the result 
of light sources being relatively concen­trated. As a result, the intensity of a source is often two 
to six orders of magnitude larger than the intensity of the non-emissive parts of an environment. However, 
it is necessary to accurately record both the large areas of indirect light from the environment and 
the con­centrated areas of direct light from the sources since both are signif­icant parts of the illumination 
solution. Using the technique introduced in [9], we can acquire correct measures of scene radiance using 
conventional imaging equipment. The images, called radiance maps, are derived from a series of im­ages 
with different sensor integration times and a technique for com­puting and accounting for the imaging 
system response function f. We can use these measures to illuminate synthetic objects exhibiting arbitrary 
material properties. Fig. 2 shows a high-dynamic range lighting environment with electric, natural, and 
indirect lighting. This environment was 3Using the surface normal indexing method, the object will not 
re.ect itself. Correct self-re.ection can be obtained through ray tracing. recorded by taking a full 
dynamic range photograph of a mirrored ball on a table (see Section 5). A digital camera was used to 
acquire a series images in one-stop exposure increments from 1to 1 4 10000 second. The images were fused 
using the technique in [9]. The environment is displayed at three exposure levels (-0, -3.5, and -7.0 
stops) to show its full dynamic range. Recovered RGB ra­diance values for several points in the scene 
and on the two major light sources are indicated; the color difference between the tung­sten lamp and 
the sky is evident. A single low-dynamic range pho­tograph would be unable to record the correct colors 
and intensities over the entire scene. Fig. 3(a-e) shows the results of using this panoramic radiance 
map to synthetically light a variety of materials using the RADI-ANCE global illumination algorithm [33]. 
The materials are: (a) perfectly re.ective, (b) rough gold, (c) perfectly diffuse gray ma­terial, (d) 
shiny green plastic, and (e) dull orange plastic. Since we are computing a full illumination solution, 
the objects exhibit self-re.ection and shadows from the light sources as appropriate. Note that in (c) 
the protrusions produce two noticeable shadows of slightly different colors, one corresponding to the 
ceiling light and a softer shadow corresponding to the window. The shiny plastic object in (d) has a 
4 percent specular component with a Gaussian roughness of 0.04 [32]. Since the object s surface both 
blurs and attenuates the light with its rough specular compo­nent, the re.ections fall within the dynamic 
range of our display de­vice and the different colors of the light sources can be seen. In (e) the rough 
plastic diffuses the incident light over a much larger area. To illustrate the importance of using high 
dynamic range radi­ance maps, the same renderings were produced using just one of the original photographs 
as the lighting environment. In this single image, similar in appearance to Fig. 2(a), the brightest 
regions had been truncated to approximately 2 percent of their true values. The rendering of the mirrored 
surface (f) appears similar to (a) since it is displayed in low-dynamic range printed form. Signi.cant 
errors are noticeable in (g-j) since these materials blur the incident light. In (g), the blurring of 
the rough material darkens the light sources, whereas in (b) they remain saturated. Renderings (h-j) 
are very dark due to the missed light; thus we have brightened by a factor of eight on the right in order 
to make qualitative comparisons to (c-e) pos­sible. In each it can be seen that the low-dynamic range 
image of the lighting environment fails to capture the information necessary to simulate correct color 
balance, shadows, and highlights. Fig. 4 shows a collection of objects with different material prop­erties 
illuminated by two different environments. A wide variety of light interaction between the objects and 
the environment can be seen. The (synthetic) mirrored ball re.ects both the synthetic ob­jects as well 
as the environment. The .oating diffuse ball shows a subtle color shift along its right edge as it shadows 
itself from the windows and is lit primarily by the incandescent lamp in Fig. 4(a). The re.ection of 
the environment in the black ball (which has a specular intensity of 0.04) shows the colors of the light 
sources, which are too bright to be seen in the mirrored ball. A variety of shadows, re.ections, and 
focused light can be observed on the rest­ing surface. The next section describes how the technique of 
using radiance maps to illuminate synthetic objects can be extended to compute the proper photometric 
interaction of the objects with the scene. It also describes how high dynamic range photography and image-based 
modeling combine in a natural manner to allow the simulation of arbitrary (non-in.nite) lighting environments. 
 4 The General Method This section explains our method for adding new objects to light­based scene representations. 
As in Fig. 1, we partition our scene into three parts: the distant scene, the local scene, and the synthetic 
 Figure 2: An omnidirectional radiance map This full dynamic range lighting environment was acquired 
by photographing a mirrored ball balanced on the cap of a pen sitting on a table. The environment contains 
natural, electric, and indirect light. The three views of this image adjusted to (a) +0 stops, (b) -3.5 
stops, and (c) -7.0 stops show that the full dynamic range of the scene has been captured without saturation. 
As a result, the image usefully records the direction, color, and intensity of all forms of incident 
light. Figure 3: Illuminating synthetic objects with real light (Top row: a,b,c,d,e) With full dynamic 
range measurements of scene radiance from Fig. 2. (Bottom row: f,g,h,i,j) With low dynamic range information 
from a single photograph of the ball. The right sides of images (h,i,j) have been brightened by a factor 
of six to allow qualitative comparison to (c,d,e). The high dynamic range measurements of scene radiance 
are necessary to produce proper lighting on the objects. Figure 4: Synthetic objects lit by two different 
environments (a) A collection of objects is illuminated by the radiance information in 2. The objects 
exhibit appropriate interre.ection. (b) The same objects are illuminated by different radiance information 
obtained in an outdoor urban environment on an overcast day. The radiance map used for the illumination 
is shown in the upper left of each image. Candle holder model courtesy of Gregory Ward Larson. objects. 
We describe the geometric and photometric requirements for each of these components. 1. A light-based 
model of the distant scene The distant scene is constructed as a light-based model. The synthetic objects 
will receive light from this model, so it is nec­essary that the model store true measures of radiance 
rather than low dynamic range pixel values from conventional im­ages. The light-based model can take 
on any form, using very little explicit geometry [23, 17], some geometry [24], moder­ate geometry [10], 
or be a full 3D scan of an environment with view-dependent texture-mapped [11] radiance. What is im­portant 
is for the model to provide accurate measures of inci­dent illumination in the vicinity of the objects, 
as well as from the desired viewpoint. In the next section we will present a convenient procedure for 
constructing a minimal model that meets these requirements. In the global illumination computation, the 
distant scene ra­diates light toward the local scene and the synthetic objects, but ignores light re.ected 
back to it. We assume that no area of the distant scene will be signi.cantly affected by light re­.ecting 
from the synthetic objects; if that were the case, the area should instead belong to the local scene, 
which contains the BRDF information necessary to interact with light. In the RADIANCE [33] system, this 
exclusively emissive behavior can be speci.ed with the glow material property. 2. An approximate material-based 
model of the local scene The local scene consists of the surfaces that will photomet­rically interact 
with the synthetic objects. It is this geome­try onto which the objects will cast shadows and re.ect 
light. Since the local scene needs to fully participate in the illumina­tion solution, both its geometry 
and re.ectance characteristics should be known, at least approximately. If the geometry of the local 
scene is not readily available with suf.cient accuracy from the light-based model of the distant scene, 
there are vari­ous techniques available for determining its geometry through active or passive methods. 
In the common case where the lo­cal scene is a .at surface that supports the synthetic objects, its geometry 
is determined easily from the camera pose. Meth­ods for estimating the BRDF of the local scene are discussed 
in Section 7. Usually, the local scene will be the part of the scene that is geo­metrically close to 
the synthetic objects. When the local scene is mostly diffuse, the rendering equation shows that the 
visible effect of the objects on the local scene decreases as the inverse square of the distance between 
the two. Nonetheless, there is a variety of circumstances in which synthetic objects can signif­icantly 
affect areas of the scene not in the immediate vicinity. Some common circumstances are: .If there are 
concentrated light sources illuminating the object, then the object can cast a signi.cant shadow on a 
distant surface collinear with it and the light source. .If there are concentrated light sources and 
the object is .at and specular, it can focus a signi.cant amount of light onto a distant part of the 
scene. .If a part of the distant scene is .at and specular (e.g. a mirror on a wall), its appearance 
can be signi.cantly af­fected by a synthetic object. .If the synthetic object emits light (e.g. a synthetic 
laser), it can affect the appearance of the distant scene signi.­cantly.   These situations should 
be considered in choosing which parts of the scene should be considered local and which parts dis­tant. 
Any part of the scene that will be signi.cantly affected in 5 its appearance from the desired viewpoint 
should be included as part of the local scene. Since the local scene is a full BRDF model, it can be 
added to the global illumination problem as would any other object. The local scene may consist of any 
number of surfaces and ob­jects with different material properties. For example, the local scene could 
consist of a patch of .oor beneath the synthetic object to catch shadows as well as a mirror surface 
hanging on the opposite wall to catch a re.ection. The local scene re­places the corresponding part of 
the light-based model of the distant scene. Since it can be dif.cult to determine the precise BRDF char­acteristics 
of the local scene, it is often desirable to have only the change in the local scene s appearance be 
computed with the BRDF estimate; its appearance due to illumination from the distant scene is taken from 
the original light-based model. This differential rendering method is presented in Section 6. 3. Complete 
material-based models of the objects The synthetic objects themselves may consist of any variety of shapes 
and materials supported by the global illumination software, including plastics, metals, emitters, and 
dielectrics such as glass and water. They should be placed in their desired geometric correspondence 
to the local scene. Once the distant scene, local scene, and synthetic objects are properly modeled and 
positioned, the global illumination software can be used in the normal fashion to produce renderings 
from the desired viewpoints.  5 Compositing using a light probe This section presents a particular technique 
for constructing a light­based model of a real scene suitable for adding synthetic objects at a particular 
location. This technique is useful for compositing objects into actual photography of a scene. In Section 
4, we mentioned that the light-based model of the dis­tant scene needs to appear correctly in the vicinity 
of the synthetic objects as well as from the desired viewpoints. This latter require­ment can be satis.ed 
if it is possible to directly acquire radiance maps of the scene from the desired viewpoints. The former 
require­ment, that the appear photometrically correct in all directions in the vicinity of the synthetic 
objects, arises because this information comprises the incident light which will illuminate the objects. 
To obtain this part of the light-based model, we acquire a full dy­namic range omnidirectional radiance 
map near the location of the synthetic object or objects. One technique for acquiring this radi­ance 
map is to photograph a spherical .rst-surface mirror, such as a polished steel ball, placed at or near 
the desired location of the syn­thetic object4. This procedure is illustrated in Fig. 7(a). An actual 
radiance map obtained using this method is shown in Fig. 2. The radiance measurements observed in the 
ball are mapped onto the geometry of the distant scene. In many circumstances this model can be very 
simple. In particular, if the objects are small and resting on a .at surface, one can model the scene 
as a horizontal plane for the resting surface and a large dome for the rest of the environment. Fig. 
7(c) illustrates the ball image being mapped onto a table surface and the walls and ceiling of a .nite 
room; 5 shows the resulting light­based model. 5.1 Mapping from the probe to the scene model To precisely 
determine the mapping between coordinates on the ball and rays in the world, one needs to record the 
position of the ball 4Parabolic mirrors combined with telecentric lenses [34] can be used to obtain hemispherical 
.elds of view with a consistent principal point, if so desired. relative to the camera, the size of 
the ball, and the camera param­eters such as its location in the scene and focal length. With this information, 
it is straightforward to trace rays from the camera cen­ter through the pixels of the image, and re.ect 
rays off the ball into the environment. Often a good approximation results from assum­ing the ball is 
small relative to the environment and that the camera s view is orthographic. The data acquired from 
a single ball image will exhibit a num­ber of artifacts. First, the camera (and possibly the photographer) 
will be visible. The ball, in observing the scene, interacts with it: the ball (and its support) can 
appear in re.ections, cast shadows, and can re.ect light back onto surfaces. Lastly, the ball will not 
re­.ect the scene directly behind it, and will poorly sample the area nearby. If care is taken in positioning 
the ball and camera, these ef­fects can be minimized and will have a negligible effect on the .nal renderings. 
If the artifacts are signi.cant, the images can be .xed manually in image editing program or by selectively 
combining im­ages of the ball taken from different directions; Fig. 6 shows a rela­tively artifact-free 
enviroment constructed using the latter method. We have found that combining two images of the ball taken 
ninety degrees apart from each other allows us to eliminate the camera s appearance and to avoid poor 
sampling. (a) (b) Figure 6: Rendering with a Combined Probe Image The full dy­namic range environment 
map shown at the top was assembled from two light probe images taken ninety degrees apart from each other. 
As a result, the only visible artifact is small amount of the probe sup­port visible on the .oor. The 
map is shown at -4.5, 0, and +4.5 stops. The bottom rendering was produced using this lighting information, 
and exhibits diffuse and specular re.ections, shadows from different sources of light, re.ections, and 
caustics. 6 5.2 Creating renderings To render the objects into the scene, a synthetic local scene model 
is created as described in Section 4. Images of the scene from the de­sired viewpoint(s) are taken (Fig. 
7(a)), and their position relative to the scene is recorded through pose-instrumented cameras or (as 
in our work) photogrammetry. The location of the ball in the scene is also recorded at this time. The 
global illumination software is then run to render the objects, local scene, and distant scene from the 
de­sired viewpoint (Fig. 7(d)). The objects and local scene are then composited onto the back­ground 
image. To perform this compositing, a mask is created by rendering the objects and local scene in white 
and the distant scene in black. If objects in the distant scene (which may appear in front of the objects 
or local scene from certain viewpoints) are geomet­rically modeled, they will properly obscure the local 
scene and the objects as necessary. This compositing can be considered as a subset of the general method 
(Section 4) wherein the light-based model of the distant scene acts as follows: if (Vx;V;V)corresponds 
to an actual view of the scene, return the radiance value looking in direc­tion ((;q). Otherwise, return 
the radiance value obtained by casting the ray ((;q;Vx;V;Vz)onto the radiance-mapped distant scene yz 
y model. In the next section we describe a more robust method of com­positing the local scene into the 
background image.  6 Improving quality with differential ren­dering The method we have presented so 
far requires that the local scene be modeled accurately in both its geometry and its spatially varying 
material properties. If the model is inaccurate, the appearance of the local scene will not be consistent 
with the appearance of adjacent distant scene. Such a border is readily apparent in Fig. 8(c), since 
the local scene was modeled with a homogeneous BRDF when in reality it exhibits a patterned albedo (see 
[21]). In this section we describe a method for greatly reducing such effects. Suppose that we compute 
a global illumination solution for the local and distant scene models without including the synthetic 
ob­jects. If the BRDF and geometry of the local scene model were per­fectly accurate, then one would 
expect the appearance of the ren­dered local scene to be consistent with its appearance in the light­based 
model of the entire scene. Let us call the appearance of the lo­cal scene from the desired viewpoint 
in the light-based model LSb. In the context of the method described in Section 5, LSbis simply the background 
image. We will let LSnoobjdenote the appearance of the local scene, without the synthetic objects, as 
calculated by the global illumination solution. The error in the rendered local scene (without the objects) 
is thus: Errls=LSnoobj,LSb. This error results from the difference between the BRDF characteristics of 
the actual local scene as compared to the modeled local scene. Let LSobjdenote the appearance of the 
local environment as cal­culated by the global illumination solution with the synthetic objects in place. 
We can compensate for the error if we compute our .nal rendering LSfinalas: LSfinal=LSobj,Err ls Equivalently, 
we can write: LSfinal=LSb+(LSobj,LSnoobj) In this form, we see that whenever LSobjand LSnoobjare the 
same (i.e. the addition of the objects to the scene had no effect on the local scene) the .nal rendering 
of the local scene is equivalent to LSb(e.g. the background plate). When LSobjis darker than LSnoobj, 
light is subtracted from the background to form shadows,  Figure 5: A Light-Based Model A simple light-based 
model of a room is constructed by mapping the image from a light probe onto a box. The box corresponds 
to the upper half of the room, with the bottom face of the box being coincident with the top of the table. 
The model contains the full dynamic range of the original scene, which is not reproduced in its entirety 
in this .gure. and when LSobjis lighter than LSnoobjlight is added to the back­ground to produce re.ections 
and caustics. Stated more generally, the appearance of the local scene without the objects is computed 
with the correct re.ectance characteristics lit by the correct environment, and the change in appearance 
due to the presence of the synthetic objects is computed with the modeled re.ectance characteristics 
as lit by the modeled environment. While the realism of LSfinalstill bene.ts from having a good model 
of the re.ectance characteristics of the local scene, the perceptual ef­fect of small errors in albedo 
or specular properties is considerably reduced. Fig. 8(g) shows a .nal rendering in which the local en­vironment 
is computed using this differential rendering technique. The objects are composited into the image directly 
from the LSobj solution shown in Fig. 8(c). It is important to stress that this technique can still produce 
abi­trarily wrong results depending on the amount of error in the es­timated local scene BRDF and the 
inaccuracies in the light-based model of the distance scene. In fact, Errmay be larger than ls LSobj, 
causing LSfinalto be negative. An alternate approach is to compensate for the relative error in the appearance 
of the local scene: LSfinal =LSb(LSobjILSnoobj). Inaccuracies in the local scene BDRF will also be re.ected 
in the objects. In the next section we discuss techniques for estimating the BRDF of the local scene. 
 7 Estimating the local scene BRDF Simulating the interaction of light between the local scene and the 
synthetic objects requires a model of the re.ectance characteristics of the local scene. Considerable 
recent work [32, 20, 8, 27] has pre­sented methods for measuring the re.ectance properties of mate­rials 
through observation under controlled lighting con.gurations. Furthermore, re.ectance characteristics 
can also be measured with commercial radiometric devices. It would be more convenient if the local scene 
re.ectance could be estimated directly from observation. Since the light-based model contains information 
about the radiance of the local scene as well as its irradiance, it actually contains information about 
the local scene re.ectance. If we hypothesize re.ectance characteristics for the lo­cal scene, we can 
illuminate the local scene with its known irradi­ance from the light-based model. If our hypothesis is 
correct, then the appearance should be consistent with the measured appearance. This suggests the following 
iterative method for recovering the re­.ectance properties of the local scene: 1. Assume a re.ectance 
model for the local scene (e.g. diffuse only, diffuse + specular, metallic, or arbitrary BRDF, including 
spatial variation) 2. Choose approximate initial values for the parameters of the re­.ectance model 
3. Compute a global illumination solution for the local scene with the current parameters using the observed 
lighting con­.guration or con.gurations. 4. Compare the appearance of the rendered local scene to its 
ac­tual appearance in one or more views. 5. If the renderings are not consistent, adjust the parameters 
of the re.ectance model and return to step 3.  Ef.cient methods of performing the adjustment in step 
5 that ex­ploit the properties of particular re.ectance models are left as future work. However, assuming 
a diffuse-only model of the local scene in step 1 makes the adjustment in step 5 straightforward. We 
have: ZZ 2 2 Lr1((r;qr)= PdLi((i;qi)cos(isin(id(idqi= 00 ZZ 2 2 Pd Li((i;qi)cos(isin(id(idqi 00 If we 
initialize the local scene to be perfectly diffuse (Pd =1) everywhere, we have: ZZ 2 2 Lr2((r;qr)= Li((i;qi)cos(isin(id(idqi 
00 The updated diffuse re.ectance coef.cient for each part of the lo­cal scene can be computed as: 0 
Lr1((r;qr)P= dLr2((r;qr) In this manner, we use the global illumination calculation to ren­der each patch 
as a perfectly diffuse re.ector, and compare the re­sulting radiance to the observed value. Dividing 
the two quantities yields the next estimate of the diffuse re.ection coef.cient P0.If d there is no interre.ection 
within the local scene, then the P0 esti­ d mates will make the renderings consistent. If there is interre.ection, 
then the algorithm should be iterated until there is convergence. For a trichromatic image, the red, 
green, and blue diffuse re­.ectance values are computed independently. The diffuse charac­teristics of 
the background material used to produce Fig. 8(c) were  (a) Acquiring the background photograph light 
probe  (b) Using the light probe (d) Computing the global illumination solution Figure 7: Using a light 
probe (a) The background plate of the scene (some objects on a table) is taken. (b) A light probe (in 
this case, the camera photographing a steel ball) records the incident radiance near the location of 
where the synthetic objects are to be placed. (c) A simpli.ed light-based model of the distant scene 
is created as a planar surface for the table and a .nite box to represent the rest of the room. The scene 
is texture-mapped in high dynamic range with the radiance map from the light probe. The objects on the 
ta­ble, which were not explicitly modeled, become projected onto the table. (d) Synthetic objects and 
a BRDF model of the local scene are added to the light-based model of the distant scene. A global illumi­nation 
solution of this con.guration is computed with light coming from the distant scene and interacting with 
the local scene and syn­thetic objects. Light re.ected back to the distant scene is ignored. The results 
of this rendering are composited (possibly with differ­ential rendering) into the background plate from 
(a) to achieve the .nal result. computed using this method, although it was assumed that the entire local 
scene had the same diffuse re.ectance. In the standard plastic illumination model, just two more co­ef.cients 
 those for specular intensity and roughness need to be speci.ed. In Fig. 8, the specular coef.cients 
for the local scene were estimated manually based on the specular re.ection of the window in the table 
in Fig. 2.  8 Compositing Results Fig. 5 shows a simple light-based model of a room constructed us­ing 
the panoramic radiance map from Fig. 2. The room model be­gins at the height of the table and continues 
to the ceiling; its mea­surements and the position of the ball within it were measured man­ually. The 
table surface is visible on the bottom face. Since the room model is .nite in size, the light sources 
are effectively local rather than in.nite. The stretching on the south wall is due to the poor sam­pling 
toward the silhouette edge of the ball. Figs. 4 and 6 show complex arrangements of synthetic objects 
lit entirely by a variety of light-based models. The selection and com­position of the objects in the 
scene was chosen to exhibit a wide vari­ety of light interactions, including diffuse and specular re.ectance, 
multiple soft shadows, and re.ected and focused light. Each ren­dering was produced using the RADIANCE 
system with two dif­fuse light bounces and a relatively high density of ambient sample points. Fig. 8(a) 
is a background plate image into which the synthetic ob­jects will be rendered. In 8(b) a calibration 
grid was placed on the table in order to determine the camera pose relative to the scene and to the mirrored 
ball, which can also be seen. The poses were deter­mined using the photogrammetric method in [10]. In 
8(c), a model of the local scene as well as the synthetic objects is geometrically matched and composited 
onto the background image. Note that the local scene, while the same average color as the table, is readily 
dis­tinguishable at its edges and because it lacks the correct variations in albedo. Fig. 8(d) shows 
the results of lighting the local scene model with the light-based model of the room, without the objects. 
This image will be compared to 8(c) in order to determine the effect the syn­thetic objects have on the 
local scene. Fig. 8(e) is a mask image in which the white areas indicate the location of the synthetic 
objects. If the distant or local scene were to occlude the objects, such regions would be dark in this 
image. Fig. 8(f) shows the difference between the appearance of the lo­cal scene rendered with (8(c)) 
and without (8(d)) the objects. For il­lustration purposes, the difference in radiance values have been 
off­set so that zero difference is shown in gray. The objects have been masked out using image 8(e). 
This difference image encodes both the shadowing (dark areas) and re.ected and focussed light (light 
areas) imposed on the local scene by the addition of the synthetic objects. Fig. 8(g) shows the .nal 
result using the differential rendering method described in Section 6. The synthetic objects are copied 
directly from the global illumination solution 8(c) using the object mask 8(e). The effects the objects 
have on the local scene are in­cluded by adding the difference image 8(f) (without offset) to the background 
image. The remainder of the scene is copied directly from the background image 8(a). Note that in the 
mirror ball s re­.ection, the modeled local scene can be observed without the effects of differential 
rendering a limitation of the compositing tech­nique. In this .nal rendering, the synthetic objects 
exhibit a consistent appearance with the real objects present in the background image 8(a) in both their 
diffuse and specular shading, as well as the di­rection and coloration of their shadows. The somewhat 
speckled nature of the object re.ections seen in the table surface is due to  (a) Background photograph 
(b) Camera calibration grid and light probe (c) Objects and local scene matched to background (d) Local 
scene, without objects, lit by the model (e) Object matte (f) Difference in local scene between c and 
d (g) Final result with differential rendering Figure 8: Compositing synthetic objects into a real scene 
using a light probe and differential rendering   the stochastic nature of the particular global illumination 
algorithm used. The differential rendering technique successfully eliminates the border between the local 
scene and the background image seen in 8(c). Note that the albedo texture of the table in the local scene 
area is preserved, and that a specular re.ection of a background object on the table (appearing just 
to the left of the .oating sphere) is cor­rectly preserved in the .nal rendering. The local scene also 
exhibits re.ections from the synthetic objects. A caustic from the glass ball focusing the light of the 
ceiling lamp onto the table is evident.  9 Future work The method proposed here suggests a number of 
areas for future work. One area is to investigate methods of automatically recov­ering more general re.ectance 
models for the local scene geome­try, as proposed in Section 7. With such information available, the 
program might also also be able to suggest which areas of the scene should be considered as part of the 
local scene and which can safely be considered distant, given the position and re.ectance character­istics 
of the desired synthetic objects. Some additional work could be done to allow the global illumina­tion 
algorithm to compute the ilumination solution more ef.ciently. One technique would be to have an algorithm 
automatically locate and identify concentrated light sources in the light-based model of the scene. With 
such knowledge, the algorithm could compute most of the direct illumination in a forward manner, which 
could dra­matically increase the ef.ciency with which an accurate solution could be calculated. To the 
same end, use of the method presented in [15] to expedite the solution could be investigated. For the 
case of compositing moving objects into scenes, greatly increased ef.­ciency could be obtained by adapting 
incremental radiosity methods to the current framework. 10 Conclusion We have presented a general framework 
for adding new objects to light-based models with correct illumination. The method lever­ages a technique 
of using high dynamic range images of real scene radiance to synthetically illuminate new objects with 
arbitrary re­.ectance characteristics. We leverage this technique in a general method to simulate interplay 
of light between synthetic objects and the light-based environment, including shadows, re.ections, and 
caustics. The method can be implemented with standard global il­lumination techniques. For the particular 
case of rendering synthetic objects into real scenes (rather than general light-based models), we have 
presented a practical instance of the method that uses a light probe to record inci­dent illumination 
in the vicinity of the synthetic objects. In addition, we have described a differential rendering technique 
that can con­vincingly render the interplay of light between objects and the local scene when only approximate 
re.ectance information for the local scene is available. Lastly, we presented an iterative approach for 
determining re.ectance characteristics of the local scene based on measured geometry and observed radiance 
in uncontrolled lighting conditions. It is our hope that the techniques presented here will be useful 
in practice as well as comprise a useful framework for com­bining material-based and light-based graphics. 
 Acknowledgments The author wishes to thank Chris Bregler, David Forsyth, Jianbo Shi, Charles Ying, Steve 
Chenney, and Andrean Kalemis for the various forms of help and advice they provided. Special gratitude 
is also due to Jitendra Malik for helping make this work possible. Discussions with Michael Naimark and 
Steve Saunders helped motivate this work. Tim Hawkins provided extensive assistance on improving and 
revising this pa­per and provided invaluable assistance with image acquisition. Gregory Ward Larson deserves 
great thanks for the RADIANCE lighting simulation system and his invalu­able assistance and advice in 
using RADIANCE in this research, for assisting with re­.ectance measurements, and for very helpful comments 
and suggestions on the paper. This research was supported by a Multidisciplinary University Research 
Initiative on three dimensional direct visualization from ONR and BMDO, grant FDN00014-96-1­1200. References 
[1] ADELSON, E. H., AND BERGEN, J. R. Computational Models of Visual Pro­cessing. MIT Press, Cambridge, 
Mass., 1991, ch. 1. The Plenoptic Function and the Elements of Early Vision. [2] AZARMI, M. Optical Effects 
Cinematography: Its Development, Methods, and Techniques. University Micro.lms International, Ann Arbor, 
Michigan, 1973. [3] BLINN, J. F. Texture and re.ection in computer generated images. Communica­tions 
of the ACM 19, 10 (October 1976), 542 547. [4] CHEN, E. QuickTime VR -an image-based approach to virtual 
environment nav­igation. In SIGGRAPH 95 (1995). [5] CHEN, S. E. Incremental radiosity: An extension of 
progressive radiosity to an interactive synthesis system. In SIGGRAPH 90 (1990), pp. 135 144. [6] COHEN,M.F.,CHEN,S.E.,WALLACE,J.R., 
AND GREENBERG,D.P.Apro­gressive re.nement approach to fast radiosity image generation. In SIGGRAPH 88 
(1988), pp. 75 84. [7] CURLESS, B., AND LEVOY, M. A volumetric method for building complex mod­els from 
range images. In SIGGRAPH 96 (1996), pp. 303 312. [8] DANA, K. J., GINNEKEN, B., NAYAR, S. K., AND KOENDERINK, 
J. J. Re­.ectance and texture of real-world surfaces. In Proc. IEEE Conf. on Comp. Vision and Patt. Recog. 
(1997), pp. 151 157. [9] DEBEVEC, P. E., AND MALIK, J. Recoveringhighdynamicrangeradiancemaps from photographs. 
In SIGGRAPH 97 (August 1997), pp. 369 378. [10] DEBEVEC, P. E., TAYLOR, C. J., AND MALIK, J. Modeling 
and rendering ar­chitecture from photographs: A hybrid geometry-and image-based approach. In SIGGRAPH 
96 (August 1996), pp. 11 20. [11] DEBEVEC, P. E., YU, Y., AND BORSHUKOV, G. D. Ef.cient view-dependent 
image-based rendering with projective texture-mapping. Tech. Rep. UCB//CSD­98-1003, University of California 
at Berkeley, 1998. [12] DRETTAKIS, G., ROBERT, L., AND BOUGNOUX, S. Interactive common illu­mination 
for computer augmented reality. In 8th Eurographics workshop on Ren­dering, St. Etienne, France (May 
1997), J. Dorsey and P. Slusallek, Eds., pp. 45 57. [13] FIELDING, R. The Technique of Special Effects 
Cinematography. Hastings House, New York, 1968. [14] FOURNIER, A., GUNAWAN, A., AND ROMANZIN, C. Common 
illumination between real and computer generated scenes. In Graphics Interface (May 1993), pp. 254 262. 
[15] GERSHBEIN, R., SCHRODER, P., AND HANRAHAN, P. Textures and radiosity: Controlling emission and re.ection 
with texture maps. In SIGGRAPH 94 (1994). [16] GORAL, C. M., TORRANCE, K. E., GREENBERG, D. P., AND BATTAILE, 
B. Modeling the interaction of light between diffuse surfaces. In SIGGRAPH 84 (1984), pp. 213 222. [17] 
GORTLER, S. J., GRZESZCZUK, R., SZELISKI, R., AND COHEN, M. F. The Lumigraph. In SIGGRAPH 96 (1996), 
pp. 43 54. [18] HECKBERT, P. S. Survey of texture mapping. IEEE Computer Graphics and Applications 6, 
11 (November 1986), 56 67. [19] KAJIYA, J. The rendering equation. In SIGGRAPH 86 (1986), pp. 143 150. 
[20] KARNER, K. F., MAYER, H., AND GERVAUTZ, M. An image based measure­ment system for anisotropic re.ection. 
In EUROGRAPHICS Annual Conference Proceedings (1996). [21] KOENDERINK,J.J., ANDVAN DOORN,A.J.Illuminancetextureduetosurface 
mesostructure. J. Opt. Soc. Am. 13, 3 (1996). [22] LAVEAU, S., AND FAUGERAS, O. 3-D scene representation 
as a collection of images. In Proceedings of 12th International Conference on Pattern Recognition (1994), 
vol. 1, pp. 689 691. [23] LEVOY, M., AND HANRAHAN, P. Light .eld rendering. In SIGGRAPH 96 (1996), pp. 
31 42. [24] MCMILLAN, L., AND BISHOP, G. Plenoptic Modeling: An image-based ren­dering system. In SIGGRAPH 
95 (1995). [25] NAKAMAE, E., HARADA, K., AND ISHIZAKI, T. A montage method: The over­laying of the computer 
generated images onto a background photograph. In SIG-GRAPH 86 (1986), pp. 207 214. [26] PORTER,T., AND 
DUFF,T.Compositingdigitalimages.In SIGGRAPH 84 (July 1984), pp. 253 259. [27] SATO, Y., WHEELER, M. D., 
AND IKEUCHI, K. Object shape and re.ectance modeling from observation. In SIGGRAPH 97 (1997), pp. 379 
387. [28] SMITH, T. G. Industrial Light and Magic: The Art of Special Effects. Ballantine Books, New 
York, 1986. [29] SZELISKI, R. Image mosaicing for tele-reality applications. In IEEE Computer Graphics 
and Applications (1996). [30] TURK, G., AND LEVOY, M. Zippered polygon meshes from range images. In SIGGRAPH 
94 (1994), pp. 311 318. [31] VEACH, E., AND GUIBAS, L. J. Metropolis light transport. In SIGGRAPH 97 
(August 1997), pp. 65 76. [32] WARD, G. J. Measuring and modeling anisotropic re.ection. In SIGGRAPH 
92 (July 1992), pp. 265 272. [33] WARD, G. J. The radiance lighting simulation and rendering system. 
In SIG-GRAPH 94 (July 1994), pp. 459 472. [34] WATANABE, M., AND NAYAR, S. K. Telecentric optics for 
computational vi­sion. In Proceedings of Image Understanding Workshop (IUW 96) (February 1996). [35] 
Y.CHEN, AND MEDIONI, G. Object modeling from multiple range images. Im­age and Vision Computing 10, 3 
(April 1992), 145 155.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280871</article_id>
		<sort_key>199</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>20</seq_no>
		<title><![CDATA[Multiple-center-of-projection images]]></title>
		<page_from>199</page_from>
		<page_to>206</page_to>
		<doi_number>10.1145/280814.280871</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280871</url>
		<keywords>
			<kw><![CDATA[image-based rendering]]></kw>
			<kw><![CDATA[multiple-center-of-projection images]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Digitizing and scanning</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Viewing algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.10</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010241</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Image representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010506</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Document scanning</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31033807</person_id>
				<author_profile_id><![CDATA[81100246554]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rademacher]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of North Carolina at Chapel Hill, Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14081142</person_id>
				<author_profile_id><![CDATA[81100206034]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gary]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bishop]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of North Carolina at Chapel Hill, Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[E.H. Adelson and J. R. Bergen. The Plenoptic Function And The Elements Of Early Vision. In Computational Models of Visual Processing, pp. 3-20, Edited by Michael Landy and J. Anthony Movshon, The MIT Press, Cambridge, 1991.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Robert C. Bolles, H. Harlyn Baker, and David H. Marimont. Epipolar-Plane Image Analysis: An Approach To Determining Structure From Motion. In International Journal of Computer Vision, volume 1, page 7-55. Boston, 1987.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166153</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Shenchang Eric Chen and Lance Williams. View Interpolation For Image Synthesis. In Proceedings of SIGGRAPH 93, pp. 279-288, New York, 1993. ACM.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218395</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Shenchang Eric Chen. Quicktime VR: An Image-Based Approach To Virtual Environment Navigation. In Proceedings of SIGGRAPH 95, pp. 29-38, New York, 1995. ACM.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237269</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Brian Curless and Marc Levoy. A Volumetric Method For Building Complex Models From Range Images. In Proceedings of SIGGRAPH 96, pp. 303-312. NewYork, 1996. ACM.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>889151</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[William J. Dally, Leonard McMillan, Gary Bishop, and Henry Fuchs. The Delta Tree: An Object-Centered Approach To Image-Based Rendering. MIT AI Lab Technical Memo 1604, May 1996.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>253298</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Lucia Darsa, Bruno Costa Silva, and Amitabh Varshney. Navigating Static Environments Using Image-Space Simplification And Morphing. In Proceedings of 1997 Symposium on Interactive 3D Graphics, pp. 25-34. April 1997.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>171658</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Olivier Faugeras. Three-Dimensional Computer Vision: A Geometric Approach. MIT Press, Cambridge, Massachusetts, 1993.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Sanjib K. Ghosh. Analytical Photogrammetry, second edition. Pergamon, 1988.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237200</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Steven J. Gortler, Radek Grzeszczuk, Richard Szeliski, and Michael F. Cohen. The Lumigraph. In Proceedings of SIGGRAPH 96, pp. 43-54, New York, 1996. ACM.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Eduard Gr611er and Helwig L6ffelmann. Extended Camera Specifications for Image Synthesis. In Machine Graphics and Vision, 3 (3), pp. 513-530. 1994.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280884</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Michael Halle. Multiple Viewpoint Rendering. To appear in Proceedings of SIGGRAPH 98. New York, 1998. ACM]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>189597</ref_obj_id>
				<ref_obj_pid>189359</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Richard Hartley and Rajiv Gupta. Linear Pushbroom Cameras. In Proceedings of Third European Conference on Computer Vision, pp. 555-566. New York, 1994.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134011</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Hugues Hoppe, Tony DeRose, Tom Duchamp, John McDonald, and Werner Stuetzle. Surface Reconstruction From Unorganized Points. In Computer Graphics (SIGGRAPH 92 Conference Proceedings), volume 26, pp. 71-78. New York, July 1992. ACM.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Stephane Lavaeu and Olivier Faugeras. 3-D Scene Representation As A Collection Of Images. INRIA Technical Report RR-2205. February 1994, INRIA.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237199</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Marc Levoy and Pat Hanrahan. Light Field Rendering. In Proceedings of SIGGRAPH 96, pp. 31-42, New York, 1996. ACM.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>253292</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[William R. Mark, Leonard McMillan and Gary Bishop. Post-Rendering 3D Warping. In Proceedings of the 1997 Symposium on Interactive 3D Graphics, page 7-16, Providence, Rhode Island, April 1997.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>897834</ref_obj_id>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Leonard McMillan and Gary Bishop. Shape As A Perturbation To Projective Mapping, UNC Computer Science Technical Report TR95-046, University of North Carolina, April 1995.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218398</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Leonard McMillan and Gary Bishop. Plenoptic Modeling: An Image-Based Rendering System. In Proceedings of SIGGRAPH 95, pp. 39-46, New York, 1995. ACM.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>794368</ref_obj_id>
				<ref_obj_pid>794189</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Shmuel Peleg and Joshua Herman. Panoramic Mosaics by Manifold Projection. In Proceedings of Computer Vision and Pattern Recognition, pp. 338-343, Washington, June 1997. IEEE.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>732115</ref_obj_id>
				<ref_obj_pid>647651</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Karl Pulli, Michael Cohen, Tom Duchamp, Hugues Hoppe, Linda Shapiro, Werner Stuetzle. View-Based Rendering: Visualizing Real Objects from Scanned Range and Color Data. In Proceedings of Eighth Eurographics Workshop on Rendering, pp. 23-34. Eurographics, June 1997.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192192</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Matthew Regan and Ronald Pose. Priority Rendering with a Virtual Reality Address Recalculation Pipeline. In Proceedings of SIGGRAPH 94, pp. 155- 162, New York, 1994. ACM.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280882</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Jonathan Shade, Steven Gortler, Li-wei He, and Richard Szeliski. Layered Depth Images. To appear in Proceedings of SIGGRAPH 98. New York, 1998. ACM]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Francois X. Sillion, George Drettakis and Benoit Bodelet. Efficient Impostor Manipulation For Real-Time Visualization Of Urban Scenery. In Proceedings of Eurographics 97, pp. 207-218. Budapest, Hungary, September 1997.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>267057</ref_obj_id>
				<ref_obj_pid>266989</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Edward Swan, Klaus Mueller, Torsten Moiler, Naeem Shareef, Roger Crawfis, and Roni Yagel. An Anti-Aliasing Technique For Splatting. In Proceedings oflEEE Visualization 97, pp. 197-204, 1997]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237274</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Jay Torborg, James T. Kajiya. Talisman: Commodity Real-Time Graphics For The PC. In Proceedings of SIGGRAPH 96, pp. 353-363. New York, August 1996. ACM.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192241</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Greg Turk and Marc Levoy. Zippered Polygon Meshes From Range Images. In Proceedings of SIGGRAPH 94, pp. 311-318. New York, July 1994. ACM.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258876</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Gregory Welch and Gary Bishop. SCAAT: Incremental Tracking With Incomplete Information. In Proceedings of SIGGRAPH 97, pp. 333-344. Los Angeles, August 1997. ACM.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97919</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Lee Westover. Footprint Evaluation For Volume Rendering. In Computer Graphics (SIGGRAPH 90 Conference Proceedings), volume 24, pp. 367-376. NewYork, August 1990. ACM.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258859</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Daniel N. Wood, Adam Finkelstein, John F. Hughes, Craig E. Thayer, and David H. Salesin. Multiperspective Panoramas For Cel Animation. In Proceedings of SIGGRAPH 97, pp. 243-250. New York, 1997. ACM.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>146852</ref_obj_id>
				<ref_obj_pid>146844</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Jiang Yu Zheng and Saburo Tsuji. Panoramic Representation for Route Recognition by a Mobile Robot. In International Journal of Computer Vision 9 (1): 55-76. Netherlands, 1992. Kluwer.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Multiple-Center-of-Projection Images Paul Rademacher Gary Bishop University of North Carolina at Chapel 
Hill Supplemental materials for this paper are available in the papers/rademach directory. ABSTRACT 
In image-based rendering, images acquired from a scene are used to represent the scene itself. A number 
of reference images are required to fully represent even the simplest scene. This leads to a number of 
problems during image acquisition and subsequent reconstruction. We present the multiple-center-of-projection 
image, a single image acquired from multiple locations, which solves many of the problems of working 
with multiple range images. This work develops and discusses multiple-center-of­projection images, and 
explains their advantages over conventional range images for image-based rendering. The contributions 
include greater flexibility during image acquisition and improved image reconstruction due to greater 
connectivity information. We discuss the acquisition and rendering of multiple-center-of-projection datasets, 
and the associated sampling issues. We also discuss the unique epipolar and correspondence properties 
of this class of image. CR Categories: I.3.3 [Computer Graphics]: Picture/Image Generation Digitizing 
and scanning, Viewing algorithms; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism; 
I.4.10 [Image Processing]: Scene Analysis Keywords: image-based rendering, multiple-center-of-projection 
images  INTRODUCTION In recent years, image-based rendering (IBR) has emerged as a powerful alternative 
to geometry-based representations of 3-D scenes. Instead of geometric primitives, the dataset in IBR 
is a collection of samples along viewing rays from discrete locations. Image-based methods have several 
advantages. They provide an alternative to laborious, error-prone geometric modeling. They can produce 
very realistic images when acquired from the real world, and can improve image quality when combined 
with geometry (e.g., texture mapping). Furthermore, the rendering time for an image-based dataset is 
dependent on the image sampling density, rather than the underlying spatial complexity of the scene. 
This can yield significant rendering speedups by replacing or augmenting traditional geometric methods 
[7][23][26][4]. The number and quality of viewing samples limits the quality of images reconstructed 
from an image-based dataset. CB #3175 Sitterson Hall, Chapel Hill, NC, 27599-3175 rademach@cs.unc.edu, 
bishop@cs.unc.edu http://www.cs.unc.edu/~ibr Clearly, if we sample from every possible viewing position 
and along every possible viewing direction (thus sampling the entire plenoptic function [19][1]), then 
any view of the scene can be reconstructed perfectly. In practice, however, it is impossible to store 
or even acquire the complete plenoptic function, and so one must sample from a finite number of discrete 
viewing locations, thereby building a set of reference images. To synthesize an image from a new viewpoint, 
one must use data from multiple reference images. However, combining information from different images 
poses a number of difficulties that may decrease both image quality and representation efficiency. The 
multiple­center-of-projection (MCOP) image approaches these problems by combining samples from multiple 
viewpoints into a single image, which becomes the complete dataset. Figure 1 is an example MCOP image. 
 Figure 1 Example MCOP image of an elephant The formal definition of multiple-center-of-projection images 
encompasses a wide range of camera configurations. This paper mainly focuses on one particular instance, 
based on the photographic strip camera [9]. This is a camera with a vertical slit directly in front of 
a moving strip of film (shown in Figure 2 without the lens system). As the film slides past the slit 
a continuous image-slice of the scene is acquired. If the camera is moved through space while the film 
rolls by, then different columns along the film are acquired from different vantage points. This allows 
the single image to capture continuous information from multiple viewpoints. The strip camera has been 
used extensively, e.g., in aerial photography. In this work s notion of a digital strip camera, each 
pixel-wide column of the image is acquired from a different center-of-projection. This single image becomes 
the complete dataset for IBR. Features of multiple-center-of-projection images include: greater connectivity 
information compared with collections of standard range images, resulting in improved rendering quality, 
 greater flexibility in the acquisition of image-based datasets, for example by sampling different portions 
of the scene at different resolutions, and  a unique internal epipolar geometry which characterizes 
optical flow within a single image.   strip camera scene Figure 2 A strip camera consists of a moving 
strip of film behind a vertical slit. Furthermore, MCOP images retain the desirable properties of conventional 
range images, such as fast incremental projection and moderate storage requirements. In this paper we 
formally develop the concept of multiple­center-of-projection images, and discuss their acquisition and 
reprojection for image-based rendering. We describe different data structures to maintain these images, 
and discuss the implications of sampling during their acquisition. We also show how to perform point 
correspondence using a single MCOP image. We conclude by presenting several examples of MCOP images, 
and demonstrate their advantage over conventional range images.  PREVIOUS WORK Early work in image-based 
rendering includes the rangeless panoramas of Chen and Williams [3][4] and Regan and Pose[22], which 
allow view reconstruction from a set of fixed eye locations. Plenoptic modeling [19] adds range to panoramic 
images, thereby allowing reprojection from arbitrary viewpoints. The concept of the plenoptic function 
is further explored by light slab methods [10][16], which attempt to fully sample the function within 
a subset of space. Several methods exist for handling IBR range images from multiple viewpoints. Layered 
depth images [23] store multiple hits of a viewing ray in different layers of a single image, allowing, 
e.g., the front and back of a surface to be kept in a single data structure. The delta tree [6] acquires 
a hierarchical set of reference images on a sampling sphere around a target object, discarding redundant 
information when possible. The work most closely related to this paper is the multiperspective panorama 
for cel animation [30]. This method constructs an image from multiple viewpoints for use as a backdrop 
in traditional cel animation. A continuous set of views along a pre-specified path can be extracted from 
this single backdrop. The construction of multiperspective panoramas is similar to the use of manifold 
projections in computer vision [20]. Another related work is the extended camera for ray tracing [11]. 
This method allows a ray-tracing camera to undergo arbitrary transformations as it traverses each pixel 
of the output image, thereby achieving a number of artistic effects. Imaging devices similar to strip 
cameras have recently been explored in computer vision by Zheng and Tsuji [31] and by Hartley [13]. The 
former discusses their use in robot navigation, while the latter discusses the pushbroom camera, used 
in satellite imagery. One-dimensional cameras are also the basis of Cyberware scanners, which sweep a 
linear or circular path around an object, and systems by 3D Scanners, Ltd., which attach a 1-D scanning 
head to a Faro Technologies arm. 3 MULTIPLE-CENTER-OF-PROJECTION IMAGES 3.1 Definition A multiple-center-of-projection 
image is an extension of a conventional image, characterized by having a set of cameras contributing 
to it, rather than only a single camera. Individual pixels or sets of pixels are acquired by different 
cameras, subject to certain constraints. A multiple-center-of-projection image consists of a two dimensional 
image and a parameterized set of cameras, meeting the following conditions: 1) the cameras must lie on 
either a continuous curve or a continuous surface 2) each pixel is acquired by a single camera 3) viewing 
rays vary continuously across neighboring pixels 4) two neighboring pixels must either correspond to 
the same camera or to neighboring cameras. This definition states that the camera locations are not an 
unorganized set of points, but rather define a continuous curve or surface (condition 1). Condition 2 
states each pixel is from a single camera, rather than a blend of samples from multiple cameras. Condition 
3 imposes smoothness on the viewing rays, thereby ensuring they do not vary discontinuously. The last 
condition imposes an organization on the mapping of camera samples to the resulting image; it ensures 
we move smoothly from camera to camera as we traverse from pixel to pixel in an image. scene cameras 
camera curve ... ... ci-1 ci ci+1 multiple-center-of-projection image Figure 3 A multiple-center-of-projection 
image acquired by capturing a discrete number of image-slices along a curve. This single image (bottom) 
sees three sides of the house simultaneously. A similar configuration was used to create Figure 1. Note 
that while the definition contains several parts, a continuous strip camera along any continuous path 
 automatically satisfies the four criteria (section 4.4 discusses the sampling implications for discrete 
images). The remainder of this paper thus deals exclusively with the strip camera instance, unless otherwise 
noted. 3.2 Distinctions From Other Methods Before delving into the details of MCOP images, we should 
clarify what these images are not. For example, what is the difference between an MCOP image and an arbitrary 
collection of conventional images? As will be shown in sections 4 and 5, the four constraints on an MCOP 
image yield advantages not found with conventional range images. These include improved image reconstruction 
quality, greater flexibility during image acquisition, and unique epipolar and correspondence properties. 
MCOP images are not a data structure for maintaining collections of images. What is the difference between 
an MCOP image and the multiperspective panorama [30]? Multiperspective panoramas may be considered subsets 
of MCOP images, since they meet the definition of section 3.1. However, a multiperspective panorama is 
not intended as a dataset for arbitrary view construction; it does not permit 3D reprojection, and thus 
can only provide views along a single predetermined path. Also, a primary goal in multiperspective panoramas 
is to minimize the local distortion in the bitmap otherwise, output images will suffer from perspective 
errors. This will not occur with MCOP images. What is the difference between an MCOP dataset and a polygonal 
mesh? MCOP images retain the same desirable characteristics that separate all image-based range datasets 
from polygonal datasets. Because of strong spatial coherence across neighboring pixels, the projection 
of image points can be computed incrementally using McMillan and Bishop's 3D warping equations [19]. 
Also, since the image dataset is a regular grid, the 2D pixel coordinates are implicit for all points 
 each pixel only needs to contain intensity and range, in contrast with intensity, x, y, and z. Finally, 
an MCOP image has projective and epipolar properties not found in polygonal meshes. We must also distinguish 
between MCOP images and images with arbitrary-manifold projection surfaces. That is, we can construct 
a single-COP image in which the projection surface is not a plane, cylinder, or sphere, but rather an 
arbitrary manifold surface. Each pixel in this image is then given by an intersection of a ray from the 
COP through the surface. While MCOP images do have curved image surfaces, the two are not equivalent, 
since the arbitrary-manifold image can still only capture scene points visible to the single center of 
projection, whereas MCOP images can view from more than one location.  4 ACQUIRING MCOP IMAGES Multiple-center-of-projection 
images are well-suited to applications where a useful path can be defined through or around a scene; 
by not tying every pixel in a reference image to the same viewpoint, they allow greater flexibility during 
acquisition than conventional images. For example, sampling a nearly-convex object (such as in Figure 
4) results in several poorly-sampled areas, as the cameras viewing rays approach grazing angle with the 
object. The MCOP image on the right, however, samples every point at a near-normal angle, thus acquiring 
good samples everywhere. This occurs for both quantities being measured color and range. Other relevant 
sampling issues are discussed in section 4.4.  c2 c3 poorlysampled 4.1 Data Structures for MCOP Images 
Although each pixel in an MCOP image may conceptually belong to a different camera, in practice we minimize 
storage requirements by describing the cameras parametrically in a variety of ways. At the highest level, 
for example, we can divide the camera curve into equally-spaced segments, then use the column index of 
each pixel as an implicit curve coordinate to compute the camera location. At the next level, each column 
of the image may explicitly store the parametric coordinate of the camera, thus allowing irregular camera 
spacing along the curve. Each column may instead explicitly contain the complete camera model, thus requiring 
n camera descriptions for an n·m-sized image. Or, at the extreme level, each pixel can explicitly store 
a camera model. Clearly, the most compact method should be selected given a particular application. 
4.2 Synthesizing From a 3D Model To synthesize an MCOP image from a 3D model, we first define a path 
through the 3D scene. This path need not be closed, nor do the viewing rays need to be normal to the 
curve. We then smoothly animate a camera along this curve, extracting a single­pixel-wide color image 
and depth map at each frame of the animation. As each slice is captured, we concatenate the color image 
and range map into a rectangular buffer, and store the camera information for that column (four vectors, 
described in 5.1) in an array. Since there is much coherence from one camera position to the next along 
the path, a method such as multiple viewpoint rendering [12] may be used to accelerate the rendering. 
Figures 5 through 8 show a rendered 1000·500 MCOP image of a castle model. Details of the rendering process 
are given in section 8. Note that the single image captures the complete exterior of the model, which 
is then rendered as a single mesh. This demonstrates the increased acquisition flexibility and improved 
connectivity properties of the MCOP technique.  4.3 Acquiring From Real-World Scenes We can acquire 
an MCOP image of the real world by constructing the digital equivalent of a strip camera. For example, 
we can use a 1-D CCD camera, translated along a path. One­dimensional image-strips are captured at discrete 
points on the path and concatenated into the image buffer. The CCD camera must be accurately tracked 
to prevent errors during reprojection, using for example the techniques in [28]. This method has the 
disadvantage of introducing a temporal element into the image, since every 1-D strip is captured at a 
different time. This may lead to mismatched data unless the scene is static (static scenes are a common 
assumption in IBR [4][19][16][10]). Active range-finding techniques, such as laser range­finders, can 
be applied to MCOP images almost exactly as with regular images: simply register the laser scanner with 
the color camera. A Cyberware scanner, for example, is a 1-D laser range­finder registered with a 1-D 
linear camera. Section 6 discusses how the epipolar constraint - critical to passive range-finding methods 
- can be extended to MCOP images. Figure 4 The three regular cameras on the left have difficulty sampling 
the object well. The MCOP image on the right can easily sample well the complete object.  Figure 5 Castle 
model. The red curve is the Figure 6 The resulting 1000·500 MCOP image. The first fourth of the image, 
on the leftpath the camera was swept on, and the arrows side, is from the camera sweeping over the roof. 
Note how the courtyard was sampledindicate the direction of motion. The blue more finely, for added 
resolution. triangles are the thin frusta of each camera. Every 64th camera is shown.  Figure 8 Three 
views of the castle, reconstructed solely from the single MCOP image above. This dataset captures the 
complete exterior of the castle. 4.4 Sampling Issues The construction of an MCOP image is inherently 
a sampling process; there are two primary questions that must be asked. First, how much of the plenoptic 
function does an MCOP image capture? The light field and Lumigraph methods approach this by attempting 
to fully sample the function, but only over a subset of space. The MCOP method is better suited to the 
opposite approach - sampling the plenoptic function only partially (specifically, not capturing view-dependent 
lighting), but over large regions. Furthermore, MCOP range images are not bound by the free space assumption 
of light slab methods, since the range at each pixel is used to resolve visibility. Thus, MCOP images 
can contain both foreground and background objects (by sweeping the camera over both areas), and the 
reprojection viewpoint is not limited to lying between the camera path and the projection surface. Second, 
how well does an MCOP image sample the plenoptic function? Since the functions being sampled (color and 
range) are not bandlimited, aliasing is inevitable. To minimize it, we must prefilter the signals (that 
is, perform area sampling rather than point sampling) as we must with light field rendering [16]. In 
that work the camera spacing is constant, and thus also the filtering kernel. However, in MCOP images 
the camera spacing and orientation may vary across the image. Therefore, a larger filtering kernel is 
required in regions of greater camera translation or rotation. However, as the filtering kernel is increased, 
the resolution of each sample is effectively reduced, since a greater portion of the scene is blurred 
into each sample. To avoid excessively-large kernels (that is, excessive blurring), the sampling rate 
should be increased in regions of fast camera motion.  5 REPROJECTING MCOP IMAGES This section describes 
how to render a new viewpoint using an MCOP reference image. This consists of two steps: computing each 
pixel s reprojected location in world-space, and rendering the reprojected points using an appropriate 
reconstruction method. Alternatively, we may skip the reprojection into world-space, instead projecting 
from the 2D reference image directly into the 2D output image, as described in [18]. 5.1 Reprojection 
Formula Since an MCOP image conceptually contains a full camera description plus range for each pixel, 
the reprojection step is straightforward (in the strip camera case, we need only one camera model per 
column). Our implementation stores the camera information for each column i as four vectors: a center 
of projection Ci, a vector Oi from Ci to the image plane origin, a Ui vector defining the horizontal 
axis of the projection plane, and a Vi vector defining the vertical axis. Each pixel (i,j) contains disparity 
rather then depth, defined here as the distance from Ci to the image plane at a given pixel, divided 
by the distance from Ci to the pixel s corresponding world-space point. Thus disparity is inversely proportional 
to range. Given this camera model and the disparity di,j for a pixel (i, j), the 3-space reprojection 
(x, y, z) is: ]]]  1 [ L VO i i ø x i C i 1 i xx x x ß VO i i yy y j + [[L []]l ]]]l ] = y i C i d 
i,j y U U U If the dataset is rendered in column-major order, we can single COP [[ L [[[ Ø º VO i 
z i C i l i zz z z multiple COPs reproject the pixels incrementally, since Ci, Oi, Ui, and Vi are constant 
for each column. Also note the (i, j) coordinates of a Figure 9 In the single-COP case the surface s 
is nearly parallel to pixel are implicit (since the image is a regular grid) and do not the rays of camera 
c, and so is not rendered. In the MCOP case, s have to be explicitly maintained. is not parallel to 
the ray from the second camera, and therefore is rendered. 5.2 Image Reconstruction After calculating 
the reprojected coordinates of each pixel in the reference image, there are two common methods for reconstructing 
a conventional range image from a new viewpoint: splatting and meshing. Splatting consists of directly 
rendering each point using a variable-size reconstruction kernel (e.g., a Gaussian blob), with size dependent 
on distance from the eye to the point [25][29]. Meshing consists of connecting adjacent pixels with triangles, 
quadrilaterals, or some higher-order surface. Visibility can be determined for both methods by z-buffering. 
Splatting with MCOP images is exactly as with conventional images, since each point is rendered independently: 
we compute each pixel s 3-space location, then render that point with an appropriate reconstruction kernel. 
Meshing can also be employed as with conventional images; although neighboring pixels may come from different 
cameras, the constraints of the MCOP definition ensure that neighboring pixels in the image represent 
neighboring points in space. For proper meshing of conventional or MCOP images, discontinuities in the 
range image must first be detected. For example, three adjacent pixels with one belonging to a foreground 
object and the others belonging to the background should not be connected in the rendered image. Methods 
for detecting these silhouette edges are discussed in [17][7][27]. Such algorithms can be easily extended 
to the MCOP domain. For example, our silhouette-detection implementation for a single-COP image will 
not connect adjacent pixels if the surface they define is sampled by a nearly parallel ray (see Figure 
9). It assumes these points probably span empty space, rather than a real surface. This method is directly 
extended to MCOP images by testing each triangle (from three adjacent pixels) against the rays of the 
two or three cameras that contribute to it: if the triangle is nearly parallel to them all, it is not 
rendered; otherwise, it is rendered. This comparison is only performed once, as a preprocess. MCOP images 
have an advantage over conventional images when rendering with meshes. In the conventional case a large 
number of separate images may be required to fully sample an object. One can only easily connect adjacent 
pixels within each image, but points from different images cannot be connected with a mesh unless a zippering 
preprocess [5] is performed. The amount of zippering required increases with the number of images, since 
each image's boundaries will tend to produce additional seams. With the MCOP method, this problem is 
minimized, since there is only one image in the dataset. By reducing the number of boundaries to consider, 
MCOP images can greatly reduce the amount of zippering necessary to fully connect a scene. 5.3 Multiple 
Sampling of Scene Points An MCOP image may contain multiple samplings of the same scene point. Since 
these samples map to the same area in the new, reconstructed image, simple meshing or splatting methods 
leave only the last-rendered of the frontmost samples in the image. This, however, may not be the best 
sample. A better method is to blend successive samples as they are written to the image buffer, as described 
in [7] and [21].  6 EPIPOLAR GEOMETRY A fundamental relationship between a pair of conventional images 
is the epipolar geometry they define: a pixel with unknown range in one image will map to a line in the 
other (planar) image [8][2]. This property characterizes the optical flow between the two images, and 
aids in solving the correspondence problem - if one image's pixel views a feature x, then to find that 
feature in the other image, we need only search the corresponding line that the pixel maps to. This property 
has led to the development of occlusion-compatible rendering order for range images and layered depth 
images [19][23], and greatly simplifies passive range-finding methods based on correspondence.  We can 
similarly define an epipolar geometry between an MCOP image and a conventional image, or between two 
MCOP images, by merely projecting the rays of one image's pixels into the other image. However, these 
will usually map to curves in an MCOP image, rather than to lines. We can also define an internal epipolar 
geometry using only a single MCOP image. Pick some camera ci of the MCOP image, and project its rays 
into every other camera of the image (Figure 10). The result will be a family of curves, which characterizes 
the optical flow between ci and the other cameras. If some feature x is seen multiple times in the image 
(e.g., the elephant's eye in the image, which is seen twice), then all sightings must lie along an epipolar 
curve. Thus the epipolar constraint holds for MCOP images, although it remains as future work whether 
the internal epipolar geometry will lead to useful correspondence algorithms.  7 CAMERAS ALONG A SURFACE 
This paper has dealt exclusively with MCOP images constructed by placing cameras along a curve. However, 
the definition of multiple-center-of-projection images also allows the cameras to define a surface. This 
approach allows more independence among viewing locations and rays by providing an additional dimension 
of parameterization for the cameras. We can, for example, parameterize a 3D surface by s and t, then 
define a camera for m and n discrete points along s and t, respectively. The viewing rays need not be 
normal to the surface, but must vary smoothly across neighboring pixels. A useful case where the cameras 
define a surface can be constructed as follows: consider a static rig in the shape of an arc, lined with 
a curved one-dimensional CCD array on the inside (Figure 12). This rig is then rotated around a target 
object (the CCDs need not point directly at the center of rotation). Since each CCD is considered a camera, 
this case constructs an MCOP image where the camera forms a surface, rather than a curve. Note that although 
the camera locations and orientations now differ for every pixel in the resulting image, they can be 
efficiently described by a parametric surface, and thus do not significantly increase the dataset size. 
 This configuration is very similar to Cyberware scanners, except their rig is not curved, so all of 
their viewing rays are parallel. A limitation of non-curved scanners is that they cannot appropriately 
sample the tops or bottom of many commonly scanned objects, such as the human head. The curved camera 
surface, however, will adequately handle this.  8 RESULTS Figures 5 through 8 and 13 through 17 show 
the multiple­center-of-projection method applied to two synthetic scenes. Each MCOP image was rendered 
in 3D Studio MAX as described in section 4.2: we defined one curve around the model for the camera location, 
another curve for the camera's lookat point, and then rendered a single-pixel-wide image at each step 
along these curves. We used a custom plug-in to extract the Z-buffer and camera information (C, O, U, 
and V vectors) for each pixel-wide image. The color, range, and camera information were then concatenated 
into a rectangular color image, a rectangular disparity map, and an array of camera descriptors. The 
castle model demonstrates the MCOP method's improved connectivity by rendering the entire image with 
a single mesh. The Titanic model demonstrates the method's flexible image acquisition, by sampling the 
flag at a higher resolution than the rest of the ship. Figure 11 shows the results of an early experiment 
to acquire real-world data. A UNC HiBall optoelectronic tracker was rigidly attached to an Acuity Research 
AccuRange4000 laser range finder, which reflected from a rotating mirror. This rig was moved along the 
path shown in Figure 11b, sweeping out columns of range and reflectance at discrete points on the path. 
The camera swept over the scene a total of six times (note that the scene photographed is a recreation 
of that which the laser actually (a) (b) (c) (d) (e)  (b) Camera path used to acquire it. (c) The 
grayscale acquired image. (d) Color mask applied to it. (e) Reprojected views.   scanned, since equipment 
was moved between the day of the experiment and the day of the photographs). The MCOP image shown in 
(c) is the grayscale reflectance reported by the laser. The color mask was hand-created to better distinguish 
the separate objects. Two reprojections of the entire dataset. The results from this early experiment 
were reasonable, and proved the feasibility of the MCOP method for real-world usage. The main problems 
encountered were static misregistration between the tracker and laser, and poor range data resulting 
from specular surfaces. 9 LIMITATIONS AND FUTURE WORK The tradeoffs and limitations of MCOP images are 
similar to those of conventional images. In order to capture view­dependent lighting in an MCOP image, 
we must acquire a feature multiple times; this is analogous to sampling a point from different locations 
with conventional images. As with all image­based methods, the rendering quality is limited by the original 
image sampling; this is in contrast with geometric methods, which can represent objects as continuous 
entities. Also, since the number of viewpoints in an MCOP image tends to be larger than in a set of conventional 
images, there are more opportunities for error to be introduced by the camera tracking. Finally, while 
MCOP images allow greater acquisition flexibility, they do not solve the problem of finding a set of 
viewpoints that fully cover a scene, or sample it at some optimal quality. Conventional range images 
can be rendered in occlusion­compatible order [19], a scene-independent list-priority technique that 
eliminates the need for z-buffering with a single reference image. This cannot be applied directly to 
multiple-center-of­projection images, however, due to their complex epipolar geometries. It remains as 
future work to classify what subsets of all possible MCOP configurations can be rendered in occlusion 
compatible order. Another area of future work concerns the construction of complete polygonal meshes 
for CAD or geometric rendering [5][27], given an MCOP range image. The problem is simplified somewhat 
in the MCOP domain since we have connectivity information across all neighboring pixels, as opposed to 
dealing with an unorganized set of points, or multiple reference images which only contain connectivity 
information within themselves. Nonetheless, while MCOP images may reduce the number of seams that must 
be stitched, they are not altogether eliminated. Whenever a surface is viewed multiple times, the spatial 
connectivity across the different viewings must still be determined. 10 CONCLUSION In this paper we have 
developed and discussed the multiple­center-of-projection image for image-based rendering. MCOP images 
alleviate many problems of image-based rendering by maintaining a single image, containing information 
from multiple viewpoints. They allow for better image reconstruction than conventional range images. 
They are capable of sampling different portions of a scene at different resolutions. They provide greater 
control over the sampling process, allowing the directions of viewing rays to vary over the image, thereby 
acquiring better samples than if all were bound to the same COP. They also posses a unique internal epipolar 
geometry that defines how multiple viewings of scene points relate to each other. Multiple-center-of-projection 
images have already proven their usefulness in real-world domains, though under different names. The 
strip camera, for example, has existed for almost a hundred years, and has been used for such important 
tasks as aerial photography for the last fifty. The Cyberware scanner, another MCOP device, has proven 
invaluable to a wide range of computer graphics applications. This paper presents a framework by which 
these existing methods can be exploited further. More importantly, it extends the notion of what it means 
to be an image in image-based rendering.   ACKNOLEDGEMENTS We would like to thank Mary Whitton, Nick 
England, Russ Taylor, Anselmo Lastra, Henry Fuchs, and the UNC IBR group for their assistance and insight 
at various stages of this work. Special thanks to Fred Brooks for his thorough critique of an early draft, 
and to Lars Nyland for his collaboration on the laser experiment. Thanks also to the SIGGRAPH reviewers. 
Photographs are by Todd Gaul. The models were provided by REM Infografica. This work is supported by 
DARPA ITO contract number E278, NSF MIP-9612643, DARPA ETO contract number N00019-97-C-2013, and an NSF 
Graduate Fellowship. Thanks also to Intel for their generous donation of equipment.  REFERENCES [1] 
E. H. Adelson and J. R. Bergen. The Plenoptic Function And The Elements Of Early Vision. In Computational 
Models of Visual Processing, pp. 3-20, Edited by Michael Landy and J. Anthony Movshon, The MIT Press, 
Cambridge, 1991. [2] Robert C. Bolles, H. Harlyn Baker, and David H. Marimont. Epipolar-Plane Image Analysis: 
An Approach To Determining Structure From Motion. In International Journal of Computer Vision, volume 
1, page 7-55. Boston, 1987. [3] Shenchang Eric Chen and Lance Williams. View Interpolation For Image 
Synthesis. In Proceedings of SIGGRAPH 93, pp. 279-288, New York, 1993. ACM. [4] Shenchang Eric Chen. 
Quicktime VR: An Image-Based Approach To Virtual Environment Navigation. In Proceedings of SIGGRAPH 95, 
pp. 29-38, New York, 1995. ACM. [5] Brian Curless and Marc Levoy. A Volumetric Method For Building Complex 
Models From Range Images. In Proceedings of SIGGRAPH 96, pp. 303-312. New York, 1996. ACM. [6] William 
J. Dally, Leonard McMillan, Gary Bishop, and Henry Fuchs. The Delta Tree: An Object-Centered Approach 
To Image-Based Rendering. MIT AI Lab Technical Memo 1604, May 1996. [7] Lucia Darsa, Bruno Costa Silva, 
and Amitabh Varshney. Navigating Static Environments Using Image-Space Simplification And Morphing. In 
Proceedings of 1997 Symposium on Interactive 3D Graphics, pp. 25-34. April 1997. [8] Olivier Faugeras. 
Three-Dimensional Computer Vision: A Geometric Approach. MIT Press, Cambridge, Massachusetts, 1993. [9] 
Sanjib K. Ghosh. Analytical Photogrammetry, second edition. Pergamon, 1988. [10] Steven J. Gortler, Radek 
Grzeszczuk, Richard Szeliski, and Michael F. Cohen. The Lumigraph. In Proceedings of SIGGRAPH 96, pp. 
43-54, New York, 1996. ACM. [11] Eduard Gröller and Helwig Löffelmann. Extended Camera Specifications 
for Image Synthesis. In Machine Graphics and Vision, 3 (3), pp. 513-530. 1994. [12] Michael Halle. Multiple 
Viewpoint Rendering. To appear in Proceedings of SIGGRAPH 98. New York, 1998. ACM [13] Richard Hartley 
and Rajiv Gupta. Linear Pushbroom Cameras. In Proceedings of Third European Conference on Computer Vision, 
pp. 555-566. New York, 1994. [14] Hugues Hoppe, Tony DeRose, Tom Duchamp, John McDonald, and Werner Stuetzle. 
Surface Reconstruction From Unorganized Points. In Computer Graphics (SIGGRAPH 92 Conference Proceedings), 
volume 26, pp. 71-78. New York, July 1992. ACM. [15] Stephane Lavaeu and Olivier Faugeras. 3-D Scene 
Representation As A Collection Of Images. INRIA Technical Report RR-2205. February 1994, INRIA. [16] 
Marc Levoy and Pat Hanrahan. Light Field Rendering. In Proceedings of SIGGRAPH 96, pp. 31-42, New York, 
1996. ACM. [17] William R. Mark, Leonard McMillan and Gary Bishop. Post-Rendering 3D Warping. In Proceedings 
of the 1997 Symposium on Interactive 3D Graphics, page 7-16, Providence, Rhode Island, April 1997. [18] 
Leonard McMillan and Gary Bishop. Shape As A Perturbation To Projective Mapping, UNC Computer Science 
Technical Report TR95-046, University of North Carolina, April 1995. [19] Leonard McMillan and Gary Bishop. 
Plenoptic Modeling: An Image-Based Rendering System. In Proceedings of SIGGRAPH 95, pp. 39-46, New York, 
1995. ACM. [20] Shmuel Peleg and Joshua Herman. Panoramic Mosaics by Manifold Projection. In Proceedings 
of Computer Vision and Pattern Recognition, pp. 338-343, Washington, June 1997. IEEE. [21] Kari Pulli, 
Michael Cohen, Tom Duchamp, Hugues Hoppe, Linda Shapiro, Werner Stuetzle. View-Based Rendering: Visualizing 
Real Objects from Scanned Range and Color Data. In Proceedings of Eighth Eurographics Workshop on Rendering, 
pp. 23-34. Eurographics, June 1997. [22] Matthew Regan and Ronald Pose. Priority Rendering with a Virtual 
Reality Address Recalculation Pipeline. In Proceedings of SIGGRAPH 94, pp. 155­162, New York, 1994. ACM. 
[23] Jonathan Shade, Steven Gortler, Li-wei He, and Richard Szeliski. Layered Depth Images. To appear 
in Proceedings of SIGGRAPH 98. New York, 1998. ACM [24] François X. Sillion, George Drettakis and Benoit 
Bodelet. Efficient Impostor Manipulation For Real-Time Visualization Of Urban Scenery. In Proceedings 
of Eurographics 97, pp. 207-218. Budapest, Hungary, September 1997. [25] Edward Swan, Klaus Mueller, 
Torsten Moller, Naeem Shareef, Roger Crawfis, and Roni Yagel. An Anti-Aliasing Technique For Splatting. 
In Proceedings of IEEE Visualization 97, pp. 197-204, 1997 [26] Jay Torborg, James T. Kajiya. Talisman: 
Commodity Real-Time Graphics For The PC. In Proceedings of SIGGRAPH 96, pp. 353-363. New York, August 
1996. ACM. [27] Greg Turk and Marc Levoy. Zippered Polygon Meshes From Range Images. In Proceedings of 
SIGGRAPH 94, pp. 311-318. New York, July 1994. ACM. [28] Gregory Welch and Gary Bishop. SCAAT: Incremental 
Tracking With Incomplete Information. In Proceedings of SIGGRAPH 97, pp. 333-344. Los Angeles , August 
1997. ACM. [29] Lee Westover. Footprint Evaluation For Volume Rendering. In Computer Graphics (SIGGRAPH 
90 Conference Proceedings), volume 24, pp. 367-376. New York, August 1990. ACM. [30] Daniel N. Wood, 
Adam Finkelstein, John F. Hughes, Craig E. Thayer, and David H. Salesin. Multiperspective Panoramas For 
Cel Animation. In Proceedings of SIGGRAPH 97, pp. 243-250. New York, 1997. ACM. [31] Jiang Yu Zheng and 
Saburo Tsuji. Panoramic Representation for Route Recognition by a Mobile Robot. In International Journal 
of Computer Vision 9 (1): 55-76. Netherlands, 1992. Kluwer.    Figure 17 The rear of the ship. The 
flag was sampled at the highest resolution of the image (Figure 14), allowing an extreme close-up of 
it. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280874</article_id>
		<sort_key>207</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>21</seq_no>
		<title><![CDATA[Recovering photometric properties of architectural scenes from photographs]]></title>
		<page_from>207</page_from>
		<page_to>217</page_to>
		<doi_number>10.1145/280814.280874</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280874</url>
		<keywords>
			<kw><![CDATA[BRDG]]></kw>
			<kw><![CDATA[illumination]]></kw>
			<kw><![CDATA[image-based rendering]]></kw>
			<kw><![CDATA[photometric properties]]></kw>
			<kw><![CDATA[photometric stereo]]></kw>
			<kw><![CDATA[reflectance]]></kw>
			<kw><![CDATA[sky model]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Modeling and recovery of physical attributes</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Visible line/surface algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Photometry</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Shading</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Color</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010377</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Visibility</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP40027646</person_id>
				<author_profile_id><![CDATA[81100472713]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yizhou]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of California at Berkeley, Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15029665</person_id>
				<author_profile_id><![CDATA[81100342430]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jitendra]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Malik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of California at Berkeley, Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BRUNGER, A., AND HOOPER, F. Anisotropic sky radiance model based on narrow field of view measurements of shortwave radiance. Solar Energy 51, 1 (1993), 53-64.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>794511</ref_obj_id>
				<ref_obj_pid>794189</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[DANA, K., VAN GINNEKEN, B., NAYAR, S., AND KOEN- DERINK, J. Reflectance and texture of real-world surfaces. In proceedings of CVPR (1997), pp. 151-157.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258884</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[DEBEVEC, P., AND MALIK, J. Recovering high dynamic range radiance maps from photographs. In Computer Graphics Proceedings, Annual Conference Series (1997), pp. 369- 378.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237191</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[DEBEVEC, P., TAYLOR, C., AND MALIK, J. Modeling and rendering architecture from photographs: A hybrid geometryand image-based approach. In Computer Graphics Proceedings, Annual Conference Series (1996), pp. 11-20.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>893689</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[DEBEVEC, P., YU, Y., AND BORSHUKOV, a. Efficient viewdependent image-based rendering with projective texturemapping. UC Berkeley technical report #UCB//CSD-98- 1003.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237200</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[GORTLER, S., GRZESZCZUK, R., SZELISKI, R., AND CO- HEN, M. The lumigraph. In Computer Graphics Proceedings, Annual Conference Series (1996), pp. 43-54.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[HAMPEL, F., ROUSSEEUW, P., RONCHETTI, E., AND STA- HEL, W. Robust Statistics. John Wiley &amp; Sons, New York, 1986.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[INEICHEN, P., MOLINEAUX, B., AND PEREZ, e. Sky luminance data validation: Comparison of seven models with four data banks. Solar Energy 52, 4 (1994), 337-346.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>35071</ref_obj_id>
				<ref_obj_pid>35068</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[KLASSEN, R. Modeling the effect of the atmosphere on light. ACM Transactions on Graphics 6, 3 (1987), 215-237.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[KOENDERINK, J., AND VAN DOORN, A. Illuminance texture due to surface mesostructure. J. Opt. Soc. Am.A 13, 3 (1996), 452-463.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258801</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[LAFORTUNE, E., FOO, S., TORRANCE, K., AND GREEN- BERG, O. Non-linear approximation of reflectance functions. In Computer Graphics Proceedings, Annual Conference Series (1997), pp. 117-126.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237199</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[LEVOY, M., AND HANRAHAN, P. Light field rendering. In Computer Graphics Proceedings, Annual Conference Series (1996), pp. 31-42.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[LITTLEFAIR, P. A comparison of sky luminance models with measured data from garston, united kingdom. Solar Energy 53, 4 (1994), 315-322.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218398</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[MCMmLAN, L., AND BISHOP, G. Plenoptic modeling: An image-based rendering system. In Computer Graphics Proceedings, Annual Conference Series (1995), pp. 39-46.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15900</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[NISHITA, T., AND NAKAMAE, E. Continuous tone representation of three-dimensional objects illuminated by sky light. Computer Graphics 20, 4 (1986), 125-132.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[PEREZ, R., SEALS, R., AND MICHALSKY, J. All-weather model for sky luminance distribution-preliminary configuration and validation. Solar Energy 50, 3 (1993), 235-245.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>42249</ref_obj_id>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[PRESS, W., FLANNERY, B., TEUKOLSKY, S., AND VETTER- LING, W. Numerical Recipes in C. Cambridge Univ. Press, New York, 1988.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[REES, W. Physical Principles of Remote Sensing. Cambridge Univ. Press, 1990.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>244312</ref_obj_id>
				<ref_obj_pid>244304</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[SATO, Y., AND IKEUCHI, K. Reflectance analysis for 3d computer graphics model generation. Graphical Models and Image Processing 58, 5 (1996), 437-451.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258885</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[S ATO, Y., WHEELER, M., AND IKEUCHI, K. Object shape and reflectance modeling from observation. In Computer Graphics Proceedings, Annual Conference Series (1997), pp. 379-388.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258861</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[SZELISKI, e., AND SHUM, U. Creating full view panoramic mosaics and environment maps. In Computer Graphics Proceedings, Annual Conference Series (1997), pp. 251-258.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[TADAMURA, K., NAKAMAE, E., KANEDA, K., BABA, M., YAMASHITA, H., AND NISHITA, T. Modeling of skylight and rendering of outdoor scenes. Proceedings of EUROGRAPH- ICS'93, Computer Graphics Forum 12, 3 (1993), 189-200.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97908</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[TAKAGI, A., TAKAOKA, H., OSHIMA, T., AND OGATA, Y. Accurate rendering technique based on colorimetric conception. Computer Graphics 24, 4 (1990), 1990.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134078</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[WA~D, G. Measuring and modeling anisotropic reflection. Computer Graphics 26, 2 (1992), 265-272.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>93888</ref_obj_id>
				<ref_obj_pid>93871</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[WOODHAM, R. Photometric method for determining surface orientation from multiple images. In Shape f~vm Shading, B. Horn and M. Brooks, Eds. MIT Press, 1989, pp. 513-532.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Copyright &#38;#169;1998 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers or to redistribute to lists, requires specific permission and/or a fee. Recovering Photometric 
Properties Of Architectural Scenes From Photographs Yizhou Yu Jitendra Malik Computer Science Division 
University Of California At Berkeley . Abstract In this paper, we present a new approach to producing 
photoreal­istic computer renderings of real architectural scenes under novel lighting conditions, such 
as at different times of day, starting from a small set of photographs of the real scene. Traditional 
texture map­ping approaches to image-based modeling and rendering are unable to do this because texture 
maps are the product of the interaction between lighting and surface re.ectance and one cannot deal with 
novel lighting without dissecting their respective contributions. To obtain this decomposition into lighting 
and re.ectance, our basic ap­proach is to solve a series of optimization problems to .nd the pa­rameters 
of appropriate lighting and re.ectance models that best ex­plain the measured values in the various photographs 
of the scene. The lighting models include the radiance distributions from the sun and the sky, as well 
as the landscape to consider the effect of sec­ondary illumination from the environment. The re.ectance 
models are for the surfaces of the architecture. Photographs are taken for the sun, the sky, the landscape, 
as well as the architecture at a few different times of day to collect enough data for recovering the 
var­ious lighting and re.ectance models. We can predict novel illumi­nation conditions with the recovered 
lighting models and use these together with the recovered re.ectance values to produce renderings of 
the scene. Our results show that our goal of generating photore­alistic renderings of real architectural 
scenes under novel lighting conditions has been achieved. CR Categories: I.2.10 [Arti.cial Intelligence]: 
Vision and Scene Understanding modeling and recovery of physical at­tributes I.3.7 [Computer Graphics]: 
Three-dimensional Graph­ics and Realism color, shading, shadowing, and texture , vis­ible line/surface 
algorithms I.4.8 [Image Processing]: Scene Analysis color, photometry, shading Keywords: Photometric 
Properties, Image-based Rendering, Illu­mination, Sky Model, Re.ectance, BRDF, Photometric Stereo 1 
INTRODUCTION It is light that reveals the form and material of architecture. In keep­ing with its rhythms 
of light and dark, clear and cloudy, the architec­ture evokes distinct visual moods and impressions, 
something that many photographers and painters have sought to capture. Perhaps the most noteworthy of 
these attempts is the famous series of stud­ies of the Cathedral at Rouen by Claude Monet he painted 
the same facade at many different times of day and in different seasons of the year, seeking to capture 
the different impressions of the scene. Our goal in this paper is to develop this theme in the context 
of computer graphics. We will develop and demonstrate techniques to produce photorealistic computer renderings 
of real architectural scenes under different lighting conditions, such as at different times of day, 
starting from a small set of photographs of the real scene. Previous work on the FACADE system[4] has 
shown that it is pos­sible to use a combination of geometric models recovered from pho­tographs, and 
projective texture mapping with textures derived from the same photographs, to generate extremely photorealistic 
render­ings of the scene from novel viewpoints. However while we have the ability to vary viewpoint, 
we are unable to produce render­ings under new lighting conditions the texture maps are the prod­uct 
of the interaction between the lighting and surface re.ectance and one cannot deal with novel lighting 
without dissecting their re­spective contributions. Other approaches to image-based render­ing [14, 12, 
6, 21] share the same general dif.culty. To obtain this decomposition into lighting and re.ectance, 
our basic approach is to solve a series of optimization problems to .nd the parameters of appropriate 
lighting and re.ectance models that best explain the measured values in the various photographs of the 
scene. The lighting models include those for the radiance distri­bution from the sun and the sky, as 
well as a landscape radiance model to consider the effect of illumination from the secondary sources 
in the environment. Note that illumination from these sec­ondary sources, such as the ground near the 
.oor of a building can be very important and is often the dominant term in shadowed ar­eas. To have suf.cient 
data for parameter recovery, we take several photographs of the sun, the sky, the architecture, and the 
environ­ment surrounding the architecture. This enables us to recover radi­ance models for the sun, sky 
and environment for that time of day. The process is repeated for a few different times of the day; collec­tively 
all these data are used to estimate the re.ectance properties of the architecture. It is assumed that 
a geometric model of the archi­tecture had previously been created using a modeling system such as FACADE, 
so at this stage enough information is available to reren­der the building under novel lighting conditions. 
The data-.ow di­agram of the system is given in Figure 1. There are several technical challenges that 
must be overcome. We highlight a few of them here: 1. The photographs do not directly give us radiance 
measurements there is a nonlinear mapping which re­lates the digital values from the photograph to the 
radiance in the direction of that image pixel. This can be estimated using the technique from [3], and 
subsequent processing performed using radiance images. 2. Any measurements that we make from photographs 
cannot be used to recover the full spectral BRDF. We need to de.ne a new concept, the pseudo-BRDF associated 
with a particular spectral distribution of the illuminant. This is done in Sec­tion 2. Our system is 
based on recovering pseudo-BRDFs for the architecture, and then subsequently using them for re­rendering. 
We recover two pseudo-BRDFs, one correspond­ing to the spectral distribution of the sun and one correspond­ing 
to the integrated light from the sky and landscape.  *Berkeley, CA 94720, e-mail: fyyz,malikg@cs.Berkeley.edu, 
website: 3. Producing renderings of the scene at novel times of day re­http://http.cs.berkeley.edu/.fyyz,malikg 
quires being able to predict lighting from the sun, sky and en­Figure 1: Data-.ow diagram of the re-rendering 
system. vironment at such times. For the sun and sky, we rely on inter­polated/extrapolated radiance 
models of the sun and sky (Sec­tion 5). Prediction of radiance from the environment at a novel time requires 
use of the computer vision technique of photo­metric stereo to recover a low resolution surface normal 
map of the environment, which can then be used in conjunction with the new sun position to yield the 
new environment radi­ance map. This paper is organized as follows. In Section 2, we will dis­cuss the 
pseudo-BRDF. In Section 3, we will introduce the methods for measuring the illumination. In Section 4, 
we will introduce the methods for recovering re.ectance. In Section 5, we will propose approaches for 
simulating novel lighting conditions. In Section 6, we will give re-rendering results. Conclusions and 
future work will be given in the last section. In the appendices, we will give an algo­rithm for irradiance 
calculation and an algorithm for visibility pro­cessing.  2 THE PSEUDO-BRDF CONCEPT The traditional 
way to formally de.ne re.ectance is using the con­cept of the bidirectional re.ectance distribution function 
(BRDF) de.ned as follows: p((i; i; ( r; r; ' ) = dI((r;I((i;i;' r;') )cos(id!i (1) where I((i; i; ' )is 
the incident radiance and dI((r; r; ' )is the re.ected differential radiance. Note the dependence on 
wavelength '. There has been some some previous work using a spectrophotometer to carefully measure spectral 
BRDFs [2]. However, we concluded that it is impractical to use such a technique to measure the BRDFs 
of complex, outdoor scenes. Our philosophy is to work with whatever information can be extracted from 
photographs, and we will use just an ordinary hand­held digital video camcorder to acquire these photographs. 
Assume that the camera is geometrically calibrated, permitting us to identify ray directions from pixel 
locations. In such a photograph, the value Vobtained at a particular pixel in a particular channel (R, 
G, B) is the result of integration with the spectral response function R(') Z V=R(')E(')d': (2) where 
E(')is the incident radiance. Suppose we take photographs of an area light source and of an object illuminated 
by this light source. Let us check the impact of this spectral integration over the traditional BRDF 
re.ection model. What we can get from the photograph of the area light source is Z Iimage((i;i)=I((i;i;')R(')d'(3) 
 and what we can get from the photograph of the object is Z Iimage((r;r)=I((r;r;')R(')d' ZZ =I((i;i;')p((i;i;(r;r;')R(')d'cos(id!i:(4) 
 If we follow the de.nition of BRDF, but use Iimage((i;i)and Iimage((r;r)instead, we can de.ne the following 
quantity which we will call the pseudo-BRDF dIimage(Or;¢r) ppseudo((i;i;(r;r)=(5) Iimage(Oi;¢i)cosOid!i 
R I(Oi;¢i;>)p(Oi;¢i;Or;¢r;>)R(>)d> =R(6) I(Oi;¢i;>)R(>)d> We note some properties of the pseudo-BRDF 
here: The pseudo-BRDF is equal to the real BRDF when the real BRDF does not vary with the wavelength. 
So they usually are not the same.  In general, the pseudo-BRDF varies as the spectral distribution of 
the light source varies.  If the spectral response function R(')=0(','0),then ppseudo((i;i;(r;r)=p((i;i;(r;r;'0). 
  Suppose we have a geometric model of some building. For the purpose of re-rendering under different 
lighting conditions, we need to recover the re.ectance of the faces in the model. Since only pseudo-BRDFs 
can be recovered directly from photographs for each color channel and pseudo-BRDFs are sensitive to the 
spectral dis­tribution of the light source, theoretically, we should divide the sky and the environment 
into small regions which have almost uniform spectral distributions spatially and recover distinct pseudo-BRDFs 
for each region. This is impractical because all these regions have their lighting effects on the considered 
architecture altogether and it is impossible to turn on only one of them and shut down the rest to recover 
individual pseudo-BRDFs. What we want to do is to re­cover as few pseudo-BRDFs as possible, but still 
get good approxi­mations in rendering. It is possible to separate the sun from the sky since the solar 
position changes a lot during a day and a face of a building can be lit or unlit at different times. 
This has the same ef­fect as turning the sun on or off for that face. It is also necessary to do this 
separation because the sun is the most important light source and its spectral distribution is so different 
from the blue sky. As to the rest of the sky and the environment, we .nd from experiments that recovering 
only one set of pseudo-BRDFs for them works very well. From now on, we will always recover two sets of 
pseudo-BRDFs, one of which corresponds to the spectral distribution of the sun, and the other to the 
integrated effect of the sky and environ­ment. They will be used for re-rendering under novel lighting 
condi­tions under the assumption that the spectral distribution of daylight does not change much. Under 
extreme conditions, sunrise and sun­set, we may expect these pseudo-BRDF s to cease being accurate. 
(a) (b) (c) (d)  Figure 2: (a)Solar image obtained using a couple of neutral density .lters, (b) Solar 
aureole obtained using fast shutter speed, (c) a pho­tograph for the zenith, (d) a photograph for the 
landscape and the sky near the horizon.  3 MEASURING AND MODELING ILLUMI-NATION We consider three sources 
of illumination. Light can be from the sun, the sky and the surrounding environment which serves as a 
sec­ondary light source. Of course, in some fundamental sense, the sun is the only true light source. 
Both skylight and the light from the environment are ultimately derived from the sun. However, with an 
image-based approach, we need to measure and model these three sources separately. We shall not be constructing 
a physically cor­rect global illumination model of the atmosphere and environment taking into account 
all the scattering and re.ection effects! To model these illumination sources, we take photographs of 
the sun, the sky and environment using a handheld CCD camera. To ac­curately measure the radiance, we 
need to convert the photographs into radiance images by inverting the nonlinear mapping between the incident 
radiance of the camera and its digital output. To recover this nonlinear mapping, we use the technique 
described in [3]. 3.1 The Sun We can measure the radiance of the sun with a camera and a cou­ple of neutral 
density .lters(Figure 2(a)) to make it unsaturated so that we can recover its dynamic radiance using 
the nonlinear map­ping introduced before. The solid angle subtended by the sun can be obtained from the 
diameter of the sun and the distance between the sun and the earth. The solar position(altitude and azimuth) 
can be obtained from formula given in the appendix of [18], provided that the latitude and longitude 
of the site on the earth s surface, and the time and date are known. We model the sun as a parallel light 
source.  3.2 The Sky We can take photographs of the sky in order to measure its radiance distribution. 
But there exist a couple of problems. First, it is hard to know the camera pose because there is no feature 
in the sky to calibrate camera orientation if the sky is clear; second, it is hard to get a picture of 
the whole sky even with a .sh-eye lens because there might be some objects occluding part of the sky, 
such as trees, buildings, and mountains; third, the intensity of circumsolar region or solar aureole 
can be very high, and can easily get saturated at a normal shutter speed. To solve the .rst problem, 
we decided to in­clude some buildings as landmarks in each photograph so that we can use them to recover 
the camera pose later. But this means we are going to have more occlusions. While we will take multiple 
pho­tographs(Figure 2(c)(d)) and hope the invisible part of the sky in one photograph will become visible 
in some other photograph, there is no way to guarantee that every part of the sky will be seen. Our ap­proach 
to solve this dif.culty is to have a sky model which we can .t to the visible parts of the sky and extrapolate 
into the invisible parts. To solve the last problem, we use a set of different shutter speeds for the 
solar aureole with each speed capturing the radiance inside a circular band centered at the solar position(Figure 
2(b)). Several papers present physical models of sky radiance [22, 9, 23]. However, we do not know how 
closely they approximate the real sky. Furthermore, physical models often give the spectral dis­tribution 
of any point in the sky. It is very hard to .t these models to RGB data taken from photographs. On the 
other hand, there are also many empirical models for sky luminance or radiance distribution [16, 1, 13, 
8]. All CIE standard sky formulae are .xed sky luminance distributions. They can not be used for the 
purpose of data-.tting. The all-weather sky luminance model proposed in [16] is a generalization of the 
CIE standard clear sky formula. It is given by Ls(E;,)=Lvzf(E;,)/f(0;Z) (7) where Eis the zenith angle 
of the considered sky element and ,is the angle between this sky element and the position of the sun, 
Lvz is zenith luminance, Zis the zenith angle of the sun, and 2 f(E;,)=[1+aexp(b/cosE)][1+cexp(d,)+ecos,](8) 
 where a;b;c;d;and eare adjustable coef.cients. These variable coef.cients make this empirical model 
more .exible than others, which means we might have a better .t by using this model. Since both Lvzand 
f(0;Z)in the above model are unknown constants, we replace them with one new variable coef.cient which 
can be optimized during data .tting. Empirically, we also .nd it is better to have one more variable 
coef.cient as the exponent of ,in the term with cand d. Thus, we obtain the following revised seven­parameter 
sky model h2 Ls(E;,)=Lz[1+aexp(b/cosE)][1+cexp(d,)+ecos,] (9) where a;b;c;d;e;h;and Lzare variable coef.cients. 
Up to now, we still only have a sky luminance model which does not have colors. We have not seen in the 
literature any approach converting sky luminance models to RGB color distributions. The method proposed 
in [23] converts luminance data to color tempera­tures and then to spectral distributions. The relationship 
they use be­tween luminance and color temperatures is not necessarily accurate for different weather 
conditions. Based on the fact that the sky radi­ance distribution at each color channel has a similar 
shape, we de­cided to use the same model but a distinct set of coef.cients for each color channel by 
.tting the above revised model to the data from each channel. In practice, the error of data-.tting remains 
very small (a) (b) (c) (d) Figure 3: (a) A sky radiance model obtained by data-.tting,(b)-(d) the R,G,B 
channels of the sky model in (a). All color channels are generated using the same sky luminance model, 
but each color channel has its own distinct parameters. for each channel, which means our method is appropriate. 
Skies thus obtained have convincing colors. Since there might be trees, buildings or mountains in pho­tographs, 
we interactively pick some sky regions from each pho­tograph and .t the revised sky model to the chosen 
sky radiance data by using Levenberg-Marquardt method [17] to minimize the weighted least-square N X 
yi,Ls(Ei;,i) []2 (10) Oi i 1 where yi s are the chosen sky radiance data from photographs and Oi s are 
weights. We tried different weighting schemes, such as Oi=1;yi/log(yi);yi/sqrt(yi);or yi, and found the 
best result was obtained when Oi=yi/log(yi). With this weighting scheme, the .tting error at most places 
is within 5%. A recovered sky radi­ance model is given in Figure 3. Figure 4: Two views of a spherical 
environment map. The upper hemisphere corresponds to the sky and the lower hemisphere has the radiance 
values from the surrounding landscape.  3.3 The Environment By our de.nition, the environment of an 
outdoor object is its sur­rounding landscape. It can be a more signi.cant light source than the dark 
side of a clear sky. There are mutual interre.ections be­tween an object and its environment. For re.ectance 
recovery, we need to measure the radiance distribution of the whole environment which includes radiance 
from all visible objects and is the equilib­rium state of mutual interre.ections. It is assumed that 
we do not interfere with this equilibrium state when we take photographs of the environment. For our 
purpose, we only need a coarse-grain environment ra­diance map to do irradiance calculation because irradiance 
results from an integrated effect of the incident radiance distribution. High­frequency components can 
therefore be ignored. We subdivide the environment sphere along latitudinal and longitudinal direc­tions 
and get a set of rectangular spherical regions. Once we have those environment photographs(Figure 2(d)) 
and their camera ori­entations, we project every pixel into one of the spherical regions. Finally, we 
average the color of the pixels projected into each re­gion and give the result as the average radiance 
from that region. If the architecture has large size, we may need to capture more than one environment 
map at different locations because the surrounding light .eld is a four dimensional distribution. However, 
since the in­tegrated irradiance over the surfaces changes smoothly and slowly, we do not need to capture 
more than a small number of coarse-grain environment maps. Two images of a spherical environment map 
including radiance distribution from both the sky and the landscape are shown in Figure 4.  4 RECOVERING 
REFLECTANCE We need to recover the re.ectance of the faces in the geometric model for the purpose of 
re-rendering under different lighting condi­tions. There has been a lot of previous work [24, 19, 20, 
11, 2] trying to .t empirical or physics-based models to measured data and then using the obtained model 
into illumination calculation. The exper­iments were done for small objects or material samples in labora­tory 
settings where only one single point light source was used and global illumination effects could be ignored. 
Usually only one set of pseudo-BRDF s were recovered if the data were obtained from images, which, as 
we know, is not adequate in our outdoor natural lighting context. (a) (b) (c) (d) (e) Figure 6: (a) 
A simple geometric model of a bell tower, (b)-(c) Diffuse pseudo-albedo recovered by using irradiance 
from both the sky and the landscape, (d) diffuse pseudo-albedo recovered incorrectly by only using irradiance 
from the sky, (e) diffuse pseudo-albedo corresponding to the spectral distribution of the sun. Recall 
from Section 2, that we decided to recover two sets of pseudo-BRDFs: one corresponding to the spectral 
distribution of the sun, and the other corresponding to the spectral distribution of the irradiance from 
both the sky and environment. 4.1 Recovering Diffuse Pseudo-Albedos We use Lambertian model for diffuse 
component. So the recovery of diffuse pseudo-albedos at each surface point needs the incident irradiance 
and the outgoing diffuse radiance. The incident irradi­ance is obtained by gathering light from the sun, 
the sky, the envi­ronment, and possibly other polygonal faces occluding part of the previous three sources. 
We can get the irradiance from the sun by using the surface normal, the color and solid angle of the 
sun which we got from Section 3.1. We will discuss gathering light from the sky and environment in Appendix 
A. Gathering light from occlud­ing faces can be done using the method in [15]. We use one-bounce re.ection 
to approximate the interre.ection among different faces. Multiple photographs are taken for the considered 
building at dif­ferent viewing directions and times(Figure 5). Since most architec­tural materials are 
only weakly specular except for windows, if our viewing direction is far away from the mirror angle of 
the current solar position, we can assume only diffuse radiance is captured in the photograph. Since 
each photograph can only cover some part of the architecture and there are occlusions among different 
faces, we need to decide which face is visible to which photographs. Visibility testing and polygon clipping 
will be discussed in Appendix B. Since every surface of the building has its own surface texture, we 
need to incorporate these spatial variations into its pseudo-albedos. Each polygon in the geometric model 
is .rst triangulated and a dense grid is set up on each triangle in order to capture the varia­tions. 
This step is similar to that introduced in [20]. Each grid point is projected onto the photographs to 
which it is visible and a radiance value is taken from each photograph. The diffuse pseudo-albedo at 
the grid point is obtained by dividing the average radiance by the irradiance. We need at least two photographs 
for each face of the building to recover both sets of pseudo-BRDF s. And it should not be lit by the 
sun in one photograph and should be lit in the other. Thus we have two equations for each surface point, 
one from each photograph. (1)se(1) I =pE(11) se (2)se(2)sun I =pEse+pEsun (12) where I(1)and I(2)are 
radiance values obtained from the two pho­ (1)(2) tographs, Eseand Eseare the irradiance from the sky 
and environ­ se ment, Esunis the irradiance from the sun, pis the pseudo-albedo corresponding to the 
spectral distribution of the sky and environ­ sun ment, and pis the pseudo-albedo corresponding to the 
spectral distribution of the sun. From (11), we can solve p se. By substituting it into (12), we can 
sun solve ptoo. Of course, if we have more than two photographs, these estimations can be made more 
robust. Figure 6 displays the recovered diffuse pseudo-albedo. Figure 6(b)&#38;(c) shows the dif­fuse 
pseudo-albedos of four different sides of a bell tower. These recovered pseudo-albedos are quite consistent 
with each other, pro­viding an independent veri.cation of our procedure since we recov­ered them from 
different photographs shot at different times. We can choose solar positions to avoid large shadows cast 
on the architecture. When large shadows are inavoidable, we can interac­tively label the shadow boundaries 
to separate sunlit regions from shadowed ones. If there are several buildings located close to each other 
such that some sides of the buildings can not be lit by the sun or we can not simply take photographs 
for them, our method can not be used. A solution to this dif.culty might be to .ll in re.ectance values 
from adjacent faces. 4.2 Recovering The Specular Lobes Of The Pseudo-BRDFs We adopt the empirical model 
in [11] to recover specular lobes be­cause this model can effectively simulate effects such as specular­ity 
at grazing angles, off-specular re.ections and etc. Putting diffuse and specular lobes together, for 
each color channel, we have the fol­lowing re.ection model expressed in a local coordinate system of 
each triangular patch Figure 7: RGB specular lobes, recovered for the sun, of the asso­ciated pseudo-BRDF 
of the lower block of the geometric model in Fig. 6(a) at incident direction (0.5, 0.0, 0.86). p(u;v) 
=pd+ps[Cxuxvx+Cyuyvy+Czuzvz]n :(13) where u=(ux;uy;uz)is the incident direction, v=(vx;vy;vz) is the 
viewing direction and pd;ps;Cx;Cy;Cz;nare adjustable co­ef.cients. We take multiple photographs at diferent 
times and viewing di­rections(Figure 5), such as grazing angles, directions close to mir­ror angles of 
the solar positions and other directions, to sample the radiance distribution from the architecture. 
4.2.1 Specular Lobe For The Sky And Environment Pseudo-BRDF Since the sky and the environment are extended 
light sources, to recover specular lobe of the associated pseudo-BRDF, we need to divide them into small 
pieces and plug the vector .ux from each piece into the specular model. Let the set of incident irradiance 
from these pieces are fe1;e2;;eng, the set of corresponding in­cident directions are fu1;u2;;ung, the 
set of viewing direc­tions are fv1;v2;;vmgand the corresponding radiance values are fI1;I2;;Img, this 
problem can be considered as minimiz­ing the following least-square objective P m X ( n j 1ejp(uj;Oi 
v i),Ii )2 : (14) i 1 This double summation needs to be evaluated at each iteration of the optimization. 
The number of patches in the sky and environment might be quite large, so it is time-consuming to run 
the optimiza­tion. This prevents us from using optimization at each grid point in the model. On the other 
hand, the parameter estimation can be­come unreliable at places where there are not enough data available 
for the specular component. Therefore, we assume each block in the model has the same specular lobe except 
for windows which are left for further investigation. For each block, we interactively pick some regions 
on the sur­face that are visible to multiple photographs. Thus each grid point in the regions has multiple 
radiance values corresponding to differ­ent viewing directions. Subtracting the estimated diffuse compo­nent 
from these radiance values and then running the optimization for each block with Levenberg-Marquardt 
method, we can get the coef.cients related to the specular lobe.  4.2.2 Specular Lobe For The Sun Pseudo-BRDF 
Recovering specular lobe corresponding to the spectral distribution of the sun is less time-consuming 
because the sun is considered as a directional(parallel) light source, we do not need to evaluate the 
in­ner summation in (14) any more. This means it is possible to apply better but more expensive global 
optimization techniques. We use the downhill simplex method with simulated annealing [17], which allows 
us to apply some techniques [17, 7] for robust parameter es­timation. Robust estimation tries to minimize 
 N X yi,y(xi;() (() (15) Oi i1 where ((z)is a nonlinear function of a single variable z=[yi, y(xi)]/Oi, 
in order to estimate (, the vector of parameters. Classic least squares corresponds to using ((z)=z 2, 
and is very sensitive to outliers. By a suitable choice of ((z), in our experiments ((z)= 1,exp(,jzj/2) 
, one can suppress the in.uence of outliers in the data. 1+exp(,jzj/2) We refer the reader to [7] for 
extensive discussion on this topic, as well as a technique for estimating Oi. The recovered RGB specular 
lobes of a block of the bell tower is given in Figure 7.  5 MODELING ILLUMINATION AT NOVEL TIMES OF 
DAY To generate renderings of the scene at a novel time of day, we need to predict what the illumination 
will be at that time. This requires us to construct sun, sky and environment illumination models appropriate 
to that time. We have available as a starting point, the illumination models for a few times of day where 
we took the initial photographs, recovered using the techniques introduced in Section 3. 5.1 The Sun 
And Sky Given the local time of day, the solar position(altitude and azimuth) can be obtained directly 
from formula given in the appendix of [18], provided that the latitude and longitude of the site on the 
earth s sur­face and the day number in a year are all known. Finding the appropriate sky model requires 
more work. First we consider sky interpolation during the main part of the day, ignoring sunrise and 
sunset. Note that the sky radiance distribution changes with the solar position, and naive pointwise 
radiance interpolation at each point in the sky would not work as shown in Figure 8. (a) (b) Figure 
8: 1D Schematic of sky interpolation where peaks repre­sent sky radiance at solar aureole. (a) A new 
sky(solid) obtained by pointwise interpolating two sky models(dot). It is not correct be­cause it has 
two peaks. (b) A new sky(solid) obtained with our in­terpolating scheme. Instead, let s examine the sky 
model in (9). It has three parts. The .rst part is the scaling factor Lzwhich controls the overall bright­ness 
of the sky. If not during sunrise or sunset, it should be almost a constant. The second part is the sky 
background. We denote it by Bg(E). The third part is the solar aureole. We denote it by Sa(,). The shapes 
of Bg(E)and Sa(,)remain unchanged during most times of a day. What changes is their relative position. 
Sa(,)ro­tates relative to Bg(E)as the sun moves across the sky. Based on this observation, we derive 
a sky interpolation scheme. Suppose we have recovered ksky models. If we need a new sky model at a dif­ferent 
time, a grid is .rst set up on the sky hemisphere. At each grid point with parameters (Ei;,i)corresponding 
to the new solar (a) (b) (c) (d) (e) (f) (g) (h)  Figure 9: (a)-(d) Environment maps for four different 
times, obtained from multiple photographs, (e)-(h) corresponding environment maps generated with the 
recovered environment radiance models which were obtained by data-.tting. There is one recovered radiance 
model for each environment region. jj position, we can get three data sets fLzj;Bg(Ei);Sa(,i);j= 1;;kgfrom 
the existing models. Set the sky radiance at the grid point to be the product of three weighted averages 
of the three data sets. The weight for each existing sky model is proportional to the reciprocal of the 
angular distance between the new solar position and the solar position of that sky model. Finally, with 
the radiance values at the grid points, we can run an optimization to .t a new sky model. During sunrise 
or sunset, there is less light from short wave­lengths. So the sun and solar aureole appear more red. 
The whole sky is darker. But the color of the rest of the sky only changes a little. It is well known, 
e.g. [9], that the color of the sky and sun is caused by scattering in the atmosphere. If a light beam 
travels a distance d in a medium with scattering particles, its intensity will be decreased by a factor 
of exp(,fd)where fis a constant coef.cient. With dif­ferent f s for different wavelengths, the color 
of the beam will also change. The distance dthat the sunlight travels through the atmo­sphere is the 
smallest when solar direction is perpendicular to the ground and it increases when the sun moves closer 
to the horizon. The optical depth of the atmosphere at the horizon is about 38 times that at the zenith. 
A formula to compute dfor any solar position can be found in [9] which tries to get the color of the 
sun and sky from physics-based models. However, we want to .t the above scattering model to real measurements. 
We measured solar radiance during the day and sunset and .t a distinct ffor each color channel. We use 
the same coef.cients to get the color of solar aureole. For the sky back­ground, we use an average ffor 
all three color channels to decrease the brightness but keep the color unchanged.  5.2 Environment Radiance 
Model Predicting radiance models for the sun and sky is not enough, be­cause the building also receives 
light re.ected from other surfaces in the environment. Predicting the environment radiance map at a novel 
time is a challenging problem, and it may appear that the only solution would be to completely geometrically 
model the rest of the environment and then solve a global illumination problem to render the entire scene. 
However we have found an acceptable approxima­tion for our purposes by a much simpler technique. The 
idea is to recover not the detailed geometric structure of the environment, but rather a very crude, 
low frequency model adequate enough for our purpose obtaining an approximation to the illumi­nation 
resulting from it on the primary architectural piece of interest. We use the technique of photometric 
stereo for shape-from­shading in computer vision [25] to recover the average re.ectance, assumed lambertian, 
and surface normal for each region of the en­vironment. One can solve for the albedo and normal orientation 
at each pixel location in an overdetermined system by taking multiple images of the same object with 
the same camera position but differ­ent positions of the single light source. In our context, the different 
positions of the light source are generated by the movement of the sun during the day. Interre.ections 
within the environment are ne­glected. The Lambertian model appears reasonable because most surfaces 
in an outdoor scene are pretty diffuse. The big change is that we do not have a single light source, 
but must consider both the sky and the sun as light sources. Considering the sky as an ambient light 
source and the sun as a directional light source, we have the following formulation  skysun pEsky+pEsun(nenvlsun) 
Ienv =;if nenvlsun?0;(16) pskyEsky ;otherwise: where pskyis the pseudo-albedo corresponding to the spectral 
dis­tribution of the sky, Eskyis the magnitude of the total .ux from sun the sky because we consider 
the sky as an ambient source, pis the pseudo-albedo corresponding to the spectral distribution of the 
sun, Esunis the irradiance from the sun, nenvis the normal of the considered region and lsunis the solar 
position. The reason why skysun we allow pand pto be independent because they are related to pseudo-BRDF 
s corresponding to the spectral distributions of the sky and the sun and we expect them to be very different. 
skysun Since we have three color channels, both pand phave three components, and nenvhas two degrees 
of freedom because it has unit length. There are eight unknowns for each environment region. If we photograph 
the environment for at least three solar po­sitions and get the corresponding sky .ux values, we would 
have at least nine equations at each environment region and the unknowns can be estimated by weighted 
least-square method. Note that the tra­jectory of the sun seen from the surface of the earth is not a 
planar curve, otherwise the three solar positions would not give us inde­pendent information. The estimated 
pseudo-albedos and normal can then be used to predict radiance under new lighting conditions. How can 
we impose the constraint that nenvhas unit length ? We could just add a penalty term in (16) to do this. 
However we .nd, sun among the six variables in pand nenv, there are only .ve degrees of freedom. We 
can just set one of them to be a constant to impose the constraint more strictly. Any component of nenvcan 
be either sun zero or nonzero. It is more appropriate to set one component of p to be a positive constant, 
say 0.1. Now nenvdoes not necessarily have unit length. (16) should be rewritten in the following way 
8 nenv skyEskysunknenv knenvkIenv = ;if nenvlsun?0; {p+(pk)Esun(lsun) : pskyEsky ;otherwise: (17) We 
use the Levenberg-Marquardt method to solve this nonlinear least-squares problem. However this technique 
requires the objec­tive function to have a derivative everywhere while our formulation above does not 
have one when nenvlsun =0.One way to get around this is to reformulate (17) as follows 8 nenv {pskyEsky+(p 
sunknenvk)Esun(lsun) knenvk;if nenvlsun?0;Ienv = { pskyEsky+(p sunknenvk)Esun :1l fl[exp(nenvlsun),1]g;otherwise: 
knenvk (18) where acan be any large positive constant, say 1000. We can check that (18) has derivative 
everywhere and its second term keeps very close to zero when nenvlsun<0, which is a good approximation 
to (17). Levenberg-Marquardt method can be easily used to minimize the least-square error criterion for 
(18). The start point of pskyis set to the ratio between the average radiance and the average magnitude 
of incident .ux from the sky, and the start point sun of pis set to the ratio between the average radiance 
and the aver­age irradiance from the sun. We may obtain meaningless values for the normal if some region 
is never lit by the sun. To alleviate this problem, during the optimization, if the data .tting error 
at some region is larger than a threshold and the obtained normal is point­ing away from the building, 
we remove the solar term in the above model and only try to get an estimation for psky. Adding a smooth­ing 
term between adjacent regions may also help. Some recovered environment radiance maps with the above 
modeling method are given in Figure 9(e)-(h). For every region of the environment, we have eight unknowns 
in the model and twelve equations obtained from four different times of day. Since the system is overdeter­mined, 
the good .t in Figure 9 provides justi.cation for our sim­plifying assumptions that the environment is 
Lambertian and that interre.ections within the environment can be neglected.  6 RESULTS We chose the 
Berkeley bell tower(Campanile) as our target archi­tecture and took a total of about 100 photographs 
for the tower, the sky and the landscape at four different times. These photographs are used as source 
in data-.tting. They can be considered as train­ing data. From the various measurements and recovered 
models, we found the relative importance of each illumination component and re.ectance component in our 
example. On shaded sides of the tower, the irradiance from both the sky and landscape has the same order 
of magnitude, but the irradiance from the landscape is larger. On sunlit sides, the sun dominates the 
illumination if its incident an­gle is not too large. The percentage varies with different color chan­nels. 
If the incident angle is less than 60 degrees, the light from the sun may exceed 90% in the red channel, 
and 60% in the blue chan­nel. As to re.ectance models, the ratio between the maximum spec­ular re.ectance 
and the diffuse re.ectance is about 1 : 18. So we only kept the specular re.ection from the sun and ignored 
the rest of the light sources to speed up re-rendering. We also took photographs at a .fth time. Those 
photographs are used for comparison with re-rendered images. They can be consid­ered as testing data. 
Relative positions and orientations of the cameras are currently calibrated by using the FACADE system 
in [4]. Alternatively, we could use any standard mosaicing technique for the environment photographs. 
Exterior orientation is calibrated with a compass map or the solar position. Photometric calibration 
of the camera is done using the technique in [3]. Once we have recovered the nonlinear mapping between 
in­cident radiance and camera output, we can use it to further recover the radiance at each pixel. To 
extend the dynamic range, it is neces­sary to take photographs at different shutter speeds. The technique 
from [3] enables the combined use of these to recover a high dy­namic range radiance image. All subsequent 
processing in the sys­tem uses radiance values. At the end, re-rendered radiance images are converted 
back to normal images using the nonlinear response curve of the sensor. 6.1 Comparison With Ground Truth 
Our approach makes a number of simplifying assumptions and ap­proximations. It is therefore necessary 
to check the accuracy of our re-rendering by rendering the bell tower at the .fth time and compar­ing 
the synthetic images with real photographs shot at the same time. Three pairs of images from three different 
viewpoints are shown in Figure 10. The sky in the synthetic images are obtained by clear sky interpolation 
introduced in Section 5.1. 6.2 Sunrise To Sunset Simulation A sequence of images are shown in Figure 
11. It includes im­ages at sunrise and sunset simulated with the technique in Section 5.1. Images rendered 
for sunrise and sunset can only be consid­ered as approximations to real photographs because the solar 
spec­trum changes at these periods, but we still use previously recovered pseudo-BRDF s. However, these 
approximations look realistic. 6.3 Intermediate And Overcast Sky Simulation By intermediate and overcast 
skies, we mean there is a uniform layer of clouds covering the sky which blocks some or all of the sunlight. 
To simulate this kind of sky, we can either get a overcast sky model by data .tting or use CIE standard 
overcast sky luminance model along with a user-speci.ed color for the clouds which is usually close to 
gray. A coef.cient specifying the percentage of the sun­light blocked by the clouds should also be given. 
Then the color at a point in the sky is simply a linear interpolation between the color of a clear sky 
and the color of the overcast sky. Actually some sky luminance models reviewed in [13] really use this 
kind of interpo­lation between two extreme sky models. A sequence of images are shown in Figure 12. It 
gives re­rendering results with various sky interpolation coef.cients. The above simulation sequences 
may be found in the SIG-GRAPH video tape. 6.4 High Resolution Re-Rendering Since we used a .xed size 
grid on each triangular patch to capture the spatial variation of surface re.ectance, as the viewpoint 
moves suf.ciently close to the surface of the object, each grid cell will cor­respond to multiple image 
pixels. The resulting rendering then takes on a somewhat blurred appearance, as variation in surface 
texture at a resolution .ner than the grid size is lost. In this section we show results from a simple 
technique by which the resolution can be boosted to the pixel resolution. The basic idea is to use a 
high resolu­tion zoom photograph of the architecture available as a texture map in the right way. Since 
the lighting conditions can be different, we need pixel wise re.ectance values. Let I(x;y)be the radiance 
measured from the high-resolution photograph at pixel (x;y)and p(x;y),and E(x;y)be the corresponding 
high-resolution pseudo­albedo and irradiance at the surface point corresponding to pixel (x;y). I(x;y), 
p~(x;y)and E(x;y)are the corresponding low­ ~~ resolution versions. Both p(x;y)and E(x;y)are unknown, 
but we can exploit the fact that the spatial variation in lighting E(x;y)has only low frequency components, 
and therefore is quite well approx­ ~ imated by E(x;y). We can obtain p~(x;y)from previously recov­~ 
ered pseudo-albedo at surface grid points, and I(x;y)by smooth­~ I(x;y) ing I(x;y);then E(x;y)= ~and 
the high resolution pseudo­ p~(x;y)  (a) (b) (c) (d) (e) (f) the synthetic images. albedo p(x;y)can 
be estimated by I(x;y)I(x;y) p(x;y)= . (19) E(x;y)E~(x;y) The recovered high-resolution pseudo-albedo 
p(x;y)can be used for re-rendering under novel lighting conditions. In Figure 13, we give a resulting 
image from this kind of re-rendering. A low­resolution image from previously recovered re.ectance is 
also given for comparison. By taking zoom-in photographs at various camera positions, we can combine 
this technique with view-dependent tex­ture mapping [4].  7 CONCLUSIONS AND FUTURE WORK In this paper, 
we proposed a method to extend image-based model­ing and rendering techniques to deal with producing 
renderings un­der novel lighting conditions. The input to the process is a small number of photographs 
of the architectural scene, at a few different times of day, taken using a handheld camera. These photographs 
are used to recover underlying radiance and re.ectance models, which are subsequently used for producing 
re-renderings of the scene un­der novel illumination conditions. As part of this process, we introduced 
the pseudo-BRDF concept. We recovered two sets of pseudo-BRDF s for re-rendering under daylight. This 
approach is reasonable so long as the spectral distri­bution of the sunlight and skylight doesn t change 
too signi.cantly. Extending the approach to work under more extreme conditions is left for further investigation. 
For more complex situations, such as a cluster of buildings, our approach can still work if we have the 
geometric models of these buildings and recover the re.ectance of the buildings one by one in a sequential 
mode. We need some new techniques if we want to re­cover their re.ectance simultaneously. Acknowledgments 
This research was supported by a Multidisciplinary University Research Initiative on three dimensional 
direct visualization from ONR and BMDO, grant FDN00014-96-1-1200, the California MI-CRO program and Philips 
Corporation. The authors wish to thank Paul E. Debevec and George Borshukov for providing the bell tower 
models, Charles Ying for helping make the video sequences, David Forsyth, Gregory Ward Larson, Carlo 
Sequin, Charles Benton and our reviewers for their valuable comments during the preparation of this paper. 
  References [1] BRUNGER,A., AND HOOPER, F. Anisotropic sky radiance model based on narrow .eld of view 
measurements of short­wave radiance. Solar Energy 51, 1 (1993), 53 64. [2] DANA,K., VAN GINNEKEN,B., 
NAYAR, S., AND KOEN-DERINK, J. Re.ectance and texture of real-world surfaces. In proceedings of CVPR 
(1997), pp. 151 157. [3] DEBEVEC,P., AND MALIK, J. Recovering high dynamic range radiance maps from photographs. 
In Computer Graph­ics Proceedings, Annual Conference Series (1997), pp. 369 378. [4] DEBEVEC,P., TAYLOR,C., 
AND MALIK, J. Modeling and rendering architecture from photographs: A hybrid geometry­and image-based 
approach. In Computer Graphics Proceed­ings, Annual Conference Series (1996), pp. 11 20. [5] DEBEVEC,P.,YU,Y., 
ANDBORSHUKOV,G.Ef.cientview­dependent image-based rendering with projective texture­mapping. UC Berkeley 
technical report #UCB//CSD-98­1003. [6] GORTLER, S., GRZESZCZUK,R., SZELISKI,R., AND CO-HEN, M. The lumigraph. 
In Computer Graphics Proceedings, Annual Conference Series (1996), pp. 43 54.  (a) (b) (c) (d) (e) 
(a) 7am, (b) 1pm, (c) 4pm, (d) 6pm, (e) 6:30pm.  (a) (b) (c) (d) Figure 12: Four synthetic images of 
a bell tower with shutter duration 1/1500 a second under an overcast sky with different percentages of 
blocked sunlight(PBS). (a) PBS=0.0, (b) PBS=0.5, (c) PBS=0.9, (d) PBS=0.95. [7] HAMPEL,F., ROUSSEEUW,P., 
RONCHETTI,E., AND STA-HEL,W. Robust Statistics. John Wiley &#38; Sons, New York, 1986. [8] INEICHEN,P., 
MOLINEAUX,B., AND PEREZ,R. Sky lumi­nance data validation: Comparison of seven models with four data 
banks. Solar Energy 52, 4 (1994), 337 346. [9] KLASSEN, R. Modeling the effect of the atmosphere on light. 
ACM Transactions on Graphics 6, 3 (1987), 215 237. [10] KOENDERINK,J., AND VAN DOORN, A. Illuminance 
texture due to surface mesostructure. J. Opt. Soc. Am.A 13, 3 (1996), 452 463. [11] LAFORTUNE,E., FOO, 
S., TORRANCE,K., AND GREEN-BERG, D. Non-linear approximation of re.ectance functions. In Computer Graphics 
Proceedings, Annual Conference Se­ries (1997), pp. 117 126. [12] LEVOY,M., AND HANRAHAN, P. Light .eld 
rendering. In Computer Graphics Proceedings, Annual Conference Series (1996), pp. 31 42. [13] LITTLEFAIR, 
P. A comparison of sky luminance models with measured data from garston, united kingdom. Solar Energy 
53, 4 (1994), 315 322. [14] MCMILLAN,L., AND BISHOP, G. Plenoptic modeling: An image-based rendering 
system. In Computer Graphics Pro­ceedings, Annual Conference Series (1995), pp. 39 46. [15] NISHITA,T., 
AND NAKAMAE, E. Continuous tone represen­tation of three-dimensional objects illuminated by sky light. 
Computer Graphics 20, 4 (1986), 125 132. [16] PEREZ,R., SEALS,R., AND MICHALSKY, J. All-weather model 
for sky luminance distribution preliminary con.gura­tion and validation. Solar Energy 50, 3 (1993), 235 
245. [17] PRESS,W.,FLANNERY,B.,TEUKOLSKY,S., ANDVETTER-LING,W. Numerical Recipes in C. Cambridge Univ. 
Press, New York, 1988. [18] REES,W. Physical Principles of Remote Sensing. Cambridge Univ. Press, 1990. 
[19] SATO,Y., ANDIKEUCHI,K.Re.ectanceanalysisfor3dcom­puter graphics model generation. Graphical Models 
and Im­age Processing 58, 5 (1996), 437 451. [20] SATO,Y., WHEELER,M., AND IKEUCHI, K. Object shape and 
re.ectance modeling from observation. In Computer Graphics Proceedings, Annual Conference Series (1997), 
pp. 379 388. [21] SZELISKI,R., AND SHUM, H. Creating full view panoramic mosaics and environment maps. 
In Computer Graphics Pro­ceedings, Annual Conference Series (1997), pp. 251 258. [22] TADAMURA,K., NAKAMAE,E., 
KANEDA,K., BABA,M., YAMASHITA,H., AND NISHITA,T. Modelingofskylightand rendering of outdoor scenes. Proceedings 
of EUROGRAPH­ICS 93, Computer Graphics Forum 12, 3 (1993), 189 200. [23] TAKAGI,A., TAKAOKA,H., OSHIMA,T., 
AND OGATA,Y. Accurate rendering technique based on colorimetric concep­tion. Computer Graphics 24, 4 
(1990), 1990. [24] WARD, G. Measuring and modeling anisotropic re.ection. Computer Graphics 26, 2 (1992), 
265 272. [25] WOODHAM, R. Photometric method for determining surface orientation from multiple images. 
In Shape from Shading, B. Horn and M. Brooks, Eds. MIT Press, 1989, pp. 513 532. A IRRADIANCE CALCULATION 
We designed an ef.cient algorithm for gathering light from the sky based on adaptive subdivision. Since 
irradiance is an integration of the incident radiance, it varies slowly over the surface of the archi­tecture. 
Thus we assume the irradiance over a triangular patch is a constant. For each triangle, we only gather 
the light at its centroid, and the centroid can always be handled as the effective center of the sky 
dome hemisphere because of the dome s very large radius. Each triangle de.nes a plane and only the part 
of the sky which is on the correct side of this plane, can be seen by the triangle. Further, there might 
be other faces in front of the triangle occluding part of the sky. So clipping the sky is necessary. 
The algorithm is summarized as follows Give each original polygon in the architecture model an id number; 
for each triangle, set its centroid as the viewpoint, Z­buffer the polygons with their id numbers as 
their color, scan the color buffer to retrieve the polygons in front of the current triangle.  Discretize 
the sky hemisphere into a small set of large rect­angular spherical polygons. For each triangle, use 
its tan­gent plane and those occluding polygons to clip these spher­ical polygons. As a result, we get 
back a list of visible spheri­cal polygons. Subdivide these spherical polygons until the sky radiance 
over each of them is almost uniform. The sky vector .ux is the summation of the .ux vectors of these 
subdivided sky patches. Finally, the irradiance from the sky is the inner product between the sky vector 
.ux and the local surface nor­mal.  The vector .ux of a sky patch gives the direction and magnitude 
of the .ux of that sky patch [10]. This algorithm is ef.cient because we only do visibility clipping 
on the initial small set of spherical poly­gons. This does not affect the accuracy because we do adaptive 
sub­division after the clipping. The vector .ux of a spherical triangle with uniform unit radiance can 
be obtained using a formula from [10]. We can assume the sky hemisphere has unit radius and its center 
is Obecause the irradiance from the sky is determined by its solid angle which is .xed no matter how 
large the radius is. Let A;B;Cbe three vertices on the sphere, LABbe the length of the arc on the great 
circle passing through A ** and B, IABbe normalized ,(OAxOB). Then the vector .ux of the spherical triangle 
ABCis LABLBCLCA F(4ABC)=IAB+IBC+ICA:(20) 222 This formula can be easily generalized to compute the vector 
.ux of any kind of spherical polygons. Clipping a spherical polygon with a planar polygon can be done 
by connecting its vertices with straight line segments and treating it as a planar polygon. The only 
thing we need to remedy after clip­ping is pushing back onto the sphere every new vertex generated by 
clipping. We calculate the irradiance from the environment in the same way except that we do not subdivide 
each environment region adap­tively. We only have a constant radiance value over each region and adaptive 
subdivision will not help improve the accuracy here. B VISIBILITY PREPROCESSING We need to decide in 
which photographs a particular triangular patch from the model is visible. If a triangle is partially 
visible in a pho­tograph, we should clip it so that each resulting triangle is either to­tally visible 
or totally invisible. The reason to do this is to correctly and ef.ciently assign radiance values from 
the photographs to the visible triangles. This preprocessing operates in both image space and object 
space. It is outlined as follows. Clip the triangles against all image boundaries so that any re­sulting 
triangle is either totally inside an image or totally out­side the image.  Set each camera position 
as the viewpoint in turn, Z-buffer the original large polygons from the geometric model using their id 
numbers as their colors.  At each camera position, scan-convert each triangle so we can know which pixels 
are covered by it. If at some covered pixel location, the retrieved polygon id from the color buffer 
is dif­ferent from the current polygon id, we .nd an occluding poly­gon.  Clip each triangle with its 
list of occluders in the object space.  Associate with each triangle a list of photographs to which 
it is totally visible.  Clipping in object-space takes very little time and the per­formance of this 
algorithm is almost determined by the scan­conversion part because we use the original large polygons 
in Z­buffering, which results in a very small set of occluding polygons for each triangle. So this algorithm 
has nearly the speed of image-space algorithms and the accuracy of object-space algorithms as long as 
the original polygons in the model are all larger than a pixel. This is a modi.ed version of the visibility 
algorithm presented in [5]. (a) (b) (c) Figure 13: (a) A re-rendered zoom-in image with shutter duration 
1/500 a second with the sun behind the bell tower using the previ­ously recovered surface pseudo-BRDF 
s, (b) a reference photograph at the same viewpoint, but with a different solar position, (c) a syn­thetic 
image with the same illumination and shutter speed as in (a), but with higher resolution, rendered using 
the view-dependent re­rendering technique. It uses both the reference photograph and the previously recovered 
low-resolution surface pseudo-BRDF s.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280878</article_id>
		<sort_key>219</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>22</seq_no>
		<title><![CDATA[Visibility sorting and compositing without splitting for image layer decompositions]]></title>
		<page_from>219</page_from>
		<page_to>230</page_to>
		<doi_number>10.1145/280814.280878</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280878</url>
		<keywords>
			<kw><![CDATA[compositing]]></kw>
			<kw><![CDATA[kd-tree]]></kw>
			<kw><![CDATA[nonsplitting layered decomposition]]></kw>
			<kw><![CDATA[occlusion cycle]]></kw>
			<kw><![CDATA[occlusion graph]]></kw>
			<kw><![CDATA[sprite]]></kw>
			<kw><![CDATA[visibility sorting]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.2.2</cat_node>
				<descriptor>Graph algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Hidden line/surface removal</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003624.10003633.10010917</concept_id>
				<concept_desc>CCS->Mathematics of computing->Discrete mathematics->Graph theory->Graph algorithms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39079346</person_id>
				<author_profile_id><![CDATA[81100167784]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Snyder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P137139</person_id>
				<author_profile_id><![CDATA[81100361368]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jed]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lengyel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>806007</ref_obj_id>
				<ref_obj_pid>800196</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Appel A., "The Notion of Quantitative Invisibility and the Machine Rendering of Solids," In P1vceedings of the ACMNational Conference, pp. 387-393, 1967.]]></ref_text>
				<ref_id>Appel67</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97881</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Baraff, David, "Curved Surfaces and Coherence for Non-Penetrating Rigid Body Simulation," Siggraph '90, August 1990, pp. 19-28.]]></ref_text>
				<ref_id>Baraff90</ref_id>
			</ref>
			<ref>
				<ref_obj_id>361007</ref_obj_id>
				<ref_obj_pid>361002</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bentley, J.L., "Multidimensional Binary Search Trees Used for Associative Searching," Communications of the ACM, 18(1975), pp. 509-517.]]></ref_text>
				<ref_id>Bentley75</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237204</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Chen, Han-Ming, and Wen-Teng Wang, "The Feudal Priority Algorithm on Hidden-Surface Removal," Siggraph '96, August 1996, pp. 55-64.]]></ref_text>
				<ref_id>Chen96</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Chung, Kelvin, and Wenping Wang, "Quick Collision Detection of Polytopes in Virtual Environments," ACM Symposium on Virtual Reality Software and Technology 1996, July 1996, pp. 1-4.]]></ref_text>
				<ref_id>Chung96a</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Chung, Tat Leung (Kelvin), "An Efficient Collision Detection Algorithm for Polytopes in Virtual Environments," M. Phil Thesis at the University of Hong Kong, 1996 {www.cs.hku.hk/tlchung/collision_library.html}.]]></ref_text>
				<ref_id>Chung96b</ref_id>
			</ref>
			<ref>
				<ref_obj_id>199437</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Cohen, D.J., M.C. Lin, D. Manocha, and M. Ponamgi, "I-Collide: An Interactive and Exact Collision Detection System for Large-Scale Environments," P1vceedings of the Symposium on Interactive 3D Graphics,, 1995, pp. 189-196.]]></ref_text>
				<ref_id>Cohen95</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808590</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Cook, Robert, "Distributed Ray Tracing," Siggraph '84, July 1984, pp. 137-145.]]></ref_text>
				<ref_id>Cook84</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258785</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Durand, Fredo, George Drettakis, and Claude Puech, "The Visibility Skeleton: A Powerful and Efficient Multi-Purpose Global Visibility Tool," Siggraph '97, August 1997, pp. 89-100.]]></ref_text>
				<ref_id>Durand97</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807481</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Fuchs, H., Z.M. Kedem, and B.F. Naylor, "On Visible Surface Generation by A Priori Tree Structures," Siggraph '80, July 1980, pp. 124-133.]]></ref_text>
				<ref_id>Fuchs80</ref_id>
			</ref>
			<ref>
				<ref_obj_id>147158</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Funkhouser, T.A., C.H. Sequin, and S.J. Teller, "Management of Large Amounts of Data in Interactive Building Walkthroughs," Proceedings of 1992 Symposium on Interactive 3D Graphics, July 1991, pp. 11-20.]]></ref_text>
				<ref_id>Funkhouser92</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Gilbert, Elmer G., Daniel W. Johnson, and S. Sathiya A. Keerthi, "A Fast Procedure for Computing the Distance Between Complex Objects in Three- Dimensional Space," IEEE Journal of Robotics and Automation, 4(2), April 1988, pp. 193-203.]]></ref_text>
				<ref_id>Gilbert88</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166147</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Greene, N., M. Kass, and G. Miller, "Hierarchical Z-buffer Visibility," Siggraph '93, August 1993, pp. 231-238.]]></ref_text>
				<ref_id>Greene93</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97913</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Haeberli, Paul, and Kurt Akeley, "The Accumulation Buffer: Hardware Support for High-Quality Rendering," Siggraph '90, August 1990, pp. 309-318.]]></ref_text>
				<ref_id>Haeberli90</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15916</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Kay, Tim, and J. Kajiya, "Ray Tracing Complex Scenes," Siggraph '86, August 1986, pp. 269-278.]]></ref_text>
				<ref_id>Kay86</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258856</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Lengyel, Jed, and John Snyder, "Rendering with Coherent Layers," Siggraph '97, August 1997, pp. 233-242.]]></ref_text>
				<ref_id>Lengyel97</ref_id>
			</ref>
			<ref>
				<ref_obj_id>199420</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Maciel, Paolo W.C. and Peter Shirley, "Visual Navigation of Large Environments Using Textured Clusters," Proceedings 1995 Symposium on Interactive 3D Graphics, April 1995, pp. 95-102.]]></ref_text>
				<ref_id>Maciel95</ref_id>
			</ref>
			<ref>
				<ref_obj_id>253292</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Mark, William R., Leonard McMillan, and Gary Bishop, "Post-Rendering 3D Warping," Proceedings 1997 Symposium on Interactive 3D Graphics, April 1997, pp. 7-16.]]></ref_text>
				<ref_id>Mark97</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258894</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Markosian, Lee, M.A. Kowalski, S.J. Trychin, L.D. Bourdev, D. Goodstein, and J.F. Hughes, "Real-Time Nonphotorealistic Rendering," Siggraph '97, August 1997, pp. 415-420.]]></ref_text>
				<ref_id>Markosian97</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325188</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Max, Nelson, and Douglas Lerner, "A Two-and-a-Half-D Motion-Blur Algorithm," Siggraph '85, July 1985, pp. 85-93.]]></ref_text>
				<ref_id>Max85</ref_id>
			</ref>
			<ref>
				<ref_obj_id>27627</ref_obj_id>
				<ref_obj_pid>27625</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[McKenna, M., "Worst-Case Optimal Hidden Surface Removal," ACM Transactions on Graphics, 1987, 6, pp. 19-28.]]></ref_text>
				<ref_id>McKenna87</ref_id>
			</ref>
			<ref>
				<ref_obj_id>2322744</ref_obj_id>
				<ref_obj_pid>2322483</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Ming-Chieh Lee, Wei-ge Chen, Chih-lung Bruce Lin, Chunag Gu, Tomislav Markoc, Steven I. Zabinsky, and Richard Szeliski, "A Layered Video Object Coding System Using Sprite and Affine Motion Model," IEEE Transactions on Circuits and Systems for Video Technology, 7(1), February 1997, pp. 130-145.]]></ref_text>
				<ref_id>Ming97</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134067</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Molnar, Steve, John Eyles, and John Poulton, "PixelFlow: High-Speed Rendering Using Image Compositing," Siggraph '92, August 1992, pp. 231-140.]]></ref_text>
				<ref_id>Molnar92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74372</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Mulmuley, K., "An Efficient Algorithm for Hidden Surface Removal," Siggraph '89, July 1989, pp. 379-388.]]></ref_text>
				<ref_id>Mulmuley89</ref_id>
			</ref>
			<ref>
				<ref_obj_id>155318</ref_obj_id>
				<ref_obj_pid>155294</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Naylor, B.F., "Partitioning Tree Image Representation and Generation from 3D Geometric Models," P1vceedings of Graphics Intelface '92, May 1992, pp. 201-212.]]></ref_text>
				<ref_id>Naylor92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>569954</ref_obj_id>
				<ref_obj_pid>800193</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Newell, M. E., R. G. Newell, and T. L. Sancha, "A Solution to the Hidden Surface Problem," P1vc. ACM National Conf., 1972.]]></ref_text>
				<ref_id>Newell72</ref_id>
			</ref>
			<ref>
				<ref_obj_id>614362</ref_obj_id>
				<ref_obj_pid>614265</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Ponamgi, Madhav K., Dinesh Manocha, and Ming C. Lin, "Incremental Algorithms for Collision Detection between Polygonal Models," IEEE Transactions on Visualization and Computer Graphics, 3(1), March 1997, pp 51-64.]]></ref_text>
				<ref_id>Ponamgi97</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808606</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Porter, Thomas, and Tom Duff, "Compositing Digital Images," Siggraph '84, July 1984, pp. 253-258.]]></ref_text>
				<ref_id>Porter84</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806818</ref_obj_id>
				<ref_obj_pid>800224</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Potmesil, Michael, and Indranil Chakravarty, "A Lens and Aperture Camera Model for Synthetic Image Generation," Siggraph '81, August 1981, pp. 389-399.]]></ref_text>
				<ref_id>Potmesil81</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801169</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Potmesil, Michael, and Indranil Chakravarty, "Modeling Motion Blur in Computer-Generated Images," Siggraph '83, July 1983, pp. 389-399.]]></ref_text>
				<ref_id>Potmesil83</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192192</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Regan, Matthew, and Ronald Pose, "Priority Rendering with a Virtual Address Recalculation Pipeline," Siggraph '94, August 1994, pp. 155-162.]]></ref_text>
				<ref_id>Regan94</ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Rokita, Przemyslaw, "Fast Generation of Depth of Field Effects in Computer Graphics," Computers and Graphics, 17(5), 1993, pp. 593-595.]]></ref_text>
				<ref_id>Rokita93</ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Schaufler, Gernot, and Wolfgang Stfirzlinger, "A Three Dimensional Image Cache for Virtual Reality," P~vceedings of Emvgraphics '96, August 1996, pp. 227-235.]]></ref_text>
				<ref_id>Schaufler96</ref_id>
			</ref>
			<ref>
				<ref_obj_id>731976</ref_obj_id>
				<ref_obj_pid>647651</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Schaufler, Gernot, "Nailboards: A Rendering Primitive for Image Caching in Dynamic Scenes," in P~vceedings of the 8th Emvgraphics Workshop on Rendering '97, St. Etienne, France, June 16-18, 1997, pp. 151-162.]]></ref_text>
				<ref_id>Schaufler97</ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Schumacker, R.A., B. Brand, M. Gilliland, and W. Sharp, "Study for Applying Computer-Generated Images to Visual Simulation," AFHRL-TR- 69-14, U.S. Air Force Human Resources Laboratory, Sept. 1969.]]></ref_text>
				<ref_id>Schumacker69</ref_id>
			</ref>
			<ref>
				<ref_obj_id>42466</ref_obj_id>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Sedgewick, Robert, Algorithms, Addison-Wesley, Reading, MA, 1983.]]></ref_text>
				<ref_id>Sedgewick83</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237209</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Shade, Jonathan, Dani Lischinski, David H. Salesin, Tony DeRose, and John Snyder, "Hierarchical Image Caching for Accelerated Walkthroughs of Complex Environments," Siggraph '96, August 1996, pp. 75-82.]]></ref_text>
				<ref_id>Shade96</ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Sillion, Francois, George Drettakis, and Benoit Bodelet, "Efficient Impostor Manipulation for Real-Time Visualization of Urban Scenery," P~vceedings of Emvgraphics '97, Sept 1997, pp. 207-218.]]></ref_text>
				<ref_id>Sillion97</ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Snyder, John, and Jed Lengyel, "Visibility Sorting and Compositing for Image-Based Rendering," Microsoft Technical Report, MSR-TR-97-11, April 1997.]]></ref_text>
				<ref_id>Snyder97</ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Snyder, John, Jed Lengyel, and Jim Blinn, "Resolving Non-Binary Cyclic Occlusions with Image Compositing," Microsoft Technical Report, MSR- TR-98-05, March 1998.]]></ref_text>
				<ref_id>Snyder98</ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[Sudarsky, Oded, and Craig Gotsman, "Output-Sensitive Visibility Algorithms for Dynamic Scenes with Applications to Virtual Reality," Computer Graphics Forum, 15(3), P1vceedings of Emvgraphics '96, pp. 249-258.]]></ref_text>
				<ref_id>Sudarsky96</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356626</ref_obj_id>
				<ref_obj_pid>356625</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[Sutherland, Ivan E., Robert F. Sproull, and Robert A. Schumacker, "A Characterization of Ten Hidden-Surthce Algorithms," Computing Surveys, 6(1), March 1974, pp. 293-347.]]></ref_text>
				<ref_id>Sutherland74</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122725</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[Teller, Seth, and C.H. Sequin, "Visibility Preprocessing for Interactive Walkthroughs," Siggraph ' 91, July 1991, pp. 61 - 19.]]></ref_text>
				<ref_id>Teller91</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166148</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[Teller, Seth, and R Hanrahan, "Global Visibility Algorithms for Illumination Computations," Siggraph '93, August 1993, pp. 239-246.]]></ref_text>
				<ref_id>Teller93</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237274</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[Torborg, Jay, and James T. Kajiya, "Talisman: Commodity Realtime 3D Graphics for the PC," Siggraph '96, August 1996, pp. 353-364.]]></ref_text>
				<ref_id>Torborg96</ref_id>
			</ref>
			<ref>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[Torres, E., "Optimization of the Binary Space Partition Algorithm (BSP) for the Visualization of Dynamic Scenes," Proceedings of Eurographics '90, Sept. 1990, pp. 507-518.]]></ref_text>
				<ref_id>Torres90</ref_id>
			</ref>
			<ref>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[Wang, J.Y.A., and E.H. Adelson, "Representing Moving Images with Layers," IEEE Trans. Image Processing, vol. 3, September 1994, pp. 625-638.]]></ref_text>
				<ref_id>Wang94</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258781</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[Zhang, Hansong, Dinesh Manocha, Thomas Hudson, and Kenneth Hoff III, "Visibility Culling Using Hierarchical Occlusion Maps," Siggraph '97, August 1997, pp. 77-88.]]></ref_text>
				<ref_id>Zhang97</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Copyright &#38;#169;1998 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers or to redistribute to lists, requires specific permission and/or a fee. Visibility Sorting 
and Compositing without Splitting for Image Layer Decompositions John Snyder and Jed Lengyel Microsoft 
Research Abstract We present an ef.cient algorithm for visibility sorting a set of moving geometric 
objects into a sequence of image layers which are composited to produce the .nal image. Instead of splitting 
the geometry as in previous visibility approaches, we detect mutual occluders and resolve them using 
an appropriate image compositing expression or merge them into a single layer. Such an algorithm has 
many applications in computer graphics; we demonstrate two: rendering acceleration using image interpolation 
and visibility-correct depth of .eld using image blurring. We propose a new, incremental method for identifying 
mutually oc­cluding sets of objects and computing a visibility sort among these sets. Occlusion queries 
are accelerated by testing on convex bounding hulls; less conservative tests are also discussed. Kd-trees 
formed by combinations of directions in object or image space provide an initial cull on potential occluders, 
and incremental collision detection algorithms are adapted to resolve pairwise occlusions, when necessary. 
Mutual occluders are further analyzed to generate an image compositing expression; in the case of non­binary 
occlusion cycles, an expression can always be generated without merging the objects into a single layer. 
Results demonstrate that the algo­rithm is practical for real-time animation of scenes involving hundreds 
of objects each comprising hundreds or thousands of polygons. CR Categories: I.3.3 [Computer Graphics]: 
Picture/Image Generation -Display algorithms. Additional Keywords: visibility sorting, compositing, nonsplitting 
lay­ered decomposition, occlusion cycle, occlusion graph, sprite, kd-tree. 1 Introduction This paper 
addresses the problem of how to ef.ciently sort dynamic geom­etry into image layers. Applications include: 
1. image-based rendering acceleration by using image warping tech­niques rather than re-rendering to 
approximate appearance, render­ing resources can be conserved [Shade96,Schau.er96,Torborg96, Lengyel97]. 
 2. image stream compression segmenting a synthetic image stream into visibility-sorted layers yields 
greater compression by exploiting the greater coherence present in the segmented layers [Wang94,Ming97]. 
 3. fast special effects generation effects such as motion blur and depth­of-.eld can be ef.ciently 
computed via image post-processing tech­niques [Potmesil81,Potmesil83,Max85,Rokita93]. Visibility sorting 
corrects errors due to the lack of information on occluded surfaces in [Potmesil81,Potmesil83,Rokita93] 
(see [Cook84] for a discussion of these errors), and uses a correct visibility sort instead of the simple 
depth sort proposed in [Max85].  Address: 1 Microsoft Way, Redmond WA 98052. Email: johnsny@microsoft.com, 
jedl@microsoft.com 4. animation playback with selective display/modi.cation by storing the image layers 
associated with each object and the image com­positing expression for these layers, layers may be selectively 
added, removed, or modi.ed for fast preview or interactive playback. Un­changed layers require no re-rendering. 
 5. incorporation of external image streams hand-drawn character an­imation, recorded video, or off-line 
rendered images can be inserted into a 3D animation using a geometric proxy which is ordered along with 
the 3D synthetic elements, but drawn using the 2D image stream. 6. rendering with transparency while 
standard z-buffers fail to properly render arbitrarily-ordered transparent objects, visibility sorting 
solves this problem provided the (possibly grouped) objects themselves can be properly rendered. 7. 
fast hidden-line rendering by factoring the geometry into sorted lay­ers, we reduce the hidden line 
problem [Appel67,Markosian97] into a set of much simpler problems. Rasterized hidden line renderings 
of occluding layers simply overwrite occluded layers beneath. 8. rendering without or with reduced z-buffer 
use the software visibility sort allows elimination or reduced use of hardware z-buffers. Z­buffer resolution 
can also be targeted to the extent of small groups of mutually occluding objects, rather than the whole 
scene s.  To understand the usefulness of visibility sorting, we brie.y focus on rendering acceleration. 
The goal is to render each coherent object at the ap­propriate spatial and temporal resolution and interpolate 
with image warps between renderings. Several approaches have been used to compose the set of object images. 
We use pure image sprites without z information, requir­ing software visibility sorting [Lengyel97]. 
Another approach caches im­ages with sampled (per-pixel) z information [Molnar92,Regan94,Mark97, Schau.er97], 
but incurs problems with antialiasing and depth uncover­ing (disocclusion). A third approach is to use 
texture-mapped geometric impostors like single quadrilaterals [Shade96,Schau.er96] or polygonal meshes 
[Maciel95,Sillion97]. Such approaches use complex 3D render­ing rather than simple 2D image transformations 
and require geometric impostors suitable for visibility determination, especially demanding for dynamic 
scenes. By separating visibility determination from appearance approximation, we exploit the simplest 
appearance representation (a 2D image without z) and warp (af.ne transformation), without sacri.cing 
cor­rect visibility results. In our approach, the content author identi.es geometry that forms the lowest 
level layers, called parts.Parts (e.g., a tree, car, space vehicle, or joint in an articulated .gure) 
contain many polygons and form a perceptual object or object component that is expected to have coherent 
motion. Very large continuous objects, like terrain, are apriori split into component ob­jects. At runtime, 
for every frame, the visibility relations between parts are incrementally analyzed to generate a sorted 
list of layers, each containing one or more parts, and an image compositing expression ([Porter84]) on 
these layers that produces the .nal image. We assume the renderer can correctly produce hidden-surface-eliminated 
images for each layer when necessary, regardless of whether the layer contains one or more parts. Once 
de.ned, our approach never splits parts at run-time as in BSP-tree or octree visibility algorithms; the 
parts are rendered alone or in groups. There are two reasons for this. First, real-time software visibility 
sorting is practical for hundreds of parts but not for the millions of polygons they Figure 1: Sorting 
without splitting: This con.guration can be visibility sorted for any viewpoint even though no non-splitting 
partitioning plane exists. contain. Second, splitting is undesirable and often unnecessary. In dynamic 
situations, the number of splits and their location in image space varies as the corresponding split 
object or other objects in its environment move. Not only is this a major computational burden, but it 
also destroys coherence, thereby reducing the reuse rate in image-based rendering acceleration or the 
compression ratio in a layered image stream. BSP and octree decompositions require global separating 
planes which induce unnecessary splitting, even though a global separating plane is not required for 
a valid visibility ordering (Figure 1). We use pairwise occlusion tests between convex hulls or unions 
of convex hulls around each part. Such visibility testing is conservative, since it .lls holes in objects 
and counts intersections as occlusions even when the intersection occurs only in the invisible (back-facing) 
part of one object. This compromise permits fast sorting and, in practice, does not cause undue occlusion 
cycle growth. Less conservative special cases can also be developed, such as between a sphere/cylinder 
joint (see Appendix B). Our algorithm always .nds a correct visibility sort if it exists, with respect 
to a pairwise occlusion test, or aggregates mutual occluders and sorts among the resulting groups. Moreover, 
we show that splitting is unnecessary even in the presence of occlusion cycles having no mutually occluding 
pairs (i.e.,no binary occlusion cycles). The main contribution of this work is the identi.cation of a 
new and useful problem in computer graphics, that of visibility sorting and occlu­sion cycle detection 
on dynamic, multi-polygon objects without splitting, and the description of a fast algorithm for its 
solution. We introduce the notion of an occlusion graph, which de.nes the layerability criterion using 
pairwise occlusion relations without introducing unnecessary global partitioning planes. We present a 
fast method for occlusion culling, and a hybrid incremental algorithm for performing occlusion testing 
on convex bounding polyhedra. We show how non-binary occlusion cycles can be dynamically handled without 
grouping the participating objects, by com­piling and evaluating an appropriate image compositing expression. 
We also show how binary occlusion cycles can be eliminated by pre-splitting geometry. Finally, we demonstrate 
the practicality of these ideas in sev­eral situations and applications. Visibility sorting of collections 
of several hundred parts can be computed at more than 60Hz on a PC. 2 Previous Work The problem of visibility 
has many guises. Recent work has con­sidered invisibility culling [Greene93,Zhang97], analytic hidden 
sur­face removal [McKenna87,Mulmuley89,Naylor92], and global visibility [Teller93,Durand97]. The problem 
we solve, production of a layered de­composition that yields the hidden-surface eliminated result, is 
an old problem in computer graphics of particular importance before hardware z­buffers became widely 
available [Schumacker69,Newell72,Sutherland74, Fuchs80]. In our approach, we do not wish to eliminate 
occluded surfaces, but to .nd the correct layering order, since occluded surfaces in the current frame 
might be revealed in the next. Unlike the early work, we handle dynamic, multi-polygon objects without 
splitting; we call this variant of the visibility problem non-splitting layered decomposition. Much previous 
work in visibility focuses on walkthroughs of static scenes, but a few do consider dynamic situations. 
[Sudarsky96] uses oc­trees for the invisibility culling problem, while [Torres90] uses dynamic BSP trees 
to compute a visibility ordering on all polygons in the scene. Neither technique treats the non-splitting 
layered decomposition problem, and the algorithms of [Torres90] remain impractical for real-time anima­tions. 
Visibility algorithms can not simply segregate the dynamic and static elements of the scene and process 
them independently. A dynamic object can form an occlusion cycle with static objects that were formerly 
orderable. Our algorithms detect such situations without expending much geometric con.guration occlusion 
graph A B C (a) A B C (b) (c)  Figure 2: Occlusion graphs: The .gure illustrates the occlusion graphs 
for some simple con.gurations. (a) and (b) are acyclic, while (c) contains a cycle. computation on static 
components. To accelerate occlusion testing, we use a spatial hierarchy (kd-tree) to or­ganize parts. 
Such structures (octrees, bintrees, kd-trees, and BSP trees) are a staple of computer graphics algorithms 
[Fuchs80,Teller91,Funkhouser92, Naylor92,Greene93,Sudarsky96,Shade96]. Our approach generalizes oc­trees 
[Greene93,Sudarsky96] and 3D kd-trees [Teller91,Funkhouser92, Shade96] in that it allows a .xed but arbitrarily 
chosen number of direc­tions in both object and image space. This allows maximum .exibility to tightly 
bound scenes with a few directions. Our hierarchy is also dynamic, allowing fast rebalancing, insertion, 
and deletion of objects. Collision and occlusion detection are similar. We use convex bound­ing volumes 
to accelerate occlusion testing, as in [Baraff90,Cohen95, Ponamgi97], and track extents with vertex descent 
on the convex poly­hedra, as in [Cohen95] (although this technique is generalized to angular, or image 
space, extent tracking as well as spatial). Still, occlusion detec­tion has several peculiarities, among 
them that an object A can occlude B even if they are nonintersecting, or in fact, very far apart. For 
this reason, the sweep and prune technique of [Cohen95,Ponamgi97] is inapplicable to occlusion detection. 
We instead use kd-trees that allow dynamic deactiva­tion of objects as the visibility sort proceeds. 
Pairwise collision of convex bodies can be applied to occlusion detection; we hybridize techniques from 
[Chung96a,Chung96b,Gilbert88]. The work of [Max85] deserves special mention as an early example of applying 
visibility sorting and image compositing to special effects generation. Our work develops the required 
sorting theory and algorithms. 3 Occlusion Graphs The central notion for our visibility sorting algorithms 
is the pairwise occlusion relation. We use the notation A !EB meaning object A occludes object B with 
respect to eye point E. Mathematically, this relation signi.es that there exists a ray emanating from 
E such that the ray intersects A and then B.1 It is useful notationally to make the dependence on the 
eye point implicit so that A !B means that A occludes B with respect to an implicit eye point. The arrow 
points to the object that is occluded. The set of occlusion relations between pairs of the n parts compris­ing 
the entire scene forms a directed graph, called the occlusion graph. This notion of occlusion graph is 
very similar to the priority graph of 1A de.nition for A !EB more suitable for closed objects but harder 
to compute is that a ray emanating from E hits a front face of A followed by a front face of B. E zB 
 Figure 3: Depth ordering does not indicate visibility ordering: While the mini­mum depth of object B 
is smaller than A s (zB <zA), A occludes B as seen from eye point E. Similarly, by placing E on the right 
side of the diagram, it can be seen that maximum depth ordering also fails to correspond to visibility 
ordering. [Schumacker69] but uses actual occlusion of the objects rather than the results of plane equation 
tests for pairwise separating planes chosen apri­ori (view independently). Figure 2 illustrates some 
example occlusion graphs. When the directed occlusion graph is acyclic, visibility sorting is equivalent 
to topological sorting of the occlusion graph, and produces a (front-to-back) ordering of the objects 
hO1;O2;:::;Onisuch that i <j implies Oj 6!Oi. Objects so ordered can thus be rendered with correct hidden 
surface elimination simply by using Painter s algorithm ;i.e.,by rendering On, followed by On,1, and 
so on until O1. Thus the .nal image, I, can be constructed by a sequence of over operations on the image 
layers of each of the objects: I =I1 over I2 over :::over In (1) where Ii is the shaped image of Oi, 
containing both color and cover­age/transparency information [Porter84]. Cycles in the occlusion graph 
mean that no visibility ordering exists (see Figure 2c). In this case, parts in the cycle are grouped 
together and analyzed further to generate an image compositing expression (Section 5). The resulting 
image for the cycle can then be composited in the chain of over operators as above. This notion of occlusion 
ignores the viewing direction; only the eye point matters. By taking account of visibility relationships 
all around the eye, the algorithm described here can respond to rapid shifts in view direction common 
in interactive settings and critical in VR applications [Regan94]. 4 Incremental Visibility Sorting 
Our algorithm for incremental visibility sorting and occlusion cycle detec­tion (IVS) is related to the 
Newell, Newell, and Sancha (NNS) algorithm for visibility ordering a set of polygons [Newell72,Sutherland74]. 
In brief, NNS sorts a set of polygons by furthest depth and tests whether the result­ing order is actually 
a visibility ordering. NNS traverses the depth-sorted list of polygons; if the next polygon does not 
overlap in depth with the re­maining polygons in the list, the polygon can be removed and placed in the 
ordered output. Otherwise, NNS examines the collection of polygons that overlap in depth using a series 
of occlusion tests of increasing complexity. If the polygon is not occluded by any of these overlapping 
polygons, it can be sent to the output; otherwise, it is marked and reinserted behind the overlapping 
polygons. When NNS encounters a marked polygon, a cyclic occlusion is indicated and NNS splits the polygon 
to remove the cycle. IVS differs from NNS in that it orders aggregate geometry composed of many polygons 
rather than individual polygons, and identi.es and groups occlusion cycles rather than splitting to remove 
them. Most important, IVS orders incrementally, based on the visibility ordering computed previously, 
rather than starting from an ordering based on depth. This fundamental change has both advantages and 
disadvantages. It is advantageous because depth sorting is an unreliable indicator of visibility order 
as shown in Figure 3. Applying the NNS algorithm to a coherently changing scene repeatedly computes the 
same object reorderings (with their attendant costly occlusion tests) to convert the initial depth sort 
to a visibility sort. The disadvantage is that the sort from the last invocation provides no restriction 
on the set of objects that can occlude a given object for the current invocation. The NNS depth sort, 
in contrast, has the useful IVS(L,G) [computes visibility sort] Input: ordering of non-grouped objects 
from previous invocation (L) Output: front-to-back ordering with cyclic elements grouped together (G) 
Algorithm: G +; unmark all elements of L while L is nonempty pop off top(L): A if A is unmarked if 
nothing else in L occludes A insert A onto G unmark everything in L else [reinsert A onto L] mark A 
.nd element in L occluding A furthest from top(L): FA reinsert A into L after FA endif else [A is marked] 
 form list ShA;L1;L2;:::;Lniwhere L1;:::;Ln are the largest consecutive sequence of marked elements, 
starting from top(L) if detect cycle(S) then [insert cycle as grouped object onto L] group cycle-forming 
elements of S into grouped object C delete all members of C from L insert C (unmarked) as top(L) else 
[reinsert A onto L] .nd element in L occluding A furthest from top(L): FA reinsert A into L after FA 
endif endif endwhile Figure 4: IVS algorithm. The top object, A, in the current ordering (list L)is 
examined for occluders. If nothing occludes A, it is inserted in the output list G. Otherwise, A is marked 
and reinserted behind the furthest object in the list that occludes it, FA. When a marked object is encountered, 
the sequence of consecutively marked objects starting at the top of the list is checked for an occlusion 
cycle using detect cycle. If an occlusion cycle is found, the participating objects are grouped and reinserted 
on top of L. This loop is repeated until L is empty; G then contains the sorted list of parts with mutual 
occluders grouped together. detect cycle (S) [.nds a cycle] Input: list of objects ShS1;S2;:::;Sni Output: 
determination of existence of a cycle and a list of cycle-forming objects, if a cycle is found. Algorithm: 
if n :1 return NO CYCLE i1 +1 for j =2 to n +1 if Sik occludes Sij,1 for k <j ,1 then cycle is hSik 
;:::;i ;Sik+1 Sij,1 return CYCLE else if no occluder of Sij,1 exists in S then return NO CYCLE else 
let Sk be an occluder of Sij,1 ij +k endif endfor Figure 5: Cycle detection algorithm used in IVS. 
This algorithm will .nd a cycle if any initial contiguous subsequence of 1 <m :n vertices hS1;S2;:::;Smiforms 
a cyclically-connected subgraph; i.e., a subgraph in which every part is occluded by at least one other 
member of the subgraph. For subgraphs which are not cyclically connected, the algorithm can fail to .nd 
existing cycles, but this is not necessary for the correctness of IVS (for example, consider the occlusion 
graph with three nodes A, B,and C where A !B !A and initial list hC;A;Bi).  LG comment ABC ;initial 
state BC A insert A onto G C AB insert B onto G ;ABC insert C onto G Figure 6: IVS Example 1: Each line 
shows the state of L and G after the next while loop iteration, using the graph of Figure 2(b) and initial 
ordering ABC. LG comment CBA ;initial state BC.A ;mark C and reinsert after B C.AB. ;mark B and reinsert 
after A AB.C. ;A unmarked, so reinsert C BC A insert A onto G, unmark everything C AB insert B onto G 
;ABC insert C onto G Figure 7: IVS Example 2: Using the graph of Figure 2(b), this time with initial 
ordering CBA. The notation P.is used to signify marking. The step from 3 to 4 reinserts C into L because 
there is an unmarked element, A, between C and the furthest element occluding it, B. LG comment ABC ;initial 
state BCA. ;mark A and reinsert CA.B. ;mark B and reinsert A.B.C. ;mark C and reinsert (ABC) ;group cycle 
;(ABC) insert (ABC) onto G Figure 8: IVS Example 3: Using the graph of Figure 2(c) with initial ordering 
ABC. The notation (P1;P2;:::;Pr) denotes grouping. property that an object Q further in the list from 
a given object P,and all objects after Q, can not occlude P if Q s min depth exceeds the max depth of 
P. Naively, IVS requires testing potentially all n objects to see if any occlude a given one, resulting 
in an O(n2) algorithm. Fortunately, we will see in the next section how the occlusion culling may be 
sped up using simple hierarchical techniques, actually improving upon NNS occlusion culling (see Section 
8). The IVS algorithm is presented in Figures 4 and 5. Mathematically, the IVS algorithm computes an 
incremental topological sort on the strongly connected components of the directed occlusion graph (see 
[Sedgewick83] for background on directed graphs, strongly connected components, and topological sort). 
A strongly connected component (SCC) in the occlusion graph is a set of mutually occluding objects, in 
that for any object pair A and B in the SCC, either A !B or there exist objects, X1;X2;:::;Xs also in 
the SCC such that A !X1 !X2 !:::!Xs !B: The IVS algorithm .nds the parts comprising each SCC, and optionally 
computes the occlusion subgraph of the members of each SCC to resolve nonbinary cycles without aggregating 
layers (Section 5). A series of example invocations of the IVS algorithm for some of the occlusion graphs 
in Figure 2 are presented in Figures 6, 7, and 8. A proof of correctness is contained in [Snyder97]. 
The IVS algorithm takes advantage of coherence in the visibility ordering from the previous frame. When 
a given object A is popped off the list, it is likely that few objects further in the list will occlude 
it. Typically, no objects will be found to occlude A and it will be immediately inserted onto G. If we 
can quickly determine that no objects occlude A,and thenew ordering requires no rearrangements, the algorithm 
veri.es that the new order is identical to the old with computation O(n log n) in the total number of 
objects. In essence, the algorithm s incrementality allows it to examine only a small subset of the potentially 
O(n2) arcs in the occlusion graph. We assume that occlusion cycles (SCCs) will be small and of limited 
duration in typical scenes. This assumption is important since the cycle de­tection algorithm has quadratic 
complexity in the number of cycle elements. The visibility sorting algorithm does not attempt to exploit 
coherence in persistent occlusion cycles. a 1 a 0 E a 0 ß 1 X1 Z X2 ß 0 T angular extent: [a0;a1] [a0;a1] 
[;0;;1]= ;=B 6!A Figure 9: Angular extent occlusion culling: Angular extents are de.ned with respect 
to an eye point E and a orthogonal coordinate frame (X1;X2;Z)where X2 (out of the page) is perpendicular 
to the plane in which angles are measured, Z de.nes the zero angle, and X1 de.nes an angle of +2 radians. 
The resulting extent is simply an interval: [a0;a1]. To determine that B 6!A (right side of .gure), we 
test for empty interval intersection. As the number of re-arrangements required in the new order increases 
(i.e., as the coherence of the ordering decreases) the IVS algorithm slows down, until a worst case scenario 
of starting from what is now a completely reversed ordering requires O(n2) outer while loop iterations. 
This is anal­ogous to using insertion sort for repeatedly sorting a coherently changing list: typically, 
the sort is O(n), but can be O(n2) in pathologically incoherent situations. The algorithm s complexity 
is bounded by (n + r)(s + co + c2)where r is the number of reinsertions required, c is the maximum number 
of objects involved in an occlusion cycle, o is the maximum number of primitive occluders of a (possibly 
grouped) object, and s is the complexity of the search for occluders of a given object. The .rst factor 
represents the number of outer while-loop iterations of IVS. In the second factor, the three terms represent 
time to .nd potential occluders, to reduce this set to actual occluders (see Section 4.1.3), and to detect 
occlusion cycles. Typically, r .O(n), c .O(1), o .O(1), and s .O(log n) resulting in an O(n log n) algorithm. 
In the worst case, many reinsertions are required, many objects are involved in occlusion cycles, and 
many objects occlude any given object so that r .O(n2), c .O(n), o .O(n), and s .O(n) resulting in an 
O(n4) algorithm. This analysis assumes that occlusion detection between a pair of parts requires constant 
time. When the animation is started and at major changes of scene, there is no previous visibility sort 
to be exploited. In this case, we use an initial sort by distance from the eye point to the centroid 
of each part s bounding hull. Using a sort by z is less effective because it sorts objects behind the 
eye in reverse order; sorting by distance is effective even if the view direction swings around rapidly. 
 4.1 Occlusion Culling The fundamental query of the IVS algorithm determines which current objects occlude 
a given (possibly grouped) object. To quickly cull the list of candidate occluders to as small a set 
as possible, we bound each part with a convex polyhedron and determine the spatial extent of this convex 
bound with respect to a predetermined set of directions, as in [Kay86]. These directions are of two types. 
Spatial extents are projections along a given 3D vector. Angular extents are projected angles with respect 
to a given eye point and axis. Spatial extents are de.ned by extremizing (maximizing and minimizing) 
S(P) =D .P over all points P in the convex hull. Angular extents are de.ned similarly by extremizing2 
() (P ,E) .Z A(P) =tan,1 (2) (P ,E) .X1 where E is the eye point,Z de.nes the zero angle direction, and 
X1 de.nes the positive angles. Given two objects, A and B, with interval bounds for each of their extents, 
occlusion relationships can be tested with simple interval intersection tests performed independently 
for each extent, as shown in Figures 9 and 10. The content author chooses the number of spatial (ks ) 
and angular (ka) extents and their directions; let k =ks + ka be the total number. If any of 2Care must 
be taken when the denominator is close to 0. This is easily accomplished in the C math library by using 
the function atan2. a0 a1 a0 a1 a0 a1 E   E E D  D b0 b1 D b0 b1 b0 b1 a'0 a'1 a'0 a'1 a'0 a'1b0 
b1 b0 b1 b0 b1 (a) B 6!A (b) B !A (c) B !A Figure 10: Spatial extent occlusion culling: Spatial extents 
are de.ned with respect to a direction D. To test whether B !A, A s spatial extent [a0;a1] is expanded 
00 00 by E .D to yield [a0;a1]. Three cases can occur. In (a), [a0;a1] is disjoint from B s extent [b0;b1], 
so B 6!A.In (b), [a0 0;a0 1] overlaps with [b0;b1], so B !A is possible. In (c), [a00;a10] overlaps with 
[b0;b1] even though A s extent [a0;a1] is disjoint from B.Again, B !A is possible. Note that in case 
(b) and (c), the occlusion cull tests must determine B !A for all k extents before concluding B is a 
possible occluder of A. the k tests .nds that B 6!A then the test can be concluded and B rejected as 
an occluder without testing more extents. Note that the algorithm computes all intersecting pairs, which 
is a useful computational by-product for simulation. View frustum culling is made trivial by computing 
the angular extents of the visible region once at the start of each frame and determining whether each 
objects angular extents intersect it. 4.1.1 Tracking Extents on Convex Hulls Spatial extent directions 
can be .xed in space (e.g., the coordinate axes, but note that arbitrary directions are allowed) or tied 
to the camera. Camera­independent spatial extents only need to be updated when the object moves; camera-dependent 
spatial extents must be updated when the object or the camera moves. Angular extents must also be updated 
whenever the object or camera moves. For the results in Section 8, in one case (Tumbling Toothpicks) 
we used two orthogonal angular extents (screen x and y direc­tions) and the orthogonal camera-dependent 
spatial extent (Z). In another case with many unmoving objects (Canyon Flyby), we used 3 mutually orthogonal 
camera-independent spatial extents. For convex bounding polyhedra, spatial and angular extents can be 
updated simply by sliding downhill (i.e., gradient descent) from vertex to neighboring vertex, evaluating 
the objective function (Sor A) at each vertex. At each iteration, the neighboring vertex having the minimum 
value is accepted as the starting point for the next iteration. If no neighbors have a smaller objective 
function, then the computation is halted with the current vertex returned as the minimizer. Small motions 
of the convex hull or the spatial/angular reference frame move the new extremal vertex at most a few 
neighbors away from the last one. By starting with the extremal vertex from the last query, coherence 
in object and camera motions is thus exploited. 4.1.2 Accelerating Occlusion Queries with Kd-Trees We 
have reduced the problem of .nding all potential occluders of an object A to 1. forming a query extent 
for A, in which an k-dimensional interval is created by taking the angular extents without change and 
the spatial extents after enlarging by E .D,and 2. .nding all objects that overlap this query.  We 
hierarchically organize part extents using a kd-tree to accelerate .nding the set of overlapping extents 
for a given query. A kd-tree [Bentley75] is a binary tree which subdivides along k .xed dimensions. Each 
node T in the tree stores both the dimension subdivided (T:i) and the location of the partitioning point 
(T:v). Object extents whose T:i-th dimension interval lower bound is less than T:v are placed in the 
left child of node T; those whose upper bound is greater than T:v are placed in the right child. Objects 
which straddle kd-planes are simply inserted into both subtrees. Note that the planes are not used directly 
to determine the visibility order; the structure simply accelerates occlusion queries. A simple minimum 
cost metric is used to determine a subdivision point for a list of intervals, representing the 1D extents 
of the set of objects with respect to one of the ka angular or ks spatial directions. Our cost metric 
sums the length of the longer of the left and right sublists and the number of intervals shared between 
left and right. Avoiding lopsided trees and trees in which many objects are repeated in both subtrees 
is desirable since such trees tend to degrade query performance in the average case. The cost can be 
computed with a simple traversal of a sorted list containing both upper and lower bounds; details can 
be found in [Snyder97]. To build the kd-tree, we begin by sorting each of the k interval sets to produce 
k 1D sorted bound lists, containing both upper and lower bounds. The kd-tree is then built recursively 
in a top-down fashion. To subdivide a node, the partitioning cost is computed for each of the k bound 
lists, and the dimension of lowest cost actually used to partition. Bound lists for the partitioned children 
are built in sorted order by traversing the sorted parent s list, inserting to either or both child lists 
according to the computed partitioning. We then recurse to the left and right sides of the kd-tree. The 
algorithm is terminated when the longer child list is insuf.ciently smaller than its parent (we use a 
threshold of 10). A node T in the .nal kd-tree stores the 1D sorted bound list only for dimension T:i, 
which is used to update the subdivision value T:v in future queries, and to shift objects between left 
and right subtrees as they move. The other lists are deleted. Since rebuilding is relatively expensive, 
the algorithm also incorporates a quick kd-tree rebalancing pass. To rebalance the kd-tree as object 
extents change, we visit all its nodes depth-.rst. At each node T, the 1D sorted bound list is re-sorted 
using insertion sort and the cost algorithm is invoked to .nd a new optimal subdivision point, T:v. Extents 
are then repartitioned with respect to the new T:v, shifting extents between left and right subtrees. 
Extent addition is done lazily (i.e., only to the immediate child), with further insertion occurring 
when the child nodes are visited. Extent deletion is done immediately for all subtrees in which the extent 
appears, an operation that can be done ef.ciently by recording a (possibly null) left and right child 
pointer for each extent stored in T. Note that coherent changes to the object extents yield an essentially 
linear re-sort of bound lists, and few objects that must be shifted between subtrees. It is important 
to realize that the coherence of kd-tree rebalancing de­pends on .xing the subdivision dimension T:i 
at each node. If changes in the subdivided dimension were allowed, large numbers of extents could be 
shifted between left and right subtrees, eliminating coherence in all descendants. Fixing T:i but not 
T:v restores coherence, but since T:i is computed only once, the tree can gradually become less ef.cient 
for query acceleration as object extents change. This problem can be dealt with by rebuilding the tree 
after a speci.ed number of frames or after measures of tree effectiveness (e.g., tree balance) so indicate. 
A new kd-tree can then be rebuilt as a background process over many frames while simultaneously rebalancing 
and querying an older version. Querying the kd-tree involves simple descent guided by the query. At a 
given node T, if the query s T:i-th interval lower bound is less than T:v, then the left subtree is recursively 
visited. Similarly, if the query s T:i-th interval upper bound is greater than T:v then the right subtree 
is recursively visited. When a terminal node is reached, extents stored there are tested for overlap 
with respect to all k dimensions. Overlapping extents are accumulated into an occluder list. An extent 
is inserted only once in the occluder list, though it may occur in multiple leaf nodes. An additional 
concern is that the occlusion query should return occluders of an object A that have not already been 
inserted into the output list.Re­stricting the set of occluders to the set remaining in L can be accomplished 
by activating/deactivating extents in the kd-tree.When A is popped off the list L in the IVS algorithm, 
all objects grouped within it are deactivated. Deactivated objects are handled by attaching a .ag to 
each object in the list stored at each terminal node of the kd-tree. Deactivating an object involves 
following its left and right subtree pointers, beginning at the kd root, to arrive at terminal lists 
containing the object to be deactivated. Activation is done similarly, with the .ag set oppositely. Counts 
of active objects within each kd-tree node are kept so that nodes in which all objects have been deactivated 
can be ignored during queries. 4.1.3 Avoiding Occlusion Cycle Growth The occlusion testing described 
so far is conservative, in the sense that possible occluders of an object can be returned which do not 
in fact occlude it. There are two sources of this conservativeness. First, occlusion is tested with respect 
to a .xed set of spatial and/or angular extents, which essentially creates an object larger than the 
original convex hull and thus more likely to be occluded. Second, extents for grouped objects are computed 
by simple  A B  Figure 11: Occlusion cycle growth with grouped objects: In this example, A and B have 
been grouped because they are mutually occluding. A simple bound around their union, shown by the dashed 
lines, is occluded by object C, even though the objects themselves are not. We therefore use the bounded 
extents around grouped objects for a quick cull of nonoccluders, but further test objects which are not 
so culled to make sure they occlude at least one primitive element of the grouped object. unioning of 
the extents of the members, even though the unioned bound may contain much empty space, as shown in Figure 
11. The next section will show how to compute an exact occlusion test between a pair of convex objects, 
thus handling the .rst problem. This section describes a more stringent test for grouped objects which 
removes the second problem. Occlusion testing that is too conservative can lead very large groupings 
of objects in occlusion cycles. In the extreme case every object is inserted into a single SCC. This 
is especially problematic because of the second source of conservatism that bounds essentially grow 
to encompass all members of the current SCC, which in turn occlude further objects, and so on, until 
the SCC becomes very large. To handle this problem, we perform additional tests when a grouped object 
A is tested for occlusion. A s unioned extents are used to return a candidate list of possible occluders, 
as usual. Then the list of occluders is scanned to make sure each occludes at least one of the primitive 
members of A, using a simple k-dimensional interval intersection test. Any elements of the list that 
do not occlude at least one member of A are rejected, thus ensuring that holes within the grouped object 
can be seen through without causing occlusions. Finally, remaining objects can be tested against primitive 
members of A using the exact occlusion test.  4.2 Occlusion Testing The algorithms in Section 4.1 provide 
a fast but conservative pruning of the set of objects that can possibly occlude a given object A. To 
produce the set of objects that actually occlude A with respect to the convex hull bounds, we apply an 
exact test of occlusion for primitive object pairs (A;B), which determines whether B !A. The test is 
used in the IVS algorithm by scanning the list of primitive elements of the possibly grouped object A 
and ensuring that at least one occluder in the returned list occludes it, with respect to the exact test. 
The exact test is thus used as a last resort when the faster methods fail to reject occluders. The exact 
occlusion test algorithm is as follows: ExactConvexOcclusion(A,B,E) [returns whether B !E A] if all (non 
eye-expanded) spatial extents of A and B intersect initiate 3D collision tracking for hA;Bi, if not already 
if A and B collide, return B !A if E on same side of separating plane as A, return B 6!A endif if B contains 
eye point E, return B !A [B occludes everything] initiate occlusion tracking for hB;Ai, if not already 
return result of occlusion test Both the collision and occlusion query used in the above algorithm can 
be computed using the algorithm in Appendix A. While the collision query is not strictly necessary, it 
is more ef.cient in the case of a pair of colliding objects to track the colliding pair once rather than 
tracking two queries which bundle the eye point with each of the respective objects. For scenes in which 
collisions are rare, the direct occlusion test should be used. The IVS algorithm is extended to make 
use of a hash table of object pairs for which 3D collision or occlusion tracking have been initialized, 
allowing fast access to the information. Tracking is discontinued for a pair if the information is not 
accessed after more than one frame. Note that further occlusion resolution is also possible with respect 
to the actual objects rather than convex bounds around them. It is also possible to inject special knowledge 
in the occlusion resolution process, such as the fact that a given separating plane is known to exist 
between certain pairs of objects, like joints in an articulated character or adjacent cells in a pre-partitioned 
terrain. Special purpose pairwise visibility codes can also be developed; Appendix B provides an example 
for a cylinder with endcap tangent to a sphere that provides a visibility heuristic for articulated joints 
in animal-like creatures. 4.3 Conditioning Sort After each IVS invocation, we have found it useful to 
perform a condi­tioning sort on the output that bubbles up SCCs based on their midpoint with respect 
to a given extent. More precisely, we reorder according to the absolute value of the difference of the 
midpoint and the projection of the eye point along the spatial extents. The camera-dependent Z direction 
is typically used as the ordering extent, but other choices also provide ben­e.t. An SCC is only moved 
up in the order if doing so does not violate the visibility ordering; i.e., the object does not occlude 
the object before which it is inserted. This conditioning sort smooths out computation over many queries. 
Without it, unoccluding objects near the eye can remain well back in the ordering until they .nally occlude 
something, when they must be moved in front of many objects in the order, reducing coherence. The conditioning 
sort also sorts parts within SCCs according to extent mid­point, but ignoring occlusion relationships 
(since the SCC is not visibility sortable).  5 Resolving Non-Binary Cycles Following [Porter84], we 
represent a shaped image as a 2D array of 4­tuples, written A =[Ar;Ag;Ab;Au] where Ar;Ag;Ab are the color 
components of the image and Auis the transparency, in the range [0;1]. Consider the cyclic occlusion 
graph and geometric situation shown in Figure 2c. Clearly, A over B over C produces an incorrect image 
because C !A but no part of C comes before A in the ordering. A simple modi.cation though produces the 
correct answer: A out C + B out A + C out B: where out is de.ned as A out B =A(1 ,Bu): This expression 
follows from the idea that objects should be attenuated by the images of all occluding objects. Another 
correct expression is (C atop A) over B over C where atop is de.ned as A atop B =ABu+(1 ,Au)B: In either 
case, the expression correctly overlays the relevant parts of oc­cluding objects over the occluded objects, 
using only shaped images for the individual objects (refer to Figure 12). Technically, the result is 
not correct at any pixels partially covered by all three objects, since the matte channel encodes coverage 
as well as transparency. Such pixels tend to be isolated, if they exist, and the resulting errors of 
little signi.cance.3 The above example can be generalized to any collection of objects with a known occlusion 
graph having no binary cycles: cycles of the form A !B;B !A. The reason binary cycles can not be handled 
is that in the region of intersection of the bounding hulls of A and B,we simply have no information 
about which object occludes which. Note also that the compositing expression in this case reduces to 
A out B + B out A which T incorrectly eliminates the part of the image where AB projects. A correct compositing 
expression for n shaped images Ii is given by n X Ii OUT Ij (3) fj jOj!Oig i=1 The notation OUT with 
a set subscript is analogous to the multiplication accumulator operator I, creating a chain of out operations, 
as in D OUT = D out A out B out C: fA;B;Cg 3Recall too that overlaying shaped images where the matte 
channel encodes coverage is itself an approximation since it assumes uncorrelated silhouette edges. A 
B C A2 = C atop A A2 over B over C A1 = A out C B1 = B out A C1 = C out B A1 + B1 + C1 Figure 12: Compositing 
expressions for cycle breaking: The original sprite images are shown as A, B, C. Using over-atop , the 
.nal image is formed by (C atop A) over B over C. Using sum-of-outs , the .nal image is formed by (A 
out C)+(B out A)+ (C out B).  In words, (3) sums the image for each object Oi, attenuated by the out 
chain of products for each object Oj that occludes Oi (Figure 12, bottom row). An alternate recursive 
formulation using atop is harder to compile but generates simpler expressions. As before, we have a set 
of objects O = fOigtogether with an occlusion graph G for O containing no binary cycles. The subgraph 
of G induced by an object subset X CO is written GX.Then for any O*2O , I(GO)= I(GfOijOi!O.g)atop I(O*)over 
I(GO,fO.g) (4) where I(G) represents the shaped image of the collection of objects using the occlusion 
graph G. In other words, to render the scene, we can pick any isolated object O*, .nd the expression 
for the subgraph induced by those objects occluding O*, and compute that expression atop O*(Figure 12, 
top right). That result is then placed over the expression for the subgraph induced by removing O*from 
the set of objects O. Note also that the above expression assumes I(G;)= 0. Proofs of correctness of 
the two expressions is available in a technical report [Snyder98]. Compositing Expression Compilation 
An ef.cient approach to generating an image compositing expression for the scene uses the IVS algorithm 
to produce a visibility sorted list of SCCs. Thus the images for each SCC can be combined using a simple 
sequence of over operations as in Expression (1). Most SCCs aresingletons (contain­ing a single object). 
Non-singleton SCCs are further processed to merge binary cycles, using the occlusion subgraph of the 
parts comprising the SCC. Merging must take place iteratively in case binary cycles are present between 
objects that were merged in a previous step, until there are no bi­nary cycles between merged objects. 
We call such merged groups BMCs, for binary merged components. Expression (3) or (4) is then evaluated 
using the resulting merged occlusion graph to produce an expression for the SCC. Each BMC must be grouped 
into a single layer, but not the entire SCC. For example, Figure 2(c) involves one SCC but three BMCs, 
since there are no binary cycles. It is clear that Expression (3) can be evaluated using two image registers: 
one for accumulating a series of out operations for all image occluders of a given object, and another 
for summing the results. Expression (4) can be similarly compiled into an expression using two image 
resisters: one for in or out operations and one for sum accumulation [Snyder98]. Two image registers 
thus suf.ce to produce the image result for any SCC. An ef.cient evaluation for the scene s image requires 
a third register to accumulate the results of the over operator on the sorted sequence of SCCs. This 
third register allows segregation of the SCCs into separately compilable units. Given such a three-register 
implementation, it can be seen why Expres­sion (4) is more ef.cient. For example, for a simple ring cycle 
of n objects; i.e.,agraph O1 !O2 !...!On !O1 the sum-of-outs formulation (Expression 3) produces I(O1)outI(On)+ 
I(O2)outI(O1)+ I(O3)outI(O2)+ ...+ I(On)outI(On,1) with n out andn ,1 addition operations, while the 
over-atop formu­lation (Expression 4) produces .I(On)in I(O1)+ I(O1) out I(On).over I(O2) over ...over 
I(On) with n ,1 over , 1 in , 1 out , and 1 addition operators. Assuming over is an indispensable operator 
for hardware implementations and is thus atomic, the second formulation takes advantage of over to reduce 
the expression complexity. 6 Pre-Splitting to Remove Binary Cycles The use of convex bounding hulls 
in occlusion testing is sometimes overly conservative. For example, consider a pencil in a cup or an 
aircraft .ying within a narrow valley. If the cup or valley form a single part, our visibility sorting 
algorithm will always group the pencil and cup, and the aircraft and valley, in a single layer (BMC) 
because their convex hulls intersect. In fact, in the case of the valley it is likely that nearly all 
of the scene s geometry will be contained inside the convex hull of the valley, yielding a single layer 
for the entire scene. To solve this problem, we pre-split objects that are likely to cause un­wanted 
aggregation of parts. Objects that are very large, like terrain, are obvious candidates. Foreground objects 
that require large rendering resources and are known to be containers , like the cup, may also be pre-split. 
Pre-splitting means replacing an object with a set of parts, called split parts, whose convex hull is 
less likely to intersect other moving parts. With enough splitting, the layer aggregation problem can 
be suf.ciently reduced or eliminated. Simple methods for splitting usually suf.ce. Terrain height .elds 
can be split using a 2D grid of splitting planes, while rotationally symmetric containers, like a cup, 
can be split using a cylindrical grid. A 3D grid of splitting planes can be used for objects without 
obvious projection planes or symmetry (e.g., trees). On the other hand, less naive methods that split 
more in less convex regions can reduce the number of split parts, improving performance. Such methods 
remain to be investigated in future work. Pre-splitting produces a problem however. At the seam between 
split neighbors the compositor produces a pixel-wide gap, because its assump­tion of uncorrelated edges 
is incorrect. The split geometry exactly tessel­lates any split surfaces; thus alpha (coverage) values 
should be added at the seam, not over ed. The result is that seams become visible. To solve this problem, 
we extend the region which is included in each split object to produce overlapping split parts, a technique 
also used in [Shade96]. While this eliminates the visible seam artifact, it causes split parts to intersect, 
and the layer aggregation problem recurs. Fortunately, adjacent split parts contain the same geometry 
in their region of overlap. We therefore add pairwise separating planes between neighbors, because both 
agree on the appearance within the region of overlap so either may be drawn. This breaks the mutual occlusion 
relationship between neighbors and avoids catastrophic layer growth. But we use the convex hulls around 
the in.ated split parts for testing with all other objects, so that the correct occlusion relationship 
is still computed. Note that the occlusion sort does not preclude splitting arrangements like hexagonal 
terrain cells that permit no global partitioning planes. All that is required is pairwise separation. 
 7 Visibility Correct Depth of Field 2D image blurring is a fast method for simulating depth of .eld 
effects amenable to hardware implementation [Rokita93]. Unfortunately, as ob­served in [Cook84], any 
approximation that uses a single hidden-surface­eliminated image, including [Potmesil81,Rokita93], causes 
artifacts be­cause no information is available for occluded surfaces made visible by depth of .eld. The 
worst case is when a blurry foreground object occludes a background object in focus (Figure 13). As shown 
in the .gure, the approximation of [Potmesil81] sharpens the edge between the foreground and background 
objects, greatly reducing the illusion. Following [Max85], but using correct visibility sorting, we take 
advantage of the information in layer images that would ordinarily be eliminated to correct these problems. 
The individual image layers are still approximated by spatially invariant blurring in the case of objects 
having small depth extent, or by the spatially varying convolution from [Potmesil81]. Image compositing 
is used be­tween layers. Since a substantial portion of depth of .eld cues come from (a) no depth of 
.eld (b) single layer depth of .eld approx. (c) two layer visibility compositing approx. Figure 13: Simulating 
depth of .eld with image blurring. Figure 14: Toothpick example (nobjs=800, uniform scale): This image 
shows a frame from the .rst experiment, drawn with hidden line elimination by using Painter s algorithm 
with the computed visibility order. For the hidden line processing, singleton SCCs are simply drawn by 
.nding the part s silhouette, .lling its interior in white and then its boundary as a black polyline. 
Nonsingleton SCCs are further processed to .nd visible intersection and silhouette edges dynamically, 
but only the few objects comprising the SCC need be considered, not the entire scene. edge relations 
between foreground and background objects, we consider this a good approximation, although blurring without 
correctly integrating over the lens only approximates the appearance of individual parts. Grouping parts 
in a BMC because of occlusion undecomposability ex­acts a penalty. Such grouping increases the depth 
extent of the members of the group so that the constant blur approximation or even the more ex­pensive 
depth-based convolution incur substantial error. For groupings of large extent, the renderer could resort 
to rendering integration using the ac­cumulation buffer [Haeberli90]. Such integration requires many 
rendering passes (23 were used in images from [Haeberli90]), representing a large allocation of system 
resources to be avoided when simple blurring suf.ces.  8Results All timings are reported for one processor 
of a Gateway E5000-2300MMX PC with dual Pentium II 300MHz processors and 128MB of memory. Measured computation 
includes visibility sorting and kd-tree building and rebalancing. The kd-tree was built only once at 
the start of each animation; the amortized cost to build it is included in the average cpu times reported. 
Tumbling Toothpicks The .rst results involve a simulation of tumbling toothpicks , eccentric ellipsoids, 
moving in a cubical volume (Figure 14). The toothpicks bounce off the cube sides, but are allowed to 
pass through each other. Each toothpick contains 328 polygons and forms one part. There are 250 frames 
in the animation. In the .rst series of experiments, we measured cpu time per frame, as a function of 
number of toothpicks (Figure 15). Time per frame averaged over the entire animation and maximum time 
for any frame are both re­ported. One experiment, labeled us foruniform scale in the .gure, adds more 
toothpicks of the same size to the volume. This biases the occlusion complexity superlinearly with number 
of objects, since there are many more collisions and the size of the average occlusion cycle increases. 
With enough toothpicks, the volume becomes .lled with a solid mass of moving geometry, forming a single 
SCC. As previously discussed, the algorithm is designed for situations in which occlusion cycles are 
relatively small. A more suitable measure of the algorithm s complexity preserves the av­erage complexity 
per unit volume and simply increases the visible volume. This effect can be achieved by scaling the toothpicks 
by the cube root of their number ratio, so as to preserve average distance between toothpicks as a fraction 
of their length. The second experiment, labeled ud foruni­form density presents these results. The results 
demonstrate the expected O(n log n) rate of growth. The two experiments are normalized so that the simulations 
are identical within timing noise for nobjs=200: the uniform 1 density experiment applies the scale (200.nobjs) 
3 to the toothpicks of the other trials. In particular, note that a simulation with 200 toothpicks (220 
total objects including cube parts), can be computed at over 100Hz, making it practical for real-time 
applications. To verify the above scaling assump­tions, the following table summarizes some visibility 
statistics (averaged over all frames of the animation) for the baseline scene with 200 toothpicks and 
the two scenes with 1600 toothpicks (uniform density, uniform scale): measurement nobjs=200 nobjs=1600 
nobjs=1600 (uniform density) (uniform scale) fraction of SCCs that .0454 .04633 .2542 are nonsingleton 
fraction of parts in non­ .0907 0.0929 .5642 singleton SCCs average size of nonsin­ 2.097 2.107 3.798 
gleton SCCs max size of SCCs 2.64 3.672 45.588 With the exception of the max size of nonsingleton SCC 
which we would expect to increase given that the 1600 object simulation produces greater probability 
that bigger SCCs will develop, the .rst two columns in the table are comparable, indicating a reasonable 
scaling, while the third indicates much greater complexity. Note also that the large maximum cpu time 
for the 1400 and 1600 uniform scale trials is due to the brief existence of much larger than average 
sized occlusion cycles. The second experiment measures cpu time with varying coherence. We globally scale 
the rate of camera movement and the linear and angular velocities of the toothpicks (Figure 16). The 
number of toothpicks was .xed at 200; the trial with velocity scale of 1 is thus identical to the trial 
with nobjs=200 in Figure 15. The algorithm is clearly sensitive to changing coherence, but exhibits only 
slow growth as the velocities become very large. Not surprisingly, the difference between average and 
worst case query times increases as coherence decreases, but the percentage difference remains fairly 
constant, between 17% and 30%. To calibrate the results of the second experiment, let S be the length 
of the image window and W the length of the cube side containing the toothpicks. For the unit scale trial 
the velocity measured at one toothpick 1 0.8 cpu (seconds/frame) 0.6 0.4 0.2 0 no. objects no. objs 
25 50 100 200 400 800 1600 end and averaged over all frames and toothpicks was 0.117% S per frame (image 
space) and 0.269% W per frame (world space). This amounts to an average of 14.2 and 6.2 seconds to traverse 
the image or cube side respectively at a 60Hz frame rate.4 In a third experiment (Figure 17), we compared 
performance of the algorithm using kd-trees that sort by different numbers of extents. The same simulations 
were run as in the .rst experiment, either exactly as before (k = 3, using two angular extents and the 
perpendicular camera-dependent spatial extent Z), or using kd-tree partitioning only in a single dimension 
(k = 1, using only Z). In the second case, the two angular extents were still used for occlusion culling, 
but not for kd-tree partitioning. This roughly simulates the operation of the NNS algorithm, which .rst 
examines objects that overlap in depth before applying further culls using screen bounding boxes. It 
can be seen that simultaneously searching all dimensions is much preferable, especially as the number 
of objects increases. For example, in the uniform density case, using a single direction rather than 
three degrades performance by 14% for 100 objects, 28% for 200, 65% for 400, up to 267% for 1600. The 
differences in the uniform scale case are still signi.cant but less dramatic, since occlusion culling 
forms a less important role than layer reordering and occlusion cycle detection. We used the visibility 
sorting results to create a depth of .eld blurred re­sult using compositing operations as described in 
Section 7, and compared it to a version created with 21 accumulation buffer passes. The results are shown 
in Figure 18. For the visibility compositing result, a constant blur factor was determined from the circle 
of confusion at the centroid of the avg. cpu (ms) [ud] 1.51 2.51 4.81 9.97 23.1 51.7 122 max cpu (ms) 
[ud] 2.40 3.55 6.28 11.8 26.3 56.9 131 avg. cpu (ms) [us] 1.32 2.17 4.21 10.1 27.0 92.5 849 max cpu (ms) 
[us] 2.28 3.11 5.57 11.9 30.5 134 3090 Figure 15: Performance with increasing number of objects. 0.035 
object or object group, for all objects except the cube sides. Because of the large depth extent of the 
cube sides, these few parts were generated using the accumulation buffer technique on the individual 
layer parts and 0.025 0.02 composited into the result with the rest. 0.015 Canyon Flyby The second results 
involve a set of aircraft .ying in formation inside a winding valley (Figure 19). We pre-split the valley 
terrain (see Section 6) 0.01 0.005 into split parts using 2D grids of separating planes and an in.ation 
factor of 20%. The animation involves six aircraft each divided into six parts 0 (body, wing, rudder, 
engine, hinge, and tail); polygon counts are given in the table below: velocity scale object polygons 
hull polygons body 1558 192vel.scl. .25 .5 1 2 4 8163264 engine 1255 230 0.03 cpu (seconds/frame) avg 
cpu (ms) 9.15 9.52 9.78 11.0 12.6 13.8 16.1 19.3 25.2 max cpu (ms) 10.9 11.2 11.6 13.2 15.3 16.7 19.5 
25.0 32.0 Figure 16: Performance with increasing velocity (decreasing coherence). 1 wing tail rudder 
hinge sky (sphere) terrain (unsplit) 1421 22 48 64 480 2473 80 22 28 46 - - Using terrain splitting 
grids of various resolutions, we investigated ren­dering acceleration using image-based interpolation 
of part images. The following table shows average polygon counts per split part for terrain splits using 
2D grids of 20x20, 14x14, 10x10, 7x7, and 5x5: 0.8 cpu (seconds/frame) 0.6 grid 20 X20 14 X14 0.4 10 
X10 7 X7 5 X5 0.2 split objects 390 191 100 49 25 polygons/object hull polygons/object 31.98 29.01 48.91 
37.78 76.48 42.42 130.45 57.51 225.32 72.72 Note that the polygons and polygons/object column in the 
above tables are a measure of the average rendering cost of each part, while the 0 hull polygons and 
hull polygons/object column is an indirect measure of computational cost for the visibility sorting algorithm, 
since it deals with no. objects hulls rather than actual geometry. Following results from [Lengyel97], 
we assumed the 6 parts of each 50 100 200 400 800 1600no. objs k = 3 cpu (ms) [ud] 2.51 4.81 9.97 23.1 
51.7 122 k = 1 cpu (ms) [ud] 2.65 5.48 12.7 38.0 124 447 % diff. [ud] 5.55% 14.0% 27.7% 64.6% 140% 267% 
k = 3 cpu (ms) [us] 2.17 4.21 10.1 27.0 92.5 849 k = 1 cpu (ms) [us] 2.28 4.79 12.9 47.2 215 1780 % diff. 
[us] 5.05% 13.6% 28.7% 74.4% 132% 109% Figure 17: Comparison of kd-tree culling with different numbers 
of extents. Cpu times are for the average case. aircraft required a 20% update rate (i.e., could be rendered 
every .fth frame and interpolated the rest), the terrain a 70% update rate, and the sky a 40% update 
rate. These choices produce a result in which the interpolation artifacts are almost imperceptible. To 
account for the loss 4While this baseline may seem somewhat slow-moving, it should be noted that small 
movements of the parts in this simulation can cause large changes in their occlusion graph with attendant 
computational cost. We believe this situation to be more dif.cult than typical computer graphics animations. 
Stated another way, most computer graphics animations will produce similar occlusion topology changes 
only at much higher velocities.  (b) Visibility compositing Figure 18: Comparison of depth of .eld generation 
methods: The images show two different depth of .eld renderings from the tumbling toothpicks experiment. 
Toothpicks comprising a multi-object layer share a common color; singleton layers are drawn in white. 
Note the occlusion relationships between the sphere/cylinder joints at the cube sides, computed using 
the algorithm in Appendix B. While pairs of spheres and cylinders are sometimes mutually occluding, the 
algorithm is able to prevent any further occlusion cycle growth. of coherence which occurs when parts 
are aggregated into a single layer, we conservatively assumed that all parts so aggregated must be rendered 
every frame (100% update rate), which we call the aggregation penalty. The results are summarized in 
Figure 20. The column cpu shows average and maximum cpu time per frame in milliseconds. The next column 
( terrain expansion factor ) is the factor increase in number of polygons due to splitting and overlap; 
this is equal to the total number of polygons in the split terrain divided by the original number, 2473. 
The next columns show the fraction of visible layers that include more than one part ( aggregate layers 
fraction ), followed by the fraction of visible parts that are aggregated ( aggregated parts fraction 
). Visible in this context means not outside the viewable volume. Average re­rendering (update) rates 
under various weightings and assumptions follow: unit weighting per part with and without the aggregation 
penalty ( update rate, unit weighting, (agg) and :::(no agg) ), followed by the analogs for polygon number 
weighting. Smaller rates are better in that they indicate greater reuse of image layers through interpolation 
and less actual render­ing. The factors without aggregation are included to show how much the rendering 
rate is affected by the presence of undecomposable multi-object layers. The polygon-weighted rates account 
for the fact that the terrain has been decomposed into an increased number of polygons. This is done 
by scaling the rates of all terrain objects by the terrain expansion factor. In summary, the best polygon-weighted 
reuse rate in this experiment, 38%, is achieved by the 14 x14 split. Finer splitting incurs a penalty 
for increasing the number of polygons in the terrain, without enough payoff in terms of reducing aggregation. 
Coarser splitting decreases the splitting penalty but also increases the number of layer aggregations, 
in turn reducing the reuse rate via the aggregation penalty. Note the dramatic increase from 7 x7to 5 
x5 in poly-weighted update rate with aggregation penalty (second rightmost column) splits below this 
level .ll up concavities in the valley too much, greatly increasing the portion of aggregated objects. 
It should be noted that the reuse numbers in this experiment become higher if the fraction of polygons 
in objects with more coherence (in this case, the aircraft) are increased or more such objects are added. 
Allow­ing independent update of the terrain s layers would also improve reuse, although as pointed out 
in [Lengyel97] this results in arti.cial apparent motion between terrain parts.  9 Conclusion Many applications 
exist for an algorithm that performs visibility sorting without splitting, including rendering acceleration, 
fast special effects gen­eration, animation design, and incorporation of external image streams into 
a synthetic animation. These techniques all derive from the observation that 2D image processing is cheaper 
than 3D rendering and often suf.ces. By avoiding unnecessary splitting, these techniques better exploit 
the tem­poral coherence present in most animations, and allow sorting at the level of objects rather 
than polygons. We have shown that the non-splitting visibility sorting required in these applications 
can be computed in real­time on PCs, for scenes of high geometric and occlusion complexity, and demonstrated 
a few of the many applications. Much future work remains. Using more adaptive ways of splitting container 
objects is a straightforward extension. Incorporation of space­time volumes would allow visibility-correct 
motion blur using 2D image processing techniques. Further work is needed to incorporate visibility sorting 
in animation design systems allowing preview of modi.cations in their complete context without re-rendering 
unmodi.ed elements. Op­portunities also exist to invent faster and less conservative occlusion tests 
for special geometric cases. Finally, further development is needed for fast hardware which exploits 
software visibility sorting and performs 3D rendering and 2D real-time image operations, such as compositing 
with multiple image registers, blurring, warping, and interpolation.  Acknowledgments We thank the Siggraph 
reviewers for their careful reading and many helpful suggestions. Jim Blinn suggested the sum of outs 
resolution for nonbi­nary cycles. Brian Guenter provided a most helpful critical reading. Susan Temple 
has been an early adopter of a system based on these ideas and has contributed many helpful suggestions. 
Jim Kajiya and Conal Elliot were involved in many discussions during the formative phase of this work. 
 split poly weighting update rate layers agg. terrain cpu (ms) parts agg. avg max expan. fraction 
 fraction unit weighting fac. (agg) (no agg) (agg) (no agg)   20x20 17.33 29.82 5.04 0.1% 0.2% 58.3% 
58.1% 41.8% 40.7% 14x14 8.08 14.59 3.78 0.4% 1.0% 51.3% 50.2% 38.0% 36.0% 10x10 5.17 9.88 3.09 1.9% 6.4% 
48.7% 40.9% 40.4% 30.7%    7x7 4.51 9.68 2.58 5.2% 22.5% 51.9% 37.8% 42.4% 27.8% 5x5 5.37 11.01 2.28 
13.0% 53.1% 71.5% 32.3% 72.0% 26.1% Figure 20: Canyon .yby results.  References [Appel67] Appel A., 
The Notion of Quantitative Invisibility and the Machine Ren­dering of Solids, InProceedings of the ACM 
National Conference, pp. 387-393, 1967. [Baraff90] Baraff, David, Curved Surfaces and Coherence for Non-Penetrating 
Rigid Body Simulation, Siggraph 90, August 1990, pp. 19-28. [Bentley75] Bentley, J.L., Multidimensional 
Binary Search Trees Used for Asso­ciative Searching, Communications of the ACM, 18(1975), pp. 509-517. 
[Chen96] Chen, Han-Ming, and Wen-Teng Wang, The Feudal Priority Algorithm on Hidden-Surface Removal, 
Siggraph 96, August 1996, pp. 55-64. [Chung96a] Chung, Kelvin, and Wenping Wang, Quick Collision Detection 
of Polytopes in Virtual Environments, ACM Symposium on Virtual Reality Soft­ware and Technology 1996, 
July 1996, pp. 1-4. [Chung96b] Chung, Tat Leung (Kelvin), An Ef.cient Collision Detection Algo­rithm 
for Polytopes in Virtual Environments, M. Phil Thesis at the University of Hong Kong, 1996 [www.cs.hku.hk/ 
tlchung/collision library.html]. [Cohen95] Cohen, D.J., M.C. Lin, D. Manocha, and M. Ponamgi, I-Collide: 
An In­teractive and Exact Collision Detection System for Large-Scale Environments, Proceedings of the 
Symposium on Interactive 3D Graphics,, 1995, pp. 189-196. [Cook84] Cook, Robert, Distributed Ray Tracing, 
Siggraph 84, July 1984, pp. 137-145. [Durand97] Durand, Fredo, George Drettakis, and Claude Puech, The 
Visibility Skeleton: A Powerful and Ef.cient Multi-Purpose Global Visibility Tool, Sig­graph 97, August 
1997, pp. 89-100. [Fuchs80] Fuchs, H., Z.M. Kedem, and B.F. Naylor, On Visible Surface Generation by 
A Priori Tree Structures, Siggraph 80, July 1980, pp. 124-133. [Funkhouser92] Funkhouser, T.A., C.H. 
Sequin, and S.J. Teller, Management of Large Amounts of Data in Interactive Building Walkthroughs, Proceedings 
of 1992 Symposium on Interactive 3D Graphics, July 1991, pp. 11-20. [Gilbert88] Gilbert, Elmer G., Daniel 
W. Johnson, and S. Sathiya A. Keerthi, A Fast Procedure for Computing the Distance Between Complex Objects 
in Three-Dimensional Space, IEEE Journal of Robotics and Automation, 4(2), April 1988, pp. 193-203. [Greene93] 
Greene, N., M. Kass, and G. Miller, Hierarchical Z-buffer Visibility, Siggraph 93, August 1993, pp. 231-238. 
[Haeberli90] Haeberli, Paul, and Kurt Akeley, The Accumulation Buffer: Hardware Support for High-Quality 
Rendering, Siggraph 90, August 1990, pp. 309-318. [Kay86] Kay, Tim, and J. Kajiya, Ray Tracing Complex 
Scenes, Siggraph 86, August 1986, pp. 269-278. [Lengyel97] Lengyel, Jed, and John Snyder, Rendering with 
Coherent Layers, Siggraph 97, August 1997, pp. 233-242. [Maciel95] Maciel, Paolo W.C. and Peter Shirley, 
Visual Navigation of Large Envi­ronments Using Textured Clusters, Proceedings 1995 Symposium on Interactive 
3D Graphics, April 1995, pp. 95-102. [Mark97] Mark, William R., Leonard McMillan, and Gary Bishop, Post-Rendering 
3D Warping, Proceedings 1997 Symposium on Interactive 3D Graphics, April 1997, pp. 7-16. [Markosian97] 
Markosian, Lee, M.A. Kowalski, S.J. Trychin, L.D. Bourdev, D. Goodstein, and J.F. Hughes, Real-Time Nonphotorealistic 
Rendering, Sig­graph 97, August 1997, pp. 415-420. [Max85] Max, Nelson, and Douglas Lerner, A Two-and-a-Half-D 
Motion-Blur Algorithm, Siggraph 85, July 1985, pp. 85-93. [McKenna87] McKenna, M., Worst-Case Optimal 
Hidden Surface Removal, ACM Transactions on Graphics, 1987, 6, pp. 19-28. [Ming97] Ming-Chieh Lee, Wei-ge 
Chen, Chih-lung Bruce Lin, Chunag Gu, Tomis­lav Markoc, Steven I. Zabinsky, and Richard Szeliski, A Layered 
Video Object Coding System Using Sprite and Af.ne Motion Model, IEEE Transactions on Circuits and Systems 
for Video Technology, 7(1), February 1997, pp. 130-145. [Molnar92] Molnar, Steve, John Eyles, and John 
Poulton, PixelFlow: High-Speed Rendering Using Image Compositing, Siggraph 92, August 1992, pp. 231-140. 
[Mulmuley89] Mulmuley, K., An Ef.cient Algorithm for Hidden Surface Re­moval, Siggraph 89, July 1989, 
pp. 379-388. [Naylor92] Naylor, B.F., Partitioning Tree Image Representation and Generation from 3D Geometric 
Models, Proceedings of Graphics Interface 92, May 1992, pp. 201-212. [Newell72] Newell, M. E., R. G. 
Newell, and T. L. Sancha, A Solution to the Hidden Surface Problem, Proc. ACM National Conf., 1972. [Ponamgi97] 
Ponamgi, Madhav K., Dinesh Manocha, and Ming C. Lin, Incre­mental Algorithms for Collision Detection 
between Polygonal Models, IEEE Transactions on Visualization and Computer Graphics, 3(1), March 1997, 
pp 51-64. [Porter84] Porter, Thomas, and Tom Duff, Compositing Digital Images, Siggraph 84, July 1984, 
pp. 253-258. [Potmesil81] Potmesil, Michael, and Indranil Chakravarty, A Lens and Aperture Camera Model 
for Synthetic Image Generation, Siggraph 81, August 1981, pp. 389-399. [Potmesil83] Potmesil, Michael, 
and Indranil Chakravarty, Modeling Motion Blur in Computer-Generated Images, Siggraph 83, July 1983, 
pp. 389-399. [Regan94] Regan, Matthew, and Ronald Pose, Priority Rendering with a Virtual Address Recalculation 
Pipeline, Siggraph 94, August 1994, pp. 155-162. [Rokita93] Rokita, Przemyslaw, Fast Generation of Depth 
of Field Effects in Com­puter Graphics, Computers and Graphics, 17(5), 1993, pp. 593-595. [Schau.er96] 
Schau.er, Gernot, and Wolfgang St¨urzlinger, A Three Dimensional Image Cache for Virtual Reality, Proceedings 
of Eurographics 96, August 1996, pp. 227-235. [Schau.er97] Schau.er, Gernot, Nailboards: A Rendering 
Primitive for Image Caching in Dynamic Scenes, inProceedings of the 8th Eurographics Workshop on Rendering 
97, St. Etienne, France, June 16-18, 1997, pp. 151-162. [Schumacker69] Schumacker, R.A., B. Brand, M. 
Gilliland, and W. Sharp, Study for Applying Computer-Generated Images to Visual Simulation, AFHRL-TR­69-14, 
U.S. Air Force Human Resources Laboratory, Sept. 1969. [Sedgewick83] Sedgewick, Robert, Algorithms, Addison-Wesley, 
Reading, MA, 1983. [Shade96] Shade, Jonathan, Dani Lischinski, David H. Salesin, Tony DeRose, and John 
Snyder, Hierarchical Image Caching for Accelerated Walkthroughs of Complex Environments, Siggraph 96, 
August 1996, pp. 75-82. [Sillion97] Sillion, Franc¸ois, George Drettakis, and Benoit Bodelet, Ef.cient 
Im­postor Manipulation for Real-Time Visualization of Urban Scenery, Proceed­ings of Eurographics 97, 
Sept 1997, pp. 207-218. [Snyder97] Snyder, John, and Jed Lengyel, Visibility Sorting and Compositing 
for Image-Based Rendering, Microsoft Technical Report, MSR-TR-97-11, April 1997. [Snyder98] Snyder, John, 
Jed Lengyel, and Jim Blinn, Resolving Non-Binary Cyclic Occlusions with Image Compositing, Microsoft 
Technical Report, MSR­TR-98-05, March 1998. [Sudarsky96] Sudarsky, Oded, and Craig Gotsman, Output-Sensitive 
Visibility Al­gorithms for Dynamic Scenes with Applications to Virtual Reality, Computer Graphics Forum, 
15(3), Proceedings of Eurographics 96, pp. 249-258. [Sutherland74] Sutherland, Ivan E., Robert F. Sproull, 
and Robert A. Schumacker, A Characterization of Ten Hidden-Surface Algorithms, Computing Surveys, 6(1), 
March 1974, pp. 293-347. [Teller91] Teller, Seth, and C.H. Sequin, Visibility Preprocessing for Interactive 
Walkthroughs, Siggraph 91, July 1991, pp. 61-19. [Teller93] Teller, Seth, and P. Hanrahan, Global Visibility 
Algorithms for Illumi­nation Computations, Siggraph 93, August 1993, pp. 239-246. [Torborg96] Torborg, 
Jay, and James T. Kajiya, Talisman: Commodity Realtime 3D Graphics for the PC, Siggraph 96, August 1996, 
pp. 353-364. [Torres90] Torres, E., Optimization of the Binary Space Partition Algorithm (BSP) for the 
Visualization of Dynamic Scenes, Proceedings of Eurographics 90, Sept. 1990, pp. 507-518. [Wang94] Wang, 
J.Y.A., and E.H. Adelson, Representing Moving Images with Layers, IEEE Trans. Image Processing, vol. 
3, September 1994, pp. 625-638. [Zhang97] Zhang, Hansong, Dinesh Manocha, Thomas Hudson, and Kenneth 
Hoff III, Visibility Culling Using Hierarchical Occlusion Maps, Siggraph 97, Au­ gust 1997, pp. 77-88. 
 A Convex Collision/Occlusion Detection To incrementally detect collisions and occlusions between moving 
3D convex polyhedra, we use a modi.cation of Chung s algorithm [Chung96a, Chung96b]. The main idea is 
to iterate over a potential separating plane direction between the two objects. Given a direction, it 
is easy to .nd the extremal vertices with respect to that direction as already discussed in Section 4.1.1. 
If the current direction D points outward from the .rst object A, and the respective extremal vertices 
with respect to D are vA on object A and vB on object B,5 then D is a separating direction if D .vA <D 
.vB: If D fails to separate the objects, then it is updated by re.ecting with respect to the line joining 
the two extremal points. Mathematically, D0 =D ,2(R .D) R where R is the unit vector in the direction 
vB ,vA. [Chung96b] proves that if the objects are indeed disjoint, then this algorithm converges to a 
separating direction for the objects A and B. Coherence is achieved for disjoint objects because the 
separating direction from the previous invocation often suf.ces as a witness to their disjointness in 
the current invocation, or suf.ces after a few of the above iterations. While it is well known that collisions 
between linearly transforming and translating convex polyhedra can be detected with ef.cient, coherent 
algo­rithms, Chung s algorithm has several advantages over previous methods, notably Voronoi feature 
tracking algorithm ([Ponamgi97]) and Gilbert s algorithm ([Gilbert88]). The inner loop of Chung s algorithm 
.nds the extremal vertex with respect to a current direction, a very fast algorithm for convex polyhedra. 
Also, the direction can be transformed to the local space of each convex hull once and then used in the 
vertex gradient de­scent algorithm. Chung found a substantial speedup factor in experiments comparing 
his algorithm with its fastest competitors. Furthermore, Chung found that most queries were resolved 
with only a few iterations (<4) of the separating direction. To detect the case of object collision, 
Chung s algorithm keeps track of the directions from vA to vB generated at each iteration and detects 
when these vectors span greater than a hemispherical set of directions in S2. This approach works well 
in the 3D simulation domain where collision responses are generated that tend to keep objects from interpenetrating, 
making collisions relatively evanescent. In the visibility sorting domain however, there is no guarantee 
that a collision between the convex hulls of some object pairs will not persist in time. For example, 
a terrain cell s convex hull may encompasses several objects for many frames. In this case, Chung s algorithm 
is quite inef.cient. To achieve coherence for colliding objects, we use a variant of Gilbert s algorithm 
[Gilbert88]. In brief, Gilbert s algorithm iterates over vertices on the Minkowski difference of the 
two objects, by .nding extremal vertices on the two objects with respect to computed directions. A set 
of up to four vertex pairs are stored, and the closest point to the origin on the convex hull of these 
points computed at each iteration, using Johnson s algorithm for computing the closest point on a simplex 
to the origin. If the convex hull contains the origin, then the two objects intersect. Otherwise, the 
direction to this point becomes the direction to locate extremal vertices for the next iteration. In 
the case of collision, a tetrahedron on the Minkowski difference serves as a coherent witness to the 
objects collision. We also note that the extremal vertex searching employed in Gilbert s algorithm can 
be made more spatially coherent by caching the vertex from the previous 5Here, vA maximizes the dot product 
with respect to D over object A and vB minimizes the dot product over object B, in a common coordinate 
system. PV search on each of the two objects and always starting from that vertex in a search query. 
The .nal, hybrid algorithm uses up to 4 Chung iterations if in the previous invocation the objects were 
disjoint. If the algorithm .nds a separating plane, it is halted. Otherwise, Gilbert s iteration is used 
to .nd a witness to collision or .nd a separating plane. In the case in which Chung iteration fails, 
Gilbert s algorithm is initialized with the 4 pairs of vertices found in the Chung iterations. The result 
is an algorithm which functions incrementally for both colliding and disjoint objects and requires only 
a single query on geometry that returns the extremal vertex on the object given a direction. The algorithm 
can be used to detect collisions between two convex polyhedra, or for point inclusion queries (i.e., 
single point vs. convex polyhedron). It can also be used for occlusion detection between convex polyhedra 
given an eye point E. To detect whether A !B, we can test S whether B0=convex hull(BE) intersects with 
A. Fortunately, there is no need to actually compute the polytope B0 . Instead, the extremal direction 
search of B0is computed by .rst searching B as before. We then simply compare that result with the dot 
product of the direction with E to see if is more extreme and, if so, return E. B Occlusion Testing 
for Sphere/Cylinder Joint This section presents an method for testing occlusion between a sphere and 
a cylinder tangent to it with respect to its end cap. Let the sphere have center at C and radius s. The 
cylinder has unit-length central axis in direction V away from the sphere, and radius r, r :s. Note that 
the convex hulls of such a con.guration intersect (one cylindrical endcap is entirely inside the sphere), 
and thus the methods of Section 4.2 always indicate mutual occlusion. However, two exact tests can be 
used to split the cycle, indicating a single occlusion arc between the sphere and cylinder. We assume 
the eye point E is not inside either object. The cylinder occludes the sphere (and not vice versa) if 
the eye is on cylinder side of endcap plane; i.e. V .(E ,C) ,h 20 pwhere E is the eye point, and where 
h =s2 ,r2 is the distance from C to the plane of intersection. The sphere occludes the cylinder (and 
not vice versa) if the circle where the sphere and cylinder intersect is invisible. This can be tested 
using the cone formed by the apex P along the cylinder s central axis for which emanating rays are tangent 
to the sphere at the circle of intersection. If the eye point is inside this cone, then the circle of 
intersection is entirely sr occluded by the sphere. We de.ne l = h + h representing the distance from 
P to C; P is thus given by C+lV. Then the sphere completely occludes the circle of intersection if (E 
,P) .(C ,P) 20 and [(E ,P) .(C ,P)]2 2(l2 ,s2)(E ,P) .(E ,P) where the .rst test indicates whether E 
is in front of the cone apex, and the second ef.ciently tests the square of the cosine of the angle, 
without using square roots. Note that h and l can be computed once as a preprocess, even if C and v vary 
as the joint moves. If both these tests fails, then the sphere and cylinder are mutually oc­cluding. 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280882</article_id>
		<sort_key>231</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>23</seq_no>
		<title><![CDATA[Layered depth images]]></title>
		<page_from>231</page_from>
		<page_to>242</page_to>
		<doi_number>10.1145/280814.280882</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280882</url>
		<categories>
			<primary_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Depth cues</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Computations on discrete structures</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010243</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Appearance and texture representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P147447</person_id>
				<author_profile_id><![CDATA[81100530281]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jonathan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shade]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Washington, Seattle]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P269896</person_id>
				<author_profile_id><![CDATA[81100259454]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Steven]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gortler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Harvard Univ., MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP311643200</person_id>
				<author_profile_id><![CDATA[81542056356]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Li-wei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[He]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford Univ., Stanford, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15023234</person_id>
				<author_profile_id><![CDATA[81100122769]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Szeliski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>794727</ref_obj_id>
				<ref_obj_pid>794191</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[S. Baker, R. Szeliski, and P. Anandan. A Layered Approach to Stereo Reconstruction. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'98). Santa Barbara, June 1998.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166153</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Shenchang Eric Chen and Lance Williams. View Interpolation for Image Synthesis. In James T. Kajiya, editor, Computer Graphics (SIG- GRAPH'93 Proceedings), volume 27, pages 279-288. August 1993.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>889151</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[William Dally, Leonard McMillan, Gary Bishop, and Henry Fuchs. The Delta Tree: An Object Centered Approach to Image Based Rendering. AI technical Memo 1604, MIT, 1996.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>253298</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Lucia Darsa, Bruno Costa Silva, and Amitabh Varshney. Navigating Static Environments Using Image-Space Simplification and Morphing. In Proc. 1997 Symposium on Interactive 3D Graphics, pages 25-34. 1997.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237191</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Paul E. Debevec, Camillo J. Taylor, and Jitendra Malik. Modeling and Rendering Architecture from Photographs: A Hybrid Geometryand Image-Based Approach. In Holly Rushmeier, editor, SIGGRAPH 96 Conference Proceedings, Annual Conference Series, pages 11-20. ACM SIGGRAPH, Addison Wesley, August 1996.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>171658</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[O. Faugeras. Three-dimensional computer vision: A geometric viewpoint. MIT Press, Cambridge, Massachusetts, 1993.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237200</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Steven J. Gortler, Radek Grzeszczuk, Richard Szeliski, and Michael E Cohen. The Lumigraph. In Holly Rushmeier, editor, SIGGRAPH 96 Conference Proceedings, Annual Conference Series, pages 43-54. ACM SIGGRAPH, Addison Wesley, August 1996.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>13027</ref_obj_id>
				<ref_obj_pid>13021</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Paul S. Heckbert. Survey of Texture Mapping. IEEE Computer Graphics and Applications, 6(11):56-67, November 1986.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Paul S. Heckbert and Henry P. Moreton. Interpolation for Polygon Texture Mapping and Shading. In David Rogers and Rae Earnshaw, editors, State of the Art in Computer Graphics: Visualization and Modeling, pages 101-111. Springer-Verlag, 1991.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258854</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Youichi Horry, Ken ichi Anjyo, and Kiyoshi Arai. Tour Into the Picture: Using a Spidery Mesh Interface to Make Animation from a Single Image. In Turner Whitted, editor, SIGGRAPH 97 Conference Proceedings, Annual Conference Series, pages 225-232. ACM SIGGRAPH, Addison Wesley, August 1997.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[R. Kumar, P. Anandan, and K. Hanna. Direct recovery of shape from multiple views: A parallax based approach. In Twelfth International Conference on Pattern Recognition (ICPR'94), volume A, pages 685- 688. IEEE Computer Society Press, Jerusalem, Israel, October 1994.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>923788</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Anthony G. LaMarca. Caches and Algorithms. Ph.D. thesis, University of Washington, 1996.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[S. Laveau and O. D. Faugeras. 3-D Scene Representation as a Collection of Images. In Twelfth International Conference on Pattern Recognition (ICPR'94), volume A, pages 689-691. IEEE Computer Society Press, Jerusalem, Israel, October 1994.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258856</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Jed Lengyel and John Snyder. Rendering with Coherent Layers. In Turner Whitted, editor, SIGGRAPH 97 Conference Proceedings, Annual Conference Series, pages 233-242. ACM SIGGRAPH, Addison Wesley, August 1997.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237199</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Marc Levoy and Pat Hanrahan. Light Field Rendering. In Holly Rushmeier, editor, SIGGRAPH 96 Conference Proceedings, Annual Conference Series, pages 31-42. ACM SIGGRAPH, Addison Wesley, August 1996.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Mark Levoy and Turner Whitted. The Use of Points as a Display Primitive. Technical Report 85-022, University of North Carolina, 1985.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>253292</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[William R. Mark, Leonard McMilland, and Gary Bishop. Post- Rendering 3D Warping. In Proc. 1997 Symposium on Interactive 3D Graphics, pages 7-16. 1997.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>275476</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Nelson Max. Hierarchical Rendering of Trees from Precomputed Multi-Layer Z-Buffers. In Xavier Pueyo and Peter Schr6der, editors, Eurographics Rendering Workshop 1996, pages 165-174. Eurographics, Springer Wein, New York City, NY, June 1996.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>897835</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Leonard McMillan. Computing Visibility Without Depth. Technical Report 95-047, University of North Carolina, 1995.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>897810</ref_obj_id>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Leonard McMillan. A List-Priority Rendering Algorithm for Redisplaying Projected Surfaces. Technical Report 95-005, University of North Carolina, 1995.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218398</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Leonard McMillan and Gary Bishop. Plenoptic Modeling: An Image- Based Rendering System. In Robert Cook, editor, SIGGRAPH 95 Conference Proceedings, Annual Conference Series, pages 39-46. ACM SIGGRAPH, Addison Wesley, August 1995.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>897834</ref_obj_id>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Leonard McMillan and Gary Bishop. Shape as a Pertebation to Projective Mapping. Technical Report 95-046, University of North Carolina, 1995.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Don E Mitchell. personal communication. 1997.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258791</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Matt Pharr, Craig Kolb, Reid Gershbein, and Pat Hanrahan. Rendering Complex Scenes with Memory-Coherent Ray Tracing. In Turner Whitted, editor, SIGGRAPH 97 Conference Proceedings, Annual Conference Series, pages 101-108. ACM SIGGRAPH, Addison Wesley, August 1997.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[H. S. Sawhney. 3D Geometry from Planar Parallax. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'94), pages 929-934. IEEE Computer Society, Seattle, Washington, June 1994.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Gernot Schaufler and Wolfgang Stfirzlinger. A Three-Dimensional Image Cache for Virtual Reality. In Proceedings of Eurographics '96, pages 227-236. August 1996.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134071</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Mark Segal, Carl Korobkin, Roll van Widenfelt, Jim Foran, and Paul E. Haeberli. Fast shadows and lighting effects using texture mapping. In Edwin E. Catmull, editor, Computer Graphics (SIGGRAPH '92 Proceedings), volume 26, pages 249-252. July 1992.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237196</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[StevenM. Seitz and Charles R. Dyer. View Morphing: Synthesizing 3D Metamorphoses Using Image Transforms. In Holly Rushmeier, editor, SIGGRAPH 96 Conference Proceedings, Annual Conference Series, pages 21-30. ACM SIGGRAPH, Addison Wesley, August 1996.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>794361</ref_obj_id>
				<ref_obj_pid>794189</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Steven M. seitz and Charles R. Dyer. Photorealistic Scene Reconstruction by Voxel Coloring. In Proc. Computer Vision and Pattern Recognition Conf., pages 1067-1073. 1997.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237209</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Jonathan Shade, Dani Lischinski, David Salesin, Tony DeRose, and John Snyder. Hierarchical Image Caching for Accelerated Walkthroughs of Complex Environments. In Holly Rushmeier, editor, SIG- GRAPH 96 Conference Proceedings, Annual Conference Series, pages 75-82. ACM SIGGRAPH, Addison Wesley, August 1996.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237274</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Jay Torborg and Jim Kajiya. Talisman: Commodity Real-time 3D Graphics for the PC. In Holly Rushmeier, editor, SIGGRAPH 96 Conference Proceedings, Annual Conference Series, pages 353-364. ACM SIGGRAPH, Addison Wesley, August 1996.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[J.Y.A. Wang and E. H. Adelson. Layered Representation for Motion Analysis. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR' 93), pages 361-366. New York, New York, June 1993.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97919</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Lee Westover. Footprint Evaluation for Volume Rendering. In Forest Baskett, editor, Computer Graphics (SIGGRAPH '90 Proceedings), volume 24, pages 367-376. August 1990.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_obj_id>528718</ref_obj_id>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[G. Wolberg. Digital Image Warping. IEEE Computer Society Press, Los Alamitos, California, 1990.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Layered Depth Images Copyright &#38;#169;1998 by the Association for Computing Machinery, Inc. Permission 
to make digital or hard copies of part or all of this work for personal or classroom use is granted without 
fee provided that copies are not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. Copyrights for components of this work owned 
by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, 
to post on servers or to redistribute to lists, requires specific permission and/or a fee. Jonathan Shade 
Steven Gortlerr Li-wei He Richard Szeliski University of Washington rHarvard University Stanford University 
Microsoft Research Abstract In this paper we present a set of ef.cient image based rendering methods 
capable of rendering multiple frames per second on a PC. The .rst method warps Sprites with Depth representing 
smooth sur­faces without the gaps found in other techniques. A second method for more general scenes 
performs warping from an intermediate rep­resentation called a Layered Depth Image (LDI). An LDI is a 
view of the scene from a single input camera view, but with multiple pixels along each line of sight. 
The size of the representation grows only linearly with the observed depth complexity in the scene. Moreover, 
because the LDI data are represented in a single image coordinate system, McMillan s warp ordering algorithm 
can be successfully adapted. As a result, pixels are drawn in the output image in back­to-front order. 
No z-buffer is required, so alpha-compositing can be done ef.ciently without depth sorting. This makes 
splatting an ef.cient solution to the resampling problem. 1 Introduction Image based rendering (IBR) 
techniques have been proposed as an ef.cient way of generating novel views of real and synthetic objects. 
With traditional rendering techniques, the time required to render an image increases with the geometric 
complexity of the scene. The rendering time also grows as the requested shading computations (such as 
those requiring global illumination solutions) become more ambitious. The most familiar IBR method is 
texture mapping. An image is remapped onto a surface residing in a three-dimensional scene. Tra­ditional 
texture mapping exhibits two serious limitations. First, the pixelization of the texture map and that 
of the .nal image may be vastly different. The aliasing of the classic in.nite checkerboard .oor is a 
clear illustration of the problems this mismatch can cre­ate. Secondly, texture mapping speed is still 
limited by the surface the texture is applied to. Thus it would be very dif.cult to create a texture 
mapped tree containing thousands of leaves that exhibits appropriate parallax as the viewpoint changes. 
Two extensions of the texture mapping model have recently been presented in the computer graphics literature 
that address these two dif.culties. The .rst is a generalization of sprites. Once a complex scene is 
rendered from a particular point of view, the image that would be created from a nearby point of view 
will likely be similar. In this case, the original 2D image, or sprite, can be slightly altered by a 
2D af.ne or projective transformation to approximate the view from the new camera position [30, 26, 14]. 
The sprite approximation s .delity to the correct new view is highly dependent on the geometry being 
represented. In particular, the errors increase with the amount of depth variation in the real part of 
the scene captured by the sprite. The amount of virtual camera motion away from the point of view of 
sprite creation also increases the error. Errors decrease with the distance of the geometry from the 
virtual camera. The second recent extension is to add depth information to an image to produce a depth 
image and to then use the optical .ow that would be induced by a camera shift to warp the scene into 
an approximation of the new view [2, 21]. Each of these methods has its limitations. Simple sprite warping 
cannot produce the parallax induced when parts of the scenes have sizable differences in distance from 
the camera. Flowing a depth image pixel by pixel, on the other hand, can provide proper parallax but 
will result in gaps in the image either due to visibility changes when some portion of the scene become 
unoccluded, or when a surface is magni.ed in the new view. Some solutions have been proposed to the latter 
problem. Laveau and Faugeras suggest performing a backwards mapping from the output sample location to 
the input image [13]. This is an expensive operation that requires some amount of searching in the input 
image. Another possible solution is to think of the input image as a mesh of micro-polygons, and to scan-convert 
these polygons in the output image. This is an expensive operation, as it requires a polygon scan-convert 
setup for each input pixel [17], an operation we would prefer to avoid especially in the absence of specialized 
rendering hardware. Alternatively one could use multiple input images from different viewpoints. However, 
if one uses n input images, one effectively multiplies the size of the scene description by n, and the 
rendering cost increases accordingly. This paper introduces two new extensions to overcome both of these 
limitations. The .rst extension is primarily applicable to smoothly varying surfaces, while the second 
is useful primarily for very com­plex geometries. Each method provides ef.cient image based ren­dering 
capable of producing multiple frames per second on a PC. In the case of sprites representing smoothly 
varying surfaces, we introduce an algorithm for rendering Sprites with Depth. The algo­rithm .rst forward 
maps (i.e., warps) the depth values themselves and then uses this information to add parallax corrections 
to a stan­dard sprite renderer. For more complex geometries, we introduce the Layered Depth Im­age, or 
LDI, that contains potentially multiple depth pixels at each discrete location in the image. Instead 
of a 2D array of depth pixels (a pixel with associated depth information), we store a 2D array of layered 
depth pixels. A layered depth pixel stores a set of depth pixels along one line of sight sorted in front 
to back order. The front element in the layered depth pixel samples the .rst surface seen along that 
line of sight; the next pixel in the layered depth pixel samples the next surface seen along that line 
of sight, etc. When rendering from an LDI, the requested view can move away from the original LDI view 
and expose surfaces that were not visible in the .rst layer. The previously occluded regions may still 
be rendered from data stored in some later layer of a layered depth pixel. There are many advantages 
to this representation. The size of the Environment Map Figure 1 Different image based primitives can 
serve well depending on distance from the camera representation grows linearly only with the depth complexity 
of the image. Moreover, because the LDI data are represented in a single image coordinate system, McMillan 
s ordering algorithm [20] can be successfully applied. As a result, pixels are drawn in the output image 
in back to front order allowing proper alpha blending without depth sorting. No z-buffer is required, 
so alpha-compositing can be done ef.ciently without explicit depth sorting. This makes splatting an ef.cient 
solution to the reconstruction problem. Sprites with Depth and Layered Depth Images provide us with two 
new image based primitives that can be used in combination with traditional ones. Figure 1 depicts .ve 
types of primitives we may wish to use. The camera at the center of the frustum indicates where the image 
based primitives were generated from. The viewing vol­ume indicates the range one wishes to allow the 
camera to move while still re-using these image based primitives. The choice of which type of image-based 
or geometric primitive to use for each scene element is a function of its distance, its in­ternal depth 
variation relative to the camera, as well as its internal geometric complexity. For scene elements at 
a great distance from the camera one might simply generate an environment map. The environment map is 
invariant to translation and simply translates as a whole on the screen based on the rotation of the 
camera. At a somewhat closer range, and for geometrically planar elements, traditional planar sprites 
(or image caches) may be used [30, 26]. The assumption here is that although the part of the scene depicted 
in the sprite may display some parallax relative to the background environment map and other sprites, 
it will not need to depict any parallax within the sprite itself. Yet closer to the camera, for ele­ments 
with smoothly varying depth, Sprites with Depth are capable of displaying internal parallax but cannot 
deal with disocclusions due to image .ow that may arise in more complex geometric scene elements. Layered 
Depth Images deal with both parallax and dis­occlusions and are thus useful for objects near the camera 
that also contain complex geometries that will exhibit considerable parallax. Finally, traditional polygon 
rendering may need to be used for im­mediate foreground objects. In the sections that follow, we will 
concentrate on describing the data structures and algorithms for representing and rapidly rendering Sprites 
with Depth and Layered Depth Images. 2 Previous Work Over the past few years, there have been many papers 
on image based rendering. In [16], Levoy and Whitted discuss rendering point data. Chen and Williams 
presented the idea of rendering from images [2]. Laveau and Faugeras discuss IBR using a backwards map 
[13]. McMillan and Bishop discuss IBR using cylindrical views [21]. Seitz and Dyer describe a system 
that allows a user to correctly model view transforms in a user controlled image morphing sys­tem [28]. 
In a slightly different direction, Levoy and Hanrahan [15] and Gortler et al. [7] describe IBR methods 
using a large number of input images to sample the high dimensional radiance function. Max uses a representation 
similar to an LDI [18], but for a purpose quite different than ours; his purpose is high quality anti-aliasing, 
while our goal is ef.ciency. Max reports his rendering time as 5 minutes per frame while our goal is 
multiple frames per second. Max warps from n input LDIs with different camera information; the multiple 
depth layers serve to represent the high depth complexity of trees. We warp from a single LDI, so that 
the warping can be done most ef.ciently. For output, Max warps to an LDI. This is done so that, in conjunction 
with an A-buffer, high quality, but somewhat expensive, anti-aliasing of the output picture can be performed. 
Mark et al.[17] and Darsa et al.[4] create triangulated depth maps from input images with per-pixel depth. 
Darsa concentrates on limiting the number of triangles by looking for depth coherence across regions 
of pixels. This triangle mesh is then rendered tra­ditionally taking advantage of graphics hardware pipelines. 
Mark et al.describe the use of multiple input images as well. In this aspect of their work, speci.c triangles 
are given lowered priority if there is a large discontinuity in depth across neighboring pixels. In this 
case, if another image .lls in the same area with a triangle of higher priority, it is used instead. 
This helps deal with disocclusions. Shade et al.[30] and Shau.er et al.[26] render complex portions of 
a scene such as a tree onto alpha matted billboard-like sprites and then reuse them as textures in subsequent 
frames. Lengyel and Snyder [14] extend this work by warping sprites by a best .t af.ne transformation 
based on a set of sample points in the underlying 3D model. These af.ne transforms are allowed to vary 
in time as the position and/or color of the sample points change. Hardware considerations for such system 
are discussed in [31]. Horry et al. [10] describe a very simple sprite-like system in which a user interactively 
indicates planes in an image that represent areas in a given image. Thus, from a single input image and 
some user sup­plied information, they can warp an image and provide approximate three dimensional cues 
about the scene. The system presented here relies heavily on McMillan s ordering algorithm [20, 19, 21]. 
Using input and output camera information, a warping order is computed such that pixels that map to the 
same location in the output image are guaranteed to arrive in back to front order. In McMillan s work, 
the depth order is computed by .rst .nding the projection of the output camera s location in the input 
camera s image plane, that is, the intersection of the line joining the two camera locations with the 
input camera s image plane. The line joining the two camera locations is called the epipolar line, and 
the intersection with the image plane is called an epipolar point [6] (see Figure 1). The input image 
is then split horizontally and vertically at the epipolar point, generally creating 4 image quadrants. 
(If the epipolar point lies off the image plane, we may have only 2 or 1 regions.) The pixels in each 
of the quadrants are processed in a different order. Depending on whether the output camera is in front 
of or behind the input camera, the pixels in each quadrant are processed either inward towards the epipolar 
point or outwards away from it. In other words, one of the quadrants is processed left to right, top 
to bottom, another is processed left to right, bottom to top, etc. McMillan discusses in detail the various 
special cases that arise and proves that this ordering is guaranteed to produce depth ordered output 
[19]. When warping from an LDI, there is effectively only one input cam­era view. Therefore one can use 
the ordering algorithm to order the layered depth pixels visited. Within each layered depth pixel, the 
layers are processed in back to front order. The formal proof of [19] applies, and the ordering algorithm 
is guaranteed to work.  3 Rendering Sprites Sprites are texture maps or images with alphas (transparent 
pixels) rendered onto planar surfaces. They can be used either for locally caching the results of slower 
rendering and then generating new views by warping [30, 26, 31, 14], or they can be used directly as 
drawing primitives (as in video games). The texture map associated with a sprite can be computed by simply 
choosing a 3D viewing matrix and projecting some portion of the scene onto the image plane. In practice, 
a view associated with the current or expected viewpoint is a good choice. A 3D plane equation can also 
be computed for the sprite, e.g., by .tting a 3D plane to the z-buffer values associated with the sprite 
pixels. Below, we derive the equations for the 2D perspective mapping between a sprite and its novel 
view. This is useful both for implementing a backward mapping algorithm, and lays the foundation for 
our Sprites with Depth rendering algorithm. A sprite consists of an alpha-matted image I1(x1, y1), a 
4 × 4 camera matrix C1 which maps from 3D world coordinates (X, Y, Z, 1) into the sprite s coordinates 
(x1, y1, z1, 1), w1x1 X .w1y1l.Y l . = C1 . , (1) w1z1 Z w11 (z1 is the z-buffer value), and a plane 
equation. This plane equation can either be speci.ed in world coordinates, AX + BY + CZ + D =0, or it 
can be speci.ed in the sprite s coordinate system, ax1+ by1+ cz1+ d = 0. In the former case, we can form 
a new camera matrix C 1 by replacing the third row of C1 with the row [ABCD], while in the latter, we 
can compute C 1 = PC1, where 1000 .0100l P = . abcd 0010 (note that [ABCD]=[abcd]C1). In either case, 
we can write the modi.ed projection equation as w1x1 X .w1y1 l .l . = C 1 Y . , (2) w1d1 Z w11 where 
d1 = 0 for pixels on the plane. For pixels off the plane, d1 is the scaled perpendicular distance to 
the plane (the scale factor is 1 if A2+ B2+ C2 = 1) divided by the pixel to camera distance w1. Given 
such a sprite, how do we compute the 2D transformation associated with a novel view C 2? The mapping 
between pixels (x1, y1, d1, 1) in the sprite and pixels (w2x2, w2y2, w2d2, w2)inthe output camera s image 
is given by the transfer matrix T1,2 = C 2 ·C 1 -1. For a .at sprite (d1 = 0), the transfer equation 
can be written as w2x2 x1 . w2y2. = H1,2y1 (3) w21 where H1,2 is the 2D planar perspective transformation 
(homogra­phy) obtained by dropping the third row and column of T1,2. The coordinates (x2, y2) obtained 
after dividing out w2 index a pixel ad­dress in the output camera s image. Ef.cient backward mapping 
techniques exist for performing the 2D perspective warp [8, 34], or texture mapping hardware can be used. 
3.1 Sprites with Depth The descriptive power (realism) of sprites can be greatly enhanced by adding an 
out-of-plane displacement component d1 at each pixel in the sprite.1 Unfortunately, such a representation 
can no longer be rendered directly using a backward mapping algorithm. Using the same notation as before, 
we see that the transfer equation is now w2x2 x1 . . w2y2= H1,2y1+ d1e1,2, (4) w21 1The d1 values can 
be stored as a separate image, say as 8-bit signed depths. The full precision of a traditional z-buffer 
is not required, since these depths are used only to compute local parallax, and not to perform z-buffer 
merging of primitives. Furthermore, the d1 image could be stored at a lower resolution than the color 
image, if desired. where e1,2 is called epipole [6, 25, 11], and is obtained from the third column of 
T1,2. Equation (4) can be used to forward map pixels from a sprite to a new view. Unfortunately, this 
entails the usual problems associated with forward mapping, e.g., the necessity to .ll gaps or to use 
larger splatting kernels, and the dif.culty in achieving proper resampling. Notice, however, that Equation 
(4) could be used to perform a back­ward mapping step by interchanging the 1 and 2 indices, if only we 
knew the displacements d2 in the output camera s coordinate frame. A solution to this problem is to .rst 
forward map the displacements d1, and to then use Equation (4) to perform a backward mapping step with 
the new (view-based) displacements. While this may at .rst appear to be no faster or more accurate than 
simply forward warping the color values, it does have some signi.cant advantages. First, small errors 
in displacement map warping will not be as ev­ident as errors in the sprite image warping, at least if 
the displace­ment map is smoothly varying (in practice, the shape of a simple surface often varies more 
smoothly than its photometry). If bilinear or higher order .ltering is used in the .nal color (backward) 
re­sampling, this two-stage warping will have much lower errors than forward mapping the colors directly 
with an inaccurate forward map. We can therefore use a quick single-pixel splat algorithm followed by 
a quick hole .lling, or alternatively, use a simple 2 × 2 splat. The second main advantage is that we 
can design the forward warp­ing step to have a simpler form by factoring out the planar perspective warp. 
Notice that we can rewrite Equation (4) as w2x2 x3 w2y2. = H1,2 y3. , (5) w2 1 with w3x3 x1 w3y3. = y1. 
+ d1e 1,2, (6) w3 1 = H-1 where e1,2 1,2 e1,2. This suggests that Sprite with Depth rendering can be 
implemented by .rst shifting pixels by their local parallax, .lling any resulting gaps, and then applying 
a global homography (planar perspective warp). This has the advantage that it can handle large changes 
in view (e.g., large zooms) with only a small amount of gap .lling (since gaps arise only in the .rst 
step, and are due to variations in displacement). Our novel two-step rendering algorithm thus proceeds 
in two stages: 1. forward map the displacement map d1(x1, y1), using only the parallax component given 
in Equation (6) to obtain d3(x3, y3); 2a. backward map the resulting warped displacements d3(x3, y3) 
using Equation (5) to obtain d2(x2, y2) (the displacements in the new camera view); 2b. backward map 
the original sprite colors, using both the ho­mography H2,1 and the new parallax d2 as in Equation (4) 
(with the 1 and 2 indices interchanged), to obtain the image corresponding to camera C2. The last two 
operations can be combined into a single raster scan over the output image, avoiding the need to perspective 
warp d3 into d2. More precisely, for each output pixel (x2, y2), we compute (x3, y3) such that w3x3 x2 
w3y3. = H2,1 y2 (7) . w31 to compute where to look up the displacement d3(x3, y3), and form the .nal 
address of the source sprite pixel using w1x1 w3x3 w1y1. = w3y3. + d3(x3, y3)e2,1. (8) w1 w3 We can 
obtain a quicker, but less accurate, algorithm by omitting the .rst step, i.e., the pure parallax warp 
from d1 to d3. Ifweas­sume the depth at a pixel before and after the warp will not change signi.cantly, 
we can use d1 instead of d3 in Equation (8). This still gives a useful illusion of 3-D parallax, but 
is only valid for a much smaller range of viewing motions (see Figure 3). Another variant on this algorithm, 
which uses somewhat more stor­age but fewer computations, is to compute a 2-D displacement .eld in the 
.rst pass, u3(x3, y3)= x1 - x3, v3(x3, y3)= y1 - y3, where (x3, y3) is computed using the pure parallax 
transform in Equation (6). In the second pass, the .nal pixel address in the sprite is com­puted using 
. . . x1 x3 u3(x3, y3) =+, (9) y1 y3 v3(x3, y3) where this time (x3, y3) is computed using the transform 
given in Equation (7). We can make the pure parallax transformation (6) faster by avoiding the per-pixel 
division required after adding homogeneous coordi­nates. One way to do this is to approximate the parallax 
transforma­tion by .rst moving the epipole to in.nity (setting its third compo­nent to 0). This is equivalent 
to having an af.ne parallax component (all points move in the same direction, instead of towards a common 
vanishing point). In practice, we .nd that this still provides a very compelling illusion of 3D shape. 
Figure 3 shows some of the steps in our two-pass warping algorithm. Figures 3a and 3f show the original 
sprite (color) image and the depth map. Figure 3b shows the sprite warped with no parallax. Figures 3g, 
3h, and 3i shows the depth map forward warped with only pure parallax, only the perspective projection, 
and both. Figure 3c shows the backward warp using the incorrect depth map d1 (note how dark background 
colors are mapped onto the bump ), whereas Figure 3d shows the backward warp using the correct depth 
map d3. The white pixels near the right hand edge are a result of using only a single step of gap .lling. 
Using three steps results in the better quality image shown in Figure 3e. Gaps also do not appear for 
a less quickly slanting d maps, such as the pyramid shown in Figure 3j. The rendering times for the 256 
×256 image shown in Figure 3 on a 300 MHz Pentium II are as follows. Using bilinear pixel sampling, the 
frame rates are 30 Hz for no z-parallax, 21 Hz for crude one­pass warping (no forward warping of d1 values), 
and 16 Hz for two-pass warping. Using nearest-neighbor resampling, the frame rates go up to 47 Hz, 24 
Hz, and 20 Hz, respectively.  3.2 Recovering sprites from image sequences While sprites and sprites 
with depth can be generated using com­puter graphics techniques, they can also be extracted from image 
sequences using computer vision techniques. To do this, we use a layered motion estimation algorithm 
[32, 1], which simultaneously segments the sequence into coherently moving regions, and com­putes a parametric 
motion estimate (planar perspective transforma­tion) for each layer. To convert the recovered layers 
into sprites, we need to determine the plane equation associated with each region. We do this by tracking 
features from frame to frame and applying  (f) (g) (h) (i) (j) Figure 3 Plane with bump rendering example: 
(a) input color (sprite) image I1(x1, y1); (b) sprite warped by homography only (no parallax); (c) sprite 
warped by homography and crude parallax (d1); (d) sprite warped by homography and true parallax (d2); 
(e) with gap .ll width set to 3; (f) input depth map d1(x1, y1); (g) pure parallax warped depth map d3(x3, 
y3); (h) forward warped depth map d2(x2, y2); (i) forward warped depth map without parallax correction; 
(j) sprite with pyramid depth map. (f) (g) (h) Figure 4 Results of sprite extraction from image sequence: 
(a) third of .ve images; (b) initial segmentation into six layers; (c) recovered depth map; (d) the .ve 
layer sprites; (e) residual depth image for .fth layer; (f) re-synthesized third image (note extended 
.eld of view); (g) novel view without residual depth; (h) novel view with residual depth (note the rounding 
of the people). Figure 5 Layered Depth Image a standard structure from motion algorithm to recover the 
camera parameters (viewing matrices) for each frame [6]. Tracking several points on each sprite enables 
us to reconstruct their 3D positions, and hence to estimate their 3D plane equations [1]. Once the sprite 
pixel assignment have been recovered, we run a traditional stereo algorithm to recover the out-of-plane 
displacements. The results of applying the layered motion estimation algorithm to the .rst .ve images 
from a 40-image stereo dataset2 are shown in Figure 4. Figure 4(a) shows the middle input image, Figure 
4(b) shows the initial pixel assignment to layers, Figure 4(c) shows the recovered depth map, and Figure 
4(e) shows the residual depth map for layer 5. Figure 4(d) shows the recovered sprites. Figure 4(f) shows 
the middle image re-synthesized from these sprites, while Figures 4(g h) show the same sprite collection 
seen from a novel viewpoint (well outside the range of the original views), both with and without residual 
depth-based correction (parallax). The gaps visible in Figures 4(c) and 4(f) lie outside the area corresponding 
to the middle image, where the appropriate parts of the background sprites could not be seen.  4 Layered 
Depth Images While the use of sprites and Sprites with Depth provides a fast means to warp planar or 
smoothly varying surfaces, more general scenes require the ability to handle more general disocclusions 
and large amounts of parallax as the viewpoint moves. These needs have led to the development of Layered 
Depth Images (LDI). Like a sprite with depth, pixels contain depth values along with their colors (i.e., 
a depth pixel). In addition, a Layered Depth Image (Fig­ure 5) contains potentially multiple depth pixels 
per pixel location. The farther depth pixels, which are occluded from the LDI center, will act to .ll 
in the disocclusions that occur as the viewpoint moves away from the center. The structure of an LDI 
is summarized by the following conceptual representation: DepthPixel = ColorRGBA: 32 bit integer Z: 20 
bit integer SplatIndex: 11 bit integer LayeredDepthPixel = NumLayers: integer Layers[0..numlayers-1]: 
array of DepthPixel 2Courtesy of Dayton Taylor. LayeredDepthImage = Camera: camera Pixels[0..xres-1,0..yres-1]: 
array of LayeredDepthPixel The layered depth image contains camera information plus an array of size 
xres by yres layered depth pixels. In addition to image data, each layered depth pixel has an integer 
indicating how many valid depth pixels are contained in that pixel. The data contained in the depth pixel 
includes the color, the depth of the object seen at that pixel, plus an index into a table that will 
be used to calculate a splat size for reconstruction. This index is composed from a combination of the 
normal of the object seen and the distance from the LDI camera. In practice, we implement Layered Depth 
Images in two ways. When creating layered depth images, it is important to be able to ef.ciently insert 
and delete layered depth pixels, so the Layers array in the LayeredDepthPixel structure is implemented 
as a linked list. When rendering, it is important to maintain spatial locality of depth pixels in order 
to most effectively take advantage of the cache in the CPU [12]. In Section 5.1 we discuss the compact 
render-time version of layered depth images. There are a variety of ways to generate an LDI. Given a 
synthetic scene, we could use multiple images from nearby points of view for which depth information 
is available at each pixel. This informa­tion can be gathered from a standard ray tracer that returns 
depth per pixel or from a scan conversion and z-buffer algorithm where the z-buffer is also returned. 
Alternatively, we could use a ray tracer to sample an environment in a less regular way and then store 
com­puted ray intersections in the LDI structure. Given multiple real images, we can turn to computer 
vision techniques that can infer pixel correspondence and thus deduce depth values per pixel. We will 
demonstrate results from each of these three methods. 4.1 LDIs from Multiple Depth Images We can construct 
an LDI by warping n depth images into a com­mon camera view. For example the depth images C2 and C3 in 
Figure 5 can be warped to the camera frame de.ned by the LDI (C1 in .gure 5). 3 If, during the warp from 
the input camera to the LDI camera, two or more pixels map to the same layered depth pixel, their Z values 
are compared. If the Z values differ by more than a preset epsilon, a new layer is added to that layered 
depth pixel for each distinct Z value (i.e., NumLayers is incremented and a new depth pixel is added), 
otherwise (e.g., depth pixels c and d in .g­ure 5), the values are averaged resulting in a single depth 
pixel. This preprocessing is similar to the rendering described by Max [18]. This construction of the 
layered depth image is effectively decou­pled from the .nal rendering of images from desired viewpoints. 
Thus, the LDI construction does not need to run at multiple frames per second to allow interactive camera 
motion.  4.2 LDIs from a Modi.ed Ray Tracer By construction, a Layered Depth Image reconstructs images 
of a scene well from the center of projection of the LDI (we simply display the nearest depth pixels). 
The quality of the reconstruction from another viewpoint will depend on how closely the distribution 
of depth pixels in the LDI, when warped to the new viewpoint, corresponds to the pixel density in the 
new image. Two common events that occur are: (1) disocclusions as the viewpoint changes, 3Any arbitrary 
single coordinate system can be speci.ed here. However, we have found it best to use one of the original 
camera coordinate systems. This results in fewer pixels needing to be resampled twice; once in the LDI 
construction, and once in the rendering process.  Figure 6 An LDI consists of the 90 degree frustum 
exiting one side of a cube. The cube represents the region of interest in which the viewer will be able 
to move. and (2) surfaces that grow in terms of screen space. For example, when a surface is edge on 
to the LDI, it covers no area. Later, it may face the new viewpoint and thus cover some screen space. 
When using a ray tracer, we have the freedom to sample the scene with any distribution of rays we desire. 
We could simply allow the rays emanating from the center of the LDI to pierce surfaces, recording each 
hit along the way (up to some maximum). This would solve the disocclusion problem but would not effectively 
sample surfaces edge on to the LDI. What set of rays should we trace to sample the scene, to best ap­proximate 
the distribution of rays from all possible viewpoints we are interested in? For simplicity, we have chosen 
to use a cubical region of empty space surrounding the LDI center to represent the region that the viewer 
is able to move in. Each face of the viewing cube de.nes a 90 degree frustum which we will use to de.ne 
a single LDI (Figure 6). The six faces of the viewing cube thus cover all of space. For the following 
discussion we will refer to a single LDI. Each ray in free space has four coordinates, two for position 
and two for direction. Since all rays of interest intersect the cube faces, we will choose the outward 
intersection to parameterize the position of the ray. Direction is parameterized by two angles. Given 
no a priori knowledge of the geometry in the scene, we as­sume that every ray intersection the cube is 
equally important. To achieve a uniform density of rays we sample the positional coor­dinates uniformly. 
A uniform distribution over the hemisphere of directions requires that the probability of choosing a 
direction is pro­portional to the projected area in that direction. Thus, the direction is weighted by 
the cosine of the angle off the normal to the cube face. Choosing a cosine weighted direction over a 
hemisphere can be accomplished by uniformly sampling the unit disk formed by the base of the hemisphere 
to get two coordinates of the ray direction, say x and y if the z-axis is normal to the disk. The third 
coordinate is chosen to give a unit length (z =1 - x2 - y2). We make the selection within the disk by 
.rst selecting a point in the unit square, then applying a measure preserving mapping [23] that maps 
the unit square to the unit disk. Given this desired distribution of rays, there are a variety of ways 
to perform the sampling: Uniform. A straightforward stochastic method would take as input the number 
of rays to cast. Then, for each ray it would choose an origin on the cube face and a direction from the 
cosine distribution Figure 7 Intersections from sampling rays A and B are added to the same layered depth 
pixel. and cast the ray into the scene. There are two problems with this simple scheme. First, such white 
noise distributions tend to form unwanted clumps. Second, since there is no coherence between rays, complex 
scenes require considerable memory thrashing since rays will access the database in a random way [24]. 
The model of the chestnut tree seen in the color images was too complex to sample with a pure stochastic 
method on a machine with 320MB of memory. Strati.ed Stochastic. To improve the coherence and distribution 
of rays, we employ a strati.ed scheme. In this method, we divide the 4D space of rays uniformly into 
a grid of N × N × N × N strata. For each stratum, we cast M rays. Enough coherence exists within a stratum 
that swapping of the data set is alleviated. Typical values for N and M are 32 and 16, generating approximately 
16 million rays per cube face. Once a ray is chosen, we cast it into the scene. If it hits an object, 
and that object lies in the LDI s frustum, we reproject the intersection into the LDI, as depicted in 
Figure 7, to determine which layered depth pixel should receive the sample. If the new sample is within 
an epsilon tolerance in depth of an existing depth pixel, the color of the new sample is averaged with 
the existing depth pixel. Otherwise, the color, normal, and distance to the sample create a new depth 
pixel that is inserted into the Layered Depth Pixel.  4.3 LDIs from Real Images The dinosaur model in 
Figure 13 is constructed from 21 photographs of the object undergoing a 360 degree rotation on a computer­controlled 
calibrated turntable. An adaptation of Seitz and Dyer s voxel coloring algorithm [29] is used to obtain 
the LDI represen­tation directly from the input images. The regular voxelization of Seitz and Dyer is 
replaced by a view-centered voxelization similar to the LDI structure. The procedure entails moving outward 
on rays from the LDI camera center and projecting candidate voxels back into the input images. If all 
input images agree on a color, this voxel is .lled as a depth pixel in the LDI structure. This approach 
en­ables straightforward construction of LDI s from images that do not contain depth per pixel.  5 Rendering 
Layered Depth Images Our fast warping-based renderer takes as input an LDI along with its associated 
camera information. Given a new desired camera posi­tion, the warper uses an incremental warping algorithm 
to ef.ciently create an output image. Pixels from the LDI are splatted into the output image using the 
over compositing operation. The size and footprint of the splat is based on an estimated size of the 
reprojected pixel. 5.1 Space Ef.cient Representation When rendering, it is important to maintain the 
spatial locality of depth pixels to exploit the second level cache in the CPU [12]. To this end, we reorganize 
the depth pixels into a linear array ordered from bottom to top and left to right in screen space, and 
back to front along a ray. We also separate out the number of layers in each layered depth pixel from 
the depth pixels themselves. The layered depth pixel structure does not exist explicitly in this implementation. 
Instead, a double array of offsets is used to locate each depth pixel. The number of depth pixels in 
each scanline is accumulated into a vector of offsets to the beginning of each scanline. Within each 
scanline, for each pixel location, a total count of the depth pixels from the beginning of the scanline 
to that location is maintained. Thus to .nd any layered depth pixel, one simply offsets to the beginning 
of the scanline and then further to the .rst depth pixel at that location. This supports scanning in 
right-to-left order as well as the clipping operation discussed later.  5.2 Incremental Warping Computation 
The incremental warping computation is similar to the one used for certain texture mapping operations 
[9, 27]. The geometry of this computation has been analyzed by McMillan [22], and ef.cient computation 
for the special case of orthographic input images is given in [3]. Let C1 be the 4 × 4 matrix for the 
LDI camera. It is composed of an af.ne transformation matrix, a projection matrix, and a viewport matrix, 
C1= V1 ·P1 ·A1. This camera matrix transforms a point from the global coordinate system into the camera 
s projected image co­ordinate system. The projected image coordinates (x1, y1), obtained after multiplying 
the point s global coordinates by C1 and dividing out w1, index a screen pixel address. The z1 coordinate 
can be used for depth comparisons inazbuffer. Let C2 be the output camera s matrix. De.ne the transfer 
matrix as T1,2 = C2 · C1 -1. Given the projected image coordinates of some point seen in the LDI camera 
(e.g., the coordinates of a in Figure 5), this matrix computes the image coordinates as seen in the output 
camera (e.g., the image coordinates of a2 in camera C2 in Figure 5). x1 x2 · w2 . y1 l. y2 · w2 l T1,2 
· . = . = result z1 z2 · w2 1 w2 The coordinates (x2, y2) obtained after dividing by w2, index a pixel 
address in the output camera s image. Using the linearity of matrix operations, this matrix multiply 
can be factored to reuse much of the computation from each iteration through the layers of a layered 
depth pixel; result can be computed as x1 x10 . y1 l. y1 l. 0 l  T1,2 · . = T1,2 · . + z1 · T1,2 · . 
= start + z1 · depth z10 1 11 0 To compute the warped position of the next layered depth pixel along 
a scanline, the new start is simply incremented. C2 Figure 8 Values for size computation of a projected 
pixel. x1+1 x11 . y1 l. y1 l. 0 l T1,2 · . = T1,2 · . + T1,2 · . = start + xincr 0 00 1 10 The warping 
algorithm proceeds using McMillan s ordering algo­rithm [20]. The LDI is broken up into four regions 
above and below and to the left and right of the epipolar point. For each quadrant, the LDI is traversed 
in (possibly reverse) scan line order. At the beginning of each scan line, start is computed. The sign 
of xincr is determined by the direction of processing in this quadrant. Each layered depth pixel in the 
scan line is then warped to the output image by calling Warp. This procedure visits each of the layers 
in back to front order and computes result to determine its location in the output image. As in perspective 
texture mapping, a divide is required per pixel. Finally, the depth pixel s color is splatted at this 
location in the output image. The following pseudo code summarizes the warping algorithm ap­plied to 
each layered depth pixel. procedure Warp(ldpix, start, depth, xincr) for k f0 to dpix.NumLayers-1 z1 
fldpix.Layers[k].Z result fstart +z1 v depth //cull if the depth pixel goes behind the output camera 
//or if the depth pixel goes out of the output cam s frustum if result.w > 0 and IsInViewport(result) 
then result fresult / result.w // see next section sqrtSize fz2 v lookupTable[ldpix.Layers[k].SplatIndex] 
splat(ldpix.Layers[k].ColorRGBA, x2, y2, sqrtSize)  end if // increment for next layered pixel on this 
scan line start fstart + xincr end for end procedure  5.3 Splat Size Computation To splat the LDI into 
the output image, we estimate the projected area of the warped pixel. This is a rough approximation to 
the footprint evaluation [33] optimized for speed. The proper size can be computed (differentially) as 
(d1)2 cos(e2) res2 tan(fov1/2)size = (d2)2 cos(e1) res1 tan(fov2/2) where d1 is the distance from the 
sampled surface point to the LDI camera, fov1 is the .eld of view of the LDI camera, res1=(w1h1)-1 where 
w1 and h1 are the width and height of the LDI, and e1 is the angle between the surface normal and the 
line of sight to the LDI camera (see Figure 8). The same terms with subscript 2 refer to the output camera. 
It will be more ef.cient to compute an approximation of the square root of size, . f size = 1 d2 · d1 
cos(e2)res2tan(fov1/2) . cos(e1)res1tan(fov2/2) 1 d1 cos(<2)res2tan(fov1/2) Z2 cos(<1)res1tan(fov2/2) 
d1 cos(<2)res2tan(fov1/2) z2 · . cos(<1)res1tan(fov2/2) We approximate the es as the angles < between 
the surface nor­mal vector and the z axes of the camera s coordinate systems. We also approximate d2 
by Z2, the z coordinate of the sampled point in the output camera s unprojected eye coordinate system. 
During rendering, we set the projection matrix such that z2=1/Z2. The current implementation supports 
4 different splat sizes, so a very crude approximation of the size computation is implemented using a 
lookup table. For each pixel in the LDI, we store d1 using 5 bits. We use 6 bits to encode the normal, 
3 for nx, and 3 for ny. This gives us an eleven-bit lookup table index. Before rendering each new image, 
we use the new output camera information to precompute values for the 2048 possible lookup table indexes. 
At each pixel we f obtain size by multiplying the computed z2 by the value found in the lookup table. 
f size z2 · lookup[nx, ny, d1] To maintain the accuracy of the approximation for d1, we discretize d1 
nonlinearly using a simple exponential function that allocates more bits to the nearby d1 values, and 
fewer bits to the distant d1 values. The four splat sizes we currently use have 1 by 1, 3 by 3, 5 by 
5, and 7 by 7 pixel footprints. Each pixel in a footprint has an alpha value to approximate a Gaussian 
splat kernel. However, the alpha values are rounded to 1, 1/2, or 1/4, so the alpha blending can be done 
with integer shifts and adds. 5.4 Depth Pixel Representation The size of a cache line on current Intel 
processors (Pentium Pro and Pentium II) is 32 bytes. To .t four depth pixels into a single cache line 
we convert the .oating point Z value to a 20 bit integer. This is then packed into a single word along 
with the 11 bit splat table index. These 32 bits along with the R, G, B, and alpha values .ll out the 
8 bytes. This seemingly small optimization yielded a 25 percent improvement in rendering speed. Figure 
9 LDI with two segments 5.5 Clipping The LDI of the chestnut tree scene in Figure 11 is a large data 
set containing over 1.1 million depth pixels. If we naively render this LDI by reprojecting every depth 
pixel, we would only be able to render at one or two frames per second. When the viewer is close to the 
tree, there is no need to .ow those pixels that will fall outside of the new view. Unseen pixels can 
be culled by intersecting the view frustum with the frustum of the LDI. This is implemented by intersecting 
the view frustum with the near and far plane of the LDI frustum, and taking the bounding box of the intersection. 
This region de.nes the rays of depth pixels that could be seen in the new view. This computation is conservative, 
and gives suboptimal results when the viewer is looking at the LDI from the side (see Figure 9). The 
view frustum intersects almost the entire cross section of the LDI frustum, but only those depth pixels 
in the desired view need be warped. Our simple clipping test indicates that most of the LDI needs to 
be warped. To alleviate this, we split the LDI into two segments, a near and a far segment (see Figure 
9). These are simply two frustra stacked one on top of the other. The near frustum is kept smaller than 
the back segment. We clip each segment individually, and render the back segment .rst and the front segment 
second. Clipping can speed rendering times by a factor of 2 to 4.  6 Results Sprites with Depth and 
Layered Depth Images have been imple­mented in C++. The color .gures show two examples of rendering sprites 
and three examples of rendering LDIs. Figures 3a through 3j show the results of rendering a sprite with 
depth. The hemisphere in the middle of the sprite pops out of the plane of the sprite, and the illusion 
of depth is quite good. Figure 4 shows the process of extracting sprites from multiple images using the 
vision techniques discussed in Section 3. There is a great deal of parallax between the layers of sprites, 
resulting in a convincing and inexpensive image­based-rendering method. Figure 10 shows two views of 
a barnyard scene modeled in Sof­timage. A set of 20 images was pre-rendered from cameras that encircle 
the chicken using the Mental Ray renderer. The renderer returns colors, depths, and normals at each pixel. 
The images were rendered at 320 by 320 pixel resolution, taking approximately one minute each to generate. 
In the interactive system, the 3 images out of the 17 that have the closest direction to the current 
camera are chosen. The preprocessor (running in a low-priority thread) uses these images to create an 
LDI in about 1 second. While the LDIs are allocated with a maximum of 10 layers per pixel, the average 
depth complexity for these LDIs is only 1.24. Thus the use of three input images only increases the rendering 
cost by 24 percent. The fast   renderer (running concurrently in a high-priority thread) generates 
images at 300 by 300 resolution. On a Pentium II PC running at 300MHz, we achieved frame rate of 8 to 
10 frames per second. Figures 11 and 12 show two cross-eye stereo pairs of a chestnut tree. In Figure 
11 only the near segment is displayed. Figure 12 shows both segments in front of an environment map. 
The LDIs were created using a modi.ed version of the Rayshade raytracer. The tree model is very large; 
Rayshade allocates over 340 MB of memory to render a single image of the tree. The stochastic method 
discussed in Section 4.2 took 7 hours to trace 16 million rays through this scene using an SGI Indigo2 
with a 250 MHz processor and 320MB of memory. The resulting LDI has over 1.1 million depth pixels, 70,000 
of which were placed in the near segment with the rest in the far segment. When rendering this interactively 
we attain frame rates between 4 and 10 frames per second on a Pentium II PC running at 300MHz.  7 Discussion 
In this paper, we have described two novel techniques for image based rendering. The .rst technique renders 
Sprites with Depth without visible gaps, and with a smoother rendering than traditional forward mapping 
(splatting) techniques. It is based on the observa­tion that a forward mapped displacement map does not 
have to be as accurate as a forward mapped color image. If the displacement map is smooth, the inaccuracies 
in the warped displacement map result in only sub-pixel errors in the .nal color pixel sample positions. 
Our second novel approach to image based rendering is a Layered Depth Image representation. The LDI representation 
provides the means to display the parallax induced by camera motion as well as reveal disoccluded regions. 
The average depth complexity in our LDI s is much lower that one would achieve using multiple input images 
(e.g., only 1.24 in the Chicken LDI). The LDI representation takes advantage of McMillan s ordering algorithm 
allowing pixels to be splatted back to Front with an over compositing operation. Traditional graphics 
elements and planar sprites can be combined with Sprites with Depth and LDIs in the same scene if a back-to-front 
ordering is maintained. In this case they are simply composited onto one another. Without such an ordering 
a z-buffer approach will still work at the extra cost of maintaining depth information per frame. Choosing 
a single camera view to organize the data has the advan­tage of having sampled the geometry with a preference 
for views very near the center of the LDI. This also has its disadvantages. First, pixels undergo two 
resampling steps in their journey from in­put image to output. This can potentially degrade image quality. 
Secondly, if some surface is seen at a glancing angle in the LDIs view the depth complexity for that 
LDI increases, while the spatial sampling resolution over that surface degrades. The sampling and aliasing 
issues involved in our layered depth image approach are still not fully understood; a formal analysis 
of these issues would be helpful. With the introduction of our two new representations and rendering 
techniques, there now exists a wide range of different image based rendering methods available. At one 
end of the spectrum are tradi­tional texture-mapped models. When the scene does not have too much geometric 
detail, and when texture-mapping hardware is avail­able, this may be the method of choice. If the scene 
can easily be partitioned into non-overlapping sprites (with depth), then triangle­based texture-mapped 
rendering can be used without requiring a z buffer [17, 4]. All of these representations, however, do 
not explicitly account for certain variation of scene appearance with viewpoint, e.g., specu­larities, 
transparency, etc. View-dependent texture maps [5], and 4D representations such as light.elds or Lumigraphs 
[15, 7], have been designed to model such effects. These techniques can lead to greater realism than 
static texture maps, sprites, or Layered Depth Images, but usually require more effort (and time) to 
render. In future work, we hope to explore representations and rendering al­gorithms which combine several 
image based rendering techniques. Automatic techniques for taking a 3D scene (either synthesized or real) 
and re-representing it in the most appropriate fashion for im­age based rendering would be very useful. 
These would allow us to apply image based rendering to truly complex, visually rich scenes, and thereby 
extend their range of applicability. Acknowledgments The authors would .rst of all like to thank Michael 
F. Cohen. Many of the original ideas contained in this paper as well as much of the discussion in the 
paper itself can be directly attributable to him. The authors would also like to thank Craig Kolb for 
his help in obtaining and modifying Rayshade. Steve Seitz is responsible for creating the LDI of the 
dinosaur from a modi.ed version of his earlier code. Andrew Glassner was a great help with some of the 
illustrations in the paper. Finally, we would like to thank Microsoft Research for helping to bring together 
the authors to work on this project.  References [1] S. Baker, R. Szeliski, and P. Anandan. A Layered 
Approach to Stereo Reconstruction. In IEEE Computer Society Conference on Computer Vision and Pattern 
Recognition (CVPR 98). Santa Barbara, June 1998. [2] Shenchang Eric Chen and Lance Williams. View Interpolation 
for Im­age Synthesis. In James T. Kajiya, editor, Computer Graphics (SIG-GRAPH 93 Proceedings), volume 
27, pages 279 288. August 1993. [3] William Dally, Leonard McMillan, Gary Bishop, and Henry Fuchs. The 
Delta Tree: An Object Centered Approach to Image Based Rendering. AI technical Memo 1604, MIT, 1996. 
[4] Lucia Darsa, Bruno Costa Silva, and Amitabh Varshney. Navigating Static Environments Using Image-Space 
Simpli.cation and Morphing. In Proc. 1997 Symposium on Interactive 3D Graphics, pages 25 34. 1997. [5] 
Paul E. Debevec, Camillo J. Taylor, and Jitendra Malik. Modeling and Rendering Architecture from Photographs: 
A Hybrid Geometry­and Image-Based Approach. In Holly Rushmeier, editor, SIGGRAPH 96 Conference Proceedings, 
Annual Conference Series, pages 11 20. ACM SIGGRAPH, Addison Wesley, August 1996. [6] O. Faugeras. Three-dimensional 
computer vision: A geometric view­point. MIT Press, Cambridge, Massachusetts, 1993. [7] Steven J. Gortler, 
Radek Grzeszczuk, Richard Szeliski, and Michael F. Cohen. The Lumigraph. In Holly Rushmeier, editor, 
SIGGRAPH 96 Conference Proceedings, Annual Conference Series, pages 43 54. ACM SIGGRAPH, Addison Wesley, 
August 1996. [8] Paul S. Heckbert. Survey of Texture Mapping. IEEE Computer Graph­ics and Applications, 
6(11):56 67, November 1986. [9] Paul S. Heckbert and Henry P. Moreton. Interpolation for Polygon Texture 
Mapping and Shading. In David Rogers and Rae Earnshaw, editors, State of the Art in Computer Graphics: 
Visualization and Mod­eling, pages 101 111. Springer-Verlag, 1991. [10] Youichi Horry, Ken ichi Anjyo, 
and Kiyoshi Arai. Tour Into the Pic­ture: Using a Spidery Mesh Interface to Make Animation from a Single 
Image. In Turner Whitted, editor, SIGGRAPH 97 Conference Proceed­ings, Annual Conference Series, pages 
225 232. ACM SIGGRAPH, Addison Wesley, August 1997. [11] R. Kumar, P. Anandan, and K. Hanna. Direct recovery 
of shape from multiple views: A parallax based approach. In Twelfth International Conference on Pattern 
Recognition (ICPR 94), volume A, pages 685 688. IEEE Computer Society Press, Jerusalem, Israel, October 
1994. [12] Anthony G. LaMarca. Caches and Algorithms. Ph.D. thesis, University of Washington, 1996. [13] 
S. Laveau and O. D. Faugeras. 3-D Scene Representation as a Col­lection of Images. In Twelfth International 
Conference on Pattern Recognition (ICPR 94), volume A, pages 689 691. IEEE Computer Society Press, Jerusalem, 
Israel, October 1994. [14] Jed Lengyel and John Snyder. Rendering with Coherent Layers. In Turner Whitted, 
editor, SIGGRAPH 97 Conference Proceedings, An­nual Conference Series, pages 233 242. ACM SIGGRAPH, Addison 
Wesley, August 1997. [15] Marc Levoy and Pat Hanrahan. Light Field Rendering. In Holly Rush­meier, editor, 
SIGGRAPH 96 Conference Proceedings, Annual Confer­ence Series, pages 31 42. ACM SIGGRAPH, Addison Wesley, 
August 1996. [16] Mark Levoy and Turner Whitted. The Use of Points as a Display Primitive. Technical 
Report 85-022, University of North Carolina, 1985. [17] William R. Mark, Leonard McMilland, and Gary 
Bishop. Post-Rendering 3D Warping. In Proc. 1997 Symposium on Interactive 3D Graphics, pages 7 16. 1997. 
[18] Nelson Max. Hierarchical Rendering of Trees from Precomputed Multi-Layer Z-Buffers. In Xavier Pueyo 
and Peter Schr¨ oder, editors, Eurographics Rendering Workshop 1996, pages 165 174. Eurograph­ ics, Springer 
Wein, New York City, NY, June 1996. [19] Leonard McMillan. Computing Visibility Without Depth. Technical 
Report 95-047, University of North Carolina, 1995. [20] Leonard McMillan. A List-Priority Rendering Algorithm 
for Redis­playing Projected Surfaces. Technical Report 95-005, University of North Carolina, 1995. [21] 
Leonard McMillan and Gary Bishop. Plenoptic Modeling: An Image-Based Rendering System. In Robert Cook, 
editor, SIGGRAPH 95 Con­ference Proceedings, Annual Conference Series, pages 39 46. ACM SIGGRAPH, Addison 
Wesley, August 1995. [22] Leonard McMillan and Gary Bishop. Shape as a Pertebation to Projec­tive Mapping. 
Technical Report 95-046, University of North Carolina, 1995. [23] Don P. Mitchell. personal communication. 
1997. [24] Matt Pharr, Craig Kolb, Reid Gershbein, and Pat Hanrahan. Render­ing Complex Scenes with Memory-Coherent 
Ray Tracing. In Turner Whitted, editor, SIGGRAPH 97 Conference Proceedings, Annual Con­ference Series, 
pages 101 108. ACM SIGGRAPH, Addison Wesley, August 1997. [25] H. S. Sawhney. 3D Geometry from Planar 
Parallax. In IEEE Com­puter Society Conference on Computer Vision and Pattern Recognition (CVPR 94), 
pages 929 934. IEEE Computer Society, Seattle, Wash­ington, June 1994. [26] Gernot Schau.er and Wolfgang 
St¨A Three-Dimensional urzlinger. Image Cache for Virtual Reality. In Proceedings of Eurographics 96, 
pages 227 236. August 1996. [27] Mark Segal, Carl Korobkin, Rolf van Widenfelt, Jim Foran, and Paul E. 
Haeberli. Fast shadows and lighting effects using texture mapping. In Edwin E. Catmull, editor, Computer 
Graphics (SIGGRAPH 92 Proceedings), volume 26, pages 249 252. July 1992. [28] Steven M. Seitz and Charles 
R. Dyer. View Morphing: Synthesizing 3D Metamorphoses Using Image Transforms. In Holly Rushmeier, editor, 
SIGGRAPH 96 Conference Proceedings, Annual Conference Series, pages 21 30. ACM SIGGRAPH, Addison Wesley, 
August 1996. [29] Steven M. seitz and Charles R. Dyer. Photorealistic Scene Recon­struction by Voxel 
Coloring. In Proc. Computer Vision and Pattern Recognition Conf., pages 1067 1073. 1997. [30] Jonathan 
Shade, Dani Lischinski, David Salesin, Tony DeRose, and John Snyder. Hierarchical Image Caching for Accelerated 
Walk­throughs of Complex Environments. In Holly Rushmeier, editor, SIG-GRAPH 96 Conference Proceedings, 
Annual Conference Series, pages 75 82. ACM SIGGRAPH, Addison Wesley, August 1996. [31] Jay Torborg and 
Jim Kajiya. Talisman: Commodity Real-time 3D Graphics for the PC. In Holly Rushmeier, editor, SIGGRAPH 
96 Con­ference Proceedings, Annual Conference Series, pages 353 364. ACM SIGGRAPH, Addison Wesley, August 
1996. [32] J. Y. A. Wang and E. H. Adelson. Layered Representation for Motion Analysis. In IEEE Computer 
Society Conference on Computer Vision and Pattern Recognition (CVPR 93), pages 361 366. New York, New 
York, June 1993. [33] Lee Westover. Footprint Evaluation for Volume Rendering. In Forest Baskett, editor, 
Computer Graphics (SIGGRAPH 90 Proceedings), volume 24, pages 367 376. August 1990. [34] G. Wolberg. 
Digital Image Warping. IEEE Computer Society Press, Los Alamitos, California, 1990. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280884</article_id>
		<sort_key>243</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>24</seq_no>
		<title><![CDATA[Multiple viewpoint rendering]]></title>
		<page_from>243</page_from>
		<page_to>254</page_to>
		<doi_number>10.1145/280814.280884</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280884</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Hidden line/surface removal</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.9</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39043480</person_id>
				<author_profile_id><![CDATA[81100453651]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Halle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brigham and Women's Hospital]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>105703</ref_obj_id>
				<ref_obj_pid>105702</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Stephen J. Adelson, Jeffrey B. Bentley, In Seok Chung, Larry F. Hodges, and Joseph Winograd. Simultaneous Generation of Stereoscopic Views. Computer Graphics Forum, 10(1):3-10, March 1991.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Sig Badt, Jr. Two Algorithms for Taking Advantage of Temporal Coherence in Ray Tracing. The Visual Computer, 4(3):123-132, September 1988.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617770</ref_obj_id>
				<ref_obj_pid>616024</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[James F. Blinn.Jim Blinn's corner: Hyperbolic Interpolation. IEEE Computer Graphics and Applications, 12(4):89-94, July 1992.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[R. C. Bolles, H. H. Baker, and D. H. Marimont. Epipolar-Plane Image Analysis: An Approach to Determining Structure from Motion. Inter. J. Computer Vision, 1:7-55, 1987.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>93315</ref_obj_id>
				<ref_obj_pid>93267</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[J. Chapman, T. W. Calvert, and J. Dill. Exploiting Temporal Coherence in Ray Tracing. In Proceedings of Graphics Interface '90, pages 196-204, May 1990.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166153</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Shenchang Eric Chen and Lance Williams. View Interpolation for Image Synthesis. In James T. Kajiya, editor, Computer Graphics (SIGGRAPH 93 Proceedings), volume 27, pages 279-288, August 1993.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218391</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Michael Deering. Geometry Compression. In Robert Cook, editor, SIGGRAPH 95 Conference Proceedings, Annual Conference Series, pages 13-20, August 1995.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237200</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Steven J. Gortler, Radek Grzeszczuk, Richard Szeliski, and Michael F. Cohen. The Lumigraph. In Holly Rushmeier, editor, SIGGRAPH 96 Conference Proceedings, Annual Conference Series, pages 43-54, August 1996.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[E. Groeller and W. Purgathofer. Using Temporal and Spatial Coherence for Accelerating the Calculation of Animation Sequences. In Werner Purgathofer, editor, Eurographics '91, pages 103-113. North-Holland, September 1991.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>271309</ref_obj_id>
				<ref_obj_pid>271283</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Michael W. Halle. Autostereoscopic Displays and Computer Graphics. In Computer Graphics, ACM SIGGRAPH. 31(2), pages 58-62.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Michael W. Halle. The Generalized Holographic Stereogram. Master's thesis, Department of Architecture and Planning, Massachusetts Institute of Technology, February 1991.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Michael W. Halle. Holographic Stereograms as Discrete Imaging Systems. In Practical Holography VIII, vol. 2176, pages 73- 84, SPIE, May 1994.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>926243</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Michael W. Halle. Multiple Viewpoint Rendering for Autostereoscopic Displays. Ph.D. thesis, Media Arts and Sciences Section, Massachusetts Institute of Technology, May 1997.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237199</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Marc Levoy and Pat Hanrahan. Light Field Rendering. In Holly Rushmeier, editor, SIGGRAPH 96 Conference Proceedings, Annual Conference Series, pages 31-42, August 1996.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[T. Okoshi. Three-Dimensional Imaging Techniques. Academic Press, New York, 1976.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134071</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Mark Segal, Carl Korobkin, Roll van Widenfelt, Jim Foran, and Paul E. Haeberli. Fast Shadows and Lighting Effects using Texture Mapping. In Edwin E. Catmull, editor, Computer Graphics (SIGGRAPH 92 Proceedings), volume 26, pages 249-252, July 1992.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356626</ref_obj_id>
				<ref_obj_pid>356625</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Ivan E. Sutherland, Robert F. Sproull, and Robert A. Schumacker. A Characterization of Ten Hidden-Surface Algorithms. Computing Surveys, 6(1), March 1974.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[S. Takahashi, T. Honda, M. Yamaguchi, N. Ohyama, and F. Iwata. Generation of Intermediate Parallax-images for Holographic stereograms. In Practical Holography VII." Imaging and Materials, pages 2+, SPIE, 1993.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237274</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Jay Torborg and Jim Kajiya. Talisman: Commodity Real-time 3D Graphics for the PC. In Holly Rushmeier, editor, SIGGRAPH 96 Conference Proceedings, Annual Conference Series, pages 353- 364, August 1996.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>90714</ref_obj_id>
				<ref_obj_pid>90692</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Daniele Tost and Pere Brunet. A Definition of Frame-To- Frame Coherence. In N. Magnenat-Thalmann and D. Thalmann, editors, Computer Animation '90, pages 207-225, April 1990.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801126</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Lance Williams. Pyramidal Parametrics. In Computer Graphics (SIGGRAPH '83 Proceedings), volume 17, pages 1-11,July 1983.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618374</ref_obj_id>
				<ref_obj_pid>616042</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Andrew Woo, Andrew Pearce, and Marc Ouellette. It's Really Not a Rendering Bug, You See .. IEEE Computer Graphics and Applications, 16(5):21-25, September 1996.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>653828</ref_obj_id>
				<ref_obj_pid>645465</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Eric Zeghers, Kadi Bouatouch, Eric Maisel, and Christian Bouville. Faster Image Rendering in Animation Through Motion Compensated Interpolation. In Graphics, Design and Visualization, pages 49+. International Federation for Information Processing Transactions, 1993.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Multiple Viewpoint Rendering Michael Halle Brigham and Women s Hospital Supplemental materials for 
this paper are available in the papers/halle directory. Abstract This paper presents an algorithm for 
rendering a static scene from multiple perspectives. While most current computer graphics algorithms 
render scenes as they appear from a single viewpoint (the location of the camera) multiple viewpoint 
rendering (MVR) renders a scene from a range of spatially-varying viewpoints. By exploiting perspective 
coherence, MVR can produce a set of images orders of magnitude faster than conventional rendering methods. 
Images produced by MVR can be used as input to multiple-perspective displays such as holographic stereograms, 
len­ticular sheet displays, and holographic video. MVR can also be used as a geometry-to-image prefilter 
for image-based rendering algorithms. MVR techniques are adapted from single viewpoint computer graphics 
algorithms and can be accelerated using existing hardware graphics subsystems. This paper describes the 
characteristics of MVR algorithms in general, along with the design, implementation, and applications 
of a particular MVR rendering system. 1 Introduction Many of the important techniques and algorithms 
of computer graphics are specifically focused on accelerating the conversion of geometric primitives 
to images by using coherence of some kind. Published taxonomies of coherence [17] have presented the 
spectrum of possible coherence types, but common practice has put greater emphasis on some areas and 
left others generally untouched. In particular, most computer graphics algorithms heavily emphasize the 
use of image and geometric coherence to accelerate the rendering of a single image. These techniques 
include some of the most important in computer graphics: polygon scan conversion and incremental shading. 
Less common rendering techniques have been used to exploit coherence over multiple views of an object. 
For example, temporal coherence can be used to speed the rendition of the frames of a computer animation. 
Coherence across several images, referred to under the blanket name frame-to-frame coherence, is very 
general and scene dependent because of the sheer variety of changes that an object in a scene can experience 
from one frame to the next. Fully general temporal coherence algorithms must deal with poten­tially complex 
camera motion as well as arbitrary object transfor­ 75 Francis Street, Boston, MA 02115, USA. Email: 
mhalle@bwh.harvard.edu. This work was performed while the author was at the MIT Media Laboratory. mation 
and other changes to the scene. In part because of this gen­erality, the observation made by Sutherland 
et. al. from 1974 is still mostly true today: It is really hard to make much use of object and frame 
coherence, so perhaps it is not surprising that not much [use of it] has been made. Recent developments 
such as the Talisman graphics architecture [19] demonstrate both the promise and the complexities of 
using temporal coherence.  2 Perspective coherence While temporal coherence of time-varying image sequences 
is an important subclass of frame-to-frame coherence, it is not the only subclass. Another coherence 
type, perspective coherence, is the similarity between images of a static scene as viewed from different 
locations. Simple observation demonstrates the prevalence of per­spective coherence in common real world 
scenes: viewing typical objects by alternating between your left and right eyes produces little apparent 
change in appearance. Small shifts of your head side to side or up and down usually yields similarly 
small changes. Because perspective coherence results from the apparent change of a scene s appearance 
due solely to a change in camera perspective, it is much more restricted and less general than temporal 
coherence. Geometric and shading changes to the scene s appearance are usually related to the change 
in the camera position in a simple way. With the appropriate rendering con­structs, perspective coherence 
is easier to find and to exploit than more general frame-to-frame coherence. This paper describes a method 
of rendering whereby perspective coherence can be harnessed to efficiently render sets of perspective 
images. 3 Multiple viewpoint rendering This text refers to rendering methods that generate perspective 
image sets as multiple viewpoint rendering, or MVR, and those algo­rithms that create single images as 
single viewpoint rendering (SVR). MVR algorithms treat the process of rendering a set of perspective 
images as a unit, and use the structured coherence of spatio-per­spective space to accelerate the process 
of image data generation. For instance, using a relatively small number of transformation and shading 
calculations, MVR can interpolate location and appear­ance of an object through an entire range of views. 
 4 Applications Perspective image sets such as those generated by MVR are used less frequently than are 
animations or other temporally varying image sequences. But perspective image sets have their own important 
class of emerging uses in computer graphics. This class of applications approximate optical capture, 
distortion, or display of a field of light emitted by a scene. Two diverse examples of potential applications 
for MVR-generated image sets include synthetic three-dimensional display and image-based ren­dering. 
4.1 Three-dimensional displays Multi-perspective 3D or parallax displays, a classification which include 
lenticular sheet displays, parallax panoramagrams, holo­graphic stereograms, and holographic video displays, 
mimic the appearance of three-dimensional scenes by displaying different perspectives of the scene in 
different directions [10][15]. Most multi-perspective displays are horizontal parallax only: they use 
a range of perspectives that vary only horizontally in order to provide stereopsis to a viewer. Depending 
on the exact technology used in the display, the number of perspectives required as input to the display 
device may range from two to tens of thousands. For three-dimensional images of synthetic scenes, these 
perspectives must be rendered. The high cost of computing this large amount of image information is currently 
a major impediment for the development of three-dimensional displays; MVR can be used to produce 3D images 
of virtual scenes much faster than existing rendering methods. 4.2 Image-based rendering Another use 
for perspective image sets is as input to image­based rendering algorithms. Image-based rendering densely 
samples light traveling through a space as a set of images and transforms this data to produce new images 
seen from viewpoints spatially disparate from any of the originals. Image algorithms such as those developed 
by Gortler et. al. [8] and Levoy and Hanrahan [14] produce a single output image from a perspective image 
set, while similar algorithms developed for optical predistor­tion in synthetic holography derive not 
just one but an entirely new set of images [10]. Either of these types of image-based rendering algorithms 
requires a set of rendered perspectives in order to image a synthetic scene. MVR serves as a prefilter 
for the image-based rendering pipeline, transforming scene geometry into a basis set of the light field 
from which new images are derived. Perspective coherence, in turn, provides the means to efficiently 
compute this perspective image set.  5 Camera geometries The exact relationship between a change in 
camera viewpoint and the resulting change in the appearance of a scene depends on image plane image plane 
object object camera location camera plane full parallax the capture camera geometry. Choice of camera 
geometry depends on how the output data will be used and how well a par­ticular geometry lends itself 
to efficient use of perspective coher­ence. Image-based rendering algorithms have shown that a suffi­ciently 
dense sampling perspectives in a single plane provides enough information to synthesize arbitrary perspectives 
within a volume free of occluders; this property permits the set of perspec­tive images from one camera 
geometry to be converted to that of another, different camera geometry. The ability to convert sets of 
perspective images between different camera geometries allows the choice of a convenient geometry to 
maximize the use of perspec­tive coherence to accelerate rendering. This paper will focus on a planar 
camera geometry specifically designed to simplify interpola­tion between different views.  6 PRS camera 
geometry One of the simplest multi-perspective camera geometries consists of an planar array of cameras 
arranged in a regular grid, with all of the cameras optical axes mutually parallel. The film plane of 
each camera in the grid is sheared in a plane orthogonal to the camera s view vector in order to recenter 
the image of points on an image plane located a constant distance from the camera plane. This camera 
geometry has been used in computer vision and synthetic holography since the late 1970 s; it is identical 
to the one described by Levoy and Hanrahan [14]. We will refer to this geometry as a planar regular shearing 
camera geometry, or PRS camera. The one-dimensional analog of the PRS camera consists of a regularly 
spaced line of cameras; this geometry is called a linear regular shearing geometry, or LRS. The PRS camera 
can be decomposed into a set of simpler LRS cameras arranged in a regular array. Collectively, the PRS 
and LRS geometries are called regular shearing (RS) cameras. PRS and LRS camera geometries are shown 
in Figure 1. all camera up vectors are parallel to camera plane y vector y camera location x all camera 
optical axes are perpendicular to camera plane camera plane horizontal parallax only Figure 1: The left 
picture shows a two-dimensional array of cameras arranged in a PRS geometry, capturing an image of a 
three-dimensional object located at the recentering plane. The PRS camera geometry captures both vertical 
and horizontal parallax (full parallax information) of the object. The middle picture depicts a one-dimensional 
LRS camera geometry, which captures only horizontal parallax. A PRS camera can be created from a set 
of LRS camera positioned in a regular grid. The right picture shows a detail of the individual camera 
orientation for RS camera geometries: the cameras are positioned at regular grid locations, with their 
optical axes (view vectors) all parallel, and their film planes shifted so as to recenter the image plane 
in each view. Figure 2: This spatio-perspective volume of a simple polygonal scene is formed by the 
frames captured by an LRS camera. An epipolar plane image (EPI) is a horizontal slice through this volume. 
In the scene, the cube and star are diffuse surfaces while the cone is shiny with a specular highlight. 
The use of an RS camera introduces several simplifying con­straints to the geometry and the mathematics 
of rendering multiple images. Assuming that a pinhole camera imaging model is used, an unoccluded point 
in the scene will translate in position from one camera image to the next at a velocity constant for 
all views and linearly proportional to the point s distance from the recenter­ing plane and the spacing 
between the cameras in the grid. The relationship between camera position and the point s location in 
the corresponding image is separable. In other words, a point translating horizontally from one camera 
s image to another can only be due to a change in horizontal camera position (and similarly true in any 
other axis of lateral camera displacement). The separable linearity between camera location and image 
position is the key to maximizing perspective coherence.  7 Spatio-perspective image volume Considered 
as a single unit, the set of perspective images from an RS camera form an image volume that spans a region 
of spatio­perspective space. The three-dimensional perspective image volume from an LRS camera is formed 
by stacking the individual camera images on top of each other like playing cards. A PRS camera forms 
an analogous four-dimensional volume. For the purposes of illustration, we will for the moment restrict 
our expla­nation to a LRS camera geometry where the camera is moving strictly horizontally. The original 
perspectives of the RS camera are slices through the perspective image space. The volume can also be 
sliced in other ways. The computer vision community has used a construct known as an epipolar plane image, 
or EPI, to analyze the output of cameras arranged in (or moved through) a set of spatially disparate 
locations [4]. EPIs are slices of spatio-perspective space cut parallel to the direction of camera motion. 
The scanlines that make up EPI n are the nth scanline from each of the original camera views. Figure 
2 shows the frames of a polygonal scene stacked up to form a spatio-perspective image volume. A horizontal 
slice through the volume at the location shown forms the EPI at right. EPIs are useful because they expose 
the perspective coherence of the RS camera geometry. The linear relationship between camera position 
and the location of image detail manifests itself as linear features called tracks in the EPI. A point 
in the scene, for instance, sweeps out a linear point track in the EPI. Tracks are visible in the EPI 
as long as objects are visible and in frame as seen from a particular camera location. If an object is 
occluded by another object in one viewpoint, its track will be correspondingly occluded by the other 
point s track in the EPI. Since points in a scene tend to remain visible over a range of viewing locations, 
tracks tend to be fairly long in EPI space. Surfaces in the spatio-perspective volume swept out by lines 
in the scene are called line tracks. Line tracks are twisted quadrilater­als that interpolate between 
the point tracks of the line segment s two endpoints. A line in the scene that lies in an epipolar plane 
(a plane that includes the line of the camera track and a horizontal scanline) has a line track restricted 
to a single EPI. The EPI of the line track is formed by projecting its twisted 3D shape into 2D. A line 
track can occlude or intersect the tracks of other objects in the scene, or even twist itself into a 
bowtie shape as seen in 2D projec­tion. The occlusion relationship between two different line tracks 
can be determined by interpolating the depth coordinate of the two endpoint tracks for each line track 
and occluding the more distant of the two line tracks at every point. These occlusion calcu­lations are 
very similar to those performed in conventional single viewpoint rendering. 8 Properties of EPIs The 
simple EPI shown in Figure 2 demonstrates some of the reasons why EPIs are useful for rendering. Linear 
track features are compatible with interpolation algorithms implemented in conven­tional rendering software 
and hardware. The shape of line tracks are similar to, but even more regular than, the shape of the polygons 
that are frequently used to describe the geometry of scenes. Tracks are usually very long, spanning many 
pixels, so that the ratio of pixels spanned by a track versus the vertices needed to describe it is high. 
Occlusion relationships are essentially the same as in ordinary scenes. The color of objects tends to 
change slowly with varying viewpoint. Figure 3 shows examples of these and other graphical properties 
of objects in the EPI from Figure 2.  9 MVR rendering algorithm The length of the tracks of geometric 
primitives in spatio-per­spective space hints that EPIs of a scene contain more coherence than conventional 
perspective views of the same scene. This obser­vation, combined with the other graphical properties 
of LRS EPIs, leads to the basic idea of the MVR algorithm: decompose a geometric scene into primitives 
that are rendered efficiently into EPIs, then render the spatio-perspective volume, EPI by EPI, until 
the entire volume is computed. To render a four-dimensional PRS spatio-perspective volume, decompose 
it into simpler three-dimen­sional LRS subvolumes that can be rendered individually. (A more sophisticated 
algorithm could render the 4D spatio-perspective volume as a single unit.) In many ways, MVR can be thought 
of as a higher-dimensional version of established scan conversion algo­rithms. The basic steps of the 
MVR pipeline are as follows: Preprocessing and transformation: Perform initial scene transformation 
and view independent lighting calculations for each vertex in the scene,  Decompose the two-dimensional 
PRS camera geometry into a set of simpler horizontal LRS cameras,  For each LRS camera, transform the 
vertices of the original scene geometry to find its position as seen from the two most extreme camera 
viewpoints,  Geometric slicing: Decompose the scene polygons into horizontal slices that lie along 
the scanlines of the final image,  Sort these polygon slices by scanline into a scanline slice table, 
 Rasterization and hidden surface removal: For each scanline entry in the slice table, scan convert 
the slices for that scanline into tracks in EPI space, performing view­dependent shading calculations 
and hidden surface removal in the process,  Combine all EPIs from all LRS cameras into a complete spatio­perspective 
volume.  The rest of this section describes more specific details of the different stages of the MVR 
rendering pipeline. 9.1 Preprocessing and transformation Several of the computational steps used to 
calculate the appear­ance of a single image in conventional rendering can be performed once for the entire 
set of perspective images in multiple viewpoint rendering. For a Gouraud-shaded object, for instance, 
view independent lighting calculations such as Lambertian reflec­tion can be performed once at each vertex 
of the scene. The cost of these lighting calculations is independent of the number of views to be rendered; 
their computational expense is amortized over the entire set of images. Per-sequence MVR calculations 
proceed as follows. The geometry of the scene is either read from disk or accessed from memory. For each 
LRS image set to be rendered, the model is transformed into homogeneous screen space as seen from the 
two extreme camera viewpoints. (This paper uses a right-handed coor­dinate system where the x coordinate 
increases to the right of the screen, y increases up, and z increases out of the screen.) These two camera 
views differ only in the horizontal direction; because of the RS camera geometry, a vertex seen from 
these two view­points differs strictly in its x coordinate. The redundancy of the cal­culation means 
that the cost of performing both endpoint camera transformations is only 1.25 times the cost of performing 
a single transformation. Following this step, each vertex will have screen space coordinates (xL, xR, 
y, z, w), where xL and xR are the x coor­dinates of the vertex as seen from the extreme left and right 
camera views. Next, clipping is performed on the transformed vertex coordi­nates. Polygons lying completely 
above or below the view window can be culled, as can those outside the near and far clipping planes. 
Clipping polygons that partially fall within the view window of at least one view is straightforward 
but somewhat more complicated than for SVR. In our prototype implementation, we chose to perform no polygon 
clipping at this stage for simplicity, while risking some performance penalty. As another result of the 
RS camera geometry, both the z and w screen-space coordinates of each vertex remain constant over the 
entire range of viewpoints. Because w is fixed, the homogeneous divide required for perspective transformation 
are performed as a preprocessing step, reducing the computational cost of transforma­tion. The w coordinate 
should be maintained, however, to permit perspective-correct shading and texture interpolation during 
scan conversion [3]. 9.2 Geometric slicing Polygon tracks, or PTs, are three-dimensional volume primitives 
in the spatio-perspective space of an LRS camera geometry. Two­dimensional scan conversion of a PT requires 
the original polygon be decomposed, or sliced, into a set of line segments that lie along the scanlines 
of the image set s final perspective images. These line segments are called polygon slices. Slicing a 
polygon along scanlines is very similar to conventional scan conversion, except that the slices retain 
their continuous three-dimensionality. As each horizontal slice is generated, a data structure describing 
it is added to a scanline slice table. This table has one entry per scanline: each entry is a list of 
polygon slices that the scanline intersects. Thus, each scanline table entry contains all the information 
needed to render the scanline s EPI, which is in turn the scanline s appearance from all viewpoints. 
 9.3 Rasterization When it is rendered, each polygon slice is converted into a special kind of line track 
called a polygon slice track (PST) and rendered into the appropriate EPI. The PST is the key rendering 
primitive of MVR. Figure 4 shows some of the PST shapes from which different slice orientations can result. 
PSTs have two types of edges. P-edges (projection edges) are the projections of the polygon slice as 
seen from two most extreme camera views. I-edges (interpo­lating edges) interpolate the position of a 
slice endpoint through the range of views. The geometry of PSTs is very regular; without clipping or 
culling and disregarding degenerate cases, p-edges are always horizontal and lie at the top and bottom 
scanline of the EPI, while i-edges cross all EPI scanlines.  Figure 5: The picture on the left shows 
a slice extracted from a triangle from the original scene geometry. The coordinates E0 and E1 have been 
interpolated from the triangle s vertices and represent the homogeneous location of the vertex (xL, xR, 
y, z, w) and the view independent per-vertex color (c) calculated there. From these endpoint coordinates, 
the vertices of the PST V0L, V0R, V1L, and V1R are found. PST rasterization requires linear interpolation 
of geometric and Gouraud shading parameters in the horizontal direction. Figure 5 shows how the coordinates 
of the PST vertices in spatio-perspective space are derived from the endpoints of the polygon slice in 
screen space. Rasterization of a PST of a diffuse Gouraud-shaded polygon proceeds by interpolating the 
perspec­tive coordinate in the vertical (p) direction, and all other parame­ters in the horizontal (x) 
direction. Every horizontal scanline of the PST crosses through the same range of values for each parameter, 
but at a different rate of sampling depending on the width of the PST at that scanline. If the i-edges 
of a PST cross at a point, the direction of interpolation will be opposite from one side of the crossing 
point to the other, as the figure demonstrates. Geometric slicing and PST rasterization most distinguish 
the MVR rendering process from that of a more conventional renderer. By way of example, Figure 6 shows 
how a single triangle is rendered using MVR and an LRS camera geometry. 9.4 Hidden surface removal Hidden 
surface removal (HSR) is performed in spatio-perspec­tive space in the same way it would be performed 
in image space, and many of the algorithms for HSR can be adapted to work on PSTs instead of polygons. 
The widely-used Z-buffer algorithm for HSR can be easily implemented by storing a depth value for each 
pixel in an EPI, and comparing the interpolated depth value for each pixel of the PST being rasterized 
to see if it is in front of all previous surfaces. Some aspects of HSR can be simplified in MVR by using 
the inherent perspective coherence of the scene. Backface culling, for instance, can be performed once 
per PST instead of once per polygon per viewpoint by observing that the orientation of a polygon slice 
with respect to the camera changes slowly and pre­dictably. If the i-edges of a PST do not intersect, 
a backfacing test is required only once for the entire PST, accepting or rejecting it as a whole. If 
a PST does cross itself, one of the triangles of the PST is back facing and the other is front facing. 
The back facing piece of the PST need not be rendered.  9.5 Texture mapping Texture mapping is a type 
of shading that applies image detail to the surfaces of geometric objects. The appearance of texture 
maps is view independent: while the geometry onto which the texture is mapped may change depending on 
the location of the viewer, the appearance of the texture itself does not. Other types of image-level 
mapping algorithms such as reflection or environ­ment mapping are not view independent; a reflection 
on a surface can change in appearance as the viewer moves around it. Figure 7 shows a simple polygonal 
scene with both texture and reflection maps applied to different objects. The corresponding EPI shows 
how the texture mapped onto the cube and star changes gradually over the entire range of views, in contrast 
to the less predictable reflection mapped cone. The view independence of texture mapping lends itself 
to an efficient MVR implementation. In its simplest form, MVR texture mapping is similar to the analogous 
algorithms in SVR. Texture coordinates are assigned to each vertex of the original scene geometry, hyperbolic 
texture coordinates[3] are interpolated to find the texture of each slice endpoint, and textures are 
further interpolated across the surface of the PST. This simple texturing technique can be used when 
adapting existing rendering algorithms to render PSTs. When rendering a large number of views, a more 
efficient MVR texture map algorithm can be implemented using the fact that each horizontal line of a 
PST is a resampling of the same texture at a different rate. Figure 8 outlines the steps of this algo­rithm. 
The texture for each polygon slice is extracted from two­dimensional texture memory using MIP [21] or 
area sampling tech­ niques and stored in a one-dimensional texture map at a sampling rate appropriate 
for the slice s greatest width under transformation. Non-linear sampling of the two-dimensional texture 
takes care of distortions due to perspective: further resampling of the one­dimensional texture can be 
performed linearly without need for complex and expensive hyperbolic interpolation when rendering every 
pixel. This 1D map is the basis for a new MIP map that is applied to different regions of the PST. Using 
the 1D map has several advan­tages over existing texturing algorithms. The width of a PST changes slowly 
and regularly, so the appropriate level of the MIP map needed to avoid sampling artifacts can readily 
be chosen. 2D texture memory is probed in a more predictable way, improving  Figure 6: The basic MVR 
rendering pipeline for the LRS camera geometry, applied to rendering a single triangle. texture prefetching 
strategies. The size of the 1D map is small enough to allow efficient caching in fast memory, where the 
texture of a PST pixel can be computed using linear indexing and simple interpolation.These simple operations 
suggest the possible use of specialized hardware including image warping subsystems to perform MVR texturing. 
 9.6 View-dependent shading View dependent shading of surfaces in MVR can, like texture mapping, be 
implemented using modified SVR algorithms. Reflec­tion and environment mapping are the most common view 
dependent shading algorithms in current use. Reflection algo­rithms calculate reflection vectors at each 
polygon vertex based on eye, light, and surface orientation vectors, interpolating the reflec­tion vectors 
across the polygon, and perform a lookup into a reflection map using the interpolated vector. MVR reflection 
mapping works the same way, except that the eye vector can potentially vary over a large angle through 
the range of camera positions. Figure 9 describes the relationship between camera geometry and the assignment 
of reflection vectors to a PST. When implementing re.ection mapping in MVR, care must be taken to assure 
that reflection calculations are accurate over a large angular change in eye vectors. For example, spherical 
reflection mapping [16] substitutes complex spherical interpolation with simpler linear interpolation 
across the extent of polygons. Over large angles, this approximation becomes invalid and results in noticeably 
incorrect shading of PSTs. One recourse to solve this problem with spherical maps is to uniformly subdivide 
PSTs in the perspective dimension at the cost of performance. Cubic reflection maps, on the other hand, 
correctly interpolate reflection vectors and can be used without subdivision. Although view dependent 
shading does not attain the level of efficiency in MVR as does view independent shading, it can still 
be more efficient than a comparable SVR algorithm. Computa­tional savings result from the precalculation 
and incremental inter­polation of reflection vectors over PSTs and the regularity of the reflection vector 
s mapping into the reflection map memory. 10 Implementation The MVR algorithm described in this paper 
generates a set of perspective images from cameras arranged in a PRS camera geometry, using computer 
graphics hardware to accelerate the rendering process. The prototype implementation described here is 
designed to fairly compare the relative efficiency of MVR and conventional SVR algorithms. The implementation 
consists of shared modules for file input and output, scene transformation, and texturing and reflection 
mapping, as well as MVR or SVR­specific modules for primitive generation, rendering and image assembly. 
The code for the implementation is written in ANSI C and uses the OpenGLTM graphics library to provide 
device-inde­pendent graphics acceleration. The algorithm has been tested on a range of workstations from 
Silicon Graphics Inc., including an Indigo2 workstation with Maximum Impact graphics and a 150 MHz R4400 
CPU, and an Onyx with RealityEngine2 graphics and two 150 MHz R4400 CPUs. Further tests were done using 
a Sun Microsystems Ultra 1 workstation with Creator3D graphics. Input data for the tests consisted of 
two polygonal models: one of a teacup, the other of a Ferio automobile body shell provided by the Honda 
R&#38;D Company (Figure 10). These scenes were created using Alias/Wavefront Corporation s Alias Studio 
modeling program. From original surface models, Alias Studio cal­culates per-vertex view independent 
lighting and texture coordi­nate values, and outputs a collection of independent triangles to a file. 
This triangle data eliminates the need for view independent lighting to be implemented in the rendering 
testbed itself. In a polygon slice from the middle of a foreshortened, texture-mapped polygon  "footprint" 
of polygon in texture space "footprints" of texture samples from polygon slice extracted 1D texture map 
1D MIP texture map Figure 8: An MVR-specific algorithm for texture mapping extracts the texture for 
a polygon slice from two-dimensional memory, builds a 1D MIP map, and repeatedly resamples it to apply 
the texture to scanlines of the PST. This process eliminates the need for a per-pixel homogeneous divide. 
addition the teacup was tessellated to produce 4K, 16K, 63K, 99K, and 143K triangle count models in order 
to compare the efficiency of MVR when rendering polygons of different average sizes. 10.1 MVR renderer 
The MVR module consists of a polygon slicer, a scanline slice table, a slice-to-PST converter, and an 
EPI rasterizer. PSTs are rendered in hardware by approximating them as polygons. Although PST shading 
and interpolation is actually easier than shading triangles because of their regular geometry, current 
rendering hardware designed for optimized triangle rendering produces shading artifacts across the PST 
(and many other quadrilaterals [22]). Such errors can be reduced by minimizing the size of polygons, 
reducing the range of viewpoints, or decompos­ing the PST into smaller primitives. During testing, a 
minimal decomposition of PSTs into no more than four triangles was used for both timing and rendering 
accuracy tests. A hardware rendering system specifically designed for MVR could provide higher quality 
rendering at rates faster than the fastest rates described here. Texturing and reflection mapping were 
implemented in the MVR module without using MVR-specific algorithms in order to use existing graphics 
hardware to accelerate these operations. Reflection mapping uses spherical environment maps to simulate 
surrounding objects and Phong-like specular highlights. Figure 11 shows a two-dimensional array of images 
rendered using MVR with both texture and reflection mapping. Figure 12 is an EPI from the teacup, from 
a scanline near the cup s lip. 10.2 SVR renderer The SVR module was designed to minimize redundant opera­tions 
consistent with rendering a set of images. For instance, the initial transformation of the scene triangles, 
performed on the CPU and not in the graphics engine, is done only once for all views. The graphics hardware 
s transformation matrices were not updated from view to view for the SVR speed tests: only the image 
of the central view was used as an approximation of the per-frame rendering speed. If anything, this 
approximation should underesti­mate the rendering time for the SVR algorithm. For both the SVR and MVR 
modules, speed tests do not include the time required to read back data from framebuffer memory. Applications 
that use a set of perspective images are almost certain to need the rendered images as data in main memory, 
not just on the screen, but reading framebuffer memory requires approximately the same time in either 
SVR or MVR. The next section presents the results of speed and rendering accuracy tests performed using 
the prototype rendering implemen­tation.   11 Performance 11.1 Timing tests The graph in Figure 13 
shows the performance of the different stages of the MVR pipeline when rendering the Ferio database at 
a resolution of 640 by 480 pixels per view over a varying number of views of an LRS camera. Only view 
independent shading was performed for this test. Timings were performed on the SGI Indigo2. The cost 
of reading triangles, transforming them (includ­  Figure 9: The top figure shows an LRS camera viewing 
a polygon slice with a surface normal N across its surface. The slice endpoints each have two reflection 
vectors R that result from view vectors from the two camera track endpoints. These four reflection vectors 
are assigned to the PST shown in the EPI in the bottom figure. ing conventional transformation and the 
additional cost of calculat­ing horizontally and vertically varying parallax information), and slicing 
the polygons is constant. The cost of rendering less than about 400 primitives is also constant: the 
hardware setup time for the PSTs is greater than the cost of concurrently painting the pixels of the 
PSTs onto the screen. The line on the far left side of the graph shows the relative cost of rendering 
using the SVR module. In this test, MVR is more efficient than SVR for images sets larger than about 
10 views. Figure 14 directly compares the time required to render scenes of different tessellation using 
both MVR and SVR. The teacup models of different tessellation densities were rendered using both algorithms 
using the SGI Onyx. The graph shows that the smaller the polygons, the better MVR performed relative 
to SVR. This behavior is due to the increased spatial coherence in the smaller polygon scenes: SVR better 
amortizes per-polygon setup costs over a larger number of pixels drawn to the screen. At best case for 
this resolution, MVR is about 26 times faster than SVR. Figure 15 shows a comparison between SVR and 
MVR using a polygon database of fixed size, but with a varying pixel resolution of the output images. 
The Indigo2 system was used to render the Ferio database for this test. In the SVR timing results, the 
hardware is not pixel fill rate limited at any of the resolutions. Thus, rendering times are independent 
of the pixel size of the image. MVR performance is, on the other hand, dependent on image resolution. 
Smaller images result in fewer PSTs to render. At the lowest image resolution, for example, MVR is more 
than 200 times faster than SVR. These low-resolution images have applica­tion in three-dimensional display 
devices, where light modulators may have a low pixel count. The shape of the MVR timing curves reflects 
the different types of cost savings when rendering different numbers of views. For a small view count, 
geometry costs dominate pixel fill and adding more views are essentially free. The knee of the curve 
rep­resents the point where the cost of the concurrently-performed geometry and fill operations are equal. 
The slope to the right of the knee of the curve levels off as the costs of sequential prepro­cessing 
operations are amortized over increasingly many views.  11.2 Rendering accuracy Ideally in the most 
common case, there should be no differ­ence between the image sets rendered using MVR and those rendered 
conventionally with MVR. However, some errors in shading may occur because of algorithm-dependent differences 
in rasterization or shading. To test the accuracy of MVR, the 143K teacup was used as a model for both 
MVR and SVR to render sixteen views at 640 by 480 pixels. From these two collections of  stages of MVR 
ferio (136858 triangles, 640x480 pixels) 20.0 cumulative time (seconds) 10.0 render primitives polygon 
slicing MVR transform SVR transform read triangles 0.0 views, four pairs were extracted, and the absolute 
values of their pixel-by-pixel differences computed. Rendering was done using the Sun Ultra 1. No PST 
subdivision was used when rendering using MVR. The result of this difference is shown in Figure 16. An 
enlarged piece of the error image is shown in Figure 17. The largest errors located along the edge of 
the cup are most likely due to small dif­ferences in transformation between the two rendering modules; 
these differences are never more than one pixel wide in any view. Errors on the interior of the teacup 
result from differences in Figure 13: Relative costs of the different stages of MVR, using an LRS camera, 
when rendering different numbers of views. Total rendering time of SVR is included for reference. shading 
between the two algorithms. Neither error is generally noticeable in practice. When rendering small numbers 
of widely disparate views, however, shading differences between MVR and SVR can be sig­nificant. The 
reason for this difference is SVR makes no guaran­tees that the track of a object in spatio-perspective 
space is continu­uous, while MVR does; the track of any MVR-rendered feature is bandlimited so that the 
images of the feature abut from view to view. The MVR behavior, while different than that of SVR, provides 
sufficient sampling to avoid aliasing artifacts in image­based rendering and synthetic holographic displays 
[12]. MVR vs. SVR (scene complexity) teacup (640x480 pixels)  11.3 Interpreting the results These results 
for scenes with view independent shading demon­strate that MVR is capable of exceeding the performance 
of SVR algorithms by one to two orders of magnitude. Further testing confirms that these savings are 
also true for texture- and reflection­mapped scenes, and for PRS cameras constructed from multiple LRS 
cameras. MVR is faster than SVR for rendering large sets of perspective images for several reasons. First, 
a significant number of transformation and shading operations are performed as prepro­cessing steps, 
incurring a constant cost that is amortized over the entire set of images. In the RS camera geometry, 
this preprocess­ing can include the otherwise-costly homogeneous divide required during perspective transformations. 
Second, the ratio of the pixel size of rendered primitives to the number of vertices that describe those 
primitives geometry is much higher for PSTs in MVR than for ordinary polygons when rendering many viewpoints. 
Since rendering hardware often uses more expensive floating point representations to describe and transform 
vertices, and fixed-point or integer calculations to deal with pixels, improving the pixel-to-vertex 
ratio of geometric primi­tives can often lead to dramatic improvements in performance. Many other techniques 
exist for changing the pixel-to-vertex ratio, including building geometry strips and compressing the 
scene s geometric description [7]. The use of these techniques and the tuning of software and hardware 
that makes up a specific rendering pipeline can control whether rendering of a given scene is geometry 
or pixel fill limited. MVR is an additional technique that shifts the balance towards high pixel-to-vertex 
ratios; it can also be combined with other techniques such as geometry strips or compression to achieve 
still more pixels per vertex. Third, shading and texturing PSTs is less complex than the equivalent operations 
on polygons. PSTs have a more regular shape and size than do polygons from the same scene. PSTs can be 
shaded and textured using only horizontal interpolation between i-edges for each scanline of a PST. Perspective-correct 
texture mapping can be performed using only linear resampling of a perspective-predistorted subtexture, 
eliminating a per-pixel divide, improving memory access and cache performance, and simplifying possible 
hardware implementation. Backface culling and hidden surface removal can both be implemented to take 
advantage of perspective coherence. The exact impact of these cost-saving properties on the time required 
to render a perspective image set depends on the archi­tecture of the computer rendering subsystem (including 
the relative costs of vertex operations, pixel fills, memory access and communication), the resolution 
and count of the output images, and the properties of the particular scene being rendered. A graphics 
system with a very limited pixel fill rate, for example, may experience little or no savings from MVR. 
The following rule can be used to determine the general applicability of MVR to a particular application: 
if the height in pixels of an average polygon in a scene is smaller than the number of viewpoints to 
be rendered, MVR will likely be as fast or faster than an SVR algo­rithm. For this number of views, the 
pixel-to-vertex ratio for SVR and MVR is approximately equal.  12 Comparison to other work Several other 
researchers have developed computer graphics algorithms that use some form of frame-to-frame coherence. 
Badt [2], Chapman et. al. [5], and Groeller and Purgathofer [9] have produced ray-tracing algorithms 
that use temporal coherence to improve multi-frame rendering performance. Each of these algo­rithms produce 
modest computational savings over conventional ray-tracing techniques. Tost and Brunet characterized 
a variety of frame coherent algorithms in a 1990 taxonomy[20]. Adelson et. al. [1] used perspective coherence 
to compute pairs of images for stereoscopic displays. Because their algorithm only computes pairs of 
images, only limited acceleration due to perspective coherence is possible. The computer vision and image 
processing fields have used epipolar plane image analysis as a way to interpolate intermediate viewpoints 
from a set of photographically acquired images. Takahashi et. al. [18] have used these methods to generate 
images for holographic stereograms. Image interpolation of this kind requires finding corresponding points 
in different images (the underconstrained correspondence problem of computer vision).  Hybrid computer 
graphics and image processing algorithms reduce the need to solve the correspondence problem by aug­ 
menting image information with more data from the original scene. Zeghers et. al. [23] use motion-compensated 
interpolation to produce a series of intermediate frames in an image sequence of fully computed frames. 
The disadvantages of this algorithm are the need to compute a motion field, and the loss of fine detail 
in the scene because of image space interpolation. Chen and Williams [6] also use geometry information 
to guide viewpoint interpolation. Using known camera geometries, their algorithm builds an image space 
morph map from two images and depth buffers. This technique can be used to reduce the cost of shadow 
and motion blur generation, since intermediate images can be computed in time independent of the geometric 
complexity of the scene. They propose methods for reducing the overlaps and holes in the data between 
two images. These problems can only be minimized, not eliminated, however. The image space interpolation 
cannot correctly deal with anti-aliased input images, specular highlights, and other view dependent scene 
changes. Instead of using a limited amount of scene information from an image space buffer, MVR interpolates 
intermediate views using the object precision of the original scene geometry. It incorporates image interpolation 
as part of the rendering process without any need to deal with the difficult computer vision problems 
of corre­spondence. The cost of rendering intermediate views is dependent on the complexity of the scene 
geometry. However, MVR is more compatible with the interpolation hardware found in hardware graphics 
systems.  13 Conclusions and future work New applications in computer graphics such as three-dimen­sional 
display and image-based rendering need large sets of per­spective images as input. MVR extends conventional 
scanline rendering algorithms to provide these sets of images at rates one to two orders of magnitude 
faster than existing methods. MVR can generate high quality images using texture and reflection maps, 
and can be accelerated using both existing and future graphics hardware. Many rendering techniques not 
described here can be adapted for use with MVR. MVR techniques can also be extended to alternate camera 
geometries, geometry compression, and trans­mission of three-dimensional geometry information. Acknowledgments 
The work described in this paper was done as part of my doctoral dissertation at the Spatial Imaging 
Group of the MIT Media Laboratory. Thanks to everyone in the group who helped me think through these 
ideas and reviewed drafts of this paper, including Wendy Plesniak, Ravikanth Pappu, and John Underkof­fler. 
Wendy Plesniak also modeled the teacup used in the figures. My advisor Stephen Benton and my thesis committee 
members V. Michael Bove and Seth Teller provided great comments and support. Ron Kikinis and Ferenc Jolesz 
at BWH supported me during the last year of this work, in part through funding from Robert Sproull at 
Sun Microsystems. Both Silicon Graphics and Sun Microsystems provided equipment used in this research. 
Also, thanks to the SIGGRAPH reviewers for their thoughtful comments and suggestions. This work was funded 
in part by the Design Staff of the General Motors Corporation, the Honda R&#38;D Company, IBM, NEC, and 
the Office of Naval Research (Grant N0014-96-11200). References [1] Stephen J. Adelson, Jeffrey B. Bentley, 
In Seok Chung, Larry F. Hodges, and Joseph Winograd. Simultaneous Generation of Ste­reoscopic Views. 
Computer Graphics Forum, 10(1):3-10, March 1991. [2] Sig Badt, Jr. Two Algorithms for Taking Advantage 
of Temporal Coherence in Ray Tracing. The Visual Computer, 4(3):123-132, September 1988. [3] James F. 
Blinn. Jim Blinn's corner: Hyperbolic Interpolation. IEEE Computer Graphics and Applications, 12(4):89-94, 
July 1992. [4] R. C. Bolles, H. H. Baker, and D. H. Marimont. Epipolar-Plane Image Analysis: An Approach 
to Determining Structure from Motion. Inter. J. Computer Vision, 1:7-55, 1987. [5] J. Chapman, T. W. 
Calvert, and J. Dill. Exploiting Temporal Coherence in Ray Tracing. In Proceedings of Graphics Interface 
'90, pages 196-204, May 1990. [6] Shenchang Eric Chen and Lance Williams. View Interpolation for Image 
Synthesis. In James T. Kajiya, editor, Computer Graphics (SIGGRAPH 93 Proceedings), volume 27, pages 
279-288, August 1993. [7] Michael Deering. Geometry Compression. In Robert Cook, editor, SIGGRAPH 95 
Conference Proceedings, Annual Conference Series, pages 13 20, August 1995. [8] Steven J. Gortler, Radek 
Grzeszczuk, Richard Szeliski, and Michael F. Cohen. The Lumigraph. In Holly Rushmeier, editor, SIGGRAPH 
96 Conference Proceedings, Annual Conference Series, pages 43-54, August 1996. [9] E. Groeller and W. 
Purgathofer. Using Temporal and Spatial Coherence for Accelerating the Calculation of Animation Sequences. 
In Werner Purgathofer, editor, Eurographics '91, pages 103-113. North-Holland, September 1991. [10] Michael 
W. Halle. Autostereoscopic Displays and Computer Graphics. In Computer Graphics, ACM SIGGRAPH. 31(2), 
pages 58-62. [11] Michael W. Halle. The Generalized Holographic Stereogram. Master s thesis, Department 
of Architecture and Planning, Massa­chusetts Institute of Technology, February 1991. [12] Michael W. 
Halle. Holographic Stereograms as Discrete Imaging Systems. In Practical Holography VIII, vol. 2176, 
pages 73 84, SPIE, May 1994. [13] Michael W. Halle. Multiple Viewpoint Rendering for Autoste­reoscopic 
Displays. Ph.D. thesis, Media Arts and Sciences Section, Massachusetts Institute of Technology, May 1997. 
[14] Marc Levoy and Pat Hanrahan. Light Field Rendering. In Holly Rushmeier, editor, SIGGRAPH 96 Conference 
Proceedings, Annual Conference Series, pages 31-42, August 1996. [15] T. Okoshi. Three-Dimensional Imaging 
Techniques. Academic Press, New York, 1976. [16] Mark Segal, Carl Korobkin, Rolf van Widenfelt, Jim Foran, 
and Paul E. Haeberli. Fast Shadows and Lighting Effects using Texture Mapping. In Edwin E. Catmull, editor, 
Computer Graphics (SIGGRAPH 92 Proceedings), volume 26, pages 249-252, July 1992. [17] Ivan E. Sutherland, 
Robert F. Sproull, and Robert A. Schu­macker. A Characterization of Ten Hidden-Surface Algorithms. Computing 
Surveys, 6(1), March 1974. [18] S. Takahashi, T. Honda, M. Yamaguchi, N. Ohyama, and F. Iwata. Generation 
of Intermediate Parallax-images for Holo­graphic stereograms. In Practical Holography VII: Imaging and 
Materials, pages 2+, SPIE, 1993. [19] Jay Torborg and Jim Kajiya. Talisman: Commodity Real-time 3D Graphics 
for the PC. In Holly Rushmeier, editor, SIGGRAPH 96 Conference Proceedings, Annual Conference Series, 
pages 353­364, August 1996. [20] Daniele Tost and Pere Brunet. A Definition of Frame-To-Frame Coherence. 
In N. Magnenat-Thalmann and D. Thalmann, editors, Computer Animation '90, pages 207-225, April 1990. 
[21] Lance Williams. Pyramidal Parametrics. In Computer Graphics (SIGGRAPH '83 Proceedings), volume 17, 
pages 1-11, July 1983. [22] Andrew Woo, Andrew Pearce, and Marc Ouellette. It's Really Not a Rendering 
Bug, You See.... IEEE Computer Graphics and Applications, 16(5):21-25, September 1996. [23] Eric Zeghers, 
Kadi Bouatouch, Eric Maisel, and Christian Bouville. Faster Image Rendering in Animation Through Motion 
Compensated Interpolation. In Graphics, Design and Visualization, pages 49+. International Federation 
for Information Processing Transactions, 1993.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280888</article_id>
		<sort_key>255</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>25</seq_no>
		<title><![CDATA[Progressive radiance evaluation using directional coherence maps]]></title>
		<page_from>255</page_from>
		<page_to>266</page_to>
		<doi_number>10.1145/280814.280888</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280888</url>
		<keywords>
			<kw><![CDATA[directional coherence]]></kw>
			<kw><![CDATA[image-space discontinuities]]></kw>
			<kw><![CDATA[progressive refinement]]></kw>
			<kw><![CDATA[radiance evaluation]]></kw>
			<kw><![CDATA[rendering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP311541000</person_id>
				<author_profile_id><![CDATA[81537893756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Baining]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel Corp.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>192250</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[J. Arvo. The Irradiance Jacobian/or Partially Occluded Polyhedral Sources. In A. Glassner, editor, Computer Graphics Proceedings, Annual Conference Series, pages 75-84, July 1994.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15889</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[L.D. Bergman, H. Fuchs, E. Grant, and S. Spach. Image Rendering by Adaptive Refinement. In D. C. Evans and R. J. Athay, editors, Computer Graphics (SIGGRAPH '86 Proceedings), volume 20, pages 29-37, August 1986.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801145</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[J. Bloomenthal. Edge Inference with Applications to Antialiasing. In Computer Graphics (SIGGRAPH '83 Proceedings), volume 17, pages 157-162, July 1983.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218497</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[M. R. Bolin and G. W. Meyer. A Frequency Based Ray Tracer. In R. Cook, editor, Computer Graphics Proceedings, Annual Conference Series, pages 409- 418, August 1995.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97896</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[A.T. Campbell III and D. S. Fussell. Adaptive Mesh Generation for Global Diffuse Illumination. In Computer Graphics (SIGGRAPH '90 P1vceedings), volume 24, pages 155-164, August 1990.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122737</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[S.E. Chen, H. E. Rushmeier, G. Miller, and D. Turner. A Progressive Multi-Pass Method for Global Illumination. In T. W. Sederberg, editor, Computer Graphics (SIGGRAPH '91 P1vceedings), volume 25, pages 165-174, July 1991.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74343</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[N. Chin and S. Feiner. Near Real-Time Shadow Generation Using BSP Trees. In J. Lane, editor, Computer Graphics (SIGGRAPH '89 P~vceedings), volume 23, pages 99-106, July 1989.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378487</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[M. F. Cohen, S. E. Chen, J. R. Wallace, and D. P. Greenberg. A Progressive Refinement Approach to Fast Radiosity Image Generation. In J. Dill, editor, Computer Graphics (SIGGRAPH '88 P~vceedings), volume 22, pages 75-84, August 1988.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808590</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[R.L. Cook, T. Porter, and L. Carpenter. Distributed Ray Tracing. In Computer Graphics (SIGGRAPH '84 P~vceedings), volume 18, pages 137-145, July 1984.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563901</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[F. Crow. Shadow Algorithms for Computer Graphics. In Computer Graphics (SIGGRAPH'77 P~vceedings), volume 11, pages 242-248, July 1977.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192207</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[G. Dretakkis and E. Fiume. A Fast Shadow Algorithm for Area Light Sources Using Backprojection. In A. Glassner, editor, Computer Graphics P~vceedings, Annual Conference Series, pages 223-230, July 1994.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>13044</ref_obj_id>
				<ref_obj_pid>13043</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[A. Fujimoto, T. Tanaka, and K. Iwata. ARTS: Accelerated Ray Tracing System. IEEE Computer Graphics and Applications, 6(4): 16-26, July 1986.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>527570</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[A. Glassner. Principles of Digital Image Synthesis, volume 1. Morgan Kaufmann, 1995.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[R Heckbert. Discontinuity Meshing for Radiosity. Third Ewvgraphics Workshop on Rendering, pages 203-226, May 1992.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>59921</ref_obj_id>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[A. Jain. Fundamentals of Digital Image P~vcessing. Prentice Hall, 1989.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15902</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[J. Kajiya. The Rendering Equation. In Computer Graphics (SIGGRAPH '86 P~vceedings), volume 20, pages 143-150, August 1986.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[C. Kolb. Rayshade User's Guide and Reference Manual. Rayshade home page at graphics, stanford, edu, January 1992.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[M. Kunt, A. Ikonomopoulos, and M. Kocher. Second-Generation Image Coding Techniques. P1vc. oflEEE, 73(4):549-574, 1985.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325179</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[M.E. Lee, R. A. Redner, and S. P. Uselton. Statistically Optimized Sampling for Distributed Ray Tracing. In B. A. Barsky, editor, Computer Graphics (SIG- GRAPH '85 Proceedings), volume 19, pages 61-67, July 1985.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166143</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[D. Lischinski, F. Tampieri, and D. R Greenberg. Combining Hierarchical Radiosity and Discontinuity Meshing. In Computer Graphics Proceedings, Annual Conference Series, pages 199-208, 1993.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>132007</ref_obj_id>
				<ref_obj_pid>132005</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[S. Mallat and S. Zhong. Characterization of Signals from Multiscale Edges. IEEE Trans. on Pattern Analysis and Machine Intelligence, 14(7):710-732, 1992.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37410</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[D. R Mitchell. Generating Antialiased Images at Low Sampling Densities. In M. C. Stone, editor, Computer Graphics (SIGGRAPH '87 Proceedings), volume 21, pages 65-72, July 1987.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74362</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[J. Painter and K. Sloan. Antialiased Ray Tracing by Adaptive Progressive Refinement. In J. Lane, editor, Computer Graphics (SIGGRAPH '89 P~vceedings), volume 23, pages 281-288, July 1989.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>731973</ref_obj_id>
				<ref_obj_pid>647651</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[F. Pighin, D. Lischinski, and D. Salesin. Progressive Previewing of Ray-Traced Images Using Image-Plane Discontinuity Meshing. Ewvgraphics Workshop on Rendering 1997, May 1997.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192189</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[H. Rushmeier and G. Ward. Energy Preserving Non-Linear Filters. In A. Glassner, editor, Computer Graphics P~vceedings, Annual Conference Series, pages 131-138, July 1994.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[R Schroeder and R Hanrahan. On the Form Factor between Two Polygons. Technical Report CS-404-93, Princeton University, Computer Science Department, 1993.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[R Shirley. Hybrid Radiosity/Monte Carlo Methods. In Siggraph 94 Course on Advanced Radiosity, August 1994.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192210</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[A.J. Stewart and S. Ghali. Fast Computation of Shadow Boundaries Using Spatial Coherence and Backprojections. In A. Glassner, editor, Computer Graphics P1vceedings, Annual Conference Series, pages 231-238, July 1994.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356626</ref_obj_id>
				<ref_obj_pid>356625</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[I. Sutherland, R. Sproull, and R. Schumacker. A Characterization of Ten Hidden- Surface Algorithms. ACM Computing Surveys, 6(1):387-441, March 1974.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134029</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[S. Teller. Computing the Antipenumbra of an Area Light Source. In Computer Graphics (SIGGRAPH '92 P~vceedings), volume 26, pages 139-148, July 1992.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>275485</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[S. Teller, K. Bala, and J. Dorsey. Conservative Radiance Interpolants for Ray Tracing. Eulvgraphics Workshop on Rendering 1996, May 1996.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[C. Vedel. Computing Illumination from Area Light Sources by Approximate Contour Integration. In P~vceedings of Graphics Interface'93, Toronto, Canada, May 1993.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192286</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[G. J. Ward. The RADIANCE Lighting Simulation and Rendering System. In A. Glassner, editor, Computer Graphics P1vceedings, Annual Conference Series, pages 459-472, July 1994.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378490</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[G. J. Ward, F. M. Rubinstein, and R. D. Clear. A Ray Tracing Solution for Diffuse Interreflection. In Computer Graphics (SIGGRAPH '88 P1vceedings), pages 85-92, August 1988.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807419</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[T. Whitted. An Improved Illumination Model for Shaded Display. In Computer Graphics (SIGGRAPH '79 P~vceedings), volume 13, pages 1-14, August 1979.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[M. Woo, J. Neider, and T. Davis. OpenGL P1vgramming Guide. Addison Wesley Developers Press, 1996.]]></ref_text>
				<ref_id>36</ref_id>
			</ref>
			<ref>
				<ref_obj_id>212194</ref_obj_id>
				<ref_obj_pid>212189</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[G. Wyvill, C. Jay, D. McRobie, and C. McNaughton. Pixel Independent Ray Tracing. In Computer Graphics: Developments in Virtual Environments (Proc. CG International '95), pages 43-55. Springer-Verlag, 1995.]]></ref_text>
				<ref_id>37</ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[G. Wyvill and R Sharp. Fast Antialiasing of Ray Traced Images. In New Advances in Computer Graphics (P1vc. CG International '95), pages 579-588. Springer-Verlag, 1989.]]></ref_text>
				<ref_id>38</ref_id>
			</ref>
			<ref>
				<ref_obj_id>2322988</ref_obj_id>
				<ref_obj_pid>2322500</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[W. Zeng and B. Liu. Geometric-Structure-Based Error Concealment with Novel Applications in Block-Based Low Bit Rate Coding. IEEE Trans. Ci~: and Sys. for Video Tech., to appear, 1998.]]></ref_text>
				<ref_id>39</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Copyright &#38;#169;1998 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers or to redistribute to lists, requires specific permission and/or a fee. Progressive Radiance 
Evaluation Using Directional Coherence Maps Baining Guo Intel Corporation. Abstract We develop a progressive 
re.nement algorithm that generates an approximate image quickly, then gradually re.nes it towards the 
.­nal result. Our algorithm can reconstruct a high-quality image after evaluating only a small percentage 
of the pixels. For a typical scene, evaluating only 6% of the pixels yields an approximate image that 
is visually hard to distinguish from an image with all the pixels evaluated. At this low sampling rate, 
previous techniques such as adaptive stochastic sampling suffer from artifacts including heav­ily jagged 
edges, missing object parts, and missing high-frequency details. A key ingredient of our algorithm is 
the directional coherence map (DCM), a new technique for handling general radiance dis­continuities in 
a progressive ray tracing framework. Essentially an encoding of the directional coherence in image space, 
the DCM per­forms well on discontinuities that are usually considered extremely dif.cult, e.g. those 
involving non-polygonal geometry or caused by secondary light sources. Incorporating the DCM into a ray 
tracing system incurs only a negligible amount of additional computation. More importantly, the DCM uses 
little memory and thus it preserves the strengths of ray tracing systems in dealing with complex scenes. 
We have implemented our algorithm on top of RADIANCE. Our enhanced system can produce high-quality images 
signi.­cantly faster than RADIANCE sometimes by orders of magni­tude. Moreover, when the baseline system 
becomes less effective as its Monte Carlo components are challenged by dif.cult lighting con.gurations, 
our system will still produce high quality images by redistributing computation to the small percentage 
of pixels as dictated by the DCM. CR Categories: I.3.7 [Computer Graphics]: Three-Dimensional Graphics 
and Realism; I.3.3 [Computer Graphics]: Picture/Image Generation. Keywords: Progressive re.nement, image-space 
discontinuities, directional coherence, radiance evaluation, rendering 1 Introduction Despite the rapidly 
increasing power of computers, global illumina­tion is far from being a real-time process. Accurate radiance 
eval­uations often require hours of computation for complex scenes. To *Microcomputer Research Labs, 
RN6-35, 2200 Mission College Blvd, Santa Clara, CA 95052, email: baining guo@ccm.sc.intel.com balance 
rendering speed and visual realism, global illumination al­gorithms often take a progressive re.nement 
approach. In the radiosity framework, Cohen [8] extended earlier work by Bergman [2] and developed a 
progressive re.nement algorithm that produces successive approximations, re.ning continuously towards 
the .nal radiosity solution. As an acceleration strategy, the idea of progressive re.nement is readily 
applicable to radiance evaluation, which must account for all major modes of light transport. One approach 
to progres­sive radiance evaluation is to combine progressive radiosity with ray tracing in a multi-pass 
system (e.g. [6, 27]). Alternatively, we can use a progressive ray tracing algorithm based on Monte Carlo 
light transport [16]. Compared to the multi-pass approach, pro­gressive ray tracing has several important 
advantages in real-world applications. In particular, ray tracing uses much less memory than radiosity, 
while placing fewer restrictions on surface geometry and re.ectance models. This paper develops a radiance 
evaluation algorithm using pro­gressive ray tracing. Based on the hierarchical integration technique 
by Kajiya [16], Painter has proposed an adaptive sampling method for progressively re.ning a ray traced 
image [23] (see also [37]). One of his important contributions was to recognize that in a pro­gressive 
ray tracing system, different sampling strategies apply to the task of locating image features and the 
task of increasing pixel con.dence [23].1 Our main goal is to capture image features early and generate 
high-quality images as quickly as possible. A funda­mental obstacle facing adaptive sampling techniques 
(including the edge following methods for anti-aliasing [3, 12, 38]) is that these techniques cannot 
produce high-quality images before densely sam­pling all the high-frequency details. We overcome this 
obstacle with knowledge about discontinuities. Researchers have been aware of the importance of discontinu­ities 
for decades, and investigations in this area have led to algo­rithms for shadows (e.g. [10, 7, 5, 30, 
32, 28, 11]) and disconti­nuity meshing (e.g. [14, 20]). The discontinuities computed by these object-space 
algorithms may be projected and inserted into an image-plane discontinuity mesh (IPDM), as was proposed 
by Pighin [24]. The IPDM produces dramatically better shadows from early on, but there are problems, 
including dif.culties in handling non-polygonal geometry and discontinuities caused by secondary sources, 
as well as the substantial space requirements for discon­tinuity computations with a complex scene. A 
big source of in­ef.ciency in the existing discontinuity algorithms is that they try to locate potential 
discontinuities instead of the actual discontinu­ities. In addition, these object-space algorithms cannot 
deal with the view-dependent specular components of radiance discontinuities. A key ingredient of our 
algorithm is the directional coherence map (DCM), a new technique for handling general radiance discon­tinuities 
in a progressive ray tracing framework. The DCM includes 1The task of increasing pixel con.dence has 
attracted extensive research in the context of anti-aliasing [13]. We do not develop new anti-aliasing 
techniques; instead we construct our progressive renderer on top of a base­line ray tracer such that 
the pixel con.dences of our system depend on the sampling strategy of the baseline system. By building 
our progressive ren­derer this way, we know in advance that we will capture the same set of features 
as the baseline system does. two main components: an adaptive partition of the image plane into square 
blocks, such that each block is simple enough to have at most one discontinuity edge, and an estimation 
of the orientation of the discontinuity edge in each block. The DCM helps us to capture ra­diance discontinuities 
through a .nite element approximation to the radiance function, with the .nite elements on each block 
oriented in accordance with the orientation of the discontinuity within that block. In facing the challenge 
of treating general radiance discontinu­ities, our overall strategy is to combine object-space and image­space 
data. Speci.cally, we ef.ciently obtain object boundaries in the image using the Z-buffer hardware [36]. 
By doing so we bene.t from the apriori knowledge of the scene. We extract other discon­tinuities from 
densely evaluated pixels on the block boundaries in the DCM. Extracting discontinuity information from 
radiance sam­ples is a unique feature of our algorithm. This feature not only allows us to treat several 
types of discontinuities ignored by pre­vious algorithms, but also saves us time by focusing on the actual 
discontinuities. We have implemented our progressive rendering algorithm us­ing RADIANCE [33] as the 
baseline system. To render images of comparable quality, our system typically takes 1.4to 1.15of the 
time needed by RADIANCE. When their Monte Carlo components are challenged by dif.cult lighting con.gurations, 
RADIANCE and other ray tracing systems for global illumination [27] will become less effective. In this 
situation, our system can still produce high quality images by reallocating computational resources to 
increase the accuracy of the small percentage of pixels needed by the DCM. Fig. 12 demonstrates this 
capability. The DCM performs well on a variety of discontinuities, including those that can be handled 
by ex­isting discontinuity algorithms (see, e.g., Fig. 10 with .ne shadows cast by polygonal occluders) 
and those that cannot (see, e.g., Fig. 13 with closely packed surfaces which are both curved and specular). 
Finally, compared to Painter s successful system based on progres­sive adaptive sampling [23], our system 
generates far better images for the same amount of computation time, as Fig. 2 (c) and Fig. 4 show. The 
remainder of the paper is organized as follows. In Section 2, we give a high level overview of the progressive 
rendering pipeline in our system. Section 3 describes the treatment of a block with simple discontinuities, 
which serves as the foundation of the DCM and is based on the least discrepancy direction and oriented 
.nite el­ements. Section 4 discusses the initialization and re.nements of the DCM, detailing steps for 
partitioning the image plane into blocks and tests for determining if a block has only simple discontinuities. 
Section 5 provides the details of our implementation. Results are presented in Section 6, followed in 
Section 7 by conclusions and suggestions for future work. 2 System Overview Our progressive rendering 
system relies on a baseline ray tracing system for pixel radiance evaluations. This type of ray tracing 
sys­tem was .rst proposed by Kajiya [16], building on earlier work by Cook [9] and Whitted [35]. For 
simplicity, we assume the base­line system generates an anti-aliased image by .ltering an enlarged work 
image of super-sample resolution (this assumption can be re­laxed to allow baseline ray tracers that 
collect super-samples for individual pixels of the output image). Our progressive rendering system augments 
the baseline system with a component responsible for deciding where to sample and how to reconstruct 
an approxi­mate image on user demand. The rendering pipeline of our system, shown in Fig. 1, has two 
main stages. The .rst is the regular subdivision stage, in which we use a quadtree to partition the image 
plane into small blocks. We refer to these blocks as elementary blocks. To perform the regular image 
plane block refinement Figure 1: The rendering pipeline of our system. The approximate image is available 
any time, on demand. When all blocks are of the pixel size, the iterative block re.nement terminates 
with the approximate image output as the .nal image. subdivision, we start with the entire image plane 
as the root block and recursively subdivide each block in four until the current block has become an 
elementary one. During the regular subdivision, the four corners of each block are sampled, and an approximate 
image may be created for display at any time by interpolating the corner values. Fig. 2 (a) shows an 
example image at the end of the regular subdivision stage. The second stage is an iterative process in 
which we begin con­structing and re.ning the DCM. In each iteration, we select a sub­set of blocks as 
edge blocks and analyze them for discontinuities; blocks not selected simply go through another step 
of regular subdi­vision. On each edge block, we densely sample the block boundary (not just the four 
corners) and subdivide the block into four quads for the next iteration. From the evaluated boundary 
pixels and some additional object-space data, we infer the discontinuities on each edge block and record 
the information into the DCM. With this in­formation, an oriented .nite element approximation is constructed 
on the block. The oriented .nite elements on edge blocks and the bilinear interpolants on the other blocks 
may be resampled into an approximate image at the user s request. Figs. 2 (b) and (c) are images from 
the second stage. Fig. 2 (d) is the .nal image.  3 Blocks with Simple Discontinuities Taking a divide-and-conquer 
approach, the DCM treats discontinu­ities by partitioning the image plane into small blocks so that most 
blocks are crossed by no more than one discontinuity edge. More­over, the edge is expected to have small 
curvature like the example in Fig. 3 (a) as opposed to the corner in Fig. 3 (b). Since the ef­fectiveness 
of the DCM depends on its performance on blocks with simple discontinuities, we wish to capture such 
discontinuities us­ing a small number of samples. Traditionally, adaptive sampling techniques [22, 23, 
13] are used to reduce the amount of sampling needed to capture features or discontinuities. Adaptive 
sampling is effective in that it signi.cantly reduces sampling in areas away from discontinuities. However, 
adaptive sampling is not completely sat­isfactory for us because it does not produce good images unless 
the discontinuity areas have been densely sampled. In this section, we explore an alternative based on 
least discrepancy directions and oriented .nite elements. 3.1 Discontinuity Characteristics A simple 
way to capture a discontinuity edge within a block is to build a mathematical model for the edge. Since 
the block is small, the edge can be regarded as straight, and we can model its behavior by locating the 
endpoints on the block boundary. This is essentially the approach we take, even though the basic idea 
is modi.ed in several ways to accommodate the special properties of image data.  Figure 2: Progressive 
renderings of an of.ce scene lit by sunlight transferred through a light shelf. (a) The approximate image 
at the end of the regular subdivision, with 1.6% evaluated pixels located at the corners of the 8x8blocks 
in the work image. (b) The approximate image after boundary evaluations for all 8x8edge blocks in the 
work image, with 5% of pixels evaluated. (c) The approximate image after evaluating about 6% of the pixels, 
whose locations are shown in Fig. 5 (bottom left). (d) The .nal image as rendered by the baseline RADIANCE 
system. The scene model was supplied courtesy of Greg W. Larson. y contour C z" z  z 4 3 z 2 
x z node z 1 value (a) (b) (c) (d) 02468 8 6 4 2 0   (e) (f) (g) (h)  Working with image data differs 
fundamentally from working with geometric data. The discontinuity edges computed by discon­tinuity meshing 
algorithms have two properties. First, the discon­tinuities are abstract mathematical lines with no shape 
or width. Second, discontinuities are either present or absent at a given lo­cation. In contrast, image 
edges have spatial scales (e.g. sharp or fuzzy) and their existence at a given location is modulated 
by their strength [21] (a weak edge may be just a faint wisp). These properties of image edges create 
challenges. For example, it is no longer trivial to de.ne the location of the edge. In addition, the 
fact that image edges have characteristics such as shape and strength re­quires more information to be 
estimated with the small amount of sampling at our disposal. Figure 3: Discontinuity analysis on a block. 
The discontinuity in (a) is considered simple whereas that in (b) is not, because of the corner. The 
geometry for the least-discrepancy direction is given in (c). In (d) we show the construction of a typical 
bilinear element fe{x)on a quadrilateral Q=[z1z2z3z4]with known node val­ues fn{zi);i=1:4. Essentially 
this construction is a Gouraud interpolation with the scanline rotated to be parallel with the least­discrepancy 
direction. Note that (d) is a zoomed version of the shaded element in (f). Finally, oriented .nite elements 
are shown in (e) through (h) for four different orientations.  The least discrepancy direction and 
oriented .nite element dis­cussed below form an integrated approach to extracting discontinu­ity information 
from image data and modeling the discontinuities in a .nite element approximation to the block radiance 
function. This approach does not explicitly refer to discontinuity locations, and it models edge shape 
as well as strength. Our discussions on the least-discrepancy direction and oriented .nite elements can 
be eas­ily extended to any convex image blocks, including the non-square blocks often encountered in 
a quadtree subdivision of the image plane.  3.2 The Least Discrepancy Direction The least discrepancy 
direction m{Bk)of a kxkblock Bkis de.ned to be the unit vector that minimizes the contour integral Z 
d{n)=1{f{x+t{x)n),f{x))2ds; s C where Cis the boundary contour of Bkand sis arc length (the reader may 
observe that the integration actually only needs to ex­tend over half the contour). For a .xed direction 
nand a point xon C, the scalar t{x)is chosen such that the parametric line y{t)=x+tnintersects the contour 
Cat xand y=x+t{x)n, as is shown Fig. 3 (c). Once the radiance function f{x)is known on the contour Cthrough 
boundary evaluation, the directional dis­crepancy d{n)is a well-de.ned function of the direction n. For 
implementation, we let n=[cosO;sinO]and discretize the angular range 0:O<ninto hdifferent directions 
Oi = in.h;i=0:{h,1). For each direction ni =[cosOi;sinOi],the directional discrepancy d{ni)is evaluated 
as 1X 2 di =d{ni)= {f{p+t{p)ni),f{p)); 4{k,1) p2P where Pis the set of all pixels in Cand t{p)is chosen 
such that the line y{t)=p+tniintersects the contour Cat pand p 0 = p+t{p)ni. Even though pis a pixel 
location, p 0may not be, in which case f{p 0)is linearly interpolated from two adjacent pixel values. 
From the evaluated sequence fd0;:::;dh,1g,we .nd the minimizer dj=min fd0;:::;dh,1gand set the least 
discrepancy direction m{Bk)=nj. 3.3 Oriented Finite Elements Once the least discrepancy direction is 
known, the radiance func­tion is approximated by a .nite element function whose elements are oriented 
along the least discrepancy direction. This .nite ele­ment approximation is a continuous function consisting 
of bilinear elements (quadratic polynomials). Fig. 3 (e) through (h) describes oriented .nite elements 
for an 8x8block with h=8(the angular range 0:O<nis discretized into eight different directions). In this 
case, there are eight different types of oriented .nite elements, one for each discretized direction. 
Only four of them are shown in the .gure; the other four are obtained from the ones shown by a 90­degree 
rotation. The integer values in Fig. 3 (e) mark the locations of the pixel centers on the block boundary. 
These locations are also the locations for the node values of the .nite elements. In Fig. 3 (e) and (h), 
there are nodes situated halfway between two pixels. For a node of this sort, the node value is taken 
to be the average of the two adjacent pixels. In general, for an arbitrary Osome nodes of the bilinear 
elements may not coincide with the pixel locations, and these nodes values are linearly interpolated 
from the adjacent pixels. To compare the quality of images generated using oriented .nite elements and 
Painter s adaptive stochastic sampling method [23], Fig. 4 (bottom) examines zoomed views of the same 
region in Fig. 2 (c) and Fig. 4 (top) (both images are .ltered down from their work images using a Gaussian). 
For the same sampling rate, the DCM al­ready produces a high-quality image while Painter s method suffers 
from artifacts including heavily jagged edges, missing objects parts, and missing high-frequency details. 
Fig. 5 shows the sampling pat­terns of Painter s method and the DCM. Also compare the zoomed views in 
Fig. 4 with the zoomed sampling patterns in Fig. 5 (the zoomed sampling pattern of the DCM does not include 
the extra samples needed for the .rst-order test described in Section 4.3, but the extra sampling is 
included in the 6% sampling rate reported).  Figure 4: Comparison between the DCM and adaptive stochastic 
sampling with the of.ce example. The top image, rendered by sam­pling 6% of the pixels using Painter 
s adaptive stochastic sampling technique, should be compared with the 6% DCM image in Fig. 2 (c). The 
bottom image contains four zoomed views of the same re­gion: 6% Painter image in (a) with its work image 
in (b), and 6% DCM image in (c) with its work image in (d). Notice that Painter s method very gracefully 
locates the features, but the number of samples at our disposal is just too small to make this method 
effective. We have also compared the DCM with adap­tive super-sampling [35] and strati.ed sampling [19]. 
Painter s method performs much better than the other two because its un­derlying hierarchical integration 
sampling technique combines the strengths of adaptive and strati.ed sampling by stratifying samples with 
strata that are dynamically adjusted as more samples are taken [16].  3.4 Directional Coherence Why 
does the least discrepancy direction work? Simply stated, it works because of image-space coherence. 
According to Sutherland [29], coherence is the degree to which parts of a scene or its pro­jection exhibit 
local similarities. Often we think of a discontinuity edge as the break of coherence, since image data 
change abruptly across the edge. However, discontinuities do not break all forms of coherence. Speci.cally, 
image data are typically coherent along the direction of the discontinuity edge even if they change abruptly 
across the edge [21]. Contour-based image coding (e.g. [21]) and more broadly, second generation image 
coding [18] takes advan­tage of this form of coherence. For a block with a simple discon­tinuity edge, 
the least discrepancy direction is really the direction Figure 5: Comparison of the sampling patterns 
of adaptive stochastic sampling (top row) and the DCM (bottom row). The patterns in the left column are 
taken from RADIANCE work images described in Section 5. Some of the .ne features are shown in zoomed 
views of the sampling patterns in the right column. These zoomed views correspond to the same region 
as the zoomed views in Fig. 4.  of maximal coherence as can be inferred from the evaluated bound­ary 
pixels. By orienting the .nite elements along this direction, we maximize the likelihood of capturing 
the discontinuity edge along with its characteristics (section 4.3 on block simplicity tests for the 
treatment of more complex discontinuities such as corners).  4 Coherence Map Construction and Re­.nements 
A DCM consists of a partition of the image plane into square blocks, with some selected blocks having 
a direction O(0:O<n) assigned to each of them. These selected blocks are called edge blocks; all other 
blocks are smooth blocks. The boundary of every edge block is densely evaluated, whereas a smooth block 
only has its four corners evaluated. To reconstruct an approximate image, we bilinearly interpolate each 
smooth block and build a .nite element approximation on every edge block, with the elements oriented 
to the direction recorded in the DCM. The main reason for separating edge and smooth blocks is ef.­ciency. 
Since it is much more expensive to sample an edge block, we wish to reserve edge blocks for areas with 
discontinuities. In our block classi.cation procedure, we select edge blocks based on evaluated pixel 
radiance and object-space data. Thus projections of object boundaries are taken into consideration from 
the begin­ning even for small objects. To further reduce the chance of miss­ing blocks with discontinuities, 
a smooth block is not automatically subdivided into four smooth quads in the block re.nement step. In­stead, 
each quad is reevaluated to see if it contains discontinuities. The DCM is a divide-and-conquer technique: 
it aims to parti­tion the image plane into blocks with simple structures (i.e., blocks crossed by at 
most one discontinuity edge). At any given stage of the progressive radiance evaluation, the oriented 
.nite elements produce good results on edge blocks with simple structures; the re­sults on more complex 
edge blocks are less certain. Naturally, we wish to identify these complex blocks so that we can focus 
our sam­pling efforts on them at the next stage of the radiance evaluation. With this goal in mind, we 
have designed block simplicity tests for edge blocks. An edge block is a simple edge block if it passes 
these simplicity tests; otherwise it is a complex edge block. A big concern with the block simplicity 
tests is that, in general, it is not possible to guarantee that a block is crossed by at most one edge 
as long as the block interior is not fully sampled. Nev­ertheless, with the evaluated block boundary 
we can identify many offending blocks. Our experiments indicate that with carefully de­signed simplicity 
tests, we can identify and subdivide suf.ciently many offending blocks for the purpose of generating 
high-quality images. To further avoid mistreating a block with complex interior discontinuities, we constantly 
reassess the simplicity of a block as more evaluated pixels become available. This veri.cation is done 
as part of the lazy boundary evaluation described later. (a) (b) (c) (d) Figure 6: Lazy boundary evaluation 
on a simple edge block. (a) A simple edge block, with the light grey boundary representing evaluated 
pixels. (b) The difference between a newly evaluated pixel (marked by the grey dot) and the value predicted 
by the .­nite element approximation is within the prescribed tolerance, and no boundary evaluation is 
invoked. (c) One of the newly evaluated pixels, marked by the black dot, deviates too much from the value 
predicted by the current .nite element approximation. (d) Bound­ary evaluations are triggered for the 
surrounding blocks. 4.1 Re.nement Steps for the DCM As mentioned in Section 2, the iterations for the 
DCM construc­tion and re.nements begin after the regular subdivision stage has partitioned the image 
plane into elementary blocks. Each iteration of the DCM re.nement takes .ve steps. First, the block classi­.cation 
step examines the pool of smooth blocks to select edge blocks. For the .rst iteration, this pool consists 
of all elementary blocks. In any later iteration, the pool is formed by collecting the four quads subdivided 
from the smooth blocks in the previous itera­tion. The classi.cation also marks as edge blocks the four 
quads of every complex edge block from the previous iteration. In the sec­ond step, the boundary evaluation 
procedure densely samples the block boundary of every edge block. Then, the simplicity test step analyzes 
every edge block and labels as complex edge blocks those that fail any block simplicity test. The fourth 
step builds oriented .nite elements, and the .fth step subdivides every block into four quads for the 
next iteration. The four quads Bi;i=1:4of a simple edge block Bfrom the previous iteration are computed 
using a lazy boundary evaluation procedure as shown in Fig. 6. Since Bis a simple edge block, we already 
have a .nite element approximation f{x)on B.Our ex­periments indicate that this .nite element approximation 
is usually of very good quality unless some complex structures in the interior of Bhave gone undetected 
in the previous iteration. Thus before going through the normal boundary evaluation with Bi,we evalu­ate 
the corner pixels of Biand compare the resulting pixel values with the pixel values predicted by the 
existing .nite element ap­proximation f{x). If the predicted values are within a prescribed tolerance 
(1% relative error in our system) from the evaluated pixel values, then the simplicity of Bis re-con.rmed 
and we continue to use f{x)on the new blocks Biwith small modi.cations. More speci.cally, we skip the 
normal boundary evaluation procedure and substitute f{x)for the pixel values everywhere on the boundary 
of Biexcept at the corners, where the evaluated pixels are used. With the block boundary so obtained, 
we construct a .nite element ap­proximation fi{x)on Biwith the least discrepancy direction of B. By incorporating 
the evaluated pixels into fi{x), weforce theap­proximate image to converge to the .nal image as rendered 
by the baseline system. 4.2 Block Classi.cation A smooth block can be reclassi.ed as an edge block through 
the fol­lowing two steps. First, a block contrast value is computed for each block and this value is 
tested against the prescribed contrast thresh­old tc. The block is classi.ed as an edge block if its 
block contrast value exceeds the threshold tc. Second, a visible-line rendering of (a) (b) (c) Figure 
7: Block simplicity test examples. Case (a) passes both the zero order and .rst order tests. Case (b) 
passes the zero order test but not the .rst order. Case (c) fails both tests. The black and light grey 
line segments on the block boundary are spans. the scene is generated, and every block crossed by a visible 
line is classi.ed as an edge block. Contrast Thresholding. For an elementary block with corner lu­minance 
values fg1;:::;g4g, the block contrast quanti.es the ratio between the average luminance g and the deviation 
tgfrom the average. Following Mitchell [22], we compute the block contrast max,min as ,where maxand minare 
the maximum and minimum max+minof the corner luminance values fg1;:::;g4grespectively. The cri­terion 
for locating high-frequency details has a signi.cant impact on the effectiveness of the initial edge 
block selection. The issue here is not the loss of details since the progressive rendering even­tually 
produces the same image as the baseline rendering system. The main concern is whether certain details 
will appear at earlier stages of the rendering process. In this regard, a criterion based on contrast 
tg.g compares favorably with methods that use devia­tion tgalone [19], because the nonlinear human visual 
sensitivity to the change in light intensity is closely modeled by the contrast tg.g rather than just 
tg. This logarithmic contrast perception model is the most widely used among other models [15]. In our 
implementation, we set tc =0:05. Computing Visible Lines. The visible lines that we choose in­clude both 
object boundaries from the scene geometry and their re­.ections in planar mirrors. These visible lines 
are ef.ciently com­putable through the standard graphics pipeline [36], which supports both polygonal 
objects and commonly-used curved objects. We use polygon offset to avoid the stitching artifact that 
could result from a naive Z-buffer implementation [36], because stitching turns a solid line into a dotted 
line and thus allows it to pass through an elementary block undetected. For a curved object the visible-line 
renderer is instructed to draw the silhouette only [36]. The mirror re.ections of visible lines are computed 
as in [24]. Note that a pure contrast-driven classi.cation can be deceptive for large blocks. To alleviate 
this problem, we use visible lines to account for object boundaries and at each iteration we reclassify 
smooth blocks to recover features missed in the previous iteration due to the larger blocks and fewer 
available samples. 4.3 Block Simplicity Tests The simplicity tests in our system are designed systematically 
based on the traditional methodology of proof-by-contradiction. First we assume that the block is crossed 
by at most one edge. From this assumption, we derive facts about the discontinuities on the block boundary. 
The derived facts can be veri.ed using the known ra­diance values on the block boundary. The block fails 
the tests if any contradiction arises. Fig. 7 provides examples for some of the following tests. Zero 
Order Test. For a block crossed by a single edge, we should be able to .nd a luminance threshold tb, 
such that the block can be divided into two simply connected regions (connected and having no holes): 
one for pixels with luminance above the threshold and S estimated tangent x 0  x 0 x 0 x 1 inner 
layer boundary layer (a) (b) Figure 8: The .rst order test for block simplicity. (a) The tangent direction 
for the edge passing through x0can be estimated from the transition points x0and x 0on two adjacent layers. 
(b) To pass the 0.rst order test, the line segment [x0x1](the thin black line) and the tangent directions 
at the two transition points x0and x1(the two thick black lines) must be nearly parallel. the other for 
those below. The zero order test identi.es the blocks that cannot be so divided, using the evaluated 
boundary pixels of the block. In our system, we set tb =0:5{b1+d1),where b1is the highest luminance on 
the block boundary and d1the lowest. Thresholding with tbconverts the block boundary into a binary pattern 
consisting of 1-pixels that exceed tcand 0-pixels that do not. Juxtaposed 1-pixels can be collected together 
to form a 1­span , and likewise 0-pixels form 0-spans (Fig. 7). Two spans are separated by a transition 
point, which is de.ned as a 1-pixel having at least one 0-pixel neighbor. To simplify this binary pattern, 
we apply a median .lter of length three to eliminate spans of one-pixel long. This simpli.cation is needed 
because of the Monte Carlo component in the baseline ray tracing system, and the median .l­tering helps 
to eliminate the spurious spans caused by Monte Carlo noise. To reduce noise effects, we also apply a 
length-three Bartlett .lter to the luminance values on the block boundary before thresh­olding. The .ltered 
values are only used for computing the spans, not for constructing the block radiance approximation. 
With spans of length one removed, the number of transition points on the block boundary must be even. 
If there are more than two transition points, the block fails the zero order test. Otherwise the block 
survives the test and moves onto the .rst order test. First Order Test. A block with no transition points 
passes the .rst order test by default. For a block having two transition points, let the transition points 
be x0and x1. If the block is crossed by a single edge passing through x0and x1, the tangent vectors of 
the edge at the two points should be close in direction. The .rst order test com­putes the tangent vectors 
at x0and x1and measures the difference between the tangent directions against a prescribed tangent thresh­old 
tm. If the measured difference exceeds the threshold, the block fails the test. Let O01be the direction 
of the line segment [x0x1] that connects x0and x1, whose tangent directions are determined by angles 
O0and O1respectively. The difference of the tangent di­rections is measured by m01 =max{jO0,O01j;jO1,O01j).Fig. 
8 (b) illustrates the geometry. The threshold tmis set to 0:05ntimes the L2norm jjx1,x0jj2in our implementation. 
A challenging technical problem in the .rst order test is the ef­.cient computation of tangent vectors 
at the transition points. We have developed a technique that estimates tangent directions at the cost 
of a few additional pixel radiance evaluations. To estimate the angle O0at x0, we evaluate the luminance 
function at pixels along ashort linesegment Snext to x0on the inner layer of pixels, as is shown in Fig. 
8 (a). The luminance threshold tband a length-three median .lter are applied to these additional luminance 
values to ex­tend the binary pattern from the boundary layer to the line segment S. The evaluation process 
starts from the pixel next to x0in the inner layer and elongates Sin both directions, stopping as soon 
as the transition point x00corresponding to x0is found on S.At pixel resolution, the transition points 
x 0 0and x0determine the tangent angle O0. Binarizing images to extract geometric patterns is not new; 
re­searchers have used this technique in the .eld of video coding [15]. An example is the geometric-structure-based 
directional .ltering proposed by Zeng [39]. Even though the patterns extracted in his work are complex 
and uncertain, he has successfully used these pat­terns to improve block-based video coding at low bit 
rates. Object Test. We enforce the constraint that no more than two object tags are allowed on the boundary 
of a block. In addition, the object tags also form object spans similar to the spans in the zero order 
test, and only two object spans are allowed on the block boundary. A block violating these constraints 
will not be consid­ered simple. The object tags are returned by the baseline ray tracer as by-products 
of pixel radiance evaluations. If a block fails any of the above block simplicity tests, it is la­beled 
as a complex edge block. In the next iteration of block re­.nement, we devote more sampling to a complex 
edge block Bc by performing real (i.e. not lazy) boundary evaluations on the four quads subdivided from 
Bc. 4.4 Discussion The DCM is useful because it allows us to generate high-quality im­ages from a small 
percentage of evaluated pixels. Unfortunately, the danger of serious approximation errors also grows 
with the num­ber of unevaluated pixels. For this reason, it is desirable to have a technique that uses 
known data (e.g. scene geometry and evaluated pixels) to bound the errors, possibly with the help of 
some analyti­cal formulation [31, 26, 1]. We have not derived such a technique. Instead, we have built 
two simple principles into the DCM con­struction. First, we always incorporate the newly evaluated pixels 
into the DCM and never overwrite them. As a result, the approx­imate image is guaranteed to converge 
to the .nal image as ren­dered by the baseline system, and all approximation errors are thus eliminated 
eventually. The second principle is that we regularly re-examine our previous decisions in partitioning 
blocks to detect errors: smooth blocks are tested for discontinuities and simple edge blocks are probed 
for complex structures. These error detections are done using newly evaluated pixels as part of the block 
classi.cation and lazy boundary evaluation. In practice, failure to detect discontinuities means the 
delay of high-quality images. In this regard, the DCM performs better with object boundaries than shadows 
and highlights, which can go un­detected with larger blocks. In fact, small highlights and shadows in 
the block interior will certainly go undetected until the block is subdivided. Fig. 2 (c) contains errors 
of this sort (e.g. the cup on the table). These performance problems often have solutions, albeit at 
additional cost. For example, one way to improve on shadows is to include shadow edges as in [24].  
5 Implementation We have implemented our progressive rendering algorithm using the RADIANCE system developed 
by Ward [33]. RADIANCE is a physics-based lighting simulation system that has gained consider­able reputation 
because of its successful use in real-world projects [33]. Our progressive rendering system allows the 
user to exam­ine the approximate image any time during the rendering process. When their Monte Carlo 
components are challenged by dif.cult lighting con.gurations, RADIANCE and other ray tracing systems 
for global illumination [27] will become less effective. In this sort of situation, our system uses the 
con.dence relocation technique described in Section 6.2 to generate images of high visual quality. Progressive 
Rendering with RADIANCE. In the baseline RADI-ANCE system, a work image is .rst generated at a super-sample 
(a) (b) (c) (d) (e) (f) (g) (h)  Figure 9: Boundary evaluation procedures for a 8x8block. The small 
grey squares stand for evaluated pixels. Other pixels on the block boundary are linearly interpolated. 
For a simple edge block, we follow the sequence (a), (b), (c), and (d), which evenly dis­tributes evaluated 
pixels on the block boundary. For a complex edge block, we also need to distribute evaluated pixels in 
the block inte­rior, and we follow the sequence (a), (b), (c), (e), (f), (g), and (h). resolution by 
a hybrid deterministic and Monte Carlo ray tracing program. This work image is then .ltered down to the 
resolution of the output image for anti-aliasing. To generate high-quality images with penumbra from 
a scene with .ne geometric details, as is the case with our examples, the work image is 3x3times as large 
as the output image, and every pixel is evaluated through ray tracing. For pixel radiance evaluation, 
our progressive rendering system uses exactly the same calculation parameters as the baseline sys­tem. 
Like the baseline system, our progressive system performs the rendering in batch mode. Anytime during 
the rendering process, the .nite element approximation to the radiance function can be as­sembled on-demand 
in a few seconds, and then re-sampled onto the work image. For the images in this paper, we .ltered the 
work image using the Gauss .lter provided with RADIANCE to produce the output image. For the DCM construction, 
we choose the size of the elementary block to be 8x8and discretize the angular range [0;n]into 8different 
directions. All the other DCM-related param­eters have been given in the previous sections. Fig. 9 explains 
boundary evaluations for 8x8blocks. The idea is to evenly distribute evaluated pixels in order to allow 
the con­struction of oriented .nite elements even before every pixel on the block boundary is evaluated. 
Note that for a complex edge block, we jump from Fig. 9 (c) to (d) because in (c) the distance between 
evaluated pixels (2) is already smaller than the size of the quads (4) and when this happens we start 
distributing evaluated pixels on the boundaries of the four quads of a complex edge block. Generating 
Better Images by Con.dence Relocation. Normally our approximate image progresses towards a .nal image 
deter­mined by some given calculation parameters. If this .nal image suffers from severe artifacts, we 
must raise the quality standard and progress towards a better .nal image. This approach is especially 
relevant to the Monte Carlo components of radiance evaluation. The Monte Carlo computations introduce 
noise while capturing dif­fuse interre.ections. A common approach to noise reduction is to increase the 
sampling rates. However, as Rushmeier argues, the sampling rates needed can be impractically high for 
certain dif.­cult lighting con.gurations [25]. With the DCM, it is possible to generate images with high 
vi­sual quality even when the Monte Carlo calculations make it too costly to evaluate every pixel accurately. 
The basic idea is sim­ple: the Monte Carlo noise is not part of the radiance function by nature, but 
a manifestation of the limitations of our radiance evalua­tion techniques. This means that when the pixel 
values are accurate, they will exhibit the image-space coherence we see in photographs. image L1{L2)error 
time (C) time (NC) of.ce 6% 0.008(0.02) 0.9 hrs 8.9 hrs of.ce .nal - 16.5 hrs 38.5 hrs museum 7% 0.005(0.016) 
0.05 hrs 1.2 hrs museum .nal - 1hrs 5hrs Table 1: Progressive rendering statistics. The time (C) column 
lists timings for scenes with cached irradiances, while the time (NC) column lists timings for scenes 
without caching. In particular, directional coherence will be present and we can use the DCM to reconstruct 
high-quality images from a small percent­age of evaluated pixels. Improvement of pixel accuracy is usually 
achieved by increasing our con.dence in the pixel values through variance reduction. When the available 
computation is more or less uniformly spread over the entire image plane, so is our con.dence in the 
pixel values. If the resulting image is poor, we can try to improve the accuracy of every pixel, but 
that usually means a dra­matic increase in computational costs. An alternative is to improve the accuracy 
of pixel values in a progressive system based on the DCM. Since the progressive system can generate high-quality 
im­ages with a small percentage of evaluated pixels, we usually obtain high-quality images without additional 
computation. When taking the second approach, we relocate the uniformly spread low con.­dence to concentrated 
high con.dence in the small percentage of pixels needed by the DCM. In our implementation, we use hierarchical 
integration [16] to reduce the variance within each pixel, stopping when a con.dence interval test passes 
[23]. Time and Space Considerations. Compared to the cost of pixel radiance evaluations, the time needed 
for managing the DCM and oriented .nite elements is negligible. More speci.cally, the time complexity 
of DCM-related construction is no greater than an in­verse discrete cosine transform (DCT), which is 
used for decoding JPEG images [15]. For a typical image included in this paper, gen­erating an approximate 
image can be generated on the order of sec­onds as opposed to the few hours needed for evaluating the 
pixel radiance. The memory requirements are also modest. In addition to the storage needed by the baseline 
RADIANCE, we need only store a list of edge blocks and a horizontal strip of the work image. The complete 
work image is stored on the disk and it is retrieved only when the user demands the system to display 
the current ap­proximate image.  6 Results All the high-quality approximate images reported in this 
paper are obtained after the progressive rendering system .nished processing all 4x4edge blocks. At this 
point, the approximate images become visually hard to distinguish from the .nal images for most scenes. 
 6.1 Progressive Rendering Fig. 2 shows a series of images progressively rendered from an of­.ce scene 
lit by sunlight transferred through a light shelf. This scene was introduced in [33] to demonstrate how 
RADIANCE pre­processes virtual light sources to optimize light calculations for certain dif.cult environments. 
Both the baseline and the progres­sive systems have included that optimization. As is typical with the 
other scenes we have tested, 6% evaluated pixels allow us to gener­ate a high-quality image that is hard 
to distinguish visually from the .nal image. See Fig. 5 for the locations of the 6% samples. Fig. 10 
Figure 10: Progressive renderings of a museum scene lit by skylight through the window. Notice the .ne 
shadows cast by the polygonal occluders. The top image is the approximate image after evaluating about 
7% of the pixels. The bottom image is the image rendered by the baseline RADIANCE system. The scene model 
was provided courtesy of Charles Ehrlich.  shows another example, which is a museum lit by skylight 
through the window. To quantify the errors in a high-quality approximate image, we subtract it from the 
.nal image Fto get an error image E.Then we compute the relative error as either jjEjj1.jjFjj1or jjEjj2.jjFjj2, 
where jj:jj1and jj:jj2is the L1and L2norms respectively. The resulting L1and L2errors for the two examples 
are in Table 1. Table 1 also compares the computation times for the high-quality approximate images and 
the .nal images. All timings are taken on a 180 MHz SGI Indy workstation with 64 Mb of main memory. The 
time indicated for each .nal image is the time needed by the baseline RADIANCE system. The time for an 
approximate image includes not only the time for pixel radiance evaluations but also all the computation 
related to the DCM. An important factor that affects the timings is the irradiance caching in RADIANCE 
[34]. The diffuse interre.ections in the rendering equation can be calculated using Monte Carlo ray trac­ing 
[16], but to reduce the variance to a tolerable level, hundreds of rays must be spawned for each eye-ray 
that strikes a surface. To avoid invoking this expensive calculation at every pixel, RADI-ANCE caches 
indirect diffuse contributions and interpolates them over each surface in the scene. At an early rendering 
stage, there is little irradiance cached in the scene to interpolate from, and an eye­ray is more expensive 
to evaluate. As time goes by, more cached values become available, and the radiance evaluation accelerates. 
Figure 11: Error distributions for the of.ce example. The top im­age shows the errors in the approximate 
image with 1.6% evalu­ated pixels. The bottom image is for the approximate image with 6% evaluated pixels. 
Even though the two distributions are taken from very different stages of the rendering process, the 
most signif­icant errors are clustered around the image discontinuities for both distributions. This 
attests to the importance of properly handling discontinuities when generating high-quality images. We 
tabulate two timings for each example: one measured with the lighting simulation starting without irradiance 
cached, and the other with the irradiance cached from a previous rendering from a dif­ferent viewpoint. 
Note that even when irradiance caching is not a dominant factor, there is no exact correspondence between 
the time percentage and the percentage of pixels evaluated. In fact, the time percentage can be smaller 
than the pixel percentage when there is a large amount of irradiance cached in a scene, as is the case 
with the museum example. Fig. 11 provides error images at different stages of the rendering process. 
The intensities of both images have been scaled up to make the errors more visible. As a result, the 
error images mainly show the error distributions.  6.2 Con.dence Relocation Fig. 12 provides an example 
of generating better images with con­.dence relocation.2 Each pixel in the 512x342image on the left corresponds 
to 3x3pixels in the work image, but for the penumbra areas this sampling rate is insuf.cient. Fig. 12 
(a) and (c) show ar­tifacts in the left image with zoomed views of two regions. Fig. 12 2For information 
on luminaires, the reader is referred to the scene model at http://radsite.lbl.gov/radiance/pub/models/bath.tar.Z 
 (b) and (d) are zoomed views of the same two regions from an im­age rendered using con.dence relocation. 
The image generated by con.dence relocation is also a 512x342image, .ltered down from a work image 3x3times 
as large. But this time, instead of eval­uating every pixel of the work image, we only evaluate 6.7% 
of the pixels and we super-sample these pixels to increase pixel con­.dence. The computation times for 
the left image of Fig. 12 and the image with con.dence relocation are 5.5 hours and 2.7 hours respectively 
on our SGI Indy with each computation initiated with­out irradiance caching. When there is caching, the 
left image takes 1.4 hours and the image generated with con.dence relocation takes about 1.7of the time. 
The correctness of the image by con.dence relocation has been veri.ed with a benchmark image generated 
by taking 256super-samples for each pixel of the .nal image.  6.3 Discontinuity Varieties We have tested 
the DCM on a variety of scenes with very differ­ent discontinuities. Fig. 10 is a scene with .ne shadows 
cast by polygonal occluders. For this sort of scene, discontinuity meshing works well. The main advantage 
of the DCM in this case is the abil­ity to handle large scenes without suffering the storage overhead 
of meshing. Fig. 13 is from a scene .lled with specular surfaces placed next to each other. Discontinuity 
meshing cannot cope with this scene well because of the view-dependent specular highlights and curved 
geometry. Moreover, discontinuity meshing seeks to track down all potential discontinuities prior to 
radiance evaluation [20], which is extremely dif.cult to do here due to the closely packed specular sur­faces. 
To render the scene, we incorporate the DCM into Rayshade [17], a well-known system for classical ray 
tracing [35]. We choose to do so partly because RADIANCE does not handle torus primi­tives we also wanted 
to see how easy it is to incorporate the DCM into a typical ray tracing system. Rayshade collects radiance 
sam­ples for individual pixels of the output image as follows. First, one sample is collected for each 
output pixel and a contrast value is computed based on the current pixel and its four neighbors. If high 
contrast is encountered, the current pixel and the four neighbors are anti-aliased by taking 3x3samples 
for each pixel (the default setting). To incorporate the DCM into the system, we .rst create a work image 
3x3times as large as the output image. Then we carry out DCM-related construction as in our RADIANCE 
implementa­tion. The top right image in Fig. 13 is rendered after evaluating 10% of the pixels of the 
work image. The percentage is higher than usual because the complex interactions between the tightly 
packed specular surfaces lead to high contrast almost everywhere. As shown in Fig. 13 (bottom left), 
the high contrast also causes much super-sampling in Rayshade. Fig. 13 (bottom row) compares the Rayshade 
and DCM sampling patterns in the RADIANCE work im­age using zoomed views of a chosen region (for clarity, 
the zoomed sampling pattern for the DCM does not include the extra samples needed for the .rst-order 
test described in Section 4.3, but the extra sampling is included in the 10% samples reported). On our 
Indy workstation, the top image of Fig. 13 takes 10 minutes to render with Rayshade, whereas the bottom 
image takes 1.4 minutes when the DCM is used.  Figure 13: The top left image is a benchmark image rendered 
by Rayshade. The top right image is rendered using the DCM after a small percentage of pixels have been 
evaluated. The main purpose of this experiment is to test the DCM s capability in treating discontinuities 
that cannot be handled by existing discontinuity algorithms. The bottom row provide zoomed views of the 
sampling patterns for the same region of the work image (left pattern for Rayshade and right for the 
DCM). The grey dots indicate the locations of the samples. Compared to Rayshade, the DCM performs much 
less sampling yet can produce an image of similar quality because of the effective treatment of discontinuities. 
The scene model was provided courtesy of Stuart Warmink.  7 Conclusions We have presented a progressive 
re.nement algorithm for radiance evaluation, by showing how to handle general radiance discontinu­ities 
using a novel technique called the Directional Coherence Map. The DCM subdivides the image plane into 
blocks with simple dis­continuities and captures the discontinuities on each block with the least discrepancy 
direction and oriented .nite elements. By com­bining object-space data with discontinuity information 
extracted from evaluated pixel radiance, the DCM achieves both time and storage ef.ciency. Thus it is 
possible to treat discontinuities in a complex scene that could barely be loaded into the main memory 
of our computer. The DCM is also shown capable of effectively treat­ing a variety of discontinuities, 
including several types that cannot be handled by existing discontinuity algorithms. For a global illu­mination 
scene consisting of smooth surfaces, the DCM generates high-quality images much faster than progressive 
ray tracing sys­tems based on adaptive sampling. When the Monte Carlo compo­nents of a lighting calculation 
system are challenged by very dif­.cult lighting con.gurations, our algorithm can still produce high­quality 
images ef.ciently by relocating the computational resources to the small percentage of pixels needed 
by the DCM. Several related research topics remain to be explored. The DCM gains its power from the directional 
coherence in the image plane. Other forms of coherence in the radiance function should also be investigated. 
An example is Teller s radiance interpolant, which makes use of the coherence between images from nearby 
view­points [31]. Another area of research is the use of sophisticated color vision models to improve 
our re.nement strategy so that less computation is distributed to areas of little perceptual importance. 
Bolin s work has shown promising results in this direction [4]. Fi­nally, we expect to see growing interest 
in image-space discontinu­ities in the near future. Our experiments not only indicate the im­portance 
of properly treating image-space discontinuities, but also demonstrate the power of image-space discontinuity 
information. In general, we believe techniques for manipulating image data will become more important 
as the average size of the polygons passing through the graphics pipeline approach the size of individual 
pix­els, and we hope that our work stimulates future research in this increasingly exciting area. Acknowledgments 
I would like to thank Demetri Terzopoulos for his help at several critical moments. Without his help 
this work would not be possi­ble. Thanks also to Greg W. Larson for his insightful comments on some of 
the ideas presented and for numerous consultations on RA-DIANCE, to Bede Liu and Wenjun Zeng for helpful 
discussions, to John Funge for proofreading part of this paper, and to the anony­mous reviewers for their 
constructive critique.  References [1] J. Arvo. The Irradiance Jacobian for Partially Occluded Polyhedral 
Sources. In A. Glassner, editor, Computer Graphics Proceedings, Annual Conference Series, pages 75 84, 
July 1994. [2] L. D. Bergman, H. Fuchs, E. Grant, and S. Spach. Image Rendering by Adap­tive Re.nement. 
In D. C. Evans and R. J. Athay, editors, Computer Graphics (SIGGRAPH 86 Proceedings), volume 20, pages 
29 37, August 1986. [3] J. Bloomenthal. Edge Inference with Applications to Antialiasing. In Computer 
Graphics (SIGGRAPH 83 Proceedings), volume 17, pages 157 162, July 1983. [4] M. R. Bolin and G. W. Meyer. 
A Frequency Based Ray Tracer. In R. Cook, editor, Computer Graphics Proceedings, Annual Conference Series, 
pages 409 418, August 1995. [5] A. T. Campbell III and D. S. Fussell. Adaptive Mesh Generation for Global 
Dif­fuse Illumination. In Computer Graphics (SIGGRAPH 90 Proceedings),vol­ume 24, pages 155 164, August 
1990. [6] S. E. Chen, H. E. Rushmeier, G. Miller, and D. Turner. A Progressive Multi-Pass Method for 
Global Illumination. In T. W. Sederberg, editor, Computer Graphics (SIGGRAPH 91 Proceedings), volume 
25, pages 165 174, July 1991. [7] N. Chin and S. Feiner. Near Real-Time Shadow Generation Using BSP Trees. 
In J. Lane, editor, Computer Graphics (SIGGRAPH 89 Proceedings), volume 23, pages 99 106, July 1989. 
[8] M. F. Cohen, S. E. Chen, J. R. Wallace, and D. P. Greenberg. A Progressive Re.nement Approach to 
Fast Radiosity Image Generation. In J. Dill, editor, Computer Graphics (SIGGRAPH 88 Proceedings), volume 
22, pages 75 84, August 1988. [9] R. L. Cook, T. Porter, and L. Carpenter. Distributed Ray Tracing. In 
Computer Graphics (SIGGRAPH 84 Proceedings), volume 18, pages 137 145, July 1984. [10] F. Crow. Shadow 
Algorithms for Computer Graphics. In Computer Graphics (SIGGRAPH 77 Proceedings), volume 11, pages 242 
248, July 1977. [11] G. Dretakkis and E. Fiume. A Fast Shadow Algorithm for Area Light Sources Using 
Backprojection. In A. Glassner, editor, Computer Graphics Proceedings, Annual Conference Series, pages 
223 230, July 1994. [12] A. Fujimoto, T. Tanaka, and K. Iwata. ARTS: Accelerated Ray Tracing System. 
IEEE Computer Graphics and Applications, 6(4):16 26, July 1986. [13] A. Glassner. Principles of Digital 
Image Synthesis, volume 1. Morgan Kauf­mann, 1995. [14] P. Heckbert. Discontinuity Meshing for Radiosity. 
Third Eurographics Workshop on Rendering, pages 203 226, May 1992. [15] A. Jain. Fundamentals of Digital 
Image Processing. Prentice Hall, 1989. [16] J. Kajiya. The Rendering Equation. In Computer Graphics (SIGGRAPH 
86 Proceedings), volume 20, pages 143 150, August 1986. [17] C. Kolb. Rayshade User s Guide and Reference 
Manual. Rayshade home page at graphics.stanford.edu, January 1992. [18] M. Kunt, A. Ikonomopoulos, and 
M. Kocher. Second-Generation Image Coding Techniques. Proc. of IEEE, 73(4):549 574, 1985. [19] M. E. 
Lee, R. A. Redner, and S. P. Uselton. Statistically Optimized Sampling for Distributed Ray Tracing. In 
B. A. Barsky, editor, Computer Graphics (SIG-GRAPH 85 Proceedings), volume 19, pages 61 67, July 1985. 
[20] D. Lischinski, F. Tampieri, and D. P. Greenberg. Combining Hierarchical Ra­diosity and Discontinuity 
Meshing. In Computer Graphics Proceedings, Annual Conference Series, pages 199 208, 1993. [21] S. Mallat 
and S. Zhong. Characterization of Signals from Multiscale Edges. IEEE Trans. on Pattern Analysis and 
Machine Intelligence, 14(7):710 732, 1992. [22] D. P. Mitchell. Generating Antialiased Images at Low 
Sampling Densities. In M. C. Stone, editor, Computer Graphics (SIGGRAPH 87 Proceedings),vol­ume 21, pages 
65 72, July 1987. [23] J. Painter and K. Sloan. Antialiased Ray Tracing by Adaptive Progressive Re­.nement. 
In J. Lane, editor, Computer Graphics (SIGGRAPH 89 Proceedings), volume 23, pages 281 288, July 1989. 
[24] F. Pighin, D. Lischinski, and D. Salesin. Progressive Previewing of Ray-Traced Images Using Image-Plane 
Discontinuity Meshing. Eurographics Workshop on Rendering 1997, May 1997. [25] H. Rushmeier and G. Ward. 
Energy Preserving Non-Linear Filters. In A. Glass­ner, editor, Computer Graphics Proceedings, Annual 
Conference Series, pages 131 138, July 1994. [26] P. Schroeder and P. Hanrahan. On the Form Factor between 
Two Polygons. Tech­nical Report CS-404-93, Princeton University, Computer Science Department, 1993. [27] 
P. Shirley. Hybrid Radiosity/Monte Carlo Methods. In Siggraph 94 Course on Advanced Radiosity, August 
1994. [28] A. J. Stewart and S. Ghali. Fast Computation of Shadow Boundaries Using Spa­tial Coherence 
and Backprojections. In A. Glassner, editor, Computer Graphics Proceedings, Annual Conference Series, 
pages 231 238, July 1994. [29] I. Sutherland, R. Sproull, and R. Schumacker. A Characterization of Ten 
Hidden-Surface Algorithms. ACM Computing Surveys, 6(1):387 441, March 1974. [30] S. Teller. Computing 
the Antipenumbra of an Area Light Source. In Computer Graphics (SIGGRAPH 92 Proceedings), volume 26, 
pages 139 148, July 1992. [31] S. Teller, K. Bala, and J. Dorsey. Conservative Radiance Interpolants 
for Ray Tracing. Eurographics Workshop on Rendering 1996, May 1996. [32] C. Vedel. Computing Illumination 
from Area Light Sources by Approximate Contour Integration. In Proceedings of Graphics Interface 93, 
Toronto, Canada, May 1993. [33] G. J. Ward. The RADIANCE Lighting Simulation and Rendering System. In 
A. Glassner, editor, Computer Graphics Proceedings, Annual Conference Series, pages 459 472, July 1994. 
[34] G. J. Ward, F. M. Rubinstein, and R. D. Clear. A Ray Tracing Solution for Diffuse Interre.ection. 
In Computer Graphics (SIGGRAPH 88 Proceedings), pages 85 92, August 1988. [35] T. Whitted. An Improved 
Illumination Model for Shaded Display. In Computer Graphics (SIGGRAPH 79 Proceedings), volume 13, pages 
1 14, August 1979. [36] M. Woo, J. Neider, and T. Davis. OpenGL Programming Guide. Addison Wesley Developers 
Press, 1996. [37] G. Wyvill, C. Jay, D. McRobie, and C. McNaughton. Pixel Independent Ray Tracing. In 
Computer Graphics: Developments in Virtual Environments (Proc. CG International 95), pages 43 55. Springer-Verlag, 
1995. [38] G. Wyvill and P. Sharp. Fast Antialiasing of Ray Traced Images. In New Ad­vances in Computer 
Graphics (Proc. CG International 95), pages 579 588. Springer-Verlag, 1989. [39] W. Zeng and B. Liu. 
Geometric-Structure-Based Error Concealment with Novel Applications in Block-Based Low Bit Rate Coding. 
IEEE Trans. Cir. and Sys. for Video Tech., to appear, 1998. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280889</article_id>
		<sort_key>267</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>26</seq_no>
		<title><![CDATA[Reproducing color images using custom inks]]></title>
		<page_from>267</page_from>
		<page_to>274</page_to>
		<doi_number>10.1145/280814.280889</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280889</url>
		<keywords>
			<kw><![CDATA[Kubelka-Munk model]]></kw>
			<kw><![CDATA[Neugebauer model]]></kw>
			<kw><![CDATA[color printing]]></kw>
			<kw><![CDATA[color reproduction]]></kw>
			<kw><![CDATA[gamut mapping]]></kw>
			<kw><![CDATA[ink selection]]></kw>
			<kw><![CDATA[separations]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.4</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>G.1.6</cat_node>
				<descriptor>Gradient methods</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.6</cat_node>
				<descriptor>Simulated annealing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003648.10003670.10003677.10003680</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic reasoning algorithms->Markov-chain Monte Carlo methods->Simulated annealing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003716</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Mathematical optimization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10003716</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Mathematical optimization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P78500</person_id>
				<author_profile_id><![CDATA[81100083423]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Stollnitz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Washington, Seattle]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P290853</person_id>
				<author_profile_id><![CDATA[81100237726]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Victor]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ostromoukhov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ecole polytechnique F&#233;d&#233;rale de Lausanne, Lausanne, Switzerland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P63622</person_id>
				<author_profile_id><![CDATA[81100188207]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Salesin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Washington, Seattle]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Harold Boll. A Color to Colorant Transformation for a Seven Ink Process. In Device-Independent Color Imaging, volume 2170 of Proceedings of the SPIE, pages 108-118, 1994.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[R J. Burt and E. H. Adelson. The Laplacian Pyramid as a Compact Image Code. IEEE Transactions on Communications, 31(4):532-540, April 1983.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lawrence Davis. Handbook of Genetic Algorithms. Van Nostrand Reinhold, New York, 1991.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[M.D. Fairchild and R. S. Berns. Image Color-Appearance Specification Through Extension of CIELAB. Color Research and Application, 18(3):178-190, June 1993.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[R. S. Gentile, E. Walowit, and J. R Allebach. A Comparison of Techniques for Color Gamut Mismatch Compensation. Journal of Imaging Technology, 16(5): 176-181, October 1990.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Arthur C. Hardy and F. L. Wurzburg, Jr. Color Correction in Color Printing. Journal of the Optical Society of America, 38(4):300-307, April 1948.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801294</ref_obj_id>
				<ref_obj_pid>800064</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Paul Heckbert. Color Image Quantization for Frame Buffer Display. In Proceedings of SIGGRAPH 82, pages 297-307, 1982.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[R. W. G. Hunt. Revised Colour-Appearance Model for Related and Unrelated Colours. Color Research and Application, 16(3):146-165, June 1991.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Kansei Iwata and Gabriel Marcu. Computer Simulation of Printed Colors on Textile Materials. In Color Hard Copy and Graphic Arts III, volume 2171 of Proceedings of the SPIE, pages 228-238, 1994.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Tony Johnson. A Complete Colour Reproduction Model for Graphic Arts. In Proceedings of the Technical Association of the Graphic Arts, pages 1061-1076, 1996.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[D.B. Judd and G. Wyszecki. Color in Business, Science, andlndustry. John Wiley and Sons, New York, 1975.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Henry R. Kang. Comparisons of Color Mixing Theories for Use in Electronic Printing. In Proceedings of the IS&amp;T/SID Color Imaging Conference: Transforms &amp; Transportability of Color, pages 78-82, 1993.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Gustav Kortfim. Reflectance Spectroscopy: Principles, Methods, Applications, chapter 4, pages 103-169. Springer, New York, 1969.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[R Laihanen. Colour Reproduction Theory Based on the Principles of Colour Science. In Proceedings of the International Association of Research Institutes for the Graphic Arts Industry, volume 19, pages 1-36, 1987.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74345</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Bruce J. Lindbloom. Accurate Color Reproduction for Computer Graphics Applications. In Proceedings of SIGGRAPH 89, pages 117- 126, 1989.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Yan Liu. Spectral Reflectance Modification of Neugebauer Equations. In Proceedings of the Technical Association of the Graphic Arts, pages 154-172, 1991.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Lindsay W. MacDonald. Gamut Mapping in Perceptual Color Space. In Proceedings of the IS&amp;T/SID Color Imaging Conference: Transforms &amp; Transportability of Color, pages 193-196, 1993.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Marc Mahy and Paul Delabastita. Inversion of the Neugebauer Equations. Color Research and Application, 21(6):401-411, December 1996.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Gabriel Marcu and Satoshi Abe. Color Designing and Simulation in Non-Conventional Printing Process. In Applications of Digital Image Processing XVII, volume 2298 of Proccedings of the SPIE, pages 216- 223, 1994.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>2319386</ref_obj_id>
				<ref_obj_pid>2318950</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Ethan D. Montag and Mark D. Fairchild. Psychophysical Evaluation of Gamut Mapping Techniques Using Simple Rendered Images and Artificial Gamut Boundaries. IEEE Transactions on Image Processing, 6(7):977-989, July 1997.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[H. E. J. Neugebauer. Die Theoretischen Grundlagen des Mehrfarbenbuchdrucks (The Theoretical Foundation for Multicolor Printing). Zeitschrift fuer Wissenschaftliche Photographie, 36(4):73-89, 1937. Reprinted in Neugebauer Memorial Seminar on Color Reproduction, volume 1184 of Proceedings of the SPIE, pages 194-202. SPIE, Bellingham, WA, 1990.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Victor Ostromoukhov. Chromaticity Gamut Enhancement by Heptatone Multi-color Printing. In Device-Independent Color Imaging and Imaging Systems Integration, volume 1909 of Proceedings of the SPIE, pages 139-151, 1993.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237261</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Joanna L. Power, Brad S. West, Eric J. Stollnitz, and David H. Salesin. Reproducing Color Images as Duotones. In Proceedings of SIG- GRAPH 96, pages 237-248, 1996.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[William H. Press, Brian R Flannery, Saul A. Teukolsky, and William T. Fetterling. Numerical Recipes. Cambridge University Press, New York, second edition, 1992.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[J. L. Saunderson. Calculation of the Color of Pigmented Plastics. Journal of the Optical Society of America, 32(12):727-736, December 1942.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>927384</ref_obj_id>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Eric J. Stollnitz. Reproducing Color Images with Custom Inks. Ph.D. thesis, University of Washington, 1998.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>48045</ref_obj_id>
				<ref_obj_pid>46165</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Maureen C. Stone, William B. Cowan, and John C. Beatty. Color Gamut Mapping and the Printing of Digital Color Images. ACM Transactions on Graphics, 7(4):249-292, October 1988.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Atsushi Takaghi, Toru Ozeki, Yoshinori Ogata, and Sachie Minato. Faithful Color Printing for Computer Generated Image Syntheses with Highly Saturated Component Inks. In Proceedings of the IS&amp;T/SID Color Imaging Conference: Color Science, Systems and Applications, pages 108-111, 1994.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[M. Wolski, J. R Allebach, and C. A. Bouman. Gamut Mapping: Squeezing the Most out of Your Color System. In Proceedings of the IS&amp;T/SID Color Imaging Conference: Color Science, Systems andApplications, pages 89-92, 1994.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Reproducing Color Images Using Custom Inks Copyright &#38;#169;1998 by the Association for Computing 
Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal or 
classroom use is granted without fee provided that copies are not made or distributed for profit or commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for components 
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy 
otherwise, to republish, to post on servers or to redistribute to lists, requires specific permission 
and/or a fee. Eric J. Stollnitz Victor Ostromoukhov. David H. Salesin University of Washington .Ecole 
Polytechnique F´ed´erale de Lausanne Abstract We investigate the general problem of reproducing color 
images on an offset press using custom inks in any combination and number. While this problem has been 
explored previously for the case of two inks, there are a number of new mathematical and algorithmic 
chal­lenges that arise as the number of inks increases. These challenges include more complex gamut mapping 
strategies, more ef.cient ink selection strategies, and fast and numerically accurate methods for computing 
ink separations in situations that may be either over-or under-constrained. In addition, the demands 
of high-quality color printing require an accurate physical model of the colors that result from overprinting 
multiple inks using halftoning, including the ef­fects of trapping, dot gain, and the interre.ection 
of light between ink layers. In this paper, we explore these issues related to printing with multiple 
custom inks, and address them with new algorithms and physical models. Finally, we present some printed 
examples demonstrating the promise of our methods. CR Categories: I.3.4 [Computer Graphics]: Graphics 
Utilities; G.1.6 [Nu­merical Analysis]: Optimization Additional Keywords: color reproduction, color printing, 
gamut mapping, ink selection, Kubelka-Munk model, Neugebauer model, separations Introduction It is of 
interest :::that, regardless of the number of impres­sions, the inks may be selected solely on the basis 
of their color gamut. Their colors need not be cyan, magenta, and yellow; nor is it required that they 
be transparent. The way is therefore opened for entirely new printing processes. Hardy and Wurzburg, 
1948 [6] Fifty years ago, the promise of color printing with custom inks ap­peared imminent. The advantages 
of such a process are clearly nu­merous. Freed from the same .xed set of process color inks cyan, magenta, 
yellow, and black it should be possible to print more vi­brant colors for art reproductions, annual reports, 
and packaging. Moreover, if the inks are chosen speci.cally for the particular im­age being reproduced, 
it should be possible in many cases to achieve these vivid colors with just a small number of inks perhaps 
four and perhaps at no greater cost than using the four process colors. In addition, it is common today 
to print boxes and wrappers with four process inks (for images) plus two spot colors for corporate logos 
or large areas of background. By selecting custom inks that comple­ment the required spot colors, we 
might achieve better quality with six inks or comparable quality with fewer inks. In recent years, several 
new color printing processes have been pro­posed that use a .xed set of six or more standard printing 
inks [1, 22, 28]. For those willing to use more inks, these new processes do provide more vivid color 
reproduction. However, Hardy and Wurzburg s .fty-year-old vision of printing with arbitrary custom inks 
remains elusive. Indeed, there are quite a few dif.cult prob­lems that stand in the way. For one, it 
is very dif.cult to derive a physical model that accurately predicts how arbitrary inks will interact 
when printed together, in su­perposition and in juxtaposition using halftoning. In addition to op­tical 
effects, the model must take into account physical effects such as trapping and dot gain. Furthermore, 
the gamuts produced by multiple custom inks have ir­regular, nonconvex shapes. Creating ef.cient, reliable 
gamut map­ping algorithms for smoothly mapping image colors to the colors that can be achieved with a 
given set of inks is a nontrivial problem. Choosing the best set of custom inks to use for a given image 
is an­other dif.cult problem in this case, a combinatorial challenge, par­ticularly as the number of 
inks used for printing gets large. Finally, computing ink separations becomes more dif.cult for mul­tiple 
inks. While for two inks there is always a simple analytic so­lution, for three or more inks the problem 
can become either over­or under-constrained. The problem becomes over-constrained when the color to be 
printed cannot be achieved with quantities of ink be­tween 0 and 100%. The problem is under-constrained 
when there are two or more ways of achieving the same color, using different ink combinations. This situation 
arises wherever the gamut is dou­bly covered, a commonplace occurrence with four or more inks. This paper 
addresses these challenges in detail with new physical models and algorithms, then demonstrates the potential 
of our ap­proach with printed examples. Although a great deal more work re­mains to be done before Hardy 
and Wurzburg s vision is achieved in its entirety, this paper at least takes some steps toward that goal. 
Related work Power et al. [23] showed that for duotone printing, in which just two inks are used, choosing 
the optimal inks for the particular image at hand can result in remarkably good reproductions. Our paper 
dis­cusses the many issues involved in generalizing their work to three or more custom inks what we refer 
to as n-tone printing. These issues can be broken down into a number of subproblems. First, for any given 
choice of paper and inks, we require a model of the gamut of printable colors. Many existing models have 
been developed for speci.c inks and printing processes; unfortunately, these models typically do not 
apply when printing with custom inks. More general models include the color halftoning model developed 
by Neugebauer [21], and colorant layering models such as the Beer-Bouguer, Kubelka-Munk, and Clapper-Yule 
equations (described by Kang [12]). Liu describes a model for process color printing similar to both 
the Kubelka-Munk layering model and the Neuge­bauer halftoning model [16]. We model the printing gamuts 
of cus­tom inks using a similar approach in Section 2, where we combine the Kubelka-Munk and Neugebauer 
equations while taking into ac­count the effects of dot gain and trapping. The second subproblem, that 
of mapping the original image colors into the gamut of available colors, has been addressed previously 
for monitors and various types of printers. Studies have shown that the least objectionable mappings 
are those that preserve hue at the expense of luminance and saturation [5, 20]. In accordance with these 
.ndings, most existing gamut mapping techniques maintain hue while compressing each color s luminance 
and saturation in one of two ways: either toward a gray of equal luminance, or toward a .xed gray of 
medium luminance [10, 14, 27, 29]. In Section 3, we develop a continuous family of gamut mappings .lling 
the gap be­tween these two predominant strategies. The problem of choosing inks has most often been framed 
as a search for one .xed palette that reproduces all images well. A number of multicolor printing approaches 
have been developed to achieve greater .delity than process inks: the PANTONE Hex­achrome system adds 
an orange and a green to the four process col­ors; Ostromoukhov [22] adds orange, green, and purple; 
Boll [1] adds red, green, and blue; and Takaghi et al. [28] mention a nine-ink process. Iwata and Marcu 
[9] touch on the subject of choosing the optimal printing order for a .xed set of inks printed on fabric. 
How­ever, in none of the previous literature (aside from the work done by Power et al. for duotones) 
are the inks chosen to be optimal for a given image. We discuss criteria and algorithms for choosing 
op­timal inks in Section 4. The multicolor printing processes mentioned above have corre­sponding algorithms 
for computing ink separations. A number of these methods assume the printing gamut is a convex union 
of tetra­hedra [9, 19, 22, 28], but many gamuts violate this assumption. Other separation algorithms, 
like those for process inks, are tailored for particular inks. Still others use Newton s method [15] 
or an analytic solution [18, 23] to invert the gamut model, but these ap­proaches do not generalize to 
more than three inks. In Section 5, we present a robust separation method for arbitrary inks, paying 
partic­ular attention to the dif.culty of obtaining smooth results. Each of the topics above is treated 
in more detail in the .rst au­thor s dissertation [26]. In addition to describing our models and algorithms 
in the main text of this article, we display our printed results and discuss our experimental procedures 
in a set of appen­dices (printed with custom inks and inserted into the proceedings).1 We conclude in 
Section 6 with a summary of our work and ways in which it can be extended.  Modeling printing gamuts 
In order to .nd the best combination of paper and inks from many possible choices, we need a mathematical 
model of the gamut of printable colors that results from any particular choice. Power et al. use the 
Neugebauer model of color halftoning in concert with a sim­ple ink layering model to predict duotone 
gamuts. The accuracy of the gamut model is not crucial for duotone printing, since the user cannot expect 
a perfect reproduction from only two inks. By con­trast, users can be expected to be much more critical 
when print­ing with three or more inks, and therefore n-tone printing requires a much more accurate gamut 
model. We develop a model below that extends the Neugebauer model of color halftoning to account for 
the fact that an ink does not always adhere to paper and to other inks. This gamut model further requires 
that we know the colors achieved by overprinting combinations of inks. As we cannot always measure these 
overprinted colors, we rely on a mathematical model of layered media in addition to the Neugebauer model. 
We postpone until an appendix a discussion of the experimental procedure we followed to .t the model 
s parame­ters to measured data. 1The appendices are not included with the CD-ROM version of this paper. 
2.1 Modeling color halftoning Most models of color halftone printing used today are based on the equations 
published by Neugebauer in 1937 [21]. His model as­sumes that small dots of color are printed in such 
a way that their edges are sharply de.ned, their overlapping areas are distributed randomly, and within 
each overlapping area each ink is either com­pletely present or completely absent. His model also assumes 
we know the colors of the printing primaries: the paper color, the color of each ink printed on paper 
alone, and the color of each overprinted combination of inks. Under these conditions, the Neugebauer 
equa­tions state that the overall color of a small area is simply the area­weighted average of the colors 
of the printing primaries. Neugebauer s model is easily generalized from its original three­color formulation 
to incorporate any number of inks. For n inks, there are 2n printing primaries (since each ink is either 
present or absent in a primary). The colors of the primaries are typically rep­resented using coordinates 
in the XYZ color space, though the model applies equally well to re.ectance spectra or any linear transforma­tion 
of XYZ coordinates. With a slight modi.cation of the notation used by Power et al. [23], we will refer 
to the color of paper as gp, the color of paper covered by the .rst ink as gp1, the color of paper printed 
with the .rst and second inks as gp12, and so on. The frac­tion of area in which ink i is actually printed 
is denoted by ai,and we write these ink amounts collectively as o=(a1, :::, an). For a given set of printing 
primaries, the Neugebauer equations give a printable color c as a function of the ink amounts o. For 
example, three-ink printing yields eight printing primaries, and the Neuge­bauer model is written as 
follows: c(o)= (1 ,a1)(1 ,a2)(1 ,a3) gp +(a1)(1 ,a2)(1 ,a3) g p1 +(1 ,a1) (a2)(1 ,a3) gp2 +(a1) (a2)(1 
,a3) gp12 +(1 ,a1)(1 ,a2) (a3) gp3 +(a1)(1 ,a2) (a3) gp13 +(1 ,a1) (a2) (a3) gp23 +(a1) (a2) (a3) gp123 
 2.2 Adding trapping to the Neugebauer model Implicit in the Neugebauer model is the assumption that 
if we intend to cover a fraction ai of an area with ink i, we can actually achieve that fractional coverage. 
In reality, because of the physical proper­ties of inks and papers, some of the ink on the printing plate 
may not stick to the printed page. The portion of ink that does stick is said to be trapped by the paper. 
We will denote bytp1 the fraction of ink 1 that sticks to paper, and by tpij the fraction of ink j that 
sticks to ink i (the trapping fraction for ink j on ink i on paper). With this convention, we can model 
the color c we get by trying to cover a fraction a1 of the paper with ink 1: c(o)= (1 ,tp1a1) gp + tp1a1 
gp1 (1) Now suppose we print a second ink on top of that result. Of the area that was the color of paper 
gp, a fraction tp2a2 will get covered by ink2and become gp2, while the rest will stay the same. Likewise, 
in the area that was colored gp1, a fraction tp12a2 will be overprinted with ink 2 and become gp12, while 
the rest will stay the same. Thus, the result is a weighted average of four colors (as in the Neugebauer 
model): c(o)=(1 ,tp1a1)(1 ,tp2a2) gp +(tp1a1)(1 ,tp12a2) g p1 (2) +(1 ,tp1a1) (tp2a2) g p2 +(tp1a1) (tp12a2) 
gp12 If we add a third ink, the result will be a weighted average of eight colors, and we need seven 
trapping fractions: c(o)=(1 ,tp1a1)(1 ,tp2a2)(1 ,tp3a3) gp +(tp1a1)(1 ,tp12a2)(1 ,tp13a3) g p1 +(1 ,tp1a1) 
(tp2a2)(1 ,tp23a3) g p2 +(tp1a1) (tp12a2)(1 ,tp123a3) gp12 (3)+(1 ,tp1a1)(1 ,tp2a2) (tp3a3) gp3 +(tp1a1)(1 
,tp12a2) (tp13a3) gp13 +(1 ,tp1a1) (tp2a2) (tp23a3) gp23 +(tp1a1) (tp12a2) (tp123a3) gp123 In general, 
for n inks the resulting color will be a weighted average of the2n printing primaries, where the weights 
depend on 2n ,1 trapping fractions.  2.3 Adding dot gain to the Neugebauer model In addition to the 
effects of trapping, offset printing is subject to dot gain. The halftoned dots of an ink appear larger 
than they should for two reasons: ink spreads out on the paper (physical dot gain), and some of the light 
entering the paper is scattered until it emerges through dots of ink (optical dot gain). We can account 
for both va­rieties of dot gain using an empirical model that corrects the value of ai for each ink. 
When we produce a halftone separation that spec­i.es a coverage ¯ai, we .nd that one minus the actual 
coverage in the printed result is approximated very closely by a power law: Si 1 ,ai =(1 ,a¯i)(4) The 
parameter Ii associated with ink i can be determined from experimental data using standard curve-.tting 
techniques, as dis­cussed in an appendix. Note that if we desire an actual coverage of ai, we can always 
solve the equation above for a¯i, the coverage we should specify.  2.4 Modeling the printing primaries 
We have so far assumed that we know the colors of the printing pri­maries. While it is straightforward 
to measure these colors for a small set of inks (like the process inks) on a small set of papers, it 
is impractical to do so for all the combinations that could be chosen from large sets of inks and papers. 
If we want to print on a new pa­per without measuring all our inks on that paper, we need a model capable 
of predicting the primaries. There are many levels of com­plexity we can introduce into a model; we will 
start with a simple model and progress to more complicated ones. If we assume that a layer of ink acts 
as an ideal .lter, we need to know only how much light it transmits at each wavelength A.We will write 
the transmittance of the ink as Ti, and the re.ectance of paper as Rp (for some wavelength A). The re.ectance 
of ink on pa­per is given by the amount of light that penetrates the ink (Ti), re­.ects off the paper 
(Rp), and emerges through the ink again (Ti): Rpi = Ti 2Rp (5) We can measure Rpi and Rp using a spectrophotometer, 
but not Ti because it is a property of the ink layer without paper. However, we can characterize an ink 
by printing an identical layer of that ink on a variety of papers, measuring Rp and Rpi for each paper, 
then .tting Ti to the model. Unfortunately, a single transmittance spectrum may not be enough information 
to accurately model an ink on paper, let alone one ink atop another. One problem with the simple model 
above is that inks re.ect some light in addition to absorbing and transmitting light. If we introduce 
a re.ectance Ri for the ink, we have Rpi = Ri + Ti 2Rp (6) Figure 1 The light re.ected by a single layer 
of ink on paper. Once again, we can measure Rp and Rpi for a single ink on a variety of papers, and .t 
Ri and Ti to the model. Equation (6) is subsumed by a more general model, known as the Kubelka-Munk model 
(described by Judd and Wyszecki [11, pages 420 438] and by Kort¨um [13], among others). As Figure 1 illus­trates, 
light can re.ect any number of times between the ink and pa­per before .nally exiting the ink layer, 
making the re.ectance of ink on paper an in.nite sum of terms: T2 i Rp Rpi = Ri + Ti 2Rp(1 + R iRp + 
R 2 iR2 p +)= Ri + (7)1 ,R iRp The re.ectance of the back side of the ink layer, R i, can differ from 
the re.ectance of its front side because the layer may be inhomo­geneous. Now we must .t three re.ectance 
spectra (Ri, R i,and Ti) to measured data in order to characterize an ink. If R i is identically zero, 
we are left with equation (6); if Ri is also zero, we are left with equation (5). Note that we can modify 
equation (7) to predict the re.ectance of one ink on another ink (on paper), assuming the top ink layer 
be­haves the same as it would on paper. If we print on paper p using ink i followed by ink j, we can 
compute Rpij from quantities we have measured or .t to measurements: T2 j Rpi Rpij = Rj + (8) 1 ,R jRpi 
The Kubelka-Munk model can be derived from physical principles, but only under certain assumptions. One 
assumption is that all the layers have the same index of refraction. However, the index of re­fraction 
of a colorant layer is typically between 1.45 and 1.6 [11, page 398], while that of air is very nearly 
1. As a result of the dif­ference in indices, some of the incident light will undergo Fresnel re.ection 
at the material interface. We can correct for Fresnel re.ection at the boundary between ink and air using 
a construction similar to that of the Kubelka-Munk model. Suppose pai is the fraction of diffuse light 
traveling from air to ink that is re.ected by the surface of the ink layer, and pia is the surface re.ectance 
for light going from ink to air. Then, according to Saunderson [25], we .nd the corrected re.ectance 
R0 pi of a layer of ink on paper by modifying the prediction given by equation (5), (6), or (7) as follows: 
Rpi R0 pi = pai +(1 ,pai)(1 ,pia) (9)1 ,piaRpi Assuming dried ink has an index of refraction of 1.5, 
pai is approx­imately 0.1 and pia is about 0.6 for all wavelengths [11, page 417]. Fresnel re.ection 
may also occur at the boundary between ink and paper if the interface is planar. In this case, we adjust 
our earlier equations using surface re.ection coef.cients pip (for light going white yellow orange yellow 
orange purple purple green green black Figure 2 Front and top views of a four-ink gamut. from ink to 
paper) and ppi (for the opposite direction). We simply re­place Rp in equations (5), (6), or (7) with 
a corrected re.ectance R0 p: RpR0 p = pip +(1 ,pip)(1 ,ppi) (10) 1 ,ppiRp Uncoated papers consist largely 
of air, so their indices of refraction are close to 1 and we can use pip =0.6 and ppi = 0.1. For lack 
of better knowledge, we use the same numbers for coated papers; how­ever, these papers and modern plastic 
substrates deserve more study. Note that in the previous two equations Rp and R0 pi are measurable quantities, 
while Ri, R i,and Ti are characteristics of the ink that we need to derive from other measurements. Regardless 
of the level of complexity we choose for our ink layering model equation (5), (6), or (7), with or without 
the corrections in equations (9) and (10) we often need to convert re.ectance spectra into tristimulus 
XYZ colors for use in the color halftoning model of Section 2.2. Judd and Wyszecki describe this conversion 
in detail and provide the necessary data for standard illuminants and the XYZ matching functions [11, 
pages 125 153 and 472 479].  3 Gamut mapping Suppose we want to reproduce an image using a particular 
combi­nation of paper and inks. We can use the model presented in the previous section to predict the 
gamut of all printable colors associ­ated with this choice of paper and inks. In most cases, there will 
be a number of image colors that are outside the gamut of printable colors. We therefore need to de.ne 
a gamut mapping function that associates a printable color with each of the original image colors without 
introducing unnecessary color distortion into the image s appearance. According to a number of articles 
that address gamut mapping, it is most important to maintain the hue of a color, while allowing its lightness 
and saturation to change in order to .t within the printing gamut [5, 10, 14, 20, 27, 29]. In the sections 
that follow, we devise a hue-preserving gamut mapping strategy that is more general than existing ones, 
then present its coordinate system and algorithmic de­tails. We conclude our discussion of gamut mapping 
with some re­marks on the special steps we need to take when printing with only one or two inks. 3.1 
Strategy of n-tone mapping In general, the gamut of three or more inks occupies a volume in color space. 
A typical example is shown in Figure 2, illustrating the fact that printing gamuts can take on unusual 
nonconvex shapes. Because an n-tone gamut occupies a volume, we can attempt to pre­serve hues (though 
for some choices of inks, not all hues may be printable). Among the hue-preserving gamut mapping algorithms 
in the literature, there are two predominant strategies for altering lumi­nance and saturation: the .rst 
reduces saturation, leaving luminance .xed [14, 17, 27]; the second simultaneously alters luminance and 
saturation toward the central gray of the gamut [14, 17]. Because the .rst approach maps colors into 
the printing gamut by re­ducing their radial distances from a central gray axis, we refer to it as a 
cylindrical mapping. Likewise, because the second approach reduces each color s distance from a single 
central gray point, we re­fer to it as a spherical mapping. Laihanen notes that depending on the image 
being reproduced, one may be preferable to the other [14]. The cylindrical mapping has the advantage 
of preserving luminance relationships, but it tends to desaturate brightly colored highlights until they 
become white. The spherical mapping keeps the high­lights more saturated, but reduces their luminance 
at the same time, resulting in a reordering of brightnesses in the image. In order to obtain some of 
the advantages of both the cylindrical mapping and the spherical mapping, we developed a parameterized 
family of intermediate mappings. While we could simply interpo­late between the color given by the cylindrical 
mapping and the color given by the spherical mapping, there would be no guarantee that the result would 
lie in the printing gamut (because gamuts are not necessarily convex). Instead, we vary the locus of 
colors that serve as the centers of projection in the mapping: the cylindrical mapping moves colors toward 
a .xed line segment along rays or­thogonal to a cylinder; the spherical mapping moves colors toward a 
single point along rays orthogonal to a sphere; our new mapping moves colors toward a line segment whose 
length is parameterized, along rays orthogonal to an ellipsoid. Figure 3 illustrates the di­rections 
in which colors are compressed by each type of mapping. These directions are made explicit in the following 
section.  3.2 Coordinate system of n-tone mapping The implementation of our n-tone gamut mapping makes 
use of a special-purpose coordinate system that varies according to the pa­rameter k. The coordinate 
system yields a cylindrical mapping when k = 0, a spherical mapping when k = 1, and an ellipsoidal map­ping 
for intermediate values. Transforming an XYZ color into this coordinate system takes place in two stages. 
The .rst is a linear transform that rewrites the color as a triple (u, v, y), where y repre­sents luminance 
and u and v hold the chrominance information. This linear transform shears the dark-to-light axis of 
the printing gamut (while preserving luminance) until it parallels the luminance direc­tion, then applies 
a uniform scaling and translation that brings the darkest point of the printing gamut to (0, 0, 1) and 
the lightest to (0, 0, 1). Our linear transform is similar to that of Stone et al. [27, Section 5.2], 
but we have replaced their rotation with a shear in order to preserve luminance relationships throughout 
the gamut mapping process. The second stage of the transformation converts (u, v, y) to curvilin­ear 
coordinates (r, h, q), where h represents hue and r and qindi­rectly encode luminance and saturation. 
These new coordinates are found by inverting the following equations: u = r cos h cos q v = r sin h cos 
q k2 y =(1 ,+ kr)sin q In this coordinate system, lines of constant h and qtrace out the nor­mals to 
the ellipsoid (u2+ v2)/k2+ y2 = 1; these are the lines along which we compress out-of-gamut colors. Note 
that when k =1, the transformation above gives the standard conversion between spher­ical and Cartesian 
coordinates, which is easily inverted. Likewise, when k = 0 the equations are only a slight modi.cation 
of standard cylindrical coordinates, and are also easily inverted. However, for intermediate values of 
k, we lack an analytic solution and therefore resort to Newton s method to solve a nonlinear equation 
for q. y y y f f f r r r h h h u vu vu v (a) (b) (c) Figure 3 The coordinate systems used by three 
gamut mapping methods chosen from a continuum: (a) cylindrical (k = 0), (b) ellipsoidal (k = 0.3), and 
(c) spherical (k = 1). Arrows on the cutaway surfaces indicate the directions in which colors are compressed. 
 3.3 Steps in n-tone mapping The .rst step in our gamut mapping technique is to apply a global mapping 
to the luminance values of the source image s colors. As noted by Power et al. [23], we can use any monotonically 
increas­ing function to compress the image s luminance range into the range of printable luminance values. 
Experiments in the literature typi­cally rely on clamped or linear mappings [5, 20], but we often use 
a cubic mapping that has the advantages of both (see Figure 4). This mapping is inspired by the B´ezier-curve 
mapping described by Power et al., but is more easily constructed. We determine a unique cubic function 
by constraining the minimum and maximum input values to map to the minimum and maximum output values, 
while choosing for each endpoint a slope between 0 and 1 that yields an in­creasing function as close 
as possible to the identity function (Stoll­nitz [26] provides further details). For most values of k 
we can skip this .rst step because subsequent steps will also adjust luminance; it is only when k = 0 
that we must compress luminance in order for a cylindrical mapping to get all colors into the printing 
gamut. The second step is to divide the set of directions that are parameter­ized by h and qinto a two-dimensional 
array of bins, as indicated by the grid lines in Figure 3. The number of divisions in each direc­tion 
determines the storage, ef.ciency, and accuracy of subsequent mapping steps; numbers near 20 are adequate 
for quick previews, while numbers near 100 are more suitable for high-quality output. Next, we determine 
the maximum extent in the r direction of the printing gamut within each (h, q) bin. For each bin, we 
construct a ray centered within that bin, and intersect it with each of the bilin­ear surfaces that bound 
the gamut model described in Section 2.2. We associate with the bin the largest r value encountered in 
these intersection tests, which we call ¯rprint. We also store with each bin a quantity called ¯rimage, 
de.ned as the largest r value of all image colors lying in that bin. If the printing output output output 
input input input (a) (b) (c) Figure 4 Monotonically increasing mappings: (a) Clamped map­ping preserves 
exact values, except at the endpoints. (b) Linear map­ping preserves relationships. (c) Cubic mapping 
provides a good compromise between the two. gamut exceeds the image gamut for some bin, we set ¯rimage 
equal to ¯rprint, so that gamut mapping will not spread similar colors apart. The .nal step is to apply 
a mapping to the r value of each image color, where the mapping varies from one bin to the next. Given 
an image color, we determine the four bins closest to the (h, q)co­ordinates of that color, and apply 
bilinear interpolation to the cor­responding values of ¯rimage and ¯rprint. We construct a function that 
maps zero to zero and the interpolated value of ¯rimage to the interpo­lated value of ¯rprint. The intermediate 
values of r can be computed using a clamped, linear, or cubic mapping, as shown in Figure 4. Once all 
the image colors have been mapped, we can convert them from (r, h, q) coordinates back to XYZ coordinates, 
and they should all lie within the printing gamut.  3.4 Monotone and duotone mappings The gamut mapping 
steps described above (and many other color gamut mapping techniques) rely on assumptions about the shape 
of the printing gamut that do not always hold. In particular, we as­sumed that the gamut consists of 
a volume of colors that includes the line segment connecting the darkest and lightest printable col­ors. 
If we are printing with one ink, however, the gamut is a line segment of colors rather than a volume. 
With two inks, the gamut is a surface rather than a volume. We treat one-ink (monotone) and two-ink (duotone) 
printing as special cases. The gamut of colors that can be printed with one ink is given by equation 
(1); it consists of a line segment parameterized by a1.Ac­cordingtoStone et al. [27], the most important 
quality to preserve (aside from the gray axis, which we cannot keep gray) is maximum luminance contrast. 
Therefore, we .rst remap the input image s luminance values to lie within the luminance range of the 
print­inggamut. As inthe n-tone mapping, we can use any one of the clamped, linear, or cubic mappings 
illustrated in Figure 4. Once we .nish this remapping, we can safely project each image color onto the 
printing gamut while preserving luminance relationships. The gamut produced by two inks is a bilinear 
surface given by equa­tion (2). Once again, because the gamut has lower dimensionality than the space 
of colors, we must resort to projection within any gamut mapping. The core of the work presented by Power 
et al. [23] is a method of mapping image colors to a duotone gamut while pre­serving as much color information 
as possible. Their algorithm .rst remaps the input image s luminance values as described above, then 
remaps a second component (s for spread ) of the image colors that is orthogonal to luminance. This second 
remapping is a function of luminance, so that the most compression is applied to the darkest and lightest 
colors, where the printing gamut is the most narrow (and the least compression to the mid-luminance colors, 
where the gamut is widest). Finally, each color is projected onto the printing gamut in the direction 
orthogonal to the .rst two mappings. We offer two minor improvements to the method described by Power 
et al. First, we avoid discontinuities in our duotone gamut mapping by applying piecewise-linear (rather 
than piecewise­constant) interpolation to the bin values approximating the gamut s extent in the s direction. 
Second, we offer a choice of luminance mappings, including the clamped and cubic mappings shown in Fig­ure 
4 in addition to the linear mapping used by Power et al.  4 Selecting inks Throughout Sections 2 and 
3 we treated the paper and inks as though they were known. Our goal, however, is to .nd the optimal com­bination 
of paper and inks for a given image. This task is dif.cult because there may be a huge number of possible 
choices, most of which will result in poor reproductions. Often it is not obvious even to an experienced 
user whether or not a choice of paper and inks will reproduce an image well; therefore, we are not yet 
willing to rely on heuristic rules for accepting or rejecting combinations. Instead, we pose the problem 
of selecting paper and inks as a combinatorial op­timization problem and apply a general-purpose algorithm 
to solve it, just as Power et al. [23] did for duotones. We describe below our objective function and 
optimization algorithm for choosing inks. 4.1 Ink-selection objective function Our objective function 
for ink selection is very similar to the one de­scribedbyPower et al. Given an image and a combination 
of paper and inks, we apply the gamut mapping algorithm discussed in Sec­tion 3 to obtain a preview image. 
Then we compare the preview im­ .b. age pixel-by-pixel to the original image, using the L2 norm in L.acolor 
space. The value we assign to our objective function is the average over all pixels of the L2 distance 
between the preview and original image. The ef.ciency of the objective function is an important concern 
be­cause it gets evaluated often. Fortunately, we don t need to apply our gamut mapping algorithm to 
each of the tens or hundreds of thousands of distinct colors contained in a typical high-resolution scan. 
Instead, we can use Heckbert s median-cut algorithm [7] to quantize the image to about 2,000 distinct 
colors while maintaining its general appearance. Then we need only apply the gamut map­ping algorithm 
to these quantized colors, and weight each color s L.a.b.distance according to the number of pixels of 
that color in the original image. 4.2 Ink-selection algorithm Power et al. chose a simulated annealing 
algorithm for their com­binatorial optimizer because of its ability to .nd global minima and relatively 
good local minima. We prefer to use a genetic algorithm, mainly because it maintains a population of 
candidate solutions, each of which can be presented to the user as a possibility when the optimizer is 
.nished. Simulated annealing and the genetic al­gorithm are both stochastic optimization techniques, 
making use of random changes to intermediate solutions in order to avoid local minima. In fact, Power 
et al. made their simulated annealer slightly closer to a genetic algorithm by using multiple initial 
conditions to obtain a variety of solutions; likewise, we made our genetic algo­rithm slightly closer 
to simulated annealing by assigning a time­varying probability distribution to each of the genetic operators. 
Following suggestions made by Davis [3], we include in our imple­mentation a number of variations on 
the standard genetic algo­rithm. We maintain a population of candidate combinations, each of which is 
distinct from the others, and we keep them sorted ac­cording to their respective objective function values. 
At each itera­tion, we choose a genetic operator according to a time-varying prob­ability distribution, 
and we choose its operands randomly from the current population. The operators for our application include 
global mutation, local mutation, and crossover. Global mutation operates on one combination, changing 
the paper and inks completely ran­domly; this operator is more likely to be chosen in early iterations. 
Local mutation also operates on one combination, but changes the paper and inks only to nearby colors; 
this operator is more likely in later iterations. Crossover takes two combinations and exchanges each 
of the papers and inks with 50% probability. The likelihood of choosing crossover starts out high, and 
gradually declines to zero, so as to prevent one solution from dominating the population. Every time 
an operator produces a previously untested combination of paper and inks, we evaluate the objective function 
and compare the result to the current members of the population. If the new com­bination outperforms 
the worst member of the population, we re­place the old one with the new one. Thus, each iteration maintains 
or improves the population. We repeat the process for a .xed num­ber of iterations, or until a .xed number 
of iterations fails to yield a decrease in the objective function. The results are presented to the user 
as a list of possible paper and ink combinations, sorted from best to worst, from which he or she can 
choose any one to preview.  5 Computing separations Once the paper and inks have been selected and 
a preview has been computed using our gamut mapping algorithm, our only remaining task is to produce 
separations for each of the inks. More precisely, for each color generated by the gamut mapping algorithm, 
we need to .nd the amount of each ink required to reproduce that color. The mathematical model of printing 
gamuts that we developed in Sec­tion 2 gives tristimulus colors as a function of the ink amounts ai, 
whereas now we want to .nd the ink amounts as a function of color. Determining this inverse function 
is no simple matter, mainly be­cause each tristimulus component of our model is a nonlinear func­tion 
of the ovalues. In addition, the inverse is underdetermined when there are more than three inks; there 
may be many ovalues that yield the same color. There is another dif.culty that arises in practice: some 
of the colors for which we wish to compute separa­tions may be slightly out of gamut because of imprecisions 
in the gamut mapping stage, yet we still need to .nd ovalues between zero and one that reproduce similar 
colors. For more than three inks, we cannot invert the gamut model ana­lytically as Power et al. [23] 
did for two inks, and Mahy and De­labastita [18] did for three. We also cannot treat the printing gamut 
as a convex union of tetrahedra, as some authors have [9, 19, 22, 28]; many gamuts are actually concave. 
Instead, we rely on a continu­ous optimization technique to .nd the separations that most closely reproduce 
a desired color while meeting the physical constraints of the printing process. The details of our objective 
function and opti­mization algorithm are given below. 5.1 Separation objective function The goal of 
the current optimization is to .nd the ink amounts o= (a1, :::, an) for which our gamut model yields 
the color c(o)clos­est to a given tristimulus color ¯c. We formulate the objective func­tion as the sum 
of four terms: f (o;¯c, oref)= w1 kc(o) ,c¯k2 P + w2 i max(0, ,ai, ai ,1)2 ,P)2 + w3 max0, i ai ,alimit 
 + w4 ko,orefk2  The .rst term of the objective function is just the square of the dis­tance in XYZ 
color space between the desired color ¯c and the color our gamut model predicts from the ovalues. For 
an in-gamut color, the optimizer should .nd a solution where this term is zero. For an out-of-gamut color, 
minimizing the .rst term is akin to projecting onto the nearest point of the gamut, as was done by Stone 
et al. be­fore computing separations [27]. We arbitrarily choose the weight w1 = 0.005, and set the remaining 
weights by trial and error based on the magnitudes of the terms. The second and third terms introduce 
penalties for violating con­straints inherent in the printing process. The second term bounds each of 
the ink amounts between 0 and 1, thereby moving out-of­gamut colors to in-gamut separations. The third 
term ensures that the total amount of ink does not go over the ink limit, the point at which ink no longer 
adheres to the page. The ink limit depends upon the press and the paper; we use alimit = 3.7. We typically 
set w2= w3 = 1000 so that ink amounts violating these constraints are strongly penalized. The .nal term 
of the objective function allows us to achieve a unique solution when there are multiple ways to produce 
the same color. We do so by .nding the solution that is closest to a given set of ref­erence values oref. 
We use a relatively small weight for this term (w4 = 0.025) so that it does not prevent the color c(o) 
from match­ing ¯c. We can set each component of oref to 0 or 1 to minimize or maximize the amount of 
ink used, or choose 0.5 for an intermediate solution. Unfortunately, we .nd that these simple choices 
of reference values often lead to separations containing arti.cial discontinuities. Be­cause the ink 
amounts used for adjacent pixels are computed inde­pendently, slightly different colors may result in 
very different sep­arations. While in theory these separations will produce similar col­ors when printed, 
in reality even the slightest misregistration reveals the discontinuities. To avoid such artifacts, we 
try to compute sepa­rations that are just as smooth as the input image. One possible solu­tion is to 
set oref to the ink amounts of the most similar color among the four adjacent pixels that have already 
been separated. This ap­proach eliminates many artifacts, but because of the asymmetry in­herent in processing 
pixels from left to right and top to bottom, it may still produce discontinuities in some directions. 
We can generate much better separations using a multiresolution al­gorithm based on image pyramids. The 
central idea is to compute for each pixel the ink amounts that produce the right color and are as close 
as possible to the ink amounts of the entire surrounding neigh­borhood. Of course, the ink amounts that 
best reproduce a pixel s neighborhood depend in turn on a larger neighborhood, and thus we rely on a 
recursively de.ned image pyramid. First, we construct a pyramid of reduced images from the gamut-mapped 
source image by repeatedly applying a low-pass .lter followed by downsampling. We use a separable low-pass 
.lter with coef.cients 161 (1,4,6,4, 1); see Burt and Adelson [2] for further details on image pyramids. 
Next, we compute separations for the lowest-resolution image (with each component of oref set to 0.5). 
Then we calculate new separa­tions for each higher resolution image, using as the reference val­ues oref 
an enlarged version of the current separations (where en­largement consists of upsampling followed by 
low-pass .ltering). The result is a set of separations at the highest resolution that main­tain the smoothness 
of the original color image. Smoothness comes at the cost of computing separations for the entire image 
pyramid, but this is only 4/3 the work of computing separations just at the highest level.  5.2 Separation 
algorithm There are a plethora of continuous optimization algorithms we could apply to the separation 
problem. We can choose among them by considering the amount of information they require and their rates 
of convergence. Because we can ef.ciently compute the .rst par­tial derivatives of all the terms in our 
objective function, we can use optimization techniques that achieve quadratic convergence rates. These 
include the conjugate gradient method and quasi-Newton methods, among others. We found the BFGS quasi-Newton 
method to be the most ef.cient for our problem (even when compared to Newton s method, which uses costly 
second derivative information as well). Detailed descriptions of these algorithms are given by Press 
et al. [24, pages 420 430].  6 Conclusion In this paper, we have laid out a general framework for multi­color 
printing with custom inks, and described algorithms that show promise for solving longstanding problems 
in color printing. Our gamut model combines previous ink layering and halftoning mod­els with modi.cations 
for trapping and dot gain. We introduced a new ellipsoidal gamut mapping that effectively .lls a gap 
between the existing cylindrical and spherical variations of gamut mapping. We described the operators 
needed to adapt a genetic algorithm to the selection of papers and inks. Finally, we developed a robust 
multiresolution algorithm that, given any combination of inks, com­putes separations that are as smooth 
as the input image. The appendices present practical results of our experiments in the realm of custom-ink 
color image reproduction. We outline there the steps required to .t the parameters of our gamut model 
to measured data, and exhibit a variety of printed images produced with our tech­niques. While we still 
see room for improvement in these results, they demonstrate the potential for making color reproductions 
with custom inks that are more accurate or less costly to produce than with standard process inks. In 
general, n-tone printing offers the opportunity to match monitor colors better than process color printing 
because the inks are cho­sen speci.cally for the image. Moreover, because we construct a gamut mapping 
that is customized for the image at hand, we can achieve a much more accurate reproduction than is possible 
with a gamut mapping designed to bring all monitor colors into the same printing gamut. As a case in 
point, our gamut mapping will not alter an image whose colors all happen to fall within the printing 
gamut, while many other algorithms will shift the image colors merely be­cause some colors in the monitor 
gamut (but not in the image) are not printable. In the near future, we hope to eliminate some of the 
remaining arti­facts from our results. In particular, we would like to eliminate the extreme desaturation 
of out-of-gamut hues by introducing a selec­tive hue compression method. By using stochastic screening 
for our future printing experiments, we will be able to avoid the moir´einter­ference patterns present 
in traditional halftones when four or more inks are assigned different screen angles. We are also interested 
in the effects achievable by measuring and printing opaque inks on dark papers. There are a number of 
other ways in which this work can be ex­tended. We could model metallic inks by including angular varia­tion 
in the ink layering model, or capture the behavior of .uorescent inks and papers by treating re.ectance 
and transmittance as func­tions of both incoming and outgoing wavelength. By substituting a model of 
the inks and halftoning process of ink-jet printers for our current gamut model, we could suggest custom 
ink choices and compute separations for these widely available devices (assuming cartridges of custom 
inks were available). Our gamut mapping algorithm might be improved by performing .b. the mapping in 
a perceptually uniform color space like L.aor L.u.v., as recommended by MacDonald [17] and Wolski et 
al. [29]. Montag and Fairchild [20] suggest using different gamut mapping strategies for light colors 
and dark colors. We are considering a variety of changes to the way in which the op­timizer chooses inks. 
The user could indicate to the optimizer which colors are most important by painting a weighting function 
over the .b. original image; these weights would multiply each pixel s L.a distance in the objective 
function. We could also use the RLab color .b. space [4] or Hunt s color-appearance space [8] instead 
of L.ato obtain a more accurate estimate of color differences. With more terms in the objective function, 
we could try to minimize the cost of the materials or their environmental impact (favoring recycled pa­pers 
and soy inks), maximize the longevity of the reproduction (fa­voring acid-free papers and fade-resistant 
inks), or reduce the im­pact of misregistration artifacts by favoring inks similar in color to the image 
subject matter. As a more general extension, we might optimize not only the paper and inks, but also 
the gamut mapping parameters and even the choice of which images to print. As mentioned earlier, well-chosen 
heuristics may help to speed up the selection of inks by eliminating poor combinations before any time 
is spent evaluating them. We hope to accelerate the separation algorithm as well, perhaps by exploiting 
coherence of image colors in color space, or using a local approximation to our gamut model that is more 
easily inverted.  Acknowledgments We are grateful to Safeco Insurance s Graphics &#38; Printing Services 
for their donation of time and resources. Thanks to Roger Hersch, Pat Lewis, Fr´ed´eric Pighin, and Joanna 
Power for helpful discus­sions. This work was supported by a grant from the Washington Technology Center 
and Numinous Technologies, an NSF Presiden­tial Faculty Fellow award (CCR-9553199), an ONR Young Inves­tigator 
award (N00014-95-1-0728), and industrial gifts from Mi­crosoft and Pixar.  References [1] Harold Boll. 
A Color to Colorant Transformation for a Seven Ink Pro­cess. In Device-Independent Color Imaging, volume 
2170 of Proceed­ings of the SPIE, pages 108 118, 1994. [2] P. J. Burt and E. H. Adelson. The Laplacian 
Pyramid as a Compact Image Code. IEEE Transactions on Communications, 31(4):532 540, April 1983. [3] 
Lawrence Davis. Handbook of Genetic Algorithms.Van Nostrand Reinhold, New York, 1991. [4] M. D. Fairchild 
and R. S. Berns. Image Color-Appearance Speci.ca­tion Through Extension of CIELAB. Color Research and 
Application, 18(3):178 190, June 1993. [5] R. S. Gentile, E. Walowit, and J. P. Allebach. A Comparison 
of Tech­niques for Color Gamut Mismatch Compensation. Journal of Imaging Technology, 16(5):176 181, October 
1990. [6] Arthur C. Hardy and F. L. Wurzburg, Jr. Color Correction in Color Printing. Journal of the 
Optical Society of America, 38(4):300 307, April 1948. [7] Paul Heckbert. Color Image Quantization for 
Frame Buffer Display. In Proceedings of SIGGRAPH 82, pages 297 307, 1982. [8] R. W. G. Hunt. Revised 
Colour-Appearance Model for Related and Unrelated Colours. Color Research and Application, 16(3):146 
165, June 1991. [9] Kansei Iwata and Gabriel Marcu. Computer Simulation of Printed Col­ors on Textile 
Materials. In Color Hard Copy and Graphic Arts III, volume 2171 of Proceedings of the SPIE, pages 228 
238, 1994. [10] Tony Johnson. A Complete Colour Reproduction Model for Graphic Arts. In Proceedings of 
the Technical Association of the Graphic Arts, pages 1061 1076, 1996. [11] D. B. Judd and G. Wyszecki. 
Color in Business, Science, and Industry. John Wiley and Sons, New York, 1975. [12] Henry R. Kang. Comparisons 
of Color Mixing Theories for Use in Electronic Printing. In Proceedings of the IS&#38;T/SID Color Imaging 
Conference: Transforms &#38; Transportability of Color, pages 78 82, 1993. [13] Gustav Kort¨um. Re.ectance 
Spectroscopy: Principles, Methods, Ap­plications, chapter 4, pages 103 169. Springer, New York, 1969. 
[14] P. Laihanen. Colour Reproduction Theory Based on the Principles of Colour Science. In Proceedings 
of the International Association of Re­search Institutes for the Graphic Arts Industry, volume 19, pages 
1 36, 1987. [15] Bruce J. Lindbloom. Accurate Color Reproduction for Computer Graphics Applications. 
In Proceedings of SIGGRAPH 89, pages 117 126, 1989. [16] Yan Liu. Spectral Re.ectance Modi.cation of 
Neugebauer Equations. In Proceedings of the Technical Association of the Graphic Arts, pages 154 172, 
1991. [17] Lindsay W. MacDonald. Gamut Mapping in Perceptual Color Space. In Proceedings of the IS&#38;T/SID 
Color Imaging Conference: Trans­forms &#38; Transportability of Color, pages 193 196, 1993. [18] Marc 
Mahy and Paul Delabastita. Inversion of the Neugebauer Equa­tions. Color Research and Application, 21(6):401 
411, December 1996. [19] Gabriel Marcu and Satoshi Abe. Color Designing and Simulation in Non-Conventional 
Printing Process. In Applications of Digital Image Processing XVII, volume 2298 of Proccedings of the 
SPIE, pages 216 223, 1994. [20] Ethan D. Montag and Mark D. Fairchild. Psychophysical Evaluation of Gamut 
Mapping Techniques Using Simple Rendered Images and Ar­ti.cial Gamut Boundaries. IEEE Transactions on 
Image Processing, 6(7):977 989, July 1997. [21] H. E. J. Neugebauer. Die Theoretischen Grundlagen des 
Mehr­farbenbuchdrucks (The Theoretical Foundation for Multicolor Print­ing). Zeitschrift fuer Wissenschaftliche 
Photographie, 36(4):73 89, 1937. Reprinted in Neugebauer Memorial Seminar on Color Repro­duction, volume 
1184 of Proceedings of the SPIE, pages 194 202. SPIE, Bellingham, WA, 1990. [22] Victor Ostromoukhov. 
Chromaticity Gamut Enhancement by Hep­tatone Multi-color Printing. In Device-Independent Color Imaging 
and Imaging Systems Integration, volume 1909 of Proceedings of the SPIE, pages 139 151, 1993. [23] Joanna 
L. Power, Brad S. West, Eric J. Stollnitz, and David H. Salesin. Reproducing Color Images as Duotones. 
In Proceedings of SIG-GRAPH 96, pages 237 248, 1996. [24] William H. Press, Brian P. Flannery, Saul A. 
Teukolsky, and William T. Fetterling. Numerical Recipes. Cambridge University Press, New York, second 
edition, 1992. [25] J. L. Saunderson. Calculation of the Color of Pigmented Plastics. Journal of the 
Optical Society of America, 32(12):727 736, Decem­ber 1942. [26] Eric J. Stollnitz. Reproducing Color 
Images with Custom Inks. Ph.D. thesis, University of Washington, 1998. [27] Maureen C. Stone, William 
B. Cowan, and John C. Beatty. Color Gamut Mapping and the Printing of Digital Color Images. ACM Trans­actions 
on Graphics, 7(4):249 292, October 1988. [28] Atsushi Takaghi, Toru Ozeki, Yoshinori Ogata, and Sachie 
Minato. Faithful Color Printing for Computer Generated Image Syntheses with Highly Saturated Component 
Inks. In Proceedings of the IS&#38;T/SID Color Imaging Conference: Color Science, Systems and Applications, 
pages 108 111, 1994. [29] M. Wolski, J. P. Allebach, and C. A. Bouman. Gamut Mapping: Squeezing the Most 
out of Your Color System. In Proceedings of the IS&#38;T/SID Color Imaging Conference: Color Science, 
Systems and Ap­plications, pages 89 92, 1994.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280898</article_id>
		<sort_key>275</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>27</seq_no>
		<title><![CDATA[Realistic modeling and rendering of plant ecosystems]]></title>
		<page_from>275</page_from>
		<page_to>286</page_to>
		<doi_number>10.1145/280814.280898</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280898</url>
		<keywords>
			<kw><![CDATA[approximate instancing]]></kw>
			<kw><![CDATA[ecosystem simulation]]></kw>
			<kw><![CDATA[modeling of natural phenomena]]></kw>
			<kw><![CDATA[plant model]]></kw>
			<kw><![CDATA[realistic image synthesis]]></kw>
			<kw><![CDATA[self-thinning]]></kw>
			<kw><![CDATA[vector quantization]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.3</cat_node>
				<descriptor>Biology and genetics</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Graphics editors</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010095</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Systems biology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010087</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Computational biology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010935</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Genetics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Experimentation</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP15021799</person_id>
				<author_profile_id><![CDATA[81100092246]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Deussen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Otto-von-Guericke Univ. of Magdeburg]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15033698</person_id>
				<author_profile_id><![CDATA[81100482576]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pat]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hanrahan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford Univ., Stanford, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35025495</person_id>
				<author_profile_id><![CDATA[81408601767]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Bernd]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lintermann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ZKM Center for Art and Media Karlsruhe]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39075845</person_id>
				<author_profile_id><![CDATA[81100163820]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Radom&#237;r]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[M&#283;ch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Calgary, Calgary, Alta., CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31090209</person_id>
				<author_profile_id><![CDATA[81100088896]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Matt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pharr]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford Univ., Stanford, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14162862</person_id>
				<author_profile_id><![CDATA[81100465812]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Przemyslaw]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Prusinkiewicz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Calgary, Calgary, Alta., CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Alias/Wavefront; a division of Silicon Graphics Ltd. Studio V8. SGI program, 1996.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[AnimaTek, Inc. AnimatTek's World Builder. PC program, 1996.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[R.A. Armstrong. A comparison of index-based and pixel-based neighborhood simulations of forest growth. Ecology, 74(6):1707-1712, 1993.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237276</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[A. C. Beers, M. Agrawala, and N. Chaddha. Rendering from compressed textures. In SIGGRAPH 96 Conference Proceedings, pages 373-378, August 1996.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218405</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[B. M. Blumberg and T. A. Galyean. Multi-level direction of autonomous creatures for real-time virtual environments. In SIGGRAPH 95 Conference Proceedings, pages 47-54, August 1995.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>135734</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[B.N. Boots. Spatial tesselations: concepts and applications of Voronoi diagrams. John Wiley, 1992.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>287787</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[A. Brownbill. Reducing the storage required to render L-system based models. Master's thesis, University of Calgary, October 1996.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[N. Chiba, K. Muraoka, A. Doi, and J. Hosokawa. Rendering of forest scenery using 3D textures. The Journal of Visualization and Computer Animation, 8:191-199, 1997.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Adobe Corporation. Adobe Photoshop.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>266799</ref_obj_id>
				<ref_obj_pid>266774</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[O. Deussen and B. Lintermann. A modelling method and user interface for creating plants. In Proceedings of Graphics Interface 97, pages 189-197, May 1997.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237280</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[J. Dorsey, H. K~hling Pedersen, and E Hanrahan. Flow and changes in appearance. In SIGGRAPH 96 Conference Proceedings, pages 411- 420, August 1996.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325174</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[T. Duff. Compositing 3-D rendered images. Computer Graphics (SIGGRAPH 85 Proceedings), 19(3):41-44, 1985.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[F. G. Firbank and A. R. Watkinson. A model of interference within plant monocultures. Journal of Theoretical Biology, 116:291-311, 1985.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218447</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[K.W. Fleischer, D. H. Laidlaw, B. L. Currin, and A. H. Bart. Cellular texture generation. In SIGGRAPH 95 Conference Proceedings, pages 239-248, August 1995.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[R.W. Floyd and L. Steinberg. An adaptive algorithm for spatial gray scale. In SID 75, Int. Symp. Dig. Tech. Papers, pages 36-37, 1975.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134093</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[D.R. Fowler, E Prusinkiewicz, and J. Battjes. A collision-based model of spiral phyllotaxis. Computer Graphics (SIGGRAPH 92 Proceedings), 26(2):361-368, 1992.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808572</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[G. Y. Gardner. Simulation of natural scenes using textured quadric surfaces. Computer Graphics (SIGGRAPH 84 Proceedings), 18(3): 11- 20, 1984.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>128857</ref_obj_id>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[A. Gersho and R. M. Gray. Vector quantization and signal compression. Kluwer Academic Publishers, 1991.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[D. G. Green. Modelling plants in landscapes. In M. T. Michalewicz, editor, Plants to ecosystems. Advances in computational life sciences I, pages 85-96. CSIRO Publishing, Melbourne, 1997.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122728</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[J. C. Hart and T. A. DeFanti. Efficient anti-aliased rendering of 3D linear fractals. Computer Graphics (SIGGRAPH 91 Proceedings), 25:91-100, 1991.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>155320</ref_obj_id>
				<ref_obj_pid>155294</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[J.C. Hart. The object instancing paradigm for linear fractal modeling. In Proceedings of Graphics Interface 92, pages 224-231, 1992.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801294</ref_obj_id>
				<ref_obj_pid>800064</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[E Heckbert. Color image quantization for frame buffer display. Computer Graphics (SIGGRAPH 82 Proceedings), 16:297-307, 1982.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[S.I. Higgins and D. M. Richardson. A review of models of alien plant spread. Ecological Modelling, 87:249-265, 1996.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258843</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[H. Hoppe. View-dependent refinement of progressive meshes. In SIG- GRAPH 97 Conference Proceedings, pages 189-198, August 1997.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618490</ref_obj_id>
				<ref_obj_pid>616050</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[D. H. House, G. S. Schmidt, S. A. Arvin, and M. Kitagawa-DeLeon. Visualizing a real forest. IEEE Computer Graphics and Applications, 18(1):12-15, 1998.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74361</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[J.T. Kajiya and T. L. Kay. Rendering fur with three dimensional textures. Computer Graphics (SIGGRAPH 89 Proceedings), 23(3):271- 289, 1989.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15916</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[T. L. Kay and J. T. Kajiya. Ray tracing complex scenes. Computer Graphics (SIGGRAPH 86 Proceedings), 20(4):269-278, 1986.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378519</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[A. D. Kelley, M. C. Malin, and G. M. Nielson. Terrain simulation using a model of stream erosion. Computer Graphics (SIGGRAPH 88 Proceedings), 22(4):263-268, 1988.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[C. Kolb. Rayshade. http://graphics.stanford.edu/~cek/rayshade.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[M. E Kumler. An intensive comparison of triangulated irregular networks (TINs) and digital elevation models (DEMs). Cartographica, 31(2), 1994.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237199</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[M. Levoy and E Hanrahan. Light field rendering. In SIGGRAPH 96 Conference Proceedings, pages 31-42, August 1996.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[B. Lintermann and O. Deussen. Interactive structural and geometrical modeling of plants. To appear in the IEEE Computer Graphics and Applications.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_obj_id>274986</ref_obj_id>
				<ref_obj_pid>274976</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[B. Lintermann and O. Deussen. Interactive modelling and animation of natural branching structures. In R. Boulic and G. H6gron, editors, Computer Animation and Simulation 96. Springer, 1996.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Lucasfilm Ltd. The Adventures of Andr6 and Wally B. Film, 1984.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_obj_id>266788</ref_obj_id>
				<ref_obj_pid>266774</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[D. Marshall, D. S. Fussel, and A. T. Campbell. Multiresolution rendering of complex botanical scenes. In Proceedings of Graphics Interface 97, pages 97-104, May 1997.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
			<ref>
				<ref_obj_id>275476</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[N. Max. Hierarchical rendering of trees from precomputed multi-layer Z-buffers. In X. Pueyo and E Schr6der, editors, Rendering Techniques 96, pages 165-174 and 288. Springer Wien, 1996.]]></ref_text>
				<ref_id>36</ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[N. Max and K. Ohsaki. Rendering trees from precomputed Z-buffer views. In E M. Hanrahan and W. Purgathofer, editors, Rendering Techniques 95, pages 74-81 and 359-360. Springer Wien, 1995.]]></ref_text>
				<ref_id>37</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74337</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[F. K. Musgrave, C. E. Kolb, and R. S. Mace. The synthesis and rendering of eroded fractal terrains. Computer Graphics (SIGGRAPH 89 Proceedings), 23(3):41-50, 1989.]]></ref_text>
				<ref_id>38</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237279</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[R. M6ch and E Prusinkiewicz. Visual models of plants interacting with their environment. In SIGGRAPH 96 Conference Proceedings, pages 397-410, August 1996.]]></ref_text>
				<ref_id>39</ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[F. Neyret. A general and multiscale model for volumetric textures. In Proceedings of Graphics Interface 95, pages 83-91, 1995.]]></ref_text>
				<ref_id>40</ref_id>
			</ref>
			<ref>
				<ref_obj_id>275481</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[F. Neyret. Synthesizing verdant landscapes using volumetric textures. In X. Pueyo and E Schro6der, editors, Rendering Techniques 96, pages 215-224 and 291, Wien, 1996. Springer-Verlag.]]></ref_text>
				<ref_id>41</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325247</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[K. Perlin. An image synthesizer. Computer Graphics (SIGGRAPH 85 Proceedings), 19(3):287-296, 1985.]]></ref_text>
				<ref_id>42</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258791</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[M. Pharr, C. Kolb, R. Gershbein, and E Hanrahan. Rendering complex scenes with memory-coherent ray tracing. In SIGGRAPH 97 Conference Proceedings, pages 101-108, August 1997.]]></ref_text>
				<ref_id>43</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1668019</ref_obj_id>
				<ref_obj_pid>1668014</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[E Prusinkiewicz. Visual models of morphogenesis. Artificial Life, 1(1/2):61-74, 1994.]]></ref_text>
				<ref_id>44</ref_id>
			</ref>
			<ref>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[E Prusinkiewicz. Modeling spatial structure and development of plants: a review. Scientia Horticulturae, 74(1/2), 1998.]]></ref_text>
				<ref_id>45</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166161</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[E Prusinkiewicz, M. Hammel, and E. Mjolsness. Animation of plant development. In SIGGRAPH 93 Conference Proceedings, pages 351- 360, August 1993.]]></ref_text>
				<ref_id>46</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83596</ref_obj_id>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[E Prusinkiewicz and A. Lindenmayer. The algorithmic beauty of plants. Springer-Verlag, New York, 1990. With J. S. Hanan, F. D. Fracchia, D. R. Fowler, M. J. M. de Boer, and L. Mercer.]]></ref_text>
				<ref_id>47</ref_id>
			</ref>
			<ref>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[E Prusinkiewicz, W. Remphrey, C. Davidson, and M. Hammel. Modeling the architecture of expanding Fraxinus pennsylvanica shoots using L-systems. Canadian Journal of Botany, 72:701-714, 1994.]]></ref_text>
				<ref_id>48</ref_id>
			</ref>
			<ref>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[Questar Productions, LLC. World Construction Set Version 2. PC program, 1997.]]></ref_text>
				<ref_id>49</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325250</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[W.T. Reeves and R. Blau. Approximate and probabilistic algorithms for shading and rendering structured particle systems. Computer Graphics (SIGGRAPH 85 Proceedings), 19(3):313-322, 1985.]]></ref_text>
				<ref_id>50</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37406</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[C.W. Reynolds. Flocks, herds, and schools: A distributed behavioral model. Computer Graphics (SIGGRAPH 87 Proceedings), 21(4):25- 34, 1987.]]></ref_text>
				<ref_id>51</ref_id>
			</ref>
			<ref>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[R. E. Ricklefs. Ecology. Third Edition. W. H. Freeman, New York, 1990.]]></ref_text>
				<ref_id>52</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237209</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[J. Shade, D. Lischinski, D. Salesin, T. DeRose, and J. Snyder. Hierarchical image caching for accelerated walkthroughs of complex environments. In SIGGRAPH 96 Conference Proceedings, pages 75-82, August 1996.]]></ref_text>
				<ref_id>53</ref_id>
			</ref>
			<ref>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[H. Sinoquet and R. Rivet. Measurement and visualization of the architecture of an adult tree based on a three-dimensional digitising device. Trees, 11:265-270, 1997.]]></ref_text>
				<ref_id>54</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808571</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[A.R. Smith. Plants, fractals, and formal languages. Computer Graphics (SIGGRAPH 84 Proceedings), 18(3):1-10, 1984.]]></ref_text>
				<ref_id>55</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37417</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[J.M. Snyder and A. H. Bart. Ray tracing complex models containing surface tessellations. Computer Graphics (SIGGRAPH 87 Proceedings), 21(4):119-128, 1987.]]></ref_text>
				<ref_id>56</ref_id>
			</ref>
			<ref>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[R.R. Sokal and F. J. Rohlf. Biometry. Third Edition. W. H. Freeman, New York, 1995.]]></ref_text>
				<ref_id>57</ref_id>
			</ref>
			<ref>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[K. A. Sorrensen-Cothern, E. D. Ford, and D. G. Sprugel. A model of competition incorporating plasticity through modular foliage and crown development. Ecological Monographs, 63(3):277-304, 1993.]]></ref_text>
				<ref_id>58</ref_id>
			</ref>
			<ref>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[I.E. Sutherland. Sketchpad: A man-machine graphical communication system. Proceedings of the Spring Joint Computer Conference, 1963.]]></ref_text>
				<ref_id>59</ref_id>
			</ref>
			<ref>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[J.H.M. Thornley and I. R. Johnson. Plant and crop modeling: A mathematical approach to plant and crop physiology. Oxford University Press, New York, 1990.]]></ref_text>
				<ref_id>60</ref_id>
			</ref>
			<ref>
				<ref_obj_id>45056</ref_obj_id>
				<ref_obj_pid>45054</ref_obj_pid>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[S.J. Wan, S. K. M. Wong, and E Prusinkiewicz. An algorithm for multidimensional data clustering. ACM Trans. Math. Software, 14(2): 135- 162, 1988.]]></ref_text>
				<ref_id>61</ref_id>
			</ref>
			<ref>
				<ref_obj_id>107217</ref_obj_id>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[A. Watt and M. Watt. Advanced animation and rendering techniques: Theory and practice. Addison-Wesley, Reading, 1992.]]></ref_text>
				<ref_id>62</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218427</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[J. Weber and J. Penn. Creation and rendering of realistic trees. In SIG- GRAPH 95 Conference Proceedings, pages 119-128, August 1995.]]></ref_text>
				<ref_id>63</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807402</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[L. Williams. Casting curved shadows on curved surfaces. Computer Graphics (SIGGRAPH 78 Proceedings), 12(3):270-274, 1978.]]></ref_text>
				<ref_id>64</ref_id>
			</ref>
			<ref>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[L. Williams. Shading in two dimensions. In Proceedings of Graphics Interface 91, pages 143-151, June 1991.]]></ref_text>
				<ref_id>65</ref_id>
			</ref>
			<ref>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[H. Wu, K. W. Malafant, L. K. Pendridge, E J. Sharpe, and J. Walker. Simulation of two-dimensional point patterns: application of a latice framework approach. Ecological Modelling, 38:299-308, 1997.]]></ref_text>
				<ref_id>66</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Copyright &#38;#169;1998 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers or to redistribute to lists, requires specific permission and/or a fee. Realistic modeling 
and rendering of plant ecosystems Oliver Deussen1 Pat Hanrahan2 Bernd Lintermann3 Radom´ir M.ech4 Matt 
Pharr2 Przemyslaw Prusinkiewicz4 1 Otto-von-Guericke University of Magdeburg 2 Stanford University 3 
The ZKM Center for Art and Media Karlsruhe 4 The University of Calgary. Abstract Modeling and rendering 
of natural scenes with thousands of plants poses a number of problems. The terrain must be modeled and 
plants must be distributed throughout it in a realistic manner, re.ecting the interactions of plants 
with each other and with their environment. Geometric models of individual plants, consistent with their 
po­sitions within the ecosystem, must be synthesized to populate the scene. The scene, which may consist 
of billions of primitives, must be rendered ef.ciently while incorporating the subtleties of lighting 
in a natural environment. We have developed a system built around a pipeline of tools that address these 
tasks. The terrain is designed using an interactive graphical editor. Plant distribution is determined 
by hand (as one would do when designing a garden), by ecosystem simulation, or by a combination of both 
techniques. Given parametrized procedural models of individual plants, the geometric complexity of the 
scene is reduced by approximate instancing, in which similar plants, groups of plants, or plant organs 
are replaced by instances of representative objects before the scene is rendered. The paper includes 
examples of visually rich scenes synthesized using the system. CR categories: I.3.7 [Computer Graphics]: 
Three-Dimensional Graphics and Realism, I.6.3 [Simulation and Modeling]: Appli­cations, J.3 [Life and 
Medical Sciences]: Biology. Keywords: realistic image synthesis, modeling of natural phenom­ena, ecosystem 
simulation, self-thinning, plant model, vector quan­tization, approximate instancing. 1 INTRODUCTION 
Synthesis of realistic images of terrains covered with vegetation is a challenging and important problem 
in computer graphics. The challenge stems from the visual complexity and diversity of the modeled scenes. 
They include natural ecosystems such as forests or .Department of Computer Science, University of Calgary, 
Cal­gary, Alberta, Canada T2N 1N4 (pwp@cpsc:ucalgary:ca) grasslands, human-made environments, for instance 
parks and gar­dens, and intermediate environments, such as lands recolonized by vegetation after forest 
.res or logging. Models of these ecosystems have a wide range of existing and potential applications, 
including computer-assisted landscape and garden design, prediction and vi­sualization of the effects 
of logging on the landscape, visualization of models of ecosystems for research and educational purposes, 
and synthesis of scenes for computer animations, drive and .ight simulators, games, and computer art. 
Beautiful images of forests and meadows were created as early as 1985 by Reeves and Blau [50] and featured 
in the computer animation The Adventures of Andr´ e and Wally B. [34]. Reeves and Blau organized scene 
modeling as a sequence of steps: speci.cation of a terrain map that provides the elevation of points 
in the scene, interactive or procedural placement of vegetation in this terrain, modeling of individual 
plants (grass and trees), and rendering of the models. This general scheme was recently followed by Chiba 
et al. [8] in their work on forest rendering, and provides the basis for commercial programs devoted 
to the synthesis of landscapes [2, 49]. The complexity of nature makes it necessary to carefully allot 
com­puting resources CPU time, memory, and disk space when recreating natural scenes with computer 
graphics. The size of the database representing a scene during the rendering is a particularly critical 
item, since the amount of geometric data needed to represent a detailed outdoor scene is more than can 
be represented on modern computers. Consequently, a survey of previous approaches to the synthesis of 
natural scenes re.ects the quest for a good tradeoff be­tween the realism of the images and the amount 
of resources needed to generate them. The scenes synthesized by Reeves and Blau were obtained using (structured) 
particle systems, with the order of one million particles per tree [50]. To handle large numbers of primitive 
elements con­tributing to the scene, the particle models of individual trees were generated procedurally 
and rendered sequentially, each model dis­carded as soon as a tree has been rendered. Consequently, the 
size of memory needed to generate the scene was proportional to the number of particles in a single tree, 
rather than the total number of particles in the scene. This approach required approximate shading calculations, 
since the detailed information about the neighborhood trees was not available. Approximate shading also 
reduced the time needed to render the scenes. Another approach to controlling the size of scene representation 
is the reduction of visually unimportant detail. General methods for achieving this reduction have been 
the subject of intense re­search (for a recent example and further references see [24]), but the results 
do not easily apply to highly branching plant structures. Consequently, Weber and Penn [63] introduced 
a heuristic multires­olution representation speci.c to trees, which allows for reducing the number of 
geometric primitives in the models that occupy only a small portion on the screen. A multiresolution 
representation of botanical scenes was also explored by Marshall et al. [35], who in­tegrated polygonal 
representations of larger objects with tetrahedral approximations of the less relevant parts of a scene. 
A different strategy for creating visually complex natural scenes was proposed by Gardner [17]. In this 
case, the terrain and the trees were modeled using a relatively small number of geometric primitives 
(quadric surfaces). Their perceived complexity resulted from procedural textures controlling the color 
and the transparency of tree crowns. In a related approach, trees and other plants were represented as 
texture-mapped .at polygons (for example, see [49]). This approach produced visible artifacts when the 
position of the viewpoint was changed. A more accurate image-based representa­tion was introduced by 
Max [37], who developed an algorithm for interpolating between precomputed views of trees. A multiresolu­tion 
extension of this method, taking advantage of the hierarchical structure of the modeled trees, was presented 
in [36]. Shade et al. described a hybrid system for walkthroughs that uses a combination of geometry 
and textured polygons [53]. Kajiya and Kay [26] introduced volumetric textures as an alter­native paradigm 
for overcoming the limitations of texture-mapped polygons. A method for generating terrains with volumetric 
textures representing grass and trees was proposed by Neyret [40, 41]. Chiba et al. [8] removed the deformations 
of plants caused by curvatures of the underlying terrain by allowing texels to intersect. The use of 
volumetric textures limits the memory or disk space needed to represent a scene, because the same texel 
can be re-applied to multiple areas. The same idea underlies the oldest approach to harnessing visually 
complex scenes, object instancing [59]. Ac­cording to the paradigm of instancing, an object that appears 
sev­eral times in a scene (possibly resized or deformed by an af.ne transformation) is de.ned only once, 
and its different occurrences (instances) are speci.ed by af.ne transformations of the prototype. Since 
the space needed to represent the transformations is small, the space needed to represent an entire scene 
depends primarily on the number and complexity of different objects, rather than the number of their 
instances. Plants are particularly appealing objects of instancing, because repetitive occurrences can 
be found not only at the level of plant species, but also at the level of plant organs and branching 
structures. This leads to compact hierarchical data structures conducive to fast ray tracing, as discussed 
by Kay and Kajiya [27], and Snyder and Barr [56]. Hart and DeFanti [20, 21] further extended the paradigm 
of instancing from hierarchical to recursive (self-similar) structures. All the above papers contain 
examples of botanical scenes generated using instancing. The complexity of natural scenes makes them 
not only dif.cult to render, but also to specify. Interactive modeling techniques, avail­able in commercial 
packages such as Alias/Wavefront Studio 8 [1], focus on the direct manipulation of a relatively small 
number of surfaces. In contrast, a landscape with plants may include many millions of individual surfaces 
 representing stems, leaves, .ow­ers, and fruits arranged into complex branching structures, and further 
organized in an ecosystem. In order to model and render such scenes, we employ the techniques summarized 
below. Multilevel modeling and rendering pipeline. Following the ap­proach initiated by Reeves and Blau 
[50], we decompose the process of image synthesis into stages: modeling of the terrain, speci.ca­tion 
of plant distribution, modeling of the individual plants, and rendering of the entire scene. Each of 
these stages operates at a different level of abstraction, and provides a relatively high-level input 
for the next stage. Thus, the modeler is not concerned with plant distribution when specifying the terrain, 
and plant distribution is determined (interactively or algorithmically) without considering details of 
the individual plants. This is reminiscent of the simu­lations of .ocks of birds [51], models of .ower 
heads with phyl­lotactic patterns [16], and models of organic structures based on morphogenetic processes 
[14], where simulations were performed using geometrically simpler objects than those used for visualiza­tion. 
Blumberg and Galyean extended this paradigm to multi-level direction of autonomous animated agents [5]. 
In an analogous way, we apply it to multi-level modeling. Open system architecture. By clearly specifying 
the formats of inputs and outputs for each stage of the pipeline, we provide a framework for incorporating 
independently developed modules into our system. This open architecture makes it possible to augment 
the complexity of the modeled scenes by increasing the range of the available modules, and facilitates 
experimentation with various approaches and algorithms. Procedural models. As observed by Smith [55], 
procedural models are often characterized by a signi.cant data-base ampli.cation, which means that they 
can generate complex geometric structures from small input data sets. We bene.t from this phenomenon 
by employing procedural models in all stages of the modeling pipeline. Approximate instancing. We use 
object instancing as the primary paradigm for reducing the size of the geometric representation of the 
rendered scenes. To increase the degree of instancing, we cluster scene components (plants and their 
parts) in their parameter spaces, and approximate all objects within a given cluster with instances of 
a single representative object. This idea was initially investigated by Brownbill [7]; we extend it further 
by applying vector quantization (c.f. [18]) to .nd the representative objects in multidimensional parameter 
spaces. Ef.cient rendering. We use memory-and time-ef.cient render­ing techniques: decomposition of the 
scenes into subscenes that are later composited [12], ray tracing with instancing and a sup­port for 
rendering many polygons [56], and memory-coherent ray tracing [43] with instancing. By employing these 
techniques, we have generated scenes with up to 100,000 detailed plant models. This number could be increased 
even further, since none of the scenes required more than 150MB to store. However, with 100,000 plants, 
each plant is visible on average only in 10 pixels of a 1K .1K image. Consequently, we seem to have reached 
the limits of useful scene complexity, because the level of visible detail is curbed by the size and 
resolution of the output device. 2 SYSTEM ARCHITECTURE The considerations presented in the previous 
section led us to the modular design of our modeling and rendering system EcoSys, shown schematically 
in Figure 1. The modeling process begins with the speci.cation of a terrain. For this purpose, we developed 
an interactive editor TerEdit,which integrates a number of terrain modeling techniques (Section 3). Its 
output, a terrain data .le, includes the altitudes of a grid of points su­perimposed on the terrain, 
normal vectors indicating the local slope of the terrain, and optional information describing environmental 
conditions, such as soil humidity. The next task is to determine plant distribution on a given terrain. 
We developed two techniques for this purpose: visual editing of plant densities and simulation of plant 
interactions within an ecosystem Figure 1: Architecture of the scene synthesis system. Bold frames indicate 
interactive programs and input .les speci.ed by the user. (Section 4). The editing approach is particularly 
well suited to model environments designed by people, for example orchards, gardens, or parks. The user 
speci.es the distribution of plant densities using a paint program. To convert this information into 
positions of individual plants, we developed the program densedisbased on a half-toning algorithm: each 
dot becomes a plant. We can also specify positions of individual plants explicitly; this is particularly 
important in the synthesis of scenes that include detailed views of individual plants in the foreground. 
To de.ne plant distribution in natural environments, such as forests or meadows, we apply an ecosystem 
simulation model. Its input consists of terrain data, ecological characteristics of plant species (for 
example, annual or perennial growth and reproduction cycle, preference for wet or dry soil, and shade 
tolerance) and, optionally, the initial distribution of plants. The growth of plants is simu­lated accounting 
for competition for space, sunlight, resources in the soil, aging and death, seed distribution patterns, 
etc.We per­form these simulations using the L-system-based plant modeling program cpfg [47], extended 
with capabilities for simulating in­teractions between plants and their environments [39]. To allow for 
simulations involving thousands of plants, we use their simpli.ed geometrical representations, which 
are subsequently replaced by detailed plant models for visualization purposes. Speci.cation of a plant 
distribution may involve a combination of interactive and simulation techniques. For example, a model 
of an orchard may consist of trees with explicitly speci.ed positions and weeds with positions determined 
by a simulation. Conversely, the designer of a scene may wish to change its aspects after an ecological 
simulation for aesthetic reasons. To allow for these operations, both densedis and cpfg can take a given 
plant distribution as input for further processing. Plant distribution, whether determined interactively 
or by ecosystem simulation, is represented in an ecosystem .le. It contains the information about the 
type, position and orientation of each plant (which is needed to assemble the .nal scene), and parameters 
of individual plants (which are needed to synthesize their geometric models). Since we wish to render 
scenes that may include thousands of plants, each possibly with many thousands of polygons, the creation 
and storage of a separate geometric plant model for each plant listed in the ecosystem .le is not practical. 
Consequently, we developed a program quantvthat clusters plants in their parameter space and determines 
a representative plant for each cluster (Section 6). The algorithm performs quantization adaptively, 
thus the result depends on the characteristics of plants in the ecosystem. The quantization process produces 
two outputs: a plant parameter .le, needed to create geometric models of the representative plants, and 
a quantized ecosystem .le, which speci.es positions and orientations of the instances of representative 
plants throughout the scene. We employ two modeling programs to create the representative plants: the 
interactive plant modeler xfrog [10, 32, 33] and the L-system-based simulator cpfg [39, 47]. These programs 
input parametrized procedural plant models and generate speci.c geo­metric plant models according to 
the values in the plant parameter .le (Section 5). To reduce the amount of geometric data, we ex­tended 
the concept of instancing and quantization to components of plants. Thus, if a particular plant or group 
of plants has several parts (such as branches, leaves, or .owers) that are similar in their respective 
parameter spaces, we replace all occurrences of these parts with instances of a representative part. 
Finally, the ecosystem is rendered. The input for rendering con­sists of the quantized ecosystem .le 
and the representative plant models. Additional information may include geometry of the ter­rain and 
human-made objects, such as houses or fences. In spite of the quantization and instancing, the resulting 
scene descriptions may still be large. We experimented with three renderers to handle this complexity 
(Section 7). One renderer, called fshade,de­composes the scene into sub-scenes that are rendered individually 
and composited to form .nal images. Unfortunately, separating the scene into sub-scenes makes it impossible 
to properly capture global illumination effects. To alleviate this problem, we use the ray-tracer rayshade[29], 
which offers support for instancing and time-ef.cient rendering of scenes with many polygons, as long 
as the scene description .ts in memory. When the scene description exceeds the available memory, we employ 
the memory-ef.cient ray-tracer toro[43], extended with a support for instancing. In the following sections 
we describe the components of the EcoSysmodeling pipeline in more detail. In Section 8, we present examples 
that illustrate the operation of the system as a whole.  3 TERRAIN SPECIFICATION We begin the modeling 
of a scene with the speci.cation of a terrain. The goal of this step is to determine elevation data, 
local orienta­tions of the terrain, and additional characteristics, such as the water content in the 
soil, which may affect the type and vigor of plants at different locations. Terrain data may have several 
sources. Representations of real terrains are available, for example, from the U.S. Geological Sur­vey 
[30]. Several techniques have also been developed for creating synthetic terrains. They include: hand-painted 
height maps [65], methods for generating fractal terrains (reviewed in [38]), and mod­els based on the 
simulation of soil erosion [28, 38]. In order to provide detailed control over the modeled terrain while 
taking advantage of the data ampli.cation of fractal methods [55], and with a stream cut using the stream 
mask (right).  we designed and implemented an interactive terrain editing sys­tem TerEdit, which combines 
various techniques in a procedural manner. Terrain editing consists of operations, which modify the terrain 
geometry and have the spatial scope limited by masks.A similar paradigm is used in Adobe Photoshop [9], 
where selections can be used to choose an arbitrary subset of an image to edit. We assume that masks 
have values between zero and one, allowing for smooth blending of the effects of operations. Both masks 
and operations can depend on the horizontal coordinates and the altitude of the points computed so far. 
Thus, it is possible to have masks that select terrain above some altitude or operations that are functions 
of the current altitude. The user s editing actions create a pipeline of operations with associated masks; 
to compute the terrain altitude at a point, the stages of this pipeline are evaluated in order. Undo 
and redo operations are easily supported by removing and re-adding operations from the pipeline and re-evaluating 
the terrain. Examples of editing operations include translation, scaling, non­linear scaling, and algorithmic 
synthesis of the terrain. The syn­thesis algorithm is based on noise synthesis [38], which generates 
realistic terrains by adding multiple scales of Perlin s noise func­tion [42]. The user can adjust a 
small number of parameters that control the overall roughness of the terrain, the rate of change in roughness 
across the surface of the terrain, and the frequency of the noise functions used. Noise synthesis allows 
terrain to be eas­ily evaluated at a single point, without considering the neighboring points; this makes 
it possible to have operations that act locally. Another advantage of noise synthesis is ef.ciency of 
evaluation; updating the wireframe terrain view (based on 256 .256 samples of the region of interest) 
after applying an operation typically takes under a second. On a multiprocessor machine, where terrain 
evalu­ation is multi-threaded, the update time is not noticeable. The editor provides a variety of masks, 
including ones that select rectangular regions of terrain from a top view, masks that select regions 
based on their altitude, and masks de.ned by image .les. One of the most useful masks is designed for 
cutting streams through terrain. The user draws a set of connected line segments over the terrain, and 
the in.uence of the mask is based on the minimum distance from a sample point to any of these segments. 
A spline is applied to smoothly increase the in.uence of the mask close to the segments. When used with 
a scaling operation, the terrain inside and near the stream is scaled towards the water level, and nearby 
terrain is ramped down, while the rest of the terrain is unchanged. The speci.cation of a terrain using 
TerEditis illustrated in Fig­ure 2. In the .rst snapshot, the hill in the far corner was de.ned by loading 
in a height map that had been painted by hand. Next, small hills were added to the entire terrain using 
noise synthesis. The last image shows the .nal terrain, after the stream mask was used to cut the path 
of a stream. A total of .ve operators were applied to make this terrain, and the total time to model 
it was approximately .fteen minutes. Once the elevations have been created, additional parameters of 
the terrain can be computed as input for ecosystem simulations or a direct source of parameters for plant 
models. Although the user can interactively paint parameters on the terrain, simulation provides a more 
sound basis for the modeling of natural ecosystems. Consequently, TerEditincorporates a simulator of 
rain water .ow and distribution in the soil, related to both the erosion algorithm by Musgrave et al. 
[38] and the particle system simulation of water on building facades by Dorsey et al. [11]. Water is 
dropped onto the terrain from above; some is absorbed immediately while the rest .ows downhill and is 
absorbed by the soil that it passes through. A sample terrain with the water distribution generated using 
this approach is shown in Figure 3. Figure 3: A sample terrain with the water concentration ranging 
from high (blue) to low (yellow)  4 SPECIFICATION OF PLANT POPULATIONS The task of populating a terrain 
with plants can be addressed using methods that offer different tradeoffs between the degree of user 
control, time needed to specify plant distribution, and biological validity of the results. The underlying 
techniques can be divided into space-occupancy or individual-based techniques. This clas­si.cation is 
related to paradigms of spatially-explicit modeling in ecology [3, 19], and parallels the distinction 
between space-based and structure-based models of morphogenesis [44]. The space-occupancy techniques 
describe the distribution of the densities of given plant species over a terrain. In the image synthesis 
context, this distribution can be be obtained using two approaches: Explicit speci.cation. The distribution 
of plant densities is mea­sured in the .eld (by counting plants that occupy sample plots) or created 
interactively, for example using a paint program. Procedural generation. The distributions of plant densities 
is ob­tained by simulating interactions between plant populations using an ecological model. The models 
described in the literature are commonly expressed in terms of cellular automata [19] or reaction­diffusion 
processes [23]. The individual-based techniques provide the location and attributes of individual plants. 
Again, we distinguish two approaches: Explicit speci.cation. Plant positions and attributes represent 
.eld data, for example obtained by surveying a real forest [25], or speci­.ed interactively by the user. 
Procedural generation. Plant positions and attributes are obtained using a point pattern generation model, 
which creates a distribu­tion of points with desired statistical properties [66], or using an individual-based 
population model [13, 58], which is applied to simulate interactions between plants within an ecosystem. 
Below we describe two methods for specifying plant distribution that we have developed and implemented 
as components of EcoSys. The .rst method combines interactive editing of plant densities with a point 
pattern generation of the distribution of individual plants. The second method employs individual-based 
ecological simulations. 4.1 Interactive speci.cation of plant populations To specify a plant population 
in a terrain, the user creates a set of gray-level images with a standard paint program. These im­ages 
de.ne the spatial distributions of plant densities and of plant characteristics such as the age and vigor. 
Given an image that speci.es the distribution of densities of a plant species, positions of individual 
plants are generated using a half­toning algorithm. We have used the Floyd-Steinberg algorithm [15] for 
this purpose. Each black pixel describes the position of a plant in the raster representing the terrain. 
We also have implemented a relaxation method that iteratively moves each plant position towards the center 
of mass of its Voronoi polygon [6]. This reduces the variance of distances between neighboring plants, 
which sometimes produces visually pleasing effects. Once the position of a plant has been determined, 
its parameters are obtained by referring to the values of appropriate raster images at the same point. 
These values may control the plant model directly or provide arguments to user-speci.ed mathematical 
expressions, which in turn control the models. This provides the user with an additional means for .exibly 
manipulating the plant population. Operations on raster images make it possible to capture some inter­actions 
between plants. For example, if the radius of a tree crown is known, the image representing the projection 
of the crown on the ground may be combined with user-speci.ed raster images to decrease the density or 
vigor of plants growing underneath.  4.2 Simulation of ecosystems Individual-based models of plant ecosystems 
operate at various lev­els of abstraction, depending on the accuracy of the representation of individual 
plants [58]. Since our goal is to simulate complex scenes with thousands of plants, we follow the approach 
of Firbank and Watkinson [13], and represent plants coarsely as circles posi­tioned in a continuous landscape. 
Each circle de.nes the ecological neighborhood of the plant in its center, that is the area within which 
the plant interacts with it neighbors. Biologically motivated rules govern the outcomes of interactions 
between the intersecting circles. Global behavior of the ecosystem model is an emergent property of a 
system of many circles. We implemented the individual-based ecosystem models using the framework of open 
L-systems [39]. Since L-systems operate on branching structures, we represent each plant as a circle 
located at the end of an invisible positioning line. All lines are connected into a branching structure 
that spreads over the terrain.  A 100 10 1 0.1 d 0.01 0.1 1 10 100 1000 Figure 5: The average area 
of plants as a function of their density. Small dots represent the results of every tenth simulation 
step. Large dots correspond to the states of simulation shown in Figure 4. For example, let us consider 
a model of plant distribution due to a fundamental phenomenon in plant population dynamics, self­thinning. 
This phenomenon is described by Ricklefs as follows [52, page 339]: If one plots the logarithm of average 
plant weight as a function of the logarithm of density, data points fall on a line with a slope of approximately 
,23 [called the self-thinning curve]. [...] When seeds are planted at a moderate density, so that the 
begin­ning combination of density and average dry weight lies below the self-thinning curve, plants grow 
without appreciable mortality until the population reaches its self-thinning curve. After that point, 
the intense crowding associated with further increase in average plant size causes the death of smaller 
individuals. Our model of self-thinning is a simpli.ed version of that by Firbank and Watkinson [13]. 
The simulation starts with an initial set of circles, distributed at random in a square .eld, and assigned 
random initial radii from a given interval. If the circles representing two plants intersect, the smaller 
plant dies and its corresponding circle is removed from the scene. Plants that have reached a limit size 
are considered old and die as well. Figure 4 shows three snapshots of the simulation. The correspond­ing 
plot shows the average area of the circles as a function of their density (Figure 5). The slope of the 
self-thinning curve is equal to ,1; assuming that mass is proportional to volume, which in turn is proportional 
to area raised to the power of ,23, the self-thinning curve in the density-mass coordinates would have 
the slope of ,23. Thus, in spite of its simplicity, our model captures the essential characteristic of 
plant distribution before and after it has reached the self-thinning curve.     Figure 6: Simulated 
distribution of eight plant species in a ter­rain from Figure 3. Colors indicate plant species. Plants 
with a preference for wet areas are shown in blue. A slightly more complicated model describes plant 
distribution in a population of different plant species. Each species is de.ned by a set of values that 
determine: (i) the number of new plants added to the .eld per simulation step, (ii) the maximum size 
of the plants, (iii) their average growth rate, (iv) the probability of surviving the domination by a 
plant with a competitive advantage, and (v) a preference for wet or dry areas. An individual plant is 
characterized by: (i) the species to which it belongs, (ii) its size, and (iii) its vigor. The vigor 
is a number in the range from 0 to 1, assigned to each plant as a randomized function of water concentration 
at the plant s location and the plant s preference for wet or dry areas. The competitive ability of a 
plant is determined as a product of its vigor and its relative size (the ratio between the actual and 
maximum size). When the circles representing two plants intersect, their competitive abilities are compared. 
The plant with a smaller competitive ability is dominated by the other plant and may die with the de.ned 
probability. Figure 6 presents the result of a simulation involving a mix of eight plant species growing 
in a terrain shown in Figure 3. Plants with a preference for wet areas are represented by blue circles. 
Plants with a preference for dry areas have been assigned other colors. Through the competition between 
the species, a segregation of plants between the wet and dry areas has emerged. Similar models can be 
developed to capture other phenomena that govern the development of plant populations.  5 MODELING 
OF PLANTS Interactive editing of plant populations and the simulation of ecosys­tems determine positions 
and high-level characteristics of all plants in the modeled scene. On this basis, geometric models of 
individual plants must now be found. Recent advances in plant measuring techniques have made it possi­ble 
to construct a geometric model of a speci.c plant according to detailed measurements of its structure 
[54]. Nevertheless, for the purpose of visualizing plants differing by age, vigor, and possibly other 
parameters, it is preferable to treat geometric models as a product of the underlying procedural models. 
Construction of such models for computer graphics and biological purposes has been a .eld of active study, 
recently reviewed in [45]. Consequently, below we discuss only the issue of model parametrization, that 
is the incor­poration of high-level parameters returned by the population model into the plant models. 
We consider two different approaches, which re.ect different predictive values of mechanistic and descriptive 
models [60]. Mechanistic models operate by simulating the processes that control the development of plants 
over time. They inherently capture how the resulting structure changes with age [46, 47]. If a mechanistic 
model incorporates environmental inputs, the dependence of the resulting structure on the corresponding 
environmental parameters is an emergent feature of the model [39]. The model predicts the effect of various 
combinations of environmental parameters on the structure, and no explicit parametrization is needed. 
L-systems [47] and their extensions [39] provide a convenient tool for expressing mechanistic models. 
Within EcoSys, mechanistic models have been generated using cpfg. Descriptive models capture plant architecture 
without simulating the underlying developmental processes. Consequently, they do not have an inherent 
predictive value. Nevertheless, if a family of geometric models is constructed to capture the key postures 
of a plant at different ages and with different high-level characteristics, we can obtain the in-between 
geometries by interpolation. This is equivalent to .tting functions that map the set of high-level param­eters 
to the set of lower-level variables present in the model, and can be accomplished by regression [57] 
(see [48] for an application example). In the special case of plant postures characterized by a single 
parameter, the interpolation between key postures is analo­gous to key-framing [62], and can be accomplished 
using similar techniques. We applied interpolation to parametrize models created using both xfrogand 
cpfg. 6 APPROXIMATE INSTANCING Geometric plant models are often large. A detailed polygonal rep­resentation 
of a herbaceous plant may require over 10MB to store; a scene with one thousand plants (a relatively 
small number in ecosys­tem simulations) would require 10GB. One approach for reducing such space requirements 
is to simplify geometric representations of objects that have a small size on the screen. We incorporated 
a version of this technique into our system by parameterizing the procedural plant models so that they 
can produce geometries with different polygonizations of surfaces. However, this technique alone was 
not suf.cient to reduce the amount of data to manageable levels. Instancing was used successfully in 
the past for compactly rep­resenting complex outdoor scenes (e.g. [56]). According to the paradigm of 
instancing [59], geometric objects that are identical up to af.ne transformations become instances of 
one object. To achieve a further reduction in the size of geometric descriptions, we extended the paradigm 
of instancing to objects that resemble each other, but are not exactly the same. Thus, sets of similar 
plants are represented by instances of a single representative plant. Fur­thermore, the hierarchical 
structure of plant scenes, which may be decomposed into groups of plants, individual plants, branches 
of different order, plant organs such as leaves and .owers, etc., lends itself to instancing at different 
levels of the hierarchy. We create hierarchies of instances by quantizing model components in their respective 
parameter spaces, and reusing them. Automatic generation of instance hierarchies for plant models ex­pressed 
using a limited class of L-systems was considered by Hart [20, 21]. His approach dealt only with exact 
instancing. Brownbill [7] considered special cases of approximate instancing of plants, and analyzed 
tradeoffs between the size of the geometric models and their perceived distortion (departure from the 
original geometry caused by the reduction of diversity between the compo­nents). He achieved reductions 
of the database size ranging from 5:1 to 50:1 with a negligible visual impact on the generated images 
(a tree and a .eld of grass). This result is reminiscent of the observation by Smith [55] that the set 
of random numbers used in stochastic al­gorithms for generating fractal mountains and particle-system 
trees can be reduced to a few representative values without signi.cantly affecting the perceived complexity 
of the resulting images. We generalize Brownbill s approach by relating it to clustering. Assuming that 
the characteristics of each plant are described by a vector of real numbers, we apply a clustering algorithm 
to the set of these vectors in order to .nd representative vectors. Thus, we reduce the problem of .nding 
representative plants and instancing them to the problem of .nding a set of representative points in 
the parameter space and mapping each point to its representative. We assume that plants with a similar 
appearance are described by close points in their parameter space; if this is not the case (for example, 
because the size of a plant is a nonlinear function of its age), we transform the parameter space to 
become perceptually more linear. We cluster and map plant parts in the same manner as the entire plants. 
This clustering and remapping can be stated also in terms of vector quantization [18]: we store a code 
book of plants and plant parts and, for each input plant, we store a mapping to an object in the code 
book rather than the plant geometry itself. In computer graphics, vector quantization has been widely 
used for color image quantization [22]; more recent applications include reduction of memory needs for 
texture mapping [4] and representing light .elds [31]. We use a multi-dimensional clustering algorithm 
developed by Wan et al. [61], which subdivides the hyperbox containing data points by choosing splitting 
planes to minimize the variance of the resulting clusters of data. We extended this algorithm to include 
an impor­tance weight with each input vector. The weights make it possible to further optimize the plant 
quantization process, for example by allocating more representative vectors to the plants that occupy 
a largeareaofthe screen. 7 RENDERING Rendering natural scenes raises two important questions: (i) dealing 
with scene complexity, and (ii) simulating illumination, materials and atmospheric effects. Within EcoSys, 
we addressed these ques­tions using two different strategies. The .rst strategy is to to split the scene 
into sub-scenes of man­ageable complexity, render each of them independently using ray­casting, and composite 
the resulting RGBaZ images into the .nal image [12]. The separation of the scene into sub-scenes is a 
byprod­uct of the modeling process: both densedisand cpfgcan output the distribution of a single plant 
species to form a sub-scene. The ray-casting algorithm is implemented in fshade, which creates the scene 
geometry procedurally by invoking the xfrogplant modeler at run time. This reduces .le I/O and saves 
disk space compared to storing all of the geometric information for the scene on disk and reading it 
in while rendering. For example, the poplar tree shown in Figure 16 is 16 KB as a procedural model (plant 
template), but 6.7 MB in a standard text geometry format. A number of operations can be applied to the 
RGBaZ images before they are combined. Image processing operations, such as saturation and brightness 
adjustments, are often useful. Atmospheric effects can be introduced in a post process, by modifying 
colors according to the pixel depth. Shadows are computed using shadow maps [64]. The scene separation 
makes it possible to render the scene quickly and re-render its individual sub-scenes as needed to improve 
the image. However, complex lighting effects cannot be easily included, since the renderer doesn t have 
access to the entire scene description at once. The second rendering strategy is ray tracing. It lacks 
the capability to easily re-render parts of scenes that have been changed, but makes it possible to include 
more complex illumination effects. In both ray-tracers that we have used, rayshade[29] and toro[43], 
pro­cedural geometry descriptions are expanded into triangle meshes, complemented with a hierarchy of 
grids and bounding boxes needed to speed up rendering [56]. Rayshade requires the entire scene description 
(object prototypes with their bounding boxes and a hier­archy of instancing transformations) to be kept 
in memory, otherwise page swapping signi.cantly decreases the ef.ciency of rendering. In the case of 
toro, meshes are stored on disk; these are read in parts to a memory cache as needed for rendering computations 
and removed from memory when a prescribed limit amount of memory has been used. Consequently, the decrease 
in performance when the memory size has been reached is much slower [43]. We have made the straightforward 
extension of memory-coherent ray-tracing al­gorithms to manage instanced geometry: along with non-instanced 
geometry, the instances in the scene are also managed by the geom­etry cache. Because rays can be traced 
that access the entire scene, more com­plex lighting effects can be included. For example, we have found 
that attenuating shadow rays as they pass through translucent leaves of tree crowns greatly improves 
their realism and visual richness.  8 EXAMPLES We evaluated our system by applying it to create a number 
of scenes. In the examples presented below, we used two combinations of the modules: (i) ecosystem simulation 
and plant modeling using cpfg followed by rendering using rayshade or toro, and (ii) interactive speci.cation 
of plant distribution using densedis in conjunction with plant generation using xfrogand rendering using 
fshade. Figure 7 presents visualizations of two stages of the self-thinning process, based on distributions 
shown in Figure 4. The plants represent hypothetical varieties of Lychnis coronaria [47] with red, blue, 
and white .owers. Plant size values returned by the ecosystem simulation were quantized to seven representative 
values for each plant variety. The quantized values were mapped to the age of the modeled plants. The 
scene obtained after 99 simulation steps had 16,354 plants. The rayshade.le representing this scene without 
instancing would be 3.5 GB (estimated); with instancing it was 6.7 MB, resulting in the compression ratio 
of approximately 500:1. For the scene after 164 steps, the corresponding values were: 441 plants, 125 
MB, 5.8 MB, compression 21:1. The mountain meadow (Figure 8 top) was generated by simulating an ecosystem 
of eight species of herbaceous plants, as discussed in Section 5. The distribution of plants is qualitatively 
similar to that shown schematically in Figure 6, but it includes a larger number of smaller plants. The 
individual plants were modeled with a high level of detail, which made it possible to zoom in on this 
scene and view individual plants. The complete scene has approximately 102,522 plants, comprising approximately 
2 .109 primitives (polygons and cylinders). The rayshade .le representing this scene without instancing 
would be 200 GB (estimated), with the instancing it was 151 MB, resulting in a compression ratio of approximately 
1,300:1.  Figure 7: A Lychnis coronaria .eld after 99 and 164 simulation steps The time needed to model 
this scene on a 150 MHz R5000 Silicon Graphics Indy with 96 MB RAM was divided as follows: simulation 
of the ecosystem (25 steps): 35 min, quantization (two-dimensional parameter space, each of the 8 plant 
species quantized to 7 levels): 5 min, generation of the 56 representative plants using cpfg:9 min. The 
rendering time using rayshadeon a 180 MHz R10000 Silicon Graphics Origin 200 with 256 MB RAM (1024.756 
pixels, 4 samples per pixel) was approximately 8 hours. (It was longer using toro, but in that case the 
rendering time did not depend critically on the amount of RAM.) In the next example, the paradigm of 
parameterizing, quantizing, and instancing was applied to entire groups of plants: tufts of grass with 
daisies. The number of daisies was controlled by a parameter (Figure 9). The resulting lawn is shown 
in Figure 10. For this image, ten different sets of grass tufts were generated, each instanced twenty 
times on average. The total reduction in geometry due to quantization and instancing (including instancing 
of grass blades and daisies within the tufts) was by a factor of 130:1. In Figure 11, a model parameter 
was used to control the size of the heaps of leaves. The heap around the stick and the stick itself were 
modeled manually. Interactive creation of complex scenes requires the proper use of techniques to achieve 
an aesthetically pleasing result. To illustrate the process that we followed, we retrace the steps that 
resulted in the stream scene shown in Figure 15. We began with the de.nition of a hilly terrain crossed 
by a little stream (Figure 2). To cover it with plants, we .rst created procedural models of plant species 
.tting this environment (Figure 12). Next, we extracted images representing the terrain altitudes and 
the stream position (Figures 13a and 13b) from the original terrain data. This Figure 8: Zooming in 
on a mountain meadow provided visual cues needed while painting plant distributions, for example, to 
prevent plants from being placed in the stream. After that, we interactively chose a viewpoint, approximately 
at hu­man height. With the resulting perspective view of the terrain as a reference, we painted a gray 
scale image for each plant species to de.ne its distribution. We placed vegetation only in the areas 
visible from the selected viewpoint to speed up the rendering later on. For example, Figure 13c shows 
the image that de.nes the den­sity distribution of stinging nettles. Since the stinging nettles grow 
on wet ground, we speci.ed high density values along the stream. The density image served as input to 
densedis, which deter­mined positions of individual plants. The dot diagram produced by densedis(Figure 
13d) provided visual feedback that was used to re.ne the density image step by step until the desired 
distribution was found. Figure 9: Grass tufts with varying daisy concentrations a bcd  Figure 12: 
Sample plant models used in the stream scene. Top row: apple, stinging nettle, dandelion; bottom row: 
grass tuft, reed, yellow .ower. Figure 13: Creating distribution of stinging nettle: the heightmap of 
the covered area (a), the river image (b), the plant density distribution painted by the user (c), and 
the resulting plant positions (d). Once the position of plants was established, we employed additional 
parameters to control the appearance of the plants. The vigor of stinging nettle plants, which affects 
the length of their stems and the number of leaves, was controlled using the density image for the nettles. 
To control the vigor of grass we used the height map: as a result, grass tufts have a slightly less saturated 
color on top of the hill than in the lower areas. Each tuft was oriented along a weighted sum of the 
terrain s surface normal vector and the up vector. At this point, the scene was previewed and further 
changes in the density image were made until a satisfying result was obtained.  Figure 14 shows the 
preview of the distribution of stinging nettles and yellow .owers. To prevent intersections between these 
plants, the painted density image for the yellow .owers was multiplied by the inverted density image 
for the nettles. The apple trees were placed by painting black dots on a white image. The .nal scene 
(Figure 15) was rendered using fshade.Images representing each species were rendered separately, and 
the result­ing sub-scenes were composited as described in Section 7. The clouds were then added using 
a real photograph as a texture map. To increase the impression of depth in the scene, color saturation 
and contrast were decreased with increasing depth in a postprocessing step, and colors were shifted towards 
blue. Table 1 provides statis­tics about the instancing and geometric compression for this scene. The 
creation of this image took two days plus one day for de.ning the plant models. The actual compute time 
needed to synthesize this scene on a 195 MHz R10000 8-processor Silicon Graphics Onyx with 768MB RAM 
(1024 .756 pixels, 9 samples per pixel) was 75 min. Figures 16 and 17 present further examples of scenes 
with interac­tively created plant distributions. To simulate the effect of shad­owing on the distribution 
of the yellow .owers in Figure 16, we rendered a top view of the spheres that approximate the shape of 
the apple trees, and multiplied the resulting image (further modi.ed in­teractively) with the initial 
density image for the yellow .owers. We followed a similar strategy in creating Figure 17: the most impor­tant 
trees were positioned .rst, then rendered from above to provide visual cues for the further placements. 
Table 2 contains statistics about the geometry quantization in Figure 17. plant obj. inst. plant obj. 
inst. apple 1 4 grass tuft 15 2577 reed 140 140 stinging nettle 10 430 dandelion 10 55 yellow .ower 10 
2751 Table 1: Number of prototype objects and their instances in the stream scene (Figure 15). Number 
of polygons without instancing: 16,547,728, with instancing: 992,216. Compression rate: 16.7:1. plant 
obj. inst. plant obj. inst. weeping willow 16 16 reed 15 35 birch 43 43 poppy 20 128 distant tree 20 
119 corn.ower 72 20 St. John s wort 20 226 dandelion 20 75 grass tuft 15 824 Table 2: Number of prototype 
objects and their instances in the Dutch scene (Figure 17). Number of polygons without instancing: 40,553,029, 
with instancing: 6,737,036. Compression rate: 6.0:1  9 CONCLUSIONS We presented the design and experimental 
implementation of a sys­tem for modeling and rendering scenes with many plants. The cen­tral issue of 
managing the complexity of these scenes was addressed with a combination of techniques: the use of different 
levels of ab­straction at different stages of the modeling and rendering pipeline, procedural modeling, 
approximate instancing, and the employment of space-and time-ef.cient rendering methods. We tested our 
sys­tem by generating a number of visually complex scenes. Conse­quently, we are con.dent that the presented 
approach is operational and can be found useful in many practical applications. Our work is but an early 
step in the development of techniques for creating and visualizing complex scenes with plants, and the 
pre­sented concepts require further research. A fundamental problem is the evaluation of the impact of 
quantization and approximate in­stancing on the generated scenes. The dif.culty in studying this problem 
stems from: (i) the dif.culty in generating non-instanced reference images for visual comparison purposes 
(the scenes are too large), (ii) the lack of a formally de.ned error metric needed to eval­uate the artifacts 
of approximate instancing in an objective manner, and (iii) the dif.culty in generalizing results that 
were obtained by the analysis of speci.c scenes. A (partial) solution to this prob­lem would set the 
stage for the design and analysis of methods that may be more suitable for quantizing plants than the 
general-purpose variance-based algorithm used in our implementation. Other research problems exposed 
by our experience with EcoSys include: (i) improvement of the terrain model through its coupling with 
the plant population model (in nature vegetation affects terrain, for example by preventing erosion); 
(ii) design of algorithms for converting plant densities to positions, taking into account statistical 
properties of plant distributions found in natural ecosystems [66]); (iii) incorporation of morphogenetic 
plasticity (dependence of the plant shape on its neighbors [58]) into the multi-level modeling framework; 
this requires transfer of information about plant shapes between the population model and the procedural 
plant models; (iv) extension of the modeling method presented in this paper to ani­mated scenes (with 
growing plants and plants moving in the wind); (v) design of methods for conveniently previewing scenes 
with bil­lions of geometric primitives (for example, to select close views of details); and (vi) application 
of more faithful local and global illumination models to the rendering of plant scenes (in particular, 
consideration of the distribution of diffuse light in the canopy).  Acknowledgements We would like to 
acknowledge Craig Kolb for his implementation of the variance-based quantization algorithm, which we 
adapted to the needs of our system, and Christain Jacob for his experi­mental implementations and discussions 
pertinent to the individual­based ecosystem modeling. We also thank: Stefania Bertazzon, Jim Hanan, Richard 
Levy, and Peter Room for discussions and point­ers to the relevant literature, the referees for helpful 
comments on the manuscript, Chris Prusinkiewicz for editorial help, and Darcy Grant for system support 
in Calgary. This research was sponsored in part by the National Science Foundation grant CCR-9508579­001 
to Pat Hanrahan, and by the Natural Sciences and Engineering Research Council of Canada grant OGP0130084 
to Przemyslaw Prusinkiewicz. REFERENCES [1] Alias/Wavefront; a division of Silicon Graphics Ltd. Studio 
V8. SGI program, 1996. [2] AnimaTek, Inc. AnimatTek s World Builder. PC program, 1996. [3] R. A. Armstrong. 
A comparison of index-based and pixel-based neigh­borhood simulations of forest growth. Ecology, 74(6):1707 
1712, 1993. [4] A. C. Beers, M. Agrawala, and N. Chaddha. Rendering from com­pressed textures. In SIGGRAPH 
96 Conference Proceedings, pages 373 378, August 1996. [5] B. M. Blumberg and T. A. Galyean. Multi-level 
direction of au­tonomous creatures for real-time virtual environments. In SIGGRAPH 95 Conference Proceedings, 
pages 47 54, August 1995. [6] B.N. Boots. Spatial tesselations: concepts and applications of Voronoi 
diagrams. John Wiley, 1992. [7] A. Brownbill. Reducing the storage required to render L-system based 
models. Master s thesis, University of Calgary, October 1996. [8] N. Chiba, K. Muraoka, A. Doi, and J. 
Hosokawa. Rendering of forest scenery using 3D textures. The Journal of Visualization and Computer Animation, 
8:191 199, 1997. [9] Adobe Corporation. Adobe Photoshop. [10] O. Deussen and B. Lintermann. A modelling 
method and user interface for creating plants. In Proceedings of Graphics Interface 97, pages 189 197, 
May 1997. [11] J. Dorsey, H. Køhling Pedersen, and P. Hanrahan. Flow and changes in appearance. In SIGGRAPH 
96 Conference Proceedings, pages 411 420, August 1996. [12] T. Duff. Compositing 3-D rendered images. 
Computer Graphics (SIGGRAPH 85 Proceedings), 19(3):41 44, 1985. [13] F. G. Firbank and A. R. Watkinson. 
A model of interference within plant monocultures. Journal of Theoretical Biology, 116:291 311, 1985. 
[14] K. W. Fleischer, D. H. Laidlaw, B. L. Currin, and A. H. Barr. Cellular texture generation. In SIGGRAPH 
95 Conference Proceedings, pages 239 248, August 1995. [15] R. W. Floyd and L. Steinberg. An adaptive 
algorithm for spatial gray scale. In SID 75, Int. Symp. Dig. Tech. Papers, pages 36 37, 1975. [16] D. 
R. Fowler, P. Prusinkiewicz, and J. Battjes. A collision-based model of spiral phyllotaxis. Computer 
Graphics (SIGGRAPH 92 Proceed­ings), 26(2):361 368, 1992. [17] G. Y. Gardner. Simulation of natural scenes 
using textured quadric surfaces. Computer Graphics (SIGGRAPH 84 Proceedings), 18(3):11 20, 1984. [18] 
A. Gersho and R. M. Gray. Vector quantization and signal compres­sion. Kluwer Academic Publishers, 1991. 
[19] D. G. Green. Modelling plants in landscapes. In M. T. Michalewicz, editor, Plants to ecosystems. 
Advances in computational life sciences I, pages 85 96. CSIRO Publishing, Melbourne, 1997. [20] J. C. 
Hart and T. A. DeFanti. Ef.cient anti-aliased rendering of 3D linear fractals. Computer Graphics (SIGGRAPH 
91 Proceedings), 25:91 100, 1991. [21] J.C. Hart. The object instancing paradigm for linear fractal modeling. 
In Proceedings of Graphics Interface 92, pages 224 231, 1992. [22] P. Heckbert. Color image quantization 
for frame buffer display. Com­puter Graphics (SIGGRAPH 82 Proceedings), 16:297 307, 1982. [23] S. I. 
Higgins and D. M. Richardson. A review of models of alien plant spread. Ecological Modelling, 87:249 
265, 1996. [24] H. Hoppe. View-dependent re.nement of progressive meshes. In SIG-GRAPH 97 Conference 
Proceedings, pages 189 198, August 1997. [25] D. H. House, G. S. Schmidt, S. A. Arvin, and M. Kitagawa-DeLeon. 
Visualizing a real forest. IEEE Computer Graphics and Applications, 18(1):12 15, 1998. [26] J. T. Kajiya 
and T. L. Kay. Rendering fur with three dimensional tex­tures. Computer Graphics (SIGGRAPH 89 Proceedings), 
23(3):271 289, 1989. [27] T. L. Kay and J. T. Kajiya. Ray tracing complex scenes. Computer Graphics (SIGGRAPH 
86 Proceedings), 20(4):269 278, 1986. [28] A. D. Kelley, M. C. Malin, and G. M. Nielson. Terrain simulation 
using a model of stream erosion. Computer Graphics (SIGGRAPH 88 Proceedings), 22(4):263 268, 1988. [29] 
C. Kolb. Rayshade. http://graphics.stanford.edu/.cek/rayshade. [30] M. P. Kumler. An intensive comparison 
of triangulated irregular net­works (TINs) and digital elevation models (DEMs). Cartographica, 31(2), 
1994. [31] M. Levoy and P. Hanrahan. Light .eld rendering. In SIGGRAPH 96 Conference Proceedings, pages 
31 42, August 1996. [32] B. Lintermann and O. Deussen. Interactive structural and geometrical modeling 
of plants. To appear in the IEEE Computer Graphics and Applications. [33] B. Lintermann and O. Deussen. 
Interactive modelling and animation of natural branching structures. In R. Boulic and G. H´ egron, editors, 
Computer Animation and Simulation 96. Springer, 1996. [34] Lucas.lm Ltd. The Adventures of Andr´e and 
Wally B. Film, 1984. [35] D. Marshall, D. S. Fussel, and A. T. Campbell. Multiresolution render­ing of 
complex botanical scenes. In Proceedings of Graphics Interface 97, pages 97 104, May 1997. [36] N. Max. 
Hierarchical rendering of trees from precomputed multi-layer Z-buffers. In X. Pueyo and P. Schr¨ oder, 
editors, Rendering Techniques 96, pages 165 174 and 288. Springer Wien, 1996. [37] N. Max and K. Ohsaki. 
Rendering trees from precomputed Z-buffer views. In P. M. Hanrahan and W. Purgathofer, editors, Rendering 
Techniques 95, pages 74 81 and 359 360. Springer Wien, 1995. [38] F. K. Musgrave, C. E. Kolb, and R. 
S. Mace. The synthesis and rendering of eroded fractal terrains. Computer Graphics (SIGGRAPH 89 Proceedings), 
23(3):41 50, 1989. [39] R. M.Visual models of plants interacting ech and P. Prusinkiewicz. with their 
environment. In SIGGRAPH 96 Conference Proceedings, pages 397 410, August 1996. [40] F. Neyret. A general 
and multiscale model for volumetric textures. In Proceedings of Graphics Interface 95, pages 83 91, 1995. 
[41] F. Neyret. Synthesizing verdant landscapes using volumetric textures. In X. Pueyo and P.Schro¨ oder, 
editors, Rendering Techniques 96, pages 215 224 and 291, Wien, 1996. Springer-Verlag. [42] K. Perlin. 
An image synthesizer. Computer Graphics (SIGGRAPH 85 Proceedings), 19(3):287 296, 1985. [43] M. Pharr, 
C. Kolb, R. Gershbein, and P. Hanrahan. Rendering com­plex scenes with memory-coherent ray tracing. In 
SIGGRAPH 97 Conference Proceedings, pages 101 108, August 1997. [44] P. Prusinkiewicz. Visual models 
of morphogenesis. Arti.cial Life, 1(1/2):61 74, 1994. [45] P. Prusinkiewicz. Modeling spatial structure 
and development of plants: a review. Scientia Horticulturae, 74(1/2), 1998. [46] P. Prusinkiewicz, M. 
Hammel, and E. Mjolsness. Animation of plant development. In SIGGRAPH 93 Conference Proceedings, pages 
351 360, August 1993. [47] P. Prusinkiewicz and A. Lindenmayer. The algorithmic beauty of plants. Springer-Verlag, 
New York, 1990. With J. S. Hanan, F. D. Fracchia, D. R. Fowler, M. J. M. de Boer, and L. Mercer. [48] 
P. Prusinkiewicz, W. Remphrey, C. Davidson, and M. Hammel. Model­ing the architecture of expanding Fraxinus 
pennsylvanica shoots using L-systems. Canadian Journal of Botany, 72:701 714, 1994. [49] Questar Productions, 
LLC. World Construction Set Version 2. PC program, 1997. [50] W. T. Reeves and R. Blau. Approximate and 
probabilistic algorithms for shading and rendering structured particle systems. Computer Graphics (SIGGRAPH 
85 Proceedings), 19(3):313 322, 1985. [51] C. W. Reynolds. Flocks, herds, and schools: A distributed 
behavioral model. Computer Graphics (SIGGRAPH 87 Proceedings), 21(4):25 34, 1987. [52] R. E. Ricklefs. 
Ecology. Third Edition. W. H. Freeman, New York, 1990. [53] J. Shade, D. Lischinski, D. Salesin, T. DeRose, 
and J. Snyder. Hierar­chical image caching for accelerated walkthroughs of complex envi­ronments. In 
SIGGRAPH 96 Conference Proceedings, pages 75 82, August 1996. [54] H. Sinoquet and R. Rivet. Measurement 
and visualization of the archi­tecture of an adult tree based on a three-dimensional digitising device. 
Trees, 11:265 270, 1997. [55] A. R. Smith. Plants, fractals, and formal languages. Computer Graph­ics 
(SIGGRAPH 84 Proceedings), 18(3):1 10, 1984. [56] J. M. Snyder and A. H. Barr. Ray tracing complex models 
containing surface tessellations. Computer Graphics (SIGGRAPH 87 Proceed­ings), 21(4):119 128, 1987. 
[57] R. R. Sokal and F. J. Rohlf. Biometry. Third Edition. W. H. Freeman, New York, 1995. [58] K. A. 
Sorrensen-Cothern, E. D. Ford, and D. G. Sprugel. A model of competition incorporating plasticity through 
modular foliage and crown development. Ecological Monographs, 63(3):277 304, 1993. [59] I. E. Sutherland. 
Sketchpad: A man-machine graphical communication system. Proceedings of the Spring Joint Computer Conference, 
1963. [60] J. H. M. Thornley and I. R. Johnson. Plant and crop modeling: A math­ematical approach to 
plant and crop physiology. Oxford University Press, New York, 1990. [61] S. J. Wan, S. K. M. Wong, and 
P. Prusinkiewicz. An algorithm for mul­tidimensional data clustering. ACM Trans. Math. Software, 14(2):135 
162, 1988. [62] A. Watt and M. Watt. Advanced animation and rendering techniques: Theory and practice. 
Addison-Wesley, Reading, 1992. [63] J. Weber and J. Penn. Creation and rendering of realistic trees. 
In SIG-GRAPH 95 Conference Proceedings, pages 119 128, August 1995. [64] L. Williams. Casting curved 
shadows on curved surfaces. Computer Graphics (SIGGRAPH 78 Proceedings), 12(3):270 274, 1978. [65] L. 
Williams. Shading in two dimensions. In Proceedings of Graphics Interface 91, pages 143 151, June 1991. 
[66] H. Wu, K. W. Malafant, L. K. Pendridge, P. J. Sharpe, and J. Walker. Simulation of two-dimensional 
point patterns: application of a latice framework approach. Ecological Modelling, 38:299 308, 1997. 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280922</article_id>
		<sort_key>287</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>28</seq_no>
		<title><![CDATA[A multiscale model of adaptation and spatial vision for realistic image display]]></title>
		<page_from>287</page_from>
		<page_to>298</page_to>
		<doi_number>10.1145/280814.280922</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280922</url>
		<keywords>
			<kw><![CDATA[adaptation]]></kw>
			<kw><![CDATA[realistic imaging]]></kw>
			<kw><![CDATA[spatial vision]]></kw>
			<kw><![CDATA[tone reproduction]]></kw>
			<kw><![CDATA[visual perception]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.0</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor>Human factors</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Viewing algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14020467</person_id>
				<author_profile_id><![CDATA[81350599694]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sumanta]]></first_name>
				<middle_name><![CDATA[N.]]></middle_name>
				<last_name><![CDATA[Pattanaik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cornell Univ., Ithaca, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P132198</person_id>
				<author_profile_id><![CDATA[81100459651]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Ferwerda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cornell Univ., Ithaca, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P190169</person_id>
				<author_profile_id><![CDATA[81100503839]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Fairchild]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cornell Univ., Ithaca, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P68459</person_id>
				<author_profile_id><![CDATA[81100196982]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Donald]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Greenberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cornell Univ., Ithaca, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Adelson, E.H. (1993) Perceptual organization and judgment of brightness. Science, 262, 2042-2044.]]></ref_text>
				<ref_id>Adelson93</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Barlow, H.B. (1972) Dark and Light Adaptation: Psychophysics. In D. Jameson and L. Hurvich (Eds.), Handbook of Sensory Physiology, V. 7(4), New York: Springer, 2-27.]]></ref_text>
				<ref_id>Barlow72</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Brady, N. and Field, D.J. (1995) What's Constant in Contrast Constancy? The Effects of Scaling on the Perceived Contrast of Bandpass Patterns. Vision Res., 35(6), 739-756.]]></ref_text>
				<ref_id>Brady95</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Burt, RJ., and Adelson, E.H. (1983) The Laplacian Pyramid as a Compact Image Code. IEEE Transaction on Communication, 31(4), 532-540.]]></ref_text>
				<ref_id>Burt83</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Chiu, K., Herf, M., Shirley, R, Swamy, S., Wang, C., and Zimmerman, K. (1993) Spatially Nonuniform Scaling Functions for High Contrast Images. Proceedings Graphics Interface 93,245-254.]]></ref_text>
				<ref_id>Chiu93</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258884</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Debevec, RE. and Malik, J. (1997) Recovering High Dynamic Range Radiance Maps from Images. Proceedings SIGGRAPH 97,369-378.]]></ref_text>
				<ref_id>Debevec97</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Fairchild, M.D. (1998) Color Appearance Models. Reading, MA: Addison-Wesley.]]></ref_text>
				<ref_id>Fairchild98</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237262</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Ferwerda, J.A., Pattanaik, S.N., Shirley, R, and Greenberg, D. (1996) A Model of Visual Adaptation for Realistic Image Synthesis. Proceedings SIG- GRAPH 96, 249-258.]]></ref_text>
				<ref_id>Ferwerda96</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Georgeson, M.A. and Sullivan, G.D. (1975) Contrast Constancy: Deblurring in Human Vision by Spatial Frequency Channels. J. Physiol., 252, 627- 656.]]></ref_text>
				<ref_id>Georgeson75</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Gilchrist, A.L. (1977) Perceived Lightness Depends on Perceived Spatial Arrangement. Science, 195, 185-187.]]></ref_text>
				<ref_id>Gilchrist77</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Hunt, R.W.G. (1995) The Replvduction of Color. 5th edition, Kingstonupon-Thames, England: Fountain Press.]]></ref_text>
				<ref_id>Hunt95</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Hurvich, L. (1981) Color Vision. Sunderland, MA: Sinauer Assoc.]]></ref_text>
				<ref_id>Hurvich81</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Jobson, D.J., Rahman, Z., and Woodell, G.A. (1996) Retinex Image Processing: Improved Fidelity to Direct Visual Observation. Proceedings 4th Color Imaging Conference, Society for Imaging Science and Technology, 124-126.]]></ref_text>
				<ref_id>Jobson96</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Laming D. (1991) Contrast Sensitivity. In J.J. Kulikowski, V. Walsh, and I.J. Murray (Eds.) Limits' of Vision, Vol. 5, Vision and Visual Dysfunction. Boca Raton, FL, CRC Press, 35-43.]]></ref_text>
				<ref_id>Laming91</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Lubin, J. (1995). A Visual Discrimination Model for Imaging System Design and Evaluation. In E. Peli (Ed.) Vision Models for Target Detection. Singapore, World Scientific, 245-283.]]></ref_text>
				<ref_id>Lubin95</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Losada, M.A., and Mullen, K.T. (1994) The Spatial Tuning of Chromatic Mechanisms Identified by Simultaneous Masking. Vision Res., 34(3), 331-341.]]></ref_text>
				<ref_id>Losada94</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Mullen, K.T. (1985) The Contrast Sensitivity of Human Color Vision to Red-Green and Blue-Yellow Chromatic Gratings. J. Physiol., 359, 381-400.]]></ref_text>
				<ref_id>Mullen85</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Peli, E. (1990) Contrast in Complex Images. J. Opt. Soc. Am. A, 7(10), 2032- 2040.]]></ref_text>
				<ref_id>Peli90</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Schlick, C. (1995) Quantization Techniques for High Dynamic Range Pictures. In G. Sakas, R Shirley, and S. Mueller, (Eds.), Photorealistic Rendering Techniques, Berlin: Springer-Verlag, 7-20.]]></ref_text>
				<ref_id>Schlick95</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Shapley, R. and Enroth-Cugell, C. (1984) Visual Adaptation and Retinal Gain Controls. In N. Osborne and G. Chader (Eds.). P~vgress in Retinal Resemvh, V. 3, Oxford: Pergamon Press., 263-347.]]></ref_text>
				<ref_id>Shapley84</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218466</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Spencer, G., Shirley, P., Zimmerman, K., Greenberg, D. P. (1995) Physically-based Glare Effects for Computer Generated Images. Proceedings SIG- GRAPH 95,325-334.]]></ref_text>
				<ref_id>Spencer95</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Stevens, S.S. (1961) To Honor Fechner and Repeal His Law. Science, 133, 13 Jan., 80-86.]]></ref_text>
				<ref_id>Stevens61</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617873</ref_obj_id>
				<ref_obj_pid>616030</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Tumblin, J., and Rushmeier, H. (1993) Tone Reproduction for Realistic Images, IEEE Computer Graphics and Applications, 13(6), 42-48.]]></ref_text>
				<ref_id>Tumblin93</ref_id>
			</ref>
			<ref>
				<ref_obj_id>259241</ref_obj_id>
				<ref_obj_pid>259081</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Tumblin, J., Hodgkins, J. and Guenter, B. (1997) Display of High Contrast Images Using Models of Visual Adaptation. Visual proceedings SIGGRAPH 97, 154.]]></ref_text>
				<ref_id>Tumblin97</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[van Nes F.L. and Bouman M.A. (1967) Spatial Modulation Transfer in the Human Eye. J. Opt. Soc. Am., 57,401-406.]]></ref_text>
				<ref_id>vanNes67</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[van der Horst, G.J.C. and Bouman, M.A. (1969) Spatiotemporal Chromaticity Discrimination. J. Opt. Soc. Am., 59(11), 1482-1488.]]></ref_text>
				<ref_id>vanderHorst69</ref_id>
			</ref>
			<ref>
				<ref_obj_id>180934</ref_obj_id>
				<ref_obj_pid>180895</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Ward, G. (1994) A Contrast-based Scalefactor for Luminance Display. In RS. Heckbert (Ed.), Graphics Gems IV, Boston: Academic Press Professional.]]></ref_text>
				<ref_id>Ward94</ref_id>
			</ref>
			<ref>
				<ref_obj_id>614380</ref_obj_id>
				<ref_obj_pid>614268</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Ward-larson, G., Rushmeier, H., and Piatko, C. (1997) A Visibility Matching Tone Reproduction Operator for High Dynamic Range Scenes. IEEE Transactions on Visualization and Computer Graphics, 3(4), 291-306.]]></ref_text>
				<ref_id>Ward-Larson97</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Watson, A.B. and Solomon, J.A. (1997) Model of Visual Contrast Gain Control and Pattern Masking. J. Opt. Soc. Am. A, 14(9), 2379-2391.]]></ref_text>
				<ref_id>Watson97</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Westheimer, G. (1986) The Eye as an Optical Instrument. In K. Boff, L. Kaufman, and J. Thomas (Ed.), Handbook of Pelveption and Human Pelformance, New York: Wiley and Sons.]]></ref_text>
				<ref_id>Westheimer86</ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Wilson, H.R. (1991). Psychophysical Models of Spatial Vision and Hyperacuity, in D. Regan (Ed.) Spatial Vision, Vol. 10, Vision and Visual Dysfunction. Boca Raton, FL, CRC Press, 64-81.]]></ref_text>
				<ref_id>Wilson91</ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Wyszecki G., and Stiles W.S. (1982) Color Science: Concepts and Methods, Quantitative Data and Formulae (2nd edition). New York: Wiley.]]></ref_text>
				<ref_id>Wyszecki82</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Copyright &#38;#169;1998 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers or to redistribute to lists, requires specific permission and/or a fee. A Multiscale Model 
of Adaptation and Spatial Vision for Realistic Image Display Sumanta N. Pattanaik James A. Ferwerda Mark 
D. Fairchild* Donald P. Greenberg Program of Computer Graphics , Cornell University  Abstract In this 
paper we develop a computational model of adaptation and spatial vision for realistic tone reproduction. 
The model is based on a multiscale representation of pattern, luminance, and color pro­cessing in the 
human visual system. We incorporate the model into a tone reproduction operator that maps the vast ranges 
of radiances found in real and synthetic scenes into the small .xed ranges avail­able on conventional 
display devices such as CRT s and printers. The model allows the operator to address the two major problems 
in realistic tone reproduction: wide absolute range and high dynamic range scenes can be displayed; and 
the displayed images match our perceptions of the scenes at both threshold and suprathresh­old levels 
to the degree possible given a particular display device. Although in this paper we apply our visual 
model to the tone re­production problem, the model is general and can be usefully ap­plied to image quality 
metrics, image compression methods, and perceptually-based image synthesis algorithms. CR Categories: 
I.3.0 [Computer Graphics]: General; Keywords: realistic imaging, visual perception, tone reproduc­tion, 
adaptation, spatial vision 1 INTRODUCTION The range of light we encounter in natural scenes is vast. 
The ab­solute level of illumination provided by direct sunlight can be 100 million times more intense 
than starlight. The dynamic range of light energy can also be large, on the order of 10,000 to 1 from 
highlights to shadows, or higher if light sources are visible. Although physically-based rendering methods 
and new tech­niques that utilize the output of digital cameras [Debevec97] now allow us to produce radiance 
maps that accurately represent the wide variations of light energy in scenes, neither of these methods 
specify how to realistically display these images on conventional *On sabbatical leave from: Munsell 
Color Science Laboratory, Cen­ter for Imaging Science, Rochester Institute of Technology, 54 Lomb Memorial 
Drive, Rochester, NY 14623-5604, USA. Web address: http://www.cis.rit.edu/people/faculty/fairchild 580 
Rhodes Hall, Cornell University, Ithaca, NY 14853, USA. E-mail addresses: {sumant,jaf,mdf,dpg}@graphics.cornell.edu 
Web address: http://www.graphics.cornell.edu. electronic and print-based media which have only moderate 
output levels and typical dynamic ranges of less than 100 to 1. Recently graphics researchers have started 
to address this issue by developing tone reproduction operators that map scene radi­ances to display 
outputs with the goal of producing a visual match between the scene and the display. There are two major 
problems to be solved in realistic tone reproduction: to .nd an operator that maps the vast ranges of 
radiances found in scenes into the range that can be produced by a given display device.  to be certain 
that this operator produces images that match our perceptions of the scenes.  The critical element that 
links these two problems is the visual model used in the tone reproduction operator. Visual models are 
used to relate the perceptual responses of a scene observer to the responses of the display observer 
in order to specify a mapping that produces a visual match between the scene and the display. A cen­tral 
issue is that different tone reproduction operators have made use of different visual models to determine 
what constitutes a match. Tumblin and Rushmeier s [1993] operator is based on Stevens [1961] model of 
brightness and contrast perception illustrated in Figure 1b. The operator attempts to produce images 
that capture the changes in suprathreshold brightness and apparent contrastthat occur with changes in 
the level of illumination. Ward [1994] intro­duced an operator based on a model of contrast sensitivity 
derived from threshold vs. intensity (TVI) functions similar to those shown in Figure 1a. Its goal is 
to match the threshold visibility of features in the image to features in the scene. Ferwerda [1996] 
developed an operator based on a model of adaptation that like Ward s matches threshold visibility, but 
also accounts for the changes in visual acu­ity and color discriminability that occur with the changes 
in the level of illumination. Both threshold and suprathreshold models of vision capture im­portant aspects 
of our visual experience, and a realistic tone repro­duction operator should produce a mapping that matches 
both as­pects. Unfortunately, threshold models don t scale well to predict suprathreshold appearance, 
and suprathreshold models don t accu­rately predict visual thresholds. Recently much effort has been 
devoted to developing tone re­production operators for high dynamic range scenes. Chiu [1993], Schlick 
[1995], and Jobson [1996] introduced spatially-varying op­erators that compress high dynamic range scenes 
into the limited range available on display devices, but the ad-hoc visual models they incorporate limits 
what can be said about the visual .delity of the mappings. Tumblin [1997] has recently introduced an 
opera­tor for high dynamic range scenes based on a model of perceptual constancy. Although this operator 
produces attractive images, the model it uses is not quantitative, and therefore the operator can t predict 
whether an image will be a visual match to a scene. Fi­nally, Ward-Larson [1997] has introduced an operator 
that extends the work of Ward [1994] and Ferwerda [1996] with a model of local adaptation, to produce 
a threshold-based operator that can handle  Figure 1: Threshold and suprathreshold models of vision: 
a) Threshold vs. intensity (TVI) functions for the rod and cone systems. The curves plot the smallest 
threshold increment .L necessary to see a spot against a uniform background with luminance L. b) Stevens 
model of suprathreshold brightness and apparent contrast. The curves plot the changes in brightness and 
apparent contrast of gray targets and a white surround as the level of illumination rises (1 Bril = apparent 
brightness of a target with a luminance of 1µLambert). Adapted from [Ferwerda96, Stevens61]. high dynamic 
range scenes, and also match the changes in threshold visibility, visual acuity and color discriminability 
that occur with changes in the level of illumination. Although the innovations introduced in each of 
these operators represent signi.cant advances toward addressing the two funda­mental problems of realistic 
tone reproduction, overall the prob­lems have been attacked piecemeal. Thus some operators can only handle 
achromatic scenes, or scenes illuminated at daylight levels. Others can handle wide absolute ranges of 
illumination in chro­matic scenes, but can t handle high dynamic ranges. Still others can handle full 
ranges of scene radiances, but can t guarantee that the images will match the scenes in any meaningful 
way. Finally, those that do produce visual matches differ on whether they match threshold measures like 
visibility and visual acuity, or suprathresh­old measures like brightness and apparent contrast. Since 
the im­ages these operators produce depend upon the visual models they incorporate, a comprehensive solution 
to the problems of realistic tone reproduction requires a more complete model of visual per­ception. 
In this paper we develop a computational model of adaptation and spatial vision for realistic tone reproduction. 
The model incor­porates a multiscale representation of luminance, pattern, and color processing in human 
vision that accounts for the changes in thresh­old visibility, visual acuity, color discriminability, 
and suprathresh­old brightness, colorfulness, and apparent contrast that occur with changes in scene 
illumination. We incorporate the model into a tone reproduction operator that maps the vast ranges of 
radiances found in real and synthetic scenes into the small .xed ranges avail­able on conventional display 
devices such as CRT s and printers. The model allows the operator to address the two major problems in 
realistic tone reproduction: images of wide absolute range and high dynamic range scenes can be displayed; 
and the displayed images match our perceptions of the scenes at both threshold and suprathreshold levels 
to the limits possible on a given display.  2 BACKGROUND 2.1 Adaptation and Visual Thresholds The range 
of light we encounter in natural scenes is vast, but the responsive range of the neurons that make up 
the visual system is small. Our visual system copes with this vast range of illumination through adaptation. 
Although adaptation allows the visual system to function over a wide range of illumination levels, this 
does not mean that we see equally well at all levels. At low, scotopic levels our eyes are very sensitive 
and can detect small luminance differ­ences, however visual acuity and the ability to discriminate colors 
are both poor. In contrast, at high, photopic levels, we have sharp color vision, but absolute sensitivity 
is low and luminance differ­ences have to be large to be detectable. To produce realistic images that 
capture the visual appearance of scenes we need to understand these adaptation-related changes in vision. 
The effects of adaptation on visual sensitivity have been mea­sured in threshold experiments. Figure 
1a shows the results of a threshold experiment that measured the changes in visibility that occur with 
changes in the level of illumination. The curves plot the smallest luminance increment .L that can be 
detected at a par­ticular background luminance L and are known as threshold-vs.­intensity (TVI) functions. 
The two curves show the TVI functions for the rod and cone systems. Over a wide range of background luminances, 
the size of the threshold increment increases in proportion to the background lu­minance making the functions 
linear on a log-log scale. This linear relationship .L = kL is known as Weber s law and indicates that 
the visual system has constant contrast sensitivity since the Weber contrast .L/L is constant over this 
range. Constant contrast sensitivity is a desirable attribute for the vi­sual system to have, since contrast 
in the retinal image is a func­tion of surface re.ectances and is invariant with respect to changes in 
the level of illumination. This discounting of the illuminant through adaptation is a major factor in 
perceptual constancy which underlies our ability to recognize objects under different illumi­nation conditions 
[Shapley84]. Weber-like adaptation processes within the different cone systems (known as von Kries adaptation 
[Wyszecki82]) can also help explain chromatic adaptation and the perceived constancy of surface colors 
as the chromaticity of the il­luminant changes. 2.2 Threshold Models of Spatial Vision Although the 
TVI functions shown in Figure 1a give us useful in­formation about the changes in visual sensitivity 
that occur with Figure 2: Threshold contrast sensitivity as a function of spatial frequency for a monochromatic 
luminance grating (.; green; 526nm) and a isolu­minant chromatic grating (; red/green; 602, 526nm). Adapted 
from [Mullen85]. changes in the level of illumination, it s dif.cult to understand how to generalize 
from the results of these studies on the detectability of spots on uniform backgrounds to predicting 
the visibility of real objects (e.g. tanks, tumors) in complex scenes. In the 1960 s vi­sion researchers 
began to measure visual sensitivity for sinusoidal grating patterns to understand the properties of spatial 
vision. The contrast sensitivity functions (CSF s) shown in Figure 2 plot visual sensitivity for detecting 
sinusoidal gratings as a func­tion of their spatial frequency. Here sensitivity is de.ned as (1/threshold 
contrast) using the Michaelson de.nition of contrast: (Lmax - Lmin)/(Lmax + Lmin) where Lmax and Lmin 
are the luminances at the peaks and troughs of the gratings [Laming91]. There is substantial evidence 
that the responses of the rod and cone photoreceptors are organized at an early stage in the visual system 
into responses in an achromatic channel sensitive to lumi­nance variations and two chromatic channels, 
one sensitive to vari­ations along a red/green axis and the other sensitive along a yel­low/blue axis 
[Hurvich81]. The two curves in Figure 2 show the CSF s of the achromatic and red/green chromatic channels. 
There are several things to notice about the CSF s. The .rst is that the spatial frequency response of 
the achromatic channel (.) has the characteristics of a bandpass .lter. Contrast sensitivity is highest 
for gratings with frequencies around 2 to 4 cycles/degree of visual angle (cpd) and sensitivity drops 
for both higher and lower spatial frequencies. On the other hand, the spatial frequency re­sponse of 
the red/green chromatic channel () has the characteris­ 2 tic of a lowpass .lter. Sensitivity is good 
for low spatial frequen­cies, but declines at higher frequencies. The contrast sensitivity function of 
the yellow/blue channel shows a similar pattern of re­sponse after correction for chromatic aberration. 
The high frequency cutoffs of the CSF s indicate the limits of spatial resolution in the two channels. 
The achromatic channel has a cutoff at approximately 30 cpd which is in good correspondence with the 
limits of visual acuity measured in clinical tests. The high frequency cutoff for the chromatic channels 
is only around 11 cpd. This means that the chromatic channels have much lower spatial resolution than 
the achromatic channel. The contrast sensitivity functions have been widely used to model the visual 
system s response to complex objects. If the im­age of an object can be described in terms of its sinusoidal 
Fourier components, then the visibility of that object can be measured by applying the contrast sensitivity 
function to the components. When the components are above threshold the object will be seen, when they 
re below threshold it will be invisible. This approach to predicting the visibility of complex objects 
has Figure 3: Contrast sensitivity functions for sinusoidal gratings illuminated at different mean luminance 
levels. Levels are speci.ed in Troland (Td) units of retinal illuminance (Trolands = luminance incd/m2 
x pupil area). Adapted from [vanNes67]. been widely used, but there is a severe limit on its generality 
that is often overlooked which will lead to gross errors in the predictions, namely that all the grating 
patterns used to measure the CSF s have the same mean luminance. While the contrast sensitivity functions 
show how sensitivity varies with spatial frequency, they do not take into account the changes in sensitivity 
caused by adaptation. To account for changes in the visibility of real objects in real scenes, we need 
to understand the interactions of adaptation with threshold spatial vision.  2.3 Adaptation and Threshold 
Spatial Vision The results of a classic study on the effects of adaptation on thresh­old spatial vision 
are shown in Figure 3. van Nes [1967] measured contrast sensitivity functions for achromatic gratings 
illuminated at a wide range of different levels. Each curve in the graph represents the CSF measured 
at a particular luminance level. There are several things to notice in the graph. The .rst is that overall, 
contrast sensitivity improves with the level of illumination. Peak contrast sensitivity changes from 
a value of 8 (threshold con­trast of 12%) at an illumination level of 0.0009 Trolands (Td) to a value 
of 500 (threshold contrast of 0.2%) at 5900 Td. The next thing to notice is that the shape of the CSF 
changes from being lowpass at the lowest illumination levels to being band­pass at higher levels. This 
re.ects the transition from rod vision in the scotopic range to cone vision at photopic levels. The .nal 
thing to notice is that as the level of illumination in­creases, the high frequency cutoff of the CSF 
moves to higher and higher spatial frequencies. This corresponds to the improvement in spatial resolution 
and visual acuity that we experience at higher lu­minance levels. The cutoff changes from about 4 cpd 
at 0.0009 Td to about 50 cpd at 5900 Td which corresponds to an improvement in acuity from around 20/150 
at the lowest level to almost 20/10 at the highest. The curves in Figure 3 show the effects of adaptation 
on spatial contrast sensitivity in the achromatic channel of the visual system. Data from van der Horst 
[1969] shows a similar pattern of results in the chromatic channels. These data begin to give us a clearer 
picture of the interactions between adaptation and threshold spatial vision. From these data we can begin 
to understand in a uni.ed framework, the changes in  Figure 4: Multiscale bandpass mechanisms underlying 
the contrast sensi­tivity functions. Adapted from [Lubin95]. visibility, acuity, and color discrimination 
that occur with changes in the level of illumination. However there is one more aspect of these interactions 
that we need to investigate to have a more com­plete understanding, and this is the effect of local adaptation. 
 2.4 Local Adaptation and Multiscale Models of Threshold Spatial Vision As we look from place to place 
in a scene our eyes adapt locally to the prevailing conditions of illumination. This local adaptation 
greatly enhances our ability to see in high dynamic range scenes where some portions of the scene are 
brightly illuminated and oth­ers are in shadow. How does local adaptation work? Physiological and psychophysical 
evidence now indicates that the early stages of visual processing can be described as the .lter­ing of 
the retinal image by visual mechanisms sensitive to patterns of different scale whose response characteristics 
are bandpass in the spatial frequency domain [Wilson91]. These multiple mecha­nisms are sensitive to 
different ranges of spatial frequencies, and the CSF s that are measured in psychophysical experiments 
are the en­velope of these mechanism sensitivities. Figure 4 shows the achro­matic CSF described in this 
way. Losada [1994] has shown that the chromatic CSF s can be described in a similar way. Now if we look 
back at van Nes s data (Figure 3) on the changes in spatial contrast sensitivity that occur with changes 
in the level of illumination, it can be seen that the CSF curves don t simply shift upwards with increasing 
illumination, but change shape as well. This is a re.ection of the fact that these bandpass mechanisms 
adapt to the average luminance within a region of a scene de.ned by their spatial scale. In a complex 
scene, this average is going to be dif­ferent at different scales so the mechanisms will all be in different 
states of adaptation. Thus local adaptation is not only spatially local within different regions of the 
visual .eld, but is also local with respect to the scale and spatial frequency .ltering characteristics 
of the bandpass mech­anisms involved in early visual processing. Therefore, to correctly account for 
the changes in visual sensitivity that occur with changes in the level of illumination, we need to describe 
the effects of local adaptation at different spatial scales. Peli [1990] has suggested that an appropriate 
way to characterize the effects of local adaptation is to determine the band-limited local contrast at 
each location in the scene. Band-limited local contrast is calculated by .rst .ltering the retinal image 
into a set of band­pass images de.ned by the .lter characteristics of the visual mech- Figure 5: Threshold-vs.-intensity 
functions for spot patterns with different spatial and temporal parameters. Adapted from [Barlow72]. 
anisms, and then dividing the signals in these images by lowpass images that represent the average local 
luminance at each location in the image at different spatial scales. This produces a multiscale representation 
of the image where the signals in each band repre­sent the effective contrasts at each scale, having 
taken the effects of local adaptation into account. Both Peli and Lubin [1995] have shown that this kind 
of representation corresponds well with per­ceived threshold contrasts in complex images.  2.5 Suprathreshold 
Models of Vision Threshold models of vision allow us to de.ne the borders between the visible and the 
invisible. These models have a long and use­ful history in applied vision research, but because threshold 
models only de.ne the limits of vision, they don t really tell us much about ordinary seeing where the 
contrasts, sizes of spatial details, and color saturations are typically well above threshold. To character­ize 
how changes in the level of illumination affect the everyday appearances of objects in scenes, suprathreshold 
models of vision are needed. Stevens [1961] model of brightness and apparent contrast shown in Figure 
1b summarizes much of what is known about the intensity dependence of surface appearance at suprathreshold 
lev­els. Stevens had subjects estimate the apparent brightnesses of gray patches seen against a white 
surround at different illumination lev­els. The brightness of the surround increased as a power function 
(exponent 0.33) of its luminance. The brightnesses of the gray patches either increased, decreased or 
remained constant depend­ing on their contrast with respect to the surround. Overall, the di­verging 
curves quantify a familiar aspect of our visual experience: as we turn up the light, the world becomes 
more vivid. Whites become brighter, blacks become deeper and the whole range of ap­parent contrasts expands. 
Although Stevens only tested achromatic surfaces, Hunt [1995] has measured a related set of phenomena 
for chromatic displays, where the vibrancy and colorfulness of colored surfaces increases at higher levels 
of illumination. While these suprathreshold changes in brightness, colorfulness, and apparent contrast 
are certainly true to our everyday experience, it is dif.cult to reconcile these results with the predictions 
of thresh­old models and Weber s law which show that adaptation produces a visual system with constant 
contrast sensitivity, and imply that apparent suprathreshold contrasts should be constant over changes 
in the level of illumination within the Weber range. Differences in the TVI functions for different kinds 
of visual stimuli suggest a solution to this conundrum.  Figure 6: Suprathreshold contrast constancy 
and non-linear contrast transducers in human vision. Adapted from [Georgeson75, Watson97b]. 2.6 Adaptation 
and Suprathreshold Vision Figure 5 shows photopic TVI functions measured by Barlow [1972] for incremental 
spot patterns with different spatial and temporal characteristics. The lower curve shows thresholds measured 
for a large spot presented in a long exposure. The upper curve shows thresholds for a small, brie.y .ashed 
spot. There are two im­portant differences between the curves. First, threshold values for the large/long 
spot are everywhere lower than thresholds for the small/brief spot. Second, although the slope of the 
large/long TVI curve follows Weber s law at higher background levels, the short/brief TVI has a lower 
slope indicating sub-Weber behavior. The signi.cance of the differences between these two TVI s is that 
low threshold values, and the constant contrast sensitivity im­plied by Weber s law, are only obtained 
under optimal conditions in laboratory experiments, such as those shown in the lower curve in this experiment 
or those given in the experiments that measured the rod and cone TVI s shown in Figure 1a. The TVI function 
for the small/brief spot is more like what we should expect under natural conditions where our eyes are 
continually moving across a complex scene and both the visual stimuli and our state of adaptation will 
be changing rapidly. Here threshold sensitivity is limited by both in­complete adaptation and quantal 
.uctuations in the stimulus, mak­ing thresholds higher-than-optimal, but also producing a TVI with a 
sub-Weber slope, where threshold contrast sensitivity continues to improve at higher illumination levels 
because the magnitude of the visual response to a constant physical contrast increases as the level of 
illumination rises. The insights provided by this experi­ment are the key that allow us to unify the 
predictions of threshold and suprathreshold models in one coherent framework. To com­plete the picture 
we ve been developing of a model that can ex­plain the interactions of adaptation and spatial vision 
at threshold and suprathreshold levels, we need to understand the properties of suprathreshold spatial 
vision. 2.7 Adaptation and Suprathreshold Spatial Vision A CSF describes how threshold contrast sensitivity 
varies for sinu­soidal gratings of different spatial frequencies. The upper curve in Figure 6a shows 
an achromatic CSF measured by Georgeson [1975]. This typical curve shows that we are most sensitive to 
grat­ings with frequencies around 4-5 cpd and that sensitivity falls off at both higher and lower frequencies. 
This CSF was measured as part of a suprathreshold contrast matching experiment. In this experi­ment subjects 
matched the apparent contrast of different frequency test gratings to the apparent contrast of a standard 
grating of 5 cpd. In separate trials, the physical contrast of the standard was varied from less than 
1% to more than 75%. The lower curves in Figure 6a summarize the results. At low standard contrasts, 
the matches followed the form of the threshold CSF. High and low frequency gratings had to have higher 
physical contrast to have the same apparent contrast as the stan­dard. But at standard contrasts above 
about 20% the curves .at­tened out. Gratings matched in apparent contrast when they had the same physical 
contrast. Georgeson called this phenomenon con­trast constancy. The differences in the shapes of the 
threshold CSF and the suprathreshold contrast matching functions indicates the existence of nonlinear 
contrast transduction processes in the visual system. Brady [1995] has suggested that these contrast 
nonlinearities re.ect differences in the signal/noise characteristics of the bandpass visual mechanisms 
which can explain both the curvature of the threshold CSF and the .atness of the suprathreshold contrast 
matching func­tions. Watson and Solomon [1997b] have developed a model of these contrast nonlinearities 
as part of their work on the closely related phenomenon of visual masking. The transducer functions for 
a set of hypothetical bandpass mechanisms are shown in Figure 6b. The horizontal axis indicates the input 
contrast, the vertical axis plots the response. At low input contrast levels, the transducers all have 
different cutoffs. The differences in these cutoffs imply that the mechanisms all have different sensitivities, 
since at any particu­lar low level, the input signal will be above the cutoffs for some mechanisms and 
below for others. This characteristic of the trans­ducer functions accounts for CSF-like responses at 
low contrast lev­els. But at higher input contrast levels, the transducer functions converge, which means 
that given the same input contrast, all the mechanisms will produce the same response. This characteristic 
ac­counts for the contrast constancy observed at higher contrast levels. The action of these transducer 
functions, which mimics the contrast nonlinearity in the visual system, provides a coherent framework 
for understanding threshold and suprathreshold spatial vision. 2.8 Summary In the previous sections 
we have outlined a coherent framework for understanding the changes in vision that occur with changes 
in the level of illumination in scenes. The framework relates research on adaptation with research on 
spatial vision to provide a uni.ed view of variations in threshold performance and suprathreshold appear­ance 
at different illumination levels. The framework allows us to account for the changes in threshold visibility, 
visual acuity, and color discrimination, and suprathreshold brightness, colorfulness, and apparent contrast 
that occur under different illumination condi­tions. The key features of the framework are: Multiscale 
processing of the retinal image by visual mech­anisms in the rod and cone pathways with bandpass spatial 
frequency response characteristics. Adaptation processes operating within these bandpass mech­anisms 
that act as luminance gain controls to produce vi­sual signals that are primarily correlated with scene 
contrasts, but are still luminance dependent and increase in magnitude with increasing luminance. Independent 
adaptation processes within the bandpass mechanisms in the rod and cone path­ways account for local adaptation, 
chromatic adaptation, and changes in sensitivity over the scotopic to photopic range.  Organization 
of the bandpass mechanisms in the rod and cone pathways into an achromatic and two chromatic channels 
with different spatial frequency response characteristics.  Nonlinear transducers operating on the adapted 
outputs of the mechanisms in these achromatic and chromatic channels that scale the visual signals to 
produce CSF-like response at threshold levels, and contrast constancy at suprathreshold lev­els.  In 
the following section we will develop a computational model of vision based on this framework and apply 
it to the problem of realistic tone reproduction. The unique features of the model will allow our tone 
reproduction operator to address the two major prob­lems in realistic tone reproduction: images of wide 
absolute range and high dynamic range scenes can be displayed on conventional display devices; and these 
images should match our perceptions of the scenes at threshold and suprathreshold levels.  3 THE COMPUTATIONAL 
MODEL 3.1 Overview We will now draw on the psychophysical framework outlined in the previous section 
to develop a computational model of adapta­tion and spatial vision for realistic tone reproduction. Figure 
7 pro­vides a .ow chart of each major step in this computational model. The model has two main parts: 
the Visual model and the Display model. The visual model processes an input image to encode the perceived 
contrasts for the chromatic and achromatic channels in their band-pass mechanisms. The display model 
then takes this en­coded information and reconstructs an output image. The model must be inverted in 
order to produce equivalent appearances un­der the viewing conditions of the display device. This procedure 
does not undo the processes of the model since the thresholding and saturation procedures are accomplished 
and the gain control parameters differ for the original scene and the display. The recon­struction process 
creates an output image that reproduces the visual appearance of the input image to the limits possible 
on a given dis­play device. The speci.c computational procedures that were used to implement each step 
of the model are described below. A pic­torial representation of the signals at each stage of the model 
is presented in Figure 8. 3.2 Input Image Preprocessing Prior to applying the visual model, certain 
preprocessing steps are required to assure that the image data appropriately correspond to the information 
accessible to the early stages of the human visual system. First, the image must be spatially sampled 
such that the band pass signals represent appropriate spatial frequencies. For this implementation, band 
pass mechanisms with peak spatial frequen­cies of 16, 8, 4, 2, 1, and 0.5 cpd were required. The spatial 
sam­pling necessary to produce these band-pass signals depends upon the Gaussian .lters chosen for the 
image decomposition. With the scene representation Figure 7: Flow chart of the computational model of 
adaptation and spatial vision for realistic tone reproduction. .lter described in Section 3.3 it is necessary 
to sample the image at a rate of 130 pixels/degree. The next step is to introduce compensations for optical 
point­spread in the eye and disability glare. Optical point-spread is incor­porated via a convolution 
with a function described by Westheimer [1986] and glare effect is introduced via convolutions with func­tions 
described by Spencer [1995]. The image must then be spectrally sampled to represent the vi­sual system 
s initial photoreceptor responses. This is accomplished by integrating the spectral radiance distribution 
for each pixel after multiplication by the spectral responsivities of the long-, middle­and short-wavelength 
sensitive cones (LMS) and the rods. We use the Hunt-Pointer-Estevez [Fairchild98] responsivities for 
the cones and the CIE scotopic luminous ef.ciency function, V i(.), [Wyszecki82] for the rods. In many 
applications, a spectral radiance image is not available. In such cases, the cone signals can be calculated 
as a linear trans­form of the CIE 1931 XYZ tristimulus values as shown in Equa­tion 1. M S =  Y Z 
 (1) 0.3897 0.6890 -0.0787 -0.2298 1.1834 0.0464 001 However, it is impossible to obtain the proper rod 
signals. We derived a linear transform of XYZ tristimulus values as a rough approximation to the rod 
signal via linear regression of the color matching functions and the V i(.) curve. The resulting transform 
is given in Equation 2 where R represents the rod response for a pixel. R = -0.702X +1.039Y +0.433Z (2) 
Since it is possible to obtain negative values of R when Equation 2 is applied to some saturated colors, 
it must be clipped to zero. We chose a simple linear transformation for this approximation since it scales 
over any range of luminance levels. Finally the input signals must be calibrated prior to input to the 
visual transforms. We chose to calibrate the model such that the LMS cone signals and the rod signal 
are all equal to unity for an equal-radiance spectrum at a luminance of 1.0 cd/m2 . 3.3 Spatial Decomposition 
The 4 images representing the calibrated photoreceptor responses are then subjected to spatial processing. 
The .rst step is to carry out the spatial decomposition of these images. We carry out this decomposition 
by the Laplacian pyramid (difference-of-Gaussian pyramid) approach proposed by Burt and Adelson [1983]. 
This approach guarantees the construction of a non-negative low-pass image in high dynamic range situations, 
and is perfectly invertible. We .rst calculate a Gaussian pyramid using a 5 tap .lter (with 1D weights: 
.05 .25 .4 .25 .05) [Burt83]. Each level of the Gaussian pyramid represents a low-pass image limited 
to spatial frequencies half of those of the next higher level. Our Gaussian pyramid has 7 levels. Each 
level of the Gaussian pyramid is then upsampled such that each image is returned to the size of the initial 
image. Difference­of-Gaussian images are then calculated by taking the image at each level and subtracting 
the image from the next lower level. This re­sults in 6 levels of band-pass images with peak spatial 
frequencies at 16, 8, 4, 2, 1, and 0.5 cpd. These images can be thought of as representations of the 
signals in six band-pass mechanisms in the human visual system. The lowest-level low pass image is retained 
since it must be used to reconstruct the image for reproduction ap­plications.  3.4 Gain Control The 
difference-of-Gaussian images are then converted to adapted contrast signals using a luminance gain control. 
The gains are set using TVI-like functions that represent the increment thresholds of the rod and cone 
systems and the growth in response required to allow perceived contrast to increase with luminance level 
(sub­Weber s law behavior). The gain functions are given for the cones in Equation 3 and the rods in 
Equation 4. 1 Gcone(I)= (3) 0.555(I +1.0)0.85   Figure 8: Pictorial representation of the computational 
model. Note that only 2 out of the 6 spatial mechanisms of one of the channels have been shown for the 
purpose of illustration. Original image is a Snellen chart with a 30:1 shadow boundary. their derivation 
were that both the rod and cone gains were set equal to 1.0 at a 1.0 cd/m2, the absolute thresholds would 
be around 1.0 cd/m2 for cones and 0.001 cd/m2 for rods, the ultimate slopes of the functions would be 
0.85 for sub-Weber s Law behavior, and the rods would saturate, losing 50% of their responsivity at roughly 
3 cd/m2 . In our model, each pixel in a given difference-of-Gaussian image is multiplied by the gain 
derived from the corresponding pixel in the lower-level low-pass image that was used in its deriva­tion. 
This is illustrated in Equation 5. ACIn = G (LPn+1)[LPn - LPn+1] (5) 10 1 (4) Grod(I)= I2 +100.908(I 
+0.001)0.85 ACIn is the adapted contrast image at level n and LP repre­ sents the various low-pass images. 
The adapted contrast images are In the above equations, I represents the rod or cone signal that analogous 
to the contrast images that Peli [1990] obtained. How­is used to set the level of adaptation and G(I) 
is the gain-control ever, in our model the magnitude of these images is a function of factor. Equations 
3 and 4 were derived to match available psy-luminance level as speci.ed by the gain control functions. 
This is chophysical TVI and brightness matching data. The constraints in necessary to allow prediction 
of luminance-dependent appearance effects. The luminance gain controls are applied in the same man­ner 
to each of the difference-of-Gaussian images for each of the photoreceptors. Equation 3 is used to calculate 
the gains for each of the cones and Equation 4 is used for the rods. Note that per­forming the gain control 
at this point in the model allows proper Peak(cpd) .5 1.0 2.0 4.0 8.0 16.0 p for A 1.93 1.35 1.15 1.04 
1.15 1.40 p for C1&#38;C2 1.93 1.93 2.35 2.85 - - p for Rod 3.39 3.39 4.50 7.64 - - If c is negative, 
the absolute value of c is taken and then the negative prediction of chromatic-adaptation effects.  
3.5 Opponent Color Processing The next stage of the model is to transform the adapted contrast images 
for the cones into opponent signals. We use the transform of Hunt [1995] that has also been recently 
adopted in the CIE color appearance model, CIECAM97s [Fairchild98] as given in Equation 6. A 2.01.00.05 
L C1 =1.0 -1.09 0.09 M (6) C2 0.11 0.11 -0.22 S In the above equation, L, M, S represent the cone signals 
and A, C1, C2 represent luminance, red-green, and yellow-blue opponent signals respectively. This transform 
is applied without modi.cation to the adapted contrast signals to obtain adapted contrast signals in 
an opponent color space. This transformation is necessary to model differences in the spatial processing 
of luminance and chromatic signals. At this stage, the rod images are retained separately since their 
spatial processing attributes also differ from the cones. 3.6 Adapted Contrast Transducers The adapted 
contrast signals are then passed through contrast trans­ducer functions similar to those described by 
Watson and Solomon [1997]. Different transducer functions are applied to each spatial frequency mechanism 
in order to model psychophysically derived human spatial contrast sensitivity functions. For example, 
the trans­ducer for the 16 cpd achromatic mechanism has a higher threshold than the transducer for the 
4 cpd achromatic mechanism since we are less sensitive to the higher spatial frequencies. The transducers 
are also different for the chromatic channels to represent their lower sensitivities and low-pass, rather 
than band-pass nature. Finally, the rod system has a distinct set of transducers to represent its unique 
spatial characteristics. At high contrast levels the transducer func­tions converge to a common square-root 
form to properly represent perceived contrast constancy and introduce a compressive nonlin­earity typically 
found in color appearance models. The functional form of our transducer functions vary from that proposed 
by Wat­son and Solomon [1997] since their function was not analytically invertible and it is necessary 
to invert our model for image repro­duction applications. We chose to use a two-part function consist­ing 
of two power functions in order to replicate the two regions of distinct slope in the transducer functions. 
The contrast transducers used in our model are given by Equations 7 and 8 for the cones and Equation 
9 for the rods. sign is replaced after transformation. Equations 7 through 9 were derived by specifying 
the desired thresholds for the various spatial and chromatic mechanisms at 1000 cd/m2 for the cones and 
0.5 cd/m2 for the rods. The lower parts of the functions were forced to pass through 1.0 for the de­sired 
threshold contrast. At sinusoidal contrasts greater than 5% (at these calibration luminance levels) the 
functions converge to the square-root form that produces contrast constancy. The square root is derived 
to mimic the compressive nonlinearities typically found in color appearance models. These transducers 
do produce CSF be­havior that changes with luminance as illustrated in Figure 3 since the input adapted 
contrast signals vary with luminance due to the sub-Weber gain control functions. Transducers for the 
chromatic cone channels and the rod channel do not exist for spatial frequen­cies above those represented 
by the mechanism centered at 4 cpd since these systems cannot resolve contrast at higher spatial fre­quencies. 
(Note that the 4 cpd mechanism carries information out to about 16 cpd which is thus the acuity limit 
of the mechanism.) The contrast transducer functions are calibrated such that psychophys­ical contrast 
sensitivity function data are modeled and sinusoidal contrasts above about 5% produce contrast constancy 
as a func­tion of spatial frequency and sinusoidal contrasts of 100% produce transducer output of approximately 
100. It should be recalled that in our model, these levels will be luminance dependent. The con­trast 
transducer functions are also designed such that contrasts that are below threshold have an output level 
with magnitude less than 1.0. One of the key functions of the transducers is to set the threshold level 
such that image content that is imperceptible for a given set of viewing conditions can be removed. To 
accomplish this, the output of the transducer functions is thresholded such that all absolute val­ues 
less than 1.0 are set to 0.0. An alternative approach would be to replace all absolute values less than 
1.0 with a random number between 0.0 and 1.0. This might better replicate the appearance of visual noise 
at low contrast levels. In addition to establishing thresholds, the transducer functions are used to 
model saturation of the visual neurons that signal con­trast. Thus, the transducer functions are limited 
to maximum values of 50 to simulate the typical physiological dynamic range. [Hunt95] Since the contrast 
mechanisms are bipolar, this represents a 100:1 dynamic range in each spatial mechanism and therefore 
even larger perceptual dynamic ranges in fully reconstructed images. This sat­uration is also not a severe 
restriction on the image content since the gain control mechanisms already accomplish a high degree of 
dynamic-range compression.  3.7 Combination of Rod and Cone Signals 1 2 if c = 0.536 Up to this point 
in the model it is necessary to keep the rod signals (7) 22.4(c/0.536) Tcone,Achromatic(c)= 22.4(c/0.536)p 
otherwise. separate in order to appropriately integrate their unique adaptation and spatial vision properties. 
After the contrast transducers, the rod if c = 0.176 1 2 22.4(c/0.176) (8) and cone signals can be 
combined to produce signals that represent Tcone,Chromatic(c)= 22.4(c/0.176)p otherwise. the three-dimensional 
color appearances of the input image. We as­ sume that the rods contribute only to the luminance signal 
and thus if c = 0.0335 1 2 22.4(c/0.0335) (9) combine the A signal from the cones with the rod signal, 
denoted Trod(c)= 22.4(c/0.0335)p otherwise. Arod, to produce a total achromatic signal, Atotal, using 
Equation 10. In the above equations, c represents the adapted contrast signals Atotal = Acone + Arod/7 
(10) (ACI s) and T (c) represents the output of the transducers. The ex­ponent, p, in Equations 7, 8 
and 9 differs for each spatial frequency The differential weighting of the rod and cone signals is a 
result mechanism as given in the following table. of the model calibration necessary to establish the 
rod and cone gain controls and transducer functions. It results in a total achromatic output that is 
monotonic with luminance. At this stage in the model we have three channels represent­ing achromatic, 
red-green, and yellow-blue apparent contrast for 6 band-pass mechanisms. These signals model threshold 
behavior, in that any contrast signals that could not be perceived have been eliminated at the contrast 
transducer functions. They also repre­sent suprathreshold appearance since the contrast signals grow 
with luminance and the chromatic channels will become zero at lumi­nance levels below the cone threshold. 
At this stage, the model has also accomplished a signi.cant level of dynamic-range compres­sion since 
the contrast signals range only 2 orders of magnitude (1 to around 100) for luminance differences ranging 
over 10 orders of magnitude. This compression is accomplished by both the gain control functions and 
the nonlinear transducers.  3.8 Treatment of the Low Pass Image The lowest level low-pass image from 
the upsampled Gaussian pyramid must be retained in order to reconstruct an image from the adapted contrast 
images that have been passed through the model (each a band-pass image). To this point, we have not discussed 
the application of the visual model to this low pass image. The best approach to processing the low-pass 
image depends on the applica­tion. For simple images of low dynamic range (e.g., less than 50:1), an 
appropriate treatment of the low-pass image is to multiply it by a constant gain factor derived from 
the image mean. This technique will do little to compress the range of high-dynamic range images since 
the contrast within the low pass image will be preserved. An alternative that produces maximum dynamic-range 
compression is to multiply each pixel in the low-pass image by a gain factor derived from the pixel itself. 
(The gain factors are derived using Equations 3 and 4.) Techniques intermediate between these two might 
pro­duce optimal image reproductions for various applications. The above treatment of the low-pass image 
is consistent with the full model of visual perception and can be thought of as a treatment of the effects 
of eye movements on the perception of a scene. In the extreme case of adapting the low pass image to 
its own values, the treatment mimics the visual response assuming that observer .x­ated on each and every 
image location and judged them completely independent of one another. For the other extreme case of adapting 
the low pass image using the mean signal, the treatment simulates completely random and continuous eye 
movements uniformly dis­tributed across the scene. Intermediate treatments between these two extremes 
might more accurately model real world eye move­ments which are scene dependent and represent some average 
be­tween .xating each image element of interest and randomly view­ing all locations in a scene. Transducer 
functions are necessary for the low pass image as well since the rod and cone information is combined 
after the trans­ducer stage. We have adopted low-pass transducers that are simple power functions based 
on typical practice in color appearance mod­eling. [Fairchild98] The scaling of the low-pass transducers 
is set to preserve equal magnitude of signals for the low-pass and band-pass model output for a sinusoidal 
grating. The low-pass transducers are given in Equations 11, 12 and 13 for the achromatic and chromatic 
cone signals and rod signals respectively.  3.9 Image Reconstruction for Display The output of the visual 
model consists of appearance signals in an achromatic and two chromatic channels and six spatial band-pass 
mechanisms plus a low-pass image. We now take these appearance signals backward through the model to 
recreate cone signals (and ultimately device signals such as RGB or CMYK) that replicate the full color 
appearance of the image on a photopic, trichromatic display device such as a CRT display. The .rst step 
of the inversion process is to go through the inverse of the transducer functions given in Equations 
7, 8, 11 and 12. The AC1C2 signals are then transformed to adapted LMS cone signals using the inverse 
of the matrix transformation given in Equation 6. At this point we have adapted contrast signals that 
have been subjected to the appropriate visibility thresholding and saturation by the contrast transducer 
functions. The next step is to reverse the gain control process for the view­ing conditions of the output 
display. This begins by determining the gain control factors for the mean luminance of the target display 
device using Equation 3. The adapted low-pass images are then di­vided by the display-mean gain control 
factors to produce images that represent the appropriate LMS cone responses for the display low-pass 
image. This display low-pass image is then used to begin the process of reconstruction of a full-resolution 
image from the six adapted contrast signal images. Gain control factors are calculated for each pixel 
of the display low-pass image using Equation 3 and these are used to scale the lowest frequency (0.5 
cpd peak) adapted contrast signal image back to the display. This image is then added to the low-pass 
image to produce a new low-pass image that includes the contrast informa­tion from the 0.5 cpd image. 
This image is then used to calculate gain control factors that are applied to the next level (1.0 cpd 
peak). The resulting image is again added to the new low-pass image to generate yet another low-pass 
image that incorporates the informa­tion from both the 0.5 and 1.0 cpd mechanisms. The process is repeated 
at each level until all of the spatial frequency mechanisms have been scaled to the display and added 
to the output image. At this point in the reconstruction we have an LMS image rep­resenting the cone 
signals that are desired when viewing the out­put display. These must be converted to signals appropriate 
for the given display device using typical device-independent color imag­ing procedures. For a CRT this 
involves a linear transform from LMS to CIE XYZ (inverse of Equation 1) followed by a second lin­ear 
transform from CIE XYZ to device RGB. The second transform is de.ned by the CIE tristimulus values of 
the display primaries. At this point we have linear RGB signals that must either be trans­formed through 
the inverse of the CRT display transfer function (often referred to as gamma correction) or displayed 
on a system with linearizing video look-up tables. A similar, although more complex, process is required 
for printing devices. Finally, it is not uncommon that the desired display colors simply cannot be produced 
on a given device (i.e., they are out of gamut). (11) 1 21 2 TLP cone,Achromatic(LP ) = 30.5(LP ) TLP 
cone,Chromatic(LP ) = 53.3(LP ) This includes the mapping of the desired luminance level and range into 
that of the display. There are a wide variety of techniques that (12) have been suggested to address 
this issue the details of which are beyond the scope of this paper. The approach that is taken depends 
 1 2 TLP rod(LP ) = 122(LP ) (13) on the particular application. Some of the issues that are encoun-T 
represents the output of the low-pass transducers and LP repre-tered with this particular model are discussed 
in the next section sents the pixel values in the adapted, opponent-transformed, low-with respect to 
the rendering on paper and on CRT monitor of the pass image. various example images. Figure 9: Application 
of the model to a wide range of illumination levels.  4 APPLYING THE MODEL 4.1 Wide Absolute Range The 
series of images in Figure 9 illustrate application of the model to a wide range of luminance levels 
spanning six orders of mag­nitude from 0.1 to 10,000 cd/m2 . These images were created us­ing the model 
as described in section 3 with the low-pass images adapted to the mean luminance of the input images. 
The size of the original image was about 15. × 10.. For image reconstruction as printed images, a mean 
adapting luminance of 700 cd/m2 was assumed. This is approximately the luminance of a standard print 
viewing booth. Thus this series of images should provide faithful reproductions of the visual impression 
at the various luminance lev­els when the printed .gure is viewed at a mean luminance of 700 cd/m2 . 
The gamut-mapping selected for this demonstration was a linear scaling that placed the white areas of 
the 1000 cd/m2 image at the paper white. While the model can be applied successfully over a wider absolute 
range, it is impossible to reproduce the re­sults within the limited dynamic range (approximately 50:1) 
of the printed images unless a variable scaling is used. Features to note in Figure 9 include: the decrease 
in luminance contrast and colorfulness as luminance is decreased, the loss of color vision upon the transition 
from cone to rod vision below 1 cd/m2, the decrease in spatial acuity with decrease in luminance, and 
the changes in relative visibility of various colors and patterns. The Purkinje shift (blue to gray and 
red to black) is also correctly predicted upon changes from photopic to scotopic luminance levels. All 
of these features illustrate that the model has appropriately en­coded aspects of threshold visibility 
and suprathreshold appearance over a wide range of luminance levels. 4.2 Chromatic Adaptation Figure 
10 shows the unique feature of this model that it can handle changes in chromatic, as well as luminance-level, 
adaptation. The top row of images illustrate a scene illuminated by a very reddish light source, a nearly-white 
incandescent light source, and a very blue light source as they would be rendered by a system incapable 
of chromatic adaptation. The shift in color balance of the repro­duced prints is objectionable since 
the human visual system largely compensates for these changes in illumination color through its mechanisms 
of chromatic adaptation. Since our model treats gain control in each of the classes of cone photoreceptors 
independently, it is capable of predicting changes in chromatic adaptation simi­lar to those that would 
be predicted by a von Kries model. How­ever, due to the nature of the gain control functions used to 
obtain increases in contrast and colorfulness with luminance, the degree of chromatic adaptation predicted 
by the model is less than 100% complete. The bottom row of images illustrate the output of the visual 
model when the low-pass images are adapted to the mean signal levels in the image and the reconstructed 
images are created assum­ing adaptation to an equal-energy white. All of the computations were completed 
at a mean luminance of 50 cd/m2 . The gamut­mapping selected for these images was a linear scaling that 
mapped 100 cd/m2 in the reconstructed image to the monitor white. 100 cd/m2 is approximately the maximum 
luminance of a display mon­itor. The sizes of the original images were 10.× 8.. These images illustrate 
that the model almost completely accounts for the changes in illumination color. However, as expected 
the reproduced appear­ance from the reddish light source retains a slight reddish cast while the reproduction 
from the bluish light source retains a slight bluish cast. These reproductions match our perceptions 
of changes in il­lumination color and replicate the incomplete nature of chromatic adaptation that is 
widely recognized in the color science literature. [Fairchild98]  4.3 High Dynamic Range Figure 11 illustrates 
application of the model to the tone mapping of high-dynamic range images. The original images have areas 
of detail that are in high illumination levels and other areas that are in low illumination levels. The 
left most image in Figure 11 is a global illumination rendering. The other two were constructed from 
suc­cessive photographic exposures using the technique of Debevec and Malik [1997]. To provide the higher 
degree of compression nec­essary for high-dynamic-range mapping, the low pass image was adapted to itself. 
The reproduced images were reconstructed for display at a mean luminance of 50 cd/m2 .  The images on 
the top row of Figure 11 are linear mappings of the original high-dynamic range images into the limited 
dynamic range of the output device. The original images had luminance ranges of approximately 10,000:1. 
The images on the bottom row represent the mapping obtained by application of the visual model. In Figure 
11 it is clear that far more detail can be observed in both shadow and highlight regions when the images 
are mapped using the visual model.  5 CONCLUSIONS AND FUTURE WORK In this paper we have introduced 
a new visual model for realistic tone reproduction. The model is based on a multiscale representa­tion 
of luminance, pattern, and color processing in the human visual system, and provides a coherent framework 
for understanding the effects of adaptation on spatial vision. The model allows us to ac­count for the 
changes in threshold visibility, visual acuity, and color discrimination, and suprathreshold brightness, 
colorfulness and ap­parent contrast that occur with changes in the level of illumination in scenes. We 
have applied the visual model to the problem of realistic tone reproduction and have developed a tone 
reproduction operator that addresses the two major problems in realistic tone reproduction: images of 
wide absolute range and high dynamic range scenes can now be displayed on conventional display devices 
like CRTs and printers; and these images are faithful visual representations that match our perceptions 
of the scenes at threshold and suprathreshold levels to the limits possible on a given display. This 
work should have major impact on the .eld of digital imaging. Scenes that could never be reproduced before 
can now be imaged with high visual .delity. Beyond the clear applications of this work in realistic tone 
re­production, the visual model presented in this paper can be use­fully applied in a variety of other 
areas in digital imaging where the characteristics of threshold and suprathreshold vision are im­portant. 
Potential application areas include: image quality metrics; image coding and compression methods; perceptually-based 
image synthesis algorithms; image-based rendering; and advanced display system design. There is still 
much work to be done in this area. First, this is a static model of vision. Future models should incorporate 
knowl­edge about the temporal aspects of visual processing in order to allow both dynamic scenes, and 
scenes where the level of illumi­nation is dynamically changing to be properly displayed. Second, we 
hope to integrate our visual model with ongoing work in the color science community on appearance models 
[Fairchild98] for predicting how images look under different viewing conditions. We should also draw 
on the testing methods developed by researchers in this community to verify that our images are in fact 
good vi­sual matches to actual scenes. Finally, a number of researchers [Gilchrist77, Adelson93] in the 
vision community have shown that threshold and suprathreshold properties of scenes aren t simply a function 
of the two dimensional patterns of luminances in the reti­nal image, but also depend upon our perceptions 
of the spatial ar­rangement of surfaces and illumination in three dimensions. Future work should address 
how these 3D issues affect our perceptions of scenes and in.uence the development of operators for realistic 
tone reproduction.  ACKNOWLEDGEMENTS Special thanks to David Hart, Steve Marschner, Hurf Sheldon, Chris 
Williams, Ben Trumbore, Eric Lafortune and Dan Kartch for their help in preparing this paper. This work 
was supported by the NSF Science and Technology Center for Computer Graphics and Scienti.c Visualization 
(ASC-8920219) and by NSF grant ASC-9523483 and was performed on workstations generously provided by the 
Hewlett-Packard Cor­poration.  References [Adelson93] Adelson, E.H. (1993) Perceptual organization and 
judgment of bright­ness. Science, 262, 2042-2044. [Barlow72] Barlow, H.B. (1972) Dark and Light Adaptation: 
Psychophysics. In D. Jameson and L. Hurvich (Eds.), Handbook of Sensory Physiology, V. 7(4), New York: 
Springer, 2-27.  [Brady95] Brady, N. and Field, D.J. (1995) What s Constant in Contrast Constancy? The 
Effects of Scaling on the Perceived Contrast of Bandpass Patterns. Vision Res., 35(6), 739-756. [Burt83] 
Burt, P.J., and Adelson, E.H. (1983) The Laplacian Pyramid as a Compact Image Code. IEEE Transaction 
on Communication, 31(4), 532-540. [Chiu93] Chiu, K., Herf, M., Shirley, P., Swamy, S., Wang, C., and 
Zimmerman, K. (1993) Spatially Nonuniform Scaling Functions for High Contrast Images. Pro­ceedings Graphics 
Interface 93, 245-254. [Debevec97] Debevec, P.E. and Malik, J. (1997) Recovering High Dynamic Range Radiance 
Maps from Images. Proceedings SIGGRAPH 97, 369-378. [Fairchild98] Fairchild, M.D. (1998) Color Appearance 
Models. Reading, MA: Addison-Wesley. [Ferwerda96] Ferwerda, J.A., Pattanaik, S.N., Shirley, P., and Greenberg, 
D. (1996) A Model of Visual Adaptation for Realistic Image Synthesis. Proceedings SIG-GRAPH 96, 249-258. 
[Georgeson75] Georgeson, M.A. and Sullivan, G.D. (1975) Contrast Constancy: De­blurring in Human Vision 
by Spatial Frequency Channels. J. Physiol., 252, 627­ 656. [Gilchrist77] Gilchrist, A.L. (1977) Perceived 
Lightness Depends on Perceived Spa­tial Arrangement. Science, 195, 185-187. [Hunt95] Hunt, R.W.G. (1995) 
The Reproduction of Color. 5th edition, Kingston­upon-Thames, England: Fountain Press. [Hurvich81] Hurvich, 
L. (1981) Color Vision. Sunderland, MA: Sinauer Assoc. [Jobson96] Jobson, D.J., Rahman, Z., and Woodell, 
G.A. (1996) Retinex Image Pro­cessing: Improved Fidelity to Direct Visual Observation. Proceedings 4th 
Color Imaging Conference, Society for Imaging Science and Technology, 124-126. [Laming91] Laming D. (1991) 
Contrast Sensitivity. In J.J. Kulikowski, V. Walsh, and I.J. Murray (Eds.) Limits of Vision, Vol. 5, 
Vision and Visual Dysfunction. Boca Raton, FL, CRC Press, 35-43. [Lubin95] Lubin, J. (1995). A Visual 
Discrimination Model for Imaging System De­sign and Evaluation. In E. Peli (Ed.) Vision Models for Target 
Detection. Singapore, World Scienti.c, 245-283. [Losada94] Losada, M.A., and Mullen, K.T. (1994) The 
Spatial Tuning of Chromatic Mechanisms Identi.ed by Simultaneous Masking. Vision Res., 34(3), 331-341. 
[Mullen85] Mullen, K.T. (1985) The Contrast Sensitivity of Human Color Vision to Red-Green and Blue-Yellow 
Chromatic Gratings. J. Physiol., 359, 381-400. [Peli90] Peli, E. (1990) Contrast in Complex Images. J. 
Opt. Soc. Am. A, 7(10), 2032­2040. [Schlick95] Schlick, C. (1995) Quantization Techniques for High Dynamic 
Range Pictures. In G. Sakas, P. Shirley, and S. Mueller, (Eds.), Photorealistic Rendering Techniques, 
Berlin: Springer-Verlag, 7-20. [Shapley84] Shapley, R. and Enroth-Cugell, C. (1984) Visual Adaptation 
and Retinal Gain Controls. In N. Osborne and G. Chader (Eds.). Progress in Retinal Research, V. 3, Oxford: 
Pergamon Press., 263-347. [Spencer95] Spencer, G., Shirley, P., Zimmerman, K., Greenberg, D. P. (1995) 
Physically-based Glare Effects for Computer Generated Images. Proceedings SIG-GRAPH 95, 325-334. [Stevens61] 
Stevens, S.S. (1961) To Honor Fechner and Repeal His Law. Science, 133, 13 Jan., 80-86. [Tumblin93] Tumblin, 
J., and Rushmeier, H. (1993) Tone Reproduction for Realistic Images, IEEE Computer Graphics and Applications, 
13(6), 42-48. [Tumblin97] Tumblin, J., Hodgkins, J. and Guenter, B. (1997) Display of High Con­trast 
Images Using Models of Visual Adaptation. Visual proceedings SIGGRAPH 97, 154. [vanNes67] van Nes F.L. 
and Bouman M.A. (1967) Spatial Modulation Transfer in the Human Eye. J. Opt. Soc. Am., 57, 401-406. [vanderHorst69] 
van der Horst, G.J.C. and Bouman, M.A. (1969) Spatiotemporal Chromaticity Discrimination. J. Opt. Soc. 
Am., 59(11), 1482-1488. [Ward94] Ward, G. (1994) A Contrast-based Scalefactor for Luminance Display. 
In P.S. Heckbert (Ed.), Graphics Gems IV, Boston: Academic Press Professional. [Ward-Larson97] Ward-larson, 
G., Rushmeier, H., and Piatko, C. (1997) A Visibil­ity Matching Tone Reproduction Operator for High Dynamic 
Range Scenes. IEEE Transactions on Visualization and Computer Graphics, 3(4), 291-306. [Watson97] Watson, 
A.B. and Solomon, J.A. (1997) Model of Visual Contrast Gain Control and Pattern Masking. J. Opt. Soc. 
Am. A, 14(9), 2379-2391. [Westheimer86] Westheimer, G. (1986) The Eye as an Optical Instrument. In K. 
Boff, L. Kaufman, and J. Thomas (Ed.), Handbook of Perception and Human Perfor­mance, New York: Wiley 
and Sons. [Wilson91] Wilson, H.R. (1991). Psychophysical Models of Spatial Vision and Hy­peracuity. in 
D. Regan (Ed.) Spatial Vision, Vol. 10, Vision and Visual Dysfunction. Boca Raton, FL, CRC Press, 64-81. 
[Wyszecki82] Wyszecki G., and Stiles W.S. (1982) Color Science: Concepts and Methods, Quantitative Data 
and Formulae (2nd edition). New York: Wiley.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280924</article_id>
		<sort_key>299</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>29</seq_no>
		<title><![CDATA[A perceptually based adaptive sampling algorithm]]></title>
		<page_from>299</page_from>
		<page_to>309</page_to>
		<doi_number>10.1145/280814.280924</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280924</url>
		<keywords>
			<kw><![CDATA[adaptive sampling]]></kw>
			<kw><![CDATA[masking]]></kw>
			<kw><![CDATA[perception]]></kw>
			<kw><![CDATA[vision models]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.4.0</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Viewing algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Sampling</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010383</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Image processing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39069629</person_id>
				<author_profile_id><![CDATA[81339490696]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Bolin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Oregon, Eugene]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP42053997</person_id>
				<author_profile_id><![CDATA[81339516843]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gary]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Meyer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Oregon, Eugene]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Barten, E G. J., "The Square Root Integral (SQRI): A New Metric to Describe the Effect of Various Display Parameters on Perceived Image Quality," Human Vision, Visual Processing, and Digital Display, Proc. SPIE, Vol. 1077, pp. 73-82, 1989.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218497</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bolin, M. R. and Meyer G. W., "A Frequency Based Ray Tracer," Computer Graphics, Annual Conference Series, ACM SIG- GRAPH, pp. 409-418, 1995.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>732103</ref_obj_id>
				<ref_obj_pid>647651</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bolin, M. R. and Meyer G. W., "An Error Metric for Monte Carlo Ray Tracing," Rendering Techniques '97, J. Dorsey and P. Slusallek, Editors, Springer-Verlag, New York, pp. 57-68, 1997.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Cohen, A., Daubechies, I., and Feauveau, J. C., "Biorthogonal Bases of Compactly Supported Wavelets," Communications on Pure and Applied Mathematics, Vol. 45, No. 5, pp. 485-500, 1992.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>197783</ref_obj_id>
				<ref_obj_pid>197765</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Daly, S., "The Visible Differences Predictor: An Algorithm for the Assessment of Image Fidelity," Digital Images and Human Vision, A. B. Watson, Editor, MIT Press, Cambridge, MA, pp. 179-206, 1993.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Daubechies, I., "Orthonormal Bases of Compactly Supported Wavelets," Communications on Pure and Applied Mathematics, Vol. 41, No. 7, pp. 909-996, 1988.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258818</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Ferwerda, J. A., Shirley, E, Pattanaik, S. N., and Greenberg, D. E, "A Model of Visual Masking for Computer Graphics," Computer Graphics, Annual Conference Series, ACM SIGGRAPH, pp. 143- 152, 1997.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Field, D. J., "Relations Between the Statistics of Natural Images and the Response Properties of Cortical Cells," J. Opt. Soc. Am. A, Vol. 4, pp. 2379-2394, 1987.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Field, D. J., "What the Statistics of Natural Images Tell Us About Visual Coding," Human Vision, Visual Processing, and Digital Display, Proc. SPIE, Vol. 1077, pp. 269-276, 1989.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Gibson, S. and Hubbold, R. J., "Perceptually-Driven Radiosity," Computer Graphics Forum, Vol. 16, pp. 129-140, 1997.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237200</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Gortler, S. J., Grzeszczuk, R., Szeliski, R., and Cohen, M. E, "The Lumigraph," Computer Graphics, Annual Conference Series, ACM SIGGRAPH, pp. 43-54, 1996.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122735</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Kirk, D. and Arvo, J., "Unbiased Sampling Techniques for Image Synthesis," Computer Graphics, Annual Conference Series, ACM SIGGRAPH, pp. 153-156, 1991.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325179</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Lee, M. E., Redner, R. A., and Uselton, S. P., "Statistically Optimized Sampling for Distributed Ray Tracing," Computer Graphics, Annual Conference Series, ACM SIGGRAPH, pp. 61-67, 1985.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Legge, G. E. and Foley, J. M., "Contrast Masking in human vision," Journal of the Optical Society of America, Vol. 70, pp. 1458- 1470, 1980.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>268598</ref_obj_id>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Li, B., "An Analysis and Comparison of Two Visual Discrimination Models," Master's Thesis, University of Oregon, June 1997.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Li, B., Meyer, G. W., and Klassen, R. V., "A Comparison of Two Image Quality Models," to appear in Human Vision and Electronic Imaging III, B. E. Rogowitz and T. N. Pappas, Editors, Proc. SPIE, Vol. 3299, 1998.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Lubin, J., "A Visual Discrimination Model for Imaging System Design and Evaluation," Vision Models for Target Detection and Recognition, Eli Peli, Editor, World Scientific, New Jersey, pp. 245- 283, 1995.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Maloney, L. T., "Evaluation of linear models of surface spectral reflectance with small numbers of parameters," J. Opt. Soc. Am. A, Vol. 3, pp. 1673-1683. 1986.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Marimont, D. H. and Wandell, B. A., "Matching Color Images: The Impact of Axial Chromatic Aberration," J. Opt. Soc. Am. A, Vol. 12, pp. 3113-3122, 1993.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Meyer, G. W. and Liu, A., "Color Spatial Acuity Control of a Screen Subdivision Image Synthesis Algorithm," Human Vision, Visual Processing, and Digital Display III, Bernice E. Rogowitz, Editor, Proc. SPIE, Vol. 1666, pp. 387-399, 1992.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37410</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Mitchell, D. P., "Generating Antialiased Images at Low Sampling Densities," Computer Graphics, Annual Conference Series, ACM SIGGRAPH, pp. 65-72, 1987.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Mullen, K. T., "The Contrast Sensitivity of Human Colour Vision to Red-Green and Blue-Yellow Chromatic Gratings," J. Physiol. (Lond.), Vol. 359, pp. 381-400, 1985.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74362</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Painter, J. and Sloan, K. "Antialiased Ray Tracing by Adaptive Progressive Refinement," Computer Graphics, Annual Conference Series, ACM SIGGRAPH, pp. 281-288, 1989.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Ruderman, D. L., "Origins of Scaling in Natural Images," Human Vision, Visual Processing, and Digital Display, Proc. SPIE, Vol. 2657, pp. 120-131, 1996.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Rushmeier, H., Ward, G., Piatko, C., Sanders, P., and Rust, B., "Comparing Real and Synthetic Images: Some Ideas About Metrics," Rendering Techniques '95, P. M. Hanrahan and W. Purgathofer, Editors Springer-Verlag, New York, pp. 82-91, 1995.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Schreiber, W. F., Fundamentals of Electronic Imaging Systems, Springer-Verlag: Berlin Heidelberg, 1993.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Copyright &#38;#169;1998 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers or to redistribute to lists, requires specific permission and/or a fee. A Perceptually Based 
Adaptive Sampling Algorithm Mark R. BolinGary W. Meyer Department of Computer and Information Science 
University of Oregon Eugene, OR 97403  Abstract A perceptually based approach for selecting image samples 
has been developed. An existing image processing vision model has been extended to handle color and has 
been simpli.ed to run ef.­ciently. The resulting new image quality model was inserted into an image synthesis 
program by .rst modifying the rendering algo­rithm so that it computed a wavelet representation. In addition 
to allowing image quality to be determined as the image was gener­ated, the wavelet representation made 
it possible to use statistical information about the spatial frequency distribution of natural im­ages 
to estimate values where samples were yet to be taken. Tests on the image synthesis algorithm showed 
that it correctly handled achromatic and chromatic spatial detail and that it was able pre­dict and compensate 
for masking effects. The program was also shown to produce images of equivalent visual quality while 
using different rendering techniques. CR Categories and Subject Descriptors: I.3.3 [Computer Graphics]: 
Picture/Image Generation; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism; I.4.0 [Image 
Process­ing and Computer Vision]: General. Additional Key Words and Phrases: Adaptive Sampling, Perception, 
Masking, Vision Models. 1 Introduction The synthesis of realistic images would be greatly facilitated 
by employing an algorithm that makes image quality judgements while the picture is being created instead 
of relying upon the user of the software to make these evaluations once the image is complete. In this 
way it would be possible to .nd the artifacts in a picture as it was being rendered and to invest additional 
effort on those areas. By targeting those parts of the picture where problems are visible, the overall 
time necessary to compute the picture could be reduced. It would also be possible to have the algorithm 
stop when the picture quality had reached a predetermined level. This hyhn mhee pi ctq y e qlu a@erit 
y h a y grha ti  "hn mhee pi mhb tlu a@erit a h y a grha ti would permit the use of radically different 
rendering algorithms but still have them produce an equivalent visual result. Image quality evaluation 
programs have been developed by vi­sion scientists and image processors to determine the differences 
between two pictures. Given a pair of input images, this software returns a visibility map of the variations 
between the two image arrays. While these programs are capable of making the visual judgements required 
by a perceptually based image synthesis al­gorithm, they are currently too expensive to execute every 
time a decision is necessary about where to cast a ray into an image or when the overall visual quality 
of the picture is acceptable. Their ef.cient evaluation also requires a frequency or a wavelet repre­sentation 
for the images instead of the usual pixel based scheme. The objective of this paper is to integrate an 
existing image quality evaluation algorithm into a realistic image synthesis pro­gram. This is to be 
done in such a way that image quality judge­ments can be made as the image is produced without severely 
impacting the overall execution time of the rendering program. This will require that the image quality 
metric be made to run more ef.ciently without sacri.cing its ability to detect visible ar­tifacts. It 
will also necessitate that the coef.cients of a frequency or a wavelet representation are computed by 
the image synthesis algorithm instead of the individual pixels of the .nal image. This will have the 
side bene.t of allowing the algorithm to make use of statistical information about the frequency content 
of natural images when actual data from the scene being rendered is not available. Including this introduction, 
the paper is divided into seven ma­jor sections. In the second section, previous work on vision based 
rendering algorithms is reviewed and existing image processing based vision models are described. A simpli.ed 
version of a vi­sion model is developed in the third section and is integrated into a rendering algorithm 
in the fourth section. In the .fth section the statistics of natural images are used to make guesses 
about un­known values as the image is computed. Finally, the results of the algorithm are discussed in 
the sixth section and some conclusions are drawn in the seventh section. 2 Background A few attempts 
have been made to develop image synthesis algorithms that, as the picture is created, detect threshold 
visual differences and direct the algorithm to work on those parts of the image that are in most need 
of re.nement. There have also been image processing algorithms invented, both inside and outside the 
.eld of computer graphics, that can be used to determine the visibility of differences between two images. 
In this section we review work in each of these areas in preparation for describing how we have combined 
an image processing and image synthesis algorithmto create a new image rendering technique. w 2.1 Vision 
Based Rendering Algorithms Mitchell [21] was the .rst to develop a ray tracing algorithm that considered 
the perception of noise and attempted to operate near its threshold. He adopted a Poisson disk sampling 
pattern to concentrate aliasing noise in high frequencies where the arti­fact is less conspicuous. He 
also employed an adaptive sampling technique to vary the sampling rate according to frequency con­tent. 
A contrast calculation was performed in order to obtain a perceptually based measure of the variation 
in the signal. Differ­ential weighting was applied to the red, green, and blue contrasts to account for 
color variation in the eye s spatial sensitivity. Meyer and Liu [20] developed an image synthesis algorithm 
that took full advantage of the visual system s limited color spatial acuity. To accomplish this they 
used an opponents color space with chromatic and achromatic color channels. They employed the Painter 
and Sloan [23] adaptive subdivision algorithm to compute a k-D tree representation for an image. Because 
lower levels of the tree contained the higher spatial frequency content of the picture, they descended 
the k-D tree to a lesser depth in order to determine the chromatic channels of the .nal image. Bolin 
and Meyer [2] were the .rst to use a simple vision model to make decisions about where to cast rays into 
a scene and how to spawn rays that intersect objects in the environment. The model that they employed 
consisted of three stages: recep­tors with logarithmic response to light, opponents processing into achromatic 
and chromatic channels, and spatial frequency .lter­ing that is stronger for the color channels. They 
computed a spatial frequency representation from the samples that they took. As higher image frequencies 
were determined the number of rays spawned was decreased. This allowed them to exploit the phe­nomena 
of masking in their algorithm. Gibson and Hubbold [10] have used a tone reproduction op­erator to determine 
display colors during the rendering process. This made it possible to compute color differences in a 
perceptu­ally uniform color space and control the production of radiosity solutions. They used this technique 
on the adaptive element re­.nement, shadow detection, and mesh optimization portions of the radiosity 
algorithm. 2.2 Image Processing Based Models of the Visual Sys­tem The architectures of image processing 
based models of the vi­sual system share a number of common elements. The .rst stage of the models is 
usually a nonlinear intensity transformation. This is done to account for the visual system s difference 
in detection capability for information in light and dark areas of a scene. The second stage typically 
involves some spatial frequency processing. Most contemporary models break the spatial frequency spectrum 
into separate channels. The sensitivity of the individual channels is controlled so that the overall 
bandpass corresponds to the con­trast sensitivity function. The spatial frequency hierarchy makes it 
possible to determine whether a signal will be masked or fa­cilitated by the presence or absence of background 
information with a similar frequency composition. Finally the outputs of the separate frequency channels 
that are above threshold are summed to create a .nal representation. Two important examples of image 
processing based models of the visual system are the Daly Visual Difference Predictor (VDP) [5] and the 
Sarnoff Visual Discrimination Model (VDM) [17]. The Daly VDP takes a more psychophysically based ap­proach 
to vision modeling. As such it uses a power law represen­tation for the initial nonlinearity and it transforms 
the image into Figure 1: Block diagram of vision model. the frequency domain in order to perform its 
.ltering operations. The Sarnoff VDM focuses more attention on modeling the phys­iology of the visual 
pathway. It therefore operates in the spatial domain and does a careful simulation of such things as 
the optical point spread function. In recent work, the Daly VDP and the Sarnoff VDM have been applied 
to precomputed computer graphic imagery. Rushmeier, et. al [25] used the initial stages of the Daly VDP 
(and other vision metrics) to compare a simulated and a measured image. Ferwerda, et. al. [7] extended 
the Daly VDP to include color and modi.ed how it handles masking. The result was a new image processing 
based model of the visual system that they used to demonstrate how surface texture can mask polygonal 
tessellation. Li [15,16] has used computer graphic pictures to compare the Daly VDP and the Sarnoff VDM. 
She found that the two models performed com­parably, but that the Sarnoff VDM gave better image difference 
maps and required less recalibration. The Sarnoff VDM was also determined to have better execution speed 
than the Daly VDP but required the use of signi.cantly more memory. As a result of this comparison we 
have decided to use the Sarnoff VDM as the basis for our new vision based rendering algorithm.  3 Simpli.ed 
Vision Model The vision model that we have developed bears many simi­larities to the Sarnoff VDM discussed 
in the previous section. In creating a new model of visual perception we were motivated by two primary 
factors. The .rst and foremost criteria is the speed of the visual model. Modern visual difference predictors 
have gone to great lengths to accurately model the perceptual sensitivity of the human visual system. 
However, ef.ciency is seldom a design criteria in developing these systems. This fact limits the utility 
of these algorithms in applications where speed is a primary concern. The second factor that motivated 
our development of a new model is the correct handling of color. The majority of visual difference predictors 
have been designed only for gray scale images, and the ones that include color have neglected the signi.cant 
effect of chromatic aberration. The perceptual model that will be described has been imbed­ded into a 
visual difference predictor. This difference predictor receivesas input two images speci.ed in CIE XYZ 
color space. It returns as output a map of the perceptual difference between the two images speci.ed 
in terms of just noticeable differences (JND s). One JND corresponds to a 75% probability that an ob­server 
viewing the two images would be able to detect a differ­ence, and the units correspond to a roughly linear 
magnitude of subjective visual differences [17]. A block diagram of our visual difference predictor is 
given in Figure 1. The steps cone fundamentals through spatial pooling are carried out independently 
on both input images. The differ­ences between the two images are accumulated in the distance summation 
step. In the .rst stage of the vision model entitled cone fundamen­tals, the pixels of the input image 
are encoded into the responses of the short (S), medium (M) and long (L) receptors found in the retina 
of the eye. This is accomplished using the transformation from CIE XYZ to SML space speci.ed by Bolin 
and Meyer [2]. There is now abundant evidence for the existence of channels in the visual pathway that 
are tuned to a number of speci.c fre­quencies and orientations [17]. The visual processing that occurs 
on a channel is relatively independent of all other channels. In the Sarnoff VDM this cortex .ltering 
stage is accomplished by transforming the image into a Laplacian pyramid and applying a set of oriented 
.lters. The net result is a pyramidal image decom­position that is tuned to seven spatial frequencies 
and four angular directions. This transform is the primary source of expense in the Sarnoff VDM. In order 
to reduce the cost of this operation we decided to model the spatial frequency and orientation selectivity 
of the visual system through the use of a simple Haar wavelet transform. A number of other wavelet bases 
were considered, in­cluding Daubechies family of wavelets [6] and the biorthogonal bases of Cohen, et. 
al. [4]. However, these transforms were dis­carded due to their expense. The two-dimensional non-standard 
Haar decomposition can be expressed as:  ey[ s [x cf y th ey[ xfh ey[ qfh yxy ea[ xfyh ey[ x qfyah] 
t y s [ s [x f y h ey[ xfhi 1ey[ qfh yxy ea[ xf yhi 1ey[ x qfyah] t y q [ s [x cf y th e[ fh e[ 
qfhi xyxy e[ f yhi 1e[ qfyah] t xyx q [ s [x cf y th e[ fhi 1e[ qfhi xyxy e[ fyh e[ qfyah] t 1 xyx 
ey[ where speci.es the lowpass coef.cients of the level h Haar Figure 3: Effect of chromatic aberration 
on the S-cone photopig­ment sensitivity. Right diagram takes cross sections through left diagram at intervals 
of 4 cpd (from Marimont and Wandell [19]).  [s [ [ basis,,andare the detail coef.cients of the three 
two­dimensional level h Haar wavelets, and ey[f a a a[ [ ts xfyh corresponds to the response of either 
the small, medium or long receptors at a pixel location (whereh e1v e1h e represents the number of levels 
in the quad-tree). This decomposition is carried out for each of the S, M and L channels and is stored 
in a quad-tree representation with the highest frequency details at the bottom and lowest frequency at 
the top. The detail coef.cients of the Haar transform constitute our cortex transform. These detail terms 
represent variations in the image that are localized in space, frequency and angular direction. The frequency 
selectivity of the detail terms at a given level of the tree is de.ned as the frequency in cycles per 
degree (cpd) to which the wavelet at that level is optimally tuned. The detail coef.cients are tuned 
to three angular directions as illustrated in Figure 2. We acknowledge that the poor .ltering and limited 
orientation tuning of the Haar wavelet is a limitation of this approach. However, the ef.ciency gains 
are substantial. In the next stage labeled local contrast the eye s non-linear contrast response to light 
is modeled. This is accomplished by dividing the detail coef.cients of each color channel by the as­sociated 
lowpass coef.cient one level up in the quad-tree. This operation produces a local contrast value which 
is functionally equivalent to the standard cone contrast calculation of ncc , na;; , c and nal . It additionally 
avoids the assumption, found in other l models [5,7], that the eye can adapt at the resolution of a pixel. 
The next step in the visual model incorporates the effect of chromatic aberration. Chromatic aberration 
describes the defo­cusing of light as a function of wavelength by the optics of the eye. The original 
chromatic contrast sensitivity experiments per­formed by Mullen [22] corrected for chromatic aberration. 
In order to accurately apply the results of her work it is necessary to reintroduce this effect. Chromatic 
aberration most strongly af­fects the sensitivity of the short wavelength receptors. The loss of sensitivity 
in the short wavelength receptors is demonstrated in Figure 3. This illustration shows that the sensitivity 
drops to less than half its original value at 4 cpd and is virtually non-existent at frequencies higher 
than 8 cpd. Chromatic aberration is simu­lated in our model by lowpass .ltering the local contrasts of 
the S cone receptors as a function of spatial frequency. The lowpass .lter used was generated by a .t 
to the data of Marimont and Wandell [19]. The following stage in the model consists of a transformation 
of the cone contrasts to an opponents contrast space. This space consists of a single achromatic (A) 
and two opponent color chan­nels (ss and s ). There is signi.cant evidence that the signals produced 
by the cones undergo this type of transformation. The transformation matrix used to convert the cone 
contrasts is found in [2]. The sixth step of the vision model, labeled CSF .ltering, in-Figure 2: Angular 
tuning of Haar coef.cients. Figure 4: Achromatic and chromatic contrast sensitivity functions and comparison 
against a uniform gray .eld. corporates variations in achromatic and chromatic contrast sensi­tivity 
as a function of spatial frequency. The gray scale contrast sensitivity illusion in the top left of Figure 
4 demonstrates the sen­sitivity variation of the achromatic channel. In this demonstration contrast increases 
logarithmically from top to the bottom of the image and frequency increases logarithmically from left 
to right. The subjective contour in the shape of an inverted U that can be seen along the top of the 
image is generated by the points at which the contrast of the sinusoidal grating becomes just noticeably 
dif­ferent from the gray background. This image demonstrates that achromatic sensitivity reaches its 
peak at around 4 cpd and drops off signi.cantly at higher and lower spatial frequencies. The equa­tion 
for the achromatic contrast sensitivity function that is used in our model is presented by Barten [1]. 
The middle and bottom images on the left side of Figure 4 con­ s tain contrast sensitivity illusions 
for the s and s color channels respectively. In these illustrations it should be observed that the peak 
sensitivity to chromatic contrast is less than that for achro­matic contrast, and that the cutoff for 
the chromatic sensitivity function occurs at a much lower spatial frequency than in the achromatic illustration. 
The reader should also see that the shape of the subjective contour is strictly lowpass, with no drop-off 
at low spatial frequencies. The fact that the cutoff for the s s color channel is less than that for 
the s is the result of axial chromatic aberration which was modeled at an earlier stage of the algorithm. 
In our algorithm the chromatic contrast sensitivity function is modeled with a Butterworth .lter that 
has been .t to the chromatic contrast sensitivity data from Mullen [22]. At this stage in the algorithm 
the square of the contrast for each of the A , ss and s channels is multiplied by the square of that 
channel s contrast sensitivity as a function of spatial fre­quency. The square of the contrast and contrast 
sensitivity func­tion is used to model the energy response that occurs for complex cells, as described 
in the Sarnoff VDM. This transformation has the result of making the model less sensitive to the exact 
position of an edge, which is a property shared by the human visual system as well [17]. The illustrations 
on the right side of Figure 4 show the output of our visual difference predictor when comparing the contrast 
sensitivity illusions on the left side of this .gure with a constant gray image. White indicates areas 
of large visual dif­ference while black denotes regions of low visual difference. In these images we 
see that the algorithm is able to correctly predict the shape and cutoff of the subjective contour. The 
next stage of the model labeled masking transducer, in­corporates the effect of visual masking. Masking 
describes the phenomena where strong signals of a given color, frequency and orientation can reduce the 
visibility of similar signals. This prop­erty of the visual system is incorporated through the use of 
a non-linear, sigmoid transducer described in the Sarnoff VDM A 2 m n A a A m e 2 [ f where T(A) is 
the transducer output and A is the weighted contrast output from the previous stage of the model. This 
transducer is applied independently to the contrasts of each of theA , ss , and s color channels. In 
computer graphic renderings, error primarily is manifested in the form of noise. Therefore, it is worthwhile 
to give special attention to the issue of noise masking. Noise in the achromatic channel is often the 
result of aliasing due to undersampling or can result from poor Monte Carlo light source integration. 
An illustration of the grayscale contrast sensitivity illusion perturbed by the introduction of random 
noise is given in the upper left of Figure 5. In this image the noise is readily apparent above and to 
the sides of the subjective contrast sensitivity contour, but is less perceptible in areas where the 
sinusoidal grating is visible. This result occurs because the strong visual sensitivity to these frequen­cies 
masks the presence of a portion of the frequency spectrum of the noise. The image in the upper right 
of this .gure shows the output of our visual difference predictor when comparing the original contrast 
sensitivity illustration to the contrast sensitivity illustration with noise added. In this image we 
see that the visual model has correctly predicted that the error is less visible in the lower-center 
region where masking is strongest. Noise in the chromatic channels can arise when Monte Carlo integration 
is performed with multiple colored lights or is used to compute diffuse inter-re.ections. Fine grained 
noise is not masked signi.cantly in the color channels due to the lower frequency cutoff for the chromatic 
contrast sensitivity function. However, masking can still have a strong affect on the visibility of coarse 
grained noise. In the middle left and bottom left images in Fig­ure 5 we have overlaid the chromatic 
contrast sensitivity illusions with coarse grained noise. In these illustrations the noise is very apparent 
in regions where sensitivity to the chromatic grating is low (top and right of the images), but less 
visible in regions where the chromatic grating is very perceptible (lower left of the images). The images 
on the right once again show the output of the visual difference predictor when comparing the images 
with noise to the original chromatic contrast sensitivity illustrations. In these illus­trations we see 
that the algorithm has correctly predicted that the coarse grained noise is less perceptible in the lower 
left region of Figure 5: Achromatic and chromatic contrast sensitivity func­tions with noise, and comparison 
with noiseless contrast sensitiv­ity functions. the images. In the next stage of the model labeled spatial 
pooling, the transducer outputs are .ltered over a small neighborhood of sur­rounding nodes at each level 
of the quad-tree. This is similar to the pooling operation performed in the Sarnoff VDM. It captures 
the fact that foveal human sensitivity is at a maximum for sine wave gratings containing at least 5 cycles. 
The pooling .lter that is used in our model is:g sss  s[ 1s[ sss 1r1]1 n q sss s[ 1s[ The decision 
to use a 3x3 .lter rather than the 5x5 .lter speci­.ed in the Sarnoff VDM was made to improve the speed 
of the algorithm. In the .nal distance summation stage the differences between the pooling stages of 
the two input images are computed and used to generate a visual difference map. The local visual difference 
at each node of the quad-tree is de.ned to be the sum across all orientations (0 ) and color channels 
(e ) of the differences of the pooling stages (Ps and P ) of the two images raised to the 2.4 Figure 
6: Top -Original chapel (left) and chapel with sinusoidal distortion (right). Bottom -Results of the 
Sarnoff VDM (left) and simpli.ed vision model (right) visual difference predictions. power:  e n r 
 s 0fbe hi P0fbe h m ssP The .nal difference map is generated by accumulating visual dif­ferences across 
levels. This is accomplished by summing local difference down each path in the quad-tree and storing 
the result in the leaves. The visual difference map that is the output of the algorithm is given by the 
leaf differences raised to the 1/2.4 power. Figure 6 shows a comparison between the results of the origi­nal 
Sarnoff VDM and our simpli.ed version for a set of complex images. The inputs are illustrated in the 
top row of the .gure and consist of a chapel image and the chapel image perturbed by a si­nusoidal grating. 
A visual comparison of these two images shows that the sinusoidal distortion is most evident in the dark 
regions at the base of the chapel. This is due to the eye s non-linear con­trast response to light. Within 
the arches at the top of the chapel, there is no perceptible difference between the two images. This 
is because the lattice-work in these regions masks the presence of the sinusoidal grating. The visual 
difference map that is produced by the new algorithm contains a number of blocking artifacts that are 
caused by the Haar wavelet decomposition. However, the re­sults of both algorithms are similar and correspond 
well with a subjective comparison of the input images. The Sarnoff VDM processed one channel in a gray-scale 
image representation and the new model processed three color channels. The new model 1 qll qt r executed 
inof the time of the original model. 4 Adaptive Sampling Algorithm An adaptive sampling algorithm has 
been developed that is based on the visual model described in the preceding section. This algorithm receives 
sample values as input, and speci.es the placement of samples at the image plane as output. The goal 
of Figure 7: Block diagram of adaptive sampling algorithm. the adaptive algorithm is to iteratively 
place each sample at the location that currently contains the largest perceptual error. The key to developing 
this perceptually based adaptive sam­pling algorithm comes from two primary insights. The .rst is that 
an estimate of the image and its error can be used to construct two boundary images that may be used 
as input into a visual difference predictor. The output of this difference prediction can then be used 
to direct the placement of subsequent samples. The second insight is that a given sample only affects 
the value and accuracy of a very limited number of terms at each level of a Haar wavelet image representation. 
This fact makes the algorithm tractable because it implies that only a small number of operations are 
necessary to re.ne the image approximation, its error estimate and the visual difference prediction for 
any given sample. The algorithm proceeds through a few basic steps. First, as samples are taken of the 
scene, a Haar wavelet image approxi­mation is generated and re.ned. Next, a multi-resolution error estimate 
is developed and similarly re.ned. This error estimate is expressed in terms of the variance of the detail 
terms in the Haar representation. The image approximation and error estimate is then used to construct 
two boundary images which serve as input to the visual difference predictor. The output of the difference 
predictor is accumulated in a hierarchical tree. The nodes of this tree specify the maximum visual difference 
present at the current nodes and the children below it. This tree is traversed choosing the branch with 
the largest visual difference in order to determine the location on the image plane with the greatest 
perceptual error. A block diagram of the algorithm is illustrated in Figure 7. As samples are taken their 
values are .rst transformed from CIE XYZ to SML space in the step labeled cone fundamentals. The Haar 
image representation and its error estimate are constructed in this space. In the re.ne cortex representation 
stage the Haar image ap­proximation is created and re.ned. This is done through a tech­nique similar 
to the splat and pull method used by Gortler, et. al. [11]. The Haar image representation is stored in 
a quad-tree data structure. The leaves of this structure are de.ned to contain the intensity of single 
pixels in the image plane and the interior nodes contain the lower resolution lowpass and detail terms 
of the Haar representation. As a sample is passed into this stage it is splatted at the leaf containing 
the sample. The intensity at this leaf is simply the average of the samples taken within the pixel it 
is de.ned to cover. The lower resolution lowpass and detail terms are generated by pulling the updated 
leaf intensity up through the tree. During this process, if all children of a node contain at least a 
single sample, then the lowpass and detail terms are given by Equation 1. If only a single child contains 
a sample, then the detail terms are left unde.ned and the lowpass term is set equal to the lowpass of 
the child containing the sample. If only two or three children contain samples, then a simple scheme 
is used to .t the lowpass and one or two detail terms, respectively, to the values of the de.ned children. 
In this manner the image representation is gradually resolved as samples are taken of the scene. It is 
also worth noting that this process is very fast since the addition of a sample only requires the updating 
of a single path up the tree. At the next step labeled re.ne error estimate, the error of the current 
Haar approximation is determined. This process is similar in some respects with the algorithm described 
by Painter and Sloan [23]. The error estimate is expressed in terms of the variance of the lowpass and 
detail coef.cients. For leaf nodes containing at least two samples the variance of the pixel approxi­mation 
is given by the variance of the samples in the leaf divided by the number of samples in the leaf [3,13]. 
The error of the lowpass and detail terms in the interior nodes is de.ned with re­spect to the error 
of their children. If the variance is de.ned for all children of a node, then the variance of the lowpass 
and detail terms at the node is equal to the sum of the variance of the four children divided by 16. 
This result comes from the rule   ht h n q x x (where V denotes variance) and inspection of Equation 
1. If the error is not de.ned for all children of a node and at least 2 samples have been taken, then 
the variance is given by the variance of the samples taken within the node divided by the number of samples 
in the node. As in the case of re.ning the Haar representation, updating the multi-resolution error estimate 
requires that only a single path in the tree be modi.ed for the addition of each sample. The next stage 
in the algorithm labeled construct boundary images is concerned with de.ning the two input images for 
the vi­sual difference predictor. These input images are described by the magnitude of their detail coef.cients 
which are used to determine the local contrast at an early stage in the vision model. Since the image 
approximation and error estimate has only changed along a single path in the tree, the detail terms for 
the boundary images only need to be updated along this path as well. The details for the two boundary 
images are derived from the details in the cur­rent image approximation and the variance of those details. 
The magnitude of the approximated detail speci.es a mean value and the square root of the variance de.nes 
the spread of the standard deviation curve. The magnitudes of the details for the boundary images are 
taken from the 25% and 75% points on this curve. In this manner two boundary images are speci.ed which 
should contain the true value 50% of the time. The boundary images are organized so that image 1 contains 
the detail of minimum en­ergy contrast and image 2 contains the detail of maximum energy contrast. A 
local visual difference prediction is performed at the updated nodes in the next step of the algorithm. 
The detail terms in the boundary images are passed through the local contrast to spatial pooling stages 
of the vision model. The transducer outputs at the current node is stored in the tree for fast re-use 
in the pooling stages of neighbor nodes. In the step labeled update maximum error tree a value is stored 
at each updated node in the quad-tree which represents the maxi­mum visual difference contained at the 
current node and the nodes below it. The local error at a node is de.ned as in the distance summation 
stage of the vision model (i.e. by the sum of visual distance between the boundary images across each 
detail and color channel raised to the 2.4 power). The maximum error is de.ned to be the local error 
plus the largest maximum error contained in either of the four children. The maximum error of the root 
node is raised to the 1/2.4 power and represents the largest visual error contained at any location within 
the image plane. The maximum error in the interior nodes are used to determine which branch of the tree 
contains the greatest visual difference for the purpose of .nding the next location to sample. In the 
.nal stage labeled determine next sample location a sample location is selected at the point of maximum 
visual dif­ference. The location is selected by traversing the quad-tree in a top-down fashion and, at 
each node, selecting the branch of max­imum visual error. This traversal continues until a leaf node 
is encountered or an interior node is found which contains less than eight samples. If a leaf node is 
reached, a sample is randomly placed within it. If the traversal stops at an interior node, then a sample 
location is chosen randomly from a child s quadrant so that the number of samples in each child node 
is balanced to a tolerance of one sample. The discussion thus far has assumed that only a single path 
in the quad-tree is affected by a given sample. However, this is not strictly the case. Due to the local 
contrast and spatial pooling stages of the vision model the modi.cation of one node in the quad-tree 
can have an affect on the visual difference at neigh­boring nodes. One solution to this problem is to 
update multiple paths up the tree. However, this approach was deemed too ex­pensive. Instead the problem 
can be effectively solved by adding a small amount of randomness to the traversal of the maximum error 
tree. As each node in the tree is visited, there is some like­lihood a neighboring node will be chosen 
instead. In this manner, if a particular path is traversed often, there is a chance of selecting neighboring 
paths. This creates the opportunity to incorporate up­dated values into the local contrast and spatial 
pooling calculations for these paths. The algorithm continues recursively until the maximum error of 
the root node drops below a speci.ed tolerance. The output image is reconstructed by simply doing an 
inverse Haar transform of the image representation and converting pixel values from SML to the frame 
buffer space. This technique can also be used to construct an iterative display of the image during the 
progression of the algorithm. 5 Selecting Values for Unknown Quantities A dif.culty with adaptive sampling 
algorithms that are based on the sample variance is knowing when and to what extent to believe the error 
estimate obtained from the samples. This is es­pecially true for the hierarchical variance estimation 
scheme de­scribed in this paper. If the .rst two samples obtained from the scene return exactly the same 
values and therefore have zero vari­ance, can we conclude that the image has been computed exactly and 
stop? What if the image has been sampled densely and two samples from within a particular pixel of the 
image plane are the same, can we say that the intensity of the pixel has been com­puted correctly? A 
person analyzing these two situations would certainly believe that the scene has not been adequately 
sampled in the .rst case, but would probably be willing to stop sampling in the second case. The reason 
for this difference stems from the statistics of natural images. A number of authors have analyzed the 
statistics of images commonly encountered in nature [8,9,24,26]. These authors have found that the frequency 
spectra of natural images is not random, but tends to be highly correlated and contains a 1/f drop-off 
in the magnitude of the frequency terms. Therefore, if only two samples have been taken of a scene, we 
have just begun to compute the low end of the frequency spectra. Based on our experience with images 
found in the natural world, we know that an average image contains higher frequency detail, and therefore 
believe that the scene has not been adequately sampled. Thus, we have some apriori knowledge about the 
error of an image approximation. If a portion of the frequency spectra has not been computed, then, on 
average, the approximation of the image will contain an amount of error that is equivalent to the 1/f 
magnitude of the uncomputed spectra. We can also draw upon the statistics of natural scenes when we must 
choose unknown values for the chromatic channels. The frequency content of naturally occurring spectral 
re.ectances is known to be very low [18]. This means that re.ectances are more likely to be uniform across 
the spectrum than they are to be spikey. The result of this is that the average color in the natural 
world is quite desaturated. This implies that in the absence of other knowledge about the chromatic content 
of an object, setting the chromatic channels close to zero is as good a choice as one can make. The statistics 
of natural images discussed in this section have been employed within our adaptive sampling algorithm. 
This is accomplished by initializing the two boundary images to a uni­form gray for one, and a statistically 
average image for the other. The visual difference predictor is run on these two input images and the 
output is used to seed the visual difference at each node in the quad-tree. Initially, the estimated 
visual difference of the rendering is based on the comparison of the gray and statistically average image. 
As the algorithm progresses and the image ap­proximation and error estimate is calculated at new nodes 
in the tree, the visual difference based on the average statistics is traded for the visual difference 
that is based on the variance and content of the scene samples. 6 Results In this section we discuss 
the results of applying our image synthesis algorithm to three dimensional environments. Simple texture 
mapped disks are considered .rst followed by a scene with more complicated geometry and lighting. Two 
shading tech­niques will be used in these examples, direct and Monte Carlo light source sampling. The 
direct sampling method uses a simple shading algorithm in which point light sources are directly sam­pled 
each time a ray strikes a surface. The Monte Carlo method uses area light sources and blind Monte Carlo 
integration to eval­uate the shading integral. In this approach the incident radiance at a surface point 
is evaluated by spawning a number of rays at random orientations across the positive hemisphere. We realize 
that blind integration is not the most ef.cient means of evaluating the shading integral. However, this 
technique provides a simple means of demonstrating a situation where noise is present within the illumination 
calculation. Figure 8 shows three arrays of texture mapped disks in which the spatial frequency of the 
texture increases from left to right but the contrast of the texture decreases from bottom to top. In 
the top disk array the color of the texture varies along the A axis of Atsss space, in the middle disk 
array along the ss axis, and in the bottom array along the s axis. The three arrays of texture mapped 
disks are rendered using direct light source sampling. In this case there is no noise generated and the 
spatial frequency content of the textures is the primary determinant of the sampling rate that is used. 
All of the disk arrays were rendered to the same visual tolerance. As can be seen in the .gure, the sampling 
den­sity decreases from high frequency to low frequency. Achromatic colors receive far more samples than 
chromatic colors due to the higher spatial frequency cut off of the achromatic contrast sen­ s sitivity 
function, while colors that vary in s are sampled more often than colors that change in s . This difference 
in sampling between the two color channels is clear evidence of the .ltering Figure 8: Sampling densities 
for direct light source sampling. that is done by the visual model due to chromatic aberration in the 
eye. In Figure 9 the achromatic disks from Figure 8 are rendered again using Monte Carlo light source 
sampling. In this case a signi.cant amount of noise is generated and the effect of visual masking becomes 
important. As can be seen from the .gure, the spatial sampling pattern is radically different from the 
direct light source sampling case in Figure 8. While disks with high frequency textures still receive 
the most samples, in this case the low frequency disks also get many samples because the noise can be 
seen on their surfaces. On the other hand, the middle of the spatial frequency range receives relatively 
few samples because the noise is less visible due to masking. In Figure 10 the en­vironment and the lighting 
is made more complex but a similar result is obtained. When there is no noise, high achromatic spa­tial 
frequency transitions receive the most samples. When noise is present, more samples across the entire 
image are required, but fewer are necessary for frequencies where the noise is masked. As a .nal example, 
identical scenes were synthesized to the same visual tolerance using two different rendering techniques. 
As can be seen in Figure 11, the images that resulted are comparable even though the sampling patterns 
and illumination calculations are very different. In the case where direct sampling of the light sources 
is performed, aliasing artifacts are the most prevalent de­fect; while for the scene where Monte Carlo 
sampling of the light sources was done, noise is the dominant problem. However, for a given perceptual 
tolerance, the algorithm holds each type of artifact to a similar level of visual impact. The approach 
taken in this algorithm is to compute the per­ceptual metric for every ray that is cast into the scene. 
The cost to do this computation is 1 ms on a 100 MHz processor. Evaluation of the algorithm on a number 
of different test environments shows that it takes fewer samples than either a uniform sampling method 
or an adaptive approach with an objective error metric (90% less in certain cases). Timing tests reveal 
that the algorithm is able to provide the perceptual stopping criteria demonstrated in Figure 11 while 
remaining competitive with either the uniform sampling or standard adaptive sampling techniques. The 
method is faster than either uniform or standard adaptive sampling on every envi­ronment where it was 
tested, but it was not the overall winner in all cases. Additional work is necessary to exploit the algorithm 
s excellent spatial sampling rates and determine the optimal num­ber of samples to be taken between evaluations 
of the perceptual metric.  7 Conclusion An existing vision model has been incorporated into an im­age 
synthesis algorithm to control where samples are taken as a picture is created. The results obtained 
with the new algorithm on three dimensional scenes track the results obtained using the visual model 
by itself on two dimensional images. The impact on the execution time of the rendering program has been 
minimized while the amount of memory required has been increased. The contributions of this work can 
be summarized as follows: 1. A new image quality model has been developed. This new model is an ef.cient 
implementation of an existing algo­rithm. It executes in a fraction of the time of the origi­nal method 
without a signi.cant sacri.ce in accuracy. The model has also been extended for color including the effect 
of chromatic aberration in the optics of the eye. 2. An image synthesis system has been created that 
directly computes a wavelet representation for an image. This is a functional (instead of an explicit 
pixel based) scheme for describing a picture that facilitates the computation of a visual metric. It 
also permits the use of statistical informa­tion regarding the frequency distribution of natural images 
to estimate values in regions where samples have yet to be taken. In the same manner, guesses regarding 
unsampled colors were improved by using an opponents color space to store color. 3. A perceptually based 
approach to image synthesis has been produced. An image quality model was used to decide where to take 
the next sample in a picture. This can result in a savings in execution time because samples are only 
  Figure 9: Sampling density for Monte Carlo light source sampling.  taken in areas where there are 
visible artifacts. The image quality model is also used to decide when enough samples have been taken 
across the entire image. This provides a visual stopping condition and makes it possible to employ different 
rendering algorithms but still produce equivalent pictures. This work represents a .rst attempt to imbed 
a sophisticated image processing vision model into an image synthesis algorithm. While the results are 
encouraging it is clear that the approach taken here puts a certain amount of overhead onto every ray 
that is cast into the scene. An alternative tactic might be to initially sample the image at a low rate 
and compute the visual difference map from these values. The visual difference map can then be used to 
select regions of the image which require further sampling. The use of the imbedded version of the vision 
model might be saved until the image is more fully developed and the masking effects have become completely 
apparent.  8 Acknowledgements The authors would like to thank Jae H. Kim for his help in creating Figures 
8, 9, 10, and 11 and for his assistance in as­sembling all of the color .gures in this paper. This research 
was funded by the National Science Foundation under grant number CCR 96-19967.  9 References [1] Barten, 
P. G. J., The Square Root Integral (SQRI): A New Metric to Describe the Effect of Various Display Parameters 
on Perceived Image Quality, Human Vision, Visual Processing, and Digital Display, Proc. SPIE, Vol. 1077, 
pp. 73-82, 1989. [2] Bolin, M. R. and Meyer G. W., A Frequency Based Ray Tracer, Computer Graphics, Annual 
Conference Series, ACM SIG-GRAPH, pp. 409-418, 1995. [3] Bolin, M. R. and Meyer G. W., An Error Metric 
for Monte Carlo Ray Tracing, Rendering Techniques 97, J. Dorsey and P. Slusallek, Editors, Springer-Verlag, 
New York, pp. 57-68, 1997. [4] Cohen, A., Daubechies, I., and Feauveau, J. C., Biorthogonal Bases of 
Compactly Supported Wavelets, Communications on Pure and Applied Mathematics, Vol. 45, No. 5, pp. 485-500, 
1992. [5] Daly, S., The Visible Differences Predictor: An Algorithm for the Assessment of Image Fidelity, 
Digital Images and Human Vision, A. B. Watson, Editor, MIT Press, Cambridge, MA, pp. 179-206, 1993. [6] 
Daubechies, I., Orthonormal Bases of Compactly Supported Wavelets, Communications on Pure and Applied 
Mathemat­ics, Vol. 41, No. 7, pp. 909-996, 1988. [7] Ferwerda, J. A., Shirley, P., Pattanaik, S. N., 
and Greenberg, D. P., A Model of Visual Masking for Computer Graphics, Computer Graphics, Annual Conference 
Series, ACM SIGGRAPH, pp. 143­ 152, 1997.  [8] Field, D. J., Relations Between the Statistics of Natural 
Images and theResponse Properties of Cortical Cells, J. Opt. Soc. Am. A, Vol. F 4, pp. 2379-2394, 1987. 
[9] Field, D. J., What the Statistics of Natural Images Tell Us About Visual Coding, Human Vision, Visual 
Processing, and Digital Display, Proc. SPIE, Vol. 1077, pp. 269-276, 1989. [10] Gibson, S. and Hubbold, 
R. J., Perceptually-Driven Radiosity, Computer Graphics Forum, Vol. 16, pp. 129-140, 1997. [11] Gortler, 
S. J., Grzeszczuk, R., Szeliski, R., and Cohen, M. F., The Lumigraph, Computer Graphics, Annual Conference 
Series, ACM SIGGRAPH, pp. 43-54, 1996. [12] Kirk, D. and Arvo, J., Unbiased Sampling Techniques for Im­age 
Synthesis, Computer Graphics, Annual Conference Series, ACM SIGGRAPH, pp. 153-156, 1991. [13] Lee, M. 
E., Redner, R. A., and Uselton, S. P., Statistically Opti­mized Sampling for Distributed Ray Tracing, 
Computer Graphics, Annual Conference Series, ACM SIGGRAPH, pp. 61-67, 1985. [14] Legge, G. E. and Foley, 
J. M., Contrast Masking in human vision, Journal of the Optical Society of America, Vol. 70, pp. 1458­1470, 
1980. [15] Li, B., An Analysis and Comparison of Two Visual Discrimination Models, Master s Thesis, University 
of Oregon, June 1997. [16] Li, B., Meyer, G. W., and Klassen, R. V., A Comparison of Two Image Quality 
Models, to appear in Human Vision and Electronic Imaging III, B. E. Rogowitz and T. N. Pappas, Editors, 
Proc. SPIE, Vol. 3299, 1998. [17] Lubin, J., A Visual Discrimination Model for Imaging System De­sign 
and Evaluation, Vision Models for Target Detection and Recognition, Eli Peli, Editor, World Scienti.c, 
New Jersey, pp. 245­283, 1995. [18] Maloney, L. T., Evaluation of linear models of surface spectral re.ectance 
with small numbers of parameters, J. Opt. Soc. Am. A, Vol. 3, pp. 1673-1683. 1986. [19] Marimont, D. 
H. and Wandell, B. A., Matching Color Images: The Impact of Axial Chromatic Aberration, J. Opt. Soc. 
Am. A, Vol. 12, pp. 3113-3122, 1993. [20] Meyer, G. W. and Liu, A., Color Spatial Acuity Control of a 
Screen Subdivision Image Synthesis Algorithm, Human Vision, Visual Processing, and Digital Display III, 
Bernice E. Rogowitz, Editor, Proc. SPIE, Vol. 1666, pp. 387-399, 1992. [21] Mitchell, D. P., Generating 
Antialiased Images at Low Sampling Densities, Computer Graphics, Annual Conference Series, ACM SIGGRAPH, 
pp. 65-72, 1987. [22] Mullen, K. T., The Contrast Sensitivity of Human Colour Vision to Red-Green and 
Blue-Yellow Chromatic Gratings, J. Physiol. (Lond.), Vol. 359, pp. 381-400, 1985. [23] Painter, J. and 
Sloan, K. Antialiased Ray Tracing by Adaptive Pro­gressive Re.nement, Computer Graphics, Annual Conference 
Series, ACM SIGGRAPH, pp. 281-288, 1989. [24] Ruderman, D. L., Origins of Scaling in Natural Images, 
Human Vision, Visual Processing, and Digital Display, Proc. SPIE, Vol. 2657, pp. 120-131, 1996. [25] 
Rushmeier, H., Ward, G., Piatko, C., Sanders, P., and Rust, B., Comparing Real and Synthetic Images: 
Some Ideas About Metrics, Rendering Techniques 95, P. M. Hanrahan and W. Purgathofer, Editors Springer-Verlag, 
New York, pp. 82-91, 1995. [26] Schreiber, W. F., Fundamentals of Electronic Imaging Systems, Springer-Verlag: 
Berlin Heidelberg, 1993.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280925</article_id>
		<sort_key>311</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>30</seq_no>
		<title><![CDATA[Efficient simulation of light transport in scences with participating media using photon maps]]></title>
		<page_from>311</page_from>
		<page_to>320</page_to>
		<doi_number>10.1145/280814.280925</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280925</url>
		<keywords>
			<kw><![CDATA[anisotropic scattering]]></kw>
			<kw><![CDATA[global illumination]]></kw>
			<kw><![CDATA[light transport]]></kw>
			<kw><![CDATA[multiple scattering]]></kw>
			<kw><![CDATA[nonhomogeneous media]]></kw>
			<kw><![CDATA[participating media]]></kw>
			<kw><![CDATA[photo-realism]]></kw>
			<kw><![CDATA[photon map]]></kw>
			<kw><![CDATA[photon tracing]]></kw>
			<kw><![CDATA[ray marching]]></kw>
			<kw><![CDATA[rendering]]></kw>
			<kw><![CDATA[volume caustics]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Monte Carlo</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010349</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003670.10003677</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic reasoning algorithms->Markov-chain Monte Carlo methods</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003670.10003682</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic reasoning algorithms->Sequential Monte Carlo methods</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14219695</person_id>
				<author_profile_id><![CDATA[81100640205]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Henrik]]></first_name>
				<middle_name><![CDATA[Wann]]></middle_name>
				<last_name><![CDATA[Jensen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[mental images, Berlin, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14106347</person_id>
				<author_profile_id><![CDATA[81100285050]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Per]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Christensen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[mental images, Berlin, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[James R. Arvo. Backward ray tracing. ACM SIGGRAPH 86 Course Notes Developments in Ray Tracing, 12, 1986.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>361007</ref_obj_id>
				<ref_obj_pid>361002</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Jon L. Bentley. Multidimensional binary search trees used for associative searching. Communications of the ACM, 18(9):509- 517, 1975.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[N. Bhate and A. Tokuta. Photorealistic volume rendering of media with directional scattering. Proceedings of the 3rd Eurographics Workshop on Rendering, pages 227-245, 1992.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Philippe Blasi, Bertrand Le Sa~c, and Christophe Schlick. A rendering algorithm for discrete volume density objects. Computer Graphics Forum (Proceedings of Eurographics '93), 12(3):201- 210, 1993.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801255</ref_obj_id>
				<ref_obj_pid>800064</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[James F. Blinn. Light reflection functions for simulation of clouds and dusty surfaces. Proceedings of A CM SIGGRAPH 82, pages 21-29, 1982.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>732108</ref_obj_id>
				<ref_obj_pid>647651</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Per H. Christensen. Global illumination for professional 3D animation, visualization, and special effects. Rendering Techniques '97 (Proceedings of the 8th Eurographics Workshop on Rendering), pages 321-326, 1997.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[David Doubilet. Light in the Sea. National Geographic, 1989.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>259233</ref_obj_id>
				<ref_obj_pid>259081</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[David S. Ebert. Volumetric modeling with implicit functions (A cloud is born). Visual Proceedings of ACM SIGGRAPH 97, page 147, 1997. Technical Sketch.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>184932</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[David S. Ebert, F. Kenton Musgrave, Darwyn Peachey, Ken Perlin, and Steven Worley. Texturing and Modeling: A Procedural Approach. AP Professional, 1994.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>527570</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Andrew S. Glassner. Principles of Digital Image Synthesis. Morgan Kaufmann, San Francisco, CA, 1995.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122740</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Pat Hanrahan, David Salzman, and Larry Aupperle. A rapid hierarchical radiosity algorithm. Proceedings of A CM SIG- GRAPH 91, pages 197-206, 1991.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97895</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Paul Heckbert. Adaptive radiosity textures for bidirectional ray tracing. Proceedings of ACM SIGGRAPH 90, pages 145-154, 1990.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>275461</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Henrik Wann Jensen. Global illumination using photon maps. Rendering Techniques '96 (Proceedings of the 7th Eurographics Workshop on Rendering), pages 21-30, 1996.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Henrik Wann Jensen. The Photon Map in Global Illumination. PhD thesis, Technical University of Denmark, Lyngby, Denmark, 1996.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>241056</ref_obj_id>
				<ref_obj_pid>241020</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Henrik Wann Jensen. Rendering caustics on non-Lambertian surfaces. Proceedings of Graphics Interface '96, pages 116-121, 1996.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808594</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[James T. Kajiya and Brian P. von Herzen. Ray tracing volume densities. Proceedings of ACM SIGGRAPH 84, pages 165-174, 1984.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>35071</ref_obj_id>
				<ref_obj_pid>35068</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[R. Victor Klassen. Modeling the effect of the atmosphere on light. ACM Transactions on Graphics, 6(3):215-237, 1987.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Eric P. Lafortune and Yves D. Willems. Bi-directional path tracing. Proceedings of Compugraphics '93, pages 145-153, 1993.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>275468</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Eric P. Lafortune and Yves D. Willems. Rendering participating media with bidirectional path tracing. Rendering Techniques '96 (Proceedings of the 7th Eurographics Workshop on Rendering), pages 92-101, 1996.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Eric Langu6nou, Kadi Bouatouch, and Michelle Chelle. Global illumination in presence of participating media with general properties. Proceedings of the 5th Eurographics Workshop on Rendering, pages 69-85, 1994.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5515</ref_obj_id>
				<ref_obj_pid>5513</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Nelson L. Max. Light diffusion through clouds and haze. Compurer Vision, Graphics, and Image Processing, 33(3):280-292, March 1986.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Nelson L. Max. Efficient light propagation for multiple anisotropic volume scattering. Proceedings of the 5th Eurographics Workshop on Rendering, pages 87-104, 1994.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Gustav Mie. Beitrgge zur optik trfiber medien, speziell kolloidaler metallSsungen. Annalen der Physik, 25(3):377-445, 1908.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>130653</ref_obj_id>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Harald Niederreiter. Random Number Generation and Quasi- Monte Carlo Methods, volume 63 of Regional Conference Seties in Applied Mathematics. Society for Industrial and Applied Mathematics (SIAM), Philadelphia, Pennsylvania, 1992.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237277</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Tomoyuki Nishita, Yoshinori Dobashi, and Eihachiro Nakamae. Display of clouds taking into account multiple anisotropic scattering and sky light. Proceedings of A CM SIGGRAPH 96, pages 379-386, 1996.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[S. N. Pattanaik and S. P. Mudur. Computation of global illumination in a participating medium by Monte Carlo simulation. Journal on Visualization and Computer Animation, 4(3):133- 152, 1993.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>732114</ref_obj_id>
				<ref_obj_pid>647651</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Frederic P6rez, Xavier Pueyo, and Francois X. Sillion. Global illumination techniques for the simulation of participating media. Rendering Techniques '97 (Proceedings of the 8th Eurographics Workshop on Rendering), pages 309-320, 1997.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325247</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Ken Perlin. An image synthesizer. Proceedings of A CM SIG- GRAPH 85, pages 287-296, 1985.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>914720</ref_obj_id>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Holly E. Rushmeier. Realistic Image Synthesis for Scenes with Radiatively Participating Media. PhD thesis, Cornell University, Ithaca, New York, 1988.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Holly E. Rushmeier. Rendering participating media: Problems and solutions from application areas. Proceedings of the 5th Eurographics Workshop on Rendering, pages 35-56, 1994.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37436</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Holly E. Rushmeier and Kenneth E. Torrance. The zonal method for calculating light intensities in the presence of a participating medium. Proceedings of ACM SIGGRAPH 87, pages 293-302, 1987.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Robert Siegel and John R. Howell. Thermal Radiation Heat Transfer, 3rd Edition. Hemisphere Publishing Corporation, New York, 1992.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Jos Stam. Multiple scattering as a diffusion process. Rendering Techniques '95 (Proceedings of the 6th Eurographics Workshop on Rendering), pages 41-50, 1995.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Eric Veach and Leonidas Guibas. Bidirectional estimators for light transport. Proceedings of the 5th Eurographics Workshop on Rendering, pages 147-162, 1994.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258775</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Eric Veach and Leonidas J. Guibas. Metropolis light transport. Proceedings of ACM SIGGRAPH 97, pages 65-76, 1997.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
			<ref>
				<ref_obj_id>256158</ref_obj_id>
				<ref_obj_pid>256157</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Bruce Walter, Philip M. Hubbard, Peter Shirley, and Donald P. Greenberg. Global illumination using local linear density estimation. ACM Transactions on Graphics, 16(3):217-259, 1997.]]></ref_text>
				<ref_id>36</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192286</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Gregory J. Ward. The RADIANCE lighting simulation and rendering system. Proceedings of A CM SIGGRAPH 94, pages 459- 472, 1994.]]></ref_text>
				<ref_id>37</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97920</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Mark Watt. Light-water interaction using backward beam tracing. Proceedings of ACM SIGGRAPH 90, pages 377-385, 1990.]]></ref_text>
				<ref_id>38</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Copyright &#38;#169;1998 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers or to redistribute to lists, requires specific permission and/or a fee. E.cient Simulation 
of Light Transport in Scenes with Participating Media using Photon Maps Henrik Wann Jensen Per H. Christensen 
mental images* Abstract This paper presents a new method for computing global il­lumination in scenes 
with participating media. The method is based on bidirectional Monte Carlo ray tracing and uses photon 
maps to increase e.ciency and reduce noise. We re­move previous restrictions limiting the photon map 
method to surfaces by introducing a volume photon map contain­ing photons in participating media. We 
also derive a new radiance estimate for photons in the volume photon map. The method is fast and simple, 
but also general enough to handle nonhomogeneous media and anisotropic scattering. It can e.ciently simulate 
e.ects such as multiple volume scattering, color bleeding between volumes and surfaces, and volume caustics 
(light re.ected from or transmitted through specular surfaces and then scattered by a medium). The photon 
map is decoupled from the geometric representa­tion of the scene, making the method capable of simulat­ing 
global illumination in scenes containing complex objects. These objects do not need to be tessellated; 
they can be in­stanced, or even represented by an implicit function. Since the method is based on a bidirectional 
simulation, it au­tomatically adapts to illumination and view. Furthermore, because the use of photon 
maps reduces noise and aliasing, the method is suitable for rendering of animations. CR Descriptors: 
I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism color, shading, shad­owing, and texture; 
raytracing; I.6.8 [Simulation and Modeling]: Types of Simulation Monte Carlo. Additional keywords: participating 
media, light trans­port, global illumination, multiple scattering, volume caus­tics, nonhomogeneous media, 
anisotropic scattering, render­ing, photo-realism, photon tracing, photon map, ray march­ing. *mental 
images GmbH &#38; Co. KG, Fasanenstraße 81, D-10623 Berlin, Germany. E-mail: {henrik,per}@mental.com. 
1 Introduction A physically correct simulation of light transport in partici­pating media is necessary 
to render realistic images of fog, clouds, dusty air, smoke, .re, silty water, etc. Such images are important 
for applications dealing with the visibility of objects (for example road signs in fog or .re exit signs 
in smoke-.lled rooms), for visualization of underwater scenes, and for high-quality visual special e.ects. 
See [30] for a more detailed discussion of the application areas. The .rst methods taking participating 
media into ac­count only considered direct illumination (single scatter­ing) [5, 17, 21]. This approximation 
is applicable to opti­cally thin media such as the atmosphere (where secondary scattering is less important), 
but fails to capture important e.ects in optically thick media such as clouds. Single scat­tering in 
water has been simulated in [38] where refracted light from a water surface is approximated using polygonal 
illumination volumes. Multiple scattering has been simulated with .nite ele­ment, point collocation, 
and Monte Carlo methods. Finite element methods divide participating media into discrete volume elements 
and simulate the exchange of light between these volume elements. Point collocation methods choose a 
set of points in the volume and compute the illumi­nation at these points. Isotropic scattering can be 
simulated by representing the radiance from each volume element or point by one value [29, 31]. To simulate 
anisotropic scat­tering, directional distributions are needed at each volume element or point. Some methods 
use spherical harmonics [3, 16, 33], while others divide the sphere of directions into a .nite number 
of solid angles [20, 22, 25]. Finite element and point collocation methods are most useful when simulating 
softly illuminated isotropic media. The simulation of sharp illumination edges requires many volume elements, 
and di­rectional distributions require complex representations of the illumination. In both cases a huge 
amount of memory is re­quired. Monte Carlo methods are more versatile and require less memory. However, 
pure Monte Carlo path tracing (includ­ing bidirectional path tracing) uses signi.cant amounts of computation 
time to render images without noise, particu­larly in the presence of participating media [19]. One way 
to speed up rendering is to use particle tracing combined with a discretization of the scene and the 
volume for representing the illumination [26]. However, this imposes the same lim­itations on the algorithm 
as is the case with .nite element methods. A hybrid method [29] uses a .nite element preprocess to increase 
the e.ciency of Monte Carlo rendering. This makes the method less general than pure Monte Carlo techniques 
and imposes limitations on the supported types of scattering. The .nite element preprocess increases 
memory usage and makes it necessary to discretize the volumes. Even though this is more e.cient than 
pure Monte Carlo rendering, it still uses considerable amounts of computation time to render images without 
noise. A detailed overview of previous solution methods and ad­ditional references can be found in [27]. 
None of the existing methods can e.ciently simulate e.ects such as volume caus­tics where light is focused 
in a medium. Furthermore, all of the methods except the Monte Carlo path tracing methods are based on 
a tight coupling of geometry and volumes with the simulation of light transport. This makes them unsuit­able 
for complex scenes. In this paper we present an extension of the photon map method for global illumination 
[13, 14] to scenes with par­ticipating media. We introduce a volume photon map con­taining photons (particles 
with energy) within participating media and derive a formula for estimating radiance in a par­ticipating 
medium based on these photons. We also present techniques for using this radiance estimate to e.ciently 
ren­der e.ects such as multiple volume scattering, color bleeding between surfaces and volumes, and volume 
caustics. Volume caustics are formed when light re.ected from or transmitted through specular surfaces 
is scattered in a medium. An ex­ample of volume caustics is the beams of light in a silty un­derwater 
environment [7]. The use of photons makes it easy to simulate homogeneous as well as nonhomogeneous media, 
and the fact that we include the incoming direction with each photon allows us to directly render media 
with anisotropic scattering. The photon map is decoupled from the geometry and the volumes, and it is 
capable of handling any type of representation that can be ray traced. This makes it possi­ble to render 
for example an implicit function representing a cloud without using any geometry. Another notable aspect 
of the method is the automatic adaptation to illumination and view: photons are only stored in the illuminated 
parts of the scene and photon map lookups are only done where required for rendering. The photon map 
approach provides the same .exibility as pure Monte Carlo path tracing, but is signi.cantly more e.cient. 
It has very little noise and aliasing, making it suitable for rendering animations with participating 
media. The remainder of the paper is organized as follows: In sec­tion 2, we give an overview of light 
transport in participating media. The photon map method for surfaces is summarized in section 3, and 
in section 4 we describe our new extension for handling participating media. In section 5, we present 
our results, and in section 6 we discuss the method and ideas for future work. Lastly, in section 7 we 
close with a conclu­sion. 2 Light transport in participating media Light transported through a participating 
medium is af­fected by emission, in-scattering, absorption, and out­scattering [32]. Taking these four 
terms into account, the change in (.eld) radiance L at a point x in direction .. is .L(x,..) = o(x) Le(x,..)+ 
.(x)Li(x,..) .x -o(x) L(x,..) -.(x) L(x,..) , (1) where Le is the emitted radiance, Li is the in-scattered 
radi­ance, o is the absorption coe.cient, and . is the scattering coe.cient. This equation holds for 
each wavelength sepa­rately, but we will ignore wavelength here. If the scatter­ing and absorption coe.cients 
are constant throughout the medium, we call the medium homogeneous or uniform. The last two terms of 
equation (1), for absorption and out-scattering, can be combined to an extinction term e(x) L(x,..), 
where the extinction coe.cient e is de.ned as e(x)= o(x)+ .(x) . (2) The in-scattered radiance Li depends 
on radiance L from all directions ... over the sphere O. The in-scattered radiance is Li(x, ..)= f(x, 
...,..) L(x, ...) d.. . (3) O While the scattering coe.cient . determines how much of the incident light 
is scattered at point x, the normalized phase function f determines how much of the scattered light at 
x is scattered in direction ... The product .(x) f(x,...,..) expresses the fraction of the di.erential 
irradiance from di­rection ... to point x that is scattered (as radiance) in di­rection ... The normalized 
phase function integrates to 1 over the sphere. If the phase function is constant (1/4t), we call the 
scattering isotropic or di.use; if not, we call the scattering anisotropic or directional. Inserting 
equations (2) and (3) into equation (1) we get the integro-di.erential equation .L(x,..) = o(x) Le(x,..).x 
 + .(x)f(x,...,..) L(x,...) d.. O - e(x) L(x,..) . Integrating both sides of the equation along a straight 
path from x0 to x (in direction ..) gives the following integral equation [32]: x . ... L(x,..)=T(x 
,x) o(x ) Le(x ,..) dx x0 x . ....... +T(x ,x) .(x )f(x ,..,..) L(x ,..) d.dx x0 O + T(x0,x) L(x0,..) 
, (4) where T(x . ,x) is the transmittance along the line segment from x . to x, x -e(O) dO x. T(x . 
,x)= e, and similar for T(x0,x). Equation (4) can be simpli.ed if the medium is homo­geneous or the scattering 
is isotropic, but not in the general case which we consider here. 3 The photon map method for surfaces 
The photon map method was originally developed for global illumination simulation in scenes without participating 
me­dia [13]. It is a two-pass method where the .rst pass is the construction of two view-independent 
photon maps and the second pass is optimized rendering using these photon maps. The photon maps are generated 
by emitting photons from the light sources and tracing these through the scene using photon tracing. 
Every time a photon hits a nonspecular surface it is stored in the photon map(s). The result is a large 
number of photons distributed on the surfaces within the scene. The .rst photon map is a high quality 
caustics photon map which consists of all photons that have been traced from the light source through 
a number of specular re.ections or transmissions before intersecting a di.use surface. This path canbe 
expressedas LS+D using the notation introduced in [12]. The second photon map is a global photon map 
which is less accurate than the caustics photon map. It contains all photons representing indirect illumination 
on a nonspecular surface, L(S|D)+D. A key aspect of the method is the use of a balanced kd­tree [2, 15] 
to e.ciently and compactly handle these photons. This structure makes it possible to represent each photon 
using only 20 bytes [13]. The rendering pass is a distribution ray tracer optimized in several ways using 
the two photon maps. The caustics photon map is used to render caustics directly. This is a signi.cant 
optimization since caustics are nearly impossible to compute using ray tracing from the eye [37]. The 
global photon map is used to limit the number of re.ections traced by the distribution ray tracer and 
to sample indirect illumi­nation more e.ciently. The photon map can be used to estimate radiance at any 
given surface position x using the information about the .ux /.p carried by each photon p in direction 
.. . p .By locating the n photons with the shortest distance to x it is possible to estimate the photon 
density around x. The estimate of the re.ected radiance from a surface is then computed as [13] n . Lr(x,..) 
 fr(x,.. . p ,..) /.p(x,..p) . tr2 , (5) The illumination in participating media is mostly soft due to 
the continuous scattering taking place everywhere in the medium. This makes a possible extension of the 
Metropolis algorithm less attractive for this application. 4 Extending the photon map to partici­pating 
media Simulating light transport in participating media requires extension of several aspects of the 
photon map method. Pho­tons can be scattered and absorbed by the media. To e.­ciently render the medium, 
it is necessary to store informa­tion about these scattering events. For this purpose several strategies 
have been considered. 4.1 Algorithmic considerations One might consider discretizing the media into a 
.nite set of volume elements for which the volume radiance is computed. This strategy would, however, 
result in unnecessary restric­ tions and aliasing problems. Another alternative would be to store photons 
on the surfaces of the media. In this way, the surface radiance estimate in equation (5) could be used 
to extract information about the radiance leaving or entering a medium. The drawback of this approach 
is that it would fail to e.ciently capture illumination details such as volume p=1 where fr is the bidirectional 
re.ectance distribution function and r is the distance to the nth nearest photon. This ap­proach can 
be seen as expanding a sphere centered at x until it contains n photons. Then r is the radius of the 
sphere and the denominator in the formula is the projected area of this sphere as illustrated in .gure 
1(a). The photon map method has the full .exibility of bidi­rectional Monte Carlo path tracing [18, 34]: 
all possible light paths can be simulated, including caustics. Tessellat­ing the geometry is not necessary, 
and the aliasing problems in methods using texture maps [1, 12] or simple geometric primitives [36] to 
represent the illumination are signi.cantly reduced. Furthermore, since the photon map radiance es­timate 
is inherently a low-pass .lter, the high-frequency, grainy noise present in pure Monte Carlo methods 
disap­pears. At the same time, the Monte Carlo method s ability to compute precise illumination details 
is maintained since the photon density is high where the illumination is intense. The photon map radiance 
estimate is valid as long as the sur­face is locally .at, and the estimate converges to the correct value 
as more photons are used. The Metropolis light transport algorithm [35], a recent im­provement over bidirectional 
path tracing, focuses its com­putations on the light paths that contribute most to the rendered image. 
This gives improved performance for scenes containing for example underwater caustics seen through a 
wavy water surface. The photon map method also handles such scenes e.ciently: in the .rst pass, photons 
are refracted by the water surface, hit the di.use pool bottom, and are stored in the caustics photon 
map; in the second pass, rays from the eye are refracted to the appropriate position on the pool bottom, 
and photon map lookups return the intensities of the caustics. For soft indirect illumination, the Metropo­lis 
algorithm tends to use the same amount of computation time as brute force bidirectional Monte Carlo path 
tracing. caustics inside the medium. Also, it would not work in situ­ations where the viewpoint is within 
the medium. A better strategy is to store the photons explicitly in the volume. Since the photon map 
is based on a three-dimensional data structure, this can be done without changing the underlying algorithms. 
Storing the photons explicitly in the volume has several advantages: the photons can be concentrated 
where necessary to represent intense illumination, the media do not have to be discretized (it is possible 
to directly render im­plicitly de.ned volumes), and anisotropic scattering can be handled by storing 
the incoming direction of each photon. The photons in volumes and on surfaces must be separated when 
the photon map is queried for information about the incoming .ux. This is necessary because the relationship 
between the density of the photons and the illumination is di.erent on surfaces and in volumes (as described 
in sec­tion 4.3). The separation of the two types of photons could be done by tagging the volume photons 
and storing them in the global photon map. This would, however, result in more computationally expensive 
lookups due to the larger size of the kd-tree and the need to separate the photons. Instead we introduce 
a separate volume photon map for the photons that are scattered in participating media. The vol­ume photon 
map is used to compute the illumination inside a participating medium, and the global photon map is used 
as before to compute the illumination on surfaces. The direct illumination of participating media is 
easy to compute using traditional ray tracing techniques. Therefore we only use the photon map to represent 
indirect illumina­tion. That is, we only store photons that have been re.ected or transmitted by surfaces 
before interacting with the me­dia, and photons that have been scattered at least once in the media. 
This is a tradeo. between memory and speed since the photon map is perfectly capable of computing the 
direct illumination at the expense of using more photons to obtain the desired accuracy. 4.2 Photon 
tracing In the .rst pass, the photon maps are built using photon tracing. A photon traced within a participating 
medium can either pass una.ected through the medium, or it can interact with it (be scattered or absorbed). 
If the photon in­teracts with the medium, and does not come directly from a light source, it is stored 
in the photon map. The cumulative probability density function, F(x), expressing the probabil­ity of 
a photon interacting with a participating medium at position x is x - e(O) dO F(x)=1 -T(xs,x)=1 -e xs 
, where xs is the point at which the photon enters the medium. The transmittance T(xs,x) is computed 
using ray marching. If a photon interacts with the medium, Russian roulette decides whether the photon 
is scattered or absorbed. The probability of a photon being scattered is given by the scat­tering albedo 
.(x)/e(x). The new direction of a scattered photon is chosen using importance sampling based on the phase 
function at x. 4.3 Estimating radiance The photons stored in a volume photon map can be used to compute 
an estimate of the in-scattered radiance Li(x,..). This illumination is determined by the photons closest 
to x, as is the case with the technique used for surfaces. However, we cannot directly use the radiance 
estimate for surfaces since it is based on the de.nition of radiance for surfaces. Instead, we can utilize 
the relationship between scattered .ux . and radiance L in a participating medium [32], d2.(x,..) L(x,..)= 
. .(x)d. dV By combining this relation with equation (3) we get Li(x,..)= f(x,...,..) L(x,...) d.. O 
d2.(x,...) = f(x,...,..) d.. O .(x) d.. dV 1 .d2.(x,...) = f(x,..,..) .(x) O dV . . .. 1 n/.p(x,.p) f(x,..p,..) 
4 , (6) .(x) 3 tr3 p=1  where . is the in-scattered .ux and dV is the di.erential volume containing 
the photons. This di.erential volume is approximated by 43 tr3, corresponding to the smallest sphere 
containing the n nearest photons (this is e.ectively a nth nearest neighbor density estimate). Notice 
the relationship between this formula and formula (5) for estimating radiance on surfaces. The main di.erence 
is that the density on a surface is computed using the projected area, whereas the density in a medium 
is computed using the full volume as shown in .gure 1. Using formula (6) we can compute a radiance estimate 
at any given point inside a participating medium. Since we know the incoming direction .p of each photon, 
we can .. handle anisotropic phase functions as well as isotropic phase functions.  L (a) (b) Figure 
1: Radiance estimate for (a) surfaces and (b) vol­umes. As already mentioned, we split the computation 
of in­scattered radiance into two parts: direct Li,d and indi­rect Li,i. The direct part Li,d is computed 
by sampling of the light sources using ray marching. The indirect part Li,i is estimated using the volume 
radiance estimate from equa­tion (6), . . .. 1 n/.p,i(x,.p) Li,i(x,..) f(x,..p,..) 4 , .(x) 3 tr3 p=1 
 where /.p,i is the .ux carried by the photons that corre­spond to the indirect illumination. Since we 
store and use all photons that interact with a medium (even photons that are subsequently absorbed), 
Li,i has to be multiplied by the scattering albedo .(x)/e(x). This leads to the following formula for 
computing the in­scattered radiance: .(x) Li(x,..)= Li,d(x,..)+ Li,i(x,..) . e(x) 4.4 Rendering In the 
second pass, the image is rendered using the photon maps. We render the surfaces in the scene using the 
same approach as described in [13]. To incorporate participating media into this method we need to consider 
those rays that pass through a medium. The radiance of a ray traversing a participating medium is computed 
with an adaptive ray marching algorithm [9] that iteratively computes radiance at points along the ray. 
In each step, the radiance from the previous point is atten­uated and the contribution from emission 
and in-scattering within the step is added, corresponding to equation (4). The emitted and in-scattered 
radiance is approximated as being constant within each step. With this approximation, the ra­diance at 
points xk along a ray in direction .. is computed iteratively as L(xk,..)= o(xk) Le(xk,..)/xk + .(xk) 
Li(xk,..)/xk + e -e(xk)6xk L(xk-1,..) ,  where /xk = |xk-xk-1|is the step size and x0 is the nearest 
intersection point of the ray with a surface (or the back side of the volume). The step size is recursively 
halved if the currently com­puted radiance di.ers too much from the radiance in the previous point. This 
adaptation to illumination makes the ray marcher capable of rendering media e.ciently while still capturing 
small illumination details. The size of each step is also varied using jittering to eliminate the aliasing 
problems that can occur with a .xed step size.  5 Results We have implemented the presented method 
as a part of mental ray, a commercial rendering program that supports parallel ray tracing and has a 
.exible shader interface [6]. We use a parallelized photon map algorithm as described in [14], where 
one photon map is shared between all proces­sors. The implementation supports nonhomogeneous media and 
anisotropic scattering. We use Schlick s two-lobed phase function [4] to approximate Mie scattering [23]. 
Other phase functions such as the Rayleigh or Henyey-Greenstein phase functions [10] could be used as 
well. We have not imple­mented emitting media, but this could easily be added. The images in .gures 2 
5 have been rendered on an HP S-class computer with sixteen 180 MHz PA-8000 processors, while the images 
in .gures 6 8 have been rendered on an SGI Origin 2000 computer with sixteen 195 MHz MIPS R10000 processors. 
Each image is 1024 pixels wide and rendered usingupto16samples per pixel. Figure 2 shows a volume caustic 
created as light is fo­cused by a glass sphere in an isotropic (di.use) homogeneous medium. The light 
is emitted from a point light source (a big bright halo around the light source is visible in the upper 
left corner of the image). The volume photon map contains 100,000 photons, and 500 photons were used 
in the radiance estimate. Building the volume photon map took 5 seconds, and rendering the image took 
41 seconds. Figure 2: A volume caustic. Figure 3 illustrates the e.ect of multiple scattering in a nonhomogeneous, 
anisotropic medium. The cloud is modeled using an implicitly de.ned function consisting of 10 blobs combined 
with turbulent noise [8]. The anisotropic scattering is modeled using an approximation of hazy Mie scattering. 
In .gure 3(a) only single scattering has been simulated (no photons), whereas .gure 3(b) demonstrates 
multiple scattering (using just 10,000 photons in the photon map and 60 photons in the radiance estimate). 
Despite the low number of photons, the cloud in 3(b) is brighter and looks more realistic. Figure 3(a) 
took 61 seconds to render. The photon map for image 3(b) was generated in 8 seconds and the image was 
rendered in 92 seconds. Our next test case, a variation of the Cornell box , is (a) (b)  Figure 3: 
The cloud is an anisotropic, nonhomogeneous par­ticipating medium: (a) direct illumination (single scatter­ing), 
(b) global illumination (multiple scattering). shown in .gure 4. The box has di.use re.ection on the 
walls, .oor, and ceiling. It contains a glass sphere, a mirror sphere, and a participating medium. To 
make it easier to see the participating medium, the box has no front or back wall. The geometry is represented 
using approximately 25,000 tri­angles, and the scene is illuminated by a square-shaped area light source. 
Figure 4(a) shows direct illumination on surfaces and in the volume (an isotropic, homogeneous medium). 
It took 1 minute 49 seconds to render this image. Because of the area light source, the shadow of the 
sphere is sharp close to the sphere and gets blurry further away. Figure 4(b) has full global illumination 
on the surfaces and in the isotropic, homogeneous participating medium. Notice that the caustic created 
by the glass sphere becomes visible in the fog, that the halo around the light source gets slightly bigger 
because of multiple volume scattering, and that the fog gets red and blue tints near the colored walls 
of the box. The volume caustic under the glass sphere is wider than in .gure 2 since the light source 
is an area light source. The image was computed using a total of 200,000 photons in the photon maps, 
out of which 65,000 were in the volume photon map. The radiance estimate in the volume used 100 photons. 
It took 4 seconds to generate the photon maps, and 3 min­utes 32 seconds to render the image. In .gure 
4(c) the medium has anisotropic scattering. The scattering is mainly forward; it is modeled using Schlick 
s scattering model with a single lobe of eccentricity k =0.8. As a result of the forward scattering, 
the halo around the light source changes shape. Also, the volume caustic gets dimmer since most of the 
light in it gets scattered in a di­rection perpendicular to the eye. Photon map building took 4 seconds 
and rendering took 4 minutes 3 seconds.  (a) (b) Figure 4: Cornell boxes : (a) direct illumination; 
isotropic, homogeneous medium. (b) global illumination; isotropic, homoge­neous medium. (c) global illumination; 
anisotropic, homogeneous medium. (d) global illumination; isotropic, nonhomogeneous medium. In .gure 
4(d) the participating medium is nonhomo­geneous. A 3D turbulence function [28] was used to model the 
spatial variation of scattering and extinction coe.cients. Generating the photon maps with 200,000 photons 
took 6 seconds, and rendering took 7 minutes 54 seconds. The ra­diance estimate in the volume used 50 
photons. The higher rendering time is caused by the evaluation of the turbulence function and the .ne 
illumination details which must be re­solved by the ray marcher. Figure 5 shows scattering of sunlight 
shining through a stained glass window into a dusty room. Some of the photons that pass through the stained 
glass window are scattered by the dust in the air (using Schlick s approximation of murky Mie scattering). 
This is particularly visible near the cognac glass on the table top. An important optimization in this 
scene is that the bright beam of light shining through the window is rendered as direct illumination. 
This is a rea­sonable approximation considering the fact that the window glass is thin. The optimization 
allows the scattering e.ects in the dusty air to be represented using just 80,000 photons. The illumination 
of the surfaces in the room is represented using 220,000 photons. The walls, the .oor and the ceil­ing 
are all displacement mapped and the geometry is ap­proximated using 2.3 million triangles. The photon 
maps were generated in 27 seconds and the image was rendered in 5 minutes 27 seconds.  Figure 6: Caustics 
in a swimming pool seen through a wavy water surface. Figure 5: Dusty room illuminated by sunlight through 
a stained glass window. Figure 6 illustrates a swimming pool with caustics cre­ated as light is focused 
by a wavy water surface. In this scene, the photon maps are used only to simulate the caus­tics (including 
volume caustics) resulting from the water in the swimming pool. This is achieved by ignoring all pho­tons 
which have not intersected the water surface upon the interaction with a di.use surface. The water surface 
is dis­placement mapped and the scene is represented using a to­tal of 2.0 million triangles. Since the 
water is clear and its extinction coe.cient is low, few photons interact with the water before hitting 
the pool bottom. Therefore only 25,000 photons are stored in the volume photon map. The caustics photon 
map representing the caustics on the pool bottom and sides contains 475,000 photons. The caustics in 
the pool are crisp even through the number of photons in the caustics photon map is less than the number 
of pixels in the image. This is possible since the photons inherently are concentrated where light is 
focused and intense. The photon maps for this scene were generated in 19 seconds and ren­dering with 
a lens simulation to achieve depth-of-.eld took 1 minute 56 seconds. Figure 7 shows a simulation of surface 
and volume caus­tics in an an underwater scene. The scene is modeled using 1.5 million triangles these 
triangles are used primarily for the plants, the pu.er.sh and the displacement mapped water and sand 
surfaces. The scene also contains reaction­di.usion textures on the humphead wrasse and glossy re.ec­tion 
on the lowrider crab. The water is modeled as a par­ticipating medium to capture the e.ect of silt particles 
and plankton, and has highly directional forward scattering. To simulate the caustics and the beams of 
light in the water, we used 3.0 million photons of which 2.0 million were used in the volume photon map. 
In order to get su.ciently many pho­tons in the volume photon map, we arti.cially increased the probability 
of a photon interacting with the medium (similar to the forced interaction technique used in [26]). This 
tech­nique can be used to increase the density of photons in the parts of the scene where they contribute 
most to the overall quality of the illumination representation. The photon maps were generated in 4 minutes 
20 seconds and the image was rendered in 24 minutes. The two images in .gure 8 illustrate another simulation 
of volume caustics in an underwater environment. The two images are from an animation sequence demonstrating 
gen­tly moving volume caustics. The volume caustics are seen as bright beams of light formed as sunlight 
is focused due to refraction by the water surface. This particular view of the sun is often seen in pictures 
of underwater environments;  Figure 7: Underwater scene with volume caustics. striking examples can 
be found in [7]. We used 3.0 million photons in the volume photon map for both pictures and a few thousand 
photons in the caustic photon map to repre­sent the caustics on the .sh. For the image in .gure 8(a), 
the photon maps were constructed in 28 seconds and the rendering time was 31 minutes. The image in .gure 
8(b) re­quired 37 seconds for construction of the photon maps and the rendering time was 32 minutes. 
The longer rendering time for these images are caused by the fact that the ray marcher must perform a 
very dense sampling of the medium to adequately capture the crisp beams of light. The underwater scene 
in .gure 7 was the most memory consuming scene that we rendered (mainly due to the geom­etry and textures). 
For the simulation on sixteen processors it reached a maximum resident memory size of 660 MB. By contrast 
the cloud used 7 MB on one processor of which 4 MB was used for the frame-bu.er. We have tested the method 
presented here with both Monte Carlo and strictly deterministic quasi-Monte Carlo [24] techniques. In 
our experience, quasi-Monte Carlo sampling outperforms classical Monte Carlo sampling by dis­tributing 
the photons more evenly. This leads to an im­proved photon map radiance estimate with less noise. We 
have measured the speedups of the parallel implemen­tation, and for our scenes we achieved speedups between 
14.1 and 15.2 on the HP, and speedups between 12.0 and 14.2 on the SGI.  6 Discussion and future work 
As our results indicate, the extension of the photon map method to scenes with participating media works 
very well. The storing of photons in volumes provides a .exible frame­work and enables this method to 
simulate complicated me­dia. Our test scenes demonstrate that, in some cases, even a low number of photons 
in the volume photon map can give very good results. If the lighting situation is more complex, a higher 
number of photons is required. For the underwa­ter scenes, which contain multiple volume caustics and 
have large extents, we had to use up to 3.0 million photons in the volume photon map in order to adequately 
capture the volume caustics. Fortunately, the use of a balanced kd-tree provides an e.cient and compact 
way of handling such a large number of photons. The radiance estimate for participating media works sur­prisingly 
well even with a low number of photons. We com­pute multiple estimates along each ray traced through 
a medium, which gives good results using only 50 100 pho­tons per estimate. We found that the combination 
of ray traced direct illumination and photon map based indirect illumination gives good results. We avoid 
the costly use of a distribution ray tracer (ie., a .nal gathering step) to compute the directly visible 
in-scattered radiance in the participating media. One potential problem with this approach is that the 
radiance estimate can be incorrect close to the boundary of  (a) (b) Figure 8: Underwater sunbeams. 
a volume if there are too few photons in the photon map. In this situation, the sphere containing the 
nearest volume photons extends outside of the medium. The volume is still computed as the full sphere 
and the estimated density of the photons will therefore be too low. Similarly, false photons might be 
included in the estimate. Most of these problems can be avoided by looking at the incoming direction 
of the photons, and in our test scenes we have not experienced any visible artifacts due to this. The 
memory usage of the photon map method is very favorable in complex scenes. We were able to render high­quality 
solutions of complex scenes using less than one pho­ton per triangle. The photon map method does not 
require any additional links between geometry as in for example hi­erarchical radiosity [11]. It does 
not use any more memory than what is required for just the photons. This decoupling from the geometry 
is the primary reason why it can render very complex scenes. It also makes it possible to handle scene 
geometry represented using for example instancing or implicitly de.ned functions. Although the scenes 
in the result section have only one light source, it would not be a problem to simulate scenes with many 
light sources. Each light source then contributes relatively less to the illumination in the scene, and 
therefore less photons should be emitted from each light source. Even though our results are very encouraging, 
we believe there is room for further improvements. In particular one might consider the use of visual 
importance to guide the pho­tons towards the parts of the scene which contribute most to the image. This 
could be used to reduce the number of photons required in for example the underwater scene. One way of 
implementing this could be to use an impor­tance map (along the lines of the photon map) which stores 
importance-carrying particles emitted from the camera. Vi­sual importance might also help address the 
di.cult problem of automatically determining the number of photons needed in the photon maps to obtain 
an image of a given quality. Another interesting possibility would be to implement emitting media such 
as .ames. For complex emitting me­dia the sampling of the direct illumination might be too costly. In 
this case, one could consider storing the photons corresponding to the direct illumination from the media. 
 7 Conclusion We have extended the photon map method to simulate global illumination in scenes with 
participating media by introducing a volume photon map in which photons repre­senting indirect illumination 
are stored. Furthermore, we have derived a new technique for estimating radiance based on photons stored 
in volumes. Since the photon map is de­tached from the geometry, the method is capable of simu­lating 
global illumination in complex scenes. For the same reason, the method can use implicit volumetric representa­tions 
directly. Even though the method is simple, it provides a general framework for simulating light transport 
in various types of participating media. It can e.ciently handle both homogeneous and nonhomogeneous 
media with isotropic or anisotropic scattering. The method can simulate global illu­mination e.ects such 
as volume caustics, multiple scattering, and color bleeding between surfaces and volumes. The results 
demonstrate that the method can handle com­plex illumination and geometry, has a low level of noise, 
and is applicable to animations. While simulation of global il­lumination in scenes with participating 
media is still com­putationally expensive, it is now within reach of high-end animation production. 
 Acknowledgements Many thanks to Charlotte Manning for modeling the scenes in .gures 5 8 and for use 
of the lowrider crab. Ste.en Volz animated the .sh in .gure 8. Modeling and animation was done using 
Softimage|3D. Hoang-My Christensen and Rolf Herken proofread several drafts of this paper. Eric Lafor­tune, 
Alexander Keller, and the SIGGRAPH reviewers pro­vided helpful comments. Finally, we would like to thank 
our colleagues at mental images for providing a creative environ­ment and for discussions about this 
paper. The research and development described herein is funded in part by the Euro­pean Commission in 
ESPRIT project 22765 (DESIRE II). References [1] James R. Arvo. Backward ray tracing. ACM SIGGRAPH 86 
Course Notes Developments in Ray Tracing, 12, 1986. [2] Jon L. Bentley. Multidimensional binary search 
trees used for associative searching. Communications of the ACM, 18(9):509 517, 1975. [3] N. Bhate and 
A. Tokuta. Photorealistic volume rendering of media with directional scattering. Proceedings of the 3rd 
Euro­graphics Workshop on Rendering, pages 227 245, 1992. [4] Philippe Blasi, Bertrand Le Sa¨ec, and 
Christophe Schlick. A ren­dering algorithm for discrete volume density objects. Computer Graphics Forum 
(Proceedings of Eurographics 93), 12(3):201 210, 1993. [5] James F. Blinn. Light re.ection functions 
for simulation of clouds and dusty surfaces. Proceedings of ACM SIGGRAPH 82, pages 21 29, 1982. [6] Per 
H. Christensen. Global illumination for professional 3D animation, visualization, and special e.ects. 
Rendering Tech­niques 97 (Proceedings of the 8th Eurographics Workshop on Rendering), pages 321 326, 
1997. [7] David Doubilet. Light in the Sea. National Geographic, 1989. [8] David S. Ebert. Volumetric 
modeling with implicit functions (A cloud is born). Visual Proceedings of ACM SIGGRAPH 97, page 147, 
1997. Technical Sketch. [9] David S. Ebert, F. Kenton Musgrave, Darwyn Peachey, Ken Per­lin, and Steven 
Worley. Texturing and Modeling: A Procedural Approach. AP Professional, 1994. [10] Andrew S. Glassner. 
Principles of Digital Image Synthesis. Morgan Kaufmann, San Francisco, CA, 1995. [11] Pat Hanrahan, David 
Salzman, and Larry Aupperle. A rapid hierarchical radiosity algorithm. Proceedings of ACM SIG-GRAPH 91, 
pages 197 206, 1991. [12] Paul Heckbert. Adaptive radiosity textures for bidirectional ray tracing. Proceedings 
of ACM SIGGRAPH 90, pages 145 154, 1990. [13] Henrik Wann Jensen. Global illumination using photon maps. 
Rendering Techniques 96 (Proceedings of the 7th Eurographics Workshop on Rendering), pages 21 30, 1996. 
[14] Henrik Wann Jensen. The Photon Map in Global Illumination. PhD thesis, Technical University of Denmark, 
Lyngby, Denmark, 1996. [15] Henrik Wann Jensen. Rendering caustics on non-Lambertian surfaces. Proceedings 
of Graphics Interface 96, pages 116 121, 1996. [16] James T. Kajiya and Brian P. von Herzen. Ray tracing 
volume densities. Proceedings of ACM SIGGRAPH 84, pages 165 174, 1984. [17] R. Victor Klassen. Modeling 
the e.ect of the atmosphere on light. ACM Transactions on Graphics, 6(3):215 237, 1987. [18] Eric P. 
Lafortune and Yves D. Willems. Bi-directional path trac­ing. Proceedings of Compugraphics 93, pages 145 
153, 1993. [19] Eric P. Lafortune and Yves D. Willems. Rendering participating media with bidirectional 
path tracing. Rendering Techniques 96 (Proceedings of the 7th Eurographics Workshop on Rendering), pages 
92 101, 1996. [20] Eric Langu´enou, Kadi Bouatouch, and Michelle Chelle. Global il­lumination in presence 
of participating media with general prop­erties. Proceedings of the 5th Eurographics Workshop on Ren­dering, 
pages 69 85, 1994. [21] Nelson L. Max. Light di.usion through clouds and haze. Com­puter Vision, Graphics, 
and Image Processing, 33(3):280 292, March 1986. [22] Nelson L. Max. E.cient light propagation for multiple 
aniso­tropic volume scattering. Proceedings of the 5th Eurographics Workshop on Rendering, pages 87 104, 
1994. [23] Gustav Mie. Beitr¨age zur optik tr¨uber medien, speziell kol­loidaler metall¨osungen. Annalen 
der Physik, 25(3):377 445, 1908. [24] Harald Niederreiter. Random Number Generation and Quasi-Monte Carlo 
Methods,volume 63 of Regional Conference Se­ries in Applied Mathematics. Society for Industrial and Applied 
Mathematics (SIAM), Philadelphia, Pennsylvania, 1992. [25] Tomoyuki Nishita, Yoshinori Dobashi, and Eihachiro 
Nakamae. Display of clouds taking into account multiple anisotropic scat­tering and sky light. Proceedings 
of ACM SIGGRAPH 96, pages 379 386, 1996. [26] S. N. Pattanaik and S. P. Mudur. Computation of global 
illu­mination in a participating medium by Monte Carlo simulation. Journal on Visualization and Computer 
Animation, 4(3):133 152, 1993. [27] Frederic P´erez, Xavier Pueyo, and Fran¸cois X. Sillion. Global illumination 
techniques for the simulation of participating media. Rendering Techniques 97 (Proceedings of the 8th 
Eurographics Workshop on Rendering), pages 309 320, 1997. [28] Ken Perlin. An image synthesizer. Proceedings 
of ACM SIG-GRAPH 85, pages 287 296, 1985. [29] Holly E. Rushmeier. Realistic Image Synthesis for Scenes 
with Radiatively Participating Media. PhD thesis, Cornell Univer­sity, Ithaca, New York, 1988. [30] Holly 
E. Rushmeier. Rendering participating media: Problems and solutions from application areas. Proceedings 
of the 5th Eurographics Workshop on Rendering, pages 35 56, 1994. [31] Holly E. Rushmeier and Kenneth 
E. Torrance. The zonal method for calculating light intensities in the presence of a participating medium. 
Proceedings of ACM SIGGRAPH 87, pages 293 302, 1987. [32] Robert Siegel and John R. Howell. Thermal Radiation 
Heat Transfer, 3rd Edition. Hemisphere Publishing Corporation, New York, 1992. [33] Jos Stam. Multiple 
scattering as a di.usion process. Rendering Techniques 95 (Proceedings of the 6th Eurographics Workshop 
on Rendering), pages 41 50, 1995. [34] Eric Veach and Leonidas Guibas. Bidirectional estimators for light 
transport. Proceedings of the 5th Eurographics Workshop on Rendering, pages 147 162, 1994. [35] Eric 
Veach and Leonidas J. Guibas. Metropolis light transport. Proceedings of ACM SIGGRAPH 97, pages 65 76, 
1997. [36] Bruce Walter, Philip M. Hubbard, Peter Shirley, and Donald P. Greenberg. Global illumination 
using local linear density esti­mation. ACM Transactions on Graphics, 16(3):217 259, 1997. [37] Gregory 
J. Ward. The RADIANCE lighting simulation and ren­dering system. Proceedings of ACM SIGGRAPH 94, pages 
459 472, 1994. [38] Mark Watt. Light-water interaction using backward beam trac­ing. Proceedings of ACM 
SIGGRAPH 90, pages 377 385, 1990.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280927</article_id>
		<sort_key>321</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>31</seq_no>
		<title><![CDATA[Fast calculation of soft shadow textures using convolution]]></title>
		<page_from>321</page_from>
		<page_to>332</page_to>
		<doi_number>10.1145/280814.280927</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280927</url>
		<keywords>
			<kw><![CDATA[convolution]]></kw>
			<kw><![CDATA[error-driven illumination]]></kw>
			<kw><![CDATA[shadow map]]></kw>
			<kw><![CDATA[soft shadows]]></kw>
			<kw><![CDATA[texture]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003736.10003737</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Functional analysis->Approximation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10003636</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Approximation algorithms analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31095315</person_id>
				<author_profile_id><![CDATA[81100200748]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Cyril]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Soler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[<italic>i</italic>MAGIS&#8212;GRAVIR/IMAG, Grenoble, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P84966</person_id>
				<author_profile_id><![CDATA[81100402503]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Fran&#231;ois]]></first_name>
				<middle_name><![CDATA[X.]]></middle_name>
				<last_name><![CDATA[Sillion]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[<italic>i</italic>MAGIS&#8212;GRAVIR/IMAG, Grenoble, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>74367</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Daniel R. Baum, Holly E. Rushmeier, and James M. Winget. Improving Radiosity Solutions Through the Use of Analytically Determined Form-Factors. Computer Graphics, 23(3):325-334, July 1989. Proceedings SIGGRAPH '89.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lynne Shapiro Brotman and Norman I. Badler. Generating soft shadows with a depth buffer algorithm. IEEE Computer Graphics and Applications, 4(10):5-24, Oct. 1984.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74343</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Norman Chin and Steven Feiner. Near real-time shadow generation using bsp trees. Computer Graphics, 23(3):99-106, July 1989. Proceedings SIG- GRAPH '89.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325171</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Michael F. Cohen and Donald P. Greenberg. The hemi-cube : A radiosity solution for complex environments. Computer Graphics, 19(3):31-40, July 1985. Proceedings SIGGRAPH '85.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808590</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Robert L. Cook, Thomas Porter, and Loren Carpenter. Distributed Ray Tracing. Computer Graphics, 18(3):137-145, July 1984. Proceedings SIGGRAPH '84.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808600</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Franklin C. Crow. Summed-area tables for texture mapping. Computer Graphics, 18:207-212, July 1984. Proceedings SIGGRAPH '84.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>253308</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Paul J. Diefenbach and Norman I. Badler. Multi-pass pipeline rendering: Realism for dynamic environments. In 1997 Symposium on Interactive 3D Graphics. ACM SIGGRAPH, 1997.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192207</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[George Drettakis and Eugene Fiume. A fast shadow algorithm for area light sources using backprojection. In Proceedings SIGGRAPH '94, pages 223-230, 1994.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97913</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Paul Haeberli and Kurt Akeley. The accumulation buffer: Hardware support for high-quality rendering. Computer Graphics, 24(4):309-318, August 1990. Proceedings SIGGRAPH '90.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122740</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Pat Hanrahan, David Saltzman, and Larry Aupperle. A rapid hierarchical radiosity algorithm. Computer Graphics, 25(4):197-206, August 1991. Proceedings SIGGRAPH '91.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Paul Heckbert. Discontinuity meshing for radiosity. Third Eurographics Workshop on Rendering, pages 203-226, May 1992.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97895</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Paul S. Heckbert. Adaptive radiosity textures for bidirectional ray tracing. Computer Graphics, 24(4):145-154, August 1990. Proceedings SIGGRAPH '90.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Paul S. Heckbert and Michael Herf. Simulating soft shadows with graphics hardware. Technical Report TR CMU-CS-97-104, Carnegie Mellon University, January 1997. See also the technical sketch ~Fast Soft Shadows~ at SIGGRAPH'96.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258769</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Alexander Keller. Instant radiosity. In Proceedings SIGGRAPH '97, pages 49- 56, 1997.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>142453</ref_obj_id>
				<ref_obj_pid>142443</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Daniel Lischinski, Filippo Tampieri, and Donald P. Greenberg. Discontinuity Meshing for Accurate Radiosity. IEEE Computer Graphics and Applications, 12(6):25-39, November 1992.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Nelson Max. Sun and shade, 1988. SIGGRAPH Video Review, ACM. Issue 36, Segment 8. Available from First Priority, P.O. Box 576, Itasca, IL 60143.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>108695</ref_obj_id>
				<ref_obj_pid>108693</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Nelson Max. Unified sun and sky illumination for shadows under trees. CVGIP: Graphical Models and Image Processing, 53(3):223-230, May 1991.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Karol Myszkowski and Tosiyasu L. Kunii. Texture mapping as an alternative for meshing during walkthrough animation. In Photorealistic rendering techniques (Proceedings of the Fifth Eurographics Workshop on Rendering), pages 375- 388. Springer-Verlag, June 1994.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Jackie Neider, Tom Davis, and Mason Woo. OpenGL Programming Guide. Addison-Wesley, Reading MA, 1993.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Pierre Poulin and John Amanatides. Shading and shadowing with linear light sources. In Eurographics '90, pages 377-386. North-Holland, September 1990.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37435</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[William T. Reeves, David H. Salesin, and Robert L. Cook. Rendering antialiased shadows with depth maps. Computer Graphics, 21(4):283-291, July 1987. Proceedings SIGGRAPH '87.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166138</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Peter Schroder&lt;error&#62;~&lt;error&#62; and Pat Hanrahan. On the Form Factor Between Two Polygons. In Proceedings SIGGRAPH '93, pages 163-164, 1993.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134071</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Mark Segal, Carl Korobkin, Rolf van Widenfelt, Jim Foran, and Paul Haeberli. Fast shadows and lighting effects using texture mapping. Computer Graphics, 18(2):249-252, July 1992.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218434</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Fran~ois Sillion and George Drettakis. Feature-Based Control of Visibility Error: A Multiresolution Clustering Algorithm for Global Illumination. In Proceedings SIGGRAPH '95, pages 145-152, 1995.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>561383</ref_obj_id>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Fran~ois Sillion and Claude Puech. Radiosity and Global Illumination. Morgan Kaufmann publishers, San Francisco, 1994.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>275472</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Cyril Soler and Fran~ois Sillion. Accurate Error Bounds for Multi-Resolution Visibility. In Rendering Techniques '96 (Proceedings of the Seventh Eurographics Workshop on Rendering), pages 133-142. Springer-Verlag/Wien, 1996.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74366</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[John R. Wallace, Kells A. Elmquist, and Eric A. Haines. A ray tracing algorithm for progressive radiosity. Computer Graphics, 23(3):315-324, July 1989. Proceedings SIGGRAPH '89 in Boston.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617750</ref_obj_id>
				<ref_obj_pid>616023</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Leonard R. Wanger, James A. Ferwerda, and Donald P. Greenberg. Perceiving spatial relationships in computer-generated images. IEEE Computer Graphics and Applications, 12(3):44-58, May 1992.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358882</ref_obj_id>
				<ref_obj_pid>358876</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Turner Whitted. An Improved Illumination Model for Shaded Display. Communications of the ACM, 23:343-349, 1980.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807402</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Lance Williams. Casting curved shadows on curved surfaces. Computer Graphics, 12(3):270-274, August 1978. Proceedings SIGGRAPH '78.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617600</ref_obj_id>
				<ref_obj_pid>616014</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Andrew Woo, Pierre Poulin, and Alain Fournier. A survey of shadow algorithms. IEEE Computer Graphics and Applications, 10(6):13-32, Nov. 1990.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166145</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Harold R. Zatz. Galerkin radiosity: A higher-order solution method for global illumination. In Proceedings SIGGRAPH '93, pages 213-220, August 1993.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280929</article_id>
		<sort_key>333</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>32</seq_no>
		<title><![CDATA[Interactive reflections on curved objects]]></title>
		<page_from>333</page_from>
		<page_to>342</page_to>
		<doi_number>10.1145/280814.280929</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280929</url>
		<keywords>
			<kw><![CDATA[explosion map]]></kw>
			<kw><![CDATA[interactive reflections]]></kw>
			<kw><![CDATA[ray tracing]]></kw>
			<kw><![CDATA[reflection subdivision]]></kw>
			<kw><![CDATA[virtual objects method]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Computations on discrete structures</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP48022914</person_id>
				<author_profile_id><![CDATA[81100064088]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eyal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ofek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hebrew Univ., Jerusalem, Israel]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P21341</person_id>
				<author_profile_id><![CDATA[81100134359]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ari]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rappoport]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hebrew Univ., Jerusalem, Israel]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>360353</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Blinn, J., Newell, M., Texture and reflection in computer generated images. Comm. ACM, 19:542-546, 1976.]]></ref_text>
				<ref_id>Blinn76</ref_id>
			</ref>
			<ref>
				<ref_obj_id>253308</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Diefenbach, EJ., Badler, N.I., Multi-pass pipeline rendering: realism for dynamic environments. Proceedings, 1997 Symposium on Interactive 3D Graphics, ACM Press, 1997.]]></ref_text>
				<ref_id>Diefenbach97</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83821</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Foley, J.D., Van Dam A., Feiner, S.K., Hughes, J.F., Computer Graphics: Principles and Practice, 2nd ed., Addison-Wesley, 1990.]]></ref_text>
				<ref_id>Foley90</ref_id>
			</ref>
			<ref>
				<ref_obj_id>94788</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Glassner, A. (ed), An Introduction to Ray Tracing. Academic Press, 1989.]]></ref_text>
				<ref_id>Glassner89</ref_id>
			</ref>
			<ref>
				<ref_obj_id>13023</ref_obj_id>
				<ref_obj_pid>13021</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Greene, N., Environment mapping and other applications of world projections. IEEE CG&amp;A, 6(11), Nov. 1986.]]></ref_text>
				<ref_id>Greene86</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Haeberli, E, Segal, M., Texture mapping as a fundamental drawing primitive. Proceedings, Fourth Eurographics Workshop on Rendering, Cohen, Puech, Sillion (eds), 1993, pp. 259-266.]]></ref_text>
				<ref_id>Haeberli93</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Hall, T., Tutorial on planar mirrors in OpenGL, posted to comp.graphics.api.opengl, Aug. 1996.]]></ref_text>
				<ref_id>Hall96</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134082</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Hanrahan, E, Mitchell, D., Illumination from curved reflectors. Proceedings, Siggraph '92, ACM Press, pp. 283-291.]]></ref_text>
				<ref_id>Hanrahan92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808588</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Heckbert, ES., Hanrahan, E, Beam tracing polygonal objects. Computer Graphics, 18:119-127, 1984 (Siggraph '84).]]></ref_text>
				<ref_id>Heckbert84</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Jansen, F.W., Realism in real-time? Proceedings, Fourth Eurographics Workshop on Rendering, Cohen, Puech, Sillion (eds), 1993.]]></ref_text>
				<ref_id>Jansen93</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[McReynolds, T., Blythe, D., Programming with OpenGL: Advanced Rendering, course #23, Siggraph '96.]]></ref_text>
				<ref_id>McReynolds96</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Ofek, E., Modeling and Rendering 3-D Objects. Ph.D. thesis, Institute of Computer Science, The Hebrew University, 1998.]]></ref_text>
				<ref_id>Ofek98</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Rushmeier, H.E., Extending the radiosity method to transmitting and specularly reflecting surfaces. Masters's thesis, Cornell University, 1986.]]></ref_text>
				<ref_id>Rushmeier86</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134071</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Segal, M., Korobkin, C., van Widenfelt, R., Foran, J., Haeberli, E, Fast shadows and lighting effects using texture mapping. Computer Graphics, 26:249-252, 1992 (Siggraph '92).]]></ref_text>
				<ref_id>Segal92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74368</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Sillion, F., Puech, C., A general two-pass method integrating specular and diffuse reflection. Computer Graphics, 23(3):335-344 (Siggraph '89).]]></ref_text>
				<ref_id>Sillion89</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Spanguolo, M., Polyhedral surface decomposition based on curvature analysis. In: Modern Geometric Computing for Visualization, T.L. Kunii and Y. Shinagawa (Eds.), Springer-Verlag, 1992.]]></ref_text>
				<ref_id>Spanguolo92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192193</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Voorhies, D., Foran, J., Reflection vector shading hardware. Proceedings, Siggraph '94, ACM Press, pp. 163-166.]]></ref_text>
				<ref_id>Voorhies94</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37438</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Wallace, J.R., Cohen, M.F., Greenberg, D.P, A two-pass solution to the rendering equation: a synthesis of ray tracing and radiosity methods. Computer Graphics, 21:311-320, 1987 (Siggraph '87).]]></ref_text>
				<ref_id>Wallace87</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358882</ref_obj_id>
				<ref_obj_pid>358876</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Whitted, T., An improved illumination model for shaded display. Comm. of the ACM, 23(6):343-349, 1980.]]></ref_text>
				<ref_id>Whitted80</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Interactive Re.ections on Curved Objects Eyal Ofek Ari Rappoport Institute of Computer Science, The 
Hebrew University Copyright &#38;#169;1998 by the Association for Computing Machinery, Inc. Permission 
to make digital or hard copies of part or all of this work for personal or classroom use is granted without 
fee provided that copies are not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. Copyrights for components of this work owned 
by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, 
to post on servers or to redistribute to lists, requires specific permission and/or a fee. Abstract Global 
view-dependent illumination phenomena, in particular re­.ections, greatly enhance the realism of computer-generated 
im­agery. Current interactive rendering methods do not provide satis­factory support for re.ections on 
curved objects. In this paper we present a novel method for interactive computation of re.ections on 
curved objects. We transform potentially re.ected scene objects according to re.ectors, to generate virtual 
objects. These are rendered by the graphics system as ordinary objects, cre­ating a re.ection image that 
is blended with the primary image. Vir­tual objects are created by tessellating scene objects and computing 
a virtual vertex for each resulting scene vertex. Virtual vertices are computed using a novel space subdivision, 
the re.ection subdivi­sion. For general polygonal mesh re.ectors, we present an associ­ated approximate 
acceleration scheme, the explosion map.For spe­ci.c types of objects (e.g., linear extrusions of planar 
curves) the re.ection subdivision can be reduced to a 2-D one that is utilized more accurately and ef.ciently. 
CR Categories: I.3.3 [Computer Graphics]: Picture/Image Gener­ation; I.3.7 [Computer Graphics]: Three-Dimensional 
Graphics and Realism. Keywords: ray tracing, interactive re.ections, virtual objects method, re.ection 
subdivision, explosion map. Introduction Interactive photo-realistic rendering is a major goal of computer 
graphics. Global view-dependent illumination phenomena greatly enhance image quality. An extremely important 
type of view­dependent phenomenon is re.ection. Re.ections on curved object are not supported well by 
current interactive rendering techniques. In this paper we address the problem of interactive rendering 
of re.ections on curved objects. Background. Current interactive graphics systems utilize hardware acceleration 
that directly supports hidden surfaces removal, sim­ple local shading models and texture mapping. While 
the polygon throughput of these systems is impressive, the range of shading ef­fects they provide hasn 
t changed much since their introduction. In particular, they lack support for global illumination phenomena 
in Institute of Computer Science, The Hebrew University, Jerusalem 91904, Israel. http://www.cs.huji.ac.il/.arir,.eyalp 
arir,eyalp@cs.huji.ac.il dynamic scenes. Global illumination phenomena greatly enhance the quality of 
syn­thetic imagery. They can be coarsely classi.ed to view-independent and view-dependent phenomena. 
Among the former, diffuse illumi­nation in static scenes [Sillion89] and shadows [Segal92] can be in­teractively 
rendered using current hardware. However, global view­dependent phenomena are crucial for providing life-like 
realism. When only view-independent effects are provided, the visual na­ture of the result can be dull 
and lifeless, even when the scene is dynamic. An extremely important view-dependent illumination phenomenon 
is re.ection. The dominant method for generating re.ections is ray tracing [Whitted80, Glassner89]. In 
spite of extensive work on ray tracing acceleration schemes, [Jansen93] states that the only hope for 
interactive ray tracing lies in massively parallel computers, and even then satisfactory performance 
is not guaranteed. Environment mapping [Blinn76, Greene86, Haeberli93, Voorhies94] generates at interactive 
rates re.ections that are approximately correct when the re.ected objects are relatively far from the 
re.ector. However, when this condition is violated the results are of very poor accuracy. It is well-known 
that re.ections on planar surfaces can be generated by (1) mirroring the viewer along the re.ecting plane, 
(2) creating a re.ection image by rendering the scene from the new point of view, and (3) merging the 
main image with the visible portion of the re­.ector in the re.ection image. Surprisingly, although this 
method can signi.cantly accelerate ray tracing, it has been accurately doc­umented only recently. The 
descriptions in [Foley90] (in which the method is called re.ection mapping ) and [McReynolds96] are correct 
only when the original viewer and all objects lie on the same side of the re.ecting plane. A correct 
description is given in [Hall96]. [Diefenbach97] shows how to use variants of this method for interactive 
simulation of various general re.ectance functions of planar objects. The concept of a re.ected virtual 
world was also used in [Rushmeier86, Wallace87, Sillion89] for supporting specu­lar re.ections from planar 
objects in a radiosity context. Contribution. In this paper we present a method for interactive ren­dering 
of re.ections on curved objects, based on merging a primary image and a re.ection image. The re.ection 
image is generated by creating and rendering virtual objects corresponding to re.ections of scene objects. 
Virtual objects are rendered like ordinary poly­gons, thus taking advantage of the features supported 
by the graph­ics system. They are created using a structure called the re.ection subdivision and an associated 
approximate acceleration scheme, the explosion map. The method presents a novel approach to the computation 
of re­.ections in computer graphics, and is unique in providing approx­imate re.ections on curved objects 
at interactive rates. Moreover, the rendered scenes can be completely dynamic; no pre-processing is necessary. 
The method provides higher quality than environment mapping, because it allows re.ected objects to be 
nearby the re.ec­tor and it supports equally well re.ectors having a large curvature. For scenes in which 
re.ected images of objects occupy more than a few pixels and in which the depth complexity of the re.ection 
im­age is not large, the method is much more ef.cient than ray tracing, because it ef.ciently exploits 
the spatial coherency of the re.ec­tion image. The price paid for the advantages of the method is that 
its performance is less ef.cient than that of environment mapping and the generated images are only polygonal 
approximations (as in most interactive systems). In addition, its accuracy depends upon the geometric 
nature of the re.ector. The paper is structured as follows. Section 2 gives an overview of the method. 
Sections 3, 4 and 5 deal with convex re.ectors, dis­cussing respectively the re.ection subdivision, the 
explosion map, and special re.ectors. Section 6 deals with non-convex re.ectors. Results and an in-depth 
discussion are given in Sections 7 and 8. 2 Method Overview In this section we give an overview of the 
virtual objects method. We present the general idea (2.1), image merging alternatives (2.2), a brief 
discussion on planar re.ectors (2.3), and a high-level outline on non-planar re.ectors (2.4). 2.1 General 
Idea The virtual objects method is inspired by the following observation. Consider an image containing 
re.ections. Two kinds of entities are visible: re.ecting objects, or re.ectors,and re.ected images of 
re­.ected 3-D objects. When the re.ector is a perfect planar one, the geometry of the re.ected images 
is identical to images of the re­.ected objects from some other viewpoint. In fact, we cannot dis­tinguish 
between real objects and re.ected images of objects. In­terior designers utilize this phenomenon when 
covering walls with mirrors in order to make rooms seem larger. For non-planar re.ec­tors, the appearance 
of re.ected objects is a deformed version of their ordinary appearance. In general, there is no viewpoint 
from which they appear identical to their re.ected images. The nature of the deformation depends upon 
the geometry of the re.ector. Con­vex re.ectors deform re.ected objects to seem smaller, and concave 
re.ectors produce re.ected images that may seem larger than the re.ected object or degenerate into strange 
chaotic images. This observation inspires the following algorithm for generation of re.ections (Figure 
1): for every re.ector and every object poten­tially re.ected in it, compute a 3-D virtual object, that, 
when ren­dered using ordinary 3-D rendering methods, will produce an image having a visual appearance 
similar to the object s re.ected image. If depth relationships between the virtual objects are still 
correct, the rendered images of the virtual objects can be merged together us­ing some hidden surfaces 
removal algorithm. The result can now be alpha blended with a re.ector image containing view-independent 
lighting to produce the .nal image. The alpha blending coef.cients are determined by the relative re.ectivity 
of the re.ector. SceneRender (Scene S,View E): (1) Render S without re.ections into primary image I. 
 (2) For every visible re.ector R 2S (3) For every potentially re.ected object O 2S (4) O0 .VirtualObject 
(R, O, E). (5) Render O0 into a re.ection image I0 . (6) If multiple levels of re.ections are desiredCall 
the algorithm recursively. (7) Alpha blend I0 and I, according to the re.ectivity of R.  Figure 1 The 
virtual objects method. When virtual objects can be computed ef.ciently, the resulting method is very 
attractive, since re.ected images are generated at the object, rather than the pixel, level. Most of 
this paper deals with step 4, ef.cient generation of virtual objects. Naturally, only visible re.ectors 
are considered, and the scene can be stored in a data struc­ture that supports culling of scene objects 
that cannot be re.ected. A comment about shading: for planar re.ectors we can re.ect the light sources 
as well as the scene objects and simply use the re­.ected ones. For non-planar re.ectors, it is more 
accurate to com­pute shading values for vertices at the world coordinate system, and then use these values 
for the virtual vertices. On current architec­tures, this shading is most ef.ciently computed in software, 
and the hardware is used for rasterization and texturing. 2.2 Image Merging The primary and re.ection 
images can be merged in two ways. First, the re.ection image can be used as a texture when render­ing 
a re.ector. Alternatively, the re.ection image can be directly rendered on the screen (using a stencil 
bit-plane de.ning the screen image of the re.ector.) The view-independent component of the re­.ector 
is now rendered, alpha blending it with the re.ection image. Texture mapping and stencil-guided image 
merging are standard features in interactive graphics systems, even current low-end ones. The choice 
of method depends on the actual graphics architecture available, especially on its memory organization. 
For more details, see [Ofek98, McReynolds96, Hall96]. 2.3 Planar Re.ectors The method of [McReynolds96, 
Hall96, Diefenbach97] is a special case of the virtual objects method, when the re.ectors are planar 
and when we consider the objects, rather than the viewpoint, as being mirrored. Note that in this case 
the method is essentially an image-space version of beam tracing [Heckbert84]. An attractive property 
of planar re.ectors is that the location of a virtual point is a simple af.ne transformation, mirroring, 
of the real point. Moreover, this transformation does not depend on the viewer location, only on that 
of the re.ector. In Figure 2(a), the location of the virtual image Q 0 of a scene point Q remains constant 
for two viewpoints E1 and E2. Hence, the same simple af.ne transformation can be used for all re.ected 
polygons. Full details on how to generate the mirroring transformation are given in the above references. 
 E1Q E1Q E2 E2 NN N1 N2 Q (a) (b) Figure 2 For planar re.ectors, the virtual location of a point does 
not depend upon the viewpoint (a). This does not hold for curved re.ectors (b). Note that as presented 
so far, the method produces correct results only when the viewpoint and the re.ected polygon are on the 
same side of the re.ector. Consider a polygon lying on the other side of the re.ector. After the mirroring 
transformation, it can erroneously obscure the re.ector from the viewer, because they lie on the same 
side of it. This problem can be overcome by not mirroring a poly­gon if all of its vertices lie behind 
the re.ector. The test is done by plugging the vertex coordinates into the re.ector plane equation and 
testing the sign of the result. However, this method does not solve the case when the polygon lies only 
partially behind the re.ector. In many cases such polygons do not cause incorrect results because the 
virtual front part falls outside of the re.ector stencil anyway. For planar re.ectors, the problem can 
be solved very ef.ciently by de.ning the re.ector plane as a front clipping plane. 2.4 Non-Planar Re.ectors 
Generation of virtual objects for non-planar re.ectors is more dif­.cult than for planar re.ectors, because 
the main property of the planar case does not hold: the location of a virtual point is not a simple af.ne 
transformation independent of the viewer position (Figure 2(b)). In general, every re.ected point is 
transformed dif­ferently. Our approach is outlined in Figure 3. The re.ected object is tes­sellated into 
polygons (step 1). The .neness of the tessellation de­pends upon the desired accuracy of the resulting 
re.ection image. Tessellations are further discussed in Sections 7 and 8. In steps 2 5, virtual polygons 
are generated by computing virtual vertices for the tessellation vertices. The collection of all virtual 
polygons forms the desired virtual object rendered in step 5 of Figure 1. The main step is 4, computing 
a single virtual vertex; its description occupies much of the rest of the paper. VirtualObject (Re.ector 
R, Object O,View E): (1) Tessellate O into polygons. (2) For each polygon P (3) For each vertex Q of 
P (4) Q0 .VirtualVertex (R, Q, E). (5) Connect the Q0s to form a virtual polygon P0 . (6) Connect 
the P0s to form the virtual object O0 .  Figure 3 Computing a virtual object O0 for a potentially re.ected 
object O on a non-planar re.ector R. Rendered polygons are consistent and possess no holes, because virtual 
objects are formed by connecting virtual vertices. Visibility relationships between virtual objects are 
preserved due to the usage of a hidden surfaces removal mechanism (in practice, a z-buffer) for them. 
 The Re.ection Subdivision In this section we start detailing our approach towards computing virtual 
vertices for curved re.ectors. We assume here that the re­.ector is convex. Concave and other non-convex 
re.ectors are dis­cussed in Section 6. Our approach is based on approximating the re.ector by a polygonal 
mesh. In many cases this is the format in which objects are given anyway; when they are given in a higher­level 
representation (e.g., a NURBS surface) they are tessellated. For simplicity, we assume that mesh polygons 
are triangles, but this is not necessary. Intuition. Given a re.ector R and an arbitrary scene point 
Q,we want to generate the corresponding virtual point Q0 (consult Fig­ure 2(b)). If we knew the point 
of re.ection J and normal N on the boundary surface of R, we could easily compute Q0 by mirroring Q along 
the tangent plane to R at J. In some cases, when we know the geometric nature of the re.ector (e.g., 
a sphere), J can be computed by a direct formula. However, for a general convex polygonal mesh there 
is no direct formula. We use an approximation. Every re.ector triangle de.nes two space cells: a re.ected 
cell and a hidden cell. Suppose that we can .nd the cell C, de.ned by triangle T, in which the scene 
point Q lies. A naive method would mirror Q across the plane containing T.How­ever, this would clearly 
show the linear approximation of the re.ec­tor (imagine a re.ecting sharply cut diamond!). Instead, we 
use the relative location of Q inside C to de.ne a triplet of barycentric coef­.cients. These coef.cients 
are used to interpolate the three tangent planes at the vertices of T, yielding a new tangent plane that 
is now used for mirroring Q. In this section we study the space subdivision de.ned by the re­.ector and 
also explain why we need to compute virtual points for points that are not re.ected. The full details 
of the computation are given in Sections 4 and 5. The subdivision. Each vertex Vi of the tessellated 
re.ector pos­sesses a normal Ni. Re.ector vertices are either front-facing or back-facing, according 
to whether their normals point towards or away from the viewer (a normal orthogonal to the line of sight 
is considered front-facing). Due to the convexity of the re.ector, ev­ery front-facing vertex is visible 
by the viewer (when there are no other obscuring objects). Note that back-facing vertices might still 
be visible (this is a tessellation artifact). When all vertices of a mesh triangle are front-facing (back-facing), 
we refer to the triangle as being front-facing (back-facing). Otherwise we say that the triangle is a 
pro.le triangle. For each front-facing vertex Vi we de.ne two rays: (1) a re.ection ray Ri, mirroring 
the ray from Vi to the viewer across the normal Ni, and (2) a hidden ray Hi, originating at Vi and extending 
to in.nity in the opposite direction to that of the viewer. Figure 4 shows a 2-D version of the situation. 
In (a), re.ection rays are shown in red and hidden rays in blue.  Ri+1 Q1 Vk Q2 Ri (b)(a) Figure 4 (a) 
The re.ection subdivision in 2-D. Ci and Di are the re.ected and hidden cells de.ned by re.ector vertices 
Vi, Vi+1.The ray Z bisects the unre.ected region on the right into two parts Z1, Z2. (b) Computation 
of virtual vertices: the point Q1 in the re.ected cell Ci is transformed to Q0 1 inside the hidden cell 
Di; the point Q2 in the hidden cell Dk is transformed to Q0 outside the re.ector in the 2 re.ected cell 
Ck . Two re.ection rays Ri, Rj corresponding to adjacent front-facing mesh vertices Vi, Vj de.ne a ruled 
bi-linear parametric surface s(Vi + tRi)+ (1 ,s)(Vj + tRj). Note that in general this surface is not 
planar, because the two rays are usually not co-planar. The two hidden rays Hi, Hj span an in.nite truncated 
triangle containing the edge Vi, Vj. Now consider the three vertices Vi, Vj, Vk of a front-facing mesh 
triangle Vijk. The triangle induces two space regions: (1) A re.ected cell Cijk bounded by the three 
ruled surfaces corresponding to the triangle edges and by Vijk itself (.gure 10). (2) A hidden cell Dijk, 
which is the in.nite part of the truncated pyramid bounded by Vijk and the triangles spanned by the hidden 
rays. We refer to the union of the re.ected (hidden) cells as the re.ected (hidden) region. An important 
property of the re.ected and hidden cells is that they do not intersect each other, since the re.ector 
is convex. Therefore, we can de.ne the re.ection subdivision as the subdivision of space induced by these 
cells. Note, however, that these cells do not cover space; we call the part of space not covered by re.ection 
or hidden cells the unre.ected region. In Figure 4(a), the part of the unre­.ected region lying on the 
right side of the re.ector is the union of Z1, Z2 (the reason for subdividing this region and the meaning 
of the ray Z are explained below). Points in the unre.ected region can (in principle) be seen by the 
viewer, but cannot be re.ected by the re.ector. A point is potentially re.ected by the re.ector if and 
only if it lies in the re.ected region. We say potentially because its re.ection may be obscured by the 
re.ection of another point. The unre.ected and hidden regions. We compute virtual images for vertices 
of potentially re.ected scene polygons (Figure 3, step 4). These virtual vertices are connected in order 
to generate virtual polygons, which are then rendered to create the re.ection image (Figure 1, step 5). 
Scene polygons that lie completely in the hidden or unre.ected regions can be discarded. However, mixed 
polygons, lying partially in these regions and partially in the re.ected region, pose a problem. For 
such polygons, we would like to render the re­.ection of the part that lies in the re.ected region. However, 
if we compute only one or two virtual vertices, we would not be able to connect these in order to generate 
virtual polygons. In some sense, the vertices lying in hidden or unre.ected regions are representa­tives 
of a polygon area that we want to see re.ected. A naive way to deal with mixed polygons is to intersect 
them (ex­actly or approximately) with the region boundaries, thus forcing them to have a uniform classi.cation. 
However, this is inef.cient because the regions depend on the viewpoint. Another way is to subdivide 
them into smaller polygons, effectively doing an adap­tive tessellation of scene objects. Subdivision 
is stopped when the lost areas are deemed to be small enough. A more ef.cient and elegant method is to 
de.ne a virtual vertex for every polygon vertex, even for hidden and unre.ected ones (e.g., vertex Q2 
in Figure 4(b)). These doubly virtual vertices are not real re.ections; their sole purpose is to close 
virtual polygons so that the graphics system could render them. In general, they lie outside the image 
of the re.ector. Note that this actually is the approach taken in the planar re.ector case. Hidden cells 
are easy to take care of, because there is a one-to-one correspondence between hidden and re.ected cells. 
Moreover, it is possible to de.ne a transforma­tion that maps a re.ected cell to exactly cover the corresponding 
hidden cell, and maps a hidden cell to exactly cover its correspond­ing re.ected cell (see Section 4.2). 
The unre.ected region is more problematic. We would like to de.ne a transformation for this region such 
that (1) the part of the region adjacent to a re.ected cell will be transformed to be adjacent to its 
corresponding hidden cell (and vice versa), and (2) there is some continuity of the transformation between 
the unre.ected and the re.ected regions. To achieve such a transformation, we de.ne for every contour 
edge of the re.ector an auxiliary bisecting surface Z, which extends the edge into the unre.ected region. 
Figure 4(a) shows a 2-D example. In 2-D we have a contour vertex and not a contour edge (it is simply 
the extreme vertex Vi+1), and the bisecting surface is simply a ray Z. Z is orthogonal to the normal 
Ni+1 at Vi+1 and extends Vi+1 into the unre.ected region, thus bisecting the region into two parts Z1, 
Z2. The desired transformation is simply a linear mirroring transformation that mirrors Z1 into Z2 and 
vice versa. In 3-D, the bisecting surface is non-linear, and we do not de.ne it explicitly; it is de.ned 
implicitly by the transformation we use for computing virtual vertices (Section 4.2). As in the planar 
case, doubly virtual vertices might cause their vir­tual polygon to obscure the re.ector. The solution 
in the planar case, a front clipping plane, can be generalized to non-planar re.ectors by utilizing a 
second z-buffer containing the re.ector s geometry. Every pixel generated during rendering of the virtual 
polygons will be tested twice: once against the ordinary z-buffer, in order to pro­duce correct depth 
relationships between all virtual polygons, and once against the re.ector z-buffer, to ensure that pixels 
in front of the re.ectors are discarded. A second z-buffer is not easy to de.ne ef.ciently on today s 
graph­ics architectures. Alternatives that are currently more practical are: (1) do not do anything, 
anticipating that the obscuring pixels will fall outside the screen mask of the re.ector, (2) approximate 
the re­.ector using six clipping planes, an option available on standard ar­chitectures, and (3) tessellate 
the scene so that mixed polygons are very small. Surprisingly, the .rst approach works well in the vast 
majority of cases, due to the way objects are usually positioned rel­ative to each other and the way 
they are viewed. The second option reduces the problem but does not guarantee the resulting quality. 
The third option also reduces the problem, but requires more com­putations since there are more virtual 
vertices to compute. Tessella­tions are discussed in Sections 7 and 8.  4 The Explosion Map Acceleration 
Method In some cases it is very ef.cient to compute the re.ection subdivi­sion and search it to .nd the 
cell in which a point lies (Section 5). In the general 3-D case, a faster indexing scheme is preferable. 
In this section we describe an approximation method, the explosion map, which is a data structure for 
accelerating the computation of virtual vertices. It is prepared for each re.ector separately, and re­computed 
whenever the viewpoint or the re.ector are moved. The map is an image whose pixel values hold IDs of 
re.ector triangles, and which represents a spherical 2-D cross section of the subdi­vision. To compute 
a virtual image of a scene point, we compute explosion map coordinates for it, thus yielding the ID of 
a speci.c triangle. The virtual image is computed using that triangle. The explosion map is somewhat 
similar to a circular environment map [Haeberli93] in that it is an image in which a circle corresponds 
to the re.ection directions (Figure 5(b)). However, it is unlike an en­vironment map in that the latter 
contains renderings of other scene objects, while the explosion map contains only the re.ector (Fig­ure 
5(a)). We next detail the computation (4.1) and utilization (4.2) of the map. 4.1 Computing an Explosion 
Map An explosion map is a function of the tessellated re.ector, the view­point, a 3-D sphere, and a desired 
resolution. The sphere should be centered at a point that is an intuitive center of the re.ector (as 
in environment mapping), and its radius should be large enough so that it bounds the re.ector (actually, 
a sphere is not essential; we need any convex geometric object that approximates the re.ector s shape). 
The map resolution should be large enough so that there are substantially more map pixels than re.ector 
triangles. In practice, a resolution of 2002 is suf.cient when the re.ector has been tes­sellated into 
several hundred triangles. The depth resolution of the map should have enough bits to hold unique IDs 
for all re.ector vertices plus one more bit (needed to distinguish between ordinary triangles and extension 
polygons, de.ned below). The basic operation in computing the map, MapCoords,involves ................. 
................. ................. ................. ................. ................. F .... AO ................. 
................. ................. ................. C .... .... .... BF ................. ................. 
................. ................. ................. ................. D E (a) (b) Figure 5 Explosion 
map: (a) re.ection rays and intersection points on a bounding sphere; (b) the resulting map. C and D 
are extension vertices of A and B. deriving the map coordinates T =(tx, ty) corresponding to a nor­malized 
direction vector N =(x, y, z) going from the center of the sphere to an arbitrary direction. If the resolution 
of the map is r2, sx sy N is mapped to T =( + s.2, + s.2), where s is a (2(z+1))1.2 (2(z+1))1.2 number 
a little smaller than r. This mapping is similar to that used for generating a circular environment map 
from a map rendered on the faces of a box [Haeberli93]. The pixels to which directions are mapped all 
fall inside a circle of radius s.2. The circle represents all possible re.ection directions. The map 
itself is computed as follows (Figure 6). For every front­facing re.ector vertex, we compute map coordinates 
by intersecting its re.ection ray with the sphere and calling MapCoords with the direction from the sphere 
s center to the intersection point (step 3). Back-facing vertices are denoted as such (step 4) to facilitate 
fast identi.cation of pro.le triangles in step 6. For each front-facing re.ector triangle (recall that 
a triangle is called front-facing if all its vertices are front-facing, and is called pro.le if only 
some of its vertices are front-facing), the corresponding triangle de.ned by the map coordinates is .lled 
on the map, using its unique ID as color (step 5). Polygon .ll can be done by the graphics hardware. 
For pro.le triangles, the normals of their back-facing vertices are projected in the direction of the 
viewer such that they are orthogonal to the line from the viewer through the vertex (step 7). The triangles 
thus become front-facing, and are now .lled on the map as done for triangles that were front-facing originally 
(step 8). So far, the interior of a map circle of radius s.2 has been partially .lled, but not completely. 
This is due to the existence of the unre­.ected region and the fact that the .lled map triangles are 
linear. As we explained in Section 3, we want the directions into the un­re.ected region to be .lled 
on the map as well so that we could use it to compute doubly virtual vertices. To ensure that all direc­tions 
are .lled on the map, in ExtendMap the map is extended to cover the circle as follows. For each pro.le 
triangle Vijk having two back-facing vertices (say Vi, Vj), we de.ne an extension polygon Eij in map 
coordinates and .ll it with the ID of Vijk. The vertices of Eij are Ti, Tj, and extensions of each of 
these vertices in the direc­tion away from the circle s center (in Figure 5(b), the extensions of vertices 
A, B are C, D). The extensions should be long enough so that the circle is completely covered. In practice, 
it is enough that the length of the segment from the center to each extended vertex is 0.6s. Extension 
polygons effectively comprise an implicit repre­sentation of the bisecting surfaces Z explained in Section 
3. Other methods for representing the unre.ected region on the map are dis­cussed in [Ofek98]. ExplosionMap 
(Re.ector R,View E,Center C, Distance d, Resolution r): (1) Let S be a sphere centered at C having radius 
d. (2) Let M be an image of size r .r. (3) For each re.ector vertex Vi  If Vi is front-facing Let 
Ri be the re.ection ray of Vi. Let Ii be the intersection of Ri with S. Let Ji be the normalized direction 
from C to Ii.  Ti .MapCoords (Ji, r). Else (4) Denote Vi as back-facing. (5) For each re.ector triangle 
Vijk If Vijk is front-facing Fill the triangle Ti, Tj, Tk on M, using the ID of Vijk as the color. 
(6) Else if Vijk is a pro.le triangle (7) Fix its back-facing normals. (8) Compute and .ll Ti, Tj, 
Tk as before. (9) ExtendMap (R, M).  Figure 6 Computing an explosion map. 4.2 Computing Virtual Vertices 
The explosion map circle represents a mapping of all possible re­.ection directions. We use it to directly 
generate the .nal virtual vertex Q0 corresponding to a potentially re.ected scene vertex Q. For each 
re.ector we compute two explosion maps: a near map and a far map. The near map is computed using a sphere 
that bounds the object but does not intersect any other object, and the far map is computed using a sphere 
that bounds all the scene. It is important to understand that although the topologies of the two maps 
are quite similar (because cells do not intersect each other), their geometries are different; re.ection 
rays, which determine the geometry of map vertices, evolve non-linearly. In addition to the explosion 
maps, we store a hidden map and an auxiliary z-buffer of the re.ector. The hidden map is simply an item 
buffer of the visible mesh triangles. In other words, it is a discrete map in which a visible mesh triangle 
is mapped to a 2-D triangle .lled by the ID of the mesh triangle. The map resolution can be smaller than 
that of the frame buffer (say, 2002). The basic operation needed is MapToVirtualVertex, whose argu­ments 
are a map M, a 3-D point Q and a corresponding map point I. Assume that the ID in M(I) is that of an 
ordinary mesh triangle V (not an extension polygon) having 2-D vertices A, B, C (these are the Ti s computed 
in step 3 of Figure 6). The output is the virtual point Q0. The operation is implemented in three steps: 
(1) compute barycentric coordinates s, t of I relative to V by solving the two lin­ear equations in two 
variables (1 ,(s + t))A + sB + tC = I; (2) use s, t as weights in a weighted average of the 3-D vertices 
and normals of V that yields a plane of re.ection U; and (3) mirror Q across U to produce Q0. Note that 
negative barycentric coordinates are per­fectly acceptable. The computation can be performed in integers 
or .oating point, to reduce aliasing artifacts resulting from the dis­crete nature of the map. Extension 
polygons are handled similarly, using four bilinear coordinates instead of three. This treatment of extension 
polygons effectively implements the non-linear mirror­ing transformation of the unre.ected region motivated 
in Section 3. Computation of virtual vertices for a scene vertex Q is shown in Figure 7. We .rst determine 
if Q is hidden (steps 1, 2), by testing it in screen coordinates against the re.ector s z-buffer. If 
it is, Q s virtual image is computed by the hidden map (step 3). Note that an obvious optimization here 
is to do this only for hidden vertices that belong to mixed polygons, since we don t need virtual images 
for polygons that are hidden completely. VirtualVertex (Re.ector R, Point Q,View E): (1) Let I be the 
screen coordinates of Q (using E). (2) If Q is hidden by a mesh triangle V (3) Return MapToVirtualVertex 
(HiddenMap, Q, I). (4) Let c be the direction from the center of R to Q. (5) T .MapCoords (c, r). 
(6) Q0 .MapToVirtualVertex (NearMap, Q, T).  n Q 0 f .MapToVirtualVertex (FarMap, Q, T). (7) Let dn, 
df be the relative distances of Q from the near and far spheres. Q0.dn+Q0 f .df Return n. 1.dn+1.df Figure 
7 Computing virtual vertices using the explosion and hidden maps. When Q is not hidden we use the explosion 
maps. The normalized direction from the center of the re.ector to Q is used to obtain map coordinates, 
in the same way used for creating the maps (steps 4, 5). Note that the map coordinates T are the same 
for both maps, but the triangle IDs found at T are different. In general, none of these triangles corresponds 
to the correct re.ection cell in which Q is lo­cated, because we approximated the correct ray of re.ection 
of Q by a ray from the center of the re.ector (when higher accuracy is desired, we can use an improved 
approximation or locally search the correct cell [Ofek98].) Each of these triangles de.nes an auxil­iary 
virtual vertex (step 6), and a weighted average of those is taken to obtain the .nal virtual vertex (step 
7). There may be other ways to choose the weights than the obvious one shown. Figure 13 shows near and 
far explosion maps, in which polygon IDs are encoded by colors for visualization purposes. 5 Improved 
Ef.ciency for Linear Extrusions For some common re.ectors, it is possible to compute virtual ver­tices 
more ef.ciently than the explosion map, by directly utilizing the re.ection subdivision to .nd the cell 
in which a scene point lies. Among these re.ector are linear extrusions of planar curves (e.g., cylinders) 
and cones. For spheres, there is an ef.cient method that does not use the re.ection subdivision at all. 
In general, if an implicit equation de.ning the re.ector is available, the re.ection point can be computed 
as in [Hanrahan92] (although this method is slow). Below we detail the case of an extruded re.ector. 
The direct computation for cones and spheres is simple and given in [Ofek98]. Consider a 2-D re.ection 
subdivision, as shown for example in Fig­ure 4. We can optimize the step of identifying the cell in which 
a point lies by organizing the re.ection cells in a hierarchy. De.ne Ci,j to be the region bounded by 
re.ection rays Ri, Rj and the line seg­ment (Vi, Vj). Note that Ci,j contains every cell Ck,r , i .k 
.r .j. Classifying a point with respect to a cell Ci,j amounts to a few line side tests, implemented 
by plugging the point into the line s equa­tion and testing the sign of the result. If we .nd that the 
point is not contained in Ci,j, we know that it is outside all contained cells Ck,r . A binary search 
can thus be performed on the hierarchy. Note that there are no actual computations involved in generating 
the hi­erarchy, since it is implicitly represented by the numbering of the re.ector vertices. A similar 
hierarchy can be de.ned for the hidden cells as well. Membership in the two (at most) unre.ected cells 
can be tested easily. Consequently, the cell in which a point is located can be found using a small number 
(O(log n)where n is the re.ector tessellation resolution) of line side tests. Suppose that the re.ector 
is a linear extrusion of a convex 2-D pla­nar curve. We can reduce the computation of a virtual vertex 
to 2-D by (1) projecting the viewer and all scene points onto the plane, (2) performing the 2-D computation, 
obtaining a line of re.ection LQ for each scene vertex, (3) extruding LQ to 3-D to form a plane of re.ection 
T, and (4) computing a .nal virtual vertex by mirror­ing the original vertex across T. The screen in 
Figure 17 is a linear extrusion of a convex planar curve. 6 Non-Convex Re.ectors Concave re.ectors. The 
computations we perform for concave re­.ectors are identical to those for convex ones, but it is interesting 
to note that concave re.ectors produce signi.cantly more complicated visual results. In Figure 8 we see 
a viewer E in front of a concave re.ector and three re.ection rays. The re.ection of an object lo­cated 
in region A (left) looks like an enlarged, deformed version of the object. The re.ection of an object 
located in region B (middle) looks like an enlarged, deformed, upside-down version of the ob­ject. The 
re.ection of objects located in region C (right) is utterly chaotic. This chaotic nature is inherent 
in the physics of re.ections and is not an artifact of computations or approximations. Q2 Figure 8 Behavior 
of re.ections on concave re.ectors. Re.ections of objects lying in regions A and B can be computed exactly 
as for convex re.ectors, because in these regions the re.ec­tion subdivision is well-de.ned (since the 
re.ection cells are dis­joint). Re.ections of objects lying in region C or intersecting that region are 
unpredictable and chaotic anyway, so almost any policy for computing virtual vertices will be satisfactory. 
In particular, we can simply use the value computed by the explosion map, thereby treating concave re.ectors 
exactly as convex ones. Figure 11 shows a concave re.ector. On the right, we see the re­.ector, the re.ection 
rays, a re.ected planar object, and the com­puted virtual object, all these from a point of view different 
from the viewer s. On the left we see the .nal image from the viewer s point of view. The two explosion 
maps and the hidden map are shown at the bottom right. Note that re.ected objects must be very close 
to this re.ector to cross from region B to regions A or C. Re.ectors of mixed convexity. Re.ectors that 
are neither con­vex nor concave should be decomposed into convex and concave parts. For many objects 
this can be done fully automatically [Span­guolo92]. Some polygonal surfaces contain saddles, resulting 
in a decomposition that is too .ne. In such cases it is advised that users decompose the object manually. 
Note that the actual requirement is not of pure convexity of concavity, but rather that the re.ec­tion 
cells would not self-intersect in areas where re.ected objects lie. Devising automatic algorithms that 
take this into consideration when decomposing the object is an interesting topic for future work. When 
manual decomposition is used, re.ectors cannot dynamically change their shape in an arbitrary way, but 
the scene can still be dy­namic. Figure 9 shows a re.ector with a convex part (red) and a concave part 
(green). Note the seamless transition of the re.ection image between the convex and concave parts. 7Results 
We have implemented our algorithms using OpenGL on SGIs run­ning Irix and on PCs running Windows 95 and 
NT. Figure 12 shows a cylinder re.ector modeled as a linear extrusion of a circle. Figure 13 demonstrates 
the effect of varying re.ector tessellation resolution. The bottom part shows the near and far explosion 
maps. We see that using 128 triangles the re.ection image already has an approximately correct geometric 
form, and that using 2048 rather than 512 triangles barely makes a difference. Figure 14 shows the effect 
of varying the tessellation of the re.ected object (using 512 re.ector triangles). A tessellation of 
7x7 is suf.cient. A lower res­olution would suf.ce for objects farther away from the re.ector, Figure 
16 shows a scene with four re.ecting spheres, a table, and a window, rendered by our method (top) and 
by Rayshade, a well­known raytracer (bottom). A checkerboard texture was used in or­der to emphasize 
the re.ections. The geometric shapes of the re­.ections in the two images are visually very similar. 
The texture in the bottom image is sharper because we use the graphics hardware for texture mapping. 
On an SGI O2, the top image required less than a second,and the bottom one required 50 seconds. For Rayshade, 
we turned off shad­ows rays, highlights, and anti-aliasing, and we used a single sam­ple per pixel and 
a manually tuned uniform grid as an acceleration scheme. Image resolution is 5122. Figure 15 shows the 
same scene, from a slightly different view­point and using real textures. The shadows are pre-computed 
tex­tures. Figure 17 shows a re.ecting TV modeled as an extrusion of a convex planar curve. Figure 18 
shows recursive re.ections on a planar mirror. Figure 19 shows a mask composed of several con­vex and 
concave pieces. Note the correct re.ections of the red and green spheres on both cheeks of the mask and 
on the nose. Fig­ure 20 shows several re.ecting polyhedra and a re.ecting sphere. All of these scenes 
(except Figure 19) are displayed in real-time on an SGI In.nite Reality. We haven t tried the scene of 
Figure 19 on such a machine; on an SGI O2, Figure 19 requires about a second with our method, and 1.5 
minutes using Rayshade. Discussion The virtual objects method is the .rst method capable of accurately 
approximating re.ections on curved objects at interactive rates. In this paper we presented the basic 
method for a single level of re.ec­tion and its implementation for general polygonal meshes and for linear 
extrusions. Clearly, the method possesses both advantages and disadvantages. We discuss these below, 
both in isolation and in the context of other methods. Quality. In general, the quality produced by the 
method is satisfac­tory, especially for interactive use. The explosion map gives good results even for 
planar or nearly planar re.ectors. Like any approx­imation method, ours might produce visible artifacts. 
The most no­ticeable ones occur when objects are not tessellated .nely enough, in which case their re.ections 
look too much like their real-world images and are not deformed according to the geometry of the re­.ector. 
In addition, re.ections might be slightly translated inaccu­rately because we do not compute the exact 
explosion map cells to which vertices are mapped. Other visible artifacts can be seen near the boundaries 
of the re.ec­tor, when the transformation used to create doubly virtual vertices is not a good approximation 
to the correct re.ection. In this case the seam between convex and concave regions might be visible. 
Even in this case, re.ections are self-consistent and do not exhibit holes. When the re.ector shape on 
the explosion map is far from con­vex, our heuristic for representing the unre.ected region (extension 
polygons) might yield visible artifacts. Obviously, doubly virtual vertices might still hide the re.ector 
when not using a second z­buffer. However, as we noted earlier, this usually does not happen because 
their screen images tend to fall outside the screen image of the re.ector. An attractive property of 
the method that has not been mentioned so far is that it supports interactive rendering of refractions, 
by using refraction rays instead of re.ection rays. There are some additional differences, detailed in 
[Ofek98]. Tessellation strategies. As shown in Section 7, some tessellation of re.ected objects is usually 
essential for providing suf.cient ac­curacy. The .ner the tessellation, the more accurate the re.ections. 
At the same time, increasing the tessellation has an adverse impact on performance. These considerations 
are identical to those em­ployed in interactive rendering of curved objects in general. There are two 
standard approaches: (1) usage of uniform tessellations, pre-computed such that quality is satisfactory, 
and (2) usage of hi­erarchical tessellations (levels of detail, etc). Both approaches can be taken in 
our case as well. When the dis­tances from a re.ector to re.ected objects and viewer remain ap­proximately 
constant, we can pre-compute a uniform tessellation. The tessellation resolution of the re.ected object 
should be chosen such that its virtual polygons cover several dozen pixels. Otherwise, hierarchical tessellations 
can be used. These can exhibit the same artifacts as when they are used for ordinary objects, e.g. discontinu­ities 
during animation. Note that the re.ector tessellation resolution can be lower than that used when rendering 
its view-independent image. Using hierarchical tessellations is a topic for future work. Performance. 
In the worst-case, all scene points can indeed be re­.ected on every convex part and every concave part 
of every re­.ector. Denote by r the number of visible re.ectors and by n(n 0) the number of vertices 
in the original (tessellated) scene. The time complexity of the method is O(r .n0), which is thus worst-case 
optimal for a given degree of tessellation. For a single re.ector, the step of computing the explosion 
and hidden maps is linear in the size of the re.ector and is roughly equivalent to rendering the re­.ector 
three times at low resolution. The step of computing a virtual vertex for a scene vertex requires a relatively 
small constant num­ber of operations. Moreover, the operations performed are highly regular, and are 
probably not too dif.cult to parallelize or imple­ment in hardware. The cost of rendering virtual polygons 
is similar to rendering the whole scene. If deforming re.ectors are desired, they should be subdivided 
into convex and concave parts on each frame, which costs time linear in their size. Naturally, as the 
depth complexity of the re.ection image increases, the time complexity of our method diverges from the 
optimal. The scenes shown in this paper run interactively (1-30 frames per second) on an SGI O2 workstation. 
This performance was achieved without any optimization; in particular, no method for culling ob­jects 
that cannot be re.ected has been used. On today s systems, without further optimizations the number of 
re.ected objects can­not be much larger than shown while still guaranteeing interactive performance. 
Comparison to other methods. We can compare our method to environment mapping or ray tracing, which are 
currently the only techniques capable of computing re.ections on curved objects. Both visual accuracy 
and ef.ciency should be considered. Environment mapping is relatively accurate only when re.ected ob­jects 
are relatively far from the re.ector and when the curvature of the re.ector is not large. When the scene 
is static, time complexity is linear in the size of the re.ector, because the map can be pre­computed. 
This is in general much faster than our method. When the scene is dynamic, the map must be recomputed 
on each frame for each re.ector. This also holds when only the viewer changes, unless the special hardware 
of [Voorhies94] is used. Complexity is r .n, which is closer to our method but still more ef.cient. To 
what degree depends on the amount of tessellation. However, environ­ment mapping simply does not provide 
realistic accuracy. Seeing re.ections of objects that are nearby as if they are very far creates an uneasy 
feeling and de.nitely cannot be quali.ed as realistic. Ray tracing obviously produces higher quality 
images than our method and supports a wider range of illumination phenomena. Re­garding ef.ciency, the 
relevant characteristics of our method are: (1) it operates at the object level rather than the pixel 
level (we have an object and we want to know where it is re.ected, rather than having the point of re.ection 
and seeking an object), (2) it transforms the problem into one that standard graphics systems can handle, 
(3) it transforms the computation into a local one involv­ing a single re.ector-re.ected pair, instead 
of the global ray trac­ing computation ( .nd the nearest object ); global visibility rela­tionships are 
automatically handled by the z-buffer, and (4) it uses both the CPU and the graphics system, dividing 
(but not necessar­ily balancing) the load between them. When these properties are signi.cant, our method 
is more ef.cient than ray tracing. Ray trac­ing can be expected to perform better when (1) re.ected objects 
do not cover many pixels, (2) there are many curved re.ectors, or (3) the depth complexity of the re.ected 
images is large. It may or may not be faster when there is no graphics hardware. Note that our method 
scales much better than ray tracing to larger image resolu­tions, while ray tracing scales better with 
scene depth complexity.  It is very dif.cult to predict the point from which ray tracing is more ef.cient. 
On the relatively simple scenes shown in this paper, the method is at least an order of magnitude more 
ef.cient than Rayshade, a well-known available ray tracer, even when it uses a manually tuned acceleration 
scheme. Future work. Both ef.ciency and quality issues should be further investigated. Ef.ciency issues 
include: acceleration using global scene organization techniques, hierarchical tessellations, possible 
hardware implementation, acceleration using time coherence, and usage of the method to accelerate other 
illumination methods. Qual­ity issues include re.ning the initial approximation given by the ex­plosion 
map, improved methods for .lling the unre.ected region on the map, using the method for rendering refractions, 
automatic decomposition of re.ectors of mixed convexity, quantifying the de­gree of error introduced 
by our approximations, and additional lev­els of recursive re.ections. Conclusion. We feel that correct 
re.ections from small objects are not very important. Such re.ections, re.ections on complex mixed convexity 
objects, and re.ections of distant objects can be convinc­ingly emulated using environment mapping. High 
quality re.ec­tions are therefore needed for relatively large objects with relatively uniform convexity 
(or concavity). A typical scene does not contain too many curved objects like these. As a result, although 
the time complexity of the method is theoretically quadratic in the number of re.ectors, in practice 
its complexity is linear in the size of the scene (it can be sub-linear if scene databases are used for 
culling objects). Applicability will increase with increases in processing power and graphics hardware. 
Even today, there are many applica­tions in which the number of objects in the scene is less important 
than the rendering quality. In these cases, our method is at least an order of magnitude faster than 
ray tracing and provides higher visual quality than environment mapping. Our experience is that interacting 
with scenes containing re.ections is immensely more enjoyable than with scenes without re.ections. Re.ections 
bring dull and lifeless scenes to life. Acknowledgements. We thank Dani Lischinski for commenting on 
a draft of this paper and for fruitful discussions. We also thank Amichai Nitsan for his involvement 
in part of the implementation. Lastly, we warmly thank Leo Krieger for his continuous support. References 
[Blinn76] Blinn, J., Newell, M., Texture and re.ection in computer gener­ated images. Comm. ACM, 19:542 
546, 1976. [Diefenbach97] Diefenbach, P.J., Badler, N.I., Multi-pass pipeline render­ing: realism for 
dynamic environments. Proceedings, 1997 Sympo­sium on Interactive 3D Graphics, ACM Press, 1997. [Foley90] 
Foley, J.D., Van Dam A., Feiner, S.K., Hughes, J.F., Computer Graphics: Principles and Practice, 2nd 
ed., Addison-Wesley, 1990. [Glassner89] Glassner, A. (ed), An Introduction to Ray Tracing. Academic Press, 
1989. [Greene86] Greene, N., Environment mapping and other applications of world projections. IEEE CG&#38;A, 
6(11), Nov. 1986. [Haeberli93] Haeberli, P., Segal, M., Texture mapping as a fundamental drawing primitive. 
Proceedings, Fourth Eurographics Workshop on Rendering, Cohen, Puech, Sillion (eds), 1993, pp. 259 266. 
[Hall96] Hall, T., Tutorial on planar mirrors in OpenGL, posted to comp.graphics.api.opengl, Aug. 1996. 
[Hanrahan92] Hanrahan, P., Mitchell, D., Illumination from curved re.ec­tors. Proceedings, Siggraph 92, 
ACM Press, pp. 283 291. [Heckbert84] Heckbert, P.S., Hanrahan, P., Beam tracing polygonal ob­jects. Computer 
Graphics, 18:119 127, 1984 (Siggraph 84). [Jansen93] Jansen, F.W., Realism in real-time? Proceedings, 
Fourth Euro­graphics Workshop on Rendering, Cohen, Puech, Sillion (eds), 1993. [McReynolds96] McReynolds, 
T., Blythe, D., Programming with OpenGL: Advanced Rendering, course #23, Siggraph 96. [Ofek98] Ofek, 
E., Modeling and Rendering 3-D Objects. Ph.D. thesis, Institute of Computer Science, The Hebrew University, 
1998. [Rushmeier86] Rushmeier, H.E., Extending the radiosity method to trans­mitting and specularly re.ecting 
surfaces. Masters s thesis, Cornell University, 1986. [Segal92] Segal, M., Korobkin, C., van Widenfelt, 
R., Foran, J., Haeberli, P., Fast shadows and lighting effects using texture mapping. Computer Graphics, 
26:249 252, 1992 (Siggraph 92). [Sillion89] Sillion, F., Puech, C., A general two-pass method integrating 
specular and diffuse re.ection. Computer Graphics, 23(3):335 344 (Siggraph 89). [Spanguolo92] Spanguolo, 
M., Polyhedral surface decomposition based on curvature analysis. In: Modern Geometric Computing for 
Visual­ization, T.L. Kunii and Y. Shinagawa (Eds.), Springer-Verlag, 1992. [Voorhies94] Voorhies, D., 
Foran, J., Re.ection vector shading hardware. Proceedings, Siggraph 94, ACM Press, pp. 163 166. [Wallace87] 
Wallace, J.R., Cohen, M.F., Greenberg, D.P, A two-pass solu­tion to the rendering equation: a synthesis 
of ray tracing and radiosity methods. Computer Graphics, 21:311 320, 1987 (Siggraph 87). [Whitted80] 
Whitted, T., An improved illumination model for shaded dis­play. Comm. of the ACM, 23(6):343 349, 1980. 
...... ......... ...... ......... ........ ...... ......... ........ ...... ......... ........ ...... 
... ......... ........ ...... Q ... ........ ...... ...... Fig. 9: Mixed convexity reflector, with ........ 
seamless reflections. E Fig. 10: A 3-D reflected cell. Fig. 11: Virtual object and reflection rays. 
Fig. 12: Linear extrusion. 32 128 512 2048 Fig. 13: Varying the tessellation resolution of the reflector. 
none 3x3 7x7 13x13 Fig. 14: Varying the tessellation resolution of the reflected object. Fig. 17: TV. 
Fig. 16: Top: our method. Bottom: Rayshade.   Fig. 18: Recursive reflections.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280930</article_id>
		<sort_key>343</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>33</seq_no>
		<title><![CDATA[Non-distorted texture mapping for sheared triangulated meshes]]></title>
		<page_from>343</page_from>
		<page_to>352</page_to>
		<doi_number>10.1145/280814.280930</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280930</url>
		<keywords>
			<kw><![CDATA[discrete smooth interpolation]]></kw>
			<kw><![CDATA[non distored texture mapping]]></kw>
			<kw><![CDATA[optimization]]></kw>
			<kw><![CDATA[parametrization]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.1</cat_node>
				<descriptor>Smoothing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.6</cat_node>
				<descriptor>Constrained optimization</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Geometric correction</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.6</cat_node>
				<descriptor>Global optimization</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003716</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Mathematical optimization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010243</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Appearance and texture representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003722</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Interpolation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10003716</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Mathematical optimization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003716</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Mathematical optimization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10003716</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Mathematical optimization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31086746</person_id>
				<author_profile_id><![CDATA[81100154829]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bruno]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[L&#233;vy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GOCAD]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P136608</person_id>
				<author_profile_id><![CDATA[81100328941]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jean-Laurent]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mallet]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GOCAD]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>325249</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[J. Bloomenthal. Modeling the Mighty Maple. In SIGGRAPH 85 Conference Proceedings, volume 19, pages 305-311. ACM, July 1985.]]></ref_text>
				<ref_id>Blo85</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[E. Bier and K. Sloan. Two-Part Texture Mapping. IEEE Computer Graphics and Applications, pages 40-53, September 1986.]]></ref_text>
				<ref_id>BS86</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122744</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[C. Bennis, J.M. V6zien, and G. Igl6sias. Piecewise Surface Flattening for Non-Distorted Texture Mapping. In SIGGRAPH 91 Conference Proceedings, volume 25, pages 237-246. ACM, July 1991.]]></ref_text>
				<ref_id>BVI91</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[M.E Do Carmo. Differential Geometry of Curves and Smfaces. Prentice Hall, Englewood Cliffs, Inc., 1976.]]></ref_text>
				<ref_id>Car76</ref_id>
			</ref>
			<ref>
				<ref_obj_id>907242</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[E. Catmull. A Subdivision Algorithm for Computer Display of Curved Surfaces. PhD thesis, Dept. of Computer Sciences, University of Utah, December 1974.]]></ref_text>
				<ref_id>Cat74</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[R. Cognot, T. AYt Ettajer and J.L. Mallet. Modeling Discontinuities on Faulted Geological Surfaces. In SEG Technical Program, pages 1711- 1718, November 1997.]]></ref_text>
				<ref_id>CEM97</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218440</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[M. Eck, T. DeRose, T. Duchamp, H. Hoppes, M. Lounsbery and W. Stuetzle. Multiresolution Analysis of Arbitrary Meshes. In SIGGRAPH 95 Conference P1vceedings, pages 173-182. ACM, August 1995.]]></ref_text>
				<ref_id>ERDH95</ref_id>
			</ref>
			<ref>
				<ref_obj_id>248308</ref_obj_id>
				<ref_obj_pid>248299</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[M.S. Floater. Parametrization and Smooth Approximation of Surface Triangulations. Computer Aided Geometric Design, 14(3):231-250, April 1997.]]></ref_text>
				<ref_id>Flo97</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237270</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[V. Krishnamurthy and M. Levoy. Fitting Smooth Surfaces to Dense Polygon Meshes. SIGGRAPH 96 Conference P1vceedings, pages 313-324. ACM, August 1996.]]></ref_text>
				<ref_id>KL96</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192187</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[R Litwinowicz and G. Miller. Efficient Techniques for Interactive Texture Placement. In SIGGRAPH 94 Conference P1vceedings, pages 119-122. ACM, July 1994.]]></ref_text>
				<ref_id>LM94</ref_id>
			</ref>
			<ref>
				<ref_obj_id>62057</ref_obj_id>
				<ref_obj_pid>62054</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[J.L. Mallet. Discrete Smooth Interpolation in Geometric Modeling. ACM-Transactions on Graphics, 8(2): 121-144, 1989.]]></ref_text>
				<ref_id>Mal89</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[J.L. Mallet. Discrete Smooth Interpolation. Computer Aided Design Journal, 24(4):263-270, 1992.]]></ref_text>
				<ref_id>Mal92</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[S.D. Ma and H. Lin. Optimal Texture Mapping. In EUROGRAPHICS'88, pages 421-428, September 1988.]]></ref_text>
				<ref_id>ML88</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166120</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[J. Maillot, H. Yahia, and A. Verroust. Interactive Texture Mapping. In SIGGRAPH 93 Conference P1vceedings, volume 27. ACM, 1993.]]></ref_text>
				<ref_id>MYV93</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325246</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Peachey, R. Darwyn. Solid Texturing of Complex Surfaces. In SIG- GRAPH 85 Conference P1vceedings, volume 19, pages 287-296. ACM, July 1985.]]></ref_text>
				<ref_id>Pea85</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218458</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[H.K. Pedersen. Decorating Implicit Surfaces. In SIGGRAPH 95 Conference P1vceedings, pages 291-300. ACM, 1995.]]></ref_text>
				<ref_id>Ped95</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Samek, Marcel, C. Slean, and H. Weghorst. Texture Mapping and Distortions in Digital Graphics. The Visual Computer, 2(5):313-320, September 1986.]]></ref_text>
				<ref_id>SMSW86</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122749</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[G. Turk. Generating Textures on Arbitrary Surfaces Using Reaction- Diffusion. In SIGGRAPH 91 Conference Proceedings, pages 289-298. ACM, 1991.]]></ref_text>
				<ref_id>Tur91</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[W.T. Tutte. Convex Representation of Graphs. In Proc. London Math. Soc., volume 10, 1960.]]></ref_text>
				<ref_id>Tut60</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Copyright &#38;#169;1998 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers or to redistribute to lists, requires specific permission and/or a fee. Non-Distorted Texture 
Mapping For Sheared Triangulated Meshes Bruno L´evy*Jean-Laurent Mallety GOCAD ENSG, rue du doyen Marcel 
Roubeault, 54500 Vandoeuvre Abstract This article introduces new techniques for non-distorted texture 
mapping on complex triangulated meshes. Texture coordinates are assigned to the vertices of the triangulation 
by using an iterative op­timization algorithm, honoring a set of constraints minimizing the distortions. 
As compared to other global optimization techniques, our method allows the user to specify the surface 
zones where dis­tortions should be minimized in order of preference. The modular approach described in 
this paper results in a highly .exible method, facilitating a customized mapping construction. For instance, 
it is easy to align the texture on the surface with a set of user de.ned isoparametric curves. Moreover, 
the mapping can be made con­tinuous through cuts, allowing to parametrize in one go complex cut surfaces. 
It is easy to specify other constraints to be honored by the so-constructed mappings, as soon as they 
can be expressed by linear (or linearizable) relations. This method has been inte­grated successfully 
within a widely used C.A.D. software dedicated to geosciences. In this context, applications of the method 
com­prise numerical computations of physical properties stored in .ne grids within texture space, unfolding 
geological layers and generat­ing grids that are suitable for .nite element analysis. The impact of the 
method could be also important for 3D paint systems. CR Categories: I.3.3 [Computer Graphics] Picture/Image 
Gen­eration ; I.3.5 [Computer Graphics]: Three-Dimenstional Graphics and Realism Color, shading, shadowing 
and texture; I.4.3 [Image processing]: Enhancement Geometric Correction, Texture Keywords: Non Distorted 
Texture Mapping, Parametrization, Discrete Smooth Interpolation, Optimization 1 INTRODUCTION Texture 
mapping is widely used to improve the visual richness of 3D surfaces in computer generated images. Each 
3D surface is put in correspondence with a planar image through a function called a mapping.Such a mapping 
assigns a pair of coordinates (u;v) referring to a pixel of the planar image to each point of a surface. 
Thus, for instance, the latitude and longitude can de.ne a trivial mapping of a sphere. This technique 
was introduced by Catmull in [Cat74], and .rst applied to bicubic patches using a recursive *levy@ensg.u-nancy.fr, 
GOCAD and INRIA Lorraine/CNRS ymallet@ensg.u-nancy.fr, director of the GOCAD consortium subdivision 
algorithm. Unfortunately, these methods often produce highly distorted textures in the resulting images. 
First attempts to minimize these distortions were made in [Pea85] and in [BS86] by separating the process 
into two steps. The texture pattern is .rst applied to a simple intermediate surface such as a box or 
a cylinder for which texture mapping is trivial. Then, this intermediate surface is projected on the 
target object. The choice of the intermediate surface, its orientation together with the projection method 
dramatically affect the results, and great deal of user interaction is therefore required. Another idea 
is to consider that assigning texture coordinates to any surface is equivalent to .attening it. Such 
a technique is described in [SMSW86]. The idea consists in unfolding a polygonal surface from a user 
selected seed. A similar idea has been developed in [BVI91]. This latter method means a parametric surface 
may be unfolded by allowing cuts to appear on the mapped texture when the discrepancy of the geodesic 
curvature goes beyond a given threshold. Minimizing the distortions induced by texture mapping can be 
also realized using optimization techniques. In the method proposed in [ML88], a mapping of any surface 
is constructed by starting from a grid of points sampled on the surface. The grid is then iteratively 
optimized by minimizing a global distortion criterion. Krishnamurthy proposes in [KL96] a similar approach 
for converting a triangulated mesh into a set of B-Spline surfaces. It is also possible to construct 
a mapping by assigning (u;v) coordinates to the vertices of the mesh. This naturally leads to the use 
of harmonic maps, as described in [ERDH95]. This method consists in minimizing a metric dispersion criterion. 
Unfortunately, this does not always preserve angles accurately. Another approach is introduced in [Flo97], 
generalizing the barycentric mapping method introduced in [Tut60]. The (u;v)texture coordinates are found 
to be the solution of a linear system, where each (u;v)point is a convex combination of its neighbors. 
Floater[Flo97] proposes a way of choosing the coef.cients of these convex combinations to mimic the chord 
length parametrization for curves. These global methods give good results for most surfaces, but suffer 
from several limitations when applied to complex surfaces. For these kinds of methods, since the criterion 
to be minimized is hardwired in the optimization algorithm, it is often dif.cult to take into account 
user de.ned information. For instance, as most surfaces are not developable, distortions will still remain, 
and the user may want to specify the distribution of these distortions. This article proposes a new global 
optimization method. As compared to other similar techniques, the method is based on a modular approach 
enabling the way the mapping is constructed to be customized. For instance, it is possible to tune the 
perpendicu­larity and homogeneous spacing of isoparametric curves all over the surface, thus specifying 
the surface zones where distortions should be minimized in order of preference. It is also possible to 
make the mapping respect a set of user speci.ed isoparametric curves. Moreover, the mapping can be made 
continuous through cuts, hence allowing the mapping of a texture on a complex cut surface in one go. 
The method can be extended easily to honor other kind of linear constraints. All these constraints allow 
the method to take into account user speci.ed information while beeing much more automatic than other 
interactive mapping methods [Ped95, LM94, MYV93], where the parametrization is partially or completely 
de.ned by the user. The .rst section of the paper summarizes the notions involved in texture mapping 
on triangulated meshes, and shows how a map­ping can be constructed using an iterative optimization algorithm. 
In Section 2, the criteria to be met in constructing a non-distorting mapping are introduced. These criteria 
are expressed as a set of linear constraints in Section 3, where the algorithm previously in­troduced 
is modi.ed in order to honor them. This results in a gen­eral algorithm that can be extended easily to 
take into account user speci.ed information, as shown in Section 4. In this latter section, we show how 
to respect user speci.ed isoparametric curves, and how to make mappings continuous through cuts. Section 
5 presents some applications and results. The paper concludes with some sug­gestions for future developments. 
 2 PARAMETRIZING A TRIANGULATION In this section, the notion of mapping function de.ned on a trian­gulated 
mesh is recalled, and a new method for constructing such mapping functions is described, based on an 
iterative optimization algorithm. How these mappings can be optimized in order to mini­mize the distortions 
is then explained in Section 3. 2.1 Mapping Function .and Discrete Mapping ' D u R3 R2 Figure 1: Mapping 
<from a surface Sof R3to DR2 . As shown in Figure 1, given an open surface Sof R3,a mapping < is a one-to-one 
transform that maps the surface Sto a subset Dof R2 . " qu(x;y;z) (x;y;z)2S!q(x;y;z) qv(x;y;z) Regarding 
a mapping, the following de.nitions can be given: Dis called the parametric (u;v)domain.  As <is, by 
de.nition, a one-to-one function, it has an inverse function x=<,1, called a parametrization of the surface: 
 " x(u;v) (u;v)2D!x(u;v)q,1(u;v)y(u;v) z(u;v)  If a surface has a parametrization xde.ned, the inverse 
<= x,1of this parametrization naturally provides a mapping function. Catmull applied this technique to 
cubic splines in [Cat74], in which a recursive subdivision scheme is described, making it possible to 
avoid inverting of the parametrization directly. Figure 2: Mapping function <interpolated over a triangle. 
 In what follows, the surface Sis provided with a triangulation G=fn;Tg,where nis the set of the vertices 
of the triangulation, and Tthe set of the triangles of G, de.ned as vertex triplets. For the sake of 
simplicity, nwill be identi.ed with the interval [1:::M]of integers, where M=jnjdenotes the number of 
vertices of the triangulation. The geometric location at a vertex a2nis denoted p(a)in what follows. 
For this kind of surface, it is natural to de.ne the value of < at the vertices nof the triangulation 
G. This information can be stored as a set of (ui;vi)values, where 1:i:M.How to choose these (ui;vi)values 
is explained in Section 4. This de.nes a discrete function ':n!such that 8ai2n;'(ai)= R2 v f'u(ai);'(ai)g=(ui;vi). 
As shown in Figure 2, a mapping function <can be then de.ned as the linear interpolation of 'over each 
triangle T=(ai;aj;ak)of T. For each point pin T, <is given by: <(p)=(1, , ):'(ai) + :'(aj) + :'(ak) 
  where: and are the local barycentric coordinates at the point pin T '( i)(ui;vi);'( j)(uj;vj);'( k)(uk;vk) 
 2.2 Discrete Smooth Interpolation Given a triangulation G=fn;Tg, we want to assign (u;v)coor­dinates 
to each vertex a2n. Floater has shown in [Flo97] that verifying the following two suf.cient conditions 
constructs a map­ping: 1. The image of the border of the surface through ' in the parametric (u;v)domain 
is a convex poly­gon. 2. Each internal node is a convex combination of its neighbours.  One must keep 
in mind that these two conditions are suf.cient and not necessary to de.ne mappings. We show in Section 
4 how the .rst one can be replaced by a less restrictive condition. More formally, the second condition 
can be written as follows (see Equation 1): X u 8k2n;v(k):'(a)=0 (1) u2N(k) where: N(k)denotes the set 
of nodes directly connected to k, includ­ing k. u the v(k)are given coef.cients such that: 8 v u(k) 
> 0 8a2N(k), fkg : v k(k) = , P a2N(k) u6 k vu(k)6= 0 8k2n (2) Once boundary nodes have been mapped 
to a convex polygon in parametric domain space, (u;v)coordinates must be assigned to the internal nodes 
of the triangulation. Instead of .nding 'by directly solving Equation 1 as done in more classical approaches, 
the method described in this article consists of minimizing a global criterion in a least square sense, 
honoring at the same time a set of linear constraints, as will be shown in the next section. The algorithm 
is based on the Discrete Smooth Interpolation (D.S.I.), that we describe in [Mal89, Mal92]. The reader 
is referred to these two articles where the notions of generalized roughness, linear con­straints, and 
the iterative D.S.I. algorithm introduced further on in this document are described in depth. The criterion 
minimized by the D.S.I. method is called the roughness R, and is de.ned in Equa­tion 3 below: 892 } XXX 
uV R(')= v(k):'(k)(3) :; k2lV2fu;vgu2N(k) The minimum of this functionnal is reached if @R(')/@'V(a)=0for 
each a2nand for each v2fu;vg, where vdenotes one of the two components of '. This yields the following 
equation: GV(uj') 'V(a)=, gV(u) where: 89 }PP GV(aj')= v u(k):v (k):'V(P) k2N(u) : 2N(k) ; 6 u VP 
u2 g(a)=fv(k)g u2N(u) (4) The following algorithm computes iteratively the assignments of (u;v)coef.cients 
minimizing the roughness given in Equation 3. We have proven in Mallet[Mal89] that it does converge to 
a unique solution, as soon as at least one node ahas its value '(a).xed, u and provided that the chosen 
v(k)coef.cients honor Equation 2. Later in this document, we show how this method can be enhanced using 
D.S.I. constraints. let Ibe the set of nodes where 'is unknown let ' [0]be a given initial approximated 
solution while (more iterations are needed) f for all( a2I) f for all( v2fu;vg) f GV(u) 'V(a):=, gV(u) 
g g g u Where the v(k)coef.cients are concerned, several choices are available. One possible choice 
described by Floater[Flo97] is re­ferred to as the shape preserving weighting, and ensures that the location 
of a vertex in parametric space relative to its neighbors mimics the local geometry around the vertex 
being considered. The approach described in this article is quite different, as by separat­ing the criteria 
minimizing the distortions from those which ensure that a valid mapping is constructed, we can obtain 
a .ner control on the way the surface is parametrized. For this reason, the simple u harmonic weighting 
de.ned as follows is used for the fv(k)g: u1 if a2N(k),fkg v(k)= ,degree(k)if a=k where degree(k)denotes 
the number of neighbors of k. Clearly, one of the previously mentioned more sophisticated weightings 
such as the shape preserving or gaussian weightings could have been used instead, since they both satisfy 
Equation 2, but it is shown in the next section that by using linear constraints, the same effect can 
be obtained with higher .exibility. 3 NON-DISTORTED MAPPING Figure 3: Isoparametric curves obtained 
without (A) and with (B) non-distortion constraints. In this section, we de.ne the criterion to be minimized 
in order to construct a non-distorted texture mapping. In a nutshell, this cri­terion preserves the perpendicularity 
and constant spacing of the isoparametric curves traced on the surface, as shown in Figure 3. In other 
words, the gradients of uand vshould be perpendicular one to another and constant all over the surface 
(see [Car76]). This requires de.ning the gradient of a function interpolated over a trian­gulated mesh 
from the vertices of the triangulation, de.nition given below. The way the algorithm presented in the 
previous section can be modi.ed to take into account this criterion is then explained. Data in Figure 
3 shows the effect of the constraints described in this section as applied while parametrizing a triangulated 
mesh. The isoparametric curves obtained when applying the algorithm de­scribed in Section 2 are shown 
in Figure 3-A, whereas the con­straints described further on give the result shown in Figure 3-B. 3.1 
Gradient of a Discrete Function 'Interpolated over a Triangulation G a P( a)-P( a0) 1 NY 2 X = P( a 
)-P( a0) 1 a0 X X {P( a 2 )-P( a0 )} N = {P( a )-P( a0 )} X 2  Y = N X a 1 Figure 4: Local orthonormal 
basis (X, Y)ofatriangle T= (a0;a1;a2). As shown in Figure 4, each triangle T=(a0;a1;a2)of Tcan be provided 
with a local orthonormal basis (p(a0);X;Y).The function 'VT(X;Y)denotes the linear interpolation of 'Vover 
the triangle T,where v2fu;vgrepresents one of the two components of 'and where (X, Y) are the local coordinates 
in the orthonormal basis (p(a0);X;Y)of T. In this basis, one can check that the gradient of 'Vis constant 
T over Tand is a linear combination of the values of 'Vat the T three vertices of the triangle T. The 
six coef.cients DX(aj)and DY(aj)given in Equation 5 below are solely dependent on the ge­ometry of the 
triangle T. 2 V @'P T =DX(aj):'V(aj) @X j0 2 V @'P T =DY(aj):'V(aj) @Y j0 8 DX(a0)=(y1,y2)/d DX(a1)=(y2,y0)/d 
DX(a2)=(y0,y1)/d DY(a0)=(x2,x1)/d DY(a1)=(x0,x2)/d where: DY(a2)=(x1,x0)/d d=(x1,x0):(y2,y0),(x2,x0):(y1,y0) 
xj=(p(aj),p(a0)):X : 8j2f0;1;2g yj=(p(aj),p(a0)):Y (5) Using this de.nition of the gradient of ', it 
is possible to write the equations corresponding to the orthogonality and homogeneous spacing of the 
isoparametric curves. The orthogonality of the iso-u and iso-v curves in a triangle Tis given by: 2 @'v3 
T huui @'@'T@X T:45 =0 (6) @X@Y@'v T @Y If we consider that 'uis .xed and that 'vis to be determined, 
re­placing in Equation 6 the gradient of 'vwith its expression given in Equation 5 yields the following 
equation, which linearly com­bines the values of 'vat the three vertices (a0;a1;a2)of T.The equationtobeusedwhen 
is interpolated can be obtained by 'u exchanging uand v in Equation 7. X n 'v(aj):(@ ' u T :DX(aj) + 
@ ' u T :DY(aj))o = 0 @ X @ Y j2f0;1;2g (7) The remaining condition on 'concerns the homogeneous spac­ing 
of the isoparametric curves. In other words, the gradient must not vary from one triangle to another. 
This requires that a common ~ basis for two adjacent triangles Tand Tbe de.ned, as shown in Figure 5. 
The same expressions as introduced in Figure 4 are used. ~ The vector Xis shared by the two bases, and 
Yis such that Y ~ ~ and Ywould become colinear if the pair of triangles (T;T)was unfolded along their 
common edge [a0;a1]. a0 ~ T   Figure 5: Constant gradient across the common edge of two trian­ ~ gles 
Tand T. The homogeneous spacing of the isoparametric curves is veri.ed if, and only if, for each edge 
of Tthe equation below is veri.ed: u u 8 uu @'@'~@'@'~ TTTT =;=, @X@X@Y@Y~ (8) v v v v @' @' : @' TT~@'TT~ 
=;=, @X@X@Y@Y~ 'u'v By replacing in Equation 8 the gradients of and by their ~ expressions in Tand 
T, the following four linear equations are obtained (see Equation 9), concerning the two components Xand 
Yof the gradients of 'uand 'v.The term oWtakes into account ~ the fact that Yand Ypoint in an opposite 
direction. 'V(a0): { DW(a0) + oW: ~DW(a0) +} { } 8v2 f u;vg; 'V(a1): DW(a1) +oW: ~DW(a1) + 8W2 f X ; 
Y g; 'V(a2):fDW(a2)g + { } 'V(~a2): oW: ~DW(~a2) = 0 ,1 if W=X where oW = +1 if W=Y (9)  3.2 Honoring 
Linear Constraints We have shown in Section 2 how D.S.I. can be used to construct a mapping of a triangulated 
mesh. What we want to do now is to take into account the two criteria minimizing the distortions of the 
mapping, namely the perpendicularity and homogeneity criteria previously introduced. These two criteria 
can be written as a set of linear equations. As it is not possible to honor these constraints for a non-developable 
surface, they will be respected in a least square sense, thus minimizing the distortions. The general 
form of such a constraint is given in Equation 10 below: X V fAcV(a):'(a)g=bcV (10) u2l where the values 
AcV(a)and the scalar bcVare constant given co­ef.cients de.ning the constraint c. Equation 7, corresponding 
to the perpendicularity of the isopara­metric curves in the triangle T=(a0;a1;a2), yields two con­ uv 
'u'v v straints cTand cTto be honored when interpolating and respectively. The expression of cTis given 
below in Equation 11. u The expression of the twin constraint cTcan be obtained by per­muting uand vin 
this equation. 8j2f0;1;2g; uu @'@' TT Ac (aj) :DX(aj):DY(aj) += v @X@Y T SS uv uXuYvXvY C=fc;cTgUfc;c;c;cg 
TEEEET2T E2E (13) The generalized roughness R * ('), taking into account the de­gree of violation of 
the constraints C, is given by Equation 14 be­low. In addition to the equation of the roughness given 
in Section 3 (Equation 3), several terms correspond to the linear constraints, as described further on: 
R * (')=R(')+ ! 2 PPP(14) :$c:AcV(a):'V(a),bcc2C V2fu;vgu2l In Equation 14, the term R(')is the roughness 
(see Equation 3), and the second term represents the degree of violation of the linear constraints. Each 
constraint cis ponderated by a given $c>0coef.cient, allowing to tune the relative importance of the 
constraints. For instance, it is possible to make the mapping respect the perpendicularity rather than 
the homogeneity. Moreover, since each triangle T and edge Ehas an individual constraint de.ned, 8a2/fa0;a1;a2g; 
as well as an individual associated $ccoef.cient, it is possible to Acv(a)=0 select the surface zones 
where the distortions are to be minimized T in order of preference. The remaining coef.cient 2]0;+1[is 
a given parameter called the .tting factor and representing the bcv=0 T importance of the constraints 
relative to the roughness. (11) The functionnal R * (')is a quadratic form, whose minimum is reached 
if @R * (')/@'V(a)=0for each v2fu;vgand for each The homogeneity criterion speci.ed by Equation 9 can 
be ex­ uXuYvXvY pressed by the following four constraints cE, cE, cEand cE a2n. This yields the following 
equation, which solution mini­ yielded by Equation 12 below, to be taken into account at each edge mizes 
R * ('): E=(a0;a1)of the triangulation G. The vertices a2and a~2denote V GV (aj )+( :$):,(aj') ~V the 
two remaining vertices of the two triangles Tand Tsharing the '(a)=, (15) gV(a)+( :$): V(a) edge E. {} 
~ DW(a0)+oW:DW(a0) {} DW(a1) AVW(a0) c = VP V ,(aj')=$c:,(aj') c c2C E AVW(a1) c DW(a1) ~ +oW: (16) 
= E AVW(a2)=DW(a2) c P V(a)=$c: cV(a) c2C E with: ~ AVW(a~2)=oW:DW(a~2) c E 8 V(a)=(AV(a)2 cc AVW(a)=08a2/fa0;a1;a2;a~2g 
c E P (17) V bVW =0 :,(aj')=AV(a):AV(P):'V(P),bc ccc E 6u where: n ,1if WX v2fu;vg;W2fX;Yg;oW = 
+1if WY (12) The roughness criterion which D.S.I. minimizes can be general­ized in order to honor a 
set Cof linear constraints in a least square sense. In our case, the set Cof constraints is given by 
Equation 13, where Edenotes the set of the edges of the triangulation G(n;T). The orthogonality constraint 
suggests a modi.cation in the iterative D.S.I. algorithm. The two internal loops iterating on the components 
of 'and on the nodes of nrespectively have been inverted. At each iteration, is interpolated while 'u'v 
 'u'vpermuted. The resulting algorithm given below assigns (u;v) coordinates to the vertices of the triangulation 
while respecting the speci.ed set of constraints. is considered to be constant, then the roles of and 
are let Ibe the set of nodes where 'is unknown let ' [0]be a given initial approximated solution while 
(more iterations are needed) f for all( v2fu;vg) f for all( a2I) f GV(u)+,V(uj¢) 'V (a):=, gV(u)+,V(u) 
g g g    4 LOCALLY CONSTRAINING A MAPPING The constraints de.ned so far in this paper provide the 
user with a global control on the mapping function. Even if the orthogonality and perpendicularity constraints 
can be weighted locally to spec­ify the zones where distortions are preferably to be minimized, this 
may be not suf.cient for some applications, where a more precise set of local constraints is required. 
For instance, it may be neces­sary to align some details of textures with details of models, which can 
be achieved by specifying isoparametric curves. Moreover, the model to be texture mapped can present 
cuts, and the user may want do de.ne a single mapping function for a cut model instead of sewing together 
several patches. This requirement can be ful­.lled by making the mapping continuous through cuts as described 
further on. 4.1 Specifying an Isoparametric Curve As shown in Figure 6, we consider that we have a given 
polygo­nal curve L=fp0;:::;pmgassociated with a given value u0of the parameter u. We describe here the 
constraints to be honored for making the isoparametric of the mapping de.ned by (u=u0) correspond to 
the projection of Lon the surface S. Each point pi of Lyields a constraint cpiensuring that the isoparametric 
curve u=u0of the mapping 'passes near the projection p 0 iof pion S.  Figure 6: Aligning details of 
the texture to details of the model by specifying an isoparametric curve. The triangle T=(a0;a1;a2)is 
the triangle of Sthat contains pi0,and (0;1;2)are the barycentric coordinates of pi0in T.The linear relation 
to be honored is given in Equation 18 below. P j:'u(aj) = u0 j2f0;1;2g 8 P j:p(aj) = p 0 i (18) j2f0;1;2g 
where: P j= 1 : j2f0;1;2g  Equation 19 below gives the expression of the constraint cpiin the form of 
Equation 10 in Section 2. Such a constraint per point piis added to the set Cto be honored by D.S.I., 
introduced in the previous section. 8 Acpi(aj)= j8j2f0;1;2g (19) Acpi(a)=08a2/fa0;a1;a2g : bcpi =u0 
 Figure 7: Extrapolating a mapping from four user speci.ed isopara­metric curves. Remark: As mentioned 
in Section 2, the two following suf.cient conditions ensure that a discrete function 'de.nes a mapping: 
1. The image of the border of the surface through ' in the parametric (u;v)domain is a convex poly­gon. 
 2. Each internal node is a convex combination of its neighbors.  Introducing the constraints to ensure 
that the isoparametric curves are orthogonal, with homogeneous spacing means the .rst condi­tion can 
be replaced by a less restrictive one. As shown in Figure 7, it is then suf.cient to specify four arcs 
of isoparametric curves fu1;u2;v1;v2gusing the constraint previously introduced. Thus, by enabling us 
to use the algorithm not only as an interpolator,but also as an extrapolator, it is possible to construct 
mappings for sur­faces having complex shaped borders by leaving 'unspeci.ed on the border. 4.2 Constructing 
a Mapping for a Cut Surface Let us now consider that the surface has cuts, and that we want the mapping 
function <to be continuous through these cuts. Instead of using several distinct patches and making the 
edges of the patches match as described in [Blo85], the surface is considered here as a single patch 
(as it was before being cut), as suggested in [CEM97]. The set of constraints described below allows 
us to assign (u;v) coordinates to the vertices of the triangulation in such a way that the two borders 
of a cut are mapped to the same curve by the interpolated <mapping function. In other words, the cuts 
are sewn in (u;v)domain space. i i Figure 8: Connecting two borders of a cut in texture space. As shown 
in Figure 8, twin set of points fqi;i=0:::ngand fq~i;i=0:::ngare sampled on the twin borders of the cut. 
We describe now how to make the mapping match at each pair (qi;q~i) of points. More precisely, we want 
to respect the following condi­tions: (1) 'V T(qi) = 'V ~T(~qi) 8v2 f u;vg (20) (2) grad'V T = grad'V 
~T ~ where Tand Tdenote the triangles containing qiand q~irespec­ tively. The gradient grad'Vis computed 
as described in Section T 3 (see Equation 5), using the basis shown in Figure 4. Using the methods introduced 
in the previous two sections, it is easy to translate these two conditions into the D.S.I. constraints 
c Vand c VW, given below in the equations 21 and 22 respec­ qi;q~iqi;q~i tively. AcV qi;~qi (aj) = j 
8j2 f 0;1g AcV qi;~qi (~aj) = , ~ j 8j2 f 0;1g (21) AcV qi;~qi (aj) = 0 8a /2 fa0;a 1;~a0;~a1g bcV qi;~qi 
(aj) = 0  where j;j2f0;1gand ~ j;j2f0;1gdenote the barycentric coordinates of qiin [p(a0);p(a1)]and 
q~iin [p(a~0);p(a~1)]respectively. uXuYvXvY The four constraints c, c, c,and cyielded qi;q~iqi;q~iqi;q~iqi;q~i 
 by Equation 22 below ensure a constant gradient of the mapping through the cut. In other words, an isoparametric 
curve points in ~ the same direction in the two corresponding triangles Tand T. A c VW qi;~qi (aj) = 
Dj 8j2 f 0;1;2g A c VW qi;~qi (~aj) = oW: ~Dj 8j2 f 0;1;2g A c VW qi;~qi (a) = 0 8a /2 f a0; a 1; a 2;~a0;~a1;~a2g 
b c VW qi;~qi (a) = 0  where: n ,1if WX v2fu;vg;W2fX;Yg;oW = +1if WY (22)  5 RESULTS AND APPLICATIONS 
One can see in Figure 9 the results of the method applied to a triangulated mesh representing a face 
(see Figure 9-A). The effect of the orthogonality and homogeneity constraints can be brought to the fore 
by comparing Figure 9-B (no constraint used) and Figure 9-E (orthogonality and homogeneity enforced), 
where a checker pattern is mapped to the mesh. The isoparametric curves corresponding to this latter 
image are displayed in Figure 9-D, where one can check that the iso-u curves shown in red are perpendicular 
to the iso-v shown in blue. In Figure 9-C, the same non-distorting mapping function is used with a fancier 
texture. For all these pictures, the constraints ensuring the continuity of the mapping through cuts 
have been speci.ed at the mouth and the eyes of the model. This model has 3000triangles, and has been 
parametrized after 100 iterations in approximatively one minute using an R4000 machine. As with any other 
texture mapping method, or more precisely as with any parametrization algorithm, our techniques may be 
ap­plied to problems other than those associated with texture mapping. In the realm of geosciences, several 
different methods based on our technique have been implemented into a widely used geology oriented C.A.D. 
software. Among all the possible applications, to name but a few: Unfolding surfaces representing the 
boundaries of geological layers while preserving the volume of the layers;  Generating grids suitable 
for .nite elements analysis;  Beautifying triangulated meshes by remeshing in (u;v)do­main space.  
Constructing Spline surfaces from triangulated meshes;  Performing computations such as geostatistical 
simulations in (u;v)domain space.  Not only do these applications require that mappings present non-distorting 
properties, which is ful.lled by our method, but in addition, these applications will bene.t from the 
ability of our method to take into account additional information expressed in the form of linear constraints. 
The method applied to geological data is demonstrated in Figure 10. In Figure 10-A, one can see a mapping 
of a complex cut surface, corresponding to a boundary of a geological layer presenting faults. In Figure 
10-B, the isoparametric curves of the mapping are dis­played, and one can see that the mapping is continuous 
through the cuts of the surface. In Figure 10-(C,D,E), a surface representing a References dome of salt 
is parametrized. For this kind of surfaces which are far from developable, distortions will still remain, 
and one can choose a compromise between the orthogonality and the homogeneity of the mapping by tuning 
the weightings $cof the two constraints. In Figure 10-C, the orthogonality is respected, but the sizes 
of the squares differ in a great deal, whereas in Figure 10-E the squares have approximatively the same 
size while the isoparametric curves are far from orthogonal. An average solution is shown in Figure 10-D, 
where the same weighting has been used for the two constraints. One can see in Figure 10-F a mapped surface 
with an isoparamet­ric curve speci.ed. As shown in Figure 10-G, the texture has been aligned to this 
curve. CONCLUSIONS We have presented in this paper new techniques for non-distorted mapping. In addition 
to the other methods based on global minimization of distortions, our method can easily take into account 
various additional information. It is thus possible to specify the zones where distortions should be 
minimized in order of preference, to make a set of isoparametric passes through user speci.ed curves, 
and to sew the cuts of a surface in texture space. Moreover, it is very easy to extend the method by 
de.ning new constraints, once these constraints can be expressed as linear (or linearizable) relations. 
The method can be easily implemented, since it does only require an ef.cient representation of triangulated 
meshes, which is provided by most C.A.D. packages. Thus, the algorithm has been integrated as a basic 
algorithm into a widely used C.A.D. software dedicated to geology, and several methods other than these 
associated with texture mapping have been developped based on this algorithm, such as unfolding geological 
layers and performing computations in texture space. One of the limitations of the technique is that 
it can be applied to planar graphs only, i.e. to surfaces topologically equivalent to a disk. A generalization 
of the method working on arbitrary topol­ogy could be realized, by dividing the surface into (topological) 
disks using a Voronoi based approach, as proposed in [ERDH95]. A method such as the one descrined in 
[Tur91] could be also used to choose the sites of the Voronoi diagram. A constraint ensuring the continuity 
of the gradient from one domain to another could be added (see Equation 22), thus blurring the limits 
of the base tri­angles that appear when directly applying the method described in [ERDH95]. The interactivity 
of the tool could also be improved by speeding up the algorithm, using a conjugate gradient method. This 
latter improvement together with a large set of possible local constraints could have an important impact 
on 3D paint systems. Future research also comprise the extension of the method to tetra­hedralized meshes, 
enabling to assign (u;v;w)coordinates to the vertices of tetrahedralizations. AKNOWLEDGEMENTS This research 
has been performed in the frame of the G.CAD project, and the authors want to thank here the sponsors 
of the con­sortium, especially Gaz de France who supports this work. Thanks also to the reviewers for 
their interresting comments. [Blo85] J. Bloomenthal. Modeling the Mighty Maple. In SIGGRAPH 85 Confer­ 
ence Proceedings, volume 19, pages 305 311. ACM, July 1985. [BS86] E. Bier and K. Sloan. Two-Part Texture 
Mapping. IEEE Computer Graphics and Applications, pages 40 53, September 1986. [BVI91] C. Bennis, J.M. 
V´ezien, and G. Igl´esias. Piecewise Surface Flattening for Non-Distorted Texture Mapping. In SIGGRAPH 
91 Conference Proceed­ ings, volume 25, pages 237 246. ACM, July 1991. [Car76] M.F. Do Carmo. Differential 
Geometry of Curves and Surfaces. Prentice Hall, Englewood Cliffs, Inc., 1976. [Cat74] E. Catmull. A Subdivision 
Algorithm for Computer Display of Curved Surfaces. PhD thesis, Dept. of Computer Sciences, University 
of Utah, December 1974. [CEM97] R. Cognot, T. A¨it Ettajer and J.L. Mallet. Modeling Discontinuities 
on Faulted Geological Surfaces. In SEG Technical Program, pages 1711 1718, November 1997. [ERDH95] M. 
Eck, T. DeRose, T. Duchamp, H. Hoppes, M. Lounsbery and W. Stuet­ zle. Multiresolution Analysis of Arbitrary 
Meshes. In SIGGRAPH 95 Conference Proceedings, pages 173 182. ACM, August 1995. [Flo97] M.S. Floater. 
Parametrization and Smooth Approximation of Surface Tri­ angulations. Computer Aided Geometric Design, 
14(3):231 250, April 1997. [KL96] V. Krishnamurthy and M. Levoy. Fitting Smooth Surfaces to Dense Poly­ 
gon Meshes. SIGGRAPH 96 Conference Proceedings, pages 313 324. ACM, August 1996. [LM94] P. Litwinowicz 
and G. Miller. Ef.cient Techniques for Interactive Texture Placement. In SIGGRAPH 94 Conference Proceedings, 
pages 119 122. ACM, July 1994. [Mal89] J.L. Mallet. Discrete Smooth Interpolation in Geometric Modeling. 
ACM-Transactions on Graphics, 8(2):121 144, 1989. [Mal92] J.L. Mallet. Discrete Smooth Interpolation. 
Computer Aided Design Journal, 24(4):263 270, 1992. [ML88] S.D. Ma and H. Lin. Optimal Texture Mapping. 
In EUROGRAPHICS 88, pages 421 428, September 1988. [MYV93] J. Maillot, H. Yahia, and A. Verroust. Interactive 
Texture Mapping. In SIGGRAPH 93 Conference Proceedings, volume 27. ACM, 1993. [Pea85] Peachey, R. Darwyn. 
Solid Texturing of Complex Surfaces. In SIG- GRAPH 85 Conference Proceedings, volume 19, pages 287 296. 
ACM, July 1985. [Ped95] H.K. Pedersen. Decorating Implicit Surfaces. In SIGGRAPH 95 Confer­ [Tur91] G. 
Turk. Generating Textures on Arbitrary Surfaces Using Reaction-Diffusion. In SIGGRAPH 91 Conference Proceedings, 
pages 289 298. ACM, 1991.  ence Proceedings, pages 291 300. ACM, 1995. [SMSW86] Samek, Marcel, C. Slean, 
and H. Weghorst. Texture Mapping and Distor­tions in Digital Graphics. The Visual Computer, 2(5):313 
320, Septem­ber 1986. [Tut60] W.T. Tutte. Convex Representation of Graphs. In Proc. London Math. Soc., 
volume 10, 1960.  AB C  DE Figure 9: Texture mapping on a face.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280933</article_id>
		<sort_key>353</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>34</seq_no>
		<title><![CDATA[Techniques for handling video in virtual environments]]></title>
		<page_from>353</page_from>
		<page_to>360</page_to>
		<doi_number>10.1145/280814.280933</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280933</url>
		<keywords>
			<kw><![CDATA[VRML]]></kw>
			<kw><![CDATA[camera placement]]></kw>
			<kw><![CDATA[virtual environments]]></kw>
			<kw><![CDATA[virtual worlds]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.2</cat_node>
				<descriptor>Distributed/network graphics</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Video (e.g., tape, disk, DVI)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.7.2</cat_node>
				<descriptor>VRML</descriptor>
				<type>P</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010230</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Video summarization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010510</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document preparation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010248</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Video segmentation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010542.10011714</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Other architectures->Special purpose systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Languages</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P97430</person_id>
				<author_profile_id><![CDATA[81100186957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gianpaolo]]></first_name>
				<middle_name><![CDATA[U.]]></middle_name>
				<last_name><![CDATA[Carraro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bell Labs, Holmdel, NJ]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P143669</person_id>
				<author_profile_id><![CDATA[81100579055]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[T.]]></middle_name>
				<last_name><![CDATA[Edmark]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bell Labs, Holmdel, NJ]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP42052651</person_id>
				<author_profile_id><![CDATA[81341489991]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[Robert]]></middle_name>
				<last_name><![CDATA[Ensor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bell Labs, Holmdel, NJ]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>274372</ref_obj_id>
				<ref_obj_pid>271897</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Carraro, G., Cortes, M., Edmark, J., and Ensor, J., "The Peloton Bicycling Simulator," Proc. VRML '98, Monterey, CA, 16-19 February, 1998.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166153</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Chen, S. and Williams, L., "View Interpolation for Image Synthesis," Proc. SIGGRAPH 93, 1-6 August, 1993, Anaheim, CA, pp. 279-288.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Cuesta J., Cycling World. In El Faro Web Site: http://www.elfaro.com/vrml20/cycling/thegame/]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237191</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Debevec, P., Taylor, C., and Malik, J., "Modeling and Rendering Architecture from Photographs: A hybrid geometryand image-based approach," Proc. SIGGRAPH 96, 4-9 August, 1996, New Orleans, LA, pp. 11-20.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Ensor, J. and Carraro, G., "Peloton: A Distributed Simulation for the World Wide Web," Proc. 1998 International Conf. On Web-based Modeling and Simulation, San Diego, CA, 12-14 January, 1998.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>159587</ref_obj_id>
				<ref_obj_pid>159544</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Feiner, S., Macintyre, B., and Seligmann, D., "Knowledge- Based Augmented Reality," Communications of the ACM, (36, 7), June 1993, pp. 53-62.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[3DK: The Virtual Studio. In GMD Web Site: http://viswiz.gmd.de/DML/vst/vst.html]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[GPIR. In Rich Gossweiler Web Site: http://reality.sgi.com/rcg/vrml/gpir/playground/playground.html]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258854</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Horry, Y., Anjyo, K., and Arai, K., "Tour Into the Picture: Using a Spidery Mesh Interface to Make Animation from a Single Image," Proc. SIGGRAPH ,97, 3-8 August, 1997, Los Angeles,CA, pp. 225-232.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>959147</ref_obj_id>
				<ref_obj_pid>959145</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Katkere, A., Moessi, S., Kuramura, D., Kelly, P., and Jain, R., "Towards Video-based Immersive Environments," Multimedia Systems, May 1997, pp. 69-85.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>215267</ref_obj_id>
				<ref_obj_pid>217279</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Kelly, P., Katkere, A., Kuramura, D., Moezzi, S., Chatterjee, S., and Jain, R., "An Architecture for Multiple Perspective Interactive Video," Proc. Multimedia '95, San Francisco, CA, 1995, pp. 201-212.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218398</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[McMillan, L. and Bishop, G., "Plenoptic Modeling: An Image-Based Rendering System," Proc. SIGGRAPH 95, 6-11 August, 1995, Los Angeles, CA, pp. 39-46.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[MPEG Home Page. In http://drogo.cselt.stet.it/mpeg]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Interspace VR Browser. In NTT Software Corp. Web Site: http://www.ntts.com Interspace]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Oh, S., Sugano, H., Fujikawa, K., Matsuura, T., Shimojo, S., Arikawa, M., and Miyahara, H., "A Dynamic QoS Adaptation Mechanism for Networked Virtual Reality," Proc. Fifth IFIP International Workshop on Quality of Service, New York, May 1997, pp. 397-400.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Virtual Reality Bike. In Tectrix Web Site: http://www.tectrix.com/ products/VRBike/VR_Bike.html]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>959155</ref_obj_id>
				<ref_obj_pid>959145</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Thalmann, N., and Thalmann, D., "Animating Virtual Actors in Real Environments," Multimedia Systems, May 1997, pp. 113- 125.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[UltraCoach VR. In Ultracoach Web Site: http://www.ultracch.com]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Virtual Reality Modeling Language (VRML) Version 2.0. In VRML Consortium Web Site: http://www.vrml.org/ Specifications/VRML2.0/]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Waters, R. et al., Diamond Park and Spline: Social Virtual Reality with 3D Animation, Spoken Interaction, and Runtime Extendability. Presence. Vol. 6 No. 4 pp.461-481. MIT Press]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>223946</ref_obj_id>
				<ref_obj_pid>223904</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Yamaashi, K., Kawanata, Y., Tani, M., and Matsumoto, H., "User-Centered Video: Transmitting Video Images Based on the User's Interest," Proc. Chi '95.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218449</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Zorin, D. and Barr, A., "Correction of Geometric Perceptual Distortions in Pictures," Proc. SIGGRAPH 95, 6-11 August, 1995, Los Angeles, CA, pp. 257-264.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Copyright &#38;#169;1998 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers or to redistribute to lists, requires specific permission and/or a fee. Techniques for Handling 
Video in Virtual Environments Gianpaolo U. Carraro, John T. Edmark, J. Robert Ensor Bell Laboratories 
 Abstract This paper discusses ways to incorporate video displays into virtual environments. It focuses 
on the virtual worlds created by a distributed multi-user simulator. Still images or video streams represent 
spaces within these three-dimensional worlds. The paper introduces techniques to deal with avatar movement 
into and out of video regions. In one technique media melding when an object moves from one region to 
another, the media used to represent that object correspondingly change. In a second technique object 
tracing when an object moves from one region to another, its actions in the second region are represented 
by a trace object in the first region. Pyramidic panels provide a means of dealing with viewpoint changes 
so that two-dimensional images and video clips can successfully simulate three­dimensional spaces. The 
paper concludes by suggesting ways to extend our techniques and by listing possible future studies. CR 
Categories and Subject Descriptors: H.5.1 [Information Systems]: Multimedia Information Systems Artificial 
Realities; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism Virtual Reality; I.3.2 [Computer 
Graphics]: Graphics Systems Distributed/Network Graphics Additional Keywords: virtual worlds, virtual 
environments, camera placement, VRML 1 INTRODUCTION We have built a sports simulator, called Peloton[1, 
5], suitable for athletic training and competition. It creates virtual road courses for walking, running, 
and bicycle riding. Users participate in simulations by walking or running on treadmills or by pedaling 
stationary bicycles. An exercise apparatus is attached to each user s local computer and serves as the 
primary input/output device for the simulation participant. The simulator senses the user s progress 
along a virtual road course and changes the incline of the treadmill or alters the pedaling resistance 
of the bicycle to create force feedback corresponding to the ________________ 4F-601, 101 Crawfords Corner 
Road, Holmdel, NJ 07733 paolo@bell-labs.com changing terrain. Users may exercise alone or they may communicate 
over the World Wide Web to share Peloton virtual spaces with each other. The visual component of the 
simulator s virtual environment is a synthetic, three-dimensional landscape, modeled in the Virtual Reality 
Modeling Language (VRML)[19]. Parts of these worlds are displayed as three-dimensional graphical objects; 
other regions are displayed as still images or video streams. We chose this combination of media to create 
worlds that show the positions of simulation participants on three-dimensional terrains and to enrich 
these terrains through photo-realistic display elements. The combination also gives us a chance to tailor 
displays to the computing and communication resources of system users. People with powerful machines 
but poor network connections can specify that a world be displayed primarily (or exclusively) as locally 
rendered three-dimensional objects. On the other hand, users with network computers offering high-end 
network connections but less processing power can specify that a world be displayed primarily as streamed 
video. Figure 1 is a view from a Peloton virtual world, which models New York City s Central Park. The 
central portion of this view is a video stream, which is displayed as a texture on a large rectangle 
a two-dimensional video screen. In the surrounding region, graphical elements represent the road and 
some roadside objects. The bicyclist avatars represent multiple users who are exploring this world concurrently. 
As simulation participants move along a virtual roadway, their avatars can move from one region to another. 
Participants are often spread over different regions of a virtual world, and each participant is likely 
to see competitors avatars move to and from image/video regions. In Figure 1, for example, the blue avatar 
is on a section of road displayed as three-dimensional objects, while the red and green avatars have 
moved ahead to a road segment displayed as a video region. Also, as Peloton users move around the virtual 
environment, they see image/video displays as parts of an encompassing, coherent three-dimensional space. 
To maintain visual continuity with their surrounding regions, these two­dimensional displays must dynamically 
respond to the moving user viewpoints. We have developed techniques to deal with the movement of objects 
into and out of video regions and also with the integration of images into their three-dimensional contexts. 
Two techniques deal with object movement. In one media melding when an object moves from one region to 
another, the media used to represent that object correspondingly change. The red avatar of Figure 1 demonstrates 
media melding. Upon entering a video region, it became a video object. In the second technique object 
tracing when an object moves from one region to another, its actions in the second region are represented 
by a trace object in the first region. The green avatar of Figure 1 is a three­  Figure 1: A Peloton 
Virtual Environment Representing New York City s Central Park dimensional trace object representing a 
cyclist in the video region. A third technique deals with viewing images from multiple viewpoints. Pyramidic 
panels and their associated transforms provide means for dealing with viewpoint changes, so that two­dimensional 
images and video clips can successfully represent three-dimensional spaces. These three techniques are 
the main contribution of our work and are the focus of this paper.  2 BACKGROUND Peloton has basic characteristics 
in common with other distributed, multi-party simulators. However, Peloton s virtual environments are 
distinctive because they permit objects to move between video and graphical regions in response to user 
actions. In addition, image and video displays within Peloton virtual worlds undergo specialized distortions 
as they respond to changing user viewpoints. 2.1 Simulating Bicycle Rides A few bicycle simulation programs 
are commercially available. For example, CompuTrainer[3], the Virtual Reality Bike[16], and UltraCoach 
VR[18] create shared virtual environments for simulations among people who ride computer-based bicycling 
devices. Similarly, people can use exercycles to navigate Diamond Park[20], an experimental three-dimensional 
synthetic environment for social interactions. In addition, Cuesta has posted a VRML based bicycling 
game, called Cycling World: The Virtual Tour[3], on the World Wide Web. However, this game does not involve 
exercise devices, and it does not create multi­user worlds. Unlike the virtual worlds of these games 
and simulators, Peloton worlds contain video regions as well as three­dimensional graphical regions. 
 2.2 Combining Graphics and Images/Videos Photographs are commonly used as textures and background elements 
in three-dimensional models. Image based modeling techniques, e.g., [4] and [12], permit creation of 
three­dimensional models from two-dimensional photographs, and objects in the resulting virtual worlds 
can be textured with the images used to generate their structure. In a virtual world by Gossweiler[8], 
surface textures are selected according to viewpoint location and velocity. However, none of these systems 
treat the image textures themselves as three-dimensional spaces. Hence they do not accommodate object 
movement to and from the textures. Computer-generated graphics and recordings of live action are often 
combined in special effects for movies. Roger Rabbit, for example, illustrates a successful combination 
of these media. Of course, these movies are fixed compositions; their effects cannot vary. On the other 
hand, Peloton s effects are generated as real­time responses to user actions and viewpoint changes. Virtual 
sets, e.g., [7], [10], and [17], are a form of electronic theater in which live actors can move within 
computer-generated settings. Augmented reality systems, e.g., [6], create another type of composition 
they lay computer-generated graphics over video inputs. The Interspace system[14], which supports real-time 
multimedia, multiparty conferences on the Internet, creates virtual spaces in which avatars have live 
video streams as heads. [15] and [21] discuss virtual reality systems, containing video displays, in 
which graphical objects are assigned priorities, and these priorities help control video transmission 
streams. The graphical objects in these systems can be manipulated in response to real­time events, e.g., 
changes of camera position. However, none support movement of objects between their graphical and video 
elements. The MPEG-4 proposal[13], currently under development, is expected to permit specification of 
data displays as compositions of video and graphical objects. Hence, this standard might provide a means 
of specifying hardware and software infrastructure to support our techniques straightforwardly and efficiently. 
 2.3 Manipulating Images A significant body of work, e.g., [2] and [11], involves interpolations among 
images to create visual continuity during motion or other changes within three-dimensional virtual spaces. 
Pyramidic panel transforms can be used to generate interpolations, but do not perform this role in Peloton. 
[9] discusses an approach for creating a static three-dimensional model from a single image. Parts of 
the original image provide texture for a background, while separate model elements are generated from 
foreground portions of the image. Pyramidic panel transforms do not produce spatial models from images, 
nor do they create multiple panels from a single image. Rather, our technique distorts an image in response 
to viewpoint changes. Other techniques for applying distortions to images, e.g., [22], have been described. 
Pyramidic panel transforms are more specialized than these general deformation processes. Hence, they 
are appropriate only for a limited image domain. However, the domain can be broadened somewhat by modifying 
the transforms according to information about the virtual world that surrounds an image. Furthermore, 
our techniques avoid complex texture mapping calculations by taking advantage of rendering functions 
found in most three-dimensional graphics engines.  3 CREATING REGIONS Each Peloton virtual world is 
partitioned into regions. To create visual continuity among these subspaces, we calibrate their basic 
geometric properties. We have given most of our attention to the integration of image/video regions and 
graphical regions, and, in this section, we describe how we calibrated these region interfaces when we 
built a model of Central Park. (For the remainder of this paper, we shall use the terms video region 
and video panel to denote a region displayed as either still images or video clips.) We gathered data 
for the Central Park course from a topographical map, measuring road coordinates at regular intervals. 
We measured the road s width directly at a few locations. We modeled the road from these data only. By 
using such an elementary description of the real world, we created a somewhat stylized virtual road it 
does not change widths, it has no banked turns, and it contains no complex curves between elevation data 
points. The roadside terrain is modeled as extensions of road elevations. Meanwhile, we recorded our 
video clips of the road course. We mounted our camera at a known height and orientation on a car, and 
we then taped the video while driving along the center of the road. The tape provided us with information 
used to build additional objects in the graphical region. We were able to identify objects, such as trees, 
by their appearance in the video recording. Using a triangulation process based on the video images, 
we placed these items at their proper positions in the graphical model. A static two-dimensional image 
of an arbitrary space can fit into an encompassing three-dimensional world only when it i) is placed 
in a unique position in this world, and ii) is viewed from a unique position in this world. The unique 
placement position corresponds to the image s original context, while the unique viewing location corresponds 
to the position from which the image was recorded. Section 5 will discuss pyramidic panel transforms, 
a set of image deformations that relax the viewing restriction. However, we must still place the images 
in their unique correct positions within the graphical model. We used a single video frame as a calibration 
reference for all video images within the Central Park world. We were able to use this simplified calibration 
for two reasons. Our graphical road closely approximates the videotaped road segment, and our videotape 
images are all constrained with respect to the road surface. (The taping constraints include a fixed 
focal length camera, a camera platform at a fixed height and orientation to the road surface, and a camera 
path along the center of the road.) While viewing the reference image from its center of projection, 
we scaled and cropped it to create an alignment of the road s display in the video and graphical regions. 
The alignment produced by this calibration is the basis for continuity between the video displays and 
their surroundings. The continuity is enforced by other structures that appear in both the video panel 
and the adjacent graphical region, e.g., fences, sidewalks, and trees, (as shown in Figure 1). Visual 
continuity among regions depends on more than geometric parameters. Lighting, for example, greatly impacts 
a region s appearance. Even carefully aligned objects may exhibit significant discontinuities when straddling 
regions with lighting differences. In our Central Park world, we reduced discontinuities between graphical 
and video regions by using a high level of ambient lighting in the graphical region and by shooting video 
footage on an overcast day. The resulting video images have several desirable properties, including the 
lack of strong distinct shadows and shading, and a featureless, uniformly colored sky. Thus, we have 
avoided the need to render shadows in the corresponding graphical regions, and we can more easily match 
colors between regions. Peloton video panels can move. This capability causes time to become a parameter 
of our calibration process. Our videotape of Central Park provides us with a collection of images, and 
each video frame corresponds to a position in our graphical model of the park. To maintain alignment 
with adjacent graphical objects, these images must be displayed at the correct positions. There are two 
means of controlling this display synchronization. One is to update images according to panel positions; 
the other is to update panel positions according to images. In the current version of Peloton, the video 
panel moves along the road at a speed determined by the video frame update rate. Because we traveled 
at a constant speed when taping our Central Park video, this frame update rate produces a fixed distance 
between the locations for successive frame displays. Since we did not individually calibrate the frames 
of our videotape with the corresponding locations of the Central Park virtual world, most of these images 
do not align exactly with their surroundings. However, simulation participants report that they still 
perceive this virtual world as an effective integration of video and graphical regions.  Figure 4: 
Merge-in, Behind-the-scenes  4 MOVING OBJECTS One of Peloton s distinguishing characteristics is the 
movement of objects between its video regions and its graphical regions. The system generates these moves 
during real-time responses to events. When an object moves into a new region, it is handled by one of 
two techniques. When being handled by the media melding transformation, the medium in which the object 
is represented is changed to match the medium of the region it has just entered. Alternatively, a moving 
object can be handled by the object tracing transformation. In this approach, when an object leaves a 
three-dimensional foreground region to enter a video region, it is represented by a trace object in the 
foreground. 4.1 Media Melding To undergo media melding, an object must have different media representations. 
In Peloton, each cyclist avatar has three representations a three-dimensional graphical object, a still 
image, and a video clip that allow the avatar to meld into any region of a Peloton world. 4.1.1 Merge-in 
A merge-in is a particular type of media melding. It occurs when a graphical object changes to a video 
element in response to its movement from a graphical region into a video-based one. We call this transform 
a merge-in because, in a sense, the object merges into the two-dimensional video display. In Peloton 
simulations, this transform typically occurs when a cyclist gains a big lead over the viewing cyclist. 
For example, Figure 2 illustrates a situation in which the red cyclist has moved into the video region 
ahead of the viewer the yellow cyclist and has become a video element. Figure 4 is a behind-the-scenes 
view of this merge-in. On the far left, the semi-transparent avatar represents the red cyclist s real 
Figure 5: Trace Object, Behind-the-scenes position in the virtual world. In conventional three-dimensional 
worlds, the video panel would occlude the yellow cyclist s view of the red cyclist s position. However, 
by performing this merge­in, Peloton allows the leading cyclist to remain visible from the yellow cyclist 
s point of view. When Peloton s animation module detects that the red avatar has intersected the video 
panel, the avatar is removed from the module s list of graphical objects and is added to its list of 
the video elements. The red cyclist s representation then becomes a small video panel, which is placed 
between the existing video panel and the yellow cyclist s point of view. (In this case, the small video 
is positioned directly in front of the large video panel.) The relationship between the red cyclist s 
real position and the yellow cyclist s point of view is now used to control scaling and translation of 
the red cyclist s video panel. These transforms attempt to create the illusion that the merge-in video 
panel is moving within the space represented by the existing video panel. However, Peloton s implementation 
of the technique has significant limitations. Most obviously, the cyclist s video is displayed on a rectangular 
panel that creates noticeable discontinuities with its surrounding video. Also, objects within the surrounding 
video cannot occlude the cyclist s video, and the cyclist s actual orientation is not reproduced in the 
merge-in video panel. 4.1.2 Pop-out A pop-out is the complementary operation to a merge-in. It occurs 
when a previously merged-in video element changes back to a graphical object in response to its movement 
from a video-based region into a graphical one. We call this transform a pop-out because the two-dimensional 
element seems to pop out of the video. In Peloton simulations, this transform typically occurs when the 
viewing cyclist moves closer to a leading cyclist one who had been riding ahead in a video region. The 
cyclist returns to its three-dimensional form in order to keep its representation visible in the graphical 
region.  4.2 Trace Objects We have developed an alternative to media melding; it s called object tracing. 
Objects may be designated as traceable objects. When a traceable object moves from a three-dimensional 
region to intersect a video panel, it does not become a video element. Instead, it is replaced in the 
three-dimensional foreground by a trace object. Figure 3 shows the red cyclist s trace object from the 
yellow cyclist s point of view. Figure 5 is a behind-the-scenes view of the same situation. As in Figure 
4, the semi-transparent red avatar on the left shows the red cyclist s real position. When Peloton s 
animation module detects that the red avatar has intersected the video panel, it creates a trace object 
to represent the red cyclist from the yellow cyclist s point of view. The trace object is a copy of the 
red avatar, and it is placed just in front of the existing video panel. As with media melding, the red 
cyclist s real simulation position is used to control scaling and translation of the trace object. Furthermore, 
the trace object reproduces the cyclist s actual orientation. In this case, the desired illusion is successful 
from yellow s viewpoint, one cannot distinguish between the red cyclist s avatar and its trace object. 
  5 MOVING VIEWPOINTS As discussed in Section 3, achieving continuity between an image and its surrounding 
environment requires careful placement and sizing of the image. Even after an image and its surroundings 
have been calibrated, there exists only one viewpoint from which that image s contents properly correspond 
to the surrounding environment. We call this unique location the image s IVP (Ideal Viewing Point). Figure 
2 contains an image seen from its IVP. From this view, the image aligns well with surrounding objects. 
Peloton users rarely view an image from its IVP. As simulation participants move left or right on a road, 
as they round curves, or as they move closer to or farther from a video panel, they see an image from 
positions other than its IVP. Figure 7a shows an image seen from a point to the left of its IVP; objects 
within the image do not align with surrounding objects. Figure 6: Pyramidic Panel Structure 5.1 Pyramidic 
Panels We have developed a structure, called the pyramidic panel, for displaying images within a surrounding 
three-dimensional space. The transforms associated with a pyramidic panel dynamically distort images 
according to viewer positions. As the viewer moves away from an image s IVP, the distortions act to limit 
the discontinuities between the image and its surroundings. The pyramidic panel technique exploits a 
characteristic common to all Peloton course images they are views down a road. In these images, the road 
and its immediate vicinity are treated as a kind of corridor whose floor is formed by the roadbed, whose 
ceiling is formed by the sky, and whose walls are formed by the roadside objects. This treatment allows 
single point perspective principles to be used for distorting the Peloton course images according to 
the movement of the viewer. Pyramidic panels for images of straight road segments are created as follows: 
1) An image of the road, captured and positioned according to the procedure described in Section 3, is 
clipped so that the left and right road edges pass through the left and right bottom corners of the image, 
respectively. This clipping ensures that the roadbed maps to the floor of the hypothetical corridor. 
2) The location of the vanishing point for this image is determined. Using the virtual world s road model, 
a vector corresponding to the road s direction is projected from the image s IVP through the image panel. 
The point of intersection with the panel is the image s vanishing point. (If the road direction is very 
different from a normal to the image panel, the vanishing point may be out of bounds of the image. In 
this case, it is adjusted to bring it within the image.) As shown in the left-hand side of Figure 6, 
the image is then segmented into four triangular faces one for each of the hypothetical corridor s surfaces. 
The intersection point of the four faces corresponds to the vanishing point for the corridor. 3) The 
intersection point of the four faces is then coupled with the viewer s location in the following manner. 
Coupling vector.C.projects from IVP to the image s vanishing point, P, found in step 2. Translation vector 
T projects from IVP to the viewer s current location, V. As the viewer moves, the new vanishing point, 
P , is calculated as P = V + C + T. As shown in the right-hand side of Figure 6, this coupling results 
in a four sided pyramid. Its fixed base corresponds to the original image panel, and its peak moves in 
concert with the viewer s location. Figures 7 through 11 compare the display of an image on a flat panel 
with the display of the same image on a pyramidic panel. Part a of each figure shows the image texture-mapped 
onto a flat panel, while part b of the corresponding figure shows the same view of the image texture-mapped 
onto a pyramidic panel. Part c of each figure is a behind-the-scenes view of the pyramidic panel that 
is producing the distortion for part b. The yellow movie camera represents the viewpoint for parts a 
and b. Red lines clarify the pyramidic panel s distortion. In Figure 7, the viewpoint is to the left 
of the image s IVP. Part a shows a discontinuity of the road edge between the three­dimensional region 
and the video panel. Part b shows how the pyramidic panel transforms have eliminated this discontinuity. 
In Figure 8, the viewpoint is higher than the image s IVP. Part a again shows a discontinuity in the 
road edge. In addition, the horizon lines do not align between the graphical and video regions. The road 
seems to be heading down into the ground in   Figure 7a: Flat Panel, Left of IVP    Figure 7b: 
Pyramidic Panel, Left of IVP    Figure 7c: Behind-the-scenes View Figure 8a: Flat Panel, Above IVP 
    Figure 8b: Pyramidic Panel, Above IVP    Figure 8c: Behind-the-scenes View Figure 9a: Flat 
Panel, Close Range    Figure 9b: Pyramidic Panel, Close Range    Figure 9c: Behind-the-scenes View 
 Figure 10a: Flat Panel, Side Corner    Figure 10b: Pyramidic Panel, Side Corner    Figure 10c: 
Behind-the-scenes View Figure 11a: Flat Panel, Looking Back  Figure 11b: Pyramidic Panel, Looking 
Back   Figure 11c: Behind-the-scenes View   the first image, whereas in part b the roadbed appears 
flush with the surrounding model s ground. In Figure 9, the viewpoint is very close to the image. Part 
a shows a nearly unintelligible collection of pixels, whereas part b shows a comprehensible view of the 
road. In Figure 10, the viewpoint is far to the right, close to the panel, and turned to the left. Part 
a shows a large discontinuity between the road in the graphical region and the road on the panel. Part 
b shows that the pyramidic panel s image maintains continuity quite well. In Figure 11, the viewpoint 
has gone through the flat panel and turned around to face backwards. In part a we see only the back of 
the panel, whereas in part b we see the pyramidic panel surrounding our view onto the model. (In part 
c, the camera cannot be seen because it is inside the pyramid.)  5.2 Articulated Pyramidic Panels The 
basic pyramidic panel technique described above is less effective when applied to an image of a curved 
road. Such a road contains multiple vanishing points. Choosing a single vanishing point based on one 
section of the road leads to distortions in other sections of the road. For example, Figure 12 shows 
a pyramidic panel containing an image of a curved road. The panel is being viewed from a point higher 
than the image s IVP. The vanishing point was chosen to correspond to the direction of the road in the 
foreground of the image. Although the technique yields reasonable results for this foreground road section, 
it breaks down for the more distant sections of the road. Figure 13 shows the same image from the same 
viewpoint. However, the image is now displayed using an articulated pyramidic panel (APP). An articulated 
pyramidic panel uses multiple vanishing points to segment the basic pyramidic panel. The APP in Figures 
13 contains two vanishing points. Figure 14 shows the same APP from the image s IVP. A two-vanishing-point 
APP is created as follows: 1. An image of the road is captured, placed and clipped as in step 1 of the 
basic pyramid procedure. 2. The road is treated as two straight corridors placed end-to­end, extending 
back from the panel. Each corridor s direction and length is calculated from the virtual world s model 
of the road. Using the directions of the two road segments the two corridors the corresponding vanishing 
points, P1 and P2, are determined. Using the first corridor s vanishing point, P1, an initial pyramidic 
panel is constructed as in step 2 of the basic pyramid procedure. (See Figure 14.) 3. The coupling ratio, 
a, for the first corridor is calculated: a = l/(l+d), where l is the length of the first corridor, and 
d is the distance between the image s IVP and the base of the pyramidic panel. 4. Each line segment 
connecting a corner of the panel to the vanishing point is divided in two segments by a point placed 
according to the coupling ratio. Specifically, the length l of the line segment from the corner of the 
panel to this point is given by the following formula: l = al , where l is the total length of the 
segment between the corner of the panel and the vanishing point. These four points Q1 through Q4 in Figure 
14 are connected to form the base of a smaller pyramidic panel embedded within the larger one. The intersection 
point of the four triangles of this embedded pyramidic panel is then moved to the location of the second 
vanishing point, P2.  This APP now has five internal points that must be coupled with the viewer s movement. 
The coupling of the second vanishing point is the same as for the basic pyramidic panels. The coupling, 
Qc, for the other four points, Q1 through Q4, is computed as follows: Qc= V + C + aT. (Recall that V, 
C, and T are defined in Section 5.1.)   6 FUTURE WORK The techniques described in this paper have 
several apparent extensions. Media melding can be enhanced in several ways. The Peloton implementation 
of merge-ins is based on a stacking of video panels. This approach has important shortcomings and often 
yields crude visual effects. By replacing the stacking approach with a composition of video objects (which 
might be possible with future MPEG-4 terminals), we could reduce our present occlusion deficiencies as 
well as achieve more cohesive combinations of video elements. Our current implementation of pop-outs 
is also restricted; we only allow pop-outs of avatars. By using various image-processing techniques on 
video frames, a system could automatically detect video elements of interest and determine when they 
leave a video panel. Image-based modeling techniques could then create and place the pop-out objects 
in three-dimensional regions. Pyramidic panels could be extended to handle a wider range of image content. 
For example, non­rectilinear corridors and corridors with changing cross sections could be accommodated 
by additional pyramidic constructions and manipulations. Media combinations other than graphics and video 
are possible and potentially useful. For example audio-only regions could be added to Peloton worlds. 
An object represented as a video or graphical element could meld into an audio clip, (i.e., presenting 
the sound of a spinning bicycle wheel or the live speech of a user) when moving into this new region. 
A variety of new applications could be developed with the techniques introduced here. For example, education 
applications could allow students to pick objects from a movie, move them into a region of three-dimensional 
objects, and study them there by viewing and animating their three-dimensional representations. Similarly, 
shopping applications could allow shoppers to pick objects from a catalog and see them displayed in a 
movie. A region can serve as a level of detail specification for groups of objects. For example, a region 
near the viewer (camera) can be displayed in one medium, while more distant parts of a virtual world 
can be displayed through other media. These multimedia levels of detail, then, support applications that 
highlight information for users as they explore virtual worlds. 7 SUMMARY This paper has described techniques 
useful for building three­dimensional virtual worlds with subspaces displayed as two­dimensional images 
or video clips. Two techniques for representing movement of objects between graphical and video regions 
media melding and object tracing were described. Pyramidic panels were introduced as a means of dealing 
with viewpoint changes so that two-dimensional images and video clips can better simulate three-dimensional 
spaces. We have used these techniques in creating virtual worlds for a bicycling simulator. Simulator 
users report that these techniques help create virtual worlds in which still images and video clips successfully 
represent parts of a single, cohesive three­dimensional space.  References [1] Carraro, G., Cortes, 
M., Edmark, J., and Ensor, J., The Peloton Bicycling Simulator, Proc. VRML 98, Monterey, CA, 16-19 February, 
1998. [2] Chen, S. and Williams, L., View Interpolation for Image Synthesis, Proc. SIGGRAPH 93, 1-6 August, 
1993, Anaheim, CA, pp. 279-288. [3] Cuesta J., Cycling World. In El Faro Web Site: http://www.elfaro.com/vrml20/cycling/thegame/ 
[4] Debevec, P., Taylor, C., and Malik, J., Modeling and Rendering Architecture from Photographs: A hybrid 
geometry­and image-based approach, Proc. SIGGRAPH 96, 4-9 August, 1996, New Orleans, LA, pp. 11-20. [5] 
Ensor, J. and Carraro, G., Peloton: A Distributed Simulation for the World Wide Web, Proc. 1998 International 
Conf. On Web-based Modeling and Simulation, San Diego, CA, 12-14 January, 1998. [6] Feiner, S., Macintyre, 
B., and Seligmann, D., Knowledge-Based Augmented Reality, Communications of the ACM, (36, 7), June 1993, 
pp. 53-62. [7] 3DK: The Virtual Studio. In GMD Web Site: http://viswiz.gmd.de/DML/vst/vst.html [8] GPIR. 
In Rich Gossweiler Web Site: http://reality.sgi.com/rcg/vrml/gpir/playground/playground.html [9] Horry, 
Y., Anjyo, K., and Arai, K., Tour Into the Picture: Using a Spidery Mesh Interface to Make Animation 
from a Single Image, Proc. SIGGRAPH ,97, 3-8 August, 1997, Los Angeles,CA, pp. 225-232. [10] Katkere, 
A., Moessi, S., Kuramura, D., Kelly, P., and Jain, R., Towards Video-based Immersive Environments, Multimedia 
Systems, May 1997, pp. 69-85. [11] Kelly, P., Katkere, A., Kuramura, D., Moezzi, S., Chatterjee, S., 
and Jain, R., An Architecture for Multiple Perspective Interactive Video, Proc. Multimedia 95, San Francisco, 
CA, 1995, pp. 201-212. [12] McMillan, L. and Bishop, G., Plenoptic Modeling: An Image-Based Rendering 
System, Proc. SIGGRAPH 95, 6-11 August, 1995, Los Angeles, CA, pp. 39-46. [13] MPEG Home Page. In http://drogo.cselt.stet.it/mpeg 
[14] Interspace VR Browser. In NTT Software Corp. Web Site: http://www.ntts.com Interspace [15] Oh, S., 
Sugano, H., Fujikawa, K., Matsuura, T., Shimojo, S., Arikawa, M., and Miyahara, H., A Dynamic QoS Adaptation 
Mechanism for Networked Virtual Reality, Proc. Fifth IFIP International Workshop on Quality of Service, 
New York, May 1997, pp. 397-400. [16] Virtual Reality Bike. In Tectrix Web Site: http://www.tectrix.com/ 
products/VRBike/VR_Bike.html [17] Thalmann, N., and Thalmann, D., Animating Virtual Actors in Real Environments, 
Multimedia Systems, May 1997, pp. 113­ 125. [18] UltraCoach VR. In Ultracoach Web Site: http://www.ultracch.com 
[19] Virtual Reality Modeling Language (VRML) Version 2.0. In VRML Consortium Web Site: http://www.vrml.org/ 
Specifications/VRML2.0/ [20] Waters, R. et al., Diamond Park and Spline: Social Virtual Reality with 
3D Animation, Spoken Interaction, and Runtime Extendability. Presence. Vol. 6 No. 4 pp.461-481. MIT Press 
[21] Yamaashi, K., Kawanata, Y., Tani, M., and Matsumoto, H., User-Centered Video: Transmitting Video 
Images Based on the User s Interest, Proc. Chi 95. [22] Zorin, D. and Barr, A., Correction of Geometric 
Perceptual Distortions in Pictures, Proc. SIGGRAPH 95, 6-11 August, 1995, Los Angeles, CA, pp. 257-264. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280935</article_id>
		<sort_key>361</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>35</seq_no>
		<title><![CDATA[A distributed 3D graphics library]]></title>
		<page_from>361</page_from>
		<page_to>370</page_to>
		<doi_number>10.1145/280814.280935</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280935</url>
		<keywords>
			<kw><![CDATA[distributed shared memory]]></kw>
			<kw><![CDATA[distributed virtual environments]]></kw>
			<kw><![CDATA[object-oriented graphics]]></kw>
			<kw><![CDATA[shared-data object model]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>D.1.3</cat_node>
				<descriptor>Distributed programming</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.2</cat_node>
				<descriptor>Distributed/network graphics</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Graphics data structures and data types</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.4.1</cat_node>
				<descriptor>Groupware</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010394</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics file formats</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003130.10003131.10003570</concept_id>
				<concept_desc>CCS->Human-centered computing->Collaborative and social computing->Collaborative and social computing theory, concepts and paradigms->Computer supported cooperative work</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003457.10003567.10003570</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing and business->Computer supported cooperative work</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010542.10011714</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Other architectures->Special purpose systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010919.10010177</concept_id>
				<concept_desc>CCS->Computing methodologies->Distributed computing methodologies->Distributed programming languages</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011008.10011009.10010177</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->General programming languages->Language types->Distributed programming languages</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14044595</person_id>
				<author_profile_id><![CDATA[81100099162]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Blair]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[MacIntyre]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Columbia Univ., New York, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39042315</person_id>
				<author_profile_id><![CDATA[81100427474]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Steven]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Feiner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Columbia Univ., New York, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[D.B. Anderson, J. W. Barrus, J. H. Howard, C. Rich, C. Shen, and R. C. Waters. Building Multi-User Interactive Multimedia Environments at MERL. Technical Report Research Report TR95-17, Mitsubishi Electric Research Laboratory, November 1995.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>129811</ref_obj_id>
				<ref_obj_pid>129809</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[H. Bal, M. Kaashoek, and A. Tanenbaum. Orca: A Language for Parallel Programming of Distributed Systems. IEEE Transactions on Software Engineering, 18(3): 190-205, March 1992.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>215711</ref_obj_id>
				<ref_obj_pid>215585</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[K. Bharat and L. Cardelli. Migratory Applications. In A CM UIST '95, pages 133-142, November 1995.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>163303</ref_obj_id>
				<ref_obj_pid>163298</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[K.P. Birman. The Process Group Approach to Reliable Distributed Computing. CACM, 36(12):36-53, Dec 1993.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>168637</ref_obj_id>
				<ref_obj_pid>168619</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[A. Birrell, G. Nelson, S. Owicki, and E. Wobber. Network Objects. In Proc. 14th ACM Symp. on Operating Systems Principles, 1993.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1867601</ref_obj_id>
				<ref_obj_pid>1867406</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[A Butz, Animation with CATHI, In Proceedings of AAAI/IAAI '97, pages 957-962, 1997.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[J. Calvin, A. Dickens, B. Gaines, P. Metzger, D. Miller, and D. Owen. The SIMNET Virtual World Architecture. In Proc. IEEE VRAIS '93, pages 450-455, Sept 1993.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[L. Cardelli. A Language with Distributed Scope. Computing Systems, 8(1):27-59, Jan 1995.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>759638</ref_obj_id>
				<ref_obj_pid>646790</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[L. Cardelli and A. Gordon. Mobile Ambients. In Foundations of Software Science and Computational Structures, Maurice Nivat (Ed.), LNCE 1378, Springer, 140-155. 1998.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>270060</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[R. Carey and G. Bell. The Annotated VRML 2.0 Reference Manual. Addison-Wesley, Reading, MA, 1997.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[C. Carlsson and O. Hagsand. DIVE--A Multi-User Virtual Reality System. In Proc. IEEE VRAIS '93, pages 394-400, Sept 1993.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[C.F. Codella, R. Jalili, L. Koved, and J. B. Lewis. A Toolkit for Developing Multi-User, Distributed Virtual Environments. In Proc. IEEE VRAIS '93, pages 401.407, Sept 1993.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192276</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[C. Elliott, G. Schechter, R. Yeung and S. Abi-Ezzi. TBAG: A High Level Framework for Interactive, Animated 3D Graphics Applications, In Proc. A CM SIGGRAPH 94, pages 421-434, August, 1994.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[M. Fairen and A. Vinacua, ATLAS, A Platform for Distributed Graphics Applications, In Proc. VI Eurographics Workshop on Programming Paradigms in Graphics, pages 91-102, September, 1997.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>168657</ref_obj_id>
				<ref_obj_pid>168642</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[S. Feiner, B. MacIntyre, M. Haupt, and E. Solomon. Windows on the World: 2D Windows for 3D Augmented Reality. In Proc. A CM UIST '93, pages 145-155, 1993.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>199418</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[T.A. Funkhouser. RING: A Client-Server System for Multi-User Virtual Environments. In Proc. 1995 ACM Symp. on Interactive 3D Graphics, pages 85-92, March 1995.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[G. Grimsdale. dVS--Distributed Virtual Environment System. In Proc. Computer Graphics '91 Conference, 1991.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>116210</ref_obj_id>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[S.P. Harbison. Modula-3. Prentice-Hall, 1992.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>217468</ref_obj_id>
				<ref_obj_pid>217382</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[H.W. Holbrook, S.K. Singhal and D.R. Cheriton, Log-Based Receiver-Reliable Multicast for Distributed Interactive Simulation, Proc. ACM SIGCOMM '95, pages 328-341, 1995.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>141665</ref_obj_id>
				<ref_obj_pid>141661</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[W. Levelt, M. Kaashoek, H. Bal, and A. Tanenbaum. A Comparison of Two Paradigms for Distributed Shared Memory. Software Practice and Experience, 22(11):985-1010, Nov 1992.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>949727</ref_obj_id>
				<ref_obj_pid>949685</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[B. Lucas. A Scientific Visualization Renderer. In Proc. IEEE Visualization '92, pp. 227-233, October 1992.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[V. Machiraju, A Framework for Migrating Objects in Distributed Graphics Applications, Masters Thesis, University of Utah, Department of Computer Science, Salt Lake City, UT, June, 1997.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[B. MacIntyre. Repo: Obliq with Replicated Objects. Programmers Guide and Reference Manual. Columbia University Computer Science Department Research Report CUCS-023-97, 1997. }]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237104</ref_obj_id>
				<ref_obj_pid>237091</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[B. MacIntyre, and S. Feiner. Language-level Support for Exploratory Programming of Distributed Virtual Environments. In Proc. ACM UIST '96, pages 83-94, Seattle, WA, November 6-8, 1996.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>614305</ref_obj_id>
				<ref_obj_pid>614258</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[M.A. Najork and M. H. Brown. Obliq-3D: A High-level, Fast-turnaround 3D Animation System. IEEE Transactions on Visualization and Computer Graphics, 1(2): 175-145, June 1995.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>217721</ref_obj_id>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[R. Ben-Natan. CORBA: A Guide to the Common Object Request Broker Architecture, McGraw Hill, 1995.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>77291</ref_obj_id>
				<ref_obj_pid>77276</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[D. Phillips, M. Pique, C. Moler, J. Torborg, D. Greenberg. Distributed Graphics: Where to Draw the Lines? Panel Transcript, SIGGRAPH 89, available at: http://www.siggraph.org:443/publications/panels/siggraphi89/]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192895</ref_obj_id>
				<ref_obj_pid>192844</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[A. Prakash and H. S. Shim. DistView: Support for Building Efficient Collaborative Applications Using Replicated Objects. In Proc. A CM CSCW '94, pages 153-162, October 1994.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192262</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[J. Rohlf and J. Helman, IRIS Performer: A High Performance Multiprocessing Toolkit for Real-Time {3D} Graphics, In Proc. ACM SIGGRAPH 94, pages 381-394, 1994.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>226162</ref_obj_id>
				<ref_obj_pid>226159</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[M. Roseman and S. Greenberg. Building Real-Time Groupware with GroupKit, a Groupware Toolkit. ACM Transactions on Computer- Human Interaction, 3(1):66-106, March 1996.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[C. Shaw and M. Green. The MR Toolkit Peers Package and Experiment. In Proc. IEEE VRAIS '93, pages 18-22, Sept 1993.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>835994</ref_obj_id>
				<ref_obj_pid>527216</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[G. Singh, L. Serra, W. Png, A. Wong, and H. Ng. BrickNet: Sharing Object Behaviors on the Net. In Proc. IEEE VRAIS '95, pages 19-25, 1995.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_obj_id>550653</ref_obj_id>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[H. Sowizral, K. Rushforth, and M. Deering. The Java 3D API Specification, Addison-Wesley, Reading, MA, 1998.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_obj_id>7887</ref_obj_id>
				<ref_obj_pid>7885</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[M. Stefik, G. Foster, D.G. Bobrow, K. Kahn, S. Lanning, and L. Suchman. Beyond The Chalkboard: Computer Support for Collaboration and Problem Solving in Meetings. CACM, 30(1):32- 47, January 1987.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134089</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[P.S. Strauss and R. Carey, An Object-Oriented 3D Graphics Toolkit, In Computer Graphics (Proc. ACM SIGGRAPH 92), pages 341-349, Aug, 1992.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Sun Microsystems, Inc. The Java Shared Data Toolkit, 1998. Unsupported software, available at: http://developer.javasoft.com/developer/earlyAccess/jsdt/]]></ref_text>
				<ref_id>36</ref_id>
			</ref>
			<ref>
				<ref_obj_id>187677</ref_obj_id>
				<ref_obj_pid>187660</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[I. Tou, S. Berson, G. Estrin, Y. Eterovic, and E. Wu. Prototyping Synchronous Group Applications. IEEE Computer, 27(5):48-56, May 1994.]]></ref_text>
				<ref_id>37</ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[R. Waters and D. Anderson. The Java Open Community Version 0.9 Application Program Interface. Feb, 1997. Available online at: http ://www.merl.com/opencom/opencom-j ava-api.html]]></ref_text>
				<ref_id>38</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1268066</ref_obj_id>
				<ref_obj_pid>1268049</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[A. Wollrath, R. Riggs, and J. Waldo. A Distributed Object Model for the Java System, In Proc. USENIX COOTS '96, pages 219-231, July 1996.]]></ref_text>
				<ref_id>39</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122730</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[R. Zeleznik, D. Conner, M. Wloka, D. Aliaga, N. Huang, P. Hubbard, B. Knep, H. Kaufman, J. Hughes, and A. van Dam. An Object-oriented Framework for the Integration of Interactive Animation Techniques. In Computer Graphics (SIGGRAPH '91 Proceedings), pages 105-112, July, 1991.]]></ref_text>
				<ref_id>40</ref_id>
			</ref>
			<ref>
				<ref_obj_id>147186</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[M.J. Zyda, D. R. Pratt, J. G. Monahan, and K. P. Wilson. NPSNET: Constructing a 3D Virtual World. In Proc. 1992 ACM Syrup. on Interactive 3D Graphics, pages 147-156, Mar. 1992.]]></ref_text>
				<ref_id>41</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Distributed 3D Graphics Library Blair MacIntyre and Steven Feiner1 Department of Computer Science 
Columbia University Supplemental materials for this paper are available in the papers/macintyr directory. 
Abstract We present Repo-3D, a general-purpose, object-oriented library for developing distributed, interactive 
3D graphics applications across a range of heterogeneous workstations. Repo-3D is designed to make it 
easy for programmers to rapidly build prototypes using a familiar multi-threaded, object-oriented programming 
paradigm. All data sharing of both graphical and non-graphical data is done via general-purpose remote 
and replicated objects, presenting the illusion of a single distributed shared memory. Graphical objects 
are directly distributed, circumventing the duplicate database problem and allowing programmers to focus 
on the application details. Repo-3D is embedded in Repo, an interpreted, lexically-scoped, distributed 
programming language, allowing entire applications to be rapidly prototyped. We discuss Repo-3D s design, 
and introduce the notion of local variations to the graphical objects, which allow local changes to be 
applied to shared graphical structures. Local variations are needed to support transient local changes, 
such as highlighting, and responsive local editing operations. Finally, we discuss how our approach could 
be applied using other program­ming languages, such as Java. CR Categories and Subject Descriptors: D.1.3 
[Program­ming Techniques]: Concurrent Programming Distributed Pro­gramming; H.4.1 [Information Systems 
Applications]: Office Automation Groupware; I.3.2 [Computer Graphics]: Graphics Systems Distributed/network 
graphics; I.3.6 [Computer Graph­ics]: Methodology and Techniques Graphics data structures and data types; 
I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism Virtual reality. Additional Keywords 
and Phrases: object-oriented graphics, distributed shared memory, distributed virtual environments, shared-data 
object model. 1 INTRODUCTION Traditionally, distributed graphics has referred to the architecture of 
a single graphical application whose components are distributed over multiple machines [14, 15, 19, 27] 
(Figure 1a). By taking advantage of the combined power of multiple machines, and the particular features 
of individual machines, otherwise impractical applications became feasible. However, as machines have 
grown more powerful and application domains such as Computer 1. {bm,feiner}@cs.columbia.edu, http://www.cs.columbia.edu/graphics 
Figure 1: Two meanings of distributed graphics: (a) a single logical graphics system with distributed 
components, and (b) multiple dis­tributed logical graphics systems. We use the second definition here. 
Supported Cooperative Work (CSCW) and Distributed Virtual Environments (DVEs) have been making the transition 
from research labs to commercial products, the term distributed graphics is increasingly used to refer 
to systems for distributing the shared graphical state of multi-display/multi-person, distributed, interac­tive 
applications (Figure 1b). This is the definition that we use here. While many excellent, high-level programming 
libraries are available for building stand-alone 3D applications (e.g. Inventor [35], Performer [29], 
Java 3D [33]), there are no similarly powerful and general libraries for building distributed 3D graphics 
applica­tions. All CSCW and DVE systems with which we are familiar (e.g., [1, 7, 11, 12, 16, 28, 30, 
31, 32, 34, 37, 41]) use the following approach: A mechanism is provided for distributing application 
state (either a custom solution or one based on a general-purpose distributed programming environment, 
such as ISIS [4] or Obliq [8]), and the state of the graphical display is maintained separately in the 
local graphics library. Keeping these dual databases syn­chronized is a complex, tedious, and error-prone 
endeavor. In con­trast, some non-distributed libraries, such as Inventor [35], allow programmers to avoid 
this problem by using the graphical scene description to encode application state. Extending this single 
data­base model to a distributed 3D graphics library is the goal of our work on Repo-3D. Repo-3D is an 
object-oriented, high-level graphics package, derived from Obliq-3D [25]. Its 3D graphics facilities 
are similar to those of other modern high-level graphics libraries. However, the objects used to create 
the graphical scenes are directly distribut­able from the programmer s viewpoint, the objects reside 
in one large distributed shared memory (DSM) instead of in a single process. The underlying system replicates 
any of the fine-grained objects across as many processes as needed, with no additional effort on the 
part of the programmer. Updates to objects are automatically reflected in all replicas, with any required 
objects automatically distributed as needed. By integrating the replicated objects into the programming 
languages we use, distributed applications may be built using Repo-3D with little more difficulty than 
building applications in a single process. No matter how simple the construction of a distributed applica­tion 
may be, a number of differences between distributed and monolithic applications must be addressed. These 
include: Distributed control. In a monolithic application, a single com­ponent can oversee the application 
and coordinate activities among the separate components by notifying them of changes to the application 
state. This is not possible in a non-trivial dis­tributed application. Therefore, we must provide mechanisms 
for different components to be notified of changes to the distributed state.  Interactivity. Updates 
to distributed state will be slower than updates to local state, and the amount of data that can be distributed 
is limited by network bandwidth. If we do not want to sacrifice interactive speed, we must be able to 
perform some operations locally. For example, an object could be dragged locally with the mouse, with 
only a subset of the changes applied to the replicated state.  Local variations. There are times when 
a shared graphical scene may need to be modified locally. For example, a programmer may want to highlight 
the object under one user s mouse pointer without affecting the scene graph viewed by other users.  
Repo-3D addresses these problems in two ways. First, a programmer can associate a notification object 
with any replicated object. The notification object s methods will be invoked when the replicated object 
is updated. This allows reactive programs to be built in a straightforward manner. To deal with the second 
and third problems, we introduce the notion of local variations to graphical objects. That is, we allow 
the properties of a graphical object to be modified locally, and parts of the scene graph to be locally 
added, removed, or replaced. In Section 2 we describe how we arrived at the solution presented here. 
Section 3 discusses related work, and Section 4 offers a detailed description of the underlying infrastructure 
that was used. The design of Repo-3D is presented in Section 5, followed by some examples and concluding 
remarks in Sections 6 and 7. 2 BACKGROUND Repo-3D was created as part of a project to support rapid 
prototyp­ing of distributed, interactive 3D graphical applications, with a particular focus on DVEs. 
Our fundamental belief is that by providing uniform high-level support for distributed programming in 
the languages and toolkits we use, prototyping and experiment­ing with distributed interactive applications 
can be (almost) as simple as multi-threaded programming in a single process. While care must be taken 
to deal with network delays and bandwidth limitations at some stage of the program design (the languages 
and toolkits ought to facilitate this), it should be possible to ignore such issues until they become 
a problem. Our view can be summarized by a quote attributed to Alan Kay, Simple things should be simple; 
complex things should be possible. This is especially true during the exploration and prototyping phase 
of application programming. If programmers are forced to expend significant effort building the data-distribution 
components of the application at an early stage, not only will less time be spent exploring different 
prototypes, but radical changes in direction will become difficult, and thus unlikely. For example, the 
implementa­tion effort could cause programs to get locked into using a commu­nication scheme that may 
eventually prove less than ideal, or even detrimental, to the program s final design. Since we are using 
object-oriented languages, we also believe that data distribution should be tightly integrated with the 
language s general-purpose objects. This lets the language s type system and programming constructs reduce 
or eliminate errors in the use of the data-distribution system. Language-level integration also allows 
the system to exhibit a high degree of network data transparency, or the ability for the programmer to 
use remote and local data in a uniform manner. Without pervasive, structured, high-level data-distribution 
support integrated into our program­ming languages and libraries, there are applications that will never 
be built or explored, either because there is too much programming overhead to justify trying simple 
things ( simple things are not simple ), or because the added complexity of using relatively primitive 
tools causes the application to become intractable ( com­plex things are not possible ). Of the tools 
available for integrating distributed objects into programming languages, client-server data sharing 
is by far the most common approach, as exemplified by CORBA [26], Modula-3 Network Objects [5], and Java 
RMI [39]. Unfortunately, interactive graphical applications, such as virtual reality, require that the 
data used to refresh the display be local to the process doing the rendering or acceptable frame refresh 
rates will not be achieved. Therefore, pure client-server approaches are inappropri­ate because at least 
some of the shared data must be replicated. Furthermore, since the time delay of synchronous remote method 
calls is unsuitable for rapidly changing graphical applications, shared data should be updated asynchronously. 
Finally, when data is replicated, local access must still be fast. The most widely used protocols for 
replicated data consistency, and thus many of the toolkits (e.g., ISIS [4] and Visual-Obliq [3]), allow 
data updates to proceed unimpeded, but block threads read­ing local data until necessary updates arrive. 
The same reason we need replicated data in the first place fast local read access to the data makes these 
protocols unsuitable for direct replication of the graphical data. Of course, these protocols are fine 
for replicating application state that will then be synchronized with a parallel graphical scene description, 
but that is what we are explicitly try­ing to avoid. Fortunately, there are replicated data systems (e.g., 
Orca [2] or COTERIE [24]) that provide replicated objects that are well suited to interactive applications, 
and it is upon the second of these systems that Repo-3D is built. 3 RELATED WORK There has been a significant 
amount of work that falls under the first, older definition of distributed graphics. A large number of 
systems, ranging from established commercial products (e.g., IBM Visualization Data Explorer [21]) to 
research systems (e.g., PARADISE [19] and ATLAS [14]), have been created to distribute interactive graphical 
applications over a set of machines. However, the goal of these systems is to facilitate sharing of application 
data between processes, with one process doing the rendering. While some of these systems can be used 
to display graphics on more than one display, they were not designed to support high-level sharing of 
graphical scenes. Most high-level graphics libraries, such as UGA [40], Inventor [35] and Java 3D [33], 
do not provide any support for distribution. Others, such as Performer [29], provide support for distributing 
components of the 3D graphics rendering system across multiple processors, but do not support distribution 
across multiple machines. One notable exception is TBAG [13], a high-level constraint-based, declarative 
3D graphics framework. Scenes in TBAG are defined using constrained relationships between time­varying 
functions. TBAG allows a set of processes to share a single, replicated constraint graph. When any process 
asserts or retracts a constraint, it is asserted or retracted in all processes. However, this means that 
all processes share the same scene, and that the system s scalability is limited because all processes 
have a copy of (and must evaluate) all constraints, whether or not they are interested in them. There 
is also no support for local variations of the scene in different processes. Machiraju [22] investigated 
an approach similar in flavor to ours, but it was not aimed at the same fine-grained level of interactivity 
and was ultimately limited by the constraints of the implementa­tion platform (CORBA and C++). For example, 
CORBA objects are heavyweight and do not support replication, so much of their effort was spent developing 
techniques to support object migration and fine-grained object sharing. However, their fine-grained objects 
are coarser than ours, and, more importantly, they do not support the kind of lightweight, transparent 
replication we desire. A programmer must explicitly choose whether to replicate, move, or copy an object 
between processes when the action is to occur (as opposed to at object creation time). Replicated objects 
are indepen­dent new copies that can be modified and used to replace the origi­nal simultaneous editing 
of objects, or real-time distribution of changes as they are made is not supported. Of greater significance 
is the growing interest for this sort of sys­tem in the Java and VRML communities. Java, like Modula-3, 
is much more suitable as an implementation language than C or C++ because of its cross-platform compatibility 
and support for threads and garbage collection: Without the latter two language features, implementing 
complex, large-scale distributed applications is extremely difficult. Most of the current effort has 
been focused on using Java as a mechanism to facilitate multi-user VRML worlds (e.g., Open Communities 
[38]). Unfortunately, these efforts concentrate on the particulars of implementing shared virtual environments 
and fall short of providing a general-purpose shared graphics library. For example, the Open Communities 
work is being done on top of SPLINE [1], which supports only a single top-level world in the local scene 
database. Most DVEs [11, 12, 16, 31, 32] provide support for creating shared virtual environments, not 
general purpose interactive 3D graphics applications. They implement a higher level of abstrac­tion, 
providing support for rooms, objects, avatars, collision detec­tion, and other things needed in single, 
shared, immersive virtual environments. These systems provide neither general-purpose programming facilities 
nor the ability to work with 3D scenes at a level provided by libraries such as Obliq-3D or Inventor. 
Some use communication schemes that prevent them from scaling beyond a relatively small number of distributed 
processes, but for most the focus is explicitly on efficient communication. SIMNET [7], and the later 
NPSNet [41], are perhaps the best known large-scale distributed virtual-environment systems. They use 
a fixed, well­defined communication protocol designed to support a single, large-scale, shared, military 
virtual environment. The techniques for object sharing implemented in recent CSCW toolkits [28, 30, 34, 
37] provide some of the features we need, particularly automatic replication of data to ease construction 
of distributed applications. However, none of these toolkits has integrated the distribution of data 
into its programming language s object model as tightly as we desire. As a result, they do not pro­vide 
a high enough level of network data transparency or suffi­ciently strong consistency guarantees. In groupware 
applications, inconsistencies tend to arise when multiple users attempt to per­form conflicting actions: 
the results are usually obvious to the users and can be corrected using social protocols. This is not 
an acceptable solution for a general-purpose, distributed 3D graphics toolkit. Furthermore, none of these 
CSCW systems provides any support for asynchronous update notification, or is designed to support the 
kind of large-scale distribution we have in mind. Finally, while distributed games, such as Quake, have 
become very popular, they only distribute the minimum amount of applica­tion state necessary. They do 
not use (or provide) an abstract, high­level distributed 3D graphics system. Network Figure 2: The architecture 
of Repo-3D. Aside from native graphics libraries (X, Win32, OpenGL, Renderware) the Modula-3 runtime 
shields most of the application from the OS. The Replicated Object package uses an Event communication 
package and the Network Object package. DistAnim-3D is implemented on top of a variety of native graphics 
libraries and Replicated Objects. Repo exposes most of the useful Modula-3 packages, as well as using 
Network Objects and Replicated Objects to present a distributed shared memory model to the programmer. 
 4 UNDERLYING INFRASTRUCTURE Our work was done in the Modula-3 programming language [18]. We decided 
to use Modula-3 because of the language itself and the availability of a set of packages that provide 
a solid foundation for our infrastructure. Modula-3 is a descendant of Pascal that corrects many of its 
deficiencies, and heavily influenced the design of Java. In particular, Modula-3 retains strong type 
safety, while adding facilities for exception handling, concurrency, object-oriented programming, and 
automatic garbage collection2. One of its most important features for our work is that it gives us uniform 
access to these facilities across all architectures. Repo-3D relies on a number of Modula-3 libraries, 
as illustrated in Figure 2. Distributed data sharing is provided by two packages, the Network Object 
client-server object package [5], and the Replicated Object shared object package [24] (see Section 4.1). 
DistAnim-3D is derived from Anim-3D [25], a powerful, non­distributed, general-purpose 3D library originally 
designed for 3D algorithm animation (see Section 4.2). Finally, Repo itself is a direct descendant of 
Obliq [8], and uses the Replicated Object package to add replicated data to Obliq (see Section 4.3). 
4.1 Distributed Shared Memory Repo-3D s data sharing mechanism is based on the Shared Data-Object Model 
of Distributed Shared Memory (DSM) [20]. DSM allows a network of computers to be programmed much like 
a mul­tiprocessor, since the programmer is presented with the familiar paradigm of a common shared memory. 
The Shared Data-Object Model of DSM is particularly well suited to our needs since it is a high-level 
approach that can be implemented efficiently at the application level. In this model, shared data is 
encapsulated in user-defined objects and can only be accessed through those objects method calls. The 
DSM address space is partitioned implicitly by the application programmer, with an object being the smallest 
unit of sharing. All shared data is fully network transpar­ 2. The Modula-3 compiler we used is available 
from Critical Mass, Inc. as part of the Reactor programming environment. The compiler, and thus our system, 
runs on all the operating systems we have available (plus others): Solaris, IRIX, HP-UX, Linux, and Windows 
NT and 95. ent because it is encapsulated within the programming language objects. Distribution of new 
objects between the processes is as simple as passing them back and forth as parameters to, or return 
values from, method calls the underlying systems take care of the rest.3 Objects are only distributed 
to new processes as necessary, and (in our system) are removed by the garbage collector when they are 
no longer referenced. Furthermore, distributed garbage collection is supported, so objects that are no 
longer referenced in any process are removed completely. There are three kinds of distributed object 
semantics in our DSM: Simple objects correspond to normal data objects, and have no special distributed 
semantics. When a simple object is copied between processes, a new copy is created in the destination 
process that has no implied relationship to the object in the source process.  Remote objects have client-server 
distribution semantics. When a remote object is copied between processes, all processes except the one 
in which the object was created end up with a proxy object that forwards method invocations across the 
network to the original object.  Replicated objects have replicated distribution semantics. When a replicated 
object is passed between processes, a new replica is created in the destination process. If any replica 
is changed, the change is reflected in all replicas.  The Network Object package provides support for 
remote objects. It implements distributed garbage collection, exception propagation back to the calling 
site, and automatic marshalling and unmarshalling of method arguments and return values of virtually 
any data type between heterogeneous machine architectures. The package is similar to other remote method 
invocation (RMI) pack­ages developed later, such as the Java RMI library [39]. All method invocations 
are forwarded to the original object, where they are executed in the order they are received. The Replicated 
Object package supports replicated objects. Each process can call any method of an object it shares, 
just as it can with a simple or remote object. We will describe the Replicated Object package in more 
detail, as Repo-3D relies heavily on its design, and the design of a replicated object system is less 
straight­forward than a remote one. The model supported by the Replicated Object package follows two 
principles: All operations on an instance of an object are atomic and serializable. All operations are 
performed in the same order on all copies of the object. If two methods are invoked simulta­neously, 
the order of invocation is nondeterministic, just as if two threads attempted to access the same memory 
location simultaneously in a single process.  The above principle applies to operations on single objects. 
Making sequences of operations atomic is up to the program­mer.  The implementation of the Replicated 
Object package is based on the approach used in the Orca distributed programming language [2]. A full 
replication scheme is used, where a single object is either fully replicated in a process or not present 
at all. Avoiding partial replication significantly simplifies the implemen­tation and the object model, 
and satisfies the primary rationale for replication: fast read-access to shared data. To maintain replication 
consistency an update scheme is used, where updates to the object are applied to all copies. 3. An important 
detail is how the communication is bootstrapped. In the case of the Network and Replicated Object packages, 
to pass a first object between processes, one of them exports the object to a special network object 
demon under some known name on some known machine. The second process then retrieves the object. The 
method of deciding what is and is not an update is what makes the Orca approach particularly interesting 
and easy to implement. All methods are marked as either read or update meth­ods by the programmer who 
creates the object type. Read methods are assumed to not change the state of the object and are therefore 
applied immediately to the local object without violating consis­tency. Update methods are assumed to 
change the state. To distrib­ute updates, arguments to the update method are marshalled into a message 
and sent to all replicas. To ensure all updates are applied in the same order, the current implementation 
of the Replicated Object package designates a sequencer process for each object. There may be more than 
one sequencer in the system to avoid overloading one process with all the objects (in this case, each 
object has its updates managed by exactly one of the sequencers.) The sequencer is responsible for assigning 
a sequence number to each message before it is sent to all object replicas. The replicas then execute 
the incoming update messages in sequence. The pro­cess that initiated the update does not execute the 
update until it receives a message back from the sequencer and all updates with earlier sequence numbers 
have been executed. There are three very important reasons for choosing this approach. First, it is easy 
to implement on top of virtually any object-oriented language, using automatically generated object subtypes 
and method wrappers that communicate with a simple runtime system. We do this in our Modula-3 implementation, 
and it would be equally applicable to an implementation in C++ or Java. For example, the JSDT [36] data-sharing 
package in Java uses a similar approach. Second, the Replicated Object package does not pay attention 
to (or even care) when the internal data fields of an object change. This allows the programmer great 
flexibility in deciding exactly what constitutes an update or not, and what constitutes the shared state4. 
For example, objects could have a combination of global and local state, and the methods that change 
the local state could be classified as read methods since they do not modify the global state. Alternatively, 
read methods could do some work locally and then call an update method to propagate the results, allowing 
time­consuming computation to be done once and the result distributed in a clean way. We took advantage 
of both of these techniques in implementing Repo-3D. Finally, the immediate distribution of update methods 
ensures that changes are distributed in a timely fashion, and suggests a straightforward solution to 
the asynchronous notification problem. The Replicated Object package generates a Notification Object 
type for each Replicated Object type. These new objects have methods corresponding to the update methods 
of their associated Replicated Object. The arguments to these methods are the same as the corresponding 
Replicated Object methods, plus an extra argument to hold the Replicated Object instance. These notifiers 
can be used by a programmer to receive notification of changes to a Replicated Object in a structured 
fashion. To react to updates to a Replicated Object instance, a programmer simply overrides the methods 
of the corresponding Notification Object with methods that react appropriately to those updates, and 
associates an instance 4. Of course, it falls squarely on the shoulders of the programmer to ensure that 
the methods provided always leave the object in a consistent state. This is not significantly different 
than what needs to be done when building a complex object that is simultaneously accessed by multiple 
threads in a non-distributed system. For example, if a programmer reads an array of numbers from inside 
the object and then uses an update method to write a computed average back into the object, the internal 
array may have changed before the average is written, resulting in a classic inconsistency problem. In 
general, methods that perform computations based on internal state (rather than on the method arguments) 
are potentially problematic and need to be considered carefully. RootGO GroupGO   ChoiceGroupGO OrthoCameraGO 
CameraGO PerspCameraGO AmbientLightGO VectorLightGO LightGO PointLightGO SpotLightGO GO LineGO  IndexedLineSetGO 
NonSurfaceGO MarkerGO TextGO Text2DGO PolygonGO BoxGO SphereGO CylinderGO SurfaceGO DiskGO TorusGO 
QuadMeshGO IndexedPolygonSetGO Figure 3: The Repo-3D GO class hierarchy. Most of the classes are also 
in Obliq-3D; the italicized ones were added to Repo-3D. Property Request Request Name . . . Figure 4: 
The relationship between properties, names, values, and behaviors. Each oval represents an object and 
arrows show contain­ment. of it with the Replicated Object instance. Each time an update method of the 
Replicated Object is invoked, the corresponding method of the Notifier Object is also invoked. Notification 
Objects eliminate the need for object polling and enable a data-driven flow of control. 4.2 Obliq-3D 
Obliq-3D is composed of Anim-3D, a 3D animation package written in Modula-3, and a set of wrappers that 
expose Anim-3D to the Obliq programming language (see Section 4.3). Anim-3D is based on three simple 
and powerful concepts: graphical objects for building graphical scenes, properties for specifying the 
behavior of the graphical objects, and input event callbacks to support interac­tive behavior. Anim-3D 
uses the damage-repair model: whenever a graphical object or property changes (is damaged), the image 
is repaired without programmer intervention. Graphical objects (GOs) represent all the logical entities 
in the graphical scene: geometry (e.g., lines, polygons, spheres, polygon sets, and text), lights and 
cameras of various sorts, and groups of other GOs. One special type of group, the RootGO, represents 
a window into which graphics are rendered. GOs can be grouped together in any valid directed acyclic 
graph (DAG). The GO class hierarchy is shown in Figure 3. A property is a defined by a name and a value. 
The name deter­mines which attribute is affected by the property, such as Texture Mode or Box Corner1 
. The value specifies how it is affected and is determined by its behavior, a time-variant function that 
takes the current animation time and returns a value. Properties, property values, and behaviors are 
all objects, and their relation­ships are shown in Figure 4. When a property is created, its name and 
value are fixed. However, values are mutable and their behav­ior may be changed at any time. There are 
four kinds of behaviors for each type of properties: constant (do not vary over time), synchronous (follow 
a programmed set of requests, such as move from A to B starting at time t=1 and taking 2 seconds ), asynchro­nous 
(execute an arbitrary time-dependent function to compute the value) and dependent (asynchronous properties 
that depend on other properties). Synchronous properties are linked to animation handles and do not start 
satisfying their requests until the anima­tion handle is signalled. By linking multiple properties to 
the same handle, a set of property value changes can be synchronized.    Associated with each GO g 
is a partial mapping of property names to values determined by the properties that have been asso­ciated 
with g. A property associated with g affects not only g but all the descendants of g that do not override 
the property. A single property may be associated with any number of GOs. It is perfectly legal to associate 
a property with a GO that is not affected by it; for example, attaching a Surface Color property to a 
GroupGO does not affect the group node itself, but could potentially affect the surface color of any 
GO contained in that group. A RootGO sets an initial default value for each named property. There are 
three types of input event callbacks in Anim-3D, corre­sponding to the three kinds of interactive events 
they handle: mouse callbacks (triggered by mouse button events), motion call­backs (triggered by mouse 
motion events) and keyboard callbacks (triggered by key press events). Each object has three callback 
stacks, and the interactive behavior of an object can be redefined by pushing a new callback onto the 
appropriate stack. Any event that occurs within a root window associated with a RootGO r will be delivered 
to the top handler on r s callback stack. The handler could delegate the event to one of r s children, 
or it may handle it itself, perhaps changing the graphical scene in some way. DistAnim-3D is a direct 
descendant of Anim-3D. In addition to the objects being distributed, it has many additional facilities 
that are needed for general-purpose 3D graphical applications, such as texture mapping, indexed line 
and polygon sets, choice groups, projection and transformation callbacks, and picking. Since DistAnim-3D 
is embedded in Repo instead of Obliq (see Section 4.3), the resulting library is called Repo-3D. 4.3 
Obliq and Repo Obliq [8] is a lexically-scoped, untyped, interpreted language for distributed object-oriented 
computation. It is implemented in, and tightly integrated with, Modula-3. An Obliq computation may involve 
multiple threads of control within an address space, multi­ple address spaces on a machine, heterogeneous 
machines over a local network, and multiple networks over the Internet. Obliq uses, and supports, the 
Modula-3 thread, exception, and garbage-collec­tion facilities. Its distributed-computation mechanism 
is based on Network Objects, allowing transparent support for multiple processes on heterogeneous machines. 
Objects are local to a site, while computations can roam over the network. Repo [23] is a descendant 
of Obliq that extends the Obliq object model to include replicated objects. Therefore, Repo objects have 
state that may be local to a site (as in Obliq) or replicated across multiple sites.  5 DESIGN OF REPO-3D 
Repo-3D s design has two logical parts: the basic design and local variations. The basic design encompasses 
the changes to Obliq-3D to carry it into a distributed context, and additional enhancements that are 
not particular to distributed graphics (and are therefore not discussed here). Local variations are introduced 
to handle two issues mentioned in Section 1: transient local changes and respon­ sive local editing. 
 5.1 Basic Repo-3D Design The Anim-3D scene-graph model is well suited for adaptation to a distributed 
environment. First, in Anim-3D, properties are attached to nodes, not inserted into the graph, and the 
property and child lists are unordered (i.e., the order in which properties are assigned to a node, or 
children are added to a group, does not affect the final result). In libraries that insert properties 
and nodes in the graph and execute the graph in a well-defined order (such as Inventor), the siblings 
of a node (or subtree) can affect the attributes of that node (or subtree). In Anim-3D, and similar libraries 
(such as Java 3D), properties are only inherited down the graph, so a node s properties are a function 
of the node itself and its ancestors its siblings do not affect it. Therefore, subtrees can be added 
to different scene graphs, perhaps in different processes, with predictable results. Second, the interface 
(both compiled Anim-3D and interpreted Obliq-3D) is programmatical and declarative. There is no graphi­cal 
scene file format per se: graphical scenes are created as the side effect of executing programs that 
explicitly create objects and manipulate them via the object methods. Thus, all graphical objects are 
stored as the Repo-3D programs that are executed to create them. This is significant, because by using 
the Replicated Object library described in Section 4.1 to make the graphical objects distributed, the 
file format (i.e., a Repo-3D program) is updated for free. Converting Anim-3D objects to Replicated Objects 
involved three choices: what objects to replicate, what methods update the object state, and what the 
global, replicated state of each object is. Since replicated objects have more overhead (e.g., method 
execu­tion time, memory usage, and latency when passed between processes), not every category of object 
in Repo-3D is replicated. We will consider each of the object categories described in Figure 4.2 in turn: 
graphical objects (GOs), properties (values, names, behaviors, animation handles) and callbacks. For 
each of these objects, the obvious methods are designated as update meth­ods, and, as discussed in Section 
4.1, the global state of each object is implicitly determined by those update methods. Therefore, we 
will not go into excessive detail about either the methods or the state. Finally, Repo-3D s support for 
change notification will be discussed. 5.1.1 Graphical Objects GOs are the most straightforward. There 
are currently twenty-one different types of GOs, and all but the RootGOs are replicated. Since RootGOs 
are associated with an onscreen window, they are not replicated window creation remains an active decision 
of the local process. Furthermore, if replicated windows are needed, the general-purpose programming 
facilities of Repo can be used to support this in a relatively straightforward manner, outside the scope 
of Repo-3D. A GO s state is comprised of the properties attached to the object, its name, and some other 
non-inherited property attributes.5 The methods that modify the property list are update methods. Group 
GOs also contain a set of child nodes, and have update methods that modify that set. 5.1.2 Properties 
Properties are more complex. There are far more properties in a graphical scene than there are graphical 
objects, they change much more rapidly, and each property is constructed from a set of Modula-3 objects. 
There are currently 101 different properties of 5. Some attributes of a GO, such as the arrays of Point3D 
properties that define the vertices of a polygon set, are not attached to the object, but are manipulated 
through method calls. seventeen different types in Repo-3D, and any of them can be attached to any GO. 
A typical GO would have anywhere from two or three (e.g., a BoxGO would have at least two properties 
to define its corners) to a dozen or more. And, each of these proper­ties could be complex: in the example 
in Section 6, a single synchronous property for a long animation could have hundreds of requests enqueued 
within it. Consider again the object structure illustrated in Figure 4. A property is defined by a name 
and a value, with the value being a container for a behavior. Only one of the Modula-3 objects is replicated, 
the property value. Property values serve as the repli­cated containers for property behaviors. To change 
a property, a new behavior is assigned to its value. The state of the value is the current behavior. 
Animation handles are also replicated. They tie groups of related synchronous properties together, and 
are the basis for the interac­tion in the example in Section 6. In Anim-3D, handles have one animate 
method, which starts an animation and blocks until it finishes. Since update methods are executed everywhere, 
and block access to the object while they are being executed, they should not take an extended period 
of time. In creating Repo-3D, the animate method was changed to call two new methods: an update method 
that starts the animation, and a non-update method that waits for the animation to finish. We also added 
methods to pause and resume an animation, to retrieve and change the current rela­tive time of an animation 
handle, and to stop an animation early. The state of an Animation handle is a boolean value that says 
if it is active or not, plus the start, end, and current time (if the handle is paused). Most of the 
Modula-3 objects that comprise a property are not replicated, for a variety of reasons: Properties represent 
a permanent binding between a property value and a name. Since they are immutable, they have no syn­chronization 
requirements and can simply be copied between processes.  Names represent simple constant identifiers, 
and are therefore not replicated either.  Behaviors and requests are not replicated. While they can 
be modified after being created, they are treated as immutable data types for two reasons. First, the 
vast majority of behaviors, even complex synchronous ones, are not changed once they have been created 
and initialized. Thus, there is some justifica­tion for classifying the method calls that modify them 
as part of their initialization process. The second reason is practical and much more significant. Once 
a scene has been created and is being used by the application, the bulk of the time-critical changes 
to it tend to be assignments of new behaviors to the existing property values. For example, an object 
is moved by assigning a new (often constant) behavior to its GO_Transformproperty value. Therefore, the 
overall perfor­mance of the system depends heavily on the performance of property value behavior changes. 
By treating behaviors as immutable objects, they can simply be copied between processes without incurring 
the overhead of the replicated object system.   5.1.3 Input Callbacks In Repo-3D, input event callbacks 
are not replicated. As discussed in Section 4.2, input events are delivered to the callback stacks of 
a RootGO. Callbacks attached to any other object receive input events only if they are delivered to that 
object by the programmer, perhaps recursively from another input event callback (such as the one attached 
to the RootGO). Therefore, the interactive behavior of a root window is defined not only by the callbacks 
attached to its RootGO, but also by the set of callbacks associated with the graph rooted at that RootGO. 
Since the RootGOs are not replicated, the  (a) (b) (c) (d) Figure 5: Simultaneous images from a session 
with the distributed CATHI animation viewer, running on four machines, showing an anima­tion of an engine. 
(a) Plain animation viewer, running on Windows NT. (b) Overview window, running on Windows 95. (c) Animation 
viewer with local animation meter, running on IRIX. (d) Animation viewer with local transparency to expose 
hidden parts, running on Solaris. callbacks that they delegate event handling to are not replicated either. 
If a programmer wants to associate callbacks with objects as they travel between processes, Repo s general-purpose 
program­ming facilities can be used to accomplish this in a straightforward manner.  5.1.4 Change Notification 
The final component of the basic design is support for notification of changes to distributed objects. 
For example, when an object s position changes or a new child is added to a group, some of the processes 
containing replicas may wish to react in some way. For­tunately, as discussed in Section 4.1, the Replicated 
Object package automatically generates Notification Object types for all replicated object types, which 
provide exactly the required behavior. The Notification Objects for property values allow a programmer 
to be notified of changes to the behavior of a property, and the Notification Objects for the various 
GOs likewise allow notification of updates to them.  5.2 Local Variations Repo-3D s local variations 
solve a set of problems particular to the distributed context in which Repo-3D lives: maintaining interactiv­ity 
and supporting local modifications to the shared scene graph. If the graphical objects and their properties 
were always strictly replicated, programmers would have to create local variations by copying the objects 
to be modified, creating a set of Notification Objects on the original objects, the copies of those objects, 
and all their properties (to be notified when either change), and reflecting the appropriate changes 
between the instances. Unfortunately, while this process could be automated somewhat, it would still 
be extremely tedious and error prone. More seriously, the overhead of creating this vast array of objects 
and links between them would make this approach impractical for short transient changes, such as highlighting 
an object under the mouse. To overcome this problem, Repo-3D allows the two major elements of the shared 
state of the graphical object scene the properties attached to a GO and the children of a group to have 
local variations applied to them. (Local variations on property values or animation handles are not supported, 
although we are considering adding support for the latter.) Conceptually, local state is the state added 
to each object (the additions, deletions, and replacements to the properties or children) that is only 
accessible to the local copies and is not passed to remote processes when the object is copied to create 
a new replica. The existence of local state is possible because, as discussed in Section 4.1, the shared 
state of a replicated object is implicitly defined by the methods that update it6. Therefore, the new 
methods that manipulate the local variations are added to the GOs as non-update methods. Repo-3D combines 
both the global and local state when creating the graphical scene using the under­lying graphics package. 
As mentioned above, local variations come in two flavors: Property variations. There are three methods 
to set, unset, and get the global property list attached to a GO. We added the following methods to manipulate 
local variations: add or remove local properties (overriding the value normally used for the object), 
hide or reveal properties (causing the property value of the parent node to be inherited), and flush 
the set of local variations (removing them in one step) or atomically apply them to the global state 
of the object.  Child variations. There are five methods to add, remove, replace, retrieve, and flush 
the set of children contained in a group node. We added the following ones: add a local node, remove 
a global node locally, replace a global node with some other node locally, remove each of these local 
variations, flush the local variations (remove them all in one step), and atomi­cally apply the local 
variations to the global state.  This set of local operations supports the problems local variations 
were designed to solve, although some possible enhancements are discussed in Section 7.  6 EXAMPLE: 
AN ANIMATION EXAMINER As an example of the ease of prototyping distributed applications with Repo-3D, 
we created a distributed animation examiner for the CATHI [6] animation generation system. CATHI generates 
short informational animation clips to explain the operation of technical devices. It generates full-featured 
animation scripts, including camera and object motion, color and opacity effects, and lighting setup. 
It was reasonably straightforward to modify CATHI to generate Repo-3D program files, in addition to the 
GeomView and Render-Man script files it already generated. The resulting output is a Repo-3D program 
that creates two scene DAGs: a camera graph and a scene graph. The objects in these DAGs have synchronous 
behaviors specified for their surface and transformation properties. An entire animation is enqueued 
in the requests of these behaviors, lasting anywhere from a few seconds to a few minutes. We built a 
distributed, multi-user examiner over the course of a weekend. The examiner allows multiple users to 
view the same animation while discussing it (e.g., via electronic chat or on the phone). Figure 5 shows 
images of the examiner running on four 6. The local state is not copied when a replicated object is first 
passed to a new process because the Repo-3D objects have custom serialization routines (or Picklers, 
in Modula-3 parlance). These routines only pass the global state, and initialize the local state on the 
receiving side to reasonable default values corresponding to the empty local state. machines, each with 
a different view of the scene. The first step was to build a simple loader that reads the animation file, 
creates a window, adds the animation scene and camera to it, and exports the animation on the network, 
requiring less than a dozen lines of Repo-3D code. A network version, that imports the animation from 
the network instead of reading it from disk, replaced the lines of code to read and export the animation 
with a single line to import it. Figure 5(a) shows an animation being viewed by one of these clients. 
The examiner program is loaded by both these simple clients, and is about 450 lines long. The examiner 
supports: Pausing and continuing the animation, and changing the current animation time using the mouse. 
Since this is done by operating on the shared animation handle, changes performed by any viewer are seen 
by all. Because of the consistency guar­antees, all users can freely attempt to change the time, and 
the system will maintain all views consistently.  A second overview window (Figure 5(b)), where a new 
camera watches the animation scene and camera from a distant view. A local graphical child (representing 
a portion of the animation camera s frustum) was added to the shared anima­tion camera group to let the 
attributes of the animation camera be seen in the overview window.  A local animation meter (bottom 
of Figure 5(c)), that can be added to any window by pressing a key, and which shows the current time 
offset into the animation both graphically and numerically. It was added in front of the camera in the 
anima­tion viewer window, as a local child of a GO in the camera graph, so that it would be fixed to 
the screen in the animation viewer.  Local editing (Figure 5(d)), so that users can select objects and 
make them transparent (to better see what was happening in the animation) or hide them completely (useful 
on slow machines, to speed up rendering). Assorted local feedback (highlighting the object under the 
mouse and flashing the selected object) was done with local property changes to the shared GOs in the 
scene graph.  Given the attention paid to the design of Repo-3D, it was not necessary to be overly concerned 
with the distributed behavior of the application (we spent no more than an hour or so). Most of that 
time was spent deciding if a given operation should be global or a local variation. The bulk of programming 
and debugging time was spent implementing application code. For example, in the overview window, the 
representation of the camera moves dynamically, based on the bounding values of the animation s scene 
and camera graphs. In editing mode, the property that flashes the selected node bases its local color 
on the current global color (allowing a user who is editing while an animation is in progress to see 
any color changes to the selected node.) 7 CONCLUSIONS AND FUTURE WORK We have presented the rationale 
for, and design of, Repo-3D, a general-purpose, object-oriented library for developing distributed, interactive 
3D graphics applications across a range of heteroge­neous workstations. By presenting the programmer 
with the illusion of a large shared memory, using the Shared Data-Object model of DSM, Repo-3D makes 
it easy for programmers to rapidly prototype distributed 3D graphics applications using a familiar object-oriented 
programming paradigm. Both graphical and general-purpose, non-graphical data can be shared, since Repo-3D 
is embedded in Repo, a general-purpose, lexically-scoped, distrib­uted programming language. Repo-3D 
is designed to directly support the distribution of graph­ical objects, circumventing the duplicate database 
problem and allowing programmers to concentrate on the application function­ality of a system, rather 
than its communication or synchronization components. We have introduced a number of issues that must 
be considered when building a distributed 3D graphics library, espe­cially concerning efficient and clean 
support for data distribution and local variations of shared graphical scenes, and discussed how Repo-3D 
addresses them. There are a number of ways in which Repo-3D could be improved. The most important is 
the way the library deals with time. By default, the library assumes all machines are running a time-synchronization 
protocol, such as NTP, and uses an internal animation time offset7 (instead of the system-specific time 
offset) because different OSs (e.g., NT vs. UNIX) start counting time at different dates. Hooks have 
been provided to allow a programmer to specify their own function to compute the current animation time 
offset within a process. Using this facility, it is possible to build inter-process time synchronization 
protocols (which we do), but this approach is not entirely satisfactory given our stated goal of relieving 
the programmer of such tedious chores. Future systems should integrate more advanced solutions, such 
as adjust­ing time values as they travel between machines, so that users of computers with unsynchronized 
clocks can collaborate8. This will become more important as mobile computers increase in popular­ity, 
as it may not be practical to keep their clocks synchronized. The specification of local variations in 
Repo-3D could benefit from adopting the notion of paths (as used in Java 3D and Inventor, for example). 
A path is an array of objects leading from the root of the graph to an object; when an object occurs 
in multiple places in one or more scene graphs, paths allow these instances to be differ­entiated. By 
specifying local variations using paths, nodes in the shared scene graphs could have variations within 
a process as well as between processes. One other limitation of Repo-3D, arising from our use of the 
Replicated Object package, is that there is no way to be notified when local variations are applied to 
an object. Recall that the methods of an automatically generated Notification Object correspond to the 
update methods of the corresponding Replicated Object. Since the methods that manipulate the local variations 
are non-update methods (i.e., they do not modify the replicated state), there are no corresponding methods 
for them in the Notification Objects. Of course, it would be relatively straight­forward to modify the 
Replicated Object package to support this, but we have not yet found a need for these notifiers. A more 
advanced replicated object system would also improve the library. Most importantly, support for different 
consistency semantics would be extremely useful. If we could specify semantics such as all updates completely 
define the state of an object, and only the last update is of interest, the efficiency of the distribution 
of property values would improve significantly; in this case, updates could be applied (or discarded) 
when they arrive, without waiting for all previous updates to be applied, and could be applied locally 
without waiting for the round trip to the sequencer. There are also times when it would be useful to 
have support for consistency across multiple objects, either using causal ordering (as provided by systems 
such as ISIS and Visual-Obliq), or some kind of transaction protocol to allow large groups of changes 
to be applied either as a unit, or not at all. It is not clear how one would provide these features with 
a replicated object system such as the one used here. While a library such as Repo-3D could be built 
using a variety of underlying platforms, the most likely one for future work is Java. Java shares many 
of the advantages of Modula-3 (e.g., threads and garbage collection are common across all architectures) 
and the 7. Computed as an offset from January 1, 1997. 8. Implementation details of the combination 
of Network and Replicated Objects made it difficult for us to adopt a more advanced solution.  packages 
needed to create a Repo-3D-like toolkit are beginning to appear. While Java does not yet have a replicated 
object system as powerful as the Replicated Object package, a package such as JSDT [36] (which focuses 
more on data communication than high­level object semantics) may be a good starting point. Work is also 
being done on interpreted, distributed programming languages on top of Java (e.g., Ambit [9]). Finally, 
Java 3D is very similar to Anim-3D, even though its design leans toward efficiency instead of generality 
when there are trade-offs to be made. For example, the designers chose to forgo Anim-3D s general property 
inheritance mechanism because it imposes computational overhead. By com­bining packages such as Java 
3D, JSDT, and Ambit, it should be possible to build a distributed graphics library such as Repo-3D in 
Java.  Acknowledgments We would like to thank the reviewers for their helpful comments, as well as 
the many other people who have contributed to this project. Andreas Butz ported CATHI to use Repo-3D 
and helped with the examples and the video. Clifford Beshers participated in many lively discussions 
about the gamut of issues dealing with language-level support for 3D graphics. Tobias Höllerer and Steven 
Dossick took part in many other lively discussions. Xinshi Sha implemented many of the extensions to 
Obliq-3D that went into Repo-3D. Luca Cardelli and Marc Najork of DEC SRC created Obliq and Obliq-3D, 
and provided ongoing help and encouragement over the years that Repo and Repo-3D have been evolving. 
This research was funded in part by the Office of Naval Research under Contract N00014-97-1-0838 and 
the National Tele-Immer­sion Initiative, and by gifts of software from Critical Mass and Microsoft. 
References [1] D. B. Anderson, J. W. Barrus, J. H. Howard, C. Rich, C. Shen, and R. C. Waters. Building 
Multi-User Interactive Multimedia Environ­ments at MERL. Technical Report Research Report TR95-17, Mit­subishi 
Electric Research Laboratory, November 1995. [2] H. Bal, M. Kaashoek, and A. Tanenbaum. Orca: A Language 
for Parallel Programming of Distributed Systems. IEEE Transactions on Software Engineering, 18(3):190 
205, March 1992. [3] K. Bharat and L. Cardelli. Migratory Applications. In ACM UIST '95, pages 133-142, 
November 1995. [4] K. P. Birman. The Process Group Approach to Reliable Distributed Computing. CACM, 
36(12):36 53, Dec 1993. [5] A. Birrell, G. Nelson, S. Owicki, and E. Wobber. Network Objects. In Proc. 
14th ACM Symp. on Operating Systems Principles, 1993. [6] A Butz, Animation with CATHI, In Proceedings 
of AAAI/IAAI '97, pages 957 962, 1997. [7] J. Calvin, A. Dickens, B. Gaines, P. Metzger, D. Miller, and 
D. Owen. The SIMNET Virtual World Architecture. In Proc. IEEE VRAIS 93, pages 450 455, Sept 1993. [8] 
L. Cardelli. A Language with Distributed Scope. Computing Sys­tems, 8(1):27 59, Jan 1995. [9] L. Cardelli 
and A. Gordon. Mobile Ambients. In Foundations of Software Science and Computational Structures, Maurice 
Nivat (Ed.), LNCE 1378, Springer, 140 155. 1998. [10] R. Carey and G. Bell. The Annotated VRML 2.0 Reference 
Manual. Addison-Wesley, Reading, MA, 1997. [11] C. Carlsson and O. Hagsand. DIVE A Multi-User Virtual 
Reality System. In Proc. IEEE VRAIS 93, pages 394 400, Sept 1993. [12] C. F. Codella, R. Jalili, L. Koved, 
and J. B. Lewis. A Toolkit for Developing Multi-User, Distributed Virtual Environments. In Proc. IEEE 
VRAIS 93, pages 401 407, Sept 1993. [13] C. Elliott, G. Schechter, R. Yeung and S. Abi-Ezzi. TBAG: A 
High Level Framework for Interactive, Animated 3D Graphics Applications, In Proc. ACM SIGGRAPH 94, pages 
421 434, August, 1994. [14] M. Fairen and A. Vinacua, ATLAS, A Platform for Distributed Graphics Applications, 
In Proc. VI Eurographics Workshop on Pro­gramming Paradigms in Graphics, pages 91 102, September, 1997. 
[15] S. Feiner, B. MacIntyre, M. Haupt, and E. Solomon. Windows on the World: 2D Windows for 3D Augmented 
Reality. In Proc. ACM UIST 93, pages 145 155, 1993. [16] T. A. Funkhouser. RING: A Client-Server System 
for Multi-User Virtual Environments. In Proc. 1995 ACM Symp. on Interactive 3D Graphics, pages 85 92, 
March 1995. [17] G. Grimsdale. dVS Distributed Virtual Environment System. In Proc. Computer Graphics 
91 Conference, 1991. [18] S. P. Harbison. Modula-3. Prentice-Hall, 1992. [19] H.W. Holbrook, S.K. Singhal 
and D.R. Cheriton, Log-Based Receiver-Reliable Multicast for Distributed Interactive Simulation, Proc. 
ACM SIGCOMM 95, pages 328 341, 1995. [20] W. Levelt, M. Kaashoek, H. Bal, and A. Tanenbaum. A Comparison 
of Two Paradigms for Distributed Shared Memory. Software Practice and Experience, 22(11):985 1010, Nov 
1992. [21] B. Lucas. A Scientific Visualization Renderer. In Proc. IEEE Visualization '92, pp. 227-233, 
October 1992. [22] V. Machiraju, A Framework for Migrating Objects in Distributed Graphics Applications, 
Masters Thesis, University of Utah, Depart­ment of Computer Science, Salt Lake City, UT, June, 1997. 
[23] B. MacIntyre. Repo: Obliq with Replicated Objects. Programmers Guide and Reference Manual. Columbia 
University Computer Science Department Research Report CUCS-023-97, 1997.} [24] B. MacIntyre, and S. 
Feiner. Language-level Support for Exploratory Programming of Distributed Virtual Environments. In Proc. 
ACM UIST 96, pages 83 94, Seattle, WA, November 6 8, 1996. [25] M. A. Najork and M. H. Brown. Obliq-3D: 
A High-level, Fast-turn­around 3D Animation System. IEEE Transactions on Visualization and Computer Graphics, 
1(2):175 145, June 1995. [26] R. Ben-Natan. CORBA: A Guide to the Common Object Request Broker Architecture, 
McGraw Hill, 1995. [27] D. Phillips, M. Pique, C. Moler, J. Torborg, D. Greenberg. Distribut­ed Graphics: 
Where to Draw the Lines? Panel Transcript, SIGGRAPH 89, available at: http://www.siggraph.org:443/publications/panels/siggraphi89/ 
[28] A. Prakash and H. S. Shim. DistView: Support for Building Efficient Collaborative Applications Using 
Replicated Objects. In Proc. ACM CSCW 94, pages 153 162, October 1994. [29] J. Rohlf and J. Helman, IRIS 
Performer: A High Performance Multiprocessing Toolkit for Real-Time {3D} Graphics, In Proc. ACM SIGGRAPH 
94, pages 381 394, 1994. [30] M. Roseman and S. Greenberg. Building Real-Time Groupware with GroupKit, 
a Groupware Toolkit. ACM Transactions on Computer-Human Interaction, 3(1):66 106, March 1996. [31] C. 
Shaw and M. Green. The MR Toolkit Peers Package and Experiment. In Proc. IEEE VRAIS 93, pages 18 22, 
Sept 1993. [32] G. Singh, L. Serra, W. Png, A. Wong, and H. Ng. BrickNet: Sharing Object Behaviors on 
the Net. In Proc. IEEE VRAIS 95, pages 19 25, 1995. [33] H. Sowizral, K. Rushforth, and M. Deering. The 
Java 3D API Specification, Addison-Wesley, Reading, MA, 1998. [34] M. Stefik, G. Foster, D. G. Bobrow, 
K. Kahn, S. Lanning, and L. Suchman. Beyond The Chalkboard: Computer Support for Collaboration and Problem 
Solving in Meetings. CACM, 30(1):32 47, January 1987. [35] P. S. Strauss and R. Carey, An Object-Oriented 
3D Graphics Toolkit, In Computer Graphics (Proc. ACM SIGGRAPH 92), pages 341 349, Aug, 1992. [36] Sun 
Microsystems, Inc. The Java Shared Data Toolkit, 1998. Unsupported software, available at: http://developer.javasoft.com/developer/earlyAccess/jsdt/ 
[37] I. Tou, S. Berson, G. Estrin, Y. Eterovic, and E. Wu. Prototyping Synchronous Group Applications. 
IEEE Computer, 27(5):48 56, May 1994. [38] R. Waters and D. Anderson. The Java Open Community Version 
0.9 Application Program Interface. Feb, 1997. Available online at: http://www.merl.com/opencom/opencom-java-api.html 
[39] A. Wollrath, R. Riggs, and J. Waldo. A Distributed Object Model for the Java System, In Proc. USENIX 
COOTS 96, pages 219 231, July 1996. [40] R. Zeleznik, D. Conner, M. Wloka, D. Aliaga, N. Huang, P. Hubbard, 
B. Knep, H. Kaufman, J. Hughes, and A. van Dam. An Object-oriented Framework for the Integration of Interactive 
Animation Techniques. In Computer Graphics (SIGGRAPH '91 Proceedings), pages 105 112, July, 1991. [41] 
M. J. Zyda, D. R. Pratt, J. G. Monahan, and K. P. Wilson. NPSNET: Constructing a 3D Virtual World. In 
Proc. 1992 ACM Symp. on Interactive 3D Graphics, pages 147 156, Mar. 1992.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280937</article_id>
		<sort_key>371</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>36</seq_no>
		<title><![CDATA[Constellation]]></title>
		<subtitle><![CDATA[a wide-range wireless motion-tracking system for augmented reality and virtual set applications]]></subtitle>
		<page_from>371</page_from>
		<page_to>378</page_to>
		<doi_number>10.1145/280814.280937</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280937</url>
		<keywords>
			<kw><![CDATA[accuracy]]></kw>
			<kw><![CDATA[augmented reality]]></kw>
			<kw><![CDATA[inertial ultrasonic]]></kw>
			<kw><![CDATA[kalman filtering]]></kw>
			<kw><![CDATA[latency]]></kw>
			<kw><![CDATA[motion tracking]]></kw>
			<kw><![CDATA[sensor fusion]]></kw>
			<kw><![CDATA[virtual sets]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Input devices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.4</cat_node>
				<descriptor>Kalman filtering</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Range data</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010391</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003670.10003683</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic reasoning algorithms->Kalman filters and hidden Markov models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31078399</person_id>
				<author_profile_id><![CDATA[81100572481]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Foxlin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[InterSense Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31080935</person_id>
				<author_profile_id><![CDATA[81100147789]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Harrington]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[InterSense Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P95876</person_id>
				<author_profile_id><![CDATA[81100627421]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[George]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pfeifer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[InterSense Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>192199</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[R. Azuma and G. Bishop. Improving Static and Dynamic Registration in an Optical See-through HMD. In SIGGRAPH 94 Conference Proceedings, ACM Annual Conference Series, Orlando, FL, August 1994.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>897860</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[D.K. Bhatnagar, Position Trackers for Head Mounted display Systems: A Survey. University of North Carolina, Chapel Hill TR93-O1 O, March 1993.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[R. G. Brown and P. Y. C. Hwang. Introduction to Random Signals and Applied Kalman Filtering, 2nd ed. New York: John Wiley &amp; Sons, 1992.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[S. Emura and S. Tachi. Compensation of Time Lag Between Actual and Virtual Spaces by Multi-Sensor Integration. In Proc. IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI 94), pp. 463--469.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[B.T. Fang. Trilateration and Extension to Global Positioning System Navigation. Journal of Guidance, Control and Dynamics, 9(6), Nov.-Dec. 1986.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[S. K. Feiner, A. C. Webster, T. E. Krueger III, B. MacIntyre, and E. J. Keller. Architectural Anatomy. Presence, 4(3): 318-325, Summer 1995.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[F.J. Ferrin. Survey of Helmet Tracking Technologies. In Proc. SPIE, vol. 1456, pages 86-94, April 1991.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[J.F. Figueroa. Ranging Errors Caused by Angular Misalignment Between Ultrasonic Transducer Pairs. Journal of the Acoustical Society of America, 87(3), Mar. 1990.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>207140</ref_obj_id>
				<ref_obj_pid>207072</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[E. Foxlin and N. Durlach. An Inertial Head-Orientation Tracker with Automatic Drift Compensation for use with HMD's. In Proc. Virtual Reality Software &amp; Technology 94, G. Singh, S. K. Feiner, and D. Thalmann, Eds. Singapore: World Scientific, pages 159-174, August 1994.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[E. Foxlin. A Complementary Separate-Bias Kalman Filter for Inertial Head-Tracking. In Proc. IEEE VRAIS 96. IEEE Computer Society Press, March-April 1996.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[E. Foxlin, M. Harrington, and Y. Altshuler. Miniature 6- DOF Inertial System for Tracking HMDs. In Proc. SPIE Helmet and Head-Mounted Displays III, vol. 3362, Orlando, April, 1998.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[W. H. Foy. Position-Location Solutions by Taylor Series Estimation. IEEE Transactions on Aerospace and Electronic Systems, 12(2), Mar. 1976.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[K.C. Ho and Y.T. Chan. Solution and Performance Analysis of Geolocation by TDOA. IEEE Transactions on Aerospace and Electronic Systems, 29(4), Oct. 1993.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>836088</ref_obj_id>
				<ref_obj_pid>523977</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[D. Kim, S. Richards, T. Caudell. An Optical Tracker for Augmented Reality and Wearable Computers. In Proc. IEEE VRAIS 97, p.p. 146-151, IEEE Computer Society Press.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[U. H. List. Nonlinear Prediction of Head Movements for Helmet-Mounted Displays. Air Force Human Resources Laboratory, Technical Paper AFHRL 83-45]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[D. Manolakis. Efficient Solution and Performance Analysis of 3-D Position Estimation by Trilateration. IEEE Trans. on Aerospace and Electronic Systems, 32(4), Oct. 1996.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>196568</ref_obj_id>
				<ref_obj_pid>196564</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[K. Meyer, H. L. Applewhite, and F. A. Biocca. A Survey of Position Trackers. Presence: 1(2), pp. 173--200, 1992.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[J. Nash. Wiring the Jet Set. Wired Magazine, Oct. 1997.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>856446</ref_obj_id>
				<ref_obj_pid>851036</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[J. Seagull and M. Beauer. A Field Usability Evaluation of a Wearable System In Proc. International Symposium on Wearable Computers, Oct. 1997]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>2062414</ref_obj_id>
				<ref_obj_pid>583135</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[H. Sowizral and D. Barnes. Tracking Position and Orientation in a Large Volume. In Proc. IEEE VRAIS 93, p.p. 132- 139. IEEE Computer Society Press.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237283</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[A. State, M. A. Livingston, W. F. Garrett, G. Hirota, M. C. Whitton, E. D. Pisano, and H. Fuchs. Technologies for Augmented Reality Systems: Realizing Ultrasound-Guided Needle Biopsies. In SIGGRAPH 96 Conference Proceedings, ACM Annual Conference Series, pages 439-446.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>147162</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[M. Ward, R. Azuma, R. Bennet, S. Gottschalk, H. Fuchs. A Demonstrated Optical Tracker with Scalable Work Area for Head-Mounted Display Systems. In Proc. 1992 Symposium on Interactive 3D Graphics, Cambridge, MA, March 1992.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[G. Welch and G. Bishop. Single-Constraint-at-a-Time Tracking. In SIGGRAPH 97 Conference Proceedings, ACM Annual Conference Series.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Copyright &#38;#169;1998 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers or to redistribute to lists, requires specific permission and/or a fee. Constellation : A 
Wide-Range Wireless Motion- Tracking System for Augmented Reality and Virtual Set Applications Eric 
Foxlin*, Michael Harrington, and George Pfeifer InterSense Incorporated Abstract We present a new tracking 
system for augmented reality and virtual set applications, based on an inertial navigation system aided 
by ultrasonic time-of-flight range measurements to a constellation of wireless transponder beacons. An 
extended Kalman filter operating on 1-D range measurements allows the inertial sensors to filter out 
corrupt range measurements and perform optimal smoothing and prediction, while at the same time using 
the pre-screened range measurements to correct the drift of the inertial system. The use of inside-out 
ultrasonic tracking allows for tetherless tracking over a building-wide range with no acoustic propagation 
latency. We have created a simulation to account for error sources in the ultrasonic ranging system. 
The fully implemented tracking system is tested and found to have accuracy consistent with the simulation 
results. The simulation also predicts that with some further compensation of transducer misalignment, 
accuracies better than 2 mm can be achieved. CR Categories and Subject Descriptors: I.3.6 [Computer Graphics]: 
Methodology and Techniques - Interaction Techniques I.3.7 [Computer Graphics]: 3-Dimensional Graphics 
and Realism- Virtual reality; I.3.1 [Computer Graphics]: Hardware Architecture-Input devices. Additional 
Keywords: motion tracking, inertial, ultrasonic, kalman filtering, augmented reality, virtual sets, accuracy, 
latency, sensor fusion  1.INTRODUCTION There is an ever expanding set of interactive graphics applications 
which require smooth and fast free-space tracking of some part of the user's body, or some hand-held 
object. Head­mounted displays (HMDs) for immersive virtual environment simulations have stimulated a 
tremendous amount of activity since the early 1990s. Many virtual prototyping systems were developed, 
often using "goggles and gloves" for interaction. While the media has been distracted by the new phenomenon 
of the world-wide web, virtual environment technology has made great strides, especially in the area 
of real-time rendering on affordable hardware, and has been silently catapulted out of the laboratory 
and into real-world applications. 73 Second Ave., Burlington, MA 01803, ericf@isense.com To appear in 
Proceedings of SIGGRAPH 98 (Orlando, Florida, July 19-24, 1998) Computer Graphics Proceedings, Annual 
Conference Series, ACM SIGGRAPH Recently, there has been considerable interest in wearable Augmented 
Reality (AR) systems and virtual set generation for television studios. While these seem to present fairly 
dissimilar tracking problems (tracking a headset v. tracking a camera), they both require a long-range 
tracking solution with very high accuracy that will work reliably in an uncontrolled environment full 
of interference sources. The most immediately promising applications for AR seem to be wearable or mobile 
computers to assist workers in assembly or maintenance of complex machinery from aircraft [18,19] to 
buildings [6] to human patients [21]. In the case of assembling wire bundles for aircraft, the workpiece 
may be over 100 feet long, and the AR tracking system must operate over this span with undiminished performance. 
Likewise, in the virtual studio it is necessary to track a camera which is being carried about freely 
in a very large space full of metal, electronics and bright dynamic lighting. In addition to long range 
and difficult operating environments, both applications share the need for tracking accuracy sufficient 
for visual registration of computer generated and real objects. Tracking is an urgent unsolved problem 
for these two applications. This paper is an effort to address it. With such a plethora of different 
graphics applications that depend on motion-tracking technology for their very existence, a wide range 
of interesting motion-tracking solutions have been invented and brought to various stages of maturity 
over the years. Surveys of the myriad magnetic, optical, acoustic, and mechanical tracking systems are 
available in [2,7,17]. Many HMD applications only require motion over a small region, and these traditional 
tracking approaches are usable, although there are still difficulties with interference, line-of-sight, 
jitter, and latency. We have previously described an alternative solution based on inertial sensing technology 
with automatic drift correction [9] which overcomes the problems with interference, line-of-sight, jitter 
and latency. In fact, that drift-corrected inertial tracking system is sourceless and operates over an 
unlimited range. However, it is only able to track 3-DOF orientation. To correct positional drift in 
a 6-DOF inertial tracking system requires some type of range or bearing measurements to fiducial points 
in the environment. In this paper we present a new tracking system concept, a working system based on 
this concept, test results and a demonstration of the capabilities of this system in a mock virtual set 
camera-tracking application. The new concept is an extension of our previous work on inertial orientation 
tracking technology. The inertial tracker provides a self-contained orientation tracking system with 
unlimited range which does not suffer from the drawbacks associated with source-based or mechanically-linked 
tracking systems. It also contains triaxial accelerometers which are double integrated to obtain changes 
in position, relative to a known starting position. The double integration leads to an unacceptable rate 
of positional drift and must be corrected frequently by some external source. Figure 1: General idea 
of the Constellation system The CONSTELLATION tracking system is similar in its basic principles of 
operation to an aided inertial navigation system (INS), except that it operates indoors, has much finer 
resolution and accuracy, and uses acoustic rather than RF technology for range measurements. Figure 1 
illustrates the system, configured for tracking an HMD in a wide-range VR or AR application. The HMD 
is equipped with an integrated inertial sensing instrument called the InertiaCube and, in this example, 
3 ultrasonic rangefinder modules (URMs). The rangefinder modules communicate with a constellation of 
transponder beacons which may be mounted at any known locations in the environment. The beacons are activated 
one-at-a-time by infrared trigger codes emitted by the rangefinder modules. As each beacon receives its 
own unique code, it responds by emitting an ultrasonic pulse. The rangefinders count the time-of-flight 
(TOF) until the pulse arrives, and use the speed of sound to convert the TOF into a distance. These range 
measurements are fed into an extended Kalman filter (EKF) which makes small adjustments to the position 
and orientation trajectory which is being update at a high rate by the strapdown INS. At least 6 range 
measurements, connecting between at least 3 HMD-mounted microphones and at least 3 fixed transponder 
beacons, are required to completely determine the position and orientation of the HMD. Figure 1 shows 
an example of 6 suitable ranges, which illustrates that multiple nearly simultaneous measurements from 
each triggered beacon can be used if available, but are not required. Two degrees of freedom can be resolved 
by stabilizing with respect to gravity, so only 4 of the myriad potential lines-of-sight need to be open 
to continue tracking indefinitely, and fewer than 4 can be sufficient to sustain reasonable tracking 
for a while. We believe this new tracking system architecture has several compelling advantages: It 
is simple and practical compared to other systems with scaleable-range capabilities  It is possible 
to wear the whole tracking system, including all of the sensors and the computational unit. This results 
in a tracker that is completely untethered.  It is inertial sensor-based, conferring high update rates 
and superior smoothness and predictive capability. It can withstand the loss or corruption of a large 
portion of its acoustic range  measurements without a significant degradation in performance. The acoustic 
ranging system is inside-out compared to other acoustic trackers. Since the sound waves propagate spherically 
from the fixed beacons to the moving target, the TOF recorded at the moment of detection represents the 
instantaneous radius measurement with no latency. 1.1 Previous Work We are not the first to brave the 
design of a scaleable-range tracking system. A system called the optical ceiling tracker has been in 
development for many years at UNC-Chapel Hill [22]. It uses a cluster of head-mounted cameras looking 
at an array of computer-controlled infrared-emitting diodes (IREDs) mounted in ceiling tiles. Although 
it is an optical tracker and ours is a hybrid acousto-inertial tracker, both systems are based on an 
array of fiducial markers on the ceiling and designed to offer the same advantages of high accuracy, 
potentially limitless range, and relative immunity to occlusion through redundancy. Further, both use 
extended Kalman filtering algorithms to process single measurements at a time [23]. Another optical constellation-based 
approach was recently proposed [14] which makes use of quadcells instead of lateral effect photodiode 
cameras. Quadcells are extremely simple and inexpensive optical direction-sensors which eliminate the 
need for lenses and the weight and optical distortion they introduce. However, quadcells detect the direction 
to a light source based on the ratios of the illumination received on each of four photocells, and these 
ratios may be affected by both diffuse and specular reflection of the LED beacon strobes off of various 
surfaces. There are a variety of reasons why we chose to employ acoustic range-finding instead of optical 
bearing-angle measurement to correct the positional drift in our system: It requires no head-mounted 
cameras, only a few tiny ultrasonic microphones, leading to lower weight, power consumption and cost. 
 The orientation is already available from the inertial system. The cumbersome head-mounted camera approach 
was developed to achieve superior orientation resolution. The simpler acoustic and outside-in optical 
trackers are sufficient for tracking position even though they are not very precise for orientation. 
 The mathematics are simpler. Three range measurements pin down the position. Six bearing angles (normally 
measured two at a time) are required to solve for position and orientation.  Microphones are available 
with very wide fields of view compared to cameras. Thus it is possible to use fewer beacons in the constellation 
and still be sure there will be several redundant lines of sight available.  In addition to the two 
aforementioned constellation-type tracking systems, there has been much previous work on inertial and 
acoustic technologies. At least three authors have exploited the motion derivatives provided by inertial 
sensors to add prediction capability to HMD tracking systems [1][4][15]. In the navigation arena, the 
aided inertial navigation approach used in this paper has been well known, and a wide variety of radio-frequency 
navigational aids have been used, including LORAN, OMEGA, radar, GLONASS, and GPS for maritime and aviation 
applications, as well as star-trackers for space navigation. Finally, ultrasonic time-of-flight ranging 
techniques have been used in numerous commercial products for 3-D motion tracking (Logitech 6-D Mouse, 
Mattel Power Glove, Lipman VSCOPE, Kantek Ringmouse). In particular, the Lipman VSCOPE and Kantek ring-mouse 
have wireless infrared-triggered transponders. Also, [20] describes a large-volume extension of the Logitech 
device in which, based on the current position of the tracked object, the nearest of a number of switchable 
reference triangles is automatically selected and used.  1.2 Contribution This paper contributes the 
following new concepts and results in motion tracking for interactive graphics: A novel acousto-inertial 
hybrid tracking approach and a working system. (Demonstrated on video performing an uninterrupted tracking 
sequence spanning several rooms.)  The first TOF motion tracker with latency less than the flight time 
of the ranging signals. This is possible due to the unique inside-out configuration of the transmit-receive 
pairs, which in turn is possible because the use of inertial tracking allows for processing of non-simultaneous 
range measurements.  An example of the usefulness of single-constraint-at-a-time Kalman filtering for 
designing robust sensor-fusion based  this and related applications [11]. The InertiaCube senses angular 
rate about and linear acceleration along each of three orthogonal body axes, as illustrated in Figure 
3. A portion of a floppy disk is visible in the photograph to highlight the InertiaCube's compact dimensions: 
2.7 X 3.4 X 3 cm. Each URM consists of a 40 kHz ultrasonic microphone, 4-8 infrared emitting diodes 
(IREDs) and the necessary electronics, as illustrated in Figure 4. It is not necessary for the IREDs 
and microphones to be physically mounted together, but it makes logical sense since a blocked line of 
sight between a beacon and a microphone makes it futile to trigger that beacon.  Figure 2:Schematic 
overview of hardware Figure 2 illustrates the main hardware components of the tracking system. Just as 
GPS has a space-based constellation of satellites and a vehicle-born receiver with antennae, this system 
has a ceiling-based constellation of transponder beacons and a camera- or person-worn tracker unit with 
ultrasonic rangefinder modules (URMs) and an InertiaCube inertial sensing device. Figure 3 shows a diagram 
and a photograph of the InertiaCube integrated inertial sensing device manufactured by InterSense for 
1. Identify the nearest 4 beacons. 2. Search throughout the entire constellation (which must be pre-known 
to the tracker) for places that have this combination of beacons in close proximity. Test each such hypothesis 
to see if a self-consistent trilateration solution can be found using the actual range measurements with 
the hypothesized beacons .  3. If only one combination of beacons passes the test, use the starting 
pose determined in this self-consistent trilateration of all three microphones and move on to tracking. 
 4. If there are multiple 4-tuples in the constellation which are consistent with the initial set of 
range measurements, try to use range measurements to other beacons to resolve ambiguities.  Note that 
with only 8 different beacon codes, there are only C(8,4) =70 different combinations, which means that 
a large constellation would have a lot of repetitions of the same group of four adjacent codes. To overcome 
this we are increasing the number of beacon codes to 16, which would provide 1820 unique 4-tuples. Even 
larger constellations would require a different scheme using zone codes and specific beacon codes, because 
the acquisition time to sequence through more than 16 beacons would be too long. Once there is a successful 
acquisition, the state and covariance matrix of the EKF get initialized and tracking begins. Figure 5 
illustrates the tracking algorithm. The most important point to note is that the integrated inertial 
sensors have direct feed­through to the outputs, which insures low latency. The angular rates measured 
by the gyros are integrated once to obtain orientation, which is output directly. The orientation is 
also used to transform the accelerations measured by the accelerometers in the constantly changing body-referenced 
frame into a steady and level navigation frame (hereafter "nav-frame" or N-frame) with its z-axis vertical. 
The unwanted effect of gravity on this virtual z­accelerometer is first canceled, and then the nav-frame 
acceleration is double integrated to obtain position, which is output directly. The EKF uses the range 
measurements to estimate the amount of accumulated error in the orientation, gyro biases, position, and 
velocity. It applies these error estimates immediately to the appropriate integrator outputs as tiny 
corrections which prevent the accumulation of error and insure that the EKF is always linearizing about 
the most accurate possible state. Complementary Kalman filtering is discussed in [3], and the details 
of our complementary EKF approach are provided in [10]. Figure 5: Tracking algorithm flow chart The 
selection and utilization of the range measurements is very interesting and deserves some elaboration. 
First of all, the tracker makes immediate use of individual range measurements as they come in, rather 
than saving up measurements from 3 beacons, performing a trilateration and feeding the computed position 
into the Kalman filter as a measurement vector. This technique of processing several scalar measurements 
instead of one vector measurement is called sequential update Kalman filtering, and it is known to be 
both faster and numerically more robust because it avoids the matrix inversion in the Kalman gain update 
step. When measurements containing only partial information about the state are applied during different 
update cycles, the process has been called single-constraint-at-a-time (SCAAT) tracking [23]. In non­inertial 
tracking systems, this allows the tracker to have partial updates at a higher update rate, resulting 
in lower latency and jitter. In our aided inertial design, the pose output gets essentially complete 
updates (with a little drift) at a high rate of about 500 Hz, but it is still more convenient to make 
partial drift correction updates immediately upon receiving each range measurement, because at this time 
an accurate measurement residual can be formed by differencing the measured range and the predicted range 
computed using the most recent inertial state update. Secondly, when the tracker receives a new range 
measurement, it already knows where it is, and it also knows, based on the diagonal elements of the error 
covariance matrix, approximately how much uncertainty there is in this self-position estimate. Since 
it knows the location of the beacon that sent the pulse, it can predict what the range measurement should 
be. If the range measurement doesn't match within the tolerance computed from the covariance matrix, 
it can be rejected. This is an extremely useful feature in an acousto-inertial tracker. Acoustic range 
measurement devices always detect the first arrival: a pulse is sent and a counter is started. Since 
the direct pulse arrives before its echo, the counter is stopped by the first detected pulse at the receiver. 
Unfortunately, there is occasionally a random background noise or an echo from a previous sampling period 
which arrives before the real pulse and stops the counter. In our system, we know when to expect the 
real pulse and can gate the receiver open only during the window of time when the returned pulse is expected. 
This can likely prevent over 90% of premature pulse detection problems. Because we use the diagonal elements 
of the covariance to dynamically adjust the acceptance time window, if the tracker misses some measurements 
it will widen the window and accept subsequent measurements to bring it back on course, instead of becoming 
completely lost.  3. Constellation Geometry and Error Sensitivity The constellation may be set up in 
many geometrical configurations in order to adapt to different types of surroundings. This invites the 
questions 1) What geometry will result in the highest tracking performance? , 2) What will the performance 
be for some particular geometry? and 3) How many transponders are really needed? In this section, we 
develop a simulation to evaluate the sensitivity of the position and orientation calculations to all 
the known error sources, both random and systematic. A standard metric used in GPS and other range-based 
position location systems to evaluate the effect of geometry on positioning uncertainty is the Geometric 
Dilution of Precision (GDOP): 2 22 s + s + s  tr P ) ( xyz GDOP = , (1) s r s r where P is the error 
covariance of the position solution [16]. This expression is a function of position and it describes, 
at a given point (x,y,z), how much positional uncertainty there will be if all of the range measurements 
have the same uncertainty of sr . The GDOP is useful for estimating the amplification of random noise 
in the range measurements, but there are other systematic error sources which may be present: 1. error 
in the beacon positions 2. temperature error 3. constant time-delay errors in beacons (due to part-to-part 
variation or electronics drift) 4. constant time-delay errors in URMs (due to part-to-part variation 
or electronics drift) 5. transducer angle related errors We have developed a simulation in MATLAB to 
probe the sensitivity to all these error sources for any desired geometry. The simulation allows the 
user to enter magnitudes for all of the above systematic error sources, and any desired constellation 
geometry,  then it computes the resulting systematic error and GDOP at a sampling of points within a 
user-defined test volume. Figure 6: Error sensitivity simulation Figure 6 shows a block diagram of the 
simulation. The main flow of the simulation is as follows. First the user is given the opportunity to 
set up a "trial" constellation for evaluation, and to specify the desired tracking region beneath the 
constellation. A random horizontal and vertical error are then applied to each beacon in the constellation, 
uniformly distributed over user­specified intervals. In the main loop of the simulation, the program 
steps through test positions within the desired tracking volume. At each position it rotates the whole 
simulated tracker (headset or camera configuration) to a variety of angles, generating the appropriate 
set of truth positions for all the microphones on the tracker. All beacons which are within range of 
the tracker are selected using rangelimit = (5m) cos( q t ) cos( q r ) where cos(q t ) and cos(q r) approximate 
the off-axis attenuation patterns of our 40 kHz transmitters and receivers. Having selected an active 
set of beacons, range measurements are calculated, including all appropriate error perturbations, and 
fed into a multi-lateration algorithm which solves for the positions of all the receivers. There are 
numerous trilateration and multilateration algorithms in the literature [16][5][13][12]. Although [16] 
provides an exact closed form solution that is both general and computationally efficient, we chose to 
employ the classic iterative least-squares approach [12], because it is a closer simulation of the extended 
Kalman filter used in the tracker. In fact, in the absence of motion, the EKF converges to the same solution 
as the recursive least-squares approximation used here [12]. Figures 7 and 8 display some simulation 
results for a constellation which consists of an infinite square array with 2 foot (61cm) grid-spacing 
and 3 meter height. In all cases, the test volume extended from 0-2.5 meters in the z (height) dimension, 
and 0-1 foot in the x and y dimensions. Due to the symmetries of an infinite square array, any (x,y) 
point is equivalent to some point inside of this single-quadrant test region. Therefore, the range of 
errors displayed in these volumetric visualizations represents the whole range of errors that a tracker 
would experience over any size workspace, as long as it does not approach too closely the edge of the 
constellation. (We have not simulated edge effects, but would expect higher errors near the edges). The 
systematic error levels were set for this simulation run to the following values: error in beacon positions: 
+/-2mm horizontal/ +/-4mm vertical, uniform distribution temperature error: 0.2.C beacon variations: 
+/- 1mm, uniform distribution URM variations: +/- 1mm, uniform distribution transducer angle related 
errors: +/-2.5mm range perturbations at 60.off axis Table 1: Error source inputs for simulation in Fig. 
7-8 These error levels were chosen to reflect what we believe to be the actual systematic error levels 
in the current setup in our laboratory. They result in a combined systematic positional error shown in 
Figure 7 which ranges from 2.3-4.7 mm from floor level to 2.5 meters height. By contrast, the positional 
resolution of pure ultrasonic range measurements, shown in Figure 8, is 0.7-1.5 mm in most of the active 
volume, increasing to 2.5 mm near the floor. These numbers are obtained via Equation 1 from the estimation 
error covariance returned by the multilateration algorithm which has been fed individual range noise 
sigmas corresponding to our hardware test results in Section 4.1.  0.25 degrees because the inertial 
sensor is able to correct pitch and roll to this level without any ultrasonic aiding [11]. This data 
is based on the improved error compensation used in Figure 9. As can be seen from the plot, 15 cm of 
microphone separation, which can be conveniently arranged on an AR head-mounted display, is sufficient 
to achieve good orientation accuracy. Wider separations, which can easily be arranged on a camera, lead 
to even higher accuracy. 0.7 0.6 0.5 Orientation error (deg) 0.4 0.3 0.2 Figure 8: Position resolution 
The errors in Figure 7 are largely caused by the transducer misalignment angle errors. We are in the 
process of developing calibration procedures to better model and compensate for these effects. In Figure 
9 we show simulation results predicting approximately 1 mm accuracy when the residual error due to mis­modeled 
transducer angle effects has been reduced to 1 mm per radian of misalignment, and URM part-to-part variations 
are measured and compensated out. This excellent accuracy is achieved even in the presence of random 
beacon placement errors of +/-2 mm horizontal and +/-4 mm vertical, probably because the random errors 
from the 20-40 beacons participating in the multilateration tend to cancel each other out. 0.1 0 Baseline 
separation (cm) Figure 10: Orientation errors v. mic. baseline separation  4. Test Results 4.1 1-D 
Ranging Results Error From Straight Line Fit  Figure 9: Position errors, assuming improved compensation 
of systematic error sources Accuracy of acoustically measured orientation is a function of baseline separation 
of microphones, so the simulation was run at several different baselines, and the resulting minimum, 
maximum, and average orientation error throughout a test volume ranging from 1-2 meters below the beacon 
constellation are plotted in Stepper Distance (m) Figure 11: One dimensional ultrasonic ranging results 
Figure 11 shows results of testing our ultrasonic ranging hardware prototypes for resolution and linearity. 
The test was performed using one URM and a transponder mounted on the carriage plate of a leadscrew-driven 
4-foot long linear actuator. The rail was moved 4 times to collect approximately 5 meters of data. A 
single straight line was fit to the entire collective data set, and the residual errors are plotted in 
Figure 11. The discontinuities in the data were caused by inexact placement of the rail after moving 
it. Despite these discontinuities, the experiment provides a meaningful assessment of the 1-D ultrasonic 
ranging performance. The linearity is approximately 0.1% FS, and the range noise of 1mm per meter of 
range used in the previous section to generate the GDOP appears justified.  4.2 3-DOF Position Tracking 
Accuracy To test the accuracy of the 6-DOF tracking system, we set up a 3 by 3 grid (with one corner 
missing) of transponder beacons on 2 foot centers on a drop ceiling grid. 1.5 meters below the grid we 
leveled a table with a 1" grid marked on it, and registered this grid to the constellation coordinates 
using plumb bobs. A 5-DOF digitizer arm (Immersion Corp.) was placed on this table and registered to 
its grid with a calibration procedure that involves touching four reference points. The tip of the arm 
was then attached to a camera-tracker head containing an InertiaCube and 3 URMs separated with horizontal 
and vertical baseline distances of about 28 cm and 25 cm respectively. The camera tracker was manually 
moved to 30 locations spaced throughout the test volume reachable by the arm. For comparison to the simulation, 
Figure 12 plots the root-sum-square (RSS) of the 3 position error components at each point. While the 
average error is the same as the simulation results in Figure 7, the worst case error is 2 mm larger. 
This is to be expected because the constellation for this test was equipped with only 8 beacons. When 
the simulation is run with 8 beacons, it also predicts errors of about 1 to 8 mm. 0.5 X Position(m) The 
prototype ultrasonic distance measuring hardware was tested in a controlled 1-dimensional measurement 
set-up in order to characterize the error performance and provide meaningful input data for the simulation. 
The 3-D position locating accuracy was tested in a benchtop configuration with 8 beacons, and found to 
conform with the simulation's predictions. Finally, the fully functional 6-DOF tracking system was used 
to record a videotape to demonstrate qualitatively it's resolution, dynamic performance, and range. The 
dynamic performance has not yet been tested quantitatively, but in theory there are expected to be no 
appreciable sources of latency in this system. The range measurements reflect the instantaneous position 
when received, and the integration of the rate and acceleration data and incorporation of range measurements 
by means of Kalman filter updates runs at 400-600 Hz. Thus the effective latency of the system is expected 
to be about 2.5 ms, even though range measurements are received much less frequently. The simulation 
indicates that higher levels of accuracy can be obtained by compensating for part-to-part transducer 
variations. Ultrasonic transducers have an angular dependence which causes a shift of up to 1 mm at 20o 
misalignment angle [8], and we find even more at larger angles. We have constructed a computer controlled 
2-axis rotation device to characterize this dependency and begun to use the data in the firmware to compensate 
for the effects. While the wireless nature of the transponder beacons makes this system easier to set 
up than other wide-range tracking systems, it is still necessary to accurately measure the beacon locations 
and download them into the tracking system prior to tracking. This can be time-consuming, and if not 
done very carefully can become a  dominant source of tracking error. In subsequent work, we plan to 
0 51015202530 explore the feasibility of an auto-mapping algorithm that enables a -0.5 Error (mm) user 
to install the constellation using the following procedure: min=0.5705 max=6.464 avg=3.412 1. A "seed" 
constellation consisting of 3 rigidly mounted beacons is first hung which establishes the reference frame. 
10 5 2. The rest of the beacons are hung randomly in any convenient locations. They need not be coplanar. 
 3. The tracker begins tracking using only the seed beacons.  0 Then it starts trying one additional 
beacon code at a time until it finds one that responds. The new beacon's position is test point estimated 
and entered into the constellation database. As the Figure 12: RSS of X,Y, and Z errors at 30 test points 
user walks around the workspace, the tracker finds and auto­installs all the beacons with approximate 
positions. 5. Conclusions &#38; Future Work A new type of motion tracker has been presented which combines 
inertial orientation and acceleration sensors with ultrasonic ranging devices in a fashion that allows 
the inertial sensors to filter out corrupt range measurements (ie. echoes and acoustic interference), 
and perform optimal motion smoothing and prediction, while at the same time using the pre-screened range 
measurements to correct the drift of the inertial sensors. Some simulations were performed which indicated 
that the tracker will be capable of achieving 1-3 mm accuracy levels if used beneath a transponder constellation 
with 2 ft. spacing that extends 9 ft. beyond the tracking region on each side. Edge effects have not 
been simulated, but we expect that any dilution of precision near the edges of a room could be overcome 
by extending the transponder array part-way down the wall. 4. In a subsequent auto-calibration step, 
or during normal tracking, there is continuous slow refinement of beacon positions using recursive estimation. 
Step 4 has been shown to work in the UNC optical ceiling tracker [23]. It is likely to work here as well 
if systematic errors other than beacon placement errors have been sufficiently compensated such that 
the system can track its position to an accuracy substantially better than the placement accuracy of 
the individual beacons. The simulation results in Figure 9 show that even with random beacon placement 
errors of +/- 2-4 mm, the tracker is able to find its own position to about 1 mm accuracy. This suggests 
that auto-calibration in this system may indeed lead to successive refinement of accuracy rather than 
degradation. 6. Acknowledgments We are thankful to Wallace VanderVelde for many invaluable insights and 
lessons on inertial navigation and Kalman filtering, to Gary Bishop, Greg Welch, and David Mizell for 
enlightening discussions, and to our SIGGRAPH reviewers for many insightful and useful comments and suggestions. 
7. References [1] R. Azuma and G. Bishop. Improving Static and Dynamic Registration in an Optical See-through 
HMD. In SIGGRAPH 94 Conference Proceedings, ACM Annual Conference Series, Orlando, FL, August 1994. [2] 
D. K. Bhatnagar, Position Trackers for Head Mounted display Systems: A Survey. University of North Carolina, 
Chapel Hill TR93-010, March 1993. [3] R. G. Brown and P. Y. C. Hwang. Introduction to Random Signals 
and Applied Kalman Filtering, 2nd ed. New York: John Wiley &#38; Sons, 1992. [4] S. Emura and S. Tachi. 
Compensation of Time Lag Between Actual and Virtual Spaces by Multi-Sensor Integration. In Proc. IEEE 
International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI 94), pp. 
463--469. [5] B.T. Fang. Trilateration and Extension to Global Positioning System Navigation. Journal 
of Guidance, Control and Dynamics, 9(6), Nov.-Dec. 1986. [6] S. K. Feiner, A. C. Webster, T. E. Krueger 
III, B. MacIntyre, and E. J. Keller. Architectural Anatomy. Presence, 4(3): 318-325, Summer 1995. [7] 
F. J. Ferrin. Survey of Helmet Tracking Technologies. In Proc. SPIE, vol. 1456, pages 86-94, April 1991. 
[8] J.F. Figueroa. Ranging Errors Caused by Angular Misalignment Between Ultrasonic Transducer Pairs. 
Journal of the Acoustical Society of America, 87(3), Mar. 1990. [9] E. Foxlin and N. Durlach. An Inertial 
Head-Orientation Tracker with Automatic Drift Compensation for use with HMD's. In Proc. Virtual Reality 
Software &#38; Technology 94, G. Singh, S. K. Feiner, and D. Thalmann, Eds. Singapore: World Scientific, 
pages 159-174, August 1994.  [10] E. Foxlin. A Complementary Separate-Bias Kalman Filter for Inertial 
Head-Tracking. In Proc. IEEE VRAIS 96. IEEE Computer Society Press, March-April 1996. [11] E. Foxlin, 
M. Harrington, and Y. Altshuler. Miniature 6-DOF Inertial System for Tracking HMDs. In Proc. SPIE Helmet 
and Head-Mounted Displays III, vol. 3362, Orlando, April, 1998. [12] W. H. Foy. Position-Location Solutions 
by Taylor Series Estimation. IEEE Transactions on Aerospace and Electronic Systems, 12( 2), Mar. 1976. 
[13] K.C. Ho and Y.T. Chan. Solution and Performance Analysis of Geolocation by TDOA. IEEE Transactions 
on Aerospace and Electronic Systems, 29(4), Oct. 1993. [14] D. Kim, S. Richards, T. Caudell. An Optical 
Tracker for Augmented Reality and Wearable Computers. In Proc. IEEE VRAIS 97, p.p. 146-151, IEEE Computer 
Society Press. [15] U. H. List. Nonlinear Prediction of Head Movements for Helmet-Mounted Displays. Air 
Force Human Resources Laboratory , Technical Paper AFHRL 83-45 [16] D. Manolakis. Efficient Solution 
and Performance Analysis of 3-D Position Estimation by Trilateration. IEEE Trans. on Aerospace and Electronic 
Systems, 32(4), Oct. 1996. [17] K. Meyer, H. L. Applewhite, and F. A. Biocca. A Survey of Position Trackers. 
Presence: 1(2), pp. 173--200, 1992. [18] J. Nash. Wiring the Jet Set. Wired Magazine, Oct. 1997. [19] 
J. Seagull and M. Beauer. A Field Usability Evaluation of a Wearable System In Proc. International Symposium 
on Wearable Computers, Oct. 1997 [20] H. Sowizral and D. Barnes. Tracking Position and Orientation in 
a Large Volume. In Proc. IEEE VRAIS 93, p.p. 132­ 139. IEEE Computer Society Press. [21] A. State, M. 
A. Livingston, W. F. Garrett, G. Hirota, M. C. Whitton, E. D. Pisano, and H. Fuchs. Technologies for 
Augmented Reality Systems: Realizing Ultrasound-Guided Needle Biopsies. In SIGGRAPH 96 Conference Proceedings, 
ACM Annual Conference Series, pages 439-446. [22] M. Ward, R. Azuma, R. Bennet, S. Gottschalk, H. Fuchs. 
A Demonstrated Optical Tracker with Scalable Work Area for Head-Mounted Display Systems. In Proc. 1992 
Symposium on Interactive 3D Graphics, Cambridge, MA, March 1992. [23] G. Welch and G. Bishop. Single-Constraint-at-a-Time 
Tracking. In SIGGRAPH 97 Conference Proceedings, ACM Annual Conference Series.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280940</article_id>
		<sort_key>379</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>37</seq_no>
		<title><![CDATA[mediaBlocks]]></title>
		<subtitle><![CDATA[physical containers, transports, and controls for online media]]></subtitle>
		<page_from>379</page_from>
		<page_to>386</page_to>
		<doi_number>10.1145/280814.280940</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280940</url>
		<keywords>
			<kw><![CDATA[phicons]]></kw>
			<kw><![CDATA[physical constraints]]></kw>
			<kw><![CDATA[tangible bits]]></kw>
			<kw><![CDATA[tangible user interface]]></kw>
			<kw><![CDATA[ubiquitous computing]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP17009862</person_id>
				<author_profile_id><![CDATA[81100226633]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Brygg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ullmer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14129647</person_id>
				<author_profile_id><![CDATA[81100363076]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hiroshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ishii]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P70552</person_id>
				<author_profile_id><![CDATA[81350582585]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Dylan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Glas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Apple Computer, Inc. Apple Human Interface Guidelines: The Apple Desktop Interface. New York: Addison Wesley, 1987.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1120435</ref_obj_id>
				<ref_obj_pid>1120212</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Brave, S., and Dahley, A. inTouch: A Medium for Haptic Interpersonal Communication. In CHI'97 Extended Abstracts, pp. 363-364.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Crampton Smith, G. The Hand That Rocks the Cradle. I.D., May/June 1995, pp. 60-65.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Denckla, B. Rogus C++ MIDI Suite. http ://theremin. media, mit. edu/rogus/]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>223964</ref_obj_id>
				<ref_obj_pid>223904</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Fitzmaurice, G., Ishii, H., and Buxton, W. (1995). Bricks: Laying the Foundations for Graspable User Interfaces. Proc. of CHI' 95, pp. 442-449.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>925309</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Fitzmaurice, G. Graspable User Interfaces. Ph.D. Thesis, University of Toronto, 1996.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>274652</ref_obj_id>
				<ref_obj_pid>274644</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Gorbet, M., Orth, M., and Ishii, H. Triangles: Tangible Interface for Manipulation and Exploration of Digital Information Topography. In Proc. of CHI'98.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258715</ref_obj_id>
				<ref_obj_pid>258549</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Ishii, H., and Ullmer, B. Tangible Bits: Towards Seamless Interfaces between People, Bits, and Atoms. In Proc. of CHI' 97, pp. 234-241.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>108870</ref_obj_id>
				<ref_obj_pid>108844</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Mackinlay, J., Robertson, G., &amp; Card, S. The Perspective Wall: Detail and context smoothly integrated. In Proc. of CHI'91, pp. 173-179.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Poor, R. The iRX 2.0 ..where Atoms meet Bits. http ://ttt. media, mit. edu/pia/Research/iRX2/]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>263505</ref_obj_id>
				<ref_obj_pid>263407</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Rekimoto, J. Pick-and-Drop: A Direct Manipulation Technique for Multiple Computer Environments. In Proc. of UIST'97, pp. 31-39.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>274684</ref_obj_id>
				<ref_obj_pid>274644</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Resnick, M., Berg, F., et al. Digital Manipulatives: New Toys to Think With. In Proc. of CHI'98.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Suzuki, H., Kato, H. AlgoBlock: a Tangible Programming Language, a Tool for Collaborative Learning. In Proc. of 4th European Logo Conference, Aug. 1993, Athens Greece, pp. 297-303.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1267488</ref_obj_id>
				<ref_obj_pid>1267463</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Ullmer, B. 3wish: Distributed {incr Tcl} Extensions for Physical-World Interfaces. In Proc. of Tcl/Tk'97, pp. 169- 170.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>263551</ref_obj_id>
				<ref_obj_pid>263407</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Ullmer, B., and Ishii, H. The metaDESK: Models and Prototypes for Tangible User Interfaces. In Proc. of UIST'97, pp. 223-232.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Weiser, M. The Computer for the 21st Century. In Scientifi'c American, 265(3), pp. 94-104.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 mediaBlocks: Physical Containers, Transports, and Controls for Online Media Brygg Ullmer, Hiroshi Ishii, 
and Dylan Glas* Tangible Media Group MIT Media Lab Copyright &#38;#169;1998 by the Association for Computing 
Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal or 
classroom use is granted without fee provided that copies are not made or distributed for profit or commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for components 
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy 
otherwise, to republish, to post on servers or to redistribute to lists, requires specific permission 
and/or a fee. Abstract We present a tangible user interface based upon mediaBlocks: small, electronically 
tagged wooden blocks that serve as physical icons ( phicons ) for the containment, transport, and manipula­tion 
of online media. MediaBlocks interface with media input and output devices such as video cameras and 
projectors, allow­ing digital media to be rapidly copied from a media source and pasted into a media 
display. MediaBlocks are also compatible with traditional GUIs, providing seamless gateways between tangible 
and graphical interfaces. Finally, mediaBlocks act as physical controls in tangible interfaces for tasks 
such as se­quencing collections of media elements. CR Categories and Subject Descriptors: H.5.2 [User 
Inter­faces] Input devices and strategies; H.5.1 [Multimedia Informa­tion Systems] Artificial, augmented, 
and virtual realities Additional Keywords: tangible user interface, tangible bits, phicons, physical 
constraints, ubiquitous computing 1 INTRODUCTION Computers have traditionally recorded digital information 
on both fixed and removable storage media. While removable media have often been limited in capacity, 
speed, and accessi­bility, these factors have been offset by the expanded storage and mobility removable 
media affords. However, the rise of widespread online connectivity for both computers and other digital 
media devices (cameras, projectors, etc.) alters this historical role division. Extended capacity and 
instant mobility are now better afforded by keeping media online. Does removable media risk obsolescence 
in the online age? From a user interface standpoint, the process of online media exchange between digital 
whiteboards, projectors, computers, and other devices is still far from seamless. Reference and manipulation 
of online media is at present generally limited to GUI-based interaction with file paths, URLs, and hyperlinks 
 a process quite at odds with most media interfaces. We believe that coupling the physicality of removable 
media with the connectivity and unlimited capacity of online content offers a potential solution to this 
problem. Moreover, we believe this approach suggests new physical-world interface possibilities that 
go beyond the pervasive graphical user interface. *{ullmer,ishii,krill}@media.mit.edu 20 Ames St., E15-445, 
Cambridge, MA 02139 http://tangible.media.mit.edu/ In this paper, we introduce a tangible user interface 
(TUI) based upon mediaBlocks*: small wooden blocks that serve as physical icons ( phicons ) [15] for 
the containment, transport, and ma­nipulation of online media. MediaBlocks do not actually store media 
internally. Instead, they are embedded with ID tags that allow them to function as containers for online 
content, or alternately expressed, as a kind of physically embodied URL. MediaBlocks interface with media 
input and output devices such as video cameras and projectors, allowing digital media to be rapidly copied 
from a media source and pasted into a media display. MediaBlocks are also compatible with traditional 
GUIs, providing seamless gateways between tangible and graphical interfaces. Finally, mediaBlocks are 
used as physical controls in tangible interfaces for tasks such as sequencing collections of media elements. 
 Figure 1: mediaBlocks design space The mediaBlocks design space is illustrated in Figure 1. Media-Blocks 
serve as a medium of interchange between media source and display devices; between media devices and 
GUI-based computers; and between these pre-existing devices and new tangible interfaces for media manipulation. 
In this fashion, mediaBlocks fill the user interface gap between physical devices, digital media, and 
online content. *Note: Our mediaBlocks have no relation to the Magnifi Inc. software product of the same 
name. 2 FUNCTIONALITY OVERVIEW The following paragraphs describe the basic functionality of our mediaBlocks 
interfaces. Detailed consideration of interface use and implementation follows later in the paper. 2.1 
Physical Containers MediaBlocks are phicons (physical icons, [15]) embodied as small wooden blocks. MediaBlocks 
do not actually store digital media internally. Instead, they physically contain ID tags that are dynamically 
associated with sequences of online media elements. When used in environments where many media devices 
are linked to online computation, mediaBlocks act as physical containers for online media. As such, mediaBlocks 
have a variety of interesting properties. Because contents remains online, mediaBlocks have unlimited 
capacity and rapid transfer speed (copying is instantaneous, while playback is a function of network 
bandwidth). For the same reason, a lost block is easily replaced. MediaBlocks may also contain live streaming 
media. 2.2 Physical Transports One role of mediaBlocks is support for simple physical transport and 
interchange of media between media devices. While inter­application copy and paste is core to the modern 
GUI, compa­rably lightweight equivalents have not existed for physical media devices. We have realized 
a physical analog of copy and paste by combining mediaBlocks with physical slots mounted upon associated 
media devices. We have implemented mediaBlock support for four media devices: a desktop video projector, 
network printer, video camera, and digital whiteboard. Inserting a block into the slot of a media source 
begins recording to an online server. Recording stops when the block is removed. This can be understood 
as copying from the media source into the block. Similarly, contents may be pasted into a media display 
by inserting a block into the associated slot. This will display block contents, with removal halting 
playback. Figure 2: Whiteboard, printer mediaBlock slots  2.3 Physical Gateways MediaBlock slots have 
also been implemented for use on gen­eral-purpose computers. Slots are mounted on the right margins of 
computer monitors. When a mediaBlock is inserted into the slot, a GUI window scrolls out of the block 
from the slot s left edge (contiguous with the screen). This window provides GUI access to block contents. 
(Figure 3)  Figure 3: mediaBlock monitor slot MediaBlock contents may then be transferred to the desktop 
or to GUI applications with conventional mouse-based drag and drop support. Media may also be copied 
into blocks in this fashion. In this way, mediaBlocks can be used to seamlessly exchange content between 
computers and media sources, dis­plays, or other computers. 2.4 Physical Browsers While the transport 
function of mediaBlocks allows media to be exchanged between various output devices, it does not address 
interactive control of media playback, especially for mediaBlocks containing multiple media elements. 
The media browser is a simple tangible interface for navigating sequences of media elements stored in 
mediaBlocks. (Figure 4) The browser is composed of a detented browse wheel, video monitor, and mediaBlock 
slot. Useful both in casual viewing and formal presentation contexts, the browser supports the interactive 
navigation of mediaBlocks sequences for projector-based display, as well as displaying media on its local 
screen.  Figure 4: Media browser device 2.5 Physical Sequencers The media browser provides interactive 
physical control of mediaBlock display, but does not support modification of media-Block contents. The 
media sequencer is a tangible interface using mediaBlocks both as containers and controls for physically 
sequencing media elements. (Figure 5) Where earlier sections have introduced mediaBlock slots, the sequencer 
uses physical racks, stacks, chutes, and pads as structures that physically and digitally operate upon 
media­Blocks. In particular, the rack is a physical constraint used to digitally index and sequence mediaBlock 
contents as a function of the blocks physical configuration on the rack. Figure 5: Media sequencer device 
 2.6 Integrated Functionality In addition to illustrating the above mediaBlock functions, the accompanying 
video figure demonstrates the creation of a multi­media presentation integrating all behaviors we have 
described. In less than four minutes of uncut footage, we show recording and transport of digital video 
and whiteboard content; selecting photographs and authoring textual slides for inclusion; assem­bling 
these contents into an integrated presentation; rendering the presentation to a network printer; and 
presenting this content on the browser-controlled video projector. The resulting content is also Web-accessible 
at each stage of authoring and delivery. We believe this interface example is significant for several 
reasons. First, this example shows the creation, manipulation, and use of complex multimedia content 
with a simplicity and speed that we believe to be highly competitive with other ap­proaches. Simultaneously, 
it is key to note that the only key­board, mouse, or other GUI interaction present in the entire sequence 
is the composition of a textual slide. In the remainder of the paper, we will continue to develop the 
interface process, technical operation, and functional roles that we believe represent a powerful new 
approach for interaction between people, physical objects, and online digital information.  3 RELATED 
WORK The mediaBlocks project was most directly influenced by the metaDESK/Tangible Geospace prototype 
[15, 8] that introduced the phicon concept. Tangible Geospace was based upon an augmented physical desktop 
and phicons representing geographi­cal landmarks. Manipulation of phicons controlled the position, rotation, 
and scaling of spatially coincident graphical landscapes. Tangible Geospace was developed in part to 
explore physical instantiation of the GUI metaphor. As the GUI system of win­dows, controls, and icons 
itself drew from a metaphor of the physical desktop, the physical analogs of lenses, instruments, and 
phicons (physical icons) seemed a promising first step towards the realization of tangible interfaces. 
 In practice, Tangible Geospace served to illustrate a number of major differences between graphical 
and tangible UIs. The fundamental malleability and extent of control over the GUI s graphical workspace 
differs substantially from the object persis­tence and cumulative, potentially inconsistent physical 
degrees of freedom expressed by TUI elements. In short, the GUI metaphor appeared unable to generalize 
across the potential design space of tangible user interfaces. Subsequent research with Triangles and 
inTouch made a key insight towards resolving this shortcoming [7, 2]. Instead of relying upon pre-existing 
metaphors of GUI or the physical world (e.g., the metaDESK s optical metaphor [8]), these projects developed 
new interface metaphors based on the affordances unique to physical/digital artifacts. Both projects 
served as partial inspiration for mediaBlocks, which continues this design aesthetic. The whiteboard-based 
mediaBlock functionality draws upon an earlier whiteboard TUI called the transBOARD [8]. The trans-BOARD 
used paper cards called hypercards as physical carriers for live and recorded whiteboard sessions. However, 
the hyper­card interaction relied upon barcode wanding, which was found cumbersome in practice. The ubiquitous 
computing vision of [16] speaks to moving computation and networking off the desktop and into many devices 
occupying niche contexts within the physical environ­ment. This insight is core to the mediaBlocks system. 
Still, the interface prototypes of [16] continued to rely primarily upon GUI-based approaches. The Bricks 
work [5] made dynamic association between digital properties and physical handles through the tray device. 
Also an inspiration for the mediaBlocks work, the Bricks work did not develop physical objects as digital 
containers or physical ma­nipulation outside of the Active Desk context. Bishop s Marble Answering Machine 
[3] made compelling demonstration of passive marbles as containers for voice messages. Later work by 
Bishop demonstrated physical objects associated with diverse digital content, and prototyped an early 
object-GUI gateway. The Pick-and-Drop work of [11] provides strong support for file exchange between 
palmtop, desktop, and wall-based GUIs with a pen stylus. However, the technique less directly addresses 
media exchange between non-GUI devices, or with devices that are not spatially adjacent. Molenbach s 
LegoWall prototype (discussed in [6]) used LEGO structures to contain information about ocean-going ships. 
These objects were combined with display and control objects that, when plugged adjacent to containers, 
could display shipping schedules, send this data to hardcopy printers, etc. The AlgoBlock system uses 
the manipulation of physical blocks to create computer programs [13]. Connections between the blocks 
create LOGO programs that may be rearranged by the user, esp. in an educational context. The Digital 
Manipulatives research of Resnick, Borovoy, Kramer, et al. [12] has developed societies of objects including 
badges, buckets, beads, balls, and stacks. Each is associated with digital semantics responsive to physical 
manipulations such as shaking, tossing, and stacking objects, or dunking objects in buckets of digital 
paint. The manipulatives work makes strong progress towards developing objects with rich digital/physical 
couplings. 4 SYSTEM OVERVIEW Figure 6 illustrates our current implementation of the media-Blocks interface. 
The center column, media devices, lists the physical devices for which we have integrated mediaBlock 
support. The left column shows devices supporting operation of mediaBlock slots. The right column presents 
the computers that manage media recording and playback. The media browser and sequencer SGI machines 
play additional roles as the controllers for these tangible interfaces. However, these details are beyond 
the scope of our illustration. Figure 6: First-generation mediaBlocks system diagram The mediaBlocks 
system was implemented with a Tcl- and [incr Tcl]-based tangible interface software/hardware toolkit 
called 3wish [14]. 3wish includes modules supporting MIDI-based digitizers and synthesizers, Inventor-based 
3D graphics, com­puter vision and magnetic field trackers, etc. 3wish also supports a distributed architecture 
called proxy-distributed or proxdist computation [15], which provides strong abstractions for mixed physical/digital 
systems such as the mediaBlocks interface. Development of 3wish support for the media sequencer and mediaBlock 
slots has consumed a large part of the mediaBlocks effort. The mediaBlock idea of objects as physical 
proxies for online content and computation grew out of early proxdist re­search, and the influence of 
3wish s distributed architecture is visible in the half-dozen computers and tangible interfaces composing 
Figure 6. However, detailed discussion is beyond the scope of this paper, and is left to [14,15] and 
future treatments.  5 PHYSICAL CONTAINERS The paper has emphasized the role of mediaBlocks as contain­ers 
for media, as well as the use of mediaBlock slots for media interchange between various devices. However, 
a range of removable media devices have realized this basic function. For instance, videotapes and floppy 
disks are both physical contain­ers for electronic media that support media interchange through systems 
of physical slots. How do mediaBlocks relate to these well-known technologies? The comparison will be 
explored for floppy disks (and other removable media analogs), which share the ability to store digital 
media of various formats. First, it is clear that mediaBlocks and floppy disks are technically quite 
different. Floppy disks func­tion by taking information offline, recording media onto the disk s own 
storage. MediaBlocks instead work by moving information online, referenced by the internal ID of the 
media-Block object. It is also interesting to note that mediaBlocks transparently support media with 
widely varying bandwidths. For instance, mediaBlocks are equally practical for digital whiteboard and 
digital video recordings, even though the characteristic bit rates differ by five orders of magnitude 
(~100KB vs. ~10GB per hour). From a user interface standpoint, mediaBlocks and floppy disks also have 
a number of differences. Floppy disk contents are accessed indirectly through graphical or textual interaction 
on a host computer. In contrast, mediaBlock contents may be ac­cessed through physical manipulation of 
the mediaBlock object itself. For example, inserting a target mediaBlock into a digital whiteboard s 
slot initiates recording into the block. Similarly, moving a host mediaBlock on our media sequencer s 
position rack allows sequences of images to be navigated (see section 6.1). Building from this distinction, 
mediaBlocks possess a simplicity and lightweight mode of operation rarely found with the floppy disk 
medium. Media recording, playback, and exchange between our example whiteboard, camera, printer, projector, 
and com­puter are all as simple as inserting and withdrawing a media-Block from the respective slots. 
The mediaBlock support for physical media exchange does not force a sneaker-net ethic upon users. Instead, 
mediaBlocks offer the simplicity and directness of physically referencing which data and which device 
when physical proximity is a convenient delimiter. Common reference in absence tasks such as dispatching 
jobs to remote printers for later pick-up or delivery may be supported by shortcut controls (e.g., a 
print button on the whiteboard), or by inserting mediaBlocks into TUI or GUI devices providing remote 
printer access. Thus, mediaBlocks act not as a medium of storage, but rather a mechanism of physical 
reference and exchange. In this sense, the use of mediaBlocks more closely resembles the interactive 
process of copy and paste propagated out into the physical world than the storage medium of floppy disks. 
Conceptually consistent with the premise of tangible user interface, this is a major distinction that 
colors the spectrum of mediaBlocks appli­cations. Also in point of fact, neither floppy disks nor other 
removable media are native to our projector, printer, or whiteboard, nor do we know of these features 
in comparable products. The absence of media drives from these well established and commercially competitive 
products provides market evidence that mediaBlocks serve new or different conceptual roles. 5.1 Implementation 
MediaBlocks are constructed as 5x5x2cm wooden blocks em­bedded with electronic ID tags. First-generation 
blocks are tagged with resistor IDs. New efforts use Dallas Semiconductor iButton devices, which incorporate 
both digital serial numbers and nonvolatile memory. It is worth noting that the physical form of mediaBlocks 
is a product of their intended use, not of technical limitations. Both tag technologies are available 
as surface-mount devices a few millimeters in diameter, rendering block size technically near arbitrary. 
Both resistor- and iButton-based mediaBlocks couple to slots, racks, and other TUI elements with a pair 
of electrical contacts. For resistor-based blocks, ID is determined with a voltage­divider circuit against 
a reference resistor, sampled by an Infu­sion Icube MIDI A/D converter. For the iButton implementation, 
an interface using Microchip s PIC 16F84 microcontroller was implemented, based upon the iRX 2.0 board 
[10]. MediaBlocks operate in conjunction with a number of different interface devices. The most common 
of these is the slot. Slots were prototyped in foamcore, with copper tape contacts. Audio feedback for 
slot entrance, exit, and status events is currently supported by an external MIDI synthesizer. MediaBlock 
phicons are separated from their media contents by several levels of indirection. These mappings include: 
a) physical block . network address b) network address . mediaBlock data structure c) data structure 
element . individual media contents The individual steps of this mapping process are illustrated in Figure 
7. First, insertion of an ID-tagged block (1) is detected electronically by the slot (2). The ID value 
is registered by a computer hosting the slot s tag reader (3), and transmitted as a block entrance event 
to the display device s media manager (4). The slot and media managers could be hosted on the same machine. 
In our implementation, the libraries supporting tag readers and media displays were specific to PC and 
SGI plat­forms, respectively, requiring separate computers for (3) and (4). LOCAL Slot Manager Media 
Manager COMPUTATION ONLINE CONTENT  Figure 7: mediaBlock display flow diagram Once an ID value has been 
obtained for a mediaBlock, the block server (5) provides a data structure describing the block s digital 
contents, presented in Table 1. With the resistor ID scheme, a central block server is responsible for 
mapping block IDs to block data structures for all compatible block devices. However, the resistor-based 
approach does not scale for distributed opera­tion. The iButton-based mediaBlocks solve this problem 
by storing the URL of their hosting block server within internal nonvolatile RAM (4096 bits for our chosen 
model), allowing truly distrib­uted operation. iButtons also support storage of encrypted data, potentially 
useful for authenticating mediaBlocks and granting read or write permissions by a given media device. 
After retrieving the block data structure, the device media man­ager retrieves the specified media contents 
from one or more media servers (6). This content is finally sent to the media display under control of 
display-specific MIME registries (7), similar to Web browser plug-in registries. mediaList: List of contained 
media element addresses physidType: Type of physical ID tag on block physidInst: ID of block tag (usually 
a number) mediaHost: Media stored on media- or block-server? recordBehavior: New media appends or overwrites 
old? lastObservedLocale: Last location block observed lastObservedTime: Timestamp of last block sighting 
blockLabel: Text describing block contents blockLabelColor: Color of paper block label Table 1: mediaBlock 
data structure Earlier sections have discussed the use of mediaBlocks as a medium of exchange between 
graphical and tangible interfaces. This works particularly well in conjunction with Microsoft s Internet 
Explorer 4.0 Internet shortcuts feature (and equivalen­cies provided by other operating systems). Internet 
shortcuts allow distributed online media (e.g., URL-referenced HTTP sources) to be manipulated by the 
Windows95 desktop and applications indistinguishably from files on local disk drives. We have explored 
the synthesis of Internet Shortcuts from media elements dragged out of monitor-slot mediaBlocks with 
GUI drag and drop. This combination represents a significant step towards truly seamless integration 
between the online media spaces of graphical and tangible interfaces.   6 PHYSICAL CONTROLS The previous 
section discussed the physical containment, trans­port, and interchange aspects of mediaBlocks. The section 
also noted that in this capacity, the use of mediaBlocks more closely resembles the software process 
of copy and paste than the storage function of floppy disks. However, the earlier section did not discuss 
how users might actively manipulate mediaBlock contents. For example, while both video decks and MS Windows95 
CD-ROM drives support auto-launch capabilities analogous to mediaBlock slot play­back, these systems 
also provide additional controls for more sophisticated interactions. Following these examples, we might 
model interfaces on the physical play/stop buttons and jog/shuttle wheels of VCRs, as we have done with 
our media browser. We might also use tradi­tional graphical interfaces to manipulate mediaBlock contents, 
as we have done with the GUI monitor slot. At the same time, it is interesting to explore new interface 
possibilities specific to tangible user interfaces. The slot-based copy and paste functionality illustrates 
the first step of such an approach, binding digital semantics to the insertion of medi­aBlocks into physical 
slots. We have carried this approach forward in our work with the media sequencer interface. Here, we 
introduce racks, stacks, chutes, and pads as physical constraints that enable mediaBlocks to act as physical 
controls for directly manipulating block con­tents (Fig. 8). Figure 8: Media sequencer physical constraints, 
used in combination with mediaBlocks as physical controls The mediaBlock rack is the primary physical 
constraint elements used in the media sequencer. The mediaBlocks rack was in­spired by the Scrabble board 
game s combination of letter tiles and the tile rack. In Scrabble, the rack allows letter tiles to be 
rapidly inserted, sequenced, and grouped into meaningful pat­terns. In the sequencer context, these physical 
attributes of position, sequence, and adjacency may be digitally recast as indexing, sequencing, and 
Boolean AND/OR operations, respec­tively. 6.1 User Interface The media sequencer prototype is illustrated 
in Figures 5, 9, and 10. Sequencer operation is dominated by two physical constraint structures: the 
position rack and sequence rack. When a mediaBlock is placed in the position rack, its contents are shown 
on the sequencer display as a perspective wall [9]. The focus position of the perspective wall is interactively 
con­trolled by the relative position of the mediaBlock on the position rack (Figure 10). Moving the block 
to the rack s left edge moves the perspective wall to focus on the block s first element. Simi­larly, 
the right edge of the position rack corresponds to the block s last element. The combined position rack 
and perspective wall serve several purposes. First, they support the interactive viewing of media-Block 
contents with imaging of both detail and context [9]. Secondly, they allow the position rack to select 
an individual media element for copying between mediaBlocks. Towards this end, a destination mediaBlock 
can be placed in the sequencer s target pad, physically adjacent the position rack (see Figure 9). Pressing 
the block on the target pad will append the currently selected media element into the destination block. 
This is confirmed with audio feedback and a haptic click, as well as an animation of the selected media 
transferring into the target block. Pressing and holding the block, followed by moving the source mediaBlock 
to a new location, will copy a range of ele­ments into the target block (analogous to dragging out a 
selection range with a mouse). Figure 9: Media sequencer components We believe this concept of using 
mediaBlock phicons to physi­cally manipulate their internal contents generalizes to a powerful new interaction 
technique. Here, we are using mediaBlocks as physical controls for directly acting upon their internal 
state. The nearest equivalent in GUIs might be dragging a graphical folder icon across a scrollbar to 
index through folder contents. While a somewhat bizarre behavior for the GUI, we believe that use of 
mediaBlocks as physical controls holds substantial prom­ise in the TUI context. The sequence rack extends 
the control functionality of media-Blocks. This rack allows the user to combine the contents of multiple 
mediaBlocks into a single sequence, which can then be associated with a new mediaBlock carrier on the 
target pad. When a mediaBlock is placed onto the sequence rack, its con­tents scroll out of the block 
into the sequencer display space (Figure 9). Multiple mediaBlocks may be arranged to construct a new 
sequence. associated with movement of mediaBlock on position rack; a compact disk s music is contained 
within this block Both the sequencer screen and target pad are shared between the position and sequence 
racks. When a mediaBlock is located on the position rack, the target pad is bound to selections from 
the perspective wall display. When the position rack is clear, the target pad is associated with recording 
of the sequence rack s aggregate elements. The use of mediaBlocks as physical controls services only 
part of the sequence rack s behavior. Especially when a source media-Block contains many elements, navigating 
the perspective wall by incremental steps may be more convenient than using the position rack. The position 
wheel supports such incremental navigation. Haptic detents provide the user with physical feed­back, 
where each detent corresponds with movement of one media element.  6.2 Implementation The media sequencer 
platform was assembled of wood and acrylic, and embedded with a 1280x1024-pixel 32cm-diagonal flat panel 
display. Position and sequence rack sensing was performed with a custom contact-grid board. The position 
wheel was tracked with a shaft encoder. Sensor inputs were digitized with the Infusion Systems Icube 
device, and acquired through the 3wish extensions to the Tcl language [14] using C++ wrappers over the 
Rogus MIDI interface suite [4]. The position rack perspective wall and other graphical visualizations 
were written with 3wish s Open Inventor-based routines, and executed on an SGI Octane workstation.  
7 DISCUSSION While mediaBlocks make progress towards broader vocabularies for tangible interface design, 
the project also illustrates some of the challenges in designing TUIs. As a case in point, we con­sider 
some of the design decisions leading to our sequencer prototype s current form. Some readers have questioned 
the sequencer s integration of a central graphic display, given our research focus upon tangible interface. 
While the sequencer indeed integrates a flat-panel display, we argue that the design is the product of 
a particular interface task and set of design constraints. Our motivating task was the sequencing of 
presentation media, especially images and video. Our interface inspirations were the Scrabble tile/rack 
constraint system, and the brick/tray function of [5]. As our task centered upon manipulation of visual 
media, a graphic display of some sort was essential. Our original hope was to integrate this display 
into a rack s footprint, displaying the graphical contents of transparent media-Blocks in a fashion following 
the metaDESK s passive lens [15]. However, given that mediaBlocks usually contain multiple media elements, 
we had difficulty determining an effective method of display within a block s 5x5cm footprint. Thus, 
we decided upon a visual display external to the mediaBlock/rack system, while maintaining visual contiguity 
with the associated medi­aBlock container. Here, we explored use of back-projected, front-projected, 
and integrated displays. While each had advantages, we selected a 32cm-diagonal 1280x1024-pixel integrated 
flat panel display on the basis of resolution, dot pitch, and compactness of integration. We wished adequate 
display resolution to support the visually­intensive task. We were also interested in a relatively small, 
compact device, even at the cost of greater display real estate such as provided by the metaDESK [15]. 
This was because we wished to make extensive use of physical constraints to support the sequencing task. 
Simultaneously, we imagined our users making simultaneous use of multiple com­plementary devices, such 
as the combination of a general-purpose computer for authoring new content, and the sequencer for assembling 
and manipulating the presentation. These usage constraints, coupled with the drive for proximity between 
physi­cal controls and visual displays, drove the system to its current form. Aspects of the sequencer 
design remain challenging, including potential inconsistency between the position wheel and rack; linkage 
between rack-based mediaBlocks and screen-based displays; and the shared screen use by position and sequence 
racks. Additionally, it is possible that the position rack + media-Block selection mechanism is less 
efficient than (say) directly selecting contents with one s finger. A second-generation se­quencer design 
is under development to add this direct content­selection ability, increase display real estate, and 
rationalize the behavior of sequencer racks, while maintaining mediaBlock controls for sorting, sequencing, 
and transporting media into and out of the sequencer. We believe mediaBlocks underlying containment and 
transport functions are fundamentally sound. MediaBlocks use as physi­cal controls makes a significant 
extension to this transport function, and has been demonstrated useful in tasks like the video figure 
s presentation example. Thus, while less well­established than the transport function, our implementation 
leaves us optimistic about the interface potential of phicon controls. 8 FUTURE WORK While part of our 
research motivation lies in seeking new para­digms for interface outside the well-explored GUI context, 
it is interesting to explore parallels between graphical and physical interaction techniques. One such 
instance is the historical emergence of consistent interface behavior across multiple GUI applications, 
notably articulated in the Apple Human Interface Guidelines [1]. Our design of mediaBlock slots and sequencer 
constraints has been shaped by an interest in TUI analogs for GUI inter­application behaviors. Our use 
of mediaBlocks for media trans­port and interchange develops a physical analog of the GUI copy-and-paste 
functionality. Similarly, the sequencer s racks, chute, and other constraints have parallels to GUI interaction 
primitives (e.g., desktop-based clicking, dragging, etc. of icons), without directly embodying GUI widgetry 
as with the metaDESK [15]. As much of the TUI appeal lies in a diversity of physical em­bodiments to 
address specific interface tasks, it is unclear how far analogies to [1] might extend. Nonetheless, prospects 
for consistent TUI interface vocabularies and widespread TUI/GUI interoperability are highly attractive. 
Finally, mediaBlock s online aspect suggests their ability to contain live, streaming content: in other 
words, the ability to operate as media conduits. We have demonstrated medi­aBlocks containing both streaming 
media sources such as Re­alAudio and RealVideo media, as well as pairs of medi­aBlock conduits which 
together broadcast and receive streaming video. At the same time, the conduit functionality of mediaBlocks 
represents a significant conceptual expansion with its own user interface questions. For instance, if 
a user wishes to both record and broadcast a whiteboard session, or both display and store a live video 
stream, how are these aggregate behaviors best ac­commodated? These and other open questions remain. 
As a result, we leave conduits for further discussion in future work. 9 CONCLUSION We have presented 
a system of tangible user interface based upon mediaBlocks: small, electronically tagged wooden blocks 
that serve as physical containers, transports, and controls for online media. MediaBlocks do not store 
media internally, instead serving as phicons (physical icons) which give physical embodiment to online 
media. MediaBlocks provide a mechanism for seamlessly exchanging digital contents between diverse media 
devices, including media sources, displays, general-purpose computers, and specialized tangible interfaces. 
Towards these ends, we have demonstrated the ability of mediaBlocks to physically copy media from a whiteboard 
and camera source, and paste this content into a printer and projector. We have also shown the use of 
mediaBlock slots as physical gateways between tangible and graphical interfaces, allowing media to be 
swiftly exchanged with traditional GUIs. Additionally, mediaBlocks are used as physical controls for 
operating upon their own digital contents. We have demon­strated this ability in the media sequencer 
by swiftly composing a presentation integrating video, photographs, whiteboard record­ings, and text. 
While the sequencer supports this task with a graphical display, the entire task is accomplished without 
a keyboard, pointer, or cursor, except for the GUI-based entry of the textual slide. Finally, we have 
discussed analogs between TUI media exchange between diverse physical devices, and GUI support for consistent 
multi-application operation and communication. Similarly, we have demonstrated new roles and visions 
for seamless interaction with online content beyond the traditional GUI context. In conclusion, we believe 
mediaBlocks are a powerful tangible interface for the seamless exchange and manipulation of online content 
between diverse media devices and people. More gener­ally, we believe mediaBlocks represent a step towards 
broader tangible interface use as an interaction technique uniting people, computational media, and the 
physical environment. 10 ACKNOWLEDGEMENTS Many people have contributed to this work. John Alex and Paul 
Grayson helped with software and hardware implementation. John Underkoffler and Paul Yarin assisted video 
production. Andrew Dahley and Andrew Hsu provided design assistance. Matt Gorbet, Scott Brave, and Victor 
Su provided additional helpful input. Ben Denckla helped with early Rogus efforts. Paul Grayson, Hannes 
Vilhjalmsson, Joe Marks, Tara Rosenber­ger, and others provided feedback on paper drafts. This research 
was sponsored in part by the MIT Media Lab s Things That Think consortium and a Mitsubishi fellowship. 
11 REFERENCES [1] Apple Computer, Inc. Apple Human Interface Guidelines: The Apple Desktop Interface. 
New York: Addison Wesley, 1987. [2] Brave, S., and Dahley, A. inTouch: A Medium for Haptic Interpersonal 
Communication. In CHI 97 Extended Ab­stracts, pp. 363-364. [3] Crampton Smith, G. The Hand That Rocks 
the Cradle. I.D., May/June 1995, pp. 60-65. [4] Denckla, B. Rogus C++ MIDI Suite. http://theremin.media.mit.edu/rogus/ 
 [5] Fitzmaurice, G., Ishii, H., and Buxton, W. (1995). Bricks: Laying the Foundations for Graspable 
User Interfaces. Proc. of CHI 95, pp. 442-449. [6] Fitzmaurice, G. Graspable User Interfaces. Ph.D. Thesis, 
University of Toronto, 1996. [7] Gorbet, M., Orth, M., and Ishii, H. Triangles: Tangible Interface for 
Manipulation and Exploration of Digital Infor­mation Topography. In Proc. of CHI 98. [8] Ishii, H., and 
Ullmer, B. Tangible Bits: Towards Seamless Interfaces between People, Bits, and Atoms. In Proc. of CHI 
97, pp. 234-241. [9] Mackinlay, J., Robertson, G., &#38; Card, S. The Perspective Wall: Detail and context 
smoothly integrated. In Proc. of CHI'91, pp. 173-179. [10] Poor, R. The iRX 2.0 ...where Atoms meet Bits. 
http://ttt.media.mit.edu/pia/Research/iRX2/ [11] Rekimoto, J. Pick-and-Drop: A Direct Manipulation Technique 
for Multiple Computer Environments. In Proc. of UIST 97, pp. 31-39. [12] Resnick, M., Berg, F., et al. 
Digital Manipulatives: New Toys to Think With. In Proc. of CHI 98. [13] Suzuki, H., Kato, H. AlgoBlock: 
a Tangible Programming Language, a Tool for Collaborative Learning. In Proc. of 4th European Logo Conference, 
Aug. 1993, Athens Greece, pp. 297-303. [14] Ullmer, B. 3wish: Distributed [incr Tcl] Extensions for 
Physical-World Interfaces. In Proc. of Tcl/Tk 97, pp. 169­ 170. [15] Ullmer, B., and Ishii, H. The metaDESK: 
Models and Prototypes for Tangible User Interfaces. In Proc. of UIST 97, pp. 223-232. [16] Weiser, M. 
The Computer for the 21st Century. In Scien­tific American, 265(3), pp. 94-104.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280942</article_id>
		<sort_key>387</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>38</seq_no>
		<title><![CDATA[Non-uniform recursive subdivision surfaces]]></title>
		<page_from>387</page_from>
		<page_to>394</page_to>
		<doi_number>10.1145/280814.280942</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280942</url>
		<keywords>
			<kw><![CDATA[B-splines]]></kw>
			<kw><![CDATA[Catmull-Clark surfaces]]></kw>
			<kw><![CDATA[Doo-Sabin surfaces]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.4</cat_node>
				<descriptor>Iterative methods</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003724</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Numerical differentiation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003740</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Quadrature</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P281616</person_id>
				<author_profile_id><![CDATA[81100400673]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Sederberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brigham Young Univ., Provo, UT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP33034686</person_id>
				<author_profile_id><![CDATA[81452599700]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jianmin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zheng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brigham Young Univ., Provo, UT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P63729</person_id>
				<author_profile_id><![CDATA[81100511771]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sewell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sewell Development, Provo, UT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14112529</person_id>
				<author_profile_id><![CDATA[81100307545]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Malcolm]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sabin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Numerical Geometry Ltd., Cambridge, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[A A Ball and D J T Storry. Recursively Generated B- spline Surfaces. Proc. CAD84, pages 112-119, 1984. ISBN 0408 01 4407.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>42459</ref_obj_id>
				<ref_obj_pid>42458</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[A A Ball and D J T Storry. Conditions For Tangent Plane Continuity Over Recursively Generated B-spline Surfaces. ACM ToG, 7:83-102, 1988.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[W Boehm. Inserting New Knots Into B-spline Curves. Computer-Aided Design, 12:199-201, 1980.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[E Catmull and J Clark. Recursively Generated B-spline Surfaces On Arbitrary Topological Meshes. Computer- Aided Design, 10:350-355, 1978.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[G Chaikin. An Algorithm For High-speed Curve Generation. Computer Graphics and Image Processing, 3:346-349, 1974.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[E Cohen, T Lyche, and R F Riesenfeld. Discrete B-splines And Subdivision Techniques In Computer Aided Design And Computer Graphics. Computer Graphics and Image Processing, 14:87-111, 1980.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>36723</ref_obj_id>
				<ref_obj_pid>36713</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[C de Boor. Cutting Corners Always Works. Computer Aided Geometric Design, 4:125-131, 1987.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[D Doo and M Sabin. Behaviour Of Recursive Division Surfaces Near Extraordinary Points. Computer-Aided Design, 10:356-360, 1978.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[N Dyn, D Levin, and J A Gregory. A 4-point Interpolatory Subdivision Scheme For Curve Design. Computer Aided Geometric Design, 4:257-268, 1987.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[G Farin. Designing C1 Surfaces Consisting Of Triangular Cubic Patches. Computer-Aided Design, 14:253- 256, 1982.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[A R Forrest. Notes On Chaikin's Algorithm. Technical Report Memo CGP74/1, University of East Anglia, Norwich, UK, 1974.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166121</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[M Halstead, M Kass, and T DeRose. Efficient, Fair Interpolation Using Catmull-Clark Surfaces. Computer Graphics (SIGGRAPH 93 Conference Proceedings), 27:35-44, 1993.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192233</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[H Hoppe, T DeRose, T Duchamp, M Halstead, H Jin, J McDonald, J Schweitzer, and W Stuetzle. Piecewise Smooth Surface Reconstruction. Computer Graphics (SIGGRAPH 94 Conference Proceedings), 28:295- 302, 1994.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[L Kobbelt. Interpolatory Subdivision On Open Quadrilateral Nets With Arbitrary Topology. Computer Graphics Forum (Eurographics 96), 1996.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[C Loop. Smooth Subdivision Surfaces Based On Triangles. Master's thesis, University of Utah, Dept. of Mathematics, 1987.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192238</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[C Loop. Smooth Spline Surfaces Over Irregular Meshes. Computer Graphics (SIGGRAPH 94 Conference Proceedings), 28:303-310, 1994.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[A Nasri. Polyhedral Subdivision Methods For Freeform Surfaces. PhD thesis, University of East Anglia, 1984.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>27628</ref_obj_id>
				<ref_obj_pid>27625</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[A Nasri. Polyhedral Subdivision Methods For Freeform Surfaces. ACM ToG, 6:29-73, 1987.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>116112</ref_obj_id>
				<ref_obj_pid>116105</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[A Nasri. Surface Interpolation On Irregular Network With Normal Conditions. Computer Aided Geometric Design, 8:89-96, 1991.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>255783</ref_obj_id>
				<ref_obj_pid>255780</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[A Nasri. Curve Interpolation In Recursively Generated B-spline Surfaces Over Arbitrary Topology. Computer Aided Geometric Design, 14:13-30, 1997.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>172330</ref_obj_id>
				<ref_obj_pid>172315</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[J Peters. Joining Smooth Patches Around A Vertex To Form A Ck Surface. Computer Aided Geometric Design, 9:387-411, 1992.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>172389</ref_obj_id>
				<ref_obj_pid>172372</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[J Peters. Smooth Free-form Surfaces Over Irregular Meshes Generalizing Quadratic Splines. Computer Aided Geometric Design, 10:347-361, 1993.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>207497</ref_obj_id>
				<ref_obj_pid>207475</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[J Peters. C1 Surface Splines. SlAM J Num Anal, 32:645-666, 1995.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>248304</ref_obj_id>
				<ref_obj_pid>248299</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[H Prautzsch. Freeform Splines. Computer Aided Geometric Design, 14:201-206, 1997.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>211166</ref_obj_id>
				<ref_obj_pid>211163</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[U. Reif. A Unified Approach To Subdivision Algorithms Near Extraordinary Vertices. Computer Aided Geometric Design, 12:153-174, 1995.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[R F Riesenfeld. On Chaikin's Algorithm. Computer Graphics and Image Processing, 4:304-310, 1975.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[M A Sabin. Recursive Division. In J Gregory, editor, The Mathematics of Surfaces, pages 269-282. Clarendon Press, Oxford, 1986. ISBN 0 19 853609 7.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>114245</ref_obj_id>
				<ref_obj_pid>114172</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[M A Sabin. Cubic Recursive Division With Bounded Curvature. In P J Laurent, Ale Mehaute, and L L Schumaker, editors, Curves and Surfaces, pages 411-414. Academic Press, 1991. ISBN 0 12 438660 1.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>64570</ref_obj_id>
				<ref_obj_pid>64567</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[D J T Storry and A A Ball. Design Of An N-sided Surface Patch From Hermite Boundary Data. Computer Aided Geometric Design, 6:111-120, 1989.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[J Zheng, T Sederberg, and M A Sabin. Eigenanalysis Of Non-Uniform Doo-Sabin Surfaces. Technical report, Brigham Young University, Department of Computer Science (appears as an appendix in the electronic version of this paper), November 1997.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237254</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[D Zorin, P Schr6der, and W Sweldens. Interpolating Subdivision For Meshes With Arbitrary Topology. Computer Graphics (SIGGRAPH 96 Conference Proceedings), 30:189-192, 1996.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Non-Uniform Recursive Subdivision Surfaces Copyright &#38;#169;1998 by the Association for Computing 
Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal or 
classroom use is granted without fee provided that copies are not made or distributed for profit or commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for components 
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy 
otherwise, to republish, to post on servers or to redistribute to lists, requires specific permission 
and/or a fee. Thomas W. Sederberg1 David Sewell3 Malcolm Sabin4 Jianmin Zheng2 Sewell Development Numerical 
Geometry Ltd. Brigham Young University Provo, Utah Cambridge, UK Abstract Doo-Sabin and Catmull-Clark 
subdivision surfaces are based on the notion of repeated knot insertion of uniform tensor product B-spline 
surfaces. This paper develops rules for non-uniform Doo-Sabin and Catmull-Clark surfaces that generalize 
non-uniform tensor product B­spline surfaces to arbitrary topologies. This added .exibility allows, among 
other things, the natural introduction of features such as cusps, creases, and darts, while elsewhere 
maintaining the same order of con­tinuity as their uniform counterparts. Categories and Subject Descriptors: 
I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling surfaces and object representations. 
Additional Key Words and Phrases: B-splines, Doo-Sabin surfaces, Catmull-Clark surfaces.  1 INTRODUCTION 
Tensor product non-uniform rational B-spline surfaces have become an industry standard in computer graphics, 
as well as in CAD/CAM systems. Because surfaces of arbitrary topological genus cannot be represented 
using a single B­spline surface, there has been considerable interest in the generalization, based on 
knot insertion, called recursive sub­division, which removes this limitation. However, despite being 
based on knot insertion, the re­cursive subdivision techniques published so far are the ana­logues of 
equal interval, uniform B-splines rather than of non-uniform B-splines. 1tom@byu.edu 2zheng@cs.byu.edu 
(On leave from Zhejiang University) 3dave@sewelld.com 4malcolm@geometry.demon.co.uk This paper explores 
the possibility of achieving the extra .exibility of unequal knot intervals in a recursive subdivi­sion 
scheme including, for example, the ability to express features such as creases and darts by simply setting 
some of the knot intervals to zero. Schemes are presented for achiev­ing non-uniform Doo-Sabin and Catmull-Clark 
surfaces. We will refer to these collectively as Non-Uniform Recursive Subdivision Surfaces (NURSSes). 
Figure 1 (left) shows a Doo-Sabin surface, and Figure 1 (right) shows an example of a non-uniform Doo-Sabin 
sur­face in which the knot spacings along certain control edges have been set to zero (as labeled), thereby 
creating a G0dis­continuity along the oval edge on the left. Figure 2 shows Figure 1: Uniform and non-uniform 
Doo-Sabin surfaces. two non-uniform Catmull-Clark surfaces. The one on the left contains a dart formed 
by setting two pairs of control­edge knot spacings to zero. The one on the right shows shape modi.cation 
induced by changing the knot spacing along the top edges to 10 and along the center horizontal edges 
to 0.1. The control net used here has the topology of a B-spline con­trol net, but these shapes cannot 
be obtained using NURBS or uniform Catmull-Clark surfaces. 1.1 Background The concept of image space 
subdivision as a graphics tech­nique had been around for a long time when recursive sub­division appeared 
as an object de.nition technique. The .rst Figure 2: Non-uniform Catmull-Clark surfaces. relevant result 
in parametric space subdivision of sculptured surfaces was the de Casteljau algorithm, which both evalu­ated 
a point on a B´ezier curve and provided the control points for the parts of the curve meeting there. 
This was general­ized to B-splines in the form of the Oslo algorithm [6] and Boehm subdivision [3] and 
used as a basis for interrogation methods applying parametric space subdivision. However, what sparked 
the imagination of the graphics and modeling communities in 1975 was a much more spe­ci.c subdivision 
of a quadratic B-spline, proposed by Chaikin as a curve rendering technique [5], and recognized for what 
it was by Forrest [11] and by Riesenfeld [26]. It later turned out that the concept of a curve being 
the limit of a polygon under the operation of cutting off the corners had been ex­plored by de Rham in 
the 1940s and 50s. His results were translated into modern terminology by de Boor [7]. It was quickly 
appreciated that the curve ideas could give surface techniques just by applying the concept of tensor 
product, but the important key concept, that subdivision could overcome the rigid rectangular partitioning 
of the paramet­ric domain one of the major limitations of tensor products was reached more or less simultaneously 
by Catmull and Clark [4] and by Doo and Sabin [8]. Since then there have been .ve major directions of 
development: 1. The analysis of what happens near an extraordinary point, started in the Doo-Sabin paper 
[8], was taken up by Ball and Storry. This led to an optimization of the coef.cients for the cubic case 
[1, 2] which unfor­tunately missed one of the possible variations, and the task was completed by Sabin 
[28], in a paper which also identi.ed that a cubic construction could never give full G2continuity at 
the singular points, and that continuity at such points was in fact a much more com­plicated question 
than had been assumed. Further anal­ysis was carried out by Reif [25]. The nature of the behavior around 
the extraordinary points is now well understood.  2. Constructions based on box-splines, rather than 
on ten­sor products, were explored by Farin [10] and by Loop [15], and a collection of possible constructions 
was assem­bled by Sabin [27].  3. Constructions that interpolate the control points were explored by 
Dyn, Gregory and Levin [9], and an im­proved scheme was derived by Zorin, Schr¨oder and Sweldens [31]. 
Kobbelt [14] proposed an alternative for quadrilateral nets with arbitrary topology. The sim­pler ones 
in this category can be viewed as duals of quadratic B-spline constructions. 4. Nasri [17] studied the 
problems of ef.cient implemen­tation and practical edge-conditions and extended this to modi.cations 
of the basic technique to achieve var­ious interpolation conditions [18, 19, 20]. Halstead, Kass, and 
DeRose showed that a fairness norm could be computed exactly for Catmull-Clark surfaces [12], enabling 
the determination of more fair limit surfaces. 5. The idea of using just a small number of subdivision 
steps, and then using n-sided combinations of patches to .ll in a con.guration made more regular in some 
sense by those steps, was explored by Loop [16], Pe­ters [21, 22, 23] and Prautzsch [24]. Ball and Storry 
[29] took the opposite line, of using subdivision to de.ne an n-sided patch.  What was not explored 
until now was that the general topology subdivision schemes were as rigid as the equal in­terval splines 
from which they were derived.  1.2 Overview Section 2 reviews knot-doubling for non-uniform B-spline 
curves of degree two and three and introduces a simple ap­proach for labeling the knot intervals on the 
control polygon an idea that is crucial for the extension to subdivision sur­faces. Section 3 then gives 
the corresponding expression for knot doubling of non-uniform tensor product B-spline sur­faces.Section 
4 proposes subdivision rules for non-uniform Doo-Sabin and Catmull-Clark surfaces, which reduce to non­uniform 
B-spline surfaces when the control net is a rectan­gular grid and when all knot intervals along every 
given row and column are the same. A continuity analysis is given in section 5, showing that non-uniform 
Doo-Sabin surfaces are G1and non-uniform Catmull-Clark surfaces are generally G2,but G1at certain points. 
Section 6 makes some obser­vations on NURSSes, and offers a conclusion.  2 CURVE KNOT DOUBLING For a 
quadratic periodic B-spline curve, each vertex of the control polygon corresponds to a single quadratic 
curve seg­ment. It is convenient then to express the knot vector by writing the knot interval diof each 
curve segment next to its corresponding control vertex Pi. If a new knot is inserted at the midpoint 
of each current knot interval, the resulting control polygon has twice as many control points, and their 
coordinates Qkare: (di+2di+1)Pi+diPi+1 Q2i= 2(di+di+1) di+1Pi+(2di+di+1)Pi+1 Q2i+1= (1)2(di+di+1) as 
illustrated in Figure 3. Pi Pi-1 Pi+1 di-1 di+1 Figure 3: Non-uniform quadratic B-spline curve. For cubic 
periodic B-spline curves, each edge of the con­trol polygon corresponds to a single cubic curve segment, 
and so we write the knot intervals adjacent to each edge of the control polygon. The equations for the 
new control points Qkgenerated upon inserting a knot midway through each knot interval are: (di+2di+1)Pi+(di+2di,1)Pi+1Q2i+1=(2)2(di,1+di+di+1) 
diQ2i,1+(di,1+di)Pi+di,1Q2i+1 Q2i= (3)2(di,1+di) as showninFigure4. Pi di-1 Q2i+1 Q2i-1 di-1/2 di/2 
di Pi-1 Pi+1 di-2 di+1 Figure 4: Non-uniform cubic B-spline curve.  3 SURFACE KNOT DOUBLING The knot-doubling 
formulae for B-spline curves extend eas­ily to surfaces. Non-uniform B-spline surfaces are de.ned in 
terms of a control net that is topologically a rectangular grid, for which all horizontal knot vectors 
are scales of each other, and all vertical knot vectors are scales of each other. 3.1 Quadratic Case 
The formulae for the new control points FAcan be written in Doo-Sabin form, which is signi.cant because 
in this form the new control points are seen as being in groups, creating a new face in each old face, 
and the vertices of each such new face are in 1:1 correspondence with the vertices of the old, whereas 
under the tensor product form we merely see all the new vertices as forming a new regular array (see 
Figure 5). V+Aac(B+C,A,D) FA=+ ;(4) 24(ad+ac+bc+bd) where bdA+adB+bcC+acD V= :(5) bd+ad+bc+ac Figure 
5: Knot doubling, quadratic B-spline.  3.2 Cubic Case For non-uniform cubic B-spline surfaces, the re.nement 
rules can be written as follows (see Figure 6). First, each face is replaced with a new vertex Fi. For 
example, F1=[(e3+2e4)(d2+2d1)P0+(e3+2e4)(d2+2d3)P1 +(e3+2e2)(d2+2d3)P5+(e3+2e2)(d2+2d1)P2J /[4(e2+e3+e4)(d1+d2+d3)J:(6) 
Then, each edge is split with an edge vertex Ei,e.g. e2F1+e3F4+(e2+e3)M1 E1= ;(7) 2(e2+e3) where (2d1+d2)P0+(d2+2d3)P1 
M1=:(8) 2(d1+d2+d3) Finally, each original control point is replaced with a ver­tex point V P0d3e2F1+d2e2F2+d2e3F3+d3e3F4 
V=+ + 44(d2+d3)(e2+e3)[d3(e2+e3)M1+e2(d2+d3)M2+d2(e2+e3)M3 +e3(d2+d3)M4J/[4(d2+d3)(e2+e3)J:(9) e4 e4 
e4 P5 P2 P6 d1 d2 d3 d4 e3 F1e3 M2 F2 e3E2E3 E1P1 M1 P0 M3 P3 d1 d2 d3 d4 MV 4 F3e2 F4 e2 e2 E4 d1 P8 
P4 P7 d2 d3 d4e1 e1 e1 Figure 6: Face, edge and vertex points.   4 NURSS REFINEMENT In uniform B-spline 
surfaces, all knot spacings are the same. Doo-Sabin and Catmull-Clark proposed generalizations of uniform 
B-spline surface schemes that allow for vertices of the control mesh to have valence other than four, 
and the faces of the control mesh to have other than four sides. Their subdivision rules were designed 
such that when the control mesh happens to be a rectangular grid, the subdivision rules are equivalent 
to knot doubling of uniform B-spline surfaces. The subdivision surfaces are then de.ned as the limit 
of the control meshes when these subdivision rules are applied an in.nite number of times. We here de.ne 
generalizations of non-uniform B-spline surfaces. As in the cubic curve case, each edge in the con­trol 
polyhedron of a non-uniform Catmull-Clark surface is assigned a knot spacing. For a non-uniform Doo-Sabin 
sur­face, each vertex is assigned a knot spacing (possibly dif­ferent) for each edge radiating from it. 
Our objective is to devise a set of re.nement rules for NURSSes such that if all knot intervals are equal, 
the quadratic NURSS reduces to Doo-Sabin and the cubic NURSS reduces to Catmull-Clark. There are actually 
two distinct rules to be devised. First, we need to revise the Doo-Sabin and Catmull-Clark rules for 
the new point coordinates, taking the knot spacings into ac­count. Second, we need rules for determining 
the new knot spacings. Note that NURSS could just as well stand for Non-Uniform Rational Subdivision 
Surfaces, because it is a sim­ple matter to .rst project rational control points to 4-D, then apply our 
rules, and .nally to project back to 3-D. In this section, bold capital letters stand for points, and 
non-bold typeface for knot spacings. The indices for knot spacing dkindicate that the spacing pertains 
to an edge with ijPias one endpoint. Referring to Figure 8, the notation d0 ijindicates the knot spacing 
for edge Pi Pj. Rotating counter-clockwise about Pi, d1denotes the knot spacing ij for the .rst edge 
encountered, d2indicates that of the sec­ ij ond edge, etc. For the cubic case, each edge has a single 
knot spacing, so d0=d0 . ijji 4.1 Quadratic Case In the quadratic case, re.nement proceeds in a manner 
iden­tical to Doo-Sabin subdivision: A polyhedron spawns a re­.ned polyhedron for which new faces (of 
type F, type E and type V respectively) are created for each face, edge, and ver­tex of the previous 
polyhedron. During the subdivision step, each face is replaced by a new face connected across the old 
edges and across the old vertices by other new faces. In such re.nement schemes, the extraordinary points 
are at the cen­ter of n-sided faces with n6=4. After one iteration, every vertex of the new polyhedron 
will have valence four, and the number of faces with other than four sides will remain con­stant. Refer 
to Figure 7 for labels. The new vertex dis com- Pi puted: d=+(d0 +d0 Pi V+Pii+1;i+2di0+3;i+2i,1;i,2di0 
,3;i,2) 2 (()) Pn 27ji,jj ,nPi+j=11+2cosnPj Pn (10) 8d0d0 k=1k,1;kk+1;k where Pn d0d0 Pk k=1k,1;kk+1;k 
V= Pnd0d0: k=1k,1;kk+1;k Figure 7: Quadratic re.nement rules. 4.1.1 New knot spacings New knot spacings 
ddijkcan be speci.ed in numerous ways. Here are two straightforward options: dd0 dd,1d0 == i;i+1i;i,1i;i+1/2 
dd0 dd1 d0 == i;i,1i;i+1i;i,1/2 or dd0 =d0 dd,1 =(d0+d,1)/4 i;i+1i;i+1/2;i;i,1i;i+1i;i,1 dd0 d0 dd1 d0+d1 
==( The former allows the re.nement matrix to remain constant after a few iterations. The latter seems 
to produce more sat­isfactory shapes.  4.2 Cubic Case =d1d,1 (18) fij0i0j Our development parallels 
that for Catmull-Clark surfaces. n,3Pn c=if (mi+fi;i+1)6=0(19) As shown in Figure 8, the face point 
for a face with nsides ni=1is computed as otherwise, c=1. Pn,1 i=0wiPi F= P; (11) n,1 wi 4.2.1 New 
knot spacings i=0 where Each n-sided face is split into nfour sided faces, whose knot spacings are determined 
as shown in Figure 10. wi =(d0 i+1;i+d2 i+1;i+d,2+d0 i,2;i,1+d2 i,2;i,1)i+1;ii,2;i,1+d,2 P4 (d0 i,1;i+d2 
i,1;i+d,2+d0 i+2;i+1+d,2)(12) i,1;ii+2;i+1+d2 i+2;i+1 P3 P0  P2 P1 d12  5 CONTINUITY ANALYSIS Figure 
8: Face point. The edge point is computed (see Figure 9a): E=(1,.ij,.ji)M+.ijFij+.jiFji;(13) where d1+d,1 
.ij = jiij (14)2(d1+d,1+d,1+d1) jiijjiij if d1+d,1+d,1+d16=0and .ij =0otherwise. jiijjiij (d0+d2+d,2)Pi+(d0+d2+d,2)Pj 
jijijiijijij M= (15) d0+d2+d,2+d0+d2+d,2 jijijiijijij For each construction, we consider the behavior 
of the limit surface for all the knot spacings being positive. 5.1 Quadratic case 5.1.1 Limit surface 
structure After one iteration, all type V and type E faces are four­sided, with the property that the 
intervals crossing the orig­inal edges are equal as seen from the two sides. Thus each such type V face 
reduces to a mesh that is equivalent to a uni­form biquadratic B-spline, while each type E face reduces 
to a mesh that is equivalent to a non-uniform biquadratic B­spline. This leaves only the regression in 
the type F faces to  P0 Mi structure as shown in Figure 11a. The face is four-sided in analyze for 
continuity. Pi The limit surface for a face consists of patches in the 0dij Eij VFi,i+1 this example. 
Mi+1 Figure 9: a) Edge point. b) Vertex point. The vertex point for a point of valence nis expressed 
(see Figure 9b): Pn3(miMi+fi;i+1Fi;i+1) a. b. V=cP0+i=1Pn ;(16) ni=1(mi+fi;i+1) Figure 11: a) Sequence 
of quadratic polynomial pieces for a where Miare de.ned as (15), Fi;i+1as (11), and four-sided face; 
b) Con.guration surrounding type F face. mi =(d1+d,1)(d2+d,2)/2 (17) 0i0i0i0i  5.1.2 Continuity at face-centers 
It is convenient to consider four-sided faces alongside the more general n-sided faces. After at most 
two subdivisions, the con.guration surrounding a type Fface may be rep­resented as in Figure 11b. In 
the center lies the face (of type F) P1P2:::Pn. Each edge of this face (for example PjPj+1) is adjacent 
to a four-sided face of type E with ver­tices PjPj+1Ej2Ej1. The neighborhood of each vertex Pj is completed 
by a four-sided face of type V with vertices PjEj1CjEUj,1)2. Let the con.guration around this type F 
face be repre­sented by the vector of points M=[P1;:::;Pn;E11;E12;:::;En1;En2;C1;:::;CnJT; and Mdbe the 
corresponding con.guration after subdivision. d Then M=SnM,where Snis a 4n4nmatrix called the re.nement 
matrix. Here we only consider the .rst option for the new knot spacings in which case Snremains constant 
through all the subsequent subdivision steps. Thus we can use the eigenstructure of Snto analyze continuity. 
We carried out an algebraic eigenanalysis for orders 3to 8based on the discrete Fourier transform technique 
in an exercise reported in [30]. This leads us to Theorem 1 For orders 3to 8, if all knot spacings di;j>0, 
then the re.nement matrix Snis not defective and its eigen­values are 1 A1 =1>A2 =A3 = >jA4j;jA5j;;jA4nj: 
2 By an argument similar to one used in [12] we can con­clude that, provided that all the knot spacings 
are greater than zero, the limit surface generated by the non-uniform Doo-Sabin scheme is G1continuous 
both at all ordinary points and at extraordinary points of valence less than 9.  5.2 Cubic Case 5.2.1 
Limit surface structure After one subdivision, every face is four-sided, and after two more, the pattern 
of knot intervals over the group of 16 sub­faces replacing each original face is as shown in Figure 12a. 
The hiare in arithmetic sequence as are the vi. When these values are substituted into (11), (13) and 
(16), the positions of the new vertices in, or on the boundary of, the innermost four sub-faces are exactly 
the same as if all the horizontal intervals had been equal and all the vertical intervals equal likewise. 
The innermost four subfaces there­fore converge towards uniform bicubic B-splines, giving a pattern of 
bicubic pieces as shown in Figure 12b, where the largest square represents the face of a non-uniform 
Catmull-Clark net with different knot intervals along each edge. The pattern of smaller squares shows 
schematically the in.nite progression of B´ezier patches that make up the limit surface. The interior 
of the limit surface of every such face is there­fore G2. We need only concern ourselves with the continuity 
at the edges and at the vertices, where there is a regression. h1 h1 h1 h1 h2v5 h2v4 h2v3 h2v2 v1 h3v5 
h3v4 h3v3 h3v2 v1 h4v5 h4v4 h4v3 h4v2 v1 h5v5 h5v4 h5v3 h5v2 v1  a. 5.2.2 Continuity at vertices In 
this section we consider continuity at vertex points of va­lence 23(i.e., exceptional points, as well 
as vertex points of valence four). Unfortunately, since the re.nement matrix changes at each iteration, 
it is dif.cult to perform an eigen­analysis to determine if non-uniform Catmull-Clark surfaces are G1at 
vertex points, except for simple numerical cases. One of the few cases that yield a constant re.nement 
matrix is the valence three vertex in Figure 13 (right). In this case, the second and the third eigenvalues 
are generally different. To .nd out what is going on in the neighborhood of this point, we performed 
a numerical study, chosing a cube as a control polyhedron with various knot spacings. Figure 13 E2i 
shows the neighborhood of a vertex point Vi(that began as a corner of the cube) after 25 iterations. 
The .gure on the left shows the uniform knot spacing; everything is symmet­ric as expected. The .gure 
on the right came from the same vertex on the same cube, only with knot intervals of value 100 assigned 
to one set of four parallel edges on the cube; of value 10 assigned to another set of four parallel edges, 
and of 1 assigned to the remaining four edges, again after i=25iterations. Notice that the angles are 
no longer equal. In fact, it turns out that angle (i=6 Ei ,Vi ,Eitends 13 to zero as i!1.However, the 
three faces become copla­nar at a much faster rate, as shown in the following table. Here, .Nirefers 
to the maximum angle between the planes 12, Ei 1,and Ei 3. Ei ,Vi ,Ei ,Vi ,Ei ,Vi ,Ei 32 Uniform Non-uniform 
i Ni (i Ni (i 1radians 1radians 1radians 1radians 0 5 10,1 0:5 5 10,1 0:5 5 5 10,3 0:6665 2 10,2 0:37 
25 7 10,10 0:666667 5 10,8 0:15 50 1 10,20 0:666667 9 10,15 0:043 75 2 10,30 0:666667 1 10,21 0:013 100 
6 10,40 0:666667 2 10,28 0:004 The normals are becoming parallel at a rate that is roughly 107times 
faster than the rate at which (iis approaching zero. After 100 iterations, the con.guration is as close 
to G1as anyone could possibly have need for. The facets are 25 orders of magnitude smaller than they 
need to be for any practical use (.ve iterations are plenty for most graphics ap­plications). We also 
did a similar study on valence four, using widely varying knot spacings, and again observed a fast conver­gence 
of normal vectors, but no tendency of any face angles to tend to zero. Hence, we are con.dent that valence 
four points are G1. Preliminary experiments with n>4indicate very similar behavior. 5.2.3 Continuity 
across edges Across the interior of an edge, the situation may be regarded as a standard non-uniform 
B-spline with a perturbation due to the original variation between the knot intervals. This per­turbation 
tends to zero with a convergence rate of O(2,i). Note that the non-uniform B-spline is C2and the surround­ing 
vertices converge to a plane con.guration with a rate of O(4,i). Therefore, in the limit, the non-uniform 
Catmull-Clark surface is G1across every edge.  6 DISCUSSION Figure 14 shows the effect that knot spacing 
can have on a surface. In the grid at the left, all edges are assigned knot spacing of 1, except for 
the four edges labeled with a 0. Two steps of non-uniform Catmull-Clark subdivision result in the meshes 
shown (minus a few outer layers of quadrilaterals). This con.guration of knot spacings causes the limit 
surface to interpolate the center point with G0continuity. 00 0 0 Figure 14: Effect of non-uniform knot 
spacing. In the absence of non-uniform subdivision surfaces, Hoppe et. al. proposed a scheme for imposing 
features such as creases, corners, and darts on an otherwise G1subdivision surface that uses special-case 
masks [13]. NURSSes can provide for such features without the need for special masks. For example, Figure 
1 shows a crease imposed on a Doo-Sabin surface by setting three knot spacings to zero. Figure 2 (left) 
shows a dart on a non-uniform Catmull-Clark surface, created by setting four knot intervals to zero. 
Figures 16 19 show a variety of shapes that can be at­tained using NURSSes, but not using uniform subdivision 
surfaces. The initial control polyhedra are shown in wire­frame. Sharp features can be imposed by setting 
to zero the knot spacing of appropriate edges. Another use for knot spacing is in shape modi.cation. 
Figure 2 (right) shows the effect of altering the knot spacing on several control polygon edges of a 
torus-shaped Catmull-Clark surface. Figure 17 shows a uniform Doo-Sabin sur­face (on the left) and two 
non-uniform counterparts, formed by choosing different knot spacings as shown. The sphere in Figure 18 
cannot be expressed exactly using uniform sub­division surfaces, rational or otherwise. Figure 19c,e,h 
are other examples of non-uniform Catmull-Clark surfaces. In summary, this method extends the known 
general topology meth­ods by permitting unequal knot intervals, thus allowing a single surface description 
the strengths of both the standard non-uniform tensor products and the uniform recursive subdivision 
surfaces in one representation.  Even in the situation where there are no extraordinary points, this 
theory extends current capability by giving a G1 surface when the knot intervals are chosen individually 
for every edge in the control polygon, not constrained to support a tensor product structure.  This 
scheme provides a lot of freedom to adjust the shape of the surface. In particular, it can model sharp 
features by properly setting certain knot spacings to zero.  Future work will design a convenient modeling 
interface for the interactive purpose and determine how to use knot spacing to best advantage. ACKNOWLEDGEMENTS 
The .rst two authors received partial .nancial support for this project through an NSF grant. Kris Klimaszewski 
made several helpful suggestions, as did the referees.  REFERENCES [1] A A Ball and D J T Storry. Recursively 
Generated B­spline Surfaces. Proc. CAD84, pages 112 119, 1984. ISBN 0408 01 4407. [2] A A Ball and D 
J T Storry. Conditions For Tangent Plane Continuity Over Recursively Generated B-spline Surfaces. ACM 
ToG, 7:83 102, 1988. [3] W Boehm. Inserting New Knots Into B-spline Curves. Computer-Aided Design, 12:199 
201, 1980. [4] E Catmull and J Clark. Recursively Generated B-spline Surfaces On Arbitrary Topological 
Meshes. Computer-Aided Design, 10:350 355, 1978. [5] G Chaikin. An Algorithm For High-speed Curve Gen­eration. 
Computer Graphics and Image Processing, 3:346 349, 1974. [6] E Cohen, T Lyche, and R F Riesenfeld. Discrete 
B-splines And Subdivision Techniques In Computer Aided Design And Computer Graphics. Computer Graphics 
and Image Processing, 14:87 111, 1980. [7] C de Boor. Cutting Corners Always Works. Computer Aided Geometric 
Design, 4:125 131, 1987. [8] D Doo and M Sabin. Behaviour Of Recursive Division Surfaces Near Extraordinary 
Points. Computer-Aided Design, 10:356 360, 1978. [9] N Dyn, D Levin, and J A Gregory. A 4-point Interpola­tory 
Subdivision Scheme For Curve Design. Computer Aided Geometric Design, 4:257 268, 1987. [10] G Farin. 
Designing C1Surfaces Consisting Of Trian­gular Cubic Patches. Computer-Aided Design, 14:253 256, 1982. 
[11] A R Forrest. Notes On Chaikin s Algorithm. Techni­cal Report Memo CGP74/1, University of East Anglia, 
Norwich, UK, 1974. [12] M Halstead, M Kass, and T DeRose. Ef.cient, Fair In­terpolation Using Catmull-Clark 
Surfaces. Computer Graphics (SIGGRAPH 93 Conference Proceedings), 27:35 44, 1993. [13] H Hoppe, T DeRose, 
T Duchamp, M Halstead, H Jin, J McDonald, J Schweitzer, and W Stuetzle. Piecewise Smooth Surface Reconstruction. 
Computer Graph­ics (SIGGRAPH 94 Conference Proceedings), 28:295 302, 1994. [14] L Kobbelt. Interpolatory 
Subdivision On Open Quadri­lateral Nets With Arbitrary Topology. Computer Graphics Forum (Eurographics 
96), 1996. [15] C Loop. Smooth Subdivision Surfaces Based On Tri­angles. Master s thesis, University 
of Utah, Dept. of Mathematics, 1987. [16] C Loop. Smooth Spline Surfaces Over Irregular Meshes. Computer 
Graphics (SIGGRAPH 94 Confer­ence Proceedings), 28:303 310, 1994. [17] A Nasri. Polyhedral Subdivision 
Methods For Free­form Surfaces. PhD thesis, University of East Anglia, 1984. [18] A Nasri. Polyhedral 
Subdivision Methods For Free­form Surfaces. ACM ToG, 6:29 73, 1987. [19] A Nasri. Surface Interpolation 
On Irregular Network With Normal Conditions. Computer Aided Geometric Design, 8:89 96, 1991. [20] A Nasri. 
Curve Interpolation In Recursively Generated B-spline Surfaces Over Arbitrary Topology. Computer Aided 
Geometric Design, 14:13 30, 1997. [21] J Peters. Joining Smooth Patches Around A Vertex To Form A CkSurface. 
Computer Aided Geometric Design, 9:387 411, 1992. [22] J Peters. Smooth Free-form Surfaces Over Irregu­lar 
Meshes Generalizing Quadratic Splines. Computer Aided Geometric Design, 10:347 361, 1993. [23] J Peters. 
C1Surface Splines. SIAM J Num Anal, 32:645 666, 1995. [24] H Prautzsch. Freeform Splines. Computer Aided 
Geo­metric Design, 14:201 206, 1997. [25] U. Reif. A Uni.ed Approach To Subdivision Algo­rithms Near 
Extraordinary Vertices. Computer Aided Geometric Design, 12:153 174, 1995. [26] R F Riesenfeld. On Chaikin 
s Algorithm. Computer Graphics and Image Processing, 4:304 310, 1975. [27] M A Sabin. Recursive Division. 
In J Gregory, editor, The Mathematics of Surfaces, pages 269 282. Claren­don Press, Oxford, 1986. ISBN 
0 19 853609 7. [28] M A Sabin. Cubic Recursive Division With Bounded Curvature. In P J Laurent, A le 
Mehaute, and L L Schu­maker, editors, Curves and Surfaces, pages 411 414. Academic Press, 1991. ISBN 
0 12 438660 1. [29] D J T Storry and A A Ball. Design Of An N-sided Sur­face Patch From Hermite Boundary 
Data. Computer Aided Geometric Design, 6:111 120, 1989. [30] J Zheng, T Sederberg, and M A Sabin. Eigenanalysis 
Of Non-Uniform Doo-Sabin Surfaces. Technical re­port, Brigham Young University, Department of Com­puter 
Science (appears as an appendix in the electronic version of this paper), November 1997. [31] D Zorin, 
P Schr¨oder, and W Sweldens. Interpolat­ing Subdivision For Meshes With Arbitrary Topology. Computer 
Graphics (SIGGRAPH 96 Conference Pro­ceedings), 30:189 192, 1996.   Appendix: Eigenanalysis Of Non-Uniform 
Doo-Sabin Surfaces 1 Re.nement matrix Refer to Figure 11b in the paper for the labels. In the center 
lies the shrunken polygon P1P2Pn (type F). Each edge of this face (for example PjPj+1) is adjacent to 
a four-sided face of type E with vertices Pj;Pj+1;Ej2;Ej1. The neighborhood of each vertex Pjcorresponds 
to a four-sided face of type V with vertices Pj;Ej1;Cj;EUj,1)2. Here we only consider the .rst option 
for the new knot spacings and use di;jinstead of d0to denote the knot spacing for the notational simplicity. 
Noting i;j the speci.c characteristics of the knot spacings in the faces of type V and type E after 
at most two subdivisions, we rewrite the re.nement formulas as follows: 8P V+Pj n P j +aj[,nPj+(1+2cos27k)Pj+k] 
 2k=1n  Ej1 (6(dj;j,1dj+1;j+3dj;j+1dj+1;j+2)Pj+3dj;j+1dj+1;j+2Pj+1+  (dj;j+1dj+1;j+2+2dj;j,1dj+1;j)Ej1+dj;j+1dj+1;j+2Ej2)/ 
  (8(dj;j+1dj+1;j+2+dj;j,1dj+1;j)) (1)  Ej2 (3dj;j,1dj+1;jPj+6(dj;j+1dj+1;j+2+3dj;j,1dj+1;j)Pj+1+ 
  dj;j,1dj+1;jEj1+(dj;j,1dj+1;j+2dj;j+1dj+1;j+2)Ej2)/   (8(dj;j+1dj+1;j+2+dj;j,1dj+1;j)) : C j 
(9Pj+3Ej1+3EUj,1)2+Cj)/16 Let the con.guration around this type F face be represented by the vector of 
points ]T M[P1;;Pn;E11;E12;;En1;En2;C1;;Cn; and Mbe the corresponding con.guration after subdivision. 
Then MSnMwhere the re.nement matrix Snis a 4nx4nmatrix, i.e. 0 B B B B Qn 0 0 SE1 0 * .. . 0 0 SEn 
* * 1 16 0 0 .. . 1 16 BC B BE11C B BC B BE12C B BC B B. C B B . C . B BC B BC B B En1 C B BC 
 En2 B BC B BC C 1 B BC B B. C@ @. A . Cn where SEjare 2x2matrices: . dj;j+1dj+1;j+2+2dj;j,1dj+1;j 
dj;j,1dj+1;j SE P1 1 0 . C . B . C B C Pn C B B j 8(dj;j+1dj+1;j+2+dj;j,1dj+1;j)  2 Eigenvalues of 
the re.nement matrix 0 B B Lemma 1. Let AB @ ues Ajof Asatisfy 0< a 11a12 . B a 21a22 B B.. . .. . @.. 
. .. aa k1k2  Proof: Since Aj> a1k a2k . . . akk 0, A hand, Aj<cholds for j a 11 . a 21 . . . . a n1Aj 
1 C 01 1 P 1 B . C C. B. C C BC C B C Pn C BC C BE11C C BC C BE12C C BC C B. C C. C B(2) C 
. BC C BC C En1 BC C BC CEn2 BC C BC CC1 C B C BC A . @. A . Cn . dj;j+1dj+1;j+2 dj;j,1dj+1;j+2dj;j+1dj+1;j+2 
(3) 1 a12 a1n a22 a2n C . . . ... . . . C C A be an nxnHessian matrix. a . n2 ann If the eigenval­ <cwith 
a constant c>0;j1;;n, then for matrix Ak C C, 1SkSn, its eigenvalues fjalso satisfy 0<fj<c. A is a 
positive de.nite matrix. So is Ak. Therefore fj>0. On the other 1;;n,so cI,Ais a positive de.nite matrix. 
Therefore cIk,Akis a positive de.nite matrix, too. Thus we conclude fj<c. Since the re.nement matrix 
Snremains constant through all the subsequent subdivision steps, we can use the eigenstructure of Snto 
analyze continuity. The eigenvalues of Snconsist of those of It is easy to verify that the eigenvalues 
of SEkare matrices Qn; S E k;(k 1 ; ; n )and 1 16In,where In is an nxnidentity matrix. Matrix 1 16In 
has n equal eigenvalues AC 1 AC n 1 /16: (4) AEk AEk 1/4;1/8;k1;;n (5) 12 Now we have to look into the 
eigenvalues of Qnwhich satis.es from (6): 0101 P 1 P1 B. CB. C @. AQn@. A (6).. Pn Pn We use the discrete 
Fourier transform to analyze the eigenproperties. Let p!;p!be Fourier vectors corresponding to Pj;Pj.For 
j1;;n, P n,1 !=0 Pj a!jpj Pn,1 (7) Pj a!jpj !=0 where 2hki 2k2k akecos()+isin( ) (8) n nn hki ., n 
 The conjugate of akis denoted by ake 2. The re.nement may now be formulated in terms of the Fourier 
vectors: n,1 n,1 n,2 XX1X1 a!jp!c!p!+p0+ajp1+Pja!jp!+aUn,1)jpn,1 (9) 22 !=0 !=0 !=2 where 1 Pj ,naj 
(10) 2 nn 1XX c! a!ldl,1;ldl+1;l/dk,1;kdk+1;k (11) 2 l=1 k=1 It should be noted that n X n;ifk0mod(n) 
ak 0;otherwise k=1 . Multiplying equation (13) by alj;(l0;;n,1)and adding up from j1to n, yield 010 
10 c112 * cn,1 0 0... 0 0 ... 0 Bn,3 0 ... 0 0 0 * 12 101 p 0 p 0 Bp1CBCBC p 1 BCBCBC BCBCBC p 2 p 
2 BCBCBC B. CBCB. C(12) . B . CBCBC . . BCBCBC @A@A@A pn,2 pn,2 pn,1 pn,1 where PPP 01 nnn j=1Pja0jj=1Pja1jj=1PjaUn,4)j 
PPP nnn . BC 1Bj=1Pja1jj=1Pja0jj=1PjaUn,5)j C Bn,3B.. .. C(13) n@.. .. . A.. . PPP nnn .. j=1PjaUn,4)jj=1PjaUn,5)jj=1Pja0j 
 Thus three of the eigenvalues of Qnare immediately obtained: 1 A11;A2A3 (14) 2 P If n4, Bn,3jPja0j(n/2,n/4)/n1/4, 
and thus 1 A 4 (15) 4 Therefore in the following we only consider the case of n>4. From (14), we have 
1 Bn,3I,An,3 (16) 2 where PPP 01 nnn j=1aja0jj=1aja1jj=1ajaUn,4)j PPP nnn . B j=1aja1jj=1aja0jj=1ajaUn,5)jC 
BC An,3B.. .. C(17) .. .. @.. .. A PP P nnn aja . aja . aja0jj=1Un,4)jj=1Un,5)jj=1 Obviously, A. An,3,i.e. 
An,3are n,3is a Hessian matrix. Therefore the eigenvalues of An,3real numbers. Also, An,3can be rewritten 
as: PPP 01 nnn j=1aja0jj=1aja1jj=1ajaUn,4)j PPP nnn BC j=1ajaUn,1)jj=1aja0jj=1ajaUn,5)j BC An,3B. ... 
C(18) @. ... . A.. . PP P nnn j=1j=1j=1 aja4j aja5j aja0j Now we construct a new matrix Gn: PPP 01 
nnn j=1aja0jj=1ajaUn,4)jj=1ajaUn,1)j PPP nnn BC j=1ajaUn,1)jj=1ajaUn,5)jj=1ajaUn,2)j BC B... C .. B 
. ... .. . C... BC Gn Pn Pn Pn (19) BC aja4j aja0j aja3j j=1j=1j=1 BC B... C .. @. ... ... A ... PPP 
nnn j=1aja1jj=1ajaUn,3)jj=1 aja0j This is a circulant matrix. If we let p(z)be a polynomial of z: PPP 
n,1 p(z)aja0j+aja1jz++ajaUn,1)jz P jjj(20) n,1)n aj(a0j+a1jz++aUn,1)jz j=1 the eigenvalues fkof Gnare: 
(cf. page 73 of Philip J. Davis: Circulant Matrices, John Wiley &#38; Sons, 1979. ) 2hiUk,1)) n fkp(e 
n P 2hi 2hi 2hi Uk,1)2Uk,1)Un,1)Uk,1)) n aj(a0j+a1jen +a2jen ++aUn,1)je(21) j=1 nan,k+1;k1;;n From 
the de.nition of aj, we know that 0<aj<1if all knot spacings di;j>0and n64. 8 Therefore it holds that 
0<fk<1for k1;;nif n<9. By Lemma 1, all eigenvalues AAj of matrix An,3also satisfy 0<AA<1.Since Bn,31I,An,3, 
the eigenvalues ABof Bn,3 j2j satisfy 11 B ,<Aj< (22) 22 Combining (8), (9), (18), (19) and (26), we 
obtain: Theorem 1 If all knot spacings di;jin Snare positive and n<9, then the eigenvalues of Snare 1 
 A11>AA3>jA4j;jA5j;;jA4nj: 2 2  3 Continuity at face-centres We now look into the eigenvectors of the 
re.nement matrix Sn.First An,3is a Hessian matrix and has n,3independent eigenvectors. Thus Bn,3and Qnhave 
n,3and nindependent eigenvec­tors respectively. Second each SEkhas two independent eigenvectors and 1Inhas 
n.From the 16 structure of Snin (6), we know that the matrix Snhas 4nindependent eigenvectors. So Snis 
not de.ctive. Also note that the sum of each row in Snis 1, and every element is not negative. Therefore 
Sn describes an af.ne invariant process. Let l1;l2and l3be the left eigenvectors of Sncorresponding 11 
to eigenvalues A11;A2;2. Then carrying out an analysis similar to that described in 2A3 [12], we can 
get: .the points on the type F face will converge to the limit point l1M .the normal vector to the 
subdivision surface at the limit point l1Mis the vector (l2M)x(l3M).  Therefore Theorem 2 If all knot 
spacings di;j>0, and all valences and all numbers of the edges of each face are less than 9, then the 
limit surface generated by non-uniform subdivision scheme (1)-(4) is G1 continuous everywhere. Remark: 
Although we have proved G1continuity for the case of n<9, we conjecture the conclu­sion (G1continuity) 
is also true for n29. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280945</article_id>
		<sort_key>395</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>39</seq_no>
		<title><![CDATA[Exact evaluation of Catmull-Clark subdivision surfaces at arbitrary parameter values]]></title>
		<page_from>395</page_from>
		<page_to>404</page_to>
		<doi_number>10.1145/280814.280945</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280945</url>
		<keywords>
			<kw><![CDATA[Catmull-Clark surfaces]]></kw>
			<kw><![CDATA[eigenanalysis]]></kw>
			<kw><![CDATA[linear algebra]]></kw>
			<kw><![CDATA[parametrizations]]></kw>
			<kw><![CDATA[subdivison surfaces]]></kw>
			<kw><![CDATA[surface evaluation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>F.2.1</cat_node>
				<descriptor>Computations on matrices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.3</cat_node>
				<descriptor>Eigenvalues and eigenvectors (direct and iterative methods)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.6</cat_node>
				<descriptor>Computer-aided design (CAD)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010472.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Architecture (buildings)->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010439.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003719</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computations on matrices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31029451</person_id>
				<author_profile_id><![CDATA[81100148921]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stam]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Alias/wavefront, Inc., Seattle, WA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>42459</ref_obj_id>
				<ref_obj_pid>42458</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[A. A. Ball and J. Y. Storry. Conditions For Tangent Plane Continuity Over Recursively Defined B-spline Surfaces. ACM Transactions on Graphics, 7(2):83-102, April 1988.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[E. Catmull and J. Clark. Recursively Generated B-Spline Surfaces On Arbitrary Topological Meshes. Computer Aided Design, 10(6):350-355, 1978.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[D. Doo and M. A. Sabin. Behaviour Of Recursive Subdivision Surfaces Near Extraordinary Points. Computer Aided Design, 10(6):356-360, 1978.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166121</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[M. Halstead, M. Kass, and T. DeRose. Efficient, Fair Interpolation Using Catmull-Clark Surfaces. In Proceedings of SIG- GRAPH '93, pages 35-44. Addison-Wesley Publishing Company, August 1993.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[C.T. Loop. Smooth Subdivision Smfaces Based on Triangles. M.S. Thesis, Department of Mathematics, University of Utah, August 1987.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>276485</ref_obj_id>
				<ref_obj_pid>276465</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[J. Peters and U. Reif. Analysis Of Generalized B-Splines Subdivision Algorithms. To appear in SIAM Journal of Numerical Analysis.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>211166</ref_obj_id>
				<ref_obj_pid>211163</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[U. Reif. A Unified Approach To Subdivision Algorithms Near Extraordinary Vertices. Computer Aided Geometric Design, 12:153-174, 1995.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[J. Stare. Evaluation Of Loop Subdivision Surfaces. SIG- GRAPH'98 CDROM Proceedings, 1998.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>580358</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[J. Warren. Subdivision Methods For Geometric Design. Unpublished manuscript. Preprint available on the web at http://www, cs. rice. edu/- jwarren/papers/book, ps. gz.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>927147</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[D. N. Zorin. Subdivision and Multiresolution Smface Representations. PhD thesis, Caltech, Pasadena, California, 1997.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Copyright &#38;#169;1998 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers or to redistribute to lists, requires specific permission and/or a fee. Exact Evaluation Of 
Catmull-Clark Subdivision Surfaces At Arbitrary Parameter Values Jos Stam. Alias wavefront, Inc.  Abstract 
In this paper we disprove the belief widespread within the computer graphics community that Catmull-Clark 
subdivision surfaces can­not be evaluated directly without explicitly subdividing. We show that the surface 
and all its derivatives can be evaluated in terms of aset of eigenbasis functions which depend only on 
the subdivi­sion scheme and we derive analytical expressions for these basis functions. In particular, 
on the regular part of the control mesh where Catmull-Clark surfaces are bi-cubic B-splines, the eigenba­sis 
is equal to the power basis. Also, our technique is both easy to implement and ef.cient. We have used 
our implementation to compute high quality curvature plots of subdivision surfaces. The cost of our evaluation 
scheme is comparable to that of a bi-cubic spline. Therefore, our method allows many algorithms developed 
for parametric surfaces to be applied to Catmull-Clark subdivision surfaces. This makes subdivision surfaces 
an even more attractive tool for free-form surface modeling. CR Categories: I.3.5 [Computer Graphics]: 
Computational Ge­ometry and Object Modeling Curve, Surface, Solid, and Object Representations J.6 [Computer 
Applications]: Computer-Aided Engineering Computer Aided Design (CAD) Keywords: subdivision surfaces, 
eigenanalysis, linear algebra, parametrizations, surface evaluation, Catmull-Clark surfaces 1 Introduction 
Subdivision surfaces have emerged recently as a powerful and use­ful technique in modeling free-form 
surfaces. However, although in theory subdivision surfaces admit local parametrizations, there is a strong 
belief within the computer graphics community that these parametrizations cannot be evaluated exactly 
for arbitrary parame­ter values. In this paper we disprove this belief and provide a non­iterative technique 
that ef.ciently evaluates Catmull-Clark subdi­vision surfaces and their derivatives up to any order. 
The cost of our technique is comparable to the evaluation of a bi-cubic surface spline. The rapid and 
precise evaluation of surface parametriza­tions is crucial for many standard operations on surfaces such 
as picking, rendering and texture mapping. Our evaluation technique .Alias wavefront, 1218 Third Ave, 
8th Floor, Seattle, WA 98101, U.S.A. jstam@aw.sgi.com  allows a large body of useful techniques from 
parametric surfaces to be transfered to subdivision surfaces, making them even more attractive as a free-form 
surface modeling tool. Our evaluation is based on techniques .rst developed to prove smoothness theorems 
for subdivision schemes [3, 5, 1, 4, 7, 6]. These proofs are constructed by transforming the subdivision 
into its eigenspace1. In its eigenspace, the subdivision is equivalent to a simple scaling of each of 
its eigenvectors by their eigenvalue. These techniques allow us to compute limit points and limit normals 
at the vertices of the mesh, for example. Most of the proofs, however, consider only a subset of the 
entire eigenspace and do not address the problem of evaluating the surface everywhere. We, on the other 
hand, use the entire eigenspace to derive an ef.ciently evaluated analytical form of the subdivision 
surface everywhere, even in the neighborhood of extraordinary vertices. In this way, we have ex­tended 
a theoretical tool into a very practical one. In this paper we present an evaluation scheme for Catmull-Clark 
subdivision surfaces [2]. However, our methodology is not lim­ited to these surfaces. Whenever subdivision 
on the regular part of the mesh coincides with a known parametric representation [7], our approach should 
be applicable. We have decided to present the technique for the special case of Catmull-Clark subdivision 
sur­faces in order to show a particular example fully worked out. In fact, we have implemented a similar 
technique for Loop s triangu­lar subdivision scheme [5]. The details of that scheme are given in a paper 
on the CDROM Proceedings [8]. We believe that Catmull-Clark surfaces have many properties which make 
them attractive as a free-form surface design tool. For example, after one subdivision step each face 
of the initial mesh is a quadrilateral, and on the regu­lar part of the mesh the surface is equivalent 
to a piecewise uniform B-spline. Also, algorithms have been written to fair these surfaces [4]. In order 
to de.ne a parametrization, we introduce a new set of eigenbasis functions. These functions were .rst 
introduced by War­ren in a theoretical setting for curves [9] and used in a more general setting by Zorin 
[10]. In this paper, we show that the eigenbasis of the Catmull-Clark subdivision scheme can be computed 
analyt­ically. Also, for the .rst time we show that in the regular case the eigenbasis is equal to the 
power basis and that the eigenvectors then correspond to the change of basis matrix from the power basis 
to the bi-cubic B-spline basis. The eigenbasis introduced in this pa­per can thus be thought of as a 
generalization of the power basis at extraordinary vertices. Since our eigenbasis functions are an­alytical, 
the evaluation of Catmull-Clark subdivision surfaces can be expressed analytically. As shown in the results 
section of this paper, we have implemented our evaluation scheme and used it in many practical applications. 
In particular, we show for the .rst time high resolution curvature plots of Catmull-Clark surfaces precisely 
computed around the irregular parts of the mesh. The paper is organized as follows. Section 2 is a brief 
review of the Catmull-Clark subdivision scheme. In Section 3 we cast this subdivision scheme into a mathematical 
setting suitable for analy­sis. In Section 4 we compute the eigenstructure, to derive our eval­ 1To be 
de.ned precisely below. 13 14 15 16 .......... 9 v .......... (1,1) .......... .......... .......... 
.......... .......... .......... 5 .......... (0,0) .......... u 1 10 11 12 6 (0,0) (1,1) 7 ........ 
........ ........ ........ ........ ........ ........ ........ 8 2 3 4 Figure 1: A bi-cubic B-spline 
is de.ned by 16 control vertices. The numbers on the right show the ordering of the corresponding B­spline 
basis functions in the vector b.u;v.. Figure 2: Initial mesh and two levels of subdivision. The shaded 
faces correspond to regular bi-cubic B-spline patches. The dots are extraordinary vertices. uation. Section 
5 is a discussion of implementation issues. In Sec­tion 6 we exhibit results created using our technique 
and compare it to straightforward subdivision. Finally in Section 7 we conclude, mentioning promising 
directions for future research. 1.1 Notations In order to make the derivations below as clear and compact 
as pos­sible we adopt the following notational conventions. All vectors are assumed to be columns and 
are denoted by boldface lower case ro­man characters, e.g., v. The components of the vector are denoted 
by the corresponding italicized character: the i-th component of a vector is thus denoted vi. The component 
of a vector should not be confused with an indexed vector such as vk. Matrices are de­noted by uppercase 
boldface characters, e.g., M. The transpose of T a vector v(resp. matrix M) is denoted by v(resp. MT). 
The transpose of a vector is simply the same vector written row-wise. Therefore the dot product between 
two vectors uand vis written T uv . The vector or matrix having only zero elements is denoted by 0. The 
size of this vector (matrix) should be obvious from the context.  2 Catmull-Clark Subdivision Surfaces 
The Catmull-Clark subdivision scheme was designed to general­ize uniform B-spline knot insertion to meshes 
of arbitrary topology [2]. An arbitrary mesh such as the one shown on the upper left 2N+1 2 9 3 1 2N+8 
8 4 6 72N+7 5 2N+6 2N+5 2N+4 2N+3 2N+2 Figure 3: Surface patch near an extraordinary vertex with its 
con­trol vertices. The ordering of the control vertices is shown on the bottom. Vertex 1 is the extraordinary 
vertex of valence N.5. hand side of Figure 2 is used to de.ne a smooth surface. The sur­face is de.ned 
as the limit of a sequence of subdivision steps. At each step the vertices of the mesh are updated and 
new vertices are introduced. Figure 2 illustrates this process. On each vertex of the initial mesh, the 
valence is the number of edges that meet at the vertex. A vertex having a valence not equal to four is 
called an extraordinary vertex. The mesh on the upper left hand side of Figure 2 has two extraordinary 
vertices of valence three and one of valence .ve. Away from extraordinary vertices, the Catmull-Clark 
subdivision is equivalent to midpoint uniform B-spline knot inser­tion. Therefore, the 16vertices surrounding 
a face that contains no extraordinary vertices are the control vertices of a uniform bi-cubic B-spline 
patch (shown schematically Figure 1). The faces which correspond to a regular patch are shaded in Figure 
2. The .gure shows how the portion of the surface comprised of regular patches grows with each subdivision 
step. In principle, the surface can thus be evaluated whenever the holes surrounding the extraordinary 
ver­tices are suf.ciently small. Unfortunately, this iterative approach is too expensive near extraordinary 
vertices and does not provide exact higher derivatives. Because the control vertex structure near an 
extraordinary vertex is not a simple rectangular grid, all faces that contain extraordinary vertices 
cannot be evaluated as uniform B-splines. We assume that the initial mesh has been subdivided at least 
twice, isolating the ex­traordinary vertices so that each face is a quadrilateral and contains at most 
one extraordinary vertex. In the rest of the paper, we need to demonstrate only how to evaluate a patch 
corresponding to a face with just one extraordinary vertex, such as the region near vertex 1 in Figure 
3. Let us denote the valence of that extraordinary vertex by N. Our task is then to .nd a surface patch 
s.u;v.de.ned over the unit square ...0;1...0;1.that can be evaluated directly in terms of the K.2N+8vertices 
that in.uence the shape of 00  0 00 0 0 0 0 0 0 .... .... 0 .... ....0 0 0  Figure 4: The effect 
of the seven outer control vertices does not depend on the valence of the extraordinary vertex. When 
the 2N+1 control vertices in the center are set to zero the same limit surface is obtained. the patch 
corresponding to the face. We assume in the following that the surface point corresponding to the extraordinary 
vertex is s.0;0.and that the orientation of .is chosen such that su.sv points outside of the surface. 
A simple argument shows that the in.uence on the limit surface of the seven outer control vertices numbered 
2N+2through 2N+8in Figure 3 can be accounted for directly. Indeed, consider the situation depicted in 
Figure 4 where we show a mesh containing a vertex of valence 5and a regular mesh side by side. Let us 
assume that all the control vertices are set to zero except for the seven con­trol vertices highlighted 
in Figure 4. If we repeat the Catmull-Clark subdivision rules for both meshes we actually obtain the 
same limit surface, since the exceptional control vertex at the center of the patch remains equal to 
zero after each subdivision step. Therefore, the effect of the seven outer control vertices is simply 
each con­trol vertex multiplied by its corresponding bi-cubic B-spline tensor product basis function. 
In the derivation of our evaluation technique we do not need to make use of this fact. However, it explains 
the simpli.cations which occur at the end of the derivation. 3 Mathematical Setting In this section 
we cast the informal description of the previous sec­tion into a rigorous mathematical setting. We denote 
by T C..c0;1;...;c0;K.; 0 the initial control vertices de.ning the surface patch shown in Fig­ure 3. 
The ordering of these vertices is de.ned on the bottom of Figure 3. This peculiar ordering is chosen 
so that later computa­tions become more tractable. Note that the vertices do not result in the 16control 
vertices of a uniform bi-cubic B-spline patch, except when N.4. Through subdivision we can generate a 
new set of M.K+9 vertices shown as circles super-imposed on the initial vertices in Figure 5. Subsets 
of these new vertices are the control vertices of three uniform B-spline patches. Therefore, three-quarters 
of our surface patch is parametrized, and could be evaluated as simple bi­cubic B-splines (see top left 
of Figure 6). We denote this new set of vertices by .,. T TT C1..c1;1;...;c1;K.andC1.C1;c1;K+1;...;c1;M: 
 2N+17 2N+5 2N+13 Figure 5: Addition of new vertices by applying the Catmull-Clark subdivision rule 
to the vertices in Figure 3. With these matrices, the subdivision step is a multiplication by an K.K(extended) 
subdivision matrix A: C1.AC0: (1) Due to the peculiar ordering that we have chosen for the vertices, 
the extended subdivision matrix has the following block structure: .. A. S0 ;(2) S11S12 where Sis the 
2N+1.2N+1subdivision matrix usually found in the literature [4]. The remaining two matrices correspond 
to the regular midpoint knot insertion rules for B-splines. Their exact def­inition can be found in Appendix 
A. The additional points needed to evaluate the three B-spline patches are de.ned using a bigger matrix 
A.of size M.K: .. C1.AC0; where .! S0 A..S11S12:(3) S21S22 The matrices S21and S22are de.ned in Appendix 
A. The sub­division step of Equation 1 can be repeated to create an in.nite sequence of control vertices: 
n Cn.ACn,1.AC0and n,1 ... Cn.ACn,1.AAC0;n.1: As noted above, for each level n.1, a subset of the vertices 
of C.nbecomes the control vertices of three B-spline patches. These  2N+17 2N+16 2N+15 2N+14 P3 .... 
2N+8 2N+7 2N+6 2N+2 .... 3 452N+3 P2 P1 2 162N+4 2N+7 2N+6 2N+2 2N+10 2N+16 2N+15 2N+14 2N+9 ..... ..... 
2N+7 2N+6 2N+2 2N+10 4 52N+3 2N+11 ..... .... 6 2N+4 2N+12 452N+3 2N+111  8 72N+5 2N+13 162N+4 2N+12 
Figure 6: Indices of the control vertices of the three bi-cubic B­spline patches obtained from C.n. v 
u O3 1 O2 1 O3 2 O2 2 O1 1 O3 3 O2 3 O1 2 O1 3  Figure 7: Partition of the unit square into an in.nite 
family of tiles. control vertices can be de.ned by selecting 16control vertices from C.nand storing them 
in 16.3matrices: . Bk;n.PkCn; where Pkis a 16.M picking matrix and k.1;2;3.Let b.u;v. be the vector containing 
the 16cubic B-spline basis functions (see Appendix B). If the control vertices are ordered as shown on 
the left of Figure 1, then the surface patch corresponding to each matrix of control vertices is de.ned 
as T TT . sk;n .u;v..Bk;nb.u;v..CnPkb.u;v.;(4) where .u;v.2., n.1and k.1;2;3. Using the ordering convention 
for the B-spline control vertices of Figure 1, the de.ni­tion of the picking matrices is shown in Figure 
6. Each row of Pkis .lled with zeros except for a one in the column corresponding to the index shown 
in Figure 6 (see Appendix B for more details). The in­.nite sequence of uniform B-spline patches de.ned 
by Equation 4 form our surface s.u;v., when stitched together . More formally, let us partition the unit 
square .into an in.nite set of tiles f.n g, k n.1;k.1;2;3, as shown in Figure 7. Each tile with index 
nis four times smaller than the tiles with index n,1. More precisely: hihi 1 11 .n .;.0;; 1 ,1 2n 2n2n 
hihi 1 111 .n 2 .;.;;(5) ,1 ,1 2n 2n2n2n hihi 111 .n .0;.;: 3 ,1 2n2n2n A parametrization for s.u;v.is 
constructed by de.ning its restric­tion to each tile .nto be equal to the B-spline patch de.ned by the 
kcontrol vertices Bk;n: s.u;v..n.sk;n.tk;n.u;v..:(6) k The transformation tk;nmaps the tile .nkonto the 
unit square .: t1;n.u;v...2n u,1;2n v.;(7) t2;n.u;v...2n u,1;2n v,1.and(8) t3;n.u;v...2n u;2n v,1.:(9) 
Equation 6 gives an actual parametrization for the surface. How­ever, it is very costly to evaluate, 
since it involves n,1multipli­cations of the K.Kmatrix A. The evaluation can be simpli.ed considerably 
by computing the eigenstructure of A. This is the key idea behind our new evaluation technique and is 
the topic of the next section. 4 Eigenstructure, Eigenbases and Evalu­ation The eigenstructure of the 
subdivision matrix Ais de.ned as the set of its eigenvalues and eigenvectors. In our case the matrix 
A is non-defective for any valence. Consequently, there always ex­ists Klinearly independent eigenvectors 
[4]. Therefore we denote this eigenstructure by ..;V.,where .is the diagonal matrix con­taining the eigenvalues 
of A,and Vis an invertible matrix whose columns are the corresponding eigenvectors. The computation of 
the eigenstructure is then equivalent to the solution of the following matrix equation: AV.V.; (10) where 
the i-th diagonal element of .is an eigenvalue with a cor­responding eigenvector equal to the i-th column 
of the matrix V (i.1;...;K). There are many numerical algorithms which can compute solutions for such 
equations. Unfortunately for our purposes, these numerical routines do not always return the cor­rect 
eigenstructure. For example, in some cases the solver returns complex eigenvalues. For this reason, we 
must explicitly com­pute the eigenstructure. Since the subdivision matrix has a de.nite block structure, 
our computation can be done in several steps. In Appendix A we analytically compute the eigenstructure 
..;U0. (resp. ..;W1.) of the diagonal block S(resp. S12) of the subdi­vision matrix de.ned in Equation 
2. The eigenvalues of the subdivi­sion matrix are the union of the eigenvalues of its diagonal blocks: 
.. .0 .. : 0 . Using the eigenvectors of Sand S12, it can be proven that the eigen­vectors for the subdivision 
matrix must have the following form: .. U00 V. : U1W1 The matrix U1is unknown and is determined from 
Equation 10. If we replace the matrices ., Vand Aby their block representations, we obtain the following 
matrix equation: S11U0+S12U1.U1.:(11) Since U0is known, U1is computed by solving the 2N+1linear systems 
of Equation 11. In principle, this equation could be solved symbolically. In practice, however, because 
of the small sizes of the linear systems (7.7) we can compute the solution up to ma­chine accuracy (see 
the next section for details). The inverse of our eigenvector matrix is equal to .. U,1 V,10 .0 ;(12) 
,W,1U1U,1W,1 101 where both U0and W1can be inverted exactly (see Appendix A). This fact allows us to 
rewrite Equation 10: A.V.V,1: This decomposition is the crucial result that we use in constructing a 
fast evaluation scheme of the surface patch. Indeed, the subdivided control vertices at level nare now 
equal to n,1n,1n,1 ...V,1.^ Cn.AAC0.AV.C0.AV.C0; ^V,1 where C0.C0is the projection of the Kcontrol vertices 
into the eigenspace of the subdivision matrix. Using this new ex­pression for the control vertices at 
the n-th level of subdivision, Equation 4 can be rewritten in the following form: ,.T Tn,1 . sk;n.u;v..C^0.PkAVb.u;v.: 
We observe that the right most terms in this equation are indepen­dent of the control vertices and the 
power n. Therefore, we can precompute this expression and de.ne the following three vectors: ,.T . x.u;v;k..PkAVb.u;v.k.1;2;3:(13) 
The components of these three vectors correspond to a set of K bi-cubic splines. In Appendix B we show 
how to compute these splines. Notice that the splines xi.u;v;k.depend only on the va­lence of the extraordinary 
vertex. Consequently, we can rewrite the equation for each patch more compactly as: ^Tn,1 sk;n.u;v..C0.x.u;v;k.k.1;2;3:(14) 
To make the expression for the evaluation of the surface patch more T ^ concrete, let pidenote the rows 
of C0. Then the surface patch can be evaluated as: K X .n s.u;v..n...i,1 xi.tk;n.u;v.;k.pi:(15) k i.1 
Therefore, in order to evaluate the surface patch, we must .rst com­pute the new vertices pi(only once 
for a given mesh). Next, for each evaluation we determine nand then scale the contribution from each 
of the splines by the relevant eigenvalue to the power n,1. Since all but the .rst of the eigenvalues 
are smaller than one, their contribution decreases as nincreases. Thus, for large n, i.e., for surface-points 
near the extraordinary vertex, only a few terms make a signi.cant contribution. In fact for .u;v...0;0.the 
sur­face point is p1, which agrees with the de.nition of a limit point in [4]. Alternatively, the bi-cubic 
spline functions x.u;v;k.can be used to de.ne a set of eigenbasis functions for the subdivision. For 
a given eigenvector .iwe de.ne the function ' iby its restrictions on the domains .nkas follows: .n ' 
i.u;v..n...i,1 xi.tk;n.u;v.;k.; k with i.1;...;K. By the above de.nition these functions satisfy the 
following scaling relation: ' i.u.2;v.2...i ' i.u;v.: The importance of these functions was .rst noted 
by Warren in the context of subdivision curves [9]. More recently, Zorin has de.ned and used eigenbasis 
functions to prove smoothness conditions for very general classes of subdivision schemes [10]. However, 
ex­plicit analytical expressions for particular eigenbases have never appeared before. On the other hand, 
we can compute these bases analytically. Figures 8 and 9 show the complete sets of eigenbasis functions 
for valences 3 and 5. In the .gures we have normalized each function such that its range is bounded within 
,1and 1.In particular, the .rst eigenbasis corresponding to an eigenvalue of one is always a constant 
function for any valence. A closer look at Fig­ures 8 and 9 reveals that they share seven identical functions. 
In fact as shown in Appendix B, the last seven eigenbasis functions for any valence are always equal 
to no 1111111 3333323323 uv;u;uv;uv;v;uv;uv: 36662662 Furthermore, by transforming these functions back 
from the eigenspace using W1,1we obtain the seven tensor B-spline basis functions b4.u;v.;b8.u;v.;b12.u;v.;...;b16.u;v.; 
i.e., the basis functions corresponding to the outer layer of control vertices of Figure 3. This should 
not come as a surprise since as we noted above, the in.uence of the outer layer does not depend on the 
valence of the extraordinary vertex (see Figure 4). In the regular bi-cubic B-spline case (N.4), the 
remaining eigenbasis can be chosen to be equal to the power basis 222222 f1;u;v;u;uv;v;uv;uv;uvg: The 
scaling property of the power basis is obvious. For example, the basis function u 2 vcorresponds to the 
eigenvalue 1.8: 1 2222 .u.2..v.2...1.2..1.2.uv.uv: 8 This relationship between the Catmull-Clark subdivision 
and the power basis in the regular case has not been noted before. Note also that the eigenvectors in 
this case correspond to the change of basis matrix from the bi-cubic B-spline basis to the power basis. 
The eigenbasis functions at extraordinary vertices can thus be inter­preted as a generalization of the 
power basis. However, the eigen­bases are in general not polynomials. In the case of the Catmull-Clark 
subdivision they are piece-wise bi-cubic polynomials. The evaluation of the surface patch given by Equation 
15 can now be rewritten exactly as: K X s.u;v..' i.u;v.pi:(16) i.1 This is the key result of our paper, 
since this equation gives a parametrization for the surface corresponding to any face of the control 
mesh, no matter what the valence is. There is no need to subdivide. Equation 16 also allows us to compute 
derivatives of the surface up to any order. Only the corresponding derivatives of the basis functions 
appearing in Equation 16 are required. For example, the partial derivative of the i-th eigenbasis with 
respect to uis: @n,1@ .n ' i.u;v..n.2..ixi.tk;n.u;v.;k.; @uk @u where the factor 2nis equal to the derivative 
of the af.ne transfor­mation tk;n. Generally a factor 2pnwill be present when the order of differentiation 
is p. 5 Implementation Although the derivation of our evaluation technique is mathemati­cally involved, 
its implementation is straightforward. The tedious task of computing the eigenstructure of the subdivision 
matrix only has to be performed once and is provided in Appendix A. In prac­tice, we have precomputed 
these eigenstructures up to some maxi­mum valence, say NMAX=500, and have stored them in a .le. Any program 
using our evaluation technique can read in these precom­puted eigenstructures. In our implementation 
the eigenstructure for each valence Nis stored internally as typedef struct f double L[K]; /*eigenvalues 
*/ double iV[K][K]; /*inv of the eigenvectors */ double x[K][3][16]; /*coeffs of the splines */ gEIGENSTRUCT; 
EIGENSTRUCT eigen[NMAX];, where K=2*N+8. At the end of this section we describe how we computed these 
eigenstructures. We emphasize that this step only has to be performed once and that its computational 
cost is irrele­vant to the ef.ciency of our evaluation scheme. Given that the eigenstructures have been 
precomputed and read in from a .le, we evaluate a surface patch around an extraordinary vertex in two 
steps. First, we project the control vertices surround­ing the patch into the eigenspace of the subdivision 
matrix. Let the control vertices be ordered as shown in Figure 3 and stored in an ar­ray C[K]. The projected 
vertices Cp[K]are then easily computed by using the precomputed inverse of the eigenvectors: ProjectPoints(point 
*Cp,point *C,int N)f for ( i=0 ; i<2*N+8 ; i++ )f Cp[i] = (0,0,0); for ( j=0 ; j<2*N+8 ; j++ )f Cp[i] 
+= eigen[N].iV[i][j] * C[j]; g g g This routine is called only whenever one of the patches is evaluated 
for the .rst time or after an update of the mesh. This step is, there­fore, called at most once per surface 
patch. The second step of our evaluation, on the other hand, is called whenever the surface has to be 
evaluated at a particular parameter value (u,v). The second step is a straightforward implementation 
of the sum appearing in Equation 15. The following routine computes the surface patch at any parameter 
value. EvalSurf ( point P, double u, double v, point *Cp, int N ) f /*determine in which domain .nthe 
parameter lies */ k n = floor(min(-log2(u),-log2(v))); pow2 = pow(2,n-1); u *= pow2; v *= pow2; if 
(v < 0.5) f k=0; u=2*u-1; v=2*v; g elseif (u < 0.5) f k=2; u=2*u; v=2*v-1; g else f k=1; u=2*u-1; v=2*v-1; 
 g /*Now evaluate the surface */ P = (0,0,0); for ( i=0 ; i<2*N+8 ; i++ ) f P += pow(eigen[N].L[i],n-1) 
* EvalSpline(eigen[N].x[i][k],u,v)*Cp[i]; g g The function EvalSpline computes the bi-cubic polynomial 
whose coef.cients are given by its .rst argument at the parameter value (u,v). When either one of the 
parameter values u or v is zero, we set it to a suf.ciently small value near the precision of the machine, 
to avoid an over.ow that would be caused by the log2 function. Because EvalSplineevaluates a bi-cubic 
polynomial, the cost of EvalSurf is comparable to that of a bi-cubic surface spline. The extra cost due 
to the logarithm and the elevation to an integer power is minimal, because these operations are ef.ciently 
implemented on most current hardware. Since the projection step is only called when the mesh is updated, 
the cost of our evaluation depends predominantly on EvalSurf. The computation of the p-th derivative 
is entirely analogous. In­stead of using the routine EvalSplinewe employ a routine that returns the p-th 
derivative of the bi-cubic polynomial. In addition, the .nal result is scaled by a factor pow(2,n*p). 
The evaluation of derivatives is essential in applications that require precise surface normals and curvature. 
For example, Newton iteration schemes used in ray surface computations require higher derivatives of 
the surface at arbitrary parameter values. We now describe how we compute the eigenstructure of the sub­division 
matrix. This step only has to performed once for a given set of valences. The ef.ciency of this step 
is not crucial. Accuracy is what matters here. As shown in the appendix, the eigenstructure of the two 
matrices Sand S12can be computed analytically. The cor­responding eigenstructure of the extended subdivision 
matrix Are­quires the solution of the 2N+1linear systems of Equation 11. We did not solve these analytically 
because these systems are only of size 7.7. Consequently, these systems can be solved up to machine accuracy 
using standard linear solvers. We used the dgesvroutine from LINPACK to perform the task. The inverse 
of the eigenvec­tors is computed by carrying out the matrix products appearing in Equation 12. Using 
the eigenvectors, we also precompute the coef­.cients of the bi-cubic splines x.u;v;k.as explained in 
Appendix B. For each valence Nwe stored the results in the data structure eigen[NMAX] and saved them 
in a .le to be read in at the start of any application which uses the routines ProjectPointsand EvalSurfdescribed 
above. 6 Results In Figure 10 we depict several Catmull-Clark subdivision surfaces. The extraordinary 
vertex whose valence Nis given in the .gure is located in the center of each surface. The position informa­tion 
within the blue patches surrounding the extraordinary vertex are computed using our new evaluation technique. 
The remaining patches are evaluated as bi-cubic B-splines. Next to each surface we also depict the curvature 
of the surface. We map the value of the Gaussian curvature onto a hue angle. Red corresponds to a .at 
surface, while green indicates high curvature. We have pur­posely made the curvature plot discontinuous 
in order to empha­size the iso-contour lines. Both the shaded surface and the cur­vature plot illustrate 
the accuracy of our method. Notice especially how the curvature varies smoothly across the boundary between 
the patches evaluated using our technique and the regular bi-cubic B­spline patches. The curvature plots 
also indicate that for N64 .the Gaussian curvature takes on arbitrarily large values near the ex­traordinary 
vertex. The curvature at the extraordinary vertex is in fact in.nite, which explains the diverging energy 
functionals in [4]. Figure 11 depicts more complex surfaces. The patches in blue are evaluated using 
our technique. 7 Conclusion and Future Work In this paper we have presented a technique to evaluate 
Catmull-Clark subdivision surfaces. This is an important contribution since the lack of such an evaluation 
scheme has been sited as the chief argument against the use of subdivision scheme in free-form sur­face 
modelers. Our evaluation scheme permits many algorithms and analysis techniques developed for parametric 
surfaces to be extended to Catmull-Clark surfaces. The cost of our algorithm is comparable to the evaluation 
of a bi-cubic spline. The imple­mentation of our evaluation is straightforward and we have used it to 
plot the curvature near extraordinary vertices. We believe that the same methodology can be applied to 
many other subdivision schemes sharing the features of Catmull-Clark subdivision: regular parametrization 
away from extraordinary vertices. We have worked out the details for Loop s triangular scheme, and the 
derivation can be found in the accompanying paper on the CDROM proceedings [8]. Catmull-Clark surfaces 
and Loop surfaces share the property that their extended subdivision matrices are non-defective. In gen­eral, 
this is not the case. For example, the extended subdivision matrix of Doo-Sabin surfaces cannot generally 
be diagonalized. In this case, however, we can use the Jordan normal form of the ex­tended subdivision 
matrix and employ Zorin s general scaling rela­tions [10].  Acknowledgments I wish to thank the following 
individuals for their help: Eugene Lee for assisting me in .ne tuning the math, Michael Lounsbery and 
Gary Herron for many helpful discussions, Darrek Rosen for creat­ing the models, Pamela Jackson for proofreading 
the paper, Gregg Silagyi for his help during the submission, and Milan Novacek for his support during 
all stages of this work. A Subdivision Matrices and Their Eigen­structures The matrix Scorresponds to 
the extraordinary rules around the ex­traordinary vertex. With our choice of ordering of the control 
ver­tices the matrix is: 0 aN bNcNbNcNbN ... bNcNbNcN 1 d d e e 0 0 ... 0 0 e e B C B f f f f 0 0 ... 
0 0 0 0 C B B d e e d e e ... 0 0 0 0 C C S.B f 0 0 f f f ... 0 0 0 0 C B C B B . . . . . . . . . . . 
. C C @ df ef 00 00 00 00 ...... e0 e0 df e f A where 731311 aN.1,;bN.;cN.;d.;e.;f.: 4N2N24N28164 Since 
the lower right 2N.2Nblock of Shas a cyclical structure, we can use the discrete Fourier transform to 
compute the eigen­structure of S. This was .rst used in the context of subdivision sur­faces by Doo and 
Sabin [3]. The discrete Fourier transform can be written compactly by introducing the following 2N.2N 
Fourier matrix ; 10 1010...10 B 0101...01 C B!,1!,.NC 100...,1.0 C B B!,1!,.N,1. C 010...0 F.B C;  B. 
. C . B . .. C B . .. C  ,1.,1.2 @!,.N!,.NA 100...0 !,.N,1.!,.N,1.2 010...0 where !.exp.i2..N.. Using 
these notations we can write down the Fourier transform of the matrix Scompactly as: 0 ^S0 0 0 0 0 ^S1 
0 0 ... 0 00 ... 0 0 ^SN,1 1 BC S^. B C ,1 B C .TST; @ A where .... 1 0T,110 T.1;. ; F. 0F0 N . ! aNNbNNcN 
S^0.d2f2eand f2ff . ,.,.. !,l !,l e+!l+de1+ S^ l.,. ; !l f1 +f l.1;...;N,1. The eigenstructure of the 
Fourier transform S^is computed from the eigenstructures of its diagonal blocks. The .rst block S^0has 
eigenvalues 1.p. .1.1;.2;.3.,7+3N.49,30N+5N2 8N and eigenvectors . ! 116.2 ,12.2+116.2 ,12.3+1 23 ^ K0.16.2,16.3,1: 
14.2+14.3+1 Similarly, the two eigenvalues of each block S^l(l.1;...;N,1) are equal to: . r! . ..... 
12.l.l2.l .. l.5+cos .cos18+2cos ; 16NNN where we have used some trigonometric relations to simplify 
the re­sulting expressions. The corresponding eigenvectors of each block are .. 4., l ,14.+ l,1 ^. : 
Kl !l!l 1+1+ We have to single out the special case when Nis even and l.N.2. In this case the corresponding 
block is .. 10^ KN.2. : 01 The eigenvalues of the matrix S^are the union of the eigenvalues of its blocks 
and the eigenvectors are 01 BC ^B C K.B C : @ A ^K0 0 0 0 0 ^K1 0 0 . . . 0 . . . 0 0 0 0 ^KN,1  Since 
the subdivision matrix Sand its Fourier transform S^are simi-The remaining blocks of the subdivision 
matrix Adirectly fol­lar, they have the same eigenvalues. The eigenvectors are computed low from the 
usual B-spline knot-insertion rules. by inverse Fourier transforming these eigenvectors: 0101 cbc0bc0 
c00bab000 T,1^0ee0000 e00edd000 K.K: BCBC B0cbc000CBb00cbabc0C BCBC Consequently, we have computed the 
eigenvalues and eigenvectors S12.B 00ee000C;S11.B e0000dde0C; BCBC . of S. However, in this form the 
eigenvectors are complex valued @0000ee0 A@e00dde000 A and most of the eigenvalues are actually of multiplicity 
two, since 0000cbcbcbabc000 ,..+and .+..,We relabel these eigenvalues as 00000eeeedd00000 lN,llN,l. follows: 
where 931 .4..,;.5..+ ;.6..,;.7..+ ;... 1122a.;b.andc.: 163264 Since we have rearranged the eigenvalues, 
we have to rearrange the For the case N.3, there is no control vertex c8(c8.c2)and the eigenvectors. 
At the same time we make these eigenvectors real. second column of the matrix S11is equal to .0;0;c;e;0;c;e.T 
. Let k1;...;k2N+1be the columns of K, then we can construct the The eigenstructure of the matrix S12can 
be computed manually, columns of a matrix U0as follows: since this matrix has a simple form. Its eigenvalues 
are: .. u1.k1;u2.k2;u3.k3; 1111111 ..diag ;;;;;;; 1 648163281632 u2l+2..kl+3+k2N,l+2.and 2 with corresponding 
eigenvectors: 1 u2l+3.2i .kl+3,k2N,l+2.: 01112111211 0112000 More precisely u1, u2, u3, u2l+2and u2l+3are 
equal to BC B010,1000C BC 010101W1.B 01,12000C : 116.2 ,12.2+116.2 ,12.3+1 BC 23 0000112 B 1 CB 6.2,1 
CB 6.3,1 C@A 000010,1 B1CB4.2+1CB4.3+1C BCBCBC00001,12 B. C;B. C;B. C;  B . CB . CB . C .. . @A@A@A 
 16.2,16.3,1 The inverse W1,1of this matrix is easily computed manually. 14.2+1 4.3+1 The other two matrices 
appearing in A.are: 0010010101 0000f000ff00f00 4.l+3,10 BCBCB 0000de00CB ede0e00C B1+ClCBSlCBCBC 0000ff000ff0000 
BCBC BCBC B .4.l+3,1.ClCB .4.l+3,1.SlCBCBC 0000ede00ede000 BCandBC;BCBC Cl+C2l Sl+S2l BCBCS21.B 00000ff0C;S22.B 
00ff000C : B. CBCCBC . B000ed000ee00de0 B . CB . CBCBC .. @.4.l+3,1.C.N,1.l A@.4.l+3,1.S.N,1.l A@B000ff000 
AC@B0000ff0CA 00ede0000000ede C.N,1.l+1 S.N,1.l 00ff000000000ff respectively, where l.1;...;N2, N2.N,1when 
Nis odd and N2.N,2when Nis even, and B Eigenbasis Functions .... 2.k 2.k In this appendix we compute 
the bi-cubic spline pieces x.u;v;k.of Ck.cosandSk.sin : NNthe eigenbasis de.ned in Equation 13. The 
vector b.u;v.contains the 16tensor B-spline basis functions (i.1;...;16): When Nis even the last two 
eigenvectors are T bi.u;v..N.i,1..4.u.N.i,1..4.v.; u2N..0;1;0;,1;0;1;0;...;,1;0.and T where . and . 
stand for the remainder and the division respec­ u2N+1..0;0;1;0;,1;0;1;...;0;,1.: tively. The functions 
Ni.t.are the uniform B-spline basis func­tions: Finally, the diagonal matrix of eigenvalues is 6N0.t..1,3t+3t 
2 ,t 3 ; ..diag.1;.2;.3;.4;.4;...;.N+2;.N+2.: 6N1.t..4,6t 2 +3t 3 ; The inverse of the eigenvectors 
U0can be computed likewise by 23 6N2.t..1+3t+3t,3tand^ .rst computing the inverses of each block Klin 
the Fourier domain and then setting 6N3.t..t 3 : K,1K^,1 .T: The projection matrices P1, P2and P3are 
de.ned by introducing With the same reshuf.ing as above we can then compute U,1.The the following three 
permutation vectors (see Figure 6): 0 resulting expressions are, however, rather ugly and are not repro­duced 
in this paper. q 1..8;7;2N+5;2N+13;1;6;2N+4;2N+12; 4;5;2N+3;2N+11;2N+7;2N+6;2N+2; 2N+10.; 2 q..1;6;2N+4;2N+12;4;5;2N+3;2N+11; 
2N+7;2N+6;2N+2;2N+10;2N+16; 2N+15;2N+14;2N+9.; 3 q..2;1;6;2N+4;3;4;5;2N+3;2N+8;2N+7; 2N+6;2N+2;2N+17;2N+16;2N+15; 
2N+14.: Since for the case N.3the vertices c2and c8are the same vertex, q11.2instead of 8for N.3. Using 
these permutation vectors we can compute each bi-cubic spline as follows: 16 X xi.u;v;k..Vbj.u;v.; q 
k;i j j.1 where i.1;...;Kand Vare the eigenvectors of the subdivision matrix.  References [1] A. A. 
Ball and J. T. Storry. Conditions For Tangent Plane Continuity Over Recursively De.ned B-spline Surfaces. 
ACM Transactions on Graphics, 7(2):83 102, April 1988. [2] E. Catmull and J. Clark. Recursively Generated 
B-Spline Sur­faces On Arbitrary Topological Meshes. Computer Aided De­sign, 10(6):350 355, 1978. [3] 
D. Doo and M. A. Sabin. Behaviour Of Recursive Subdivision Surfaces Near Extraordinary Points. Computer 
Aided Design, 10(6):356 360, 1978. [4] M. Halstead, M. Kass, and T. DeRose. Ef.cient, Fair Interpo­lation 
Using Catmull-Clark Surfaces. In Proceedings of SIG-GRAPH 93, pages 35 44. Addison-Wesley Publishing 
Com­pany, August 1993. M.S. Thesis, Department of Mathematics, University of Utah, August 1987. [6] J. 
Peters and U. Reif. Analysis Of Generalized B-Splines Sub­division Algorithms. To appear in SIAM Journal 
of Numerical Analysis. [7] U. Reif. A Uni.ed Approach To Subdivision Algorithms Near Extraordinary Vertices. 
Computer Aided Geometric Design, 12:153 174, 1995. [8] J. Stam. Evaluation Of Loop Subdivision Surfaces. 
SIG-GRAPH 98 CDROM Proceedings, 1998. [9] J. Warren. Subdivision Methods For Geometric Design. Unpublished 
manuscript. Preprint available on the web at http://www.cs.rice.edu/ jwarren/papers/book.ps.gz. [10] 
D. N. Zorin. Subdivision and Multiresolution Surface Repre­sentations. PhD thesis, Caltech, Pasadena, 
California, 1997. Figure 8: The complete set of 14eigenbasis functions for extraor­dinary vertices of 
valence N.3.  Figure 9: The complete set of 18eigenbasis functions for extraor­dinary vertices of valence 
N.5.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280946</article_id>
		<sort_key>405</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>40</seq_no>
		<title><![CDATA[Wires]]></title>
		<subtitle><![CDATA[a geometric deformation technique]]></subtitle>
		<page_from>405</page_from>
		<page_to>414</page_to>
		<doi_number>10.1145/280814.280946</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280946</url>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39079097</person_id>
				<author_profile_id><![CDATA[81335497253]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Karan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Singh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Alias|wavefront, Toronto, Ont., Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43117278</person_id>
				<author_profile_id><![CDATA[81100188679]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Eugene]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fiume]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Alias|wavefront, Toronto, Ont., Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[A. Barr. Superquadrics and angle-preserving transformations. IEEE Computer Graphics and Applications, 1:1-20, 1981.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134003</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[T. Beier and S. Neely. Feature based image metamorphosis. Computer Graphics, 26(2):35-42, 1992.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>91427</ref_obj_id>
				<ref_obj_pid>91394</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[J. Bloomenthal and B. Wyvill. Interactive techniques for implicit modeling. Computer Graphics, 24(4):109-116, 1990.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122757</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[J. Bloomenthal and K. Shoemake. Convolution surfaces. Computer Graphics, 25(4):251-256, 1991.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74358</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[J. Chadwick, D. Haumann and R. Parent. Layered construction for deformable animated characters. Computer Graphics, 23(3):234-243, 1989.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192220</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Y.K. Chang and A.E Rockwood. A generalized de Casteljau approach to 3D free-form deformation. Computer Graphics, 28(4):257-260, 1994.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97900</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[S. Coquillart. Extended free-form deformations: A sculpting tool for 3D geometric modeling. Computer Graphics, 24(4):187-196, 1990.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166157</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[M.-E Gascuel. An implicit formulation for precise contact modeling between flexible solids. Proc. of SIGGRAPH, pages 313-320, 1993.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134036</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[W. Hsu, J. Hughes and H. Kaufman. Direct manipulation of free-form deformations. Computer Graphics, 26(2): 177-184, 1992.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237247</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[R. MacCracken and K. Joy. Free-form deformations with lattices of arbitrary topology. Computer Graphics, 181-189, 1996.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[N. Magnenat-Thalmann and Y. Yang. Techniques for cloth animation. SIGGRAPH Course Notes C20, 151-163, 1991.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[F. Lazarus, S. Coquillart, and E Jancene. Axial deformations: an intuitive deformation technique. Computer-Aided Design, 26(8):607-613, August 1994.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15903</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[T. Sederberg and S. Parry. Free-form deformation of solid geometric models. Computer Graphics, 20:151-160, 1986.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>90939</ref_obj_id>
				<ref_obj_pid>90767</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[E Schneider. Solving the Nearest-Point-on-Curve Problem. Graphics Gems, Academic Press, vo1.1:607-612, 1990.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[G. Wyvill, C. McPheeters and B. Wyvill. Animating soft objects. Visual Computer, 2:235-242, 1986.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Copyright &#38;#169;1998 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers or to redistribute to lists, requires specific permission and/or a fee. Wires: A Geometric 
Deformation Technique Karan Singh Eugene Fiume. Aliasjwavefront Abstract Finding effective interactive 
deformation techniques for complex geometric objects continues to be a challenging problem in mod­eling 
and animation. We present an approach that is inspired by armatures used by sculptors, in which wire 
curves give de.nition to an object and shape its deformable features. We also introduce domain curves 
that de.ne the domain of deformation about an ob­ject. A wire together with a collection of domain curves 
provide a new basis for an implicit modeling primitive. Wires directly re­.ect object geometry, and as 
such they provide a coarse geometric representation of an object that can be created through sketching. 
Furthermore, the aggregate deformation from several wires is easy to de.ne. We show that a single wire 
is an appealing direct manipu­lation deformation technique; we demonstrate that the combination of wires 
and domain curves provide a new way to outline the shape of an implicit volume in space; and we describe 
techniques for the aggregation of deformations resulting from multiple wires, domain curves and their 
interaction with each other and other deformation techniques. The power of our approach is illustrated 
using appli­cations of animating .gures with .exible articulations, modeling wrinkled surfaces and stitching 
geometry together. Keywords: deformations, implicit models, interactive graphics, an­imation. 1 Introduction 
The modeling and animation of deformable objects is an active area of research [1, 2, 7, 8, 10, 12, 13, 
15]. Free-form deformations (FFDs) [13] and their variants [6, 7, 9, 10], for example, are popular and 
provide a high level of geometric control over the deformation. These approaches involve the de.nition 
and deformation of a lat­tice of control points. An object embedded within the lattice is then deformed 
by de.ning a mapping from the lattice to the object. The user thus deals with a level of detail dictated 
by the density of the control lattice. While very useful for coarse-scale deformations of an object, 
the technique can be dif.cult to use for .ner-scale defor­mations, where a very dense and customized 
control lattice shape [7, 10] is usually required. Arbitrarily shaped lattices can be cum­bersome to 
construct and it is often easier to deform the underlying geometry directly than to manipulate a dense 
control lattice. *Aliasjwavefront, 210 King St. E., Toronto, Canada M5A 1J7. ks­ingh@aw.sgi.com, elf@aw.sgi.com 
Axial deformations provide a more compact representation in which a one-dimensional primitive, such as 
a line segment or curve, is used to de.ne an implicit global deformation [12]. Our approach, called wire 
deformations, is related to axial deformations, although we have a different motivation and formulation. 
Our main point of departure is our desire to bring geometric and deformation model­ing closer together 
by using a collection of wires as both a coarse­scale representation of the object surface, and a directly 
manipu­lated deformation primitive that highlights and tracks the salient deformable features of the 
object. As can be seen in Figure 1, pro­jections of the wire curves provide a sketch-like representation 
of the object, which is how many artists prefer doing design. Wire deformations may be likened to a constructive 
sculpting approach in which the wires of an armature provide de.nition to the object and control its 
deformable features. As in scuplture, the wire curves themselves give a coarse approximation to the shape 
of the object being modeled. A wire deformation is independent of the complexity of the underlying object 
model while easily al­lowing .ner-scale deformations to be performed as either object or deformation 
complexity increases. In fact, an animator can interact with a deformable model, namely the wires, without 
ever having to deal directly with the object representation itself. Wires can con­trol varying geometric 
representations of the same object and can be reused on different objects with similar deformable features. 
There are two stages in the wire deformation process. In the .rst, which is typically computed once, 
an object is bound to a set of wires. In the second, any manipulation of a wire effects a defor­mation 
of the object. Implicit function based techniques are used to implement wire deformations. The deformation 
algorithm is con­ceptually simple and ef.cient. Through several examples, we shall illustrate the expressiveness 
of wires for feature based object design and animation, including facial animation. Section 2 presents 
the wire deformation algorithm. Section 3 in­troduces the use of domain curves to re.ne the regions affected 
by a wire. Section 4 describes the techniques used to provide user con­trol over aggregate wire (or other) 
deformations. Section 5 demon­strates the power of wires for the modeling and animation of wrin­kled 
surfaces, .exible articulated structures and stitched surfaces. Section 6 concludes with discussion of 
our results. 2 Wire De.nition and Algorithm A wire is a curve whose manipulation deforms the surface 
of an associated object near the curve. We de.ne a wire as a tuple hW;R;s;r;fi,where Wand Rare free-form 
parametric curves, sis a scalar that controls radial scaling around the curve, and ris a scalar value 
de.ning a radius of in.uence around the curve; the scalar function f:R +![0;1]is often referred to in 
implicit func­tion related literature as a density function [15]. Normally, fis at least C1and monotonically 
decreasing with f(0)=1;f(x)=0 for x?1and f0(0)=0;f0(1)=0.1 1Wire deformations on a surface preserve continuity 
up to the degree of continuity of the function f.As an example, we use a C1function f(x) (x2 ,1)2;x2[0;1], 
in our implementation. Figure 1: Wires: A geometric deformation technique. The parameters fand rcan 
be used to de.ne a volume about a curve bounded by an offset surface at a distance rfrom the curve [4]. 
Together with a scale factor s, and given rand f,a wire is de.ned by specifying a curve Wand a congruent 
copy of the curve, R. We refer to Was the wire curve and Ras the reference curve for a wire. When an 
object is bound to the wire, the domain of in.uence of the wire is demarkated by the offset surface of 
radius rde.ned around the reference curve R. The in.uence for points of the object within this offset 
volume are calculated using the density function f. Subsequent manipulation of Wresults in a change between 
W and R, which is used along with sto de.ne the deformation. The actual deformation applied to a point 
is modulated by its in.uence calculated when the object was bound to the wire. Let C(u)be a space curve, 
parametrized without loss of gen­erality by u2[0;1]. For any point P2R 3,let pC2[0;1]be the parameter 
value that minimizes the Euclidean distance between point Pand curve C(u). If there is more than one 
minimum, we arbitrarily de.ne pCto be the parameter with the smallest value.2 For any point Pand curve 
C, we de.ne the function F(P;C)as jjP,C(pC)jj F(P;C)=f: r From the properties of fit is clear that 
F(P;C)varies from zero for jjP,C(pC)jj?r(points on and outside the offset volume de.ned by Cand r), to 
F(P;C)=1when jjP,C(pC)jj=0(P lies on C). F(P;C)de.nes the in.uence that a curve Chas on a point P. This 
is the usual function de.nition for implicitly de.ned offset shapes [4], and it will be used below in 
de.ning the semantics of the deformation. As with any deformation, a wire deformation is a pointwise 
func­tion mapping R 3onto R 3. For each object O,let PObe the point­based representation to which the 
wire deformations will be ap­plied. Typically POcontains all points necessary to construct or approximate 
an object s surface. POcould thus be a set of control vertices for freeform surfaces, a set of vertices 
in a polymesh, or an unstructured set of points in space. When an object Ois bound to a wire hW;R;s;r;fi, 
the param­eters pRand F(P;R)are computed for every point P2PO.Only points on the object within the offset 
volume of radius rfrom the curve Rwill be deformed (i.e., points Pwith F(P;R)>0). Fig­ure 3(a) shows 
how one wire with a larger raffects a larger region of the object, the other deformation parameters being 
identical. 2In most cases, this is an effective choice, but it can be overly simple in geometrically 
delicate situations, which we discuss in Section 6.       (a) Varying r (b) Varying s Figure 3: 
Varying rand son wires. Observe the following properties of our formulation. .Objects are not deformed 
upon initial creation of wire and reference curves: R, being a copy of W, coincides with it, so no rotation 
or translation is applied. For a default scale parameter of s=1, no deformation is applied to the object. 
.Points on the object outside the offset volume of radius rfrom the reference curve (points Pwith F(P;R)=0) 
are not de­formed regardless of the value of s. This is because F(P;R) attenuates each step of the deformation. 
 .Points on the object that are on the reference curve, when the object is bound to the wire, track the 
wire curve precisely. For a point Pon the undeformed object that coincides with a point on the reference 
curve, R(pR)is identical to Pand thus F(P;R)=f(0)=1. The scale and rotation have no effect as they are 
applied about point R(pR)itself. Pthus moves to P+(W(pR),R(pR))or the point W(pR)on W.  .The deformation 
of points on the object between those on the reference curve and those outside its realm of in.uence 
is smooth and intuitive. The factor F(P;R)controls the atten­uation of the deformation, varying from 
precise tracking for points on the reference curve to no deformation at or beyond the offset volume boundary. 
The properties of the function f dictate the behavior of F(P;R)and the smoothness proper­ties of the 
deformation. .For s=1, the cross-section of the deformed object surface in a plane perpendicular to the 
wire curve at a point closely resembles the pro.le of f(see Figure 3(a)). Manipulating fprovides intuitive 
control over the shape of the deformed object surface and directly controls the degree of continuity 
preserved by the deformed surface. Figure 3(b) also shows how reducing son one wire and increasing it 
on the other provides sucking or bulging control over the deformation. Axial deformations [12] also use 
the notion of a reference curve Rand closest point computation pRfor a point P. The axial defor­mation 
technique relates two Frenet frames attached at W(pR)on the deformed curve and R(pR)on the reference 
curve. The defor­mation imparted to point Pis a portion of the transformation from the reference curve 
s Frenet frame to the Frenet frame on the de­formed curve. The proportion is based on an interpolation 
of the closest distance of Pto the reference curve jjP,R(pR)jjbetween two cut-off radii Rinand Rout. 
While axial deformations and the deformation of a single wire share some similarities, a wire has several 
differences. First, the separation of the scale, rotation and translational components of the wire deformation 
provides a user with more selective control over the resulting deformation than the integrated transformation 
of a Frenet frame. Second, Frenet frames are harder to control and have orientation problems when the 
curvature of a curve vanishes. Third, simple non-linear transformations can be incorporated seam­lessly 
into the deformation algorithm at the appropriate point. For example, as seen in Figure 4, an interpolated 
twist around the wire can be implemented by rotating the point around the axis along the reference-curve 
R0(pR)by a speci.ed angle as part of the rota­tional step of the deformation algorithm. Fourth, using 
an implicit function to control the spatial in.uence of the wire on the deformed objects makes the technique 
accessible to more general implicit sur­face animation techniques. The extensions in Section 3 will show 
how implicit functions can be overlaid by a user to determine what parts of the deformed objects are 
affected and by how much. Figure 5 shows the effect of the various deformation parame­ters. A cylindrical 
object with an associated wire is depicted in Fig­ure 5(a). Figure 5(b) shows the deformation to the 
surface as a result of moving a control point on the wire curve. A more global defor­mation to the entire 
object as result of a large increase to ris illus­trated in Figure 5(c). Another control point is moved 
in Figure 5(d). When ris large, the entire object tracks the wire. Figure 5(e) de­picts the effect of 
reducing the scale factor son the con.guration in Figure 5(d). Figure 5(f) further illustrates how the 
three stages of deformation can be tuned individually by attenuating the rotational aspect and inducing 
a shear on the con.guration in Figure 5(d). 3 Controlling Wire Parameters Our technique was designed 
with usability and direct manipulation in mind. We are thus interested in ways of giving .ner user control 
over the deformation parameters. Allowing a speci.ed portion such as a subset of control vertices on 
an object to be deformed affords some degree of control. However, continuity properties may be compromised 
in parts of the object surface de.ned by control ver­tices that are selectively deformed. This is shown 
in Figures 6(a,b). Usually one would expect a smoother dropoff based on the region selected, such as 
that shown in Figure 6(c). 3.1 Locators One solution involves using locators along a wire curve to specify 
the values of parameters along the wire. An animator can position locators along curves as needed to 
control locally not only the ra­dius of in.uence rbut any attribute related to wire deformation. We calculate 
the attribute being localized at a parameter value pas an interpolation between the attribute values 
speci.ed at the two locators that bracket p. Two wire locators are used to model the cone-spherical shape 
of an Adam s apple in Figure 7(a) by vary­ing r. Local control over the amplitude of deformation causes 
the transformation from an l inFigure7(b) toan i in Figure 7(c). Locators can also be used to incorporate 
non-linear transformations such as a twist (see Figure 4), where they are used to control the twist angle 
along the wire. As mentioned in Section 2, the implicit function Fcan be com­bined with other functions. 
In particular, we can get directional control by modulating Fwith an implicit function for an angular 
dropoff around an axis perpendicular to the wire. Both the direc­tional axis and dropoff angle can be 
interpolated by locators. 3.2 Domain Curves Locators provide radially symmetric local control along 
and around a wire curve. Anisotropic directional control is provided by domain curves as illustrated 
in Figure 6(c). Domain curves along with an associated wire s reference curve de.ne an implicit primitive 
func­tion over a .nite volume. This provides incremental, direct control over what parts of the object 
are deformed (using domain curves) and by how much they are deformed (using wire curves). We shall deal 
here with a single domain curve for a given wire. The use of multiple domain curves will be discussed 
in Section 4. As illustrated in Figure 6(c), a domain curve demarkates a region of the object surface 
to be deformed, and along with the reference Figure 4: Interpolated twist around a wire. (a) no deforma-(b) 
deform with (c) increase r(d) more defor-(e) reduce s(f) attenuate ro­tion small r mation tation Figure 
5: More variations of rand son wires. (a) sharp vertical decay (b) sharp horizontal decay (c) smooth 
decay Figure 6: Region of in.uence of a wire. (a) r (b) deformation amplitude (c) deformation amplitude 
Figure 7: Varying deformation parameters along a wire. curve it acts as an anchor for the deformation. 
More generally, we de.ned the domain curve to be a free-form curve rather than a closed curve on the 
object surface. Such a domain curve does not unambiguously determine which control points on the object 
surface will be deformed. Most animators, however, have a very good idea of how a given domain curve 
will affect the region of the object to be deformed, based on the spatial relationship between the reference 
curve, domain curve and the object surface. In our implementation we use distance and angle computations 
between points on the object surface, the domain curve and reference curve to determine if and by how 
much the point will be in.uenced. In Figure 6(c), we chose the domain curve to have a one-sided in.uence 
region affected by the wire. The other side is affected by the conventional dropoff radius r. Our formulation 
of one-sided domain curvesisasfollows. We .rst determine if the domain curve Dwill be used to de­.ne 
the function fat a point P.Let cosangle=(D(pD), R(pR)).(P,R(pR)). The domain curve will de.ne the func­tion 
if cosangle>0. This heuristic attempts to select points P that are thought to lie on the same side of 
Ras D(even though the concept of side is not well-de.ned mathematically). As can be seen in the Figure 
8, this notion of same side tends to be captured by an acute angle subtended at R(pR), for the triangle 
with vertices at P, D(pD)and R(pR). With this edge condition, jjP,R(pR)jj F(P;R)=f : jjR(pR),D(pD)jj 
 For points considered to be outside the domain de.ned by the domain curve, the conventional dropoff 
radius calculation can be applied. This formulation is likely to lead to a discontinuity in the neighbourhood 
of points where cosangle=0. The discon­tinuity may be removed by specifying a 52(0;1), so that for cosangle2[0;5], 
jjP,R(pR)jj F(P;R)=f ; Interp(cosangle) where the function Interpgives a smoothly interpolated value 
from rto jjR(pR),D(pD)jjas cosanglevaries from 0to 5. Figure 9 uses a domain curve under the eye to limit 
the in.uence of the wire to the .gure s cheek. Domain curves can easily be used to control other spatially 
variable parameters.  4 Multiple wires Recall that our approach is driven by the interaction of multiple 
wires that together provide an overall de.nition of the object s P2 D  0< cosangle < d r  Figure 8: 
Implicit function de.ned using a domain curve. shape (cf. Figure 1). We appeal to a sculptor s armature 
metaphor to give the expected behavior of a deformation in regions where more than one wire has an effect. 
In an armature, an overall shape deformation can be seen as a smoothed union of the deformations caused 
by each wire. This behavior is evident in the X pulled out of a plane by two wires in Figure 10(a). The 
results are distinct from the traditional superposition of the deformations due to each wire as in Figure 
10(b). This behavior is analogous to that discussed in [4] distinguishing implicit function based convolution 
and distance surfaces. We further require that subdividing a wire curve into two curves does not affect 
the deformation applied to the object (such as an unwanted bulge where the two curves abut). The problem 
of unwanted aggregate blobs is circumvented in [7, 10] by making deformations due to multiple deformers 
incremental. While Coquillart s technique for FFDs can be readily applied to wires[7], it would defeat 
our main purpose of getting interesting aggregate behavior from many interacting wires. Our solution 
is as follows. Let the ithwire curve deforming an object be hWi;Ri;si;ri;fii. Let us suppose the deformation 
of a point Pon an object induced by wire iresults in Pdefi (as de.ned in Section 2). Let Pi =Pdefi ,P. 
The deformed point Pdefas in.uenced by all wires is de.ned as the following blend: P n i 1 Pi.jj Pijjm 
P def =P+P n : i 1jj Pijjm The resulting behavior varies with mfrom a simple average of the Piwhen m=0,converging 
to maxf Pigfor large m(see Figure 11). When mis negative, it is technically possible to have a singular 
denominator. But if we reformulate this expression as PQ n Pi.jj Pjjjjmj i 1j 6i P def =P+P nQ; jj Pjjjjmj 
i 1j 6i we note that the singularity is removable. In practice, it is prefer­able to use the original 
formulation even for negative mand simply omit those P s that are zero. Observe that as mgets increasingly 
negative, the displacement approaches minf Pig. Indeed, each wire icould have its own exponent mi, giving 
.ner control over its contribution to the result in regions of interaction. It is easy to verify that 
the above formulation has several desir­able properties for typical values of m?1: 1. In a region where 
only one wire is relevant, the result is pre­cisely the deformation of that wire. 2. When several wires 
produce the same deformation, the result is the deformation induced by any one of those wires. 3. In 
general, the result is an algebraic combination of the indi­vidual wire deformations, with a bias (controlled 
by m)to­ward the deformations of larger magnitude.   Figure 9: Using domain curves to animate a facial 
crease. Many augmentations of our formulation are possible. For example, we can blend the above deformation 
with an aggregation of wire P n deformations given by Pdef =P+i1Pi. We can also attach different exponents 
to each domain curve, allowing us to introduce domain curves that re.ne an implicit volume in an additive 
or sub­tractive fashion controlled by the sign of each exponent. Another useful variation is to introduce 
a local in.uence of a wire at a point on an object s surface relative to other wires. In the formulation 
above, only wires that directly deform a point are of consequence. In Figure 12(b), the central straight 
wire lifts a large portion of the surface when it is translated upward. Because the outer curve did not 
move, it did not in.uence the surface. In Fig­ure 12(c), however, it acts as an anchor, exercising a 
local in.uence on the surface that is independent of the deformation it imparts (in this case none), 
but depends on the proximity of points in space to the curve. We use F(P;Ri)as a measure of proximity 
or local in.uence for the wire. The formulation used for this behavior is P n i1Pi.F(P;Ri)k Pdef =P+P 
n : i1F(P;Ri)k The factor khas a similar effect that mhad earlier. A parameter localizecombines this 
deformation with the others de.ned earlier. A similar effect can be seen in Figure 13, where wires simulate 
the behavior of an FFD lattice. A wire curve is generated along each lattice line. Large dropoff radii 
ensure that planarity is pre­served on the deformed cube when the right face of the lattice is translated 
outward, as can be seen in Figures 13(b,c). The differ­ence in behavior with and without the localized 
in.uence compu­tation is evident from the more global deformation in Figure 13(c) over 13(b). The formulations 
we have described are equally appli­cable to other deformation techniques and can be used to combine 
the results of different deformation approaches. 5 Applications We shall illustrate the versatility 
of wires with three examples that exercise different aspects of wire deformations. We show how wires 
may be used to control wrinkle formation and propagation on a sur­face. Such surface oriented deformations 
are localized to increase surface detail. We apply wires to stitching and tearing geometry, which again 
is a surface oriented deformation. Lastly, we describe a volume oriented deformation, in which a .exible 
skeletal curve is generated from a traditional joint hierarchy and is used to bind artic­ulated geometry 
as a wire. Figures 5(b) and (c) distinguish between surface and volume oriented deformations. 5.1 Wrinkles 
Wrinkles and creases can greatly enhance the realism of animated deformable objects. Cloth animation 
has become an important area of computer animation, especially related to human .gure anima­tion [11]. 
We show here how wires are effective in animating the crease lines along which wrinkles propogate. Wrinkle 
creases are either drawn as curves on the object surface by the animator or au­tomatically generated 
in a set of prede.ned patterns. Typical properties such as wrinkle thickness, intensity and stiff­ness 
of the material are easily captured by the various wire defor­mation parameters. The extent of wrinkle 
propagation can also be controlled. Figure 14(a) shows two wire curves as magni.ed wrin­kles. Figure 
14(b) shows the wrinkles propagating along the object surface. While one wrinkle is pulled along, remaining 
anchored, the other travels along the surface. The travelling wrinkle in Fig­ure 14(b) is a result of 
pulling the reference curve Ralong the object surface with the wire curve W. Figure 15 shows wrinkles 
that are procedurally generated by specifying parameters such as the number of crease lines, thickness, 
intensity, stiffness, and resistance to propagation. The approach is geometric and fast; it allows the 
animator to intuitively control over many salient visual features of wrinkle formation and propagation. 
Figure 16 illustrates this with a curtain animated using wires. A dynamic simulation of the wire curves 
results in a bead-curtain like animation. The wires then deform the object surface. 5.2 Stitching Object 
Surfaces A wire-based geometry stitcher is a two step process. The .rst is the creation of wire curves 
along two edges of the geometry to be stitched. The wire curves are then blended pairwise to common seams. 
The object surfaces track the common seam, resulting in a stitch. We reparametrize the matching edges 
to a common domain before de.ning the stitch. Figure 17 depicts the stitching of one edge onto the other. 
Figure 18 demonstrates the levels of con­trol available as various parameters are changed. These parameters 
give control not only over the stitch but also over the tearing of the stitch through the use and animation 
of locators along the seam (see Figure 18(f)). Figure 19 shows the results of a four-way stitch. (a) 
no deformation (b) localize0 (c) localize1 Figure 12: Localized in.uence of wires. (a) no deformation 
(b) deformation 1 (c) deformation 2 (d) m5    Figure 11: Integration of two squash-stretch deformations 
using wires and different values of m.   (b) Figure 14: Wrinkle propagation using reference curves. 
There are two shortcomings of the above approach. The .rst is that since each object is deformed independently, 
high orders of surface continuity across the stitch cannot be guaranteed. The con­trol afforded by wire 
parameters r,sand fin particular, alleviates this to an extent. Second, seams are currently stitched 
pair-wise, thus imposing a stitching order, which can be restrictive. 5.3 Kinematics for .exible skeletons 
Inverse kinematics on joint chains driving attached object geometry is popular for articulated-.gure 
animation. Most IK solvers, espe­cially ef.cient single chain solvers, have a problem with segments that 
scale non-uniformly during animation. This is essential if, for example, we wish to model a character 
with partially elastic bones. We replace a joint chain with a curve passing through it, so the con­trol 
polygon of the curve acts like an articulated rigid body. We also introduce a rubberband like behavior 
by transforming the con­trol points of the curve proportionally along the joint chain based on the motion 
of the end effector. The result is a semi-elastic skele­tal curve. The curve then deforms the object 
geometry associated with the joint hierarchy as a wire. We use a large dropoff radius r so we can assume 
that every point on the geometry will track the curve equally and precisely (see Figure 5(b)). This in 
itself takes care of smoothing the regions around joints that often require spe­cial techniques to solve. 
Further, the arc-length of the wire curve is used to modulate the wire scale factor s, providing visually 
realistic volume preservation of the geometry on elastic deformations. Fig­ure 20 shows the deformation 
to an arm as the kinematic solution is varied from perfectly rigid to perfectly elastic.   (a) blend 
weight=0 (b) blend weight=0.5 (c) blend weight=1 (d) Varying r   (e) Varying s (f) Tear propagation 
Figure 18: Control over stitch parameters.  6 Discussion This paper has presented an effective geometric 
deformation tech­nique, employing space curves and implicit functions that cleanly aggregate to deform 
an object. Our system has been completely implemented as a module in Aliasjwavefront s Maya production 
modeling, animation and rendering graphics product. The slow­est part of the algorithm is the closest-point 
on curve [14] calcula­tion pRfor points Pof the object geometry. Fortunately, this can be precomputed 
for each point Pand must be recalculated only if the reference curve Ris changed. In such cases, many 
values can be preprocessed, reducing the online wire deformation algorithm to a few vector operations 
per control vertex of the object geom­etry. Multiple wire interactions are accumulated incrementally 
in one pass. Wire deformations work very well alone or in combination with existing techniques. FFDs, 
for example, are well suited for volume­oriented deformations. Arbitrarily shaped lattices can be cumber­some 
to construct for .ner surface-oriented deformations. FFD lat­tices also usually have far more control 
points than wire curves for deformations of similar complexity. Wire curves can help by pro­viding higher 
level control for lattice points to make complex FFD lattices more tractable (see Figure 21). Conversely, 
wires can emu­late FFD lattices (see Figure 13). Wires allow one to localize the complexity of a deformation 
on an object, and they provide a caricature of the object being mod­eled. The coupling of deformation 
and geometry is a signi.cant advantage of wires. The technique also makes it easy to work in a multi-scale 
fashion. At the highest level a user may simply create a few wire curves, associate them with an object 
and move them around to verify that the object s surface properly tracks the mo­tion of the curves. The 
region of the surface to be in.uenced can then be re.ned by adding domain curves and locators to the 
wires; .ner-scale deformations can be added with more wires.  A point of comparison to our approach 
is curve on surface manipulation techniques that are found in some CAGD systems. There, least-squares 
techniques are used to isolate the control ver­tices relevant to a curve placed on or near a surface 
so that motion of the curve displaces the control points, which in turn changes the surface. Wires in 
most ways are a superior interaction technique because they are easier for a user to control, they are 
ef.ciently computed, and they apply to more general object representations. For surface patches with 
a low density of control points, changing a surface by deforming control points may not be as precise 
as a least squares solution, and it can suffer from aliasing artifacts.  In our implementation, some 
of our geometric algorithms could be made more ef.cient. Both .nding the closest-point on curve and .nding 
the region of in.uence of domain curves are good candi­dates for reworking. In our formulation in Section 
2, we noted that there may be several closest points on a curve to a point in space. In cases of wire 
curves with of high curvature, the policy of picking the closest point with the smallest parameter value 
can cause singu­larities in the deformation. Such cases can be handled heuristically by breaking a wire 
curve into multiple wire curves in regions of high curvature.3 While subdivision is rarely necessary, 
it is worth improving our policy to see if extreme cases can be handled auto­matically. Acknowledgements 
We thank Tom Sederberg and Kris Klimaszewski for their edito­rial help in the .nal stages of the paper, 
as well as the anony­mous reviewers. Jeff Bell, Lincoln Holme and the animator geeks of Aliasjwavefront 
were invaluable for their technical and creative help with this project. Finally, our congratulations 
and gratitude to the wonderful technical staff at Aliasjwavefront who made Maya a reality. It is a remarkable 
accomplishment. References [1] A. Barr. Superquadrics and angle-preserving transformations. IEEE Computer 
Graphics and Applications, 1:1 20, 1981. [2] T. Beier and S. Neely. Feature based image metamorphosis. 
Computer Graphics, 26(2):35 42, 1992. [3] J. Bloomenthal and B. Wyvill. Interactive techniques for im­plicit 
modeling. Computer Graphics, 24(4):109 116, 1990. [4] J. Bloomenthal and K. Shoemake. Convolution surfaces. 
Computer Graphics, 25(4):251 256, 1991. [5] J. Chadwick, D. Haumann and R. Parent. Layered construc­tion 
for deformable animated characters. Computer Graphics, 23(3):234 243, 1989. [6] Y.K. Chang and A.P. Rockwood. 
A generalized de Casteljau approach to 3D free-form deformation. Computer Graphics, 28(4):257 260, 1994. 
[7] S. Coquillart. Extended free-form deformations: A sculpt­ing tool for 3D geometric modeling. Computer 
Graphics, 24(4):187 196, 1990. [8] M.-P. Gascuel. An implicit formulation for precise contact modeling 
between .exible solids. Proc. of SIGGRAPH, pages 313 320, 1993. [9] W. Hsu, J. Hughes and H. Kaufman. 
Direct manipulation of free-form deformations. Computer Graphics, 26(2):177 184, 1992. [10] R. MacCracken 
and K. Joy. Free-form deformations with lat­tices of arbitrary topology. Computer Graphics, 181 189, 
1996. [11] N. Magnenat-Thalmann and Y. Yang. Techniques for cloth animation. SIGGRAPH Course Notes C20, 
151 163, 1991. [12] F. Lazarus, S. Coquillart, and P. Jancene. Axial deformations: an intuitive deformation 
technique. Computer-Aided Design, 26(8):607-613, August 1994. [13] T. Sederberg and S. Parry. Free-form 
deformation of solid geometric models. Computer Graphics, 20:151 160, 1986. [14] P. Schneider. Solving 
the Nearest-Point-on-Curve Problem. Graphics Gems, Academic Press, vol.1:607 612, 1990. [15] G. Wyvill, 
C. McPheeters and B. Wyvill. Animating soft ob­jects. Visual Computer, 2:235 242, 1986. 3Recall from 
Section 4 that our formulation ensures that abutting wires do not introduce seaming or bulging artifacts. 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280947</article_id>
		<sort_key>415</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>41</seq_no>
		<title><![CDATA[A new Voronoi-based surface reconstruction algorithm]]></title>
		<page_from>415</page_from>
		<page_to>421</page_to>
		<doi_number>10.1145/280814.280947</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280947</url>
		<keywords>
			<kw><![CDATA[Delaunay triangulation]]></kw>
			<kw><![CDATA[computational geometry]]></kw>
			<kw><![CDATA[medial axis]]></kw>
			<kw><![CDATA[sampling]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.2.1</cat_node>
				<descriptor>Combinatorial algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003624.10003625.10003628</concept_id>
				<concept_desc>CCS->Mathematics of computing->Discrete mathematics->Combinatorics->Combinatorial algorithms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39029844</person_id>
				<author_profile_id><![CDATA[81100150362]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nina]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Amenta]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Texas at Austin, Austin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39069446</person_id>
				<author_profile_id><![CDATA[81339490239]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marshall]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bern]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Xerox PARC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P186934</person_id>
				<author_profile_id><![CDATA[81100211735]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Manolis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kamvysselis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachussetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>289873</ref_obj_id>
				<ref_obj_pid>289867</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Nina Amenta, Marshall Bern and David Eppstein. The Crust and the/3-Skeleton: Combinatorial Curve Reconstruction. To appear in Graphical Models and Image Processing.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>276889</ref_obj_id>
				<ref_obj_pid>276884</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Nina Amenta and Marshall Bern. Surface reconstruction by Voronoi filtering. To appear in 14th ACM Symposium on Computation Geometry, June 1998.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>262980</ref_obj_id>
				<ref_obj_pid>262839</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[D. Attali. r-Regular Shape Reconstruction from Unorganized Points. In 13th ACM Symposium on Computational Geometry, pages 248-253, June 1997.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218424</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[C. Bajaj, F. Bernardini, and G. Xu. Automatic Reconstruction of Surfaces and Scalar Fields from 3D Scans. SIGGRAPH'95 Proceedings, pages 109-118, July 1995.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[F. Bernardini and C. Bajaj. Sampling and reconstructing manifolds using o~-shapes, In 9th Canadian Conference on Computational Geometry, pages 193-198, August 1997.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>357349</ref_obj_id>
				<ref_obj_pid>357346</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[J-D. Boissonnat. Geometric structures for three-dimensional shape reconstruction, ACM Transactions on Graphics 3: 266- 286, 1984.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>175397</ref_obj_id>
				<ref_obj_pid>175396</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[K. Clarkson, K. Mehlhorn and R. Seidel. Four results on randomized incremental constructions. Computational Geometry: Theory and Applications, pages 185-121, 1993.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237269</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[B. Curless and M. Levoy. A volumetric method for building complex models from range images. In SIGGRAPH '96 Proceedings, pages 303-312, July 1996.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[H. Edelsbrunner, D.G. Kirkpatrick, and R. Seidel. On the shape of a set of points in the plane, IEEE Transactions on Information Theory 29:551-559, (1983).]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>156635</ref_obj_id>
				<ref_obj_pid>174462</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[H. Edelsbrunner and E. P. Mt~cke. Three-dimensional Alpha Shapes. ACM Transactions on Graphics 13:43-72, 1994.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[L.H. de Figueiredo and J. de Miranda Gomes. Computational morphology of curves. Visual Computer 11:105-112, 1995.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192227</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[A. Witkin and P. Heckbert. Using particles to sample and control implicit surfaces, In SIGGRAPH'94 Proceedings, pages 269-277, July 1994.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>221616</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[H. Hoppe. Surface Reconstruction from Unorganized Points. Ph.D. Thesis, Computer Science and Engineering, University of Washington, 1994.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134011</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[H. Hoppe, T. DeRose, T. Duchamp, J. McDonald, and W. Stuetzle. Surface Reconstruction from Unorganized Points. In SIGGRAPH' 92 Proceedings, pages 71-78, July 1992.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>263005</ref_obj_id>
				<ref_obj_pid>262839</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[M. Melkemi, ,A-shapes and their derivatives, In 13th ACM Symposium on Computational Geometry, pages 367-369, June 1997]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[G. Taubin and J. Rossignac. Geometric compression through topological surgery. Research Report RC20340, IBM, 1996.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Copyright &#38;#169;1998 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers or to redistribute to lists, requires specific permission and/or a fee. A New Voronoi-Based 
Surface Reconstruction Algorithm Nina Amenta* Marshall Bern Manolis Kamvysselisy UT -Austin Xerox PARC 
M.I.T. Abstract We describe our experience with a new algorithm for the recon­struction of surfaces from 
unorganized sample points in IR3.The al­gorithm is the .rst for this problem with provable guarantees. 
Given a good sample from a smooth surface, the output is guaranteed to be topologically correct and convergent 
to the original surface as the sampling density increases. The de.nition of a good sample is itself interesting: 
the required sampling density varies locally, rig­orously capturing the intuitive notion that featureless 
areas can be reconstructed from fewer samples. The output mesh interpolates, rather than approximates, 
the input points. Our algorithm is based on the three-dimensional Voronoi dia­gram. Given a good program 
for this fundamental subroutine, the algorithm is quite easy to implement. Keywords: Medial axis, Sampling, 
Delaunay triangulation, Com­putational Geometry 1 Introduction The process of turning a set of sample 
points in IR3into a computer graphics model generally involves several steps: the reconstruction of an 
initial piecewise-linear model, cleanup, simpli.cation, and perhaps .tting with curved surface patches. 
We focus on the .rst step, and in particular on an abstract prob­lem de.ned by Hoppe, DeRose, Duchamp, 
McDonald, and Stuet­zle [14]. In this formulation, the input is a set of points in IR3, with­out any 
additional structure or organization, and the desired output is a polygonal mesh, possibly with boundary. 
In practice, sample sets for surface reconstruction come from a variety of sources: med­ical imagery, 
laser range scanners, contact probe digitizers, radar and seismic surveys, and mathematical models such 
as implicit sur­faces. While the most effective reconstruction scheme for any one of these applications 
should take advantage of the special proper­ties of the data, an understanding of the abstract problem 
should contribute to all of them. The problem formulation above is incomplete, since presumably we should 
require some relationship between the input and the out­put. In this and a companion paper [2], we describe 
a simple, com­binatorial algorithm for which we can prove such a relationship. A *Much of this work was 
done while the author was employed by Xerox PARC, partially supported by NSF grant CCR-9404113. yMuch 
of this work was done while the author was an intern at Xerox PARC. Figure 1. The .st mesh was reconstructed 
from the vertices alone. Notice that the sam­pling density varies. Our algorithm requires dense sampling 
only near small features; given such an input, the output mesh is provably correct. nontrivial part of 
this work is the .tting of precise de.nitions to the intuitive notions of a good sample and a correct 
reconstruction . Although the actual de.nition of a good sample is rather techni­cal, involving the medial 
axis of the original surface, Figure 1 gives the general idea: dense in detailed areas and (possibly) 
sparse in featureless ones. The algorithm is based on the three-dimensional Voronoi dia­gram and Delaunay 
triangulation; it produces a set of triangles that we call the crust of the sample points. All vertices 
of crust triangles are sample points; in fact, all crust triangles appear in the Delaunay triangulation 
of the sample points. The companion paper [2] presents our theoretical results. In that paper, we prove 
that given a good sample from a smooth surface, the output of our reconstruction algorithm is topologically 
equiva­lent to the surface, and that as the sampling density increases, the output converges to the surface, 
both pointwise and in surface nor­mal. Theoretical guarantees, however, do not imply that an algorithm 
is useful in practice. Surfaces are not everywhere smooth, samples do not everywhere meet the sampling 
density conditions, and sam­ple points contain noise. Even on good inputs, an algorithm may fail to be 
robust, and the constants on the running time might be prohibitively large. In this paper, we report 
on our implementation of the algorithm, its ef.ciency and the quality of the output. Overall, we were 
pleased. The program gave intuitively reason­able outputs on inputs for which the theoretical results 
do not ap­ply. The implementation, using a freely available exact-arithmetic Voronoi diagram code, was 
quite easy, and reasonably ef.cient: it can handle 10,000 points in a matter of minutes. The main dif.­culty, 
both in theory and in practice, is the reconstruction of sharp edges. 2 Related work The idea of using 
Voronoi diagrams and Delaunay triangulations in surface reconstruction is not new. The well known a-shape 
of Edelsbrunner et al. [9, 10] is a parameterized construction that as­sociates a polyhedral shape with 
an unorganized set of points. A simplex (edge, triangle, or tetrahedron) is included in the a-shape if 
it has some circumsphere with interior empty of sample points, of radius at most a(a circumsphere of 
a simplex has the vertices of the simplex on its boundary). The spectrum of a-shapes, that is, the a-shapes 
for all possible values of a, gives an idea of the over­all shape and natural dimensionality of the point 
set. Edelsbrunner and M¨ucke experimented with using a-shapes for surface recon­struction [10], and Bajaj, 
Bernardini, and Xu [4] have recently used a-shapes as a .rst step in the entire reconstruction pipeline. 
An early Delaunay-based algorithm, similar in spirit to our own, is the Delaunay sculpting heuristic 
of Boissonnat [6], which progressively eliminates tetrahedra from the Delaunay triangula­tion based on 
their circumspheres. In two dimensions, there are a number of recent theoretical results on various Delaunay-based 
approaches to reconstructing smooth curves. Attali [3], Bernar­dini and Bajaj [5], Figueiredo and Miranda 
Gomes [11] and our­selves [1] have all given guarantees for different algorithms. A fundamentally different 
approach to reconstruction is to use the input points to de.ne a signed distance function on IR3,and 
then polygonalize its zero-set to create the output mesh. Such zero­set algorithms produce approximating, 
rather than interpolating, meshes. This approach was taken by Hoppe et al. [14, 13] and more recently 
by Curless and Levoy [8]. Hoppe et al. determine an ap­proximate tangent plane at each sample point using 
least squares on knearest neighbors, and then take the signed distance to the nearest point s tangent 
plane as the distance function on IR3. The distance function is then interpolated and polygonalized by 
the marching cubes algorithm. The algorithm of Curless and Levoy is tuned for laser range data, from 
which they derive error and tangent plane information. They combine the samples into a continuous volumet­ric 
function, computed and stored on a voxel grid. A subsequent hole-.lling step also uses problem-speci.c 
information. Their im­plementation is especially fast and robust, capable of handling very largedatasets. 
Functionally our crust algorithm differs from both the a-shape and the zero-set algorithms. It overcomes 
the main drawback of a-shapes as applied to surface reconstruction, which is that the pa­rameter amust 
be chosen experimentally, and in many cases there is no ideal value of adue to variations in the sampling 
density. The crust algorithm requires no such parameter; it in effect auto­matically computes the parameter 
locally. Allowing the sampling density to vary locally enables detailed reconstructions from much smaller 
input sets. Like the a-shape, the crust can be considered an intrinsic con­struction on the point set. 
But unlike the a-shape, the crust is natu­rally two-dimensional. This property makes the crust more suitable 
for surface reconstruction, although less suitable for determining the natural dimensionality of a point 
set. The crust algorithm is simpler and more direct than the zero­set approach. Zero-set algorithms, 
which produce approximating rather than interpolating surfaces, inherently do some low-pass .l­tering 
of the data. This is desirable in the presence of noise, but causes some loss of information. We believe 
that some of our ideas, particularly the sampling criterion and the normal estimation method, can be 
applied to zero-set algorithms as well, and might be useful in proving some zero-set algorithm correct. 
With its explicit sampling criterion, our algorithm should be most useful in applications in which the 
sampling density is easy to control. Two examples are digitizing an object with a hand­held contact probe, 
where the operator can eyeball the re­quired density, and polygonalizing an implicit surface using sample 
points [12], where the distribution can be controlled analytically.  3 Sampling Criterion Our theoretical 
results assume a smooth surface, by which we mean a twice-differentiable manifold embedded in IRd . Notice 
that this allows all orientable manifolds, including those with multiple con­nected components. 3.1 Geometry 
We start by reviewing some standard geometric constructions. Given a discrete set Sof sample points in 
IRd,the Voronoi cell of a sample point is that part of IRdcloser to it than to any other sample. The 
Voronoi diagram is the decomposition of IRdinduced by the Voronoi cells. Each Voronoi cell is a convex 
polytope, and its vertices are the Voronoi vertices;when Sis nondegenerate, each Voronoi vertex is equidistant 
from exactly d+1points of S.These d+1points are the vertices of the Delaunay simplex, dual to the Voronoi 
vertex. A Delaunay simplex, and hence each of its faces, has a circumsphere empty of other points of 
S. The set of Delau­nay simplices form the Delaunay triangulation of S. Computing the Delaunay triangulation 
essentially computes the Voronoi dia­gram as well. See Figure 5 for two-dimensional examples. Figure 
2. The red curves are the medial axis of the black curves. Notice that compo­nents of the medial axis 
lie on either side of the black curves. Figure 3. In three dimensions, the medial axis of a surface 
is generally a two­dimensional surface. Here, the square is the medial axis of the rounded transparent 
surface. A nonconvex surface would have components of the medial axis on the out­side as well, as in 
the 2D example of Figure 2. The medial axis of a (d,1)-dimensional surface in IRdis (the closure of) 
the set of points with more than one closest point on the surface. An example in IR2is shown in Figure 
2, and in IR3in Figure 3. This de.nition of the medial axis includes components on the exterior of a 
closed surface. The medial axis is the extension to continuous surfaces of the Voronoi diagram, in the 
sense that the Voronoi diagram of Scan be de.ned as the set of points with more than one closest point 
in S. In two dimensions, the Voronoi vertices of a dense set of sam­ple points on a curve approximate 
the medial axis of the curve. Somewhat surprisingly a number of authors have been misled this nice property 
does not extend to three dimensions.  3.2 De.nition We can now describe our sampling criterion. A good 
sample is one in which the sampling density is (at least) inversely proportional to the distance to the 
medial axis. Speci.cally, a sample Sis an r-sample from a surface Fwhen the Euclidean distance from any 
point p2Fto the nearest sample point is at most rtimes the distance from pto the nearest point of the 
medial axis of F. The constant of proportionality ris generally less than one. In the companion paper 
[2], we prove our theorems for small values of rsuch as r::06, but the bounds are not tight. Hence the 
theoretical results apply only when the sampling is very dense. We observe that in practice r=:5generally 
suf.ces. Figure 4 shows a reconstruction from a dense sample, and from a sample thinned to roughly r=:5. 
We did not compute the medial axis, which can be quite a chore. Instead, we used the distance to the 
nearest pole (see Section 4.2) as a reasonable, and easily com­puted, estimate of the distance to the 
medial axis.  Figure 4. The sampling spacing required to correctly reconstruct a surface is propor­tional 
to the distance to the medial axis. On the left is a surface reconstructed from a dense sample. The color 
represents estimated distance to medial axis red means close. On the right, we use the estimated distance 
to thin the data to a :5-sample (meaning that the distance to the nearest sample for any point on the 
surface is at most half the distance to the medial axis), and then reconstruct. There were about 12K 
samples on the left and about 3K on the right. Notice that our sampling criterion places no constraints 
on the distribution of points, so long as they are suf.ciently dense. It in­herently takes into account 
both the curvature of the surface the medial axis is close to the surface where the curvature is high 
and also the proximity of other parts of the surface. For instance, although the middle of a thin plate 
has low curvature, it must be sampled densely to resolve the two sides as separate surfaces. In this 
situation an r-sample differs from the distribution of vertices typically produced by mesh simpli.cation 
algorithms, which only need to consider curvature. At sharp edges and corners, the medial axis actually 
touches the surface. Accordingly, our criterion requires in.nitely dense sam­pling to guarantee reconstruction. 
Sharp edges are indeed a prob- Figure 5. The two-dimensional algorithm. On the left, the Voronoi diagram 
of a point set Ssampled from a curve. Just as Sapproximates the curve, the Voronoi vertices V approximate 
the medial axis of the curve. On the right, the the Delaunay triangulation of SUV, with the crust edges 
in black. Theorem 1 states that when Sis an r-sample, for suf.ciently small r, the crust edges connect 
only adjacent vertices. lem in practice as well, although the reconstruction errors are not noticeable 
when the sampling is very dense. We discuss a heuris­tic approach to resolving sharp edges in Section 
6, and propose a stronger theoretical approach in Section 7.  4 The crust algorithm 4.1 Two Dimensions 
We begin with a two-dimensional version of the algorithm [1]. In this case, the crust will be a graph 
on the set of sample points S. We de.ne the crust as follows: an edge ebelongs to the crust if ehas a 
circumcircle empty not only of all other sample points but also of all Voronoi vertices of S. The crust 
obeys the following theorem [1]. Theorem 1. The crust of an r-sample from a smooth curve F,for r::25, 
connects only adjacent sample points on F. The medial axis provides the intuition behind this theorem. 
An important lemma is that for any sample S, an edge between two nonadjacent sample points cannot be 
circumscribed by a circle that misses both the medial axis and all other samples. When Sis an r-sample 
for suf.ciently small r, the Voronoi vertices approximate the medial axis, and any circumcircle of an 
edge between nonad­jacent samples contains either another sample or a Voronoi vertex. An edge between 
two adjacent samples, on the other hand, is cir­cumscribed by a small circle, far away from the medial 
axis and hence from all Voronoi vertices. The de.nition of the two-dimensional crust leads to the follow­ing 
simple algorithm, illustrated in Figure 5. First compute the Voronoi diagram of S,and let Vbe the set 
of Voronoi vertices. Then compute the Delaunay triangulation of SUV. The crust con­sists of the Delaunay 
edges between points of S, since those are the edges with circumcircles empty of points in SUV. Notice 
that the crust is also a subset of the Delaunay triangulation of the input points; adding the Voronoi 
vertices .lters out the unwanted edges from the Delaunay triangulation. We call this technique Voronoi 
.ltering.  4.2 Three Dimensions This simple Voronoi .ltering algorithm runs into a snag in three dimensions. 
The nice property that all the Voronoi vertices of a suf.ciently dense sample lie near the medial axis 
is no longer true. Figure 6 shows an example. No matter how densely we sample, Voronoi vertices can appear 
arbitrarily close to the surface. Figure 6. In three dimensions, we can use only a subset of the Voronoi 
vertices, since not all Voronoi vertices contribute to the approximation of the medial axis. Here, one 
sample on a curved surface is colored blue, and the edges of its three-dimensional Voronoi cell are drawn 
in red. One red Voronoi vertex lies near the surface, equidistant from the four samples near the center. 
The others lie near the medial axis, near the center of curvature on one side and halfway to an opposite 
patch of the surface on the other. On the other hand, many of the three-dimensional Voronoi ver­tices 
do lie near the medial axis. Consider the Voronoi cell Vsof a sample s, as in Figure 6. The sample sis 
surrounded on Fby other samples, and Vsis bounded by bisecting planes separating s from its neighbors, 
each plane nearly perpendicular to F.So the Voronoi cell Vsis long, thin and roughly perpendicular to 
Fat s. Vsextends perpendicularly out to the medial axis. Near the medial axis, other samples on Fbecome 
closer than s,and Vis cut off. sThis guarantees that some vertices of Vslie near the medial axis. We 
give a precise and quantitative version of this rough argument in [2]. This leads to the following algorithm. 
Instead of using all of the Voronoi vertices in the Voronoi .ltering step, for each sample swe use only 
the two vertices of Vsfarthest from s, one on either side of the surface F. We call these the poles of 
s, and denote them p + , and p. It is easy to .nd one pole, say p +: the farthest vertex of Vsfrom s. 
The observation that Vsis long and thin implies that the , other pole pmust lie roughly in the opposite 
direction. Thus in the , basic algorithm below, we simply choose pto be farthest vertex from s such that 
sp,and sp +have negative dot-product. Here is the basic algorithm: 1. Compute the Voronoi diagram of 
the sample points S 2. For each sample point sdo: (a) If sdoes not lie on the convex hull, let p +be 
the farthest Voronoi vertex of Vsfrom s.Let n +be the vector sp+. (b) If slies on the convex hull, let 
n +be the average of the outer normals of the adjacent triangles.   , (c) Let pbe the Voronoi vertex 
of Vswith negative pro­jection on n +that is farthest from s. 3. Let Pbe the set of all poles p +and 
p,. Compute the Delau­nay triangulation of SUP. 4. Keep only those triangles for which all three vertices 
are sam­ple points in S.  Notice that one does not need an estimate of rto use the crust algorithm; 
the basic algorithm requires no tunable parameters at all. The output of this algorithm, the three-dimensional 
crust,isa set of triangles that resembles the input surface geometrically. More precisely, we prove the 
following theorem [2]. Theorem 2. Let Sbe an r-sample from a smooth surface F,for r::06. Then 1) the 
crust of Scontains a set of triangles forming a mesh topologically equivalent to F, and 2) every point 
on the crust lies within distance 5r.d(p)of some point pon F,where d(p)is the distance from pto the medial 
axis. The crust, however, is not necessarily a manifold; for example, it often contains all four triangles 
of a very .at sliver tetrahedron. It is, however, a visually acceptable model.  Figure 7. The crust 
of a set of sample points and the poles (white points) used in its reconstruction. Each sample selects 
the two vertices of its Voronoi cell that are farthest away, one on either side of the surface, as poles. 
The poles lie near the medial axis of the surface, sketching planes separating opposite sheets of surface 
that degenerate to one-dimensional curves where the cross-section of the surface is circular.  4.3 Normal 
Estimation and Filtering Additional .ltering is required to produce a guaranteed piecewise­linear manifold 
homeomorphic to F, and to ensure that the output converges in surface normal as the sampling density 
increases. In fact, whatever the sampling density, the algorithm above may output some very thin crust 
triangles nearly perpendicular to the surface. We have an important lemma [2], however, which states 
++,, that the vectors n=spand n=spfrom a sample point to its poles are guaranteed to be nearly orthogonal 
to the surface at s. The angular error is linear in r. The intuition (put nicely by Ken Clarkson) is 
that the surface normal is easy to estimate from a point far away, such as a pole p, since the surface 
must be nearly normal to the largest empty ball centered at p. We can use these vectors in an additional 
normal .ltering step, throwing out any triangles whose normals differ too much from n + , or n. When 
normal .ltering is used, the normals of the output triangles approach the surface normals as the sampling 
density in­creases. We prove in [2] that the remaining set of triangles still con­tains a subset forming 
a piecewise-linear surface homeomorphic to F.  Figure 8. The crust of points distributed on an implicit 
surface (left). The additional normal .ltering step is needed to separate the two connected components 
(right), which are undersampled at their closest point. Triangles are deleted if their normals differ 
too much from the direction vectors from the triangle vertices to their poles. These vectors are provably 
close to the surface normals. Normal .ltering can be useful in practice as well, as shown in Figure 8. 
In the usual case in which ris unknown the allowable difference in angle must be selected experimentally. 
Normal .lter­ing can be dangerous, however, at boundaries and sharp edges. The directions of n +and n,are 
not nearly normal to all nearby tangent planes, and desirable triangles might be deleted. We note that 
n +and n,, our Voronoi-based estimates of nor­mal direction, could be useful in the zero-set reconstruction 
meth­ods, which depend on accurate estimation of the tangent planes. For the algorithm of Hoppe et al. 
[14], a Voronoi-based estimate could replace the estimate based on the k-nearest neighbors. The Voronoi-based 
estimate has the advantage that it is not sensitive to the distribution; whereas, for instance, on medical 
image data, all knearest neighbors might lie in the same slice, and so would the estimated tangent plane. 
In the algorithm of Curless and Levoy [8], the Voronoi-based estimate could be checked against the bounds 
on normal direction derived from the laser-range scanner.  4.4 Manifold Extraction After the normal 
.ltering step, all the remaining triangles are roughly parallel to the surface. We can de.ne a sharp 
edge as one which is adjacent to triangles only on one side of a plane through the edge and roughly perpendicular 
to the surface. Notice that an edge of degree one counts as a sharp edge. If the surface Fis indeed a 
smooth manifold without boundary, we are guaranteed that the normal-.ltered crust contains a piecewise-linear 
manifold homeo­morphic to F. Any triangle adjacent to a sharp edge cannot belong to this piecewise-linear 
manifold, and can be safely deleted. We continue recursively until no such triangle remains. A piecewise­linear 
manifold can then be obtained by a manifold extraction step which takes the outside surface of the remaining 
triangles on each connected component. This simple approach, however, cannot be applied when Fis not 
a smooth manifold without boundary. In that case we do not know how to prove that we can extract a manifold 
homeomorphic to F. 4.5 Complexity The asymptotic complexity of the crust algorithm is O(n 2)where n=jSj, 
since that is the worst-case time required to compute a three-dimensional Delaunay triangulation. Notice 
that the number of sample points plus poles is at most 3n. As has been frequently observed, the worst-case 
complexity for the three-dimensional De­launay triangulation almost never arises in practice. All other 
steps are linear time.  5 Implementation 5.1 Numerical Issues Robustness has traditionally been a concern 
when implementing combinatorial algorithms like this one. Our straightforward imple­mentation, however, 
is very robust. This success is due in large part to the rapidly improving state of the art in Delaunay 
triangula­tion programs. We used Clarkson s Hull program. Hull uses exact integer arithmetic, and hence 
is thoroughly robust, produces exact output, and requires no arithmetic tolerancing parameters. The per­formance 
cost for the exact arithmetic is fairly modest, due to a clever adaptive precision scheme. We chose Hull 
so that we could be sure that numerical problems that arose were our own and did not originate in the 
triangulation. Finding the exact Delaunay trian­gulation is not essential to our algorithm. Hull outputs 
a list of Delaunay tetrahedra, but not the coordi­nates of their circumcenters (the dual Voronoi vertices) 
which al­ways contain some roundoff error. Fortunately, the exact positions of the poles are not important, 
as the numerical error is tiny relative to the distance between the poles and the surface. We computed 
the location of each Voronoi vertex by solving a 4x4linear system with a solver from LAPACK. The solver 
also returns the condition number of the coef.cient matrix, which we used to reject unreli­able Voronoi 
vertices. Rejected Voronoi vertices were almost al­ways circumcenters of slivers (nearly planar tetrahedra) 
lying .at on the surface; for a good sample such vertices cannot be poles. It is possible that this method 
also rejects some valid poles induced by very .at tetrahedra spanning two patches of surface. We have 
not, however, observed any problems in practice. Presumably there is always another Voronoi vertex nearby 
that makes an equally good pole.  5.2 Ef.ciency Running times for the reconstruction of some large data 
sets are given in the table below; the reconstructions are shown in Figure 9. We used an SGI Onyx with 
512M of memory. Model Time (min) Num. Pts. Femur 2 939 Golf club 12 16864 Foot 15 20021 Bunny 23 35947 
 The running time is dominated by the time required to com­pute the Delaunay triangulations. Hull uses 
an incremental algo­rithm [7], so the running time is sensitive to the input order of the vertices. The 
triangulation algorithm builds a search structure con­currently with the triangulation itself; the process 
is analogous to sorting by incrementally building a binary search tree. When points are added in random 
order, the search structure is balanced (with extremely high probability) and the expected running time 
is opti­mal. In practice, random insertions are slow on large inputs, since both the search structure 
and the Delaunay triangulation begin pag­ing. We obtained better performance by .rst inserting a random 
subset of a few thousand points to provide a balanced initial search structure, and then inserting the 
remaining points based on a crude spatial subdivision to improve locality. Most likely much greater improvements 
in ef.ciency can be achieved by switching to a three-dimensional Delaunay triangula­tion program that, 
.rst, does not use exact arithmetic, and second, uses an algorithm with more locality of reference. 
 Figure 9. Femur, golf club, foot and bunny reconstructions. Notice the subtle 3 on the bottom of the 
club (apparently a 3-iron), showing the sensitivity of the algorithm. The foot, like all our reconstructions, 
is hollow. The bunny was reconstructed from the roughly 36K vertices of the densest of the Stanford bunny 
models in 23 minutes.  6 Heuristic Modi.cations As we have noted, our algorithm does not do well at 
sharp edges, either in theory or in practice. The reason is that the Voronoi cell of a sample son a sharp 
edge is not long and thin, so that the as­sumptions under which we choose the poles is not correct. For 
example, the Voronoi cell of a sample son a right-angled edge is roughly fan-shaped. The vector n +directed 
towards the .rst pole of smight be perpendicular to one tangent plane at s, but parallel to the other. 
The second pole would then be chosen very near the surface, punching a hole in the output mesh.  We 
experimented with other methods for choosing the second , pole. We found that choosing as pthe Voronoi 
vertex with the greatest negative projection in the direction n +gave somewhat bet­ter results. This 
modi.cation should retain the theoretical guaran­tees of the original algorithm. The best reconstructions, 
however, were produced by a different heuristic: choosing the farthest and the second farthest Voronoi 
vertices, regardless of direction, as the two poles (see Figure 10). This heuristic is strongly biased 
against choosing poles near the surface, avoiding gaps near sharp edges but sometimes allowing excess 
triangles .lling in sharp corners. We believe that pathological cases could be constructed in which this 
.ll causes a topologically incorrect reconstruction irrespective of the sampling density. Boundaries 
pose similar problems in theory, but the reconstruc­tions produced by the crust algorithm on surfaces 
with boundaries are usually acceptable. Figure 7 and the foot in Figure 9 are ex­amples of perfectly 
reconstructed boundaries. When the boundary forms a hole in an otherwise .at surface, with no other parts 
of the surface nearby, the crust algorithm .lls in the hole. Undersampling also causes holes in the output 
mesh. For ex­ample, consider a sample in the middle of a a .at plate. Although its second pole lies in 
the correct direction, if there are two few sample points on the opposite side of the plate, the pole 
may fall near the surface on the opposite side and cause a hole. We experi­mented with heuristics to 
compensate for this undersampling effect, and for similar reconstruction errors in undersampled cylindrical 
regions. We found that moving all poles closer to their samples by some constant fraction allowed thin 
plates and cylinders to be re­constructed from fewer samples, while sometimes introducing new holes on 
other parts of the model. We were sometimes able to get a perfect reconstruction by taking the union 
of a crust made with this modi.cation and one without.  7 Research Directions We have identi.ed a number 
of future research directions. 7.1 Noise Small perturbations of the input points do not cause problems 
for the crust algorithm, nor do a few outliers. But when the noise level is roughly the same as the sampling 
density, the algorithm fails, both in theory and in practice. We believe, however, that there is a Voronoi-based 
algorithm, perhaps combining aspects of crusts and a-shapes, that reconstructs noisy data into a thickened 
surface containing all the input points, some of them possibly in the interior. See Melkemi [15] for 
some suggestive experimental work in IR2 . 7.2 Sharp Edges and Boundaries We would like to modify the 
crust algorithm to handle surfaces with sharp edges and to provide theoretical guarantees for the re­construction 
of both sharp edges and boundaries. Interpolating reconstruction algorithms like ours have an advantage 
here, since approximating reconstruction algorithms smooth out sharp edges. One important goal is to 
develop reliable techniques for identify­ing samples that lie on sharp edges or boundaries. As noted, 
the Voronoi cells of such samples are not long and thin. This intuition could be made precise, and perhaps 
combined with more traditional .ltering techniques. 7.3 Using Surface Normals A variation on the problem 
is the reconstruction of surfaces from unorganized points that are equipped with normal directions. This 
problem arises in two-dimensional image processing when connect­ing edge pixels into edges. In three 
dimensions, laser range data comes with some normal information, and we have exact normals for points 
distributed on implicit surfaces. It should be possible to show that with this additional information, 
reconstruction is pos­sible from much sparser samples. In particular, when normals are available, dense 
sampling should not be needed to resolve the two sides of a thin plate, suggesting that a different sampling 
criterion than distance to medial axis is required.  7.4 Compression One intriguing potential application 
(pointed out by Frank Bossen) of interpolating, rather than approximating reconstruction, is that it 
can be used as a lossless mesh compression technique. A model cre­ated by interpolating reconstruction 
can be represented entirely by its vertices, and no connectivity information at all must be stored. A 
model which differs only slightly from the reconstruction of its vertices can be represented by the vertices 
and a short list of differ­ences. These differences might be encoded ef.ciently using some geometrically 
de.ned measure of likelihood on Delaunay trian­gles. The vertices themselves could then be ordered so 
as to opti­mize properties such as compressibility or progressive reconstruc­tion by an incremental algorithm. 
With the current best geometry compression method [16], most of the bits are already used to en­code 
the vertex positions, rather than connectivity, but the connec­tivity is encoded in the ordering of the 
vertices. Allowing arbitrary vertex orderings could improve compression; we are experimenting with an 
octree encoding. Our current crust algorithm is not incremental, and our imple­mentation is too slow 
for real-time decompression, so this applica­tion motivates work in both directions.   Acknowledgments 
We thank David Eppstein (UC Irvine) for his collaboration in the early stages of this research, and Frank 
Bossen (EPF Lausanne) and Ken Clarkson (Lucent) for interesting suggestions. We thank Ping Fu (Raindrop 
Geomagic) for the .st and the mechanical part, Hughes Hoppe (Microsoft) for the head, the golf club and 
the foot, Chandrajit Bajaj (UT Austin) for the femur, Paul Heckbert (CMU) for the hot dogs, and the Stanford 
Data Repository for the bunny. We thank Ken Clarkson and Lucent Bell Labs for Hull,and The Ge­ometry 
Center at the University of Minnesota for Geomview,which we used for viewing and rendering the models. 
 References [1] Nina Amenta, Marshall Bern and David Eppstein. The Crust and the {-Skeleton: Combinatorial 
Curve Reconstruction. To appear in Graphical Models and Image Processing. [2] Nina Amenta and Marshall 
Bern. Surface reconstruction by Voronoi .ltering. To appear in 14th ACM Symposium on Com­putation Geometry, 
June 1998. [3] D. Attali. r-Regular Shape Reconstruction from Unorganized Points. In 13th ACM Symposium 
on Computational Geometry, pages 248 253, June 1997. [4] C. Bajaj, F. Bernardini, and G. Xu. Automatic 
Reconstruction of Surfaces and Scalar Fields from 3D Scans. SIGGRAPH 95 Proceedings, pages 109 118, July 
1995. [5] F. Bernardini and C. Bajaj. Sampling and reconstructing man­ifolds using a-shapes, In 9th Canadian 
Conference on Com­putational Geometry, pages 193 198, August 1997. [6] J-D. Boissonnat. Geometric structures 
for three-dimensional shape reconstruction, ACM Transactions on Graphics 3: 266 286, 1984. [7] K. Clarkson, 
K. Mehlhorn and R. Seidel. Four results on ran­domized incremental constructions. Computational Geome­try: 
Theory and Applications, pages 185 121, 1993. [8] B. Curless and M. Levoy. A volumetric method for building 
complex models from range images. In SIGGRAPH 96 Pro­ceedings, pages 303 312, July 1996. [9] H. Edelsbrunner, 
D.G. Kirkpatrick, and R. Seidel. On the shape of a set of points in the plane, IEEE Transactions on Information 
Theory 29:551-559, (1983). [10] H. Edelsbrunner and E. P. M¨ucke. Three-dimensional Alpha Shapes. ACM 
Transactions on Graphics 13:43 72, 1994. [11] L. H. de Figueiredo and J. de Miranda Gomes. Computational 
morphology of curves. Visual Computer 11:105 112, 1995. [12] A. Witkin and P. Heckbert. Using particles 
to sample and con­trol implicit surfaces, In SIGGRAPH 94 Proceedings, pages 269 277, July 1994. [13] 
H. Hoppe. Surface Reconstruction from Unorganized Points. Ph.D. Thesis, Computer Science and Engineering, 
University of Washington, 1994. [14] H. Hoppe, T. DeRose, T. Duchamp, J. McDonald, and W. Stuetzle. Surface 
Reconstruction from Unorganized Points. In SIGGRAPH 92 Proceedings, pages 71 78, July 1992. [15] M. Melkemi, 
A-shapes and their derivatives, In 13th ACM Symposium on Computational Geometry, pages 367 369, June 
1997 [16] G. Taubin and J. Rossignac. Geometric compression through topological surgery. Research Report 
RC20340, IBM, 1996.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280948</article_id>
		<sort_key>423</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>42</seq_no>
		<title><![CDATA[Computer-generated floral ornament]]></title>
		<page_from>423</page_from>
		<page_to>434</page_to>
		<doi_number>10.1145/280814.280948</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280948</url>
		<keywords>
			<kw><![CDATA[adaptive clip art]]></kw>
			<kw><![CDATA[conventionalization]]></kw>
			<kw><![CDATA[ornamentation]]></kw>
			<kw><![CDATA[pattern generation]]></kw>
			<kw><![CDATA[plant development]]></kw>
			<kw><![CDATA[texture generation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Fine arts</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010470</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Fine arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Languages</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P199321</person_id>
				<author_profile_id><![CDATA[81332536367]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[T.]]></middle_name>
				<last_name><![CDATA[Wong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Washington, Seattle]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P70068</person_id>
				<author_profile_id><![CDATA[81100437187]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Douglas]]></first_name>
				<middle_name><![CDATA[E.]]></middle_name>
				<last_name><![CDATA[Zongker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Washington, Seattle]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P63622</person_id>
				<author_profile_id><![CDATA[81100188207]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Salesin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Washington, Seattle]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1138362</ref_obj_id>
				<ref_obj_pid>563732</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Howard Alexander. The computer/plotter and the 17 ornamental design types. In Proceedings ofSIGGRAPH'75, pages 160-167. 1975.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[J. Arvo and D. Kirk. Modeling plants with environment-sensitive automata. In Ausgraph 88 proceedings, pages 27-33. 1988.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801141</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Richard Beach and Maureen Stone. Graphical style--towards high quality illustrations. In Proceedings of SIGGRAPH '83, pages 127-135. 1983.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Albert Fidelis Butsch. Handbook of Renaissance Ornament: 1290 Designs fivm Decorated Books. Dover Publications, Inc., New York, 1969.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[William Caxton. History of Reynard the Foxe. Kelmscott, Hammersmith, England, 1892.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Archibald H. Christie. Traditional Methods of Pattern Designing. The Clarendon Press, Oxford, 1929.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Theodore A. Cook. The Curves of Life. Constable and Company, London, 1914.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Walter Crane. Line and Form. G. Bell &amp; Sons, London, 1902.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Joseph D'Addetta. Traditional Japanese Design Motifs. Dover Publications, Inc., New York, 1984.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Lewis E Day. Nature in Ornament. B.T. Batsford, London, 1898.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>2295780</ref_obj_id>
				<ref_obj_pid>616080</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Andrew Glassner. Frieze groups. IEEE Computer Graphics and Applications, 16(3):78-83, May 1996.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[E.H. Gombrich. The Sense of Order. Phaidon Press Limited, London, 1994.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74351</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[N. Greene. Voxel space automata: Modeling with stocastic growth processes in voxel space. In P1vceedings of SIGGRAPH '89, pages 175-184. 1989.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>19304</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Branko Grfinbaum and G. C. Shephard. Tilings and Patterns. W.H. Freeman, New York, 1987.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>168662</ref_obj_id>
				<ref_obj_pid>168642</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Siu Chi Hsu, I. H. H. Lee, and N. E. Wiseman. Skeletal strokes. In Proceedings of UIST '93, pages 197-206. 1993.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192186</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Siu Chi Hsu and Irene H. H. Lee. Drawing and animation using skeletal strokes. In P1vceedings of SIGGRAPH'94, pages 109-118.1994.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Owen Jones. Grammar of Ornament. Day and son, London, 1856.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Owen Jones. The Grammar of Chinese Ornament. Portland House, New York, 1987.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Sherman E. Lee. The Genius of Japanese Design. Kodansha International, Tokyo, 1981.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237279</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Radomfr Mech and Przemyslaw Prusinkiewicz. Visual models of plants interacting with their environment. In P~vceedings of SIGGRAPH' 96, pages 397-410. 1996.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Franz S. Meyer. Handbook of Ornament. Dover Publications, New York, 1957.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[William Morris. A Book of verse. Kelmscott, Hammersmith, England, 1870.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[William Morris. Ideal Book: Essays and Lectures on the Arts of the Book. University of California Press, Berkeley, 1982.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[William Morris and A.J. Wyatt. The Tale of Beowulf. Kelmscott Press, Hammersmith, England, 1895.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[K. Prakash. Paisleys and Other Textile Designs flvm India. Dover, New York, 1994.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192254</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Przemyslaw Prusinkiewicz, Mark James, and Radomir Mech. Synthetic topiary. In P1vceedings of SIGGRAPH '94, pages 351-358. 1994.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83596</ref_obj_id>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Przemyslaw Prusinkiewicz and Aristid Lindenmayer. The Algorithmic Beauty of Plants. Springer-Verlag, New York, 1990.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378503</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Przemyslaw Prusinkiewicz, Aristid Lindenmayer, and James Hanan. Developmental models of herbaceous plants for computer imagery purposes. In P1vceedings of SIGGRAPH '88, pages 141-150. 1988.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Auguste Racinet. Polychromatic Ornament. Firmin Didot freres, ills &amp; cie, Paris, 1873.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>578095</ref_obj_id>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[A. Rosenfeld and A. C. Kak. Digital Picture Processing. Academic Press, New York, 1976.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Henry Shaw. The Encylopedia of Ornament. J. Grant, Edinburgh, 1898.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>730344</ref_obj_id>
				<ref_obj_pid>647560</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Gift Siromoney and Rani Siromoney. Rosenfeld's cycle grammars and kolam. In Graph-Grammars and Their Application to Computer Science, pages 564-579. Springer-Verlag, Berlin, 1986.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808571</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Alvy Ray Smith. Plants, fractals, and formal languages. In P1vceedings of SIG- GRAPH '84, pages 1-10. 1984.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[M.R Verneuil. Floral Patterns. Dover, New York, 1981.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Otto von Falke. Decorative Silks. W. Helburn, Inc., New York, 1922.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[James Ward. The Principles of Ornament. Scribner, New York, 1896.]]></ref_text>
				<ref_id>36</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer-Generated Floral Ornament Copyright &#38;#169;1998 by the Association for Computing Machinery, 
Inc. Permission to make digital or hard copies of part or all of this work for personal or classroom 
use is granted without fee provided that copies are not made or distributed for profit or commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for components 
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy 
otherwise, to republish, to post on servers or to redistribute to lists, requires specific permission 
and/or a fee. Michael T. Wong Douglas E. Zongker David H. Salesin University of Washington Abstract This 
paper describes some of the principles of traditional .oral ornamental design, and ex­plores ways in 
which these designs can be created algorithmically. It introduces the idea of adaptive clip art, which 
encapsulates the rules for creating a speci.c ornamental pattern. Adaptive clip art can be used to generate 
pat­terns that are tailored to .t a particularly shaped region of the plane. If the region is resized 
or re­shaped, the ornament can be automatically re­generated to .ll this new area in an appropriate way. 
Our ornamental patterns are created in two steps: .rst, the geometry of the pattern is gen­erated as 
a set of two-dimensional curves and .lled boundaries; second, this geometry is ren­dered in any number 
of styles. We demonstrate our approach with a variety of .oral ornamental designs. CR Categories: I.3.3 
[Computer Graphics]: Picture/Image Generation; I.3.4 [Computer Graphics]: Graphics Utilities Picture 
description languages. Additional Keywords: adaptive clip art, conventionaliza­tion, pattern generation, 
plant development, ornamentation, texture generation Introduction If I were asked to say what is at 
once the most important production of Art and the thing most to be longed for, I should answer, A beautiful 
House; and if I were further asked to name the production next in importance and the thing next to be 
longed for, I should an­swer, A beautiful Book. To enjoy good houses and good books in self-respect and 
decent comfort, seems to me to be the pleasurable end towards which all societies of human beings ought 
now to struggle. William Morris, 1892 [23] Ornament is among the oldest forms of human expression, already 
well developed by the Ne­olithic Age [6]. Nearly all the commissioned writing of the Middle Ages was 
decorated with ornament, and the illuminated manuscripts of the 13th century rank among the most beautiful 
books ever produced. Even the earliest printed books were often il­luminated by hand, but by about 1530 
such carefully crafted illumination had all but dis­appeared [23]. Today, documents are produced with 
greater ease and in greater number than ever, thanks to ubiquitous desktop publishing tools yet, beyond 
the use of static clip art el­ements, these tools provide precious little sup­port for ornamenting the 
page. Similarly, in ar­chitecture, ornament has historically played a critical and famous role. However, 
most mod­ern buildings, despite the help of sophisticated CAD tools, are largely devoid of these beautiful 
decorations. Though technological advances have virtually ignored the creation of ornament, they have 
at the same time provided new opportunities for its use. The dynamic nature of Web documents encourages 
ornament to be generated on the .y to accommodate different browser con.gura­tions and fonts. New printing 
processes make it feasible to print on fabric or wallpaper in small runs, raising the possibility of 
their custom de­sign and production. This paper therefore provides an early explo­ration into how aesthetically 
pleasing orna­ments might be generated algorithmically. The method we describe attempts to capture the 
essence of an ornamental pattern, encoding it as a set of rules, which we call adaptive clip art. This 
encoding allows the ornament to be de.ned in a manner that is independent of a spe­ci.c areal boundary. 
The adaptive clip art so de­scribed can be used to generate ornaments that are automatically tailored 
to any particular re­ Figure 1 Design element categories. (a) Geometric forms (after Alhambra tile) 
[29, plate 29]. Natural forms (b) plants (Gothic vine) [10, .g. 82], (c) animal/human forms (border detail, 
Germany 1518) [4, plate 30], (d) physiographic forms (17th century Japanese wave motif) [9], (e) arti.cial 
objects (Renaissance torches) [21, plate 80]. gion of interest; moreover, if the region is changed, the 
ornament can be regenerated to .ll the new area in an appropriate way. The automatic creation of aesthetically 
pleasing ornament is a mon­umental challenge, which we by no means claim to solve here. Nev­ertheless, 
we hope that this paper will offer some interesting new di­rections, with the hope that further advances 
may someday help in the creation of beautiful ornaments for our houses and books and online chat rooms 
and web pages! 1.1 Problem statement The problem space of all possible ornamental design is simply enor­mous. 
In order to approach the problem at all, we need to limit our domain. We therefore make the following 
taxonomy (adapted from Meyer [21]). First, the elements of ornamental design can be broken down into 
three broad categories: 1. geometrical elements, such as lines, polygons, ovals, and the like (Figure 
1a); 2. natural forms, which can be further classi.ed as 1. plants (Figure 1b), 2. animal/human forms 
(Figure 1c), 3. physiographic features (Figure 1d); and  3. arti.cial objects, such as shields, ribbons, 
or torches (Fig­ure 1e).  Second, for our purposes we will similarly divide the applications of ornament 
into four main contexts: A. to bands, which have .nite thickness in one dimension and are in.nitely repeating 
in the other (Figure 2a); B. to half-open borders, which are tightly constrained along one or more edges, 
but open in other directions (Figure 2b); C. to panels, which are arbitrary bounded regions of the plane 
(Figure 2c); and D. to the open plane, in which the ornament typically becomes a repeating pattern (Figure 
2d). In this paper, we restrict the problem space to the case of producing .oral growth within panels 
(case 2.1-C in the classi.cation above). In particular, we will look at the challenging issues of structuring 
.o­ral ornament according to various principles of ornamental design, such as balance, analogy, and intention 
as described in Section 2. We will not, however, focus here on designs involving strict symme­tries. 
As we shall see, the resulting design space is still quite large; however, it is at least constrained 
enough that we can explore a se­ries of related approaches within the con.nes of a single research paper. 
Moreover, we expect that many of the approaches suggested here will be useful, in some form, for other 
cases in the taxonomy. In the rest of this paper, we describe a number of principles of .oral ornamental 
design, and we discuss ways in which such designs can be created algorithmically. Figure 2 Applications 
of ornament: (a) bands (16th century Germany) [31, plate 34] (b) half-open borders [24, opening page 
of chapter 7], (c) panels (oak leaf vine from the cathedral of Toledo) [10, .g. 104], (d) open plane 
[35, .g. 270]. 1.2 Related work The area of ornamental design synthesis has received relatively little 
attention in the computer graphics community, to our knowledge. At SIGGRAPH 75 (the 2nd annual SIGGRAPH 
conference), Alexander described a Fortran program for generating the 17 sym­metrypatternsintheplane[1].Gr¨unbaum 
andShephardusedamore sophisticated computer program to generate periodic tilings and pat­terns in their 
landmark text on the subject [14]. However, in both of these cases, the ornamental designs produced are 
purely geometric and purely on the open plane. Glassner examined the synthesis of frieze patterns, which 
can be used for generating textures for band ornaments [11]. Siromoney and Siromoney examined the synthesis 
of kolam pat­terns: a form of ephemeral ornament practiced in India where grains of rice are used to 
trace out designs forming intricate lattices [32]. Their goal, however, was to show how graph grammars 
could be used to generate instances of such geometric patterns, rather than to create ornament to .ll 
a speci.ed region. Arvo and Kirk introduced the modeling of plant growth with environmentally sensitive 
automata [2], Greene examined the growth of plant-like branching structures in voxel space [13], and 
Prusinkiewicz et al. examined the generation of ornamental topiary plant forms with open L-systems [26]. 
The synthetic structures de­scribed in these papers were adaptive to space, but not designed to grow 
according to conventions of 2D ornamentation. Smith introduced the graphics community to the modeling 
of plant growth with a class of parallel rewriting grammars he termed graftals [33]. The grammars were 
used to generate a branching structure, which could then be given visual character through a post­processing 
step. We use a similar two-step procedure to create .rst the structure and then the rendering of our 
ornaments. In their paper on graphical style sheets, Beach and Stone introduced the idea of procedurally 
generating a simple repeating border pat­tern that is warped to follow the path of a spline [3]. This 
idea was subsequently elaborated by Hsu and Lee, in their papers on skele­tal strokes, to the warping 
of prede.ned vector clip art along a path [15, 16]. Skeletal strokes whose commercial implementation, 
MetaCreations Expression, we have used to render many of the il­lustrations in this paper may be thought 
of as a rudimentary form of adaptive clip art along curvilinear paths. The work described in this paper 
builds on their approach by creating a higher-level mech­anism for the automatic arrangement of skeletal 
strokes within arbi­trary regions of the plane. 1.3 Overview The rest of this paper is organized as 
follows. Section 2 surveys the key principles of .oral ornamental design. Section 3 discusses how these 
principles can be encapsulated algorithmically. Section 4 dis­cusses the framework of our ornamental 
growth engine. Section 5 presents some of our results, and Section 6 suggests areas for future research. 
Finally, Appendix A shows in detail some simple exam­ples of using our system.  2 Principles of ornamental 
design For our purposes, we will de.ne ornament as the aesthetic enrich­ment of the surfaces of man-made 
objects in ways not directly con­tributing to their functional utility. In order to provide a sense of 
the richness and depth of the problems involved in creating ornament, we will brie.y describe some of 
the principles that underlay its de­sign. The system we have implemented so far addresses only a frac­tion 
of these principles. Let s .rst look at some of the methods ornamentalists use in con­veying a perception 
of order. We will then explore the particulars of .oral ornamental design. 2.1 Order in ornament If there 
is any one underlying principle of ornament, it is the con­veyance of a sense of order or design [12]. 
Ornamentalists use three principal techniques in conveying a perception of order: repetition, balance, 
and conformation to geometric constraints [10, 12, 36]. 2.1.1 Repetition Perhaps the most fundamental 
ordering principle is repetition.The repetition of even the simplest mark can form the basis of an or­nament. 
When forms are repeated, they may be repeated exactly through translation and rotation (Figure 3a). Or 
they may be re­.ected about some axis, yielding bilateral symmetry (Figure 3c) or glide re.ection (Figure 
3b). In many patterns containing rotational symmetries, the point of radiation is positioned off-center 
from the design elements it controls, leading to a bilaterally symmetric radi­ation (Figure 3d). A more 
subtle form of repetition is the use of analogy,in which sim­ilar, rhythmic controlling lines are used 
to place and constrain dif­ferent .oral or .gurative elements (Figure 3e). In addition, the re­currence 
of almost any ratio, or proportion, in a design can impart a pleasing unity of form. Color is another 
powerful attribute of pat­terns, orthogonal to shape, that can be used to unify a design through repetition. 
While designs based on rigid repetition may appeal to a clean, aus­tere aesthetic, other patterns use 
variation within a class of forms to add organic dynamism to their composition (Figure 3h). This vari­ation 
may be achieved through alternation of color or form (Fig­ure 3f), or through scaled repetition (Figure 
3g). 2.1.2 Balance The principle of balance requires that asymmetrical visual masses be made of equal 
weight. Figure 4a shows this principle applied to several compositions. We can also speak of balance 
in the implicit motion of lines. Crane [8] describes this phenomenon as each new Figure 3 Repetition: 
(a) simple translation [4, plate 142], (b) glide re.ection [10, plate 14], (c) re.ection [36, cover illustration], 
(d) radiation (late Gothic pine ornament) [10], (e) analogous (rhythmic lines in the frieze of the Parthenon) 
[8], (f) alternation [34, plate 78], (g) scaled [9], (h) organic variation [34, plate 27]. line posing 
a question that requires an answering line (Figure 4b). We can see both these principles at work in Figure 
4c. The principle of balanced masses, combined with the primal motiva­tion for ornamentation, horror 
vacui, yields the principle of uniform density: ornament should uniformly .ll its allotted space. In 
some ornaments, elements of similar mass are distributed non-uniformly in space. In this case, their 
unequal distribution can be balanced with different elements of a smaller scale. This type of ordering 
leads to a balance within and among levels of hierarchies of visual mass (Fig­ure 4d). 2.1.3 Conformation 
to geometric constraints Since ornament must live within the boundaries of the objects it seeks to enrich, 
the design process must generally begin with a con­sideration of geometric constraints. First and foremost, 
a careful .tting to boundaries is a hallmark of or­nament from many cultures. Often, the period of a 
meandering vine, for instance, has to be adjusted not only to .t properly between the Figure 4 Balance: 
(a) in composition [8], (b) question and answer within lines [8], (c) combined [36, .g. 126], (d) hierarchical 
[5, title page]. top and bottom edges of the panel, but also to provide appropriate positions for secondary 
shoots to invade other portions of the orna­mented region (Figure 5a). In addition, the shapes of the 
design ele­ments themselves are sometimes deformed to better .ll space (Fig­ure 5b). In many vining motifs, 
elements are made to grow together tangen­tially. This principle of tangential junction lends a powerful 
sense of teleological, or ends-driven, design to the composition. For obvious reasons of structural integrity, 
tangential junction is also important for ornament that is cut through or must otherwise hang together, 
such as the open-work bronze basket in Figure 5c, and the sign sup­port in Figure 5d. A further principle 
ordering the layout of motifs is placement at sig­nal geometric points such as points of maximum concavity 
or con­vexity, as in the rosettes of Figure 5e. When .lling a region that has distinct corners, a design 
element is almost always dedicated to the task of .lling each corner. When accomplishing this task with 
a growth motif, the growth is often coordinated by the skeleton of the region to be .lled, as demonstrated 
by the paisley in Figure 5f. The design of ornament frequently proceeds through the subdivision of an 
area followed by the .lling of the divisions. Figure 5g shows the sequence of steps taken by a 19th-century 
textile designer from India in laying out a woodblock print. Since the act of .lling may also be viewed 
as one of subdivision, the process may be recursively repeated, leading to a many-tiered hierarchical 
composition in the .nal design.  2.2 Floral ornament For our purposes, we will de.ne .oral ornament 
as any ornamental design process involving plant-like growth models, such as branch­ing structures; or 
plant-like elements, such as vines, leaves, or .ow­ers. In this section we will .rst examine the peculiar 
qualities of growth that distinguishes it as a progenitor of ornamental design. We will then discuss 
how plant-like structures can be transformed into orna­mental elements through the process of conventionalization. 
 Figure 5 Conformation to geometric constraints: (a) .tting meander period (drawn after [22, p. 35]), 
(b) deformation of design elements [24], (c) tangential junction (drawn after [19, p. 107]), (d) tangential 
junction (drawn after [12, .g. 66]), (e) signal geometric points [36, .g. 99], (f) following skeleton 
of a region [25], (g) hierarchical subdivision [6, .g. 213]. 2.2.1 Growth To begin with, it is worth 
noting that most of the ornamental prin­ciples discussed so far are already principles of growth. As 
Owen Jones observed in the Grammar of Ornament [17], whenever any style of ornament commands universal 
admiration, it will always be found to be in concordance with the laws which regulate the distri­bution 
of form in nature. Growth is a particularly good source for continuous patterns that .ll space and that 
can logically transport a design into new regions. In Figure 6, design elements are transported by linear 
trunks and sin­uous meanders. Space is .lled by smaller spiral branches and half­  Figure 6 Growth transporting 
a design [10, plate 77]. spiral leaves. In addition, the non-rigid repetition of forms derived from natural 
growth can be used to breathe life into a design. Another issue of growth as represented in ornament 
is that it tends to be more highly structured, or ordered, in this context. This ordering property can 
be described as intention. Intention can be de.ned as the aesthetic perception of teleological growth 
or placement of form, discernible from multiscale features of a design: its high-level lay­out; its sinuous 
sub-motifs and their serial and hierarchical compo­sitions; and, at the lowest level, the continuous 
change in curvature along a line, a line s modulation in width, and the angles of crossings of lines. 
In other words, intention is not just the process of growth in the absence of external in.uences, but 
rather a way of express­ing growth even under such in.uences. Examples include growth toward pre-placed 
.owers, or the cooperative formation of symmet­ric structures, sometimes even from non-analogous locations 
in an overall branch structure. 2.2.2 Conventionalization While in common usage the term convention 
has a pejorative ring, implying lack of invention, in ornamental design it can have just the opposite 
meaning. Conventionalization in ornament is the de­velopment of abstractions of natural form, a highly 
creative process. When artists develop a conventionalization they perform a sort of inventive pre.ltering 
of phenomenal reality followed by a creative resynthesis of form. The focus is to extract essential features 
of form from the vagaries of environmental in.uence. In Figure 7 we see a side-by-side comparison of 
a study drawn from nature and a conventional representation based on that study. Note how the subtle 
wave of the leaf margins of the poppy get ampli.ed and regularized in its conventionalization. Note also 
how the form of the seed pods has been stylized to .ll space.  3 Approach We will represent a given 
adaptive clip art pattern as a set of ele­ments, which describe the geometric primitives that comprise 
the or­nament, together with a set of growth rules, which describe how the elements are structured in 
relation to one another and to the bound­aries of the panel. The growth rules are invoked by a controlling 
framework to produce the ornamental pattern, customized for any planar region. L-systems would appear 
to be the natural choice for expressing our growth rules, as they have been used to model many plant-like 
struc­tures. In the rest of this section, therefore, we will take a closer look at the use of L-systems 
for ornament and discuss the reasons we ul­timately chose not to use them. We will then discuss the approach 
Figure 7 Natural vs. conventional representation [8]. we took in encoding our adaptive clip art in more 
detail. 3.1 Using L-systems for ornament L-systems were developed by biologists seeking to model the 
de­velopment of plants, and they have been extended by the computer graphics community [27, 28, 33] to 
create realistic plant images and animations. Traditional L-systems do not receive information about 
the environment. More recently, open L-systems have been intro­duced to allow information from the model 
s environment to also affect growth [20, 26]. Open L-systems are therefore a reasonable choice for encoding 
growth rules for ornament. As we discuss be­low, however, the generation of ornament differs from the 
growth of real plants in several signi.cant ways that we felt limited the ap­plicability of open L-systems 
in this context. First, while .oral ornaments may involve leaves, .owers, vines, and so forth, in their 
conventionalization these elements are often con­nected and arranged in ways that no plant would ever 
produce. Bio­logical models are therefore not directly applicable. Indeed, we felt it would be easier, 
in most cases, to model the appearance of an ornament rather than some underlying process to produce 
it. Also, by modeling the appearance of the output directly, we felt we could have tighter control over 
it. Second, the environmental feedback loop for real plant growth is indirect: the environment at a given 
point in space produces chem­ical changes in the plant that act to alter its further growth. Open L-systems 
model this loop by alternating rule application phases with environment query phases productions leave 
symbols in the L-string to indicate where queries should be answered by the environment process. These 
answers can only affect productions in future iterations of the simulation. Thus, a rule for growth that 
incor­porates environment queries must be split into a set of productions. We felt that in our case it 
would be easier to design rules in the form of procedures, which could both query the environment and 
directly act on the results of those queries in placing graphical elements of the ornament. Finally, 
L-systems apply all productions to a string in parallel: each element in the string is simultaneously 
replaced with the result of a rule acting on the element. Rather than trying to de.ne the semantics of 
parallel rule application when each rule is a procedure, we have chosen to apply our rules serially. 
A successful iteration of our sys­tem, then, consists of the selection of a single element, followed 
by the incremental growth of that element according to a certain growth rule associated with it. This 
process also provides an opportunity to integrate some form of global planning into both the selection 
of the element and the rule being applied. 3.2 Adaptive clip art Adaptive clip art consists of two parts: 
elements and growth rules. Elements correspond to the 2D geometric primitives that appear in the ornament 
(e.g., .owers, leaves, and stems); they are the objects upon which the growth rules operate. To provide 
simplicity without sacri.cing the ability to draw detail, each element is de.ned as a col­lection of 
one or more proxies. A proxy is a relatively simple geo­metric shape that represents the element (or 
a part of the element) for the purposes of locating empty spaces and testing for intersections. When 
producing .nal output, a more complicated rendering proce­dure can be invoked. The use of proxies, therefore, 
keeps the details of rendering an element separate from the mechanics of positioning it in the design. 
Our growth rules are speci.ed as procedures. When a rule is invoked on a parent element, the code associated 
with that rule (the rule body) is executed. This code can perform environmental queries and create child 
elements, among other things. A support library is pro­vided for common environmental queries and for 
conveniently ma­nipulating geometrical primitives such as proxy shapes. Finally, our framework for elaborating 
adaptive clip art uses a lim­ited form of planning in selecting the element for growth on each new iteration. 
As described in more detail in the next section, the framework attempts .rst to grow the ornament into 
large open space, then shifts to .lling in corners of the desired region.  4 Implementation The current 
implementation consists of approximately 600 lines of Perl (the preprocessor) and 3,600 lines of C++ 
(the framework). The preprocessor reads a rule .le which encodes an ornamental pattern. The preprocessor 
output is a C++ source .le and a corresponding header .le, which are compiled with the framework code 
to pro­duce an executable. This executable can take a region speci.cation and produce the ornamental 
pattern to .ll that particular region. The output generated is a PostScript .le. A default rendering 
is provided for every element, which simply draws each proxy of the element in outline form. The user 
can attach arbitrary C++ code to each element type within the rule .le to generate custom PostScript 
output if de­sired. Alternatively, the PostScript output can be converted to paths and rendered with 
skeletal strokes [15, 16] to produce a wide variety of effects. We will take a top-down approach to describing 
the implementation in the next three sections, .rst describing the way in which elements and rules are 
selected for growth, then covering the details of how they are speci.ed. 4.1 Rule invocation The main 
job of the framework is to decide which elements to grow with the rules in order to .ll the given space. 
LetR repre­sent the region to be .lled with a pattern. Our heuristic is simple: it .nds the largest circle 
C (modulo some approximation error) that does not intersect the boundary R or any element of the design, 
and tries invoking rules on the elements within a distance 8of that circle. Elements are tried in order 
of their distance from the circle. When a rule succeeds (or when all possibilities are exhausted), the 
iteration ends and a new circle C is selected. To .nd the desired circle, we keep a (relatively) low-resolution 
buffer into which we render the proxies of already placed elements, along with the boundary of the region 
R. We start small test circles at various points within the region and increase the radius of each cir­cle 
until it intersects an element or the boundary. If the in.ation pro­cedure for a given circle is stopped 
because the circle hits a bound­ary, the circle is discarded, since the circle is not adjacent to the 
ex­isting ornament. If in.ation is stopped by hitting an element, the cir­cle is kept. The largest kept 
circle is chosen as C. The center and radius of C are made available within rule bodies so that rules 
may direct their growth based on the circle s location. To determine at which points to center the test 
circles, we perform a medial axis transformation (MAT) [30] using the Manhattan dis­tance metric. A circle 
is centered on each pixel whose transform value is at least as great as those of its neighbors. We use 
these skele­ (a) (b) (c) (d) Figure 8 One iteration of the main loop. (a) Elements already in place 
at the start of the iteration. (b) The render buffer, with points covered by elements and/or the region 
boundary (in red). Eligible empty-region circles are superimposed in yellow, ineligi­ble circles (on 
the exclude list) in green. (c) The selected empty circle C (dashed blue lines) and the nearby elements 
that are candidates for growth (thick purple lines). (d) The ornament after a rule has placed a new leaf. 
ton points as centers of the candidate circles to avoid having to per­form the circle in.ation, which 
is relatively slow, starting at every uncovered point in the region. The MAT is updated incrementally 
after each new element is placed. It is possible that all the rules on all the elements near a given 
circle C may fail to place new elements. In this case, C would continue to be the largest empty circle 
available and would immediately be tried again. To prevent the algorithm from falling into an in.nite 
loop, we keep a list of points called the exclude list. No circle that intersects a point on the exclude 
list can be selected as C. If all the elements near a given circle fail to produce new elements, then 
the center of the circle is placed on the exclude list. A point can be removed from the exclude list 
in one of two ways. Whenever a rule is successful in placing elements for a circle C, all points within 
Eof that circle s center are removed from the list. The idea is that we want to prevent a failed circle 
from being eligible until some change has occurred in its vicinity; then it can be tried again. The other 
way is for a rule body to explicitly clear the list (useful, for instance, if some state change within 
the rule code allows previously unavailable possibilities for placing elements). The overall algorithm 
can be summarized with the following pseu­docode. The FindEmptyCircle procedure locates the largest empty 
circle in the region, subject to the two restrictions above. The effects of one iteration of the main 
loop are illustrated in Figure 8. initialize element tree with seed points render boundary elements into 
buffer compute initial MAT initialize empty exclude list repeat C .FindEmptyCircle() .nd elements within 
8of C try elements in order of distance from C try rules in order speci.ed in rule .le if rule succeeds, 
break if some rule succeeded update element tree render new elements into buffer incrementally update 
MAT remove points on exclude list within of C else add center of empty circle to exclude list 4.2 Elements 
and rules Each design element has a type. The set of available types is declared in the rule .le. Each 
element type is associated with one or more proxies, and zero or more user .elds. Available proxies include 
cir­cles (circle), arcs (arc), cubic B´ezier segments (bezier), line segments (linesegment), etc. Each 
element contains a few stan­dard .elds (such as the number of children the element has), any user .elds 
given in the element declaration, and proxy objects of the types speci.ed in the declaration. The .elds 
of each proxy are de­pendent on its type: a circleproxy, for instance, has centerand radius.elds. Each 
rule .le must declare the element type seed, with a single pointproxy. Seed elements are placed by the 
framework in user­selected locations at the beginning of the run to start the ornament. After the element 
declaration section of the .le is the rule section. Each rule speci.es what element type the rule acts 
on (the parent) and what types of children the rule produces. The set of children cre­ated by the rule 
consists of the static children declared in the rule preamble, plus any dynamic children created within 
the rule. The only difference between static and dynamic children is how they are initialized and how 
they are referenced within the rule body. The body of the rule looks very much like a block of C++ code. 
Any­thing that is legal within a C++ function is legal within a rule body. Additionally, special dollar-sign 
tokens provide convenient access to the .elds of the parent and child elements. The preprocessor trans­lates 
these tokens into C++ expressions referring into the data struc­tures of the elements. Each rule returns 
a .ag to indicate success or failure. On success, the children created by the rule are permanently added 
to the orna­ment and a new iteration begins. On failure, the children elements are discarded, and the 
framework proceeds to try other element/rule combinations as discussed in Section 4.1. Two detailed examples 
of patterns implemented with this system are given in Appendix A.  5Results Our .rst set of results 
shows four different ornamental patterns, each elaborated over two regions. The .rst pattern (Figure 
9) is based on a pattern taken from a Chi­nese vase [18, plate 47]. The pattern has two types of stylized 
.ow­ers laid down in a grid pattern and connected by curving stems. The remaining space is .lled with 
small hook-shaped curves, which themselves are adorned with smaller teardrop shapes. In addition to exhibiting 
constraints to geometric bounds, this example was cho­sen to demonstrate intentional growth: the large 
vine appears to deliver its .owers to prede.ned locations on the grid. The second pattern (Figure 10) 
demonstrates the principle of hierar­chical growth. The pattern starts from the seed points by growing 
the vines. It then adds the red .owers and the yellow and blue shapes, connecting them to the main vine 
structure with shorter subsidiary vines. Next, leaves are added, either attached to a vine or .oating 
on their own, and .nally the small double-quote-shaped structure is used to .ll in small gaps. This ordering 
of rule phases is imposed on the system by adding state preconditions to each rule, so that any rule 
that is invoked when the program is not in the right state automatically fails. The third pattern (Figure 
11) is a somewhat less successful attempt, motivated by a William Morris willow-leaf wallpaper. There 
is only one rule, which grows through the empty circle by adding a curved stem with alternating leaves 
while preventing the leaves from over­lapping too much. This pattern illustrates a shortcoming in our 
ap­proach, which is that it is dif.cult to do signi.cant global planning of a design. In our current 
system, rule invocation is controlled by the empty-space-.nding algorithm, so growth always proceeds 
from the nearest element. For many patterns, it would be better to .ll a given space with growth from 
a more distant element that curves so as to naturally pass through that space. Our willow pattern, while 
cov­ering the region well, is jumbled in comparison to the more elegant original. The fourth pattern 
(Figure 12) uses a motif based on an equal-angle spiral, a shape that can be seen in diverse natural 
forms, from the spiral of a nautilus shell to the curve of a vine tendril [7]. This same pattern is also 
used, with a different rendering style, to generate the border on the .rst page of this paper. Each spiral 
is composed of multiple curved segments, making heavy use of dynamic child cre­ation, since spirals of 
different lengths require different numbers of segments in order to appear smooth. Each spiral curve 
is given an orientation opposite to that of its parent. Note how the pattern gen­erates a rhythmic repeat 
with a period that is related to the changing width of the space; the pattern also simpli.es as it wanders 
into nar­rower spaces. Although the rules that generate this pattern are not explicitly hierarchical, 
the appearance of hierarchical structuring is nonetheless formed by placing new elements in, and scaling 
them to, the largest empty circle adjacent to the growing ornament. The resulting ornament reveals large-scale 
structures placed in relation to the outline of the boundary space, with .ner-scale details placed in 
relation to both the boundary space and the evolving ornament. Figure 13 shows each of these four patterns 
again, elaborated over differently-shaped panels. Figure 17 shows the breadth of rendering possibilities 
provided by the skeletal strokes technique [16]. The same spiral design is ren­dered with four different 
strokes, producing a variety of effects. Al­though the underlying spiral growth motif is more subtly 
felt in the more abstract renderings, its ordering properties structure the distri­bution and scaled 
repetition of design elements, creating an organic feel to the compositions. 6 Conclusion In this paper, 
we have described a mechanism for encapsulating growth principles for ornamental design into adaptive 
clip art pat­terns. Although we have so far implemented only a rudimentary testbed for these ideas, we 
envision, ultimately, a powerful interactive au­thoring system for designing these patterns. The artistic 
tool so derived unlike most previous work in computer-generated artis­tic rendering might be more than 
just a digital form of an existing artistic medium: it could essentially provide a new medium of artis­tic 
expression, one that yields living, dynamic patterns that adapt to their environments. As we gain more 
experience with the novel parameter space of this new medium we hope to encapsulate our knowledge in 
high-level, interactive tools that novices and artists alike will be able to use for creating new instances 
of these patterns. In addition to creating better high-level design tools, there are a huge number of 
other important areas for future research: Ornaments over manifolds. We would like to extend our work 
to creating ornaments over arbitrary manifolds. Such techniques would allow the ornamentation of 3D objects 
(vases, mugs, T-shirts, etc.) without the distortion that results from simply mapping a pla­nar ornament 
onto the surface. Incorporating global planning strategies. Our strategy of growth towards the largest 
empty region is a simple, relatively local one. A more sophisticated approach might be developed to look 
at the de­sign more globally and better incorporate ornamental design princi­ples such as balance and 
symmetry. Putting an artist in the loop. In applications such as web page ornamentation, the adaptive 
clip art must be generated purely auto­matically, on the .y. However, in other applications, such as 
wallpa­per design, there is no reason not to put an artist in front of the com­puter to help guide the 
growth of the pattern and improve its appear­ance artistically, since both the cost of manufacturing 
the resulting artwork and the longevity of the .nished piece are both relatively high. It would be interesting 
to explore semi-automatic algorithmic design processes and user interfaces for use in these situations. 
Acknowledgements We would like to thank Przemyslaw Prusinkiewicz, Ned Greene, and Victor Ostromoukhov 
for many helpful discussions. This work was supported by an NSF Presidential Faculty Fellow award (CCR­9553199), 
an ONR Young Investigator award (N00014-95-1-0728), an NSF Graduate Research Fellowship, and industrial 
gifts from Mi­crosoft and Pixar. References [1] Howard Alexander. The computer/plotter and the 17 ornamental 
design types. In Proceedings of SIGGRAPH 75, pages 160 167. 1975. [2] J. Arvo and D. Kirk. Modeling plants 
with environment-sensitive automata. In Ausgraph 88 proceedings, pages 27 33. 1988. [3] Richard Beach 
and Maureen Stone. Graphical style towards high quality illus­trations. In Proceedings of SIGGRAPH 83, 
pages 127 135. 1983. [4] Albert Fidelis Butsch. Handbook of Renaissance Ornament: 1290 Designs from 
Decorated Books. Dover Publications, Inc., New York, 1969. [5] William Caxton. History of Reynard the 
Foxe. Kelmscott, Hammersmith, Eng­land, 1892. [6] Archibald H. Christie. Traditional Methods of Pattern 
Designing. The Clarendon Press, Oxford, 1929. [7] Theodore A. Cook. The Curves of Life. Constable and 
Company, London, 1914. Figure 9 Red Chinese vase pattern. [8] Walter Crane. Line and Form. G. Bell &#38; 
Sons, London, 1902. [9] Joseph D Addetta. Traditional Japanese Design Motifs. Dover Publications, Inc., 
New York, 1984. [10] Lewis F. Day. Nature in Ornament. B.T. Batsford, London, 1898. [11] Andrew Glassner. 
Frieze groups. IEEE Computer Graphics and Applications, 16(3):78 83, May 1996. [12] E.H. Gombrich. The 
Sense of Order. Phaidon Press Limited, London, 1994. [13] N. Greene. Voxel space automata: Modeling with 
stocastic growth processes in voxel space. In Proceedings of SIGGRAPH 89, pages 175 184. 1989. [14] Branko 
Gr¨unbaum and G. C. Shephard. Tilings and Patterns. W.H. Freeman, New York, 1987. [15] Siu Chi Hsu, I. 
H. H. Lee, and N. E. Wiseman. Skeletal strokes. In Proceedings of UIST 93, pages 197 206. 1993. [16] 
Siu Chi Hsu and Irene H. H. Lee. Drawing and animation using skeletal strokes. In Proceedings of SIGGRAPH 
94, pages 109 118. 1994. [17] Owen Jones. Grammar of Ornament. Day and son, London, 1856. [18] Owen Jones. 
The Grammar of Chinese Ornament. Portland House, New York, 1987. Figure 10 Flowers and leaves pattern. 
[19] Sherman E. Lee. The Genius of Japanese Design. Kodansha International, Tokyo, 1981. [20] Radom´ir 
M.ech and Przemyslaw Prusinkiewicz. Visual models of plants interact­ing with their environment. In Proceedings 
of SIGGRAPH 96, pages 397 410. 1996. [21] Franz S. Meyer. Handbook of Ornament. Dover Publications, New 
York, 1957. [22] William Morris. A Book of verse. Kelmscott, Hammersmith, England, 1870. [23] William 
Morris. Ideal Book: Essays and Lectures on the Arts of the Book.Uni­versity of California Press, Berkeley, 
1982. [24] William Morris and A.J. Wyatt. The Tale of Beowulf. Kelmscott Press, Hammer­smith, England, 
1895. [25] K. Prakash. Paisleys and Other Textile Designs from India. Dover, New York, 1994. [26] Przemyslaw 
Prusinkiewicz, Mark James, and Radom´ir M.ech. Synthetic topiary. In Proceedings of SIGGRAPH 94, pages 
351 358. 1994. [27] Przemyslaw Prusinkiewicz and Aristid Lindenmayer. The Algorithmic Beauty of Plants. 
Springer-Verlag, New York, 1990.  [28] Przemyslaw Prusinkiewicz, Aristid Lindenmayer, and James Hanan. 
Develop­mental models of herbaceous plants for computer imagery purposes. In Proceed­ings of SIGGRAPH 
88, pages 141 150. 1988. [29] Auguste Racinet. Polychromatic Ornament. Firmin Didot freres, .ls &#38; 
cie, Paris, 1873. [30] A. Rosenfeld and A. C. Kak. Digital Picture Processing. Academic Press, New York, 
1976. [31] Henry Shaw. The Encylopedia of Ornament. J. Grant, Edinburgh, 1898. [32] Gift Siromoney and 
Rani Siromoney. Rosenfeld s cycle grammars and kolam. In Graph-Grammars and Their Application to Computer 
Science, pages 564 579. Springer-Verlag, Berlin, 1986. [33] Alvy Ray Smith. Plants, fractals, and formal 
languages. In Proceedings of SIG-GRAPH 84, pages 1 10. 1984. [34] M.P. Verneuil. Floral Patterns. Dover, 
New York, 1981. [35] Otto von Falke. Decorative Silks. W. Helburn, Inc., New York, 1922. [36] James Ward. 
The Principles of Ornament. Scribner, New York, 1896.  A Examples Our .rst example pattern is a simple 
cluster of circular dots (Fig­ure 14). The .rst dot is centered on the seed point, and subsequent dots 
are placed adjacent to the existing ornament. Dots may be at most 3 units in radius. Table 1 explains 
the dollar-sign tokens used within the rules in this appendix. %element seed point %endelement // the 
seed element is required %element dot circle int order; %endelement // number each dot in order of placement 
%source // declare a global variable int dot_count = 0; // to count the dots placed %endsource  %rule 
seed --> dot { // place a maximum-sized dot on the initial seed $0.set( $, 3.0 ); $$0.order = dot_count++; 
 // prevent the seed from being used again $$.sterile = 1; return SUCCESS; } %endrule %rule dot --> 
dot { // ignore tiny empty spaces. if ( $goal.radius < 0.5 ) return FAILURE; // determine the radius 
of the new dot. double r = min( 3.0, $goal.radius ); // place the new dot adjacent to the parent dot. 
$0.set( $.center.offset( $goal.center -$.center, $.radius + r ), r ); $$0.order = dot_count++; return 
SUCCESS; } %endrule The $0.setcall is the critical line. It places the new child dot by taking the center 
of the parent dot ($.center), and offsetting it by the sum of the parent and child radii ($.radius + 
r)in the direction from the center of the parent dot to the center of the goal circle C. Figure 14 shows 
this pattern applied to three different regions. For this example, we have colored the dots, using the 
order .eld, to indicate the order in which they were placed, from oldest (red) to newest (purple). This 
example illustrates how the empty-circle heuristic .rst extends the ornament along the skeleton of the 
region, token meaning $$ the parent element $ proxy 0 of the parent $(k) proxy k of the parent $$j the 
j th declared child element $j proxy 0 of the j th declared child $j(k) proxy k of the j th child $$var 
dynamic child element in var $var proxy 0 of a dynamic child $var(k) proxy k of a dynamic child $goal 
the empty circle C Table 1 Explanation of dollar-sign tokens used in rule bodies. (a) (b) (c) Figure 
14 A simple example applied to three different regions. Parts (a) and (b) were seeded near the lower 
left corner, while part (c) was seeded at the center. then .lls in smaller and smaller regions successively. 
A more complex example involves an arrangement of .owers, leaves, and stems. This rule .le has the following 
declaration sec­tion: (a) (b) (c) (d) (e) (f) (g) Figure 15 Two applications of the second example 
rule. Part (a) shows the parent .ower (bold), and the empty goal circle (dashed). The center of the goal 
circle lies more than 8 units away, so the rule produces (b) a .ower, a stem, and a leaf. When the rule 
is applied in the situation of part (c), where the goal circle is smaller, only the .ower and stem are 
produced (d). Parts (e) (g) show the elements of part (d) in a variety of rendering styles. %element 
seed point %endelement // the required seed element type %element flower circle int color; %endelement 
// flower: a circle proxy // this element type has a user field %element stem linesegment %endelement 
// stem: a line segment proxy %element leaf bezier bezier %endelement // leaf: two Bezier segment proxies 
 Here three element types are declared, in addition to the required seed type: flower, which is proxied 
by a single circle; stem, which is proxied by a line segment; and leaf, proxied by two B´ezier segments. 
The flower element contains an integer user .eld called color. This pattern also has only two rules. 
One rule places a .ower on top of the initial seed point, and so is invoked only once. We will omit the 
code for of this rule. The other rule is more interesting: it places a new .ower connected to an existing 
.ower with a new stem seg­ment, and adds a leaf to the stem only if the stem is long enough. Thus, the 
number of children produced is variable (two or three). The effects of this rule are pictured in Figure 
15. Here is the preamble to the rule, which creates a set of elements whose relationships are depicted 
in Figure 16: %rule flower --> stem flower * x leaf The last line of the preamble tells the preprocessor 
that the variable xwithin the rule body will point to an element of type leaf.This declaration is necessary 
so that when the preprocessor sees a dollar­sign construction involving the variable x, it knows the 
type of x and can insert appropriate typecasts. Here is the remainder of the rule: $$ $$1 $ $1 parent 
 $$x $$0 $x $0 $x(0) $x(1)  { Direction to_goal = $goal.center -$.center; double distance; // determine 
how far away to place the child flower. // on the difference between two points gives // the distance 
between them. distance = ($goal.center -$.center); if ( distance > 10.0 ) distance = 10.0; // place 
the flower centered "distance" units away // in the direction of the goal, with a radius of 3. $1.set( 
$.center.offset( to_goal, distance ), 3.0 ); // if the new flower intersects any // already-placed element, 
cancel this rule. if ( intersection( $$1 ) ) return FAILURE; // the stem extends from the center of 
the parent // flower to the center of the child flower. $0.set( $.center, $1.center ); // if the new 
flower was placed far enough away // add a leaf as well. if ( distance > 8.0 ) { leaf *x = new leaf; 
 // the base of the leaf is the stem midpoint. Point leaf_base = ( $1.center -$.center ) / 2; // place 
the leaf at a right angle to the stem, // and make it 3 units long. Direction leaf_dir = to_goal + M_PI/2; 
Point leaf_tip = leaf_base.offset(leaf_dir, 3.0); // a leaf is proxied by two Bezier segments. // each 
is placed giving position and tangent // direction and magnitude. $x(0).set( leaf_base, leaf_dir+M_PI/4, 
0.6, leaf_tip, leaf_dir, 0.4 ); $x(1).set( leaf_base, leaf_dir-M_PI/4, 0.6, leaf_tip, leaf_dir, 0.4 
); // add the newly created leaf to the // child set of this rule. new_child( $$x ); } // commit the 
set of children to the design. return SUCCESS; } %endrule    
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280949</article_id>
		<sort_key>435</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>43</seq_no>
		<title><![CDATA[Texture mapping for cel animation]]></title>
		<page_from>435</page_from>
		<page_to>446</page_to>
		<doi_number>10.1145/280814.280949</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280949</url>
		<keywords>
			<kw><![CDATA[celanimation]]></kw>
			<kw><![CDATA[metamorphosis]]></kw>
			<kw><![CDATA[morph]]></kw>
			<kw><![CDATA[non-photorealistic rendering]]></kw>
			<kw><![CDATA[silhouette detection]]></kw>
			<kw><![CDATA[texture mapping]]></kw>
			<kw><![CDATA[warp]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P295933</person_id>
				<author_profile_id><![CDATA[81100085540]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wagner]]></first_name>
				<middle_name><![CDATA[Toledo]]></middle_name>
				<last_name><![CDATA[Corr&#234;a]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton Univ., Princeton, NJ]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31081207</person_id>
				<author_profile_id><![CDATA[81332506720]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Jensen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton Univ., Princeton, NJ]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P50488</person_id>
				<author_profile_id><![CDATA[81100556795]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Craig]]></first_name>
				<middle_name><![CDATA[E.]]></middle_name>
				<last_name><![CDATA[Thayer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Feature Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P10635</person_id>
				<author_profile_id><![CDATA[81100576882]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Adam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Finkelstein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton Univ., Princeton, NJ]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kendall E. Atkinson. An Introduction to Numerical Analysis. John Wiley &amp; Sons, New York, 1988.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134003</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Thaddeus Beier and Shawn Neely. Feature-Based Image Metamorphosis. In Edwin E. Catmull, editor, SIGGRAPH 92 Conference Proceedings, Annual Conference Series, pages 35-42. ACM SIGGRAPH, Addison Wesley, July 1992.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>80156</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. Introduction to Algorithms. MIT Press, Cambridge, Mass., 1990.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258896</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Cassidy J. Curtis, Sean E. Anderson, Joshua E. Seims, Kurt W. Fleischer, and David H. Salesin. Computer-Generated Watercolor. In Turner Whitted, editor, SIGGRAPH 97 Conference Proceedings, Annual Conference Series, pages 421-430. ACM SIGGRAPH, Addison Wesley, August 1997.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>151048</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Gerald Farin. Curves and Sulfaces for Computer Aided Geometric Design: a Practical Guide. Academic Press, 1997.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218417</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Jean-Daniel Fekete, l~rick Bizouarn, l~ric Cournarie, Thierry Galas, and Fr6d6ric Taillefer. TicTacToon: A Paperless System for Professional 2-D Animation. In Robert Cook, editor, SIGGRAPH 95 Conference Proceedings, Annual Conference Series, pages 79-90. ACM SIGGRAPH, Addison Wesley, August 1995.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83821</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[James D. Foley, Andries van Dam, Steven K. Feiner, and John F. Hughes. Computer Graphics, Principles and Practice. Addison-Wesley, Reading, Massachusetts, second edition, 1990.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218441</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Michael Gleicher. Image Snapping. In Robert Cook, editor, SIGGRAPH 95 Conference Proceedings, Annual Conference Series, pages 183-190. ACM SIGGRAPH, Addison Wesley, August 1995.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Michael Kass, Andrew Witkin, and Demetri Terzopoulos. Snakes: Active Contour Models. International Journal of Computer Vision, pages 321-331, 1988.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218501</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Seung-Yong Lee, Kyung-Yong Chwa, Sung Yong Shin, and George Wolberg. Image Metamorphosis Using Snakes and Free-Form Deformations. In Robert Cook, editor, SIG- GRAPH 95 Conference Proceedings, Annual Conference Series, pages 439-448. ACM SIGGRAPH, Addison Wesley, August 1995.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218502</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Apostolos Lerios, Chase D. Garfinkle, and Marc Levoy. Feature-Based Volume Metamorphosis. In Robert Cook, editor, SIGGRAPH 95 Conference Proceedings, Annual Conference Series, pages 449-456. ACM SIGGRAPH, Addison Wesley, August 1995.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258893</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Peter Litwinowicz. Processing Images and Video for an Impressionist Effect. In Turner Whitted, editor, SIGGRAPH 97 Conference Proceedings, Annual Conference Series, pages 407-414. ACM SIGGRAPH, Addison Wesley, August 1997.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192270</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Peter Litwinowicz and Lance Williams. Animating Images with Drawings. In Andrew Glassner, editor, SIGGRAPH 94 Conference Proceedings, Annual Conference Series, pages 409-412. ACM SIGGRAPH, Addison Wesley, July 1994.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258894</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Lee Markosian, Michael A. Kowalski, Samuel J. Trychin, Lubomir D. Bourdev, Daniel Goldstein, and John F. Hughes. Real-time Nonphotorealistic Rendering. In Turner Whitted, editor, SIGGRAPH 97 Conference Proceedings, Annual Conference Series, pages 415-420. ACM SIGGRAPH, Addison Wesley, August 1997.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237288</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Barbara J. Meier. Painterly Rendering for Animation. In Holly Rushmeier, editor, SIGGRAPH 96 Conference Proceedings, Annual Conference Series, pages 477-484. ACM SIGGRAPH, Addison Wesley, August 1996.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218442</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Eric N. Mortensen and William A. Barrett. Intelligent Scissors for Image Composition. In Robert Cook, editor, SIGGRAPH 95 Conference Proceedings, Annual Conference Series, pages 191-198. ACM SIGGRAPH, Addison Wesley, August 1995.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801153</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Michael Plass and Maureen Stone. Curve Fitting with Piecewise Parametric Cubics. In Peter Tanner, editor, SIGGRAPH 83 Conference Proceedings, Annual Conference Series, pages 229-239. ACM SIGGRAPH, July 1983.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Barbara Robertson. Disney Lets CAPS out of the Bag. Computer Graphics World, pages 58-64, July 1994.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>63448</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[D. F. Rogers and J. A. Adams. Mathematical Elements for Computer Graphics. McGraw-Hill, New York, second edition, 1990.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Walter Roberts Sabiston. Extracting 3D Motion from Hand- Drawn Animated Figures. M.Sc. Thesis, Massachusetts Institute of Technology, 1991.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97901</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Takafumi Saito and Tokiichiro Takahashi. Comprehensible Rendering of 3-D Shapes. In Forest Baskett, editor, SIG- GRAPH 90 Conference Proceedings, Annual Conference Series, pages 197-206. ACM SIGGRAPH, Addison Wesley, August 1990.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>90941</ref_obj_id>
				<ref_obj_pid>90767</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Philip J. Schneider. An Algorithm for Automatically Fitting Digitized Curves. In Andrew S. Glassner, editor, Graphics Gems, number I, pages 612-626. Academic Press, 1990.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166118</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Thomas W. Sederberg, Peisheng Gao, Guojin Wang, and Hong Mu. 2D Shape Blending: An Intrinsic Solution to the Vertex Path Problem. In James T. Kajiya, editor, SIGGRAPH 93 Conference Proceedings, Annual Conference Series, pages 15-18. ACM SIGGRAPH, Addison Wesley, August 1993.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15903</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Thomas W. Sederberg and Scott R. Parry. Free-Form Deformation of Solid Geometric Models. In David C. Evans and Russell J. Athay, editors, SIGGRAPH 86 Conference Proceedings, Annual Conference Series, pages 151-160. ACM SIGGRAPH, August 1986.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192191</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Michael A. Shantzis. A Model for Efficient and Flexible Image Computing. In Andrew Glassner, editor, SIGGRAPH 94 Conference Proceedings, Annual Conference Series, pages 147-154. ACM SIGGRAPH, Addison Wesley, July 1994.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>2160</ref_obj_id>
				<ref_obj_pid>62</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Robert E. Tarjan and Jan van Leeuwen. Worst-Case Analysis of Set Union Algorithms. Journal of the ACM, 31(2):245- 281, April 1984.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Frank Thomas and Ollie Johnston. Disney Animation: The Illusion of Life. Walt Disney Productions, New York, 1981.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806813</ref_obj_id>
				<ref_obj_pid>800224</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[B.A. Wallace. Merging and Transformation of Raster Images for Cartoon Animation. In Henry Fuchs, editor, SIGGRAPH 81 Conference Proceedings, Annual Conference Series, pages 253-262. ACM SIGGRAPH, August 1981.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192198</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Dan S. Wallach, Sharma Kunapalli, and Michael F. Cohen. Accelerated MPEG Compression of Dynamic Polygonal Scenes. In Andrew Glassner, editor, SIGGRAPH 94 Conference Proceedings, Annual Conference Series, pages 193-197. ACM SIGGRAPH, Addison Wesley, July 1994.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Walt Disney Home Video. Aladdin and the King of Thieves. Distributed by Buena Vista Home Video, Dept. CS, Burbank, CA, 91521. Originally released in 1992 as a motion picture.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>897162</ref_obj_id>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Lance R. Williams. Topological Reconstruction of a Smooth Manifold-Solid from its Occluding Contour. Technical Report 94-04, University of Massachusetts, Amherst, MA, 1994.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192184</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Georges Winkenbach and David H. Salesin. Computer- Generated Pen-and-Ink Illustration. In Andrew Glassner, editor, SIGGRAPH 94 Conference Proceedings, Annual Conference Series, pages 91-100. ACM SIGGRAPH, Addison Wesley, July 1994.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218422</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Andrew Witkin and Zoran Popovi&amp; Motion Warping. In Robert Cook, editor, SIGGRAPH 95 Conference Proceedings, Annual Conference Series, pages 105-108. ACM SIG- GRAPH, Addison Wesley, August 1995.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_obj_id>528718</ref_obj_id>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[George Wolberg. Digital Image Warping. IEEE Computer Society Press, Washington, 1990.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258859</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Daniel N. Wood, Adam Finkelstein, John F. Hughes, Craig E. Thayer, and David H. Salesin. Multiperspective Panoramas for Cel Animation. In Turner Whitted, editor, SIGGRAPH 97 Conference Proceedings, Annual Conference Series, pages 243-250. ACM SIGGRAPH, Addison Wesley, August 1997.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Texture Mapping for Cel Animation Copyright &#38;#169;1998 by the Association for Computing Machinery, 
Inc. Permission to make digital or hard copies of part or all of this work for personal or classroom 
use is granted without fee provided that copies are not made or distributed for profit or commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for components 
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy 
otherwise, to republish, to post on servers or to redistribute to lists, requires specific permission 
and/or a fee. Wagner Toledo Corr ea1 Robert J. Jensen1 Craig E. Thayer2 Adam Finkelstein1 1Princeton 
University 2Walt Disney Feature Animation   (a) Flat colors (b) Complex texture Figure 1: A frame of 
cel animation with the foreground character painted by (a) the conventional method, and (b) our system. 
 Abstract We present a method for applying complex textures to hand-drawn characters in cel animation. 
The method correlates features in a simple, textured, 3-D model with features on a hand-drawn .gure, 
and then distorts the model to conform to the hand-drawn artwork. The process uses two new algorithms: 
a silhouette detection scheme and a depth-preserving warp. The silhouette detection algorithm is simple 
and ef.cient, and it produces continuous, smooth, visible contours on a 3-D model. The warp distorts 
the model in only two dimensions to match the artwork from a given camera perspective, yet preserves 
3-D effects such as self-occlusion and foreshortening. The entire process allows animators to combine 
complex textures with hand-drawn artwork, leveraging the strengths of 3-D computer graphics while retaining 
the expressiveness of traditional hand­drawn cel animation. CR Categories: I.3.3 and I.3.7 [Computer 
Graphics]. Keywords: Cel animation, texture mapping, silhouette detection, warp, metamorphosis, morph, 
non-photorealistic rendering. 1 INTRODUCTION In traditional cel animation, moving characters are illustrated 
with .at, constant colors, whereas background scenery is painted in subtle and exquisite detail (Figure 
1a). This disparity in render­ing quality may be desirable to distinguish the animated characters from 
the background; however, there are many .gures for which complex textures would be advantageous. Unfortunately, 
there are two factors that prohibit animators from painting moving charac­ters with detailed textures. 
First, moving characters are drawn dif­ferently from frame to frame, requiring any complex shading to 
be replicated for every frame, adapting to the movements of the characters an extremely daunting task. 
Second, even if an ani­mator were to re-draw a detailed texture for every frame, temporal inconsistencies 
in the painted texture tend to lead to disturbing arti­facts wherein the texture appears to boil or swim 
on the surface of the animated .gure. This paper presents a method for apply­ing complex textures to 
hand-drawn characters (Figure 1b). Our method requires relatively little effort per frame, and avoids 
boiling or swimming artifacts. In recent years, the graphics community has made great progress in 3-D 
animation, leading up to full-length feature animations created entirely with 3-D computer graphics. 
There are advantages to animating in 3-D rather than 2-D: realism, complex lighting and shading effects, 
ease of camera motion, reuse of .gures from scene to scene, automatic in-betweening, and so forth. Furthermore, 
it is easy to apply complex textures to animated .gures in 3-D. Thus, one might consider using 3-D computer 
graphics to create all animated .gures, or at least the characters that have interesting textures. However, 
it turns out that there are several reasons why hand-drawn 2-D animation will not be replaced with computer­generated 
3-D .gures. For traditional animators, it is much easier to work in 2-D rather than in 3-D. Hand-drawn 
animation enjoys an economy of line [14], where just a few gestures with a pen can suggest life and emotion 
that is dif.cult to achieve by moving 3-D models. Finally, there exists an entire art form (and industry) 
built around hand-drawn animation, whose techniques have been re.ned for more than 80 years [27]. While 
the industry is increasingly using computer-generated elements in animated .lms, the vast majority of 
characters are hand-drawn in 2-D, especially when the .gure should convey a sense of life and emotion. 
In this project, we begin with hand-drawn characters created by a traditional animator. Next, a computer 
graphics animator creates a crude 3-D model that mimics the basic poses and shapes of the hand-drawn 
art, but ignores the subtlety and expressiveness of the character. The 3-D model includes both a texture 
and the approximate camera position shown in the artwork. Our algorithm distorts the model within the 
viewing frustum of the camera, in such a way that the model conforms to the hand-drawn art, and then 
renders the model with its texture. Finally, the rendered model replaces the .at colors that would be 
used in the ink-and-paint stage of traditional cel animation. This process combines the advantages of 
2-D and 3-D animation. The critical aspects of the animation (gestures, emotion, timing, anticipation) 
are created in 2-D with the power and expressiveness of hand-drawn art; on the other hand, effects that 
give shape to the texture occur in 3-D, yielding plausible motion for the texture. The hand-drawn line 
art and the texture are merged for the .nal animation. To implement the process outlined above, this 
paper offers two new algorithms: a silhouette detection scheme and a depth­preserving warp. The silhouette 
detector (which is based on the frame buffer) is ef.cient, simple to implement, and can be used for any 
application where visible silhouette detection is necessary. The warp has two main advantages over previous 
warps that make it appropriate for our problem: it works with curved features, and it provides 3-D effects 
such as wrapping, foreshortening, and self-occlusion. It also has other potential applications: texture 
acquisition, and manipulation of 3-D objects using hand-drawn gestures. (These applications are described 
in Section 9.) The remainder of this paper is organized as follows. Section 2 surveys previous work related 
to this project. Section 3 presents an overview of our process. In Sections 4, 5, and 6, we describe 
in detail how we correlate features on the model with features on the art, how our system warps the model 
to conform to the art, and how to control the warp. In Section 7, we present some resulting animations. 
Section 8 discusses some limitations of our technique, while Section 9 describes several applications 
for this technology other than its use in traditional cel animation. Finally, Section 10 outlines some 
possible areas for future work. 2 RELATED WORK A variety of previous efforts have addressed the use 
of computer graphics for animation, though to our knowledge nobody has suc­cessfully solved the speci.c 
problem described here. Researchers have largely automated the image processing and compositing aspects 
of cel animation [6, 18, 25, 28], wherein the conventional ink-and-paint stage could be replaced with 
the textures resulting from our system. Wood et al. [35] demonstrate the use of 3-D com­puter graphics 
in the design of static background scenery; in con­trast, this paper addresses animated foreground characters. 
Sabis­ton [20] investigates the use of hand-drawn artwork for driving 3-D animation, which, although 
not the main focus of our work, is simi­lar to the application we describe in Section 9.2. Finally, animators 
at Disney actually applied a computer graphics texture to color a hand-drawn magic carpet in the .lm 
Aladdin [30], but their pro­cess involved meticulously rotoscoping a 3-D model to match each frame of 
artwork an arduous task that would have bene.tted from a system such as the one we describe. The heart 
of our method is a new warp. Critical for visual effects such as metamorphosis (or morphing), image warps 
have been extensively studied. Beier and Neely s warp [2] distorts images in 2-D based on features marked 
with line segments; it was the inspiration for the warp that we describe, which works with curved feature 
markers and provides 3-D effects such as occlusion and foreshortening. Litwinowicz and Williams [13] 
describe a warp (based on a thin-plate smoothness functional) that behaves more smoothly than that of 
Beier and Neely in the neighborhood of feature markers; perhaps a hybrid approach could combine these 
smoothness properties into the warp that we describe. Lee et al. [10] have described a user-interface 
based on snakes [9] that is useful for feature speci.cation, as well as a new warp based on free-form 
deformations [24]. Warps have been applied in other domains as well, such as the work of Sederberg et 
al. [23] on 2-D curves, Witkin and Popovi´c[33] on motion curves for 3-D animation, and Lerios et al. 
[11] on volumes. This paper also presents a scheme for silhouette detection based on rendering the 3-D 
model into a frame buffer. In general, sil­houette detection is closely-related to hidden surface removal, 
for which there are a host of methods [7]. Markosian et al. [14] present some improvements and simpli.cations 
of traditional algorithms, and are able to trade off accuracy for speed. Most algorithms deal­ing with 
polyhedral input traverse the mesh tagging edges of the model as silhouettes. In our work, we are only 
interested in visi­ble silhouettes (since they correspond to features that appear in the drawing). Furthermore, 
we want our algorithm to produce smooth, continuous silhouette curves on the actual model. As described 
in Section 4, our method generates this kind of output, and solves the problem of bifurcation along the 
silhouette edge by rendering the 3-D model colored with texture coordinates into a frame buffer. Saito 
and Takahashi [21] employed a similar technique for high­lighting edges in technical illustrations, and 
Wallach et al. [29] used this idea for .nding frame-to-frame coherence in 3-D animations. This project 
shares much in spirit with the recent progress of the computer graphics community toward non-photorealistic 
rendering (NPR), although the actual NPR aspects of our work (the shape of the animated .gure and often 
its texture) are created by an artist. For the researchers who have investigated animation and video 
in simulated media (oil paint for Meier [15] and Litwinowicz [12]; pen and ink for Markosian et al. [14] 
and Winkenbach et al. [32]; and watercolor for Curtis et al. [4]) a challenge has been to maintain temporal 
coherence in the individual strokes of the artwork to avoid boiling. In our project, this challenge is 
circumvented because we use a single texture throughout the animation.  3 THE PROCESS In this section 
we present our system from the user s point of view. The details of the algorithms mentioned here are 
explained in later sections. For each shot in the animation, we follow these steps (Figure 2): (a) A 
person scans in the cleaned-up hand-drawn artwork. (b) A person creates a simple 3-D model that approximates 
roughly the shape of the hand-drawn character. (c) The computer .nds border and silhouette edges in 
the model. (d) A person traces over edges of the line art that correspond to border and silhouette features 
of the 3-D model. (e) The computer warps the 3-D model to match the shape of the line art, and then 
renders the model. (f) The computer composites the rendered model with the hand­drawn line art and background 
scenery.   (a) Hand-drawn art (b) 3-D model (c) Edges in model (d) Edges in art (e) Warped model (f) 
Final frame Figure 2: The process of creating one texture mapped frame. Our method .ts into the existing 
production pipeline for cel ani­mation [6, 18]. Steps (a) and (f) are stages in the current digital production 
process, with the ink-and-paint stage between them. We are offering, as an alternative to the constant 
colors of the ink-and­paint stage, a process that applies complex textures to the drawings. The problem 
of applying textures to hand-drawn artwork poses a challenge: the line art must be interpreted as some 
kind of shape. Given a set of black lines on white paper, the computer must acquire at least a primitive 
model for the 3-D forms conveyed by the art. This information is necessary if we are to provide 3-D effects 
for the texture such as self-occlusion and foreshortening. (See, for example, the difference in occlusion 
between Figures 2b and 2e or the foreshortening shown in Figure 7.) Note that with the constant colors 
of the traditional ink-and-paint stage, these 3-D effects are unnecessary. The viewer does not expect 
the constant orange color of the front of the carpet in Figure 1a to appear to recede as it crosses over 
a silhouette; however the texture of the carpet in Figure 1b must recede. Thus, some form of 3-D information 
must be available to the algorithm. Since interpreting hand-drawn line art as a 3-D .gure is tantamount 
to the computer vision problem (which has not as yet been solved), we resort to human intervention for 
steps (b) and (d) above. These phases of our process can be labor-intensive, and we believe that partial 
automation of these tasks through heuristic methods is a critical area for future work. In step (b) above, 
we create a simple 3-D model that corresponds to the animated .gure. As seen from a set of speci.c camera 
perspectives, the model should have the approximate form of the hand-drawn .gure. By approximate form 
we mean that the model projected into screen space should have a similar set of features in a similar 
arrangement as the features of the line art. For example, the artwork and 3-D model and in Figures 2a 
and 2b both have four border edges and an upper and lower silhouette edge. Note that in this example 
the warp succeeds even though the order of the upper silhouette edge and the backmost edge of the carpet 
is reversed between the hand-drawn artwork and the 3-D model. For the animations shown in this paper, 
our models were represented by tensor product B-spline patches [5]. However, before performing the warp 
described in Section 5, we convert our models to polygon meshes. Thus, the method should be applicable 
to any model that can be converted to polygons, provided that the model has a global parameterization, 
which is generally necessary for texture mapping.  4 SPECIFYING MARKERS In this section we show how 
we specify marker curves curves that identify features on the 3-D model and on the 2-D artwork. We call 
feature curves on the model model markers and feature curves on the drawing drawing markers. These curves 
will be used by the warp described in Section 5 to deform the 3-D model so that it matches the drawing. 
Section 4.1 explains how we automatically .nd model markers by detecting visible border edges and silhouette 
edges on the model. Section 4.2 explains how these edges are converted to form smooth curves on the model. 
Section 4.3 shows how to guarantee that these edges can be safely used later as the input to the warp. 
Section 4.4 shows how to specify the edges on the 2-D drawing that correspond to the edges found on the 
3-D model. 4.1 Silhouette Detection In this section we describe a scheme for .nding visible silhouette 
and border edges in a 3-D model represented by a polygon mesh. These features are likely to correspond 
to features in the hand­drawn line art; such correspondences are the primary input to the warp we describe 
in Section 5. We also allow the user to specify model markers by drawing them directly on the 3-D model, 
but it would be cumbersome to have to specify all model marker curves this way. Thus, we automatically 
construct model markers for all visible border and silhouette edges, and allow the user to pick the useful 
marker curves (often, all of them). To get started, we need to de.ne some terminology, consistent with 
that of Markosian et al. [14]. A border edge is an edge adjacent to just one polygon of the mesh. A silhouette 
edge is an edge shared by a front-facing polygon and a back-facing polygon (relative to the camera). 
Standard silhouette detection algorithms will identify the subset of edges in the mesh that are silhouette 
edges. Treated as a group, these edges tend to form chains. Unfortunately, in regions of the mesh where 
many of the faces are viewed nearly edge-on, the chains of silhouette edges can bifurcate and possibly 
re-merge in a nasty tangle. For our warp, we are interested in .nding a smooth, continuous curve that 
traces the silhouette on the model, rather than identifying the exact, discrete set of silhouette edges 
in the mesh. Furthermore, we are only interested in visible silhouettes because they tend to correspond 
to features that appear in the drawing. Finally, we want to distinguish border edges from other silhouette 
edges. To detect the border and silhouette edges of a 3-D model, we pro­ceed as follows. Using Gouraud 
shading (without lighting effects or antialiasing) we render the 3-D model over a black background as 
a polygon mesh whose vertices are colored (R;G;B)(u;v;ID), where uand vare the parametric coordinates 
of each vertex, and ID identi.es the texture (Figure 3b). Let us call the resulting image the uv-image. 
The method accommodates models with multiple tex­ture maps, but so far in our animations all of our models 
have only used a single texture, whose ID is 1. The ID 0 is reserved for the background. When a pixel 
on the uv-image corre­ sponds to a point on the surface of the model, we say that the pixel is covered 
by the model. Also, a pixel corner is one of the four corners of a pixel, while a pixel boundary is the 
line segment joining two pixel corners shared by two adjacent pix­els. For example, in Figure 4, p1and 
p2 are pixels, c1and c2are pixel corners, Figure 4: A few pixels. and eis a pixel boundary. e c1 p1 
2 p c2 Borders and silhouettes generate color discontinuities in the resulting image (Figure 3c). To 
.nd these discontinuities, we construct a directed graph G(V;E),where Vare the vertices  (a) (b) (c) 
(d) (e) Figure 3: Detecting border and silhouette edges on a 3-D model. (a) Wireframe 3-D model. (b) 
3-D model shaded with vertices colored (u;v;ID). (c) Discontinuities in color. (d) Model marker curves 
in parameter space. (e) Marker curves on model. of the graph, and Eare directed edges in the graph. 
Vconsists of the pixel corners in the uv-image, and Eis constructed by the following classi.cation process: 
CLASSIFY(G) 1 for every boundary between two neighboring pixels 2 p1.pixel closer to the camera 3 p2.pixel 
farther from the camera 4 if p1.color 6¢p2.color 5 e.ADD EDGE(G, p1, p2) 6 if p1is an extremum 7 e.type 
.corresponding kind of border edge 8 else 9 e.type .silhouette edge In steps 2 and 3, we determine which 
of the two pixels of a boundary is closer to the camera, using one of three methods. First, if exactly 
one of the pixels is covered by the model, then it is the closer pixel (because the other pixel corresponds 
to background). Second, we can read from the depth buffer the z-values of the two pixels and compare 
them. Third, if the depth value is unavailable (as on some machines for which z-buffering is implemented 
in hardware) then we can read the uand vparameters of the part of the model that covered those pixels, 
evaluate the model at those parametric locations, and compare their distances from the camera. In step 
5, we add to Ga directed edge ebetween the two corners shared by p1and p2in such a way that p1is on the 
left of e,and p2 is on the right. If the parametric space is periodic then the border edges should be 
ignored. For example, in Figure 3, the carpet has two silhouette edges (upper and lower) and four border 
edges (at u0, u1, v0,and v1); the ball only has silhouette edges because the interior line in Figure 
3c is ignored. In the actual implementation, we generate for each edge a con.dence value for each of 
the possible kinds of edge. If, for example, the uparameter at p1was found to be very near 0, and uat 
p2was large, then we could say with high con.dence that the edge between these pixels represents a border 
at u0. Finally, we choose the highest con.dence value to represent the type of the edge. After the classi.cation 
process is .nished, .nding the edges on the model is equivalent to .nding the connected components of 
G, which can be done ef.ciently using depth-.rst search [3, 26]. We traverse the graph, .nding paths 
of edges that have the same edge type and the same color (within a tolerance). The running time of depth-.rst 
search is 8(jVj+jEj)[3]. In our case, both jVjand jEj are linear in the number of pixels. Furthermore, 
the classi.cation process is linear in the number of pixels because it makes a single pass over the entire 
image, adding a number of edges to the graph that is bounded by a constant multiple of the number of 
pixels. Thus, the silhouette detection scheme described here is linear in the number of pixels in the 
image. The image resolution dictates the accuracy of the resulting silhouette curves. We have found that 
a resolution of 512x512is suf.cient for the simple models we have used in our animations. Having found 
the connected components of G,we have a set of lists of pixel boundaries that correspond to border edges 
and silhouette edges on the model. In the next section we will describe how to .t smooth curves to this 
data. These curves are the model marker curves (Figure 3e) used by the warp in Section 5. The bene.ts 
of this silhouette detector are that it is simple to implement, it leverages existing support for hidden 
surface removal and texture mapping, it works for any object with a well-de.ned parameter space, and 
produces smooth, visible silhouette curves.  4.2 Curve Fitting To represent each marker curve, we use 
a chord-length parameter­ized endpoint-interpolating uniform cubic B-spline [5, 19]. These curves have 
several desirable properties: they are smooth; they can be linked head to tail (without a break between 
them); the rate of change of the curve with respect to the parameter is uniform; and they are well understood. 
We obtain each curve by .tting it to a set of data. These data are either chains of pixels (as in the 
previous section) or the result of user input (as in Section 4.4). To .t the curves, we typically have 
many more data points than degrees of freedom: a chain of hundreds of pixels generated by the silhouette 
detector (or generated by tracing on the drawing) may be smoothly approximated by a spline with only 
a few control points. To calculate the control points, we solve an overdetermined linear system using 
least squares data .tting techniques that minimize the root-mean-square error between the data and the 
resulting curve [1, 17, 19, 22]. Our .tting procedure attempts to use as few control points as possible, 
given a maximum error threshold. 4.3 Ghostbusting The marker curves created in this section drive the 
warp described in Section 5. Beier and Neely [2] observe that image warps tend to fail if feature lines 
cross, producing what they call ghosts. Near the intersection between two feature lines, both features 
exert a strong in.uence on the warp. If crossed feature lines do not agree about the warp, then there 
tend to be sharp discontinuities in the resulting warp leading to unpleasant artifacts. Thus, conventional 
wisdom regarding image warps warns: do not cross the streams. For the warp of Section 5, there dangerousare 
some con.gurations of crossed feature markers that are dangerous and others that are benign. Our benign 
warp distorts a 3-D model, so feature markers that cross on the model lead Figure 5: Crossed streams. 
to sharp discontinuities in the warp. On the other hand, feature markers that cross in image space (but 
are distant on the model) do not cause any ill artifacts. An exam­ple of each kind of crossing is shown 
in Figure 5. Our application automatically detects dangerous crossings, and splits model marker curves 
where they cross. Since the silhouette detector described in this section builds feature markers based 
on .nding discontinuities in parameter space, it is easy to use this information to distinguish dangerous 
crossings from benign crossings. At dangerous crossings we split marker curves so that they meet only 
at their endpoints. For example, after splitting, three curves in Figure 5 meet at the point labeled 
dangerous. Furthermore, our application trims the fea­ture curves a small distance away from the crossing 
point, so the resulting curves do not touch. The splitting procedure ensures that the resulting marker 
curves agree about the warp at their intersec­tion. The trimming step, though not strictly necessary, 
causes the warp to behave even more smoothly in the neighborhood. 4.4 Specifying Markers on the Drawing 
We have described the creation of model marker curves based on features in the 3-D model. Next the user 
picks model marker curves, and speci.es corresponding drawing marker curves. To specify these curves, 
the user traces over features in the hand-drawn art a time-consuming and tedious task. To reduce user-intervention, 
we use contour tracing techniques similar to those presented by Gleicher [8] and Mortensen and Barrett 
[16] in which the cursor automatically snaps onto nearby artwork. For each drawing marker, tracing results 
in a list of pixels that we subsequently approximate with a smooth curve, as described in Section 4.2. 
We now have a collection of model and drawing markers. These curves identify features on the model that 
correspond to features on the hand-drawn artwork. In the following section, we describe a warp that uses 
this information to guide the distortion of the 3-D model so that it matches the hand-drawn artwork. 
  5 THE WARP This section describes our method for warping a given 3-D model to match hand-drawn line 
art. The inputs to the warp are the model, camera parameters, and a set of pairs of (model and drawing) 
marker curves. The warp is applied to points on the model after they have been transformed into the screen 
space for the given camera but before the model has been rendered. The warp, which maps a point in screen 
space to a different point in screen space, has the following important properties: Each warped model 
marker lies on its associated drawing marker as seen from the camera.  The surface warped between two 
markers varies smoothly, avoiding puckering or buckling.  The warp preserves approximate area in image 
space, so that foreshortening effects are still apparent.  The warp leaves depth information (relative 
to the camera) unchanged.  5.1 The Warp for One Pair of Markers To begin, let us assume we only have 
a single pair of marker curves in screen space: a model marker curve M(t)(generated by the silhouette 
detector in Section 4.1) and a drawing marker curve D(t) (traced on the drawing by the user as described 
in Section 4.4). In the next section we will describe how this works with multiple pairs of marker curves. 
For a given point pin the screen-space projection of the model, we want to .nd the corresponding point 
qon the drawing. For a particular value of t, we de.ne two coordinate systems: one on the model marker 
curve and one on the drawing marker curve (Figure 6). The model coordinate system has origin at M(t),and 
abscissa direction x^m(t)given by the tangent of Mat t. Likewise, the drawing coordinate system has origin 
at D(t), and abscissa direction x^d(t)given by the tangent of Dat t. For now, the ordinate directions 
y^m(t)and y^d(t)are oriented to be perpendicular to the respective abscissa. However, in Section 6.4 
we will modify the ordinate direction to get a smoother warp. We .nd the xand ycoordinates of pin relation 
to the model coordinate system in the usual way: x(t)(p,M(t)).x^m(t) y(t)(p,M(t)).y^m(t) Next we de.ne 
a tentative drawing point q(t)corresponding to p(t): q(t)D(t)+x(t)x^d(t)+y(t)y^d(t) (1) This is the location 
to which we warp p(t), taking into account only the coordinate systems at parameter t. Ofcourse, wehavea 
continuum of coordinate systems for all parameters t, and in general they do not agree about the location 
of q(t). Thus, we take qto be a weighted average of q(t)for all t, using a weighting function c(t): Z 
1 c(t)q(t)dt q 0 Z1 (2) c(t)dt 0 We want the contribution c(t)to fall off with the growth of the distance 
d(t)between the 3-D points that project to pand M(t). Intuitively, we want nearby portions of the marker 
curves to have more in.uence than regions of the markers that are far away. We compute d(t)in one of 
two ways. We either approximate the  x(t) x(t) p y m(t) y(t) M(t) x m(t) (a) Model marker curve Figure 
6: Coordinate systems for the warp.  distance along the surface of the model, or we compute world-space 
distance on an undeformed reference mesh. This is an important difference between our warp and traditional 
warps: traditional warps would use the 2-D distance between the origins of the coordinate systems. Although 
our warp happens in 2-D space, it takes into account 3-D information. Thus, the marker curves at point 
Ain Figure 6a have little in.uence on the warp of point B, even though they are very near each other 
in the image plane. Thus, we choose the contribution to be: 1 c(t) (3) E+d(t)f where Eis a small constant 
to avoid singularities when the distance is very near zero, and fis a constant that controls how fast 
the contribution falls off with distance. We discuss the nature of these parameters in greater detail 
in Section 6.1. 5.2 The Warp for Multiple Pairs of Markers The warp in equation (2) only considers a 
single pair of marker curves. Here we generalize the warp to handle multiple pairs of curves. We assign 
a user-speci.ed weight wito each pair iof curves, i1;2;:::;m, balancing the relative contribution of 
each pair to the overall warp. Finally, we compute the drawing point qas a weighted average using qi(t)and 
ci(t)from equations (1) and (3), for each marker pair i: Z 1 ) m X wici(t)qi(t)dt 0 .1( q i)(4) m XZ1 
wici(t)dt 0 i .1( The main features of our warp are: it uses curved features; it is visually smooth; 
it is scale, translation, and screen-space rotation independent; and, most importantly, it maps model 
markers to drawing markers while preserving 3-D effects. 5.3 Computing the Warp In practice, we evaluate 
the integrals in equation (4) numerically, by uniformly sampling the marker curves. This can be computed 
quickly for all vertices in the model by caching the coordinate systems at sample locations. Our warp 
uses forward mapping [34]: for each point in the source texture, it .nds where the point is mapped in 
the destination. The motivation in this case is that we get self-occlusion and hidden­surface removal 
for free using normal z-buffered rendering. Also, this permits us to compute the warp at low resolution 
for the interactive parts of the process, and later perform the .nal rendering at high-resolution. Litwinowicz 
and Williams [13] also use forward mapping, whereas Beier and Neely [2] use inverse mapping:for each 
point in the destination, they .nd the location at which to sample the input image. We sample the map 
by calculating the warp only at vertices in the 3-D model; thus, we can render the warped model using 
conventional texture mapping simply by modifying the coordinates of vertices in the mesh and then rendering 
the usual way.  6 CONTROLLING THE WARP Many factors in.uence the behavior of the warp described in 
Section 5. In this section we describe some of these in.uences in better detail. 6.1 Contribution Parameters 
The factors Eand fof equation (3) can be modi.ed to achieve different effects. By varying E, we can get 
different levels of smoothness and precision. If Eis large, the warp is smoother. If Eis small, the warp 
is more precise. By varying f, we control how fast the contribution falls off with distance. If fis large, 
distant points will have almost no in.uence. (The factors Eand fare similar to the factors aand bof the 
warp of Beier and Neely [2].) The .gures in this paper use E10,4and fbetween 2 and 3. 6.2 Modeling and 
Viewing Parameters Our warp is based on the projection of the 3-D model into screen space. By varying 
the modeling and viewing parameters we produce different projections of the model, and obtain different 
results. Figure 7 shows the results of the warp for two different camera views. In both cases, the hand-drawn 
art and the 3-D model are the same. Notice that without the texture the drawing would be ambiguous. We 
do not know whether the drawing is smaller at the top or recedes into the distance. The foreshortening 
effect helps resolve this ambiguity. Thus, use of a texture in a .gure may provide the viewer with information 
about spatial arrangement that is not available in conventional .at-colored .gures. Since it only takes 
a few seconds to rotate the model and re-render the frame, it is easy to interactively switch between 
the two views like those of Figure 7 to choose one that has the desired effect. 6.3 Extra Markers So 
far we have only mentioned model marker curves detected automatically. In the actual implementation, 
the user can specify extra markers on the model and the corresponding ones on the drawing, to match features 
that were not detected automatically. For example, Figure 8 shows how extra markers can be used to tell 
the carpet how to fold over a set of stairs. Also, one could add marker points control points that add 
to the formula like marker curves, but have a single coordinate system that is embedded in the (a) Upright 
model warped to the art (b) Tilted model warped to the art Figure 7: In.uence of modeling and viewing 
parameters.  (a) Without extra markers (b) With extra markers Figure 8: Effect of de.ning extra markers 
on the warp. plane although we have not yet found it necessary to use marker points for any of our animations. 
 6.4 Ordinate Direction Adjustment In Section 5 we described a warp that uses coordinate systems whose 
ordinate and abscissa directions are perpendicular to each other. This approach leads to puckering when 
there is a great disparity between the orientation of nearby drawing coordinate systems (Figure 9a). 
Instead, we adjust the ordinate direction of the drawing coordinate systems so that they conform better 
to the actual artwork. The algorithm produces drawing coordinate systems whose axes are linearly independent, 
but not necessarily orthogonal. The result is a smoother and more intuitive warp (Figure 9b). To understand 
how our method works, let us suppose we have a drawing coordinate system with axes (x^d;y^d), and a correspond­ing 
model coordinate system with axes (x^m;y^m)as shown in Figures 10a and 10b. Also, let us assume that 
we have a single drawing marker curve Dand a corresponding model marker curve M.For a given value of 
t, we want to .nd the ordinate direction y^d0taking into account the drawing coordinate system with origin 
at D(t)and axes (x^d(t);y^d(t)), as well as the corresponding model coordinate system with origin at 
M(t)and axes (x^m(t);y^m(t)). We .nd the rotation that maps y^m(t)to y^m, and apply this same rotation 
to y^d(t)to obtain y^d0(t), which is where the coordinate sys­tem at D(t) thinks that y^dshould be. Using 
an approach similar to the one we used for the warp, we .nd y^d0as a weighted combina­tion of y^d0(t), 
using a weighting function c 0(t): Z 1 c 0(t)^y 0 d(t)dt ^y 0 d 0 Z 1 (5) c 0(t)dt 0 We want the contribution 
c 0(t)to have two properties. First, it should fall off with the distance d0(t)between the model coordinate 
systems, computed as in Section 5. Second, it should be larger when the corresponding coordinate systems 
in parameter space (Figure 10c) are perpendicular to each other. Intuitively, the bottom edge of the 
carpet in Figure 9 should have a strong in.uence over the ordinate direction along the right edge of 
the carpet, because they are perpendicular in parameter space. This leads to isoparameter lines in the 
drawing space that follow the shape of the lower boundary curve. The contribution is then: 01,y^p.y^p(t) 
c(t)E0+d0(t)f0 where y^pand y^p(t)are the vectors in parameter space that corre­spond to y^mand y^m(t), 
respectively. The parameters E0and f0,and the distance function d0(t)have the same roles as in equation 
(3). As with the warp in Section 5, we compute the .nal ordinate di­rection by promoting equation (5) 
to include all marker curves, but here we do not use weights wi. When all coordinate systems are parallel 
to each other in parameter space, the result of this algo­rithm is unde.ned. In this special case, we 
simply use the original orthogonal y^d.  (a) Without adjustment (a) Drawing (b) Model (b) With adjustment 
(c) Parameter (d) Adjusted Figure 9: Effect of ordinate direction adjustment on the warp. Figure 10: 
Coordinate systems for ordinate direction adjustment.  (a) Without reparametrization (b) With reparametrization 
Figure 12: Effect of reparameterizing the texture.  6.5 Reparameterizing the Texture As we mentioned 
in Section 4, the marker curves are approximately chord length parameterized. This leads to an interpretation 
of which part of the overall texture should be visible that is often, but not always, correct. We provide 
a mechanism for pulling occluded texture back over a silhouette horizon, or pushing visible texture over 
the horizon so it becomes occluded. For example, for the carpet in Figure 12, the lower silhouette curve 
corresponds to two different regions in parameter space. We simply move the texture on the underformed 
model, essentially reparameterizing the texture on the surface. We can perform such reparameterization 
easily by warping the texture in parameter space prior to rendering, guided by feature curves such as 
the borders of the texture and the desired silhouette positions. However, we have not yet found it necessary 
to use this feature for any of our animations.  7 RESULTS In this section we describe several animations 
we created with our application. Figure 11 shows an animation of a ball bouncing along a desert highway. 
For each row, the upper left picture shows the hand-drawn artwork, the lower left picture shows the 3-D 
model with the texture applied to it, and the right picture shows the .nal frame from the movie: hand-drawn 
art composited with the warped model and the background. In these frames the ball exhibits squash and 
stretch , a fundamental principle of traditional cel animation [27]. The 3-D model for this case is a 
simple sphere with a hand-painted beach­ball texture applied to it. The sphere rotates with respect to 
the camera, so that it appears to be rolling. To guide the warp for this simple model, we used exactly 
one pair of marker curves per frame. In Figure 14 we show the line art, model, and .nal frames for an 
animated carpet on a staircase. In these frames, the carpet stands up to look around. For this animation 
we used about 5 or 6 marker curves per frame. Section 6.3 describes a method for specifying extra marker 
curves that would cause the carpet to follow the contours of the steps more closely, but we did not use 
that feature in this animation. The frames demonstrate 3-D effects such as self­occlusion and foreshortening 
as well as temporal coherence in a complex texture that would be very dif.cult to produce with traditional 
cel animation. Also note that even though the character is computer-rendered using a highly-symmetric, 
repeating texture, it blends aesthetically with the background art due to the hand­drawn shape. Finally, 
Figure 15 shows a hand-drawn animation of a .sh. The 3-D model of the .sh contains only the body of the 
.sh (with a simple checkerboard texture). Not shown are the three .ns of the .sh (which are planar quadrilaterals) 
and the two eyes of the .sh (which are simple spheres). These were modeled, rendered, and composited 
as separate layers in the style of traditional, multi­layer animation [27] although they all share the 
same hand-drawn art. The model is rigid throughout the animation, only rotating back and forth with respect 
to the camera. Nonetheless, in the .nal frames of the animation the .sh appears to bend and .ex as it 
swims. The .ns and tail are semi-transparent (using a hand-painted matte embedded in the texture of the 
model) and thus it is possible to see the reeds of the background through transparent parts of the .sh. 
To create the hand-drawn artwork for the .sh, it took several hours to draw 23 frames. Building the 3-D 
model was easier (approximately an hour) because the motion is simple rotation. The computer found the 
model markers in just a few seconds per frame. Drawing markers were speci.ed by hand, requiring about 
two minutes per frame. Finally, rendering and compositing required tens of seconds per frame. For the 
ball and carpet animation (which have simpler art and simpler models) these steps required less time. 
 8 LIMITATIONS There are several classes of line art for which our process does not work well. First, 
cel animation has developed a vocabulary for conveying texture by modifying the edges of characters. 
For example, tufts of fur on a character may be suggested or by drawing a few sharp wiggles rather than 
a smooth edge (Figure 13a). A character illustrated with both hinted texture (in its line art) and the 
kind of textured-.ll described in this paper would probably suffer visually from this mixed metaphor; 
moreover, the texture near the sharp wiggles would be likely to stretch and pucker unpleasantly in order 
to conform to the wiggles of the line art. Our process also does not work well with .gures for which 
it is dif.cult to generate the 3-D model most notably, clothing. Cloth is typically drawn showing creases 
and folds, which would be dif.cult to replicate well in a 3-D model (Figure 13b). Other drawings use 
a few lines to suggest greater geometric complexity. The interior folds in the knotted carpet would have 
to be modeled explicitly if we expect the texture to look right. Figure 13c shows an example of how some 
drawings do not correspond to any reasonable 3-D geometry. Here the nose is drawn in pro.le, the eyebrows 
.oat off the surface, and the hair is a crude representation. There are other limitations inherent in 
using quadrilateral patches to represent complex shapes. The body, legs, and tail of the cat Figure 13d 
could not reasonably be represented with a single patch.   To solve these problems we either need to 
use multiple patches and solve continuity issues, or switch to a more general surface representation. 
Subdivision surfaces could be used in the warp if we devised a method of representing surface curves 
and some concept of orthogonality in parameter space.  9 OTHER APPLICATIONS In this section we brie.y 
talk about some other applications that might bene.t from this technology. 9.1 Shading Effects Once we 
have this correspondence between a 3-D model and the artwork, it is easy to incorporate many traditional 
computer graph­ics effects such as highlights, shadows, transparency, environment mapping, and so forth. 
Since this draws us further away from the look and feel of traditional animation, we have not investigated 
these effects in our work (except for the use of transparency in the .ns of the .sh in Figure 15). 9.2 
3-D Shape Control for Animation While it is not the focus of this work, we are currently developing a 
variation of this method as a new form of control in 3-D animation. It would .t into the 3-D animation 
pipeline just before rendering. An animator could add detail or deformations to the 3-D geometry by drawing 
on the image plane (Figure 16). This is better than distorting the .nal image because it affects the 
actual geometry, correctly modifying occlusion and shading. Note that after the warp, the model should 
only be viewed from the original camera position, as the .gure may appear distorted when viewed from 
other directions. (a) Original 3-D model (b) Hand-drawn art (c) Warped 3-D model (d) Perplexed face 
(e) Goofy face Figure 16: Controlling a 3-D model by drawing. (a) Texture photograph (b) 3-D model (c) 
Acquired texture Figure 17: Texture acquisition. 9.3 Texture Acquisition Sometimes we .nd pictures of 
objects with textures that we would like to apply to other objects. Often these textures are not available 
in a perfect orthogonal view as it is necessary for texture mapping. Our technique can be used in reverse 
to acquire the texture. For example, shown in Figure 17 is a photograph of a carpet on the .oor. Since 
the camera was not directly over the carpet, it appears in perspective; furthermore, since the carpet 
is not rigid the edges are not completely straight. Thus, the image of the carpet is not the kind of 
rectangle one would like to use for texture mapping. We build a 3-D model of the carpet (a rectangle), 
position and orient the model in space so its projection on screen space is similar to the picture, associate 
markers on the picture and on the model, and apply the inverse of our warp to extract the texture form 
the picture and apply it to the model. Of course, if parts of the .gure were occluded in the original 
photograph this could lead to holes in the .nal texture map.  10 FUTURE WORK This project suggests 
a number of areas for future work, several of which are described below. Computer Vision. We would like 
to reduce the amount of effort required to construct and position the 3-D model. One strategy is to investigate 
the applicability of computer vision algorithms to recon­struct the 3-D geometry from the 2-D drawings. 
Perhaps the ani­mator could draw hints in the artwork using Williams s scheme [31] for conveying depth 
information through line-drawings. Computer vision techniques would also be useful for discerning camera 
posi­tion, inferring model deformations, and applying kinematics con­straints. The computer could orient 
and deform the 3-D model based on the 2-D drawings. Finally, perhaps the computer could also guess the 
correspondence between a curve in the drawing and a curve in the 3-D model using simple heuristics based 
on location, orientation and shape. (a) Hand-drawn art (b) 3-D model (c) Resulting image Figure 18: 
Applying texture to hand-drawn cloth. Frame-to-Frame Coherence. In an animation sequence, two consecutive 
frames are likely to be similar. We would like to min­imize user intervention by exploiting frame-to-frame 
coherence, reusing information such as association between drawing and ap­proximating 3-D model, detection 
and association of feature curves, and model and camera adjustment. Cloth. As mentioned in Section 8 
there are some kinds of .gures for which our process does not yet work. Perhaps the most challenging 
(and probably the most rewarding) class of .gures would be those with complex surface textures such as 
cloth and hair. One of the dif.culties with cloth is understanding and how it folds, based on the line 
art. Given the right set of marker curves, our warp can produce the right kind of behavior. For example, 
in Figure 18 we show how adding extra marker curves by hand (shown as dashed blue) can disambiguate the 
line art. Acknowledgements We thank Kiran Joshi for posing this problem, as well as Brent Burley, Aliza 
Corson, Mauro Maressa and Paul Yanover at Disney for their help and advice on this project. We also thank 
Cassidy Curtis for his guidance, and Tim Milliron for the 3-D examples. Finally, we are grateful to Ronen 
Barzel, John Hughes, Dan Wallach, and the anonymous reviewers for many improvements to the paper. This 
work was supported in part by CNPq (Conselho Nacional de Desenvolvimento Cient´i.co e Tecnol´ogico), 
Brazil. References [1] Kendall E. Atkinson. An Introduction to Numerical Analysis. John Wiley &#38; 
Sons, New York, 1988. [2] Thaddeus Beier and Shawn Neely. Feature-Based Image Metamorphosis. In Edwin 
E. Catmull, editor, SIGGRAPH 92 Conference Proceedings, Annual Conference Series, pages 35 42. ACM SIGGRAPH, 
Addison Wesley, July 1992. [3] Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. Introduction 
to Algorithms. MIT Press, Cambridge, Mass., 1990. [4] Cassidy J. Curtis, Sean E. Anderson, Joshua E. 
Seims, Kurt W. Fleischer, and David H. Salesin. Computer-Gener­ated Watercolor. In Turner Whitted, editor, 
SIGGRAPH 97 Conference Proceedings, Annual Conference Series, pages 421 430. ACM SIGGRAPH, Addison Wesley, 
August 1997. [5] Gerald Farin. Curves and Surfaces for Computer Aided Geometric Design: a Practical Guide. 
Academic Press, 1997. ´´Galas, and Fr´ed´eric Taillefer. TicTacToon: A Paperless Sys­tem for Professional 
2-D Animation. In Robert Cook, edi­tor, SIGGRAPH 95 Conference Proceedings, Annual Confer­ence Series, 
pages 79 90. ACM SIGGRAPH, Addison Wes­ley, August 1995. [6] Jean-Daniel Fekete, Erick Bizouarn, Eric 
Cournarie, Thierry [7] James D. Foley, Andries van Dam, Steven K. Feiner, and John F. Hughes. Computer 
Graphics, Principles and Practice. Addison-Wesley, Reading, Massachusetts, second edition, 1990. [8] 
Michael Gleicher. Image Snapping. In Robert Cook, editor, SIGGRAPH 95 Conference Proceedings, Annual 
Conference Series, pages 183 190. ACM SIGGRAPH, Addison Wesley, August 1995. [9] Michael Kass, Andrew 
Witkin, and Demetri Terzopoulos. Snakes: Active Contour Models. International Journal of Computer Vision, 
pages 321 331, 1988. [10] Seung-Yong Lee, Kyung-Yong Chwa, Sung Yong Shin, and George Wolberg. Image 
Metamorphosis Using Snakes and Free-Form Deformations. In Robert Cook, editor, SIG-GRAPH 95 Conference 
Proceedings, Annual Conference Se­ries, pages 439 448. ACM SIGGRAPH, Addison Wesley, August 1995. [11] 
Apostolos Lerios, Chase D. Gar.nkle, and Marc Levoy. Feature-Based Volume Metamorphosis. In Robert Cook, 
ed­itor, SIGGRAPH 95 Conference Proceedings, Annual Con­ference Series, pages 449 456. ACM SIGGRAPH, 
Addison Wesley, August 1995. [12] Peter Litwinowicz. Processing Images and Video for an Impressionist 
Effect. In Turner Whitted, editor, SIGGRAPH 97 Conference Proceedings, Annual Conference Series, pages 
407 414. ACM SIGGRAPH, Addison Wesley, August 1997. [13] Peter Litwinowicz and Lance Williams. Animating 
Images with Drawings. In Andrew Glassner, editor, SIGGRAPH 94 Conference Proceedings, Annual Conference 
Series, pages 409 412. ACM SIGGRAPH, Addison Wesley, July 1994. [14] Lee Markosian, Michael A. Kowalski, 
Samuel J. Trychin, Lubomir D. Bourdev, Daniel Goldstein, and John F. Hughes. Real-time Nonphotorealistic 
Rendering. In Turner Whitted, editor, SIGGRAPH 97 Conference Proceedings, Annual Con­ference Series, 
pages 415 420. ACM SIGGRAPH, Addison Wesley, August 1997. [15] Barbara J. Meier. Painterly Rendering 
for Animation. In Holly Rushmeier, editor, SIGGRAPH 96 Conference Pro­ceedings, Annual Conference Series, 
pages 477 484. ACM SIGGRAPH, Addison Wesley, August 1996. [16] Eric N. Mortensen and William A. Barrett. 
Intelligent Scissors for Image Composition. In Robert Cook, editor, SIGGRAPH 95 Conference Proceedings, 
Annual Conference Series, pages 191 198. ACM SIGGRAPH, Addison Wesley, August 1995. [17] Michael Plass 
and Maureen Stone. Curve Fitting with Piece­wise Parametric Cubics. In Peter Tanner, editor, SIGGRAPH 
83 Conference Proceedings, Annual Conference Series, pages 229 239. ACM SIGGRAPH, July 1983. [18] Barbara 
Robertson. Disney Lets CAPS out of the Bag. Computer Graphics World, pages 58 64, July 1994. [19] D. 
F. Rogers and J. A. Adams. Mathematical Elements for Computer Graphics. McGraw-Hill, New York, second 
edition, 1990. [20] Walter Roberts Sabiston. Extracting 3D Motion from Hand-Drawn Animated Figures. M.Sc. 
Thesis, Massachusetts Insti­tute of Technology, 1991. [21] Takafumi Saito and Tokiichiro Takahashi. Comprehensible 
Rendering of 3-D Shapes. In Forest Baskett, editor, SIG-GRAPH 90 Conference Proceedings, Annual Conference 
Se­ries, pages 197 206. ACM SIGGRAPH, Addison Wesley, August 1990. [22] Philip J. Schneider. An Algorithm 
for Automatically Fitting Digitized Curves. In Andrew S. Glassner, editor, Graphics Gems, number I, pages 
612 626. Academic Press, 1990. [23] Thomas W. Sederberg, Peisheng Gao, Guojin Wang, and Hong Mu. 2D Shape 
Blending: An Intrinsic Solution to the Vertex Path Problem. In James T. Kajiya, editor, SIGGRAPH 93 Conference 
Proceedings, Annual Conference Series, pages 15 18. ACM SIGGRAPH, Addison Wesley, August 1993. [24] Thomas 
W. Sederberg and Scott R. Parry. Free-Form Defor­mation of Solid Geometric Models. In David C. Evans 
and Russell J. Athay, editors, SIGGRAPH 86 Conference Pro­ceedings, Annual Conference Series, pages 151 
160. ACM SIGGRAPH, August 1986. [25] Michael A. Shantzis. A Model for Ef.cient and Flexible Image Computing. 
In Andrew Glassner, editor, SIGGRAPH 94 Conference Proceedings, Annual Conference Series, pages 147 154. 
ACM SIGGRAPH, Addison Wesley, July 1994. [26] Robert E. Tarjan and Jan van Leeuwen. Worst-Case Analysis 
of Set Union Algorithms. Journal of the ACM, 31(2):245 281, April 1984. [27] Frank Thomas and Ollie Johnston. 
Disney Animation: The Illusion of Life. Walt Disney Productions, New York, 1981. [28] B. A. Wallace. 
Merging and Transformation of Raster Images for Cartoon Animation. In Henry Fuchs, editor, SIGGRAPH 81 
Conference Proceedings, Annual Conference Series, pages 253 262. ACM SIGGRAPH, August 1981. [29] Dan 
S. Wallach, Sharma Kunapalli, and Michael F. Co­hen. Accelerated MPEG Compression of Dynamic Polygonal 
Scenes. In Andrew Glassner, editor, SIGGRAPH 94 Confer­ence Proceedings, Annual Conference Series, pages 
193 197. ACM SIGGRAPH, Addison Wesley, July 1994. [30] Walt Disney Home Video. Aladdin and the King of 
Thieves. Distributed by Buena Vista Home Video, Dept. CS, Burbank, CA, 91521. Originally released in 
1992 as a motion picture. [31] Lance R. Williams. Topological Reconstruction of a Smooth Manifold-Solid 
from its Occluding Contour. Technical Report 94-04, University of Massachusetts, Amherst, MA, 1994. [32] 
Georges Winkenbach and David H. Salesin. Computer Generated Pen and Ink Illustration. In Andrew Glassner, 
ed­itor, SIGGRAPH 94 Conference Proceedings, Annual Confer­ence Series, pages 91 100. ACM SIGGRAPH, Addison 
Wes­ley, July 1994. [33] Andrew Witkin and Zoran Popovi´c. Motion Warping. In Robert Cook, editor, SIGGRAPH 
95 Conference Proceed­ings, Annual Conference Series, pages 105 108. ACM SIG-GRAPH, Addison Wesley, August 
1995. [34] George Wolberg. Digital Image Warping. IEEE Computer Society Press, Washington, 1990. [35] 
Daniel N. Wood, Adam Finkelstein, John F. Hughes, Craig E. Thayer, and David H. Salesin. Multiperspective 
Panoramas for Cel Animation. In Turner Whitted, editor, SIGGRAPH 97 Conference Proceedings, Annual Conference 
Series, pages 243 250. ACM SIGGRAPH, Addison Wesley, August 1997.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280950</article_id>
		<sort_key>447</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>44</seq_no>
		<title><![CDATA[A non-photorealistic lighting model for automatic technical illustration]]></title>
		<page_from>447</page_from>
		<page_to>452</page_to>
		<doi_number>10.1145/280814.280950</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280950</url>
		<keywords>
			<kw><![CDATA[color]]></kw>
			<kw><![CDATA[illustration]]></kw>
			<kw><![CDATA[lighting models]]></kw>
			<kw><![CDATA[non-photorealistic rendering]]></kw>
			<kw><![CDATA[shading]]></kw>
			<kw><![CDATA[silhouettes]]></kw>
			<kw><![CDATA[tone]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Fine arts</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010470</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Fine arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP35023445</person_id>
				<author_profile_id><![CDATA[81100057321]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Amy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gooch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Utah, Salt Lake City]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15020772</person_id>
				<author_profile_id><![CDATA[81100057328]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Bruce]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gooch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Utah, Salt Lake City]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39078929</person_id>
				<author_profile_id><![CDATA[81100449948]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shirley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Utah, Salt Lake City]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14061451</person_id>
				<author_profile_id><![CDATA[81100146161]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Elaine]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cohen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Utah, Salt Lake City]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Irving Biederman and Ginny Ju. Surface versus Edge-Based Determinants of Visual Recognition. Cognitive Psychology, 20:38-64, 1988.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Faber Birren. Color Perception in Art. Van Nostrand Reinhold Company, 1976.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Wendy L. Braje, Bosco S. Tjan, and Gordon E. Legge. Human Efficiency for Recognizing and Detecting Low-pass Filtered Objects. Vision Research, 35(21):2955-2966, 1995.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Tom Browning. Timeless Techniques for Better Oil Paintings. North Light Books, 1994.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Chris Christou, Jan J. Koenderink, and Andrea J. van Doom. Surface Gradients, Contours and the Perception of Surface Attitude in Images of Complex Scenes. Perception, 25:701-713, 1996.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258896</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Cassidy J. Curtis, Sean E. Anderson, Kurt W. Fleischer, and David H. Salesin. Computer-Generated Watercolor. In SIGGRAPH 97 Conference Proceedings, August 1997.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Debra Dooley and Michael F. Cohen. Automatic Illustration of 3D Geometric Models: Surfaces. IEEE Computer Graphics and Applications, 13(2):307-314, 1990.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97890</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Gershon Elber and Elaine Cohen. Hidden Curve Removal for Free-Form Surfaces. In SIGGRAPH 90 Conference P1vceedings, August 1990.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[E. Bruce Goldstein. Sensation and Perception. Wadsworth Publishing Co., Belmont, California, 1980.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97902</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Paul Haeberli. Paint By Numbers: Abstract Image Representation. In SIG- GRAPH 90 Conference P1vceedings, August 1990.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97913</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Paul Haeberli. The Accumulation Buffer: Hardware Support for High-Quality Rendering. SIGGRAPH 90 Conference P1vceedings, 24(3), August 1990.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Patricia Lambert. Contlvlling Color: A Practical Intlvduction for Designers and Artists, volume 1. Everbest Printing Company Ltd., 1991.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258893</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Peter Litwinowicz. Processing Images and Video for an Impressionistic Effect. In SIGGRAPH 97 Conference P1vceedings, August 1997.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258894</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[L. Markosian, M. Kowalski, S. Trychin, and J. Hughes. Real-Time Non- Photorealistic Rendering. In SIGGRAPH 97 Conference P~vceedings, August 1997.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Judy Martin. Technical Illustration: Materials, Methods, and Techniques, volume 1. Macdonald and Co Publishers, 1989.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237288</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Barbara J. Meier. Painterly Rendering for Animation. In SIGGRAPH 96 Conference P1vceedings, August 1996.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360839</ref_obj_id>
				<ref_obj_pid>360825</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Bui-Tuong Phong. Illumination for Computer Generated Images. Communications of the ACM, 18(6):311-317, June 1975.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Tom Ruppel, editor. The Way Science Works, volume 1. MacMillan, 1995.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97901</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Takafumi Saito and Tokiichiro Takahashi. Comprehensible Rendering of 3D Shapes. In SIGGRAPH 90 Conference P1vceedings, August 1990.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258890</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Mike Salisbury, Michael T. Wong, John F. Hughes, and David H. Salesin. Orientable Textures for Image-Based Pen-and-Ink Illustration. In SIGGRAPH 97 Conference P1vceedings, August 1997.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122732</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Doree Duncan Seligmann and Steven Feiner. Automated Generation of Intent- Based 3D Illustrations. In SIGGRAPH 91 Conference P~vceedings, July 1991.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Bosco S. Tjan, Wendy L. Braje, Gordon E. Legge, and Daniel Kersten. Human Efficiency for Recognizing 3-D Objects in Luminance Noise. Vision Research, 35(21):3053-3069, 1995.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Edward Tufte. Visual Explanations. Graphics Press, 1997.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258766</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Bruce Walter, Gun Alppay, Eric R F. Lafortune, Sebastian Fernandez, and Donald R Greenberg. Fitting Virtual Lights for Non-Diffuse Walkthroughs. In SIG- GRAPH 97 Conference P~vceedings, pages 45-48, August 1997.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Lance Williams. Shading in Two Dimensions. Graphics Intelface '91, pages 143-151, 1991.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192184</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Georges Winkenbach and David H. Salesin. Computer Generated Pen-and-Ink Illustration. In SIGGRAPH 94 Conference P~vceedings, August 1994.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Copyright &#38;#169;1998 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers or to redistribute to lists, requires specific permission and/or a fee. A Non-Photorealistic 
Lighting Model For Automatic Technical Illustration Amy Gooch Bruce Gooch Peter Shirley Elaine Cohen 
Department of Computer Science University of Utah http://www.cs.utah.edu/ Abstract Phong-shaded 3D imagery 
does not provide geometric information of the same richness as human-drawn technical illustrations. A 
non-photorealistic lighting model is presented that attempts to nar­row this gap. The model is based 
on practice in traditional tech­nical illustration, where the lighting model uses both luminance and 
changes in hue to indicate surface orientation, reserving ex­treme lights and darks for edge lines and 
highlights. The light­ing model allows shading to occur only in mid-tones so that edge lines and highlights 
remain visually prominent. In addition, we show how this lighting model is modi.ed when portraying models 
of metal objects. These illustration methods give a clearer picture of shape, structure, and material 
composition than traditional com­puter graphics methods. CR Categories: I.3.0 [Computer Graphics]: General; 
I.3.6 [Com­puter Graphics]: Methodology and Techniques. Keywords: illustration, non-photorealistic rendering, 
silhouettes, lighting models, tone, color, shading 1 Introduction The advent of photography and computers 
has not replaced artists, illustrators, or draftsmen, despite rising salaries and the decreasing cost 
of photographic and computer rendering technology. Almost all manuals that involve 3D objects, e.g., 
a car owner s manual, have illustrations rather than photographs. This lack of photography is present 
even in applications where aesthetics are a side-issue, and communication of geometry is the key. Examining 
technical man­uals, illustrated textbooks, and encyclopedias reveals illustration conventions that are 
quite different from current computer graphics methods. These conventions fall under the umbrella term 
technical illustrations. In this paper we attempt to automate some of these conventions. In particular, 
we adopt a shading algorithm based on cool-to-warm tones such as shown in the non-technical image in 
Figure 1. We adopt this style of shading to ensure that black sil­houettes and edge lines are clearly 
visible which is often not the case when they are drawn in conjunction with traditional computer graphics 
shading. The fundamental idea in this paper is that when silhouettes and other edge lines are explicitly 
drawn, then very low Figure 1: The non-photorealistic cool (blue) to warm (tan) tran­sition on the skin 
of the garlic in this non-technical setting is an example of the technique automated in this paper for 
technical il­lustrations. Colored pencil drawing by Susan Ashurst. dynamic range shading is needed for 
the interior. As artists have discovered, adding a somewhat arti.cial hue shift to shading helps imply 
shape without requiring a large dynamic range. This hue shift can interfere with precise albedo perception, 
but this is not a major concern in technical illustration where the communication of shape and form are 
valued above realism. In Section 2 we review previ­ous computer graphics work, and conclude that little 
has been done to produce shaded technical drawings. In Section 3 we review the common technical illustration 
practices. In Section 4 we describe how we have automated some of these practices. We discuss future 
work and summarize in Section 5. 2 Related Work Computer graphics algorithms that imitate non-photographic 
tech­niques such as painting or pen-and-ink are referred to as non­photorealistic rendering (NPR). The 
various NPR methods differ greatly in style and visual appearance, but are all closely related to conventional 
artistic techniques (e.g., [6, 8, 10, 13, 14, 16, 20, 26]). An underlying assumption in NPR is that artistic 
techniques devel­oped by human artists have intrinsic merit based on the evolution­ary nature of art. 
We follow this assumption in the case of technical illustration. NPR techniques used in computer graphics 
vary greatly in their level of abstraction. Those that produce a loss of detail, such as semi-randomized 
watercolor or pen-and-ink, produce a very high level of abstraction, which would be inappropriate for 
most techni­cal illustrations. Photorealistic rendering techniques provide little abstraction, so photorealistic 
images tend to be more confusing than less detailed human-drawn technical illustrations. Technical illus­trations 
occupy the middle ground of abstraction, where the im­portant three-dimensional properties of objects 
are accented while extraneous detail is diminished or eliminated. Images at any level of abstraction 
can be aesthetically pleasing, but this is a side-effect rather than a primary goal for technical illustration. 
A rationale for using abstraction to eliminate detail from an image is that, unlike the case of 3D scene 
perception, the image viewer is not able to use motion, accommodation, or parallax cues to help deal 
with visual complexity. Using abstraction to simplify images helps the user overcome the loss of these 
spatial cues in a 2D image. In computer graphics, there has been little work related to techni­cal illustration. 
Saito and Takahashi [19] use a variety of techniques to show geometric properties of objects, but their 
images do not fol­low many of the technical illustration conventions. Seligmann and Feiner present a 
system that automatically generates explanation­based drawings [21]. Their system focuses primarily on 
what to draw, with secondary attention to visual style. Our work deals pri­marily with visual style rather 
than layout issues, and thus there is little overlap with Seligmann and Feiner s system, although the 
two methods would combine naturally. The work closest to our own was presented by Dooley and Cohen [7] 
who employ a user-de.ned hierarchy of components, such as line width, transparency, and line end/boundary 
conditions to generate an image. Our goal is a sim­pler and more automatic system, that imitates methods 
for line and color use found in technical illustrations. Williams also developed similar techniques to 
those described here for non-technical appli­cations, including some warm-to-cool tones to approximate 
global illumination, and drawing conventions for specular objects [25].  3 Illustration Techniques Based 
on the illustrations in several books, e.g. [15, 18], we con­clude that illustrators use fairly algorithmic 
principles. Although there are a wide variety of styles and techniques found in technical illustration, 
there are some common themes. This is particularly I=kdka+kdmax direction. He states that this is an 
example of the strategy of the smallest effective difference: Make all visual distinctions as subtle 
as possible, but still clear and effective. Tufte feels that this principle is so important that he devotes 
an en­tire chapter to it. The principle provides a possible explanation of why cross-hatching is common 
in black and white drawings and rare in colored drawings: colored shading provides a more subtle, but 
adequately effective, difference to communicate surface orien­tation.  4 Automatic Lighting Model All 
of the characteristics from Section 3 can be automated in a straightforward manner. Edge lines are drawn 
in black, and high­lights are drawn using the traditional exponential term from the Phong lighting model 
[17]. In Section 4.1, we consider matte ob­jects and present reasons why traditional shading techniques 
are insuf.cient for technical illustration. We then describe a low dy­namic range artistic tone algorithm 
in Section 4.2. Next we provide an alogrithm to approximate the anisotropic appearance of metal objects, 
described in Section 4.3. We provide approximations to these algorithms using traditional Phong shading 
in Section 4.4. 4.1 Traditional Shading of Matte Objects In addition to drawing edge lines and highlights, 
we need to shade the surfaces of objects. Traditional diffuse shading sets luminance proportional to 
the cosine of the angle between light direction and surface normal: , (1) 0; ^l^n true when we examine 
color illustrations done with air-brush and where I pen. We have observed the following characteristics 
in many illus­is the RGB color to be displayed for a given point on the trations: surface, kdis the RGB 
diffuse re.ectance at the point, kais the RGB ambient illumination, ^l is the unit vector in the direction 
of edge lines, the set containing surface boundaries, silhouettes, the light source, andis the unit 
surface normal vector at the point. ^n and discontinuities, are drawn with black curves. matte objects 
are shaded with intensities far from black or white with warmth or coolness of color indicative of surface 
normal; a single light source provides white highlights.  shadowing is not shown.  metal objects are 
shaded as if very anisotropic.  We view these characteristics as resulting from a hierarchy of priorities. 
The edge lines and highlights are black and white, and provide a great deal of shape information themselves. 
Several stud­ies in the .eld of perception have concluded that subjects can rec­ognize 3D objects at 
least as well, if not better, when the edge lines (contours) are drawn versus shaded or textured images 
[1, 3, 5, 22]. However, when shading is added in addition to edge lines, more information is provided 
only if the shading uses colors that are vi­sually distinct from both black and white. This means the 
dynamic range available for shading is extremely limited. In most technical illustrations, shape information 
is valued above precise re.ectance information, so hue changes are used to indicate surface orientation 
rather than re.ectance. This theme will be investigated in detail in the next section. A simple low dynamic-range 
shading model is consistent with several of the principles from Tufte s recent book [23]. He has a case-study 
of improving a computer graphics animation by lower­ing the contrast of the shading and adding black 
lines to indicate This model is shown for kd=1and ka=0in Figure 3. This unsatisfactory image hides shape 
and material information in the dark regions. Additional information about the object can be pro­vided 
by both highlights and edge lines. These are shown alone in Figure 4 with no shading. We cannot effectively 
add edge lines and highlights to Figure 3 because the highlights would be lost in the light regions and 
the edge lines would be lost in the dark regions. To add edge lines to the shading in Equation 1, we 
can use either of two standard heuristics. First we could raise kauntil it is large enough that the dim 
shading is visually distinct from the black edge lines, but this would result in loss of .ne details. 
Alternatively, we could add a second light source, which would add con.icting highlights and shading. 
To make the highlights visible on top of the shading, we can lower kduntil it is visually distinct from 
white. An image with hand-tuned kaand kdis shown in Figure 5. This is the best achromatic image using 
one light source and traditional shading. This image is poor at communicating shape information, such 
as details in the claw nearest the bottom of the image. This part of the image is colored the constant 
shade kdkaregardless of surface orientation.  4.2 Tone-based Shading of Matte Objects In a colored medium 
such as air-brush and pen, artists often use both hue and luminance (greyscale intensity) shifts. Adding 
blacks and whites to a given color results in what artists call shades in the case of black, and tints 
in the case of white. When color scales are fuse shading where kcoolis pure black and kwarm =kd.This 
would look much like traditional diffuse shading, but the entire ob­ ject would vary in luminance, including 
where ^n^l <0.What we would really like is a compromise between these strategies. These transitions will 
result in a combination of tone scaled object-color and a cool-to-warm undertone, an effect which artists 
achieve by combining pigments. We can simulate undertones by a linear blend between the blue/yellow and 
black/object-color tones: kcool=kblue+Ckd kwarm =kyellow+;kd (3) Plugging these values into Equation 
2 leaves us with four free pa- Figure 2: How the tone is created for a pure red object by summing rameters: 
b, y, C,and ;. The values for band ywill determine the a blue-to-yellow and a dark-red-to-red tone. strength 
of the overall temperature shift, and the values of Cand ; will determine the prominence of the object 
color and the strength of the luminance shift. Because we want to stay away from shad­ created by adding 
grey to a certain color they are called tones [2]. Such tones vary in hue but do not typically vary much 
in luminance. When the complement of a color is used to create a color scale, they are also called tones. 
Tones are considered a crucial concept to il­lustrators, and are especially useful when the illustrator 
is restricted to a small luminance range [12]. Another quality of color used by artists is the temperature 
of the color. The temperature of a color is de.ned as being warm (red, orange, and yellow), cool (blue, 
vi­olet, and green), or temperate (red-violets and yellow-greens). The depth cue comes from the perception 
that cool colors recede while warm colors advance. In addition, object colors change tempera­ture in 
sunlit scenes because cool skylight and warm sunlight vary in relative contribution across the surface, 
so there may be ecolog­ical reasons to expect humans to be sensitive to color temperature variation. 
Not only is the temperature of a hue dependent upon the hue itself, but this advancing and receding relationship 
is ef­fected by proximity [4]. We will use these techniques and their psychophysical relationship as 
the basis for our model. We can generalize the classic computer graphics shading model ing which will 
visually interfere with black and white, we should supply intermediate values for these constants. An 
example of a resulting tone for a pure red object is shown in Figure 2. Substituting the values for kcooland 
kwarmfrom Equation 3 into the tone Equation 2 results in shading with values within the middle luminance 
range as desired. Figure 7 is shown with b=0:4, y=0:4, C=0:2,and ;=0:6. To show that the exact values 
are not crucial to appropriate appearance, the same model is shown in Figure 8 with b=0:55, y=0:3, C=0:25,and 
;=0:5. Unlike Figure 5, subtleties of shape in the claws are visible in Figures 7 and 8. The model is 
appropriate for a range of object colors. Both tra­ditional shading and the new tone-based shading are 
applied to a set of spheres in Figure 9. Note that with the new shading method objects retain their color 
name so colors can still be used to differ­entiate objects like countries on a political map, but the 
intensities used do not interfere with the clear perception of black edge lines and white highlights. 
^l to experiment with tones by using the cosine term (tion 1 to blend between two RGB colors, kcooland 
kwarm: ( 4.3 Shading of Metal Objects ) of Equa­ ^n ( 1+ 1+ ^^ warm (2) l ^n l ^n = kk 1 , cool + Illustrators 
use a different technique to communicate whether or 2 2 not an object is made of metal. In practice illustrators 
represent varies over the interval [,1;1]. To ensure a metallic surface by alternating dark and light 
bands. This tech-Note that the quantity ^n ^l nique is the artistic representation of real effects that 
can be seen the image shows this full variation, the light vector ^l should be per­on milled metal parts, 
such as those found on cars or appliances. pendicular to the gaze direction. Because the human vision 
system Milling creates what is known as anisotropic re.ection. Lines are assumes illumination comes from 
above [9], we chose to position streaked in the direction of the axis of minimum curvature, parallel 
the light up and to the right and to keep this position constant. to the milling axis. Interestingly, 
this visual convention is used even An image that uses a color scale with little luminance variation 
for smooth metal objects [15, 18]. This convention emphasizes that is shown in Figure 6. This image shows 
that a sense of depth can be realism is not the primary goal of technical illustration. To simulate a 
milled object, we map a set of twenty stripes of varying intensity along the parametric axis of maximum 
curvature. The stripes are random intensities between 0.0 and 0.5 with the stripe closest to the light 
source direction overwritten with white. Between the stripe centers the colors are linearly interpolated. 
An object is shown Phong-shaded, metal-shaded (with and without edge lines), and metal-shaded with a 
cool-warm hue shift in Fig­ure 10. The metal-shaded object is more obviously metal than the Phong-shaded 
image. The cool-warm hue metal-shaded object is not quite as convincing as the achromatic image, but 
it is more vi­sually consistent with the cool-warm matte shaded model of Sec­tion 4.2, so it is useful 
when both metal and matte objects are shown together. We note that our banding algorithm is very similar 
to the technique Williams applied to a clear drinking glass using image processing [25]. communicated 
at least partially by a hue shift. However, the lack of a strong cool to warm hue shift and the lack 
of a luminance shift makes the shape information subtle. We speculate that the unnatural colors are also 
problematic. In order to automate this hue shift technique and to add some lu­minance variation to our 
use of tones, we can examine two extreme possibilities for color scale generation: blue to yellow tones 
and scaled object-color shades. Our .nal model is a linear combination of these techniques. Blue and 
yellow tones are chosen to insure a cool to warm color transition regardless of the diffuse color of 
the object. The blue-to-yellow tones range from a fully saturated blue: kblue=0;0;b);b2[0;1]in RGB space 
to a fully saturated yel­low: kyellow=y;y;0);y2[0;1]. This produces a very sculpted but unnatural image, 
and is independent of the object s diffuse re­.ectance kd. The extreme tone related to kdis a variation 
of dif­  4.4 Approximation to new model Our model cannot be implemented directly in high-level graphics 
packages that use Phong shading. However, we can use the Phong lighting model as a basis for approximating 
our model. This is in the spirit of the non-linear approximation to global illumination used by Walter 
et al. [24]. In most graphics systems (e.g. OpenGL) we can use negative colors for the lights. We can 
approximate  References [1] Irving Biederman and Ginny Ju. Surface versus Edge-Based Determinants of 
Visual Recognition. Cognitive Psychology, 20:38 64, 1988. [2] Faber Birren. Color Perception in Art. 
Van Nostrand Reinhold Company, 1976. [3] Wendy L. Braje, Bosco S. Tjan, and Gordon E. Legge. Human Ef.ciency 
for Recognizing and Detecting Low-pass Filtered Objects. Vision Research, 35(21):2955 2966, 1995. Equation 
2 by two lights in directions , kwarm,kcool).2and kcool,kwarm).2respectively, and an ambient term of 
kcool+kwarm).2. This assumes the object color is set to white. We turn off the Phong highlight because 
the neg­ative blue light causes jarring artifacts. Highlights could be added on systems with accumulation 
buffers [11]. This approximation is shown compared to traditional Phong shading and the exact model in 
Figure 11. Like Walter et al., we need different light colors for each object. We could avoid these artifacts 
by using accumulation techniques which are available in many graphics libraries. Edge lines for highly 
complex objects can be generated inter­actively using Markosian et al. s technique [14]. This only works 
for polygonal objects, so higher-order geometric models must be tessellated to apply that technique. 
On high-end systems, image­processing techniques [19] could be made interactive. For metals on a conventional 
API, we cannot just use a light source. However, either environment maps or texture maps can be used 
to produce alternating light and dark stripes. ^l 5FutureWorkandConclusion andinteractingwith3Dtechnicalillustrations.Itmayalsobepos- 
The shading algorithm presented here is exploratory, and we ex­pect many improvements are possible. The 
most interesting open ended question in automatic technical illustrations is how illustra­tion rules 
may change or evolve when illustrating a scene instead of single objects, as well as the practical issues 
involved in viewing sible to automate other application-speci.c illustration forms, such as medical illustration. 
The model we have presented is tailored to imitate colored tech­nical drawings. Once the global parameters 
of the model are set, the technique is automatic and can be used in place of traditional il­lumination 
models. The model can be approximated by interactive graphics techniques, and should be useful in any 
application where communicating shape and function is paramount.  Acknowledgments Thanks to Bill Martin, 
David Johnson, Brian Smits and the mem­bers of the University of Utah Computer Graphics groups for help 
in the initial stages of the paper, to Richard Coffey for helping to get the paper in its .nal form, 
to Bill Thompson for getting us to reex­amine our interpretation of why some illustration rules might 
apply, to Susan Ashurst for the sharing her wealth of artistic knowledge, to Dan Kersten for valuable 
pointers into the perception literature, and to Jason Herschaft for the dinosaur claw model. This work 
was originally inspired by the talk by Jane Hurd in the SIGGRAPH 97 panel on medical visualization. This 
work was supported in part by DARPA (F33615-96-C-5621) and the NSF Science and Tech­nology Center for 
Computer Graphics and Scienti.c Visualization (ASC-89-20219). All opinions, .ndings, conclusions or recom­mendations 
expressed in this document are those of the author and do not necessarily re.ect the views of the sponsoring 
agencies. ^l and with intensities [4] Tom Browning. Timeless Techniques for Better Oil Paintings. North 
Light Books, 1994. [5] Chris Christou, Jan J. Koenderink, and Andrea J. van Doorn. Surface Gradients, 
Contours and the Perception of Surface Attitude in Images of Complex Scenes. Perception, 25:701 713, 
1996. [6] Cassidy J. Curtis, Sean E. Anderson, Kurt W. Fleischer, and David H. Salesin. Computer-Generated 
Watercolor. In SIGGRAPH 97 Conference Proceedings, August 1997. [7] Debra Dooley and Michael F. Cohen. 
Automatic Illustration of 3D Geometric Models: Surfaces. IEEE Computer Graphics and Applications, 13(2):307 
314, 1990. [8] Gershon Elber and Elaine Cohen. Hidden Curve Removal for Free-Form Sur­faces. In SIGGRAPH 
90 Conference Proceedings, August 1990. [9] E. Bruce Goldstein. Sensation and Perception. Wadsworth Publishing 
Co., Bel­mont, California, 1980. [10] Paul Haeberli. Paint By Numbers: Abstract Image Representation. 
In SIG-GRAPH 90 Conference Proceedings, August 1990. [11] Paul Haeberli. The Accumulation Buffer: Hardware 
Support for High-Quality Rendering. SIGGRAPH 90 Conference Proceedings, 24(3), August 1990. [12] Patricia 
Lambert. Controlling Color: A Practical Introduction for Designers and Artists, volume 1. Everbest Printing 
Company Ltd., 1991. [13] Peter Litwinowicz. Processing Images and Video for an Impressionistic Effect. 
In SIGGRAPH 97 Conference Proceedings, August 1997. [14] L. Markosian, M. Kowalski, S. Trychin, and J. 
Hughes. Real-Time Non-Photorealistic Rendering. In SIGGRAPH 97 Conference Proceedings, August 1997. [15] 
Judy Martin. Technical Illustration: Materials, Methods, and Techniques,vol­ume 1. Macdonald and Co Publishers, 
1989. [16] Barbara J. Meier. Painterly Rendering for Animation. In SIGGRAPH 96 Con­ference Proceedings, 
August 1996. [17] Bui-Tuong Phong. Illumination for Computer Generated Images. Communica­tions of the 
ACM, 18(6):311 317, June 1975. [18] Tom Ruppel, editor. The Way Science Works, volume 1. MacMillan, 1995. 
[19] Takafumi Saito and Tokiichiro Takahashi. Comprehensible Rendering of 3D Shapes. In SIGGRAPH 90 Conference 
Proceedings, August 1990. [20] Mike Salisbury, Michael T. Wong, John F. Hughes, and David H. Salesin. 
Ori­entable Textures for Image-Based Pen-and-Ink Illustration. In SIGGRAPH 97 Conference Proceedings, 
August 1997. [21] Doree Duncan Seligmann and Steven Feiner. Automated Generation of Intent-Based 3D Illustrations. 
In SIGGRAPH 91 Conference Proceedings, July 1991. [22] Bosco S. Tjan, Wendy L. Braje, Gordon E. Legge, 
and Daniel Kersten. Human Ef.ciency for Recognizing 3-D Objects in Luminance Noise. Vision Research, 
35(21):3053 3069, 1995. [23] Edward Tufte. Visual Explanations. Graphics Press, 1997. [24] Bruce Walter, 
Gun Alppay, Eric P. F. Lafortune, Sebastian Fernandez, and Don­ald P. Greenberg. Fitting Virtual Lights 
for Non-Diffuse Walkthroughs. In SIG-GRAPH 97 Conference Proceedings, pages 45 48, August 1997. [25] 
Lance Williams. Shading in Two Dimensions. Graphics Interface 91, pages 143 151, 1991. [26] Georges Winkenbach 
and David H. Salesin. Computer Generated Pen-and-Ink Illustration. In SIGGRAPH 94 Conference Proceedings, 
August 1994.  Figure 3: Diffuse shaded image using Equation 1 with kd=1and Figure 6: Approximately 
constant luminance tone rendering. Edge ka =0. Black shaded regions hide details, especially in the small 
lines and highlights are clearly noticeable. Unlike Figures 3 and 5 claws; edge lines could not be seen 
if added. Highlights and .ne some details in shaded regions, like the small claws, are visible. details 
are lost in the white shaded regions. The lack of luminance shift makes these changes subtle.  Figure 
4: Image with only highlights and edges. The edge lines provide divisions between object pieces and the 
highlights convey the direction of the light. Some shape information is lost, especially in the regions 
of high curvature of the object pieces. However, these highlights and edges could not be added to Figure 
3 because the highlights would be invisible in the light regions and the silhouettes would be invisible 
in the dark regions. Figure 7: Luminance/hue tone rendering. This image combines the luminance shift 
of Figure 3 and the hue shift of Figure 6. Edge lines, highlights, .ne details in the dark shaded regions 
such as the small claws, as well as details in the high luminance regions are all vis­ible. In addition, 
shape details are apparent unlike Figure 4 where the object appears .at. In this .gure, the variables 
of Equation 2 and Equation 3 are: b=0:4, y=0:4, C=0:2, ;=0:6.  Figure 5: Phong shaded image with edge 
lines and kd=0:5and ka =0:1. Like Figure 3, details are lost in the dark grey regions, especially in 
the small claws, where they are colored the constant shade of kdkaregardless of surface orientation. 
However, edge lines and highlights provide shape information that was gained in Figure 4, but couldn 
t be added to Figure 3. Figure 8: Luminance/hue tone rendering, similar to Figure 7 except b=0:55, y=0:3, 
C=0:25, ;=0:5. The different values of b and ydetermine the strength of the overall temperature shift, 
where as Cand ;determine the prominence of the object color, and the strength of the luminance shift. 
 Figure 9: Top: Colored Phong-shaded spheres with edge lines and highlights. Bottom: Colored spheres 
shaded with hue and luminance shift, including edge lines and highlights. Note: In the .rst Phong shaded 
sphere (violet), the edge lines disappear, but are visible in the corresponding hue and luminance shaded 
violet sphere. In the last Phong shaded sphere (white), the highlight vanishes, but is noticed in the 
corresponding hue and luminance shaded white sphere below it. The spheres in the second row also retain 
their color name .  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>280951</article_id>
		<sort_key>453</sort_key>
		<display_label></display_label>
		<article_publication_date>07-24-1998</article_publication_date>
		<seq_no>45</seq_no>
		<title><![CDATA[Painterly rendering with curved brush strokes of multiple sizes]]></title>
		<page_from>453</page_from>
		<page_to>460</page_to>
		<doi_number>10.1145/280814.280951</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=280951</url>
		<keywords>
			<kw><![CDATA[non-photrealistic rendering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Fine arts</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010470</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Fine arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Experimentation</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14018548</person_id>
				<author_profile_id><![CDATA[81100015154]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Aaron]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hertzmann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[New York Univ., New York]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ADOBE SYSTEMS. Adobe Photoshop 4.0]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192181</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[DEBORAH F. BERMAN, JASON T. BARTELL, DAVID H. SALESIN. Multiresolution Painting and Compositing. SIGGRAPH 94 Conference Proceedings, pp. 85-90. July 1994.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[PETER J. BURT AND EDWARD U. ADELSON. The Laplacian Pyramid as a Compact Image Code. IEEE Transactions on Communications. 31:532-540, April 1983.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>212174</ref_obj_id>
				<ref_obj_pid>212154</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[HAROLD COHEN. The Further Exploits of Aaron, Painter. Stanford Humanities Review. Vol. 4, No. 2. pp. 141-158. 1995]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258896</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[CASSlDY J. CURTIS, SEAM E. ANDERSON, JOSHUA E. SEIMS, KURT W. FLEISCHER, DAVID H. SALESIN. Computer-Generated Watercolor. SIGGRAPH 97 Conference Proceedings, pp. 421- 430. August 1997.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83821</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[JAMES FOLEY, ANDRIES VAN DAM, STEPHEN FEINER, JOHN HUGHES. Computer Graphics: Principles and Practice, Addison- Wesley, 1995.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[FRACTAL DESIGN CORPORATION. Fractal Design Painter.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97902</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[PAUL HAEBERLI. Paint by numbers: Abstract image representations. Computer Graphics (SIGGRAPH 90 Conference Proceedings), 24(4):207-214, August 1990]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[PAUL HAEBERLI. The Impressionist. http ://www. sgi.com/graphica/impression]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618268</ref_obj_id>
				<ref_obj_pid>616036</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[JOHN LANSDOWN AND SIMON SCHOFIELD. Expressive rendering: A review of nonphotorealistic techniques. IEEE Computer Graphics and Applications, 15(3):29-37, May 1995.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258893</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[PETER LITWINOWICZ. Processing Images and Video for An Impressionist Effect. SIGGRAPH 97 Conference Proceedings, pp. 407-414. August 1997.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258894</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[LEE MARKOSIAN, MICHAEL A. KOWALSKI, S AMUEL J. TRYCHIN, LUBOMIR D. BOURDEV, DANIEL GOLDSTEIN, JOHN F. HUGHES. Real-Time Nonphotorealistic Rendering. SIGGRAPH 97 Conference Proceedings, pp. 415-420. August 1997.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>96736</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[PAMELA MCCORDUCK. AARON's CODE: Meta-Art, Artificial Intelligence, and the Work of Harold Cohen. New York: W. H. Freeman &amp; Co. 225 pages. 1991.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[MICROSOFT CORPORATION. Microsoft Image Composer 1.5]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218437</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[KEN PERLIN AND LUIZ VELHO. LivePaint: Painting with Procedural Multiscale Textures, SIGGRAPH 95 Conference Proceedings, pp. 153-160. 1995.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>258890</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[MICHAELP. SALISBURY, MICHAEL T. WONG, JOHN F. HUGHES, DAVID H. SALESIN. Orientable Textures for Image- Based Pen-and-Ink Illustration. SIGGRAPH 97 Conference Proceedings, pp. 401-406. August 1997.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[S.M.F. TREAVETT AND M. CHEN. Statistical Techniques for the Automated Synthesis of Non-Photorealistic Images. Proc. 15th Eurographics UK Conference, March 1997.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237285</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[GREG TURK AND DAVID BANKS. Image-Guided Streamline Placement. SIGGRAPH 96 Conference Proceedings, pp. 453- 460. August 1996.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>2319507</ref_obj_id>
				<ref_obj_pid>2318958</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[JOACHIM WEICKERT, BART M. TER HAAR ROMNEY, MAX A. VIERGEVEP~ Efficient and reliable schemes for nonlinear diffusion filtering. IEEE Transactions on Image Processing. March 1998.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192184</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[GEORGES WINKENBACH AND DAVID H. SALESIN. Computer- Generated Pen-and-Ink Illustration. SIGGRAPH 94 Conference Proceedings, pp. 91-100. July 1994.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237287</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[GEORGES WINKENBACH AND DAVID H. SALESIN. Rendering Parametric Surfaces in Pen and Ink. SIGGRAPH 96 Conference Proceedings, pp. 469-476. August 1996.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[XAOS TOOLS. Paint Alchemy 2.0]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Copyright &#38;#169;1998 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers or to redistribute to lists, requires specific permission and/or a fee. Painterly Rendering 
with Curved Brush Strokes of Multiple Sizes Aaron Hertzmann Media Research Laboratory Department of Computer 
Science New York University ABSTRACT We present a new method for creating an image with a hand­painted 
appearance from a photograph, and a new approach to designing styles of illustration. We paint an image 
with a series of spline brush strokes. Brush strokes are chosen to match colors in a source image. A 
painting is built up in a series of layers, starting with a rough sketch drawn with a large brush. The 
sketch is painted over with progressively smaller brushes, but only in areas where the sketch differs 
from the blurred source image. Thus, visual emphasis in the painting corresponds roughly to the spatial 
energy present in the source image. We demonstrate a technique for painting with long, curved brush strokes, 
aligned to normals of image gradients. Thus we begin to explore the expressive quality of complex brush 
strokes. Rather than process images with a single manner of painting, we present a framework for describing 
a wide range of visual styles. A style is described as an intuitive set of parameters to the painting 
algorithm that a designer can adjust to vary the style of painting. We show examples of images rendered 
with different styles, and discuss long-term goals for expressive rendering styles as a general-purpose 
design tool for artists and animators. CR Categories and Subject Descriptors: I.3.3 [Computer Graphics]: 
Picture/Image Generation Display algorithms Additional Keywords: Non-photorealistic rendering 1. INTRODUCTION 
Art and illustration have historically been the sole domain of artists skilled and creative individuals 
willing to devote considerable time and resources to the creation of images. Computer technology now 
allows the quick and easy creation of highly realistic images of natural and imaginary scenes. This technology 
automates the tedious details of photorealistic rendering, although the process is still driven by the 
human user, who selects the scene and rendering parameters. The technology for producing non-photorealistic 
works such as paintings and drawings is less advanced the user must either paint the entire image interactively 
with a paint program, or else must process an image or 3D model through a narrowly­defined set of non-photorealistic 
filters. Ideally, a human user email: hertzman@mrl.nyu.edu ; URL: http://www.mrl.nyu.edu/~hertzman should 
be able to choose from a wide range of visual styles, while leaving the mechanical details of image creation 
to a computer. It is now possible to envision animating a feature­length movie in a watercolor or oil 
painting style, a feat that would be prohibitively labor-intensive with traditional media. Non-photorealistic 
rendering can also be used to inexpensively create attractive and concise images for graphic design and 
illustration. Most current computer painterly rendering algorithms use very simple brush strokes that 
are all of equal size and shape. Thus, the resulting images tend to appear mechanical in comparison to 
hand-made work. In this paper, we present techniques for painting an image with multiple brush sizes, 
and for painting with long, curved brush strokes. We find the resulting images to be more visually pleasing 
and natural than those produced with previous algorithms. Artists have long exploited the richness of 
natural media in a variety of unique styles. Naturally, we would like our computer algorithms to be capable 
of similar variety. Here we do not attempt to imbue creativity into the algorithms, but prefer a more 
cooperative relationship. Rather, the user selects a composition and a rendering style, and the computer 
produces an image from these choices. In this paper we show how to create rendering styles suitable for 
use by a human designer. 1.1 Related work Two principal challenges face the production of satisfying 
non-photorealistic images. The first of these, physical simulation, attempts to closely mimic the physical 
appearance of real-world artistic media. Impressive systems have been demonstrated for watercolor [5] 
and a variety of other media [7], in which the user places brush strokes interactively or semi­interactively. 
Wet media such as watercolor and oil paint are the most challenging media to simulate, because of the 
complex and rich set of effects produced by fluid flow and transparency. In this paper, we are not concerned 
with a convincing physical simulation; Haeberli [8] and others have shown that striking compositions 
can be produced even with very simple painting models. A related area of research is multiresolution 
painting [2,15], a set of techniques for interactive painting at all scales. This paper extends the complementary 
line of research: automatic painting and drawing without human intervention. Cohen [4,13] casts the problem 
in terms of artificial intelligence; his system, named Aaron, follows a set of randomized rules to create 
original compositions in a specific style. Aaron even has a robotic painting device. Unlike Cohen's work, 
we assume that the composition is provided to the system in the form of an input image to be painted. 
Hence, we can focus on creating a painterly style, and need not deal with the problem of creativity in 
designing a composition. Winkenbach and Salesin [20,21] describe a system for automatically creating 
pen-and-ink illustrations from 3D models, and Salisbury et al. [16] describe a technique for producing 
pen-and-ink illustrations from images. Curtis et al. [5] produce watercolor paintings by a semi-automatic 
algorithm. However, their algorithm does not necessarily produce visible brush strokes, and thus lacks 
a painterly quality. A common method for processing an image for a painterly effect [1,7,11,14,17,22] 
is to place a jittered grid of short brush strokes over an image. These brush strokes may be aligned 
to the normals of image gradients, and all have the same size and shape. (Litwinowicz [11] uses clipped 
strokes; Treavett and Chen [17] use statistical analysis of the source image to guide stroke size, orientation 
and placement.) [14] and [17] do vary brush stroke size with respect to local detail levels. They appear 
to paint each image in a single pass, and thus lack the ability to refine the painting with multiple 
passes. [14] and [22] allow rendering parameters to be encapsulated and saved as styles. 1.2 Overview 
In the next section, we present a method for painting with different brush sizes to express various levels 
of detail in an image, and a technique for painting long, curved brush strokes to express continuous 
color regions in an image. In Section 3, we show how to abstract the rendering process to provide many 
painting styles. Finally, we discuss some future directions for non-photorealistic rendering.  2. PAINTING 
TECHNIQUES 2.1 Varying the brush size Often, an artist will begin a painting as a rough sketch, and 
go back over the painting with a smaller brush to add detail. While much of the motivation for this technique 
does not apply to computer algorithms,1 it also yields desirable visual effects. In Figure 1, note the 
different character of painting used to depict the blouse, sand, children, boat and sky. Each has been 
painted with different brush sizes as well as different stroke styles. This variation in stroke quality 
helps to draw the most attention to the woman and figures, and very little to the ground; in other words, 
the artist has used fine strokes to draw our attention to fine detail. (Other compositional devices such 
as shape, contrast and color are also used. These are not addressed in this paper.) To use strokes of 
the same size for each region would flatten the painting; here the artist has chosen to emphasize the 
figure over the background. In our image processing algorithm, we use fine brush strokes only where necessary 
to refine the painting, and leave the rest of the painting alone. Our algorithm is similar to a pyramid 
algorithm [3], in that we start with a coarse approximation to the source image, and add progressive 
refinements with smaller and smaller brushes.2 Our painting algorithm (Figure 2) takes as input a source 
image and a list of brush sizes. The brush sizes are expressed in radii R1 ... Rn. The algorithm then 
proceeds by painting a series of layers, one for each radius, from largest to smallest. The initial canvas 
is a constant color image. 1 One motivation is to establish the composition before committing fine details, 
so that the artist may experiment and adjust the composition.2 In fact, our original painting algorithm 
was based on the Laplacian pyramid: difference images (Li) guided brush stroke placement. However, the 
difference images assume a perfect reconstruction of the lower levels of the pyramid, and our reconstruction 
is deliberately imperfect. Thus, refinements at later levels of the pyramid caused unwanted artifacts. 
Our present algorithm avoids this problem by creating the difference images after every step of the painting. 
 For each layer Ri, we first create a reference image by blurring the source image. Blurring is performed 
by convolution with a Gaussian kernel of standard deviation fs Ri, where fs is some constant factor.3 
The reference image represents the image we want to approximate by painting with the current brush size. 
The idea is to use each brush to capture only details which are at least as large as the brush size. 
We use a layer subroutine to paint a layer with brush Ri, based on the reference image. This procedure 
locates areas of the image that differ from the reference image and covers them with new brush strokes. 
Areas that match the source image color to within a threshold (T) are left unchanged. The threshold parameter 
can be increased to produce rougher paintings, or decreased to produce paintings that closely match the 
source image. This entire procedure is repeated for each brush stroke size. A pseudocode summary of the 
painting algorithm follows. function paint(sourceImage,R1 ... Rn) { canvas := a new constant color image 
// paint the canvas for each brush radius Ri, from largest to smallest do { // apply Gaussian blur referenceImage 
= sourceImage * G(fs Ri) // paint a layer paintLayer(canvas, referenceImage, Ri) } return canvas } 3 
Non-linear diffusion [19] may be used instead of a Gaussian blur to produce slightly better results near 
edges. (Figure 5(a)).  Each layer is painted using a simple loop over the image canvas. The approach 
is adapted from the algorithm described in [11], which placed strokes on a jittered grid. That approach 
may miss sharp details such as lines and points that pass between grid points. Instead, we search each 
grid point s neighborhood to find the nearby point with the greatest error, and paint at this location. 
All strokes for the layer are planned at once before rendering. Then the strokes are rendered in random 
order to prevent an undesirable appearance of regularity in the brush strokes. procedure paintLayer(canvas,referenceImage, 
R) { S := a new set of strokes, initially empty // create a pointwise difference image D := difference(canvas,referenceImage) 
 grid := fg R for x=0 to imageWidth stepsize grid do for y=0 to imageHeight stepsize grid do { // sum 
the error near (x,y) M := the region (x-grid/2..x+grid/2, y-grid/2..y+grid/2) areaError := . / grid2 
ij , .M Di,j if (areaError > T) then { // find the largest error point (x1,y1) := arg maxij , .M Di,j 
s :=makeStroke(R,x1,y1,referenceImage) add s to S } } paint all strokes in S on the canvas, in random 
order } The following formula for color difference is used to create the difference image:4 |(r1,g1,b1) 
 (r2,g2,b2)| = ((r1 r2)2 + (g1 )2)1/2 g2)2 + (b1 b2. In order to cover the canvas with paint, the canvas 
is initially painted a special color C such that the difference between C and any color is MAXINT. In 
practice, we avoid the overhead of storing and randomizing a large list of brush strokes by using a Z-buffer. 
Each stroke is rendered with a random Z value as soon as it is created. The Z-buffer is cleared before 
each layer. makeStroke() in the above code listing is a generic procedure that places a stroke on the 
canvas beginning at (x1,y1), given a reference image and a brush radius. fg is a constant grid size factor. 
Following [9], Figure 3(a) shows an image illustrated using a makeStroke() procedure which simply places 
a circle of the given radius at (x,y), using the color of the source image at location (x,y). Following 
[11], Figure 3(b) shows an image illustrated with short brush strokes, aligned to the normals of image 
gradients.5 Note the regular stroke appearance. In the next section, we will present an algorithm for 
placing long, curved brush strokes, closer to what one would find in a typical painting. Our technique 
focuses attention on areas of the image containing the most detail (high-frequency information) by 4 
We have also experimented with more perceptually correct metrics, such as distance in CIE LUV [6] space. 
Surprisingly, we found that these often gave worse results. 5 Note that no stroke clipping is used. Instead, 
small scale refinements of later layers automatically fix the edges of earlier layers.   placing many 
small brush strokes in these regions. Areas with little detail are painted only with very large brush 
strokes. Thus, strokes are appropriate to the level of detail in the source image. This choice of emphasis 
assumes that detail areas contain the most important visual information. Other choices of emphasis are 
also possible for example, emphasizing foreground elements or human figures but these would require 
semantic interpretation of the input images, which is known to be an extremely difficult problem in computer 
vision. The choice of emphasis could also be provided by a human user [16], or as output from a 3D renderer. 
 2.2 Creating curved brush strokes Individual brush strokes in a painting can convey shape, texture, 
overlap, and a variety of other image features. There is often something quite beautiful about a long, 
curved brush stroke that succinctly expresses a gesture, the curve of an object or the play of light 
on a surface. To our knowledge, all previous automatic painting systems use a series of small brush strokes, 
identical aside from color and orientation, or else apply pigment simultaneously to large regions of 
an image. In contrast, we present a method for painting long, continuous curves. In particular, we focus 
on painting solid strokes of constant thickness to approximate the coloration of the reference image; 
exploiting the full expressivity of brush strokes is far beyond the scope of this paper. We model brush 
strokes as anti-aliased cubic B-splines, each with a given color and thickness. Each stroke is rendered 
by dragging a circular brush mask along the sweep of the spline. In our system, we limit brush strokes 
to constant color, and use image gradients to guide stroke placement. Other authors have also used this 
concept [11,8,18] for placing strokes. The idea is that the strokes will represent contours of the image 
with roughly constant color. Our method is to place control points for the curve by following the normal 
of the gradient. When the color of the stroke deviates from the color under a control point of the curve 
by more than a specified threshold, the stroke ends at that control point. One can think of this as placing 
splines to roughly match the isocontours of the reference image. A more detailed explanation of the algorithm 
follows. The spline placement algorithm begins at a given point in the image (x0,y0), with a given a 
brush radius R. The stroke is represented as a list of control points, a color, and a brush radius. The 
control point (x0,y0) is added to the spline, and the color of the reference image at (x0,y0) is used 
as the color of the spline. We then need to compute the next point along the curve. The gradient (.0) 
for this point is computed from the Sobel­filtered luminance6 of the reference image. The next point 
(x1,y1) is placed in the direction (.0 + p/2) at a distance R from (x0,y0) (Figure 4(a)). We use the 
brush radius R as the distance between control points because R represents the level of detail we will 
capture with this brush size. This means that very large brushes create broad sketches of the image, 
to be later refined with smaller brushes. 6 The luminance of a pixel is computed with L(r,g,b) = 0.30*r 
+ 0.59*g + 0.11*b [6].  (a) (b) (c) (d) Figure 2: Painting with three brushes. (a) A source image. 
(b) The first layer of a painting, after painting with a circular brush of radius 8. (c) The image after 
painting with a brush of radius 4. (d) The final image, after painting with a brush of size 2. Note that 
brush strokes from earlier layers are still visible in the painting. (a) (b) Figure 3: Applying the 
multiscale algorithm to other types of brush strokes. Each of these paintings was created with brush 
strokes of radius 8, 4, and 2. (a) Brush strokes are circles, following [9]. (b) Brush strokes are short, 
anti­aliased lines placed normal to image gradients, following [11]. The line length is 4 times the brush 
radius. The remaining control points are computed by repeating this process of moving along the image 
normal to the image gradients and placing control points. The stroke is terminated when (a) the predetermined 
maximum stroke length is reached, or (b) the color of the stroke differs from the color under the last 
control point more than it differs from the current painting at that point. The maximum stroke length 
prevents an infinite loop from occurring. For a point (xi, yi), we compute a gradient direction .i at 
that point. Note, however, that there are actually two normal directions, and so two candidates for the 
next direction: .i + p/2, and .i -p/2. We choose the next direction so as to minimize the stroke curvature: 
we pick the direction Di so that the angle between Di and Di-1 is less than or equal to p/2. (Figure 
4(b)). We can also exaggerate or reduce the brush stroke curvature by applying an infinite impulse response 
filter to the stroke directions. The filter is controlled by a single predetermined filter constant, 
fc. Given the previous stroke direction D i-1 = (dx i-1, dy i-1), and a current stroke direction Di = 
(dxi,dyi), the filtered stroke direction is D i = fc Di + (1-fc) D i-1 = (fc dxi + (1-fc) dx i-1, fc 
dyi + (1-fc) dy i-1). A pseudocode summary of the entire stroke placement procedure follows. function 
makeSplineStroke(x0,y0,R,refImage) { strokeColor = refImage.color(x0,y0) K = a new stroke with radius 
R and color strokeColor add point (x0,y0) to K (x,y) := (x0,y0) (lastDx,lastDy) := (0,0) for i=1 to 
maxStrokeLength do { if (i > minStrokeLength and |refImage.color(x,y)-canvas.color(x,y)|< |refImage.color(x,y)-strokeColor|) 
then return K // detect vanishing gradient if (refImage.gradientMag(x,y) == 0) then return K // get 
unit vector of gradient (gx,gy) := refImage.gradientDirection(x,y) // compute a normal direction (dx,dy) 
:= (-gy, gx) // if necessary, reverse direction if (lastDx * dx + lastDy * dy < 0) then (dx,dy) := (-dx, 
-dy) // filter the stroke direction (dx,dy) :=fc*(dx,dy)+(1-fc)*(lastDx,lastDy) (dx,dy) := (dx,dy)/(dx2 
+ dy2)1/2 (x,y) := (x+R*dx, y+R*dy) (lastDx,lastDy) := (dx,dy) add the point (x,y) to K } return K 
} The minimum stroke length prevents the speckled appearance of very short strokes. To render a curved 
stroke, the spline is first computed by subdivision. An anti-aliased, circular mask is then drawn along 
the path of the curve. We have shown how to draw a long, curved brush stroke, to represent continuous 
color regions in an image. This method works best in combination with the layering method of Section 
2.1; see Figure 2(b) for an example of curved brush strokes without layering. In the future, we would 
like to enhance this (a) (b)  (c) Figure 4: Painting a brush stroke. (a) A brush stroke begins at a 
control point (x0,y0) and continues in direction D0, normal to the gradient G0. (b) From the second point 
(x1,y1), there are two normal directions to choose from: .1 + p/2, and .1 -p/2. We choose D1, in order 
to reduce the stroke curvature. (c) This procedure is repeated to draw the rest of the stroke. The stroke 
will be rendered as a cubic B-spline, with the (xi,yi) as control points. The distance between control 
points is equal to the brush radius. technique to depict other features, such as contours and texture, 
and to use a richer stroke model, including pressure, bristles, wetness, and tapering.  3. RENDERING 
STYLES There is no one right algorithm for non-photorealistic rendering, just as there is no right approach 
to painting. We believe that the graphic designer or artist using a rendering system should be allowed 
to vary the computer s artistic approach, rather than being forced to employ a single style of painting 
for every picture. In order to quantify the notion of painterly styles, we propose the use of style parameters 
to control the rendering process. These parameters should provide an intuitive way to vary visual qualities 
of the painting. Some possible style parameters include stroke curvature and how closely the painting 
should approximate the original. To be useful to a designer, style parameters should exhibit, as much 
as possible, the following four properties: Intuitiveness Each style parameter should correspond to 
a visual quality of the painting. These qualities should be intuitive to an artist without any technical 
computer knowledge.  Consistency Styles should produce the same visual character for different images. 
For example, we should be able to choose a style based on a single frame of a video sequence, and then 
render the rest of the sequence in the same style.  Robustness Each parameter should produce reasonable 
results over a predetermined range, without breaking for some values. A default value should be available, 
so that extra parameters provide the user with  more options without adding any extra burden. Increasing 
a parameter should always monotonically increase or decrease some quality of the painting, rather than 
cause it to fluctuate. Independence Style parameters should be independent of one another. Changing 
line thicknesses, for example, should not affect the saturation of an image. A group of style parameters 
describes a space of styles; a set of specific values can be encapsulated in a style. Styles may be designed 
to imitate the styles of famous artists, or may represent other approaches to painting. Styles can be 
collected into libraries, for later use by designers. Although there may conceivably be hundreds of rendering 
parameters, the designer need only adjust the parameters appropriate to an application. Some commercial 
painterly rendering products [14,22] provide the ability to vary rendering parameters and to save sets 
of parameters as distinct styles. 3.1 Some style parameters In the experiments that follow, we have used 
the following style parameters. Approximation threshold (T) How closely the painting must approximate 
the source image. Higher values of this threshold produce rougher paintings. (See Section 2.1)  Brush 
sizes Rather than requiring the user to provide a list of brush sizes (R1 ... Rn), we have found it 
more useful to use three parameters to specify brush sizes: Smallest brush radius (R1), Number of Brushes 
(n), and Size Ratio (Ri+1/Ri). We have found that a limited range of brush sizes often works best. (See 
Section 2.1)  Curvature Filter (fc) Used to limit or exaggerate stroke curvature. (See Section 2.2) 
 Blur Factor (fs) Controls the size of the blurring kernel. A small blur factor allows more noise in 
the image, and thus produces a more impressionistic image. (See Section 2.1)  Minimum and maximum stroke 
lengths  (minLength, maxLength) Used to restrict the possible stroke lengths. Very short strokes would 
be used in a pointillist image; long strokes would be used in a more expressionistic image. (See Section 
2.2) Opacity (a) Specifies the paint opacity, between 0 and 1. Lower opacity produces a wash-like effect. 
 Grid size (fg) Controls the spacing of brush strokes. The grid size times the brush radius (fg Ri) 
produces the step size in the paintLayer() procedure. (See Section 2.1)  Color Jitter Factors to randomly 
add jitter to hue (jh), saturation (js), value (jv), red (jr), green (jg) or blue (jb) color components. 
0 means no random jitter; larger values increase the factor.  The threshold (T) is defined in units 
of distance in color space. Brush sizes are defined in units of distance; we specify sizes in pixel units, 
although resolution-independent measures (such as inches or millimeters) would work equally well. Brush 
length is measured in the number of control points. The remaining parameters are dimensionless. 3.2 
Experiments In this section, we demonstrate four painting styles: Impressionist, Expressionist, Colorist 
Wash, and Pointillist. Figure 6 shows the application of the first three of these styles to two different 
images. The distinct character of each style demonstrates the consistency of the painting algorithm. 
(Figures 3(f) and 5 are also rendered in the Impressionist style.) Figure 7 shows a continuous transition 
between the Pointillist style and the Colorist Wash style. By interpolating style parameter values, we 
can interpolate the visual character of rendering styles. This demonstrates the robustness of the parameters. 
The styles are defined as follows. Impressionist A normal painting style, with no curvature filter, 
and no random color. T = 100, R=(8,4,2), fc=1, fs=.5, a=1, fg=1, minLength=4, maxLength=16  Expressionist 
 Elongated brush strokes. Jitter is added to color value. T = 50, R =(8,4,2), fc=.25, fs=.5, a=.7, fg=1, 
minLength=10, maxLength=16, jv=.5  Colorist Wash Loose, semi-transparent brush strokes. Random jitter 
is added to R, G, and B color components. T = 200, R=(8,4,2), fc=1, fs=.5, a=.5, fg=1, minLength=4, maxLength=16, 
jr =jg =jb=.3  Pointillist Densely-placed circles with random hue and saturation. T = 100, R=(4,2), 
fc=1, fs=.5, a=1, fg=.5, minLength=0, maxLength=0, jv=1, jh=.3. (This is similar to the Pointillist style 
provided by [22].)   4. DISCUSSION AND FUTURE WORK We have presented a new algorithm for producing 
paintings from images. Brush stroke sizes are selected to convey the level of detail present in the source 
image using a multiscale algorithm. Long, curved brush strokes are created by moving in a direction normal 
to image gradients. The painting may be made sketchier or more precise by changing a threshold parameter. 
Stroke curvature may be limited or exaggerated by filtering stroke direction. These and other parameters 
describe a space of rendering styles that can be created and modified by artists and graphic designers. 
Painting is a complex and rich pursuit, involving many approaches and many ways to interpret a scene. 
Our goal in developing painting algorithms is similar to a goal pursued by artists: to develop expressive 
visual languages. Future work in this area should extend the strategies available to non­photorealistic 
rendering algorithms, both image-based and model-based. We should be able to draw inspiration from various 
artistic approaches, as well as from computer vision, cognitive science, and artificial intelligence. 
Brush strokes may convey many physical properties such as color, texture, lighting, 3D shape, gesture, 
and overlap, as well as semantic elements such as emphasis, mood, and emotion. One long-term goal is 
to develop an approach to painting that will convey the important features of an image with carefully 
chosen brush strokes. A relaxation-based approach [8,18] may also be useful for computer painting. Although 
relaxation algorithms are usually more compute-intensive than are direct algorithms, they do allow many 
visual constraints to be embedded into a single energy function, some of which may be difficult to achieve 
by a direct method. Another interesting line of work is real-time processing of video [11] and models 
[12] with different styles. New techniques will be required to maintain temporal coherence for complex 
brush strokes with various size and shape attributes, while maintaining or changing rendering styles. 
One can envision a real-time interactive system in which the rendering style varies with the mood or 
the action.  (a) Figure 5: Two impressionist paintings.   Acknowledgements Many thanks to Ken Perlin 
for useful discussions and support throughout the course of this work. Thanks to Rich Radke, Jon Meyer, 
and Henning Biermann for discussions. The source images for Figures 5(b) and 7 were provided by Jon Meyer. 
The source image for Figure 6(b) was used by kind permission of CND, Inc. The author is supported by 
NSF grant DGE-9454173. 5. REFERENCES [1] ADOBE SYSTEMS. Adobe Photoshop 4.0 [2] DEBORAH F. BERMAN, JASON 
T. BARTELL, DAVID H. SALESIN. Multiresolution Painting and Compositing. SIGGRAPH 94 Conference Proceedings, 
pp. 85-90. July 1994. [3] PETER J. BURT AND EDWARD H. ADELSON. The Laplacian Pyramid as a Compact Image 
Code. IEEE Transactions on Communications. 31:532-540, April 1983. [4] HAROLD COHEN. The Further Exploits 
of Aaron, Painter. Stanford Humanities Review. Vol. 4, No. 2. pp. 141-158. 1995 [5] CASSIDY J. CURTIS, 
SEAN E. ANDERSON, JOSHUA E. SEIMS, KURT W. FLEISCHER, DAVID H. SALESIN. Computer-Generated Watercolor. 
SIGGRAPH 97 Conference Proceedings, pp. 421­ 430. August 1997. [6] JAMES FOLEY, ANDRIES VAN DAM, STEPHEN 
FEINER, JOHN HUGHES. Computer Graphics: Principles and Practice, Addison-Wesley, 1995. [7] FRACTAL DESIGN 
CORPORATION. Fractal Design Painter. [8] PAUL HAEBERLI. Paint by numbers: Abstract image representations. 
Computer Graphics (SIGGRAPH 90 Conference Proceedings), 24(4):207-214, August 1990 [9] PAUL HAEBERLI. 
The Impressionist. http://www.sgi.com/graphica/impression [10] JOHN LANSDOWN AND SIMON SCHOFIELD. Expressive 
rendering: A review of nonphotorealistic techniques. IEEE Computer Graphics and Applications, 15(3):29-37, 
May 1995. (b) [11] PETER LITWINOWICZ. Processing Images and Video for An Impressionist Effect. SIGGRAPH 
97 Conference Proceedings, pp. 407-414. August 1997. [12] LEE MARKOSIAN, MICHAEL A. KOWALSKI, SAMUEL 
J. TRYCHIN, LUBOMIR D. BOURDEV, DANIEL GOLDSTEIN, JOHN F. HUGHES. Real-Time Nonphotorealistic Rendering. 
SIGGRAPH 97 Conference Proceedings, pp. 415-420. August 1997. [13] PAMELA MCCORDUCK. AARON's CODE: Meta-Art, 
Artificial Intelligence, and the Work of Harold Cohen. New York: W. H. Freeman &#38; Co. 225 pages. 1991. 
[14] MICROSOFT CORPORATION. Microsoft Image Composer 1.5 [15] KEN PERLIN AND LUIZ VELHO. LivePaint: Painting 
with Procedural Multiscale Textures, SIGGRAPH 95 Conference Proceedings, pp. 153-160. 1995. [16] MICHAEL 
P. SALISBURY, MICHAEL T. WONG, JOHN F. HUGHES, DAVID H. SALESIN. Orientable Textures for Image-Based 
Pen-and-Ink Illustration. SIGGRAPH 97 Conference Proceedings, pp. 401-406. August 1997. [17] S. M. F. 
TREAVETT AND M. CHEN. Statistical Techniques for the Automated Synthesis of Non-Photorealistic Images. 
Proc. 15th Eurographics UK Conference, March 1997. [18] GREG TURK AND DAVID BANKS. Image-Guided Streamline 
Placement. SIGGRAPH 96 Conference Proceedings, pp. 453­ 460. August 1996. [19] JOACHIM WEICKERT, BART 
M. TER HAAR ROMNEY, MAX A. VIERGEVER. Efficient and reliable schemes for nonlinear diffusion filtering. 
IEEE Transactions on Image Processing. March 1998. [20] GEORGES WINKENBACH AND DAVID H. SALESIN. Computer-Generated 
Pen-and-Ink Illustration. SIGGRAPH 94 Conference Proceedings, pp. 91-100. July 1994. [21] GEORGES WINKENBACH 
AND DAVID H. SALESIN. Rendering Parametric Surfaces in Pen and Ink. SIGGRAPH 96 Conference Proceedings, 
pp. 469-476. August 1996. [22] XAOS TOOLS. Paint Alchemy 2.0  (a) (b) Figure 6: Applying different 
painterly styles. Left column: Impressionist. Middle column: Expressionist. Right column: Colorist Wash. 
Note that the styles have a consistent visual appearance when applied to different images.  (a) (b) 
(c) Figure 7: Interpolating rendering styles. Images (a) and (c) are rendered in the Colorist Wash and 
Pointillist styles, respectively. The average of their parameters was used to produce the style for (b). 
(The number of layers (n) was rounded up to 3.)  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1998</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
</content>
</proceeding>
